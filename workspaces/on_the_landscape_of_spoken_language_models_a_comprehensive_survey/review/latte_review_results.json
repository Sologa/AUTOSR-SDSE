[
  {
    "title": "Low Latency Time Domain Multichannel Speech and Music Source Separation",
    "abstract": "The Goal is to obtain a simple multichannel source separation with very low latency. Applications can be teleconferencing, hearing aids, augmented reality, or selective active noise cancellation. These real time applications need a very low latency, usually less than about 6 ms, and low complexity, because they usually run on small portable devices. For that we don't need the best separation, but \"useful\" separation, and not just on speech, but also music and noise. Usual frequency domain approaches have higher latency and complexity. Hence we introduce a novel probabilistic optimization method which we call \"Random Directions\", which can overcome local minima, applied to a simple time domain unmixing structure, and which is scalable for low complexity. Then it is compared to frequency domain approaches on separating speech and music sources, and using 3D microphone setups.",
    "metadata": {
      "arxiv_id": "2204.05609",
      "title": "Low Latency Time Domain Multichannel Speech and Music Source Separation",
      "summary": "The Goal is to obtain a simple multichannel source separation with very low latency. Applications can be teleconferencing, hearing aids, augmented reality, or selective active noise cancellation. These real time applications need a very low latency, usually less than about 6 ms, and low complexity, because they usually run on small portable devices. For that we don't need the best separation, but \"useful\" separation, and not just on speech, but also music and noise. Usual frequency domain approaches have higher latency and complexity. Hence we introduce a novel probabilistic optimization method which we call \"Random Directions\", which can overcome local minima, applied to a simple time domain unmixing structure, and which is scalable for low complexity. Then it is compared to frequency domain approaches on separating speech and music sources, and using 3D microphone setups.",
      "authors": [
        "Gerald Schuller"
      ],
      "published": "2022-04-12T08:17:01Z",
      "updated": "2022-04-12T08:17:01Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2204.05609v1",
      "landing_url": "https://arxiv.org/abs/2204.05609v1",
      "doi": "https://doi.org/10.48550/arXiv.2204.05609"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on low-latency time-domain multichannel speech/music source separation, not on Spoken Language Models or full-duplex speech interaction, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on low-latency time-domain multichannel speech/music source separation, not on Spoken Language Models or full-duplex speech interaction, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on low latency multichannel source separation of speech and music signals using a probabilistic optimization method, which is an audio signal processing technique. However, it does not address spoken language models, language modeling of speech, full-duplex spoken language interaction, or related conversational modeling aspects as required by the inclusion criteria. It neither discusses modeling, training, or evaluation of spoken language models nor does it emphasize synchronization in dialogue or turn-taking behavior. Hence, it does not meet the core thematic requirements for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on low latency multichannel source separation of speech and music signals using a probabilistic optimization method, which is an audio signal processing technique. However, it does not address spoken language models, language modeling of speech, full-duplex spoken language interaction, or related conversational modeling aspects as required by the inclusion criteria. It neither discusses modeling, training, or evaluation of spoken language models nor does it emphasize synchronization in dialogue or turn-taking behavior. Hence, it does not meet the core thematic requirements for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Sign Bit is Enough: A Learning Synchronization Framework for Multi-hop All-reduce with Ultimate Compression",
    "abstract": "Traditional one-bit compressed stochastic gradient descent can not be directly employed in multi-hop all-reduce, a widely adopted distributed training paradigm in network-intensive high-performance computing systems such as public clouds. According to our theoretical findings, due to the cascading compression, the training process has considerable deterioration on the convergence performance. To overcome this limitation, we implement a sign-bit compression-based learning synchronization framework, Marsit. It prevents cascading compression via an elaborate bit-wise operation for unbiased sign aggregation and its specific global compensation mechanism for mitigating compression deviation. The proposed framework retains the same theoretical convergence rate as non-compression mechanisms. Experimental results demonstrate that Marsit reduces up to 35% training time while preserving the same accuracy as training without compression.",
    "metadata": {
      "arxiv_id": "2204.06787",
      "title": "Sign Bit is Enough: A Learning Synchronization Framework for Multi-hop All-reduce with Ultimate Compression",
      "summary": "Traditional one-bit compressed stochastic gradient descent can not be directly employed in multi-hop all-reduce, a widely adopted distributed training paradigm in network-intensive high-performance computing systems such as public clouds. According to our theoretical findings, due to the cascading compression, the training process has considerable deterioration on the convergence performance. To overcome this limitation, we implement a sign-bit compression-based learning synchronization framework, Marsit. It prevents cascading compression via an elaborate bit-wise operation for unbiased sign aggregation and its specific global compensation mechanism for mitigating compression deviation. The proposed framework retains the same theoretical convergence rate as non-compression mechanisms. Experimental results demonstrate that Marsit reduces up to 35% training time while preserving the same accuracy as training without compression.",
      "authors": [
        "Feijie Wu",
        "Shiqi He",
        "Song Guo",
        "Zhihao Qu",
        "Haozhao Wang",
        "Weihua Zhuang",
        "Jie Zhang"
      ],
      "published": "2022-04-14T06:54:32Z",
      "updated": "2022-04-14T06:54:32Z",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2204.06787v1",
      "landing_url": "https://arxiv.org/abs/2204.06787v1",
      "doi": "https://doi.org/10.1145/3489517.3530417"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on sign-bit gradient compression for multi-hop all-reduce in distributed ML training and does not address Spoken Language Models or speech modeling, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on sign-bit gradient compression for multi-hop all-reduce in distributed ML training and does not address Spoken Language Models or speech modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on a compression framework for distributed training in high-performance computing, specifically addressing stochastic gradient descent in multi-hop all-reduce scenarios, without any mention of spoken language models, speech language models, full-duplex dialogue, cognitive parallelism, or speech-centric modeling and inference as required. It lacks discussion on speech-based modeling or synchronization in a spoken language context and does not address the key concepts or evaluation metrics stipulated in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on a compression framework for distributed training in high-performance computing, specifically addressing stochastic gradient descent in multi-hop all-reduce scenarios, without any mention of spoken language models, speech language models, full-duplex dialogue, cognitive parallelism, or speech-centric modeling and inference as required. It lacks discussion on speech-based modeling or synchronization in a spoken language context and does not address the key concepts or evaluation metrics stipulated in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Revisiting Vicinal Risk Minimization for Partially Supervised Multi-Label Classification Under Data Scarcity",
    "abstract": "Due to the high human cost of annotation, it is non-trivial to curate a large-scale medical dataset that is fully labeled for all classes of interest. Instead, it would be convenient to collect multiple small partially labeled datasets from different matching sources, where the medical images may have only been annotated for a subset of classes of interest. This paper offers an empirical understanding of an under-explored problem, namely partially supervised multi-label classification (PSMLC), where a multi-label classifier is trained with only partially labeled medical images. In contrast to the fully supervised counterpart, the partial supervision caused by medical data scarcity has non-trivial negative impacts on the model performance. A potential remedy could be augmenting the partial labels. Though vicinal risk minimization (VRM) has been a promising solution to improve the generalization ability of the model, its application to PSMLC remains an open question. To bridge the methodological gap, we provide the first VRM-based solution to PSMLC. The empirical results also provide insights into future research directions on partially supervised learning under data scarcity.",
    "metadata": {
      "arxiv_id": "2204.08954",
      "title": "Revisiting Vicinal Risk Minimization for Partially Supervised Multi-Label Classification Under Data Scarcity",
      "summary": "Due to the high human cost of annotation, it is non-trivial to curate a large-scale medical dataset that is fully labeled for all classes of interest. Instead, it would be convenient to collect multiple small partially labeled datasets from different matching sources, where the medical images may have only been annotated for a subset of classes of interest. This paper offers an empirical understanding of an under-explored problem, namely partially supervised multi-label classification (PSMLC), where a multi-label classifier is trained with only partially labeled medical images. In contrast to the fully supervised counterpart, the partial supervision caused by medical data scarcity has non-trivial negative impacts on the model performance. A potential remedy could be augmenting the partial labels. Though vicinal risk minimization (VRM) has been a promising solution to improve the generalization ability of the model, its application to PSMLC remains an open question. To bridge the methodological gap, we provide the first VRM-based solution to PSMLC. The empirical results also provide insights into future research directions on partially supervised learning under data scarcity.",
      "authors": [
        "Nanqing Dong",
        "Jiayi Wang",
        "Irina Voiculescu"
      ],
      "published": "2022-04-19T15:50:16Z",
      "updated": "2022-04-19T15:50:16Z",
      "categories": [
        "cs.LG",
        "cs.CV",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2204.08954v1",
      "landing_url": "https://arxiv.org/abs/2204.08954v1",
      "doi": "https://doi.org/10.48550/arXiv.2204.08954"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper addresses partially supervised multi-label learning in medical imaging, with no spoken language modeling or speech interaction, hence it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper addresses partially supervised multi-label learning in medical imaging, with no spoken language modeling or speech interaction, hence it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on partially supervised multi-label classification in medical imaging, without any mention of spoken language models, speech language modeling, full-duplex spoken language models, or related concepts such as speech-LLM architectures and synchronous interactive behaviors. The core topic and methodology do not involve speech modeling or related synchronization/interactivity aspects required by the inclusion criteria, nor does it address evaluation metrics related to real-time interaction or cognitive parallelism. Therefore, it does not meet the inclusion criteria and falls outside the scope defined for spoken language model research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on partially supervised multi-label classification in medical imaging, without any mention of spoken language models, speech language modeling, full-duplex spoken language models, or related concepts such as speech-LLM architectures and synchronous interactive behaviors. The core topic and methodology do not involve speech modeling or related synchronization/interactivity aspects required by the inclusion criteria, nor does it address evaluation metrics related to real-time interaction or cognitive parallelism. Therefore, it does not meet the inclusion criteria and falls outside the scope defined for spoken language model research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Gated Multimodal Fusion with Contrastive Learning for Turn-taking Prediction in Human-robot Dialogue",
    "abstract": "Turn-taking, aiming to decide when the next speaker can start talking, is an essential component in building human-robot spoken dialogue systems. Previous studies indicate that multimodal cues can facilitate this challenging task. However, due to the paucity of public multimodal datasets, current methods are mostly limited to either utilizing unimodal features or simplistic multimodal ensemble models. Besides, the inherent class imbalance in real scenario, e.g. sentence ending with short pause will be mostly regarded as the end of turn, also poses great challenge to the turn-taking decision. In this paper, we first collect a large-scale annotated corpus for turn-taking with over 5,000 real human-robot dialogues in speech and text modalities. Then, a novel gated multimodal fusion mechanism is devised to utilize various information seamlessly for turn-taking prediction. More importantly, to tackle the data imbalance issue, we design a simple yet effective data augmentation method to construct negative instances without supervision and apply contrastive learning to obtain better feature representations. Extensive experiments are conducted and the results demonstrate the superiority and competitiveness of our model over several state-of-the-art baselines.",
    "metadata": {
      "arxiv_id": "2204.10172",
      "title": "Gated Multimodal Fusion with Contrastive Learning for Turn-taking Prediction in Human-robot Dialogue",
      "summary": "Turn-taking, aiming to decide when the next speaker can start talking, is an essential component in building human-robot spoken dialogue systems. Previous studies indicate that multimodal cues can facilitate this challenging task. However, due to the paucity of public multimodal datasets, current methods are mostly limited to either utilizing unimodal features or simplistic multimodal ensemble models. Besides, the inherent class imbalance in real scenario, e.g. sentence ending with short pause will be mostly regarded as the end of turn, also poses great challenge to the turn-taking decision. In this paper, we first collect a large-scale annotated corpus for turn-taking with over 5,000 real human-robot dialogues in speech and text modalities. Then, a novel gated multimodal fusion mechanism is devised to utilize various information seamlessly for turn-taking prediction. More importantly, to tackle the data imbalance issue, we design a simple yet effective data augmentation method to construct negative instances without supervision and apply contrastive learning to obtain better feature representations. Extensive experiments are conducted and the results demonstrate the superiority and competitiveness of our model over several state-of-the-art baselines.",
      "authors": [
        "Jiudong Yang",
        "Peiying Wang",
        "Yi Zhu",
        "Mingchao Feng",
        "Meng Chen",
        "Xiaodong He"
      ],
      "published": "2022-04-18T05:18:00Z",
      "updated": "2022-04-18T05:18:00Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2204.10172v1",
      "landing_url": "https://arxiv.org/abs/2204.10172v1",
      "doi": "https://doi.org/10.48550/arXiv.2204.10172"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "該論文聚焦於人機對話中的發話輪替預測，使用多模态特徵與對比學習，但未在標題/摘要中以 Spoken Language Models / Speech Language Models 為核心，亦未清楚呈現語音作為主要建模對象的端到端或混合式架構訓練與評估；因此不符合「以語音語言模型為核心且可被明確評估的模型架構/訓練/評估」的納入條件，即屬於高階的多模态對話任務而非語音語言建模的研究。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "該論文聚焦於人機對話中的發話輪替預測，使用多模态特徵與對比學習，但未在標題/摘要中以 Spoken Language Models / Speech Language Models 為核心，亦未清楚呈現語音作為主要建模對象的端到端或混合式架構訓練與評估；因此不符合「以語音語言模型為核心且可被明確評估的模型架構/訓練/評估」的納入條件，即屬於高階的多模态對話任務而非語音語言建模的研究。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on turn-taking prediction in human-robot dialogue using multimodal fusion, primarily leveraging speech and text modalities. However, although it addresses turn-taking within half-duplex interaction, it does not explicitly describe modeling or training of spoken language models or speech language models as core components, nor mention full-duplex or engineered/learned synchronization approaches. The abstract discusses multimodal fusion and data augmentation but lacks mention of explicit spoken language modeling or end-to-end speech-based modeling frameworks, which are key inclusion criteria. Therefore, the study partially matches the theme but fails to meet the strict definition of spoken language models and full-duplex interaction focus required. Hence, it is better to exclude based on the criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on turn-taking prediction in human-robot dialogue using multimodal fusion, primarily leveraging speech and text modalities. However, although it addresses turn-taking within half-duplex interaction, it does not explicitly describe modeling or training of spoken language models or speech language models as core components, nor mention full-duplex or engineered/learned synchronization approaches. The abstract discusses multimodal fusion and data augmentation but lacks mention of explicit spoken language modeling or end-to-end speech-based modeling frameworks, which are key inclusion criteria. Therefore, the study partially matches the theme but fails to meet the strict definition of spoken language models and full-duplex interaction focus required. Hence, it is better to exclude based on the criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Training High-Performance Low-Latency Spiking Neural Networks by Differentiation on Spike Representation",
    "abstract": "Spiking Neural Network (SNN) is a promising energy-efficient AI model when implemented on neuromorphic hardware. However, it is a challenge to efficiently train SNNs due to their non-differentiability. Most existing methods either suffer from high latency (i.e., long simulation time steps), or cannot achieve as high performance as Artificial Neural Networks (ANNs). In this paper, we propose the Differentiation on Spike Representation (DSR) method, which could achieve high performance that is competitive to ANNs yet with low latency. First, we encode the spike trains into spike representation using (weighted) firing rate coding. Based on the spike representation, we systematically derive that the spiking dynamics with common neural models can be represented as some sub-differentiable mapping. With this viewpoint, our proposed DSR method trains SNNs through gradients of the mapping and avoids the common non-differentiability problem in SNN training. Then we analyze the error when representing the specific mapping with the forward computation of the SNN. To reduce such error, we propose to train the spike threshold in each layer, and to introduce a new hyperparameter for the neural models. With these components, the DSR method can achieve state-of-the-art SNN performance with low latency on both static and neuromorphic datasets, including CIFAR-10, CIFAR-100, ImageNet, and DVS-CIFAR10.",
    "metadata": {
      "arxiv_id": "2205.00459",
      "title": "Training High-Performance Low-Latency Spiking Neural Networks by Differentiation on Spike Representation",
      "summary": "Spiking Neural Network (SNN) is a promising energy-efficient AI model when implemented on neuromorphic hardware. However, it is a challenge to efficiently train SNNs due to their non-differentiability. Most existing methods either suffer from high latency (i.e., long simulation time steps), or cannot achieve as high performance as Artificial Neural Networks (ANNs). In this paper, we propose the Differentiation on Spike Representation (DSR) method, which could achieve high performance that is competitive to ANNs yet with low latency. First, we encode the spike trains into spike representation using (weighted) firing rate coding. Based on the spike representation, we systematically derive that the spiking dynamics with common neural models can be represented as some sub-differentiable mapping. With this viewpoint, our proposed DSR method trains SNNs through gradients of the mapping and avoids the common non-differentiability problem in SNN training. Then we analyze the error when representing the specific mapping with the forward computation of the SNN. To reduce such error, we propose to train the spike threshold in each layer, and to introduce a new hyperparameter for the neural models. With these components, the DSR method can achieve state-of-the-art SNN performance with low latency on both static and neuromorphic datasets, including CIFAR-10, CIFAR-100, ImageNet, and DVS-CIFAR10.",
      "authors": [
        "Qingyan Meng",
        "Mingqing Xiao",
        "Shen Yan",
        "Yisen Wang",
        "Zhouchen Lin",
        "Zhi-Quan Luo"
      ],
      "published": "2022-05-01T12:44:49Z",
      "updated": "2023-03-30T07:12:36Z",
      "categories": [
        "cs.NE",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.00459v2",
      "landing_url": "https://arxiv.org/abs/2205.00459v2",
      "doi": "https://doi.org/10.48550/arXiv.2205.00459"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article focuses on training spiking neural networks (SNNs) and spike representation, not on Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article focuses on training spiking neural networks (SNNs) and spike representation, not on Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on training spiking neural networks for image classification tasks (CIFAR-10, ImageNet, DVS-CIFAR10) with emphasis on low latency and high performance, but it does not address Spoken Language Models, speech as the primary modeling object, full-duplex speech interaction, or related evaluation criteria specified in the inclusion criteria. It lacks any mention of speech language modeling or speech-based interaction, and thus does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on training spiking neural networks for image classification tasks (CIFAR-10, ImageNet, DVS-CIFAR10) with emphasis on low latency and high performance, but it does not address Spoken Language Models, speech as the primary modeling object, full-duplex speech interaction, or related evaluation criteria specified in the inclusion criteria. It lacks any mention of speech language modeling or speech-based interaction, and thus does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Ultra-Reliable Low-Latency Millimeter-Wave Communications with Sliding Window Network Coding",
    "abstract": "Ultra-reliability and low-latency are pivotal requirements of the new 6th generation of communication systems (xURLLC). Over the past years, to increase throughput, adaptive active antennas were introduced in advanced wireless communications, specifically in the domain of millimeter-wave (mmWave). Consequently, new lower-layer techniques were proposed to cope with practical challenges of high dimensional and electronically-steerable beams. The transition from omni-directional to highly directional antennas presents a new type of wireless systems that deliver high bandwidth, but that are susceptible to high losses and high latency variation. Classical approaches cannot close the rising gap between high throughput and low delay in those advanced systems. In this work, we incorporate effective sliding window network coding solutions in mmWave communications. While legacy systems such as rateless codes improve delay, cross-layer results show that they do not provide low latency communications (LLC - below 10 ms), due to the lossy behaviour of mmWave channel and the lower-layers' retransmission mechanisms. On the other hand, fixed sliding window random linear network coding (RLNC) is able to achieve LLC, and even better, adaptive sliding window RLNC obtains ultra-reliable LLC (Ultra-Reliable and Low-Latency Communications (URLLC) - LLC with maximum delay below 10 ms with more than 99% success rate).",
    "metadata": {
      "arxiv_id": "2205.00793",
      "title": "Ultra-Reliable Low-Latency Millimeter-Wave Communications with Sliding Window Network Coding",
      "summary": "Ultra-reliability and low-latency are pivotal requirements of the new 6th generation of communication systems (xURLLC). Over the past years, to increase throughput, adaptive active antennas were introduced in advanced wireless communications, specifically in the domain of millimeter-wave (mmWave). Consequently, new lower-layer techniques were proposed to cope with practical challenges of high dimensional and electronically-steerable beams. The transition from omni-directional to highly directional antennas presents a new type of wireless systems that deliver high bandwidth, but that are susceptible to high losses and high latency variation. Classical approaches cannot close the rising gap between high throughput and low delay in those advanced systems. In this work, we incorporate effective sliding window network coding solutions in mmWave communications. While legacy systems such as rateless codes improve delay, cross-layer results show that they do not provide low latency communications (LLC - below 10 ms), due to the lossy behaviour of mmWave channel and the lower-layers' retransmission mechanisms. On the other hand, fixed sliding window random linear network coding (RLNC) is able to achieve LLC, and even better, adaptive sliding window RLNC obtains ultra-reliable LLC (Ultra-Reliable and Low-Latency Communications (URLLC) - LLC with maximum delay below 10 ms with more than 99% success rate).",
      "authors": [
        "Eurico Dias",
        "Duarte Raposo",
        "Homa Esfahanizadeh",
        "Alejandro Cohen",
        "Tânia Ferreira",
        "Miguel Luís",
        "Susana Sargento",
        "Muriel Médard"
      ],
      "published": "2022-05-02T10:25:38Z",
      "updated": "2022-09-15T08:55:06Z",
      "categories": [
        "cs.IT",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.00793v2",
      "landing_url": "https://arxiv.org/abs/2205.00793v2",
      "doi": "https://doi.org/10.48550/arXiv.2205.00793"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about mmWave networking and sliding window network coding for low-latency communications, which does not involve Spoken Language Models or speech-language modeling; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about mmWave networking and sliding window network coding for low-latency communications, which does not involve Spoken Language Models or speech-language modeling; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on ultra-reliable low-latency millimeter-wave communications using sliding window network coding, with no mention of spoken language models, speech language models, or any speech-based modeling or reasoning. The study does not discuss speech as the main modeling object, full-duplex spoken language models, synchronization in speech interactions, or any related evaluation metrics as per the inclusion criteria. It centers on wireless communication techniques rather than speech technology or spoken language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on ultra-reliable low-latency millimeter-wave communications using sliding window network coding, with no mention of spoken language models, speech language models, or any speech-based modeling or reasoning. The study does not discuss speech as the main modeling object, full-duplex spoken language models, synchronization in speech interactions, or any related evaluation metrics as per the inclusion criteria. It centers on wireless communication techniques rather than speech technology or spoken language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Music-Therapy Robotic Platform for Children with Autism: A Pilot Study",
    "abstract": "Children with Autism Spectrum Disorder (ASD) experience deficits in verbal and nonverbal communication skills including motor control, turn-taking, and emotion recognition. Innovative technology, such as socially assistive robots, has shown to be a viable method for Autism therapy. This paper presents a novel robot-based music-therapy platform for modeling and improving the social responses and behaviors of children with ASD. Our autonomous social interactive system consists of three modules. We adopted Short-time Fourier Transform and Levenshtein distance to fulfill the design requirements: a) \"music detection\" and b) \"smart scoring and feedback\", which allows NAO to understand music and provide additional practice and oral feedback to the users as applicable. We designed and implemented six Human-Robot-Interaction (HRI) sessions including four intervention sessions. Nine children with ASD and seven Typically Developing participated in a total of fifty HRI experimental sessions. Using our platform, we collected and analyzed data on social behavioral changes and emotion recognition using Electrodermal Activity (EDA) signals. The results of our experiments demonstrate most of the participants were able to complete motor control tasks with ~70% accuracy. Six out of the 9 ASD participants showed stable turn-taking behavior when playing music. The results of automated emotion classification using Support Vector Machines illustrate that emotional arousal in the ASD group can be detected and well recognized via EDA bio-signals. In summary, the results of our data analyses, including emotion classification using EDA signals, indicate that the proposed robot-music based therapy platform is an attractive and promising assistive tool to facilitate the improvement of fine motor control and turn-taking skills in children with ASD.",
    "metadata": {
      "arxiv_id": "2205.04251",
      "title": "A Music-Therapy Robotic Platform for Children with Autism: A Pilot Study",
      "summary": "Children with Autism Spectrum Disorder (ASD) experience deficits in verbal and nonverbal communication skills including motor control, turn-taking, and emotion recognition. Innovative technology, such as socially assistive robots, has shown to be a viable method for Autism therapy. This paper presents a novel robot-based music-therapy platform for modeling and improving the social responses and behaviors of children with ASD. Our autonomous social interactive system consists of three modules. We adopted Short-time Fourier Transform and Levenshtein distance to fulfill the design requirements: a) \"music detection\" and b) \"smart scoring and feedback\", which allows NAO to understand music and provide additional practice and oral feedback to the users as applicable. We designed and implemented six Human-Robot-Interaction (HRI) sessions including four intervention sessions. Nine children with ASD and seven Typically Developing participated in a total of fifty HRI experimental sessions. Using our platform, we collected and analyzed data on social behavioral changes and emotion recognition using Electrodermal Activity (EDA) signals. The results of our experiments demonstrate most of the participants were able to complete motor control tasks with ~70% accuracy. Six out of the 9 ASD participants showed stable turn-taking behavior when playing music. The results of automated emotion classification using Support Vector Machines illustrate that emotional arousal in the ASD group can be detected and well recognized via EDA bio-signals. In summary, the results of our data analyses, including emotion classification using EDA signals, indicate that the proposed robot-music based therapy platform is an attractive and promising assistive tool to facilitate the improvement of fine motor control and turn-taking skills in children with ASD.",
      "authors": [
        "Huanghao Fengr",
        "Mohammad H. Mahoor",
        "Francesca Dino"
      ],
      "published": "2022-05-09T13:03:56Z",
      "updated": "2022-05-09T13:03:56Z",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.04251v1",
      "landing_url": "https://arxiv.org/abs/2205.04251v1",
      "doi": "https://doi.org/10.48550/arXiv.2205.04251"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on robot-based music therapy for children with ASD and does not address Spoken Language Models or any speech-language modeling; therefore it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on robot-based music therapy for children with ASD and does not address Spoken Language Models or any speech-language modeling; therefore it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a robot-based music therapy for children with autism, emphasizing motor control and social behaviors, without explicit discussion or modeling of spoken language, speech-based language models, or full-duplex spoken language interactions. The research does not address core inclusion concepts such as spoken language modeling, speech language model architectures, synchronization in speech interaction, or low-latency dialogue management, and does not use spoken language model terminology in title or abstract, thus it does not meet the inclusion criteria nor address exclusion criteria in a relevant way.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a robot-based music therapy for children with autism, emphasizing motor control and social behaviors, without explicit discussion or modeling of spoken language, speech-based language models, or full-duplex spoken language interactions. The research does not address core inclusion concepts such as spoken language modeling, speech language model architectures, synchronization in speech interaction, or low-latency dialogue management, and does not use spoken language model terminology in title or abstract, thus it does not meet the inclusion criteria nor address exclusion criteria in a relevant way.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Microfluidic cell engineering on high-density microelectrode arrays for assessing structure-function relationships in living neuronal networks",
    "abstract": "Neuronal networks in dissociated culture combined with cell engineering technology offer a pivotal platform to constructively explore the relationship between structure and function in living neuronal networks. Here, we fabricated defined neuronal networks possessing a modular architecture on high-density microelectrode arrays (HD-MEAs), a state-of-the-art electrophysiological tool for recording neural activity with high spatial and temporal resolutions. We first established a surface coating protocol using a cell-permissive hydrogel to stably attach polydimethylsiloxane microfluidic film on the HD-MEA. We then recorded the spontaneous neural activity of the engineered neuronal network, which revealed an important portrait of the engineered neuronal network--modular architecture enhances functional complexity by reducing the excessive neural correlation between spatially segregated modules. The results of this study highlight the impact of HD-MEA recordings combined with cell engineering technologies as a novel tool in neuroscience to constructively assess the structure-function relationships in neuronal networks.",
    "metadata": {
      "arxiv_id": "2205.04342",
      "title": "Microfluidic cell engineering on high-density microelectrode arrays for assessing structure-function relationships in living neuronal networks",
      "summary": "Neuronal networks in dissociated culture combined with cell engineering technology offer a pivotal platform to constructively explore the relationship between structure and function in living neuronal networks. Here, we fabricated defined neuronal networks possessing a modular architecture on high-density microelectrode arrays (HD-MEAs), a state-of-the-art electrophysiological tool for recording neural activity with high spatial and temporal resolutions. We first established a surface coating protocol using a cell-permissive hydrogel to stably attach polydimethylsiloxane microfluidic film on the HD-MEA. We then recorded the spontaneous neural activity of the engineered neuronal network, which revealed an important portrait of the engineered neuronal network--modular architecture enhances functional complexity by reducing the excessive neural correlation between spatially segregated modules. The results of this study highlight the impact of HD-MEA recordings combined with cell engineering technologies as a novel tool in neuroscience to constructively assess the structure-function relationships in neuronal networks.",
      "authors": [
        "Yuya Sato",
        "Hideaki Yamamoto",
        "Hideyuki Kato",
        "Takashi Tanii",
        "Shigeo Sato",
        "Ayumi Hirano-Iwata"
      ],
      "published": "2022-05-09T14:43:08Z",
      "updated": "2022-05-11T11:31:37Z",
      "categories": [
        "q-bio.NC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.04342v2",
      "landing_url": "https://arxiv.org/abs/2205.04342v2",
      "doi": "https://doi.org/10.3389/fnins.2022.943310"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper concerns neuronal network engineering and HD-MEA electrophysiology, not spoken language models or speech-language modeling, thus it fails all inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper concerns neuronal network engineering and HD-MEA electrophysiology, not spoken language models or speech-language modeling, thus it fails all inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on microfluidic cell engineering and neuronal networks assessed by high-density microelectrode arrays, which is unrelated to spoken language models, speech language modeling, or full-duplex spoken language interaction systems as specified by the inclusion criteria. The research neither addresses speech sequence modeling, speech-LLM architectures, nor any interactive spoken language behaviors, so it does not meet any of the inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on microfluidic cell engineering and neuronal networks assessed by high-density microelectrode arrays, which is unrelated to spoken language models, speech language modeling, or full-duplex spoken language interaction systems as specified by the inclusion criteria. The research neither addresses speech sequence modeling, speech-LLM architectures, nor any interactive spoken language behaviors, so it does not meet any of the inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LabVIEW is faster and C is economical interfacing tool for UCT automation",
    "abstract": "An in-house developed 2D ultrasound computerized Tomography system is fully automated. Performance analysis of instrument and software interfacing soft tools, namely the LabVIEW, MATLAB, C, and Python, is presented. The instrument interfacing algorithms, hardware control algorithms, signal processing, and analysis codes are written using above mentioned soft tool platforms. Total of eight performance indices are used to compare the ease of (a) realtime control of electromechanical assembly, (b) sensors, instruments integration, (c) synchronized data acquisition, and (d) simultaneous raw data processing. It is found that C utilizes the least processing power and performs a lower number of processes to perform the same task. In runtime analysis (data acquisition and realtime control), LabVIEW performs best, taking 365.69s in comparison to MATLAB (623.83s), Python ( 1505.54s), and C (1252.03s) to complete the experiment. Python performs better in establishing faster interfacing and minimum RAM usage. LabVIEW is recommended for its fast process execution. C is recommended for the most economical implementation. Python is recommended for complex system automation having a very large number of components involved. This article provides a methodology to select optimal soft tools for instrument automation-related aspects.",
    "metadata": {
      "arxiv_id": "2205.08260",
      "title": "LabVIEW is faster and C is economical interfacing tool for UCT automation",
      "summary": "An in-house developed 2D ultrasound computerized Tomography system is fully automated. Performance analysis of instrument and software interfacing soft tools, namely the LabVIEW, MATLAB, C, and Python, is presented. The instrument interfacing algorithms, hardware control algorithms, signal processing, and analysis codes are written using above mentioned soft tool platforms. Total of eight performance indices are used to compare the ease of (a) realtime control of electromechanical assembly, (b) sensors, instruments integration, (c) synchronized data acquisition, and (d) simultaneous raw data processing. It is found that C utilizes the least processing power and performs a lower number of processes to perform the same task. In runtime analysis (data acquisition and realtime control), LabVIEW performs best, taking 365.69s in comparison to MATLAB (623.83s), Python ( 1505.54s), and C (1252.03s) to complete the experiment. Python performs better in establishing faster interfacing and minimum RAM usage. LabVIEW is recommended for its fast process execution. C is recommended for the most economical implementation. Python is recommended for complex system automation having a very large number of components involved. This article provides a methodology to select optimal soft tools for instrument automation-related aspects.",
      "authors": [
        "Ankur Kumar",
        "Mayank Goswami"
      ],
      "published": "2022-05-17T12:00:25Z",
      "updated": "2022-05-17T12:00:25Z",
      "categories": [
        "cs.PL",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.08260v1",
      "landing_url": "https://arxiv.org/abs/2205.08260v1",
      "doi": "https://doi.org/10.1038/s41598-023-45849-y"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses instrument automation tools (LabVIEW, MATLAB, C, Python) for a 2D ultrasound CT system and performance of interfacing software, with no focus on Spoken Language Models or speech-language modeling concepts or evaluation of interactive speech systems.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses instrument automation tools (LabVIEW, MATLAB, C, Python) for a 2D ultrasound CT system and performance of interfacing software, with no focus on Spoken Language Models or speech-language modeling concepts or evaluation of interactive speech systems.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on performance comparison of software tools for ultrasound tomography automation, without any mention of spoken language models, speech language modeling, full-duplex interaction, or related speech-centric modeling concepts required by the inclusion criteria. It also lacks any discussion related to speech sequence modeling, real-time spoken language interaction, or evaluations aligned with speech-language benchmarks, hence it does not meet the inclusion criteria and falls under exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on performance comparison of software tools for ultrasound tomography automation, without any mention of spoken language models, speech language modeling, full-duplex interaction, or related speech-centric modeling concepts required by the inclusion criteria. It also lacks any discussion related to speech sequence modeling, real-time spoken language interaction, or evaluations aligned with speech-language benchmarks, hence it does not meet the inclusion criteria and falls under exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Static analysis tools in the era of cloud-native systems",
    "abstract": "Microservices fuel cloud-native systems with small service sets developed and deployed independently. The independent nature of this modular architecture also leads to challenges and gaps. The intended system design might deviate far from what is eventually produced and maintained as the architecture tends to degrade over time. This paper challenges the audience on how static analysis could contribute to microservice system development and management, particularly managing architectural degradation. It elaborates on challenges and needed changes in the traditional code analysis to better fit these systems and discusses implications for practitioners once robust static analysis tools become available",
    "metadata": {
      "arxiv_id": "2205.08527",
      "title": "Static analysis tools in the era of cloud-native systems",
      "summary": "Microservices fuel cloud-native systems with small service sets developed and deployed independently. The independent nature of this modular architecture also leads to challenges and gaps. The intended system design might deviate far from what is eventually produced and maintained as the architecture tends to degrade over time. This paper challenges the audience on how static analysis could contribute to microservice system development and management, particularly managing architectural degradation. It elaborates on challenges and needed changes in the traditional code analysis to better fit these systems and discusses implications for practitioners once robust static analysis tools become available",
      "authors": [
        "Tomas Cerny",
        "Davide Taibi"
      ],
      "published": "2022-05-17T17:52:24Z",
      "updated": "2022-05-17T17:52:24Z",
      "categories": [
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.08527v1",
      "landing_url": "https://arxiv.org/abs/2205.08527v1",
      "doi": "https://doi.org/10.48550/arXiv.2205.08527"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on static analysis tools for cloud-native microservices and architectural degradation, which is not about Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on static analysis tools for cloud-native microservices and architectural degradation, which is not about Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on static analysis tools for cloud-native microservices systems and architectural management, which is unrelated to spoken language models or speech language modeling as required by the inclusion criteria; it does not mention SLMs, speech modeling, or relevant interactive speech features, thus failing to meet inclusion criteria and fitting exclusion conditions.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on static analysis tools for cloud-native microservices systems and architectural management, which is unrelated to spoken language models or speech language modeling as required by the inclusion criteria; it does not mention SLMs, speech modeling, or relevant interactive speech features, thus failing to meet inclusion criteria and fitting exclusion conditions.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Voice Activity Projection: Self-supervised Learning of Turn-taking Events",
    "abstract": "The modeling of turn-taking in dialog can be viewed as the modeling of the dynamics of voice activity of the interlocutors. We extend prior work and define the predictive task of Voice Activity Projection, a general, self-supervised objective, as a way to train turn-taking models without the need of labeled data. We highlight a theoretical weakness with prior approaches, arguing for the need of modeling the dependency of voice activity events in the projection window. We propose four zero-shot tasks, related to the prediction of upcoming turn-shifts and backchannels, and show that the proposed model outperforms prior work.",
    "metadata": {
      "arxiv_id": "2205.09812",
      "title": "Voice Activity Projection: Self-supervised Learning of Turn-taking Events",
      "summary": "The modeling of turn-taking in dialog can be viewed as the modeling of the dynamics of voice activity of the interlocutors. We extend prior work and define the predictive task of Voice Activity Projection, a general, self-supervised objective, as a way to train turn-taking models without the need of labeled data. We highlight a theoretical weakness with prior approaches, arguing for the need of modeling the dependency of voice activity events in the projection window. We propose four zero-shot tasks, related to the prediction of upcoming turn-shifts and backchannels, and show that the proposed model outperforms prior work.",
      "authors": [
        "Erik Ekstedt",
        "Gabriel Skantze"
      ],
      "published": "2022-05-19T19:16:45Z",
      "updated": "2022-05-19T19:16:45Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.09812v1",
      "landing_url": "https://arxiv.org/abs/2205.09812v1",
      "doi": "https://doi.org/10.48550/arXiv.2205.09812"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on voice activity dynamics and turn-taking in dialog, proposing self-supervised tasks for predicting turn-shifts/backchannels and discussing model architecture/training/evaluation, which aligns with Spoken Language Models/FD-SLMs and full-duplex interaction, within the 3-year window.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item centers on voice activity dynamics and turn-taking in dialog, proposing self-supervised tasks for predicting turn-shifts/backchannels and discussing model architecture/training/evaluation, which aligns with Spoken Language Models/FD-SLMs and full-duplex interaction, within the 3-year window.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning of turn-taking behavior by modeling voice activity dynamics, highlighting predictions of upcoming turn-shifts and backchannels, which relates to speech-level interaction modeling. However, the abstract does not explicitly mention Spoken Language Models, speech language modeling, or related architecture or evaluation against immediate interactive behaviors (such as full-duplex interaction, cognitive parallelism, or engineered synchronization). The contribution is limited to zero-shot tasks predicting turn-taking events without demonstrating end-to-end spoken language modeling or full-duplex synchronous interaction as per inclusion criteria. Also, no explicit mention on evaluation frameworks involving multi-dimensional assessment beyond turn-taking prediction is found. Thus, it partially meets criteria but lacks key elements explicitly required, making inclusion uncertain.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning of turn-taking behavior by modeling voice activity dynamics, highlighting predictions of upcoming turn-shifts and backchannels, which relates to speech-level interaction modeling. However, the abstract does not explicitly mention Spoken Language Models, speech language modeling, or related architecture or evaluation against immediate interactive behaviors (such as full-duplex interaction, cognitive parallelism, or engineered synchronization). The contribution is limited to zero-shot tasks predicting turn-taking events without demonstrating end-to-end spoken language modeling or full-duplex synchronous interaction as per inclusion criteria. Also, no explicit mention on evaluation frameworks involving multi-dimensional assessment beyond turn-taking prediction is found. Thus, it partially meets criteria but lacks key elements explicitly required, making inclusion uncertain.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the paper’s clear focus on modeling voice-activity dynamics and turn-taking (including backchannels) and that it proposes self-supervised objectives and zero-shot tasks (JuniorNano: evaluation 4; JuniorMini: evaluation 3). However, the inclusion rules require the title/abstract to explicitly reference Spoken Language Models / speech language modeling (or equivalent phrasing) and to address the SLM/FD‑SLM perspective; this abstract does not use those terms and primarily frames the work as predicting turn-taking events (a half‑duplex/voice-activity prediction task) rather than as a spoken language modeling or full‑duplex synchronization contribution. Because it fails the explicit-keyword requirement and appears limited to turn-taking prediction (risking exclusion under the half‑duplex-only criterion), I recommend exclusion.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the paper’s clear focus on modeling voice-activity dynamics and turn-taking (including backchannels) and that it proposes self-supervised objectives and zero-shot tasks (JuniorNano: evaluation 4; JuniorMini: evaluation 3). However, the inclusion rules require the title/abstract to explicitly reference Spoken Language Models / speech language modeling (or equivalent phrasing) and to address the SLM/FD‑SLM perspective; this abstract does not use those terms and primarily frames the work as predicting turn-taking events (a half‑duplex/voice-activity prediction task) rather than as a spoken language modeling or full‑duplex synchronization contribution. Because it fails the explicit-keyword requirement and appears limited to turn-taking prediction (risking exclusion under the half‑duplex-only criterion), I recommend exclusion.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Modular architecture facilitates noise-driven control of synchrony in neuronal networks",
    "abstract": "Brain functions require both segregated processing of information in specialized circuits, as well as integration across circuits to perform high-level information processing. One possible way to implement these seemingly opposing demands is by flexibly switching between synchronous and less synchronous states. Understanding how complex synchronization patterns are controlled by the interaction of network architecture and external perturbations is thus a central challenge in neuroscience, but the mechanisms behind such interactions remain elusive. Here, we utilise precision neuroengineering to manipulate cultured neuronal networks and show that a modular architecture facilitates desynchronization upon asynchronous stimulation, making external noise a control parameter of synchrony. Using spiking neuron models, we then demonstrate that external noise can reduce the level of available synaptic resources, which make intermodular interactions more stochastic and thereby facilitates the breakdown of synchrony. Finally, the phenomenology of stochastic intermodular interactions is formulated into a mesoscopic model that incorporates a state-dependent gating mechanism for signal propagation. Taken together, our results demonstrate a network mechanism by which asynchronous inputs tune the inherent dynamical state in structured networks of excitable units.",
    "metadata": {
      "arxiv_id": "2205.10563",
      "title": "Modular architecture facilitates noise-driven control of synchrony in neuronal networks",
      "summary": "Brain functions require both segregated processing of information in specialized circuits, as well as integration across circuits to perform high-level information processing. One possible way to implement these seemingly opposing demands is by flexibly switching between synchronous and less synchronous states. Understanding how complex synchronization patterns are controlled by the interaction of network architecture and external perturbations is thus a central challenge in neuroscience, but the mechanisms behind such interactions remain elusive. Here, we utilise precision neuroengineering to manipulate cultured neuronal networks and show that a modular architecture facilitates desynchronization upon asynchronous stimulation, making external noise a control parameter of synchrony. Using spiking neuron models, we then demonstrate that external noise can reduce the level of available synaptic resources, which make intermodular interactions more stochastic and thereby facilitates the breakdown of synchrony. Finally, the phenomenology of stochastic intermodular interactions is formulated into a mesoscopic model that incorporates a state-dependent gating mechanism for signal propagation. Taken together, our results demonstrate a network mechanism by which asynchronous inputs tune the inherent dynamical state in structured networks of excitable units.",
      "authors": [
        "Hideaki Yamamoto",
        "F. Paul Spitzner",
        "Taiki Takemuro",
        "Victor Buendía",
        "Carla Morante",
        "Tomohiro Konno",
        "Shigeo Sato",
        "Ayumi Hirano-Iwata",
        "Viola Priesemann",
        "Miguel A. Muñoz",
        "Johannes Zierenberg",
        "Jordi Soriano"
      ],
      "published": "2022-05-21T11:05:01Z",
      "updated": "2022-05-21T11:05:01Z",
      "categories": [
        "q-bio.NC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.10563v1",
      "landing_url": "https://arxiv.org/abs/2205.10563v1",
      "doi": "https://doi.org/10.1126/sciadv.ade1755"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is a neuroscience study on modular architecture and noise-driven control of synchrony in neuronal networks. It does not focus on Spoken Language Models or Speech Language Models, nor discuss speech as the core modeling target, therefore it does not meet inclusion criteria; it also does not address interactive speech synchronization or dialog behavior.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is a neuroscience study on modular architecture and noise-driven control of synchrony in neuronal networks. It does not focus on Spoken Language Models or Speech Language Models, nor discuss speech as the core modeling target, therefore it does not meet inclusion criteria; it also does not address interactive speech synchronization or dialog behavior.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on neuronal network synchronization controlled by modular architecture and external noise, with no mention of spoken language models, speech language modeling, full-duplex interaction, or any speech-related modeling or evaluation; thus, it does not meet the inclusion criteria centered around spoken language models and speech-based interaction models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on neuronal network synchronization controlled by modular architecture and external noise, with no mention of spoken language models, speech language modeling, full-duplex interaction, or any speech-related modeling or evaluation; thus, it does not meet the inclusion criteria centered around spoken language models and speech-based interaction models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Reinforcement Learning Approach for Mapping Applications to Dataflow-Based Coarse-Grained Reconfigurable Array",
    "abstract": "The Streaming Engine (SE) is a Coarse-Grained Reconfigurable Array which provides programming flexibility and high-performance with energy efficiency. An application program to be executed on the SE is represented as a combination of Synchronous Data Flow (SDF) graphs, where every instruction is represented as a node. Each node needs to be mapped to the right slot and array in the SE to ensure the correct execution of the program. This creates an optimization problem with a vast and sparse search space for which finding a mapping manually is impractical because it requires expertise and knowledge of the SE micro-architecture. In this work we propose a Reinforcement Learning framework with Global Graph Attention (GGA) module and output masking of invalid placements to find and optimize instruction schedules. We use Proximal Policy Optimization in order to train a model which places operations into the SE tiles based on a reward function that models the SE device and its constraints. The GGA module consists of a graph neural network and an attention module. The graph neural network creates embeddings of the SDFs and the attention block is used to model sequential operation placement. We show results on how certain workloads are mapped to the SE and the factors affecting mapping quality. We find that the addition of GGA, on average, finds 10% better instruction schedules in terms of total clock cycles taken and masking improves reward obtained by 20%.",
    "metadata": {
      "arxiv_id": "2205.13675",
      "title": "Reinforcement Learning Approach for Mapping Applications to Dataflow-Based Coarse-Grained Reconfigurable Array",
      "summary": "The Streaming Engine (SE) is a Coarse-Grained Reconfigurable Array which provides programming flexibility and high-performance with energy efficiency. An application program to be executed on the SE is represented as a combination of Synchronous Data Flow (SDF) graphs, where every instruction is represented as a node. Each node needs to be mapped to the right slot and array in the SE to ensure the correct execution of the program. This creates an optimization problem with a vast and sparse search space for which finding a mapping manually is impractical because it requires expertise and knowledge of the SE micro-architecture. In this work we propose a Reinforcement Learning framework with Global Graph Attention (GGA) module and output masking of invalid placements to find and optimize instruction schedules. We use Proximal Policy Optimization in order to train a model which places operations into the SE tiles based on a reward function that models the SE device and its constraints. The GGA module consists of a graph neural network and an attention module. The graph neural network creates embeddings of the SDFs and the attention block is used to model sequential operation placement. We show results on how certain workloads are mapped to the SE and the factors affecting mapping quality. We find that the addition of GGA, on average, finds 10% better instruction schedules in terms of total clock cycles taken and masking improves reward obtained by 20%.",
      "authors": [
        "Andre Xian Ming Chang",
        "Parth Khopkar",
        "Bashar Romanous",
        "Abhishek Chaurasia",
        "Patrick Estep",
        "Skyler Windh",
        "Doug Vanesko",
        "Sheik Dawood Beer Mohideen",
        "Eugenio Culurciello"
      ],
      "published": "2022-05-26T23:36:21Z",
      "updated": "2022-05-26T23:36:21Z",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.13675v1",
      "landing_url": "https://arxiv.org/abs/2205.13675v1",
      "doi": "https://doi.org/10.48550/arXiv.2205.13675"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item analyzes RL-based mapping for dataflow-based coarse-grained reconfigurable array hardware; there is no focus on Spoken Language Models, speech signals, or speech-language modeling; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item analyzes RL-based mapping for dataflow-based coarse-grained reconfigurable array hardware; there is no focus on Spoken Language Models, speech signals, or speech-language modeling; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on optimizing instruction scheduling for a Coarse-Grained Reconfigurable Array using reinforcement learning, without any mention of spoken language models, speech language modeling, or any related concepts such as full-duplex spoken language models, speech overlap, or real-time interaction in speech; thus, it does not meet the inclusion criteria which specifically require core research on spoken language models or speech-based language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on optimizing instruction scheduling for a Coarse-Grained Reconfigurable Array using reinforcement learning, without any mention of spoken language models, speech language modeling, or any related concepts such as full-duplex spoken language models, speech overlap, or real-time interaction in speech; thus, it does not meet the inclusion criteria which specifically require core research on spoken language models or speech-based language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Duplex Conversation: Towards Human-like Interaction in Spoken Dialogue Systems",
    "abstract": "In this paper, we present Duplex Conversation, a multi-turn, multimodal spoken dialogue system that enables telephone-based agents to interact with customers like a human. We use the concept of full-duplex in telecommunication to demonstrate what a human-like interactive experience should be and how to achieve smooth turn-taking through three subtasks: user state detection, backchannel selection, and barge-in detection. Besides, we propose semi-supervised learning with multimodal data augmentation to leverage unlabeled data to increase model generalization. Experimental results on three sub-tasks show that the proposed method achieves consistent improvements compared with baselines. We deploy the Duplex Conversation to Alibaba intelligent customer service and share lessons learned in production. Online A/B experiments show that the proposed system can significantly reduce response latency by 50%.",
    "metadata": {
      "arxiv_id": "2205.15060",
      "title": "Duplex Conversation: Towards Human-like Interaction in Spoken Dialogue Systems",
      "summary": "In this paper, we present Duplex Conversation, a multi-turn, multimodal spoken dialogue system that enables telephone-based agents to interact with customers like a human. We use the concept of full-duplex in telecommunication to demonstrate what a human-like interactive experience should be and how to achieve smooth turn-taking through three subtasks: user state detection, backchannel selection, and barge-in detection. Besides, we propose semi-supervised learning with multimodal data augmentation to leverage unlabeled data to increase model generalization. Experimental results on three sub-tasks show that the proposed method achieves consistent improvements compared with baselines. We deploy the Duplex Conversation to Alibaba intelligent customer service and share lessons learned in production. Online A/B experiments show that the proposed system can significantly reduce response latency by 50%.",
      "authors": [
        "Ting-En Lin",
        "Yuchuan Wu",
        "Fei Huang",
        "Luo Si",
        "Jian Sun",
        "Yongbin Li"
      ],
      "published": "2022-05-30T12:41:23Z",
      "updated": "2022-06-14T10:45:56Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2205.15060v4",
      "landing_url": "https://arxiv.org/abs/2205.15060v4",
      "doi": "https://doi.org/10.1145/3534678.3539209"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目以全雙工口語對話系統為核心，聚焦語音作為建模與推理對象，討論系統架構、訓練（半監督學習與多模態資料增強）與評估，且在近三年公開，含實驗與部署評估，未僅限於單一語音技術或半雙工情境，符合所有納入條件且無排除要件。",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "此項目以全雙工口語對話系統為核心，聚焦語音作為建模與推理對象，討論系統架構、訓練（半監督學習與多模態資料增強）與評估，且在近三年公開，含實驗與部署評估，未僅限於單一語音技術或半雙工情境，符合所有納入條件且無排除要件。",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper addresses a multimodal, multi-turn spoken dialogue system focusing on full-duplex interaction, including turn-taking, backchannel selection, and barge-in detection, which aligns well with the topic of full-duplex spoken language models and interactive behaviors; it discusses model training and evaluation and presents real-world latency improvements, indicating an engineering and learned synchronization approach. However, the abstract does not explicitly mention \"Spoken Language Models\" or equivalent terminology centered on speech language modeling, nor does it detail the architecture as a speech-based language model, which is a strict inclusion requirement. Despite this, the focus on multimodal spoken dialogue and full-duplex interaction suggests it is relevant, but the lack of explicit model naming leaves some uncertainty about meeting all criteria fully.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The paper addresses a multimodal, multi-turn spoken dialogue system focusing on full-duplex interaction, including turn-taking, backchannel selection, and barge-in detection, which aligns well with the topic of full-duplex spoken language models and interactive behaviors; it discusses model training and evaluation and presents real-world latency improvements, indicating an engineering and learned synchronization approach. However, the abstract does not explicitly mention \"Spoken Language Models\" or equivalent terminology centered on speech language modeling, nor does it detail the architecture as a speech-based language model, which is a strict inclusion requirement. Despite this, the focus on multimodal spoken dialogue and full-duplex interaction suggests it is relevant, but the lack of explicit model naming leaves some uncertainty about meeting all criteria fully.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Dual-stream spatiotemporal networks with feature sharing for monitoring animals in the home cage",
    "abstract": "This paper presents a spatiotemporal deep learning approach for mouse behavioural classification in the home-cage. Using a series of dual-stream architectures with assorted modifications to increase performance, we introduce a novel feature sharing approach that jointly processes the streams at regular intervals throughout the network. To investigate the efficacy of this approach, models were evaluated by dissociating the streams and training/testing in the same rigorous manner as the main classifiers. Using an annotated, publicly available dataset of a singly-housed mice, we achieve prediction accuracy of 86.47% using an ensemble of a Inception-based network and an attention-based network, both of which utilize this feature sharing. We also demonstrate through ablation studies that for all models, the feature-sharing architectures consistently perform better than conventional ones having separate streams. The best performing models were further evaluated on other activity datasets, both mouse and human. Future work will investigate the effectiveness of feature sharing to behavioural classification in the unsupervised anomaly detection domain.",
    "metadata": {
      "arxiv_id": "2206.00614",
      "title": "Dual-stream spatiotemporal networks with feature sharing for monitoring animals in the home cage",
      "summary": "This paper presents a spatiotemporal deep learning approach for mouse behavioural classification in the home-cage. Using a series of dual-stream architectures with assorted modifications to increase performance, we introduce a novel feature sharing approach that jointly processes the streams at regular intervals throughout the network. To investigate the efficacy of this approach, models were evaluated by dissociating the streams and training/testing in the same rigorous manner as the main classifiers. Using an annotated, publicly available dataset of a singly-housed mice, we achieve prediction accuracy of 86.47% using an ensemble of a Inception-based network and an attention-based network, both of which utilize this feature sharing. We also demonstrate through ablation studies that for all models, the feature-sharing architectures consistently perform better than conventional ones having separate streams. The best performing models were further evaluated on other activity datasets, both mouse and human. Future work will investigate the effectiveness of feature sharing to behavioural classification in the unsupervised anomaly detection domain.",
      "authors": [
        "Ezechukwu I. Nwokedi",
        "Rasneer S. Bains",
        "Luc Bidaut",
        "Xujiong Ye",
        "Sara Wells",
        "James M. Brown"
      ],
      "published": "2022-06-01T16:32:25Z",
      "updated": "2022-11-03T23:02:29Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.00614v2",
      "landing_url": "https://arxiv.org/abs/2206.00614v2",
      "doi": "https://doi.org/10.48550/arXiv.2206.00614"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on spatiotemporal video analysis of mouse behavior with dual-stream networks and feature sharing, not on Spoken Language Models or any speech-centric modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on spatiotemporal video analysis of mouse behavior with dual-stream networks and feature sharing, not on Spoken Language Models or any speech-centric modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on spatiotemporal deep learning for mouse behavioral classification using video data, which is unrelated to 'Spoken Language Models' or speech-based language modeling, and does not mention any speech, acoustic signal modeling, or spoken language interaction aspects required by the inclusion criteria. It also does not mention full-duplex or synchronous spoken interaction modeling, nor any relevant speech or language modeling architecture, training, or evaluation. Therefore, it clearly does not meet the inclusion criteria and falls under exclusion for lacking speech-related modeling focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on spatiotemporal deep learning for mouse behavioral classification using video data, which is unrelated to 'Spoken Language Models' or speech-based language modeling, and does not mention any speech, acoustic signal modeling, or spoken language interaction aspects required by the inclusion criteria. It also does not mention full-duplex or synchronous spoken interaction modeling, nor any relevant speech or language modeling architecture, training, or evaluation. Therefore, it clearly does not meet the inclusion criteria and falls under exclusion for lacking speech-related modeling focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A modular architecture for creating multimodal agents",
    "abstract": "The paper describes a flexible and modular platform to create multimodal interactive agents. The platform operates through an event-bus on which signals and interpretations are posted in a sequence in time. Different sensors and interpretation components can be integrated by defining their input and output as topics, which results in a logical workflow for further interpretations. We explain a broad range of components that have been developed so far and integrated into a range of interactive agents. We also explain how the actual interaction is recorded as multimodal data as well as in a so-called episodic Knowledge Graph. By analysing the recorded interaction, we can analyse and compare different agents and agent components.",
    "metadata": {
      "arxiv_id": "2206.00636",
      "title": "A modular architecture for creating multimodal agents",
      "summary": "The paper describes a flexible and modular platform to create multimodal interactive agents. The platform operates through an event-bus on which signals and interpretations are posted in a sequence in time. Different sensors and interpretation components can be integrated by defining their input and output as topics, which results in a logical workflow for further interpretations. We explain a broad range of components that have been developed so far and integrated into a range of interactive agents. We also explain how the actual interaction is recorded as multimodal data as well as in a so-called episodic Knowledge Graph. By analysing the recorded interaction, we can analyse and compare different agents and agent components.",
      "authors": [
        "Thomas Baier",
        "Selene Baez Santamaria",
        "Piek Vossen"
      ],
      "published": "2022-06-01T17:12:10Z",
      "updated": "2022-06-01T17:12:10Z",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.00636v1",
      "landing_url": "https://arxiv.org/abs/2206.00636v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.00636"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本篇著重於建立多模態代理的模組化架構與事件總線平台，未以 Spoken Language Models（語音語言模型）為核心，也未聚焦語音建模、端到端訓練或語音互動評估，因此不符合納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本篇著重於建立多模態代理的模組化架構與事件總線平台，未以 Spoken Language Models（語音語言模型）為核心，也未聚焦語音建模、端到端訓練或語音互動評估，因此不符合納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses a modular platform for creating multimodal agents, focusing on integrating various sensors and interaction components and analyzing multimodal data; however, it does not mention Spoken Language Models (SLMs), speech language modeling, or full-duplex spoken language interaction, nor does it focus specifically on speech as the core modeling object or address key concepts like cognitive parallelism, low-latency interaction, or learned synchronization related to SLMs or FD-SLMs. Therefore, it does not meet the inclusion criteria focused specifically on speech language modeling aspects and full-duplex spoken interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses a modular platform for creating multimodal agents, focusing on integrating various sensors and interaction components and analyzing multimodal data; however, it does not mention Spoken Language Models (SLMs), speech language modeling, or full-duplex spoken language interaction, nor does it focus specifically on speech as the core modeling object or address key concepts like cognitive parallelism, low-latency interaction, or learned synchronization related to SLMs or FD-SLMs. Therefore, it does not meet the inclusion criteria focused specifically on speech language modeling aspects and full-duplex spoken interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Is a Modular Architecture Enough?",
    "abstract": "Inspired from human cognition, machine learning systems are gradually revealing advantages of sparser and more modular architectures. Recent work demonstrates that not only do some modular architectures generalize well, but they also lead to better out-of-distribution generalization, scaling properties, learning speed, and interpretability. A key intuition behind the success of such systems is that the data generating system for most real-world settings is considered to consist of sparsely interacting parts, and endowing models with similar inductive biases will be helpful. However, the field has been lacking in a rigorous quantitative assessment of such systems because these real-world data distributions are complex and unknown. In this work, we provide a thorough assessment of common modular architectures, through the lens of simple and known modular data distributions. We highlight the benefits of modularity and sparsity and reveal insights on the challenges faced while optimizing modular systems. In doing so, we propose evaluation metrics that highlight the benefits of modularity, the regimes in which these benefits are substantial, as well as the sub-optimality of current end-to-end learned modular systems as opposed to their claimed potential.",
    "metadata": {
      "arxiv_id": "2206.02713",
      "title": "Is a Modular Architecture Enough?",
      "summary": "Inspired from human cognition, machine learning systems are gradually revealing advantages of sparser and more modular architectures. Recent work demonstrates that not only do some modular architectures generalize well, but they also lead to better out-of-distribution generalization, scaling properties, learning speed, and interpretability. A key intuition behind the success of such systems is that the data generating system for most real-world settings is considered to consist of sparsely interacting parts, and endowing models with similar inductive biases will be helpful. However, the field has been lacking in a rigorous quantitative assessment of such systems because these real-world data distributions are complex and unknown. In this work, we provide a thorough assessment of common modular architectures, through the lens of simple and known modular data distributions. We highlight the benefits of modularity and sparsity and reveal insights on the challenges faced while optimizing modular systems. In doing so, we propose evaluation metrics that highlight the benefits of modularity, the regimes in which these benefits are substantial, as well as the sub-optimality of current end-to-end learned modular systems as opposed to their claimed potential.",
      "authors": [
        "Sarthak Mittal",
        "Yoshua Bengio",
        "Guillaume Lajoie"
      ],
      "published": "2022-06-06T16:12:06Z",
      "updated": "2022-06-06T16:12:06Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.02713v1",
      "landing_url": "https://arxiv.org/abs/2206.02713v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.02713"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract investigates modular architectures in ML generally and does not focus on Spoken Language Models or speech-based modeling, nor discuss speech-specific evaluation or interaction, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract investigates modular architectures in ML generally and does not focus on Spoken Language Models or speech-based modeling, nor discuss speech-specific evaluation or interaction, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on modular architectures in machine learning from a general perspective, without any mention of spoken language models, speech language modeling, or relevant full-duplex interactive dialogue behaviors. The abstract discusses architecture and evaluation metrics but does not relate to speech or spoken language modeling specifically, nor does it concern real-time speech interaction capabilities or related evaluation frameworks. Therefore, it does not meet the inclusion criteria centered on spoken language models and their interactive features, nor does it provide the required terminology or research focus within the past three years.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on modular architectures in machine learning from a general perspective, without any mention of spoken language models, speech language modeling, or relevant full-duplex interactive dialogue behaviors. The abstract discusses architecture and evaluation metrics but does not relate to speech or spoken language modeling specifically, nor does it concern real-time speech interaction capabilities or related evaluation frameworks. Therefore, it does not meet the inclusion criteria centered on spoken language models and their interactive features, nor does it provide the required terminology or research focus within the past three years.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Wavelet Prior Attention Learning in Axial Inpainting Network",
    "abstract": "Image inpainting is the task of filling masked or unknown regions of an image with visually realistic contents, which has been remarkably improved by Deep Neural Networks (DNNs) recently. Essentially, as an inverse problem, the inpainting has the underlying challenges of reconstructing semantically coherent results without texture artifacts. Many previous efforts have been made via exploiting attention mechanisms and prior knowledge, such as edges and semantic segmentation. However, these works are still limited in practice by an avalanche of learnable prior parameters and prohibitive computational burden. To this end, we propose a novel model -- Wavelet prior attention learning in Axial Inpainting Network (WAIN), whose generator contains the encoder, decoder, as well as two key components of Wavelet image Prior Attention (WPA) and stacked multi-layer Axial-Transformers (ATs). Particularly, the WPA guides the high-level feature aggregation in the multi-scale frequency domain, alleviating the textual artifacts. Stacked ATs employ unmasked clues to help model reasonable features along with low-level features of horizontal and vertical axes, improving the semantic coherence. Extensive quantitative and qualitative experiments on Celeba-HQ and Places2 datasets are conducted to validate that our WAIN can achieve state-of-the-art performance over the competitors. The codes and models will be released.",
    "metadata": {
      "arxiv_id": "2206.03113",
      "title": "Wavelet Prior Attention Learning in Axial Inpainting Network",
      "summary": "Image inpainting is the task of filling masked or unknown regions of an image with visually realistic contents, which has been remarkably improved by Deep Neural Networks (DNNs) recently. Essentially, as an inverse problem, the inpainting has the underlying challenges of reconstructing semantically coherent results without texture artifacts. Many previous efforts have been made via exploiting attention mechanisms and prior knowledge, such as edges and semantic segmentation. However, these works are still limited in practice by an avalanche of learnable prior parameters and prohibitive computational burden. To this end, we propose a novel model -- Wavelet prior attention learning in Axial Inpainting Network (WAIN), whose generator contains the encoder, decoder, as well as two key components of Wavelet image Prior Attention (WPA) and stacked multi-layer Axial-Transformers (ATs). Particularly, the WPA guides the high-level feature aggregation in the multi-scale frequency domain, alleviating the textual artifacts. Stacked ATs employ unmasked clues to help model reasonable features along with low-level features of horizontal and vertical axes, improving the semantic coherence. Extensive quantitative and qualitative experiments on Celeba-HQ and Places2 datasets are conducted to validate that our WAIN can achieve state-of-the-art performance over the competitors. The codes and models will be released.",
      "authors": [
        "Chenjie Cao",
        "Chengrong Wang",
        "Yuntao Zhang",
        "Yanwei Fu"
      ],
      "published": "2022-06-07T08:45:27Z",
      "updated": "2022-06-14T06:54:56Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.03113v2",
      "landing_url": "https://arxiv.org/abs/2206.03113v2",
      "doi": "https://doi.org/10.48550/arXiv.2206.03113"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項為影像修補模型，未涉及語音/語音語言模型，因此不符合納入標準，且與排除條件之範圍亦無關聯。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項為影像修補模型，未涉及語音/語音語言模型，因此不符合納入標準，且與排除條件之範圍亦無關聯。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on image inpainting using deep neural networks and wavelet attention mechanisms, with no mention of spoken language models, speech language models, or speech-based modeling, which is central to the inclusion criteria. It does not discuss modeling or reasoning on speech sequences, synchronous spoken interaction, or full-duplex spoken language modeling, which are required; thus, it does not meet the thematic requirements nor the specified keywords and evaluation criteria related to speech modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on image inpainting using deep neural networks and wavelet attention mechanisms, with no mention of spoken language models, speech language models, or speech-based modeling, which is central to the inclusion criteria. It does not discuss modeling or reasoning on speech sequences, synchronous spoken interaction, or full-duplex spoken language modeling, which are required; thus, it does not meet the thematic requirements nor the specified keywords and evaluation criteria related to speech modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Depth-Adapted CNNs for RGB-D Semantic Segmentation",
    "abstract": "Recent RGB-D semantic segmentation has motivated research interest thanks to the accessibility of complementary modalities from the input side. Existing works often adopt a two-stream architecture that processes photometric and geometric information in parallel, with few methods explicitly leveraging the contribution of depth cues to adjust the sampling position on RGB images. In this paper, we propose a novel framework to incorporate the depth information in the RGB convolutional neural network (CNN), termed Z-ACN (Depth-Adapted CNN). Specifically, our Z-ACN generates a 2D depth-adapted offset which is fully constrained by low-level features to guide the feature extraction on RGB images. With the generated offset, we introduce two intuitive and effective operations to replace basic CNN operators: depth-adapted convolution and depth-adapted average pooling. Extensive experiments on both indoor and outdoor semantic segmentation tasks demonstrate the effectiveness of our approach.",
    "metadata": {
      "arxiv_id": "2206.03939",
      "title": "Depth-Adapted CNNs for RGB-D Semantic Segmentation",
      "summary": "Recent RGB-D semantic segmentation has motivated research interest thanks to the accessibility of complementary modalities from the input side. Existing works often adopt a two-stream architecture that processes photometric and geometric information in parallel, with few methods explicitly leveraging the contribution of depth cues to adjust the sampling position on RGB images. In this paper, we propose a novel framework to incorporate the depth information in the RGB convolutional neural network (CNN), termed Z-ACN (Depth-Adapted CNN). Specifically, our Z-ACN generates a 2D depth-adapted offset which is fully constrained by low-level features to guide the feature extraction on RGB images. With the generated offset, we introduce two intuitive and effective operations to replace basic CNN operators: depth-adapted convolution and depth-adapted average pooling. Extensive experiments on both indoor and outdoor semantic segmentation tasks demonstrate the effectiveness of our approach.",
      "authors": [
        "Zongwei Wu",
        "Guillaume Allibert",
        "Christophe Stolz",
        "Chao Ma",
        "Cédric Demonceaux"
      ],
      "published": "2022-06-08T14:59:40Z",
      "updated": "2022-06-08T14:59:40Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.03939v1",
      "landing_url": "https://arxiv.org/abs/2206.03939v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.03939"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item targets depth-adapted CNNs for RGB-D semantic segmentation in computer vision; it does not involve Spoken Language Models or speech-language modeling, thus it fails all inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item targets depth-adapted CNNs for RGB-D semantic segmentation in computer vision; it does not involve Spoken Language Models or speech-language modeling, thus it fails all inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on depth-adapted CNNs for RGB-D semantic segmentation, which is in the domain of computer vision and does not address spoken language models or speech language modeling, synchronization, or interaction behaviors as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on depth-adapted CNNs for RGB-D semantic segmentation, which is in the domain of computer vision and does not address spoken language models or speech language modeling, synchronization, or interaction behaviors as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Low-Latency MAC Design for Pairwise Random Networks",
    "abstract": "Feasibility of using unlicensed spectrum for ultra reliable low latency communications (URLLC) is still a question for beyond 5G wireless networks. Low latency access to the channel and efficiently sharing spectrum among the multiple users are the main requirements for exploiting unlicensed spectrum for URLLC. Listen before talk and back-off procedures implemented to avoid the collisions in channel access hinder the low latency communication. In this paper, we propose a novel low-latency medium access control (MAC) scheme based on the collision resolution for a pairwise random wireless network. We use geometric sequence decomposition for collision resolution among the competing users. This enables the system to tackle collisions and thus removing the need for carrier sensing and back-off procedures. This saves time in obtaining access to the channel and improves the efficiency of the system. We implement our approach in the synchronized time slotted system and show that it yields significant improvement over existing MAC schemes.",
    "metadata": {
      "arxiv_id": "2206.06978",
      "title": "Low-Latency MAC Design for Pairwise Random Networks",
      "summary": "Feasibility of using unlicensed spectrum for ultra reliable low latency communications (URLLC) is still a question for beyond 5G wireless networks. Low latency access to the channel and efficiently sharing spectrum among the multiple users are the main requirements for exploiting unlicensed spectrum for URLLC. Listen before talk and back-off procedures implemented to avoid the collisions in channel access hinder the low latency communication. In this paper, we propose a novel low-latency medium access control (MAC) scheme based on the collision resolution for a pairwise random wireless network. We use geometric sequence decomposition for collision resolution among the competing users. This enables the system to tackle collisions and thus removing the need for carrier sensing and back-off procedures. This saves time in obtaining access to the channel and improves the efficiency of the system. We implement our approach in the synchronized time slotted system and show that it yields significant improvement over existing MAC schemes.",
      "authors": [
        "Irshad A. Meer",
        "Woong-Hee Lee",
        "Mustafa Ozger",
        "Cicek Cavdar",
        "Ki Won Sung"
      ],
      "published": "2022-05-22T12:31:33Z",
      "updated": "2022-05-22T12:31:33Z",
      "categories": [
        "cs.IT",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.06978v1",
      "landing_url": "https://arxiv.org/abs/2206.06978v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.06978"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns low-latency MAC design for wireless networks and collision resolution, not Spoken Language Models or any speech-language modeling aspect; it does not meet inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns low-latency MAC design for wireless networks and collision resolution, not Spoken Language Models or any speech-language modeling aspect; it does not meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article centers on a low-latency medium access control mechanism for wireless networks, focusing on communication protocols and collision resolution, without addressing spoken language models, speech language modeling, or related architectures and evaluations as required by the inclusion criteria; thus, it does not meet the core thematic requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article centers on a low-latency medium access control mechanism for wireless networks, focusing on communication protocols and collision resolution, without addressing spoken language models, speech language modeling, or related architectures and evaluations as required by the inclusion criteria; thus, it does not meet the core thematic requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "All you need is feedback: Communication with block attention feedback codes",
    "abstract": "Deep learning based channel code designs have recently gained interest as an alternative to conventional coding algorithms, particularly for channels for which existing codes do not provide effective solutions. Communication over a feedback channel is one such problem, for which promising results have recently been obtained by employing various deep learning architectures. In this paper, we introduce a novel learning-aided code design for feedback channels, called generalized block attention feedback (GBAF) codes, which i) employs a modular architecture that can be implemented using different neural network architectures; ii) provides order-of-magnitude improvements in the probability of error compared to existing designs; and iii) can transmit at desired code rates.",
    "metadata": {
      "arxiv_id": "2206.09457",
      "title": "All you need is feedback: Communication with block attention feedback codes",
      "summary": "Deep learning based channel code designs have recently gained interest as an alternative to conventional coding algorithms, particularly for channels for which existing codes do not provide effective solutions. Communication over a feedback channel is one such problem, for which promising results have recently been obtained by employing various deep learning architectures. In this paper, we introduce a novel learning-aided code design for feedback channels, called generalized block attention feedback (GBAF) codes, which i) employs a modular architecture that can be implemented using different neural network architectures; ii) provides order-of-magnitude improvements in the probability of error compared to existing designs; and iii) can transmit at desired code rates.",
      "authors": [
        "Emre Ozfatura",
        "Yulin Shao",
        "Alberto Perotti",
        "Branislav Popovic",
        "Deniz Gunduz"
      ],
      "published": "2022-06-19T17:55:04Z",
      "updated": "2022-10-05T16:13:17Z",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.09457v2",
      "landing_url": "https://arxiv.org/abs/2206.09457v2",
      "doi": "https://doi.org/10.48550/arXiv.2206.09457"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on deep learning-based channel coding for feedback channels in communications, not on Spoken Language Models or speech-language modeling, and thus does not meet the inclusion criteria (and belongs outside the exclusion scope).",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on deep learning-based channel coding for feedback channels in communications, not on Spoken Language Models or speech-language modeling, and thus does not meet the inclusion criteria (and belongs outside the exclusion scope).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper titled 'All you need is feedback: Communication with block attention feedback codes' focuses on deep learning based channel code design for feedback channels in communications, without any mention of Spoken Language Models, speech language modeling, end-to-end architectures involving speech data, or the full-duplex spoken language interaction features required by the inclusion criteria. The work centers around communication channel coding and error rates rather than speech modeling or dialog systems, and it lacks discussion on synchronization, cognitive parallelism, or interactive speech behavior arbitration. Therefore, it does not meet the core subject requirements and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper titled 'All you need is feedback: Communication with block attention feedback codes' focuses on deep learning based channel code design for feedback channels in communications, without any mention of Spoken Language Models, speech language modeling, end-to-end architectures involving speech data, or the full-duplex spoken language interaction features required by the inclusion criteria. The work centers around communication channel coding and error rates rather than speech modeling or dialog systems, and it lacks discussion on synchronization, cognitive parallelism, or interactive speech behavior arbitration. Therefore, it does not meet the core subject requirements and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SSM-DTA: Breaking the Barriers of Data Scarcity in Drug-Target Affinity Prediction",
    "abstract": "Accurate prediction of Drug-Target Affinity (DTA) is of vital importance in early-stage drug discovery, facilitating the identification of drugs that can effectively interact with specific targets and regulate their activities. While wet experiments remain the most reliable method, they are time-consuming and resource-intensive, resulting in limited data availability that poses challenges for deep learning approaches. Existing methods have primarily focused on developing techniques based on the available DTA data, without adequately addressing the data scarcity issue. To overcome this challenge, we present the SSM-DTA framework, which incorporates three simple yet highly effective strategies: (1) A multi-task training approach that combines DTA prediction with masked language modeling (MLM) using paired drug-target data. (2) A semi-supervised training method that leverages large-scale unpaired molecules and proteins to enhance drug and target representations. This approach differs from previous methods that only employed molecules or proteins in pre-training. (3) The integration of a lightweight cross-attention module to improve the interaction between drugs and targets, further enhancing prediction accuracy. Through extensive experiments on benchmark datasets such as BindingDB, DAVIS, and KIBA, we demonstrate the superior performance of our framework. Additionally, we conduct case studies on specific drug-target binding activities, virtual screening experiments, drug feature visualizations, and real-world applications, all of which showcase the significant potential of our work. In conclusion, our proposed SSM-DTA framework addresses the data limitation challenge in DTA prediction and yields promising results, paving the way for more efficient and accurate drug discovery processes. Our code is available at $\\href{https://github.com/QizhiPei/SSM-DTA}{Github}$.",
    "metadata": {
      "arxiv_id": "2206.09818",
      "title": "SSM-DTA: Breaking the Barriers of Data Scarcity in Drug-Target Affinity Prediction",
      "summary": "Accurate prediction of Drug-Target Affinity (DTA) is of vital importance in early-stage drug discovery, facilitating the identification of drugs that can effectively interact with specific targets and regulate their activities. While wet experiments remain the most reliable method, they are time-consuming and resource-intensive, resulting in limited data availability that poses challenges for deep learning approaches. Existing methods have primarily focused on developing techniques based on the available DTA data, without adequately addressing the data scarcity issue. To overcome this challenge, we present the SSM-DTA framework, which incorporates three simple yet highly effective strategies: (1) A multi-task training approach that combines DTA prediction with masked language modeling (MLM) using paired drug-target data. (2) A semi-supervised training method that leverages large-scale unpaired molecules and proteins to enhance drug and target representations. This approach differs from previous methods that only employed molecules or proteins in pre-training. (3) The integration of a lightweight cross-attention module to improve the interaction between drugs and targets, further enhancing prediction accuracy. Through extensive experiments on benchmark datasets such as BindingDB, DAVIS, and KIBA, we demonstrate the superior performance of our framework. Additionally, we conduct case studies on specific drug-target binding activities, virtual screening experiments, drug feature visualizations, and real-world applications, all of which showcase the significant potential of our work. In conclusion, our proposed SSM-DTA framework addresses the data limitation challenge in DTA prediction and yields promising results, paving the way for more efficient and accurate drug discovery processes. Our code is available at $\\href{https://github.com/QizhiPei/SSM-DTA}{Github}$.",
      "authors": [
        "Qizhi Pei",
        "Lijun Wu",
        "Jinhua Zhu",
        "Yingce Xia",
        "Shufang Xie",
        "Tao Qin",
        "Haiguang Liu",
        "Tie-Yan Liu",
        "Rui Yan"
      ],
      "published": "2022-06-20T14:53:25Z",
      "updated": "2023-10-17T14:06:07Z",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.09818v3",
      "landing_url": "https://arxiv.org/abs/2206.09818v3",
      "doi": "https://doi.org/10.48550/arXiv.2206.09818"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on drug-target affinity prediction using semi-supervised learning and MLM, not on Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on drug-target affinity prediction using semi-supervised learning and MLM, not on Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on drug-target affinity prediction using a deep learning framework, which is unrelated to spoken language models, speech language models, or any aspects of speech interaction, synchronization, or evaluation criteria for spoken dialogue systems as specified in the inclusion criteria. Therefore, it does not meet the thematic requirements and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on drug-target affinity prediction using a deep learning framework, which is unrelated to spoken language models, speech language models, or any aspects of speech interaction, synchronization, or evaluation criteria for spoken dialogue systems as specified in the inclusion criteria. Therefore, it does not meet the thematic requirements and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Modular architectures to deterministically generate graph states",
    "abstract": "Graph states are a family of stabilizer states which can be tailored towards various applications in photonic quantum computing and quantum communication. In this paper, we present a modular design based on quantum dot emitters coupled to a waveguide and optical fiber delay lines to deterministically generate N-dimensional cluster states and other useful graph states such as tree states and repeater states. Unlike previous proposals, our design requires no two-qubit gates on quantum dots and at most one optical switch, thereby, minimizing challenges usually posed by these requirements. Furthermore, we discuss the error model for our design and demonstrate a fault-tolerant quantum memory with an error threshold of 0.53% in the case of a 3d graph state on a Raussendorf-Harrington-Goyal (RHG) lattice. We also provide a fundamental upper bound on the correctable loss in the fault-tolerant RHG state based on the percolation theory, which is 1.24 dB or 0.24 dB depending on whether the state is directly generated or obtained from a simple cubic cluster state, respectively.",
    "metadata": {
      "arxiv_id": "2206.11307",
      "title": "Modular architectures to deterministically generate graph states",
      "summary": "Graph states are a family of stabilizer states which can be tailored towards various applications in photonic quantum computing and quantum communication. In this paper, we present a modular design based on quantum dot emitters coupled to a waveguide and optical fiber delay lines to deterministically generate N-dimensional cluster states and other useful graph states such as tree states and repeater states. Unlike previous proposals, our design requires no two-qubit gates on quantum dots and at most one optical switch, thereby, minimizing challenges usually posed by these requirements. Furthermore, we discuss the error model for our design and demonstrate a fault-tolerant quantum memory with an error threshold of 0.53% in the case of a 3d graph state on a Raussendorf-Harrington-Goyal (RHG) lattice. We also provide a fundamental upper bound on the correctable loss in the fault-tolerant RHG state based on the percolation theory, which is 1.24 dB or 0.24 dB depending on whether the state is directly generated or obtained from a simple cubic cluster state, respectively.",
      "authors": [
        "Hassan Shapourian",
        "Alireza Shabani"
      ],
      "published": "2022-06-22T18:17:38Z",
      "updated": "2023-02-25T04:21:44Z",
      "categories": [
        "quant-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.11307v2",
      "landing_url": "https://arxiv.org/abs/2206.11307v2",
      "doi": "https://doi.org/10.22331/q-2023-03-02-935"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses quantum photonic graph states and fault-tolerant quantum memory; it is not about Spoken Language Models or speech-language modeling and thus does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses quantum photonic graph states and fault-tolerant quantum memory; it is not about Spoken Language Models or speech-language modeling and thus does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on quantum computing architectures for generating graph states, which is unrelated to spoken language models or speech language modeling; it does not mention any of the inclusion criteria related to speech modeling or real-time speech interaction and thus does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on quantum computing architectures for generating graph states, which is unrelated to spoken language models or speech language modeling; it does not mention any of the inclusion criteria related to speech modeling or real-time speech interaction and thus does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LoneSTAR: Analog Beamforming Codebooks for Full-Duplex Millimeter Wave Systems",
    "abstract": "This work develops LoneSTAR, a novel enabler of full-duplex millimeter wave (mmWave) communication systems through the design of analog beamforming codebooks. LoneSTAR codebooks deliver high beamforming gain and broad coverage while simultaneously reducing the self-interference coupled by transmit and receive beams at a full-duplex mmWave transceiver. Our design framework accomplishes this by tolerating some variability in transmit and receive beamforming gain to strategically shape beams that reject self-interference spatially while accounting for digitally-controlled analog beamforming networks and self-interference channel estimation error. By leveraging the coherence time of the self-interference channel, a mmWave system can use the same LoneSTAR design over many time slots to serve several downlink-uplink user pairs in a full-duplex fashion without the need for additional self-interference cancellation. Compared to those using conventional codebooks, full-duplex mmWave systems employing LoneSTAR codebooks can mitigate higher levels of self-interference, tolerate more cross-link interference, and demand lower SNRs in order to outperform half-duplex operation -- all while supporting beam alignment. This makes LoneSTAR a potential standalone solution for enabling simultaneous transmission and reception in mmWave systems, from which it derives its name.",
    "metadata": {
      "arxiv_id": "2206.11418",
      "title": "LoneSTAR: Analog Beamforming Codebooks for Full-Duplex Millimeter Wave Systems",
      "summary": "This work develops LoneSTAR, a novel enabler of full-duplex millimeter wave (mmWave) communication systems through the design of analog beamforming codebooks. LoneSTAR codebooks deliver high beamforming gain and broad coverage while simultaneously reducing the self-interference coupled by transmit and receive beams at a full-duplex mmWave transceiver. Our design framework accomplishes this by tolerating some variability in transmit and receive beamforming gain to strategically shape beams that reject self-interference spatially while accounting for digitally-controlled analog beamforming networks and self-interference channel estimation error. By leveraging the coherence time of the self-interference channel, a mmWave system can use the same LoneSTAR design over many time slots to serve several downlink-uplink user pairs in a full-duplex fashion without the need for additional self-interference cancellation. Compared to those using conventional codebooks, full-duplex mmWave systems employing LoneSTAR codebooks can mitigate higher levels of self-interference, tolerate more cross-link interference, and demand lower SNRs in order to outperform half-duplex operation -- all while supporting beam alignment. This makes LoneSTAR a potential standalone solution for enabling simultaneous transmission and reception in mmWave systems, from which it derives its name.",
      "authors": [
        "Ian P. Roberts",
        "Sriram Vishwanath",
        "Jeffrey G. Andrews"
      ],
      "published": "2022-06-22T23:22:21Z",
      "updated": "2022-06-22T23:22:21Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.11418v1",
      "landing_url": "https://arxiv.org/abs/2206.11418v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.11418"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe wireless communications (analog beamforming for full-duplex mmWave) and self-interference management, not Spoken Language Models or Speech-LM concepts; it does not meet inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe wireless communications (analog beamforming for full-duplex mmWave) and self-interference management, not Spoken Language Models or Speech-LM concepts; it does not meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on analog beamforming codebooks for full-duplex millimeter wave communication systems, which is a topic in wireless communication and signal processing, not related to Spoken Language Models (SLMs) or full-duplex spoken language models in the speech or language modeling domain. It does not address language modeling of speech, speech–LLM architectures, turn-taking or cognitive parallelism in spoken language interaction, nor any of the inclusion criteria focused on speech-based language modeling and interactive dialogue systems. Therefore, it does not meet the inclusion criteria and is irrelevant to the specified research scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on analog beamforming codebooks for full-duplex millimeter wave communication systems, which is a topic in wireless communication and signal processing, not related to Spoken Language Models (SLMs) or full-duplex spoken language models in the speech or language modeling domain. It does not address language modeling of speech, speech–LLM architectures, turn-taking or cognitive parallelism in spoken language interaction, nor any of the inclusion criteria focused on speech-based language modeling and interactive dialogue systems. Therefore, it does not meet the inclusion criteria and is irrelevant to the specified research scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ASR-Generated Text for Language Model Pre-training Applied to Speech Tasks",
    "abstract": "We aim at improving spoken language modeling (LM) using very large amount of automatically transcribed speech. We leverage the INA (French National Audiovisual Institute) collection and obtain 19GB of text after applying ASR on 350,000 hours of diverse TV shows. From this, spoken language models are trained either by fine-tuning an existing LM (FlauBERT) or through training a LM from scratch. New models (FlauBERT-Oral) are shared with the community and evaluated for 3 downstream tasks: spoken language understanding, classification of TV shows and speech syntactic parsing. Results show that FlauBERT-Oral can be beneficial compared to its initial FlauBERT version demonstrating that, despite its inherent noisy nature, ASR-generated text can be used to build spoken language models.",
    "metadata": {
      "arxiv_id": "2207.01893",
      "title": "ASR-Generated Text for Language Model Pre-training Applied to Speech Tasks",
      "summary": "We aim at improving spoken language modeling (LM) using very large amount of automatically transcribed speech. We leverage the INA (French National Audiovisual Institute) collection and obtain 19GB of text after applying ASR on 350,000 hours of diverse TV shows. From this, spoken language models are trained either by fine-tuning an existing LM (FlauBERT) or through training a LM from scratch. New models (FlauBERT-Oral) are shared with the community and evaluated for 3 downstream tasks: spoken language understanding, classification of TV shows and speech syntactic parsing. Results show that FlauBERT-Oral can be beneficial compared to its initial FlauBERT version demonstrating that, despite its inherent noisy nature, ASR-generated text can be used to build spoken language models.",
      "authors": [
        "Valentin Pelloin",
        "Franck Dary",
        "Nicolas Herve",
        "Benoit Favre",
        "Nathalie Camelin",
        "Antoine Laurent",
        "Laurent Besacier"
      ],
      "published": "2022-07-05T08:47:51Z",
      "updated": "2022-07-05T08:47:51Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.01893v1",
      "landing_url": "https://arxiv.org/abs/2207.01893v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.01893"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item explicitly centers on Spoken Language Models/Speech Language Models, uses ASR-generated speech data for pre-training, evaluates on multiple speech-related tasks, falls within the last 3 years, and uses terms like 'spoken language modeling', meeting all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This item explicitly centers on Spoken Language Models/Speech Language Models, uses ASR-generated speech data for pre-training, evaluates on multiple speech-related tasks, falls within the last 3 years, and uses terms like 'spoken language modeling', meeting all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on training spoken language models using ASR-generated text to improve spoken language understanding and related speech tasks, which aligns with spoken language modeling, but it mainly treats ASR output text for language modeling rather than modeling speech as a core modality with emphasis on synchronous or full-duplex interaction capabilities, low-latency, or behavioral arbitration; thus, it is more a form of text-based language modeling on ASR output than direct speech-based spoken language modeling with synchronization aspects.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on training spoken language models using ASR-generated text to improve spoken language understanding and related speech tasks, which aligns with spoken language modeling, but it mainly treats ASR output text for language modeling rather than modeling speech as a core modality with emphasis on synchronous or full-duplex interaction capabilities, low-latency, or behavioral arbitration; thus, it is more a form of text-based language modeling on ASR output than direct speech-based spoken language modeling with synchronization aspects.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed (one rated include because the abstract explicitly says \"spoken language modeling\", the other rated exclude noting it is text-based ASR output LM). After integrating: although the paper uses ASR-generated transcripts to train/finetune LMs and even calls the result \"FlauBERT-Oral\" and evaluates on spoken-related tasks, the work treats speech only as a source of noisy text rather than modeling acoustic signals, discrete/continuous speech tokens, end-to-end Speech–LLM architectures, or any full‑duplex/synchronization/overlap/low‑latency interaction behaviors required by the inclusion criteria; it therefore meets the exclusion criteria that the core research remains text-based LM on ASR output rather than true spoken-language-modeling (speech-centric) research. One-sentence explanation: Because the study trains text LMs on ASR transcripts without treating speech/acoustics or interactive/full‑duplex behaviors as the primary modeling object, it should be excluded (score 2).",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed (one rated include because the abstract explicitly says \"spoken language modeling\", the other rated exclude noting it is text-based ASR output LM). After integrating: although the paper uses ASR-generated transcripts to train/finetune LMs and even calls the result \"FlauBERT-Oral\" and evaluates on spoken-related tasks, the work treats speech only as a source of noisy text rather than modeling acoustic signals, discrete/continuous speech tokens, end-to-end Speech–LLM architectures, or any full‑duplex/synchronization/overlap/low‑latency interaction behaviors required by the inclusion criteria; it therefore meets the exclusion criteria that the core research remains text-based LM on ASR output rather than true spoken-language-modeling (speech-centric) research. One-sentence explanation: Because the study trains text LMs on ASR transcripts without treating speech/acoustics or interactive/full‑duplex behaviors as the primary modeling object, it should be excluded (score 2).",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Tackling Data Scarcity with Transfer Learning: A Case Study of Thickness Characterization from Optical Spectra of Perovskite Thin Films",
    "abstract": "Transfer learning increasingly becomes an important tool in handling data scarcity often encountered in machine learning. In the application of high-throughput thickness as a downstream process of the high-throughput optimization of optoelectronic thin films with autonomous workflows, data scarcity occurs especially for new materials. To achieve high-throughput thickness characterization, we propose a machine learning model called thicknessML that predicts thickness from UV-Vis spectrophotometry input and an overarching transfer learning workflow. We demonstrate the transfer learning workflow from generic source domain of generic band-gapped materials to specific target domain of perovskite materials, where the target domain data only come from limited number (18) of refractive indices from literature. The target domain can be easily extended to other material classes with a few literature data. Defining thickness prediction accuracy to be within-10% deviation, thicknessML achieves 92.2% (with a deviation of 3.6%) accuracy with transfer learning compared to 81.8% (with a deviation of 3.6%) 11.7% without (lower mean and larger standard deviation). Experimental validation on six deposited perovskite films also corroborates the efficacy of the proposed workflow by yielding a 10.5% mean absolute percentage error (MAPE).",
    "metadata": {
      "arxiv_id": "2207.02209",
      "title": "Tackling Data Scarcity with Transfer Learning: A Case Study of Thickness Characterization from Optical Spectra of Perovskite Thin Films",
      "summary": "Transfer learning increasingly becomes an important tool in handling data scarcity often encountered in machine learning. In the application of high-throughput thickness as a downstream process of the high-throughput optimization of optoelectronic thin films with autonomous workflows, data scarcity occurs especially for new materials. To achieve high-throughput thickness characterization, we propose a machine learning model called thicknessML that predicts thickness from UV-Vis spectrophotometry input and an overarching transfer learning workflow. We demonstrate the transfer learning workflow from generic source domain of generic band-gapped materials to specific target domain of perovskite materials, where the target domain data only come from limited number (18) of refractive indices from literature. The target domain can be easily extended to other material classes with a few literature data. Defining thickness prediction accuracy to be within-10% deviation, thicknessML achieves 92.2% (with a deviation of 3.6%) accuracy with transfer learning compared to 81.8% (with a deviation of 3.6%) 11.7% without (lower mean and larger standard deviation). Experimental validation on six deposited perovskite films also corroborates the efficacy of the proposed workflow by yielding a 10.5% mean absolute percentage error (MAPE).",
      "authors": [
        "Siyu Isaac Parker Tian",
        "Zekun Ren",
        "Selvaraj Venkataraj",
        "Yuanhang Cheng",
        "Daniil Bash",
        "Felipe Oviedo",
        "J. Senthilnath",
        "Vijila Chellappan",
        "Yee-Fun Lim",
        "Armin G. Aberle",
        "Benjamin P MacLeod",
        "Fraser G. L. Parlane",
        "Curtis P. Berlinguette",
        "Qianxiao Li",
        "Tonio Buonassisi",
        "Zhe Liu"
      ],
      "published": "2022-06-14T16:26:15Z",
      "updated": "2022-12-20T08:51:48Z",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci",
        "eess.IV",
        "physics.optics"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.02209v2",
      "landing_url": "https://arxiv.org/abs/2207.02209v2",
      "doi": "https://doi.org/10.48550/arXiv.2207.02209"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article deals with transfer learning for thickness characterization from optical spectra in perovskite thin films and does not involve Spoken Language Models or any speech-language modeling concepts, thus it fails inclusion criteria and aligns with exclusion.\n",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article deals with transfer learning for thickness characterization from optical spectra in perovskite thin films and does not involve Spoken Language Models or any speech-language modeling concepts, thus it fails inclusion criteria and aligns with exclusion.\n",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on transfer learning for thickness characterization of perovskite thin films using UV-Vis spectrophotometry data, which is unrelated to spoken language models or speech language modeling; it does not address speech modeling, full-duplex interaction, or any relevant spoken language model criteria, thus failing to meet any inclusion criteria and not meeting the thematic scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on transfer learning for thickness characterization of perovskite thin films using UV-Vis spectrophotometry data, which is unrelated to spoken language models or speech language modeling; it does not address speech modeling, full-duplex interaction, or any relevant spoken language model criteria, thus failing to meet any inclusion criteria and not meeting the thematic scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Gestural and Touchscreen Interaction for Human-Robot Collaboration: a Comparative Study",
    "abstract": "Close human-robot interaction (HRI), especially in industrial scenarios, has been vastly investigated for the advantages of combining human and robot skills. For an effective HRI, the validity of currently available human-machine communication media or tools should be questioned, and new communication modalities should be explored. This article proposes a modular architecture allowing human operators to interact with robots through different modalities. In particular, we implemented the architecture to handle gestural and touchscreen input, respectively, using a smartwatch and a tablet. Finally, we performed a comparative user experience study between these two modalities.",
    "metadata": {
      "arxiv_id": "2207.03783",
      "title": "Gestural and Touchscreen Interaction for Human-Robot Collaboration: a Comparative Study",
      "summary": "Close human-robot interaction (HRI), especially in industrial scenarios, has been vastly investigated for the advantages of combining human and robot skills. For an effective HRI, the validity of currently available human-machine communication media or tools should be questioned, and new communication modalities should be explored. This article proposes a modular architecture allowing human operators to interact with robots through different modalities. In particular, we implemented the architecture to handle gestural and touchscreen input, respectively, using a smartwatch and a tablet. Finally, we performed a comparative user experience study between these two modalities.",
      "authors": [
        "Antonino Bongiovanni",
        "Alessio De Luca",
        "Luna Gava",
        "Lucrezia Grassi",
        "Marta Lagomarsino",
        "Marco Lapolla",
        "Antonio Marino",
        "Patrick Roncagliolo",
        "Simone Macciò",
        "Alessandro Carfì",
        "Fulvio Mastrogiovanni"
      ],
      "published": "2022-07-08T09:34:56Z",
      "updated": "2022-07-08T09:34:56Z",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.03783v1",
      "landing_url": "https://arxiv.org/abs/2207.03783v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.03783"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此研究重點在於人機協作中的手勢與觸控介面的比較及使用者體驗，並未以 Spoken Language Models / 語音語言建模為核心，也缺少對語音同步互動與語音重疊處理的評估，因此不符合核心納入條件，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此研究重點在於人機協作中的手勢與觸控介面的比較及使用者體驗，並未以 Spoken Language Models / 語音語言建模為核心，也缺少對語音同步互動與語音重疊處理的評估，因此不符合核心納入條件，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on gestural and touchscreen interaction for human-robot collaboration and does not involve spoken language models or speech language modeling as the core topic; it lacks any mention of speech-based modeling, full-duplex spoken language models, or speech as the primary research object, thus failing all inclusion criteria focused on spoken language models and speech modeling aspects.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on gestural and touchscreen interaction for human-robot collaboration and does not involve spoken language models or speech language modeling as the core topic; it lacks any mention of speech-based modeling, full-duplex spoken language models, or speech as the primary research object, thus failing all inclusion criteria focused on spoken language models and speech modeling aspects.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Hybrid Spatial-Temporal Entropy Modelling for Neural Video Compression",
    "abstract": "For neural video codec, it is critical, yet challenging, to design an efficient entropy model which can accurately predict the probability distribution of the quantized latent representation. However, most existing video codecs directly use the ready-made entropy model from image codec to encode the residual or motion, and do not fully leverage the spatial-temporal characteristics in video. To this end, this paper proposes a powerful entropy model which efficiently captures both spatial and temporal dependencies. In particular, we introduce the latent prior which exploits the correlation among the latent representation to squeeze the temporal redundancy. Meanwhile, the dual spatial prior is proposed to reduce the spatial redundancy in a parallel-friendly manner. In addition, our entropy model is also versatile. Besides estimating the probability distribution, our entropy model also generates the quantization step at spatial-channel-wise. This content-adaptive quantization mechanism not only helps our codec achieve the smooth rate adjustment in single model but also improves the final rate-distortion performance by dynamic bit allocation. Experimental results show that, powered by the proposed entropy model, our neural codec can achieve 18.2% bitrate saving on UVG dataset when compared with H.266 (VTM) using the highest compression ratio configuration. It makes a new milestone in the development of neural video codec. The codes are at https://github.com/microsoft/DCVC.",
    "metadata": {
      "arxiv_id": "2207.05894",
      "title": "Hybrid Spatial-Temporal Entropy Modelling for Neural Video Compression",
      "summary": "For neural video codec, it is critical, yet challenging, to design an efficient entropy model which can accurately predict the probability distribution of the quantized latent representation. However, most existing video codecs directly use the ready-made entropy model from image codec to encode the residual or motion, and do not fully leverage the spatial-temporal characteristics in video. To this end, this paper proposes a powerful entropy model which efficiently captures both spatial and temporal dependencies. In particular, we introduce the latent prior which exploits the correlation among the latent representation to squeeze the temporal redundancy. Meanwhile, the dual spatial prior is proposed to reduce the spatial redundancy in a parallel-friendly manner. In addition, our entropy model is also versatile. Besides estimating the probability distribution, our entropy model also generates the quantization step at spatial-channel-wise. This content-adaptive quantization mechanism not only helps our codec achieve the smooth rate adjustment in single model but also improves the final rate-distortion performance by dynamic bit allocation. Experimental results show that, powered by the proposed entropy model, our neural codec can achieve 18.2% bitrate saving on UVG dataset when compared with H.266 (VTM) using the highest compression ratio configuration. It makes a new milestone in the development of neural video codec. The codes are at https://github.com/microsoft/DCVC.",
      "authors": [
        "Jiahao Li",
        "Bin Li",
        "Yan Lu"
      ],
      "published": "2022-07-13T00:03:54Z",
      "updated": "2022-07-13T00:03:54Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.05894v1",
      "landing_url": "https://arxiv.org/abs/2207.05894v1",
      "doi": "https://doi.org/10.1145/3503161.3547845"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on neural video compression and entropy modeling, not on Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on neural video compression and entropy modeling, not on Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on neural video compression and entropy modeling, which is unrelated to spoken language models or speech language modeling as defined in the inclusion criteria. There is no mention of spoken language models, speech acoustic signals, or full-duplex interaction capabilities. Therefore, it does not meet the core thematic requirements of the task.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on neural video compression and entropy modeling, which is unrelated to spoken language models or speech language modeling as defined in the inclusion criteria. There is no mention of spoken language models, speech acoustic signals, or full-duplex interaction capabilities. Therefore, it does not meet the core thematic requirements of the task.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "STEER: Beam Selection for Full-Duplex Millimeter Wave Communication Systems",
    "abstract": "Modern millimeter wave (mmWave) communication systems rely on beam alignment to deliver sufficient beamforming gain to close the link between devices. We present a novel beam selection methodology for multi-panel, full-duplex mmWave systems, which we call STEER, that delivers high beamforming gain while significantly reducing the full-duplex self-interference coupled between the transmit and receive beams. STEER does not necessitate changes to conventional beam alignment methodologies nor additional over-the-air feedback, making it compatible with existing cellular standards. Instead, STEER uses conventional beam alignment to identify the general directions beams should be steered, and then it makes use of a minimal number of self-interference measurements to jointly select transmit and receive beams that deliver high gain in these directions while coupling low self-interference. We implement STEER on an industry-grade 28 GHz phased array platform and use further simulation to show that full-duplex operation with beams selected by STEER can notably outperform both half-duplex and full-duplex operation with beams chosen via conventional beam selection. For instance, STEER can reliably reduce self-interference by more than 20 dB and improve SINR by more than 10 dB, compared to conventional beam selection. Our experimental results highlight that beam alignment can be used not only to deliver high beamforming gain in full-duplex mmWave systems but also to mitigate self-interference to levels near or below the noise floor, rendering additional self-interference cancellation unnecessary with STEER.",
    "metadata": {
      "arxiv_id": "2207.07281",
      "title": "STEER: Beam Selection for Full-Duplex Millimeter Wave Communication Systems",
      "summary": "Modern millimeter wave (mmWave) communication systems rely on beam alignment to deliver sufficient beamforming gain to close the link between devices. We present a novel beam selection methodology for multi-panel, full-duplex mmWave systems, which we call STEER, that delivers high beamforming gain while significantly reducing the full-duplex self-interference coupled between the transmit and receive beams. STEER does not necessitate changes to conventional beam alignment methodologies nor additional over-the-air feedback, making it compatible with existing cellular standards. Instead, STEER uses conventional beam alignment to identify the general directions beams should be steered, and then it makes use of a minimal number of self-interference measurements to jointly select transmit and receive beams that deliver high gain in these directions while coupling low self-interference. We implement STEER on an industry-grade 28 GHz phased array platform and use further simulation to show that full-duplex operation with beams selected by STEER can notably outperform both half-duplex and full-duplex operation with beams chosen via conventional beam selection. For instance, STEER can reliably reduce self-interference by more than 20 dB and improve SINR by more than 10 dB, compared to conventional beam selection. Our experimental results highlight that beam alignment can be used not only to deliver high beamforming gain in full-duplex mmWave systems but also to mitigate self-interference to levels near or below the noise floor, rendering additional self-interference cancellation unnecessary with STEER.",
      "authors": [
        "Ian P. Roberts",
        "Aditya Chopra",
        "Thomas Novlan",
        "Sriram Vishwanath",
        "Jeffrey G. Andrews"
      ],
      "published": "2022-07-15T04:08:32Z",
      "updated": "2022-07-15T04:08:32Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.07281v1",
      "landing_url": "https://arxiv.org/abs/2207.07281v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.07281"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article addresses beam selection for full-duplex mmWave wireless systems and self-interference mitigation, not Spoken Language Models or speech-language modeling, therefore it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article addresses beam selection for full-duplex mmWave wireless systems and self-interference mitigation, not Spoken Language Models or speech-language modeling, therefore it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on beam selection for full-duplex millimeter wave communication systems, which is a wireless communication technology topic unrelated to spoken language models or speech language models. It does not address speech modeling, language modeling of speech, or related architectures, training, or evaluation in the spoken language model domain. Therefore, it does not meet the inclusion criteria centered on spoken language models and full-duplex speech interaction, and instead focuses on beamforming and interference reduction in communication hardware.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on beam selection for full-duplex millimeter wave communication systems, which is a wireless communication technology topic unrelated to spoken language models or speech language models. It does not address speech modeling, language modeling of speech, or related architectures, training, or evaluation in the spoken language model domain. Therefore, it does not meet the inclusion criteria centered on spoken language models and full-duplex speech interaction, and instead focuses on beamforming and interference reduction in communication hardware.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "An Intelligent Deterministic Scheduling Method for Ultra-Low Latency Communication in Edge Enabled Industrial Internet of Things",
    "abstract": "Edge enabled Industrial Internet of Things (IIoT) platform is of great significance to accelerate the development of smart industry. However, with the dramatic increase in real-time IIoT applications, it is a great challenge to support fast response time, low latency, and efficient bandwidth utilization. To address this issue, Time Sensitive Network (TSN) is recently researched to realize low latency communication via deterministic scheduling. To the best of our knowledge, the combinability of multiple flows, which can significantly affect the scheduling performance, has never been systematically analyzed before. In this article, we first analyze the combinability problem. Then a non-collision theory based deterministic scheduling (NDS) method is proposed to achieve ultra-low latency communication for the time-sensitive flows. Moreover, to improve bandwidth utilization, a dynamic queue scheduling (DQS) method is presented for the best-effort flows. Experiment results demonstrate that NDS/DQS can well support deterministic ultra-low latency services and guarantee efficient bandwidth utilization.",
    "metadata": {
      "arxiv_id": "2207.08226",
      "title": "An Intelligent Deterministic Scheduling Method for Ultra-Low Latency Communication in Edge Enabled Industrial Internet of Things",
      "summary": "Edge enabled Industrial Internet of Things (IIoT) platform is of great significance to accelerate the development of smart industry. However, with the dramatic increase in real-time IIoT applications, it is a great challenge to support fast response time, low latency, and efficient bandwidth utilization. To address this issue, Time Sensitive Network (TSN) is recently researched to realize low latency communication via deterministic scheduling. To the best of our knowledge, the combinability of multiple flows, which can significantly affect the scheduling performance, has never been systematically analyzed before. In this article, we first analyze the combinability problem. Then a non-collision theory based deterministic scheduling (NDS) method is proposed to achieve ultra-low latency communication for the time-sensitive flows. Moreover, to improve bandwidth utilization, a dynamic queue scheduling (DQS) method is presented for the best-effort flows. Experiment results demonstrate that NDS/DQS can well support deterministic ultra-low latency services and guarantee efficient bandwidth utilization.",
      "authors": [
        "Yinzhi Lu",
        "Liu Yang",
        "Simon X. Yang",
        "Qiaozhi Hua",
        "Arun Kumar Sangaiah",
        "Tan Guo",
        "Keping Yu"
      ],
      "published": "2022-07-17T16:52:51Z",
      "updated": "2022-07-17T16:52:51Z",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.SI",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.08226v1",
      "landing_url": "https://arxiv.org/abs/2207.08226v1",
      "doi": "https://doi.org/10.1109/TII.2022.3186891"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article centers on deterministic scheduling for ultra-low latency IIoT communications (TSN-based edge systems) and does not involve Spoken Language Models or speech-language modeling, thus it fails the inclusion criteria and aligns with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article centers on deterministic scheduling for ultra-low latency IIoT communications (TSN-based edge systems) and does not involve Spoken Language Models or speech-language modeling, thus it fails the inclusion criteria and aligns with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on deterministic scheduling methods for low latency communication in Industrial Internet of Things, which is unrelated to Spoken Language Models or full-duplex spoken interaction with speech as a core modeled element, as required by the inclusion criteria; it also lacks any mention of speech language modeling, spoken language model architectures, training, or evaluation relevant to the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on deterministic scheduling methods for low latency communication in Industrial Internet of Things, which is unrelated to Spoken Language Models or full-duplex spoken interaction with speech as a core modeled element, as required by the inclusion criteria; it also lacks any mention of speech language modeling, spoken language model architectures, training, or evaluation relevant to the criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "NusaCrowd: A Call for Open and Reproducible NLP Research in Indonesian Languages",
    "abstract": "At the center of the underlying issues that halt Indonesian natural language processing (NLP) research advancement, we find data scarcity. Resources in Indonesian languages, especially the local ones, are extremely scarce and underrepresented. Many Indonesian researchers do not publish their dataset. Furthermore, the few public datasets that we have are scattered across different platforms, thus makes performing reproducible and data-centric research in Indonesian NLP even more arduous. Rising to this challenge, we initiate the first Indonesian NLP crowdsourcing effort, NusaCrowd. NusaCrowd strives to provide the largest datasheets aggregation with standardized data loading for NLP tasks in all Indonesian languages. By enabling open and centralized access to Indonesian NLP resources, we hope NusaCrowd can tackle the data scarcity problem hindering NLP progress in Indonesia and bring NLP practitioners to move towards collaboration.",
    "metadata": {
      "arxiv_id": "2207.10524",
      "title": "NusaCrowd: A Call for Open and Reproducible NLP Research in Indonesian Languages",
      "summary": "At the center of the underlying issues that halt Indonesian natural language processing (NLP) research advancement, we find data scarcity. Resources in Indonesian languages, especially the local ones, are extremely scarce and underrepresented. Many Indonesian researchers do not publish their dataset. Furthermore, the few public datasets that we have are scattered across different platforms, thus makes performing reproducible and data-centric research in Indonesian NLP even more arduous. Rising to this challenge, we initiate the first Indonesian NLP crowdsourcing effort, NusaCrowd. NusaCrowd strives to provide the largest datasheets aggregation with standardized data loading for NLP tasks in all Indonesian languages. By enabling open and centralized access to Indonesian NLP resources, we hope NusaCrowd can tackle the data scarcity problem hindering NLP progress in Indonesia and bring NLP practitioners to move towards collaboration.",
      "authors": [
        "Samuel Cahyawijaya",
        "Alham Fikri Aji",
        "Holy Lovenia",
        "Genta Indra Winata",
        "Bryan Wilie",
        "Rahmad Mahendra",
        "Fajri Koto",
        "David Moeljadi",
        "Karissa Vincentio",
        "Ade Romadhony",
        "Ayu Purwarianti"
      ],
      "published": "2022-07-21T15:05:42Z",
      "updated": "2022-08-01T16:55:04Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.10524v2",
      "landing_url": "https://arxiv.org/abs/2207.10524v2",
      "doi": "https://doi.org/10.48550/arXiv.2207.10524"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about data scarcity and open Indonesian NLP datasets, not about Spoken Language Models or speech-centric modeling; it does not discuss end-to-end or hybrid speech-LLM architectures, and there is no focus on speech modeling or real-time interaction, so it does not meet the inclusion criteria and does not trigger the stated exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about data scarcity and open Indonesian NLP datasets, not about Spoken Language Models or speech-centric modeling; it does not discuss end-to-end or hybrid speech-LLM architectures, and there is no focus on speech modeling or real-time interaction, so it does not meet the inclusion criteria and does not trigger the stated exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on creating a crowdsourced dataset aggregation for Indonesian NLP resources, addressing data scarcity, but it does not mention any spoken language modeling, speech as the core modeling object, full-duplex or synchronous spoken interaction, or any of the specific technical criteria related to Spoken Language Models or Full-Duplex Spoken Language Models detailed in the inclusion criteria. It appears more like a resource-building effort for text-based NLP rather than a study on spoken language model architectures, training, or evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on creating a crowdsourced dataset aggregation for Indonesian NLP resources, addressing data scarcity, but it does not mention any spoken language modeling, speech as the core modeling object, full-duplex or synchronous spoken interaction, or any of the specific technical criteria related to Spoken Language Models or Full-Duplex Spoken Language Models detailed in the inclusion criteria. It appears more like a resource-building effort for text-based NLP rather than a study on spoken language model architectures, training, or evaluation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Towards overcoming data scarcity in materials science: unifying models and datasets with a mixture of experts framework",
    "abstract": "While machine learning has emerged in recent years as a useful tool for rapid prediction of materials properties, generating sufficient data to reliably train models without overfitting is still impractical for many applications. Towards overcoming this limitation, we present a general framework for leveraging complementary information across different models and datasets for accurate prediction of data scarce materials properties. Our approach, based on a machine learning paradigm called mixture of experts, outperforms pairwise transfer learning on 16 of 19 materials property regression tasks, performing comparably on the remaining three. Unlike pairwise transfer learning, our framework automatically learns to combine information from multiple source tasks in a single training run, alleviating the need for brute-force experiments to determine which source task to transfer from. The approach also provides an interpretable, model-agnostic, and scalable mechanism to transfer information from an arbitrary number of models and datasets to any downstream property prediction task. We anticipate the performance of our framework will further improve as better model architectures, new pre-training tasks, and larger materials datasets are developed by the community.",
    "metadata": {
      "arxiv_id": "2207.13880",
      "title": "Towards overcoming data scarcity in materials science: unifying models and datasets with a mixture of experts framework",
      "summary": "While machine learning has emerged in recent years as a useful tool for rapid prediction of materials properties, generating sufficient data to reliably train models without overfitting is still impractical for many applications. Towards overcoming this limitation, we present a general framework for leveraging complementary information across different models and datasets for accurate prediction of data scarce materials properties. Our approach, based on a machine learning paradigm called mixture of experts, outperforms pairwise transfer learning on 16 of 19 materials property regression tasks, performing comparably on the remaining three. Unlike pairwise transfer learning, our framework automatically learns to combine information from multiple source tasks in a single training run, alleviating the need for brute-force experiments to determine which source task to transfer from. The approach also provides an interpretable, model-agnostic, and scalable mechanism to transfer information from an arbitrary number of models and datasets to any downstream property prediction task. We anticipate the performance of our framework will further improve as better model architectures, new pre-training tasks, and larger materials datasets are developed by the community.",
      "authors": [
        "Rees Chang",
        "Yu-Xiong Wang",
        "Elif Ertekin"
      ],
      "published": "2022-07-28T04:34:41Z",
      "updated": "2022-07-28T04:34:41Z",
      "categories": [
        "cond-mat.mtrl-sci"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.13880v1",
      "landing_url": "https://arxiv.org/abs/2207.13880v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.13880"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目未以 Spoken Language Models (SLMs) 或等同概念為核心，語音作為主要建模與推理對象；內容聚焦於材料科學的資料稀缺與跨模型/數據集的混合專家框架，未討論語音／語音語言建模、同步互動等相關評估或架構，因此不符合 inclusion criteria，亦符合排除條件的範圍。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目未以 Spoken Language Models (SLMs) 或等同概念為核心，語音作為主要建模與推理對象；內容聚焦於材料科學的資料稀缺與跨模型/數據集的混合專家框架，未討論語音／語音語言建模、同步互動等相關評估或架構，因此不符合 inclusion criteria，亦符合排除條件的範圍。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a machine learning framework to overcome data scarcity in materials science, without discussing any spoken language models, speech as core modeling objects, or full-duplex spoken interaction capabilities. It does not meet any of the inclusion criteria related to spoken language models and their interaction properties, nor does it address speech language modeling or synchronization aspects.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a machine learning framework to overcome data scarcity in materials science, without discussing any spoken language models, speech as core modeling objects, or full-duplex spoken interaction capabilities. It does not meet any of the inclusion criteria related to spoken language models and their interaction properties, nor does it address speech language modeling or synchronization aspects.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "RASIPAM: Interactive Pattern Mining of Multivariate Event Sequences in Racket Sports",
    "abstract": "Experts in racket sports like tennis and badminton use tactical analysis to gain insight into competitors' playing styles. Many data-driven methods apply pattern mining to racket sports data -- which is often recorded as multivariate event sequences -- to uncover sports tactics. However, tactics obtained in this way are often inconsistent with those deduced by experts through their domain knowledge, which can be confusing to those experts. This work introduces RASIPAM, a RAcket-Sports Interactive PAttern Mining system, which allows experts to incorporate their knowledge into data mining algorithms to discover meaningful tactics interactively. RASIPAM consists of a constraint-based pattern mining algorithm that responds to the analysis demands of experts: Experts provide suggestions for finding tactics in intuitive written language, and these suggestions are translated into constraints to run the algorithm. RASIPAM further introduces a tailored visual interface that allows experts to compare the new tactics with the original ones and decide whether to apply a given adjustment. This interactive workflow iteratively progresses until experts are satisfied with all tactics. We conduct a quantitative experiment to show that our algorithm supports real-time interaction. Two case studies in tennis and in badminton respectively, each involving two domain experts, are conducted to show the effectiveness and usefulness of the system.",
    "metadata": {
      "arxiv_id": "2208.00671",
      "title": "RASIPAM: Interactive Pattern Mining of Multivariate Event Sequences in Racket Sports",
      "summary": "Experts in racket sports like tennis and badminton use tactical analysis to gain insight into competitors' playing styles. Many data-driven methods apply pattern mining to racket sports data -- which is often recorded as multivariate event sequences -- to uncover sports tactics. However, tactics obtained in this way are often inconsistent with those deduced by experts through their domain knowledge, which can be confusing to those experts. This work introduces RASIPAM, a RAcket-Sports Interactive PAttern Mining system, which allows experts to incorporate their knowledge into data mining algorithms to discover meaningful tactics interactively. RASIPAM consists of a constraint-based pattern mining algorithm that responds to the analysis demands of experts: Experts provide suggestions for finding tactics in intuitive written language, and these suggestions are translated into constraints to run the algorithm. RASIPAM further introduces a tailored visual interface that allows experts to compare the new tactics with the original ones and decide whether to apply a given adjustment. This interactive workflow iteratively progresses until experts are satisfied with all tactics. We conduct a quantitative experiment to show that our algorithm supports real-time interaction. Two case studies in tennis and in badminton respectively, each involving two domain experts, are conducted to show the effectiveness and usefulness of the system.",
      "authors": [
        "Jiang Wu",
        "Dongyu Liu",
        "Ziyang Guo",
        "Yingcai Wu"
      ],
      "published": "2022-08-01T08:04:14Z",
      "updated": "2022-11-28T08:50:46Z",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.00671v4",
      "landing_url": "https://arxiv.org/abs/2208.00671v4",
      "doi": "https://doi.org/10.1109/TVCG.2022.3209452"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on interactive pattern mining in racket sports data and multivariate event sequences; it does not center on Spoken Language Models or any form of speech modeling, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on interactive pattern mining in racket sports data and multivariate event sequences; it does not center on Spoken Language Models or any form of speech modeling, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on interactive pattern mining for racket sports, with emphasis on data mining and visualization for tactical analysis; they do not discuss spoken language models, speech language modeling, full-duplex interaction, or any related speech modeling or synchronization concepts as specified in the inclusion criteria. The research neither models speech as a core object nor discusses relevant architectures, training, or evaluation related to spoken language models. Therefore, this study clearly does not meet the inclusion criteria and falls outside the scope of the specified research area.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on interactive pattern mining for racket sports, with emphasis on data mining and visualization for tactical analysis; they do not discuss spoken language models, speech language modeling, full-duplex interaction, or any related speech modeling or synchronization concepts as specified in the inclusion criteria. The research neither models speech as a core object nor discusses relevant architectures, training, or evaluation related to spoken language models. Therefore, this study clearly does not meet the inclusion criteria and falls outside the scope of the specified research area.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Design Guidelines for Apache Kafka Driven Data Management and Distribution in Smart Cities",
    "abstract": "Smart city management is going through a remarkable transition, in terms of quality and diversity of services provided to the end-users. The stakeholders that deliver pervasive applications are now able to address fundamental challenges in the big data value chain, from data acquisition, data analysis and processing, data storage and curation, and data visualisation in real scenarios. Industry 4.0 is pushing this trend forward, demanding for servitization of products and data, also for the smart cities sector where humans, sensors and devices are operating in strict collaboration. The data produced by the ubiquitous devices must be processed quickly to allow the implementation of reactive services such as situational awareness, video surveillance and geo-localization, while always ensuring the safety and privacy of involved citizens. This paper proposes a modular architecture to (i) leverage innovative technologies for data acquisition, management and distribution (such as Apache Kafka and Apache NiFi), (ii) develop a multi-layer engineering solution for revealing valuable and hidden societal knowledge in smart cities environment, and (iii) tackle the main issues in tasks involving complex data flows and provide general guidelines to solve them. We derived some guidelines from an experimental setting performed together with leading industrial technical departments to accomplish an efficient system for monitoring and servitization of smart city assets, with a scalable platform that confirms its usefulness in numerous smart city use cases with different needs.",
    "metadata": {
      "arxiv_id": "2208.00786",
      "title": "Design Guidelines for Apache Kafka Driven Data Management and Distribution in Smart Cities",
      "summary": "Smart city management is going through a remarkable transition, in terms of quality and diversity of services provided to the end-users. The stakeholders that deliver pervasive applications are now able to address fundamental challenges in the big data value chain, from data acquisition, data analysis and processing, data storage and curation, and data visualisation in real scenarios. Industry 4.0 is pushing this trend forward, demanding for servitization of products and data, also for the smart cities sector where humans, sensors and devices are operating in strict collaboration. The data produced by the ubiquitous devices must be processed quickly to allow the implementation of reactive services such as situational awareness, video surveillance and geo-localization, while always ensuring the safety and privacy of involved citizens. This paper proposes a modular architecture to (i) leverage innovative technologies for data acquisition, management and distribution (such as Apache Kafka and Apache NiFi), (ii) develop a multi-layer engineering solution for revealing valuable and hidden societal knowledge in smart cities environment, and (iii) tackle the main issues in tasks involving complex data flows and provide general guidelines to solve them. We derived some guidelines from an experimental setting performed together with leading industrial technical departments to accomplish an efficient system for monitoring and servitization of smart city assets, with a scalable platform that confirms its usefulness in numerous smart city use cases with different needs.",
      "authors": [
        "Theofanis P. Raptis",
        "Claudio Cicconetti",
        "Manolis Falelakis",
        "Tassos Kanellos",
        "Tomás Pariente Lobo"
      ],
      "published": "2022-08-01T11:57:58Z",
      "updated": "2022-08-01T11:57:58Z",
      "categories": [
        "cs.NI",
        "cs.ET"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.00786v1",
      "landing_url": "https://arxiv.org/abs/2208.00786v1",
      "doi": "https://doi.org/10.1109/ISC255366.2022.9922546"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses data management and distribution infrastructure (Kafka/NiFi) for smart cities, with no focus on Spoken Language Models or speech-language modeling aspects, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses data management and distribution infrastructure (Kafka/NiFi) for smart cities, with no focus on Spoken Language Models or speech-language modeling aspects, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on data management and distribution technologies (such as Apache Kafka) applied to smart cities and does not discuss any spoken language models, speech language modeling or synchronised speech interactions that are core to the inclusion criteria. It also does not mention any relevant keywords (spoken language model, speech language model) or address the specified evaluation metrics related to speech interaction or cognitive parallelism. Therefore, it does not meet the inclusion criteria and falls under exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on data management and distribution technologies (such as Apache Kafka) applied to smart cities and does not discuss any spoken language models, speech language modeling or synchronised speech interactions that are core to the inclusion criteria. It also does not mention any relevant keywords (spoken language model, speech language model) or address the specified evaluation metrics related to speech interaction or cognitive parallelism. Therefore, it does not meet the inclusion criteria and falls under exclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A data-driven modular architecture with denoising autoencoders for health indicator construction in a manufacturing process",
    "abstract": "Within the field of prognostics and health management (PHM), health indicators (HI) can be used to aid the production and, e.g. schedule maintenance and avoid failures. However, HI is often engineered to a specific process and typically requires large amounts of historical data for set-up. This is especially a challenge for SMEs, which often lack sufficient resources and knowledge to benefit from PHM. In this paper, we propose ModularHI, a modular approach in the construction of HI for a system without historical data. With ModularHI, the operator chooses which sensor inputs are available, and then ModularHI will compute a baseline model based on data collected during a burn-in state. This baseline model will then be used to detect if the system starts to degrade over time. We test the ModularHI on two open datasets, CMAPSS and N-CMAPSS. Results from the former dataset showcase our system's ability to detect degradation, while results from the latter point to directions for further research within the area. The results shows that our novel approach is able to detect system degradation without historical data.",
    "metadata": {
      "arxiv_id": "2208.05208",
      "title": "A data-driven modular architecture with denoising autoencoders for health indicator construction in a manufacturing process",
      "summary": "Within the field of prognostics and health management (PHM), health indicators (HI) can be used to aid the production and, e.g. schedule maintenance and avoid failures. However, HI is often engineered to a specific process and typically requires large amounts of historical data for set-up. This is especially a challenge for SMEs, which often lack sufficient resources and knowledge to benefit from PHM. In this paper, we propose ModularHI, a modular approach in the construction of HI for a system without historical data. With ModularHI, the operator chooses which sensor inputs are available, and then ModularHI will compute a baseline model based on data collected during a burn-in state. This baseline model will then be used to detect if the system starts to degrade over time. We test the ModularHI on two open datasets, CMAPSS and N-CMAPSS. Results from the former dataset showcase our system's ability to detect degradation, while results from the latter point to directions for further research within the area. The results shows that our novel approach is able to detect system degradation without historical data.",
      "authors": [
        "Emil Blixt Hansen",
        "Helge Langseth",
        "Nadeem Iftikhar",
        "Simon Bøgh"
      ],
      "published": "2022-08-10T08:12:43Z",
      "updated": "2022-08-10T08:12:43Z",
      "categories": [
        "cs.LG",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.05208v1",
      "landing_url": "https://arxiv.org/abs/2208.05208v1",
      "doi": "https://doi.org/10.48550/arXiv.2208.05208"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on prognostics and health management in manufacturing using a data-driven modular HI construction method; there is no discussion of Spoken Language Models, speech-language modeling, turn-taking, or real-time conversational interaction, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on prognostics and health management in manufacturing using a data-driven modular HI construction method; there is no discussion of Spoken Language Models, speech-language modeling, turn-taking, or real-time conversational interaction, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on health indicator construction in manufacturing processes using denoising autoencoders, with no mention of spoken language models, speech language modeling, or any related concepts such as full-duplex interaction, speech–LLM architectures, or evaluation of speech-related models. It does not address the core topic of speech modeling or full-duplex spoken language modeling as required by the inclusion criteria, nor does it meet any of the key conceptual requirements. Therefore, it clearly should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on health indicator construction in manufacturing processes using denoising autoencoders, with no mention of spoken language models, speech language modeling, or any related concepts such as full-duplex interaction, speech–LLM architectures, or evaluation of speech-related models. It does not address the core topic of speech modeling or full-duplex spoken language modeling as required by the inclusion criteria, nor does it meet any of the key conceptual requirements. Therefore, it clearly should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Beyond the Blue Sky of Multimodal Interaction: A Centennial Vision of Interplanetary Virtual Spaces in Turn-based Metaverse",
    "abstract": "Human habitation across multiple planets requires communication and social connection between planets. When the infrastructure of a deep space network becomes mature, immersive cyberspace, known as the Metaverse, can exchange diversified user data and host multitudinous virtual worlds. Nevertheless, such immersive cyberspace unavoidably encounters latency in minutes, and thus operates in a turn-taking manner. This Blue Sky paper illustrates a vision of an interplanetary Metaverse that connects Earthian and Martian users in a turn-based Metaverse. Accordingly, we briefly discuss several grand challenges to catalyze research initiatives for the `Digital Big Bang' on Mars.",
    "metadata": {
      "arxiv_id": "2208.05517",
      "title": "Beyond the Blue Sky of Multimodal Interaction: A Centennial Vision of Interplanetary Virtual Spaces in Turn-based Metaverse",
      "summary": "Human habitation across multiple planets requires communication and social connection between planets. When the infrastructure of a deep space network becomes mature, immersive cyberspace, known as the Metaverse, can exchange diversified user data and host multitudinous virtual worlds. Nevertheless, such immersive cyberspace unavoidably encounters latency in minutes, and thus operates in a turn-taking manner. This Blue Sky paper illustrates a vision of an interplanetary Metaverse that connects Earthian and Martian users in a turn-based Metaverse. Accordingly, we briefly discuss several grand challenges to catalyze research initiatives for the `Digital Big Bang' on Mars.",
      "authors": [
        "Lik Hang Lee",
        "Carlos Bermejo Fernandez",
        "Ahmad Alhilal",
        "Tristan Braud",
        "Simo Hosio",
        "Pan Hui",
        "Esmée Henrieke Henrieke Anne de Haas"
      ],
      "published": "2022-07-28T07:26:57Z",
      "updated": "2022-07-28T07:26:57Z",
      "categories": [
        "cs.HC",
        "physics.pop-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.05517v1",
      "landing_url": "https://arxiv.org/abs/2208.05517v1",
      "doi": "https://doi.org/10.48550/arXiv.2208.05517"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "該標題與摘要聚焦於跨行星多模態互動與 turn-based 虛擬空間的願景，未以 Spoken Language Models 或等價的語音語言建模為核心，也未討論語音建模的架構、訓練與評估，因此不符合本研究的 inclusion 條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "該標題與摘要聚焦於跨行星多模態互動與 turn-based 虛擬空間的願景，未以 Spoken Language Models 或等價的語音語言建模為核心，也未討論語音建模的架構、訓練與評估，因此不符合本研究的 inclusion 條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses an interplanetary Metaverse focusing on turn-based multimodal interaction under high latency conditions, but it does not focus on spoken language models or speech language modeling as the core topic, nor does it discuss model architectures, training, or evaluation related to spoken language modeling or full-duplex interaction, which are key inclusion criteria. It mainly presents a conceptual, high-level vision without addressing detailed speech modeling or real-time interaction capabilities central to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses an interplanetary Metaverse focusing on turn-based multimodal interaction under high latency conditions, but it does not focus on spoken language models or speech language modeling as the core topic, nor does it discuss model architectures, training, or evaluation related to spoken language modeling or full-duplex interaction, which are key inclusion criteria. It mainly presents a conceptual, high-level vision without addressing detailed speech modeling or real-time interaction capabilities central to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Transfer Ranking in Finance: Applications to Cross-Sectional Momentum with Data Scarcity",
    "abstract": "Cross-sectional strategies are a classical and popular trading style, with recent high performing variants incorporating sophisticated neural architectures. While these strategies have been applied successfully to data-rich settings involving mature assets with long histories, deploying them on instruments with limited samples generally produce over-fitted models with degraded performance. In this paper, we introduce Fused Encoder Networks -- a novel and hybrid parameter-sharing transfer ranking model. The model fuses information extracted using an encoder-attention module operated on a source dataset with a similar but separate module focused on a smaller target dataset of interest. This mitigates the issue of models with poor generalisability that are a consequence of training on scarce target data. Additionally, the self-attention mechanism enables interactions among instruments to be accounted for, not just at the loss level during model training, but also at inference time. Focusing on momentum applied to the top ten cryptocurrencies by market capitalisation as a demonstrative use-case, the Fused Encoder Networks outperforms the reference benchmarks on most performance measures, delivering a three-fold boost in the Sharpe ratio over classical momentum as well as an improvement of approximately 50% against the best benchmark model without transaction costs. It continues outperforming baselines even after accounting for the high transaction costs associated with trading cryptocurrencies.",
    "metadata": {
      "arxiv_id": "2208.09968",
      "title": "Transfer Ranking in Finance: Applications to Cross-Sectional Momentum with Data Scarcity",
      "summary": "Cross-sectional strategies are a classical and popular trading style, with recent high performing variants incorporating sophisticated neural architectures. While these strategies have been applied successfully to data-rich settings involving mature assets with long histories, deploying them on instruments with limited samples generally produce over-fitted models with degraded performance. In this paper, we introduce Fused Encoder Networks -- a novel and hybrid parameter-sharing transfer ranking model. The model fuses information extracted using an encoder-attention module operated on a source dataset with a similar but separate module focused on a smaller target dataset of interest. This mitigates the issue of models with poor generalisability that are a consequence of training on scarce target data. Additionally, the self-attention mechanism enables interactions among instruments to be accounted for, not just at the loss level during model training, but also at inference time. Focusing on momentum applied to the top ten cryptocurrencies by market capitalisation as a demonstrative use-case, the Fused Encoder Networks outperforms the reference benchmarks on most performance measures, delivering a three-fold boost in the Sharpe ratio over classical momentum as well as an improvement of approximately 50% against the best benchmark model without transaction costs. It continues outperforming baselines even after accounting for the high transaction costs associated with trading cryptocurrencies.",
      "authors": [
        "Daniel Poh",
        "Stephen Roberts",
        "Stefan Zohren"
      ],
      "published": "2022-08-21T21:34:11Z",
      "updated": "2023-02-21T16:03:35Z",
      "categories": [
        "q-fin.TR",
        "cs.IR",
        "cs.LG",
        "q-fin.PM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.09968v3",
      "landing_url": "https://arxiv.org/abs/2208.09968v3",
      "doi": "https://doi.org/10.48550/arXiv.2208.09968"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This study centers on transfer ranking models for momentum strategies in finance and does not involve Spoken Language Models or speech-language processing, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This study centers on transfer ranking models for momentum strategies in finance and does not involve Spoken Language Models or speech-language processing, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on transfer ranking models applied to cross-sectional momentum in finance, specifically cryptocurrencies, which does not relate to Spoken Language Models, speech language modeling, or full-duplex spoken interaction as specified in the inclusion criteria. There is no discussion of speech as the core modeling focus, linguistic interactions, or synchronous speech processing; hence it does not meet any inclusion criteria and falls outside the thematic scope required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on transfer ranking models applied to cross-sectional momentum in finance, specifically cryptocurrencies, which does not relate to Spoken Language Models, speech language modeling, or full-duplex spoken interaction as specified in the inclusion criteria. There is no discussion of speech as the core modeling focus, linguistic interactions, or synchronous speech processing; hence it does not meet any inclusion criteria and falls outside the thematic scope required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Schedulers Synchronization Supporting Ultra Reliable Low Latency Communications (URLLC) in Cloud-RAN over Virtualised Mesh PON",
    "abstract": "We propose a mechanism to support URLLC Open-RAN ultra-low latency over a MESH-PON, serving dense deployment of small cell and MEC nodes in an access network. We show the possibility, under given assumptions, to achieve application-to-application end-to-end latency below 1ms.",
    "metadata": {
      "arxiv_id": "2208.10938",
      "title": "Schedulers Synchronization Supporting Ultra Reliable Low Latency Communications (URLLC) in Cloud-RAN over Virtualised Mesh PON",
      "summary": "We propose a mechanism to support URLLC Open-RAN ultra-low latency over a MESH-PON, serving dense deployment of small cell and MEC nodes in an access network. We show the possibility, under given assumptions, to achieve application-to-application end-to-end latency below 1ms.",
      "authors": [
        "Sandip Das",
        "Frank Slyne",
        "Daniel Kilper",
        "Marco Ruffini"
      ],
      "published": "2022-08-23T13:06:12Z",
      "updated": "2022-08-23T13:06:12Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.10938v1",
      "landing_url": "https://arxiv.org/abs/2208.10938v1",
      "doi": "https://doi.org/10.48550/arXiv.2208.10938"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on URLLC networking and Cloud-RAN scheduling rather than spoken language modeling or speech interaction, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on URLLC networking and Cloud-RAN scheduling rather than spoken language modeling or speech interaction, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on networking mechanisms to support ultra-low latency communications in Cloud-RAN over MESH-PON, without discussing spoken language models, speech language modeling, or related architectures and evaluations central to spoken language processing as required by the inclusion criteria. It does not address any speech modeling, synchronization, or interaction behaviors such as turn-taking or full-duplex communication, and lacks relevant keywords such as spoken language models or speech language models in the title or abstract. Therefore, it does not meet the inclusion criteria and falls under exclusion criteria for unrelated subject matter.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on networking mechanisms to support ultra-low latency communications in Cloud-RAN over MESH-PON, without discussing spoken language models, speech language modeling, or related architectures and evaluations central to spoken language processing as required by the inclusion criteria. It does not address any speech modeling, synchronization, or interaction behaviors such as turn-taking or full-duplex communication, and lacks relevant keywords such as spoken language models or speech language models in the title or abstract. Therefore, it does not meet the inclusion criteria and falls under exclusion criteria for unrelated subject matter.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Intelligent Random Access Framework for Massive and Ultra-Reliable Low Latency IoT Communications",
    "abstract": "The Internet of Things (IoT) enables smart cities to achieve the vision of connecting everything by smartly linking gadgets without the need for human interaction. However, due to the rapid proliferation of IoT devices, the amount of data produced accounts for a significant share of all communication services. Hence, 6G-enabled specifications for wireless networks are required to enable both massive and ultra-reliable low-latency access to such a hybrid IoT network. In this article, we propose a smart hybrid random access (SH-RA) scheme for massive connections and ultra-reliable low-latency access in the network architecture of IoT communications. According to numerical results, compared to the other baseline schemes, the SH-RA framework enormously enhances the total access probability and fulfills the quality-of-service (QoS) requirements.",
    "metadata": {
      "arxiv_id": "2208.11956",
      "title": "Intelligent Random Access Framework for Massive and Ultra-Reliable Low Latency IoT Communications",
      "summary": "The Internet of Things (IoT) enables smart cities to achieve the vision of connecting everything by smartly linking gadgets without the need for human interaction. However, due to the rapid proliferation of IoT devices, the amount of data produced accounts for a significant share of all communication services. Hence, 6G-enabled specifications for wireless networks are required to enable both massive and ultra-reliable low-latency access to such a hybrid IoT network. In this article, we propose a smart hybrid random access (SH-RA) scheme for massive connections and ultra-reliable low-latency access in the network architecture of IoT communications. According to numerical results, compared to the other baseline schemes, the SH-RA framework enormously enhances the total access probability and fulfills the quality-of-service (QoS) requirements.",
      "authors": [
        "Muhammad Waleed Aftab",
        "Ishtiaq Ahmad"
      ],
      "published": "2022-08-25T09:23:08Z",
      "updated": "2022-08-25T09:23:08Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.11956v1",
      "landing_url": "https://arxiv.org/abs/2208.11956v1",
      "doi": "https://doi.org/10.48550/arXiv.2208.11956"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on IoT random access and ultra-reliable low-latency communication, with no emphasis on Spoken Language Models or speech-language modeling concepts, so it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on IoT random access and ultra-reliable low-latency communication, with no emphasis on Spoken Language Models or speech-language modeling concepts, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract pertain to IoT communications and wireless network frameworks, focusing on random access methods for massive and ultra-reliable low-latency communication; they do not address spoken language models, speech language modeling, or any related synchronization or conversational interaction aspects required by the inclusion criteria. Hence, the study does not fit the thematic scope focused on spoken language models or full-duplex speech interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract pertain to IoT communications and wireless network frameworks, focusing on random access methods for massive and ultra-reliable low-latency communication; they do not address spoken language models, speech language modeling, or any related synchronization or conversational interaction aspects required by the inclusion criteria. Hence, the study does not fit the thematic scope focused on spoken language models or full-duplex speech interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Mask-Mediator-Wrapper: A revised mediator-wrapper architecture for heterogeneous data source integration",
    "abstract": "This paper deals with the mediator-wrapper architecture. It is an important architectural pattern that enables a more flexible and modular architecture in opposition to monolithic architectures for data source integration systems. This paper identifies certain realistic and concrete scenarios where the mediator-wrapper architecture underperforms. These issues are addressed with the extension of the architecture via the mask component type. The mask component is detailed so it can be reasoned about without prescribing a concrete programming language or paradigm. The benefits of the new mask-mediator-wrapper architecture are analytically proven in relevant scenarios. One of the applications of the new architecture is envisioned for modern data sources integration systems backing Big data processing.",
    "metadata": {
      "arxiv_id": "2208.12319",
      "title": "Mask-Mediator-Wrapper: A revised mediator-wrapper architecture for heterogeneous data source integration",
      "summary": "This paper deals with the mediator-wrapper architecture. It is an important architectural pattern that enables a more flexible and modular architecture in opposition to monolithic architectures for data source integration systems. This paper identifies certain realistic and concrete scenarios where the mediator-wrapper architecture underperforms. These issues are addressed with the extension of the architecture via the mask component type. The mask component is detailed so it can be reasoned about without prescribing a concrete programming language or paradigm. The benefits of the new mask-mediator-wrapper architecture are analytically proven in relevant scenarios. One of the applications of the new architecture is envisioned for modern data sources integration systems backing Big data processing.",
      "authors": [
        "Juraj Dončević",
        "Krešimir Fertalj",
        "Mario Brčić",
        "Agneza Krajna"
      ],
      "published": "2022-08-25T19:38:18Z",
      "updated": "2022-08-25T19:38:18Z",
      "categories": [
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.12319v1",
      "landing_url": "https://arxiv.org/abs/2208.12319v1",
      "doi": "https://doi.org/10.3390/app13042471"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on a mediator-wrapper architecture for heterogeneous data sources and does not address Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on a mediator-wrapper architecture for heterogeneous data sources and does not address Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an architectural pattern for heterogeneous data source integration and does not mention spoken language models, speech language modeling, full-duplex interaction, or any speech-specific modeling or interaction features required by the inclusion criteria. It also lacks any discussion of speech tokens, synchronization, or evaluation for spoken language systems, thus it does not meet the thematic inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an architectural pattern for heterogeneous data source integration and does not mention spoken language models, speech language modeling, full-duplex interaction, or any speech-specific modeling or interaction features required by the inclusion criteria. It also lacks any discussion of speech tokens, synchronization, or evaluation for spoken language systems, thus it does not meet the thematic inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Toward Robust Graph Semi-Supervised Learning against Extreme Data Scarcity",
    "abstract": "The success of graph neural networks on graph-based web mining highly relies on abundant human-annotated data, which is laborious to obtain in practice. When only few labeled nodes are available, how to improve their robustness is a key to achieve replicable and sustainable graph semi-supervised learning. Though self-training has been shown to be powerful for semi-supervised learning, its application on graph-structured data may fail because (1) larger receptive fields are not leveraged to capture long-range node interactions, which exacerbates the difficulty of propagating feature-label patterns from labeled nodes to unlabeled nodes; and (2) limited labeled data makes it challenging to learn well-separated decision boundaries for different node classes without explicitly capturing the underlying semantic structure. To address the challenges of capturing informative structural and semantic knowledge, we propose a new graph data augmentation framework, AGST (Augmented Graph Self-Training), which is built with two new (i.e., structural and semantic) augmentation modules on top of a decoupled GST backbone. In this work, we investigate whether this novel framework can learn a robust graph predictive model under the low-data context. We conduct comprehensive evaluations on semi-supervised node classification under different scenarios of limited labeled-node data. The experimental results demonstrate the unique contributions of the novel data augmentation framework for node classification with few labeled data.",
    "metadata": {
      "arxiv_id": "2208.12422",
      "title": "Toward Robust Graph Semi-Supervised Learning against Extreme Data Scarcity",
      "summary": "The success of graph neural networks on graph-based web mining highly relies on abundant human-annotated data, which is laborious to obtain in practice. When only few labeled nodes are available, how to improve their robustness is a key to achieve replicable and sustainable graph semi-supervised learning. Though self-training has been shown to be powerful for semi-supervised learning, its application on graph-structured data may fail because (1) larger receptive fields are not leveraged to capture long-range node interactions, which exacerbates the difficulty of propagating feature-label patterns from labeled nodes to unlabeled nodes; and (2) limited labeled data makes it challenging to learn well-separated decision boundaries for different node classes without explicitly capturing the underlying semantic structure. To address the challenges of capturing informative structural and semantic knowledge, we propose a new graph data augmentation framework, AGST (Augmented Graph Self-Training), which is built with two new (i.e., structural and semantic) augmentation modules on top of a decoupled GST backbone. In this work, we investigate whether this novel framework can learn a robust graph predictive model under the low-data context. We conduct comprehensive evaluations on semi-supervised node classification under different scenarios of limited labeled-node data. The experimental results demonstrate the unique contributions of the novel data augmentation framework for node classification with few labeled data.",
      "authors": [
        "Kaize Ding",
        "Elnaz Nouri",
        "Guoqing Zheng",
        "Huan Liu",
        "Ryen White"
      ],
      "published": "2022-08-26T03:36:01Z",
      "updated": "2022-12-11T07:29:56Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.12422v2",
      "landing_url": "https://arxiv.org/abs/2208.12422v2",
      "doi": "https://doi.org/10.48550/arXiv.2208.12422"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on graph semi-supervised learning and data augmentation, with no focus on Spoken Language Models or speech language modeling; therefore it does not meet the inclusion criteria requiring SLM/FD-SLM core focus.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on graph semi-supervised learning and data augmentation, with no focus on Spoken Language Models or speech language modeling; therefore it does not meet the inclusion criteria requiring SLM/FD-SLM core focus.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on graph neural networks and semi-supervised learning with graph data under conditions of extreme data scarcity, which is unrelated to Spoken Language Models or full-duplex spoken interaction systems as specified in the inclusion criteria. It does not address speech or acoustic signal modeling, nor does it discuss architectures or synchronization related to speech or spoken language models. Therefore, it does not meet the inclusion criteria and is clearly outside the scope of the required research area.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on graph neural networks and semi-supervised learning with graph data under conditions of extreme data scarcity, which is unrelated to Spoken Language Models or full-duplex spoken interaction systems as specified in the inclusion criteria. It does not address speech or acoustic signal modeling, nor does it discuss architectures or synchronization related to speech or spoken language models. Therefore, it does not meet the inclusion criteria and is clearly outside the scope of the required research area.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Turn-Taking Prediction for Natural Conversational Speech",
    "abstract": "While a streaming voice assistant system has been used in many applications, this system typically focuses on unnatural, one-shot interactions assuming input from a single voice query without hesitation or disfluency. However, a common conversational utterance often involves multiple queries with turn-taking, in addition to disfluencies. These disfluencies include pausing to think, hesitations, word lengthening, filled pauses and repeated phrases. This makes doing speech recognition with conversational speech, including one with multiple queries, a challenging task. To better model the conversational interaction, it is critical to discriminate disfluencies and end of query in order to allow the user to hold the floor for disfluencies while having the system respond as quickly as possible when the user has finished speaking. In this paper, we present a turntaking predictor built on top of the end-to-end (E2E) speech recognizer. Our best system is obtained by jointly optimizing for ASR task and detecting when the user is paused to think or finished speaking. The proposed approach demonstrates over 97% recall rate and 85% precision rate on predicting true turn-taking with only 100 ms latency on a test set designed with 4 types of disfluencies inserted in conversational utterances.",
    "metadata": {
      "arxiv_id": "2208.13321",
      "title": "Turn-Taking Prediction for Natural Conversational Speech",
      "summary": "While a streaming voice assistant system has been used in many applications, this system typically focuses on unnatural, one-shot interactions assuming input from a single voice query without hesitation or disfluency. However, a common conversational utterance often involves multiple queries with turn-taking, in addition to disfluencies. These disfluencies include pausing to think, hesitations, word lengthening, filled pauses and repeated phrases. This makes doing speech recognition with conversational speech, including one with multiple queries, a challenging task. To better model the conversational interaction, it is critical to discriminate disfluencies and end of query in order to allow the user to hold the floor for disfluencies while having the system respond as quickly as possible when the user has finished speaking. In this paper, we present a turntaking predictor built on top of the end-to-end (E2E) speech recognizer. Our best system is obtained by jointly optimizing for ASR task and detecting when the user is paused to think or finished speaking. The proposed approach demonstrates over 97% recall rate and 85% precision rate on predicting true turn-taking with only 100 ms latency on a test set designed with 4 types of disfluencies inserted in conversational utterances.",
      "authors": [
        "Shuo-yiin Chang",
        "Bo Li",
        "Tara N. Sainath",
        "Chao Zhang",
        "Trevor Strohman",
        "Qiao Liang",
        "Yanzhang He"
      ],
      "published": "2022-08-29T01:09:23Z",
      "updated": "2022-08-29T01:09:23Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.13321v1",
      "landing_url": "https://arxiv.org/abs/2208.13321v1",
      "doi": "https://doi.org/10.48550/arXiv.2208.13321"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work addresses turn-taking and low-latency dialogue handling in conversational speech by adding a turn-taking predictor atop an end-to-end ASR, aligning with full-duplex Spoken Language Model themes and evaluating on realistic disfluency scenarios within the last 3 years.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The work addresses turn-taking and low-latency dialogue handling in conversational speech by adding a turn-taking predictor atop an end-to-end ASR, aligning with full-duplex Spoken Language Model themes and evaluating on realistic disfluency scenarios within the last 3 years.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on turn-taking prediction in conversational speech using an end-to-end speech recognition system, addressing challenges like disfluencies and latency, but it primarily centers on an ASR task with turn-taking detection rather than explicitly modeling spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs). The abstract does not mention spoken language modeling per se, nor does it discuss speech–LLM architectures, cognitive parallelism, or multi-dimensional evaluations related to full-duplex interactive capabilities. It aligns more with improving ASR and turn-taking detection latency, without explicit investigation into the key concepts required by the inclusion criteria. Therefore, it meets some relevant aspects (turn-taking, latency), but fails the core inclusion criteria emphasizing spoken language modeling and modeling architectures beyond ASR tasks, and also partially meets exclusion criteria concerning focus on ASR without deeper modeling or interactive modeling of speech language models.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on turn-taking prediction in conversational speech using an end-to-end speech recognition system, addressing challenges like disfluencies and latency, but it primarily centers on an ASR task with turn-taking detection rather than explicitly modeling spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs). The abstract does not mention spoken language modeling per se, nor does it discuss speech–LLM architectures, cognitive parallelism, or multi-dimensional evaluations related to full-duplex interactive capabilities. It aligns more with improving ASR and turn-taking detection latency, without explicit investigation into the key concepts required by the inclusion criteria. Therefore, it meets some relevant aspects (turn-taking, latency), but fails the core inclusion criteria emphasizing spoken language modeling and modeling architectures beyond ASR tasks, and also partially meets exclusion criteria concerning focus on ASR without deeper modeling or interactive modeling of speech language models.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "The paper proposes a turn-taking predictor built on an end-to-end ASR and jointly optimizes ASR with turn-end detection, addressing disfluencies and low (100 ms) latency — but it does not frame the work as a Spoken Language Model, does not mention “spoken language model(s)” or equivalent speech-level language modeling, and focuses on an ASR-centric turn-taking detector rather than full-duplex SLM capabilities (e.g., overlap handling, cognitive parallelism, speech-level LM architectures or learned synchronization). Both junior reviews were split, but the abstract fails the explicit inclusion requirement for SLM framing and meets exclusion criteria (primarily an ASR task), so it should be excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "The paper proposes a turn-taking predictor built on an end-to-end ASR and jointly optimizes ASR with turn-end detection, addressing disfluencies and low (100 ms) latency — but it does not frame the work as a Spoken Language Model, does not mention “spoken language model(s)” or equivalent speech-level language modeling, and focuses on an ASR-centric turn-taking detector rather than full-duplex SLM capabilities (e.g., overlap handling, cognitive parallelism, speech-level LM architectures or learned synchronization). Both junior reviews were split, but the abstract fails the explicit inclusion requirement for SLM framing and meets exclusion criteria (primarily an ASR task), so it should be excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "AudioLM: a Language Modeling Approach to Audio Generation",
    "abstract": "We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.",
    "metadata": {
      "arxiv_id": "2209.03143",
      "title": "AudioLM: a Language Modeling Approach to Audio Generation",
      "summary": "We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.",
      "authors": [
        "Zalán Borsos",
        "Raphaël Marinier",
        "Damien Vincent",
        "Eugene Kharitonov",
        "Olivier Pietquin",
        "Matt Sharifi",
        "Dominik Roblek",
        "Olivier Teboul",
        "David Grangier",
        "Marco Tagliasacchi",
        "Neil Zeghidour"
      ],
      "published": "2022-09-07T13:40:08Z",
      "updated": "2023-07-26T03:52:36Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.03143v2",
      "landing_url": "https://arxiv.org/abs/2209.03143v2",
      "doi": "https://doi.org/10.48550/arXiv.2209.03143"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers AudioLM, which treats audio generation as a language-modeling task over discrete audio tokens, demonstrates speech continuations with preserved speaker identity/prosody, and discusses architecture/training/evaluation relevant to Spoken Language Models; it fits the core SLM concept within the last 3 years and is not limited to ASR/TTS, thus include.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This item centers AudioLM, which treats audio generation as a language-modeling task over discrete audio tokens, demonstrates speech continuations with preserved speaker identity/prosody, and discusses architecture/training/evaluation relevant to Spoken Language Models; it fits the core SLM concept within the last 3 years and is not limited to ASR/TTS, thus include.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces AudioLM, a framework for language modeling of audio sequences with applications including speech continuation and music generation, focusing on audio tokenization and natural speech generation without transcripts; however, it does not explicitly mention spoken language models (SLMs), full-duplex interaction, synchronization, or evaluation on interaction-related metrics, which are core to the inclusion criteria. It mainly addresses audio generation as language modeling but lacks discussion on synchronous dialogue or behavioral arbitration. Therefore, while relevant to speech audio modeling, it may not fully satisfy the emphasis on full-duplex spoken language models and synchronized interactive evaluation required.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study introduces AudioLM, a framework for language modeling of audio sequences with applications including speech continuation and music generation, focusing on audio tokenization and natural speech generation without transcripts; however, it does not explicitly mention spoken language models (SLMs), full-duplex interaction, synchronization, or evaluation on interaction-related metrics, which are core to the inclusion criteria. It mainly addresses audio generation as language modeling but lacks discussion on synchronous dialogue or behavioral arbitration. Therefore, while relevant to speech audio modeling, it may not fully satisfy the emphasis on full-duplex spoken language models and synchronized interactive evaluation required.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "AudioLM indeed casts audio as a language-modeling task and innovates on tokenization and generation (as noted by JuniorNano), but it is a single-direction audio generation system focused on speech/music continuation without any discussion or evaluation of spoken-dialogue modeling, full‑duplex interaction, synchronization, overlap/interrupt handling, low-latency operation, or behavior arbitration (as JuniorMini pointed out); therefore it falls under the exclusion criteria for studies that only address one-way audio synthesis rather than SLMs/FD‑SLMs with interactive/synchronization concerns, so it should be excluded.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "AudioLM indeed casts audio as a language-modeling task and innovates on tokenization and generation (as noted by JuniorNano), but it is a single-direction audio generation system focused on speech/music continuation without any discussion or evaluation of spoken-dialogue modeling, full‑duplex interaction, synchronization, overlap/interrupt handling, low-latency operation, or behavior arbitration (as JuniorMini pointed out); therefore it falls under the exclusion criteria for studies that only address one-way audio synthesis rather than SLMs/FD‑SLMs with interactive/synchronization concerns, so it should be excluded.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "How Much Does Prosody Help Turn-taking? Investigations using Voice Activity Projection Models",
    "abstract": "Turn-taking is a fundamental aspect of human communication and can be described as the ability to take turns, project upcoming turn shifts, and supply backchannels at appropriate locations throughout a conversation. In this work, we investigate the role of prosody in turn-taking using the recently proposed Voice Activity Projection model, which incrementally models the upcoming speech activity of the interlocutors in a self-supervised manner, without relying on explicit annotation of turn-taking events, or the explicit modeling of prosodic features. Through manipulation of the speech signal, we investigate how these models implicitly utilize prosodic information. We show that these systems learn to utilize various prosodic aspects of speech both on aggregate quantitative metrics of long-form conversations and on single utterances specifically designed to depend on prosody.",
    "metadata": {
      "arxiv_id": "2209.05161",
      "title": "How Much Does Prosody Help Turn-taking? Investigations using Voice Activity Projection Models",
      "summary": "Turn-taking is a fundamental aspect of human communication and can be described as the ability to take turns, project upcoming turn shifts, and supply backchannels at appropriate locations throughout a conversation. In this work, we investigate the role of prosody in turn-taking using the recently proposed Voice Activity Projection model, which incrementally models the upcoming speech activity of the interlocutors in a self-supervised manner, without relying on explicit annotation of turn-taking events, or the explicit modeling of prosodic features. Through manipulation of the speech signal, we investigate how these models implicitly utilize prosodic information. We show that these systems learn to utilize various prosodic aspects of speech both on aggregate quantitative metrics of long-form conversations and on single utterances specifically designed to depend on prosody.",
      "authors": [
        "Erik Ekstedt",
        "Gabriel Skantze"
      ],
      "published": "2022-09-12T11:40:16Z",
      "updated": "2022-09-12T11:40:16Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.05161v1",
      "landing_url": "https://arxiv.org/abs/2209.05161v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.05161"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article analyzes prosody and turn-taking using a Voice Activity Projection model, which engages speech modeling and interactive dialogue behavior, but the abstract/title do not explicitly mention terms like ‘Spoken Language Models’ or ‘speech language model(s)’ or an equivalent, so it does not clearly satisfy the explicit keyword criterion, though it is still relevant to speech interaction modeling.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The article analyzes prosody and turn-taking using a Voice Activity Projection model, which engages speech modeling and interactive dialogue behavior, but the abstract/title do not explicitly mention terms like ‘Spoken Language Models’ or ‘speech language model(s)’ or an equivalent, so it does not clearly satisfy the explicit keyword criterion, though it is still relevant to speech interaction modeling.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates turn-taking in human communication using a Voice Activity Projection model, focusing on implicit prosodic features related to speech activity prediction. However, it does not explicitly discuss spoken language models or speech language models as core modeling objects, nor does it mention model architecture, training specifics, or evaluation within the spoken language model framework. The study focuses on half-duplex turn-taking rather than full-duplex synchronous interaction, and lacks evidence of addressing engineered or learned synchronization, low-latency interaction, or multi-dimensional evaluation metrics as per the inclusion criteria. Therefore, it does not fully meet the inclusion criteria and aligns with exclusion criteria regarding the lack of core spoken language modeling focus and synchronous interaction aspects.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study investigates turn-taking in human communication using a Voice Activity Projection model, focusing on implicit prosodic features related to speech activity prediction. However, it does not explicitly discuss spoken language models or speech language models as core modeling objects, nor does it mention model architecture, training specifics, or evaluation within the spoken language model framework. The study focuses on half-duplex turn-taking rather than full-duplex synchronous interaction, and lacks evidence of addressing engineered or learned synchronization, low-latency interaction, or multi-dimensional evaluation metrics as per the inclusion criteria. Therefore, it does not fully meet the inclusion criteria and aligns with exclusion criteria regarding the lack of core spoken language modeling focus and synchronous interaction aspects.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "At-the-edge Data Processing for Low Latency High Throughput Machine Learning Algorithms",
    "abstract": "High throughput and low latency data processing is essential for systems requiring live decision making, control, and machine learning-optimized data reduction. We focus on two distinct use cases for in-flight streaming data processing for a) X-ray pulse reconstruction at SLAC's LCLS-II Free-Electron Laser and b) control diagnostics at the DIII-D tokamak fusion reactor. Both cases exemplify high throughput and low latency control feedback and motivate our focus on machine learning at the edge where data processing and machine learning algorithms can be implemented in field programmable gate array based hardware immediately after the diagnostic sensors. We present our recent work on a data preprocessing chain which requires fast featurization for information encoding. We discuss several options for such algorithms with the primary focus on our discrete cosine and sine transform-based approach adapted for streaming data. These algorithms are primarily aimed at implementation in field programmable gate arrays, favoring linear algebra operations that are also aligned with the recent advances in inference accelerators for the computational edge.",
    "metadata": {
      "arxiv_id": "2209.05233",
      "title": "At-the-edge Data Processing for Low Latency High Throughput Machine Learning Algorithms",
      "summary": "High throughput and low latency data processing is essential for systems requiring live decision making, control, and machine learning-optimized data reduction. We focus on two distinct use cases for in-flight streaming data processing for a) X-ray pulse reconstruction at SLAC's LCLS-II Free-Electron Laser and b) control diagnostics at the DIII-D tokamak fusion reactor. Both cases exemplify high throughput and low latency control feedback and motivate our focus on machine learning at the edge where data processing and machine learning algorithms can be implemented in field programmable gate array based hardware immediately after the diagnostic sensors. We present our recent work on a data preprocessing chain which requires fast featurization for information encoding. We discuss several options for such algorithms with the primary focus on our discrete cosine and sine transform-based approach adapted for streaming data. These algorithms are primarily aimed at implementation in field programmable gate arrays, favoring linear algebra operations that are also aligned with the recent advances in inference accelerators for the computational edge.",
      "authors": [
        "Jack Hirschman",
        "Andrei Kamalov",
        "Razib Obaid",
        "Finn H. O'Shea",
        "Ryan N Coffee"
      ],
      "published": "2022-09-06T19:03:50Z",
      "updated": "2022-09-06T19:03:50Z",
      "categories": [
        "physics.ins-det",
        "physics.data-an"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.05233v1",
      "landing_url": "https://arxiv.org/abs/2209.05233v1",
      "doi": "https://doi.org/10.1007/978-3-031-23606-8_7"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on edge data processing for ML/FPGA-based streaming, not on Spoken Language Models or speech-language modeling, so it fails the core inclusion criteria requiring SLM/FD-SLM focus.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on edge data processing for ML/FPGA-based streaming, not on Spoken Language Models or speech-language modeling, so it fails the core inclusion criteria requiring SLM/FD-SLM focus.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on data processing and machine learning algorithms implemented on edge hardware for high-throughput, low-latency scenarios unrelated to spoken language models or speech language modeling; there is no mention of speech, spoken language modeling, full-duplex interaction, or related concepts required by the inclusion criteria, and the content pertains to hardware and signal processing for non-speech data streams.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on data processing and machine learning algorithms implemented on edge hardware for high-throughput, low-latency scenarios unrelated to spoken language models or speech language modeling; there is no mention of speech, spoken language modeling, full-duplex interaction, or related concepts required by the inclusion criteria, and the content pertains to hardware and signal processing for non-speech data streams.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The BLue Amazon Brain (BLAB): A Modular Architecture of Services about the Brazilian Maritime Territory",
    "abstract": "We describe the first steps in the development of an artificial agent focused on the Brazilian maritime territory, a large region within the South Atlantic also known as the Blue Amazon. The \"BLue Amazon Brain\" (BLAB) integrates a number of services aimed at disseminating information about this region and its importance, functioning as a tool for environmental awareness. The main service provided by BLAB is a conversational facility that deals with complex questions about the Blue Amazon, called BLAB-Chat; its central component is a controller that manages several task-oriented natural language processing modules (e.g., question answering and summarizer systems). These modules have access to an internal data lake as well as to third-party databases. A news reporter (BLAB-Reporter) and a purposely-developed wiki (BLAB-Wiki) are also part of the BLAB service architecture. In this paper, we describe our current version of BLAB's architecture (interface, backend, web services, NLP modules, and resources) and comment on the challenges we have faced so far, such as the lack of training data and the scattered state of domain information. Solving these issues presents a considerable challenge in the development of artificial intelligence for technical domains.",
    "metadata": {
      "arxiv_id": "2209.07928",
      "title": "The BLue Amazon Brain (BLAB): A Modular Architecture of Services about the Brazilian Maritime Territory",
      "summary": "We describe the first steps in the development of an artificial agent focused on the Brazilian maritime territory, a large region within the South Atlantic also known as the Blue Amazon. The \"BLue Amazon Brain\" (BLAB) integrates a number of services aimed at disseminating information about this region and its importance, functioning as a tool for environmental awareness. The main service provided by BLAB is a conversational facility that deals with complex questions about the Blue Amazon, called BLAB-Chat; its central component is a controller that manages several task-oriented natural language processing modules (e.g., question answering and summarizer systems). These modules have access to an internal data lake as well as to third-party databases. A news reporter (BLAB-Reporter) and a purposely-developed wiki (BLAB-Wiki) are also part of the BLAB service architecture. In this paper, we describe our current version of BLAB's architecture (interface, backend, web services, NLP modules, and resources) and comment on the challenges we have faced so far, such as the lack of training data and the scattered state of domain information. Solving these issues presents a considerable challenge in the development of artificial intelligence for technical domains.",
      "authors": [
        "Paulo Pirozelli",
        "Ais B. R. Castro",
        "Ana Luiza C. de Oliveira",
        "André S. Oliveira",
        "Flávio N. Cação",
        "Igor C. Silveira",
        "João G. M. Campos",
        "Laura C. Motheo",
        "Leticia F. Figueiredo",
        "Lucas F. A. O. Pellicer",
        "Marcelo A. José",
        "Marcos M. José",
        "Pedro de M. Ligabue",
        "Ricardo S. Grava",
        "Rodrigo M. Tavares",
        "Vinícius B. Matos",
        "Yan V. Sym",
        "Anna H. R. Costa",
        "Anarosa A. F. Brandão",
        "Denis D. Mauá",
        "Fabio G. Cozman",
        "Sarajane M. Peres"
      ],
      "published": "2022-09-06T18:32:08Z",
      "updated": "2022-09-06T18:32:08Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.07928v1",
      "landing_url": "https://arxiv.org/abs/2209.07928v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.07928"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on a text-based modular NLP conversational system for environmental information; it does not address Spoken Language Models, speech-language modeling, or real-time speech interaction, hence does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on a text-based modular NLP conversational system for environmental information; it does not address Spoken Language Models, speech-language modeling, or real-time speech interaction, hence does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on an AI agent providing conversational services and natural language processing modules related to the Brazilian maritime territory, but there is no mention of spoken language models, speech as a core modeling object, full-duplex or synchronous speech interaction, or related evaluation metrics as required by the inclusion criteria. It mainly presents NLP services, with no evidence of speech-level language modeling or synchronization features, and thus does not meet the key inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on an AI agent providing conversational services and natural language processing modules related to the Brazilian maritime territory, but there is no mention of spoken language models, speech as a core modeling object, full-duplex or synchronous speech interaction, or related evaluation metrics as required by the inclusion criteria. It mainly presents NLP services, with no evidence of speech-level language modeling or synchronization features, and thus does not meet the key inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Resource Allocation of STAR-RIS Assisted Full-Duplex Systems",
    "abstract": "Well-designed simultaneously transmitting and reflecting RIS (STAR-RIS), which extends the half-space coverage to full-space coverage, incurs wireless communication environments to be smart and reconfigurable. In this paper, we survey how STAR-RIS affects the performance of full-duplex communication systems with the presence of full-duplex users, wherein the base station (BS) and the uplink users are subject to maximum transmission power constraints. Firstly, the weighted sum-rate (WSR) is derived as a system performance metric. Then, we formulate the resource allocation design into an equivalent weighted minimum mean-square-error form and then transform it into several convex sub-problems to maximize the WSR as an optimization problem which jointly optimizes the beamforming and the combining vectors at the BS, the transmit powers of the uplink users, and phase shifts of STAR-RIS. Although the WSR optimization is non-convex, an efficient iterative alternating procedure is proposed to achieve a sub-optimal solution for the optimization problem. Secondly, the STAR-RIS's phase shifts are optimized via the successive convex approximation technique. Finally, numerical results are provided to explain how STAR-RIS improves the performance metric with the presence of full-duplex users.",
    "metadata": {
      "arxiv_id": "2209.08591",
      "title": "Resource Allocation of STAR-RIS Assisted Full-Duplex Systems",
      "summary": "Well-designed simultaneously transmitting and reflecting RIS (STAR-RIS), which extends the half-space coverage to full-space coverage, incurs wireless communication environments to be smart and reconfigurable. In this paper, we survey how STAR-RIS affects the performance of full-duplex communication systems with the presence of full-duplex users, wherein the base station (BS) and the uplink users are subject to maximum transmission power constraints. Firstly, the weighted sum-rate (WSR) is derived as a system performance metric. Then, we formulate the resource allocation design into an equivalent weighted minimum mean-square-error form and then transform it into several convex sub-problems to maximize the WSR as an optimization problem which jointly optimizes the beamforming and the combining vectors at the BS, the transmit powers of the uplink users, and phase shifts of STAR-RIS. Although the WSR optimization is non-convex, an efficient iterative alternating procedure is proposed to achieve a sub-optimal solution for the optimization problem. Secondly, the STAR-RIS's phase shifts are optimized via the successive convex approximation technique. Finally, numerical results are provided to explain how STAR-RIS improves the performance metric with the presence of full-duplex users.",
      "authors": [
        "Mohammad Reza Kavianinia",
        "Mohammad Javad Emadi"
      ],
      "published": "2022-09-18T16:05:39Z",
      "updated": "2022-10-16T17:11:08Z",
      "categories": [
        "cs.IT",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.08591v2",
      "landing_url": "https://arxiv.org/abs/2209.08591v2",
      "doi": "https://doi.org/10.48550/arXiv.2209.08591"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on STAR-RIS aided full-duplex wireless systems and optimization, not on Spoken Language Models (SLMs) or any form of speech–language modeling, interaction, or evaluation; thus it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on STAR-RIS aided full-duplex wireless systems and optimization, not on Spoken Language Models (SLMs) or any form of speech–language modeling, interaction, or evaluation; thus it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on the resource allocation for STAR-RIS assisted full-duplex wireless communication systems, discussing beamforming, transmission power, and optimization techniques. It does not involve spoken language models, speech language modeling, or modeling of acoustic signals, nor does it mention any aspects of synchronous speech interaction, speech-centric model architectures, or evaluation involving speech response or turn-taking behavior. The topic and technical scope are fully outside the domain of spoken language models, and it contains no relevant terminology or topic related to speech language modeling or full-duplex spoken interaction as required in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on the resource allocation for STAR-RIS assisted full-duplex wireless communication systems, discussing beamforming, transmission power, and optimization techniques. It does not involve spoken language models, speech language modeling, or modeling of acoustic signals, nor does it mention any aspects of synchronous speech interaction, speech-centric model architectures, or evaluation involving speech response or turn-taking behavior. The topic and technical scope are fully outside the domain of spoken language models, and it contains no relevant terminology or topic related to speech language modeling or full-duplex spoken interaction as required in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LL-GNN: Low Latency Graph Neural Networks on FPGAs for High Energy Physics",
    "abstract": "This work presents a novel reconfigurable architecture for Low Latency Graph Neural Network (LL-GNN) designs for particle detectors, delivering unprecedented low latency performance. Incorporating FPGA-based GNNs into particle detectors presents a unique challenge since it requires sub-microsecond latency to deploy the networks for online event selection with a data rate of hundreds of terabytes per second in the Level-1 triggers at the CERN Large Hadron Collider experiments. This paper proposes a novel outer-product based matrix multiplication approach, which is enhanced by exploiting the structured adjacency matrix and a column-major data layout. Moreover, a fusion step is introduced to further reduce the end-to-end design latency by eliminating unnecessary boundaries. Furthermore, a GNN-specific algorithm-hardware co-design approach is presented which not only finds a design with a much better latency but also finds a high accuracy design under given latency constraints. To facilitate this, a customizable template for this low latency GNN hardware architecture has been designed and open-sourced, which enables the generation of low-latency FPGA designs with efficient resource utilization using a high-level synthesis tool. Evaluation results show that our FPGA implementation is up to 9.0 times faster and achieves up to 13.1 times higher power efficiency than a GPU implementation. Compared to the previous FPGA implementations, this work achieves 6.51 to 16.7 times lower latency. Moreover, the latency of our FPGA design is sufficiently low to enable deployment of GNNs in a sub-microsecond, real-time collider trigger system, enabling it to benefit from improved accuracy. The proposed LL-GNN design advances the next generation of trigger systems by enabling sophisticated algorithms to process experimental data efficiently.",
    "metadata": {
      "arxiv_id": "2209.14065",
      "title": "LL-GNN: Low Latency Graph Neural Networks on FPGAs for High Energy Physics",
      "summary": "This work presents a novel reconfigurable architecture for Low Latency Graph Neural Network (LL-GNN) designs for particle detectors, delivering unprecedented low latency performance. Incorporating FPGA-based GNNs into particle detectors presents a unique challenge since it requires sub-microsecond latency to deploy the networks for online event selection with a data rate of hundreds of terabytes per second in the Level-1 triggers at the CERN Large Hadron Collider experiments. This paper proposes a novel outer-product based matrix multiplication approach, which is enhanced by exploiting the structured adjacency matrix and a column-major data layout. Moreover, a fusion step is introduced to further reduce the end-to-end design latency by eliminating unnecessary boundaries. Furthermore, a GNN-specific algorithm-hardware co-design approach is presented which not only finds a design with a much better latency but also finds a high accuracy design under given latency constraints. To facilitate this, a customizable template for this low latency GNN hardware architecture has been designed and open-sourced, which enables the generation of low-latency FPGA designs with efficient resource utilization using a high-level synthesis tool. Evaluation results show that our FPGA implementation is up to 9.0 times faster and achieves up to 13.1 times higher power efficiency than a GPU implementation. Compared to the previous FPGA implementations, this work achieves 6.51 to 16.7 times lower latency. Moreover, the latency of our FPGA design is sufficiently low to enable deployment of GNNs in a sub-microsecond, real-time collider trigger system, enabling it to benefit from improved accuracy. The proposed LL-GNN design advances the next generation of trigger systems by enabling sophisticated algorithms to process experimental data efficiently.",
      "authors": [
        "Zhiqiang Que",
        "Hongxiang Fan",
        "Marcus Loo",
        "He Li",
        "Michaela Blott",
        "Maurizio Pierini",
        "Alexander Tapper",
        "Wayne Luk"
      ],
      "published": "2022-09-28T12:55:35Z",
      "updated": "2024-01-09T10:05:38Z",
      "categories": [
        "cs.AR",
        "cs.LG",
        "physics.ins-det"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.14065v5",
      "landing_url": "https://arxiv.org/abs/2209.14065v5",
      "doi": "https://doi.org/10.1145/3640464"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about FPGA-based low-latency graph neural networks for high-energy physics, with no focus on spoken language models or speech-language modeling aspects.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about FPGA-based low-latency graph neural networks for high-energy physics, with no focus on spoken language models or speech-language modeling aspects.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on FPGA-based low-latency Graph Neural Networks for high energy physics event detection, which is unrelated to Spoken Language Models or speech-language modeling as required by the inclusion criteria. It neither addresses speech as the core modeling object nor discusses models for spoken language interaction or full-duplex spoken language processing.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on FPGA-based low-latency Graph Neural Networks for high energy physics event detection, which is unrelated to Spoken Language Models or speech-language modeling as required by the inclusion criteria. It neither addresses speech as the core modeling object nor discusses models for spoken language interaction or full-duplex spoken language processing.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Quantum LDPC Codes for Modular Architectures",
    "abstract": "In efforts to scale the size of quantum computers, modularity plays a central role across most quantum computing technologies. In the light of fault tolerance, this necessitates designing quantum error-correcting codes that are compatible with the connectivity arising from the architectural layouts. In this paper, we aim to bridge this gap by giving a novel way to view and construct quantum LDPC codes tailored for modular architectures. We demonstrate that if the intra- and inter-modular qubit connectivity can be viewed as corresponding to some classical or quantum LDPC codes, then their hypergraph product code fully respects the architectural connectivity constraints. Finally, we show that relaxed connectivity constraints that allow twists of connections between modules pave a way to construct codes with better parameters.",
    "metadata": {
      "arxiv_id": "2209.14329",
      "title": "Quantum LDPC Codes for Modular Architectures",
      "summary": "In efforts to scale the size of quantum computers, modularity plays a central role across most quantum computing technologies. In the light of fault tolerance, this necessitates designing quantum error-correcting codes that are compatible with the connectivity arising from the architectural layouts. In this paper, we aim to bridge this gap by giving a novel way to view and construct quantum LDPC codes tailored for modular architectures. We demonstrate that if the intra- and inter-modular qubit connectivity can be viewed as corresponding to some classical or quantum LDPC codes, then their hypergraph product code fully respects the architectural connectivity constraints. Finally, we show that relaxed connectivity constraints that allow twists of connections between modules pave a way to construct codes with better parameters.",
      "authors": [
        "Armands Strikis",
        "Lucas Berent"
      ],
      "published": "2022-09-28T18:01:43Z",
      "updated": "2023-05-15T10:36:17Z",
      "categories": [
        "quant-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.14329v3",
      "landing_url": "https://arxiv.org/abs/2209.14329v3",
      "doi": "https://doi.org/10.1103/PRXQuantum.4.020321"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目聚焦量子 LDPC 代碼與模組化架構，與本主題規範的 Spoken Language Models/語音語言模型無關，且未涵蓋語音互動與同步推理，因此不符合包含條件，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目聚焦量子 LDPC 代碼與模組化架構，與本主題規範的 Spoken Language Models/語音語言模型無關，且未涵蓋語音互動與同步推理，因此不符合包含條件，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on quantum LDPC codes for modular quantum computer architectures and does not discuss spoken language models, speech language modeling, or any related architectures and evaluations; thus, it does not meet the inclusion criteria of focusing on spoken language models and relevant synchronous interaction capabilities.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on quantum LDPC codes for modular quantum computer architectures and does not discuss spoken language models, speech language modeling, or any related architectures and evaluations; thus, it does not meet the inclusion criteria of focusing on spoken language models and relevant synchronous interaction capabilities.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Augmentation Invariant Discrete Representation for Generative Spoken Language Modeling",
    "abstract": "Generative Spoken Language Modeling research focuses on optimizing speech Language Models (LMs) using raw audio recordings without accessing any textual supervision. Such speech LMs usually operate over discrete units obtained from quantizing internal representations of self-supervised models. Although such units show impressive modeling results, their robustness capabilities have not been extensively investigated. This work focuses on improving the robustness of discrete input representations for generative spoken language modeling. First, we formally define how to measure the robustness of such representations to various signal variations that do not alter the spoken information (e.g., time-stretch). Next, we empirically demonstrate how current state-of-the-art representation models lack robustness to such variations. To overcome this, we propose an effective and efficient method to learn robust discrete speech representation for generative spoken language modeling. The proposed approach is based on applying a set of signal transformations to the speech signal and optimizing the model using an iterative pseudo-labeling scheme. Our method significantly improves over the evaluated baselines when considering encoding and modeling metrics. We additionally evaluate our method on the speech-to-speech translation task, considering Spanish-English and French-English translations, and show the proposed approach outperforms the evaluated baselines.",
    "metadata": {
      "arxiv_id": "2209.15483",
      "title": "Augmentation Invariant Discrete Representation for Generative Spoken Language Modeling",
      "summary": "Generative Spoken Language Modeling research focuses on optimizing speech Language Models (LMs) using raw audio recordings without accessing any textual supervision. Such speech LMs usually operate over discrete units obtained from quantizing internal representations of self-supervised models. Although such units show impressive modeling results, their robustness capabilities have not been extensively investigated. This work focuses on improving the robustness of discrete input representations for generative spoken language modeling. First, we formally define how to measure the robustness of such representations to various signal variations that do not alter the spoken information (e.g., time-stretch). Next, we empirically demonstrate how current state-of-the-art representation models lack robustness to such variations. To overcome this, we propose an effective and efficient method to learn robust discrete speech representation for generative spoken language modeling. The proposed approach is based on applying a set of signal transformations to the speech signal and optimizing the model using an iterative pseudo-labeling scheme. Our method significantly improves over the evaluated baselines when considering encoding and modeling metrics. We additionally evaluate our method on the speech-to-speech translation task, considering Spanish-English and French-English translations, and show the proposed approach outperforms the evaluated baselines.",
      "authors": [
        "Itai Gat",
        "Felix Kreuk",
        "Tu Anh Nguyen",
        "Ann Lee",
        "Jade Copet",
        "Gabriel Synnaeve",
        "Emmanuel Dupoux",
        "Yossi Adi"
      ],
      "published": "2022-09-30T14:15:03Z",
      "updated": "2023-05-29T10:50:29Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.15483v2",
      "landing_url": "https://arxiv.org/abs/2209.15483v2",
      "doi": "https://doi.org/10.48550/arXiv.2209.15483"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項明確以 Generative Spoken Language Modeling 為核心，討論離散語音表示的鲁棒性，並在語音到語音翻譯任務上評估，符合近3年內的研究與關鍵字包含 spoken language model(s)/speech language model(s)，且未聚焦於單純 ASR/TTS 或僅限單任務，符合所有納入條件且無排除情形。",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "此項明確以 Generative Spoken Language Modeling 為核心，討論離散語音表示的鲁棒性，並在語音到語音翻譯任務上評估，符合近3年內的研究與關鍵字包含 spoken language model(s)/speech language model(s)，且未聚焦於單純 ASR/TTS 或僅限單任務，符合所有納入條件且無排除情形。",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "該研究明確聚焦於 Generative Spoken Language Modeling，研究核心在於基於聲學信號的離散表示學習與強健性提升，符合以語音作為核心建模與推理對象的要求，且探討模型的訓練和評估面向；然而，摘要中並未提及與 full-duplex 同步互動、語音重疊管理或即時行為仲裁相關的內容，且研究主要是強化離散語音表示以提升模型表現，未明確涵蓋低延遲同步互動等面向，因此不完全符合納入條件，故評為中立偏向排除。",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "該研究明確聚焦於 Generative Spoken Language Modeling，研究核心在於基於聲學信號的離散表示學習與強健性提升，符合以語音作為核心建模與推理對象的要求，且探討模型的訓練和評估面向；然而，摘要中並未提及與 full-duplex 同步互動、語音重疊管理或即時行為仲裁相關的內容，且研究主要是強化離散語音表示以提升模型表現，未明確涵蓋低延遲同步互動等面向，因此不完全符合納入條件，故評為中立偏向排除。",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "整合兩位初審意見：JuniorNano 評為 5，JuniorMini 評為 3（擔心未涵蓋 full‑duplex 即時互動面向）。本文明確以 Generative Spoken Language Modeling 為核心，提出並評估針對離散語音表示的魯棒性改進，且在 speech‑to‑speech 翻譯上驗證，屬於語音層級的 language modeling 而非僅 ASR/TTS 或文字前處理，符合所有納入條件且不落入任一排除條件。簡短說明：直接聚焦於語音語言模型與離散語音 token 的方法與評估，故應納入。",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "整合兩位初審意見：JuniorNano 評為 5，JuniorMini 評為 3（擔心未涵蓋 full‑duplex 即時互動面向）。本文明確以 Generative Spoken Language Modeling 為核心，提出並評估針對離散語音表示的魯棒性改進，且在 speech‑to‑speech 翻譯上驗證，屬於語音層級的 language modeling 而非僅 ASR/TTS 或文字前處理，符合所有納入條件且不落入任一排除條件。簡短說明：直接聚焦於語音語言模型與離散語音 token 的方法與評估，故應納入。",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Knowing Where to Look: A Planning-based Architecture to Automate the Gaze Behavior of Social Robots",
    "abstract": "Gaze cues play an important role in human communication and are used to coordinate turn-taking and joint attention, as well as to regulate intimacy. In order to have fluent conversations with people, social robots need to exhibit human-like gaze behavior. Previous Gaze Control Systems (GCS) in HRI have automated robot gaze using data-driven or heuristic approaches. However, these systems tend to be mainly reactive in nature. Planning the robot gaze ahead of time could help in achieving more realistic gaze behavior and better eye-head coordination. In this paper, we propose and implement a novel planning-based GCS. We evaluate our system in a comparative within-subjects user study (N=26) between a reactive system and our proposed system. The results show that the users preferred the proposed system and that it was significantly more interpretable and better at regulating intimacy.",
    "metadata": {
      "arxiv_id": "2210.02866",
      "title": "Knowing Where to Look: A Planning-based Architecture to Automate the Gaze Behavior of Social Robots",
      "summary": "Gaze cues play an important role in human communication and are used to coordinate turn-taking and joint attention, as well as to regulate intimacy. In order to have fluent conversations with people, social robots need to exhibit human-like gaze behavior. Previous Gaze Control Systems (GCS) in HRI have automated robot gaze using data-driven or heuristic approaches. However, these systems tend to be mainly reactive in nature. Planning the robot gaze ahead of time could help in achieving more realistic gaze behavior and better eye-head coordination. In this paper, we propose and implement a novel planning-based GCS. We evaluate our system in a comparative within-subjects user study (N=26) between a reactive system and our proposed system. The results show that the users preferred the proposed system and that it was significantly more interpretable and better at regulating intimacy.",
      "authors": [
        "Chinmaya Mishra",
        "Gabriel Skantze"
      ],
      "published": "2022-10-06T12:37:10Z",
      "updated": "2022-10-06T12:37:10Z",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.02866v1",
      "landing_url": "https://arxiv.org/abs/2210.02866v1",
      "doi": "https://doi.org/10.1109/RO-MAN53752.2022.9900740"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on planning-based gaze control for social robots rather than on Spoken Language Models or speech-language modeling, so it does not meet the core inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on planning-based gaze control for social robots rather than on Spoken Language Models or speech-language modeling, so it does not meet the core inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on planning-based gaze behavior in social robots, which relates to social interaction and nonverbal cues rather than the core topic of Spoken Language Models or speech-level language modeling; it does not discuss speech sequence modeling, full-duplex spoken language models, or relevant architectures and evaluations related to speech language modeling as specified in the inclusion criteria, hence it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on planning-based gaze behavior in social robots, which relates to social interaction and nonverbal cues rather than the core topic of Spoken Language Models or speech-level language modeling; it does not discuss speech sequence modeling, full-duplex spoken language models, or relevant architectures and evaluations related to speech language modeling as specified in the inclusion criteria, hence it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Biological neurons act as generalization filters in reservoir computing",
    "abstract": "Reservoir computing is a machine learning paradigm that transforms the transient dynamics of high-dimensional nonlinear systems for processing time-series data. Although reservoir computing was initially proposed to model information processing in the mammalian cortex, it remains unclear how the non-random network architecture, such as the modular architecture, in the cortex integrates with the biophysics of living neurons to characterize the function of biological neuronal networks (BNNs). Here, we used optogenetics and fluorescent calcium imaging to record the multicellular responses of cultured BNNs and employed the reservoir computing framework to decode their computational capabilities. Micropatterned substrates were used to embed the modular architecture in the BNNs. We first show that modular BNNs can be used to classify static input patterns with a linear decoder and that the modularity of the BNNs positively correlates with the classification accuracy. We then used a timer task to verify that BNNs possess a short-term memory of ~1 s and finally show that this property can be exploited for spoken digit classification. Interestingly, BNN-based reservoirs allow transfer learning, wherein a network trained on one dataset can be used to classify separate datasets of the same category. Such classification was not possible when the input patterns were directly decoded by a linear decoder, suggesting that BNNs act as a generalization filter to improve reservoir computing performance. Our findings pave the way toward a mechanistic understanding of information processing within BNNs and, simultaneously, build future expectations toward the realization of physical reservoir computing systems based on BNNs.",
    "metadata": {
      "arxiv_id": "2210.02913",
      "title": "Biological neurons act as generalization filters in reservoir computing",
      "summary": "Reservoir computing is a machine learning paradigm that transforms the transient dynamics of high-dimensional nonlinear systems for processing time-series data. Although reservoir computing was initially proposed to model information processing in the mammalian cortex, it remains unclear how the non-random network architecture, such as the modular architecture, in the cortex integrates with the biophysics of living neurons to characterize the function of biological neuronal networks (BNNs). Here, we used optogenetics and fluorescent calcium imaging to record the multicellular responses of cultured BNNs and employed the reservoir computing framework to decode their computational capabilities. Micropatterned substrates were used to embed the modular architecture in the BNNs. We first show that modular BNNs can be used to classify static input patterns with a linear decoder and that the modularity of the BNNs positively correlates with the classification accuracy. We then used a timer task to verify that BNNs possess a short-term memory of ~1 s and finally show that this property can be exploited for spoken digit classification. Interestingly, BNN-based reservoirs allow transfer learning, wherein a network trained on one dataset can be used to classify separate datasets of the same category. Such classification was not possible when the input patterns were directly decoded by a linear decoder, suggesting that BNNs act as a generalization filter to improve reservoir computing performance. Our findings pave the way toward a mechanistic understanding of information processing within BNNs and, simultaneously, build future expectations toward the realization of physical reservoir computing systems based on BNNs.",
      "authors": [
        "Takuma Sumi",
        "Hideaki Yamamoto",
        "Yuichi Katori",
        "Satoshi Moriya",
        "Tomohiro Konno",
        "Shigeo Sato",
        "Ayumi Hirano-Iwata"
      ],
      "published": "2022-10-06T13:32:26Z",
      "updated": "2022-10-06T13:32:26Z",
      "categories": [
        "q-bio.NC",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.02913v1",
      "landing_url": "https://arxiv.org/abs/2210.02913v1",
      "doi": "https://doi.org/10.1073/pnas.2217008120"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns biological neurons and reservoir computing in cultured neural networks, not Spoken Language Models or any speech-language modeling aspects, and thus does not meet the inclusion criteria focused on SLM/FD-SLM with speech as the core modeling target.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns biological neurons and reservoir computing in cultured neural networks, not Spoken Language Models or any speech-language modeling aspects, and thus does not meet the inclusion criteria focused on SLM/FD-SLM with speech as the core modeling target.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on reservoir computing using biological neuronal networks and their computational capabilities for classifying patterns and spoken digit classification. However, it does not focus on spoken language models or full-duplex spoken language interactions, nor does it explicitly discuss modeling or reasoning over speech as a core objective. It lacks discussion on synchronization, turn-taking, or low-latency real-time interactive capabilities required by the inclusion criteria. Therefore, it does not meet the core requirements of the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on reservoir computing using biological neuronal networks and their computational capabilities for classifying patterns and spoken digit classification. However, it does not focus on spoken language models or full-duplex spoken language interactions, nor does it explicitly discuss modeling or reasoning over speech as a core objective. It lacks discussion on synchronization, turn-taking, or low-latency real-time interactive capabilities required by the inclusion criteria. Therefore, it does not meet the core requirements of the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Low-Latency Analog-to-Analog Signal Processing using PC Hardware and USRPs",
    "abstract": "In this paper, we implement a low-latency rapid-prototyping platform for signal processing based on software-defined radios (SDRs) and off-the-shelf PC hardware. This platform allows to evaluate a wide variety of algorithms in real-time environments, supporting new developments in the fields of classical, AI-based, and hybrid signal processing. To accomplish this, the streaming protocol of the used USRP X310 devices is implemented using the Data Plane Development Kit (DPDK), which allows to handle network communication in userspace only. This bypasses the kernel and thus avoids the latencies caused by interrupt handling, scheduling, and context switches. It allows signal processing to be performed on isolated processor cores that are protected from interrupts to a great extent. To validate our approach, linear time-invariant channel emulation has been implemented. For this, an analog-to-analog latency of 31 microseconds was achieved, demonstrating that our PC-based approach enables the implementation of rapid-prototyping systems with low latency.",
    "metadata": {
      "arxiv_id": "2210.06067",
      "title": "Low-Latency Analog-to-Analog Signal Processing using PC Hardware and USRPs",
      "summary": "In this paper, we implement a low-latency rapid-prototyping platform for signal processing based on software-defined radios (SDRs) and off-the-shelf PC hardware. This platform allows to evaluate a wide variety of algorithms in real-time environments, supporting new developments in the fields of classical, AI-based, and hybrid signal processing. To accomplish this, the streaming protocol of the used USRP X310 devices is implemented using the Data Plane Development Kit (DPDK), which allows to handle network communication in userspace only. This bypasses the kernel and thus avoids the latencies caused by interrupt handling, scheduling, and context switches. It allows signal processing to be performed on isolated processor cores that are protected from interrupts to a great extent. To validate our approach, linear time-invariant channel emulation has been implemented. For this, an analog-to-analog latency of 31 microseconds was achieved, demonstrating that our PC-based approach enables the implementation of rapid-prototyping systems with low latency.",
      "authors": [
        "Maximilian Engelhardt",
        "Carsten Andrich",
        "Alexander Ihlow",
        "Sebastian Giehl",
        "Giovanni Del Galdo"
      ],
      "published": "2022-10-12T10:17:08Z",
      "updated": "2022-10-12T10:17:08Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.06067v1",
      "landing_url": "https://arxiv.org/abs/2210.06067v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.06067"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on a low-latency signal-processing platform using SDRs and PC hardware, with no emphasis on Spoken Language Models, speech modeling, or interactive dialogue capabilities, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on a low-latency signal-processing platform using SDRs and PC hardware, with no emphasis on Spoken Language Models, speech modeling, or interactive dialogue capabilities, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on low-latency signal processing using hardware and SDRs, but it does not discuss spoken language models, speech language modeling, or any related linguistic modeling or speech interaction aspects required by the inclusion criteria. It is more about signal processing hardware implementation rather than speech modeling or real-time interactive spoken language systems.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on low-latency signal processing using hardware and SDRs, but it does not discuss spoken language models, speech language modeling, or any related linguistic modeling or speech interaction aspects required by the inclusion criteria. It is more about signal processing hardware implementation rather than speech modeling or real-time interactive spoken language systems.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "CS-Insights: A System for Analyzing Computer Science Research",
    "abstract": "This paper presents CS-Insights, an interactive web application to analyze computer science publications from DBLP through multiple perspectives. The dedicated interfaces allow its users to identify trends in research activity, productivity, accessibility, author's productivity, venues' statistics, topics of interest, and the impact of computer science research on other fields. CS-Insightsis publicly available, and its modular architecture can be easily adapted to domains other than computer science.",
    "metadata": {
      "arxiv_id": "2210.06878",
      "title": "CS-Insights: A System for Analyzing Computer Science Research",
      "summary": "This paper presents CS-Insights, an interactive web application to analyze computer science publications from DBLP through multiple perspectives. The dedicated interfaces allow its users to identify trends in research activity, productivity, accessibility, author's productivity, venues' statistics, topics of interest, and the impact of computer science research on other fields. CS-Insightsis publicly available, and its modular architecture can be easily adapted to domains other than computer science.",
      "authors": [
        "Terry Ruas",
        "Jan Philip Wahle",
        "Lennart Küll",
        "Saif M. Mohammad",
        "Bela Gipp"
      ],
      "published": "2022-10-13T10:03:52Z",
      "updated": "2023-01-29T08:18:16Z",
      "categories": [
        "cs.CL",
        "cs.DL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.06878v2",
      "landing_url": "https://arxiv.org/abs/2210.06878v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.06878"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article describes a general tool for analyzing computer science publications with no focus on Spoken Language Models or speech-language inference, nor on real-time interaction or dialogue arbitration, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article describes a general tool for analyzing computer science publications with no focus on Spoken Language Models or speech-language inference, nor on real-time interaction or dialogue arbitration, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a system for analyzing computer science research publications in general and does not mention Spoken Language Models, speech language modeling, or any aspect related to full-duplex spoken language interaction, model architectures, training, or evaluation relevant to speech as a primary modeling object.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a system for analyzing computer science research publications in general and does not mention Spoken Language Models, speech language modeling, or any aspect related to full-duplex spoken language interaction, model architectures, training, or evaluation relevant to speech as a primary modeling object.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Evaluating Data-Driven Co-Speech Gestures of Embodied Conversational Agents through Real-Time Interaction",
    "abstract": "Embodied Conversational Agents that make use of co-speech gestures can enhance human-machine interactions in many ways. In recent years, data-driven gesture generation approaches for ECAs have attracted considerable research attention, and related methods have continuously improved. Real-time interaction is typically used when researchers evaluate ECA systems that generate rule-based gestures. However, when evaluating the performance of ECAs based on data-driven methods, participants are often required only to watch pre-recorded videos, which cannot provide adequate information about what a person perceives during the interaction. To address this limitation, we explored use of real-time interaction to assess data-driven gesturing ECAs. We provided a testbed framework, and investigated whether gestures could affect human perception of ECAs in the dimensions of human-likeness, animacy, perceived intelligence, and focused attention. Our user study required participants to interact with two ECAs - one with and one without hand gestures. We collected subjective data from the participants' self-report questionnaires and objective data from a gaze tracker. To our knowledge, the current study represents the first attempt to evaluate data-driven gesturing ECAs through real-time interaction and the first experiment using gaze-tracking to examine the effect of ECAs' gestures.",
    "metadata": {
      "arxiv_id": "2210.06974",
      "title": "Evaluating Data-Driven Co-Speech Gestures of Embodied Conversational Agents through Real-Time Interaction",
      "summary": "Embodied Conversational Agents that make use of co-speech gestures can enhance human-machine interactions in many ways. In recent years, data-driven gesture generation approaches for ECAs have attracted considerable research attention, and related methods have continuously improved. Real-time interaction is typically used when researchers evaluate ECA systems that generate rule-based gestures. However, when evaluating the performance of ECAs based on data-driven methods, participants are often required only to watch pre-recorded videos, which cannot provide adequate information about what a person perceives during the interaction. To address this limitation, we explored use of real-time interaction to assess data-driven gesturing ECAs. We provided a testbed framework, and investigated whether gestures could affect human perception of ECAs in the dimensions of human-likeness, animacy, perceived intelligence, and focused attention. Our user study required participants to interact with two ECAs - one with and one without hand gestures. We collected subjective data from the participants' self-report questionnaires and objective data from a gaze tracker. To our knowledge, the current study represents the first attempt to evaluate data-driven gesturing ECAs through real-time interaction and the first experiment using gaze-tracking to examine the effect of ECAs' gestures.",
      "authors": [
        "Yuan He",
        "André Pereira",
        "Taras Kucherenko"
      ],
      "published": "2022-10-13T12:50:03Z",
      "updated": "2022-10-13T12:50:03Z",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.06974v1",
      "landing_url": "https://arxiv.org/abs/2210.06974v1",
      "doi": "https://doi.org/10.1145/3514197.3549697"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article focuses on data-driven co-speech gestures in embodied conversational agents and evaluates gesture impact via real-time interaction; it does not center on Spoken Language Models or speech-language modeling concepts, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article focuses on data-driven co-speech gestures in embodied conversational agents and evaluates gesture impact via real-time interaction; it does not center on Spoken Language Models or speech-language modeling concepts, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating data-driven co-speech gestures of Embodied Conversational Agents (ECAs) through real-time interaction, concentrating on gestural interaction rather than spoken language modeling or speech-based sequence modeling, and does not address core topics like Spoken Language Models, Full-Duplex Spoken Language Models, or speech-language model architectures and training; thus, it does not meet the inclusion criteria centered on speech-based language modeling and related real-time spoken language interactions.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating data-driven co-speech gestures of Embodied Conversational Agents (ECAs) through real-time interaction, concentrating on gestural interaction rather than spoken language modeling or speech-based sequence modeling, and does not address core topics like Spoken Language Models, Full-Duplex Spoken Language Models, or speech-language model architectures and training; thus, it does not meet the inclusion criteria centered on speech-based language modeling and related real-time spoken language interactions.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Full-Duplex Transceivers for Next-Generation Wireless Communication Systems",
    "abstract": "Wireless communication systems can be enhanced at the link level, in medium access, and at the network level when transceivers are equipped with full-duplex capability: the transformative ability to simultaneously transmit and receive over the same frequency spectrum. Effective methods to cancel self-interference are required to facilitate full-duplex operation, which we overview herein in the context of traditional radios, along with those in next-generation wireless networks. We highlight advances in self-interference cancellation that leverage machine learning, and we summarize key considerations and recent progress in full-duplex millimeter-wave systems and their application in integrated access and backhaul. We present example design problems and noteworthy findings from recent experimental research to introduce and motivate the advancement of full-duplex millimeter-wave systems. We conclude this chapter by forecasting the future of full-duplex and outlining important research directions that warrant further study.",
    "metadata": {
      "arxiv_id": "2210.08094",
      "title": "Full-Duplex Transceivers for Next-Generation Wireless Communication Systems",
      "summary": "Wireless communication systems can be enhanced at the link level, in medium access, and at the network level when transceivers are equipped with full-duplex capability: the transformative ability to simultaneously transmit and receive over the same frequency spectrum. Effective methods to cancel self-interference are required to facilitate full-duplex operation, which we overview herein in the context of traditional radios, along with those in next-generation wireless networks. We highlight advances in self-interference cancellation that leverage machine learning, and we summarize key considerations and recent progress in full-duplex millimeter-wave systems and their application in integrated access and backhaul. We present example design problems and noteworthy findings from recent experimental research to introduce and motivate the advancement of full-duplex millimeter-wave systems. We conclude this chapter by forecasting the future of full-duplex and outlining important research directions that warrant further study.",
      "authors": [
        "Ian P. Roberts",
        "Himal A. Suraweera"
      ],
      "published": "2022-10-14T20:32:44Z",
      "updated": "2022-10-14T20:32:44Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.08094v1",
      "landing_url": "https://arxiv.org/abs/2210.08094v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.08094"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article is about RF full-duplex wireless transceivers and self-interference cancellation, with no focus on Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article is about RF full-duplex wireless transceivers and self-interference cancellation, with no focus on Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on full-duplex wireless communication transceivers and self-interference cancellation methods within radio and wireless network domains, without mentioning spoken language models, speech language modeling, voice sequence modeling, or any spoken language interactive system architectures, nor discussing any speech-specific end-to-end or hybrid speech-text models or their evaluation, thus it does not align with the inclusion criteria related to spoken language models or full-duplex spoken language interactive systems.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on full-duplex wireless communication transceivers and self-interference cancellation methods within radio and wireless network domains, without mentioning spoken language models, speech language modeling, voice sequence modeling, or any spoken language interactive system architectures, nor discussing any speech-specific end-to-end or hybrid speech-text models or their evaluation, thus it does not align with the inclusion criteria related to spoken language models or full-duplex spoken language interactive systems.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Configured Grant for Ultra-Reliable and Low-Latency Communications: Standardization and Beyond",
    "abstract": "Uplink configured Grant allocation has been introduced in 3rd Generation Partnership Project New Radio Release 15. This is beneficial in supporting Ultra-Reliable and Low Latency Communication for industrial communication, a key Fifth Generation mobile communication usage scenario. This scheduling mechanism enables a user with periodic traffic to transmits its data readily and bypasses the control signaling entailed to scheduling requests and scheduling grants and provides low latency access. To facilitate ultra-reliable communication, the scheduling mechanism can allow users to transmit consecutive redundant transmissions in a pre-defined period. However, if the traffic is semi-deterministic, the current standardized configured grant allocation is not equipped to emulate the traffic as the configured grant's period is pre-configured and fixed. This article describes the recent advancements in the standardization process in Release 15 and 16 for configured grant allocation and the prospective solutions to accommodate semi-deterministic traffic behavior for configured grant allocations.",
    "metadata": {
      "arxiv_id": "2210.08940",
      "title": "Configured Grant for Ultra-Reliable and Low-Latency Communications: Standardization and Beyond",
      "summary": "Uplink configured Grant allocation has been introduced in 3rd Generation Partnership Project New Radio Release 15. This is beneficial in supporting Ultra-Reliable and Low Latency Communication for industrial communication, a key Fifth Generation mobile communication usage scenario. This scheduling mechanism enables a user with periodic traffic to transmits its data readily and bypasses the control signaling entailed to scheduling requests and scheduling grants and provides low latency access. To facilitate ultra-reliable communication, the scheduling mechanism can allow users to transmit consecutive redundant transmissions in a pre-defined period. However, if the traffic is semi-deterministic, the current standardized configured grant allocation is not equipped to emulate the traffic as the configured grant's period is pre-configured and fixed. This article describes the recent advancements in the standardization process in Release 15 and 16 for configured grant allocation and the prospective solutions to accommodate semi-deterministic traffic behavior for configured grant allocations.",
      "authors": [
        "Majid Gerami",
        "Bikramjit Singh"
      ],
      "published": "2022-10-17T11:06:44Z",
      "updated": "2022-10-17T11:06:44Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.08940v1",
      "landing_url": "https://arxiv.org/abs/2210.08940v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.08940"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses 5G NR configured grants for URLLC scheduling, not Spoken Language Models or speech-language modeling; it lacks speech-centric modeling, evaluation of speech interaction, or multi-turn dialogue aspects, so it fails all inclusion criteria and satisfies no exclusion criteria as a DSP/communication topic only.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses 5G NR configured grants for URLLC scheduling, not Spoken Language Models or speech-language modeling; it lacks speech-centric modeling, evaluation of speech interaction, or multi-turn dialogue aspects, so it fails all inclusion criteria and satisfies no exclusion criteria as a DSP/communication topic only.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on configured grant allocation for ultra-reliable and low-latency communications in 5G, which is unrelated to spoken language models or speech-based modeling and inference as required by the inclusion criteria. There is no discussion of speech language models, full-duplex interaction, or any speech-specific model architectures or evaluations. Thus, it does not meet the inclusion criteria and is irrelevant to the research scope defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on configured grant allocation for ultra-reliable and low-latency communications in 5G, which is unrelated to spoken language models or speech-based modeling and inference as required by the inclusion criteria. There is no discussion of speech language models, full-duplex interaction, or any speech-specific model architectures or evaluations. Thus, it does not meet the inclusion criteria and is irrelevant to the research scope defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Bag of Tricks for Developing Diabetic Retinopathy Analysis Framework to Overcome Data Scarcity",
    "abstract": "Recently, diabetic retinopathy (DR) screening utilizing ultra-wide optical coherence tomography angiography (UW-OCTA) has been used in clinical practices to detect signs of early DR. However, developing a deep learning-based DR analysis system using UW-OCTA images is not trivial due to the difficulty of data collection and the absence of public datasets. By realistic constraints, a model trained on small datasets may obtain sub-par performance. Therefore, to help ophthalmologists be less confused about models' incorrect decisions, the models should be robust even in data scarcity settings. To address the above practical challenging, we present a comprehensive empirical study for DR analysis tasks, including lesion segmentation, image quality assessment, and DR grading. For each task, we introduce a robust training scheme by leveraging ensemble learning, data augmentation, and semi-supervised learning. Furthermore, we propose reliable pseudo labeling that excludes uncertain pseudo-labels based on the model's confidence scores to reduce the negative effect of noisy pseudo-labels. By exploiting the proposed approaches, we achieved 1st place in the Diabetic Retinopathy Analysis Challenge.",
    "metadata": {
      "arxiv_id": "2210.09558",
      "title": "Bag of Tricks for Developing Diabetic Retinopathy Analysis Framework to Overcome Data Scarcity",
      "summary": "Recently, diabetic retinopathy (DR) screening utilizing ultra-wide optical coherence tomography angiography (UW-OCTA) has been used in clinical practices to detect signs of early DR. However, developing a deep learning-based DR analysis system using UW-OCTA images is not trivial due to the difficulty of data collection and the absence of public datasets. By realistic constraints, a model trained on small datasets may obtain sub-par performance. Therefore, to help ophthalmologists be less confused about models' incorrect decisions, the models should be robust even in data scarcity settings. To address the above practical challenging, we present a comprehensive empirical study for DR analysis tasks, including lesion segmentation, image quality assessment, and DR grading. For each task, we introduce a robust training scheme by leveraging ensemble learning, data augmentation, and semi-supervised learning. Furthermore, we propose reliable pseudo labeling that excludes uncertain pseudo-labels based on the model's confidence scores to reduce the negative effect of noisy pseudo-labels. By exploiting the proposed approaches, we achieved 1st place in the Diabetic Retinopathy Analysis Challenge.",
      "authors": [
        "Gitaek Kwon",
        "Eunjin Kim",
        "Sunho Kim",
        "Seongwon Bak",
        "Minsung Kim",
        "Jaeyoung Kim"
      ],
      "published": "2022-10-18T03:25:00Z",
      "updated": "2022-10-18T03:25:00Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.09558v1",
      "landing_url": "https://arxiv.org/abs/2210.09558v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.09558"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper investigates diabetic retinopathy analysis from UW-OCTA images and data-scarcity strategies, with no focus on Spoken Language Models, speech as core modeling target, or conversational/speech-language evaluation; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper investigates diabetic retinopathy analysis from UW-OCTA images and data-scarcity strategies, with no focus on Spoken Language Models, speech as core modeling target, or conversational/speech-language evaluation; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on diabetic retinopathy analysis using deep learning with medical imaging data, which is unrelated to spoken language models or full-duplex spoken language interactions. There is no mention of speech language modeling, acoustic signals modeling, synchronous interaction, or the specialized criteria outlined for inclusion. Therefore, it does not meet the inclusion criteria focused on spoken language models and related speech interactions.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on diabetic retinopathy analysis using deep learning with medical imaging data, which is unrelated to spoken language models or full-duplex spoken language interactions. There is no mention of speech language modeling, acoustic signals modeling, synchronous interaction, or the specialized criteria outlined for inclusion. Therefore, it does not meet the inclusion criteria focused on spoken language models and related speech interactions.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Entity-to-Text based Data Augmentation for various Named Entity Recognition Tasks",
    "abstract": "Data augmentation techniques have been used to alleviate the problem of scarce labeled data in various NER tasks (flat, nested, and discontinuous NER tasks). Existing augmentation techniques either manipulate the words in the original text that break the semantic coherence of the text, or exploit generative models that ignore preserving entities in the original text, which impedes the use of augmentation techniques on nested and discontinuous NER tasks. In this work, we propose a novel Entity-to-Text based data augmentation technique named EnTDA to add, delete, replace or swap entities in the entity list of the original texts, and adopt these augmented entity lists to generate semantically coherent and entity preserving texts for various NER tasks. Furthermore, we introduce a diversity beam search to increase the diversity during the text generation process. Experiments on thirteen NER datasets across three tasks (flat, nested, and discontinuous NER tasks) and two settings (full data and low resource settings) show that EnTDA could bring more performance improvements compared to the baseline augmentation techniques.",
    "metadata": {
      "arxiv_id": "2210.10343",
      "title": "Entity-to-Text based Data Augmentation for various Named Entity Recognition Tasks",
      "summary": "Data augmentation techniques have been used to alleviate the problem of scarce labeled data in various NER tasks (flat, nested, and discontinuous NER tasks). Existing augmentation techniques either manipulate the words in the original text that break the semantic coherence of the text, or exploit generative models that ignore preserving entities in the original text, which impedes the use of augmentation techniques on nested and discontinuous NER tasks. In this work, we propose a novel Entity-to-Text based data augmentation technique named EnTDA to add, delete, replace or swap entities in the entity list of the original texts, and adopt these augmented entity lists to generate semantically coherent and entity preserving texts for various NER tasks. Furthermore, we introduce a diversity beam search to increase the diversity during the text generation process. Experiments on thirteen NER datasets across three tasks (flat, nested, and discontinuous NER tasks) and two settings (full data and low resource settings) show that EnTDA could bring more performance improvements compared to the baseline augmentation techniques.",
      "authors": [
        "Xuming Hu",
        "Yong Jiang",
        "Aiwei Liu",
        "Zhongqiang Huang",
        "Pengjun Xie",
        "Fei Huang",
        "Lijie Wen",
        "Philip S. Yu"
      ],
      "published": "2022-10-19T07:24:40Z",
      "updated": "2023-05-26T16:14:43Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.10343v2",
      "landing_url": "https://arxiv.org/abs/2210.10343v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.10343"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item analyzes text-based data augmentation for NER and centers on NLP text manipulation, not on Spoken Language Models or speech-based modeling; it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item analyzes text-based data augmentation for NER and centers on NLP text manipulation, not on Spoken Language Models or speech-based modeling; it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on data augmentation techniques for Named Entity Recognition (NER) tasks, which are text-based natural language processing tasks and do not address spoken language models, speech-level modeling, or full-duplex spoken language interactive systems as specified in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on data augmentation techniques for Named Entity Recognition (NER) tasks, which are text-based natural language processing tasks and do not address spoken language models, speech-level modeling, or full-duplex spoken language interactive systems as specified in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Bidirectional Integrated Sensing and Communication: Full-Duplex or Half-Duplex?",
    "abstract": "A bidirectional integrated sensing and communication (ISAC) system is proposed, in which a pair of transceivers carry out two-way communication and mutual sensing. Both full-duplex and half-duplex operations in narrowband and wideband systems are conceived for the bidirectional ISAC. 1) For the narrowband system, the conventional full-duplex and half-duplex operations are redesigned to take into account sensing echo signals. Then, the transmit beamforming design of both transceivers is proposed for addressing the sensing and communication (S&C) tradeoff. A one-layer iterative algorithm relying on successive convex approximation (SCA) is proposed to obtain Karush-Kuhn-Tucker (KKT) optimal solutions. 2) For the wideband system, the new full-duplex and half-duplex operations are proposed for the bidirectional ISAC. In particular, the frequency-selective fading channel is tackled by delay pre-compensation and path-based beamforming. By redesigning the proposed SCA-based algorithm, the KKT optimal solutions for path-based beamforming for characterizing the S&C tradeoff are obtained. Finally, the numerical results show that: i) For both bandwidth scenarios, the existence of the interference introduced by sensing results in full-duplex may not always outperform half-duplex, especially in the sensing-prior regime or when the communication channel is line-of-sight-dominated; and ii) For both duplex operations, it is sufficient to reuse communication signals for sensing in the narrowband system, while an additional dedicated sensing signal is required in the wideband system.",
    "metadata": {
      "arxiv_id": "2210.14112",
      "title": "Bidirectional Integrated Sensing and Communication: Full-Duplex or Half-Duplex?",
      "summary": "A bidirectional integrated sensing and communication (ISAC) system is proposed, in which a pair of transceivers carry out two-way communication and mutual sensing. Both full-duplex and half-duplex operations in narrowband and wideband systems are conceived for the bidirectional ISAC. 1) For the narrowband system, the conventional full-duplex and half-duplex operations are redesigned to take into account sensing echo signals. Then, the transmit beamforming design of both transceivers is proposed for addressing the sensing and communication (S&C) tradeoff. A one-layer iterative algorithm relying on successive convex approximation (SCA) is proposed to obtain Karush-Kuhn-Tucker (KKT) optimal solutions. 2) For the wideband system, the new full-duplex and half-duplex operations are proposed for the bidirectional ISAC. In particular, the frequency-selective fading channel is tackled by delay pre-compensation and path-based beamforming. By redesigning the proposed SCA-based algorithm, the KKT optimal solutions for path-based beamforming for characterizing the S&C tradeoff are obtained. Finally, the numerical results show that: i) For both bandwidth scenarios, the existence of the interference introduced by sensing results in full-duplex may not always outperform half-duplex, especially in the sensing-prior regime or when the communication channel is line-of-sight-dominated; and ii) For both duplex operations, it is sufficient to reuse communication signals for sensing in the narrowband system, while an additional dedicated sensing signal is required in the wideband system.",
      "authors": [
        "Zhaolin Wang",
        "Xidong Mu",
        "Yuanwei Liu"
      ],
      "published": "2022-10-25T15:59:06Z",
      "updated": "2024-01-03T20:45:35Z",
      "categories": [
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.14112v2",
      "landing_url": "https://arxiv.org/abs/2210.14112v2",
      "doi": "https://doi.org/10.1109/TWC.2023.3344229"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on wireless Integrated Sensing and Communication with full-/half-duplex design and beamforming, not on Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on wireless Integrated Sensing and Communication with full-/half-duplex design and beamforming, not on Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on bidirectional integrated sensing and communication systems using full-duplex and half-duplex operations primarily in the context of signal processing and beamforming, not on Spoken Language Models or speech language modeling as defined by the inclusion criteria. The core research is not about language modeling of speech or synchronized interactive spoken language modeling but rather on communication and sensing trade-offs in transceiver design, without involving speech as the primary modeling object or evaluation of real-time spoken interaction features.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on bidirectional integrated sensing and communication systems using full-duplex and half-duplex operations primarily in the context of signal processing and beamforming, not on Spoken Language Models or speech language modeling as defined by the inclusion criteria. The core research is not about language modeling of speech or synchronized interactive spoken language modeling but rather on communication and sensing trade-offs in transceiver design, without involving speech as the primary modeling object or evaluation of real-time spoken interaction features.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Exploring Predictive Uncertainty and Calibration in NLP: A Study on the Impact of Method & Data Scarcity",
    "abstract": "We investigate the problem of determining the predictive confidence (or, conversely, uncertainty) of a neural classifier through the lens of low-resource languages. By training models on sub-sampled datasets in three different languages, we assess the quality of estimates from a wide array of approaches and their dependence on the amount of available data. We find that while approaches based on pre-trained models and ensembles achieve the best results overall, the quality of uncertainty estimates can surprisingly suffer with more data. We also perform a qualitative analysis of uncertainties on sequences, discovering that a model's total uncertainty seems to be influenced to a large degree by its data uncertainty, not model uncertainty. All model implementations are open-sourced in a software package.",
    "metadata": {
      "arxiv_id": "2210.15452",
      "title": "Exploring Predictive Uncertainty and Calibration in NLP: A Study on the Impact of Method & Data Scarcity",
      "summary": "We investigate the problem of determining the predictive confidence (or, conversely, uncertainty) of a neural classifier through the lens of low-resource languages. By training models on sub-sampled datasets in three different languages, we assess the quality of estimates from a wide array of approaches and their dependence on the amount of available data. We find that while approaches based on pre-trained models and ensembles achieve the best results overall, the quality of uncertainty estimates can surprisingly suffer with more data. We also perform a qualitative analysis of uncertainties on sequences, discovering that a model's total uncertainty seems to be influenced to a large degree by its data uncertainty, not model uncertainty. All model implementations are open-sourced in a software package.",
      "authors": [
        "Dennis Ulmer",
        "Jes Frellsen",
        "Christian Hardmeier"
      ],
      "published": "2022-10-20T15:42:02Z",
      "updated": "2022-10-20T15:42:02Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.15452v1",
      "landing_url": "https://arxiv.org/abs/2210.15452v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.15452"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses predictive uncertainty in NLP for low-resource languages but does not involve Spoken Language Models, speech signals, or speech-language modeling concepts, thus failing the core inclusion criteria and likely meeting at least one exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses predictive uncertainty in NLP for low-resource languages but does not involve Spoken Language Models, speech signals, or speech-language modeling concepts, thus failing the core inclusion criteria and likely meeting at least one exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on predictive uncertainty and calibration in neural classifiers for NLP in low-resource languages, without mentioning spoken language models, speech-based modeling, or relevant concepts such as full-duplex interaction or speech-LLM architectures. The work centers on traditional NLP classification tasks rather than speech or spoken language modeling, and does not discuss real-time speech interaction or related evaluation metrics. Therefore, it does not meet the inclusion criteria focused on spoken language modeling and speech-centric architectures.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on predictive uncertainty and calibration in neural classifiers for NLP in low-resource languages, without mentioning spoken language models, speech-based modeling, or relevant concepts such as full-duplex interaction or speech-LLM architectures. The work centers on traditional NLP classification tasks rather than speech or spoken language modeling, and does not discuss real-time speech interaction or related evaluation metrics. Therefore, it does not meet the inclusion criteria focused on spoken language modeling and speech-centric architectures.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Simulating realistic speech overlaps improves multi-talker ASR",
    "abstract": "Multi-talker automatic speech recognition (ASR) has been studied to generate transcriptions of natural conversation including overlapping speech of multiple speakers. Due to the difficulty in acquiring real conversation data with high-quality human transcriptions, a naïve simulation of multi-talker speech by randomly mixing multiple utterances was conventionally used for model training. In this work, we propose an improved technique to simulate multi-talker overlapping speech with realistic speech overlaps, where an arbitrary pattern of speech overlaps is represented by a sequence of discrete tokens. With this representation, speech overlapping patterns can be learned from real conversations based on a statistical language model, such as N-gram, which can be then used to generate multi-talker speech for training. In our experiments, multi-talker ASR models trained with the proposed method show consistent improvement on the word error rates across multiple datasets.",
    "metadata": {
      "arxiv_id": "2210.15715",
      "title": "Simulating realistic speech overlaps improves multi-talker ASR",
      "summary": "Multi-talker automatic speech recognition (ASR) has been studied to generate transcriptions of natural conversation including overlapping speech of multiple speakers. Due to the difficulty in acquiring real conversation data with high-quality human transcriptions, a naïve simulation of multi-talker speech by randomly mixing multiple utterances was conventionally used for model training. In this work, we propose an improved technique to simulate multi-talker overlapping speech with realistic speech overlaps, where an arbitrary pattern of speech overlaps is represented by a sequence of discrete tokens. With this representation, speech overlapping patterns can be learned from real conversations based on a statistical language model, such as N-gram, which can be then used to generate multi-talker speech for training. In our experiments, multi-talker ASR models trained with the proposed method show consistent improvement on the word error rates across multiple datasets.",
      "authors": [
        "Muqiao Yang",
        "Naoyuki Kanda",
        "Xiaofei Wang",
        "Jian Wu",
        "Sunit Sivasankaran",
        "Zhuo Chen",
        "Jinyu Li",
        "Takuya Yoshioka"
      ],
      "published": "2022-10-27T18:29:39Z",
      "updated": "2022-11-17T19:21:48Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.15715v2",
      "landing_url": "https://arxiv.org/abs/2210.15715v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.15715"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the work targets multi-talker speech and uses a language model to simulate realistic overlaps for ASR training, it does not treat Spoken Language Models or Full-Duplex SLMs as the core focus, relies on ASR/WER as the primary evaluation, and does not explicitly discuss interactive dialogue/speech-language modeling frameworks per the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the work targets multi-talker speech and uses a language model to simulate realistic overlaps for ASR training, it does not treat Spoken Language Models or Full-Duplex SLMs as the core focus, relies on ASR/WER as the primary evaluation, and does not explicitly discuss interactive dialogue/speech-language modeling frameworks per the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multi-talker ASR with simulated realistic speech overlaps, which mainly targets improving speech recognition performance rather than core spoken language modeling or full-duplex spoken language models for synchronous interaction; it does not explicitly mention spoken language models, language modeling of speech, or integration with speech-LLM architectures, nor does it address behavioral arbitration or real-time interactive aspects required by the inclusion criteria. Therefore, it predominantly fits within traditional ASR improvements without covering the key concepts of full-duplex spoken language models or cognitive parallelism, leading to exclusion based on the inclusion and exclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multi-talker ASR with simulated realistic speech overlaps, which mainly targets improving speech recognition performance rather than core spoken language modeling or full-duplex spoken language models for synchronous interaction; it does not explicitly mention spoken language models, language modeling of speech, or integration with speech-LLM architectures, nor does it address behavioral arbitration or real-time interactive aspects required by the inclusion criteria. Therefore, it predominantly fits within traditional ASR improvements without covering the key concepts of full-duplex spoken language models or cognitive parallelism, leading to exclusion based on the inclusion and exclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Self-supervised language learning from raw audio: Lessons from the Zero Resource Speech Challenge",
    "abstract": "Recent progress in self-supervised or unsupervised machine learning has opened the possibility of building a full speech processing system from raw audio without using any textual representations or expert labels such as phonemes, dictionaries or parse trees. The contribution of the Zero Resource Speech Challenge series since 2015 has been to break down this long-term objective into four well-defined tasks -- Acoustic Unit Discovery, Spoken Term Discovery, Discrete Resynthesis, and Spoken Language Modeling -- and introduce associated metrics and benchmarks enabling model comparison and cumulative progress. We present an overview of the six editions of this challenge series since 2015, discuss the lessons learned, and outline the areas which need more work or give puzzling results.",
    "metadata": {
      "arxiv_id": "2210.15759",
      "title": "Self-supervised language learning from raw audio: Lessons from the Zero Resource Speech Challenge",
      "summary": "Recent progress in self-supervised or unsupervised machine learning has opened the possibility of building a full speech processing system from raw audio without using any textual representations or expert labels such as phonemes, dictionaries or parse trees. The contribution of the Zero Resource Speech Challenge series since 2015 has been to break down this long-term objective into four well-defined tasks -- Acoustic Unit Discovery, Spoken Term Discovery, Discrete Resynthesis, and Spoken Language Modeling -- and introduce associated metrics and benchmarks enabling model comparison and cumulative progress. We present an overview of the six editions of this challenge series since 2015, discuss the lessons learned, and outline the areas which need more work or give puzzling results.",
      "authors": [
        "Ewan Dunbar",
        "Nicolas Hamilakis",
        "Emmanuel Dupoux"
      ],
      "published": "2022-10-27T20:32:41Z",
      "updated": "2022-10-27T20:32:41Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.15759v1",
      "landing_url": "https://arxiv.org/abs/2210.15759v1",
      "doi": "https://doi.org/10.1109/JSTSP.2022.3206084"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on Spoken Language Modeling from raw audio within the Zero Resource Speech Challenge, covering model architectures, training/evaluation, and benchmarks, and it aligns with the inclusion criteria while not triggering any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work focuses on Spoken Language Modeling from raw audio within the Zero Resource Speech Challenge, covering model architectures, training/evaluation, and benchmarks, and it aligns with the inclusion criteria while not triggering any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised language learning from raw audio and discusses the Zero Resource Speech Challenge involving tasks that align with acoustic unit discovery and spoken language modeling. However, the abstract does not explicitly mention \"Spoken Language Models\" or equivalent terms, nor does it address full-duplex spoken language models, cognitive parallelism, synchronous interaction features like turn-taking or interruption management, or any real-time interaction evaluations as specified in the inclusion criteria. The scope seems oriented towards unsupervised speech representations and benchmarking, without explicit emphasis on dialogue or interactive speech language modeling architectures or evaluation. Therefore, it does not fully meet all the inclusion criteria and partly aligns with exclusion criteria related to focus and evaluation aspects.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised language learning from raw audio and discusses the Zero Resource Speech Challenge involving tasks that align with acoustic unit discovery and spoken language modeling. However, the abstract does not explicitly mention \"Spoken Language Models\" or equivalent terms, nor does it address full-duplex spoken language models, cognitive parallelism, synchronous interaction features like turn-taking or interruption management, or any real-time interaction evaluations as specified in the inclusion criteria. The scope seems oriented towards unsupervised speech representations and benchmarking, without explicit emphasis on dialogue or interactive speech language modeling architectures or evaluation. Therefore, it does not fully meet all the inclusion criteria and partly aligns with exclusion criteria related to focus and evaluation aspects.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Reasoning: The abstract explicitly mentions \"Spoken Language Modeling\" as one of the four core tasks of the Zero Resource Speech Challenge and describes metrics and benchmarks that enable model comparison and evaluation, satisfying the inclusion requirement that spoken-language modeling (or equivalent) appears in the abstract and that evaluation/benchmarking is discussed. It is a recent overview of multiple editions (implying cumulative progress and evaluation) rather than a pure ASR/TTS paper, so it engages with speech-level language modeling rather than treating speech only as preprocessing for text. JuniorNano favored inclusion for these reasons, while JuniorMini was cautious because the paper is a broad survey and does not focus on full‑duplex interaction, real‑time synchronization, or detailed FD‑SLM architectures; I agree it is not centered on full‑duplex interactive behaviors. Overall it meets the core inclusion criteria (spoken language modeling present and evaluation/benchmarks discussed) but does not deeply address FD‑SLM / synchronous interaction requirements, so it is a borderline but acceptable inclusion for the SLM-focused scope. One-sentence explanation: I score this a 4 because the paper explicitly treats spoken language modeling and benchmarking of speech-level representations (meeting the required criteria), though it is a broad survey and lacks emphasis on full‑duplex interactive behavior required by more specialized FD‑SLM inclusion.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Reasoning: The abstract explicitly mentions \"Spoken Language Modeling\" as one of the four core tasks of the Zero Resource Speech Challenge and describes metrics and benchmarks that enable model comparison and evaluation, satisfying the inclusion requirement that spoken-language modeling (or equivalent) appears in the abstract and that evaluation/benchmarking is discussed. It is a recent overview of multiple editions (implying cumulative progress and evaluation) rather than a pure ASR/TTS paper, so it engages with speech-level language modeling rather than treating speech only as preprocessing for text. JuniorNano favored inclusion for these reasons, while JuniorMini was cautious because the paper is a broad survey and does not focus on full‑duplex interaction, real‑time synchronization, or detailed FD‑SLM architectures; I agree it is not centered on full‑duplex interactive behaviors. Overall it meets the core inclusion criteria (spoken language modeling present and evaluation/benchmarks discussed) but does not deeply address FD‑SLM / synchronous interaction requirements, so it is a borderline but acceptable inclusion for the SLM-focused scope. One-sentence explanation: I score this a 4 because the paper explicitly treats spoken language modeling and benchmarking of speech-level representations (meeting the required criteria), though it is a broad survey and lacks emphasis on full‑duplex interactive behavior required by more specialized FD‑SLM inclusion.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Modelling M/M/R-JSQ-PS sojourn time distribution for Ultra-Reliable Low Latency Communication services",
    "abstract": "The future Internet promises to support time-sensitive services that require ultra low latencies and reliabilities of 99.99%. Recent advances in cellular and WiFi connections enhance the network to meet high reliability and ultra low latencies. However, the aforementioned services require that the server processing time ensures low latencies with high reliability, otherwise the end-to-end performance is not met. To that end, in this paper we use queuing theory to model the sojourn time distribution for Ultra-Reliable Low Latency Communication services of M/M/R-JSQ-PS systems: Markovian queues with R CPU servers following a join shortest queue processor-sharing discipline (for example Linux systems). We develop open-source simulation software, and develop and compare six analytical approximations for the sojourn time distribution. The proposed approximations yield Wasserstein distances below 2 time units, and upon medium loads incur into errors of less than 1.78 time units (e.g., milliseconds) for the 99.99th percentile sojourn time. Moreover, the proposed sojourn time approximations are stable regardless the number of CPUs and stay close to the simulations.",
    "metadata": {
      "arxiv_id": "2210.17227",
      "title": "Modelling M/M/R-JSQ-PS sojourn time distribution for Ultra-Reliable Low Latency Communication services",
      "summary": "The future Internet promises to support time-sensitive services that require ultra low latencies and reliabilities of 99.99%. Recent advances in cellular and WiFi connections enhance the network to meet high reliability and ultra low latencies. However, the aforementioned services require that the server processing time ensures low latencies with high reliability, otherwise the end-to-end performance is not met. To that end, in this paper we use queuing theory to model the sojourn time distribution for Ultra-Reliable Low Latency Communication services of M/M/R-JSQ-PS systems: Markovian queues with R CPU servers following a join shortest queue processor-sharing discipline (for example Linux systems). We develop open-source simulation software, and develop and compare six analytical approximations for the sojourn time distribution. The proposed approximations yield Wasserstein distances below 2 time units, and upon medium loads incur into errors of less than 1.78 time units (e.g., milliseconds) for the 99.99th percentile sojourn time. Moreover, the proposed sojourn time approximations are stable regardless the number of CPUs and stay close to the simulations.",
      "authors": [
        "Geraint I. Palmer",
        "Jorge Martín-Pérez"
      ],
      "published": "2022-10-31T11:11:41Z",
      "updated": "2022-12-22T20:06:59Z",
      "categories": [
        "cs.NI",
        "math.OC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.17227v2",
      "landing_url": "https://arxiv.org/abs/2210.17227v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.17227"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on queuing theory and end-to-end latency in URLLC, with no reference to Spoken Language Models or speech-based modeling; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on queuing theory and end-to-end latency in URLLC, with no reference to Spoken Language Models or speech-based modeling; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on modeling sojourn time distribution for ultra-reliable low-latency communications using queuing theory and does not discuss Spoken Language Models (SLMs) or related concepts involving speech-based modeling and interactive dialogue behaviors. It lacks any mention of speech language modeling, full-duplex interaction, or evaluation frameworks related to speech or language processing. Therefore, it does not meet the inclusion criteria focused on Spoken Language Models or full-duplex spoken interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on modeling sojourn time distribution for ultra-reliable low-latency communications using queuing theory and does not discuss Spoken Language Models (SLMs) or related concepts involving speech-based modeling and interactive dialogue behaviors. It lacks any mention of speech language modeling, full-duplex interaction, or evaluation frameworks related to speech or language processing. Therefore, it does not meet the inclusion criteria focused on Spoken Language Models or full-duplex spoken interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "I Hear Your True Colors: Image Guided Audio Generation",
    "abstract": "We propose Im2Wav, an image guided open-domain audio generation system. Given an input image or a sequence of images, Im2Wav generates a semantically relevant sound. Im2Wav is based on two Transformer language models, that operate over a hierarchical discrete audio representation obtained from a VQ-VAE based model. We first produce a low-level audio representation using a language model. Then, we upsample the audio tokens using an additional language model to generate a high-fidelity audio sample. We use the rich semantics of a pre-trained CLIP (Contrastive Language-Image Pre-training) embedding as a visual representation to condition the language model. In addition, to steer the generation process towards the conditioning image, we apply the classifier-free guidance method. Results suggest that Im2Wav significantly outperforms the evaluated baselines in both fidelity and relevance evaluation metrics. Additionally, we provide an ablation study to better assess the impact of each of the method components on overall performance. Lastly, to better evaluate image-to-audio models, we propose an out-of-domain image dataset, denoted as ImageHear. ImageHear can be used as a benchmark for evaluating future image-to-audio models. Samples and code can be found inside the manuscript.",
    "metadata": {
      "arxiv_id": "2211.03089",
      "title": "I Hear Your True Colors: Image Guided Audio Generation",
      "summary": "We propose Im2Wav, an image guided open-domain audio generation system. Given an input image or a sequence of images, Im2Wav generates a semantically relevant sound. Im2Wav is based on two Transformer language models, that operate over a hierarchical discrete audio representation obtained from a VQ-VAE based model. We first produce a low-level audio representation using a language model. Then, we upsample the audio tokens using an additional language model to generate a high-fidelity audio sample. We use the rich semantics of a pre-trained CLIP (Contrastive Language-Image Pre-training) embedding as a visual representation to condition the language model. In addition, to steer the generation process towards the conditioning image, we apply the classifier-free guidance method. Results suggest that Im2Wav significantly outperforms the evaluated baselines in both fidelity and relevance evaluation metrics. Additionally, we provide an ablation study to better assess the impact of each of the method components on overall performance. Lastly, to better evaluate image-to-audio models, we propose an out-of-domain image dataset, denoted as ImageHear. ImageHear can be used as a benchmark for evaluating future image-to-audio models. Samples and code can be found inside the manuscript.",
      "authors": [
        "Roy Sheffer",
        "Yossi Adi"
      ],
      "published": "2022-11-06T11:48:20Z",
      "updated": "2023-02-27T11:15:40Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.03089v2",
      "landing_url": "https://arxiv.org/abs/2211.03089v2",
      "doi": "https://doi.org/10.48550/arXiv.2211.03089"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on image-guided audio generation with language models operating over audio tokens, but it does not center on Spoken Language Models or a Speech Language Modeling paradigm (no emphasis on interactive turn-taking, low latency, or speech-centric evaluation). It treats audio generation as a cross-modal task (image to audio) rather than modeling spoken language itself as the core object, hence it does not meet the inclusion criteria and aligns with exclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on image-guided audio generation with language models operating over audio tokens, but it does not center on Spoken Language Models or a Speech Language Modeling paradigm (no emphasis on interactive turn-taking, low latency, or speech-centric evaluation). It treats audio generation as a cross-modal task (image to audio) rather than modeling spoken language itself as the core object, hence it does not meet the inclusion criteria and aligns with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on generating audio (semantically relevant sounds) from images rather than modeling speech or spoken language sequences themselves. It does not explicitly discuss spoken language models, speech language modeling, or full-duplex interaction capabilities, nor does it address synchronization, real-time interaction, or behavior arbitration related to spoken dialogue systems. Instead, it centers on open-domain audio generation conditioned on images, evaluated by fidelity and relevance metrics without particular emphasis on spoken language modeling or interactive speech behaviors. Therefore, it does not meet the inclusion criteria centered on Spoken Language Models or Full-Duplex SLMs, and some exclusion criteria are implied due to lack of focus on synchronization, dialogue, or spoken language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on generating audio (semantically relevant sounds) from images rather than modeling speech or spoken language sequences themselves. It does not explicitly discuss spoken language models, speech language modeling, or full-duplex interaction capabilities, nor does it address synchronization, real-time interaction, or behavior arbitration related to spoken dialogue systems. Instead, it centers on open-domain audio generation conditioned on images, evaluated by fidelity and relevance metrics without particular emphasis on spoken language modeling or interactive speech behaviors. Therefore, it does not meet the inclusion criteria centered on Spoken Language Models or Full-Duplex SLMs, and some exclusion criteria are implied due to lack of focus on synchronization, dialogue, or spoken language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Colocating Real-time Storage and Processing: An Analysis of Pull-based versus Push-based Streaming",
    "abstract": "Real-time Big Data architectures evolved into specialized layers for handling data streams' ingestion, storage, and processing over the past decade. Layered streaming architectures integrate pull-based read and push-based write RPC mechanisms implemented by stream ingestion/storage systems. In addition, stream processing engines expose source/sink interfaces, allowing them to decouple these systems easily. However, open-source streaming engines leverage workflow sources implemented through a pull-based approach, continuously issuing read RPCs towards the stream ingestion/storage, effectively competing with write RPCs. This paper proposes a unified streaming architecture that leverages push-based and/or pull-based source implementations for integrating ingestion/storage and processing engines that can reduce processing latency and increase system read and write throughput while making room for higher ingestion. We implement a novel push-based streaming source by replacing continuous pull-based RPCs with one single RPC and shared memory (storage and processing handle streaming data through pointers to shared objects). To this end, we conduct an experimental analysis of pull-based versus push-based design alternatives of the streaming source reader while considering a set of stream benchmarks and microbenchmarks and discuss the advantages of both approaches.",
    "metadata": {
      "arxiv_id": "2211.05857",
      "title": "Colocating Real-time Storage and Processing: An Analysis of Pull-based versus Push-based Streaming",
      "summary": "Real-time Big Data architectures evolved into specialized layers for handling data streams' ingestion, storage, and processing over the past decade. Layered streaming architectures integrate pull-based read and push-based write RPC mechanisms implemented by stream ingestion/storage systems. In addition, stream processing engines expose source/sink interfaces, allowing them to decouple these systems easily. However, open-source streaming engines leverage workflow sources implemented through a pull-based approach, continuously issuing read RPCs towards the stream ingestion/storage, effectively competing with write RPCs. This paper proposes a unified streaming architecture that leverages push-based and/or pull-based source implementations for integrating ingestion/storage and processing engines that can reduce processing latency and increase system read and write throughput while making room for higher ingestion. We implement a novel push-based streaming source by replacing continuous pull-based RPCs with one single RPC and shared memory (storage and processing handle streaming data through pointers to shared objects). To this end, we conduct an experimental analysis of pull-based versus push-based design alternatives of the streaming source reader while considering a set of stream benchmarks and microbenchmarks and discuss the advantages of both approaches.",
      "authors": [
        "Ovidiu-Cristian Marcu",
        "Pascal Bouvry"
      ],
      "published": "2022-11-10T20:26:07Z",
      "updated": "2022-11-10T20:26:07Z",
      "categories": [
        "cs.DC",
        "cs.PF"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.05857v1",
      "landing_url": "https://arxiv.org/abs/2211.05857v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.05857"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses pull-based vs push-based streaming architectures for real-time data processing, with no focus on Spoken Language Models or speech-language modeling; it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses pull-based vs push-based streaming architectures for real-time data processing, with no focus on Spoken Language Models or speech-language modeling; it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on real-time big data architectures for streaming data ingestion, storage, and processing, without any reference to spoken language models or speech-based language modeling, nor does it address aspects like full-duplex spoken language interaction, speech overlapping, or cognitive parallelism as required by the inclusion criteria. Thus, it does not meet the thematic focus on speech modeling and synchronous spoken communication.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on real-time big data architectures for streaming data ingestion, storage, and processing, without any reference to spoken language models or speech-based language modeling, nor does it address aspects like full-duplex spoken language interaction, speech overlapping, or cognitive parallelism as required by the inclusion criteria. Thus, it does not meet the thematic focus on speech modeling and synchronous spoken communication.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Direct Inversion: Optimization-Free Text-Driven Real Image Editing with Diffusion Models",
    "abstract": "With the rise of large, publicly-available text-to-image diffusion models, text-guided real image editing has garnered much research attention recently. Existing methods tend to either rely on some form of per-instance or per-task fine-tuning and optimization, require multiple novel views, or they inherently entangle preservation of real image identity, semantic coherence, and faithfulness to text guidance. In this paper, we propose an optimization-free and zero fine-tuning framework that applies complex and non-rigid edits to a single real image via a text prompt, avoiding all the pitfalls described above. Using widely-available generic pre-trained text-to-image diffusion models, we demonstrate the ability to modulate pose, scene, background, style, color, and even racial identity in an extremely flexible manner through a single target text detailing the desired edit. Furthermore, our method, which we name $\\textit{Direct Inversion}$, proposes multiple intuitively configurable hyperparameters to allow for a wide range of types and extents of real image edits. We prove our method's efficacy in producing high-quality, diverse, semantically coherent, and faithful real image edits through applying it on a variety of inputs for a multitude of tasks. We also formalize our method in well-established theory, detail future experiments for further improvement, and compare against state-of-the-art attempts.",
    "metadata": {
      "arxiv_id": "2211.07825",
      "title": "Direct Inversion: Optimization-Free Text-Driven Real Image Editing with Diffusion Models",
      "summary": "With the rise of large, publicly-available text-to-image diffusion models, text-guided real image editing has garnered much research attention recently. Existing methods tend to either rely on some form of per-instance or per-task fine-tuning and optimization, require multiple novel views, or they inherently entangle preservation of real image identity, semantic coherence, and faithfulness to text guidance. In this paper, we propose an optimization-free and zero fine-tuning framework that applies complex and non-rigid edits to a single real image via a text prompt, avoiding all the pitfalls described above. Using widely-available generic pre-trained text-to-image diffusion models, we demonstrate the ability to modulate pose, scene, background, style, color, and even racial identity in an extremely flexible manner through a single target text detailing the desired edit. Furthermore, our method, which we name $\\textit{Direct Inversion}$, proposes multiple intuitively configurable hyperparameters to allow for a wide range of types and extents of real image edits. We prove our method's efficacy in producing high-quality, diverse, semantically coherent, and faithful real image edits through applying it on a variety of inputs for a multitude of tasks. We also formalize our method in well-established theory, detail future experiments for further improvement, and compare against state-of-the-art attempts.",
      "authors": [
        "Adham Elarabawy",
        "Harish Kamath",
        "Samuel Denton"
      ],
      "published": "2022-11-15T01:07:38Z",
      "updated": "2022-11-15T01:07:38Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.07825v1",
      "landing_url": "https://arxiv.org/abs/2211.07825v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.07825"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on optimization-free text-driven real image editing with diffusion models, which has no involvement of Spoken Language Models or speech-language modeling concepts; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on optimization-free text-driven real image editing with diffusion models, which has no involvement of Spoken Language Models or speech-language modeling concepts; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on text-guided real image editing using diffusion models and does not address Spoken Language Models, speech as the core modeling subject, or any aspects related to full-duplex spoken interaction, synchronization, or speech-language modeling as required by the inclusion criteria. Therefore, it does not meet the thematic requirements of Spoken Language Models and related concepts and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on text-guided real image editing using diffusion models and does not address Spoken Language Models, speech as the core modeling subject, or any aspects related to full-duplex spoken interaction, synchronization, or speech-language modeling as required by the inclusion criteria. Therefore, it does not meet the thematic requirements of Spoken Language Models and related concepts and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Utilising physics-guided deep learning to overcome data scarcity",
    "abstract": "Deep learning (DL) relies heavily on data, and the quality of data influences its performance significantly. However, obtaining high-quality, well-annotated datasets can be challenging or even impossible in many real-world applications, such as structural risk estimation and medical diagnosis. This presents a significant barrier to the practical implementation of DL in these fields. Physics-guided deep learning (PGDL) is a novel type of DL that can integrate physics laws to train neural networks. This can be applied to any systems that are controlled or governed by physics laws, such as mechanics, finance and medical applications. It has been demonstrated that, with the additional information provided by physics laws, PGDL achieves great accuracy and generalisation in the presence of data scarcity. This review provides a detailed examination of PGDL and offers a structured overview of its use in addressing data scarcity across various fields, including physics, engineering and medical applications. Moreover, the review identifies the current limitations and opportunities for PGDL in relation to data scarcity and offers a thorough discussion on the future prospects of PGDL.",
    "metadata": {
      "arxiv_id": "2211.15664",
      "title": "Utilising physics-guided deep learning to overcome data scarcity",
      "summary": "Deep learning (DL) relies heavily on data, and the quality of data influences its performance significantly. However, obtaining high-quality, well-annotated datasets can be challenging or even impossible in many real-world applications, such as structural risk estimation and medical diagnosis. This presents a significant barrier to the practical implementation of DL in these fields. Physics-guided deep learning (PGDL) is a novel type of DL that can integrate physics laws to train neural networks. This can be applied to any systems that are controlled or governed by physics laws, such as mechanics, finance and medical applications. It has been demonstrated that, with the additional information provided by physics laws, PGDL achieves great accuracy and generalisation in the presence of data scarcity. This review provides a detailed examination of PGDL and offers a structured overview of its use in addressing data scarcity across various fields, including physics, engineering and medical applications. Moreover, the review identifies the current limitations and opportunities for PGDL in relation to data scarcity and offers a thorough discussion on the future prospects of PGDL.",
      "authors": [
        "Jinshuai Bai",
        "Laith Alzubaidi",
        "Qingxia Wang",
        "Ellen Kuhl",
        "Mohammed Bennamoun",
        "Yuantong Gu"
      ],
      "published": "2022-11-24T01:03:21Z",
      "updated": "2023-01-09T01:55:30Z",
      "categories": [
        "cs.LG",
        "cs.CE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.15664v3",
      "landing_url": "https://arxiv.org/abs/2211.15664v3",
      "doi": "https://doi.org/10.48550/arXiv.2211.15664"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses physics-guided deep learning to address data scarcity and has no focus on Spoken Language Models or speech-language modeling; it does not meet the inclusion criteria and does not align with the exclusion criteria being about speech interaction tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses physics-guided deep learning to address data scarcity and has no focus on Spoken Language Models or speech-language modeling; it does not meet the inclusion criteria and does not align with the exclusion criteria being about speech interaction tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on physics-guided deep learning to overcome data scarcity in various domains such as mechanics, finance, and medical applications, without any mention of spoken language models, speech language modeling, full-duplex interaction, or relevant speech modeling architecture or evaluations. Therefore, it does not meet the inclusion criteria centered on spoken language models and associated full-duplex interactive capabilities, and it does not address any of the required speech-based modeling concepts or evaluations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on physics-guided deep learning to overcome data scarcity in various domains such as mechanics, finance, and medical applications, without any mention of spoken language models, speech language modeling, full-duplex interaction, or relevant speech modeling architecture or evaluations. Therefore, it does not meet the inclusion criteria centered on spoken language models and associated full-duplex interactive capabilities, and it does not address any of the required speech-based modeling concepts or evaluations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Modeling animal contests based on spatio-temporal dynamics",
    "abstract": "We present a general theoretical model for the spatio-temporal dynamics of animal contests. Inspired by interactions between physical particles, the model is formulated in terms of effective interaction potentials, which map typical elements of contest behaviour into empirically verifiable rules of contestant motion. This allows us to simulate the observable dynamics of contests in various realistic scenarios, notably in dyadic contests over a localized resource. Assessment strategies previously formulated in game-theoretic models, as well as the effects of fighting costs, can be described as variations in our model's parameters. Furthermore, the trends of contest duration associated with these assessment strategies can be derived and understood within the model. Detailed description of the contestants' motion enables the exploration of spatio-temporal properties of asymmetric contests, such as the emergence of chase dynamics. Overall, our framework aims to bridge the growing gap between empirical capabilities and theory in this widespread aspect of animal behaviour.",
    "metadata": {
      "arxiv_id": "2211.16542",
      "title": "Modeling animal contests based on spatio-temporal dynamics",
      "summary": "We present a general theoretical model for the spatio-temporal dynamics of animal contests. Inspired by interactions between physical particles, the model is formulated in terms of effective interaction potentials, which map typical elements of contest behaviour into empirically verifiable rules of contestant motion. This allows us to simulate the observable dynamics of contests in various realistic scenarios, notably in dyadic contests over a localized resource. Assessment strategies previously formulated in game-theoretic models, as well as the effects of fighting costs, can be described as variations in our model's parameters. Furthermore, the trends of contest duration associated with these assessment strategies can be derived and understood within the model. Detailed description of the contestants' motion enables the exploration of spatio-temporal properties of asymmetric contests, such as the emergence of chase dynamics. Overall, our framework aims to bridge the growing gap between empirical capabilities and theory in this widespread aspect of animal behaviour.",
      "authors": [
        "Amir Haluts",
        "Alex Jordan",
        "Nir S. Gov"
      ],
      "published": "2022-11-29T19:06:37Z",
      "updated": "2022-11-29T19:06:37Z",
      "categories": [
        "physics.bio-ph",
        "q-bio.QM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.16542v1",
      "landing_url": "https://arxiv.org/abs/2211.16542v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.16542"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper models spatio-temporal dynamics of animal contests using physics-inspired interactions and does not address Spoken Language Models or speech-language modeling concepts, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper models spatio-temporal dynamics of animal contests using physics-inspired interactions and does not address Spoken Language Models or speech-language modeling concepts, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on theoretical modeling of animal contest behavior using spatio-temporal dynamics and interaction potentials, with no mention of spoken language models, speech language modeling, full-duplex spoken interaction, or related core concepts specified in the inclusion criteria. It does not address any topics related to speech signal modeling, synchronous speech interaction, or the architectures/training/evaluation of speech language models. Therefore, it does not meet the inclusion criteria and falls outside the scope defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on theoretical modeling of animal contest behavior using spatio-temporal dynamics and interaction potentials, with no mention of spoken language models, speech language modeling, full-duplex spoken interaction, or related core concepts specified in the inclusion criteria. It does not address any topics related to speech signal modeling, synchronous speech interaction, or the architectures/training/evaluation of speech language models. Therefore, it does not meet the inclusion criteria and falls outside the scope defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Multi-Task Edge Prediction in Temporally-Dynamic Video Graphs",
    "abstract": "Graph neural networks have shown to learn effective node representations, enabling node-, link-, and graph-level inference. Conventional graph networks assume static relations between nodes, while relations between entities in a video often evolve over time, with nodes entering and exiting dynamically. In such temporally-dynamic graphs, a core problem is inferring the future state of spatio-temporal edges, which can constitute multiple types of relations. To address this problem, we propose MTD-GNN, a graph network for predicting temporally-dynamic edges for multiple types of relations. We propose a factorized spatio-temporal graph attention layer to learn dynamic node representations and present a multi-task edge prediction loss that models multiple relations simultaneously. The proposed architecture operates on top of scene graphs that we obtain from videos through object detection and spatio-temporal linking. Experimental evaluations on ActionGenome and CLEVRER show that modeling multiple relations in our temporally-dynamic graph network can be mutually beneficial, outperforming existing static and spatio-temporal graph neural networks, as well as state-of-the-art predicate classification methods.",
    "metadata": {
      "arxiv_id": "2212.02875",
      "title": "Multi-Task Edge Prediction in Temporally-Dynamic Video Graphs",
      "summary": "Graph neural networks have shown to learn effective node representations, enabling node-, link-, and graph-level inference. Conventional graph networks assume static relations between nodes, while relations between entities in a video often evolve over time, with nodes entering and exiting dynamically. In such temporally-dynamic graphs, a core problem is inferring the future state of spatio-temporal edges, which can constitute multiple types of relations. To address this problem, we propose MTD-GNN, a graph network for predicting temporally-dynamic edges for multiple types of relations. We propose a factorized spatio-temporal graph attention layer to learn dynamic node representations and present a multi-task edge prediction loss that models multiple relations simultaneously. The proposed architecture operates on top of scene graphs that we obtain from videos through object detection and spatio-temporal linking. Experimental evaluations on ActionGenome and CLEVRER show that modeling multiple relations in our temporally-dynamic graph network can be mutually beneficial, outperforming existing static and spatio-temporal graph neural networks, as well as state-of-the-art predicate classification methods.",
      "authors": [
        "Osman Ülger",
        "Julian Wiederer",
        "Mohsen Ghafoorian",
        "Vasileios Belagiannis",
        "Pascal Mettes"
      ],
      "published": "2022-12-06T10:41:00Z",
      "updated": "2022-12-06T10:41:00Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.02875v1",
      "landing_url": "https://arxiv.org/abs/2212.02875v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.02875"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item does not center on Spoken Language Models (speech-language modeling) or any equivalent speech-centric framework; it analyzes graph neural networks for temporally dynamic video scene graphs, which is not aligned with the inclusion criteria for spoken language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item does not center on Spoken Language Models (speech-language modeling) or any equivalent speech-centric framework; it analyzes graph neural networks for temporally dynamic video scene graphs, which is not aligned with the inclusion criteria for spoken language modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on graph neural networks for temporally-dynamic video graphs and edge prediction, without any mention of spoken language models, speech language modeling, or speech-related architectures, training, or evaluation, and thus does not meet the inclusion criteria related to spoken language models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on graph neural networks for temporally-dynamic video graphs and edge prediction, without any mention of spoken language models, speech language modeling, or speech-related architectures, training, or evaluation, and thus does not meet the inclusion criteria related to spoken language models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SpeechLMScore: Evaluating speech generation using speech language model",
    "abstract": "While human evaluation is the most reliable metric for evaluating speech generation systems, it is generally costly and time-consuming. Previous studies on automatic speech quality assessment address the problem by predicting human evaluation scores with machine learning models. However, they rely on supervised learning and thus suffer from high annotation costs and domain-shift problems. We propose SpeechLMScore, an unsupervised metric to evaluate generated speech using a speech-language model. SpeechLMScore computes the average log-probability of a speech signal by mapping it into discrete tokens and measures the average probability of generating the sequence of tokens. Therefore, it does not require human annotation and is a highly scalable framework. Evaluation results demonstrate that the proposed metric shows a promising correlation with human evaluation scores on different speech generation tasks including voice conversion, text-to-speech, and speech enhancement.",
    "metadata": {
      "arxiv_id": "2212.04559",
      "title": "SpeechLMScore: Evaluating speech generation using speech language model",
      "summary": "While human evaluation is the most reliable metric for evaluating speech generation systems, it is generally costly and time-consuming. Previous studies on automatic speech quality assessment address the problem by predicting human evaluation scores with machine learning models. However, they rely on supervised learning and thus suffer from high annotation costs and domain-shift problems. We propose SpeechLMScore, an unsupervised metric to evaluate generated speech using a speech-language model. SpeechLMScore computes the average log-probability of a speech signal by mapping it into discrete tokens and measures the average probability of generating the sequence of tokens. Therefore, it does not require human annotation and is a highly scalable framework. Evaluation results demonstrate that the proposed metric shows a promising correlation with human evaluation scores on different speech generation tasks including voice conversion, text-to-speech, and speech enhancement.",
      "authors": [
        "Soumi Maiti",
        "Yifan Peng",
        "Takaaki Saeki",
        "Shinji Watanabe"
      ],
      "published": "2022-12-08T21:00:15Z",
      "updated": "2022-12-08T21:00:15Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.04559v1",
      "landing_url": "https://arxiv.org/abs/2212.04559v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.04559"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on a Speech Language Model for evaluating generated speech across multiple tasks (voice conversion, TTS, and speech enhancement), treats speech as the core modeling object with architecture/evaluation focus, includes 'speech language model' in the title/abstract, and is published in 2025, satisfying the inclusion criteria and not falling under the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This item centers on a Speech Language Model for evaluating generated speech across multiple tasks (voice conversion, TTS, and speech enhancement), treats speech as the core modeling object with architecture/evaluation focus, includes 'speech language model' in the title/abstract, and is published in 2025, satisfying the inclusion criteria and not falling under the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes an unsupervised metric, SpeechLMScore, for evaluating generated speech using a speech-language model, focusing on speech sequence modeling rather than text-only processing, and demonstrating evaluation across various speech generation tasks; however, it does not explicitly discuss full-duplex or synchronous interactive capabilities, nor cognitive parallelism or behavior arbitration, which are key aspects of the inclusion criteria, making it uncertain if it fully aligns with the scope emphasizing full-duplex spoken language models and their synchronization/interaction aspects.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study proposes an unsupervised metric, SpeechLMScore, for evaluating generated speech using a speech-language model, focusing on speech sequence modeling rather than text-only processing, and demonstrating evaluation across various speech generation tasks; however, it does not explicitly discuss full-duplex or synchronous interactive capabilities, nor cognitive parallelism or behavior arbitration, which are key aspects of the inclusion criteria, making it uncertain if it fully aligns with the scope emphasizing full-duplex spoken language models and their synchronization/interaction aspects.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed (one recommended inclusion, the other was uncertain). Although the paper explicitly uses a \"speech-language model\" to compute unsupervised scores for generated speech and evaluates across TTS/voice conversion/speech enhancement, it focuses on an evaluation metric for generation tasks and does not address full-duplex spoken interaction, synchronization/learning-of-synchronization, overlap/interruption management, cognitive parallelism, or low-latency interactive behavior required by the inclusion criteria — therefore it should be excluded. One-sentence explanation: Exclude because it lacks any treatment of full‑duplex interaction, synchronization, or behavior arbitration central to the SLM/FD‑SLM scope.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed (one recommended inclusion, the other was uncertain). Although the paper explicitly uses a \"speech-language model\" to compute unsupervised scores for generated speech and evaluates across TTS/voice conversion/speech enhancement, it focuses on an evaluation metric for generation tasks and does not address full-duplex spoken interaction, synchronization/learning-of-synchronization, overlap/interruption management, cognitive parallelism, or low-latency interactive behavior required by the inclusion criteria — therefore it should be excluded. One-sentence explanation: Exclude because it lacks any treatment of full‑duplex interaction, synchronization, or behavior arbitration central to the SLM/FD‑SLM scope.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "GazeNeRF: 3D-Aware Gaze Redirection with Neural Radiance Fields",
    "abstract": "We propose GazeNeRF, a 3D-aware method for the task of gaze redirection. Existing gaze redirection methods operate on 2D images and struggle to generate 3D consistent results. Instead, we build on the intuition that the face region and eyeballs are separate 3D structures that move in a coordinated yet independent fashion. Our method leverages recent advancements in conditional image-based neural radiance fields and proposes a two-stream architecture that predicts volumetric features for the face and eye regions separately. Rigidly transforming the eye features via a 3D rotation matrix provides fine-grained control over the desired gaze angle. The final, redirected image is then attained via differentiable volume compositing. Our experiments show that this architecture outperforms naively conditioned NeRF baselines as well as previous state-of-the-art 2D gaze redirection methods in terms of redirection accuracy and identity preservation.",
    "metadata": {
      "arxiv_id": "2212.04823",
      "title": "GazeNeRF: 3D-Aware Gaze Redirection with Neural Radiance Fields",
      "summary": "We propose GazeNeRF, a 3D-aware method for the task of gaze redirection. Existing gaze redirection methods operate on 2D images and struggle to generate 3D consistent results. Instead, we build on the intuition that the face region and eyeballs are separate 3D structures that move in a coordinated yet independent fashion. Our method leverages recent advancements in conditional image-based neural radiance fields and proposes a two-stream architecture that predicts volumetric features for the face and eye regions separately. Rigidly transforming the eye features via a 3D rotation matrix provides fine-grained control over the desired gaze angle. The final, redirected image is then attained via differentiable volume compositing. Our experiments show that this architecture outperforms naively conditioned NeRF baselines as well as previous state-of-the-art 2D gaze redirection methods in terms of redirection accuracy and identity preservation.",
      "authors": [
        "Alessandro Ruzzi",
        "Xiangwei Shi",
        "Xi Wang",
        "Gengyan Li",
        "Shalini De Mello",
        "Hyung Jin Chang",
        "Xucong Zhang",
        "Otmar Hilliges"
      ],
      "published": "2022-12-08T13:19:11Z",
      "updated": "2023-03-28T19:41:57Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.04823v2",
      "landing_url": "https://arxiv.org/abs/2212.04823v2",
      "doi": "https://doi.org/10.48550/arXiv.2212.04823"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本研究聚焦於 3D 視覺/臉部與眼球的重定向，屬於電腦視覺/電腦圖形領域，核心議題不是語音語言建模（SLMs/FD-SLMs），也未討論語音互動或同步行為，因此不符合主題與評估框架，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本研究聚焦於 3D 視覺/臉部與眼球的重定向，屬於電腦視覺/電腦圖形領域，核心議題不是語音語言建模（SLMs/FD-SLMs），也未討論語音互動或同步行為，因此不符合主題與評估框架，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on gaze redirection using 3D-aware methods and neural radiance fields, which does not relate to spoken language models, speech language modeling, or real-time spoken interaction systems as defined in the inclusion criteria. There is no discussion of speech, language modeling of speech, synchronous full-duplex spoken interaction, or related evaluation metrics.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on gaze redirection using 3D-aware methods and neural radiance fields, which does not relate to spoken language models, speech language modeling, or real-time spoken interaction systems as defined in the inclusion criteria. There is no discussion of speech, language modeling of speech, synchronous full-duplex spoken interaction, or related evaluation metrics.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Full-Duplex Magnetic Induction Communication: Opportunities and Challenges",
    "abstract": "The demand for high data rates is rapidly increasing as the interest in Magnetic Induction (MI) communication-based underwater applications grow. However, the data rate in MI is limited by the use of low operational frequency in generating a quasi-static magnetic field. In this paper, we propose the use of full-duplex (FD) MI communication to efficiently utilize the available bandwidth and instantly double the data rate. We propose a two-dimensional transceiver architecture to achieve full-duplex communication by exploiting the directional nature of magnetic fields. We further evaluate the proposed end-to-end FD MI communication against self-interference (SI), its impact on communication distance, and robustness in view of orientation sensitivity. Finally, we conclude by discussing typical challenges in the realization of FD MI communication and highlight a few potential future research directions.",
    "metadata": {
      "arxiv_id": "2212.08906",
      "title": "Full-Duplex Magnetic Induction Communication: Opportunities and Challenges",
      "summary": "The demand for high data rates is rapidly increasing as the interest in Magnetic Induction (MI) communication-based underwater applications grow. However, the data rate in MI is limited by the use of low operational frequency in generating a quasi-static magnetic field. In this paper, we propose the use of full-duplex (FD) MI communication to efficiently utilize the available bandwidth and instantly double the data rate. We propose a two-dimensional transceiver architecture to achieve full-duplex communication by exploiting the directional nature of magnetic fields. We further evaluate the proposed end-to-end FD MI communication against self-interference (SI), its impact on communication distance, and robustness in view of orientation sensitivity. Finally, we conclude by discussing typical challenges in the realization of FD MI communication and highlight a few potential future research directions.",
      "authors": [
        "Muhammad Muzzammil",
        "Saif Al-Kuwari",
        "Niaz Ahmed",
        "Marwa Qaraqe"
      ],
      "published": "2022-12-17T15:38:11Z",
      "updated": "2022-12-17T15:38:11Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.08906v1",
      "landing_url": "https://arxiv.org/abs/2212.08906v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.08906"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article concerns full-duplex magnetic induction communication for underwater channels and focuses on transceiver architecture and self-interference, with no discussion of Spoken Language Models, speech-language modeling, or dialogue/synchronous interaction features, thus not meeting the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article concerns full-duplex magnetic induction communication for underwater channels and focuses on transceiver architecture and self-interference, with no discussion of Spoken Language Models, speech-language modeling, or dialogue/synchronous interaction features, thus not meeting the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper addresses full-duplex magnetic induction communication technology for underwater applications, focusing on data rate improvement and physical-layer communication challenges; it does not discuss Spoken Language Models, speech language modeling, voice-based turn-taking, or related aspects of speech processing, modeling, or dialogue behavior arbitration as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper addresses full-duplex magnetic induction communication technology for underwater applications, focusing on data rate improvement and physical-layer communication challenges; it does not discuss Spoken Language Models, speech language modeling, voice-based turn-taking, or related aspects of speech processing, modeling, or dialogue behavior arbitration as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling",
    "abstract": "This work profoundly analyzes discrete self-supervised speech representations (units) through the eyes of Generative Spoken Language Modeling (GSLM). Following the findings of such an analysis, we propose practical improvements to the discrete unit for the GSLM. First, we start comprehending these units by analyzing them in three axes: interpretation, visualization, and resynthesis. Our analysis finds a high correlation between the speech units to phonemes and phoneme families, while their correlation with speaker or gender is weaker. Additionally, we found redundancies in the extracted units and claim that one reason may be the units' context. Following this analysis, we propose a new, unsupervised metric to measure unit redundancies. Finally, we use this metric to develop new methods that improve the robustness of units' clustering and show significant improvement considering zero-resource speech metrics such as ABX. Code and analysis tools are available under the following link: https://github.com/slp-rl/SLM-Discrete-Representations",
    "metadata": {
      "arxiv_id": "2301.00591",
      "title": "Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling",
      "summary": "This work profoundly analyzes discrete self-supervised speech representations (units) through the eyes of Generative Spoken Language Modeling (GSLM). Following the findings of such an analysis, we propose practical improvements to the discrete unit for the GSLM. First, we start comprehending these units by analyzing them in three axes: interpretation, visualization, and resynthesis. Our analysis finds a high correlation between the speech units to phonemes and phoneme families, while their correlation with speaker or gender is weaker. Additionally, we found redundancies in the extracted units and claim that one reason may be the units' context. Following this analysis, we propose a new, unsupervised metric to measure unit redundancies. Finally, we use this metric to develop new methods that improve the robustness of units' clustering and show significant improvement considering zero-resource speech metrics such as ABX. Code and analysis tools are available under the following link: https://github.com/slp-rl/SLM-Discrete-Representations",
      "authors": [
        "Amitay Sicherman",
        "Yossi Adi"
      ],
      "published": "2023-01-02T10:36:40Z",
      "updated": "2023-03-01T09:59:54Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.00591v3",
      "landing_url": "https://arxiv.org/abs/2301.00591v3",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10097097"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract explicitly address Spoken Language Modeling (GSLM) and discrete self-supervised speech representations, analyzes speech units for language modeling, discusses modeling/training/evaluation approaches, and is within the last 3 years with evaluation beyond standard ASR metrics, aligning with the inclusion criteria and not falling under the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The title/abstract explicitly address Spoken Language Modeling (GSLM) and discrete self-supervised speech representations, analyzes speech units for language modeling, discusses modeling/training/evaluation approaches, and is within the last 3 years with evaluation beyond standard ASR metrics, aligning with the inclusion criteria and not falling under the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study analyzes discrete self-supervised speech representations within the context of Generative Spoken Language Modeling, which aligns with the focus on spoken language models where speech units are centrally modeled and evaluated. It discusses the modeling and evaluation of speech units beyond text processing, includes analysis and improvements of model units, and involves zero-resource speech metrics rather than solely ASR or TTS evaluation. However, the abstract does not explicitly mention full-duplex or synchronous interactive spoken language modeling nor detailed evaluation of real-time interaction or behavior arbitration; thus, while it aligns with spoken language modeling, it may not fully encompass the entire inclusion scope related to full-duplex and synchronized interaction aspects. Given these factors, the study is relevant but may lack some full-duplex interaction facets required.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The study analyzes discrete self-supervised speech representations within the context of Generative Spoken Language Modeling, which aligns with the focus on spoken language models where speech units are centrally modeled and evaluated. It discusses the modeling and evaluation of speech units beyond text processing, includes analysis and improvements of model units, and involves zero-resource speech metrics rather than solely ASR or TTS evaluation. However, the abstract does not explicitly mention full-duplex or synchronous interactive spoken language modeling nor detailed evaluation of real-time interaction or behavior arbitration; thus, while it aligns with spoken language modeling, it may not fully encompass the entire inclusion scope related to full-duplex and synchronized interaction aspects. Given these factors, the study is relevant but may lack some full-duplex interaction facets required.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Muse: Text-To-Image Generation via Masked Generative Transformers",
    "abstract": "We present Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens. Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing. More results are available at https://muse-model.github.io",
    "metadata": {
      "arxiv_id": "2301.00704",
      "title": "Muse: Text-To-Image Generation via Masked Generative Transformers",
      "summary": "We present Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens. Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing. More results are available at https://muse-model.github.io",
      "authors": [
        "Huiwen Chang",
        "Han Zhang",
        "Jarred Barber",
        "AJ Maschinot",
        "Jose Lezama",
        "Lu Jiang",
        "Ming-Hsuan Yang",
        "Kevin Murphy",
        "William T. Freeman",
        "Michael Rubinstein",
        "Yuanzhen Li",
        "Dilip Krishnan"
      ],
      "published": "2023-01-02T14:43:38Z",
      "updated": "2023-01-02T14:43:38Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.00704v1",
      "landing_url": "https://arxiv.org/abs/2301.00704v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.00704"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article is about a text-to-image generation model (Muse) and does not address Spoken Language Models or speech-centric modeling; it fails the inclusion criteria and lacks alignment with the exclusion categories.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article is about a text-to-image generation model (Muse) and does not address Spoken Language Models or speech-centric modeling; it fails the inclusion criteria and lacks alignment with the exclusion categories.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on text-to-image generation using Transformers, with no emphasis on spoken language modeling, speech signals, or full-duplex spoken interaction, which are the core inclusion criteria; thus, it does not meet the thematic requirements of the inclusion criteria and falls outside the scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on text-to-image generation using Transformers, with no emphasis on spoken language modeling, speech signals, or full-duplex spoken interaction, which are the core inclusion criteria; thus, it does not meet the thematic requirements of the inclusion criteria and falls outside the scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "UAV aided Metaverse over Wireless Communications: A Reinforcement Learning Approach",
    "abstract": "Metaverse is expected to create a virtual world closely connected with reality to provide users with immersive experience with the support of 5G high data rate communication technique. A huge amount of data in physical world needs to be synchronized to the virtual world to provide immersive experience for users, and there will be higher requirements on coverage to include more users into Metaverse. However, 5G signal suffers severe attenuation, which makes it more expensive to maintain the same coverage. Unmanned aerial vehicle (UAV) is a promising candidate technique for future implementation of Metaverse as a low-cost and high-mobility platform for communication devices. In this paper, we propose a proximal policy optimization (PPO) based double-agent cooperative reinforcement learning method for channel allocation and trajectory control of UAV to collect and synchronize data from the physical world to the virtual world, and expand the coverage of Metaverse services economically. Simulation results show that our proposed method is able to achieve better performance compared to the benchmark approaches.",
    "metadata": {
      "arxiv_id": "2301.01474",
      "title": "UAV aided Metaverse over Wireless Communications: A Reinforcement Learning Approach",
      "summary": "Metaverse is expected to create a virtual world closely connected with reality to provide users with immersive experience with the support of 5G high data rate communication technique. A huge amount of data in physical world needs to be synchronized to the virtual world to provide immersive experience for users, and there will be higher requirements on coverage to include more users into Metaverse. However, 5G signal suffers severe attenuation, which makes it more expensive to maintain the same coverage. Unmanned aerial vehicle (UAV) is a promising candidate technique for future implementation of Metaverse as a low-cost and high-mobility platform for communication devices. In this paper, we propose a proximal policy optimization (PPO) based double-agent cooperative reinforcement learning method for channel allocation and trajectory control of UAV to collect and synchronize data from the physical world to the virtual world, and expand the coverage of Metaverse services economically. Simulation results show that our proposed method is able to achieve better performance compared to the benchmark approaches.",
      "authors": [
        "Peiyuan Si",
        "Wenhan Yu",
        "Jun Zhao",
        "Kwok-Yan Lam",
        "Qing Yang"
      ],
      "published": "2023-01-04T07:35:09Z",
      "updated": "2023-01-04T07:35:09Z",
      "categories": [
        "eess.SY",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.01474v1",
      "landing_url": "https://arxiv.org/abs/2301.01474v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.01474"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on UAV-based network optimization for Metaverse data synchronization using PPO-based reinforcement learning, with no focus on Spoken Language Models or speech-language modeling, nor on interactive speech phenomena, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper centers on UAV-based network optimization for Metaverse data synchronization using PPO-based reinforcement learning, with no focus on Spoken Language Models or speech-language modeling, nor on interactive speech phenomena, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on UAV-aided wireless communications for Metaverse applications using reinforcement learning for channel allocation and trajectory control, without any mention of spoken language models, speech language modeling, or related concepts central to the inclusion criteria. The study does not address speech as a primary modeling object, full-duplex spoken language modeling, nor any related evaluation or synchronization aspects. Hence, it does not meet the inclusion criteria and falls under exclusion due to unrelated research focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on UAV-aided wireless communications for Metaverse applications using reinforcement learning for channel allocation and trajectory control, without any mention of spoken language models, speech language modeling, or related concepts central to the inclusion criteria. The study does not address speech as a primary modeling object, full-duplex spoken language modeling, nor any related evaluation or synchronization aspects. Hence, it does not meet the inclusion criteria and falls under exclusion due to unrelated research focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
    "abstract": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.",
    "metadata": {
      "arxiv_id": "2301.02111",
      "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
      "summary": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.",
      "authors": [
        "Chengyi Wang",
        "Sanyuan Chen",
        "Yu Wu",
        "Ziqiang Zhang",
        "Long Zhou",
        "Shujie Liu",
        "Zhuo Chen",
        "Yanqing Liu",
        "Huaming Wang",
        "Jinyu Li",
        "Lei He",
        "Sheng Zhao",
        "Furu Wei"
      ],
      "published": "2023-01-05T15:37:15Z",
      "updated": "2023-01-05T15:37:15Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.02111v1",
      "landing_url": "https://arxiv.org/abs/2301.02111v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.02111"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item presents a language-model-based approach to speech synthesis (Vall-E) and treats speech as the main modeling target, detailing architecture, training (60k hours of data), and evaluation, within the last 3 years, and aligns with the Spoken Language Models / language modeling of speech concept.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item presents a language-model-based approach to speech synthesis (Vall-E) and treats speech as the main modeling target, detailing architecture, training (60k hours of data), and evaluation, within the last 3 years, and aligns with the Spoken Language Models / language modeling of speech concept.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a neural codec language model for text-to-speech synthesis, emphasizing language modeling with discrete speech codes rather than purely text-based approaches. However, it primarily addresses zero-shot TTS synthesis without discussing spoken language modeling as a core concept, full-duplex interaction, cognitive parallelism, synchronization, or dialogue behavior arbitration. The evaluation concentrates on speech naturalness and similarity rather than synchronous interaction or behavioral metrics. Therefore, it aligns with single-task TTS work rather than the full scope of spoken language modeling and interactive aspects required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a neural codec language model for text-to-speech synthesis, emphasizing language modeling with discrete speech codes rather than purely text-based approaches. However, it primarily addresses zero-shot TTS synthesis without discussing spoken language modeling as a core concept, full-duplex interaction, cognitive parallelism, synchronization, or dialogue behavior arbitration. The evaluation concentrates on speech naturalness and similarity rather than synchronous interaction or behavioral metrics. Therefore, it aligns with single-task TTS work rather than the full scope of spoken language modeling and interactive aspects required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed (one rated include citing language-modeling of speech; the other rated exclude citing single-task TTS focus). The paper presents a neural codec language model (Vall‑E) and explicitly treats TTS as conditional language modeling of discrete speech codes, so it does instantiate speech-level language modeling; however, it is solely a zero‑shot TTS system focused on synthesis quality and speaker similarity and does not address spoken-language modeling for interactive/full‑duplex behavior, synchronization, overlap/interruptions, or the multi-dimensional evaluation (real‑time interaction metrics) required by the inclusion narrative—thus it falls under the exclusion criterion of being a single-task TTS study. Final decision: better to exclude.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed (one rated include citing language-modeling of speech; the other rated exclude citing single-task TTS focus). The paper presents a neural codec language model (Vall‑E) and explicitly treats TTS as conditional language modeling of discrete speech codes, so it does instantiate speech-level language modeling; however, it is solely a zero‑shot TTS system focused on synthesis quality and speaker similarity and does not address spoken-language modeling for interactive/full‑duplex behavior, synchronization, overlap/interruptions, or the multi-dimensional evaluation (real‑time interaction metrics) required by the inclusion narrative—thus it falls under the exclusion criterion of being a single-task TTS study. Final decision: better to exclude.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Online Fusion of Multi-resolution Multispectral Images with Weakly Supervised Temporal Dynamics",
    "abstract": "Real-time satellite imaging has a central role in monitoring, detecting and estimating the intensity of key natural phenomena such as floods, earthquakes, etc. One important constraint of satellite imaging is the trade-off between spatial/spectral resolution and their revisiting time, a consequence of design and physical constraints imposed by satellite orbit among other technical limitations. In this paper, we focus on fusing multi-temporal, multi-spectral images where data acquired from different instruments with different spatial resolutions is used. We leverage the spatial relationship between images at multiple modalities to generate high-resolution image sequences at higher revisiting rates. To achieve this goal, we formulate the fusion method as a recursive state estimation problem and study its performance in filtering and smoothing contexts. Furthermore, a calibration strategy is proposed to estimate the time-varying temporal dynamics of the image sequence using only a small amount of historical image data. Differently from the training process in traditional machine learning algorithms, which usually require large datasets and computation times, the parameters of the temporal dynamical model are calibrated based on an analytical expression that uses only two of the images in the historical dataset. A distributed version of the Bayesian filtering and smoothing strategies is also proposed to reduce its computational complexity. To evaluate the proposed methodology we consider a water mapping task where real data acquired by the Landsat and MODIS instruments are fused generating high spatial-temporal resolution image estimates. Our experiments show that the proposed methodology outperforms the competing methods in both estimation accuracy and water mapping tasks.",
    "metadata": {
      "arxiv_id": "2301.02598",
      "title": "Online Fusion of Multi-resolution Multispectral Images with Weakly Supervised Temporal Dynamics",
      "summary": "Real-time satellite imaging has a central role in monitoring, detecting and estimating the intensity of key natural phenomena such as floods, earthquakes, etc. One important constraint of satellite imaging is the trade-off between spatial/spectral resolution and their revisiting time, a consequence of design and physical constraints imposed by satellite orbit among other technical limitations. In this paper, we focus on fusing multi-temporal, multi-spectral images where data acquired from different instruments with different spatial resolutions is used. We leverage the spatial relationship between images at multiple modalities to generate high-resolution image sequences at higher revisiting rates. To achieve this goal, we formulate the fusion method as a recursive state estimation problem and study its performance in filtering and smoothing contexts. Furthermore, a calibration strategy is proposed to estimate the time-varying temporal dynamics of the image sequence using only a small amount of historical image data. Differently from the training process in traditional machine learning algorithms, which usually require large datasets and computation times, the parameters of the temporal dynamical model are calibrated based on an analytical expression that uses only two of the images in the historical dataset. A distributed version of the Bayesian filtering and smoothing strategies is also proposed to reduce its computational complexity. To evaluate the proposed methodology we consider a water mapping task where real data acquired by the Landsat and MODIS instruments are fused generating high spatial-temporal resolution image estimates. Our experiments show that the proposed methodology outperforms the competing methods in both estimation accuracy and water mapping tasks.",
      "authors": [
        "Haoqing Li",
        "Bhavya Duvvuri",
        "Ricardo Borsoi",
        "Tales Imbiriba",
        "Edward Beighley",
        "Deniz Erdogmus",
        "Pau Closas"
      ],
      "published": "2023-01-06T16:48:33Z",
      "updated": "2023-01-06T16:48:33Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.02598v1",
      "landing_url": "https://arxiv.org/abs/2301.02598v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.02598"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract discuss remote sensing image fusion with temporal dynamics, not Spoken Language Models or any speech-language modeling; it fails all inclusion criteria and meets exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract discuss remote sensing image fusion with temporal dynamics, not Spoken Language Models or any speech-language modeling; it fails all inclusion criteria and meets exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multi-resolution multispectral image fusion for satellite data and its temporal dynamics, which is unrelated to spoken language models or speech language model research; it does not address speech or voice modeling, synchronization, or interaction aspects relevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multi-resolution multispectral image fusion for satellite data and its temporal dynamics, which is unrelated to spoken language models or speech language model research; it does not address speech or voice modeling, synchronization, or interaction aspects relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Conversational Turn-taking as a Stochastic Process on Networks",
    "abstract": "Understanding why certain individuals work well (or poorly) together as a team is a key research focus in the psychological and behavioral sciences and a fundamental problem for team-based organizations. Nevertheless, we have a limited ability to predict the social and work-related dynamics that will emerge from a given combination of team members. In this work, we model vocal turn-taking behavior within conversations as a parametric stochastic process on a network composed of the team members. More precisely, we model the dynamic of exchanging the `speaker token' among team members as a random walk in a graph that is driven by both individual level features and the conversation history. We fit our model to conversational turn-taking data extracted from audio recordings of multinational student teams during undergraduate engineering design internships. Through this real-world data we validate the explanatory power of our model and we unveil statistically significant differences in speaking behaviors between team members of different nationalities.",
    "metadata": {
      "arxiv_id": "2301.04030",
      "title": "Conversational Turn-taking as a Stochastic Process on Networks",
      "summary": "Understanding why certain individuals work well (or poorly) together as a team is a key research focus in the psychological and behavioral sciences and a fundamental problem for team-based organizations. Nevertheless, we have a limited ability to predict the social and work-related dynamics that will emerge from a given combination of team members. In this work, we model vocal turn-taking behavior within conversations as a parametric stochastic process on a network composed of the team members. More precisely, we model the dynamic of exchanging the `speaker token' among team members as a random walk in a graph that is driven by both individual level features and the conversation history. We fit our model to conversational turn-taking data extracted from audio recordings of multinational student teams during undergraduate engineering design internships. Through this real-world data we validate the explanatory power of our model and we unveil statistically significant differences in speaking behaviors between team members of different nationalities.",
      "authors": [
        "Lisa O'Bryan",
        "Santiago Segarra",
        "Jensine Paoletti",
        "Stephanie Zajac",
        "Margaret E. Beier",
        "Ashutosh Sabharwal",
        "Matthew Wettergreen",
        "Eduardo Salas"
      ],
      "published": "2023-01-10T15:31:16Z",
      "updated": "2023-01-10T15:31:16Z",
      "categories": [
        "cs.SI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.04030v1",
      "landing_url": "https://arxiv.org/abs/2301.04030v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.04030"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract presents modeling of conversational turn-taking as a stochastic process on a network based on audio data, but it does not center on Spoken Language Models or full-duplex speech modeling with explicit architectures/training/evaluation of speech as the core object (nor sub-200ms latency or synchronized interaction engineering).",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract presents modeling of conversational turn-taking as a stochastic process on a network based on audio data, but it does not center on Spoken Language Models or full-duplex speech modeling with explicit architectures/training/evaluation of speech as the core object (nor sub-200ms latency or synchronized interaction engineering).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study models conversational turn-taking as a stochastic process on networks and investigates team dynamics through vocal turn-taking behavior, but it does not explicitly focus on spoken language models or speech language models as core modeling or reasoning objects, nor does it discuss full-duplex spoken language models, engineered or learned synchronization, or end-to-end speech-LLM architectures; it is more about social and behavioral modeling rather than speech language modeling per se.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study models conversational turn-taking as a stochastic process on networks and investigates team dynamics through vocal turn-taking behavior, but it does not explicitly focus on spoken language models or speech language models as core modeling or reasoning objects, nor does it discuss full-duplex spoken language models, engineered or learned synchronization, or end-to-end speech-LLM architectures; it is more about social and behavioral modeling rather than speech language modeling per se.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Heterogeneous Tri-stream Clustering Network",
    "abstract": "Contrastive deep clustering has recently gained significant attention with its ability of joint contrastive learning and clustering via deep neural networks. Despite the rapid progress, previous works mostly require both positive and negative sample pairs for contrastive clustering, which rely on a relative large batch-size. Moreover, they typically adopt a two-stream architecture with two augmented views, which overlook the possibility and potential benefits of multi-stream architectures (especially with heterogeneous or hybrid networks). In light of this, this paper presents a new end-to-end deep clustering approach termed Heterogeneous Tri-stream Clustering Network (HTCN). The tri-stream architecture in HTCN consists of three main components, including two weight-sharing online networks and a target network, where the parameters of the target network are the exponential moving average of that of the online networks. Notably, the two online networks are trained by simultaneously (i) predicting the instance representations of the target network and (ii) enforcing the consistency between the cluster representations of the target network and that of the two online networks. Experimental results on four challenging image datasets demonstrate the superiority of HTCN over the state-of-the-art deep clustering approaches. The code is available at https://github.com/dengxiaozhi/HTCN.",
    "metadata": {
      "arxiv_id": "2301.04451",
      "title": "Heterogeneous Tri-stream Clustering Network",
      "summary": "Contrastive deep clustering has recently gained significant attention with its ability of joint contrastive learning and clustering via deep neural networks. Despite the rapid progress, previous works mostly require both positive and negative sample pairs for contrastive clustering, which rely on a relative large batch-size. Moreover, they typically adopt a two-stream architecture with two augmented views, which overlook the possibility and potential benefits of multi-stream architectures (especially with heterogeneous or hybrid networks). In light of this, this paper presents a new end-to-end deep clustering approach termed Heterogeneous Tri-stream Clustering Network (HTCN). The tri-stream architecture in HTCN consists of three main components, including two weight-sharing online networks and a target network, where the parameters of the target network are the exponential moving average of that of the online networks. Notably, the two online networks are trained by simultaneously (i) predicting the instance representations of the target network and (ii) enforcing the consistency between the cluster representations of the target network and that of the two online networks. Experimental results on four challenging image datasets demonstrate the superiority of HTCN over the state-of-the-art deep clustering approaches. The code is available at https://github.com/dengxiaozhi/HTCN.",
      "authors": [
        "Xiaozhi Deng",
        "Dong Huang",
        "Chang-Dong Wang"
      ],
      "published": "2023-01-11T13:15:54Z",
      "updated": "2023-01-11T13:15:54Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.04451v1",
      "landing_url": "https://arxiv.org/abs/2301.04451v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.04451"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The HTCN paper is about contrastive deep clustering for images and does not involve Spoken Language Models or speech-language modeling, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The HTCN paper is about contrastive deep clustering for images and does not involve Spoken Language Models or speech-language modeling, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a deep clustering network for image datasets, emphasizing contrastive learning and network architecture in the vision domain; there is no mention of spoken language models, speech language modeling, full-duplex interaction, or speech as a core modeling object. It does not address speech signal processing, spoken language modeling, linguistic turn-taking, or any related concepts in the inclusion criteria, thus not meeting the core requirements of the task.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a deep clustering network for image datasets, emphasizing contrastive learning and network architecture in the vision domain; there is no mention of spoken language models, speech language modeling, full-duplex interaction, or speech as a core modeling object. It does not address speech signal processing, spoken language modeling, linguistic turn-taking, or any related concepts in the inclusion criteria, thus not meeting the core requirements of the task.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Residual Diffusion Model for High Perceptual Quality Codec Augmentation",
    "abstract": "Diffusion probabilistic models have recently achieved remarkable success in generating high quality image and video data. In this work, we build on this class of generative models and introduce a method for lossy compression of high resolution images. The resulting codec, which we call DIffuson-based Residual Augmentation Codec (DIRAC), is the first neural codec to allow smooth traversal of the rate-distortion-perception tradeoff at test time, while obtaining competitive performance with GAN-based methods in perceptual quality. Furthermore, while sampling from diffusion probabilistic models is notoriously expensive, we show that in the compression setting the number of steps can be drastically reduced.",
    "metadata": {
      "arxiv_id": "2301.05489",
      "title": "A Residual Diffusion Model for High Perceptual Quality Codec Augmentation",
      "summary": "Diffusion probabilistic models have recently achieved remarkable success in generating high quality image and video data. In this work, we build on this class of generative models and introduce a method for lossy compression of high resolution images. The resulting codec, which we call DIffuson-based Residual Augmentation Codec (DIRAC), is the first neural codec to allow smooth traversal of the rate-distortion-perception tradeoff at test time, while obtaining competitive performance with GAN-based methods in perceptual quality. Furthermore, while sampling from diffusion probabilistic models is notoriously expensive, we show that in the compression setting the number of steps can be drastically reduced.",
      "authors": [
        "Noor Fathima Ghouse",
        "Jens Petersen",
        "Auke Wiggers",
        "Tianlin Xu",
        "Guillaume Sautière"
      ],
      "published": "2023-01-13T11:27:26Z",
      "updated": "2023-03-29T16:13:22Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.05489v3",
      "landing_url": "https://arxiv.org/abs/2301.05489v3",
      "doi": "https://doi.org/10.48550/arXiv.2301.05489"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper concerns diffusion-based neural image/video codecs (DIRAC) for lossy compression, with no focus on Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper concerns diffusion-based neural image/video codecs (DIRAC) for lossy compression, with no focus on Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a diffusion probabilistic model for image compression and codec augmentation, without any mention of spoken language models, speech as the primary modeling object, or relevant synchronous interaction features such as full-duplex communication, cognitive parallelism, or turn-taking; thus, the paper does not meet the inclusion criteria centered on speech language modeling nor does it address the exclusion criteria topics related to speech interaction and modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a diffusion probabilistic model for image compression and codec augmentation, without any mention of spoken language models, speech as the primary modeling object, or relevant synchronous interaction features such as full-duplex communication, cognitive parallelism, or turn-taking; thus, the paper does not meet the inclusion criteria centered on speech language modeling nor does it address the exclusion criteria topics related to speech interaction and modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Reconfigurable Data Glove for Reconstructing Physical and Virtual Grasps",
    "abstract": "In this work, we present a reconfigurable data glove design to capture different modes of human hand-object interactions, which are critical in training embodied artificial intelligence (AI) agents for fine manipulation tasks. To achieve various downstream tasks with distinct features, our reconfigurable data glove operates in three modes sharing a unified backbone design that reconstructs hand gestures in real time. In the tactile-sensing mode, the glove system aggregates manipulation force via customized force sensors made from a soft and thin piezoresistive material; this design minimizes interference during complex hand movements. The virtual reality (VR) mode enables real-time interaction in a physically plausible fashion: A caging-based approach is devised to determine stable grasps by detecting collision events. Leveraging a state-of-the-art finite element method (FEM), the simulation mode collects data on fine-grained 4D manipulation events comprising hand and object motions in 3D space and how the object's physical properties (e.g., stress and energy) change in accordance with manipulation over time. Notably, the glove system presented here is the first to use high-fidelity simulation to investigate the unobservable physical and causal factors behind manipulation actions. In a series of experiments, we characterize our data glove in terms of individual sensors and the overall system. More specifically, we evaluate the system's three modes by (i) recording hand gestures and associated forces, (ii) improving manipulation fluency in VR, and (iii) producing realistic simulation effects of various tool uses, respectively. Based on these three modes, our reconfigurable data glove collects and reconstructs fine-grained human grasp data in both physical and virtual environments, thereby opening up new avenues for the learning of manipulation skills for embodied AI agents.",
    "metadata": {
      "arxiv_id": "2301.05821",
      "title": "A Reconfigurable Data Glove for Reconstructing Physical and Virtual Grasps",
      "summary": "In this work, we present a reconfigurable data glove design to capture different modes of human hand-object interactions, which are critical in training embodied artificial intelligence (AI) agents for fine manipulation tasks. To achieve various downstream tasks with distinct features, our reconfigurable data glove operates in three modes sharing a unified backbone design that reconstructs hand gestures in real time. In the tactile-sensing mode, the glove system aggregates manipulation force via customized force sensors made from a soft and thin piezoresistive material; this design minimizes interference during complex hand movements. The virtual reality (VR) mode enables real-time interaction in a physically plausible fashion: A caging-based approach is devised to determine stable grasps by detecting collision events. Leveraging a state-of-the-art finite element method (FEM), the simulation mode collects data on fine-grained 4D manipulation events comprising hand and object motions in 3D space and how the object's physical properties (e.g., stress and energy) change in accordance with manipulation over time. Notably, the glove system presented here is the first to use high-fidelity simulation to investigate the unobservable physical and causal factors behind manipulation actions. In a series of experiments, we characterize our data glove in terms of individual sensors and the overall system. More specifically, we evaluate the system's three modes by (i) recording hand gestures and associated forces, (ii) improving manipulation fluency in VR, and (iii) producing realistic simulation effects of various tool uses, respectively. Based on these three modes, our reconfigurable data glove collects and reconstructs fine-grained human grasp data in both physical and virtual environments, thereby opening up new avenues for the learning of manipulation skills for embodied AI agents.",
      "authors": [
        "Hangxin Liu",
        "Zeyu Zhang",
        "Ziyuan Jiao",
        "Zhenliang Zhang",
        "Minchen Li",
        "Chenfanfu Jiang",
        "Yixin Zhu",
        "Song-Chun Zhu"
      ],
      "published": "2023-01-14T05:35:50Z",
      "updated": "2023-02-02T02:09:19Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.05821v4",
      "landing_url": "https://arxiv.org/abs/2301.05821v4",
      "doi": "https://doi.org/10.48550/arXiv.2301.05821"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: It does not address Spoken Language Models or any speech-language modeling concepts; the focus is on a reconfigurable data glove for capturing hand-object interactions and related VR/simulation, which is unrelated to the SLM-centered inclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: It does not address Spoken Language Models or any speech-language modeling concepts; the focus is on a reconfigurable data glove for capturing hand-object interactions and related VR/simulation, which is unrelated to the SLM-centered inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a reconfigurable data glove for capturing human hand-object interactions and related embodied AI tasks, without addressing spoken language models, speech language modeling, or full-duplex speech interactions, which are the core inclusion criteria; hence it falls outside the defined research domain.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a reconfigurable data glove for capturing human hand-object interactions and related embodied AI tasks, without addressing spoken language models, speech language modeling, or full-duplex speech interactions, which are the core inclusion criteria; hence it falls outside the defined research domain.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Real-time, low latency virtual DBA hypervisor for SLA-compliant multi-service operations over shared Passive Optical Networks",
    "abstract": "We present a heuristic algorithm for a PON upstream scheduling hypervisor,supporting low latency services with strict service-level agreement. The algorithm achieves near-optimal performance while running in only 3.5 us, thus operating in real-time.",
    "metadata": {
      "arxiv_id": "2301.06754",
      "title": "Real-time, low latency virtual DBA hypervisor for SLA-compliant multi-service operations over shared Passive Optical Networks",
      "summary": "We present a heuristic algorithm for a PON upstream scheduling hypervisor,supporting low latency services with strict service-level agreement. The algorithm achieves near-optimal performance while running in only 3.5 us, thus operating in real-time.",
      "authors": [
        "Arijeet Ganguli",
        "Frank Slyne",
        "Marco Ruffini"
      ],
      "published": "2023-01-17T08:39:18Z",
      "updated": "2023-01-17T08:39:18Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.06754v1",
      "landing_url": "https://arxiv.org/abs/2301.06754v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.06754"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article targets PON upstream scheduling hypervisors and real-time network optimization; there is no focus on Spoken Language Models or speech-language modeling, nor any speech interaction benchmarks, so it fails the inclusion criteria and aligns with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article targets PON upstream scheduling hypervisors and real-time network optimization; there is no focus on Spoken Language Models or speech-language modeling, nor any speech interaction benchmarks, so it fails the inclusion criteria and aligns with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study describes a heuristic algorithm for real-time scheduling in Passive Optical Networks focusing on low latency for SLA compliance, but it does not mention Spoken Language Models, speech language modeling, or related architectures, training, or evaluation of speech-based models; thus, it does not meet the inclusion criteria related to spoken language modeling and full-duplex speech interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study describes a heuristic algorithm for real-time scheduling in Passive Optical Networks focusing on low latency for SLA compliance, but it does not mention Spoken Language Models, speech language modeling, or related architectures, training, or evaluation of speech-based models; thus, it does not meet the inclusion criteria related to spoken language modeling and full-duplex speech interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Temporal Dynamics of Coordinated Online Behavior: Stability, Archetypes, and Influence",
    "abstract": "Large-scale online campaigns, malicious or otherwise, require a significant degree of coordination among participants, which sparked interest in the study of coordinated online behavior. State-of-the-art methods for detecting coordinated behavior perform static analyses, disregarding the temporal dynamics of coordination. Here, we carry out the first dynamic analysis of coordinated behavior. To reach our goal we build a multiplex temporal network and we perform dynamic community detection to identify groups of users that exhibited coordinated behaviors in time. Thanks to our novel approach we find that: (i) coordinated communities feature variable degrees of temporal instability; (ii) dynamic analyses are needed to account for such instability, and results of static analyses can be unreliable and scarcely representative of unstable communities; (iii) some users exhibit distinct archetypal behaviors that have important practical implications; (iv) content and network characteristics contribute to explaining why users leave and join coordinated communities. Our results demonstrate the advantages of dynamic analyses and open up new directions of research on the unfolding of online debates, on the strategies of coordinated communities, and on the patterns of online influence.",
    "metadata": {
      "arxiv_id": "2301.06774",
      "title": "Temporal Dynamics of Coordinated Online Behavior: Stability, Archetypes, and Influence",
      "summary": "Large-scale online campaigns, malicious or otherwise, require a significant degree of coordination among participants, which sparked interest in the study of coordinated online behavior. State-of-the-art methods for detecting coordinated behavior perform static analyses, disregarding the temporal dynamics of coordination. Here, we carry out the first dynamic analysis of coordinated behavior. To reach our goal we build a multiplex temporal network and we perform dynamic community detection to identify groups of users that exhibited coordinated behaviors in time. Thanks to our novel approach we find that: (i) coordinated communities feature variable degrees of temporal instability; (ii) dynamic analyses are needed to account for such instability, and results of static analyses can be unreliable and scarcely representative of unstable communities; (iii) some users exhibit distinct archetypal behaviors that have important practical implications; (iv) content and network characteristics contribute to explaining why users leave and join coordinated communities. Our results demonstrate the advantages of dynamic analyses and open up new directions of research on the unfolding of online debates, on the strategies of coordinated communities, and on the patterns of online influence.",
      "authors": [
        "Serena Tardelli",
        "Leonardo Nizzoli",
        "Maurizio Tesconi",
        "Mauro Conti",
        "Preslav Nakov",
        "Giovanni Da San Martino",
        "Stefano Cresci"
      ],
      "published": "2023-01-17T09:52:54Z",
      "updated": "2024-05-09T10:15:28Z",
      "categories": [
        "cs.SI",
        "cs.CL",
        "cs.CY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.06774v2",
      "landing_url": "https://arxiv.org/abs/2301.06774v2",
      "doi": "https://doi.org/10.1073/pnas.2307038121"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item analyzes dynamic coordination in online social behavior, not Spoken Language Models or speech-language modeling, failing the inclusion criteria and aligning with multiple exclusion criteria (no SLM/FD-SLM focus).",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item analyzes dynamic coordination in online social behavior, not Spoken Language Models or speech-language modeling, failing the inclusion criteria and aligning with multiple exclusion criteria (no SLM/FD-SLM focus).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on coordinated online behaviors using temporal network analysis and dynamic community detection, and does not address spoken language models, speech language modeling, full-duplex spoken language models, or related core topics such as speech-based modeling, synchronization, or low-latency interactive capabilities as specified in the inclusion criteria. It also lacks discussion of model architectures, training, or evaluation related to speech language modeling, thus failing to meet the main inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on coordinated online behaviors using temporal network analysis and dynamic community detection, and does not address spoken language models, speech language modeling, full-duplex spoken language models, or related core topics such as speech-based modeling, synchronization, or low-latency interactive capabilities as specified in the inclusion criteria. It also lacks discussion of model architectures, training, or evaluation related to speech language modeling, thus failing to meet the main inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Using Large Text-to-Image Models with Structured Prompts for Skin Disease Identification: A Case Study",
    "abstract": "This paper investigates the potential usage of large text-to-image (LTI) models for the automated diagnosis of a few skin conditions with rarity or a serious lack of annotated datasets. As the input to the LTI model, we provide the targeted instantiation of a generic but succinct prompt structure designed upon careful observations of the conditional narratives from the standard medical textbooks. In this regard, we pave the path to utilizing accessible textbook descriptions for automated diagnosis of conditions with data scarcity through the lens of LTI models. Experiments show the efficacy of the proposed framework, including much better localization of the infected regions. Moreover, it has the immense possibility for generalization across the medical sub-domains, not only to mitigate the data scarcity issue but also to debias automated diagnostics from the all-pervasive racial biases.",
    "metadata": {
      "arxiv_id": "2301.07178",
      "title": "Using Large Text-to-Image Models with Structured Prompts for Skin Disease Identification: A Case Study",
      "summary": "This paper investigates the potential usage of large text-to-image (LTI) models for the automated diagnosis of a few skin conditions with rarity or a serious lack of annotated datasets. As the input to the LTI model, we provide the targeted instantiation of a generic but succinct prompt structure designed upon careful observations of the conditional narratives from the standard medical textbooks. In this regard, we pave the path to utilizing accessible textbook descriptions for automated diagnosis of conditions with data scarcity through the lens of LTI models. Experiments show the efficacy of the proposed framework, including much better localization of the infected regions. Moreover, it has the immense possibility for generalization across the medical sub-domains, not only to mitigate the data scarcity issue but also to debias automated diagnostics from the all-pervasive racial biases.",
      "authors": [
        "Sajith Rajapaksa",
        "Jean Marie Uwabeza Vianney",
        "Renell Castro",
        "Farzad Khalvati",
        "Shubhra Aich"
      ],
      "published": "2023-01-17T20:37:02Z",
      "updated": "2023-01-17T20:37:02Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.07178v1",
      "landing_url": "https://arxiv.org/abs/2301.07178v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.07178"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on image-based medical diagnosis using large text-to-image models, not on Spoken Language Models or any speech-language modeling concepts; it does not meet the inclusion criteria and aligns with exclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on image-based medical diagnosis using large text-to-image models, not on Spoken Language Models or any speech-language modeling concepts; it does not meet the inclusion criteria and aligns with exclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on using large text-to-image models for skin disease identification, which does not involve spoken language models or speech as the core modeling and reasoning object. It does not discuss speech language modeling, full-duplex spoken language models, or related topics such as cognitive parallelism, low latency interactions, speech overlap, or behavioral arbitration. Therefore, it does not meet the inclusion criteria centered on spoken language or speech modeling, nor does it directly address the exclusion criteria about speech interaction systems. Hence, it is not relevant for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on using large text-to-image models for skin disease identification, which does not involve spoken language models or speech as the core modeling and reasoning object. It does not discuss speech language modeling, full-duplex spoken language models, or related topics such as cognitive parallelism, low latency interactions, speech overlap, or behavioral arbitration. Therefore, it does not meet the inclusion criteria centered on spoken language or speech modeling, nor does it directly address the exclusion criteria about speech interaction systems. Hence, it is not relevant for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Demonstration of a low latency bandwidth allocation mechanism for mission critical applications in virtual PONs with P4 programmable hardware",
    "abstract": "We provide a real-time demonstration of a low-latency PON DBA mechanism, optimised for virtual PONs. Our implementation mixes P4 programmable data plane and software-based virtual DBA to provide efficient fast-track allocation for low latency applications",
    "metadata": {
      "arxiv_id": "2301.07371",
      "title": "Demonstration of a low latency bandwidth allocation mechanism for mission critical applications in virtual PONs with P4 programmable hardware",
      "summary": "We provide a real-time demonstration of a low-latency PON DBA mechanism, optimised for virtual PONs. Our implementation mixes P4 programmable data plane and software-based virtual DBA to provide efficient fast-track allocation for low latency applications",
      "authors": [
        "D. R. Mafioletti",
        "F. Slyne",
        "R. Giller",
        "M. OHanlon",
        "D. Coyle",
        "B. Ryan",
        "M. Ruffini"
      ],
      "published": "2023-01-18T08:38:53Z",
      "updated": "2023-01-18T08:38:53Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.07371v1",
      "landing_url": "https://arxiv.org/abs/2301.07371v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.07371"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about low-latency PON bandwidth allocation using P4 hardware, unrelated to Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about low-latency PON bandwidth allocation using P4 hardware, unrelated to Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a low-latency bandwidth allocation mechanism for virtual PONs using P4 programmable hardware, which is related to network data plane and software implementations. It does not mention spoken language models, speech language models, or any speech-based modeling or inference. There is no indication of full-duplex spoken language modeling, speech sequence modeling, or joint speech and language model architectures discussed. Hence, it does not meet the inclusion criteria focused on spoken language models, nor does it address any of the exclusion-exempt aspects like synchronous speech interaction or voice-based modeling innovation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a low-latency bandwidth allocation mechanism for virtual PONs using P4 programmable hardware, which is related to network data plane and software implementations. It does not mention spoken language models, speech language models, or any speech-based modeling or inference. There is no indication of full-duplex spoken language modeling, speech sequence modeling, or joint speech and language model architectures discussed. Hence, it does not meet the inclusion criteria focused on spoken language models, nor does it address any of the exclusion-exempt aspects like synchronous speech interaction or voice-based modeling innovation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Novel low-latency DBA for Virtualised PON implemented through P4 In-Network Processing",
    "abstract": "We present a novel dual-DBA allocation, with a fast P4-enabled scheduler to provide low latency upstream grant allocations. We show latency reduction of 37% and 43%, respectively, compared to standard and virtual PONs",
    "metadata": {
      "arxiv_id": "2301.07372",
      "title": "A Novel low-latency DBA for Virtualised PON implemented through P4 In-Network Processing",
      "summary": "We present a novel dual-DBA allocation, with a fast P4-enabled scheduler to provide low latency upstream grant allocations. We show latency reduction of 37% and 43%, respectively, compared to standard and virtual PONs",
      "authors": [
        "D. R. Mafioletti",
        "F. Slyne",
        "R. Giller",
        "M. OHanlon",
        "B. Ryan",
        "M. Ruffini"
      ],
      "published": "2023-01-18T08:41:18Z",
      "updated": "2023-01-18T08:41:18Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.07372v1",
      "landing_url": "https://arxiv.org/abs/2301.07372v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.07372"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此研究聚焦於虛擬化光纖網路與P4網路處理的低延遲排程，與 Spoken Language Models（SLMs）或語音語言建模無關，屬於無關領域，故不符合納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此研究聚焦於虛擬化光纖網路與P4網路處理的低延遲排程，與 Spoken Language Models（SLMs）或語音語言建模無關，屬於無關領域，故不符合納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a low-latency Dynamic Bandwidth Allocation (DBA) in virtualized Passive Optical Networks (PON) implemented via P4 in-network processing, which is a networking technology. It does not discuss spoken language models, speech language models, or related architectures for voice modeling or full-duplex spoken interaction. The core subject is unrelated to speech modeling or spoken language understanding, which is required by the inclusion criteria. Therefore, it does not meet the inclusion criteria and falls under exclusion for lacking relevance to spoken language modeling or speech interaction synchronization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a low-latency Dynamic Bandwidth Allocation (DBA) in virtualized Passive Optical Networks (PON) implemented via P4 in-network processing, which is a networking technology. It does not discuss spoken language models, speech language models, or related architectures for voice modeling or full-duplex spoken interaction. The core subject is unrelated to speech modeling or spoken language understanding, which is required by the inclusion criteria. Therefore, it does not meet the inclusion criteria and falls under exclusion for lacking relevance to spoken language modeling or speech interaction synchronization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Self-Supervised Learning for Data Scarcity in a Fatigue Damage Prognostic Problem",
    "abstract": "With the increasing availability of data for Prognostics and Health Management (PHM), Deep Learning (DL) techniques are now the subject of considerable attention for this application, often achieving more accurate Remaining Useful Life (RUL) predictions. However, one of the major challenges for DL techniques resides in the difficulty of obtaining large amounts of labelled data on industrial systems. To overcome this lack of labelled data, an emerging learning technique is considered in our work: Self-Supervised Learning, a sub-category of unsupervised learning approaches. This paper aims to investigate whether pre-training DL models in a self-supervised way on unlabelled sensors data can be useful for RUL estimation with only Few-Shots Learning, i.e. with scarce labelled data. In this research, a fatigue damage prognostics problem is addressed, through the estimation of the RUL of aluminum alloy panels (typical of aerospace structures) subject to fatigue cracks from strain gauge data. Synthetic datasets composed of strain data are used allowing to extensively investigate the influence of the dataset size on the predictive performance. Results show that the self-supervised pre-trained models are able to significantly outperform the non-pre-trained models in downstream RUL prediction task, and with less computational expense, showing promising results in prognostic tasks when only limited labelled data is available.",
    "metadata": {
      "arxiv_id": "2301.08441",
      "title": "Self-Supervised Learning for Data Scarcity in a Fatigue Damage Prognostic Problem",
      "summary": "With the increasing availability of data for Prognostics and Health Management (PHM), Deep Learning (DL) techniques are now the subject of considerable attention for this application, often achieving more accurate Remaining Useful Life (RUL) predictions. However, one of the major challenges for DL techniques resides in the difficulty of obtaining large amounts of labelled data on industrial systems. To overcome this lack of labelled data, an emerging learning technique is considered in our work: Self-Supervised Learning, a sub-category of unsupervised learning approaches. This paper aims to investigate whether pre-training DL models in a self-supervised way on unlabelled sensors data can be useful for RUL estimation with only Few-Shots Learning, i.e. with scarce labelled data. In this research, a fatigue damage prognostics problem is addressed, through the estimation of the RUL of aluminum alloy panels (typical of aerospace structures) subject to fatigue cracks from strain gauge data. Synthetic datasets composed of strain data are used allowing to extensively investigate the influence of the dataset size on the predictive performance. Results show that the self-supervised pre-trained models are able to significantly outperform the non-pre-trained models in downstream RUL prediction task, and with less computational expense, showing promising results in prognostic tasks when only limited labelled data is available.",
      "authors": [
        "Anass Akrim",
        "Christian Gogu",
        "Rob Vingerhoeds",
        "Michel Salaün"
      ],
      "published": "2023-01-20T06:45:32Z",
      "updated": "2023-01-20T06:45:32Z",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.08441v1",
      "landing_url": "https://arxiv.org/abs/2301.08441v1",
      "doi": "https://doi.org/10.1016/j.engappai.2023.105837"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on self-supervised learning for prognostics in fatigue damage (RUL estimation from strain data) and does not address Spoken Language Models or any speech-language modeling/interaction concepts, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on self-supervised learning for prognostics in fatigue damage (RUL estimation from strain data) and does not address Spoken Language Models or any speech-language modeling/interaction concepts, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning for fatigue damage prognostics using strain gauge data, which is unrelated to spoken language models or speech language modeling; it does not discuss speech-based architectures, modeling of speech sequences, or interactive spoken language systems as required by the inclusion criteria. It also lacks discussions on synchronization, cognitive parallelism, or real-time interactive speech capabilities. Therefore, it does not meet the inclusion criteria and falls outside the scope of spoken language modeling research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning for fatigue damage prognostics using strain gauge data, which is unrelated to spoken language models or speech language modeling; it does not discuss speech-based architectures, modeling of speech sequences, or interactive spoken language systems as required by the inclusion criteria. It also lacks discussions on synchronization, cognitive parallelism, or real-time interactive speech capabilities. Therefore, it does not meet the inclusion criteria and falls outside the scope of spoken language modeling research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning",
    "abstract": "With the rapid development of wearable cameras, a massive collection of egocentric video for first-person visual perception becomes available. Using egocentric videos to predict first-person activity faces many challenges, including limited field of view, occlusions, and unstable motions. Observing that sensor data from wearable devices facilitates human activity recognition, multi-modal activity recognition is attracting increasing attention. However, the deficiency of related dataset hinders the development of multi-modal deep learning for egocentric activity recognition. Nowadays, deep learning in real world has led to a focus on continual learning that often suffers from catastrophic forgetting. But the catastrophic forgetting problem for egocentric activity recognition, especially in the context of multiple modalities, remains unexplored due to unavailability of dataset. In order to assist this research, we present a multi-modal egocentric activity dataset for continual learning named UESTC-MMEA-CL, which is collected by self-developed glasses integrating a first-person camera and wearable sensors. It contains synchronized data of videos, accelerometers, and gyroscopes, for 32 types of daily activities, performed by 10 participants. Its class types and scale are compared with other publicly available datasets. The statistical analysis of the sensor data is given to show the auxiliary effects for different behaviors. And results of egocentric activity recognition are reported when using separately, and jointly, three modalities: RGB, acceleration, and gyroscope, on a base network architecture. To explore the catastrophic forgetting in continual learning tasks, four baseline methods are extensively evaluated with different multi-modal combinations. We hope the UESTC-MMEA-CL can promote future studies on continual learning for first-person activity recognition in wearable applications.",
    "metadata": {
      "arxiv_id": "2301.10931",
      "title": "Towards Continual Egocentric Activity Recognition: A Multi-modal Egocentric Activity Dataset for Continual Learning",
      "summary": "With the rapid development of wearable cameras, a massive collection of egocentric video for first-person visual perception becomes available. Using egocentric videos to predict first-person activity faces many challenges, including limited field of view, occlusions, and unstable motions. Observing that sensor data from wearable devices facilitates human activity recognition, multi-modal activity recognition is attracting increasing attention. However, the deficiency of related dataset hinders the development of multi-modal deep learning for egocentric activity recognition. Nowadays, deep learning in real world has led to a focus on continual learning that often suffers from catastrophic forgetting. But the catastrophic forgetting problem for egocentric activity recognition, especially in the context of multiple modalities, remains unexplored due to unavailability of dataset. In order to assist this research, we present a multi-modal egocentric activity dataset for continual learning named UESTC-MMEA-CL, which is collected by self-developed glasses integrating a first-person camera and wearable sensors. It contains synchronized data of videos, accelerometers, and gyroscopes, for 32 types of daily activities, performed by 10 participants. Its class types and scale are compared with other publicly available datasets. The statistical analysis of the sensor data is given to show the auxiliary effects for different behaviors. And results of egocentric activity recognition are reported when using separately, and jointly, three modalities: RGB, acceleration, and gyroscope, on a base network architecture. To explore the catastrophic forgetting in continual learning tasks, four baseline methods are extensively evaluated with different multi-modal combinations. We hope the UESTC-MMEA-CL can promote future studies on continual learning for first-person activity recognition in wearable applications.",
      "authors": [
        "Linfeng Xu",
        "Qingbo Wu",
        "Lili Pan",
        "Fanman Meng",
        "Hongliang Li",
        "Chiyuan He",
        "Hanxin Wang",
        "Shaoxu Cheng",
        "Yu Dai"
      ],
      "published": "2023-01-26T04:32:00Z",
      "updated": "2023-01-26T04:32:00Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.10931v1",
      "landing_url": "https://arxiv.org/abs/2301.10931v1",
      "doi": "https://doi.org/10.1109/TMM.2023.3295899"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses egocentric activity recognition with multi-modal video and sensor data, not Spoken Language Models or speech-centric modeling, thus it does not meet inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses egocentric activity recognition with multi-modal video and sensor data, not Spoken Language Models or speech-centric modeling, thus it does not meet inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses entirely on multi-modal egocentric activity recognition using wearable sensors and cameras, with no mention or relevance to Spoken Language Models, speech language modeling, or full-duplex spoken interaction systems. It addresses continual learning issues in visual and sensor data for activity recognition rather than any speech modeling or synchronous spoken language interaction, which are the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses entirely on multi-modal egocentric activity recognition using wearable sensors and cameras, with no mention or relevance to Spoken Language Models, speech language modeling, or full-duplex spoken interaction systems. It addresses continual learning issues in visual and sensor data for activity recognition rather than any speech modeling or synchronous spoken language interaction, which are the core inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Semantic Coherence Markers for the Early Diagnosis of the Alzheimer Disease",
    "abstract": "In this work we explore how language models can be employed to analyze language and discriminate between mentally impaired and healthy subjects through the perplexity metric. Perplexity was originally conceived as an information-theoretic measure to assess how much a given language model is suited to predict a text sequence or, equivalently, how much a word sequence fits into a specific language model. We carried out an extensive experimentation with the publicly available data, and employed language models as diverse as N-grams, from 2-grams to 5-grams, and GPT-2, a transformer-based language model. We investigated whether perplexity scores may be used to discriminate between the transcripts of healthy subjects and subjects suffering from Alzheimer Disease (AD). Our best performing models achieved full accuracy and F-score (1.00 in both precision/specificity and recall/sensitivity) in categorizing subjects from both the AD class and control subjects. These results suggest that perplexity can be a valuable analytical metrics with potential application to supporting early diagnosis of symptoms of mental disorders.",
    "metadata": {
      "arxiv_id": "2302.01025",
      "title": "Semantic Coherence Markers for the Early Diagnosis of the Alzheimer Disease",
      "summary": "In this work we explore how language models can be employed to analyze language and discriminate between mentally impaired and healthy subjects through the perplexity metric. Perplexity was originally conceived as an information-theoretic measure to assess how much a given language model is suited to predict a text sequence or, equivalently, how much a word sequence fits into a specific language model. We carried out an extensive experimentation with the publicly available data, and employed language models as diverse as N-grams, from 2-grams to 5-grams, and GPT-2, a transformer-based language model. We investigated whether perplexity scores may be used to discriminate between the transcripts of healthy subjects and subjects suffering from Alzheimer Disease (AD). Our best performing models achieved full accuracy and F-score (1.00 in both precision/specificity and recall/sensitivity) in categorizing subjects from both the AD class and control subjects. These results suggest that perplexity can be a valuable analytical metrics with potential application to supporting early diagnosis of symptoms of mental disorders.",
      "authors": [
        "Davide Colla",
        "Matteo Delsanto",
        "Marco Agosto",
        "Benedetto Vitiello",
        "Daniele Paolo Radicioni"
      ],
      "published": "2023-02-02T11:40:16Z",
      "updated": "2023-02-02T11:40:16Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.01025v1",
      "landing_url": "https://arxiv.org/abs/2302.01025v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.01025"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract uses NLP language models to classify Alzheimer's vs controls via perplexity on transcripts; it treats language modeling as a text task, not a speech-centric Spoken Language Model core, architecture/training/evaluation, nor interactive speech metrics. It does not meet the core inclusion criteria focused on Spoken Language Models or speech-based modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract uses NLP language models to classify Alzheimer's vs controls via perplexity on transcripts; it treats language modeling as a text task, not a speech-centric Spoken Language Model core, architecture/training/evaluation, nor interactive speech metrics. It does not meet the core inclusion criteria focused on Spoken Language Models or speech-based modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using language models to analyze text transcripts for early diagnosis of Alzheimer's disease, relying on perplexity metrics from models like N-grams and GPT-2, which are text-based models. There is no evidence that the research involves spoken language models or speech as the primary modeling target, nor does it discuss full-duplex spoken interaction, synchronization, or acoustic level modeling as detailed in the inclusion criteria. It rather treats speech data as text transcripts for NLP analysis. Therefore, it does not meet the inclusion criteria concerning Spoken Language Models and speech-level modeling, nor does it fulfill any requirements related to full-duplex or synchronous speech interaction modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using language models to analyze text transcripts for early diagnosis of Alzheimer's disease, relying on perplexity metrics from models like N-grams and GPT-2, which are text-based models. There is no evidence that the research involves spoken language models or speech as the primary modeling target, nor does it discuss full-duplex spoken interaction, synchronization, or acoustic level modeling as detailed in the inclusion criteria. It rather treats speech data as text transcripts for NLP analysis. Therefore, it does not meet the inclusion criteria concerning Spoken Language Models and speech-level modeling, nor does it fulfill any requirements related to full-duplex or synchronous speech interaction modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Mitigating Data Scarcity for Large Language Models",
    "abstract": "In recent years, pretrained neural language models (PNLMs) have taken the field of natural language processing by storm, achieving new benchmarks and state-of-the-art performances. These models often rely heavily on annotated data, which may not always be available. Data scarcity are commonly found in specialized domains, such as medical, or in low-resource languages that are underexplored by AI research. In this dissertation, we focus on mitigating data scarcity using data augmentation and neural ensemble learning techniques for neural language models. In both research directions, we implement neural network algorithms and evaluate their impact on assisting neural language models in downstream NLP tasks. Specifically, for data augmentation, we explore two techniques: 1) creating positive training data by moving an answer span around its original context and 2) using text simplification techniques to introduce a variety of writing styles to the original training data. Our results indicate that these simple and effective solutions improve the performance of neural language models considerably in low-resource NLP domains and tasks. For neural ensemble learning, we use a multilabel neural classifier to select the best prediction outcome from a variety of individual pretrained neural language models trained for a low-resource medical text simplification task.",
    "metadata": {
      "arxiv_id": "2302.01806",
      "title": "Mitigating Data Scarcity for Large Language Models",
      "summary": "In recent years, pretrained neural language models (PNLMs) have taken the field of natural language processing by storm, achieving new benchmarks and state-of-the-art performances. These models often rely heavily on annotated data, which may not always be available. Data scarcity are commonly found in specialized domains, such as medical, or in low-resource languages that are underexplored by AI research. In this dissertation, we focus on mitigating data scarcity using data augmentation and neural ensemble learning techniques for neural language models. In both research directions, we implement neural network algorithms and evaluate their impact on assisting neural language models in downstream NLP tasks. Specifically, for data augmentation, we explore two techniques: 1) creating positive training data by moving an answer span around its original context and 2) using text simplification techniques to introduce a variety of writing styles to the original training data. Our results indicate that these simple and effective solutions improve the performance of neural language models considerably in low-resource NLP domains and tasks. For neural ensemble learning, we use a multilabel neural classifier to select the best prediction outcome from a variety of individual pretrained neural language models trained for a low-resource medical text simplification task.",
      "authors": [
        "Hoang Van"
      ],
      "published": "2023-02-03T15:17:53Z",
      "updated": "2023-02-03T15:17:53Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.01806v1",
      "landing_url": "https://arxiv.org/abs/2302.01806v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.01806"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on data augmentation and ensemble methods for NLP/Large Language Models with no emphasis on spoken language, speech modeling, or real-time dialogue capabilities, thus failing to meet the required Spoken Language Models core focus and associated evaluation frameworks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on data augmentation and ensemble methods for NLP/Large Language Models with no emphasis on spoken language, speech modeling, or real-time dialogue capabilities, thus failing to meet the required Spoken Language Models core focus and associated evaluation frameworks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on mitigating data scarcity for pretrained neural language models using data augmentation and ensemble learning, but they do not mention spoken language models, speech as a core modeling object, full-duplex spoken interaction, or any related synchronization or cognitive parallelism concepts required by the inclusion criteria. The research centers purely on textual neural language models in low-resource domains, without addressing speech-level modeling or end-to-end speech-language architectures as specified. Therefore, it does not meet the inclusion criteria and aligns with exclusion criteria focusing on text-only NLP without speech modeling innovation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on mitigating data scarcity for pretrained neural language models using data augmentation and ensemble learning, but they do not mention spoken language models, speech as a core modeling object, full-duplex spoken interaction, or any related synchronization or cognitive parallelism concepts required by the inclusion criteria. The research centers purely on textual neural language models in low-resource domains, without addressing speech-level modeling or end-to-end speech-language architectures as specified. Therefore, it does not meet the inclusion criteria and aligns with exclusion criteria focusing on text-only NLP without speech modeling innovation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Framework for UAV-based Distributed Sensing Under Half-Duplex Operation",
    "abstract": "This paper proposes an unmanned aerial vehicle (UAV)-based distributed sensing framework that uses frequency-division multiplexing (OFDM) waveforms to detect the position of a ground target under half-duplex operation. The area of interest, where the target is located, is sectioned into a grid of cells, where the radar cross-section (RCS) of every cell is jointly estimated by the UAVs, and a central node acts as a fusion center by receiving all the estimations and performing information-level fusion. For local estimation at each UAV, the periodogram approach is utilised, and a digital receive beamformer is assumed. The fused RCS estimates of the grid are used to estimate the cell containing the target. Monte Carlo simulations are performed to obtain the detection probability of the proposed framework, and our results show that the proposed framework attains improved accuracy for the detection of a target than other OFDM bi-static radar approaches proposed in the literature.",
    "metadata": {
      "arxiv_id": "2302.10673",
      "title": "A Framework for UAV-based Distributed Sensing Under Half-Duplex Operation",
      "summary": "This paper proposes an unmanned aerial vehicle (UAV)-based distributed sensing framework that uses frequency-division multiplexing (OFDM) waveforms to detect the position of a ground target under half-duplex operation. The area of interest, where the target is located, is sectioned into a grid of cells, where the radar cross-section (RCS) of every cell is jointly estimated by the UAVs, and a central node acts as a fusion center by receiving all the estimations and performing information-level fusion. For local estimation at each UAV, the periodogram approach is utilised, and a digital receive beamformer is assumed. The fused RCS estimates of the grid are used to estimate the cell containing the target. Monte Carlo simulations are performed to obtain the detection probability of the proposed framework, and our results show that the proposed framework attains improved accuracy for the detection of a target than other OFDM bi-static radar approaches proposed in the literature.",
      "authors": [
        "Xavier A. Flores Cabezas",
        "Diana P. Moya Osorio",
        "Markku Juntti"
      ],
      "published": "2023-02-21T13:45:27Z",
      "updated": "2023-02-21T13:45:27Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.10673v1",
      "landing_url": "https://arxiv.org/abs/2302.10673v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.10673"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "這篇聚焦於無人機雷達感測與半雙工操作的分佈式感測框架，與語音語言模型（SLMs/FD-SLMs）或語音交互評估無關，因此不符合主題與評估框架，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "這篇聚焦於無人機雷達感測與半雙工操作的分佈式感測框架，與語音語言模型（SLMs/FD-SLMs）或語音交互評估無關，因此不符合主題與評估框架，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on UAV-based distributed sensing using OFDM waveforms for target detection, which is unrelated to spoken language models (SLMs) or full-duplex spoken language interaction; it lacks any mention or focus on speech language modeling, synchronization in turn-taking, or any of the outlined inclusion criteria related to speech processing or cognitive parallelism; thus it does not meet the core inclusion criteria and falls outside the defined research scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on UAV-based distributed sensing using OFDM waveforms for target detection, which is unrelated to spoken language models (SLMs) or full-duplex spoken language interaction; it lacks any mention or focus on speech language modeling, synchronization in turn-taking, or any of the outlined inclusion criteria related to speech processing or cognitive parallelism; thus it does not meet the core inclusion criteria and falls outside the defined research scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Data Augmentation for Neural NLP",
    "abstract": "Data scarcity is a problem that occurs in languages and tasks where we do not have large amounts of labeled data but want to use state-of-the-art models. Such models are often deep learning models that require a significant amount of data to train. Acquiring data for various machine learning problems is accompanied by high labeling costs. Data augmentation is a low-cost approach for tackling data scarcity. This paper gives an overview of current state-of-the-art data augmentation methods used for natural language processing, with an emphasis on methods for neural and transformer-based models. Furthermore, it discusses the practical challenges of data augmentation, possible mitigations, and directions for future research.",
    "metadata": {
      "arxiv_id": "2302.11412",
      "title": "Data Augmentation for Neural NLP",
      "summary": "Data scarcity is a problem that occurs in languages and tasks where we do not have large amounts of labeled data but want to use state-of-the-art models. Such models are often deep learning models that require a significant amount of data to train. Acquiring data for various machine learning problems is accompanied by high labeling costs. Data augmentation is a low-cost approach for tackling data scarcity. This paper gives an overview of current state-of-the-art data augmentation methods used for natural language processing, with an emphasis on methods for neural and transformer-based models. Furthermore, it discusses the practical challenges of data augmentation, possible mitigations, and directions for future research.",
      "authors": [
        "Domagoj Pluščec",
        "Jan Šnajder"
      ],
      "published": "2023-02-22T14:47:15Z",
      "updated": "2023-02-22T14:47:15Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.11412v1",
      "landing_url": "https://arxiv.org/abs/2302.11412v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.11412"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on data augmentation for neural NLP (text-based) and does not center Spoken Language Models or speech-based modeling; it lacks a speech-centric architecture, evaluation of speech interaction, or any content on SLMs, thus fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on data augmentation for neural NLP (text-based) and does not center Spoken Language Models or speech-based modeling; it lacks a speech-centric architecture, evaluation of speech interaction, or any content on SLMs, thus fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on data augmentation methods for neural NLP models, dealing with text data scarcity and deep learning models, without discussing spoken language modeling, speech signals as core modeling objects, or full-duplex spoken interaction aspects; thus it does not meet the inclusion criteria centered on spoken language models and their architectures, nor does it touch on evaluation metrics related to synchrony or interactive speech behaviors.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on data augmentation methods for neural NLP models, dealing with text data scarcity and deep learning models, without discussing spoken language modeling, speech signals as core modeling objects, or full-duplex spoken interaction aspects; thus it does not meet the inclusion criteria centered on spoken language models and their architectures, nor does it touch on evaluation metrics related to synchrony or interactive speech behaviors.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Auxiliary Task-based Deep Reinforcement Learning for Quantum Control",
    "abstract": "Due to its property of not requiring prior knowledge of the environment, reinforcement learning has significant potential for quantum control problems. In this work, we investigate the effectiveness of continuous control policies based on deep deterministic policy gradient. To solve the sparse reward signal in quantum learning control problems, we propose an auxiliary task-based deep reinforcement learning (AT-DRL) for quantum control. In particular, we first design a guided reward function based on the fidelity of quantum states that enables incremental fidelity improvement. Then, we introduce the concept of an auxiliary task whose network shares parameters with the main network to predict the reward provided by the environment (called the main task). The auxiliary task learns synchronously with the main task, allowing one to select the most relevant features of the environment, thus aiding the agent in comprehending how to achieve the desired state. The numerical simulations demonstrate that the proposed AT-DRL can provide a solution to the sparse reward in quantum systems, and has great potential in designing control pulses that achieve efficient quantum state preparation.",
    "metadata": {
      "arxiv_id": "2302.14312",
      "title": "Auxiliary Task-based Deep Reinforcement Learning for Quantum Control",
      "summary": "Due to its property of not requiring prior knowledge of the environment, reinforcement learning has significant potential for quantum control problems. In this work, we investigate the effectiveness of continuous control policies based on deep deterministic policy gradient. To solve the sparse reward signal in quantum learning control problems, we propose an auxiliary task-based deep reinforcement learning (AT-DRL) for quantum control. In particular, we first design a guided reward function based on the fidelity of quantum states that enables incremental fidelity improvement. Then, we introduce the concept of an auxiliary task whose network shares parameters with the main network to predict the reward provided by the environment (called the main task). The auxiliary task learns synchronously with the main task, allowing one to select the most relevant features of the environment, thus aiding the agent in comprehending how to achieve the desired state. The numerical simulations demonstrate that the proposed AT-DRL can provide a solution to the sparse reward in quantum systems, and has great potential in designing control pulses that achieve efficient quantum state preparation.",
      "authors": [
        "Shumin Zhou",
        "Hailan Ma",
        "Sen Kuang",
        "Daoyi Dong"
      ],
      "published": "2023-02-28T05:08:42Z",
      "updated": "2023-02-28T05:08:42Z",
      "categories": [
        "quant-ph",
        "cs.LG",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.14312v1",
      "landing_url": "https://arxiv.org/abs/2302.14312v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.14312"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on auxiliary-task deep reinforcement learning for quantum control and lacks any focus on Spoken Language Models or speech-language modeling, thus failing the inclusion criteria and meeting exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on auxiliary-task deep reinforcement learning for quantum control and lacks any focus on Spoken Language Models or speech-language modeling, thus failing the inclusion criteria and meeting exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on deep reinforcement learning for quantum control, a topic unrelated to spoken language models, speech language modeling, or full-duplex spoken interaction as defined in the inclusion criteria. There is no mention of spoken language models, speech as a core modeling target, or any relevant architecture or evaluation related to speech modeling or interactive speech systems, thus failing to meet the inclusion criteria and fitting none of the inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on deep reinforcement learning for quantum control, a topic unrelated to spoken language models, speech language modeling, or full-duplex spoken interaction as defined in the inclusion criteria. There is no mention of spoken language models, speech as a core modeling target, or any relevant architecture or evaluation related to speech modeling or interactive speech systems, thus failing to meet the inclusion criteria and fitting none of the inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Deep Neural Networks with Efficient Guaranteed Invariances",
    "abstract": "We address the problem of improving the performance and in particular the sample complexity of deep neural networks by enforcing and guaranteeing invariances to symmetry transformations rather than learning them from data. Group-equivariant convolutions are a popular approach to obtain equivariant representations. The desired corresponding invariance is then imposed using pooling operations. For rotations, it has been shown that using invariant integration instead of pooling further improves the sample complexity. In this contribution, we first expand invariant integration beyond rotations to flips and scale transformations. We then address the problem of incorporating multiple desired invariances into a single network. For this purpose, we propose a multi-stream architecture, where each stream is invariant to a different transformation such that the network can simultaneously benefit from multiple invariances. We demonstrate our approach with successful experiments on Scaled-MNIST, SVHN, CIFAR-10 and STL-10.",
    "metadata": {
      "arxiv_id": "2303.01567",
      "title": "Deep Neural Networks with Efficient Guaranteed Invariances",
      "summary": "We address the problem of improving the performance and in particular the sample complexity of deep neural networks by enforcing and guaranteeing invariances to symmetry transformations rather than learning them from data. Group-equivariant convolutions are a popular approach to obtain equivariant representations. The desired corresponding invariance is then imposed using pooling operations. For rotations, it has been shown that using invariant integration instead of pooling further improves the sample complexity. In this contribution, we first expand invariant integration beyond rotations to flips and scale transformations. We then address the problem of incorporating multiple desired invariances into a single network. For this purpose, we propose a multi-stream architecture, where each stream is invariant to a different transformation such that the network can simultaneously benefit from multiple invariances. We demonstrate our approach with successful experiments on Scaled-MNIST, SVHN, CIFAR-10 and STL-10.",
      "authors": [
        "Matthias Rath",
        "Alexandru Paul Condurache"
      ],
      "published": "2023-03-02T20:44:45Z",
      "updated": "2023-03-02T20:44:45Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.01567v1",
      "landing_url": "https://arxiv.org/abs/2303.01567v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.01567"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on computer-vision style invariances and multi-stream architectures rather than Spoken Language Models or speech-language modeling, so it does not meet the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on computer-vision style invariances and multi-stream architectures rather than Spoken Language Models or speech-language modeling, so it does not meet the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on deep neural networks with invariances to symmetry transformations for improving sample complexity, primarily in image recognition datasets like CIFAR-10, SVHN, and MNIST. There is no mention of spoken language models, speech as the core modeling object, full-duplex spoken language interaction, or relevant speech language modeling concepts required by the inclusion criteria. The work is unrelated to end-to-end speech modeling, synchronization, or dialogue interaction aspects described. Therefore, it does not meet the inclusion criteria and aligns with exclusion due to lack of relevance to spoken language models or speech modeling itself.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on deep neural networks with invariances to symmetry transformations for improving sample complexity, primarily in image recognition datasets like CIFAR-10, SVHN, and MNIST. There is no mention of spoken language models, speech as the core modeling object, full-duplex spoken language interaction, or relevant speech language modeling concepts required by the inclusion criteria. The work is unrelated to end-to-end speech modeling, synchronization, or dialogue interaction aspects described. Therefore, it does not meet the inclusion criteria and aligns with exclusion due to lack of relevance to spoken language models or speech modeling itself.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Butterfly: Multiple Reference Frames Feature Propagation Mechanism for Neural Video Compression",
    "abstract": "Using more reference frames can significantly improve the compression efficiency in neural video compression. However, in low-latency scenarios, most existing neural video compression frameworks usually use the previous one frame as reference. Or a few frameworks which use the previous multiple frames as reference only adopt a simple multi-reference frames propagation mechanism. In this paper, we present a more reasonable multi-reference frames propagation mechanism for neural video compression, called butterfly multi-reference frame propagation mechanism (Butterfly), which allows a more effective feature fusion of multi-reference frames. By this, we can generate more accurate temporal context conditional prior for Contextual Coding Module. Besides, when the number of decoded frames does not meet the required number of reference frames, we duplicate the nearest reference frame to achieve the requirement, which is better than duplicating the furthest one. Experiment results show that our method can significantly outperform the previous state-of-the-art (SOTA), and our neural codec can achieve -7.6% bitrate save on HEVC Class D dataset when compares with our base single-reference frame model with the same compression configuration.",
    "metadata": {
      "arxiv_id": "2303.02959",
      "title": "Butterfly: Multiple Reference Frames Feature Propagation Mechanism for Neural Video Compression",
      "summary": "Using more reference frames can significantly improve the compression efficiency in neural video compression. However, in low-latency scenarios, most existing neural video compression frameworks usually use the previous one frame as reference. Or a few frameworks which use the previous multiple frames as reference only adopt a simple multi-reference frames propagation mechanism. In this paper, we present a more reasonable multi-reference frames propagation mechanism for neural video compression, called butterfly multi-reference frame propagation mechanism (Butterfly), which allows a more effective feature fusion of multi-reference frames. By this, we can generate more accurate temporal context conditional prior for Contextual Coding Module. Besides, when the number of decoded frames does not meet the required number of reference frames, we duplicate the nearest reference frame to achieve the requirement, which is better than duplicating the furthest one. Experiment results show that our method can significantly outperform the previous state-of-the-art (SOTA), and our neural codec can achieve -7.6% bitrate save on HEVC Class D dataset when compares with our base single-reference frame model with the same compression configuration.",
      "authors": [
        "Feng Wang",
        "Haihang Ruan",
        "Fei Xiong",
        "Jiayu Yang",
        "Litian Li",
        "Ronggang Wang"
      ],
      "published": "2023-03-06T08:19:15Z",
      "updated": "2023-03-06T08:19:15Z",
      "categories": [
        "cs.CV",
        "cs.MM",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.02959v1",
      "landing_url": "https://arxiv.org/abs/2303.02959v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.02959"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on neural video compression and multi-reference frame propagation, with no focus on Spoken Language Models or speech-language modeling, thus not meeting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on neural video compression and multi-reference frame propagation, with no focus on Spoken Language Models or speech-language modeling, thus not meeting the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on neural video compression and feature propagation mechanisms related to video data, not on Spoken Language Models or speech-based language modeling; it lacks any discussion of speech, spoken language models, or relevant modeling, training, or evaluation related to speech signals or full-duplex spoken interaction, thus failing to meet all inclusion criteria and matching multiple exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on neural video compression and feature propagation mechanisms related to video data, not on Spoken Language Models or speech-based language modeling; it lacks any discussion of speech, spoken language models, or relevant modeling, training, or evaluation related to speech signals or full-duplex spoken interaction, thus failing to meet all inclusion criteria and matching multiple exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling",
    "abstract": "We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \\url{https://aka.ms/vallex}.",
    "metadata": {
      "arxiv_id": "2303.03926",
      "title": "Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling",
      "summary": "We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \\url{https://aka.ms/vallex}.",
      "authors": [
        "Ziqiang Zhang",
        "Long Zhou",
        "Chengyi Wang",
        "Sanyuan Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Yanqing Liu",
        "Huaming Wang",
        "Jinyu Li",
        "Lei He",
        "Sheng Zhao",
        "Furu Wei"
      ],
      "published": "2023-03-07T14:31:55Z",
      "updated": "2023-03-07T14:31:55Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.03926v1",
      "landing_url": "https://arxiv.org/abs/2303.03926v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.03926"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on spoken language modeling for cross-lingual speech synthesis (cross-lingual neural codec language modeling), discusses architecture/training/evaluation, is within the last 3 years, and uses terms like speech/language model in the title/abstract, thus aligning with SLM/FD-SLM themes rather than being purely ASR/TTS or non-speech NLP.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on spoken language modeling for cross-lingual speech synthesis (cross-lingual neural codec language modeling), discusses architecture/training/evaluation, is within the last 3 years, and uses terms like speech/language model in the title/abstract, thus aligning with SLM/FD-SLM themes rather than being purely ASR/TTS or non-speech NLP.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a cross-lingual neural codec language model focusing on speech synthesis by modeling acoustic token sequences and using both source speech and target text as input prompts, indicating an integration of speech and text for language modeling. However, it primarily focuses on zero-shot TTS and speech-to-speech translation, which are text-to-speech or speech-to-speech generation tasks rather than full-duplex spoken language interaction or synchronous turn-taking behavior. There is no explicit discussion about full-duplex interaction, cognitive parallelism, speech overlap management, or low-latency real-time interactive evaluation. While the model deals with speech-based language modeling, it seems more aligned with speech synthesis and cross-lingual generation than with synchronous multi-party spoken language modeling or real-time interaction paradigms the inclusion criteria emphasize. Therefore, it partially meets inclusion criteria but does not cover the interactive or full-duplex spoken language modeling focus required.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper presents a cross-lingual neural codec language model focusing on speech synthesis by modeling acoustic token sequences and using both source speech and target text as input prompts, indicating an integration of speech and text for language modeling. However, it primarily focuses on zero-shot TTS and speech-to-speech translation, which are text-to-speech or speech-to-speech generation tasks rather than full-duplex spoken language interaction or synchronous turn-taking behavior. There is no explicit discussion about full-duplex interaction, cognitive parallelism, speech overlap management, or low-latency real-time interactive evaluation. While the model deals with speech-based language modeling, it seems more aligned with speech synthesis and cross-lingual generation than with synchronous multi-party spoken language modeling or real-time interaction paradigms the inclusion criteria emphasize. Therefore, it partially meets inclusion criteria but does not cover the interactive or full-duplex spoken language modeling focus required.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "JuniorNano judged it relevant (5) noting it is a speech-centered language model; JuniorMini flagged that the work focuses on TTS/S2S generation and lacks any full‑duplex/real‑time interaction evaluation (2). I agree it should be included because the paper explicitly proposes a neural codec language model that predicts acoustic token sequences (i.e., language modeling of speech), describes model architecture/training and experimental evaluation, and thus meets the SLM inclusion requirements — though it does not address full‑duplex interactive or low‑latency evaluations (so it is relevant to SLMs but not FD‑SLM interaction specifics).",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "JuniorNano judged it relevant (5) noting it is a speech-centered language model; JuniorMini flagged that the work focuses on TTS/S2S generation and lacks any full‑duplex/real‑time interaction evaluation (2). I agree it should be included because the paper explicitly proposes a neural codec language model that predicts acoustic token sequences (i.e., language modeling of speech), describes model architecture/training and experimental evaluation, and thus meets the SLM inclusion requirements — though it does not address full‑duplex interactive or low‑latency evaluations (so it is relevant to SLMs but not FD‑SLM interaction specifics).",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Good-case Early-Stopping Latency of Synchronous Byzantine Reliable Broadcast: The Deterministic Case (Extended Version)",
    "abstract": "This paper considers the good-case latency of Byzantine Reliable Broadcast (BRB), i.e., the time taken by correct processes to deliver a message when the initial sender is correct. This time plays a crucial role in the performance of practical distributed systems. Although significant strides have been made in recent years on this question, progress has mainly focused on either asynchronous or randomized algorithms. By contrast, the good-case latency of deterministic synchronous BRB under a majority of Byzantine faults has been little studied. In particular, it was not known whether a goodcase latency below the worst-case bound of t + 1 rounds could be obtained. This work answers this open question positively and proposes a deterministic synchronous Byzantine reliable broadcast that achieves a good-case latency of max(2, t + 3 -- c) rounds, where t is the upper bound on the number of Byzantine processes and c the number of effectively correct processes.",
    "metadata": {
      "arxiv_id": "2303.05152",
      "title": "Good-case Early-Stopping Latency of Synchronous Byzantine Reliable Broadcast: The Deterministic Case (Extended Version)",
      "summary": "This paper considers the good-case latency of Byzantine Reliable Broadcast (BRB), i.e., the time taken by correct processes to deliver a message when the initial sender is correct. This time plays a crucial role in the performance of practical distributed systems. Although significant strides have been made in recent years on this question, progress has mainly focused on either asynchronous or randomized algorithms. By contrast, the good-case latency of deterministic synchronous BRB under a majority of Byzantine faults has been little studied. In particular, it was not known whether a goodcase latency below the worst-case bound of t + 1 rounds could be obtained. This work answers this open question positively and proposes a deterministic synchronous Byzantine reliable broadcast that achieves a good-case latency of max(2, t + 3 -- c) rounds, where t is the upper bound on the number of Byzantine processes and c the number of effectively correct processes.",
      "authors": [
        "Timothé Albouy",
        "Davide Frey",
        "Michel Raynal",
        "François Taïani"
      ],
      "published": "2023-03-09T10:10:27Z",
      "updated": "2023-03-10T09:19:49Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.05152v2",
      "landing_url": "https://arxiv.org/abs/2303.05152v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.05152"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about deterministic synchronous Byzantine reliable broadcast in distributed systems and does not involve Spoken Language Models or any speech-language modeling aspects, thus it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about deterministic synchronous Byzantine reliable broadcast in distributed systems and does not involve Spoken Language Models or any speech-language modeling aspects, thus it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper addresses Byzantine Reliable Broadcast protocols in distributed systems and does not discuss Spoken Language Models (SLMs), speech-based modeling, full-duplex speech interaction, or related evaluation criteria. It neither centers on speech or language modeling nor meets any aspect of the inclusion criteria focused on speech and spoken language models, therefore it falls entirely outside the scope of the topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper addresses Byzantine Reliable Broadcast protocols in distributed systems and does not discuss Spoken Language Models (SLMs), speech-based modeling, full-duplex speech interaction, or related evaluation criteria. It neither centers on speech or language modeling nor meets any aspect of the inclusion criteria focused on speech and spoken language models, therefore it falls entirely outside the scope of the topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DistillW2V2: A Small and Streaming Wav2vec 2.0 Based ASR Model",
    "abstract": "Wav2vec 2.0 (W2V2) has shown impressive performance in automatic speech recognition (ASR). However, the large model size and the non-streaming architecture make it hard to be used under low-resource or streaming scenarios. In this work, we propose a two-stage knowledge distillation method to solve these two problems: the first step is to make the big and non-streaming teacher model smaller, and the second step is to make it streaming. Specially, we adopt the MSE loss for the distillation of hidden layers and the modified LF-MMI loss for the distillation of the prediction layer. Experiments are conducted on Gigaspeech, Librispeech, and an in-house dataset. The results show that the distilled student model (DistillW2V2) we finally get is 8x faster and 12x smaller than the original teacher model. For the 480ms latency setup, the DistillW2V2's relative word error rate (WER) degradation varies from 9% to 23.4% on test sets, which reveals a promising way to extend the W2V2's application scope.",
    "metadata": {
      "arxiv_id": "2303.09278",
      "title": "DistillW2V2: A Small and Streaming Wav2vec 2.0 Based ASR Model",
      "summary": "Wav2vec 2.0 (W2V2) has shown impressive performance in automatic speech recognition (ASR). However, the large model size and the non-streaming architecture make it hard to be used under low-resource or streaming scenarios. In this work, we propose a two-stage knowledge distillation method to solve these two problems: the first step is to make the big and non-streaming teacher model smaller, and the second step is to make it streaming. Specially, we adopt the MSE loss for the distillation of hidden layers and the modified LF-MMI loss for the distillation of the prediction layer. Experiments are conducted on Gigaspeech, Librispeech, and an in-house dataset. The results show that the distilled student model (DistillW2V2) we finally get is 8x faster and 12x smaller than the original teacher model. For the 480ms latency setup, the DistillW2V2's relative word error rate (WER) degradation varies from 9% to 23.4% on test sets, which reveals a promising way to extend the W2V2's application scope.",
      "authors": [
        "Yanzhe Fu",
        "Yueteng Kang",
        "Songjun Cao",
        "Long Ma"
      ],
      "published": "2023-03-16T12:59:17Z",
      "updated": "2023-03-16T12:59:17Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.09278v1",
      "landing_url": "https://arxiv.org/abs/2303.09278v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.09278"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper concentrates on streaming ASR model distillation and does not explicitly discuss Spoken Language Models or dialogue-level synchronization, and it lacks the required spoken language model terminology in the title/abstract, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper concentrates on streaming ASR model distillation and does not explicitly discuss Spoken Language Models or dialogue-level synchronization, and it lacks the required spoken language model terminology in the title/abstract, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on improving Wav2vec 2.0 ASR models through knowledge distillation to create smaller and streaming versions, emphasizing acoustic modeling and streaming performance metrics like latency and word error rate. However, it primarily addresses ASR model compression and streaming capability without discussing spoken language modeling concepts such as full-duplex interaction, cognitive parallelism, multi-dimensional evaluation including behavior arbitration, or advanced real-time conversational features that the inclusion criteria emphasize; also, it lacks terminology related to spoken language models and does not cover dialogue interaction aspects. Therefore, it fits exclusion criteria related to focus solely on ASR without addressing spoken language modeling or synchronous interaction features.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on improving Wav2vec 2.0 ASR models through knowledge distillation to create smaller and streaming versions, emphasizing acoustic modeling and streaming performance metrics like latency and word error rate. However, it primarily addresses ASR model compression and streaming capability without discussing spoken language modeling concepts such as full-duplex interaction, cognitive parallelism, multi-dimensional evaluation including behavior arbitration, or advanced real-time conversational features that the inclusion criteria emphasize; also, it lacks terminology related to spoken language models and does not cover dialogue interaction aspects. Therefore, it fits exclusion criteria related to focus solely on ASR without addressing spoken language modeling or synchronous interaction features.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Uncertainty-Aware Optimal Transport for Semantically Coherent Out-of-Distribution Detection",
    "abstract": "Semantically coherent out-of-distribution (SCOOD) detection aims to discern outliers from the intended data distribution with access to unlabeled extra set. The coexistence of in-distribution and out-of-distribution samples will exacerbate the model overfitting when no distinction is made. To address this problem, we propose a novel uncertainty-aware optimal transport scheme. Our scheme consists of an energy-based transport (ET) mechanism that estimates the fluctuating cost of uncertainty to promote the assignment of semantic-agnostic representation, and an inter-cluster extension strategy that enhances the discrimination of semantic property among different clusters by widening the corresponding margin distance. Furthermore, a T-energy score is presented to mitigate the magnitude gap between the parallel transport and classifier branches. Extensive experiments on two standard SCOOD benchmarks demonstrate the above-par OOD detection performance, outperforming the state-of-the-art methods by a margin of 27.69% and 34.4% on FPR@95, respectively.",
    "metadata": {
      "arxiv_id": "2303.10449",
      "title": "Uncertainty-Aware Optimal Transport for Semantically Coherent Out-of-Distribution Detection",
      "summary": "Semantically coherent out-of-distribution (SCOOD) detection aims to discern outliers from the intended data distribution with access to unlabeled extra set. The coexistence of in-distribution and out-of-distribution samples will exacerbate the model overfitting when no distinction is made. To address this problem, we propose a novel uncertainty-aware optimal transport scheme. Our scheme consists of an energy-based transport (ET) mechanism that estimates the fluctuating cost of uncertainty to promote the assignment of semantic-agnostic representation, and an inter-cluster extension strategy that enhances the discrimination of semantic property among different clusters by widening the corresponding margin distance. Furthermore, a T-energy score is presented to mitigate the magnitude gap between the parallel transport and classifier branches. Extensive experiments on two standard SCOOD benchmarks demonstrate the above-par OOD detection performance, outperforming the state-of-the-art methods by a margin of 27.69% and 34.4% on FPR@95, respectively.",
      "authors": [
        "Fan Lu",
        "Kai Zhu",
        "Wei Zhai",
        "Kecheng Zheng",
        "Yang Cao"
      ],
      "published": "2023-03-18T16:22:59Z",
      "updated": "2023-03-21T13:41:03Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.10449v2",
      "landing_url": "https://arxiv.org/abs/2303.10449v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.10449"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract addresses general SCOOD with an uncertainty-aware transport mechanism, but it does not center on Spoken Language Models or any speech-focused modeling; thus it fails the core inclusion criteria for SLM/FD-SLM and related speech-language modeling evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract addresses general SCOOD with an uncertainty-aware transport mechanism, but it does not center on Spoken Language Models or any speech-focused modeling; thus it fails the core inclusion criteria for SLM/FD-SLM and related speech-language modeling evaluation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on an uncertainty-aware optimal transport method for out-of-distribution detection, which is a machine learning topic unrelated to Spoken Language Models (SLMs) or full-duplex spoken language interaction; it neither addresses speech as a core modeling object nor discusses speech language modeling or synchronous speech interaction, failing to meet any inclusion criteria and fitting none of the relevant concepts.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on an uncertainty-aware optimal transport method for out-of-distribution detection, which is a machine learning topic unrelated to Spoken Language Models (SLMs) or full-duplex spoken language interaction; it neither addresses speech as a core modeling object nor discusses speech language modeling or synchronous speech interaction, failing to meet any inclusion criteria and fitting none of the relevant concepts.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Revisiting Multimodal Representation in Contrastive Learning: From Patch and Token Embeddings to Finite Discrete Tokens",
    "abstract": "Contrastive learning-based vision-language pre-training approaches, such as CLIP, have demonstrated great success in many vision-language tasks. These methods achieve cross-modal alignment by encoding a matched image-text pair with similar feature embeddings, which are generated by aggregating information from visual patches and language tokens. However, direct aligning cross-modal information using such representations is challenging, as visual patches and text tokens differ in semantic levels and granularities. To alleviate this issue, we propose a Finite Discrete Tokens (FDT) based multimodal representation. FDT is a set of learnable tokens representing certain visual-semantic concepts. Both images and texts are embedded using shared FDT by first grounding multimodal inputs to FDT space and then aggregating the activated FDT representations. The matched visual and semantic concepts are enforced to be represented by the same set of discrete tokens by a sparse activation constraint. As a result, the granularity gap between the two modalities is reduced. Through both quantitative and qualitative analyses, we demonstrate that using FDT representations in CLIP-style models improves cross-modal alignment and performance in visual recognition and vision-language downstream tasks. Furthermore, we show that our method can learn more comprehensive representations, and the learned FDT capture meaningful cross-modal correspondence, ranging from objects to actions and attributes.",
    "metadata": {
      "arxiv_id": "2303.14865",
      "title": "Revisiting Multimodal Representation in Contrastive Learning: From Patch and Token Embeddings to Finite Discrete Tokens",
      "summary": "Contrastive learning-based vision-language pre-training approaches, such as CLIP, have demonstrated great success in many vision-language tasks. These methods achieve cross-modal alignment by encoding a matched image-text pair with similar feature embeddings, which are generated by aggregating information from visual patches and language tokens. However, direct aligning cross-modal information using such representations is challenging, as visual patches and text tokens differ in semantic levels and granularities. To alleviate this issue, we propose a Finite Discrete Tokens (FDT) based multimodal representation. FDT is a set of learnable tokens representing certain visual-semantic concepts. Both images and texts are embedded using shared FDT by first grounding multimodal inputs to FDT space and then aggregating the activated FDT representations. The matched visual and semantic concepts are enforced to be represented by the same set of discrete tokens by a sparse activation constraint. As a result, the granularity gap between the two modalities is reduced. Through both quantitative and qualitative analyses, we demonstrate that using FDT representations in CLIP-style models improves cross-modal alignment and performance in visual recognition and vision-language downstream tasks. Furthermore, we show that our method can learn more comprehensive representations, and the learned FDT capture meaningful cross-modal correspondence, ranging from objects to actions and attributes.",
      "authors": [
        "Yuxiao Chen",
        "Jianbo Yuan",
        "Yu Tian",
        "Shijie Geng",
        "Xinyu Li",
        "Ding Zhou",
        "Dimitris N. Metaxas",
        "Hongxia Yang"
      ],
      "published": "2023-03-27T00:58:39Z",
      "updated": "2023-03-27T00:58:39Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.14865v1",
      "landing_url": "https://arxiv.org/abs/2303.14865v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.14865"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on vision-language contrastive learning with patch/token embeddings and finite discrete tokens, not on spoken language models or speech-centric architectures, so it does not meet the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on vision-language contrastive learning with patch/token embeddings and finite discrete tokens, not on spoken language models or speech-centric architectures, so it does not meet the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on contrastive learning for vision-language representation, specifically dealing with image and text embeddings, and does not focus on speech or spoken language models, full-duplex spoken interaction, or speech language modeling as required by the inclusion criteria. Therefore, it does not meet any of the key inclusion criteria related to spoken language modeling or related interactive speech capabilities, nor does it discuss model architectures or evaluations centered on speech modeling, making it outside the scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on contrastive learning for vision-language representation, specifically dealing with image and text embeddings, and does not focus on speech or spoken language models, full-duplex spoken interaction, or speech language modeling as required by the inclusion criteria. Therefore, it does not meet any of the key inclusion criteria related to spoken language modeling or related interactive speech capabilities, nor does it discuss model architectures or evaluations centered on speech modeling, making it outside the scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on FPGA Devices",
    "abstract": "For Human Action Recognition tasks (HAR), 3D Convolutional Neural Networks have proven to be highly effective, achieving state-of-the-art results. This study introduces a novel streaming architecture based toolflow for mapping such models onto FPGAs considering the model's inherent characteristics and the features of the targeted FPGA device. The HARFLOW3D toolflow takes as input a 3D CNN in ONNX format and a description of the FPGA characteristics, generating a design that minimizes the latency of the computation. The toolflow is comprised of a number of parts, including i) a 3D CNN parser, ii) a performance and resource model, iii) a scheduling algorithm for executing 3D models on the generated hardware, iv) a resource-aware optimization engine tailored for 3D models, v) an automated mapping to synthesizable code for FPGAs. The ability of the toolflow to support a broad range of models and devices is shown through a number of experiments on various 3D CNN and FPGA system pairs. Furthermore, the toolflow has produced high-performing results for 3D CNN models that have not been mapped to FPGAs before, demonstrating the potential of FPGA-based systems in this space. Overall, HARFLOW3D has demonstrated its ability to deliver competitive latency compared to a range of state-of-the-art hand-tuned approaches being able to achieve up to 5$\\times$ better performance compared to some of the existing works.",
    "metadata": {
      "arxiv_id": "2303.17218",
      "title": "HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on FPGA Devices",
      "summary": "For Human Action Recognition tasks (HAR), 3D Convolutional Neural Networks have proven to be highly effective, achieving state-of-the-art results. This study introduces a novel streaming architecture based toolflow for mapping such models onto FPGAs considering the model's inherent characteristics and the features of the targeted FPGA device. The HARFLOW3D toolflow takes as input a 3D CNN in ONNX format and a description of the FPGA characteristics, generating a design that minimizes the latency of the computation. The toolflow is comprised of a number of parts, including i) a 3D CNN parser, ii) a performance and resource model, iii) a scheduling algorithm for executing 3D models on the generated hardware, iv) a resource-aware optimization engine tailored for 3D models, v) an automated mapping to synthesizable code for FPGAs. The ability of the toolflow to support a broad range of models and devices is shown through a number of experiments on various 3D CNN and FPGA system pairs. Furthermore, the toolflow has produced high-performing results for 3D CNN models that have not been mapped to FPGAs before, demonstrating the potential of FPGA-based systems in this space. Overall, HARFLOW3D has demonstrated its ability to deliver competitive latency compared to a range of state-of-the-art hand-tuned approaches being able to achieve up to 5$\\times$ better performance compared to some of the existing works.",
      "authors": [
        "Petros Toupas",
        "Alexander Montgomerie-Corcoran",
        "Christos-Savvas Bouganis",
        "Dimitrios Tzovaras"
      ],
      "published": "2023-03-30T08:25:27Z",
      "updated": "2023-05-29T11:23:45Z",
      "categories": [
        "cs.AR",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.17218v6",
      "landing_url": "https://arxiv.org/abs/2303.17218v6",
      "doi": "https://doi.org/10.1109/FCCM57271.2023.00024"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目聚焦於 HAR 的 3D-CNN 硬體加速與 FPGA toolflow，與 Spoken Language Models/Speech-Language Modeling 無關，因此不符合包含條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目聚焦於 HAR 的 3D-CNN 硬體加速與 FPGA toolflow，與 Spoken Language Models/Speech-Language Modeling 無關，因此不符合包含條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a 3D CNN accelerator toolflow for Human Action Recognition on FPGAs, which is unrelated to Spoken Language Models or speech language modeling; it neither addresses spoken language modeling nor full-duplex conversational capabilities, and does not meet the inclusion criteria centered on speech-based language models and interactive dialogue behaviors.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a 3D CNN accelerator toolflow for Human Action Recognition on FPGAs, which is unrelated to Spoken Language Models or speech language modeling; it neither addresses spoken language modeling nor full-duplex conversational capabilities, and does not meet the inclusion criteria centered on speech-based language models and interactive dialogue behaviors.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "InterFormer: Real-time Interactive Image Segmentation",
    "abstract": "Interactive image segmentation enables annotators to efficiently perform pixel-level annotation for segmentation tasks. However, the existing interactive segmentation pipeline suffers from inefficient computations of interactive models because of the following two issues. First, annotators' later click is based on models' feedback of annotators' former click. This serial interaction is unable to utilize model's parallelism capabilities. Second, in each interaction step, the model handles the invariant image along with the sparse variable clicks, resulting in a process that's highly repetitive and redundant. For efficient computations, we propose a method named InterFormer that follows a new pipeline to address these issues. InterFormer extracts and preprocesses the computationally time-consuming part i.e. image processing from the existing process. Specifically, InterFormer employs a large vision transformer (ViT) on high-performance devices to preprocess images in parallel, and then uses a lightweight module called interactive multi-head self attention (I-MSA) for interactive segmentation. Furthermore, the I-MSA module's deployment on low-power devices extends the practical application of interactive segmentation. The I-MSA module utilizes the preprocessed features to efficiently response to the annotator inputs in real-time. The experiments on several datasets demonstrate the effectiveness of InterFormer, which outperforms previous interactive segmentation models in terms of computational efficiency and segmentation quality, achieve real-time high-quality interactive segmentation on CPU-only devices. The code is available at https://github.com/YouHuang67/InterFormer.",
    "metadata": {
      "arxiv_id": "2304.02942",
      "title": "InterFormer: Real-time Interactive Image Segmentation",
      "summary": "Interactive image segmentation enables annotators to efficiently perform pixel-level annotation for segmentation tasks. However, the existing interactive segmentation pipeline suffers from inefficient computations of interactive models because of the following two issues. First, annotators' later click is based on models' feedback of annotators' former click. This serial interaction is unable to utilize model's parallelism capabilities. Second, in each interaction step, the model handles the invariant image along with the sparse variable clicks, resulting in a process that's highly repetitive and redundant. For efficient computations, we propose a method named InterFormer that follows a new pipeline to address these issues. InterFormer extracts and preprocesses the computationally time-consuming part i.e. image processing from the existing process. Specifically, InterFormer employs a large vision transformer (ViT) on high-performance devices to preprocess images in parallel, and then uses a lightweight module called interactive multi-head self attention (I-MSA) for interactive segmentation. Furthermore, the I-MSA module's deployment on low-power devices extends the practical application of interactive segmentation. The I-MSA module utilizes the preprocessed features to efficiently response to the annotator inputs in real-time. The experiments on several datasets demonstrate the effectiveness of InterFormer, which outperforms previous interactive segmentation models in terms of computational efficiency and segmentation quality, achieve real-time high-quality interactive segmentation on CPU-only devices. The code is available at https://github.com/YouHuang67/InterFormer.",
      "authors": [
        "You Huang",
        "Hao Yang",
        "Ke Sun",
        "Shengchuan Zhang",
        "Liujuan Cao",
        "Guannan Jiang",
        "Rongrong Ji"
      ],
      "published": "2023-04-06T08:57:00Z",
      "updated": "2023-08-09T08:41:39Z",
      "categories": [
        "cs.CV",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.02942v2",
      "landing_url": "https://arxiv.org/abs/2304.02942v2",
      "doi": "https://doi.org/10.48550/arXiv.2304.02942"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項聚焦於交互式圖像分割與視覺模型，並非以語音語言模型為核心，因此不符合包含條件，應予以排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項聚焦於交互式圖像分割與視覺模型，並非以語音語言模型為核心，因此不符合包含條件，應予以排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on interactive image segmentation involving computer vision techniques, without addressing spoken language models, speech modeling, or full-duplex spoken interaction capabilities as required by the inclusion criteria. It lacks any discussion on speech signal modeling, synchronous spoken interaction, or relevant evaluation metrics related to speech or language processing, hence it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on interactive image segmentation involving computer vision techniques, without addressing spoken language models, speech modeling, or full-duplex spoken interaction capabilities as required by the inclusion criteria. It lacks any discussion on speech signal modeling, synchronous spoken interaction, or relevant evaluation metrics related to speech or language processing, hence it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Leveraging GANs for data scarcity of COVID-19: Beyond the hype",
    "abstract": "Artificial Intelligence (AI)-based models can help in diagnosing COVID-19 from lung CT scans and X-ray images; however, these models require large amounts of data for training and validation. Many researchers studied Generative Adversarial Networks (GANs) for producing synthetic lung CT scans and X-Ray images to improve the performance of AI-based models. It is not well explored how good GAN-based methods performed to generate reliable synthetic data. This work analyzes 43 published studies that reported GANs for synthetic data generation. Many of these studies suffered data bias, lack of reproducibility, and lack of feedback from the radiologists or other domain experts. A common issue in these studies is the unavailability of the source code, hindering reproducibility. The included studies reported rescaling of the input images to train the existing GANs architecture without providing clinical insights on how the rescaling was motivated. Finally, even though GAN-based methods have the potential for data augmentation and improving the training of AI-based models, these methods fall short in terms of their use in clinical practice. This paper highlights research hotspots in countering the data scarcity problem, identifies various issues as well as potentials, and provides recommendations to guide future research. These recommendations might be useful to improve acceptability for the GAN-based approaches for data augmentation as GANs for data augmentation are increasingly becoming popular in the AI and medical imaging research community.",
    "metadata": {
      "arxiv_id": "2304.03536",
      "title": "Leveraging GANs for data scarcity of COVID-19: Beyond the hype",
      "summary": "Artificial Intelligence (AI)-based models can help in diagnosing COVID-19 from lung CT scans and X-ray images; however, these models require large amounts of data for training and validation. Many researchers studied Generative Adversarial Networks (GANs) for producing synthetic lung CT scans and X-Ray images to improve the performance of AI-based models. It is not well explored how good GAN-based methods performed to generate reliable synthetic data. This work analyzes 43 published studies that reported GANs for synthetic data generation. Many of these studies suffered data bias, lack of reproducibility, and lack of feedback from the radiologists or other domain experts. A common issue in these studies is the unavailability of the source code, hindering reproducibility. The included studies reported rescaling of the input images to train the existing GANs architecture without providing clinical insights on how the rescaling was motivated. Finally, even though GAN-based methods have the potential for data augmentation and improving the training of AI-based models, these methods fall short in terms of their use in clinical practice. This paper highlights research hotspots in countering the data scarcity problem, identifies various issues as well as potentials, and provides recommendations to guide future research. These recommendations might be useful to improve acceptability for the GAN-based approaches for data augmentation as GANs for data augmentation are increasingly becoming popular in the AI and medical imaging research community.",
      "authors": [
        "Hazrat Ali",
        "Christer Gronlund",
        "Zubair Shah"
      ],
      "published": "2023-04-07T08:26:12Z",
      "updated": "2023-04-07T08:26:12Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.03536v1",
      "landing_url": "https://arxiv.org/abs/2304.03536v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.03536"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses GAN-based synthetic data for COVID-19 imaging; it does not involve Spoken Language Models or any speech-language modeling, thus fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses GAN-based synthetic data for COVID-19 imaging; it does not involve Spoken Language Models or any speech-language modeling, thus fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on GANs for data augmentation in COVID-19 medical imaging and does not address spoken language models, speech language modeling, or any concepts related to full-duplex spoken interactions as required by the inclusion criteria. Therefore, it fails the thematic requirements and is unrelated to the specified domain.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on GANs for data augmentation in COVID-19 medical imaging and does not address spoken language models, speech language modeling, or any concepts related to full-duplex spoken interactions as required by the inclusion criteria. Therefore, it fails the thematic requirements and is unrelated to the specified domain.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SNIPS: Succinct Proof of Storage for Efficient Data Synchronization in Decentralized Storage Systems",
    "abstract": "Data synchronization in decentralized storage systems is essential to guarantee sufficient redundancy to prevent data loss. We present SNIPS, the first succinct proof of storage algorithm for synchronizing storage peers. A peer constructs a proof for its stored chunks and sends it to verifier peers. A verifier queries the proof to identify and subsequently requests missing chunks. The proof is succinct, supports membership queries, and requires only a few bits per chunk. We evaluated our SNIPS algorithm on a cluster of 1000 peers running Ethereum Swarm. Our results show that SNIPS reduces the amount of synchronization data by three orders of magnitude compared to the state-of-the-art. Additionally, creating and verifying a proof is linear with the number of chunks and typically requires only tens of microseconds per chunk. These qualities are vital for our use case, as we envision running SNIPS frequently to maintain sufficient redundancy consistently.",
    "metadata": {
      "arxiv_id": "2304.04891",
      "title": "SNIPS: Succinct Proof of Storage for Efficient Data Synchronization in Decentralized Storage Systems",
      "summary": "Data synchronization in decentralized storage systems is essential to guarantee sufficient redundancy to prevent data loss. We present SNIPS, the first succinct proof of storage algorithm for synchronizing storage peers. A peer constructs a proof for its stored chunks and sends it to verifier peers. A verifier queries the proof to identify and subsequently requests missing chunks. The proof is succinct, supports membership queries, and requires only a few bits per chunk.\n  We evaluated our SNIPS algorithm on a cluster of 1000 peers running Ethereum Swarm. Our results show that SNIPS reduces the amount of synchronization data by three orders of magnitude compared to the state-of-the-art. Additionally, creating and verifying a proof is linear with the number of chunks and typically requires only tens of microseconds per chunk. These qualities are vital for our use case, as we envision running SNIPS frequently to maintain sufficient redundancy consistently.",
      "authors": [
        "Racin Nygaard",
        "Hein Meling"
      ],
      "published": "2023-04-10T22:46:05Z",
      "updated": "2023-04-10T22:46:05Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.04891v1",
      "landing_url": "https://arxiv.org/abs/2304.04891v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.04891"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper addresses succinct proofs of storage for data synchronization in decentralized storage and does not involve Spoken Language Models or speech-based synchronization, failing the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper addresses succinct proofs of storage for data synchronization in decentralized storage and does not involve Spoken Language Models or speech-based synchronization, failing the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a proof of storage algorithm for data synchronization in decentralized storage systems and does not involve spoken language models, speech language modeling, or real-time interactive speech communication as defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a proof of storage algorithm for data synchronization in decentralized storage systems and does not involve spoken language models, speech language modeling, or real-time interactive speech communication as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Evaluating Strong Idempotence of Image Codec",
    "abstract": "In this paper, we first propose the concept of strong idempotent codec based on idempotent codec. The idempotence of codec refers to the stability of codec to re-compression. Similarly, we define the strong idempotence of codec as the stability of codec to multiple quality re-compression, which is an important feature of codec in the context of cloud transcoding. We provide a detailed example of strong idempotent codec with known source distribution. Further, we formalize a testing protocol to evaluate the strong idempotence of image codec. And finally, we evaluate the strong idempotence of current image codecs, including traditional codec and recent neural codec. Experimental results show that current image codecs are not stable to multi-round re-compression with different qualities, even if they are close to idempotent.",
    "metadata": {
      "arxiv_id": "2304.08269",
      "title": "Evaluating Strong Idempotence of Image Codec",
      "summary": "In this paper, we first propose the concept of strong idempotent codec based on idempotent codec. The idempotence of codec refers to the stability of codec to re-compression. Similarly, we define the strong idempotence of codec as the stability of codec to multiple quality re-compression, which is an important feature of codec in the context of cloud transcoding. We provide a detailed example of strong idempotent codec with known source distribution. Further, we formalize a testing protocol to evaluate the strong idempotence of image codec. And finally, we evaluate the strong idempotence of current image codecs, including traditional codec and recent neural codec. Experimental results show that current image codecs are not stable to multi-round re-compression with different qualities, even if they are close to idempotent.",
      "authors": [
        "Qian Zhang",
        "Tongda Xu",
        "Yanghao Li",
        "Yan Wang"
      ],
      "published": "2023-04-17T13:26:31Z",
      "updated": "2023-04-17T13:26:31Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.08269v1",
      "landing_url": "https://arxiv.org/abs/2304.08269v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.08269"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "理由：本項目聚焦影像編碼與多次重壓縮的穩定性，與 Spoken Language Models/語音語言建模核心無關，因此排除；分數：1。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "理由：本項目聚焦影像編碼與多次重壓縮的穩定性，與 Spoken Language Models/語音語言建模核心無關，因此排除；分數：1。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on the concept of strong idempotence in image codecs, which relates to image compression and codec stability, without any mention of spoken language models, speech language models, or speech-based interactive systems. It does not discuss model architectures, training, or evaluations based on spoken language or full-duplex speech interaction. Therefore, it does not meet any of the inclusion criteria relating to speech-based modeling and interaction, nor does it touch on any exclusion criteria related to speech systems.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on the concept of strong idempotence in image codecs, which relates to image compression and codec stability, without any mention of spoken language models, speech language models, or speech-based interactive systems. It does not discuss model architectures, training, or evaluations based on spoken language or full-duplex speech interaction. Therefore, it does not meet any of the inclusion criteria relating to speech-based modeling and interaction, nor does it touch on any exclusion criteria related to speech systems.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Use of social media and Natural Language Processing (NLP) in natural hazard research",
    "abstract": "Twitter is a microblogging service for sending short, public text messages (tweets) that has recently received more attention in scientific comunity. In the works of Sasaki et al. (2010) and Earle et al., (2011) the authors explored the real-time interaction on Twitter for detecting natural hazards (e.g., earthquakes, typhoons) baed on users' tweets. An inherent challenge for such an application is the natural language processing (NLP), which basically consists in converting the words in number (vectors and tensors) in order to (mathematically/ computationally) make predictions and classifications. Recently advanced computational tools have been made available for dealing with text computationally. In this report we implement a NLP machine learning with TensorFlow, an end-to-end open source plataform for machine learning applications, to process and classify evenct based on files containing only text.",
    "metadata": {
      "arxiv_id": "2304.08341",
      "title": "Use of social media and Natural Language Processing (NLP) in natural hazard research",
      "summary": "Twitter is a microblogging service for sending short, public text messages (tweets) that has recently received more attention in scientific comunity. In the works of Sasaki et al. (2010) and Earle et al., (2011) the authors explored the real-time interaction on Twitter for detecting natural hazards (e.g., earthquakes, typhoons) baed on users' tweets. An inherent challenge for such an application is the natural language processing (NLP), which basically consists in converting the words in number (vectors and tensors) in order to (mathematically/ computationally) make predictions and classifications. Recently advanced computational tools have been made available for dealing with text computationally. In this report we implement a NLP machine learning with TensorFlow, an end-to-end open source plataform for machine learning applications, to process and classify evenct based on files containing only text.",
      "authors": [
        "José Augusto Proença Maia Devienne"
      ],
      "published": "2023-04-17T15:03:05Z",
      "updated": "2023-04-17T15:03:05Z",
      "categories": [
        "cs.CL",
        "physics.geo-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.08341v1",
      "landing_url": "https://arxiv.org/abs/2304.08341v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.08341"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on text-based NLP using Twitter data and TensorFlow, with no Spoken Language Model (SLM) or speech-centric modeling, training, or evaluation, so it does not meet the inclusion criteria and aligns with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on text-based NLP using Twitter data and TensorFlow, with no Spoken Language Model (SLM) or speech-centric modeling, training, or evaluation, so it does not meet the inclusion criteria and aligns with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on natural language processing of text data from Twitter to detect natural hazards and implements machine learning using TensorFlow on text files. It does not address spoken language models or speech as the primary modeling object, nor does it discuss speech model architectures, training, or evaluation related to full-duplex spoken language models or related interaction capabilities. Therefore, it does not meet the inclusion criteria and falls into the exclusion criteria of focusing solely on textual NLP rather than speech modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on natural language processing of text data from Twitter to detect natural hazards and implements machine learning using TensorFlow on text files. It does not address spoken language models or speech as the primary modeling object, nor does it discuss speech model architectures, training, or evaluation related to full-duplex spoken language models or related interaction capabilities. Therefore, it does not meet the inclusion criteria and falls into the exclusion criteria of focusing solely on textual NLP rather than speech modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ATHEENA: A Toolflow for Hardware Early-Exit Network Automation",
    "abstract": "The continued need for improvements in accuracy, throughput, and efficiency of Deep Neural Networks has resulted in a multitude of methods that make the most of custom architectures on FPGAs. These include the creation of hand-crafted networks and the use of quantization and pruning to reduce extraneous network parameters. However, with the potential of static solutions already well exploited, we propose to shift the focus to using the varying difficulty of individual data samples to further improve efficiency and reduce average compute for classification. Input-dependent computation allows for the network to make runtime decisions to finish a task early if the result meets a confidence threshold. Early-Exit network architectures have become an increasingly popular way to implement such behaviour in software. We create: A Toolflow for Hardware Early-Exit Network Automation (ATHEENA), an automated FPGA toolflow that leverages the probability of samples exiting early from such networks to scale the resources allocated to different sections of the network. The toolflow uses the data-flow model of fpgaConvNet, extended to support Early-Exit networks as well as Design Space Exploration to optimize the generated streaming architecture hardware with the goal of increasing throughput/reducing area while maintaining accuracy. Experimental results on three different networks demonstrate a throughput increase of $2.00\\times$ to $2.78\\times$ compared to an optimized baseline network implementation with no early exits. Additionally, the toolflow can achieve a throughput matching the same baseline with as low as $46\\%$ of the resources the baseline requires.",
    "metadata": {
      "arxiv_id": "2304.08400",
      "title": "ATHEENA: A Toolflow for Hardware Early-Exit Network Automation",
      "summary": "The continued need for improvements in accuracy, throughput, and efficiency of Deep Neural Networks has resulted in a multitude of methods that make the most of custom architectures on FPGAs. These include the creation of hand-crafted networks and the use of quantization and pruning to reduce extraneous network parameters. However, with the potential of static solutions already well exploited, we propose to shift the focus to using the varying difficulty of individual data samples to further improve efficiency and reduce average compute for classification. Input-dependent computation allows for the network to make runtime decisions to finish a task early if the result meets a confidence threshold. Early-Exit network architectures have become an increasingly popular way to implement such behaviour in software.\n  We create: A Toolflow for Hardware Early-Exit Network Automation (ATHEENA), an automated FPGA toolflow that leverages the probability of samples exiting early from such networks to scale the resources allocated to different sections of the network. The toolflow uses the data-flow model of fpgaConvNet, extended to support Early-Exit networks as well as Design Space Exploration to optimize the generated streaming architecture hardware with the goal of increasing throughput/reducing area while maintaining accuracy. Experimental results on three different networks demonstrate a throughput increase of $2.00\\times$ to $2.78\\times$ compared to an optimized baseline network implementation with no early exits. Additionally, the toolflow can achieve a throughput matching the same baseline with as low as $46\\%$ of the resources the baseline requires.",
      "authors": [
        "Benjamin Biggs",
        "Christos-Savvas Bouganis",
        "George A. Constantinides"
      ],
      "published": "2023-04-17T16:06:58Z",
      "updated": "2025-04-14T14:54:00Z",
      "categories": [
        "cs.AR",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.08400v2",
      "landing_url": "https://arxiv.org/abs/2304.08400v2",
      "doi": "https://doi.org/10.48550/arXiv.2304.08400"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about hardware early-exit network automation for FPGA acceleration and does not involve Spoken Language Models, speech-language modeling, or interactive dialogue evaluation, thus fails all inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about hardware early-exit network automation for FPGA acceleration and does not involve Spoken Language Models, speech-language modeling, or interactive dialogue evaluation, thus fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on hardware optimization for early-exit neural networks on FPGAs, targeting improvements in throughput and resource usage for deep neural networks; it does not address spoken language models, speech language modeling, or aspects of full-duplex spoken interaction as required by the inclusion criteria, nor does it discuss spoken language model architectures, training, or evaluation. Therefore, it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on hardware optimization for early-exit neural networks on FPGAs, targeting improvements in throughput and resource usage for deep neural networks; it does not address spoken language models, speech language modeling, or aspects of full-duplex spoken interaction as required by the inclusion criteria, nor does it discuss spoken language model architectures, training, or evaluation. Therefore, it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ProPanDL: A Modular Architecture for Uncertainty-Aware Panoptic Segmentation",
    "abstract": "We introduce ProPanDL, a family of networks capable of uncertainty-aware panoptic segmentation. Unlike existing segmentation methods, ProPanDL is capable of estimating full probability distributions for both the semantic and spatial aspects of panoptic segmentation. We implement and evaluate ProPanDL variants capable of estimating both parametric (Variance Network) and parameter-free (SampleNet) distributions quantifying pixel-wise spatial uncertainty. We couple these approaches with two methods (Temperature Scaling and Evidential Deep Learning) for semantic uncertainty estimation. To evaluate the uncertainty-aware panoptic segmentation task, we address limitations with existing approaches by proposing new metrics that enable separate evaluation of spatial and semantic uncertainty. We additionally propose the use of the energy score, a proper scoring rule, for more robust evaluation of spatial output distributions. Using these metrics, we conduct an extensive evaluation of ProPanDL variants. Our results demonstrate that ProPanDL is capable of estimating well-calibrated and meaningful output distributions while still retaining strong performance on the base panoptic segmentation task.",
    "metadata": {
      "arxiv_id": "2304.08645",
      "title": "ProPanDL: A Modular Architecture for Uncertainty-Aware Panoptic Segmentation",
      "summary": "We introduce ProPanDL, a family of networks capable of uncertainty-aware panoptic segmentation. Unlike existing segmentation methods, ProPanDL is capable of estimating full probability distributions for both the semantic and spatial aspects of panoptic segmentation. We implement and evaluate ProPanDL variants capable of estimating both parametric (Variance Network) and parameter-free (SampleNet) distributions quantifying pixel-wise spatial uncertainty. We couple these approaches with two methods (Temperature Scaling and Evidential Deep Learning) for semantic uncertainty estimation. To evaluate the uncertainty-aware panoptic segmentation task, we address limitations with existing approaches by proposing new metrics that enable separate evaluation of spatial and semantic uncertainty. We additionally propose the use of the energy score, a proper scoring rule, for more robust evaluation of spatial output distributions. Using these metrics, we conduct an extensive evaluation of ProPanDL variants. Our results demonstrate that ProPanDL is capable of estimating well-calibrated and meaningful output distributions while still retaining strong performance on the base panoptic segmentation task.",
      "authors": [
        "Jacob Deery",
        "Chang Won Lee",
        "Steven Waslander"
      ],
      "published": "2023-04-17T22:31:23Z",
      "updated": "2023-04-17T22:31:23Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.08645v1",
      "landing_url": "https://arxiv.org/abs/2304.08645v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.08645"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on uncertainty-aware panoptic segmentation in computer vision and does not treat Spoken Language Models or any speech-centric modeling as core; therefore it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on uncertainty-aware panoptic segmentation in computer vision and does not treat Spoken Language Models or any speech-centric modeling as core; therefore it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on uncertainty-aware panoptic segmentation, a computer vision task, without any mention of spoken language models, speech language modeling, full-duplex interaction, or related speech-centric architectures or evaluations. It does not discuss spoken language as a core modeling object nor address topics like cognitive parallelism, turn-taking, or speech interaction. Therefore, it does not meet the inclusion criteria focused on spoken language modeling and full-duplex spoken interactions, nor does it avoid exclusion criteria related to speech-centered studies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on uncertainty-aware panoptic segmentation, a computer vision task, without any mention of spoken language models, speech language modeling, full-duplex interaction, or related speech-centric architectures or evaluations. It does not discuss spoken language as a core modeling object nor address topics like cognitive parallelism, turn-taking, or speech interaction. Therefore, it does not meet the inclusion criteria focused on spoken language modeling and full-duplex spoken interactions, nor does it avoid exclusion criteria related to speech-centered studies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Codesign of quantum error-correcting codes and modular chiplets in the presence of defects",
    "abstract": "Fabrication errors pose a significant challenge in scaling up solid-state quantum devices to the sizes required for fault-tolerant (FT) quantum applications. To mitigate the resource overhead caused by fabrication errors, we combine two approaches: (1) leveraging the flexibility of a modular architecture, (2) adapting the procedure of quantum error correction (QEC) to account for fabrication defects. We simulate the surface code adapted to qubit arrays with arbitrarily distributed defects to find metrics that characterize how defects affect fidelity. We then determine the impact of defects on the resource overhead of realizing a fault-tolerant quantum computer, on a chiplet-based modular architecture. Our strategy for dealing with fabrication defects demonstrates an exponential suppression of logical failure where error rates of non-faulty physical qubits are ~0.1% in a circuit-based noise model. This is a typical regime where we imagine running the defect-free surface code. We use our numerical results to establish post-selection criteria for building a device from defective chiplets. Using our criteria, we then evaluate the resource overhead in terms of the average number of fabricated physical qubits per logical qubit. We find that an optimal choice of chiplet size, based on the defect rate and target fidelity, is essential to limiting any additional error correction overhead due to defects. When the optimal chiplet size is chosen, at a defect rate of 1% the resource overhead can be reduced to below 3X and 6X respectively for the two defect models we use, for a wide range of target performance. We also determine cutoff fidelity values that help identify whether a qubit should be disabled or kept as part of the error correction code.",
    "metadata": {
      "arxiv_id": "2305.00138",
      "title": "Codesign of quantum error-correcting codes and modular chiplets in the presence of defects",
      "summary": "Fabrication errors pose a significant challenge in scaling up solid-state quantum devices to the sizes required for fault-tolerant (FT) quantum applications. To mitigate the resource overhead caused by fabrication errors, we combine two approaches: (1) leveraging the flexibility of a modular architecture, (2) adapting the procedure of quantum error correction (QEC) to account for fabrication defects. We simulate the surface code adapted to qubit arrays with arbitrarily distributed defects to find metrics that characterize how defects affect fidelity. We then determine the impact of defects on the resource overhead of realizing a fault-tolerant quantum computer, on a chiplet-based modular architecture. Our strategy for dealing with fabrication defects demonstrates an exponential suppression of logical failure where error rates of non-faulty physical qubits are ~0.1% in a circuit-based noise model. This is a typical regime where we imagine running the defect-free surface code. We use our numerical results to establish post-selection criteria for building a device from defective chiplets. Using our criteria, we then evaluate the resource overhead in terms of the average number of fabricated physical qubits per logical qubit. We find that an optimal choice of chiplet size, based on the defect rate and target fidelity, is essential to limiting any additional error correction overhead due to defects. When the optimal chiplet size is chosen, at a defect rate of 1% the resource overhead can be reduced to below 3X and 6X respectively for the two defect models we use, for a wide range of target performance. We also determine cutoff fidelity values that help identify whether a qubit should be disabled or kept as part of the error correction code.",
      "authors": [
        "Sophia Fuhui Lin",
        "Joshua Viszlai",
        "Kaitlin N. Smith",
        "Gokul Subramanian Ravi",
        "Charles Yuan",
        "Frederic T. Chong",
        "Benjamin J. Brown"
      ],
      "published": "2023-04-29T01:06:52Z",
      "updated": "2024-03-22T18:35:57Z",
      "categories": [
        "quant-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.00138v3",
      "landing_url": "https://arxiv.org/abs/2305.00138v3",
      "doi": "https://doi.org/10.1145/3620665.3640362"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on quantum error correction and modular chiplets for quantum computing, with no reference to Spoken Language Models, speech modeling, or any speech/sync interaction aspects; thus it does not meet the SLM-focused inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on quantum error correction and modular chiplets for quantum computing, with no reference to Spoken Language Models, speech modeling, or any speech/sync interaction aspects; thus it does not meet the SLM-focused inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on quantum error correction and modular chiplet architectures for quantum computing, with no mention of spoken language models, speech language modeling, or related concepts as required by the inclusion criteria. It does not deal with speech as the core modeling object, nor does it discuss full-duplex spoken language interaction, cognitive parallelism, or synchronization mechanisms. Therefore, it does not meet the inclusion criteria and falls outside the scope of the retrieval target.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on quantum error correction and modular chiplet architectures for quantum computing, with no mention of spoken language models, speech language modeling, or related concepts as required by the inclusion criteria. It does not deal with speech as the core modeling object, nor does it discuss full-duplex spoken language interaction, cognitive parallelism, or synchronization mechanisms. Therefore, it does not meet the inclusion criteria and falls outside the scope of the retrieval target.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "On the properties of Gaussian Copula Mixture Models",
    "abstract": "This paper investigates Gaussian copula mixture models (GCMM), which are an extension of Gaussian mixture models (GMM) that incorporate copula concepts. The paper presents the mathematical definition of GCMM and explores the properties of its likelihood function. Additionally, the paper proposes extended Expectation Maximum algorithms to estimate parameters for the mixture of copulas. The marginal distributions corresponding to each component are estimated separately using nonparametric statistical methods. In the experiment, GCMM demonstrates improved goodness-of-fitting compared to GMM when using the same number of clusters. Furthermore, GCMM has the ability to leverage un-synchronized data across dimensions for more comprehensive data analysis.",
    "metadata": {
      "arxiv_id": "2305.01479",
      "title": "On the properties of Gaussian Copula Mixture Models",
      "summary": "This paper investigates Gaussian copula mixture models (GCMM), which are an extension of Gaussian mixture models (GMM) that incorporate copula concepts. The paper presents the mathematical definition of GCMM and explores the properties of its likelihood function. Additionally, the paper proposes extended Expectation Maximum algorithms to estimate parameters for the mixture of copulas. The marginal distributions corresponding to each component are estimated separately using nonparametric statistical methods. In the experiment, GCMM demonstrates improved goodness-of-fitting compared to GMM when using the same number of clusters. Furthermore, GCMM has the ability to leverage un-synchronized data across dimensions for more comprehensive data analysis.",
      "authors": [
        "Ke Wan",
        "Alain Kornhauser"
      ],
      "published": "2023-05-02T14:59:37Z",
      "updated": "2023-05-24T01:41:38Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.01479v2",
      "landing_url": "https://arxiv.org/abs/2305.01479v2",
      "doi": "https://doi.org/10.48550/arXiv.2305.01479"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此篇論文聚焦於高斯 Copula 混合模型的性質與參數估計，並未以 Spoken Language Models 或語音語言建模為核心，亦未涵蓋語音互動或對話評估，因此不符合納入條件且應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此篇論文聚焦於高斯 Copula 混合模型的性質與參數估計，並未以 Spoken Language Models 或語音語言建模為核心，亦未涵蓋語音互動或對話評估，因此不符合納入條件且應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on Gaussian Copula Mixture Models for statistical modeling and parameter estimation, without any mention of spoken language models, speech language models, or related concepts such as end-to-end speech modeling, full-duplex spoken interaction, or synchronization in speech interaction. The topic is unrelated to speech or spoken language modeling and does not meet the inclusion criteria centered on speech-based language models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on Gaussian Copula Mixture Models for statistical modeling and parameter estimation, without any mention of spoken language models, speech language models, or related concepts such as end-to-end speech modeling, full-duplex spoken interaction, or synchronization in speech interaction. The topic is unrelated to speech or spoken language modeling and does not meet the inclusion criteria centered on speech-based language models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Response-conditioned Turn-taking Prediction",
    "abstract": "Previous approaches to turn-taking and response generation in conversational systems have treated it as a two-stage process: First, the end of a turn is detected (based on conversation history), then the system generates an appropriate response. Humans, however, do not take the turn just because it is likely, but also consider whether what they want to say fits the position. In this paper, we present a model (an extension of TurnGPT) that conditions the end-of-turn prediction on both conversation history and what the next speaker wants to say. We found that our model consistently outperforms the baseline model in a variety of metrics. The improvement is most prominent in two scenarios where turn predictions can be ambiguous solely from the conversation history: 1) when the current utterance contains a statement followed by a question; 2) when the end of the current utterance semantically matches the response. Treating the turn-prediction and response-ranking as a one-stage process, our findings suggest that our model can be used as an incremental response ranker, which can be applied in various settings.",
    "metadata": {
      "arxiv_id": "2305.02036",
      "title": "Response-conditioned Turn-taking Prediction",
      "summary": "Previous approaches to turn-taking and response generation in conversational systems have treated it as a two-stage process: First, the end of a turn is detected (based on conversation history), then the system generates an appropriate response. Humans, however, do not take the turn just because it is likely, but also consider whether what they want to say fits the position. In this paper, we present a model (an extension of TurnGPT) that conditions the end-of-turn prediction on both conversation history and what the next speaker wants to say. We found that our model consistently outperforms the baseline model in a variety of metrics. The improvement is most prominent in two scenarios where turn predictions can be ambiguous solely from the conversation history: 1) when the current utterance contains a statement followed by a question; 2) when the end of the current utterance semantically matches the response. Treating the turn-prediction and response-ranking as a one-stage process, our findings suggest that our model can be used as an incremental response ranker, which can be applied in various settings.",
      "authors": [
        "Bing'er Jiang",
        "Erik Ekstedt",
        "Gabriel Skantze"
      ],
      "published": "2023-05-03T11:06:50Z",
      "updated": "2023-05-03T11:06:50Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.02036v1",
      "landing_url": "https://arxiv.org/abs/2305.02036v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.02036"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此論文聚焦於文本層面的端點判定與回應排序，以對話歷史與意圖為條件，未將語音信號作為核心建模對象，也未討論同步互動、語音重疊管理或低延遲等全雙工語言模型的設計與評估，因而不符合 Spokken Language Models / FD-SLMs 的核心定義與評估框架。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此論文聚焦於文本層面的端點判定與回應排序，以對話歷史與意圖為條件，未將語音信號作為核心建模對象，也未討論同步互動、語音重疊管理或低延遲等全雙工語言模型的設計與評估，因而不符合 Spokken Language Models / FD-SLMs 的核心定義與評估框架。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on turn-taking prediction in conversational systems using a model conditioned on conversation history and intended response, but it does not mention modeling or leveraging speech-based representations or spoken language models explicitly. It treats turn-taking from a textual or abstract conversation history perspective without explicit reference to speech data or full-duplex spoken language modeling. Hence, it does not align with the inclusion criteria centered on speech-level modeling or full-duplex spoken language interaction frameworks.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on turn-taking prediction in conversational systems using a model conditioned on conversation history and intended response, but it does not mention modeling or leveraging speech-based representations or spoken language models explicitly. It treats turn-taking from a textual or abstract conversation history perspective without explicit reference to speech data or full-duplex spoken language modeling. Hence, it does not align with the inclusion criteria centered on speech-level modeling or full-duplex spoken language interaction frameworks.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The Quantum Internet: an Efficient Stabilizer states Distribution Scheme",
    "abstract": "Quantum networks constitute a major part of quantum technologies. They will boost distributed quantum computing drastically by providing a scalable modular architecture of quantum chips, or by establishing an infrastructure for measurement based quantum computing. Moreover, they will provide the backbone of the future quantum internet, allowing for high margins of security. Interestingly, the advantages that the quantum networks would provide for communications, rely on entanglement distribution, which suffers from high latency in protocols based on Bell pair distribution and bipartite entanglement swapping. Moreover, the designed algorithms for multipartite entanglement routing suffer from intractability issues making them unsolvable exactly in polynomial time. In this paper, we investigate a new approach for graph states distribution in quantum networks relying inherently on local quantum coding -- LQC -- isometries and on multipartite states transfer. Additionally, single-shot bounds for stabilizer states distribution are provided. Analogously to network coding, these bounds are shown to be achievable if appropriate isometries/stabilizer codes in relay nodes are chosen, which induces a lower latency entanglement distribution. As a matter of fact, the advantages of the protocol for different figures of merit of the network are provided.",
    "metadata": {
      "arxiv_id": "2305.02656",
      "title": "The Quantum Internet: an Efficient Stabilizer states Distribution Scheme",
      "summary": "Quantum networks constitute a major part of quantum technologies. They will boost distributed quantum computing drastically by providing a scalable modular architecture of quantum chips, or by establishing an infrastructure for measurement based quantum computing. Moreover, they will provide the backbone of the future quantum internet, allowing for high margins of security. Interestingly, the advantages that the quantum networks would provide for communications, rely on entanglement distribution, which suffers from high latency in protocols based on Bell pair distribution and bipartite entanglement swapping. Moreover, the designed algorithms for multipartite entanglement routing suffer from intractability issues making them unsolvable exactly in polynomial time. In this paper, we investigate a new approach for graph states distribution in quantum networks relying inherently on local quantum coding -- LQC -- isometries and on multipartite states transfer. Additionally, single-shot bounds for stabilizer states distribution are provided. Analogously to network coding, these bounds are shown to be achievable if appropriate isometries/stabilizer codes in relay nodes are chosen, which induces a lower latency entanglement distribution. As a matter of fact, the advantages of the protocol for different figures of merit of the network are provided.",
      "authors": [
        "Seid Koudia"
      ],
      "published": "2023-05-04T08:53:38Z",
      "updated": "2023-05-04T08:53:38Z",
      "categories": [
        "quant-ph",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.02656v1",
      "landing_url": "https://arxiv.org/abs/2305.02656v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.02656"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on quantum networks and stabilizer-state distribution, with no relation to Spoken Language Models or speech-language modeling; thus it does not meet inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on quantum networks and stabilizer-state distribution, with no relation to Spoken Language Models or speech-language modeling; thus it does not meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on quantum networks and entanglement distribution using stabilizer states, without any mention of spoken language models, speech language modeling, full-duplex interaction, or related evaluation metrics; thus, it does not meet the inclusion criteria centered on spoken language models and speech-based architectures.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on quantum networks and entanglement distribution using stabilizer states, without any mention of spoken language models, speech language modeling, full-duplex interaction, or related evaluation metrics; thus, it does not meet the inclusion criteria centered on spoken language models and speech-based architectures.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Mapping quantum circuits to modular architectures with QUBO",
    "abstract": "Modular quantum computing architectures are a promising alternative to monolithic QPU (Quantum Processing Unit) designs for scaling up quantum devices. They refer to a set of interconnected QPUs or cores consisting of tightly coupled quantum bits that can communicate via quantum-coherent and classical links. In multi-core architectures, it is crucial to minimize the amount of communication between cores when executing an algorithm. Therefore, mapping a quantum circuit onto a modular architecture involves finding an optimal assignment of logical qubits (qubits in the quantum circuit) to different cores with the aim to minimize the number of expensive inter-core operations while adhering to given hardware constraints. In this paper, we propose for the first time a Quadratic Unconstrained Binary Optimization (QUBO) technique to encode the problem and the solution for both qubit allocation and inter-core communication costs in binary decision variables. To this end, the quantum circuit is split into slices, and qubit assignment is formulated as a graph partitioning problem for each circuit slice. The costly inter-core communication is reduced by penalizing inter-core qubit communications. The final solution is obtained by minimizing the overall cost across all circuit slices. To evaluate the effectiveness of our approach, we conduct a detailed analysis using a representative set of benchmarks having a high number of qubits on two different multi-core architectures. Our method showed promising results and performed exceptionally well with very dense and highly-parallelized circuits that require on average 0.78 inter-core communications per two-qubit gate.",
    "metadata": {
      "arxiv_id": "2305.06687",
      "title": "Mapping quantum circuits to modular architectures with QUBO",
      "summary": "Modular quantum computing architectures are a promising alternative to monolithic QPU (Quantum Processing Unit) designs for scaling up quantum devices. They refer to a set of interconnected QPUs or cores consisting of tightly coupled quantum bits that can communicate via quantum-coherent and classical links. In multi-core architectures, it is crucial to minimize the amount of communication between cores when executing an algorithm. Therefore, mapping a quantum circuit onto a modular architecture involves finding an optimal assignment of logical qubits (qubits in the quantum circuit) to different cores with the aim to minimize the number of expensive inter-core operations while adhering to given hardware constraints. In this paper, we propose for the first time a Quadratic Unconstrained Binary Optimization (QUBO) technique to encode the problem and the solution for both qubit allocation and inter-core communication costs in binary decision variables. To this end, the quantum circuit is split into slices, and qubit assignment is formulated as a graph partitioning problem for each circuit slice. The costly inter-core communication is reduced by penalizing inter-core qubit communications. The final solution is obtained by minimizing the overall cost across all circuit slices. To evaluate the effectiveness of our approach, we conduct a detailed analysis using a representative set of benchmarks having a high number of qubits on two different multi-core architectures. Our method showed promising results and performed exceptionally well with very dense and highly-parallelized circuits that require on average 0.78 inter-core communications per two-qubit gate.",
      "authors": [
        "Medina Bandic",
        "Luise Prielinger",
        "Jonas Nüßlein",
        "Anabel Ovide",
        "Santiago Rodrigo",
        "Sergi Abadal",
        "Hans van Someren",
        "Gayane Vardoyan",
        "Eduard Alarcon",
        "Carmen G. Almudever",
        "Sebastian Feld"
      ],
      "published": "2023-05-11T09:45:47Z",
      "updated": "2023-05-11T09:45:47Z",
      "categories": [
        "quant-ph",
        "cs.ET"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.06687v1",
      "landing_url": "https://arxiv.org/abs/2305.06687v1",
      "doi": "https://doi.org/10.1109/QCE57702.2023.00094"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses modular quantum architectures and QUBO-based qubit allocation, with no focus on Spoken Language Models or speech-language modeling aspects.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses modular quantum architectures and QUBO-based qubit allocation, with no focus on Spoken Language Models or speech-language modeling aspects.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on quantum computing architectures and optimization techniques for quantum circuit mapping, which is unrelated to spoken language models or speech language modeling; it does not address any of the inclusion criteria such as modeling or evaluation of spoken language, full-duplex dialogue interaction, or speech-based language models, nor does it mention speech or language modeling at all.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on quantum computing architectures and optimization techniques for quantum circuit mapping, which is unrelated to spoken language models or speech language modeling; it does not address any of the inclusion criteria such as modeling or evaluation of spoken language, full-duplex dialogue interaction, or speech-based language models, nor does it mention speech or language modeling at all.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Exploring the Rate-Distortion-Complexity Optimization in Neural Image Compression",
    "abstract": "Despite a short history, neural image codecs have been shown to surpass classical image codecs in terms of rate-distortion performance. However, most of them suffer from significantly longer decoding times, which hinders the practical applications of neural image codecs. This issue is especially pronounced when employing an effective yet time-consuming autoregressive context model since it would increase entropy decoding time by orders of magnitude. In this paper, unlike most previous works that pursue optimal RD performance while temporally overlooking the coding complexity, we make a systematical investigation on the rate-distortion-complexity (RDC) optimization in neural image compression. By quantifying the decoding complexity as a factor in the optimization goal, we are now able to precisely control the RDC trade-off and then demonstrate how the rate-distortion performance of neural image codecs could adapt to various complexity demands. Going beyond the investigation of RDC optimization, a variable-complexity neural codec is designed to leverage the spatial dependencies adaptively according to industrial demands, which supports fine-grained complexity adjustment by balancing the RDC tradeoff. By implementing this scheme in a powerful base model, we demonstrate the feasibility and flexibility of RDC optimization for neural image codecs.",
    "metadata": {
      "arxiv_id": "2305.07678",
      "title": "Exploring the Rate-Distortion-Complexity Optimization in Neural Image Compression",
      "summary": "Despite a short history, neural image codecs have been shown to surpass classical image codecs in terms of rate-distortion performance. However, most of them suffer from significantly longer decoding times, which hinders the practical applications of neural image codecs. This issue is especially pronounced when employing an effective yet time-consuming autoregressive context model since it would increase entropy decoding time by orders of magnitude. In this paper, unlike most previous works that pursue optimal RD performance while temporally overlooking the coding complexity, we make a systematical investigation on the rate-distortion-complexity (RDC) optimization in neural image compression. By quantifying the decoding complexity as a factor in the optimization goal, we are now able to precisely control the RDC trade-off and then demonstrate how the rate-distortion performance of neural image codecs could adapt to various complexity demands. Going beyond the investigation of RDC optimization, a variable-complexity neural codec is designed to leverage the spatial dependencies adaptively according to industrial demands, which supports fine-grained complexity adjustment by balancing the RDC tradeoff. By implementing this scheme in a powerful base model, we demonstrate the feasibility and flexibility of RDC optimization for neural image codecs.",
      "authors": [
        "Yixin Gao",
        "Runsen Feng",
        "Zongyu Guo",
        "Zhibo Chen"
      ],
      "published": "2023-05-12T03:56:25Z",
      "updated": "2023-05-12T03:56:25Z",
      "categories": [
        "eess.IV",
        "cs.IT",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.07678v1",
      "landing_url": "https://arxiv.org/abs/2305.07678v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.07678"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on neural image compression and rate-distortion-complexity, not on Spoken Language Models or speech-language modeling, so it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on neural image compression and rate-distortion-complexity, not on Spoken Language Models or speech-language modeling, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on neural image compression optimization involving rate-distortion-complexity trade-offs in image codecs, with no mention or focus on spoken language models, speech language modeling, or related speech interaction techniques as required by the inclusion criteria. Therefore, it does not meet the core thematic requirements and is outside the study scope focused on speech-centric language modeling and interaction systems.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on neural image compression optimization involving rate-distortion-complexity trade-offs in image codecs, with no mention or focus on spoken language models, speech language modeling, or related speech interaction techniques as required by the inclusion criteria. Therefore, it does not meet the core thematic requirements and is outside the study scope focused on speech-centric language modeling and interaction systems.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Streaming 360-degree VR Video with Statistical QoS Provisioning in mmWave Networks from Delay and Rate Perspectives",
    "abstract": "Millimeter-wave(mmWave) technology has emerged as a promising enabler for unleashing the full potential of 360-degree virtual reality (VR). However, the explosive growth of VR services, coupled with the reliability issues of mmWave communications, poses enormous challenges in terms of wireless resource and quality-of-service (QoS) provisioning for mmWave-enabled 360-degree VR. In this paper, we propose an innovative 360-degree VR streaming architecture that addresses three under-exploited issues: overlapping field-of-views (FoVs), statistical QoS provisioning (SQP), and loss-tolerant active data discarding. Specifically, an overlapping FoV-based optimal joint unicast and multicast (JUM) task assignment scheme is designed to implement the non-redundant task assignments, thereby conserving wireless resources remarkably. Furthermore, leveraging stochastic network calculus, we develop a comprehensive SQP theoretical framework that encompasses two SQP schemes from delay and rate perspectives. Additionally, a corresponding optimal adaptive joint time-slot allocation and active-discarding (ADAPT-JTAAT) transmission scheme is proposed to minimize resource consumption while guaranteeing diverse statistical QoS requirements under loss-intolerant and loss-tolerant scenarios from delay and rate perspectives, respectively. Extensive simulations demonstrate the effectiveness of the designed overlapping FoV-based JUM optimal task assignment scheme. Comparisons with six baseline schemes validate that the proposed optimal ADAPTJTAAT transmission scheme can achieve superior SQP performance in resource utilization, flexible rate control, and robust queue behaviors.",
    "metadata": {
      "arxiv_id": "2305.07935",
      "title": "Streaming 360-degree VR Video with Statistical QoS Provisioning in mmWave Networks from Delay and Rate Perspectives",
      "summary": "Millimeter-wave(mmWave) technology has emerged as a promising enabler for unleashing the full potential of 360-degree virtual reality (VR). However, the explosive growth of VR services, coupled with the reliability issues of mmWave communications, poses enormous challenges in terms of wireless resource and quality-of-service (QoS) provisioning for mmWave-enabled 360-degree VR. In this paper, we propose an innovative 360-degree VR streaming architecture that addresses three under-exploited issues: overlapping field-of-views (FoVs), statistical QoS provisioning (SQP), and loss-tolerant active data discarding. Specifically, an overlapping FoV-based optimal joint unicast and multicast (JUM) task assignment scheme is designed to implement the non-redundant task assignments, thereby conserving wireless resources remarkably. Furthermore, leveraging stochastic network calculus, we develop a comprehensive SQP theoretical framework that encompasses two SQP schemes from delay and rate perspectives. Additionally, a corresponding optimal adaptive joint time-slot allocation and active-discarding (ADAPT-JTAAT) transmission scheme is proposed to minimize resource consumption while guaranteeing diverse statistical QoS requirements under loss-intolerant and loss-tolerant scenarios from delay and rate perspectives, respectively. Extensive simulations demonstrate the effectiveness of the designed overlapping FoV-based JUM optimal task assignment scheme. Comparisons with six baseline schemes validate that the proposed optimal ADAPTJTAAT transmission scheme can achieve superior SQP performance in resource utilization, flexible rate control, and robust queue behaviors.",
      "authors": [
        "Yuang Chen",
        "Hancheng Lu",
        "Langtian Qin",
        "Chang Wu",
        "Chang Wen Chen"
      ],
      "published": "2023-05-13T14:57:27Z",
      "updated": "2023-05-13T14:57:27Z",
      "categories": [
        "cs.IT",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.07935v1",
      "landing_url": "https://arxiv.org/abs/2305.07935v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.07935"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此研究聚焦於毫米波VR串流、QoS、任務分配及時延/速率的統計保證，未涉及語音語言模型或語音互動，因此不符合包含條件，屬於排除範疇。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此研究聚焦於毫米波VR串流、QoS、任務分配及時延/速率的統計保證，未涉及語音語言模型或語音互動，因此不符合包含條件，屬於排除範疇。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on streaming 360-degree VR video over mmWave networks, addressing wireless resource management and statistical QoS provisioning related to delay and rate in VR applications. It does not address spoken language models, speech language modeling, or related core speech-based modeling and interaction aspects required by the inclusion criteria. There is no discussion of speech or language modeling as a core concern, nor is there reference to full-duplex or turn-taking speech interactions, synchronization, or speech-based evaluation metrics relevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on streaming 360-degree VR video over mmWave networks, addressing wireless resource management and statistical QoS provisioning related to delay and rate in VR applications. It does not address spoken language models, speech language modeling, or related core speech-based modeling and interaction aspects required by the inclusion criteria. There is no discussion of speech or language modeling as a core concern, nor is there reference to full-duplex or turn-taking speech interactions, synchronization, or speech-based evaluation metrics relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Strix: An End-to-End Streaming Architecture with Two-Level Ciphertext Batching for Fully Homomorphic Encryption with Programmable Bootstrapping",
    "abstract": "Homomorphic encryption (HE) enables computations on encrypted data by concealing information under noise for security. However, the process of bootstrapping, which resets the noise level in the ciphertext, is computationally expensive and requires a large bootstrapping key. The TFHE scheme offers a faster and programmable bootstrapping algorithm called PBS, crucial for security-focused applications like machine learning. Nevertheless, the current TFHE scheme lacks support for ciphertext packing, resulting in low throughput. This work thoroughly analyzes TFHE bootstrapping, identifies the bottleneck in GPUs caused by the blind rotation fragmentation problem, and proposes a hardware TFHE accelerator called Strix. Strix introduces a two-level batching approach to enhance the batch size in PBS, utilizes a specialized microarchitecture for efficient streaming data processing, and incorporates a fully-pipelined FFT microarchitecture to improve performance. It achieves significantly higher throughput than state-of-the-art implementations on both CPUs and GPUs, outperforming existing TFHE accelerators by a factor of 7.4.",
    "metadata": {
      "arxiv_id": "2305.11423",
      "title": "Strix: An End-to-End Streaming Architecture with Two-Level Ciphertext Batching for Fully Homomorphic Encryption with Programmable Bootstrapping",
      "summary": "Homomorphic encryption (HE) enables computations on encrypted data by concealing information under noise for security. However, the process of bootstrapping, which resets the noise level in the ciphertext, is computationally expensive and requires a large bootstrapping key. The TFHE scheme offers a faster and programmable bootstrapping algorithm called PBS, crucial for security-focused applications like machine learning. Nevertheless, the current TFHE scheme lacks support for ciphertext packing, resulting in low throughput. This work thoroughly analyzes TFHE bootstrapping, identifies the bottleneck in GPUs caused by the blind rotation fragmentation problem, and proposes a hardware TFHE accelerator called Strix. Strix introduces a two-level batching approach to enhance the batch size in PBS, utilizes a specialized microarchitecture for efficient streaming data processing, and incorporates a fully-pipelined FFT microarchitecture to improve performance. It achieves significantly higher throughput than state-of-the-art implementations on both CPUs and GPUs, outperforming existing TFHE accelerators by a factor of 7.4.",
      "authors": [
        "Adiwena Putra",
        "Prasetiyo",
        "Yi Chen",
        "John Kim",
        "Joo-Young Kim"
      ],
      "published": "2023-05-19T04:40:04Z",
      "updated": "2023-05-19T04:40:04Z",
      "categories": [
        "cs.CR",
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.11423v1",
      "landing_url": "https://arxiv.org/abs/2305.11423v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.11423"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about a hardware accelerator for fully homomorphic encryption and does not discuss Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about a hardware accelerator for fully homomorphic encryption and does not discuss Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on homomorphic encryption and hardware acceleration for secure computation, without any mention of spoken language models, speech language models, or related core concepts such as language modeling of speech or full-duplex interactions. It does not address speech or voice as a modeling or inference object nor discuss training, architecture, or evaluation within the spoken language model domain.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on homomorphic encryption and hardware acceleration for secure computation, without any mention of spoken language models, speech language models, or related core concepts such as language modeling of speech or full-duplex interactions. It does not address speech or voice as a modeling or inference object nor discuss training, architecture, or evaluation within the spoken language model domain.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DiffCap: Exploring Continuous Diffusion on Image Captioning",
    "abstract": "Current image captioning works usually focus on generating descriptions in an autoregressive manner. However, there are limited works that focus on generating descriptions non-autoregressively, which brings more decoding diversity. Inspired by the success of diffusion models on generating natural-looking images, we propose a novel method DiffCap to apply continuous diffusions on image captioning. Unlike image generation where the output is fixed-size and continuous, image description length varies with discrete tokens. Our method transforms discrete tokens in a natural way and applies continuous diffusion on them to successfully fuse extracted image features for diffusion caption generation. Our experiments on COCO dataset demonstrate that our method uses a much simpler structure to achieve comparable results to the previous non-autoregressive works. Apart from quality, an intriguing property of DiffCap is its high diversity during generation, which is missing from many autoregressive models. We believe our method on fusing multimodal features in diffusion language generation will inspire more researches on multimodal language generation tasks for its simplicity and decoding flexibility.",
    "metadata": {
      "arxiv_id": "2305.12144",
      "title": "DiffCap: Exploring Continuous Diffusion on Image Captioning",
      "summary": "Current image captioning works usually focus on generating descriptions in an autoregressive manner. However, there are limited works that focus on generating descriptions non-autoregressively, which brings more decoding diversity. Inspired by the success of diffusion models on generating natural-looking images, we propose a novel method DiffCap to apply continuous diffusions on image captioning. Unlike image generation where the output is fixed-size and continuous, image description length varies with discrete tokens. Our method transforms discrete tokens in a natural way and applies continuous diffusion on them to successfully fuse extracted image features for diffusion caption generation. Our experiments on COCO dataset demonstrate that our method uses a much simpler structure to achieve comparable results to the previous non-autoregressive works. Apart from quality, an intriguing property of DiffCap is its high diversity during generation, which is missing from many autoregressive models. We believe our method on fusing multimodal features in diffusion language generation will inspire more researches on multimodal language generation tasks for its simplicity and decoding flexibility.",
      "authors": [
        "Yufeng He",
        "Zefan Cai",
        "Xu Gan",
        "Baobao Chang"
      ],
      "published": "2023-05-20T09:02:10Z",
      "updated": "2023-05-20T09:02:10Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.12144v1",
      "landing_url": "https://arxiv.org/abs/2305.12144v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.12144"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: This work focuses on diffusion-based image captioning and multimodal feature fusion, with no emphasis on Spoken Language Models or speech as the core modeling object, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: This work focuses on diffusion-based image captioning and multimodal feature fusion, with no emphasis on Spoken Language Models or speech as the core modeling object, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on applying diffusion models for image captioning, which is a multimodal task involving images and text, not centered on spoken language models or speech signal modeling; it does not address spoken language modeling, full-duplex capabilities, or speech-based interaction as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on applying diffusion models for image captioning, which is a multimodal task involving images and text, not centered on spoken language models or speech signal modeling; it does not address spoken language modeling, full-duplex capabilities, or speech-based interaction as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "GRACE: Loss-Resilient Real-Time Video through Neural Codecs",
    "abstract": "In real-time video communication, retransmitting lost packets over high-latency networks is not viable due to strict latency requirements. To counter packet losses without retransmission, two primary strategies are employed -- encoder-based forward error correction (FEC) and decoder-based error concealment. The former encodes data with redundancy before transmission, yet determining the optimal redundancy level in advance proves challenging. The latter reconstructs video from partially received frames, but dividing a frame into independently coded partitions inherently compromises compression efficiency, and the lost information cannot be effectively recovered by the decoder without adapting the encoder. We present a loss-resilient real-time video system called GRACE, which preserves the user's quality of experience (QoE) across a wide range of packet losses through a new neural video codec. Central to GRACE's enhanced loss resilience is its joint training of the neural encoder and decoder under a spectrum of simulated packet losses. In lossless scenarios, GRACE achieves video quality on par with conventional codecs (e.g., H.265). As the loss rate escalates, GRACE exhibits a more graceful, less pronounced decline in quality, consistently outperforming other loss-resilient schemes. Through extensive evaluation on various videos and real network traces, we demonstrate that GRACE reduces undecodable frames by 95% and stall duration by 90% compared with FEC, while markedly boosting video quality over error concealment methods. In a user study with 240 crowdsourced participants and 960 subjective ratings, GRACE registers a 38% higher mean opinion score (MOS) than other baselines.",
    "metadata": {
      "arxiv_id": "2305.12333",
      "title": "GRACE: Loss-Resilient Real-Time Video through Neural Codecs",
      "summary": "In real-time video communication, retransmitting lost packets over high-latency networks is not viable due to strict latency requirements. To counter packet losses without retransmission, two primary strategies are employed -- encoder-based forward error correction (FEC) and decoder-based error concealment. The former encodes data with redundancy before transmission, yet determining the optimal redundancy level in advance proves challenging. The latter reconstructs video from partially received frames, but dividing a frame into independently coded partitions inherently compromises compression efficiency, and the lost information cannot be effectively recovered by the decoder without adapting the encoder. We present a loss-resilient real-time video system called GRACE, which preserves the user's quality of experience (QoE) across a wide range of packet losses through a new neural video codec. Central to GRACE's enhanced loss resilience is its joint training of the neural encoder and decoder under a spectrum of simulated packet losses. In lossless scenarios, GRACE achieves video quality on par with conventional codecs (e.g., H.265). As the loss rate escalates, GRACE exhibits a more graceful, less pronounced decline in quality, consistently outperforming other loss-resilient schemes. Through extensive evaluation on various videos and real network traces, we demonstrate that GRACE reduces undecodable frames by 95% and stall duration by 90% compared with FEC, while markedly boosting video quality over error concealment methods. In a user study with 240 crowdsourced participants and 960 subjective ratings, GRACE registers a 38% higher mean opinion score (MOS) than other baselines.",
      "authors": [
        "Yihua Cheng",
        "Ziyi Zhang",
        "Hanchen Li",
        "Anton Arapin",
        "Yue Zhang",
        "Qizheng Zhang",
        "Yuhan Liu",
        "Xu Zhang",
        "Francis Y. Yan",
        "Amrita Mazumdar",
        "Nick Feamster",
        "Junchen Jiang"
      ],
      "published": "2023-05-21T03:50:44Z",
      "updated": "2024-03-12T21:40:53Z",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.12333v4",
      "landing_url": "https://arxiv.org/abs/2305.12333v4",
      "doi": "https://doi.org/10.48550/arXiv.2305.12333"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on loss-resilient real-time video and neural codecs, with no focus on Spoken Language Models or speech-language modeling, nor on interactive speech dialogue or evaluation metrics for speech interaction; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on loss-resilient real-time video and neural codecs, with no focus on Spoken Language Models or speech-language modeling, nor on interactive speech dialogue or evaluation metrics for speech interaction; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a neural video codec system for loss-resilient real-time video communication, which is unrelated to spoken language models or speech language modeling; it does not address topics such as SLMs, full-duplex speech interaction, voice language modeling, or related evaluation; thus, it does not meet the inclusion criteria and falls outside the scope of the research topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a neural video codec system for loss-resilient real-time video communication, which is unrelated to spoken language models or speech language modeling; it does not address topics such as SLMs, full-duplex speech interaction, voice language modeling, or related evaluation; thus, it does not meet the inclusion criteria and falls outside the scope of the research topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Language Models for German Text Simplification: Overcoming Parallel Data Scarcity through Style-specific Pre-training",
    "abstract": "Automatic text simplification systems help to reduce textual information barriers on the internet. However, for languages other than English, only few parallel data to train these systems exists. We propose a two-step approach to overcome this data scarcity issue. First, we fine-tuned language models on a corpus of German Easy Language, a specific style of German. Then, we used these models as decoders in a sequence-to-sequence simplification task. We show that the language models adapt to the style characteristics of Easy Language and output more accessible texts. Moreover, with the style-specific pre-training, we reduced the number of trainable parameters in text simplification models. Hence, less parallel data is sufficient for training. Our results indicate that pre-training on unaligned data can reduce the required parallel data while improving the performance on downstream tasks.",
    "metadata": {
      "arxiv_id": "2305.12908",
      "title": "Language Models for German Text Simplification: Overcoming Parallel Data Scarcity through Style-specific Pre-training",
      "summary": "Automatic text simplification systems help to reduce textual information barriers on the internet. However, for languages other than English, only few parallel data to train these systems exists. We propose a two-step approach to overcome this data scarcity issue. First, we fine-tuned language models on a corpus of German Easy Language, a specific style of German. Then, we used these models as decoders in a sequence-to-sequence simplification task. We show that the language models adapt to the style characteristics of Easy Language and output more accessible texts. Moreover, with the style-specific pre-training, we reduced the number of trainable parameters in text simplification models. Hence, less parallel data is sufficient for training. Our results indicate that pre-training on unaligned data can reduce the required parallel data while improving the performance on downstream tasks.",
      "authors": [
        "Miriam Anschütz",
        "Joshua Oehms",
        "Thomas Wimmer",
        "Bartłomiej Jezierski",
        "Georg Groh"
      ],
      "published": "2023-05-22T10:41:30Z",
      "updated": "2023-05-22T10:41:30Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.12908v1",
      "landing_url": "https://arxiv.org/abs/2305.12908v1",
      "doi": "https://doi.org/10.18653/v1/2023.findings-acl.74"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本文主題為德語文本簡化，著重於語言模型的文字處理與風格特定預訓練，未以語音為核心的語言模型或語音互動、對話地建模，因此不符合以 Spoken Language Models/語音語言建模為核心的納入條件，屬於排除範圍。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本文主題為德語文本簡化，著重於語言模型的文字處理與風格特定預訓練，未以語音為核心的語言模型或語音互動、對話地建模，因此不符合以 Spoken Language Models/語音語言建模為核心的納入條件，屬於排除範圍。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on text simplification of German language using language models, specifically dealing with text data and style-specific pre-training, with no mention of spoken language models, speech as core modeling objects, full-duplex spoken language interaction, or evaluation of live speech interaction capabilities. It clearly does not address speech-level language modeling or any of the specialized criteria about spoken or full-duplex language models outlined in the inclusion criteria. Therefore, it does not fit the thematic domain concerned with spoken language models or speech language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on text simplification of German language using language models, specifically dealing with text data and style-specific pre-training, with no mention of spoken language models, speech as core modeling objects, full-duplex spoken language interaction, or evaluation of live speech interaction capabilities. It clearly does not address speech-level language modeling or any of the specialized criteria about spoken or full-duplex language models outlined in the inclusion criteria. Therefore, it does not fit the thematic domain concerned with spoken language models or speech language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Human-in-the-Loop Approach for Information Extraction from Privacy Policies under Data Scarcity",
    "abstract": "Machine-readable representations of privacy policies are door openers for a broad variety of novel privacy-enhancing and, in particular, transparency-enhancing technologies (TETs). In order to generate such representations, transparency information needs to be extracted from written privacy policies. However, respective manual annotation and extraction processes are laborious and require expert knowledge. Approaches for fully automated annotation, in turn, have so far not succeeded due to overly high error rates in the specific domain of privacy policies. In the end, a lack of properly annotated privacy policies and respective machine-readable representations persists and enduringly hinders the development and establishment of novel technical approaches fostering policy perception and data subject informedness. In this work, we present a prototype system for a `Human-in-the-Loop' approach to privacy policy annotation that integrates ML-generated suggestions and ultimately human annotation decisions. We propose an ML-based suggestion system specifically tailored to the constraint of data scarcity prevalent in the domain of privacy policy annotation. On this basis, we provide meaningful predictions to users thereby streamlining the annotation process. Additionally, we also evaluate our approach through a prototypical implementation to show that our ML-based extraction approach provides superior performance over other recently used extraction models for legal documents.",
    "metadata": {
      "arxiv_id": "2305.15006",
      "title": "A Human-in-the-Loop Approach for Information Extraction from Privacy Policies under Data Scarcity",
      "summary": "Machine-readable representations of privacy policies are door openers for a broad variety of novel privacy-enhancing and, in particular, transparency-enhancing technologies (TETs). In order to generate such representations, transparency information needs to be extracted from written privacy policies. However, respective manual annotation and extraction processes are laborious and require expert knowledge. Approaches for fully automated annotation, in turn, have so far not succeeded due to overly high error rates in the specific domain of privacy policies. In the end, a lack of properly annotated privacy policies and respective machine-readable representations persists and enduringly hinders the development and establishment of novel technical approaches fostering policy perception and data subject informedness.\n  In this work, we present a prototype system for a `Human-in-the-Loop' approach to privacy policy annotation that integrates ML-generated suggestions and ultimately human annotation decisions. We propose an ML-based suggestion system specifically tailored to the constraint of data scarcity prevalent in the domain of privacy policy annotation. On this basis, we provide meaningful predictions to users thereby streamlining the annotation process. Additionally, we also evaluate our approach through a prototypical implementation to show that our ML-based extraction approach provides superior performance over other recently used extraction models for legal documents.",
      "authors": [
        "Michael Gebauer",
        "Faraz Maschhur",
        "Nicola Leschke",
        "Elias Grünewald",
        "Frank Pallas"
      ],
      "published": "2023-05-24T10:45:26Z",
      "updated": "2023-05-31T09:58:15Z",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.15006v2",
      "landing_url": "https://arxiv.org/abs/2305.15006v2",
      "doi": "https://doi.org/10.48550/arXiv.2305.15006"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on ML-assisted information extraction from privacy policy text with human-in-the-loop; it does not involve Spoken Language Models or speech-based modeling, thus fails the inclusion criteria for SLM/FD-SLM core focus.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on ML-assisted information extraction from privacy policy text with human-in-the-loop; it does not involve Spoken Language Models or speech-based modeling, thus fails the inclusion criteria for SLM/FD-SLM core focus.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on extracting information from privacy policies using a human-in-the-loop machine learning approach, with no mention of spoken language models, speech language modeling, full-duplex interaction, or related concepts specified in the inclusion criteria. It does not address speech as the core modeling object, nor does it discuss synchronization, real-time interaction, or speech modeling advances. Therefore, it falls outside the specified research topic and inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on extracting information from privacy policies using a human-in-the-loop machine learning approach, with no mention of spoken language models, speech language modeling, full-duplex interaction, or related concepts specified in the inclusion criteria. It does not address speech as the core modeling object, nor does it discuss synchronization, real-time interaction, or speech modeling advances. Therefore, it falls outside the specified research topic and inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Bistatic OFDM-based Joint Radar-Communication: Synchronization, Data Communication and Sensing",
    "abstract": "This article introduces a bistatic joint radar-communication (RadCom) system based on orthogonal frequency-division multiplexing (OFDM). In this context, the adopted OFDM frame structure is described and system model encompassing time, frequency, and sampling synchronization mismatches between the transmitter and receiver of the bistatic system is outlined. Next, the signal processing approaches for synchronization and communication are discussed, and radar sensing processing approaches using either only pilots or a reconstructed OFDM frame based on the estimated receive communication data are presented. Finally, proof-of-concept measurement results are presented to validate the investigated system and a trade-off between frame size and the performance of the aforementioned processing steps is observed.",
    "metadata": {
      "arxiv_id": "2305.15058",
      "title": "Bistatic OFDM-based Joint Radar-Communication: Synchronization, Data Communication and Sensing",
      "summary": "This article introduces a bistatic joint radar-communication (RadCom) system based on orthogonal frequency-division multiplexing (OFDM). In this context, the adopted OFDM frame structure is described and system model encompassing time, frequency, and sampling synchronization mismatches between the transmitter and receiver of the bistatic system is outlined. Next, the signal processing approaches for synchronization and communication are discussed, and radar sensing processing approaches using either only pilots or a reconstructed OFDM frame based on the estimated receive communication data are presented. Finally, proof-of-concept measurement results are presented to validate the investigated system and a trade-off between frame size and the performance of the aforementioned processing steps is observed.",
      "authors": [
        "Lucas Giroto de Oliveira",
        "David Brunner",
        "Axel Diewald",
        "Charlotte Muth",
        "Laurent Schmalen",
        "Thomas Zwick",
        "Benjamin Nuss"
      ],
      "published": "2023-05-24T11:48:19Z",
      "updated": "2023-05-24T11:48:19Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.15058v1",
      "landing_url": "https://arxiv.org/abs/2305.15058v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.15058"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "It focuses on bistatic radar-communication with OFDM and synchronization, not on Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "It focuses on bistatic radar-communication with OFDM and synchronization, not on Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on a bistatic joint radar-communication system using OFDM, discussing synchronization, communication, and sensing, but it does not focus on Spoken Language Models or speech language modeling as defined in the inclusion criteria; it lacks discussion on speech as the core modeling target, full-duplex spoken language interactions, or related evaluation metrics relevant to the task's focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on a bistatic joint radar-communication system using OFDM, discussing synchronization, communication, and sensing, but it does not focus on Spoken Language Models or speech language modeling as defined in the inclusion criteria; it lacks discussion on speech as the core modeling target, full-duplex spoken language interactions, or related evaluation metrics relevant to the task's focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
    "abstract": "We present Spectron, a novel approach to adapting pre-trained large language models (LLMs) to perform spoken question answering (QA) and speech continuation. By endowing the LLM with a pre-trained speech encoder, our model becomes able to take speech inputs and generate speech outputs. The entire system is trained end-to-end and operates directly on spectrograms, simplifying our architecture. Key to our approach is a training objective that jointly supervises speech recognition, text continuation, and speech synthesis using only paired speech-text pairs, enabling a `cross-modal' chain-of-thought within a single decoding pass. Our method surpasses existing spoken language models in speaker preservation and semantic coherence. Furthermore, the proposed model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken QA datasets. We release our audio samples (https://michelleramanovich.github.io/spectron/spectron) and spoken QA dataset (https://github.com/google-research-datasets/LLAMA1-Test-Set).",
    "metadata": {
      "arxiv_id": "2305.15255",
      "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
      "summary": "We present Spectron, a novel approach to adapting pre-trained large language models (LLMs) to perform spoken question answering (QA) and speech continuation. By endowing the LLM with a pre-trained speech encoder, our model becomes able to take speech inputs and generate speech outputs. The entire system is trained end-to-end and operates directly on spectrograms, simplifying our architecture. Key to our approach is a training objective that jointly supervises speech recognition, text continuation, and speech synthesis using only paired speech-text pairs, enabling a `cross-modal' chain-of-thought within a single decoding pass. Our method surpasses existing spoken language models in speaker preservation and semantic coherence. Furthermore, the proposed model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken QA datasets. We release our audio samples (https://michelleramanovich.github.io/spectron/spectron) and spoken QA dataset (https://github.com/google-research-datasets/LLAMA1-Test-Set).",
      "authors": [
        "Eliya Nachmani",
        "Alon Levkovitch",
        "Roy Hirsch",
        "Julian Salazar",
        "Chulayuth Asawaroengchai",
        "Soroosh Mariooryad",
        "Ehud Rivlin",
        "RJ Skerry-Ryan",
        "Michelle Tadmor Ramanovich"
      ],
      "published": "2023-05-24T15:39:43Z",
      "updated": "2024-05-31T01:29:27Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.15255v4",
      "landing_url": "https://arxiv.org/abs/2305.15255v4",
      "doi": "https://doi.org/10.48550/arXiv.2305.15255"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on spoken language modeling by integrating speech inputs/outputs with LLMs, discusses architecture and end-to-end training/evaluation on spectrograms, and is within the past 3 years, meeting all inclusion criteria and none of the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper centers on spoken language modeling by integrating speech inputs/outputs with LLMs, discusses architecture and end-to-end training/evaluation on spectrograms, and is within the past 3 years, meeting all inclusion criteria and none of the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a novel Spoken Language Model (Spectron) that jointly processes speech inputs and generates speech outputs using a Spectrogram-powered Large Language Model, with an end-to-end architecture emphasizing speech modeling rather than just text-based NLP. Although the abstract does not explicitly mention full-duplex interaction or real-time behavior arbitration features such as cognitive parallelism or low latency, it clearly discusses model architecture, training, and evaluation in the context of spoken question answering and speech continuation tasks, fitting within the recent three-year timeframe and embodying spoken language modeling. Given this, the study partially aligns with the inclusion criteria but lacks explicit mentions of full-duplex or synchronous interaction capabilities, which could be grounds for excluding under the exclusion criteria. However, it does not fall into outright exclusion categories such as focusing solely on ASR/TTS or purely text-based NLP over speech. Therefore, this work is better to include for its clear contribution to speech-based language modeling with an advanced architecture and evaluation.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The paper presents a novel Spoken Language Model (Spectron) that jointly processes speech inputs and generates speech outputs using a Spectrogram-powered Large Language Model, with an end-to-end architecture emphasizing speech modeling rather than just text-based NLP. Although the abstract does not explicitly mention full-duplex interaction or real-time behavior arbitration features such as cognitive parallelism or low latency, it clearly discusses model architecture, training, and evaluation in the context of spoken question answering and speech continuation tasks, fitting within the recent three-year timeframe and embodying spoken language modeling. Given this, the study partially aligns with the inclusion criteria but lacks explicit mentions of full-duplex or synchronous interaction capabilities, which could be grounds for excluding under the exclusion criteria. However, it does not fall into outright exclusion categories such as focusing solely on ASR/TTS or purely text-based NLP over speech. Therefore, this work is better to include for its clear contribution to speech-based language modeling with an advanced architecture and evaluation.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LLHR: Low Latency and High Reliability CNN Distributed Inference for Resource-Constrained UAV Swarms",
    "abstract": "Recently, Unmanned Aerial Vehicles (UAVs) have shown impressive performance in many critical applications, such as surveillance, search and rescue operations, environmental monitoring, etc. In many of these applications, the UAVs capture images as well as other sensory data and then send the data processing requests to remote servers. Nevertheless, this approach is not always practical in real-time-based applications due to unstable connections, limited bandwidth, limited energy, and strict end-to-end latency. One promising solution is to divide the inference requests into subtasks that can be distributed among UAVs in a swarm based on the available resources. Moreover, these tasks create intermediate results that need to be transmitted reliably as the swarm moves to cover the area. Our system model deals with real-time requests, aiming to find the optimal transmission power that guarantees higher reliability and low latency. We formulate the Low Latency and High-Reliability (LLHR) distributed inference as an optimization problem, and due to the complexity of the problem, we divide it into three subproblems. In the first subproblem, we find the optimal transmit power of the connected UAVs with guaranteed transmission reliability. The second subproblem aims to find the optimal positions of the UAVs in the grid, while the last subproblem finds the optimal placement of the CNN layers in the available UAVs. We conduct extensive simulations and compare our work to two baseline models demonstrating that our model outperforms the competing models.",
    "metadata": {
      "arxiv_id": "2305.15858",
      "title": "LLHR: Low Latency and High Reliability CNN Distributed Inference for Resource-Constrained UAV Swarms",
      "summary": "Recently, Unmanned Aerial Vehicles (UAVs) have shown impressive performance in many critical applications, such as surveillance, search and rescue operations, environmental monitoring, etc. In many of these applications, the UAVs capture images as well as other sensory data and then send the data processing requests to remote servers. Nevertheless, this approach is not always practical in real-time-based applications due to unstable connections, limited bandwidth, limited energy, and strict end-to-end latency. One promising solution is to divide the inference requests into subtasks that can be distributed among UAVs in a swarm based on the available resources. Moreover, these tasks create intermediate results that need to be transmitted reliably as the swarm moves to cover the area. Our system model deals with real-time requests, aiming to find the optimal transmission power that guarantees higher reliability and low latency. We formulate the Low Latency and High-Reliability (LLHR) distributed inference as an optimization problem, and due to the complexity of the problem, we divide it into three subproblems. In the first subproblem, we find the optimal transmit power of the connected UAVs with guaranteed transmission reliability. The second subproblem aims to find the optimal positions of the UAVs in the grid, while the last subproblem finds the optimal placement of the CNN layers in the available UAVs. We conduct extensive simulations and compare our work to two baseline models demonstrating that our model outperforms the competing models.",
      "authors": [
        "Marwan Dhuheir",
        "Aiman Erbad",
        "Sinan Sabeeh"
      ],
      "published": "2023-05-25T08:47:16Z",
      "updated": "2023-05-25T08:47:16Z",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.15858v1",
      "landing_url": "https://arxiv.org/abs/2305.15858v1",
      "doi": "https://doi.org/10.1109/WCNC55385.2023.10118908"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The article focuses on distributed CNN inference for UAV swarms and edge/latency optimization, with no reference to Spoken Language Models or speech-language modeling, so it fails the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The article focuses on distributed CNN inference for UAV swarms and edge/latency optimization, with no reference to Spoken Language Models or speech-language modeling, so it fails the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on distributed inference for UAV swarms concerning CNN-based tasks related to image and sensory data processing; it does not address Spoken Language Models, speech language modeling, or full-duplex spoken interaction, nor does it discuss speech modeling, training, or evaluation from a speech language perspective. Therefore, it does not meet the inclusion criteria focused on speech-based language modeling research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on distributed inference for UAV swarms concerning CNN-based tasks related to image and sensory data processing; it does not address Spoken Language Models, speech language modeling, or full-duplex spoken interaction, nor does it discuss speech modeling, training, or evaluation from a speech language perspective. Therefore, it does not meet the inclusion criteria focused on speech-based language modeling research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation",
    "abstract": "Recent research shows a big convergence in model architecture, training objectives, and inference methods across various tasks for different modalities. In this paper, we propose VioLA, a single auto-regressive Transformer decoder-only network that unifies various cross-modal tasks involving speech and text, such as speech-to-text, text-to-text, text-to-speech, and speech-to-speech tasks, as a conditional codec language model task via multi-task learning framework. To accomplish this, we first convert all the speech utterances to discrete tokens (similar to the textual data) using an offline neural codec encoder. In such a way, all these tasks are converted to token-based sequence conversion problems, which can be naturally handled with one conditional language model. We further integrate task IDs (TID) and language IDs (LID) into the proposed model to enhance the modeling capability of handling different languages and tasks. Experimental results demonstrate that the proposed VioLA model can support both single-modal and cross-modal tasks well, and the decoder-only model achieves a comparable and even better performance than the strong baselines.",
    "metadata": {
      "arxiv_id": "2305.16107",
      "title": "VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation",
      "summary": "Recent research shows a big convergence in model architecture, training objectives, and inference methods across various tasks for different modalities. In this paper, we propose VioLA, a single auto-regressive Transformer decoder-only network that unifies various cross-modal tasks involving speech and text, such as speech-to-text, text-to-text, text-to-speech, and speech-to-speech tasks, as a conditional codec language model task via multi-task learning framework. To accomplish this, we first convert all the speech utterances to discrete tokens (similar to the textual data) using an offline neural codec encoder. In such a way, all these tasks are converted to token-based sequence conversion problems, which can be naturally handled with one conditional language model. We further integrate task IDs (TID) and language IDs (LID) into the proposed model to enhance the modeling capability of handling different languages and tasks. Experimental results demonstrate that the proposed VioLA model can support both single-modal and cross-modal tasks well, and the decoder-only model achieves a comparable and even better performance than the strong baselines.",
      "authors": [
        "Tianrui Wang",
        "Long Zhou",
        "Ziqiang Zhang",
        "Yu Wu",
        "Shujie Liu",
        "Yashesh Gaur",
        "Zhuo Chen",
        "Jinyu Li",
        "Furu Wei"
      ],
      "published": "2023-05-25T14:39:47Z",
      "updated": "2023-05-25T14:39:47Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.16107v1",
      "landing_url": "https://arxiv.org/abs/2305.16107v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.16107"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on a unified spoken-language modeling framework (speech and text) with cross-modal tasks (speech-to-text, speech-to-speech, text-to-speech, translation) and discusses architecture, training, and evaluation, within the last 3 years, and uses speech-relevant language-modeling concepts (tokenizing speech via a neural codec, decoder-only LM, TID/LID), aligning with the spoken language model criteria and not falling under the exclusion cases.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This item centers on a unified spoken-language modeling framework (speech and text) with cross-modal tasks (speech-to-text, speech-to-speech, text-to-speech, translation) and discusses architecture, training, and evaluation, within the last 3 years, and uses speech-relevant language-modeling concepts (tokenizing speech via a neural codec, decoder-only LM, TID/LID), aligning with the spoken language model criteria and not falling under the exclusion cases.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces VioLA, a unified codec language model addressing multiple speech and text tasks by treating speech as discrete tokens and applying a conditional language model to sequence conversion problems. However, though it covers spoken language models and speech as a modeling object, it does not explicitly focus on full-duplex spoken language models, synchronous interaction, cognitive parallelism, or dynamic interaction behaviors like backchanneling or interruption. The evaluation does not mention metrics related to real-time interaction, overlap handling, or behavior arbitration. Hence, it only partially meets the inclusion criteria and lacks key aspects of the defined research scope on full-duplex spoken language models and interaction dynamics, putting its relevance into question.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper introduces VioLA, a unified codec language model addressing multiple speech and text tasks by treating speech as discrete tokens and applying a conditional language model to sequence conversion problems. However, though it covers spoken language models and speech as a modeling object, it does not explicitly focus on full-duplex spoken language models, synchronous interaction, cognitive parallelism, or dynamic interaction behaviors like backchanneling or interruption. The evaluation does not mention metrics related to real-time interaction, overlap handling, or behavior arbitration. Hence, it only partially meets the inclusion criteria and lacks key aspects of the defined research scope on full-duplex spoken language models and interaction dynamics, putting its relevance into question.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers' comments considered: Junior A argued inclusion (5) noting VioLA treats speech as discrete tokens and unifies speech/text tasks via a codec language model; Junior B raised a valid concern that the paper does not address full‑duplex interaction dynamics (overlap, backchanneling, low latency) and thus may not target FD‑SLM behaviors. Integrating these, VioLA nevertheless explicitly treats speech as the primary modeling object (converting audio to discrete codec tokens) and proposes a decoder‑only conditional language model with multi‑task training and experimental evaluation — satisfying the requirement that spoken-language modeling (language modeling of speech) is the core contribution and that architecture/training/evaluation are described. Although it does not cover full‑duplex interaction or real‑time overlap arbitration (which is outside its scope), that absence falls under desirable FD‑SLM extensions rather than exclusion, because the paper innovates on speech‑level language modeling rather than being merely an ASR/TTS pipeline. Therefore: 5 — absolutely include.",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers' comments considered: Junior A argued inclusion (5) noting VioLA treats speech as discrete tokens and unifies speech/text tasks via a codec language model; Junior B raised a valid concern that the paper does not address full‑duplex interaction dynamics (overlap, backchanneling, low latency) and thus may not target FD‑SLM behaviors. Integrating these, VioLA nevertheless explicitly treats speech as the primary modeling object (converting audio to discrete codec tokens) and proposes a decoder‑only conditional language model with multi‑task training and experimental evaluation — satisfying the requirement that spoken-language modeling (language modeling of speech) is the core contribution and that architecture/training/evaluation are described. Although it does not cover full‑duplex interaction or real‑time overlap arbitration (which is outside its scope), that absence falls under desirable FD‑SLM extensions rather than exclusion, because the paper innovates on speech‑level language modeling rather than being merely an ASR/TTS pipeline. Therefore: 5 — absolutely include.",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Interactive Segment Anything NeRF with Feature Imitation",
    "abstract": "This paper investigates the potential of enhancing Neural Radiance Fields (NeRF) with semantics to expand their applications. Although NeRF has been proven useful in real-world applications like VR and digital creation, the lack of semantics hinders interaction with objects in complex scenes. We propose to imitate the backbone feature of off-the-shelf perception models to achieve zero-shot semantic segmentation with NeRF. Our framework reformulates the segmentation process by directly rendering semantic features and only applying the decoder from perception models. This eliminates the need for expensive backbones and benefits 3D consistency. Furthermore, we can project the learned semantics onto extracted mesh surfaces for real-time interaction. With the state-of-the-art Segment Anything Model (SAM), our framework accelerates segmentation by 16 times with comparable mask quality. The experimental results demonstrate the efficacy and computational advantages of our approach. Project page: \\url{https://me.kiui.moe/san/}.",
    "metadata": {
      "arxiv_id": "2305.16233",
      "title": "Interactive Segment Anything NeRF with Feature Imitation",
      "summary": "This paper investigates the potential of enhancing Neural Radiance Fields (NeRF) with semantics to expand their applications. Although NeRF has been proven useful in real-world applications like VR and digital creation, the lack of semantics hinders interaction with objects in complex scenes. We propose to imitate the backbone feature of off-the-shelf perception models to achieve zero-shot semantic segmentation with NeRF. Our framework reformulates the segmentation process by directly rendering semantic features and only applying the decoder from perception models. This eliminates the need for expensive backbones and benefits 3D consistency. Furthermore, we can project the learned semantics onto extracted mesh surfaces for real-time interaction. With the state-of-the-art Segment Anything Model (SAM), our framework accelerates segmentation by 16 times with comparable mask quality. The experimental results demonstrate the efficacy and computational advantages of our approach. Project page: \\url{https://me.kiui.moe/san/}.",
      "authors": [
        "Xiaokang Chen",
        "Jiaxiang Tang",
        "Diwen Wan",
        "Jingbo Wang",
        "Gang Zeng"
      ],
      "published": "2023-05-25T16:44:51Z",
      "updated": "2023-05-25T16:44:51Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.16233v1",
      "landing_url": "https://arxiv.org/abs/2305.16233v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.16233"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on enhancing NeRF with semantics and segmentation using SAM, not on Spoken Language Models or speech-language modeling concepts, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on enhancing NeRF with semantics and segmentation using SAM, not on Spoken Language Models or speech-language modeling concepts, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on enhancing Neural Radiance Fields (NeRF) for semantic segmentation and interactive 3D scene understanding, which is a computer vision topic unrelated to Spoken Language Models or speech-based modeling as required by the inclusion criteria. It does not discuss spoken language modeling, full-duplex interaction, synchronization, or any speech-centric model architecture or evaluation. Thus, it does not meet the core inclusion criteria and falls outside the research scope defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on enhancing Neural Radiance Fields (NeRF) for semantic segmentation and interactive 3D scene understanding, which is a computer vision topic unrelated to Spoken Language Models or speech-based modeling as required by the inclusion criteria. It does not discuss spoken language modeling, full-duplex interaction, synchronization, or any speech-centric model architecture or evaluation. Thus, it does not meet the core inclusion criteria and falls outside the research scope defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Hierarchical Neural Memory Network for Low Latency Event Processing",
    "abstract": "This paper proposes a low latency neural network architecture for event-based dense prediction tasks. Conventional architectures encode entire scene contents at a fixed rate regardless of their temporal characteristics. Instead, the proposed network encodes contents at a proper temporal scale depending on its movement speed. We achieve this by constructing temporal hierarchy using stacked latent memories that operate at different rates. Given low latency event steams, the multi-level memories gradually extract dynamic to static scene contents by propagating information from the fast to the slow memory modules. The architecture not only reduces the redundancy of conventional architectures but also exploits long-term dependencies. Furthermore, an attention-based event representation efficiently encodes sparse event streams into the memory cells. We conduct extensive evaluations on three event-based dense prediction tasks, where the proposed approach outperforms the existing methods on accuracy and latency, while demonstrating effective event and image fusion capabilities. The code is available at https://hamarh.github.io/hmnet/",
    "metadata": {
      "arxiv_id": "2305.17852",
      "title": "Hierarchical Neural Memory Network for Low Latency Event Processing",
      "summary": "This paper proposes a low latency neural network architecture for event-based dense prediction tasks. Conventional architectures encode entire scene contents at a fixed rate regardless of their temporal characteristics. Instead, the proposed network encodes contents at a proper temporal scale depending on its movement speed. We achieve this by constructing temporal hierarchy using stacked latent memories that operate at different rates. Given low latency event steams, the multi-level memories gradually extract dynamic to static scene contents by propagating information from the fast to the slow memory modules. The architecture not only reduces the redundancy of conventional architectures but also exploits long-term dependencies. Furthermore, an attention-based event representation efficiently encodes sparse event streams into the memory cells. We conduct extensive evaluations on three event-based dense prediction tasks, where the proposed approach outperforms the existing methods on accuracy and latency, while demonstrating effective event and image fusion capabilities. The code is available at https://hamarh.github.io/hmnet/",
      "authors": [
        "Ryuhei Hamaguchi",
        "Yasutaka Furukawa",
        "Masaki Onishi",
        "Ken Sakurada"
      ],
      "published": "2023-05-29T02:29:16Z",
      "updated": "2023-05-29T02:29:16Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.17852v1",
      "landing_url": "https://arxiv.org/abs/2305.17852v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.17852"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on a hierarchical memory network for low-latency event processing in sensor data, not on Spoken Language Models (SLMs) or any form of speech-language modeling, and it lacks discussion of speech-specific architectures, training, or evaluation, so it does not meet inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on a hierarchical memory network for low-latency event processing in sensor data, not on Spoken Language Models (SLMs) or any form of speech-language modeling, and it lacks discussion of speech-specific architectures, training, or evaluation, so it does not meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a neural network architecture for event-based dense prediction tasks with an emphasis on low latency processing; however, it does not mention spoken language models or speech language modeling, nor does it address full-duplex spoken interaction or synchronization in speech domains as required by the inclusion criteria. The core subject is event processing in neural networks rather than speech or spoken language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a neural network architecture for event-based dense prediction tasks with an emphasis on low latency processing; however, it does not mention spoken language models or speech language modeling, nor does it address full-duplex spoken interaction or synchronization in speech domains as required by the inclusion criteria. The core subject is event processing in neural networks rather than speech or spoken language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Automatic Evaluation of Turn-taking Cues in Conversational Speech Synthesis",
    "abstract": "Turn-taking is a fundamental aspect of human communication where speakers convey their intention to either hold, or yield, their turn through prosodic cues. Using the recently proposed Voice Activity Projection model, we propose an automatic evaluation approach to measure these aspects for conversational speech synthesis. We investigate the ability of three commercial, and two open-source, Text-To-Speech (TTS) systems ability to generate turn-taking cues over simulated turns. By varying the stimuli, or controlling the prosody, we analyze the models performances. We show that while commercial TTS largely provide appropriate cues, they often produce ambiguous signals, and that further improvements are possible. TTS, trained on read or spontaneous speech, produce strong turn-hold but weak turn-yield cues. We argue that this approach, that focus on functional aspects of interaction, provides a useful addition to other important speech metrics, such as intelligibility and naturalness.",
    "metadata": {
      "arxiv_id": "2305.17971",
      "title": "Automatic Evaluation of Turn-taking Cues in Conversational Speech Synthesis",
      "summary": "Turn-taking is a fundamental aspect of human communication where speakers convey their intention to either hold, or yield, their turn through prosodic cues. Using the recently proposed Voice Activity Projection model, we propose an automatic evaluation approach to measure these aspects for conversational speech synthesis. We investigate the ability of three commercial, and two open-source, Text-To-Speech (TTS) systems ability to generate turn-taking cues over simulated turns. By varying the stimuli, or controlling the prosody, we analyze the models performances. We show that while commercial TTS largely provide appropriate cues, they often produce ambiguous signals, and that further improvements are possible. TTS, trained on read or spontaneous speech, produce strong turn-hold but weak turn-yield cues. We argue that this approach, that focus on functional aspects of interaction, provides a useful addition to other important speech metrics, such as intelligibility and naturalness.",
      "authors": [
        "Erik Ekstedt",
        "Siyang Wang",
        "Éva Székely",
        "Joakim Gustafson",
        "Gabriel Skantze"
      ],
      "published": "2023-05-29T09:29:11Z",
      "updated": "2023-05-29T09:29:11Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.17971v1",
      "landing_url": "https://arxiv.org/abs/2305.17971v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.17971"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此研究聚焦於評估 TTS 的回合切換線索（turn-taking cues）的功能性表現，雖然與語音互動有關，但未以 Spoken Language Model / FD-SLM 為核心，亦未以端到端語音語言建模或語音建模架構為主，因此不符合核心研究範疇。",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "此研究聚焦於評估 TTS 的回合切換線索（turn-taking cues）的功能性表現，雖然與語音互動有關，但未以 Spoken Language Model / FD-SLM 為核心，亦未以端到端語音語言建模或語音建模架構為主，因此不符合核心研究範疇。",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on automatic evaluation of turn-taking cues in conversational speech synthesis, which relates to managing half-duplex turn-taking rather than full-duplex synchronous interactions. The work emphasizes prosodic cues in TTS systems but does not mention Spoken Language Models or Speech Language Models explicitly, nor does it discuss model architectures, training, or evaluations from a spoken language modeling perspective. The evaluation centers on speech synthesis quality related to turn-taking cues, without covering synchronized interaction, speech overlap management, cognitive parallelism, or low-latency interaction. Therefore, it does not fully meet the inclusion criteria and partially falls into exclusion criteria regarding focus on half-duplex systems and lack of spoken language modeling approach.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on automatic evaluation of turn-taking cues in conversational speech synthesis, which relates to managing half-duplex turn-taking rather than full-duplex synchronous interactions. The work emphasizes prosodic cues in TTS systems but does not mention Spoken Language Models or Speech Language Models explicitly, nor does it discuss model architectures, training, or evaluations from a spoken language modeling perspective. The evaluation centers on speech synthesis quality related to turn-taking cues, without covering synchronized interaction, speech overlap management, cognitive parallelism, or low-latency interaction. Therefore, it does not fully meet the inclusion criteria and partially falls into exclusion criteria regarding focus on half-duplex systems and lack of spoken language modeling approach.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Wave to Syntax: Probing spoken language models for syntax",
    "abstract": "Understanding which information is encoded in deep models of spoken and written language has been the focus of much research in recent years, as it is crucial for debugging and improving these architectures. Most previous work has focused on probing for speaker characteristics, acoustic and phonological information in models of spoken language, and for syntactic information in models of written language. Here we focus on the encoding of syntax in several self-supervised and visually grounded models of spoken language. We employ two complementary probing methods, combined with baselines and reference representations to quantify the degree to which syntactic structure is encoded in the activations of the target models. We show that syntax is captured most prominently in the middle layers of the networks, and more explicitly within models with more parameters.",
    "metadata": {
      "arxiv_id": "2305.18957",
      "title": "Wave to Syntax: Probing spoken language models for syntax",
      "summary": "Understanding which information is encoded in deep models of spoken and written language has been the focus of much research in recent years, as it is crucial for debugging and improving these architectures. Most previous work has focused on probing for speaker characteristics, acoustic and phonological information in models of spoken language, and for syntactic information in models of written language. Here we focus on the encoding of syntax in several self-supervised and visually grounded models of spoken language. We employ two complementary probing methods, combined with baselines and reference representations to quantify the degree to which syntactic structure is encoded in the activations of the target models. We show that syntax is captured most prominently in the middle layers of the networks, and more explicitly within models with more parameters.",
      "authors": [
        "Gaofei Shen",
        "Afra Alishahi",
        "Arianna Bisazza",
        "Grzegorz Chrupała"
      ],
      "published": "2023-05-30T11:43:18Z",
      "updated": "2023-05-30T11:43:18Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.18957v1",
      "landing_url": "https://arxiv.org/abs/2305.18957v1",
      "doi": "https://doi.org/10.21437/Interspeech.2023-679"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly targets Spoken Language Models and probes syntactic information within their activations, discusses model architecture and evaluation, and appears within the past 3 years; it is not limited to ASR/TTS or one-way speech tasks, thus meeting inclusion criteria and avoiding exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly targets Spoken Language Models and probes syntactic information within their activations, discusses model architecture and evaluation, and appears within the past 3 years; it is not limited to ASR/TTS or one-way speech tasks, thus meeting inclusion criteria and avoiding exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates the encoding of syntax in self-supervised and visually grounded spoken language models, focusing on model activations rather than just text processing, which aligns with spoken language modeling core concepts; however, it does not explicitly address full-duplex interactive capabilities, cognitive parallelism, or dynamic synchronization and real-time interaction elements emphasized in the inclusion criteria. Additionally, it lacks explicit mention of synchronization, turn-taking, or interaction behaviors, and its main emphasis is on syntactic probing rather than direct training, evaluation, or interaction benchmarks. Therefore, it partially meets inclusion criteria but misses key aspects related to full-duplex interaction and real-time behavior arbitration.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study investigates the encoding of syntax in self-supervised and visually grounded spoken language models, focusing on model activations rather than just text processing, which aligns with spoken language modeling core concepts; however, it does not explicitly address full-duplex interactive capabilities, cognitive parallelism, or dynamic synchronization and real-time interaction elements emphasized in the inclusion criteria. Additionally, it lacks explicit mention of synchronization, turn-taking, or interaction behaviors, and its main emphasis is on syntactic probing rather than direct training, evaluation, or interaction benchmarks. Therefore, it partially meets inclusion criteria but misses key aspects related to full-duplex interaction and real-time behavior arbitration.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The title and abstract explicitly target spoken language models (self-supervised and visually grounded models of spoken language), analyze model activations for syntactic encoding using probing methods and baselines (i.e., discuss evaluation of SLM representations), and do not solely focus on ASR/TTS or purely text-based NLP; JuniorNano also recommended inclusion while JuniorMini flagged the lack of full-duplex/interaction focus — but the primary inclusion gate requires SLMs as the core modeling object and architecture/evaluation analysis, which this paper satisfies and it does not trigger any exclusion criteria. Therefore: meets all required inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "The title and abstract explicitly target spoken language models (self-supervised and visually grounded models of spoken language), analyze model activations for syntactic encoding using probing methods and baselines (i.e., discuss evaluation of SLM representations), and do not solely focus on ASR/TTS or purely text-based NLP; JuniorNano also recommended inclusion while JuniorMini flagged the lack of full-duplex/interaction focus — but the primary inclusion gate requires SLMs as the core modeling object and architecture/evaluation analysis, which this paper satisfies and it does not trigger any exclusion criteria. Therefore: meets all required inclusion criteria and none of the exclusion criteria.",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "How Generative Spoken Language Modeling Encodes Noisy Speech: Investigation from Phonetics to Syntactics",
    "abstract": "We examine the speech modeling potential of generative spoken language modeling (GSLM), which involves using learned symbols derived from data rather than phonemes for speech analysis and synthesis. Since GSLM facilitates textless spoken language processing, exploring its effectiveness is critical for paving the way for novel paradigms in spoken-language processing. This paper presents the findings of GSLM's encoding and decoding effectiveness at the spoken-language and speech levels. Through speech resynthesis experiments, we revealed that resynthesis errors occur at the levels ranging from phonology to syntactics and GSLM frequently resynthesizes natural but content-altered speech.",
    "metadata": {
      "arxiv_id": "2306.00697",
      "title": "How Generative Spoken Language Modeling Encodes Noisy Speech: Investigation from Phonetics to Syntactics",
      "summary": "We examine the speech modeling potential of generative spoken language modeling (GSLM), which involves using learned symbols derived from data rather than phonemes for speech analysis and synthesis. Since GSLM facilitates textless spoken language processing, exploring its effectiveness is critical for paving the way for novel paradigms in spoken-language processing. This paper presents the findings of GSLM's encoding and decoding effectiveness at the spoken-language and speech levels. Through speech resynthesis experiments, we revealed that resynthesis errors occur at the levels ranging from phonology to syntactics and GSLM frequently resynthesizes natural but content-altered speech.",
      "authors": [
        "Joonyong Park",
        "Shinnosuke Takamichi",
        "Tomohiko Nakamura",
        "Kentaro Seki",
        "Detai Xin",
        "Hiroshi Saruwatari"
      ],
      "published": "2023-06-01T14:07:19Z",
      "updated": "2023-06-01T14:07:19Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.00697v1",
      "landing_url": "https://arxiv.org/abs/2306.00697v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.00697"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on Spoken Language Modeling with speech as the modeling target, discusses architecture/learning/evaluation of GSLM for speech, is within the last 3 years, and explicitly uses terms foundational to SLMs, meeting all inclusion criteria and none of the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This item centers on Spoken Language Modeling with speech as the modeling target, discusses architecture/learning/evaluation of GSLM for speech, is within the last 3 years, and explicitly uses terms foundational to SLMs, meeting all inclusion criteria and none of the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates generative spoken language modeling (GSLM) explicitly as a spoken language model, focusing on speech-level encoding and decoding rather than just ASR or TTS tasks; however, the abstract does not mention elements such as full-duplex interaction, real-time turn-taking, synchronization, or dynamic behavior arbitration, which are critical inclusion criteria. Consequently, while it discusses spoken language modeling, it lacks clear coverage of interactive or synchronization aspects and comprehensive evaluation on multi-dimensional interactive metrics, placing it between uncertain inclusion and exclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study investigates generative spoken language modeling (GSLM) explicitly as a spoken language model, focusing on speech-level encoding and decoding rather than just ASR or TTS tasks; however, the abstract does not mention elements such as full-duplex interaction, real-time turn-taking, synchronization, or dynamic behavior arbitration, which are critical inclusion criteria. Consequently, while it discusses spoken language modeling, it lacks clear coverage of interactive or synchronization aspects and comprehensive evaluation on multi-dimensional interactive metrics, placing it between uncertain inclusion and exclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The title and abstract explicitly target Generative Spoken Language Modeling (GSLM), treat speech (learned symbols and resynthesis) as the primary modeling object, and report encoding/decoding evaluations—so it satisfies the core inclusion requirements; JuniorNano also recommended inclusion, while JuniorMini was cautious due to lack of full‑duplex/interaction details, but those FD‑specific features are not mandatory for inclusion. Included because it is a spoken-language-modeling study (not just ASR/TTS or text preprocessing) with architecture/evaluation focus.",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "The title and abstract explicitly target Generative Spoken Language Modeling (GSLM), treat speech (learned symbols and resynthesis) as the primary modeling object, and report encoding/decoding evaluations—so it satisfies the core inclusion requirements; JuniorNano also recommended inclusion, while JuniorMini was cautious due to lack of full‑duplex/interaction details, but those FD‑specific features are not mandatory for inclusion. Included because it is a spoken-language-modeling study (not just ASR/TTS or text preprocessing) with architecture/evaluation focus.",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models",
    "abstract": "Self-supervised techniques for learning speech representations have been shown to develop linguistic competence from exposure to speech without the need for human labels. In order to fully realize the potential of these approaches and further our understanding of how infants learn language, simulations must closely emulate real-life situations by training on developmentally plausible corpora and benchmarking against appropriate test sets. To this end, we propose a language-acquisition-friendly benchmark to probe spoken language models at the lexical and syntactic levels, both of which are compatible with the vocabulary typical of children's language experiences. This paper introduces the benchmark and summarizes a range of experiments showing its usefulness. In addition, we highlight two exciting challenges that need to be addressed for further progress: bridging the gap between text and speech and between clean speech and in-the-wild speech.",
    "metadata": {
      "arxiv_id": "2306.01506",
      "title": "BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models",
      "summary": "Self-supervised techniques for learning speech representations have been shown to develop linguistic competence from exposure to speech without the need for human labels. In order to fully realize the potential of these approaches and further our understanding of how infants learn language, simulations must closely emulate real-life situations by training on developmentally plausible corpora and benchmarking against appropriate test sets. To this end, we propose a language-acquisition-friendly benchmark to probe spoken language models at the lexical and syntactic levels, both of which are compatible with the vocabulary typical of children's language experiences. This paper introduces the benchmark and summarizes a range of experiments showing its usefulness. In addition, we highlight two exciting challenges that need to be addressed for further progress: bridging the gap between text and speech and between clean speech and in-the-wild speech.",
      "authors": [
        "Marvin Lavechin",
        "Yaya Sy",
        "Hadrien Titeux",
        "María Andrea Cruz Blandón",
        "Okko Räsänen",
        "Hervé Bredin",
        "Emmanuel Dupoux",
        "Alejandrina Cristia"
      ],
      "published": "2023-06-02T12:54:38Z",
      "updated": "2023-06-08T12:22:30Z",
      "categories": [
        "cs.CL",
        "eess.AS",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.01506v2",
      "landing_url": "https://arxiv.org/abs/2306.01506v2",
      "doi": "https://doi.org/10.21437/Interspeech.2023-978"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on Spoken Language Models (SLMs) with a self-supervised spoken-language benchmark, evaluates lexical and syntactic proficiency, considers developmentally plausible speech corpora, and discusses model architecture/training/evaluation rather than pure ASR or NLP-only tasks, all within the last 3 years and aligning with the included concepts and exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on Spoken Language Models (SLMs) with a self-supervised spoken-language benchmark, evaluates lexical and syntactic proficiency, considers developmentally plausible speech corpora, and discusses model architecture/training/evaluation rather than pure ASR or NLP-only tasks, all within the last 3 years and aligning with the included concepts and exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on self-supervised spoken language models and proposes a benchmark for assessing models on lexical and syntactic levels with vocabularies compatible with children's language acquisition, indicating research on spoken language modeling. However, it lacks explicit mention of full-duplex interaction, real-time synchronous communication, cognitive parallelism, or behavior arbitration. It is mainly about benchmarking and evaluating representations rather than addressing engineered or learned synchronization or multi-dimensional evaluation of dialogue behaviors. Thus, it partially fits the inclusion criteria but misses critical elements related to full-duplex spoken language models and synchronous interaction paradigms emphasized in the criteria.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on self-supervised spoken language models and proposes a benchmark for assessing models on lexical and syntactic levels with vocabularies compatible with children's language acquisition, indicating research on spoken language modeling. However, it lacks explicit mention of full-duplex interaction, real-time synchronous communication, cognitive parallelism, or behavior arbitration. It is mainly about benchmarking and evaluating representations rather than addressing engineered or learned synchronization or multi-dimensional evaluation of dialogue behaviors. Thus, it partially fits the inclusion criteria but misses critical elements related to full-duplex spoken language models and synchronous interaction paradigms emphasized in the criteria.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the paper clearly targets spoken language models and proposes a benchmark for probing lexical/syntactic competence in self-supervised SLMs; the title/abstract explicitly mentions “self-supervised spoken language models” and discusses corpora, benchmarking and evaluation, and the arXiv ID indicates it is recent. While the work does not address full‑duplex interaction, overlap management, low‑latency or cognitive‑parallelism (points raised by the JuniorMini reviewer), it is not merely an ASR/TTS paper nor solely text‑centric and thus meets the core inclusion requirements (SLMs as the primary modeling object, evaluation/benchmark focus, recent). Therefore it should be included, but noted that it does not cover FD‑SLM / synchronous interaction specifics.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the paper clearly targets spoken language models and proposes a benchmark for probing lexical/syntactic competence in self-supervised SLMs; the title/abstract explicitly mentions “self-supervised spoken language models” and discusses corpora, benchmarking and evaluation, and the arXiv ID indicates it is recent. While the work does not address full‑duplex interaction, overlap management, low‑latency or cognitive‑parallelism (points raised by the JuniorMini reviewer), it is not merely an ASR/TTS paper nor solely text‑centric and thus meets the core inclusion requirements (SLMs as the primary modeling object, evaluation/benchmark focus, recent). Therefore it should be included, but noted that it does not cover FD‑SLM / synchronous interaction specifics.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Low-Latency SCL Bit-Flipping Decoding of Polar Codes",
    "abstract": "Bit flipping can be used as a postprocessing technique to further improve the performance for successive cancellation list (SCL) decoding of polar codes. However, the number of bit-flipping trials could increase the decoding latency significantly, which is not welcome in practice. In this paper, we propose a low latency SCL bit flipping decoding scheme, which is restricted to just single round of post-processing. The use of multiple votes for a more accurate estimation of path survival probability is proposed to locate the first error event of SCL decoding. Simulations show the sound improvement compared to the existing SCL bit-flipping decoding methods.",
    "metadata": {
      "arxiv_id": "2306.02629",
      "title": "Low-Latency SCL Bit-Flipping Decoding of Polar Codes",
      "summary": "Bit flipping can be used as a postprocessing technique to further improve the performance for successive cancellation list (SCL) decoding of polar codes. However, the number of bit-flipping trials could increase the decoding latency significantly, which is not welcome in practice. In this paper, we propose a low latency SCL bit flipping decoding scheme, which is restricted to just single round of post-processing. The use of multiple votes for a more accurate estimation of path survival probability is proposed to locate the first error event of SCL decoding. Simulations show the sound improvement compared to the existing SCL bit-flipping decoding methods.",
      "authors": [
        "Wei Zhang",
        "Xiaofu Wu"
      ],
      "published": "2023-06-05T06:56:20Z",
      "updated": "2023-06-05T06:56:20Z",
      "categories": [
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.02629v1",
      "landing_url": "https://arxiv.org/abs/2306.02629v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.02629"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses low-latency SCL bit-flipping decoding for polar codes in communications, with no focus on Spoken Language Models or speech-language modeling concepts; it does not meet the inclusion criteria and is clearly outside the scope.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses low-latency SCL bit-flipping decoding for polar codes in communications, with no focus on Spoken Language Models or speech-language modeling concepts; it does not meet the inclusion criteria and is clearly outside the scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on low-latency bit-flipping decoding methods for polar codes, a topic in error-correction coding and communication theory, without addressing Spoken Language Models, speech-language modeling, or real-time full-duplex spoken interaction, which are the core inclusion criteria. It neither describes any speech modeling architecture, training, nor evaluation related to spoken language models or full-duplex interaction capabilities. Therefore, it clearly does not meet the inclusion criteria nor addresses any relevant evaluation metrics or synchronization behavior for spoken language models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on low-latency bit-flipping decoding methods for polar codes, a topic in error-correction coding and communication theory, without addressing Spoken Language Models, speech-language modeling, or real-time full-duplex spoken interaction, which are the core inclusion criteria. It neither describes any speech modeling architecture, training, nor evaluation related to spoken language models or full-duplex interaction capabilities. Therefore, it clearly does not meet the inclusion criteria nor addresses any relevant evaluation metrics or synchronization behavior for spoken language models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ModuleFormer: Modularity Emerges from Mixture-of-Experts",
    "abstract": "Large Language Models (LLMs) have achieved remarkable results. However, existing models are expensive to train and deploy, and it is also difficult to expand their knowledge beyond pre-training data without forgetting previous knowledge. This paper proposes a new neural network architecture, ModuleFormer, that leverages modularity to improve the efficiency and flexibility of large language models. ModuleFormer is based on the Sparse Mixture of Experts (SMoE). Unlike the previous SMoE-based modular language model, which requires domain-labeled data to learn domain-specific experts, ModuleFormer can induce modularity from uncurated data with its new load balancing and concentration losses. ModuleFormer is a modular architecture that includes two different types of modules: new stick-breaking attention heads and feedforward experts. Different modules are sparsely activated conditions on the input token during training and inference. In our experiment, we found that the modular architecture enables three important abilities for large pre-trained language models: 1) Efficiency, since ModuleFormer only activates a subset of its modules for each input token, thus it could achieve the same performance as dense LLMs with more than two times throughput; 2) Extendability, ModuleFormer is more immune to catastrophic forgetting than dense LLMs and can be easily extended with new modules to learn new knowledge that is not included in the training data; 3) Specialisation, finetuning ModuleFormer could specialize a subset of modules to the finetuning task and the task-unrelated modules could be easily pruned for a lightweight deployment.",
    "metadata": {
      "arxiv_id": "2306.04640",
      "title": "ModuleFormer: Modularity Emerges from Mixture-of-Experts",
      "summary": "Large Language Models (LLMs) have achieved remarkable results. However, existing models are expensive to train and deploy, and it is also difficult to expand their knowledge beyond pre-training data without forgetting previous knowledge. This paper proposes a new neural network architecture, ModuleFormer, that leverages modularity to improve the efficiency and flexibility of large language models. ModuleFormer is based on the Sparse Mixture of Experts (SMoE). Unlike the previous SMoE-based modular language model, which requires domain-labeled data to learn domain-specific experts, ModuleFormer can induce modularity from uncurated data with its new load balancing and concentration losses. ModuleFormer is a modular architecture that includes two different types of modules: new stick-breaking attention heads and feedforward experts. Different modules are sparsely activated conditions on the input token during training and inference. In our experiment, we found that the modular architecture enables three important abilities for large pre-trained language models: 1) Efficiency, since ModuleFormer only activates a subset of its modules for each input token, thus it could achieve the same performance as dense LLMs with more than two times throughput; 2) Extendability, ModuleFormer is more immune to catastrophic forgetting than dense LLMs and can be easily extended with new modules to learn new knowledge that is not included in the training data; 3) Specialisation, finetuning ModuleFormer could specialize a subset of modules to the finetuning task and the task-unrelated modules could be easily pruned for a lightweight deployment.",
      "authors": [
        "Yikang Shen",
        "Zheyu Zhang",
        "Tianyou Cao",
        "Shawn Tan",
        "Zhenfang Chen",
        "Chuang Gan"
      ],
      "published": "2023-06-07T17:59:57Z",
      "updated": "2023-09-11T19:31:26Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.04640v2",
      "landing_url": "https://arxiv.org/abs/2306.04640v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.04640"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article presents ModuleFormer as a modular architecture for large language models using sparse Mixture-of-Experts, with no emphasis on spoken language modeling, speech processing, or speech-based evaluation, hence it does not meet the Spoken Language Model/Speech-LM inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article presents ModuleFormer as a modular architecture for large language models using sparse Mixture-of-Experts, with no emphasis on spoken language modeling, speech processing, or speech-based evaluation, hence it does not meet the Spoken Language Model/Speech-LM inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a novel modular architecture for large language models based on mixture-of-experts but does not mention speech or spoken language models, nor does it address modeling or inference directly at the speech or acoustic level. It focuses on text-based LLM efficiency and modularity without involving spoken language modeling or full-duplex interactive speech modeling, which are key inclusion criteria. Therefore, it does not meet the core topic requirements and falls outside the scope of spoken language modeling research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a novel modular architecture for large language models based on mixture-of-experts but does not mention speech or spoken language models, nor does it address modeling or inference directly at the speech or acoustic level. It focuses on text-based LLM efficiency and modularity without involving spoken language modeling or full-duplex interactive speech modeling, which are key inclusion criteria. Therefore, it does not meet the core topic requirements and falls outside the scope of spoken language modeling research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "T5-SR: A Unified Seq-to-Seq Decoding Strategy for Semantic Parsing",
    "abstract": "Translating natural language queries into SQLs in a seq2seq manner has attracted much attention recently. However, compared with abstract-syntactic-tree-based SQL generation, seq2seq semantic parsers face much more challenges, including poor quality on schematical information prediction and poor semantic coherence between natural language queries and SQLs. This paper analyses the above difficulties and proposes a seq2seq-oriented decoding strategy called SR, which includes a new intermediate representation SSQL and a reranking method with score re-estimator to solve the above obstacles respectively. Experimental results demonstrate the effectiveness of our proposed techniques and T5-SR-3b achieves new state-of-the-art results on the Spider dataset.",
    "metadata": {
      "arxiv_id": "2306.08368",
      "title": "T5-SR: A Unified Seq-to-Seq Decoding Strategy for Semantic Parsing",
      "summary": "Translating natural language queries into SQLs in a seq2seq manner has attracted much attention recently. However, compared with abstract-syntactic-tree-based SQL generation, seq2seq semantic parsers face much more challenges, including poor quality on schematical information prediction and poor semantic coherence between natural language queries and SQLs. This paper analyses the above difficulties and proposes a seq2seq-oriented decoding strategy called SR, which includes a new intermediate representation SSQL and a reranking method with score re-estimator to solve the above obstacles respectively. Experimental results demonstrate the effectiveness of our proposed techniques and T5-SR-3b achieves new state-of-the-art results on the Spider dataset.",
      "authors": [
        "Yuntao Li",
        "Zhenpeng Su",
        "Yutian Li",
        "Hanchu Zhang",
        "Sirui Wang",
        "Wei Wu",
        "Yan Zhang"
      ],
      "published": "2023-06-14T08:57:13Z",
      "updated": "2023-06-14T08:57:13Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.08368v1",
      "landing_url": "https://arxiv.org/abs/2306.08368v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.08368"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on seq-to-seq semantic parsing for NL to SQL (SR, SSQL) and evaluates on Spider, with no emphasis on Spoken Language Models, speech-centric modeling, or real-time interaction; it does not meet the inclusion criteria for SLM/FD-SLM core concepts, though it is within the recent 3-year window.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on seq-to-seq semantic parsing for NL to SQL (SR, SSQL) and evaluates on Spider, with no emphasis on Spoken Language Models, speech-centric modeling, or real-time interaction; it does not meet the inclusion criteria for SLM/FD-SLM core concepts, though it is within the recent 3-year window.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on seq2seq semantic parsing for natural language to SQL translation, which is a NLP task without emphasis on spoken language models, speech as a core modeling target, or full-duplex spoken language interaction features outlined in the inclusion criteria. It does not address speech-level language modeling, full-duplex interaction, or speech-language model architectures as required, thus failing to meet the inclusion criteria and aligning with exclusion criteria that exclude text-focused NLP works.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on seq2seq semantic parsing for natural language to SQL translation, which is a NLP task without emphasis on spoken language models, speech as a core modeling target, or full-duplex spoken language interaction features outlined in the inclusion criteria. It does not address speech-level language modeling, full-duplex interaction, or speech-language model architectures as required, thus failing to meet the inclusion criteria and aligning with exclusion criteria that exclude text-focused NLP works.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Motion Perceiver: Real-Time Occupancy Forecasting for Embedded Systems",
    "abstract": "This work introduces a novel and adaptable architecture designed for real-time occupancy forecasting that outperforms existing state-of-the-art models on the Waymo Open Motion Dataset in Soft IOU. The proposed model uses recursive latent state estimation with learned transformer-based functions to effectively update and evolve the state. This enables highly efficient real-time inference on embedded systems, as profiled on an Nvidia Xavier AGX. Our model, MotionPerceiver, achieves this by encoding a scene into a latent state that evolves in time through self-attention mechanisms. Additionally, it incorporates relevant scene observations, such as traffic signals, road topology and agent detections, through cross-attention mechanisms. This forms an efficient data-streaming architecture, that contrasts with the expensive, fixed-sequence input common in existing models. The architecture also offers the distinct advantage of generating occupancy predictions through localized querying based on a point-of-interest, as opposed to generating fixed-size occupancy images that render potentially irrelevant regions.",
    "metadata": {
      "arxiv_id": "2306.08879",
      "title": "Motion Perceiver: Real-Time Occupancy Forecasting for Embedded Systems",
      "summary": "This work introduces a novel and adaptable architecture designed for real-time occupancy forecasting that outperforms existing state-of-the-art models on the Waymo Open Motion Dataset in Soft IOU. The proposed model uses recursive latent state estimation with learned transformer-based functions to effectively update and evolve the state. This enables highly efficient real-time inference on embedded systems, as profiled on an Nvidia Xavier AGX. Our model, MotionPerceiver, achieves this by encoding a scene into a latent state that evolves in time through self-attention mechanisms. Additionally, it incorporates relevant scene observations, such as traffic signals, road topology and agent detections, through cross-attention mechanisms. This forms an efficient data-streaming architecture, that contrasts with the expensive, fixed-sequence input common in existing models. The architecture also offers the distinct advantage of generating occupancy predictions through localized querying based on a point-of-interest, as opposed to generating fixed-size occupancy images that render potentially irrelevant regions.",
      "authors": [
        "Bryce Ferenczi",
        "Michael Burke",
        "Tom Drummond"
      ],
      "published": "2023-06-15T06:26:56Z",
      "updated": "2024-02-02T02:09:04Z",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.08879v2",
      "landing_url": "https://arxiv.org/abs/2306.08879v2",
      "doi": "https://doi.org/10.1109/LRA.2024.3360811"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on real-time occupancy forecasting and embedded-system perception for autonomous driving, not on Spoken Language Models or any speech-language modeling, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on real-time occupancy forecasting and embedded-system perception for autonomous driving, not on Spoken Language Models or any speech-language modeling, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article presents a novel model for real-time occupancy forecasting using transformer-based architectures for embedded systems; however, it neither references spoken language models, speech language models, nor language modeling of speech, nor does it discuss any speech or audio modeling or interaction-related concepts specified in the inclusion criteria. It focuses on motion forecasting and embedded system inference rather than speech or full-duplex spoken language modeling, thus not meeting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article presents a novel model for real-time occupancy forecasting using transformer-based architectures for embedded systems; however, it neither references spoken language models, speech language models, nor language modeling of speech, nor does it discuss any speech or audio modeling or interaction-related concepts specified in the inclusion criteria. It focuses on motion forecasting and embedded system inference rather than speech or full-duplex spoken language modeling, thus not meeting the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The temporal dynamics of group interactions in higher-order social networks",
    "abstract": "Representing social systems as networks, starting from the interactions between individuals, sheds light on the mechanisms governing their dynamics. However, networks encode only pairwise interactions, while most social interactions occur among groups of individuals, requiring higher-order network representations. Despite the recent interest in higher-order networks, little is known about the mechanisms that govern the formation and evolution of groups, and how people move between groups. Here, we leverage empirical data on social interactions among children and university students to study their temporal dynamics at both individual and group levels, characterising how individuals navigate groups and how groups form and disaggregate. We find robust patterns across contexts and propose a dynamical model that closely reproduces empirical observations. These results represent a further step in understanding social systems, and open up research directions to study the impact of group dynamics on dynamical processes that evolve on top of them.",
    "metadata": {
      "arxiv_id": "2306.09967",
      "title": "The temporal dynamics of group interactions in higher-order social networks",
      "summary": "Representing social systems as networks, starting from the interactions between individuals, sheds light on the mechanisms governing their dynamics. However, networks encode only pairwise interactions, while most social interactions occur among groups of individuals, requiring higher-order network representations. Despite the recent interest in higher-order networks, little is known about the mechanisms that govern the formation and evolution of groups, and how people move between groups. Here, we leverage empirical data on social interactions among children and university students to study their temporal dynamics at both individual and group levels, characterising how individuals navigate groups and how groups form and disaggregate. We find robust patterns across contexts and propose a dynamical model that closely reproduces empirical observations. These results represent a further step in understanding social systems, and open up research directions to study the impact of group dynamics on dynamical processes that evolve on top of them.",
      "authors": [
        "Iacopo Iacopini",
        "Márton Karsai",
        "Alain Barrat"
      ],
      "published": "2023-06-16T16:54:18Z",
      "updated": "2024-07-09T13:46:50Z",
      "categories": [
        "physics.soc-ph",
        "cs.SI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.09967v3",
      "landing_url": "https://arxiv.org/abs/2306.09967v3",
      "doi": "https://doi.org/10.1038/s41467-024-50918-5"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on higher-order social networks and group dynamics, not on Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and does not align with any exclusion criteria targeted at speech/interactive modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on higher-order social networks and group dynamics, not on Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and does not align with any exclusion criteria targeted at speech/interactive modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on social interactions and higher-order social networks and does not address spoken language models, speech sequence modeling, or full-duplex spoken language interaction. It neither mentions any speech language modeling techniques nor discusses speech as a core modeling object or any related model architectures, training, or evaluation relevant to the inclusion criteria. Therefore, it does not meet the inclusion criteria and is irrelevant to the specified topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on social interactions and higher-order social networks and does not address spoken language models, speech sequence modeling, or full-duplex spoken language interaction. It neither mentions any speech language modeling techniques nor discusses speech as a core modeling object or any related model architectures, training, or evaluation relevant to the inclusion criteria. Therefore, it does not meet the inclusion criteria and is irrelevant to the specified topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Low Latency Edge Classification GNN for Particle Trajectory Tracking on FPGAs",
    "abstract": "In-time particle trajectory reconstruction in the Large Hadron Collider is challenging due to the high collision rate and numerous particle hits. Using GNN (Graph Neural Network) on FPGA has enabled superior accuracy with flexible trajectory classification. However, existing GNN architectures have inefficient resource usage and insufficient parallelism for edge classification. This paper introduces a resource-efficient GNN architecture on FPGAs for low latency particle tracking. The modular architecture facilitates design scalability to support large graphs. Leveraging the geometric properties of hit detectors further reduces graph complexity and resource usage. Our results on Xilinx UltraScale+ VU9P demonstrate 1625x and 1574x performance improvement over CPU and GPU respectively.",
    "metadata": {
      "arxiv_id": "2306.11330",
      "title": "Low Latency Edge Classification GNN for Particle Trajectory Tracking on FPGAs",
      "summary": "In-time particle trajectory reconstruction in the Large Hadron Collider is challenging due to the high collision rate and numerous particle hits. Using GNN (Graph Neural Network) on FPGA has enabled superior accuracy with flexible trajectory classification. However, existing GNN architectures have inefficient resource usage and insufficient parallelism for edge classification. This paper introduces a resource-efficient GNN architecture on FPGAs for low latency particle tracking. The modular architecture facilitates design scalability to support large graphs. Leveraging the geometric properties of hit detectors further reduces graph complexity and resource usage. Our results on Xilinx UltraScale+ VU9P demonstrate 1625x and 1574x performance improvement over CPU and GPU respectively.",
      "authors": [
        "Shi-Yu Huang",
        "Yun-Chen Yang",
        "Yu-Ru Su",
        "Bo-Cheng Lai",
        "Javier Duarte",
        "Scott Hauck",
        "Shih-Chieh Hsu",
        "Jin-Xuan Hu",
        "Mark S. Neubauer"
      ],
      "published": "2023-06-20T06:57:24Z",
      "updated": "2023-06-27T16:21:32Z",
      "categories": [
        "cs.AR",
        "cs.LG",
        "hep-ex"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.11330v2",
      "landing_url": "https://arxiv.org/abs/2306.11330v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.11330"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on low-latency edge classification GNNs for particle trajectory tracking on FPGAs, with no reference to spoken language models or speech-language modeling; thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on low-latency edge classification GNNs for particle trajectory tracking on FPGAs, with no reference to spoken language models or speech-language modeling; thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a graph neural network architecture for particle trajectory tracking on FPGAs and does not address spoken language models, speech language modeling, or any related synchronous or full-duplex speech interaction topics; therefore, it does not meet any of the inclusion criteria and falls outside the research domain specified.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a graph neural network architecture for particle trajectory tracking on FPGAs and does not address spoken language models, speech language modeling, or any related synchronous or full-duplex speech interaction topics; therefore, it does not meet any of the inclusion criteria and falls outside the research domain specified.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Align, Adapt and Inject: Sound-guided Unified Image Generation",
    "abstract": "Text-guided image generation has witnessed unprecedented progress due to the development of diffusion models. Beyond text and image, sound is a vital element within the sphere of human perception, offering vivid representations and naturally coinciding with corresponding scenes. Taking advantage of sound therefore presents a promising avenue for exploration within image generation research. However, the relationship between audio and image supervision remains significantly underdeveloped, and the scarcity of related, high-quality datasets brings further obstacles. In this paper, we propose a unified framework 'Align, Adapt, and Inject' (AAI) for sound-guided image generation, editing, and stylization. In particular, our method adapts input sound into a sound token, like an ordinary word, which can plug and play with existing powerful diffusion-based Text-to-Image (T2I) models. Specifically, we first train a multi-modal encoder to align audio representation with the pre-trained textual manifold and visual manifold, respectively. Then, we propose the audio adapter to adapt audio representation into an audio token enriched with specific semantics, which can be injected into a frozen T2I model flexibly. In this way, we are able to extract the dynamic information of varied sounds, while utilizing the formidable capability of existing T2I models to facilitate sound-guided image generation, editing, and stylization in a convenient and cost-effective manner. The experiment results confirm that our proposed AAI outperforms other text and sound-guided state-of-the-art methods. And our aligned multi-modal encoder is also competitive with other approaches in the audio-visual retrieval and audio-text retrieval tasks.",
    "metadata": {
      "arxiv_id": "2306.11504",
      "title": "Align, Adapt and Inject: Sound-guided Unified Image Generation",
      "summary": "Text-guided image generation has witnessed unprecedented progress due to the development of diffusion models. Beyond text and image, sound is a vital element within the sphere of human perception, offering vivid representations and naturally coinciding with corresponding scenes. Taking advantage of sound therefore presents a promising avenue for exploration within image generation research. However, the relationship between audio and image supervision remains significantly underdeveloped, and the scarcity of related, high-quality datasets brings further obstacles. In this paper, we propose a unified framework 'Align, Adapt, and Inject' (AAI) for sound-guided image generation, editing, and stylization. In particular, our method adapts input sound into a sound token, like an ordinary word, which can plug and play with existing powerful diffusion-based Text-to-Image (T2I) models. Specifically, we first train a multi-modal encoder to align audio representation with the pre-trained textual manifold and visual manifold, respectively. Then, we propose the audio adapter to adapt audio representation into an audio token enriched with specific semantics, which can be injected into a frozen T2I model flexibly. In this way, we are able to extract the dynamic information of varied sounds, while utilizing the formidable capability of existing T2I models to facilitate sound-guided image generation, editing, and stylization in a convenient and cost-effective manner. The experiment results confirm that our proposed AAI outperforms other text and sound-guided state-of-the-art methods. And our aligned multi-modal encoder is also competitive with other approaches in the audio-visual retrieval and audio-text retrieval tasks.",
      "authors": [
        "Yue Yang",
        "Kaipeng Zhang",
        "Yuying Ge",
        "Wenqi Shao",
        "Zeyue Xue",
        "Yu Qiao",
        "Ping Luo"
      ],
      "published": "2023-06-20T12:50:49Z",
      "updated": "2023-06-20T12:50:49Z",
      "categories": [
        "cs.GR",
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.11504v1",
      "landing_url": "https://arxiv.org/abs/2306.11504v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.11504"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets sound-guided image generation rather than Spoken Language Models or speech-centric modeling; it treats audio as input to a vision diffusion model, not as primary object of speech-language modeling or synchronous dialogue behavior, thus does not meet the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets sound-guided image generation rather than Spoken Language Models or speech-centric modeling; it treats audio as input to a vision diffusion model, not as primary object of speech-language modeling or synchronous dialogue behavior, thus does not meet the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sound-guided image generation leveraging audio tokens within a multimodal diffusion model framework; however, it does not address Spoken Language Models or Full-Duplex Spoken Language Models, nor does it treat speech as the core modeling or reasoning object in the context of synchronized spoken interaction or spoken language modeling. It lacks focus on speech-level language modeling, end-to-end speech-LLM architectures, cognitive parallelism, low-latency interaction, or behavior arbitration. Therefore, it fails to meet the inclusion criteria centered on Spoken Language Models and their interaction capabilities and modeling. This research is more about multimodal generation involving sound rather than speech language modeling or synchronization in spoken dialog systems.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sound-guided image generation leveraging audio tokens within a multimodal diffusion model framework; however, it does not address Spoken Language Models or Full-Duplex Spoken Language Models, nor does it treat speech as the core modeling or reasoning object in the context of synchronized spoken interaction or spoken language modeling. It lacks focus on speech-level language modeling, end-to-end speech-LLM architectures, cognitive parallelism, low-latency interaction, or behavior arbitration. Therefore, it fails to meet the inclusion criteria centered on Spoken Language Models and their interaction capabilities and modeling. This research is more about multimodal generation involving sound rather than speech language modeling or synchronization in spoken dialog systems.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Combining Public Human Activity Recognition Datasets to Mitigate Labeled Data Scarcity",
    "abstract": "The use of supervised learning for Human Activity Recognition (HAR) on mobile devices leads to strong classification performances. Such an approach, however, requires large amounts of labeled data, both for the initial training of the models and for their customization on specific clients (whose data often differ greatly from the training data). This is actually impractical to obtain due to the costs, intrusiveness, and time-consuming nature of data annotation. Moreover, even with the help of a significant amount of labeled data, model deployment on heterogeneous clients faces difficulties in generalizing well on unseen data. Other domains, like Computer Vision or Natural Language Processing, have proposed the notion of pre-trained models, leveraging large corpora, to reduce the need for annotated data and better manage heterogeneity. This promising approach has not been implemented in the HAR domain so far because of the lack of public datasets of sufficient size. In this paper, we propose a novel strategy to combine publicly available datasets with the goal of learning a generalized HAR model that can be fine-tuned using a limited amount of labeled data on an unseen target domain. Our experimental evaluation, which includes experimenting with different state-of-the-art neural network architectures, shows that combining public datasets can significantly reduce the number of labeled samples required to achieve satisfactory performance on an unseen target domain.",
    "metadata": {
      "arxiv_id": "2306.13735",
      "title": "Combining Public Human Activity Recognition Datasets to Mitigate Labeled Data Scarcity",
      "summary": "The use of supervised learning for Human Activity Recognition (HAR) on mobile devices leads to strong classification performances. Such an approach, however, requires large amounts of labeled data, both for the initial training of the models and for their customization on specific clients (whose data often differ greatly from the training data). This is actually impractical to obtain due to the costs, intrusiveness, and time-consuming nature of data annotation. Moreover, even with the help of a significant amount of labeled data, model deployment on heterogeneous clients faces difficulties in generalizing well on unseen data. Other domains, like Computer Vision or Natural Language Processing, have proposed the notion of pre-trained models, leveraging large corpora, to reduce the need for annotated data and better manage heterogeneity. This promising approach has not been implemented in the HAR domain so far because of the lack of public datasets of sufficient size. In this paper, we propose a novel strategy to combine publicly available datasets with the goal of learning a generalized HAR model that can be fine-tuned using a limited amount of labeled data on an unseen target domain. Our experimental evaluation, which includes experimenting with different state-of-the-art neural network architectures, shows that combining public datasets can significantly reduce the number of labeled samples required to achieve satisfactory performance on an unseen target domain.",
      "authors": [
        "Riccardo Presotto",
        "Sannara Ek",
        "Gabriele Civitarese",
        "François Portet",
        "Philippe Lalanda",
        "Claudio Bettini"
      ],
      "published": "2023-06-23T18:51:22Z",
      "updated": "2023-06-23T18:51:22Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.13735v1",
      "landing_url": "https://arxiv.org/abs/2306.13735v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.13735"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目聚焦於人體活動識別(HAR)資料集的跨資料集整合以降低標注需求，未以語音語言模型、語音建模或對話互動為核心議題，亦非近年語音相關研究之範疇，因此不符合納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目聚焦於人體活動識別(HAR)資料集的跨資料集整合以降低標注需求，未以語音語言模型、語音建模或對話互動為核心議題，亦非近年語音相關研究之範疇，因此不符合納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Human Activity Recognition using supervised learning and combining public datasets for generalization, which is outside the defined scope focused on Spoken Language Models and related speech-language modeling architectures. It does not discuss spoken language models or speech-language model training, inference, or evaluation, nor does it involve full-duplex or synchronous speech interaction mechanisms, which are required by the inclusion criteria. Hence, it does not meet the inclusion criteria and falls under exclusion criteria for not focusing on the core theme of speech language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Human Activity Recognition using supervised learning and combining public datasets for generalization, which is outside the defined scope focused on Spoken Language Models and related speech-language modeling architectures. It does not discuss spoken language models or speech-language model training, inference, or evaluation, nor does it involve full-duplex or synchronous speech interaction mechanisms, which are required by the inclusion criteria. Hence, it does not meet the inclusion criteria and falls under exclusion criteria for not focusing on the core theme of speech language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "BBCA-LEDGER: High Throughput Consensus meets Low Latency",
    "abstract": "This paper presents BBCA-LEDGER, a Byzantine log replication technology for partially synchronous networks enabling blocks to be broadcast in parallel, such that each broadcast is finalized independently and instantaneously into an individual slot in the log. Every finalized broadcast is eventually committed to the total ordering, so that all network bandwidth has utility in disseminating blocks. Finalizing log slots in parallel achieves both high throughput and low latency. BBCA-LEDGER is composed of two principal protocols that interweave together, a low-latency/high-throughput happy path, and a high-throughput DAG-based fallback path. The happy path employs a novel primitive called BBCA, a consistent broadcast enforcing unique slot numbering. In steady state, BBCA ensures that a transaction can be committed with low latency, in just 3 network steps. Under network partitions or faults, we harness recent advances in BFT and build a fallback mechanism on a direct acyclic graph (DAG) created by BBCA broadcasts. In this manner, BBCA-LEDGER exhibits the throughput benefits of DAG-based BFT in face of gaps.",
    "metadata": {
      "arxiv_id": "2306.14757",
      "title": "BBCA-LEDGER: High Throughput Consensus meets Low Latency",
      "summary": "This paper presents BBCA-LEDGER, a Byzantine log replication technology for partially synchronous networks enabling blocks to be broadcast in parallel, such that each broadcast is finalized independently and instantaneously into an individual slot in the log. Every finalized broadcast is eventually committed to the total ordering, so that all network bandwidth has utility in disseminating blocks. Finalizing log slots in parallel achieves both high throughput and low latency. BBCA-LEDGER is composed of two principal protocols that interweave together, a low-latency/high-throughput happy path, and a high-throughput DAG-based fallback path. The happy path employs a novel primitive called BBCA, a consistent broadcast enforcing unique slot numbering. In steady state, BBCA ensures that a transaction can be committed with low latency, in just 3 network steps. Under network partitions or faults, we harness recent advances in BFT and build a fallback mechanism on a direct acyclic graph (DAG) created by BBCA broadcasts. In this manner, BBCA-LEDGER exhibits the throughput benefits of DAG-based BFT in face of gaps.",
      "authors": [
        "Chrysoula Stathakopoulou",
        "Michael Wei",
        "Maofan Yin",
        "Hongbo Zhang",
        "Dahlia Malkhi"
      ],
      "published": "2023-06-26T15:11:50Z",
      "updated": "2023-06-26T15:11:50Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.14757v1",
      "landing_url": "https://arxiv.org/abs/2306.14757v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.14757"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on blockchain consensus (BBCA-LEDGER) with no reference to spoken language models, speech modeling, or speech-based evaluation, so it fails the required inclusion criteria and satisfies none of the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on blockchain consensus (BBCA-LEDGER) with no reference to spoken language models, speech modeling, or speech-based evaluation, so it fails the required inclusion criteria and satisfies none of the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper BBCA-LEDGER focuses on Byzantine log replication and network protocols in partially synchronous networks, targeting blockchain or distributed consensus technologies. It does not mention any aspects of spoken language modeling, speech processing, full-duplex interaction, or speech-language models as required by the inclusion criteria. It clearly does not address any of the core topics or modeling approaches related to spoken language or speech-based interactive systems, and therefore fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper BBCA-LEDGER focuses on Byzantine log replication and network protocols in partially synchronous networks, targeting blockchain or distributed consensus technologies. It does not mention any aspects of spoken language modeling, speech processing, full-duplex interaction, or speech-language models as required by the inclusion criteria. It clearly does not address any of the core topics or modeling approaches related to spoken language or speech-based interactive systems, and therefore fails the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Sidecars on the Central Lane: Impact of Network Proxies on Microservices",
    "abstract": "Cloud applications are moving away from monolithic model towards loosely-coupled microservices designs. Service meshes are widely used for implementing microservices applications mainly because they provide a modular architecture for modern applications by separating operational features from application business logic. Sidecar proxies in service meshes enable this modularity by applying security, networking, and monitoring policies on the traffic to and from services. To implement these policies, sidecars often execute complex chains of logic that vary across associated applications and end up unevenly impacting the performance of the overall application. Lack of understanding of how the sidecars impact the performance of microservice-based applications stands in the way of building performant and resource-efficient applications. To this end, we bring sidecar proxies in focus and argue that we need to deeply study their impact on the system performance and resource utilization. We identify and describe challenges in characterizing sidecars, namely the need for microarchitectural metrics and comprehensive methodologies, and discuss research directions where such characterization will help in building efficient service mesh infrastructure for microservice applications.",
    "metadata": {
      "arxiv_id": "2306.15792",
      "title": "Sidecars on the Central Lane: Impact of Network Proxies on Microservices",
      "summary": "Cloud applications are moving away from monolithic model towards loosely-coupled microservices designs. Service meshes are widely used for implementing microservices applications mainly because they provide a modular architecture for modern applications by separating operational features from application business logic. Sidecar proxies in service meshes enable this modularity by applying security, networking, and monitoring policies on the traffic to and from services. To implement these policies, sidecars often execute complex chains of logic that vary across associated applications and end up unevenly impacting the performance of the overall application. Lack of understanding of how the sidecars impact the performance of microservice-based applications stands in the way of building performant and resource-efficient applications. To this end, we bring sidecar proxies in focus and argue that we need to deeply study their impact on the system performance and resource utilization. We identify and describe challenges in characterizing sidecars, namely the need for microarchitectural metrics and comprehensive methodologies, and discuss research directions where such characterization will help in building efficient service mesh infrastructure for microservice applications.",
      "authors": [
        "Prateek Sahu",
        "Lucy Zheng",
        "Marco Bueso",
        "Shijia Wei",
        "Neeraja J. Yadwadkar",
        "Mohit Tiwari"
      ],
      "published": "2023-06-27T20:40:43Z",
      "updated": "2023-10-17T23:04:09Z",
      "categories": [
        "cs.DC",
        "cs.AR",
        "cs.PF"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.15792v2",
      "landing_url": "https://arxiv.org/abs/2306.15792v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.15792"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on sidecar proxies and performance in microservices; it does not involve Spoken Language Models or any speech-language modeling concepts, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on sidecar proxies and performance in microservices; it does not involve Spoken Language Models or any speech-language modeling concepts, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on the performance impact of sidecar proxies in microservices architectures and does not address spoken language models, speech-language modeling, full-duplex interactions, or any aspects of speech sequence modeling. It lacks any mention of spoken language models or relevant terminologies, and does not relate to synchronous speech interaction or speech language modeling frameworks as defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on the performance impact of sidecar proxies in microservices architectures and does not address spoken language models, speech-language modeling, full-duplex interactions, or any aspects of speech sequence modeling. It lacks any mention of spoken language models or relevant terminologies, and does not relate to synchronous speech interaction or speech language modeling frameworks as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis",
    "abstract": "Generative Adversarial Networks (GAN) is a model for data synthesis, which creates plausible data through the competition of generator and discriminator. Although GAN application to image synthesis is extensively studied, it has inherent limitations to natural language generation. Because natural language is composed of discrete tokens, a generator has difficulty updating its gradient through backpropagation; therefore, most text-GAN studies generate sentences starting with a random token based on a reward system. Thus, the generators of previous studies are pre-trained in an autoregressive way before adversarial training, causing data memorization that synthesized sentences reproduce the training data. In this paper, we synthesize sentences using a framework similar to the original GAN. More specifically, we propose Text Embedding Space Generative Adversarial Networks (TESGAN) which generate continuous text embedding spaces instead of discrete tokens to solve the gradient backpropagation problem. Furthermore, TESGAN conducts unsupervised learning which does not directly refer to the text of the training data to overcome the data memorization issue. By adopting this novel method, TESGAN can synthesize new sentences, showing the potential of unsupervised learning for text synthesis. We expect to see extended research combining Large Language Models with a new perspective of viewing text as an continuous space.",
    "metadata": {
      "arxiv_id": "2306.17181",
      "title": "Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis",
      "summary": "Generative Adversarial Networks (GAN) is a model for data synthesis, which creates plausible data through the competition of generator and discriminator. Although GAN application to image synthesis is extensively studied, it has inherent limitations to natural language generation. Because natural language is composed of discrete tokens, a generator has difficulty updating its gradient through backpropagation; therefore, most text-GAN studies generate sentences starting with a random token based on a reward system. Thus, the generators of previous studies are pre-trained in an autoregressive way before adversarial training, causing data memorization that synthesized sentences reproduce the training data. In this paper, we synthesize sentences using a framework similar to the original GAN. More specifically, we propose Text Embedding Space Generative Adversarial Networks (TESGAN) which generate continuous text embedding spaces instead of discrete tokens to solve the gradient backpropagation problem. Furthermore, TESGAN conducts unsupervised learning which does not directly refer to the text of the training data to overcome the data memorization issue. By adopting this novel method, TESGAN can synthesize new sentences, showing the potential of unsupervised learning for text synthesis. We expect to see extended research combining Large Language Models with a new perspective of viewing text as an continuous space.",
      "authors": [
        "Jun-Min Lee",
        "Tae-Bin Ha"
      ],
      "published": "2023-06-19T10:22:12Z",
      "updated": "2023-10-17T10:41:12Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.17181v4",
      "landing_url": "https://arxiv.org/abs/2306.17181v4",
      "doi": "https://doi.org/10.3384/nejlt.2000-1533.2023.4855"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on unsupervised text generation using GANs for text embeddings and does not treat speech or spoken language models as the core modeling target; it does not address speech interaction, synchronization, or utterance-level speech modeling, so it fails to meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on unsupervised text generation using GANs for text embeddings and does not treat speech or spoken language models as the core modeling target; it does not address speech interaction, synchronization, or utterance-level speech modeling, so it fails to meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on text synthesis using GANs applied to text embeddings, without any mention of spoken language, speech signals, or related spoken language models, architectures, or evaluations as required by the inclusion criteria. It also does not address any synchronization or full-duplex communication features, nor discusses evaluation of speech overlap, interaction, or related multi-dimensional metrics. Hence, it does not meet the core focus on spoken language modeling from speech signal perspective and can be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on text synthesis using GANs applied to text embeddings, without any mention of spoken language, speech signals, or related spoken language models, architectures, or evaluations as required by the inclusion criteria. It also does not address any synchronization or full-duplex communication features, nor discusses evaluation of speech overlap, interaction, or related multi-dimensional metrics. Hence, it does not meet the core focus on spoken language modeling from speech signal perspective and can be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient Neural Image Compression",
    "abstract": "Recently, the performance of neural image compression (NIC) has steadily improved thanks to the last line of study, reaching or outperforming state-of-the-art conventional codecs. Despite significant progress, current NIC methods still rely on ConvNet-based entropy coding, limited in modeling long-range dependencies due to their local connectivity and the increasing number of architectural biases and priors, resulting in complex underperforming models with high decoding latency. Motivated by the efficiency investigation of the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose to enhance the latter, as first, with a more straightforward yet effective Tranformer-based channel-wise auto-regressive prior model, resulting in an absolute image compression transformer (ICT). Through the proposed ICT, we can capture both global and local contexts from the latent representations and better parameterize the distribution of the quantized latents. Further, we leverage a learnable scaling module with a sandwich ConvNeXt-based pre-/post-processor to accurately extract more compact latent codes while reconstructing higher-quality images. Extensive experimental results on benchmark datasets showed that the proposed framework significantly improves the trade-off between coding efficiency and decoder complexity over the versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec SwinT-ChARM. Moreover, we provide model scaling studies to verify the computational efficiency of our approach and conduct several objective and subjective analyses to bring to the fore the performance gap between the adaptive image compression transformer (AICT) and the neural codec SwinT-ChARM.",
    "metadata": {
      "arxiv_id": "2307.02273",
      "title": "Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient Neural Image Compression",
      "summary": "Recently, the performance of neural image compression (NIC) has steadily improved thanks to the last line of study, reaching or outperforming state-of-the-art conventional codecs. Despite significant progress, current NIC methods still rely on ConvNet-based entropy coding, limited in modeling long-range dependencies due to their local connectivity and the increasing number of architectural biases and priors, resulting in complex underperforming models with high decoding latency. Motivated by the efficiency investigation of the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose to enhance the latter, as first, with a more straightforward yet effective Tranformer-based channel-wise auto-regressive prior model, resulting in an absolute image compression transformer (ICT). Through the proposed ICT, we can capture both global and local contexts from the latent representations and better parameterize the distribution of the quantized latents. Further, we leverage a learnable scaling module with a sandwich ConvNeXt-based pre-/post-processor to accurately extract more compact latent codes while reconstructing higher-quality images. Extensive experimental results on benchmark datasets showed that the proposed framework significantly improves the trade-off between coding efficiency and decoder complexity over the versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec SwinT-ChARM. Moreover, we provide model scaling studies to verify the computational efficiency of our approach and conduct several objective and subjective analyses to bring to the fore the performance gap between the adaptive image compression transformer (AICT) and the neural codec SwinT-ChARM.",
      "authors": [
        "Ahmed Ghorbel",
        "Wassim Hamidouche",
        "Luce Morin"
      ],
      "published": "2023-07-05T13:17:14Z",
      "updated": "2024-01-22T17:37:03Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.02273v4",
      "landing_url": "https://arxiv.org/abs/2307.02273v4",
      "doi": "https://doi.org/10.48550/arXiv.2307.02273"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item analyzes neural image compression using Transformer-based priors and does not center on Spoken Language Models or speech-language modeling, thus not meeting the inclusion criteria focused on speech/language modeling and interactive speech tasks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item analyzes neural image compression using Transformer-based priors and does not center on Spoken Language Models or speech-language modeling, thus not meeting the inclusion criteria focused on speech/language modeling and interactive speech tasks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper is focused on neural image compression using Transformer-based models and does not discuss spoken language models, speech as the core modeling object, or any aspects related to full-duplex spoken language communication or speech language modeling; thus, it does not meet the inclusion criteria centered on Spoken Language Models or related speech-based modeling approaches.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper is focused on neural image compression using Transformer-based models and does not discuss spoken language models, speech as the core modeling object, or any aspects related to full-duplex spoken language communication or speech language modeling; thus, it does not meet the inclusion criteria centered on Spoken Language Models or related speech-based modeling approaches.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SpawnNet: Learning Generalizable Visuomotor Skills from Pre-trained Networks",
    "abstract": "The existing internet-scale image and video datasets cover a wide range of everyday objects and tasks, bringing the potential of learning policies that generalize in diverse scenarios. Prior works have explored visual pre-training with different self-supervised objectives. Still, the generalization capabilities of the learned policies and the advantages over well-tuned baselines remain unclear from prior studies. In this work, we present a focused study of the generalization capabilities of the pre-trained visual representations at the categorical level. We identify the key bottleneck in using a frozen pre-trained visual backbone for policy learning and then propose SpawnNet, a novel two-stream architecture that learns to fuse pre-trained multi-layer representations into a separate network to learn a robust policy. Through extensive simulated and real experiments, we show significantly better categorical generalization compared to prior approaches in imitation learning settings. Open-sourced code and videos can be found on our website: https://xingyu-lin.github.io/spawnnet.",
    "metadata": {
      "arxiv_id": "2307.03567",
      "title": "SpawnNet: Learning Generalizable Visuomotor Skills from Pre-trained Networks",
      "summary": "The existing internet-scale image and video datasets cover a wide range of everyday objects and tasks, bringing the potential of learning policies that generalize in diverse scenarios. Prior works have explored visual pre-training with different self-supervised objectives. Still, the generalization capabilities of the learned policies and the advantages over well-tuned baselines remain unclear from prior studies. In this work, we present a focused study of the generalization capabilities of the pre-trained visual representations at the categorical level. We identify the key bottleneck in using a frozen pre-trained visual backbone for policy learning and then propose SpawnNet, a novel two-stream architecture that learns to fuse pre-trained multi-layer representations into a separate network to learn a robust policy. Through extensive simulated and real experiments, we show significantly better categorical generalization compared to prior approaches in imitation learning settings. Open-sourced code and videos can be found on our website: https://xingyu-lin.github.io/spawnnet.",
      "authors": [
        "Xingyu Lin",
        "John So",
        "Sashwat Mahalingam",
        "Fangchen Liu",
        "Pieter Abbeel"
      ],
      "published": "2023-07-07T13:01:29Z",
      "updated": "2023-10-22T03:16:41Z",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.03567v2",
      "landing_url": "https://arxiv.org/abs/2307.03567v2",
      "doi": "https://doi.org/10.48550/arXiv.2307.03567"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract focus on visuomotor policy learning from pre-trained visual backbones, not on Spoken Language Models or speech-language modeling, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract focus on visuomotor policy learning from pre-trained visual backbones, not on Spoken Language Models or speech-language modeling, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on learning visuomotor skills using pre-trained visual networks, with no mention of spoken language models, speech-language modeling, or any aspect related to speech signals, turn-taking, full-duplex interaction, or related evaluation metrics; thus, it does not meet the inclusion criteria and falls outside the specified research scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on learning visuomotor skills using pre-trained visual networks, with no mention of spoken language models, speech-language modeling, or any aspect related to speech signals, turn-taking, full-duplex interaction, or related evaluation metrics; thus, it does not meet the inclusion criteria and falls outside the specified research scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "AICT: An Adaptive Image Compression Transformer",
    "abstract": "Motivated by the efficiency investigation of the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose to enhance the latter, as first, with a more straightforward yet effective Tranformer-based channel-wise auto-regressive prior model, resulting in an absolute image compression transformer (ICT). Current methods that still rely on ConvNet-based entropy coding are limited in long-range modeling dependencies due to their local connectivity and an increasing number of architectural biases and priors. On the contrary, the proposed ICT can capture both global and local contexts from the latent representations and better parameterize the distribution of the quantized latents. Further, we leverage a learnable scaling module with a sandwich ConvNeXt-based pre/post-processor to accurately extract more compact latent representation while reconstructing higher-quality images. Extensive experimental results on benchmark datasets showed that the proposed adaptive image compression transformer (AICT) framework significantly improves the trade-off between coding efficiency and decoder complexity over the versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec SwinT-ChARM.",
    "metadata": {
      "arxiv_id": "2307.06091",
      "title": "AICT: An Adaptive Image Compression Transformer",
      "summary": "Motivated by the efficiency investigation of the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose to enhance the latter, as first, with a more straightforward yet effective Tranformer-based channel-wise auto-regressive prior model, resulting in an absolute image compression transformer (ICT). Current methods that still rely on ConvNet-based entropy coding are limited in long-range modeling dependencies due to their local connectivity and an increasing number of architectural biases and priors. On the contrary, the proposed ICT can capture both global and local contexts from the latent representations and better parameterize the distribution of the quantized latents. Further, we leverage a learnable scaling module with a sandwich ConvNeXt-based pre/post-processor to accurately extract more compact latent representation while reconstructing higher-quality images. Extensive experimental results on benchmark datasets showed that the proposed adaptive image compression transformer (AICT) framework significantly improves the trade-off between coding efficiency and decoder complexity over the versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec SwinT-ChARM.",
      "authors": [
        "Ahmed Ghorbel",
        "Wassim Hamidouche",
        "Luce Morin"
      ],
      "published": "2023-07-12T11:32:02Z",
      "updated": "2023-07-12T11:32:02Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.06091v1",
      "landing_url": "https://arxiv.org/abs/2307.06091v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.06091"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe an image compression Transformer (ICT) with no reference to spoken language models, speech modeling, or any interactive speech-language tasks, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe an image compression Transformer (ICT) with no reference to spoken language models, speech modeling, or any interactive speech-language tasks, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on adaptive image compression using Transformer models and does not address spoken language models (SLMs), full-duplex spoken language interaction, speech or acoustic signal-based language modeling, or any multimodal speech-language architectures. Therefore, it does not meet the inclusion criteria centered on spoken language modeling nor discusses related evaluation metrics or synchronous interactive behavior, and it clearly falls outside the defined research scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on adaptive image compression using Transformer models and does not address spoken language models (SLMs), full-duplex spoken language interaction, speech or acoustic signal-based language modeling, or any multimodal speech-language architectures. Therefore, it does not meet the inclusion criteria centered on spoken language modeling nor discusses related evaluation metrics or synchronous interactive behavior, and it clearly falls outside the defined research scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Analysis of Half-Duplex Two-Node Slotted ALOHA Network With Asynchronous Traffic",
    "abstract": "Despite the long history of research on slotted ALOHA, the exact analysis of the average delay is still in question as the performance of each node is coupled with the activity of other nodes. In this paper, we consider a network comprised of two half-duplex transmitter nodes with asynchronous arrival traffic that follow the slotted ALOHA protocol. We propose a new queueing theoretic model based on the state-dependent queues to analyze the network. In addition, we derive the exact values of delay and stability region for each node. The numerical results demonstrate the accuracy of our proposed model.",
    "metadata": {
      "arxiv_id": "2307.06334",
      "title": "Analysis of Half-Duplex Two-Node Slotted ALOHA Network With Asynchronous Traffic",
      "summary": "Despite the long history of research on slotted ALOHA, the exact analysis of the average delay is still in question as the performance of each node is coupled with the activity of other nodes. In this paper, we consider a network comprised of two half-duplex transmitter nodes with asynchronous arrival traffic that follow the slotted ALOHA protocol. We propose a new queueing theoretic model based on the state-dependent queues to analyze the network. In addition, we derive the exact values of delay and stability region for each node. The numerical results demonstrate the accuracy of our proposed model.",
      "authors": [
        "Seyed Ali Hashemian",
        "Farid Ashtiani"
      ],
      "published": "2023-07-12T17:56:02Z",
      "updated": "2023-07-12T17:56:02Z",
      "categories": [
        "eess.SY",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.06334v1",
      "landing_url": "https://arxiv.org/abs/2307.06334v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.06334"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on a half-duplex two-node slotted ALOHA network and queueing delay analysis, which is unrelated to Spoken Language Models or any speech-language modeling and thus does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on a half-duplex two-node slotted ALOHA network and queueing delay analysis, which is unrelated to Spoken Language Models or any speech-language modeling and thus does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an analysis of a two-node half-duplex slotted ALOHA network with asynchronous traffic, primarily concerning queueing theory and delay analysis in communication networks. It does not discuss spoken language models, speech language models, or any speech-related language modeling, nor does it address full-duplex spoken interaction, cognitive parallelism, or speech behavior arbitration. Therefore, it does not meet the inclusion criteria and meets exclusion criteria around the absence of spoken language modeling focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an analysis of a two-node half-duplex slotted ALOHA network with asynchronous traffic, primarily concerning queueing theory and delay analysis in communication networks. It does not discuss spoken language models, speech language models, or any speech-related language modeling, nor does it address full-duplex spoken interaction, cognitive parallelism, or speech behavior arbitration. Therefore, it does not meet the inclusion criteria and meets exclusion criteria around the absence of spoken language modeling focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "PASS: Exploiting Post-Activation Sparsity in Streaming Architectures for CNN Acceleration",
    "abstract": "With the ever-growing popularity of Artificial Intelligence, there is an increasing demand for more performant and efficient underlying hardware. Convolutional Neural Networks (CNN) are a workload of particular importance, which achieve high accuracy in computer vision applications. Inside CNNs, a significant number of the post-activation values are zero, resulting in many redundant computations. Recent works have explored this post-activation sparsity on instruction-based CNN accelerators but not on streaming CNN accelerators, despite the fact that streaming architectures are considered the leading design methodology in terms of performance. In this paper, we highlight the challenges associated with exploiting post-activation sparsity for performance gains in streaming CNN accelerators, and demonstrate our approach to address them. Using a set of modern CNN benchmarks, our streaming sparse accelerators achieve 1.41x to 1.93x efficiency (GOP/s/DSP) compared to state-of-the-art instruction-based sparse accelerators.",
    "metadata": {
      "arxiv_id": "2307.07821",
      "title": "PASS: Exploiting Post-Activation Sparsity in Streaming Architectures for CNN Acceleration",
      "summary": "With the ever-growing popularity of Artificial Intelligence, there is an increasing demand for more performant and efficient underlying hardware. Convolutional Neural Networks (CNN) are a workload of particular importance, which achieve high accuracy in computer vision applications. Inside CNNs, a significant number of the post-activation values are zero, resulting in many redundant computations. Recent works have explored this post-activation sparsity on instruction-based CNN accelerators but not on streaming CNN accelerators, despite the fact that streaming architectures are considered the leading design methodology in terms of performance. In this paper, we highlight the challenges associated with exploiting post-activation sparsity for performance gains in streaming CNN accelerators, and demonstrate our approach to address them. Using a set of modern CNN benchmarks, our streaming sparse accelerators achieve 1.41x to 1.93x efficiency (GOP/s/DSP) compared to state-of-the-art instruction-based sparse accelerators.",
      "authors": [
        "Alexander Montgomerie-Corcoran",
        "Zhewen Yu",
        "Jianyi Cheng",
        "Christos-Savvas Bouganis"
      ],
      "published": "2023-07-15T15:03:08Z",
      "updated": "2023-07-15T15:03:08Z",
      "categories": [
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.07821v1",
      "landing_url": "https://arxiv.org/abs/2307.07821v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.07821"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on hardware acceleration for CNNs and post-activation sparsity; no relevance to Spoken Language Models or speech-language modeling, thus fails inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on hardware acceleration for CNNs and post-activation sparsity; no relevance to Spoken Language Models or speech-language modeling, thus fails inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on CNN hardware acceleration exploiting post-activation sparsity, with no mention of spoken language models, speech language models, full-duplex interactions, or related speech modeling architecture or evaluation, and thus does not meet the inclusion criteria relating to spoken language models and synchronous speech interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on CNN hardware acceleration exploiting post-activation sparsity, with no mention of spoken language models, speech language models, full-duplex interactions, or related speech modeling architecture or evaluation, and thus does not meet the inclusion criteria relating to spoken language models and synchronous speech interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs",
    "abstract": "Many real-world datasets have an underlying dynamic graph structure, where entities and their interactions evolve over time. Machine learning models should consider these dynamics in order to harness their full potential in downstream tasks. Previous approaches for graph representation learning have focused on either sampling k-hop neighborhoods, akin to breadth-first search, or random walks, akin to depth-first search. However, these methods are computationally expensive and unsuitable for real-time, low-latency inference on dynamic graphs. To overcome these limitations, we propose graph-sprints a general purpose feature extraction framework for continuous-time-dynamic-graphs (CTDGs) that has low latency and is competitive with state-of-the-art, higher latency models. To achieve this, a streaming, low latency approximation to the random-walk based features is proposed. In our framework, time-aware node embeddings summarizing multi-hop information are computed using only single-hop operations on the incoming edges. We evaluate our proposed approach on three open-source datasets and two in-house datasets, and compare with three state-of-the-art algorithms (TGN-attn, TGN-ID, Jodie). We demonstrate that our graph-sprints features, combined with a machine learning classifier, achieve competitive performance (outperforming all baselines for the node classification tasks in five datasets). Simultaneously, graph-sprints significantly reduce inference latencies, achieving close to an order of magnitude speed-up in our experimental setting.",
    "metadata": {
      "arxiv_id": "2307.08433",
      "title": "From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs",
      "summary": "Many real-world datasets have an underlying dynamic graph structure, where entities and their interactions evolve over time. Machine learning models should consider these dynamics in order to harness their full potential in downstream tasks. Previous approaches for graph representation learning have focused on either sampling k-hop neighborhoods, akin to breadth-first search, or random walks, akin to depth-first search. However, these methods are computationally expensive and unsuitable for real-time, low-latency inference on dynamic graphs. To overcome these limitations, we propose graph-sprints a general purpose feature extraction framework for continuous-time-dynamic-graphs (CTDGs) that has low latency and is competitive with state-of-the-art, higher latency models. To achieve this, a streaming, low latency approximation to the random-walk based features is proposed. In our framework, time-aware node embeddings summarizing multi-hop information are computed using only single-hop operations on the incoming edges. We evaluate our proposed approach on three open-source datasets and two in-house datasets, and compare with three state-of-the-art algorithms (TGN-attn, TGN-ID, Jodie). We demonstrate that our graph-sprints features, combined with a machine learning classifier, achieve competitive performance (outperforming all baselines for the node classification tasks in five datasets). Simultaneously, graph-sprints significantly reduce inference latencies, achieving close to an order of magnitude speed-up in our experimental setting.",
      "authors": [
        "Ahmad Naser Eddin",
        "Jacopo Bono",
        "David Aparício",
        "Hugo Ferreira",
        "João Ascensão",
        "Pedro Ribeiro",
        "Pedro Bizarro"
      ],
      "published": "2023-07-17T12:25:52Z",
      "updated": "2024-02-16T23:34:24Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.08433v5",
      "landing_url": "https://arxiv.org/abs/2307.08433v5",
      "doi": "https://doi.org/10.48550/arXiv.2307.08433"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item does not address Spoken Language Models or speech-language modeling; it focuses on graph embeddings for continuous-time dynamic graphs with no speech processing or interactive speech aspects, thus failing inclusion criteria and meeting exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item does not address Spoken Language Models or speech-language modeling; it focuses on graph embeddings for continuous-time dynamic graphs with no speech processing or interactive speech aspects, thus failing inclusion criteria and meeting exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract discuss a low-latency node embedding framework for dynamic graphs using graph-sprints, focusing on graph-based machine learning methods rather than spoken language models or speech language modeling. There is no mention or implication of speech as the core modeling target, full-duplex spoken language models, speech sequences, or any of the inclusion criteria concepts related to spoken language modeling or speech-LLM architectures. The work does not involve end-to-end speech or mixed speech-text systems, synchronization in interactive speech, or evaluation of speech-related behaviors such as turn-taking, overlap, backchanneling, or low-latency spoken interaction. Therefore, it does not fit the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract discuss a low-latency node embedding framework for dynamic graphs using graph-sprints, focusing on graph-based machine learning methods rather than spoken language models or speech language modeling. There is no mention or implication of speech as the core modeling target, full-duplex spoken language models, speech sequences, or any of the inclusion criteria concepts related to spoken language modeling or speech-LLM architectures. The work does not involve end-to-end speech or mixed speech-text systems, synchronization in interactive speech, or evaluation of speech-related behaviors such as turn-taking, overlap, backchanneling, or low-latency spoken interaction. Therefore, it does not fit the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Augmented Co-Speech Gesture Generation: Including Form and Meaning Features to Guide Learning-Based Gesture Synthesis",
    "abstract": "Due to their significance in human communication, the automatic generation of co-speech gestures in artificial embodied agents has received a lot of attention. Although modern deep learning approaches can generate realistic-looking conversational gestures from spoken language, they often lack the ability to convey meaningful information and generate contextually appropriate gestures. This paper presents an augmented approach to the generation of co-speech gestures that additionally takes into account given form and meaning features for the gestures. Our framework effectively acquires this information from a small corpus with rich semantic annotations and a larger corpus without such information. We provide an analysis of the effects of distinctive feature targets and we report on a human rater evaluation study demonstrating that our framework achieves semantic coherence and person perception on the same level as human ground truth behavior. We make our data pipeline and the generation framework publicly available.",
    "metadata": {
      "arxiv_id": "2307.09597",
      "title": "Augmented Co-Speech Gesture Generation: Including Form and Meaning Features to Guide Learning-Based Gesture Synthesis",
      "summary": "Due to their significance in human communication, the automatic generation of co-speech gestures in artificial embodied agents has received a lot of attention. Although modern deep learning approaches can generate realistic-looking conversational gestures from spoken language, they often lack the ability to convey meaningful information and generate contextually appropriate gestures. This paper presents an augmented approach to the generation of co-speech gestures that additionally takes into account given form and meaning features for the gestures. Our framework effectively acquires this information from a small corpus with rich semantic annotations and a larger corpus without such information. We provide an analysis of the effects of distinctive feature targets and we report on a human rater evaluation study demonstrating that our framework achieves semantic coherence and person perception on the same level as human ground truth behavior. We make our data pipeline and the generation framework publicly available.",
      "authors": [
        "Hendric Voß",
        "Stefan Kopp"
      ],
      "published": "2023-07-13T12:40:50Z",
      "updated": "2023-07-13T12:40:50Z",
      "categories": [
        "cs.HC",
        "cs.GR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.09597v1",
      "landing_url": "https://arxiv.org/abs/2307.09597v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.09597"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on augmented co-speech gesture generation and semantic features for gestures, but does not treat Spoken Language Models or speech-language modeling as the core object of study; no explicit focus on SLM/FD-SLM architecture, training, or evaluation is evident in the title/abstract, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on augmented co-speech gesture generation and semantic features for gestures, but does not treat Spoken Language Models or speech-language modeling as the core object of study; no explicit focus on SLM/FD-SLM architecture, training, or evaluation is evident in the title/abstract, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on generating co-speech gestures using a deep learning framework that analyzes spoken language, but it does not clearly address spoken language models or speech language modeling as the core modeling target, nor does it discuss model training or evaluation in the context of full-duplex spoken language interaction with aspects like cognitive parallelism, low-latency interaction, or speech language sequence modeling, which are required by the inclusion criteria. It rather emphasizes gesture generation guided by speech, not spoken language modeling itself, missing the main thematic focus.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on generating co-speech gestures using a deep learning framework that analyzes spoken language, but it does not clearly address spoken language models or speech language modeling as the core modeling target, nor does it discuss model training or evaluation in the context of full-duplex spoken language interaction with aspects like cognitive parallelism, low-latency interaction, or speech language sequence modeling, which are required by the inclusion criteria. It rather emphasizes gesture generation guided by speech, not spoken language modeling itself, missing the main thematic focus.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Blind Image Quality Assessment Using Multi-Stream Architecture with Spatial and Channel Attention",
    "abstract": "BIQA (Blind Image Quality Assessment) is an important field of study that evaluates images automatically. Although significant progress has been made, blind image quality assessment remains a difficult task since images vary in content and distortions. Most algorithms generate quality without emphasizing the important region of interest. In order to solve this, a multi-stream spatial and channel attention-based algorithm is being proposed. This algorithm generates more accurate predictions with a high correlation to human perceptual assessment by combining hybrid features from two different backbones, followed by spatial and channel attention to provide high weights to the region of interest. Four legacy image quality assessment datasets are used to validate the effectiveness of our proposed approach. Authentic and synthetic distortion image databases are used to demonstrate the effectiveness of the proposed method, and we show that it has excellent generalization properties with a particular focus on the perceptual foreground information.",
    "metadata": {
      "arxiv_id": "2307.09857",
      "title": "Blind Image Quality Assessment Using Multi-Stream Architecture with Spatial and Channel Attention",
      "summary": "BIQA (Blind Image Quality Assessment) is an important field of study that evaluates images automatically. Although significant progress has been made, blind image quality assessment remains a difficult task since images vary in content and distortions. Most algorithms generate quality without emphasizing the important region of interest. In order to solve this, a multi-stream spatial and channel attention-based algorithm is being proposed. This algorithm generates more accurate predictions with a high correlation to human perceptual assessment by combining hybrid features from two different backbones, followed by spatial and channel attention to provide high weights to the region of interest. Four legacy image quality assessment datasets are used to validate the effectiveness of our proposed approach. Authentic and synthetic distortion image databases are used to demonstrate the effectiveness of the proposed method, and we show that it has excellent generalization properties with a particular focus on the perceptual foreground information.",
      "authors": [
        "Muhammad Azeem Aslam",
        "Xu Wei",
        "Hassan Khalid",
        "Nisar Ahmed",
        "Zhu Shuangtong",
        "Xin Liu",
        "Yimei Xu"
      ],
      "published": "2023-07-19T09:36:08Z",
      "updated": "2024-10-07T18:56:53Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.09857v3",
      "landing_url": "https://arxiv.org/abs/2307.09857v3",
      "doi": "https://doi.org/10.48550/arXiv.2307.09857"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Not about Spoken Language Models or speech; it's a computer vision BIQA paper with no speech modeling focus, so it does not meet inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Not about Spoken Language Models or speech; it's a computer vision BIQA paper with no speech modeling focus, so it does not meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on blind image quality assessment using multi-stream architecture and attention mechanisms, which is unrelated to spoken language models or speech language modeling as described in the inclusion criteria. It does not address speech modeling, full-duplex spoken language modeling, synchronization, or real-time interaction-related aspects. Therefore, it does not meet the inclusion criteria relevant to speech-centric research and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on blind image quality assessment using multi-stream architecture and attention mechanisms, which is unrelated to spoken language models or speech language modeling as described in the inclusion criteria. It does not address speech modeling, full-duplex spoken language modeling, synchronization, or real-time interaction-related aspects. Therefore, it does not meet the inclusion criteria relevant to speech-centric research and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Real-World Evaluation of Full-Duplex Millimeter Wave Communication Systems",
    "abstract": "Noteworthy strides continue to be made in the development of full-duplex millimeter wave (mmWave) communication systems, but most of this progress has been built on theoretical models and validated through simulation. In this work, we conduct a long overdue real-world evaluation of full-duplex mmWave systems using off-the-shelf 60 GHz phased arrays. Using an experimental full-duplex base station, we collect over 200,000 measurements of self-interference by electronically sweeping its transmit and receive beams across a dense spatial profile, shedding light on the effects of the environment, array positioning, and beam steering direction. We then call attention to five key challenges faced by practical full-duplex mmWave systems and, with these in mind, propose a general framework for beamforming-based full-duplex solutions. Guided by this framework, we introduce a novel solution called STEER+, a more robust version of recent work called STEER, and experimentally evaluate both in a real-world setting with actual downlink and uplink users. Rather than purely minimize self-interference as with STEER, STEER+ makes use of additional measurements to maximize spectral efficiency, which proves to make it much less sensitive to one's choice of design parameters. We experimentally show that STEER+ can reliably reduce self-interference to near or below the noise floor while maintaining high SNR on the downlink and uplink, thus enabling full-duplex operation purely via beamforming.",
    "metadata": {
      "arxiv_id": "2307.10523",
      "title": "Real-World Evaluation of Full-Duplex Millimeter Wave Communication Systems",
      "summary": "Noteworthy strides continue to be made in the development of full-duplex millimeter wave (mmWave) communication systems, but most of this progress has been built on theoretical models and validated through simulation. In this work, we conduct a long overdue real-world evaluation of full-duplex mmWave systems using off-the-shelf 60 GHz phased arrays. Using an experimental full-duplex base station, we collect over 200,000 measurements of self-interference by electronically sweeping its transmit and receive beams across a dense spatial profile, shedding light on the effects of the environment, array positioning, and beam steering direction. We then call attention to five key challenges faced by practical full-duplex mmWave systems and, with these in mind, propose a general framework for beamforming-based full-duplex solutions. Guided by this framework, we introduce a novel solution called STEER+, a more robust version of recent work called STEER, and experimentally evaluate both in a real-world setting with actual downlink and uplink users. Rather than purely minimize self-interference as with STEER, STEER+ makes use of additional measurements to maximize spectral efficiency, which proves to make it much less sensitive to one's choice of design parameters. We experimentally show that STEER+ can reliably reduce self-interference to near or below the noise floor while maintaining high SNR on the downlink and uplink, thus enabling full-duplex operation purely via beamforming.",
      "authors": [
        "Ian P. Roberts",
        "Yu Zhang",
        "Tawfik Osman",
        "Ahmed Alkhateeb"
      ],
      "published": "2023-07-20T01:50:47Z",
      "updated": "2023-07-20T01:50:47Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.10523v1",
      "landing_url": "https://arxiv.org/abs/2307.10523v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.10523"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on full-duplex millimeter-wave wireless communications and beamforming; it does not involve spoken language models or speech-language modeling concepts, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on full-duplex millimeter-wave wireless communications and beamforming; it does not involve spoken language models or speech-language modeling concepts, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on full-duplex millimeter wave communication systems using phased arrays, experimental setups, and beamforming techniques; it addresses full-duplex wireless communication challenges but does not discuss spoken language models, speech language modeling, or any speech-related modeling or evaluation criteria central to the inclusion criteria. It lacks any mention of voice as a modeling object, speech language models, or evaluations related to synchronous spoken interaction or dialogue systems, which are essential for inclusion. Therefore, it fails all major thematic inclusion criteria and is unrelated to the research scope defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on full-duplex millimeter wave communication systems using phased arrays, experimental setups, and beamforming techniques; it addresses full-duplex wireless communication challenges but does not discuss spoken language models, speech language modeling, or any speech-related modeling or evaluation criteria central to the inclusion criteria. It lacks any mention of voice as a modeling object, speech language models, or evaluations related to synchronous spoken interaction or dialogue systems, which are essential for inclusion. Therefore, it fails all major thematic inclusion criteria and is unrelated to the research scope defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SC VALL-E: Style-Controllable Zero-Shot Text to Speech Synthesizer",
    "abstract": "Expressive speech synthesis models are trained by adding corpora with diverse speakers, various emotions, and different speaking styles to the dataset, in order to control various characteristics of speech and generate the desired voice. In this paper, we propose a style control (SC) VALL-E model based on the neural codec language model (called VALL-E), which follows the structure of the generative pretrained transformer 3 (GPT-3). The proposed SC VALL-E takes input from text sentences and prompt audio and is designed to generate controllable speech by not simply mimicking the characteristics of the prompt audio but by controlling the attributes to produce diverse voices. We identify tokens in the style embedding matrix of the newly designed style network that represent attributes such as emotion, speaking rate, pitch, and voice intensity, and design a model that can control these attributes. To evaluate the performance of SC VALL-E, we conduct comparative experiments with three representative expressive speech synthesis models: global style token (GST) Tacotron2, variational autoencoder (VAE) Tacotron2, and original VALL-E. We measure word error rate (WER), F0 voiced error (FVE), and F0 gross pitch error (F0GPE) as evaluation metrics to assess the accuracy of generated sentences. For comparing the quality of synthesized speech, we measure comparative mean option score (CMOS) and similarity mean option score (SMOS). To evaluate the style control ability of the generated speech, we observe the changes in F0 and mel-spectrogram by modifying the trained tokens. When using prompt audio that is not present in the training data, SC VALL-E generates a variety of expressive sounds and demonstrates competitive performance compared to the existing models. Our implementation, pretrained models, and audio samples are located on GitHub.",
    "metadata": {
      "arxiv_id": "2307.10550",
      "title": "SC VALL-E: Style-Controllable Zero-Shot Text to Speech Synthesizer",
      "summary": "Expressive speech synthesis models are trained by adding corpora with diverse speakers, various emotions, and different speaking styles to the dataset, in order to control various characteristics of speech and generate the desired voice. In this paper, we propose a style control (SC) VALL-E model based on the neural codec language model (called VALL-E), which follows the structure of the generative pretrained transformer 3 (GPT-3). The proposed SC VALL-E takes input from text sentences and prompt audio and is designed to generate controllable speech by not simply mimicking the characteristics of the prompt audio but by controlling the attributes to produce diverse voices. We identify tokens in the style embedding matrix of the newly designed style network that represent attributes such as emotion, speaking rate, pitch, and voice intensity, and design a model that can control these attributes. To evaluate the performance of SC VALL-E, we conduct comparative experiments with three representative expressive speech synthesis models: global style token (GST) Tacotron2, variational autoencoder (VAE) Tacotron2, and original VALL-E. We measure word error rate (WER), F0 voiced error (FVE), and F0 gross pitch error (F0GPE) as evaluation metrics to assess the accuracy of generated sentences. For comparing the quality of synthesized speech, we measure comparative mean option score (CMOS) and similarity mean option score (SMOS). To evaluate the style control ability of the generated speech, we observe the changes in F0 and mel-spectrogram by modifying the trained tokens. When using prompt audio that is not present in the training data, SC VALL-E generates a variety of expressive sounds and demonstrates competitive performance compared to the existing models. Our implementation, pretrained models, and audio samples are located on GitHub.",
      "authors": [
        "Daegyeom Kim",
        "Seongho Hong",
        "Yong-Hoon Choi"
      ],
      "published": "2023-07-20T03:28:06Z",
      "updated": "2023-07-20T03:28:06Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.10550v1",
      "landing_url": "https://arxiv.org/abs/2307.10550v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.10550"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本題雖以語音模型與語音語言建模為主題，提出以語音風格控制的 TTS，但未以 Spoken Language Models / Speech Language Models 為核心，亦未涵蓋全雙工對話互動或對話仲裁等評估，因此不符合核心納入條件，應排除。",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "本題雖以語音模型與語音語言建模為主題，提出以語音風格控制的 TTS，但未以 Spoken Language Models / Speech Language Models 為核心，亦未涵蓋全雙工對話互動或對話仲裁等評估，因此不符合核心納入條件，應排除。",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a style-controllable text-to-speech synthesizer focusing on expressive speech synthesis but does not explicitly mention spoken language models, speech language modeling, full-duplex or synchronous interaction capabilities, or the relevant evaluation aspects like cognitive parallelism, turn-taking, or low-latency interactions as required by the inclusion criteria. It primarily focuses on TTS quality and style control metrics such as WER, pitch errors, and subjective quality scores, which falls under excluded single downstream tasks (TTS) without addressing the SLMs or FD-SLMs research theme.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes a style-controllable text-to-speech synthesizer focusing on expressive speech synthesis but does not explicitly mention spoken language models, speech language modeling, full-duplex or synchronous interaction capabilities, or the relevant evaluation aspects like cognitive parallelism, turn-taking, or low-latency interactions as required by the inclusion criteria. It primarily focuses on TTS quality and style control metrics such as WER, pitch errors, and subjective quality scores, which falls under excluded single downstream tasks (TTS) without addressing the SLMs or FD-SLMs research theme.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Topologically Regularized Multiple Instance Learning to Harness Data Scarcity",
    "abstract": "In biomedical data analysis, Multiple Instance Learning (MIL) models have emerged as a powerful tool to classify patients' microscopy samples. However, the data-intensive requirement of these models poses a significant challenge in scenarios with scarce data availability, e.g., in rare diseases. We introduce a topological regularization term to MIL to mitigate this challenge. It provides a shape-preserving inductive bias that compels the encoder to maintain the essential geometrical-topological structure of input bags during projection into latent space. This enhances the performance and generalization of the MIL classifier regardless of the aggregation function, particularly for scarce training data. The effectiveness of our method is confirmed through experiments across a range of datasets, showing an average enhancement of 2.8% for MIL benchmarks, 15.3% for synthetic MIL datasets, and 5.5% for real-world biomedical datasets over the current state-of-the-art.",
    "metadata": {
      "arxiv_id": "2307.14025",
      "title": "Topologically Regularized Multiple Instance Learning to Harness Data Scarcity",
      "summary": "In biomedical data analysis, Multiple Instance Learning (MIL) models have emerged as a powerful tool to classify patients' microscopy samples. However, the data-intensive requirement of these models poses a significant challenge in scenarios with scarce data availability, e.g., in rare diseases. We introduce a topological regularization term to MIL to mitigate this challenge. It provides a shape-preserving inductive bias that compels the encoder to maintain the essential geometrical-topological structure of input bags during projection into latent space. This enhances the performance and generalization of the MIL classifier regardless of the aggregation function, particularly for scarce training data. The effectiveness of our method is confirmed through experiments across a range of datasets, showing an average enhancement of 2.8% for MIL benchmarks, 15.3% for synthetic MIL datasets, and 5.5% for real-world biomedical datasets over the current state-of-the-art.",
      "authors": [
        "Salome Kazeminia",
        "Carsten Marr",
        "Bastian Rieck"
      ],
      "published": "2023-07-26T08:14:18Z",
      "updated": "2024-03-11T11:14:15Z",
      "categories": [
        "cs.LG",
        "cs.CV",
        "eess.IV",
        "q-bio.QM",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.14025v2",
      "landing_url": "https://arxiv.org/abs/2307.14025v2",
      "doi": "https://doi.org/10.48550/arXiv.2307.14025"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about topologically regularized MIL for scarce data in biomedical imaging and does not involve Spoken Language Models or speech-language modeling, nor dialogue/synchronous interaction aspects, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about topologically regularized MIL for scarce data in biomedical imaging and does not involve Spoken Language Models or speech-language modeling, nor dialogue/synchronous interaction aspects, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Multiple Instance Learning for biomedical data analysis, particularly handling data scarcity, and does not address Spoken Language Models (SLMs), speech language modeling, or full-duplex interaction aspects outlined in the inclusion criteria. It also does not discuss synchronization, cognitive parallelism, or evaluation metrics related to real-time interactive speech systems. Therefore, it fails to meet the core thematic inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Multiple Instance Learning for biomedical data analysis, particularly handling data scarcity, and does not address Spoken Language Models (SLMs), speech language modeling, or full-duplex interaction aspects outlined in the inclusion criteria. It also does not discuss synchronization, cognitive parallelism, or evaluation metrics related to real-time interactive speech systems. Therefore, it fails to meet the core thematic inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Learning to simulate partially known spatio-temporal dynamics with trainable difference operators",
    "abstract": "Recently, using neural networks to simulate spatio-temporal dynamics has received a lot of attention. However, most existing methods adopt pure data-driven black-box models, which have limited accuracy and interpretability. By combining trainable difference operators with black-box models, we propose a new hybrid architecture explicitly embedded with partial prior knowledge of the underlying PDEs named PDE-Net++. Furthermore, we introduce two distinct options called the trainable flipping difference layer (TFDL) and the trainable dynamic difference layer (TDDL) for the difference operators. Numerous numerical experiments have demonstrated that PDE-Net++ has superior prediction accuracy and better extrapolation performance than black-box models.",
    "metadata": {
      "arxiv_id": "2307.14395",
      "title": "Learning to simulate partially known spatio-temporal dynamics with trainable difference operators",
      "summary": "Recently, using neural networks to simulate spatio-temporal dynamics has received a lot of attention. However, most existing methods adopt pure data-driven black-box models, which have limited accuracy and interpretability. By combining trainable difference operators with black-box models, we propose a new hybrid architecture explicitly embedded with partial prior knowledge of the underlying PDEs named PDE-Net++. Furthermore, we introduce two distinct options called the trainable flipping difference layer (TFDL) and the trainable dynamic difference layer (TDDL) for the difference operators. Numerous numerical experiments have demonstrated that PDE-Net++ has superior prediction accuracy and better extrapolation performance than black-box models.",
      "authors": [
        "Xiang Huang",
        "Zhuoyuan Li",
        "Hongsheng Liu",
        "Zidong Wang",
        "Hongye Zhou",
        "Bin Dong",
        "Bei Hua"
      ],
      "published": "2023-07-26T10:05:18Z",
      "updated": "2023-07-26T10:05:18Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.14395v1",
      "landing_url": "https://arxiv.org/abs/2307.14395v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.14395"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on PDE-inspired neural networks for spatio-temporal dynamics and trainable difference operators, with no reference to Spoken Language Models, speech signaling, or conversational synchronization, thus it does not meet the inclusion criteria and aligns with the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on PDE-inspired neural networks for spatio-temporal dynamics and trainable difference operators, with no reference to Spoken Language Models, speech signaling, or conversational synchronization, thus it does not meet the inclusion criteria and aligns with the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using neural networks to simulate spatio-temporal dynamics with hybrid models incorporating PDE knowledge, which does not relate to Spoken Language Models (SLMs) or full-duplex spoken language interaction as defined by the inclusion criteria. It lacks any mention of speech language modeling, real-time spoken interaction, or synchronization, and thus does not meet the core subject requirements nor addresses the specified criteria around spoken language model architectures or evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using neural networks to simulate spatio-temporal dynamics with hybrid models incorporating PDE knowledge, which does not relate to Spoken Language Models (SLMs) or full-duplex spoken language interaction as defined by the inclusion criteria. It lacks any mention of speech language modeling, real-time spoken interaction, or synchronization, and thus does not meet the core subject requirements nor addresses the specified criteria around spoken language model architectures or evaluation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Latent-Shift: Gradient of Entropy Helps Neural Codecs",
    "abstract": "End-to-end image/video codecs are getting competitive compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques such as easy adaptation on perceptual distortion metrics and high performance on specific domains thanks to their learning ability. However, state of the art neural codecs does not take advantage of the existence of gradient of entropy in decoding device. In this paper, we theoretically show that gradient of entropy (available at decoder side) is correlated with the gradient of the reconstruction error (which is not available at decoder side). We then demonstrate experimentally that this gradient can be used on various compression methods, leading to a $1-2\\%$ rate savings for the same quality. Our method is orthogonal to other improvements and brings independent rate savings.",
    "metadata": {
      "arxiv_id": "2308.00725",
      "title": "Latent-Shift: Gradient of Entropy Helps Neural Codecs",
      "summary": "End-to-end image/video codecs are getting competitive compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques such as easy adaptation on perceptual distortion metrics and high performance on specific domains thanks to their learning ability. However, state of the art neural codecs does not take advantage of the existence of gradient of entropy in decoding device. In this paper, we theoretically show that gradient of entropy (available at decoder side) is correlated with the gradient of the reconstruction error (which is not available at decoder side). We then demonstrate experimentally that this gradient can be used on various compression methods, leading to a $1-2\\%$ rate savings for the same quality. Our method is orthogonal to other improvements and brings independent rate savings.",
      "authors": [
        "Muhammet Balcilar",
        "Bharath Bhushan Damodaran",
        "Karam Naser",
        "Franck Galpin",
        "Pierre Hellier"
      ],
      "published": "2023-08-01T15:12:36Z",
      "updated": "2023-08-01T15:12:36Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.00725v1",
      "landing_url": "https://arxiv.org/abs/2308.00725v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.00725"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets neural codecs for image/video compression and discusses gradient of entropy at decoder side, with no focus on Spoken Language Models or any speech-language modeling or interactive dialogue; thus it does not meet inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets neural codecs for image/video compression and discusses gradient of entropy at decoder side, with no focus on Spoken Language Models or any speech-language modeling or interactive dialogue; thus it does not meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on neural image/video codecs and their compression efficiency, without any mention of spoken language models, speech language modeling, or related interaction paradigms such as full-duplex speech interaction or real-time speech behavior arbitration. It does not address speech as a core modeling target nor discuss model architectures or evaluations relevant to spoken language models, thus it does not meet the inclusion criteria and falls under exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on neural image/video codecs and their compression efficiency, without any mention of spoken language models, speech language modeling, or related interaction paradigms such as full-duplex speech interaction or real-time speech behavior arbitration. It does not address speech as a core modeling target nor discuss model architectures or evaluations relevant to spoken language models, thus it does not meet the inclusion criteria and falls under exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion",
    "abstract": "Deep generative models can generate high-fidelity audio conditioned on various types of representations (e.g., mel-spectrograms, Mel-frequency Cepstral Coefficients (MFCC)). Recently, such models have been used to synthesize audio waveforms conditioned on highly compressed representations. Although such methods produce impressive results, they are prone to generate audible artifacts when the conditioning is flawed or imperfect. An alternative modeling approach is to use diffusion models. However, these have mainly been used as speech vocoders (i.e., conditioned on mel-spectrograms) or generating relatively low sampling rate signals. In this work, we propose a high-fidelity multi-band diffusion-based framework that generates any type of audio modality (e.g., speech, music, environmental sounds) from low-bitrate discrete representations. At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality. Training and, evaluation code, along with audio samples, are available on the facebookresearch/audiocraft Github page.",
    "metadata": {
      "arxiv_id": "2308.02560",
      "title": "From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion",
      "summary": "Deep generative models can generate high-fidelity audio conditioned on various types of representations (e.g., mel-spectrograms, Mel-frequency Cepstral Coefficients (MFCC)). Recently, such models have been used to synthesize audio waveforms conditioned on highly compressed representations. Although such methods produce impressive results, they are prone to generate audible artifacts when the conditioning is flawed or imperfect. An alternative modeling approach is to use diffusion models. However, these have mainly been used as speech vocoders (i.e., conditioned on mel-spectrograms) or generating relatively low sampling rate signals. In this work, we propose a high-fidelity multi-band diffusion-based framework that generates any type of audio modality (e.g., speech, music, environmental sounds) from low-bitrate discrete representations. At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality. Training and, evaluation code, along with audio samples, are available on the facebookresearch/audiocraft Github page.",
      "authors": [
        "Robin San Roman",
        "Yossi Adi",
        "Antoine Deleforge",
        "Romain Serizel",
        "Gabriel Synnaeve",
        "Alexandre Défossez"
      ],
      "published": "2023-08-02T22:14:29Z",
      "updated": "2023-11-08T10:04:00Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.02560v2",
      "landing_url": "https://arxiv.org/abs/2308.02560v2",
      "doi": "https://doi.org/10.48550/arXiv.2308.02560"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work is a diffusion-based general audio generation method from discrete tokens, not centered on Spoken Language Models or speech-language modeling, and it lacks explicit FD-SLM or interactive speech dialogue evaluation, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work is a diffusion-based general audio generation method from discrete tokens, not centered on Spoken Language Models or speech-language modeling, and it lacks explicit FD-SLM or interactive speech dialogue evaluation, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on generating high-fidelity audio from low-bitrate discrete representations using multi-band diffusion models; however, it does not explicitly mention spoken language models or speech language models as core research topics, nor does it discuss synchronization, turn-taking, or full-duplex interaction capabilities. The abstract emphasizes audio generation quality rather than dialogue interaction or language modeling aspects relevant to the inclusion criteria, and lacks mention of spoken language model terminology or evaluation on interactive speech behavior, leading to exclusion based on the given criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on generating high-fidelity audio from low-bitrate discrete representations using multi-band diffusion models; however, it does not explicitly mention spoken language models or speech language models as core research topics, nor does it discuss synchronization, turn-taking, or full-duplex interaction capabilities. The abstract emphasizes audio generation quality rather than dialogue interaction or language modeling aspects relevant to the inclusion criteria, and lacks mention of spoken language model terminology or evaluation on interactive speech behavior, leading to exclusion based on the given criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Gloss Alignment Using Word Embeddings",
    "abstract": "Capturing and annotating Sign language datasets is a time consuming and costly process. Current datasets are orders of magnitude too small to successfully train unconstrained \\acf{slt} models. As a result, research has turned to TV broadcast content as a source of large-scale training data, consisting of both the sign language interpreter and the associated audio subtitle. However, lack of sign language annotation limits the usability of this data and has led to the development of automatic annotation techniques such as sign spotting. These spottings are aligned to the video rather than the subtitle, which often results in a misalignment between the subtitle and spotted signs. In this paper we propose a method for aligning spottings with their corresponding subtitles using large spoken language models. Using a single modality means our method is computationally inexpensive and can be utilized in conjunction with existing alignment techniques. We quantitatively demonstrate the effectiveness of our method on the \\acf{mdgs} and \\acf{bobsl} datasets, recovering up to a 33.22 BLEU-1 score in word alignment.",
    "metadata": {
      "arxiv_id": "2308.04248",
      "title": "Gloss Alignment Using Word Embeddings",
      "summary": "Capturing and annotating Sign language datasets is a time consuming and costly process. Current datasets are orders of magnitude too small to successfully train unconstrained \\acf{slt} models. As a result, research has turned to TV broadcast content as a source of large-scale training data, consisting of both the sign language interpreter and the associated audio subtitle. However, lack of sign language annotation limits the usability of this data and has led to the development of automatic annotation techniques such as sign spotting. These spottings are aligned to the video rather than the subtitle, which often results in a misalignment between the subtitle and spotted signs. In this paper we propose a method for aligning spottings with their corresponding subtitles using large spoken language models. Using a single modality means our method is computationally inexpensive and can be utilized in conjunction with existing alignment techniques. We quantitatively demonstrate the effectiveness of our method on the \\acf{mdgs} and \\acf{bobsl} datasets, recovering up to a 33.22 BLEU-1 score in word alignment.",
      "authors": [
        "Harry Walsh",
        "Ozge Mercanoglu Sincan",
        "Ben Saunders",
        "Richard Bowden"
      ],
      "published": "2023-08-08T13:26:53Z",
      "updated": "2023-08-08T13:26:53Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.04248v1",
      "landing_url": "https://arxiv.org/abs/2308.04248v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.04248"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly centers on Spoken Language Models (large spoken language models) to align multimodal data (sign language spottings with subtitles), discusses model approach/training/evaluation, is within the past 3 years, and is not limited to ASR/TTS or single-turn tasks, meeting all inclusion criteria and none of the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly centers on Spoken Language Models (large spoken language models) to align multimodal data (sign language spottings with subtitles), discusses model approach/training/evaluation, is within the past 3 years, and is not limited to ASR/TTS or single-turn tasks, meeting all inclusion criteria and none of the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses primarily on alignment of sign language glosses with subtitles using word embeddings and large spoken language models, but it does not explicitly frame the work around Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) as core modeling or reasoning objects based on speech signals. The research also centers on subtitle alignment rather than end-to-end speech modeling or interactive speech language modeling, and there is no explicit discussion of model architectures for speech or evaluations related to synchronous or full-duplex interaction capabilities. Therefore, the study does not meet the strict inclusion criteria focused on speech-centric modeling and real-time duplex interaction capabilities as specified.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses primarily on alignment of sign language glosses with subtitles using word embeddings and large spoken language models, but it does not explicitly frame the work around Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) as core modeling or reasoning objects based on speech signals. The research also centers on subtitle alignment rather than end-to-end speech modeling or interactive speech language modeling, and there is no explicit discussion of model architectures for speech or evaluations related to synchronous or full-duplex interaction capabilities. Therefore, the study does not meet the strict inclusion criteria focused on speech-centric modeling and real-time duplex interaction capabilities as specified.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers were considered: JuniorNano rated inclusion (5) citing the abstract’s mention of “large spoken language models,” but JuniorMini correctly noted the paper focuses on aligning sign language spottings to subtitles (text) and does not treat speech/acoustic signals or SLM architectures, evaluations, or real-time/full-duplex interaction as the core research contribution. The abstract lacks any discussion of speech-level language modeling, speech tokens/representations, Speech–LLM architectures, latency/overlap handling, or benchmarks relevant to FD-SLMs, so it fails the requirement that spoken language models and speech be the primary modeling and evaluation focus. Score: 2 — Better to exclude: the work uses text/subtitle alignment with sign spotting and does not meet the SLM/FD-SLM inclusion criteria.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers were considered: JuniorNano rated inclusion (5) citing the abstract’s mention of “large spoken language models,” but JuniorMini correctly noted the paper focuses on aligning sign language spottings to subtitles (text) and does not treat speech/acoustic signals or SLM architectures, evaluations, or real-time/full-duplex interaction as the core research contribution. The abstract lacks any discussion of speech-level language modeling, speech tokens/representations, Speech–LLM architectures, latency/overlap handling, or benchmarks relevant to FD-SLMs, so it fails the requirement that spoken language models and speech be the primary modeling and evaluation focus. Score: 2 — Better to exclude: the work uses text/subtitle alignment with sign spotting and does not meet the SLM/FD-SLM inclusion criteria.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Decoding Layer Saliency in Language Transformers",
    "abstract": "In this paper, we introduce a strategy for identifying textual saliency in large-scale language models applied to classification tasks. In visual networks where saliency is more well-studied, saliency is naturally localized through the convolutional layers of the network; however, the same is not true in modern transformer-stack networks used to process natural language. We adapt gradient-based saliency methods for these networks, propose a method for evaluating the degree of semantic coherence of each layer, and demonstrate consistent improvement over numerous other methods for textual saliency on multiple benchmark classification datasets. Our approach requires no additional training or access to labelled data, and is comparatively very computationally efficient.",
    "metadata": {
      "arxiv_id": "2308.05219",
      "title": "Decoding Layer Saliency in Language Transformers",
      "summary": "In this paper, we introduce a strategy for identifying textual saliency in large-scale language models applied to classification tasks. In visual networks where saliency is more well-studied, saliency is naturally localized through the convolutional layers of the network; however, the same is not true in modern transformer-stack networks used to process natural language. We adapt gradient-based saliency methods for these networks, propose a method for evaluating the degree of semantic coherence of each layer, and demonstrate consistent improvement over numerous other methods for textual saliency on multiple benchmark classification datasets. Our approach requires no additional training or access to labelled data, and is comparatively very computationally efficient.",
      "authors": [
        "Elizabeth M. Hou",
        "Gregory Castanon"
      ],
      "published": "2023-08-09T20:53:22Z",
      "updated": "2023-08-09T20:53:22Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.05219v1",
      "landing_url": "https://arxiv.org/abs/2308.05219v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.05219"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item investigates textual saliency in language transformers and does not treat Spoken Language Models or speech-language modeling as the core object, hence it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item investigates textual saliency in language transformers and does not treat Spoken Language Models or speech-language modeling as the core object, hence it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on identifying textual saliency in transformer-based language models applied to classification tasks with no mention of spoken language models, speech-based modeling, or synchronization/interactivity features specific to Full-Duplex Spoken Language Models; it treats language as text rather than speech and does not address speech-based architectures, training, or evaluation relevant to the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on identifying textual saliency in transformer-based language models applied to classification tasks with no mention of spoken language models, speech-based modeling, or synchronization/interactivity features specific to Full-Duplex Spoken Language Models; it treats language as text rather than speech and does not address speech-based architectures, training, or evaluation relevant to the criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SpeechX: Neural Codec Language Model as a Versatile Speech Transformer",
    "abstract": "Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.",
    "metadata": {
      "arxiv_id": "2308.06873",
      "title": "SpeechX: Neural Codec Language Model as a Versatile Speech Transformer",
      "summary": "Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.",
      "authors": [
        "Xiaofei Wang",
        "Manthan Thakker",
        "Zhuo Chen",
        "Naoyuki Kanda",
        "Sefik Emre Eskimez",
        "Sanyuan Chen",
        "Min Tang",
        "Shujie Liu",
        "Jinyu Li",
        "Takuya Yoshioka"
      ],
      "published": "2023-08-14T01:01:19Z",
      "updated": "2024-06-25T18:38:28Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.06873v2",
      "landing_url": "https://arxiv.org/abs/2308.06873v2",
      "doi": "https://doi.org/10.48550/arXiv.2308.06873"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item clearly centers on Spoken Language Models / speech-language modeling, treats speech as the core modeling target rather than just input to NLP, discusses model architecture (neural codec language modeling) and multi-task training/evaluation across diverse speech tasks, and is published within the last 3 years, hence meeting inclusion criteria and not fitting the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This item clearly centers on Spoken Language Models / speech-language modeling, treats speech as the core modeling target rather than just input to NLP, discusses model architecture (neural codec language modeling) and multi-task training/evaluation across diverse speech tasks, and is published within the last 3 years, hence meeting inclusion criteria and not fitting the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces SpeechX, a versatile speech generation model focusing on tasks like zero-shot text-to-speech and speech transformation, primarily evaluating audio-to-audio transformations rather than spoken language modeling involving turn-taking, full-duplex interaction, or speech language models as core objects; it lacks mention of spoken language models, language modeling of speech, or synchronous interaction capabilities, thus not meeting the inclusion criteria focused on spoken language modeling and full-duplex spoken language interaction, and it focuses more on speech generation and enhancement rather than modeling conversational interactions or behavior arbitration.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study introduces SpeechX, a versatile speech generation model focusing on tasks like zero-shot text-to-speech and speech transformation, primarily evaluating audio-to-audio transformations rather than spoken language modeling involving turn-taking, full-duplex interaction, or speech language models as core objects; it lacks mention of spoken language models, language modeling of speech, or synchronous interaction capabilities, thus not meeting the inclusion criteria focused on spoken language modeling and full-duplex spoken language interaction, and it focuses more on speech generation and enhancement rather than modeling conversational interactions or behavior arbitration.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Integrating the two junior reviews: although the paper does not target full‑duplex interaction, the abstract explicitly describes “neural codec language modeling” and presents a unified speech generation/transformation model that treats speech tokens as the modeling target and reports multi‑task evaluations (zero‑shot TTS, enhancement, speaker extraction, editing), so it satisfies the requirement of language modeling of speech (an equivalent to Spoken Language Models) and discusses architecture/training/evaluation—therefore it should be included.",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "Integrating the two junior reviews: although the paper does not target full‑duplex interaction, the abstract explicitly describes “neural codec language modeling” and presents a unified speech generation/transformation model that treats speech tokens as the modeling target and reports multi‑task evaluations (zero‑shot TTS, enhancement, speaker extraction, editing), so it satisfies the requirement of language modeling of speech (an equivalent to Spoken Language Models) and discusses architecture/training/evaluation—therefore it should be included.",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Text Injection for Capitalization and Turn-Taking Prediction in Speech Models",
    "abstract": "Text injection for automatic speech recognition (ASR), wherein unpaired text-only data is used to supplement paired audio-text data, has shown promising improvements for word error rate. This study examines the use of text injection for auxiliary tasks, which are the non-ASR tasks often performed by an E2E model. In this work, we use joint end-to-end and internal language model training (JEIT) as our text injection algorithm to train an ASR model which performs two auxiliary tasks. The first is capitalization, which is a de-normalization task. The second is turn-taking prediction, which attempts to identify whether a user has completed their conversation turn in a digital assistant interaction. We show results demonstrating that our text injection method boosts capitalization performance for long-tail data, and improves turn-taking detection recall.",
    "metadata": {
      "arxiv_id": "2308.07395",
      "title": "Text Injection for Capitalization and Turn-Taking Prediction in Speech Models",
      "summary": "Text injection for automatic speech recognition (ASR), wherein unpaired text-only data is used to supplement paired audio-text data, has shown promising improvements for word error rate. This study examines the use of text injection for auxiliary tasks, which are the non-ASR tasks often performed by an E2E model. In this work, we use joint end-to-end and internal language model training (JEIT) as our text injection algorithm to train an ASR model which performs two auxiliary tasks. The first is capitalization, which is a de-normalization task. The second is turn-taking prediction, which attempts to identify whether a user has completed their conversation turn in a digital assistant interaction. We show results demonstrating that our text injection method boosts capitalization performance for long-tail data, and improves turn-taking detection recall.",
      "authors": [
        "Shaan Bijwadia",
        "Shuo-yiin Chang",
        "Weiran Wang",
        "Zhong Meng",
        "Hao Zhang",
        "Tara N. Sainath"
      ],
      "published": "2023-08-14T18:28:04Z",
      "updated": "2023-08-14T18:28:04Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.07395v1",
      "landing_url": "https://arxiv.org/abs/2308.07395v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.07395"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目以語音模型為核心，涉及語音輸入的建模與對話行為（打断/ turn-taking）的輔助任務，並討論架構、訓練與評估，符合包含條件且未落入排除條件，因此給予較高分數（4，Better to include）。",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "此項目以語音模型為核心，涉及語音輸入的建模與對話行為（打断/ turn-taking）的輔助任務，並討論架構、訓練與評估，符合包含條件且未落入排除條件，因此給予較高分數（4，Better to include）。",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on applying text injection to improve auxiliary tasks in speech models, specifically capitalization and turn-taking prediction; however, it primarily addresses improvements in ASR-related tasks without explicit discussion of full-duplex spoken language modeling, cognitive parallelism, or engineered/learned synchronization for synchronous multi-party interaction as required by the inclusion criteria. It also lacks mentioning 'spoken language model(s)' or equivalent phrases explicitly. Therefore, it does not meet all inclusion criteria and partially aligns with exclusion aspects of focusing on ASR and single downstream tasks without broader spoken language modeling perspective.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on applying text injection to improve auxiliary tasks in speech models, specifically capitalization and turn-taking prediction; however, it primarily addresses improvements in ASR-related tasks without explicit discussion of full-duplex spoken language modeling, cognitive parallelism, or engineered/learned synchronization for synchronous multi-party interaction as required by the inclusion criteria. It also lacks mentioning 'spoken language model(s)' or equivalent phrases explicitly. Therefore, it does not meet all inclusion criteria and partially aligns with exclusion aspects of focusing on ASR and single downstream tasks without broader spoken language modeling perspective.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior reviewers' notes (one recommended inclusion, one exclusion). The paper studies text-injection (JEIT) to improve ASR and two auxiliary tasks (capitalization and turn-taking prediction) and reports gains in capitalization and turn-taking recall. However, the title and abstract do not mention “Spoken Language Model(s)” or equivalent, the work centers on an ASR model with auxiliary downstream tasks (half‑duplex turn-taking) rather than treating speech as the core language-modeling object or addressing full‑duplex interaction, learned/engineered synchronization, cognitive parallelism, overlap management, or low-latency real-time evaluation. Therefore it fails the inclusion requirements and matches the exclusion criteria (focus on ASR/auxiliary tasks without SLM/FD‑SLM framing). Final 1-sentence explanation: Score 2 — better to exclude because the study focuses on ASR auxiliary tasks and lacks explicit spoken-language-model framing or full‑duplex/synchronization contributions required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior reviewers' notes (one recommended inclusion, one exclusion). The paper studies text-injection (JEIT) to improve ASR and two auxiliary tasks (capitalization and turn-taking prediction) and reports gains in capitalization and turn-taking recall. However, the title and abstract do not mention “Spoken Language Model(s)” or equivalent, the work centers on an ASR model with auxiliary downstream tasks (half‑duplex turn-taking) rather than treating speech as the core language-modeling object or addressing full‑duplex interaction, learned/engineered synchronization, cognitive parallelism, overlap management, or low-latency real-time evaluation. Therefore it fails the inclusion requirements and matches the exclusion criteria (focus on ASR/auxiliary tasks without SLM/FD‑SLM framing). Final 1-sentence explanation: Score 2 — better to exclude because the study focuses on ASR auxiliary tasks and lacks explicit spoken-language-model framing or full‑duplex/synchronization contributions required by the inclusion criteria.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Towards a Modular Architecture for Science Factories",
    "abstract": "Advances in robotic automation, high-performance computing (HPC), and artificial intelligence (AI) encourage us to conceive of science factories: large, general-purpose computation- and AI-enabled self-driving laboratories (SDLs) with the generality and scale needed both to tackle large discovery problems and to support thousands of scientists. Science factories require modular hardware and software that can be replicated for scale and (re)configured to support many applications. To this end, we propose a prototype modular science factory architecture in which reconfigurable modules encapsulating scientific instruments are linked with manipulators to form workcells, that can themselves be combined to form larger assemblages, and linked with distributed computing for simulation, AI model training and inference, and related tasks. Workflows that perform sets of actions on modules can be specified, and various applications, comprising workflows plus associated computational and data manipulation steps, can be run concurrently. We report on our experiences prototyping this architecture and applying it in experiments involving 15 different robotic apparatus, five applications (one in education, two in biology, two in materials), and a variety of workflows, across four laboratories. We describe the reuse of modules, workcells, and workflows in different applications, the migration of applications between workcells, and the use of digital twins, and suggest directions for future work aimed at yet more generality and scalability. Code and data are available at https://ad-sdl.github.io/wei2023 and in the Supplementary Information",
    "metadata": {
      "arxiv_id": "2308.09793",
      "title": "Towards a Modular Architecture for Science Factories",
      "summary": "Advances in robotic automation, high-performance computing (HPC), and artificial intelligence (AI) encourage us to conceive of science factories: large, general-purpose computation- and AI-enabled self-driving laboratories (SDLs) with the generality and scale needed both to tackle large discovery problems and to support thousands of scientists. Science factories require modular hardware and software that can be replicated for scale and (re)configured to support many applications. To this end, we propose a prototype modular science factory architecture in which reconfigurable modules encapsulating scientific instruments are linked with manipulators to form workcells, that can themselves be combined to form larger assemblages, and linked with distributed computing for simulation, AI model training and inference, and related tasks. Workflows that perform sets of actions on modules can be specified, and various applications, comprising workflows plus associated computational and data manipulation steps, can be run concurrently. We report on our experiences prototyping this architecture and applying it in experiments involving 15 different robotic apparatus, five applications (one in education, two in biology, two in materials), and a variety of workflows, across four laboratories. We describe the reuse of modules, workcells, and workflows in different applications, the migration of applications between workcells, and the use of digital twins, and suggest directions for future work aimed at yet more generality and scalability. Code and data are available at https://ad-sdl.github.io/wei2023 and in the Supplementary Information",
      "authors": [
        "Rafael Vescovi",
        "Tobias Ginsburg",
        "Kyle Hippe",
        "Doga Ozgulbas",
        "Casey Stone",
        "Abraham Stroka",
        "Rory Butler",
        "Ben Blaiszik",
        "Tom Brettin",
        "Kyle Chard",
        "Mark Hereld",
        "Arvind Ramanathan",
        "Rick Stevens",
        "Aikaterini Vriza",
        "Jie Xu",
        "Qingteng Zhang",
        "Ian Foster"
      ],
      "published": "2023-08-18T19:47:59Z",
      "updated": "2023-10-17T16:48:33Z",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.09793v2",
      "landing_url": "https://arxiv.org/abs/2308.09793v2",
      "doi": "https://doi.org/10.48550/arXiv.2308.09793"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on modular architectures for science factories, robotics, and AI-enabled self-driving laboratories, with no discussion of spoken language models, speech-language modeling, dialogue, or synchronous speech interaction; it does not meet the inclusion criteria centered on SLM/FD-SLM concepts and related evaluations, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on modular architectures for science factories, robotics, and AI-enabled self-driving laboratories, with no discussion of spoken language models, speech-language modeling, dialogue, or synchronous speech interaction; it does not meet the inclusion criteria centered on SLM/FD-SLM concepts and related evaluations, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on modular architectures for robotic science factories and self-driving laboratories, which is unrelated to Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs). The abstract does not discuss speech or language modeling, training, inference, or evaluation related to speech; nor does it mention synchronous interactive speech behaviors, cognitive parallelism, or any speech-based model evaluation. Therefore, the study does not meet any of the inclusion criteria and is unrelated to the specified topic areas.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on modular architectures for robotic science factories and self-driving laboratories, which is unrelated to Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs). The abstract does not discuss speech or language modeling, training, inference, or evaluation related to speech; nor does it mention synchronous interactive speech behaviors, cognitive parallelism, or any speech-based model evaluation. Therefore, the study does not meet any of the inclusion criteria and is unrelated to the specified topic areas.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition",
    "abstract": "We present TokenSplit, a speech separation model that acts on discrete token sequences. The model is trained on multiple tasks simultaneously: separate and transcribe each speech source, and generate speech from text. The model operates on transcripts and audio token sequences and achieves multiple tasks through masking of inputs. The model is a sequence-to-sequence encoder-decoder model that uses the Transformer architecture. We also present a \"refinement\" version of the model that predicts enhanced audio tokens from the audio tokens of speech separated by a conventional separation model. Using both objective metrics and subjective MUSHRA listening tests, we show that our model achieves excellent performance in terms of separation, both with or without transcript conditioning. We also measure the automatic speech recognition (ASR) performance and provide audio samples of speech synthesis to demonstrate the additional utility of our model.",
    "metadata": {
      "arxiv_id": "2308.10415",
      "title": "TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition",
      "summary": "We present TokenSplit, a speech separation model that acts on discrete token sequences. The model is trained on multiple tasks simultaneously: separate and transcribe each speech source, and generate speech from text. The model operates on transcripts and audio token sequences and achieves multiple tasks through masking of inputs. The model is a sequence-to-sequence encoder-decoder model that uses the Transformer architecture. We also present a \"refinement\" version of the model that predicts enhanced audio tokens from the audio tokens of speech separated by a conventional separation model. Using both objective metrics and subjective MUSHRA listening tests, we show that our model achieves excellent performance in terms of separation, both with or without transcript conditioning. We also measure the automatic speech recognition (ASR) performance and provide audio samples of speech synthesis to demonstrate the additional utility of our model.",
      "authors": [
        "Hakan Erdogan",
        "Scott Wisdom",
        "Xuankai Chang",
        "Zalán Borsos",
        "Marco Tagliasacchi",
        "Neil Zeghidour",
        "John R. Hershey"
      ],
      "published": "2023-08-21T01:52:01Z",
      "updated": "2023-08-21T01:52:01Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.10415v1",
      "landing_url": "https://arxiv.org/abs/2308.10415v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.10415"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Paper centers on modeling speech as discrete token sequences for multi-task speech separation and transcription with transcript-conditioned generation, aligning with Spoken Language Model concepts and evaluating architecture/training/evaluation within the last 3 years.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "Paper centers on modeling speech as discrete token sequences for multi-task speech separation and transcription with transcript-conditioned generation, aligning with Spoken Language Model concepts and evaluating architecture/training/evaluation within the last 3 years.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a speech separation model using discrete token sequences and Transformer architecture, addressing multiple tasks (separation, transcription, synthesis). However, the abstract does not mention spoken language models, speech language modeling explicitly, nor does it discuss full-duplex interaction, cognitive parallelism, or low-latency synchronous interaction. It primarily focuses on ASR, speech separation, and synthesis without clear emphasis on the core topic of spoken language models or full-duplex spoken language models, and does not cover dialogue behavior arbitration or synchronous interactive capabilities. Hence, it does not fully meet the inclusion criteria nor does it clearly violate exclusion criteria, suggesting uncertain inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a speech separation model using discrete token sequences and Transformer architecture, addressing multiple tasks (separation, transcription, synthesis). However, the abstract does not mention spoken language models, speech language modeling explicitly, nor does it discuss full-duplex interaction, cognitive parallelism, or low-latency synchronous interaction. It primarily focuses on ASR, speech separation, and synthesis without clear emphasis on the core topic of spoken language models or full-duplex spoken language models, and does not cover dialogue behavior arbitration or synchronous interactive capabilities. Hence, it does not fully meet the inclusion criteria nor does it clearly violate exclusion criteria, suggesting uncertain inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed both junior reviewers' notes: JuniorNano rated inclusion (5) emphasizing the paper’s use of discrete speech tokens and multi-task modeling aligning with spoken-language-model concepts; JuniorMini rated it uncertain (3) noting the abstract lacks explicit mention of spoken-language models and omits full-duplex/synchronization or interactive behavior. The paper indeed operates on discrete audio tokens with multi-task separation/transcription/synthesis using a seq2seq Transformer, which is relevant to speech-token modeling, but the abstract does not explicitly use the required phrasing (\"spoken language model(s)\" or \"language modeling of speech\") nor does it address full-duplex interaction, cognitive parallelism, low-latency synchronous interaction, or dialog-behavior arbitration—requirements in the inclusion criteria. It therefore fails the strict inclusion requirement of explicit SLM/language-modeling framing and also does not cover FD-SLM aspects, so it should not be included.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed both junior reviewers' notes: JuniorNano rated inclusion (5) emphasizing the paper’s use of discrete speech tokens and multi-task modeling aligning with spoken-language-model concepts; JuniorMini rated it uncertain (3) noting the abstract lacks explicit mention of spoken-language models and omits full-duplex/synchronization or interactive behavior. The paper indeed operates on discrete audio tokens with multi-task separation/transcription/synthesis using a seq2seq Transformer, which is relevant to speech-token modeling, but the abstract does not explicitly use the required phrasing (\"spoken language model(s)\" or \"language modeling of speech\") nor does it address full-duplex interaction, cognitive parallelism, low-latency synchronous interaction, or dialog-behavior arbitration—requirements in the inclusion criteria. It therefore fails the strict inclusion requirement of explicit SLM/language-modeling framing and also does not cover FD-SLM aspects, so it should not be included.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Addressing Data Scarcity in Optical Matrix Multiplier Modeling Using Transfer Learning",
    "abstract": "We present and experimentally evaluate using transfer learning to address experimental data scarcity when training neural network (NN) models for Mach-Zehnder interferometer mesh-based optical matrix multipliers. Our approach involves pre-training the model using synthetic data generated from a less accurate analytical model and fine-tuning with experimental data. Our investigation demonstrates that this method yields significant reductions in modeling errors compared to using an analytical model, or a standalone NN model when training data is limited. Utilizing regularization techniques and ensemble averaging, we achieve < 1 dB root-mean-square error on the matrix weights implemented by a 3x3 photonic chip while using only 25% of the available data.",
    "metadata": {
      "arxiv_id": "2308.11630",
      "title": "Addressing Data Scarcity in Optical Matrix Multiplier Modeling Using Transfer Learning",
      "summary": "We present and experimentally evaluate using transfer learning to address experimental data scarcity when training neural network (NN) models for Mach-Zehnder interferometer mesh-based optical matrix multipliers. Our approach involves pre-training the model using synthetic data generated from a less accurate analytical model and fine-tuning with experimental data. Our investigation demonstrates that this method yields significant reductions in modeling errors compared to using an analytical model, or a standalone NN model when training data is limited. Utilizing regularization techniques and ensemble averaging, we achieve < 1 dB root-mean-square error on the matrix weights implemented by a 3x3 photonic chip while using only 25% of the available data.",
      "authors": [
        "Ali Cem",
        "Ognjen Jovanovic",
        "Siqi Yan",
        "Yunhong Ding",
        "Darko Zibar",
        "Francesco Da Ros"
      ],
      "published": "2023-08-10T07:33:00Z",
      "updated": "2023-11-13T15:58:10Z",
      "categories": [
        "cs.LG",
        "physics.optics"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.11630v2",
      "landing_url": "https://arxiv.org/abs/2308.11630v2",
      "doi": "https://doi.org/10.48550/arXiv.2308.11630"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns transfer learning for photonic optical matrix multipliers, not Spoken Language Models or any speech-language modeling topic, so it does not meet the inclusion criteria and clearly falls under the exclusion scope.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns transfer learning for photonic optical matrix multipliers, not Spoken Language Models or any speech-language modeling topic, so it does not meet the inclusion criteria and clearly falls under the exclusion scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on optical matrix multiplier modeling using transfer learning, which is unrelated to Spoken Language Models or speech-based language modeling; it does not address spoken language modeling, full-duplex interaction, or any speech-related architectures or evaluations as required by the inclusion criteria, thus it fails to meet the thematic inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on optical matrix multiplier modeling using transfer learning, which is unrelated to Spoken Language Models or speech-based language modeling; it does not address spoken language modeling, full-duplex interaction, or any speech-related architectures or evaluations as required by the inclusion criteria, thus it fails to meet the thematic inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Consumer-tier based Visual-Brain Machine Interface for Augmented Reality Glasses Interactions",
    "abstract": "Objective.Visual-Brain Machine Interface(V-BMI) has provide a novel interaction technique for Augmented Reality (AR) industries. Several state-of-arts work has demonstates its high accuracy and real-time interaction capbilities. However, most of the studies employ EEGs devices that are rigid and difficult to apply in real-life AR glasseses application sceniraros. Here we develop a consumer-tier Visual-Brain Machine Inteface(V-BMI) system specialized for Augmented Reality(AR) glasses interactions. Approach. The developed system consists of a wearable hardware which takes advantages of fast set-up, reliable recording and comfortable wearable experience that specificized for AR glasses applications. Complementing this hardware, we have devised a software framework that facilitates real-time interactions within the system while accommodating a modular configuration to enhance scalability. Main results. The developed hardware is only 110g and 120x85x23 mm, which with 1 Tohm and peak to peak voltage is less than 1.5 uV, and a V-BMI based angry bird game and an Internet of Thing (IoT) AR applications are deisgned, we demonstrated such technology merits of intuitive experience and efficiency interaction. The real-time interaction accuracy is between 85 and 96 percentages in a commercial AR glasses (DTI is 2.24s and ITR 65 bits-min ). Significance. Our study indicates the developed system can provide an essential hardware-software framework for consumer based V-BMI AR glasses. Also, we derive several pivotal design factors for a consumer-grade V-BMI-based AR system: 1) Dynamic adaptation of stimulation patterns-classification methods via computer vision algorithms is necessary for AR glasses applications; and 2) Algorithmic localization to foster system stability and latency reduction.",
    "metadata": {
      "arxiv_id": "2308.15056",
      "title": "A Consumer-tier based Visual-Brain Machine Interface for Augmented Reality Glasses Interactions",
      "summary": "Objective.Visual-Brain Machine Interface(V-BMI) has provide a novel interaction technique for Augmented Reality (AR) industries. Several state-of-arts work has demonstates its high accuracy and real-time interaction capbilities. However, most of the studies employ EEGs devices that are rigid and difficult to apply in real-life AR glasseses application sceniraros. Here we develop a consumer-tier Visual-Brain Machine Inteface(V-BMI) system specialized for Augmented Reality(AR) glasses interactions. Approach. The developed system consists of a wearable hardware which takes advantages of fast set-up, reliable recording and comfortable wearable experience that specificized for AR glasses applications. Complementing this hardware, we have devised a software framework that facilitates real-time interactions within the system while accommodating a modular configuration to enhance scalability. Main results. The developed hardware is only 110g and 120x85x23 mm, which with 1 Tohm and peak to peak voltage is less than 1.5 uV, and a V-BMI based angry bird game and an Internet of Thing (IoT) AR applications are deisgned, we demonstrated such technology merits of intuitive experience and efficiency interaction. The real-time interaction accuracy is between 85 and 96 percentages in a commercial AR glasses (DTI is 2.24s and ITR 65 bits-min ). Significance. Our study indicates the developed system can provide an essential hardware-software framework for consumer based V-BMI AR glasses. Also, we derive several pivotal design factors for a consumer-grade V-BMI-based AR system: 1) Dynamic adaptation of stimulation patterns-classification methods via computer vision algorithms is necessary for AR glasses applications; and 2) Algorithmic localization to foster system stability and latency reduction.",
      "authors": [
        "Yuying Jiang",
        "Fan Bai",
        "Zicheng Zhang",
        "Xiaochen Ye",
        "Zheng Liu",
        "Zhiping Shi",
        "Jianwei Yao",
        "Xiaojun Liu",
        "Fangkun Zhu",
        "Junling Li Qian Guo",
        "Xiaoan Wang",
        "Junwen Luo"
      ],
      "published": "2023-08-29T06:33:13Z",
      "updated": "2023-08-29T06:33:13Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.15056v1",
      "landing_url": "https://arxiv.org/abs/2308.15056v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.15056"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on a Visual-BMI hardware/software system for AR glasses, not on Spoken Language Models or speech-language modeling, therefore it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on a Visual-BMI hardware/software system for AR glasses, not on Spoken Language Models or speech-language modeling, therefore it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a Visual-Brain Machine Interface for AR interactions and does not address Spoken Language Models or speech-based language modeling. It lacks discussion on speech modeling architectures, training, or evaluation related to spoken language models or full-duplex spoken interaction capabilities, which are core to the inclusion criteria. Therefore, it does not meet the inclusion criteria and falls outside the defined research scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a Visual-Brain Machine Interface for AR interactions and does not address Spoken Language Models or speech-based language modeling. It lacks discussion on speech modeling architectures, training, or evaluation related to spoken language models or full-duplex spoken interaction capabilities, which are core to the inclusion criteria. Therefore, it does not meet the inclusion criteria and falls outside the defined research scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "RAMP: Retrieval-Augmented MOS Prediction via Confidence-based Dynamic Weighting",
    "abstract": "Automatic Mean Opinion Score (MOS) prediction is crucial to evaluate the perceptual quality of the synthetic speech. While recent approaches using pre-trained self-supervised learning (SSL) models have shown promising results, they only partly address the data scarcity issue for the feature extractor. This leaves the data scarcity issue for the decoder unresolved and leading to suboptimal performance. To address this challenge, we propose a retrieval-augmented MOS prediction method, dubbed {\\bf RAMP}, to enhance the decoder's ability against the data scarcity issue. A fusing network is also proposed to dynamically adjust the retrieval scope for each instance and the fusion weights based on the predictive confidence. Experimental results show that our proposed method outperforms the existing methods in multiple scenarios.",
    "metadata": {
      "arxiv_id": "2308.16488",
      "title": "RAMP: Retrieval-Augmented MOS Prediction via Confidence-based Dynamic Weighting",
      "summary": "Automatic Mean Opinion Score (MOS) prediction is crucial to evaluate the perceptual quality of the synthetic speech. While recent approaches using pre-trained self-supervised learning (SSL) models have shown promising results, they only partly address the data scarcity issue for the feature extractor. This leaves the data scarcity issue for the decoder unresolved and leading to suboptimal performance. To address this challenge, we propose a retrieval-augmented MOS prediction method, dubbed {\\bf RAMP}, to enhance the decoder's ability against the data scarcity issue. A fusing network is also proposed to dynamically adjust the retrieval scope for each instance and the fusion weights based on the predictive confidence. Experimental results show that our proposed method outperforms the existing methods in multiple scenarios.",
      "authors": [
        "Hui Wang",
        "Shiwan Zhao",
        "Xiguang Zheng",
        "Yong Qin"
      ],
      "published": "2023-08-31T06:48:01Z",
      "updated": "2023-08-31T06:48:01Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.16488v1",
      "landing_url": "https://arxiv.org/abs/2308.16488v1",
      "doi": "https://doi.org/10.21437/Interspeech.2023-851"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on MOS prediction for synthetic speech and retrieval-augmented decoding, but it does not treat speech as the core object of a Spoken Language Model or full-duplex speech interaction; it lacks discussion of SLM/FD-SLM architectures or interactive evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on MOS prediction for synthetic speech and retrieval-augmented decoding, but it does not treat speech as the core object of a Spoken Language Model or full-duplex speech interaction; it lacks discussion of SLM/FD-SLM architectures or interactive evaluation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on MOS prediction for synthetic speech quality evaluation using retrieval-augmented methods, without explicit mention of Spoken Language Models, full-duplex interaction, or core speech language modeling architectures; it thus does not meet the inclusion criteria related to SLMs or FD-SLMs and synchronous interactive behaviors, nor does it discuss language modeling of speech as a core task.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on MOS prediction for synthetic speech quality evaluation using retrieval-augmented methods, without explicit mention of Spoken Language Models, full-duplex interaction, or core speech language modeling architectures; it thus does not meet the inclusion criteria related to SLMs or FD-SLMs and synchronous interactive behaviors, nor does it discuss language modeling of speech as a core task.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Phenomenological Approach to Interactive Knot Diagrams",
    "abstract": "Knot diagrams are among the most common visual tools in topology. Computer programs now make it possible to draw, manipulate and render them digitally, which proves to be useful in knot theory teaching and research. Still, an openly available tool to manipulate knot diagrams in a real-time, interactive way is yet to be developed. We introduce a method of operating on the geometry of the knot diagram itself without any underlying three-dimensional structure that can underpin such an application. This allows us to directly interact with vector graphics knot diagrams while at the same time computing knot invariants in ways proposed by previous work. An implementation of this method is provided.",
    "metadata": {
      "arxiv_id": "2309.00445",
      "title": "A Phenomenological Approach to Interactive Knot Diagrams",
      "summary": "Knot diagrams are among the most common visual tools in topology. Computer programs now make it possible to draw, manipulate and render them digitally, which proves to be useful in knot theory teaching and research. Still, an openly available tool to manipulate knot diagrams in a real-time, interactive way is yet to be developed. We introduce a method of operating on the geometry of the knot diagram itself without any underlying three-dimensional structure that can underpin such an application. This allows us to directly interact with vector graphics knot diagrams while at the same time computing knot invariants in ways proposed by previous work. An implementation of this method is provided.",
      "authors": [
        "Lennart Finke",
        "Edmund Weitz"
      ],
      "published": "2023-09-01T13:22:16Z",
      "updated": "2024-01-03T13:49:12Z",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.00445v2",
      "landing_url": "https://arxiv.org/abs/2309.00445v2",
      "doi": "https://doi.org/10.1109/tvcg.2024.3405369"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses knot diagrams and interactive geometry without any focus on Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses knot diagrams and interactive geometry without any focus on Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on knot diagrams and their manipulation in topology, which is unrelated to research on spoken language models, speech language modeling, or real-time interactive speech systems as defined by the inclusion criteria, making it clearly out of scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on knot diagrams and their manipulation in topology, which is unrelated to research on spoken language models, speech language modeling, or real-time interactive speech systems as defined by the inclusion criteria, making it clearly out of scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Half-Duplex APs with Dynamic TDD vs. Full-Duplex APs in Cell-Free Systems",
    "abstract": "In this paper, we present a comparative study of half-duplex (HD) access points (APs) with dynamic time-division duplex (DTDD) and full-duplex (FD) APs in cell-free (CF) systems. Although both DTDD and FD CF systems support concurrent downlink (DL) transmission and uplink (UL) reception capability, the sum spectral efficiency (SE) is limited by various cross-link interferences. We first present a novel pilot allocation scheme that minimizes the pilot length required to ensure no pilot contamination among the user equipments (UEs) served by at least one common AP. Then, we derive the sum SE in closed form, considering zero-forcing combining and precoding along with the signal-to-interference plus noise ratio optimal weighting at the central processing unit. We also present a provably convergent algorithm for joint UL-DL power allocation and UL/DL mode scheduling of the APs (for DTDD) to maximize the sum SE. Further, the proposed algorithms are precoder and combiner agnostic and come with closed-form update equations for the UL and DL power control coefficients. Our numerical results illustrate the superiority of the proposed pilot allocation and power control algorithms over several benchmark schemes and show that the sum SE with DTDD can outperform an FD CF system with similar antenna density. Thus, DTDD combined with CF is a promising alternative to FD that attains the same performance using HD APs, while obviating the burden of intra-AP interference cancellation.",
    "metadata": {
      "arxiv_id": "2309.01481",
      "title": "Half-Duplex APs with Dynamic TDD vs. Full-Duplex APs in Cell-Free Systems",
      "summary": "In this paper, we present a comparative study of half-duplex (HD) access points (APs) with dynamic time-division duplex (DTDD) and full-duplex (FD) APs in cell-free (CF) systems. Although both DTDD and FD CF systems support concurrent downlink (DL) transmission and uplink (UL) reception capability, the sum spectral efficiency (SE) is limited by various cross-link interferences. We first present a novel pilot allocation scheme that minimizes the pilot length required to ensure no pilot contamination among the user equipments (UEs) served by at least one common AP. Then, we derive the sum SE in closed form, considering zero-forcing combining and precoding along with the signal-to-interference plus noise ratio optimal weighting at the central processing unit. We also present a provably convergent algorithm for joint UL-DL power allocation and UL/DL mode scheduling of the APs (for DTDD) to maximize the sum SE. Further, the proposed algorithms are precoder and combiner agnostic and come with closed-form update equations for the UL and DL power control coefficients. Our numerical results illustrate the superiority of the proposed pilot allocation and power control algorithms over several benchmark schemes and show that the sum SE with DTDD can outperform an FD CF system with similar antenna density. Thus, DTDD combined with CF is a promising alternative to FD that attains the same performance using HD APs, while obviating the burden of intra-AP interference cancellation.",
      "authors": [
        "Anubhab Chowdhury",
        "Chandra R. Murthy"
      ],
      "published": "2023-09-04T09:36:51Z",
      "updated": "2024-01-31T08:48:10Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.01481v3",
      "landing_url": "https://arxiv.org/abs/2309.01481v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.01481"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on wireless network hardware (half-/full-duplex APs in cell-free systems), not on Spoken Language Models or speech-language modeling, and it does not cover dialogue synchronization, speech evaluation, or related language-modeling aspects, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on wireless network hardware (half-/full-duplex APs in cell-free systems), not on Spoken Language Models or speech-language modeling, and it does not cover dialogue synchronization, speech evaluation, or related language-modeling aspects, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on wireless communication systems, specifically comparing half-duplex and full-duplex access points in cell-free systems, with no focus on spoken language models, speech modeling, or related language modeling architectures; it does not discuss speech as a core modeling object, synchronization for interactive speech, or any evaluation related to speech interaction tasks required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on wireless communication systems, specifically comparing half-duplex and full-duplex access points in cell-free systems, with no focus on spoken language models, speech modeling, or related language modeling architectures; it does not discuss speech as a core modeling object, synchronization for interactive speech, or any evaluation related to speech interaction tasks required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SATAY: A Streaming Architecture Toolflow for Accelerating YOLO Models on FPGA Devices",
    "abstract": "AI has led to significant advancements in computer vision and image processing tasks, enabling a wide range of applications in real-life scenarios, from autonomous vehicles to medical imaging. Many of those applications require efficient object detection algorithms and complementary real-time, low latency hardware to perform inference of these algorithms. The YOLO family of models is considered the most efficient for object detection, having only a single model pass. Despite this, the complexity and size of YOLO models can be too computationally demanding for current edge-based platforms. To address this, we present SATAY: a Streaming Architecture Toolflow for Accelerating YOLO. This work tackles the challenges of deploying stateof-the-art object detection models onto FPGA devices for ultralow latency applications, enabling real-time, edge-based object detection. We employ a streaming architecture design for our YOLO accelerators, implementing the complete model on-chip in a deeply pipelined fashion. These accelerators are generated using an automated toolflow, and can target a range of suitable FPGA devices. We introduce novel hardware components to support the operations of YOLO models in a dataflow manner, and off-chip memory buffering to address the limited on-chip memory resources. Our toolflow is able to generate accelerator designs which demonstrate competitive performance and energy characteristics to GPU devices, and which outperform current state-of-the-art FPGA accelerators.",
    "metadata": {
      "arxiv_id": "2309.01587",
      "title": "SATAY: A Streaming Architecture Toolflow for Accelerating YOLO Models on FPGA Devices",
      "summary": "AI has led to significant advancements in computer vision and image processing tasks, enabling a wide range of applications in real-life scenarios, from autonomous vehicles to medical imaging. Many of those applications require efficient object detection algorithms and complementary real-time, low latency hardware to perform inference of these algorithms. The YOLO family of models is considered the most efficient for object detection, having only a single model pass. Despite this, the complexity and size of YOLO models can be too computationally demanding for current edge-based platforms. To address this, we present SATAY: a Streaming Architecture Toolflow for Accelerating YOLO. This work tackles the challenges of deploying stateof-the-art object detection models onto FPGA devices for ultralow latency applications, enabling real-time, edge-based object detection. We employ a streaming architecture design for our YOLO accelerators, implementing the complete model on-chip in a deeply pipelined fashion. These accelerators are generated using an automated toolflow, and can target a range of suitable FPGA devices. We introduce novel hardware components to support the operations of YOLO models in a dataflow manner, and off-chip memory buffering to address the limited on-chip memory resources. Our toolflow is able to generate accelerator designs which demonstrate competitive performance and energy characteristics to GPU devices, and which outperform current state-of-the-art FPGA accelerators.",
      "authors": [
        "Alexander Montgomerie-Corcoran",
        "Petros Toupas",
        "Zhewen Yu",
        "Christos-Savvas Bouganis"
      ],
      "published": "2023-09-04T13:15:01Z",
      "updated": "2023-09-04T13:15:01Z",
      "categories": [
        "cs.AR",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.01587v1",
      "landing_url": "https://arxiv.org/abs/2309.01587v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.01587"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper is about FPGA-based streaming accelerators for YOLO in computer vision, with no discussion of Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper is about FPGA-based streaming accelerators for YOLO in computer vision, with no discussion of Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on accelerating YOLO models for object detection on FPGA devices, which is related to computer vision and hardware acceleration, not on Spoken Language Models or speech-based language modeling; it does not address any topics related to speech as a core modeling object, full-duplex interaction, or linguistic modeling of speech. Therefore, it does not meet the inclusion criteria related to Spoken Language Models or related concepts and falls outside the research scope defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on accelerating YOLO models for object detection on FPGA devices, which is related to computer vision and hardware acceleration, not on Spoken Language Models or speech-based language modeling; it does not address any topics related to speech as a core modeling object, full-duplex interaction, or linguistic modeling of speech. Therefore, it does not meet the inclusion criteria related to Spoken Language Models or related concepts and falls outside the research scope defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Towards Universal Speech Discrete Tokens: A Case Study for ASR and TTS",
    "abstract": "Self-supervised learning (SSL) proficiency in speech-related tasks has driven research into utilizing discrete tokens for speech tasks like recognition and translation, which offer lower storage requirements and great potential to employ natural language processing techniques. However, these studies, mainly single-task focused, faced challenges like overfitting and performance degradation in speech recognition tasks, often at the cost of sacrificing performance in multi-task scenarios. This study presents a comprehensive comparison and optimization of discrete tokens generated by various leading SSL models in speech recognition and synthesis tasks. We aim to explore the universality of speech discrete tokens across multiple speech tasks. Experimental results demonstrate that discrete tokens achieve comparable results against systems trained on FBank features in speech recognition tasks and outperform mel-spectrogram features in speech synthesis in subjective and objective metrics. These findings suggest that universal discrete tokens have enormous potential in various speech-related tasks. Our work is open-source and publicly available at https://github.com/k2-fsa/icefall.",
    "metadata": {
      "arxiv_id": "2309.07377",
      "title": "Towards Universal Speech Discrete Tokens: A Case Study for ASR and TTS",
      "summary": "Self-supervised learning (SSL) proficiency in speech-related tasks has driven research into utilizing discrete tokens for speech tasks like recognition and translation, which offer lower storage requirements and great potential to employ natural language processing techniques. However, these studies, mainly single-task focused, faced challenges like overfitting and performance degradation in speech recognition tasks, often at the cost of sacrificing performance in multi-task scenarios. This study presents a comprehensive comparison and optimization of discrete tokens generated by various leading SSL models in speech recognition and synthesis tasks. We aim to explore the universality of speech discrete tokens across multiple speech tasks. Experimental results demonstrate that discrete tokens achieve comparable results against systems trained on FBank features in speech recognition tasks and outperform mel-spectrogram features in speech synthesis in subjective and objective metrics. These findings suggest that universal discrete tokens have enormous potential in various speech-related tasks. Our work is open-source and publicly available at https://github.com/k2-fsa/icefall.",
      "authors": [
        "Yifan Yang",
        "Feiyu Shen",
        "Chenpeng Du",
        "Ziyang Ma",
        "Kai Yu",
        "Daniel Povey",
        "Xie Chen"
      ],
      "published": "2023-09-14T01:39:43Z",
      "updated": "2023-12-14T13:18:35Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.07377v2",
      "landing_url": "https://arxiv.org/abs/2309.07377v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.07377"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on discrete tokens for ASR and TTS via SSL, but it does not frame Speech/Linguistic models as the core object nor discuss full-duplex interactive speech modeling; it also lacks explicit keywords like ‘spoken language model’ or ‘speech language model’ in title/abstract, despite being in the last 3 years.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item centers on discrete tokens for ASR and TTS via SSL, but it does not frame Speech/Linguistic models as the core object nor discuss full-duplex interactive speech modeling; it also lacks explicit keywords like ‘spoken language model’ or ‘speech language model’ in title/abstract, despite being in the last 3 years.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on discrete tokens generated by self-supervised models for speech recognition and synthesis, but does not explicitly discuss spoken language models or full-duplex interactions, cognitive parallelism, or synchronization aspects; it mainly addresses ASR and TTS performance rather than modeling spoken language with real-time interactive capabilities as required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on discrete tokens generated by self-supervised models for speech recognition and synthesis, but does not explicitly discuss spoken language models or full-duplex interactions, cognitive parallelism, or synchronization aspects; it mainly addresses ASR and TTS performance rather than modeling spoken language with real-time interactive capabilities as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "EnCodecMAE: Leveraging neural codecs for universal audio representation learning",
    "abstract": "The goal of universal audio representation learning is to obtain foundational models that can be used for a variety of downstream tasks involving speech, music and environmental sounds. To approach this problem, methods inspired by works on self-supervised learning for NLP, like BERT, or computer vision, like masked autoencoders (MAE), are often adapted to the audio domain. In this work, we propose masking representations of the audio signal, and training a MAE to reconstruct the masked segments. The reconstruction is done by predicting the discrete units generated by EnCodec, a neural audio codec, from the unmasked inputs. We evaluate this approach, which we call EnCodecMAE, on a wide range of tasks involving speech, music and environmental sounds. Our best model outperforms various state-of-the-art audio representation models in terms of global performance. Additionally, we evaluate the resulting representations in the challenging task of automatic speech recognition (ASR), obtaining decent results and paving the way for a universal audio representation.",
    "metadata": {
      "arxiv_id": "2309.07391",
      "title": "EnCodecMAE: Leveraging neural codecs for universal audio representation learning",
      "summary": "The goal of universal audio representation learning is to obtain foundational models that can be used for a variety of downstream tasks involving speech, music and environmental sounds. To approach this problem, methods inspired by works on self-supervised learning for NLP, like BERT, or computer vision, like masked autoencoders (MAE), are often adapted to the audio domain. In this work, we propose masking representations of the audio signal, and training a MAE to reconstruct the masked segments. The reconstruction is done by predicting the discrete units generated by EnCodec, a neural audio codec, from the unmasked inputs. We evaluate this approach, which we call EnCodecMAE, on a wide range of tasks involving speech, music and environmental sounds. Our best model outperforms various state-of-the-art audio representation models in terms of global performance. Additionally, we evaluate the resulting representations in the challenging task of automatic speech recognition (ASR), obtaining decent results and paving the way for a universal audio representation.",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "published": "2023-09-14T02:21:53Z",
      "updated": "2024-05-21T00:39:47Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.07391v2",
      "landing_url": "https://arxiv.org/abs/2309.07391v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.07391"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets universal audio representations using EnCodecMAE with ASR evaluation, but it does not treat Spoken Language Models or full-duplex conversational interactions as its core focus, nor discuss dialogue-aware modeling or synchronization; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper targets universal audio representations using EnCodecMAE with ASR evaluation, but it does not treat Spoken Language Models or full-duplex conversational interactions as its core focus, nor discuss dialogue-aware modeling or synchronization; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on universal audio representation learning covering speech, music, and environmental sounds using masked autoencoder techniques but does not explicitly discuss Spoken Language Models, full-duplex interaction, speech language modeling architectures, or synchronization and real-time interactive behaviors as required by the inclusion criteria; it also emphasizes ASR evaluation but not aspects like dialogue arbitration or cognitive parallelism, thus not meeting the key inclusion aspects nor addressing the exclusion factors fully.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on universal audio representation learning covering speech, music, and environmental sounds using masked autoencoder techniques but does not explicitly discuss Spoken Language Models, full-duplex interaction, speech language modeling architectures, or synchronization and real-time interactive behaviors as required by the inclusion criteria; it also emphasizes ASR evaluation but not aspects like dialogue arbitration or cognitive parallelism, thus not meeting the key inclusion aspects nor addressing the exclusion factors fully.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "BANC: Towards Efficient Binaural Audio Neural Codec for Overlapping Speech",
    "abstract": "We introduce BANC, a neural binaural audio codec designed for efficient speech compression in single and two-speaker scenarios while preserving the spatial location information of each speaker. Our key contributions are as follows: 1) The ability of our proposed model to compress and decode overlapping speech. 2) A novel architecture that compresses speech content and spatial cues separately, ensuring the preservation of each speaker's spatial context after decoding. 3) BANC's proficiency in reducing the bandwidth required for compressing binaural speech by 48% compared to compressing individual binaural channels. In our evaluation, we employed speech enhancement, room acoustics, and perceptual metrics to assess the accuracy of BANC's clean speech and spatial cue estimates.",
    "metadata": {
      "arxiv_id": "2309.07416",
      "title": "BANC: Towards Efficient Binaural Audio Neural Codec for Overlapping Speech",
      "summary": "We introduce BANC, a neural binaural audio codec designed for efficient speech compression in single and two-speaker scenarios while preserving the spatial location information of each speaker. Our key contributions are as follows: 1) The ability of our proposed model to compress and decode overlapping speech. 2) A novel architecture that compresses speech content and spatial cues separately, ensuring the preservation of each speaker's spatial context after decoding. 3) BANC's proficiency in reducing the bandwidth required for compressing binaural speech by 48% compared to compressing individual binaural channels. In our evaluation, we employed speech enhancement, room acoustics, and perceptual metrics to assess the accuracy of BANC's clean speech and spatial cue estimates.",
      "authors": [
        "Anton Ratnarajah",
        "Shi-Xiong Zhang",
        "Dong Yu"
      ],
      "published": "2023-09-14T04:04:50Z",
      "updated": "2024-11-25T03:50:32Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.07416v4",
      "landing_url": "https://arxiv.org/abs/2309.07416v4",
      "doi": "https://doi.org/10.48550/arXiv.2309.07416"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper centers on neural binaural audio coding for overlapping speech and spatial cue preservation, a signal-processing/codec topic, not on Spoken Language Models or full/dual-duplex speech modeling with interactive turn-taking and evaluation relevant to SLM/FD-SLM criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper centers on neural binaural audio coding for overlapping speech and spatial cue preservation, a signal-processing/codec topic, not on Spoken Language Models or full/dual-duplex speech modeling with interactive turn-taking and evaluation relevant to SLM/FD-SLM criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a neural binaural audio codec for speech compression and spatial information preservation, which primarily addresses signal compression and audio processing rather than spoken language modeling or full-duplex spoken interaction systems. It lacks explicit mention of spoken language models, speech language modeling, or evaluation of synchronization, cognitive parallelism, or full-duplex interactive capabilities. Therefore, it does not meet the core inclusion criteria centered on spoken language models and behavioral arbitration in real-time interactive speech systems.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a neural binaural audio codec for speech compression and spatial information preservation, which primarily addresses signal compression and audio processing rather than spoken language modeling or full-duplex spoken interaction systems. It lacks explicit mention of spoken language models, speech language modeling, or evaluation of synchronization, cognitive parallelism, or full-duplex interactive capabilities. Therefore, it does not meet the core inclusion criteria centered on spoken language models and behavioral arbitration in real-time interactive speech systems.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Characterizing the temporal dynamics of universal speech representations for generalizable deepfake detection",
    "abstract": "Existing deepfake speech detection systems lack generalizability to unseen attacks (i.e., samples generated by generative algorithms not seen during training). Recent studies have explored the use of universal speech representations to tackle this issue and have obtained inspiring results. These works, however, have focused on innovating downstream classifiers while leaving the representation itself untouched. In this study, we argue that characterizing the long-term temporal dynamics of these representations is crucial for generalizability and propose a new method to assess representation dynamics. Indeed, we show that different generative models generate similar representation dynamics patterns with our proposed method. Experiments on the ASVspoof 2019 and 2021 datasets validate the benefits of the proposed method to detect deepfakes from methods unseen during training, significantly improving on several benchmark methods.",
    "metadata": {
      "arxiv_id": "2309.08099",
      "title": "Characterizing the temporal dynamics of universal speech representations for generalizable deepfake detection",
      "summary": "Existing deepfake speech detection systems lack generalizability to unseen attacks (i.e., samples generated by generative algorithms not seen during training). Recent studies have explored the use of universal speech representations to tackle this issue and have obtained inspiring results. These works, however, have focused on innovating downstream classifiers while leaving the representation itself untouched. In this study, we argue that characterizing the long-term temporal dynamics of these representations is crucial for generalizability and propose a new method to assess representation dynamics. Indeed, we show that different generative models generate similar representation dynamics patterns with our proposed method. Experiments on the ASVspoof 2019 and 2021 datasets validate the benefits of the proposed method to detect deepfakes from methods unseen during training, significantly improving on several benchmark methods.",
      "authors": [
        "Yi Zhu",
        "Saurabh Powar",
        "Tiago H. Falk"
      ],
      "published": "2023-09-15T01:37:45Z",
      "updated": "2023-09-15T01:37:45Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.08099v1",
      "landing_url": "https://arxiv.org/abs/2309.08099v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.08099"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on universal speech representations for deepfake detection and downstream classifiers, without treating Spoken Language Models or speech-language modeling as the core subject, nor addressing turn-taking/synchronous interaction or full-duplex speech modeling required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on universal speech representations for deepfake detection and downstream classifiers, without treating Spoken Language Models or speech-language modeling as the core subject, nor addressing turn-taking/synchronous interaction or full-duplex speech modeling required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on deepfake speech detection using universal speech representations but does not explicitly involve Spoken Language Models or the modeling and reasoning of speech as the primary focus, nor does it mention strategies related to full-duplex interaction, synchronization, or the detailed evaluation aspects required by the inclusion criteria; thus, it does not meet the criteria for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on deepfake speech detection using universal speech representations but does not explicitly involve Spoken Language Models or the modeling and reasoning of speech as the primary focus, nor does it mention strategies related to full-duplex interaction, synchronization, or the detailed evaluation aspects required by the inclusion criteria; thus, it does not meet the criteria for inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-training and Multi-modal Tokens",
    "abstract": "In this paper, we propose methods to build a powerful and efficient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehension and language modeling from a large-scale pre-trained vision-language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech features of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteristics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: https://ms-dot-k.github.io/Image-to-Speech-Captioning.",
    "metadata": {
      "arxiv_id": "2309.08531",
      "title": "Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-training and Multi-modal Tokens",
      "summary": "In this paper, we propose methods to build a powerful and efficient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehension and language modeling from a large-scale pre-trained vision-language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech features of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteristics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: https://ms-dot-k.github.io/Image-to-Speech-Captioning.",
      "authors": [
        "Minsu Kim",
        "Jeongsoo Choi",
        "Soumi Maiti",
        "Jeong Hun Yeo",
        "Shinji Watanabe",
        "Yong Man Ro"
      ],
      "published": "2023-09-15T16:48:34Z",
      "updated": "2023-09-15T16:48:34Z",
      "categories": [
        "cs.CV",
        "cs.CL",
        "eess.AS",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.08531v1",
      "landing_url": "https://arxiv.org/abs/2309.08531v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.08531"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "該論文以影像到語音標註為核心，將語音視為主要建模對象，摘要中明確出現 spoken language modeling 的概念，且探討模型架構、訓練與評估，符合「Spoken Language Models / 語音語言建模」核心並在過去三年內發表，非僅聚焦單一語音技術或僅文字處理。",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "該論文以影像到語音標註為核心，將語音視為主要建模對象，摘要中明確出現 spoken language modeling 的概念，且探討模型架構、訓練與評估，符合「Spoken Language Models / 語音語言建模」核心並在過去三年內發表，非僅聚焦單一語音技術或僅文字處理。",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on image-to-speech captioning by leveraging vision-language pre-training and generating discretized speech units for spoken language modeling. However, the study centers on generating speech captions from images and does not address spoken language models as core modeling or reasoning objects, nor does it discuss full-duplex spoken language interactions or synchronization in speech communication. The evaluation and approach focus on image understanding and speech synthesis, lacking emphasis on interactive, synchronous speech behavior or cognitive parallelism in spoken language models. Therefore, it does not meet the inclusion criteria centered on spoken language models or full-duplex interactive speech systems.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on image-to-speech captioning by leveraging vision-language pre-training and generating discretized speech units for spoken language modeling. However, the study centers on generating speech captions from images and does not address spoken language models as core modeling or reasoning objects, nor does it discuss full-duplex spoken language interactions or synchronization in speech communication. The evaluation and approach focus on image understanding and speech synthesis, lacking emphasis on interactive, synchronous speech behavior or cognitive parallelism in spoken language models. Therefore, it does not meet the inclusion criteria centered on spoken language models or full-duplex interactive speech systems.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: one recommended inclusion noting explicit 'spoken language modeling' focus and evaluation on benchmarks; the other recommended exclusion arguing the work centers on image-to-speech synthesis and lacks interactive/full‑duplex behavior. My judgement: include — the abstract explicitly states 'spoken language modeling', uses discretized speech units as core outputs (i.e., speech-level language modeling), and describes model architecture, pretraining and evaluation on benchmarks, meeting the inclusion criteria even though it does not address full‑duplex interaction features (which are not mandatory).",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: one recommended inclusion noting explicit 'spoken language modeling' focus and evaluation on benchmarks; the other recommended exclusion arguing the work centers on image-to-speech synthesis and lacks interactive/full‑duplex behavior. My judgement: include — the abstract explicitly states 'spoken language modeling', uses discretized speech units as core outputs (i.e., speech-level language modeling), and describes model architecture, pretraining and evaluation on benchmarks, meeting the inclusion criteria even though it does not address full‑duplex interaction features (which are not mandatory).",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Enhance audio generation controllability through representation similarity regularization",
    "abstract": "This paper presents an innovative approach to enhance control over audio generation by emphasizing the alignment between audio and text representations during model training. In the context of language model-based audio generation, the model leverages input from both textual and audio token representations to predict subsequent audio tokens. However, the current configuration lacks explicit regularization to ensure the alignment between the chosen text representation and the language model's predictions. Our proposal involves the incorporation of audio and text representation regularization, particularly during the classifier-free guidance (CFG) phase, where the text condition is excluded from cross attention during language model training. The aim of this proposed representation regularization is to minimize discrepancies in audio and text similarity compared to other samples within the same training batch. Experimental results on both music and audio generation tasks demonstrate that our proposed methods lead to improvements in objective metrics for both audio and music generation, as well as an enhancement in the human perception for audio generation.",
    "metadata": {
      "arxiv_id": "2309.08773",
      "title": "Enhance audio generation controllability through representation similarity regularization",
      "summary": "This paper presents an innovative approach to enhance control over audio generation by emphasizing the alignment between audio and text representations during model training. In the context of language model-based audio generation, the model leverages input from both textual and audio token representations to predict subsequent audio tokens. However, the current configuration lacks explicit regularization to ensure the alignment between the chosen text representation and the language model's predictions. Our proposal involves the incorporation of audio and text representation regularization, particularly during the classifier-free guidance (CFG) phase, where the text condition is excluded from cross attention during language model training. The aim of this proposed representation regularization is to minimize discrepancies in audio and text similarity compared to other samples within the same training batch. Experimental results on both music and audio generation tasks demonstrate that our proposed methods lead to improvements in objective metrics for both audio and music generation, as well as an enhancement in the human perception for audio generation.",
      "authors": [
        "Yangyang Shi",
        "Gael Le Lan",
        "Varun Nagaraja",
        "Zhaoheng Ni",
        "Xinhao Mei",
        "Ernie Chang",
        "Forrest Iandola",
        "Yang Liu",
        "Vikas Chandra"
      ],
      "published": "2023-09-15T21:32:20Z",
      "updated": "2023-09-15T21:32:20Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.08773v1",
      "landing_url": "https://arxiv.org/abs/2309.08773v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.08773"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The inclusion criteria require explicit focus on Spoken Language Models (SLMs) or equivalent concepts with audio as the core modeling object and coverage of architecture/training/evaluation, as well as explicit mention of SLM terms in the title/abstract; this abstract discusses language-model-based audio generation and cross-modal text-audio representations but does not explicitly frame the work as SLM/FD-SLM nor discuss turn-taking or real-time interactive aspects, so it may not meet the core SLM-focused criterion.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The inclusion criteria require explicit focus on Spoken Language Models (SLMs) or equivalent concepts with audio as the core modeling object and coverage of architecture/training/evaluation, as well as explicit mention of SLM terms in the title/abstract; this abstract discusses language-model-based audio generation and cross-modal text-audio representations but does not explicitly frame the work as SLM/FD-SLM nor discuss turn-taking or real-time interactive aspects, so it may not meet the core SLM-focused criterion.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on enhancing controllability in audio generation by aligning audio and text representations, but it does not explicitly discuss Spoken Language Models or Full-Duplex Spoken Language Models, nor does it address synchronous turn-taking, cognitive parallelism, or real-time interaction behaviors such as interruption or backchanneling; furthermore, the core of the study is about audio generation rather than speech language modeling or joint speech-text modeling focused on dialogue interaction, and there is no mention of evaluation on low-latency or multi-dimensional interaction benchmarks.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on enhancing controllability in audio generation by aligning audio and text representations, but it does not explicitly discuss Spoken Language Models or Full-Duplex Spoken Language Models, nor does it address synchronous turn-taking, cognitive parallelism, or real-time interaction behaviors such as interruption or backchanneling; furthermore, the core of the study is about audio generation rather than speech language modeling or joint speech-text modeling focused on dialogue interaction, and there is no mention of evaluation on low-latency or multi-dimensional interaction benchmarks.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "PromptVC: Flexible Stylistic Voice Conversion in Latent Space Driven by Natural Language Prompts",
    "abstract": "Style voice conversion aims to transform the style of source speech to a desired style according to real-world application demands. However, the current style voice conversion approach relies on pre-defined labels or reference speech to control the conversion process, which leads to limitations in style diversity or falls short in terms of the intuitive and interpretability of style representation. In this study, we propose PromptVC, a novel style voice conversion approach that employs a latent diffusion model to generate a style vector driven by natural language prompts. Specifically, the style vector is extracted by a style encoder during training, and then the latent diffusion model is trained independently to sample the style vector from noise, with this process being conditioned on natural language prompts. To improve style expressiveness, we leverage HuBERT to extract discrete tokens and replace them with the K-Means center embedding to serve as the linguistic content, which minimizes residual style information. Additionally, we deduplicate the same discrete token and employ a differentiable duration predictor to re-predict the duration of each token, which can adapt the duration of the same linguistic content to different styles. The subjective and objective evaluation results demonstrate the effectiveness of our proposed system.",
    "metadata": {
      "arxiv_id": "2309.09262",
      "title": "PromptVC: Flexible Stylistic Voice Conversion in Latent Space Driven by Natural Language Prompts",
      "summary": "Style voice conversion aims to transform the style of source speech to a desired style according to real-world application demands. However, the current style voice conversion approach relies on pre-defined labels or reference speech to control the conversion process, which leads to limitations in style diversity or falls short in terms of the intuitive and interpretability of style representation. In this study, we propose PromptVC, a novel style voice conversion approach that employs a latent diffusion model to generate a style vector driven by natural language prompts. Specifically, the style vector is extracted by a style encoder during training, and then the latent diffusion model is trained independently to sample the style vector from noise, with this process being conditioned on natural language prompts. To improve style expressiveness, we leverage HuBERT to extract discrete tokens and replace them with the K-Means center embedding to serve as the linguistic content, which minimizes residual style information. Additionally, we deduplicate the same discrete token and employ a differentiable duration predictor to re-predict the duration of each token, which can adapt the duration of the same linguistic content to different styles. The subjective and objective evaluation results demonstrate the effectiveness of our proposed system.",
      "authors": [
        "Jixun Yao",
        "Yuguang Yang",
        "Yi Lei",
        "Ziqian Ning",
        "Yanni Hu",
        "Yu Pan",
        "Jingjing Yin",
        "Hongbin Zhou",
        "Heng Lu",
        "Lei Xie"
      ],
      "published": "2023-09-17T12:58:27Z",
      "updated": "2023-12-26T07:48:05Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.09262v2",
      "landing_url": "https://arxiv.org/abs/2309.09262v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.09262"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work targets spoken voice style conversion driven by natural language prompts, a speech-to-speech generation task, but it does not center on Spoken Language Models or Speech Language Models as the core modeling/reasoning object (no explicit end-to-end or hybrid Speech-LLM architecture, nor evaluation of conversational dynamics). Although it uses prompts, it seems more aligned with VC/TTS-like methods rather than SLMs, so it likely does not meet the inclusion criteria while it is within the 3-year window.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work targets spoken voice style conversion driven by natural language prompts, a speech-to-speech generation task, but it does not center on Spoken Language Models or Speech Language Models as the core modeling/reasoning object (no explicit end-to-end or hybrid Speech-LLM architecture, nor evaluation of conversational dynamics). Although it uses prompts, it seems more aligned with VC/TTS-like methods rather than SLMs, so it likely does not meet the inclusion criteria while it is within the 3-year window.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a style voice conversion system using a latent diffusion model driven by natural language prompts, focusing on voice style transformation rather than on spoken language models or full-duplex spoken interaction modeling; it lacks explicit focus on speech language modeling, full-duplex interaction, or synchronization, and does not address dialogue behavior arbitration or low-latency interaction as required by the inclusion criteria. Therefore, the paper does not meet the core inclusion criteria centered on Spoken Language Models or Full-Duplex Spoken Language Models and is more related to voice conversion technology than spoken language model research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a style voice conversion system using a latent diffusion model driven by natural language prompts, focusing on voice style transformation rather than on spoken language models or full-duplex spoken interaction modeling; it lacks explicit focus on speech language modeling, full-duplex interaction, or synchronization, and does not address dialogue behavior arbitration or low-latency interaction as required by the inclusion criteria. Therefore, the paper does not meet the core inclusion criteria centered on Spoken Language Models or Full-Duplex Spoken Language Models and is more related to voice conversion technology than spoken language model research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "FoleyGen: Visually-Guided Audio Generation",
    "abstract": "Recent advancements in audio generation have been spurred by the evolution of large-scale deep learning models and expansive datasets. However, the task of video-to-audio (V2A) generation continues to be a challenge, principally because of the intricate relationship between the high-dimensional visual and auditory data, and the challenges associated with temporal synchronization. In this study, we introduce FoleyGen, an open-domain V2A generation system built on a language modeling paradigm. FoleyGen leverages an off-the-shelf neural audio codec for bidirectional conversion between waveforms and discrete tokens. The generation of audio tokens is facilitated by a single Transformer model, which is conditioned on visual features extracted from a visual encoder. A prevalent problem in V2A generation is the misalignment of generated audio with the visible actions in the video. To address this, we explore three novel visual attention mechanisms. We further undertake an exhaustive evaluation of multiple visual encoders, each pretrained on either single-modal or multi-modal tasks. The experimental results on VGGSound dataset show that our proposed FoleyGen outperforms previous systems across all objective metrics and human evaluations.",
    "metadata": {
      "arxiv_id": "2309.10537",
      "title": "FoleyGen: Visually-Guided Audio Generation",
      "summary": "Recent advancements in audio generation have been spurred by the evolution of large-scale deep learning models and expansive datasets. However, the task of video-to-audio (V2A) generation continues to be a challenge, principally because of the intricate relationship between the high-dimensional visual and auditory data, and the challenges associated with temporal synchronization. In this study, we introduce FoleyGen, an open-domain V2A generation system built on a language modeling paradigm. FoleyGen leverages an off-the-shelf neural audio codec for bidirectional conversion between waveforms and discrete tokens. The generation of audio tokens is facilitated by a single Transformer model, which is conditioned on visual features extracted from a visual encoder. A prevalent problem in V2A generation is the misalignment of generated audio with the visible actions in the video. To address this, we explore three novel visual attention mechanisms. We further undertake an exhaustive evaluation of multiple visual encoders, each pretrained on either single-modal or multi-modal tasks. The experimental results on VGGSound dataset show that our proposed FoleyGen outperforms previous systems across all objective metrics and human evaluations.",
      "authors": [
        "Xinhao Mei",
        "Varun Nagaraja",
        "Gael Le Lan",
        "Zhaoheng Ni",
        "Ernie Chang",
        "Yangyang Shi",
        "Vikas Chandra"
      ],
      "published": "2023-09-19T11:33:43Z",
      "updated": "2023-09-19T11:33:43Z",
      "categories": [
        "eess.AS",
        "cs.MM",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.10537v1",
      "landing_url": "https://arxiv.org/abs/2309.10537v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.10537"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on video-to-audio generation (FoleyGen) and uses a visual encoder plus a Transformer to generate audio tokens, but it does not treat speech as the primary modeling object nor discuss Spoken Language Models / Full-Duplex SLMs, turn-taking, or synchronous dialogue—core criteria require SLM/FD-SLM focus with speech-centric modeling and evaluation. Although the work is within the last 3 years, it does not meet the central inclusion criteria and likely falls under the exclusion criteria related to lack of speech-language modeling emphasis.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on video-to-audio generation (FoleyGen) and uses a visual encoder plus a Transformer to generate audio tokens, but it does not treat speech as the primary modeling object nor discuss Spoken Language Models / Full-Duplex SLMs, turn-taking, or synchronous dialogue—core criteria require SLM/FD-SLM focus with speech-centric modeling and evaluation. Although the work is within the last 3 years, it does not meet the central inclusion criteria and likely falls under the exclusion criteria related to lack of speech-language modeling emphasis.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on video-to-audio generation using a Transformer model conditioned on visual features and addresses temporal synchronization issues in audiovisual generation, but it does not explicitly discuss Spoken Language Models or language modeling of speech as core aspects, nor does it address full-duplex spoken interaction, behavioral arbitration, or real-time interactive evaluation as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on video-to-audio generation using a Transformer model conditioned on visual features and addresses temporal synchronization issues in audiovisual generation, but it does not explicitly discuss Spoken Language Models or language modeling of speech as core aspects, nor does it address full-duplex spoken interaction, behavioral arbitration, or real-time interactive evaluation as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition",
    "abstract": "Discrete audio representation, aka audio tokenization, has seen renewed interest driven by its potential to facilitate the application of text language modeling approaches in audio domain. To this end, various compression and representation-learning based tokenization schemes have been proposed. However, there is limited investigation into the performance of compression-based audio tokens compared to well-established mel-spectrogram features across various speaker and speech related tasks. In this paper, we evaluate compression based audio tokens on three tasks: Speaker Verification, Diarization and (Multi-lingual) Speech Recognition. Our findings indicate that (i) the models trained on audio tokens perform competitively, on average within $1\\%$ of mel-spectrogram features for all the tasks considered, and do not surpass them yet. (ii) these models exhibit robustness for out-of-domain narrowband data, particularly in speaker tasks. (iii) audio tokens allow for compression to 20x compared to mel-spectrogram features with minimal loss of performance in speech and speaker related tasks, which is crucial for low bit-rate applications, and (iv) the examined Residual Vector Quantization (RVQ) based audio tokenizer exhibits a low-pass frequency response characteristic, offering a plausible explanation for the observed results, and providing insight for future tokenizer designs.",
    "metadata": {
      "arxiv_id": "2309.10922",
      "title": "Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition",
      "summary": "Discrete audio representation, aka audio tokenization, has seen renewed interest driven by its potential to facilitate the application of text language modeling approaches in audio domain. To this end, various compression and representation-learning based tokenization schemes have been proposed. However, there is limited investigation into the performance of compression-based audio tokens compared to well-established mel-spectrogram features across various speaker and speech related tasks. In this paper, we evaluate compression based audio tokens on three tasks: Speaker Verification, Diarization and (Multi-lingual) Speech Recognition. Our findings indicate that (i) the models trained on audio tokens perform competitively, on average within $1\\%$ of mel-spectrogram features for all the tasks considered, and do not surpass them yet. (ii) these models exhibit robustness for out-of-domain narrowband data, particularly in speaker tasks. (iii) audio tokens allow for compression to 20x compared to mel-spectrogram features with minimal loss of performance in speech and speaker related tasks, which is crucial for low bit-rate applications, and (iv) the examined Residual Vector Quantization (RVQ) based audio tokenizer exhibits a low-pass frequency response characteristic, offering a plausible explanation for the observed results, and providing insight for future tokenizer designs.",
      "authors": [
        "Krishna C. Puvvada",
        "Nithin Rao Koluguri",
        "Kunal Dhawan",
        "Jagadeesh Balam",
        "Boris Ginsburg"
      ],
      "published": "2023-09-19T20:49:05Z",
      "updated": "2023-09-19T20:49:05Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.10922v1",
      "landing_url": "https://arxiv.org/abs/2309.10922v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.10922"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on discrete audio tokenization and evaluation across speaker verification and ASR tasks but does not center on Spoken Language Models or full-duplex speech–language modeling; it lacks core SLM/FD-SLM architecture, training, or evaluation framing, thus fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on discrete audio tokenization and evaluation across speaker verification and ASR tasks but does not center on Spoken Language Models or full-duplex speech–language modeling; it lacks core SLM/FD-SLM architecture, training, or evaluation framing, thus fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on discrete audio representation and compares audio tokens with mel-spectrogram features for speaker and speech recognition tasks, but it does not explicitly mention Spoken Language Models (SLMs), full-duplex spoken language models, or core language modeling of speech as required. It also centers on compression and feature representation rather than end-to-end or hybrid speech-language model architectures involving synchronization or interactive dialogue capabilities; thus, it does not meet the inclusion criteria related to SLM as a core modeling approach or discuss relevant interactive or behavioral arbitration aspects.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on discrete audio representation and compares audio tokens with mel-spectrogram features for speaker and speech recognition tasks, but it does not explicitly mention Spoken Language Models (SLMs), full-duplex spoken language models, or core language modeling of speech as required. It also centers on compression and feature representation rather than end-to-end or hybrid speech-language model architectures involving synchronization or interactive dialogue capabilities; thus, it does not meet the inclusion criteria related to SLM as a core modeling approach or discuss relevant interactive or behavioral arbitration aspects.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts",
    "abstract": "Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.",
    "metadata": {
      "arxiv_id": "2309.11977",
      "title": "Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts",
      "summary": "Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.",
      "authors": [
        "Shun Lei",
        "Yixuan Zhou",
        "Liyang Chen",
        "Dan Luo",
        "Zhiyong Wu",
        "Xixin Wu",
        "Shiyin Kang",
        "Tao Jiang",
        "Yahui Zhou",
        "Yuxing Han",
        "Helen Meng"
      ],
      "published": "2023-09-21T11:22:22Z",
      "updated": "2024-04-09T08:39:52Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.11977v3",
      "landing_url": "https://arxiv.org/abs/2309.11977v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.11977"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly uses language-model-based TTS and treats speech as the modeling target (VALL-E, multi-scale acoustic prompts), i.e., a speech-language modeling viewpoint with architecture/training/evaluation; published in 2025 (within 3 years) and mentions spoken language model(s) in its framing.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly uses language-model-based TTS and treats speech as the modeling target (VALL-E, multi-scale acoustic prompts), i.e., a speech-language modeling viewpoint with architecture/training/evaluation; published in 2025 (within 3 years) and mentions spoken language model(s) in its framing.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on zero-shot TTS synthesis using language model-based methods and neural codec language models like VALL-E, which are related to speech generation. However, it mainly addresses single-turn TTS cloning capabilities and acoustic prompt modeling, without discussing core aspects of Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), interactive or synchronous dialog behaviors, cognitive parallelism, or synchronized speech-language modeling. It primarily focuses on speech synthesis quality and speaker similarity, without involving real-time interaction, behavior arbitration, or multi-turn dialogues, which are central to the inclusion criteria. Moreover, the keywords \"spoken language model(s)\" or equivalent do not appear explicitly in the abstract or title. Therefore, the study does not fully meet the inclusion criteria nor is it excluded absolutely, but is generally not aligned with the core thematic scope required.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on zero-shot TTS synthesis using language model-based methods and neural codec language models like VALL-E, which are related to speech generation. However, it mainly addresses single-turn TTS cloning capabilities and acoustic prompt modeling, without discussing core aspects of Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), interactive or synchronous dialog behaviors, cognitive parallelism, or synchronized speech-language modeling. It primarily focuses on speech synthesis quality and speaker similarity, without involving real-time interaction, behavior arbitration, or multi-turn dialogues, which are central to the inclusion criteria. Moreover, the keywords \"spoken language model(s)\" or equivalent do not appear explicitly in the abstract or title. Therefore, the study does not fully meet the inclusion criteria nor is it excluded absolutely, but is generally not aligned with the core thematic scope required.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed: one noted it uses language-model-based acoustic tokens (VALL-E) and framed as language modeling of speech, while the other correctly pointed out it is a single-task zero-shot TTS cloning paper without any discussion of spoken language models for interactive/full‑duplex behavior or synchronization; therefore it meets none of the SLM/FD‑SLM interaction requirements and falls under the exclusion criteria. Explanation: Exclude (score 2) — the paper is a zero‑shot TTS synthesis study focused on quality and speaker similarity, not on spoken language modeling or full‑duplex/synchronous interaction.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed: one noted it uses language-model-based acoustic tokens (VALL-E) and framed as language modeling of speech, while the other correctly pointed out it is a single-task zero-shot TTS cloning paper without any discussion of spoken language models for interactive/full‑duplex behavior or synchronization; therefore it meets none of the SLM/FD‑SLM interaction requirements and falls under the exclusion criteria. Explanation: Exclude (score 2) — the paper is a zero‑shot TTS synthesis study focused on quality and speaker similarity, not on spoken language modeling or full‑duplex/synchronous interaction.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ONNX-to-Hardware Design Flow for the Generation of Adaptive Neural-Network Accelerators on FPGAs",
    "abstract": "Neural Networks (NN) provide a solid and reliable way of executing different types of applications, ranging from speech recognition to medical diagnosis, speeding up onerous and long workloads. The challenges involved in their implementation at the edge include providing diversity, flexibility, and sustainability. That implies, for instance, supporting evolving applications and algorithms energy-efficiently. Using hardware or software accelerators can deliver fast and efficient computation of the \\acp{nn}, while flexibility can be exploited to support long-term adaptivity. Nonetheless, handcrafting an NN for a specific device, despite the possibility of leading to an optimal solution, takes time and experience, and that's why frameworks for hardware accelerators are being developed. This work-in-progress study focuses on exploring the possibility of combining the toolchain proposed by Ratto et al., which has the distinctive ability to favor adaptivity, with approximate computing. The goal will be to allow lightweight adaptable NN inference on FPGAs at the edge. Before that, the work presents a detailed review of established frameworks that adopt a similar streaming architecture for future comparison.",
    "metadata": {
      "arxiv_id": "2309.13321",
      "title": "ONNX-to-Hardware Design Flow for the Generation of Adaptive Neural-Network Accelerators on FPGAs",
      "summary": "Neural Networks (NN) provide a solid and reliable way of executing different types of applications, ranging from speech recognition to medical diagnosis, speeding up onerous and long workloads. The challenges involved in their implementation at the edge include providing diversity, flexibility, and sustainability. That implies, for instance, supporting evolving applications and algorithms energy-efficiently. Using hardware or software accelerators can deliver fast and efficient computation of the \\acp{nn}, while flexibility can be exploited to support long-term adaptivity. Nonetheless, handcrafting an NN for a specific device, despite the possibility of leading to an optimal solution, takes time and experience, and that's why frameworks for hardware accelerators are being developed. This work-in-progress study focuses on exploring the possibility of combining the toolchain proposed by Ratto et al., which has the distinctive ability to favor adaptivity, with approximate computing. The goal will be to allow lightweight adaptable NN inference on FPGAs at the edge. Before that, the work presents a detailed review of established frameworks that adopt a similar streaming architecture for future comparison.",
      "authors": [
        "Federico Manca",
        "Francesco Ratto"
      ],
      "published": "2023-09-23T09:41:43Z",
      "updated": "2023-09-23T09:41:43Z",
      "categories": [
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.13321v1",
      "landing_url": "https://arxiv.org/abs/2309.13321v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.13321"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item does not address Spoken Language Models or speech-language modeling; it focuses on ONNX-to-hardware design for adaptive NN accelerators on FPGAs, unrelated to speech interaction or evaluation of spoken language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item does not address Spoken Language Models or speech-language modeling; it focuses on ONNX-to-hardware design for adaptive NN accelerators on FPGAs, unrelated to speech interaction or evaluation of spoken language modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on hardware design flows for adaptive neural-network accelerators on FPGAs, which does not address spoken language models or related speech-centric modeling as required by the inclusion criteria. It neither discusses speech language model architectures nor the core modeling or inference of speech signals, and there is no mention of full-duplex spoken language models or related evaluation metrics. Therefore, it does not meet the inclusion criteria and aligns more with hardware acceleration than speech language modeling research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on hardware design flows for adaptive neural-network accelerators on FPGAs, which does not address spoken language models or related speech-centric modeling as required by the inclusion criteria. It neither discusses speech language model architectures nor the core modeling or inference of speech signals, and there is no mention of full-duplex spoken language models or related evaluation metrics. Therefore, it does not meet the inclusion criteria and aligns more with hardware acceleration than speech language modeling research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Towards General-Purpose Text-Instruction-Guided Voice Conversion",
    "abstract": "This paper introduces a novel voice conversion (VC) model, guided by text instructions such as \"articulate slowly with a deep tone\" or \"speak in a cheerful boyish voice\". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.",
    "metadata": {
      "arxiv_id": "2309.14324",
      "title": "Towards General-Purpose Text-Instruction-Guided Voice Conversion",
      "summary": "This paper introduces a novel voice conversion (VC) model, guided by text instructions such as \"articulate slowly with a deep tone\" or \"speak in a cheerful boyish voice\". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.",
      "authors": [
        "Chun-Yi Kuan",
        "Chen An Li",
        "Tsu-Yuan Hsu",
        "Tse-Yang Lin",
        "Ho-Lam Chung",
        "Kai-Wei Chang",
        "Shuo-yiin Chang",
        "Hung-yi Lee"
      ],
      "published": "2023-09-25T17:52:09Z",
      "updated": "2024-01-16T13:53:56Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.14324v2",
      "landing_url": "https://arxiv.org/abs/2309.14324v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.14324"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The inclusion criteria require explicit core focus on Spoken Language Models or equivalent concepts with speech as the main modeling target, and explicit mention of SLM/speech-language modeling terms in title/abstract/keywords. The item discusses a voice-conversion model guided by text instructions and uses a neural codec language model, but it does not explicitly frame itself as a Spoken Language Model or Speech Language Model, nor does it discuss SLM-centric architecture, training, or evaluation in the abstract. It remains a VC task, not clearly within the SLM/FD-SLM scope per the stated criteria. Therefore it should be excluded based on the core inclusion criterion, despite its linguistic/modeling flavor.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The inclusion criteria require explicit core focus on Spoken Language Models or equivalent concepts with speech as the main modeling target, and explicit mention of SLM/speech-language modeling terms in title/abstract/keywords. The item discusses a voice-conversion model guided by text instructions and uses a neural codec language model, but it does not explicitly frame itself as a Spoken Language Model or Speech Language Model, nor does it discuss SLM-centric architecture, training, or evaluation in the abstract. It remains a VC task, not clearly within the SLM/FD-SLM scope per the stated criteria. Therefore it should be excluded based on the core inclusion criterion, despite its linguistic/modeling flavor.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a novel voice conversion model guided by text instructions, emphasizing processing discrete speech codes end-to-end. However, it does not explicitly address spoken language modeling as a core focus, nor does it mention full-duplex interaction, low-latency, cognitive parallelism, or behavioral arbitration aspects crucial to the inclusion criteria. It centers on voice conversion rather than spoken language models or full-duplex spoken language systems, and lacks reference to the specified keywords and evaluation metrics associated with the topic. Therefore, it does not meet the inclusion criteria nor addresses the synchronized interaction behaviors required.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a novel voice conversion model guided by text instructions, emphasizing processing discrete speech codes end-to-end. However, it does not explicitly address spoken language modeling as a core focus, nor does it mention full-duplex interaction, low-latency, cognitive parallelism, or behavioral arbitration aspects crucial to the inclusion criteria. It centers on voice conversion rather than spoken language models or full-duplex spoken language systems, and lacks reference to the specified keywords and evaluation metrics associated with the topic. Therefore, it does not meet the inclusion criteria nor addresses the synchronized interaction behaviors required.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator",
    "abstract": "Text-to-video is a rapidly growing research area that aims to generate a semantic, identical, and temporal coherence sequence of frames that accurately align with the input text prompt. This study focuses on zero-shot text-to-video generation considering the data- and cost-efficient. To generate a semantic-coherent video, exhibiting a rich portrayal of temporal semantics such as the whole process of flower blooming rather than a set of \"moving images\", we propose a novel Free-Bloom pipeline that harnesses large language models (LLMs) as the director to generate a semantic-coherence prompt sequence, while pre-trained latent diffusion models (LDMs) as the animator to generate the high fidelity frames. Furthermore, to ensure temporal and identical coherence while maintaining semantic coherence, we propose a series of annotative modifications to adapting LDMs in the reverse process, including joint noise sampling, step-aware attention shift, and dual-path interpolation. Without any video data and training requirements, Free-Bloom generates vivid and high-quality videos, awe-inspiring in generating complex scenes with semantic meaningful frame sequences. In addition, Free-Bloom is naturally compatible with LDMs-based extensions.",
    "metadata": {
      "arxiv_id": "2309.14494",
      "title": "Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator",
      "summary": "Text-to-video is a rapidly growing research area that aims to generate a semantic, identical, and temporal coherence sequence of frames that accurately align with the input text prompt. This study focuses on zero-shot text-to-video generation considering the data- and cost-efficient. To generate a semantic-coherent video, exhibiting a rich portrayal of temporal semantics such as the whole process of flower blooming rather than a set of \"moving images\", we propose a novel Free-Bloom pipeline that harnesses large language models (LLMs) as the director to generate a semantic-coherence prompt sequence, while pre-trained latent diffusion models (LDMs) as the animator to generate the high fidelity frames. Furthermore, to ensure temporal and identical coherence while maintaining semantic coherence, we propose a series of annotative modifications to adapting LDMs in the reverse process, including joint noise sampling, step-aware attention shift, and dual-path interpolation. Without any video data and training requirements, Free-Bloom generates vivid and high-quality videos, awe-inspiring in generating complex scenes with semantic meaningful frame sequences. In addition, Free-Bloom is naturally compatible with LDMs-based extensions.",
      "authors": [
        "Hanzhuo Huang",
        "Yufan Feng",
        "Cheng Shi",
        "Lan Xu",
        "Jingyi Yu",
        "Sibei Yang"
      ],
      "published": "2023-09-25T19:42:16Z",
      "updated": "2023-09-25T19:42:16Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.14494v1",
      "landing_url": "https://arxiv.org/abs/2309.14494v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.14494"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on text-to-video generation using LLMs and latent diffusion models and does not treat Spoken Language Models or speech as the core modeling target, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on text-to-video generation using LLMs and latent diffusion models and does not treat Spoken Language Models or speech as the core modeling target, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a zero-shot text-to-video generation model using LLMs and latent diffusion models, without any mention of spoken language models, speech signals, or related speech interaction modeling. It does not align with the inclusion criteria centered on spoken language models, full-duplex spoken interaction, or speech language modeling, nor does it meet the evaluation or architectural focus requested. Therefore, it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a zero-shot text-to-video generation model using LLMs and latent diffusion models, without any mention of spoken language models, speech signals, or related speech interaction modeling. It does not align with the inclusion criteria centered on spoken language models, full-duplex spoken interaction, or speech language modeling, nor does it meet the evaluation or architectural focus requested. Therefore, it should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Temporal graph models fail to capture global temporal dynamics",
    "abstract": "A recently released Temporal Graph Benchmark is analyzed in the context of Dynamic Link Property Prediction. We outline our observations and propose a trivial optimization-free baseline of \"recently popular nodes\" outperforming other methods on medium and large-size datasets in the Temporal Graph Benchmark. We propose two measures based on Wasserstein distance which can quantify the strength of short-term and long-term global dynamics of datasets. By analyzing our unexpectedly strong baseline, we show how standard negative sampling evaluation can be unsuitable for datasets with strong temporal dynamics. We also show how simple negative-sampling can lead to model degeneration during training, resulting in impossible to rank, fully saturated predictions of temporal graph networks. We propose improved negative sampling schemes for both training and evaluation and prove their usefulness. We conduct a comparison with a model trained non-contrastively without negative sampling. Our results provide a challenging baseline and indicate that temporal graph network architectures need deep rethinking for usage in problems with significant global dynamics, such as social media, cryptocurrency markets or e-commerce. We open-source the code for baselines, measures and proposed negative sampling schemes.",
    "metadata": {
      "arxiv_id": "2309.15730",
      "title": "Temporal graph models fail to capture global temporal dynamics",
      "summary": "A recently released Temporal Graph Benchmark is analyzed in the context of Dynamic Link Property Prediction. We outline our observations and propose a trivial optimization-free baseline of \"recently popular nodes\" outperforming other methods on medium and large-size datasets in the Temporal Graph Benchmark. We propose two measures based on Wasserstein distance which can quantify the strength of short-term and long-term global dynamics of datasets. By analyzing our unexpectedly strong baseline, we show how standard negative sampling evaluation can be unsuitable for datasets with strong temporal dynamics. We also show how simple negative-sampling can lead to model degeneration during training, resulting in impossible to rank, fully saturated predictions of temporal graph networks. We propose improved negative sampling schemes for both training and evaluation and prove their usefulness. We conduct a comparison with a model trained non-contrastively without negative sampling. Our results provide a challenging baseline and indicate that temporal graph network architectures need deep rethinking for usage in problems with significant global dynamics, such as social media, cryptocurrency markets or e-commerce. We open-source the code for baselines, measures and proposed negative sampling schemes.",
      "authors": [
        "Michał Daniluk",
        "Jacek Dąbrowski"
      ],
      "published": "2023-09-27T15:36:45Z",
      "updated": "2023-12-08T13:54:55Z",
      "categories": [
        "cs.IR",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.15730v3",
      "landing_url": "https://arxiv.org/abs/2309.15730v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.15730"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item addresses temporal graph networks and global temporal dynamics, with no coverage of spoken language models or speech-based modeling; thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item addresses temporal graph networks and global temporal dynamics, with no coverage of spoken language models or speech-based modeling; thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on temporal graph models in the context of dynamic link prediction, without any mention of spoken language models, speech language modeling, or related concepts such as full-duplex interaction, speech–LLM architectures, or voice-based modeling. The core research object is temporal graph dynamics and negative sampling in time-series network data, not speech or spoken language modeling. Therefore, it does not meet the inclusion criteria nor address the key concepts required for inclusion related to speech or spoken language models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on temporal graph models in the context of dynamic link prediction, without any mention of spoken language models, speech language modeling, or related concepts such as full-duplex interaction, speech–LLM architectures, or voice-based modeling. The core research object is temporal graph dynamics and negative sampling in time-series network data, not speech or spoken language modeling. Therefore, it does not meet the inclusion criteria nor address the key concepts required for inclusion related to speech or spoken language models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Libertas: Privacy-Preserving Collective Computation for Decentralised Personal Data Stores",
    "abstract": "Data and data processing have become an indispensable aspect for our society. Insights drawn from collective data make invaluable contribution to scientific and societal research and business. But there are increasing worries about privacy issues and data misuse. This has prompted the emergence of decentralised personal data stores (PDS) like Solid that provide individuals more control over their personal data. However, existing PDS frameworks face challenges in ensuring data privacy when performing collective computations with data from multiple users. While Secure Multi-Party Computation (MPC) offers input secrecy protection during the computation without relying on any single party, issues emerge when directly applying MPC in the context of PDS, particularly due to key factors like autonomy and decentralisation. In this work, we discuss the essence of this issue, identify a potential solution, and introduce a modular architecture, Libertas, to integrate MPC with PDS like Solid, without requiring protocol-level changes. We introduce a paradigm shift from an `omniscient' view to individual-based, user-centric view of trust and security, and discuss the threat model of Libertas. Two realistic use cases for collaborative data processing are used for evaluation, both for technical feasibility and empirical benchmark, highlighting its effectiveness in empowering gig workers and generating differentially private synthetic data. The results of our experiments underscore Libertas' linear scalability and provide valuable insights into compute optimisations, thereby advancing the state-of-the-art in privacy-preserving data processing practices. By offering practical solutions for maintaining both individual autonomy and privacy in collaborative data processing environments, Libertas contributes significantly to the ongoing discourse on privacy protection in data-driven decision-making contexts.",
    "metadata": {
      "arxiv_id": "2309.16365",
      "title": "Libertas: Privacy-Preserving Collective Computation for Decentralised Personal Data Stores",
      "summary": "Data and data processing have become an indispensable aspect for our society. Insights drawn from collective data make invaluable contribution to scientific and societal research and business. But there are increasing worries about privacy issues and data misuse. This has prompted the emergence of decentralised personal data stores (PDS) like Solid that provide individuals more control over their personal data. However, existing PDS frameworks face challenges in ensuring data privacy when performing collective computations with data from multiple users. While Secure Multi-Party Computation (MPC) offers input secrecy protection during the computation without relying on any single party, issues emerge when directly applying MPC in the context of PDS, particularly due to key factors like autonomy and decentralisation. In this work, we discuss the essence of this issue, identify a potential solution, and introduce a modular architecture, Libertas, to integrate MPC with PDS like Solid, without requiring protocol-level changes. We introduce a paradigm shift from an `omniscient' view to individual-based, user-centric view of trust and security, and discuss the threat model of Libertas. Two realistic use cases for collaborative data processing are used for evaluation, both for technical feasibility and empirical benchmark, highlighting its effectiveness in empowering gig workers and generating differentially private synthetic data. The results of our experiments underscore Libertas' linear scalability and provide valuable insights into compute optimisations, thereby advancing the state-of-the-art in privacy-preserving data processing practices. By offering practical solutions for maintaining both individual autonomy and privacy in collaborative data processing environments, Libertas contributes significantly to the ongoing discourse on privacy protection in data-driven decision-making contexts.",
      "authors": [
        "Rui Zhao",
        "Naman Goel",
        "Nitin Agrawal",
        "Jun Zhao",
        "Jake Stein",
        "Wael Albayaydh",
        "Ruben Verborgh",
        "Reuben Binns",
        "Tim Berners-Lee",
        "Nigel Shadbolt"
      ],
      "published": "2023-09-28T12:07:40Z",
      "updated": "2025-03-30T21:35:47Z",
      "categories": [
        "cs.NI",
        "cs.CR",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.16365v2",
      "landing_url": "https://arxiv.org/abs/2309.16365v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.16365"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on privacy-preserving collective computation and MPC for decentralised personal data stores, with no focus on Spoken Language Models or any speech-language modeling concepts, training, or evaluation, so it does not meet the inclusion criteria and aligns with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on privacy-preserving collective computation and MPC for decentralised personal data stores, with no focus on Spoken Language Models or any speech-language modeling concepts, training, or evaluation, so it does not meet the inclusion criteria and aligns with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on privacy-preserving decentralized data stores and multi-party computation, with no mention of spoken language models, speech-based modeling, or related interaction paradigms such as full-duplex spoken language models; thus, it does not meet the thematic inclusion criteria centered on spoken language modeling and synchronous interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on privacy-preserving decentralized data stores and multi-party computation, with no mention of spoken language models, speech-based modeling, or related interaction paradigms such as full-duplex spoken language models; thus, it does not meet the thematic inclusion criteria centered on spoken language modeling and synchronous interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "HoloAssist: an Egocentric Human Interaction Dataset for Interactive AI Assistants in the Real World",
    "abstract": "Building an interactive AI assistant that can perceive, reason, and collaborate with humans in the real world has been a long-standing pursuit in the AI community. This work is part of a broader research effort to develop intelligent agents that can interactively guide humans through performing tasks in the physical world. As a first step in this direction, we introduce HoloAssist, a large-scale egocentric human interaction dataset, where two people collaboratively complete physical manipulation tasks. The task performer executes the task while wearing a mixed-reality headset that captures seven synchronized data streams. The task instructor watches the performer's egocentric video in real time and guides them verbally. By augmenting the data with action and conversational annotations and observing the rich behaviors of various participants, we present key insights into how human assistants correct mistakes, intervene in the task completion procedure, and ground their instructions to the environment. HoloAssist spans 166 hours of data captured by 350 unique instructor-performer pairs. Furthermore, we construct and present benchmarks on mistake detection, intervention type prediction, and hand forecasting, along with detailed analysis. We expect HoloAssist will provide an important resource for building AI assistants that can fluidly collaborate with humans in the real world. Data can be downloaded at https://holoassist.github.io/.",
    "metadata": {
      "arxiv_id": "2309.17024",
      "title": "HoloAssist: an Egocentric Human Interaction Dataset for Interactive AI Assistants in the Real World",
      "summary": "Building an interactive AI assistant that can perceive, reason, and collaborate with humans in the real world has been a long-standing pursuit in the AI community. This work is part of a broader research effort to develop intelligent agents that can interactively guide humans through performing tasks in the physical world. As a first step in this direction, we introduce HoloAssist, a large-scale egocentric human interaction dataset, where two people collaboratively complete physical manipulation tasks. The task performer executes the task while wearing a mixed-reality headset that captures seven synchronized data streams. The task instructor watches the performer's egocentric video in real time and guides them verbally. By augmenting the data with action and conversational annotations and observing the rich behaviors of various participants, we present key insights into how human assistants correct mistakes, intervene in the task completion procedure, and ground their instructions to the environment. HoloAssist spans 166 hours of data captured by 350 unique instructor-performer pairs. Furthermore, we construct and present benchmarks on mistake detection, intervention type prediction, and hand forecasting, along with detailed analysis. We expect HoloAssist will provide an important resource for building AI assistants that can fluidly collaborate with humans in the real world. Data can be downloaded at https://holoassist.github.io/.",
      "authors": [
        "Xin Wang",
        "Taein Kwon",
        "Mahdi Rad",
        "Bowen Pan",
        "Ishani Chakraborty",
        "Sean Andrist",
        "Dan Bohus",
        "Ashley Feniello",
        "Bugra Tekin",
        "Felipe Vieira Frujeri",
        "Neel Joshi",
        "Marc Pollefeys"
      ],
      "published": "2023-09-29T07:17:43Z",
      "updated": "2023-09-29T07:17:43Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.17024v1",
      "landing_url": "https://arxiv.org/abs/2309.17024v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.17024"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on a multimodal dataset for interactive AI assistance and human collaboration, not on Spoken Language Models as the core object of modeling or evaluation, so it does not satisfy the inclusion criteria focused on SLM/FD-SLM with speech-centric modeling and evaluation; it also appears to target 2025 publications, but the core theme is not speech-language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on a multimodal dataset for interactive AI assistance and human collaboration, not on Spoken Language Models as the core object of modeling or evaluation, so it does not satisfy the inclusion criteria focused on SLM/FD-SLM with speech-centric modeling and evaluation; it also appears to target 2025 publications, but the core theme is not speech-language modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces a large-scale egocentric human interaction dataset focused on physical task collaboration through verbal guidance, but it does not explicitly address spoken language models, speech language modeling, or full-duplex spoken language interactions as defined in the inclusion criteria. The core focus is on mixed-reality human interaction and task guidance rather than on speech modeling or low-latency full-duplex spoken language systems, thus it does not meet the inclusion criteria nor address the core concepts required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study introduces a large-scale egocentric human interaction dataset focused on physical task collaboration through verbal guidance, but it does not explicitly address spoken language models, speech language modeling, or full-duplex spoken language interactions as defined in the inclusion criteria. The core focus is on mixed-reality human interaction and task guidance rather than on speech modeling or low-latency full-duplex spoken language systems, thus it does not meet the inclusion criteria nor address the core concepts required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Voice2Action: Language Models as Agent for Efficient Real-Time Interaction in Virtual Reality",
    "abstract": "Large Language Models (LLMs) are trained and aligned to follow natural language instructions with only a handful of examples, and they are prompted as task-driven autonomous agents to adapt to various sources of execution environments. However, deploying agent LLMs in virtual reality (VR) has been challenging due to the lack of efficiency in online interactions and the complex manipulation categories in 3D environments. In this work, we propose Voice2Action, a framework that hierarchically analyzes customized voice signals and textual commands through action and entity extraction and divides the execution tasks into canonical interaction subsets in real-time with error prevention from environment feedback. Experiment results in an urban engineering VR environment with synthetic instruction data show that Voice2Action can perform more efficiently and accurately than approaches without optimizations.",
    "metadata": {
      "arxiv_id": "2310.00092",
      "title": "Voice2Action: Language Models as Agent for Efficient Real-Time Interaction in Virtual Reality",
      "summary": "Large Language Models (LLMs) are trained and aligned to follow natural language instructions with only a handful of examples, and they are prompted as task-driven autonomous agents to adapt to various sources of execution environments. However, deploying agent LLMs in virtual reality (VR) has been challenging due to the lack of efficiency in online interactions and the complex manipulation categories in 3D environments. In this work, we propose Voice2Action, a framework that hierarchically analyzes customized voice signals and textual commands through action and entity extraction and divides the execution tasks into canonical interaction subsets in real-time with error prevention from environment feedback. Experiment results in an urban engineering VR environment with synthetic instruction data show that Voice2Action can perform more efficiently and accurately than approaches without optimizations.",
      "authors": [
        "Yang Su"
      ],
      "published": "2023-09-29T19:06:52Z",
      "updated": "2023-09-29T19:06:52Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.00092v1",
      "landing_url": "https://arxiv.org/abs/2310.00092v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.00092"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses Voice2Action for real-time VR agent control using voice signals and textual commands, but it does not center on Spoken Language Models/Speech Language Models as the core subject (no explicit SLM/FD-SLM architecture, training, or evaluation focus), thus it likely does not meet the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item discusses Voice2Action for real-time VR agent control using voice signals and textual commands, but it does not center on Spoken Language Models/Speech Language Models as the core subject (no explicit SLM/FD-SLM architecture, training, or evaluation focus), thus it likely does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study 'Voice2Action' focuses on using Large Language Models in VR environments for real-time interaction via voice signals, yet the abstract does not explicitly indicate that the core modeling and reasoning is on spoken language models or speech language models themselves; instead, it highlights voice signal analysis combined with textual commands and hierarchical task execution. There is no mention of full-duplex spoken language model architectures, cognitive parallelism, low-latency interaction at sub-200ms, nor detailed evaluation on speech modeling or behavior arbitration; it seems the research mainly applies language models for VR interaction rather than advancing spoken language modeling as core. Therefore, it partially addresses some areas but lacks key aspects required by the inclusion criteria and does not fully meet the scope focusing on spoken language model core research.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study 'Voice2Action' focuses on using Large Language Models in VR environments for real-time interaction via voice signals, yet the abstract does not explicitly indicate that the core modeling and reasoning is on spoken language models or speech language models themselves; instead, it highlights voice signal analysis combined with textual commands and hierarchical task execution. There is no mention of full-duplex spoken language model architectures, cognitive parallelism, low-latency interaction at sub-200ms, nor detailed evaluation on speech modeling or behavior arbitration; it seems the research mainly applies language models for VR interaction rather than advancing spoken language modeling as core. Therefore, it partially addresses some areas but lacks key aspects required by the inclusion criteria and does not fully meet the scope focusing on spoken language model core research.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "CPIPS: Learning to Preserve Perceptual Distances in End-to-End Image Compression",
    "abstract": "Lossy image coding standards such as JPEG and MPEG have successfully achieved high compression rates for human consumption of multimedia data. However, with the increasing prevalence of IoT devices, drones, and self-driving cars, machines rather than humans are processing a greater portion of captured visual content. Consequently, it is crucial to pursue an efficient compressed representation that caters not only to human vision but also to image processing and machine vision tasks. Drawing inspiration from the efficient coding hypothesis in biological systems and the modeling of the sensory cortex in neural science, we repurpose the compressed latent representation to prioritize semantic relevance while preserving perceptual distance. Our proposed method, Compressed Perceptual Image Patch Similarity (CPIPS), can be derived at a minimal cost from a learned neural codec and computed significantly faster than DNN-based perceptual metrics such as LPIPS and DISTS.",
    "metadata": {
      "arxiv_id": "2310.00559",
      "title": "CPIPS: Learning to Preserve Perceptual Distances in End-to-End Image Compression",
      "summary": "Lossy image coding standards such as JPEG and MPEG have successfully achieved high compression rates for human consumption of multimedia data. However, with the increasing prevalence of IoT devices, drones, and self-driving cars, machines rather than humans are processing a greater portion of captured visual content. Consequently, it is crucial to pursue an efficient compressed representation that caters not only to human vision but also to image processing and machine vision tasks. Drawing inspiration from the efficient coding hypothesis in biological systems and the modeling of the sensory cortex in neural science, we repurpose the compressed latent representation to prioritize semantic relevance while preserving perceptual distance. Our proposed method, Compressed Perceptual Image Patch Similarity (CPIPS), can be derived at a minimal cost from a learned neural codec and computed significantly faster than DNN-based perceptual metrics such as LPIPS and DISTS.",
      "authors": [
        "Chen-Hsiu Huang",
        "Ja-Ling Wu"
      ],
      "published": "2023-10-01T03:29:21Z",
      "updated": "2023-10-01T03:29:21Z",
      "categories": [
        "cs.CV",
        "cs.MM",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.00559v1",
      "landing_url": "https://arxiv.org/abs/2310.00559v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.00559"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on image compression and perceptual distance metrics, not on Spoken Language Models or speech-language modeling; it does not meet the inclusion criteria (no SLM/FD-SLM core, no speech-centric evaluation) and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on image compression and perceptual distance metrics, not on Spoken Language Models or speech-language modeling; it does not meet the inclusion criteria (no SLM/FD-SLM core, no speech-centric evaluation) and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on image compression and machine vision rather than spoken language models or speech language modeling, and does not mention any of the key criteria such as semantic modeling of speech, full-duplex spoken language models, or related evaluation metrics. Therefore, it does not meet the inclusion criteria which require the research to explicitly address spoken language models as the central modeling and inference objects.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on image compression and machine vision rather than spoken language models or speech language modeling, and does not mention any of the key criteria such as semantic modeling of speech, full-duplex spoken language models, or related evaluation metrics. Therefore, it does not meet the inclusion criteria which require the research to explicitly address spoken language models as the central modeling and inference objects.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "UniAudio: An Audio Foundation Model Toward Universal Audio Generation",
    "abstract": "Large Language models (LLM) have demonstrated the capability to handle a variety of generative tasks. This paper presents the UniAudio system, which, unlike prior task-specific approaches, leverages LLM techniques to generate multiple types of audio (including speech, sounds, music, and singing) with given input conditions. UniAudio 1) first tokenizes all types of target audio along with other condition modalities, 2) concatenates source-target pair as a single sequence, and 3) performs next-token prediction using LLM. Also, a multi-scale Transformer model is proposed to handle the overly long sequences caused by the residual vector quantization based neural codec in tokenization. Training of UniAudio is scaled up to 165K hours of audio and 1B parameters, based on all generative tasks, aiming to obtain sufficient prior knowledge not only in the intrinsic properties of audio but also the inter-relationship between audio and other modalities. Therefore, the trained UniAudio model has the potential to become a foundation model for universal audio generation: it shows strong capability in all trained tasks and can seamlessly support new audio generation tasks after simple fine-tuning. Experiments demonstrate that UniAudio achieves state-of-the-art or at least competitive results on most of the 11 tasks. Demo and code are released at https://github.com/yangdongchao/UniAudio",
    "metadata": {
      "arxiv_id": "2310.00704",
      "title": "UniAudio: An Audio Foundation Model Toward Universal Audio Generation",
      "summary": "Large Language models (LLM) have demonstrated the capability to handle a variety of generative tasks. This paper presents the UniAudio system, which, unlike prior task-specific approaches, leverages LLM techniques to generate multiple types of audio (including speech, sounds, music, and singing) with given input conditions. UniAudio 1) first tokenizes all types of target audio along with other condition modalities, 2) concatenates source-target pair as a single sequence, and 3) performs next-token prediction using LLM. Also, a multi-scale Transformer model is proposed to handle the overly long sequences caused by the residual vector quantization based neural codec in tokenization. Training of UniAudio is scaled up to 165K hours of audio and 1B parameters, based on all generative tasks, aiming to obtain sufficient prior knowledge not only in the intrinsic properties of audio but also the inter-relationship between audio and other modalities. Therefore, the trained UniAudio model has the potential to become a foundation model for universal audio generation: it shows strong capability in all trained tasks and can seamlessly support new audio generation tasks after simple fine-tuning. Experiments demonstrate that UniAudio achieves state-of-the-art or at least competitive results on most of the 11 tasks. Demo and code are released at https://github.com/yangdongchao/UniAudio",
      "authors": [
        "Dongchao Yang",
        "Jinchuan Tian",
        "Xu Tan",
        "Rongjie Huang",
        "Songxiang Liu",
        "Xuankai Chang",
        "Jiatong Shi",
        "Sheng Zhao",
        "Jiang Bian",
        "Zhou Zhao",
        "Xixin Wu",
        "Helen Meng"
      ],
      "published": "2023-10-01T15:49:46Z",
      "updated": "2024-12-10T03:17:56Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.00704v6",
      "landing_url": "https://arxiv.org/abs/2310.00704v6",
      "doi": "https://doi.org/10.48550/arXiv.2310.00704"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets universal audio generation across speech, music, and other sounds using LLM-based token prediction, but the inclusion criteria require a core focus on Spoken Language Models/Speech Language Modeling with speech as the primary modeling object; this work treats audio types broadly and is not centered on speech-language modeling.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper targets universal audio generation across speech, music, and other sounds using LLM-based token prediction, but the inclusion criteria require a core focus on Spoken Language Models/Speech Language Modeling with speech as the primary modeling object; this work treats audio types broadly and is not centered on speech-language modeling.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes UniAudio, a large-scale audio generation foundation model that handles various types of audio generation tasks by leveraging LLM techniques, but it does not explicitly focus on spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs), nor does it address synchronous interactive behaviors, turn-taking, or cognitive parallelism; it primarily focuses on audio tokenization and generation across speech, sounds, music, and singing, without explicit evaluation of real-time interaction or behavior arbitration, and does not mention spoken language modeling as a core concept in the title or abstract, which are key inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes UniAudio, a large-scale audio generation foundation model that handles various types of audio generation tasks by leveraging LLM techniques, but it does not explicitly focus on spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs), nor does it address synchronous interactive behaviors, turn-taking, or cognitive parallelism; it primarily focuses on audio tokenization and generation across speech, sounds, music, and singing, without explicit evaluation of real-time interaction or behavior arbitration, and does not mention spoken language modeling as a core concept in the title or abstract, which are key inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Towards human-like spoken dialogue generation between AI agents from written dialogue",
    "abstract": "The advent of large language models (LLMs) has made it possible to generate natural written dialogues between two agents. However, generating human-like spoken dialogues from these written dialogues remains challenging. Spoken dialogues have several unique characteristics: they frequently include backchannels and laughter, and the smoothness of turn-taking significantly influences the fluidity of conversation. This study proposes CHATS - CHatty Agents Text-to-Speech - a discrete token-based system designed to generate spoken dialogues based on written dialogues. Our system can generate speech for both the speaker side and the listener side simultaneously, using only the transcription from the speaker side, which eliminates the need for transcriptions of backchannels or laughter. Moreover, CHATS facilitates natural turn-taking; it determines the appropriate duration of silence after each utterance in the absence of overlap, and it initiates the generation of overlapping speech based on the phoneme sequence of the next utterance in case of overlap. Experimental evaluations indicate that CHATS outperforms the text-to-speech baseline, producing spoken dialogues that are more interactive and fluid while retaining clarity and intelligibility.",
    "metadata": {
      "arxiv_id": "2310.01088",
      "title": "Towards human-like spoken dialogue generation between AI agents from written dialogue",
      "summary": "The advent of large language models (LLMs) has made it possible to generate natural written dialogues between two agents. However, generating human-like spoken dialogues from these written dialogues remains challenging. Spoken dialogues have several unique characteristics: they frequently include backchannels and laughter, and the smoothness of turn-taking significantly influences the fluidity of conversation. This study proposes CHATS - CHatty Agents Text-to-Speech - a discrete token-based system designed to generate spoken dialogues based on written dialogues. Our system can generate speech for both the speaker side and the listener side simultaneously, using only the transcription from the speaker side, which eliminates the need for transcriptions of backchannels or laughter. Moreover, CHATS facilitates natural turn-taking; it determines the appropriate duration of silence after each utterance in the absence of overlap, and it initiates the generation of overlapping speech based on the phoneme sequence of the next utterance in case of overlap. Experimental evaluations indicate that CHATS outperforms the text-to-speech baseline, producing spoken dialogues that are more interactive and fluid while retaining clarity and intelligibility.",
      "authors": [
        "Kentaro Mitsui",
        "Yukiya Hono",
        "Kei Sawada"
      ],
      "published": "2023-10-02T11:03:20Z",
      "updated": "2023-10-02T11:03:20Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.01088v1",
      "landing_url": "https://arxiv.org/abs/2310.01088v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.01088"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on Spoken Language Models / speech-language modeling by proposing a system to generate spoken dialogue from written dialogue, addressing turn-taking, backchannels, laughter, and overlapping speech, with architecture and evaluation, covering within the last 3 years, thus meeting inclusion and not falling under exclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on Spoken Language Models / speech-language modeling by proposing a system to generate spoken dialogue from written dialogue, addressing turn-taking, backchannels, laughter, and overlapping speech, with architecture and evaluation, covering within the last 3 years, thus meeting inclusion and not falling under exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes CHATS, a text-to-speech system that generates human-like spoken dialogues considering turn-taking, backchannels, and overlapping speech, which aligns with the topic of full-duplex spoken language models focusing on speech-level modeling and interactive synchronization. Although the abstract does not explicitly mention \"spoken language model(s)\" or \"speech language model(s)\", it addresses engineered synchronization, turn-taking, and overlapping speech generation — key concepts of full-duplex spoken dialogue in speech-centric modeling. However, it mainly emphasizes text-to-speech generation rather than explicitly discussing model architecture or learning for spoken language modeling per se, which introduces uncertainty regarding whether it meets all the inclusion criteria fully. Due to the partial alignment with inclusion criteria and no clear exclusion criteria met, the confidence level is moderate.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study proposes CHATS, a text-to-speech system that generates human-like spoken dialogues considering turn-taking, backchannels, and overlapping speech, which aligns with the topic of full-duplex spoken language models focusing on speech-level modeling and interactive synchronization. Although the abstract does not explicitly mention \"spoken language model(s)\" or \"speech language model(s)\", it addresses engineered synchronization, turn-taking, and overlapping speech generation — key concepts of full-duplex spoken dialogue in speech-centric modeling. However, it mainly emphasizes text-to-speech generation rather than explicitly discussing model architecture or learning for spoken language modeling per se, which introduces uncertainty regarding whether it meets all the inclusion criteria fully. Due to the partial alignment with inclusion criteria and no clear exclusion criteria met, the confidence level is moderate.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: one recommended inclusion (5) noting CHATS models spoken dialogue and synchronization; the other gave a cautious 3, noting it is TTS-focused and lacks explicit spoken-language-model phrasing. The abstract describes CHATS, a discrete token-based TTS system that generates both speaker and listener speech and handles backchannels, laughter, turn-taking durations, and overlapping speech, with evaluations showing improved interactivity — so it addresses synchronization and dialog behaviors. However, it does not explicitly use wording like “spoken language model(s)”, “speech language model(s)”, or “language modeling of speech” as required by the inclusion criteria, and it primarily presents a text-to-speech dialogue-generation system rather than clearly centering on speech-as-core language modeling (end-to-end SLMs) per the strict topic definition. Therefore it does not meet the explicit inclusion requirement and should be excluded (better to exclude).",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: one recommended inclusion (5) noting CHATS models spoken dialogue and synchronization; the other gave a cautious 3, noting it is TTS-focused and lacks explicit spoken-language-model phrasing. The abstract describes CHATS, a discrete token-based TTS system that generates both speaker and listener speech and handles backchannels, laughter, turn-taking durations, and overlapping speech, with evaluations showing improved interactivity — so it addresses synchronization and dialog behaviors. However, it does not explicitly use wording like “spoken language model(s)”, “speech language model(s)”, or “language modeling of speech” as required by the inclusion criteria, and it primarily presents a text-to-speech dialogue-generation system rather than clearly centering on speech-as-core language modeling (end-to-end SLMs) per the strict topic definition. Therefore it does not meet the explicit inclusion requirement and should be excluded (better to exclude).",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Simultaneous inference for monotone and smoothly time-varying functions under complex temporal dynamics",
    "abstract": "We propose a new framework for the simultaneous inference of monotone and smoothly time-varying functions under complex temporal dynamics. This will be done utilizing the monotone rearrangement and the nonparametric estimation. We capitalize the Gaussian approximation for the nonparametric monotone estimator and construct the asymptotically correct simultaneous confidence bands (SCBs) using designed bootstrap methods. We investigate two general and practical scenarios. The first is the simultaneous inference of monotone smooth trends from moderately high-dimensional time series. The proposed algorithm has been employed for the joint inference of temperature curves from multiple areas. Specifically, most existing methods are designed for a single monotone smooth trend. In such cases, our proposed SCB empirically exhibits the narrowest width among existing approaches while maintaining confidence levels. It has also been used for testing several hypotheses tailored to global warming. The second scenario involves simultaneous inference of monotone and smoothly time-varying regression coefficients in time-varying coefficient linear models. The proposed algorithm has been utilized for testing the impact of sunshine duration on temperature which is believed to be increasing due to severe greenhouse effect. The validity of the proposed methods has been justified in theory as well as by extensive simulations.",
    "metadata": {
      "arxiv_id": "2310.02177",
      "title": "Simultaneous inference for monotone and smoothly time-varying functions under complex temporal dynamics",
      "summary": "We propose a new framework for the simultaneous inference of monotone and smoothly time-varying functions under complex temporal dynamics. This will be done utilizing the monotone rearrangement and the nonparametric estimation. We capitalize the Gaussian approximation for the nonparametric monotone estimator and construct the asymptotically correct simultaneous confidence bands (SCBs) using designed bootstrap methods. We investigate two general and practical scenarios. The first is the simultaneous inference of monotone smooth trends from moderately high-dimensional time series. The proposed algorithm has been employed for the joint inference of temperature curves from multiple areas. Specifically, most existing methods are designed for a single monotone smooth trend. In such cases, our proposed SCB empirically exhibits the narrowest width among existing approaches while maintaining confidence levels. It has also been used for testing several hypotheses tailored to global warming. The second scenario involves simultaneous inference of monotone and smoothly time-varying regression coefficients in time-varying coefficient linear models. The proposed algorithm has been utilized for testing the impact of sunshine duration on temperature which is believed to be increasing due to severe greenhouse effect. The validity of the proposed methods has been justified in theory as well as by extensive simulations.",
      "authors": [
        "Tianpai Luo",
        "Weichi Wu"
      ],
      "published": "2023-10-03T16:10:23Z",
      "updated": "2025-08-19T03:53:04Z",
      "categories": [
        "math.ST"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.02177v4",
      "landing_url": "https://arxiv.org/abs/2310.02177v4",
      "doi": "https://doi.org/10.48550/arXiv.2310.02177"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about statistical inference for monotone/time-varying functions in time-series rather than Spoken Language Models or speech-language modeling, so it fails the inclusion criteria and likely triggers the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about statistical inference for monotone/time-varying functions in time-series rather than Spoken Language Models or speech-language modeling, so it fails the inclusion criteria and likely triggers the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on simultaneous inference for monotone and smoothly time-varying functions related to temperature and environmental data, with no mention of Spoken Language Models, speech language modeling, speech signals, or interactive full-duplex speech systems, which are essential inclusion criteria. It also does not discuss any model architectures or evaluations related to speech or spoken language, thereby failing to meet the defined thematic scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on simultaneous inference for monotone and smoothly time-varying functions related to temperature and environmental data, with no mention of Spoken Language Models, speech language modeling, speech signals, or interactive full-duplex speech systems, which are essential inclusion criteria. It also does not discuss any model architectures or evaluations related to speech or spoken language, thereby failing to meet the defined thematic scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Shaping the Epochal Individuality and Generality: The Temporal Dynamics of Uncertainty and Prediction Error in Musical Improvisation",
    "abstract": "Musical improvisation, much like spontaneous speech, reveals intricate facets of the improviser's state of mind and emotional character. However, the specific musical components that reveal such individuality remain largely unexplored. Within the framework of brain's statistical learning and predictive processing, this study examined the temporal dynamics of uncertainty and surprise (prediction error) in a piece of musical improvisation. This study employed the HBSL model to analyze a corpus of 456 Jazz improvisations, spanning 1905 to 2009, from 78 distinct Jazz musicians. The results indicated distinctive temporal patterns of surprise and uncertainty, especially in pitch and pitch-rhythm sequences, revealing era-specific features from the early 20th to the 21st centuries. Conversely, rhythm sequences exhibited a consistent degree of uncertainty across eras. Further, the acoustic properties remain unchanged across different periods. These findings highlight the importance of how temporal dynamics of surprise and uncertainty in improvisational music change over periods, profoundly influencing the distinctive methodologies artists adopt for improvisation in each era. Further, it is suggested that the development of improvisational music can be attributed to the brain's adaptive statistical learning mechanisms, which constantly refine internal models to mirror the cultural and emotional nuances of their respective epochs. This study unravels the evolutionary trajectory of improvisational music and highlights the nuanced shifts artists employ to resonate with the cultural and emotional landscapes of their times.",
    "metadata": {
      "arxiv_id": "2310.02518",
      "title": "Shaping the Epochal Individuality and Generality: The Temporal Dynamics of Uncertainty and Prediction Error in Musical Improvisation",
      "summary": "Musical improvisation, much like spontaneous speech, reveals intricate facets of the improviser's state of mind and emotional character. However, the specific musical components that reveal such individuality remain largely unexplored. Within the framework of brain's statistical learning and predictive processing, this study examined the temporal dynamics of uncertainty and surprise (prediction error) in a piece of musical improvisation. This study employed the HBSL model to analyze a corpus of 456 Jazz improvisations, spanning 1905 to 2009, from 78 distinct Jazz musicians. The results indicated distinctive temporal patterns of surprise and uncertainty, especially in pitch and pitch-rhythm sequences, revealing era-specific features from the early 20th to the 21st centuries. Conversely, rhythm sequences exhibited a consistent degree of uncertainty across eras. Further, the acoustic properties remain unchanged across different periods. These findings highlight the importance of how temporal dynamics of surprise and uncertainty in improvisational music change over periods, profoundly influencing the distinctive methodologies artists adopt for improvisation in each era. Further, it is suggested that the development of improvisational music can be attributed to the brain's adaptive statistical learning mechanisms, which constantly refine internal models to mirror the cultural and emotional nuances of their respective epochs. This study unravels the evolutionary trajectory of improvisational music and highlights the nuanced shifts artists employ to resonate with the cultural and emotional landscapes of their times.",
      "authors": [
        "Tatsuya Daikoku"
      ],
      "published": "2023-10-04T01:33:26Z",
      "updated": "2023-10-04T01:33:26Z",
      "categories": [
        "cs.SD",
        "cs.IR",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.02518v1",
      "landing_url": "https://arxiv.org/abs/2310.02518v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.02518"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on musical improvisation and predictive processing with brain data, not on Spoken Language Models or speech-language modeling; it fails to meet the defined inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on musical improvisation and predictive processing with brain data, not on Spoken Language Models or speech-language modeling; it fails to meet the defined inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on musical improvisation and the brain's predictive processing in music, without addressing spoken language models, speech language modeling, or full-duplex spoken language interaction; it neither discusses relevant model architectures nor evaluation for spoken language or speech-based systems as specified in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on musical improvisation and the brain's predictive processing in music, without addressing spoken language models, speech language modeling, or full-duplex spoken language interaction; it neither discusses relevant model architectures nor evaluation for spoken language or speech-based systems as specified in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT",
    "abstract": "Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks, and have shown great potential as backbones for audio-and-text large language models (LLMs). Previous mainstream audio-and-text LLMs use discrete audio tokens to represent both input and output audio; however, they suffer from performance degradation on tasks such as automatic speech recognition, speech-to-text translation, and speech enhancement over models using continuous speech features. In this paper, we propose LauraGPT, a novel unified audio-and-text GPT-based LLM for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities. We propose a novel data representation that combines continuous and discrete features for audio: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. We propose a one-step codec vocoder to overcome the prediction challenge caused by the multimodal distribution of codec tokens. We fine-tune LauraGPT using supervised multi-task learning. Extensive experiments show that LauraGPT consistently achieves comparable to superior performance compared to strong baselines on a wide range of audio tasks related to content, semantics, paralinguistics, and audio-signal analysis, such as automatic speech recognition, speech-to-text translation, text-to-speech synthesis, speech enhancement, automated audio captioning, speech emotion recognition, and spoken language understanding.",
    "metadata": {
      "arxiv_id": "2310.04673",
      "title": "LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT",
      "summary": "Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks, and have shown great potential as backbones for audio-and-text large language models (LLMs). Previous mainstream audio-and-text LLMs use discrete audio tokens to represent both input and output audio; however, they suffer from performance degradation on tasks such as automatic speech recognition, speech-to-text translation, and speech enhancement over models using continuous speech features. In this paper, we propose LauraGPT, a novel unified audio-and-text GPT-based LLM for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities. We propose a novel data representation that combines continuous and discrete features for audio: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. We propose a one-step codec vocoder to overcome the prediction challenge caused by the multimodal distribution of codec tokens. We fine-tune LauraGPT using supervised multi-task learning. Extensive experiments show that LauraGPT consistently achieves comparable to superior performance compared to strong baselines on a wide range of audio tasks related to content, semantics, paralinguistics, and audio-signal analysis, such as automatic speech recognition, speech-to-text translation, text-to-speech synthesis, speech enhancement, automated audio captioning, speech emotion recognition, and spoken language understanding.",
      "authors": [
        "Zhihao Du",
        "Jiaming Wang",
        "Qian Chen",
        "Yunfei Chu",
        "Zhifu Gao",
        "Zerui Li",
        "Kai Hu",
        "Xiaohuan Zhou",
        "Jin Xu",
        "Ziyang Ma",
        "Wen Wang",
        "Siqi Zheng",
        "Chang Zhou",
        "Zhijie Yan",
        "Shiliang Zhang"
      ],
      "published": "2023-10-07T03:17:59Z",
      "updated": "2024-07-03T02:38:03Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.04673v4",
      "landing_url": "https://arxiv.org/abs/2310.04673v4",
      "doi": "https://doi.org/10.48550/arXiv.2310.04673"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item presents LauraGPT, a unified audio-and-text GPT-based LLM designed for audio recognition, understanding, and generation, treating speech as a core modality and detailing architecture, training, and multi-task evaluation across several speech tasks (ASR, STT, TTS, speech enhancement, captioning, emotion recognition, SLU). It aligns with the Spoken Language Models / speech-language modeling focus, is within the last 3 years, and discusses model design and evaluation rather than being limited to a single ASR/TTS task. The only potential gap is the explicit use of the exact term 'Spoken Language Models,' but the abstract conveys equivalent concepts (audio-based, language modeling of speech) and multi-task speech-language objectives.\"",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item presents LauraGPT, a unified audio-and-text GPT-based LLM designed for audio recognition, understanding, and generation, treating speech as a core modality and detailing architecture, training, and multi-task evaluation across several speech tasks (ASR, STT, TTS, speech enhancement, captioning, emotion recognition, SLU). It aligns with the Spoken Language Models / speech-language modeling focus, is within the last 3 years, and discusses model design and evaluation rather than being limited to a single ASR/TTS task. The only potential gap is the explicit use of the exact term 'Spoken Language Models,' but the abstract conveys equivalent concepts (audio-based, language modeling of speech) and multi-task speech-language objectives.\"",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes LauraGPT, a GPT-based audio-and-text large language model that processes audio and text inputs with continuous and discrete speech features, focusing on audio recognition, understanding, and generation tasks. However, it does not explicitly discuss spoken language models or full-duplex spoken language models as the core subject, nor does it address synchronous interaction, turn-taking, cognitive parallelism, low-latency interaction, or behavior arbitration in dialogue. The evaluation centers on traditional audio tasks (ASR, speech-to-text, TTS, etc.) without involving real-time interaction or multi-party synchronization, and the research seems more on multimodal modeling involving speech and text rather than exclusively modeling speech sequences or synchronization in dialogue. Therefore, while relevant to speech and audio LLMs, it does not fully meet the inclusion criteria targeting spoken language modeling with synchronous interaction capabilities or full-duplex dialogue behaviors.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes LauraGPT, a GPT-based audio-and-text large language model that processes audio and text inputs with continuous and discrete speech features, focusing on audio recognition, understanding, and generation tasks. However, it does not explicitly discuss spoken language models or full-duplex spoken language models as the core subject, nor does it address synchronous interaction, turn-taking, cognitive parallelism, low-latency interaction, or behavior arbitration in dialogue. The evaluation centers on traditional audio tasks (ASR, speech-to-text, TTS, etc.) without involving real-time interaction or multi-party synchronization, and the research seems more on multimodal modeling involving speech and text rather than exclusively modeling speech sequences or synchronization in dialogue. Therefore, while relevant to speech and audio LLMs, it does not fully meet the inclusion criteria targeting spoken language modeling with synchronous interaction capabilities or full-duplex dialogue behaviors.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "JuniorNano recommended inclusion (4) noting LauraGPT is a unified audio-and-text GPT LLM that treats speech as a core modality and presents architecture/training/evaluation across many speech tasks; JuniorMini recommended exclusion (2) because the paper does not address synchronous/full‑duplex interaction, turn‑taking or real‑time behavior arbitration and focuses on traditional audio tasks. My judgment: the abstract describes a speech-centric, audio–text LLM that encodes continuous audio representations and generates audio/text outputs, discusses model design, training, and multi-task evaluation (i.e., language modeling of speech equivalent to a Spoken Language Model), so it meets the core inclusion requirement of treating speech as the primary modeling object and presenting modeling/training details; however it does not cover full‑duplex/synchronous interaction or low‑latency behavior arbitration (which are not strict must-have items unless FD‑SLM claims are required). Therefore I recommend inclusion but not an absolute include since it lacks explicit FD‑SLM or real‑time interaction evaluation.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "JuniorNano recommended inclusion (4) noting LauraGPT is a unified audio-and-text GPT LLM that treats speech as a core modality and presents architecture/training/evaluation across many speech tasks; JuniorMini recommended exclusion (2) because the paper does not address synchronous/full‑duplex interaction, turn‑taking or real‑time behavior arbitration and focuses on traditional audio tasks. My judgment: the abstract describes a speech-centric, audio–text LLM that encodes continuous audio representations and generates audio/text outputs, discusses model design, training, and multi-task evaluation (i.e., language modeling of speech equivalent to a Spoken Language Model), so it meets the core inclusion requirement of treating speech as the primary modeling object and presenting modeling/training details; however it does not cover full‑duplex/synchronous interaction or low‑latency behavior arbitration (which are not strict must-have items unless FD‑SLM claims are required). Therefore I recommend inclusion but not an absolute include since it lacks explicit FD‑SLM or real‑time interaction evaluation.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Generative Spoken Language Model based on continuous word-sized audio tokens",
    "abstract": "In NLP, text language models based on words or subwords are known to outperform their character-based counterparts. Yet, in the speech community, the standard input of spoken LMs are 20ms or 40ms-long discrete units (shorter than a phoneme). Taking inspiration from word-based LM, we introduce a Generative Spoken Language Model (GSLM) based on word-size continuous-valued audio embeddings that can generate diverse and expressive language output. This is obtained by replacing lookup table for lexical types with a Lexical Embedding function, the cross entropy loss by a contrastive loss, and multinomial sampling by k-NN sampling. The resulting model is the first generative language model based on word-size continuous embeddings. Its performance is on par with discrete unit GSLMs regarding generation quality as measured by automatic metrics and subjective human judgements. Moreover, it is five times more memory efficient thanks to its large 200ms units. In addition, the embeddings before and after the Lexical Embedder are phonetically and semantically interpretable.",
    "metadata": {
      "arxiv_id": "2310.05224",
      "title": "Generative Spoken Language Model based on continuous word-sized audio tokens",
      "summary": "In NLP, text language models based on words or subwords are known to outperform their character-based counterparts. Yet, in the speech community, the standard input of spoken LMs are 20ms or 40ms-long discrete units (shorter than a phoneme). Taking inspiration from word-based LM, we introduce a Generative Spoken Language Model (GSLM) based on word-size continuous-valued audio embeddings that can generate diverse and expressive language output. This is obtained by replacing lookup table for lexical types with a Lexical Embedding function, the cross entropy loss by a contrastive loss, and multinomial sampling by k-NN sampling. The resulting model is the first generative language model based on word-size continuous embeddings. Its performance is on par with discrete unit GSLMs regarding generation quality as measured by automatic metrics and subjective human judgements. Moreover, it is five times more memory efficient thanks to its large 200ms units. In addition, the embeddings before and after the Lexical Embedder are phonetically and semantically interpretable.",
      "authors": [
        "Robin Algayres",
        "Yossi Adi",
        "Tu Anh Nguyen",
        "Jade Copet",
        "Gabriel Synnaeve",
        "Benoit Sagot",
        "Emmanuel Dupoux"
      ],
      "published": "2023-10-08T16:46:14Z",
      "updated": "2023-10-08T16:46:14Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.05224v1",
      "landing_url": "https://arxiv.org/abs/2310.05224v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.05224"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "符合主題定義：核心為 Spoken Language Model，使用語音作為主要建模對象，提出模型架構/訓練與評估，且在近3年內發表，標題含 spoken language model 的相關描述，未落入排除條件（非單向/僅 ASR 或 TTS、非單一任務、且提供方法與評估）。因此應該納入。",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "符合主題定義：核心為 Spoken Language Model，使用語音作為主要建模對象，提出模型架構/訓練與評估，且在近3年內發表，標題含 spoken language model 的相關描述，未落入排除條件（非單向/僅 ASR 或 TTS、非單一任務、且提供方法與評估）。因此應該納入。",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study centers on a Generative Spoken Language Model that uses word-sized continuous audio tokens, focusing on speech as the primary modeling object and discussing model architecture and evaluation. However, the abstract does not explicitly mention full-duplex interaction, synchronous conversational behaviors such as turn-taking, cognitive parallelism, low-latency interaction, or multi-dimensional evaluation related to behavior arbitration, which are required aspects of the inclusion criteria. Therefore, it partially meets the inclusion criteria but lacks indication of synchronization or conversational interaction focus required for full inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study centers on a Generative Spoken Language Model that uses word-sized continuous audio tokens, focusing on speech as the primary modeling object and discussing model architecture and evaluation. However, the abstract does not explicitly mention full-duplex interaction, synchronous conversational behaviors such as turn-taking, cognitive parallelism, low-latency interaction, or multi-dimensional evaluation related to behavior arbitration, which are required aspects of the inclusion criteria. Therefore, it partially meets the inclusion criteria but lacks indication of synchronization or conversational interaction focus required for full inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The paper explicitly targets a Generative Spoken Language Model (title and abstract mention spoken language model and word-sized continuous audio tokens), proposes architectural and training innovations, and reports automatic and human evaluations—so it satisfies the inclusion criteria of being speech-centered SLM research; although JuniorMini raised the absence of explicit full‑duplex/turn‑taking discussion, that is not a mandatory inclusion requirement and the work is not merely ASR/TTS or a single-task study, so it should be included (agreeing with JuniorNano).",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "The paper explicitly targets a Generative Spoken Language Model (title and abstract mention spoken language model and word-sized continuous audio tokens), proposes architectural and training innovations, and reports automatic and human evaluations—so it satisfies the inclusion criteria of being speech-centered SLM research; although JuniorMini raised the absence of explicit full‑duplex/turn‑taking discussion, that is not a mandatory inclusion requirement and the work is not merely ASR/TTS or a single-task study, so it should be included (agreeing with JuniorNano).",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Low-latency Speech Enhancement via Speech Token Generation",
    "abstract": "Existing deep learning based speech enhancement mainly employ a data-driven approach, which leverage large amounts of data with a variety of noise types to achieve noise removal from noisy signal. However, the high dependence on the data limits its generalization on the unseen complex noises in real-life environment. In this paper, we focus on the low-latency scenario and regard speech enhancement as a speech generation problem conditioned on the noisy signal, where we generate clean speech instead of identifying and removing noises. Specifically, we propose a conditional generative framework for speech enhancement, which models clean speech by acoustic codes of a neural speech codec and generates the speech codes conditioned on past noisy frames in an auto-regressive way. Moreover, we propose an explicit-alignment approach to align noisy frames with the generated speech tokens to improve the robustness and scalability to different input lengths. Different from other methods that leverage multiple stages to generate speech codes, we leverage a single-stage speech generation approach based on the TF-Codec neural codec to achieve high speech quality with low latency. Extensive results on both synthetic and real-recorded test set show its superiority over data-driven approaches in terms of noise robustness and temporal speech coherence.",
    "metadata": {
      "arxiv_id": "2310.08981",
      "title": "Low-latency Speech Enhancement via Speech Token Generation",
      "summary": "Existing deep learning based speech enhancement mainly employ a data-driven approach, which leverage large amounts of data with a variety of noise types to achieve noise removal from noisy signal. However, the high dependence on the data limits its generalization on the unseen complex noises in real-life environment. In this paper, we focus on the low-latency scenario and regard speech enhancement as a speech generation problem conditioned on the noisy signal, where we generate clean speech instead of identifying and removing noises. Specifically, we propose a conditional generative framework for speech enhancement, which models clean speech by acoustic codes of a neural speech codec and generates the speech codes conditioned on past noisy frames in an auto-regressive way. Moreover, we propose an explicit-alignment approach to align noisy frames with the generated speech tokens to improve the robustness and scalability to different input lengths. Different from other methods that leverage multiple stages to generate speech codes, we leverage a single-stage speech generation approach based on the TF-Codec neural codec to achieve high speech quality with low latency. Extensive results on both synthetic and real-recorded test set show its superiority over data-driven approaches in terms of noise robustness and temporal speech coherence.",
      "authors": [
        "Huaying Xue",
        "Xiulian Peng",
        "Yan Lu"
      ],
      "published": "2023-10-13T09:57:09Z",
      "updated": "2024-01-23T06:13:04Z",
      "categories": [
        "cs.SD",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.08981v3",
      "landing_url": "https://arxiv.org/abs/2310.08981v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.08981"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although it uses speech-token generation, the work is about low-latency speech enhancement rather than core Spoken Language Models / FD-SLMs with turn-taking and real-time interaction, so it does not meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although it uses speech-token generation, the work is about low-latency speech enhancement rather than core Spoken Language Models / FD-SLMs with turn-taking and real-time interaction, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on low-latency speech enhancement by framing it as a speech generation task conditioned on noisy signals, but it does not explicitly address spoken language modeling or full-duplex spoken language models, nor does it discuss the model architecture or evaluation from a speech language modeling or interactive dialogue perspective as required by the inclusion criteria; it primarily deals with speech enhancement rather than language modeling or synchronized interaction. Therefore, it does not meet the core inclusion criteria and aligns partially with exclusion criteria concerning lack of dialogue or interaction modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on low-latency speech enhancement by framing it as a speech generation task conditioned on noisy signals, but it does not explicitly address spoken language modeling or full-duplex spoken language models, nor does it discuss the model architecture or evaluation from a speech language modeling or interactive dialogue perspective as required by the inclusion criteria; it primarily deals with speech enhancement rather than language modeling or synchronized interaction. Therefore, it does not meet the core inclusion criteria and aligns partially with exclusion criteria concerning lack of dialogue or interaction modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic Organization in HuBERT",
    "abstract": "Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space and the units beyond phonemes are largely underexplored. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt \"self-distillation\" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames exhibit salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling.",
    "metadata": {
      "arxiv_id": "2310.10803",
      "title": "SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic Organization in HuBERT",
      "summary": "Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space and the units beyond phonemes are largely underexplored. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt \"self-distillation\" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames exhibit salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling.",
      "authors": [
        "Cheol Jun Cho",
        "Abdelrahman Mohamed",
        "Shang-Wen Li",
        "Alan W Black",
        "Gopala K. Anumanchipalli"
      ],
      "published": "2023-10-16T20:05:36Z",
      "updated": "2025-04-10T11:20:55Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.10803v3",
      "landing_url": "https://arxiv.org/abs/2310.10803v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.10803"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is a speech-focused study that explicitly engages with spoken language modeling concepts (sentence-level representation, data-driven units for spoken language modeling) and discusses architecture/training/evaluation, within the last 3 years, not falling into any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item is a speech-focused study that explicitly engages with spoken language modeling concepts (sentence-level representation, data-driven units for spoken language modeling) and discusses architecture/training/evaluation, within the last 3 years, not falling into any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning of speech units, specifically emergent syllabic organization in sentence-level speech representation via HuBERT fine-tuning. Although it deals with speech modeling and proposes a benchmark for sentence-level speech representation, it does not explicitly mention Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it address synchronization, cognitive parallelism, or real-time interaction aspects related to full-duplex communication. It primarily investigates emergent speech unit discovery without directly discussing dialogue interaction, synchronous behavior arbitration, or speech-LLM architecture integration as defined in the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning of speech units, specifically emergent syllabic organization in sentence-level speech representation via HuBERT fine-tuning. Although it deals with speech modeling and proposes a benchmark for sentence-level speech representation, it does not explicitly mention Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it address synchronization, cognitive parallelism, or real-time interaction aspects related to full-duplex communication. It primarily investigates emergent speech unit discovery without directly discussing dialogue interaction, synchronous behavior arbitration, or speech-LLM architecture integration as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "整合兩位初審意見後：JuniorNano 認為可納入（評分5），理由是本文以語音表示與單位發現為核心，討論了模型微調（HuBERT self-distillation）、訓練與評估（提出 Spoken Speech ABX 基準），並在摘要明確提到「provides novel data-driven units for spoken language modeling」，因此涉及以語音為建模對象的語言建模相關議題；JuniorMini 則傾向排除（評分2），指出本文並未討論 Spoken Language Models/Full‑Duplex 互動、即時同步或對話仲裁等 FD‑SLM 關鍵問題，屬於表徵/單位發現的基礎研究。審查判斷：本研究確實以語音表徵與無監督單位發現為核心，討論了模型架構與訓練流程並提出評估基準，且在摘要中出現「spoken language modeling」的等價描述，符合「需在摘要出現 language modeling of speech 類描述」與「以語音為主要建模對象並討論架構/訓練/評估」等納入條件；但它不涵蓋 full‑duplex 同步互動、低延遲或行為仲裁等 FD‑SLM 的特定議題，屬於廣義 SLM 研究光譜中的表示/單位層面而非互動系統層面。 最終判定（單句說明）：因為本文在摘要中明確提到語音語言建模的相關性、對語音表徵/模型微調與評估作了具體技術貢獻，雖非 FD‑SLM 或即時互動研究，但仍屬於 Spoken Language Models 範疇的表示與單位發現工作，建議收納。",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "整合兩位初審意見後：JuniorNano 認為可納入（評分5），理由是本文以語音表示與單位發現為核心，討論了模型微調（HuBERT self-distillation）、訓練與評估（提出 Spoken Speech ABX 基準），並在摘要明確提到「provides novel data-driven units for spoken language modeling」，因此涉及以語音為建模對象的語言建模相關議題；JuniorMini 則傾向排除（評分2），指出本文並未討論 Spoken Language Models/Full‑Duplex 互動、即時同步或對話仲裁等 FD‑SLM 關鍵問題，屬於表徵/單位發現的基礎研究。審查判斷：本研究確實以語音表徵與無監督單位發現為核心，討論了模型架構與訓練流程並提出評估基準，且在摘要中出現「spoken language modeling」的等價描述，符合「需在摘要出現 language modeling of speech 類描述」與「以語音為主要建模對象並討論架構/訓練/評估」等納入條件；但它不涵蓋 full‑duplex 同步互動、低延遲或行為仲裁等 FD‑SLM 的特定議題，屬於廣義 SLM 研究光譜中的表示/單位層面而非互動系統層面。 最終判定（單句說明）：因為本文在摘要中明確提到語音語言建模的相關性、對語音表徵/模型微調與評估作了具體技術貢獻，雖非 FD‑SLM 或即時互動研究，但仍屬於 Spoken Language Models 範疇的表示與單位發現工作，建議收納。",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Discovering High-Quality Process Models Despite Data Scarcity",
    "abstract": "Process discovery algorithms learn process models from executed activity sequences, describing concurrency, causality, and conflict. Concurrent activities require observing multiple permutations, increasing data requirements, especially for processes with concurrent subprocesses such as hierarchical, composite, or distributed processes. While process discovery algorithms traditionally use sequences of activities as input, recently introduced object-centric process discovery algorithms can use graphs of activities as input, encoding partial orders between activities. As such, they contain the concurrency information of many sequences in a single graph. In this paper, we address the research question of reducing process discovery data requirements when using object-centric event logs for process discovery. We classify different real-life processes according to the control-flow complexity within and between subprocesses and introduce an evaluation framework to assess process discovery algorithm quality of traditional and object-centric process discovery based on the sample size. We complement this with a large-scale production process case study. Our results show reduced data requirements, enabling the discovery of large, concurrent processes such as manufacturing with little data, previously infeasible with traditional process discovery. Our findings suggest that object-centric process mining could revolutionize process discovery in various sectors, including manufacturing and supply chains.",
    "metadata": {
      "arxiv_id": "2310.11332",
      "title": "Discovering High-Quality Process Models Despite Data Scarcity",
      "summary": "Process discovery algorithms learn process models from executed activity sequences, describing concurrency, causality, and conflict. Concurrent activities require observing multiple permutations, increasing data requirements, especially for processes with concurrent subprocesses such as hierarchical, composite, or distributed processes. While process discovery algorithms traditionally use sequences of activities as input, recently introduced object-centric process discovery algorithms can use graphs of activities as input, encoding partial orders between activities. As such, they contain the concurrency information of many sequences in a single graph. In this paper, we address the research question of reducing process discovery data requirements when using object-centric event logs for process discovery. We classify different real-life processes according to the control-flow complexity within and between subprocesses and introduce an evaluation framework to assess process discovery algorithm quality of traditional and object-centric process discovery based on the sample size. We complement this with a large-scale production process case study. Our results show reduced data requirements, enabling the discovery of large, concurrent processes such as manufacturing with little data, previously infeasible with traditional process discovery. Our findings suggest that object-centric process mining could revolutionize process discovery in various sectors, including manufacturing and supply chains.",
      "authors": [
        "Jan Niklas Adams",
        "Jari Peeperkorn",
        "Tobias Brockhoff",
        "Isabelle Terrier",
        "Heiko Göhner",
        "Merih Seran Uysal",
        "Seppe vanden Broucke",
        "Jochen De Weerdt",
        "Wil M. P. van der Aalst"
      ],
      "published": "2023-10-17T15:12:05Z",
      "updated": "2023-10-17T15:12:05Z",
      "categories": [
        "cs.DB"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.11332v1",
      "landing_url": "https://arxiv.org/abs/2310.11332v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.11332"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on process discovery and object-centric process mining, with no focus on Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on process discovery and object-centric process mining, with no focus on Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on process discovery algorithms for process models, particularly using object-centric event logs to reduce data requirements, which is unrelated to Spoken Language Models, speech modeling, or any of the inclusion criteria centered on speech and full-duplex spoken language interaction; thus, it does not meet the inclusion criteria at all and falls outside the scope of speech or language modeling of speech.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on process discovery algorithms for process models, particularly using object-centric event logs to reduce data requirements, which is unrelated to Spoken Language Models, speech modeling, or any of the inclusion criteria centered on speech and full-duplex spoken language interaction; thus, it does not meet the inclusion criteria at all and falls outside the scope of speech or language modeling of speech.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Self-convolved Bootstrap for M-regression under Complex Temporal Dynamics",
    "abstract": "The paper considers simultaneous nonparametric inference for a wide class of M-regression models with time-varying coefficients. The covariates and errors of the regression model are tackled as a general class of nonstationary time series and are allowed to be cross-dependent. A novel and easy-to-implement self-convolved bootstrap procedure is proposed. With only one tuning parameter, the bootstrap facilitates a $\\sqrt{n}$-consistent inference of the cumulative regression function for the M-estimators under complex temporal dynamics, even under the possible presence of breakpoints in time series. Our methodology leads to a unified framework to conduct general classes of Exact Function Tests, Lack-of-fit Tests, and Qualitative Tests for the time-varying coefficients. These tests enable one to, among many others, conduct variable selection, check for constancy and linearity, as well as verify shape assumptions, including monotonicity and convexity. As applications, our method is utilized to study the time-varying properties of global climate data and Microsoft stock return, respectively.",
    "metadata": {
      "arxiv_id": "2310.11724",
      "title": "Self-convolved Bootstrap for M-regression under Complex Temporal Dynamics",
      "summary": "The paper considers simultaneous nonparametric inference for a wide class of M-regression models with time-varying coefficients. The covariates and errors of the regression model are tackled as a general class of nonstationary time series and are allowed to be cross-dependent. A novel and easy-to-implement self-convolved bootstrap procedure is proposed. With only one tuning parameter, the bootstrap facilitates a $\\sqrt{n}$-consistent inference of the cumulative regression function for the M-estimators under complex temporal dynamics, even under the possible presence of breakpoints in time series. Our methodology leads to a unified framework to conduct general classes of Exact Function Tests, Lack-of-fit Tests, and Qualitative Tests for the time-varying coefficients. These tests enable one to, among many others, conduct variable selection, check for constancy and linearity, as well as verify shape assumptions, including monotonicity and convexity. As applications, our method is utilized to study the time-varying properties of global climate data and Microsoft stock return, respectively.",
      "authors": [
        "Miaoshiqi Liu",
        "Zhou Zhou"
      ],
      "published": "2023-10-18T05:43:49Z",
      "updated": "2024-09-09T12:37:44Z",
      "categories": [
        "stat.ME",
        "math.ST"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.11724v3",
      "landing_url": "https://arxiv.org/abs/2310.11724v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.11724"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about nonparametric M-regression and bootstrap in nonstationary time series, with applications to climate data and stock returns; it does not address Spoken Language Models or speech modeling, thus fails inclusion and aligns with exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about nonparametric M-regression and bootstrap in nonstationary time series, with applications to climate data and stock returns; it does not address Spoken Language Models or speech modeling, thus fails inclusion and aligns with exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on statistical inference methods for M-regression models with time-varying coefficients in complex temporal data, with applications to climate and financial data, and make no mention of spoken language modeling, speech language models, real-time speech interaction, or any related topics as defined in the inclusion criteria. The study does not address end-to-end or hybrid speech-language model architectures, full-duplex speech interaction, nor does it discuss modeling or evaluating speech sequences as core modeling objects. Thus, it completely falls outside the specified thematic scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on statistical inference methods for M-regression models with time-varying coefficients in complex temporal data, with applications to climate and financial data, and make no mention of spoken language modeling, speech language models, real-time speech interaction, or any related topics as defined in the inclusion criteria. The study does not address end-to-end or hybrid speech-language model architectures, full-duplex speech interaction, nor does it discuss modeling or evaluating speech sequences as core modeling objects. Thus, it completely falls outside the specified thematic scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Acoustic BPE for Speech Generation with Discrete Tokens",
    "abstract": "Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.",
    "metadata": {
      "arxiv_id": "2310.14580",
      "title": "Acoustic BPE for Speech Generation with Discrete Tokens",
      "summary": "Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.",
      "authors": [
        "Feiyu Shen",
        "Yiwei Guo",
        "Chenpeng Du",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2023-10-23T05:38:41Z",
      "updated": "2024-01-15T05:53:31Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.14580v4",
      "landing_url": "https://arxiv.org/abs/2310.14580v4",
      "doi": "https://doi.org/10.48550/arXiv.2310.14580"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此論文核心聚焦於以語音為預期建模對象的語言模型（speech/s Spoken Language Model），使用 acoustic BPE 編碼降低序列長度並改善語音生成與模型推理，並探討模型訓練與評估，且摘要中清楚出現相關術語，符合近三年的研究範疇且非僅限於單一的 ASR/TTS 技術或文字NLP介接，故符合所有納入條件且無排除條件。",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "此論文核心聚焦於以語音為預期建模對象的語言模型（speech/s Spoken Language Model），使用 acoustic BPE 編碼降低序列長度並改善語音生成與模型推理，並探討模型訓練與評估，且摘要中清楚出現相關術語，符合近三年的研究範疇且非僅限於單一的 ASR/TTS 技術或文字NLP介接，故符合所有納入條件且無排除條件。",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech generation using discrete audio tokens and proposes a method (acoustic BPE) to improve sequence modeling for speech language models, which aligns with spoken language modeling at the acoustic level. However, it does not explicitly mention full-duplex spoken language models, interactive capabilities such as turn-taking or cognitive parallelism, nor does it address evaluation from the perspective of real-time or overlapping speech interactions; key inclusion criteria involving full-duplex interaction and multi-dimensional evaluations of synchronization behaviors are not clearly covered. The research primarily targets improving speech token modeling and generation efficiency, with no clear focus on dialogue behavior arbitration or synchronous interactive speech modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech generation using discrete audio tokens and proposes a method (acoustic BPE) to improve sequence modeling for speech language models, which aligns with spoken language modeling at the acoustic level. However, it does not explicitly mention full-duplex spoken language models, interactive capabilities such as turn-taking or cognitive parallelism, nor does it address evaluation from the perspective of real-time or overlapping speech interactions; key inclusion criteria involving full-duplex interaction and multi-dimensional evaluations of synchronization behaviors are not clearly covered. The research primarily targets improving speech token modeling and generation efficiency, with no clear focus on dialogue behavior arbitration or synchronous interactive speech modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "The paper clearly centers on speech-language modeling with discrete audio tokens (satisfying the SLM requirement) but exclusively targets single‑direction speech generation/TTS improvements (sequence compression and rescore) and does not address full‑duplex/synchronous interaction, overlap handling, or behavior arbitration—therefore it meets exclusion criteria; junior reviewers’ split (one included for SLM focus, one excluded for lack of FD/interaction) supports excluding this item.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "The paper clearly centers on speech-language modeling with discrete audio tokens (satisfying the SLM requirement) but exclusively targets single‑direction speech generation/TTS improvements (sequence compression and rescore) and does not address full‑duplex/synchronous interaction, overlap handling, or behavior arbitration—therefore it meets exclusion criteria; junior reviewers’ split (one included for SLM focus, one excluded for lack of FD/interaction) supports excluding this item.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "3M-TRANSFORMER: A Multi-Stage Multi-Stream Multimodal Transformer for Embodied Turn-Taking Prediction",
    "abstract": "Predicting turn-taking in multiparty conversations has many practical applications in human-computer/robot interaction. However, the complexity of human communication makes it a challenging task. Recent advances have shown that synchronous multi-perspective egocentric data can significantly improve turn-taking prediction compared to asynchronous, single-perspective transcriptions. Building on this research, we propose a new multimodal transformer-based architecture for predicting turn-taking in embodied, synchronized multi-perspective data. Our experimental results on the recently introduced EgoCom dataset show a substantial performance improvement of up to 14.01% on average compared to existing baselines and alternative transformer-based approaches. The source code, and the pre-trained models of our 3M-Transformer will be available upon acceptance.",
    "metadata": {
      "arxiv_id": "2310.14859",
      "title": "3M-TRANSFORMER: A Multi-Stage Multi-Stream Multimodal Transformer for Embodied Turn-Taking Prediction",
      "summary": "Predicting turn-taking in multiparty conversations has many practical applications in human-computer/robot interaction. However, the complexity of human communication makes it a challenging task. Recent advances have shown that synchronous multi-perspective egocentric data can significantly improve turn-taking prediction compared to asynchronous, single-perspective transcriptions. Building on this research, we propose a new multimodal transformer-based architecture for predicting turn-taking in embodied, synchronized multi-perspective data. Our experimental results on the recently introduced EgoCom dataset show a substantial performance improvement of up to 14.01% on average compared to existing baselines and alternative transformer-based approaches. The source code, and the pre-trained models of our 3M-Transformer will be available upon acceptance.",
      "authors": [
        "Mehdi Fatan",
        "Emanuele Mincato",
        "Dimitra Pintzou",
        "Mariella Dimiccoli"
      ],
      "published": "2023-10-23T12:29:10Z",
      "updated": "2023-12-21T18:19:58Z",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.14859v3",
      "landing_url": "https://arxiv.org/abs/2310.14859v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.14859"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on embodied turn-taking prediction using multimodal data rather than treating spoken language models or their equivalent as the core modeling object, so it does not meet the explicit inclusion criteria for SLM/FD-SLM-centric research despite addressing dialogue behavior.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on embodied turn-taking prediction using multimodal data rather than treating spoken language models or their equivalent as the core modeling object, so it does not meet the explicit inclusion criteria for SLM/FD-SLM-centric research despite addressing dialogue behavior.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on predicting turn-taking using a multimodal transformer on multi-perspective data, which relates to turn-taking prediction but does not explicitly mention spoken language models or language modeling of speech as its core; also, it does not clarify end-to-end speech modeling, synchronization, or low-latency synchronous interaction as required, nor does it explicitly include key concepts such as full-duplex spoken language models or speech-LLM architectures. Therefore, it partially aligns but lacks clear emphasis on the core spoken language modeling focus required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on predicting turn-taking using a multimodal transformer on multi-perspective data, which relates to turn-taking prediction but does not explicitly mention spoken language models or language modeling of speech as its core; also, it does not clarify end-to-end speech modeling, synchronization, or low-latency synchronous interaction as required, nor does it explicitly include key concepts such as full-duplex spoken language models or speech-LLM architectures. Therefore, it partially aligns but lacks clear emphasis on the core spoken language modeling focus required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Coded Computing for Half-Duplex Wireless Distributed Computing Systems via Interference Alignment",
    "abstract": "Distributed computing frameworks such as MapReduce and Spark are often used to process large-scale data computing jobs. In wireless scenarios, exchanging data among distributed nodes would seriously suffer from the communication bottleneck due to limited communication resources such as bandwidth and power. To address this problem, we propose a coded parallel computing (CPC) scheme for distributed computing systems where distributed nodes exchange information over a half-duplex wireless interference network. The CPC scheme achieves the multicast gain by utilizing coded computing to multicast coded symbols {intended to} multiple receiver nodes and the cooperative transmission gain by allowing multiple {transmitter} nodes to jointly deliver messages via interference alignment. To measure communication performance, we apply the widely used latency-oriented metric: \\emph{normalized delivery time (NDT)}. It is shown that CPC can significantly reduce the NDT by jointly exploiting the parallel transmission and coded multicasting opportunities. Surprisingly, when $K$ tends to infinity and the computation load is fixed, CPC approaches zero NDT while all state-of-the-art schemes achieve positive values of NDT. Finally, we establish an information-theoretic lower bound for the NDT-computation load trade-off over \\emph{half-duplex} network, and prove our scheme achieves the minimum NDT within a multiplicative gap of $3$, i.e., our scheme is order optimal.",
    "metadata": {
      "arxiv_id": "2310.15598",
      "title": "Coded Computing for Half-Duplex Wireless Distributed Computing Systems via Interference Alignment",
      "summary": "Distributed computing frameworks such as MapReduce and Spark are often used to process large-scale data computing jobs. In wireless scenarios, exchanging data among distributed nodes would seriously suffer from the communication bottleneck due to limited communication resources such as bandwidth and power. To address this problem, we propose a coded parallel computing (CPC) scheme for distributed computing systems where distributed nodes exchange information over a half-duplex wireless interference network. The CPC scheme achieves the multicast gain by utilizing coded computing to multicast coded symbols {intended to} multiple receiver nodes and the cooperative transmission gain by allowing multiple {transmitter} nodes to jointly deliver messages via interference alignment. To measure communication performance, we apply the widely used latency-oriented metric: \\emph{normalized delivery time (NDT)}. It is shown that CPC can significantly reduce the NDT by jointly exploiting the parallel transmission and coded multicasting opportunities. Surprisingly, when $K$ tends to infinity and the computation load is fixed, CPC approaches zero NDT while all state-of-the-art schemes achieve positive values of NDT. Finally, we establish an information-theoretic lower bound for the NDT-computation load trade-off over \\emph{half-duplex} network, and prove our scheme achieves the minimum NDT within a multiplicative gap of $3$, i.e., our scheme is order optimal.",
      "authors": [
        "Youlong Wu",
        "Zhenhao Huang",
        "Kai Yuan",
        "Shuai Ma",
        "Yue Bi"
      ],
      "published": "2023-10-24T08:00:24Z",
      "updated": "2023-10-24T08:00:24Z",
      "categories": [
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.15598v1",
      "landing_url": "https://arxiv.org/abs/2310.15598v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.15598"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item deals with coded computing for half-duplex wireless distributed computing and interference alignment, with no relation to Spoken Language Models or speech-language modeling; it fails the inclusion criteria and does not address speech interaction or evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item deals with coded computing for half-duplex wireless distributed computing and interference alignment, with no relation to Spoken Language Models or speech-language modeling; it fails the inclusion criteria and does not address speech interaction or evaluation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on coded computing in half-duplex wireless distributed computing systems and does not address any aspect of spoken language models, speech language modeling, or related areas specified in the inclusion criteria. It does not discuss modeling or reasoning centered on speech signals or full-duplex spoken language interaction, nor does it evaluate related tasks. Thus, it falls outside the thematic scope required for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on coded computing in half-duplex wireless distributed computing systems and does not address any aspect of spoken language models, speech language modeling, or related areas specified in the inclusion criteria. It does not discuss modeling or reasoning centered on speech signals or full-duplex spoken language interaction, nor does it evaluate related tasks. Thus, it falls outside the thematic scope required for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Confucius: Achieving Consistent Low Latency with Practical Queue Management for Real-Time Communications",
    "abstract": "Real-time communication applications require consistently low latency, which is often disrupted by latency spikes caused by competing flows, especially Web traffic. We identify the root cause of disruptions in such cases as the mismatch between the abrupt bandwidth allocation adjustment of queue scheduling and gradual congestion window adjustment of congestion control. For example, when a sudden burst of new Web flows arrives, queue schedulers abruptly shift bandwidth away from the existing real-time flow(s). The real-time flow will need several RTTs to converge to the new available bandwidth, during which severe stalls occur. In this paper, we present Confucius, a practical queue management scheme designed for offering real-time traffic with consistently low latency regardless of competing flows. Confucius slows down bandwidth adjustment to match the reaction of congestion control, such that the end host can reduce the sending rate without incurring latency spikes. Importantly, Confucius does not require the collaboration of end-hosts (e.g., labels on packets), nor manual parameter tuning to achieve good performance. Extensive experiments show that Confucius outperforms existing practical queueing schemes by reducing the stall duration by more than 50%, while the competing flows also fairly enjoy on-par performance.",
    "metadata": {
      "arxiv_id": "2310.18030",
      "title": "Confucius: Achieving Consistent Low Latency with Practical Queue Management for Real-Time Communications",
      "summary": "Real-time communication applications require consistently low latency, which is often disrupted by latency spikes caused by competing flows, especially Web traffic. We identify the root cause of disruptions in such cases as the mismatch between the abrupt bandwidth allocation adjustment of queue scheduling and gradual congestion window adjustment of congestion control. For example, when a sudden burst of new Web flows arrives, queue schedulers abruptly shift bandwidth away from the existing real-time flow(s). The real-time flow will need several RTTs to converge to the new available bandwidth, during which severe stalls occur. In this paper, we present Confucius, a practical queue management scheme designed for offering real-time traffic with consistently low latency regardless of competing flows. Confucius slows down bandwidth adjustment to match the reaction of congestion control, such that the end host can reduce the sending rate without incurring latency spikes. Importantly, Confucius does not require the collaboration of end-hosts (e.g., labels on packets), nor manual parameter tuning to achieve good performance. Extensive experiments show that Confucius outperforms existing practical queueing schemes by reducing the stall duration by more than 50%, while the competing flows also fairly enjoy on-par performance.",
      "authors": [
        "Zili Meng",
        "Nirav Atre",
        "Mingwei Xu",
        "Justine Sherry",
        "Maria Apostolaki"
      ],
      "published": "2023-10-27T10:09:22Z",
      "updated": "2024-02-07T07:26:37Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.18030v2",
      "landing_url": "https://arxiv.org/abs/2310.18030v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.18030"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article addresses practical queue management and low-latency real-time communications in networks, not SPoken Language Models or speech-language modeling; it does not meet the inclusion criteria and does meet none of the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article addresses practical queue management and low-latency real-time communications in networks, not SPoken Language Models or speech-language modeling; it does not meet the inclusion criteria and does meet none of the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on queue management to reduce latency in real-time communications but does not address Spoken Language Models, speech language modeling, or full-duplex speech interaction capabilities, nor does it discuss model architecture, training, or evaluation specific to speech-based language models as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on queue management to reduce latency in real-time communications but does not address Spoken Language Models, speech language modeling, or full-duplex speech interaction capabilities, nor does it discuss model architecture, training, or evaluation specific to speech-based language models as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Faster Peace via Inclusivity: An Efficient Paradigm to Understand Populations in Conflict Zones",
    "abstract": "United Nations practice shows that inclusivity is vital for mediation to be successful in helping end violent conflict and establish lasting peace. However, current methods for understanding the views and needs of populations during dynamic situations create tension between inclusivity and efficiency. This work introduces a novel paradigm to mitigate such tension. In partnership with collaborators at the United Nations we develop a realtime large-scale synchronous dialogue process (RLSDP) to understand stakeholder populations on an hour timescale. We demonstrate a machine learning model which enables each dialogue cycle to take place on a minute-timescale. We manage a key risk related to machine learning result trustworthiness by computing result confidence from a fast and reliable estimation of posterior variance. Lastly, we highlight a constellation of risks stemming from this new paradigm and suggest policies to mitigate them.",
    "metadata": {
      "arxiv_id": "2311.00816",
      "title": "Faster Peace via Inclusivity: An Efficient Paradigm to Understand Populations in Conflict Zones",
      "summary": "United Nations practice shows that inclusivity is vital for mediation to be successful in helping end violent conflict and establish lasting peace. However, current methods for understanding the views and needs of populations during dynamic situations create tension between inclusivity and efficiency. This work introduces a novel paradigm to mitigate such tension. In partnership with collaborators at the United Nations we develop a realtime large-scale synchronous dialogue process (RLSDP) to understand stakeholder populations on an hour timescale. We demonstrate a machine learning model which enables each dialogue cycle to take place on a minute-timescale. We manage a key risk related to machine learning result trustworthiness by computing result confidence from a fast and reliable estimation of posterior variance. Lastly, we highlight a constellation of risks stemming from this new paradigm and suggest policies to mitigate them.",
      "authors": [
        "Jordan Bilich",
        "Michael Varga",
        "Daanish Masood",
        "Andrew Konya"
      ],
      "published": "2023-11-01T20:00:12Z",
      "updated": "2023-11-01T20:00:12Z",
      "categories": [
        "cs.CY",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.00816v1",
      "landing_url": "https://arxiv.org/abs/2311.00816v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.00816"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a real-time dialogue paradigm for conflict-population understanding and ML confidence estimation, but it does not center on Spoken Language Models or speech-language modeling, nor discuss speech-specific architecture/evaluation, thus fails all inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a real-time dialogue paradigm for conflict-population understanding and ML confidence estimation, but it does not center on Spoken Language Models or speech-language modeling, nor discuss speech-specific architecture/evaluation, thus fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract discuss a realtime large-scale synchronous dialogue process and machine learning for understanding stakeholder populations in conflict zones but do not mention Spoken Language Models, speech language modeling, or any related architectures and evaluations focused on speech as the core modeling object; nor do they address full-duplex spoken language interaction or synchronized speech behaviors as per inclusion criteria. Therefore, it does not meet the inclusion criteria and appears unrelated to speech modeling or full-duplex interaction research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract discuss a realtime large-scale synchronous dialogue process and machine learning for understanding stakeholder populations in conflict zones but do not mention Spoken Language Models, speech language modeling, or any related architectures and evaluations focused on speech as the core modeling object; nor do they address full-duplex spoken language interaction or synchronized speech behaviors as per inclusion criteria. Therefore, it does not meet the inclusion criteria and appears unrelated to speech modeling or full-duplex interaction research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "FLAP: Fast Language-Audio Pre-training",
    "abstract": "We propose Fast Language-Audio Pre-training (FLAP), a self-supervised approach that efficiently and effectively learns aligned audio and language representations through masking, contrastive learning and reconstruction. For efficiency, FLAP randomly drops audio spectrogram tokens, focusing solely on the remaining ones for self-supervision. Through inter-modal contrastive learning, FLAP learns to align paired audio and text representations in a shared latent space. Notably, FLAP leverages multiple augmented views via masking for inter-modal contrast and learns to reconstruct the masked portion of audio tokens. Moreover, FLAP leverages large language models (LLMs) to augment the text inputs, contributing to improved performance. These approaches lead to more robust and informative audio-text representations, enabling FLAP to achieve state-of-the-art (SoTA) performance on audio-text retrieval tasks on AudioCaps (achieving 53.0% R@1) and Clotho (achieving 25.5% R@1).",
    "metadata": {
      "arxiv_id": "2311.01615",
      "title": "FLAP: Fast Language-Audio Pre-training",
      "summary": "We propose Fast Language-Audio Pre-training (FLAP), a self-supervised approach that efficiently and effectively learns aligned audio and language representations through masking, contrastive learning and reconstruction. For efficiency, FLAP randomly drops audio spectrogram tokens, focusing solely on the remaining ones for self-supervision. Through inter-modal contrastive learning, FLAP learns to align paired audio and text representations in a shared latent space. Notably, FLAP leverages multiple augmented views via masking for inter-modal contrast and learns to reconstruct the masked portion of audio tokens. Moreover, FLAP leverages large language models (LLMs) to augment the text inputs, contributing to improved performance. These approaches lead to more robust and informative audio-text representations, enabling FLAP to achieve state-of-the-art (SoTA) performance on audio-text retrieval tasks on AudioCaps (achieving 53.0% R@1) and Clotho (achieving 25.5% R@1).",
      "authors": [
        "Ching-Feng Yeh",
        "Po-Yao Huang",
        "Vasu Sharma",
        "Shang-Wen Li",
        "Gargi Gosh"
      ],
      "published": "2023-11-02T21:58:50Z",
      "updated": "2023-11-02T21:58:50Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.01615v1",
      "landing_url": "https://arxiv.org/abs/2311.01615v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.01615"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item targets Spoken Language Models / speech-language modeling by learning aligned audio and language representations (audio-text pre-training) with architecture/training/evaluation details, published within the last 3 years, and discusses cross-modal LM design rather than only ASR/TTS, meeting all inclusion and none of the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item targets Spoken Language Models / speech-language modeling by learning aligned audio and language representations (audio-text pre-training) with architecture/training/evaluation details, published within the last 3 years, and discusses cross-modal LM design rather than only ASR/TTS, meeting all inclusion and none of the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study FLAP focuses on self-supervised learning of aligned audio and text representations primarily for audio-text retrieval tasks, without explicitly addressing spoken language models (SLMs) or full-duplex spoken language interaction, nor discussing synchronization, cognitive parallelism, or real-time interactive behaviors emphasized in the inclusion criteria. The description lacks mention of speech-level language modeling, full-duplex interaction capabilities, or related evaluations such as low latency or turn-taking management, indicating it does not meet the inclusion criteria centered on spoken language modeling as a core aspect.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study FLAP focuses on self-supervised learning of aligned audio and text representations primarily for audio-text retrieval tasks, without explicitly addressing spoken language models (SLMs) or full-duplex spoken language interaction, nor discussing synchronization, cognitive parallelism, or real-time interactive behaviors emphasized in the inclusion criteria. The description lacks mention of speech-level language modeling, full-duplex interaction capabilities, or related evaluations such as low latency or turn-taking management, indicating it does not meet the inclusion criteria centered on spoken language modeling as a core aspect.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": {
      "reasoning": "JuniorNano argued inclusion (rating 5) claiming FLAP is audio-text pretraining for speech-language modeling, while JuniorMini argued exclusion (rating 1) noting the paper targets audio-text retrieval rather than spoken language modeling; after review I agree with JuniorMini—the abstract describes self-supervised audio-text representation learning for retrieval (contrastive/masking/reconstruction, LLM-augmented text) and does not claim or evaluate Spoken Language Models, speech-level language modeling, full‑duplex interaction, latency/overlap handling, or any of the required SLM/FD‑SLM architectural or evaluation components. Therefore it fails the inclusion criteria and meets the exclusion criteria (only using speech as input for cross‑modal retrieval rather than centering speech as the modeled language object). 1-sentence explanation: Exclude (1) — FLAP is an audio–text representation/retrieval method, not a study of Spoken Language Models or full‑duplex speech language modeling required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "JuniorNano argued inclusion (rating 5) claiming FLAP is audio-text pretraining for speech-language modeling, while JuniorMini argued exclusion (rating 1) noting the paper targets audio-text retrieval rather than spoken language modeling; after review I agree with JuniorMini—the abstract describes self-supervised audio-text representation learning for retrieval (contrastive/masking/reconstruction, LLM-augmented text) and does not claim or evaluate Spoken Language Models, speech-level language modeling, full‑duplex interaction, latency/overlap handling, or any of the required SLM/FD‑SLM architectural or evaluation components. Therefore it fails the inclusion criteria and meets the exclusion criteria (only using speech as input for cross‑modal retrieval rather than centering speech as the modeled language object). 1-sentence explanation: Exclude (1) — FLAP is an audio–text representation/retrieval method, not a study of Spoken Language Models or full‑duplex speech language modeling required by the inclusion criteria.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Detecting agreement in multi-party dialogue: evaluating speaker diarisation versus a procedural baseline to enhance user engagement",
    "abstract": "Conversational agents participating in multi-party interactions face significant challenges in dialogue state tracking, since the identity of the speaker adds significant contextual meaning. It is common to utilise diarisation models to identify the speaker. However, it is not clear if these are accurate enough to correctly identify specific conversational events such as agreement or disagreement during a real-time interaction. This study uses a cooperative quiz, where the conversational agent acts as quiz-show host, to determine whether diarisation or a frequency-and-proximity-based method is more accurate at determining agreement, and whether this translates to feelings of engagement from the players. Experimental results show that our procedural system was more engaging to players, and was more accurate at detecting agreement, reaching an average accuracy of 0.44 compared to 0.28 for the diarised system.",
    "metadata": {
      "arxiv_id": "2311.03021",
      "title": "Detecting agreement in multi-party dialogue: evaluating speaker diarisation versus a procedural baseline to enhance user engagement",
      "summary": "Conversational agents participating in multi-party interactions face significant challenges in dialogue state tracking, since the identity of the speaker adds significant contextual meaning. It is common to utilise diarisation models to identify the speaker. However, it is not clear if these are accurate enough to correctly identify specific conversational events such as agreement or disagreement during a real-time interaction. This study uses a cooperative quiz, where the conversational agent acts as quiz-show host, to determine whether diarisation or a frequency-and-proximity-based method is more accurate at determining agreement, and whether this translates to feelings of engagement from the players. Experimental results show that our procedural system was more engaging to players, and was more accurate at detecting agreement, reaching an average accuracy of 0.44 compared to 0.28 for the diarised system.",
      "authors": [
        "Angus Addlesee",
        "Daniel Denley",
        "Andy Edmondson",
        "Nancie Gunson",
        "Daniel Hernández Garcia",
        "Alexandre Kha",
        "Oliver Lemon",
        "James Ndubuisi",
        "Neil O'Reilly",
        "Lia Perochaud",
        "Raphaël Valeri",
        "Miebaka Worika"
      ],
      "published": "2023-11-06T11:00:44Z",
      "updated": "2023-11-06T11:00:44Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.03021v1",
      "landing_url": "https://arxiv.org/abs/2311.03021v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.03021"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on speaker diarisation and detecting agreement in multi-party dialogue for engagement, but does not present Spoken Language Model or full-duplex FD-SLM architecture/training/evaluation as core; it seems more like a dialogue systems HCI study rather than SLM modeling.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item centers on speaker diarisation and detecting agreement in multi-party dialogue for engagement, but does not present Spoken Language Model or full-duplex FD-SLM architecture/training/evaluation as core; it seems more like a dialogue systems HCI study rather than SLM modeling.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on detecting agreement in multi-party dialogues using speaker diarisation and procedural methods, aiming to enhance user engagement; however, it lacks emphasis on spoken language modeling as a core concept with speech as the primary modeling object and does not discuss relevant model architectures, training, or evaluation of SLMs or FD-SLMs. It also does not address full-duplex interaction, synchronization, or cognitive parallelism directly, which are key inclusion criteria, and does not mention spoken language models or equivalent terms in title or abstract. Therefore, it does not meet the inclusion criteria and aligns more with traditional dialogue event detection rather than the spoken language modeling theme required.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on detecting agreement in multi-party dialogues using speaker diarisation and procedural methods, aiming to enhance user engagement; however, it lacks emphasis on spoken language modeling as a core concept with speech as the primary modeling object and does not discuss relevant model architectures, training, or evaluation of SLMs or FD-SLMs. It also does not address full-duplex interaction, synchronization, or cognitive parallelism directly, which are key inclusion criteria, and does not mention spoken language models or equivalent terms in title or abstract. Therefore, it does not meet the inclusion criteria and aligns more with traditional dialogue event detection rather than the spoken language modeling theme required.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Detecting Agreement in Multi-party Conversational AI",
    "abstract": "Today, conversational systems are expected to handle conversations in multi-party settings, especially within Socially Assistive Robots (SARs). However, practical usability remains difficult as there are additional challenges to overcome, such as speaker recognition, addressee recognition, and complex turn-taking. In this paper, we present our work on a multi-party conversational system, which invites two users to play a trivia quiz game. The system detects users' agreement or disagreement on a final answer and responds accordingly. Our evaluation includes both performance and user assessment results, with a focus on detecting user agreement. Our annotated transcripts and the code for the proposed system have been released open-source on GitHub.",
    "metadata": {
      "arxiv_id": "2311.03026",
      "title": "Detecting Agreement in Multi-party Conversational AI",
      "summary": "Today, conversational systems are expected to handle conversations in multi-party settings, especially within Socially Assistive Robots (SARs). However, practical usability remains difficult as there are additional challenges to overcome, such as speaker recognition, addressee recognition, and complex turn-taking. In this paper, we present our work on a multi-party conversational system, which invites two users to play a trivia quiz game. The system detects users' agreement or disagreement on a final answer and responds accordingly. Our evaluation includes both performance and user assessment results, with a focus on detecting user agreement. Our annotated transcripts and the code for the proposed system have been released open-source on GitHub.",
      "authors": [
        "Laura Schauer",
        "Jason Sweeney",
        "Charlie Lyttle",
        "Zein Said",
        "Aron Szeles",
        "Cale Clark",
        "Katie McAskill",
        "Xander Wickham",
        "Tom Byars",
        "Daniel Hernández Garcia",
        "Nancie Gunson",
        "Angus Addlesee",
        "Oliver Lemon"
      ],
      "published": "2023-11-06T11:04:39Z",
      "updated": "2023-11-06T11:04:39Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.03026v1",
      "landing_url": "https://arxiv.org/abs/2311.03026v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.03026"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此摘要未以 Spoken Language Models（或等價概念）為核心，亦未明確討論語音層級建模、語音–文字混合架構、或端到端與模組化同步的評估框架，僅描述多方對話中的同意偵測，符合排除條件，因此應該排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此摘要未以 Spoken Language Models（或等價概念）為核心，亦未明確討論語音層級建模、語音–文字混合架構、或端到端與模組化同步的評估框架，僅描述多方對話中的同意偵測，符合排除條件，因此應該排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study describes a multi-party conversational system that detects user agreement in a trivia quiz game, but it does not explicitly mention Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) as core components, nor does it discuss model architectures, training, or evaluation related specifically to speech language modeling. The focus appears to be on detecting agreement in conversation rather than on speech language modeling or full-duplex synchronous interaction mechanisms, and the abstract lacks keywords or concepts directly related to SLMs or FD-SLMs. Therefore, it does not meet the inclusion criteria focusing on speech language models as core research objects and is likely better excluded.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study describes a multi-party conversational system that detects user agreement in a trivia quiz game, but it does not explicitly mention Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) as core components, nor does it discuss model architectures, training, or evaluation related specifically to speech language modeling. The focus appears to be on detecting agreement in conversation rather than on speech language modeling or full-duplex synchronous interaction mechanisms, and the abstract lacks keywords or concepts directly related to SLMs or FD-SLMs. Therefore, it does not meet the inclusion criteria focusing on speech language models as core research objects and is likely better excluded.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Optimizing Distributed Networking with Big Data Scheduling and Cloud Computing",
    "abstract": "With the rapid transformation of computer hardware and algorithms, mobile networking has evolved from low data carrying capacity and high latency to better-optimized networks, either by enhancing the digital network or using different approaches to reduce network traffic. This paper discusses the big data applications and scheduling in the distributed networking and analyzes the opportunities and challenges of data management systems. The analysis shows that the big data scheduling in the cloud computing environment produces the most efficient way to transfer and synchronize data. Since scheduling problems and cloud models are very complex to analyze in different settings, we set it to the typical software defined networks. The development of cloud management models and coflow scheduling algorithm is proved to be the priority of the digital communications and networks development in the future.",
    "metadata": {
      "arxiv_id": "2311.04482",
      "title": "Optimizing Distributed Networking with Big Data Scheduling and Cloud Computing",
      "summary": "With the rapid transformation of computer hardware and algorithms, mobile networking has evolved from low data carrying capacity and high latency to better-optimized networks, either by enhancing the digital network or using different approaches to reduce network traffic. This paper discusses the big data applications and scheduling in the distributed networking and analyzes the opportunities and challenges of data management systems. The analysis shows that the big data scheduling in the cloud computing environment produces the most efficient way to transfer and synchronize data. Since scheduling problems and cloud models are very complex to analyze in different settings, we set it to the typical software defined networks. The development of cloud management models and coflow scheduling algorithm is proved to be the priority of the digital communications and networks development in the future.",
      "authors": [
        "Wenbo Zhu"
      ],
      "published": "2023-11-08T06:22:44Z",
      "updated": "2023-11-08T06:22:44Z",
      "categories": [
        "cs.NI",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.04482v1",
      "landing_url": "https://arxiv.org/abs/2311.04482v1",
      "doi": "https://doi.org/10.1117/12.2642577"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on big data scheduling in distributed networking and cloud computing, with no mention of Spoken Language Models or speech-language modeling, no discussion of end-to-end or full-duplex speech interaction, and no evaluation frameworks for speech benchmarks, thus it does not meet the inclusion criteria and should be excluded per the stated criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on big data scheduling in distributed networking and cloud computing, with no mention of Spoken Language Models or speech-language modeling, no discussion of end-to-end or full-duplex speech interaction, and no evaluation frameworks for speech benchmarks, thus it does not meet the inclusion criteria and should be excluded per the stated criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on big data scheduling and cloud computing in distributed networking, without any mention of spoken language models, speech-level language modeling, or related full-duplex spoken language interaction concepts; thus, it does not meet the core topic inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on big data scheduling and cloud computing in distributed networking, without any mention of spoken language models, speech-level language modeling, or related full-duplex spoken language interaction concepts; thus, it does not meet the core topic inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Incorporating temporal dynamics of mutations to enhance the prediction capability of antiretroviral therapy's outcome for HIV-1",
    "abstract": "Motivation: In predicting HIV therapy outcomes, a critical clinical question is whether using historical information can enhance predictive capabilities compared with current or latest available data analysis. This study analyses whether historical knowledge, which includes viral mutations detected in all genotypic tests before therapy, their temporal occurrence, and concomitant viral load measurements, can bring improvements. We introduce a method to weigh mutations, considering the previously enumerated factors and the reference mutation-drug Stanford resistance tables. We compare a model encompassing history (H) with one not using it (NH). Results: The H-model demonstrates superior discriminative ability, with a higher ROC-AUC score (76.34%) than the NH-model (74.98%). Significant Wilcoxon test results confirm that incorporating historical information improves consistently predictive accuracy for treatment outcomes. The better performance of the H-model might be attributed to its consideration of latent HIV reservoirs, probably obtained when leveraging historical information. The findings emphasize the importance of temporal dynamics in mutations, offering insights into HIV infection complexities. However, our result also shows that prediction accuracy remains relatively high even when no historical information is available. Supplementary information: Supplementary material is available.",
    "metadata": {
      "arxiv_id": "2311.04846",
      "title": "Incorporating temporal dynamics of mutations to enhance the prediction capability of antiretroviral therapy's outcome for HIV-1",
      "summary": "Motivation: In predicting HIV therapy outcomes, a critical clinical question is whether using historical information can enhance predictive capabilities compared with current or latest available data analysis. This study analyses whether historical knowledge, which includes viral mutations detected in all genotypic tests before therapy, their temporal occurrence, and concomitant viral load measurements, can bring improvements. We introduce a method to weigh mutations, considering the previously enumerated factors and the reference mutation-drug Stanford resistance tables. We compare a model encompassing history (H) with one not using it (NH). Results: The H-model demonstrates superior discriminative ability, with a higher ROC-AUC score (76.34%) than the NH-model (74.98%). Significant Wilcoxon test results confirm that incorporating historical information improves consistently predictive accuracy for treatment outcomes. The better performance of the H-model might be attributed to its consideration of latent HIV reservoirs, probably obtained when leveraging historical information. The findings emphasize the importance of temporal dynamics in mutations, offering insights into HIV infection complexities. However, our result also shows that prediction accuracy remains relatively high even when no historical information is available. Supplementary information: Supplementary material is available.",
      "authors": [
        "Giulia Di Teodoro",
        "Martin Pirkl",
        "Francesca Incardona",
        "Ilaria Vicenti",
        "Anders Sönnerborg",
        "Rolf Kaiser",
        "Laura Palagi",
        "Maurizio Zazzi",
        "Thomas Lengauer"
      ],
      "published": "2023-11-08T17:29:41Z",
      "updated": "2024-06-24T15:07:56Z",
      "categories": [
        "cs.LG",
        "q-bio.QM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.04846v2",
      "landing_url": "https://arxiv.org/abs/2311.04846v2",
      "doi": "https://doi.org/10.1093/bioinformatics/btae327"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item concerns HIV mutation dynamics and treatment outcome prediction, which has no relevance to Spoken Language Models or speech-based modeling, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item concerns HIV mutation dynamics and treatment outcome prediction, which has no relevance to Spoken Language Models or speech-based modeling, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on predicting HIV therapy outcomes using historical mutation data, which is unrelated to Spoken Language Models (SLMs) or full-duplex spoken interaction modeling emphasized in the inclusion criteria. It neither discusses speech modeling, synchronization, nor any speech-centric evaluation metrics, and thus falls outside the specified research scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on predicting HIV therapy outcomes using historical mutation data, which is unrelated to Spoken Language Models (SLMs) or full-duplex spoken interaction modeling emphasized in the inclusion criteria. It neither discusses speech modeling, synchronization, nor any speech-centric evaluation metrics, and thus falls outside the specified research scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Generative De-Quantization for Neural Speech Codec via Latent Diffusion",
    "abstract": "In low-bitrate speech coding, end-to-end speech coding networks aim to learn compact yet expressive features and a powerful decoder in a single network. A challenging problem as such results in unwelcome complexity increase and inferior speech quality. In this paper, we propose to separate the representation learning and information reconstruction tasks. We leverage an end-to-end codec for learning low-dimensional discrete tokens and employ a latent diffusion model to de-quantize coded features into a high-dimensional continuous space, relieving the decoder's burden of de-quantizing and upsampling. To mitigate the issue of over-smooth generation, we introduce midway-infilling with less noise reduction and stronger conditioning. In ablation studies, we investigate the hyperparameters for midway-infilling and latent diffusion space with different dimensions. Subjective listening tests show that our model outperforms the state-of-the-art at two low bitrates, 1.5 and 3 kbps. Codes and samples of this work are available on our webpage.",
    "metadata": {
      "arxiv_id": "2311.08330",
      "title": "Generative De-Quantization for Neural Speech Codec via Latent Diffusion",
      "summary": "In low-bitrate speech coding, end-to-end speech coding networks aim to learn compact yet expressive features and a powerful decoder in a single network. A challenging problem as such results in unwelcome complexity increase and inferior speech quality. In this paper, we propose to separate the representation learning and information reconstruction tasks. We leverage an end-to-end codec for learning low-dimensional discrete tokens and employ a latent diffusion model to de-quantize coded features into a high-dimensional continuous space, relieving the decoder's burden of de-quantizing and upsampling. To mitigate the issue of over-smooth generation, we introduce midway-infilling with less noise reduction and stronger conditioning. In ablation studies, we investigate the hyperparameters for midway-infilling and latent diffusion space with different dimensions. Subjective listening tests show that our model outperforms the state-of-the-art at two low bitrates, 1.5 and 3 kbps. Codes and samples of this work are available on our webpage.",
      "authors": [
        "Haici Yang",
        "Inseon Jang",
        "Minje Kim"
      ],
      "published": "2023-11-14T17:19:40Z",
      "updated": "2023-11-15T15:23:03Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.08330v2",
      "landing_url": "https://arxiv.org/abs/2311.08330v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.08330"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此研究專注於端到端語音編碼與使用潛在擴散模型進行去量化，非以語音語言模型（SLMs）或與語音語言建模相關的對話/互動框架為核心，且未提供可評估的即時互動或語音語言建模指標，因此不符合收錄條件，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此研究專注於端到端語音編碼與使用潛在擴散模型進行去量化，非以語音語言模型（SLMs）或與語音語言建模相關的對話/互動框架為核心，且未提供可評估的即時互動或語音語言建模指標，因此不符合收錄條件，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on low-bitrate speech coding using neural networks and latent diffusion for de-quantization, targeting speech signal compression and quality improvement rather than spoken language modeling, full-duplex interaction, or synchronization in speech-language models; it neither mentions spoken language models nor full-duplex capabilities or interactive dialogue behavior arbitration, and the evaluation centers on speech quality not synchronization or interaction metrics, thus failing to meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on low-bitrate speech coding using neural networks and latent diffusion for de-quantization, targeting speech signal compression and quality improvement rather than spoken language modeling, full-duplex interaction, or synchronization in speech-language models; it neither mentions spoken language models nor full-duplex capabilities or interactive dialogue behavior arbitration, and the evaluation centers on speech quality not synchronization or interaction metrics, thus failing to meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "NGEL-SLAM: Neural Implicit Representation-based Global Consistent Low-Latency SLAM System",
    "abstract": "Neural implicit representations have emerged as a promising solution for providing dense geometry in Simultaneous Localization and Mapping (SLAM). However, existing methods in this direction fall short in terms of global consistency and low latency. This paper presents NGEL-SLAM to tackle the above challenges. To ensure global consistency, our system leverages a traditional feature-based tracking module that incorporates loop closure. Additionally, we maintain a global consistent map by representing the scene using multiple neural implicit fields, enabling quick adjustment to the loop closure. Moreover, our system allows for fast convergence through the use of octree-based implicit representations. The combination of rapid response to loop closure and fast convergence makes our system a truly low-latency system that achieves global consistency. Our system enables rendering high-fidelity RGB-D images, along with extracting dense and complete surfaces. Experiments on both synthetic and real-world datasets suggest that our system achieves state-of-the-art tracking and mapping accuracy while maintaining low latency.",
    "metadata": {
      "arxiv_id": "2311.09525",
      "title": "NGEL-SLAM: Neural Implicit Representation-based Global Consistent Low-Latency SLAM System",
      "summary": "Neural implicit representations have emerged as a promising solution for providing dense geometry in Simultaneous Localization and Mapping (SLAM). However, existing methods in this direction fall short in terms of global consistency and low latency. This paper presents NGEL-SLAM to tackle the above challenges. To ensure global consistency, our system leverages a traditional feature-based tracking module that incorporates loop closure. Additionally, we maintain a global consistent map by representing the scene using multiple neural implicit fields, enabling quick adjustment to the loop closure. Moreover, our system allows for fast convergence through the use of octree-based implicit representations. The combination of rapid response to loop closure and fast convergence makes our system a truly low-latency system that achieves global consistency. Our system enables rendering high-fidelity RGB-D images, along with extracting dense and complete surfaces. Experiments on both synthetic and real-world datasets suggest that our system achieves state-of-the-art tracking and mapping accuracy while maintaining low latency.",
      "authors": [
        "Yunxuan Mao",
        "Xuan Yu",
        "Kai Wang",
        "Yue Wang",
        "Rong Xiong",
        "Yiyi Liao"
      ],
      "published": "2023-11-16T03:05:58Z",
      "updated": "2024-08-21T04:19:20Z",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.09525v2",
      "landing_url": "https://arxiv.org/abs/2311.09525v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.09525"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on SLAM with neural implicit representations and loop closure, not on Spoken Language Models or speech-language modeling; it lacks the core subject and evaluation aspects required by the inclusion criteria, thus should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on SLAM with neural implicit representations and loop closure, not on Spoken Language Models or speech-language modeling; it lacks the core subject and evaluation aspects required by the inclusion criteria, thus should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a SLAM system using neural implicit representations for low-latency, globally consistent mapping and tracking, which is fundamentally related to robotic vision and mapping rather than spoken language models or speech-based language modeling. It does not address speech as a core modeling or inference object, nor does it relate to full-duplex spoken language interaction, speech-language modeling architectures, or end-to-end speech-language model training or evaluation. Therefore, it fails to meet the inclusion criteria and seems completely unrelated to the research scope of spoken language models and their interactive aspects.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a SLAM system using neural implicit representations for low-latency, globally consistent mapping and tracking, which is fundamentally related to robotic vision and mapping rather than spoken language models or speech-based language modeling. It does not address speech as a core modeling or inference object, nor does it relate to full-duplex spoken language interaction, speech-language modeling architectures, or end-to-end speech-language model training or evaluation. Therefore, it fails to meet the inclusion criteria and seems completely unrelated to the research scope of spoken language models and their interactive aspects.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Overcoming Data Scarcity in Biomedical Imaging with a Foundational Multi-Task Model",
    "abstract": "Foundational models, pretrained on a large scale, have demonstrated substantial success across non-medical domains. However, training these models typically requires large, comprehensive datasets, which contrasts with the smaller and more heterogeneous datasets common in biomedical imaging. Here, we propose a multi-task learning strategy that decouples the number of training tasks from memory requirements. We trained a Universal bioMedical PreTrained model (UMedPT) on a multi-task database including tomographic, microscopic, and X-ray images, with various labelling strategies such as classification, segmentation, and object detection. The UMedPT foundational model outperformed ImageNet pretraining and the previous state-of-the-art models. For tasks related to the pretraining database, it maintained its performance with only 1% of the original training data and without fine-tuning. For out-of-domain tasks it required not more than 50% of the original training data. In an external independent validation imaging features extracted using UMedPT proved to be a new standard for cross-center transferability.",
    "metadata": {
      "arxiv_id": "2311.09847",
      "title": "Overcoming Data Scarcity in Biomedical Imaging with a Foundational Multi-Task Model",
      "summary": "Foundational models, pretrained on a large scale, have demonstrated substantial success across non-medical domains. However, training these models typically requires large, comprehensive datasets, which contrasts with the smaller and more heterogeneous datasets common in biomedical imaging. Here, we propose a multi-task learning strategy that decouples the number of training tasks from memory requirements. We trained a Universal bioMedical PreTrained model (UMedPT) on a multi-task database including tomographic, microscopic, and X-ray images, with various labelling strategies such as classification, segmentation, and object detection. The UMedPT foundational model outperformed ImageNet pretraining and the previous state-of-the-art models. For tasks related to the pretraining database, it maintained its performance with only 1% of the original training data and without fine-tuning. For out-of-domain tasks it required not more than 50% of the original training data. In an external independent validation imaging features extracted using UMedPT proved to be a new standard for cross-center transferability.",
      "authors": [
        "Raphael Schäfer",
        "Till Nicke",
        "Henning Höfener",
        "Annkristin Lange",
        "Dorit Merhof",
        "Friedrich Feuerhake",
        "Volkmar Schulz",
        "Johannes Lotz",
        "Fabian Kiessling"
      ],
      "published": "2023-11-16T12:20:25Z",
      "updated": "2023-11-16T12:20:25Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.09847v1",
      "landing_url": "https://arxiv.org/abs/2311.09847v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.09847"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on foundational models for biomedical imaging with multi-task pretraining, and has no emphasis on Spoken Language Models or any speech-language modeling concepts, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on foundational models for biomedical imaging with multi-task pretraining, and has no emphasis on Spoken Language Models or any speech-language modeling concepts, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on foundational models in biomedical imaging and does not address Spoken Language Models or full-duplex spoken language interaction, nor does it mention speech-level language modeling, synchronous interaction, or related evaluation metrics aligned with the inclusion criteria focused on spoken language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on foundational models in biomedical imaging and does not address Spoken Language Models or full-duplex spoken language interaction, nor does it mention speech-level language modeling, synchronous interaction, or related evaluation metrics aligned with the inclusion criteria focused on spoken language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Enhancing Object Coherence in Layout-to-Image Synthesis",
    "abstract": "Layout-to-image synthesis is an emerging technique in conditional image generation. It aims to generate complex scenes, where users require fine control over the layout of the objects in a scene. However, it remains challenging to control the object coherence, including semantic coherence (e.g., the cat looks at the flowers or not) and physical coherence (e.g., the hand and the racket should not be misaligned). In this paper, we propose a novel diffusion model with effective global semantic fusion (GSF) and self-similarity feature enhancement modules to guide the object coherence for this task. For semantic coherence, we argue that the image caption contains rich information for defining the semantic relationship within the objects in the images. Instead of simply employing cross-attention between captions and latent images, which addresses the highly relevant layout restriction and semantic coherence requirement separately and thus leads to unsatisfying results shown in our experiments, we develop GSF to fuse the supervision from the layout restriction and semantic coherence requirement and exploit it to guide the image synthesis process. Moreover, to improve the physical coherence, we develop a Self-similarity Coherence Attention (SCA) module to explicitly integrate local contextual physical coherence relation into each pixel's generation process. Specifically, we adopt a self-similarity map to encode the physical coherence restrictions and employ it to extract coherent features from text embedding. Through visualization of our self-similarity map, we explore the essence of SCA, revealing that its effectiveness is not only in capturing reliable physical coherence patterns but also in enhancing complex texture generation. Extensive experiments demonstrate the superiority of our proposed method.",
    "metadata": {
      "arxiv_id": "2311.10522",
      "title": "Enhancing Object Coherence in Layout-to-Image Synthesis",
      "summary": "Layout-to-image synthesis is an emerging technique in conditional image generation. It aims to generate complex scenes, where users require fine control over the layout of the objects in a scene. However, it remains challenging to control the object coherence, including semantic coherence (e.g., the cat looks at the flowers or not) and physical coherence (e.g., the hand and the racket should not be misaligned). In this paper, we propose a novel diffusion model with effective global semantic fusion (GSF) and self-similarity feature enhancement modules to guide the object coherence for this task. For semantic coherence, we argue that the image caption contains rich information for defining the semantic relationship within the objects in the images. Instead of simply employing cross-attention between captions and latent images, which addresses the highly relevant layout restriction and semantic coherence requirement separately and thus leads to unsatisfying results shown in our experiments, we develop GSF to fuse the supervision from the layout restriction and semantic coherence requirement and exploit it to guide the image synthesis process. Moreover, to improve the physical coherence, we develop a Self-similarity Coherence Attention (SCA) module to explicitly integrate local contextual physical coherence relation into each pixel's generation process. Specifically, we adopt a self-similarity map to encode the physical coherence restrictions and employ it to extract coherent features from text embedding. Through visualization of our self-similarity map, we explore the essence of SCA, revealing that its effectiveness is not only in capturing reliable physical coherence patterns but also in enhancing complex texture generation. Extensive experiments demonstrate the superiority of our proposed method.",
      "authors": [
        "Yibin Wang",
        "Changhai Zhou",
        "Honghui Xu"
      ],
      "published": "2023-11-17T13:43:43Z",
      "updated": "2025-03-31T03:32:16Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.10522v7",
      "landing_url": "https://arxiv.org/abs/2311.10522v7",
      "doi": "https://doi.org/10.48550/arXiv.2311.10522"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on improving object coherence in layout-to-image synthesis using diffusion models and does not address Spoken Language Models, speech-language modeling, or real-time interactive speech dialogue; thus it does not meet inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on improving object coherence in layout-to-image synthesis using diffusion models and does not address Spoken Language Models, speech-language modeling, or real-time interactive speech dialogue; thus it does not meet inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on developing a novel diffusion model for layout-to-image synthesis to enhance object coherence, which is unrelated to spoken language models or speech-based language modeling. The research does not address speech sequence modeling, full-duplex spoken language models, or any real-time speech interaction capabilities required by the inclusion criteria, nor does it discuss any aspect of speech or audio modeling, synchronization, or evaluation relevant to spoken language models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on developing a novel diffusion model for layout-to-image synthesis to enhance object coherence, which is unrelated to spoken language models or speech-based language modeling. The research does not address speech sequence modeling, full-duplex spoken language models, or any real-time speech interaction capabilities required by the inclusion criteria, nor does it discuss any aspect of speech or audio modeling, synchronization, or evaluation relevant to spoken language models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Web-Based Dynamic Paintings: Real-Time Interactive Artworks in Web Using a 2.5D Pipeline",
    "abstract": "In this work, we present a 2.5D pipeline approach to creating dynamic paintings that can be re-rendered interactively in real-time on the Web. Using this 2.5D approach, any existing simple painting such as portraits can be turned into an interactive dynamic web-based artwork. Our interactive system provides most global illumination effects such as reflection, refraction, shadow, and subsurface scattering by processing images. In our system, the scene is defined only by a set of images. These include (1) a shape image, (2) two diffuse images, (3) a background image, (4) one foreground image, and (5) one transparency image. A shape image is either a normal map or a height. Two diffuse images are usually hand-painted. They are interpolated using illumination information. The transparency image is used to define the transparent and reflective regions that can reflect the foreground image and refract the background image, both of which are also hand-drawn. This framework, which mainly uses hand-drawn images, provides qualitatively convincing painterly global illumination effects such as reflection and refraction. We also include parameters to provide additional artistic controls. For instance, using our piecewise linear Fresnel function, it is possible to control the ratio of reflection and refraction. This system is the result of a long line of research contributions. On the other hand, the art-directed Fresnel function that provides physically plausible compositing of reflection and refraction with artistic control is completely new. Art-directed warping equations that provide qualitatively convincing refraction and reflection effects with linearized artistic control are also new. You can try our web-based system for interactive dynamic real-time paintings at http://mock3d.tamu.edu/.",
    "metadata": {
      "arxiv_id": "2311.15354",
      "title": "Web-Based Dynamic Paintings: Real-Time Interactive Artworks in Web Using a 2.5D Pipeline",
      "summary": "In this work, we present a 2.5D pipeline approach to creating dynamic paintings that can be re-rendered interactively in real-time on the Web. Using this 2.5D approach, any existing simple painting such as portraits can be turned into an interactive dynamic web-based artwork. Our interactive system provides most global illumination effects such as reflection, refraction, shadow, and subsurface scattering by processing images. In our system, the scene is defined only by a set of images. These include (1) a shape image, (2) two diffuse images, (3) a background image, (4) one foreground image, and (5) one transparency image. A shape image is either a normal map or a height. Two diffuse images are usually hand-painted. They are interpolated using illumination information. The transparency image is used to define the transparent and reflective regions that can reflect the foreground image and refract the background image, both of which are also hand-drawn. This framework, which mainly uses hand-drawn images, provides qualitatively convincing painterly global illumination effects such as reflection and refraction. We also include parameters to provide additional artistic controls. For instance, using our piecewise linear Fresnel function, it is possible to control the ratio of reflection and refraction. This system is the result of a long line of research contributions. On the other hand, the art-directed Fresnel function that provides physically plausible compositing of reflection and refraction with artistic control is completely new. Art-directed warping equations that provide qualitatively convincing refraction and reflection effects with linearized artistic control are also new. You can try our web-based system for interactive dynamic real-time paintings at http://mock3d.tamu.edu/.",
      "authors": [
        "Ergun Akleman",
        "Youyou wang",
        "Yinan Xiong",
        "Anusha Shanker",
        "Fermi Perumal",
        "Ozgur Gonen",
        "Motahareh Fard"
      ],
      "published": "2023-11-26T17:09:37Z",
      "updated": "2023-11-26T17:09:37Z",
      "categories": [
        "cs.GR",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.15354v1",
      "landing_url": "https://arxiv.org/abs/2311.15354v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.15354"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on a 2.5D web-based dynamic painting pipeline and real-time rendering, not on Spoken Language Models or speech-language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on a 2.5D web-based dynamic painting pipeline and real-time rendering, not on Spoken Language Models or speech-language modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on a 2.5D pipeline for creating interactive web-based dynamic paintings and does not discuss Spoken Language Models, speech language modeling, or related real-time spoken interaction concepts as defined in the inclusion criteria. It lacks any mention of speech as a core modeling object, model architecture, training or evaluation related to speech or full-duplex spoken language interaction. Therefore, it does not meet the inclusion criteria and aligns with exclusion criteria about unrelated topics.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on a 2.5D pipeline for creating interactive web-based dynamic paintings and does not discuss Spoken Language Models, speech language modeling, or related real-time spoken interaction concepts as defined in the inclusion criteria. It lacks any mention of speech as a core modeling object, model architecture, training or evaluation related to speech or full-duplex spoken language interaction. Therefore, it does not meet the inclusion criteria and aligns with exclusion criteria about unrelated topics.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Efficient and Scalable Architecture for Multiple-chip Implementation of Simulated Bifurcation Machines",
    "abstract": "Ising machines are specialized computers for finding the lowest energy states of Ising spin models, onto which many practical combinatorial optimization problems can be mapped. Simulated bifurcation (SB) is a quantum-inspired parallelizable algorithm for Ising problems that enables scalable multi-chip implementations of Ising machines. However, the computational performance of a previously proposed multi-chip architecture tends to saturate as the number of chips increases for a given problem size because both computation and communication are exclusive in the time domain. In this paper, we propose a streaming architecture for multi-chip implementations of SB-based Ising machines with full spin-to-spin connectivity. The data flow in in-chip computation is harmonized with the data flow in inter-chip communication, enabling the computation and communication to overlap and the communication time to be hidden. Systematic experiments demonstrate linear strong scaling of performance up to the vicinity of the ideal communication limit determined only by the latency of chip-to-chip communication. Our eight-FPGA (field-programmable gate array) cluster can compute a 32,768-spin problem with a high pipeline efficiency of 97.9%. The performance of a 79-FPGA cluster for a 100,000-spin problem, projected using a theoretical performance model validated on smaller experimental clusters, is comparable to that of a state-of-the-art 100,000-spin optical Ising machine.",
    "metadata": {
      "arxiv_id": "2311.17370",
      "title": "Efficient and Scalable Architecture for Multiple-chip Implementation of Simulated Bifurcation Machines",
      "summary": "Ising machines are specialized computers for finding the lowest energy states of Ising spin models, onto which many practical combinatorial optimization problems can be mapped. Simulated bifurcation (SB) is a quantum-inspired parallelizable algorithm for Ising problems that enables scalable multi-chip implementations of Ising machines. However, the computational performance of a previously proposed multi-chip architecture tends to saturate as the number of chips increases for a given problem size because both computation and communication are exclusive in the time domain. In this paper, we propose a streaming architecture for multi-chip implementations of SB-based Ising machines with full spin-to-spin connectivity. The data flow in in-chip computation is harmonized with the data flow in inter-chip communication, enabling the computation and communication to overlap and the communication time to be hidden. Systematic experiments demonstrate linear strong scaling of performance up to the vicinity of the ideal communication limit determined only by the latency of chip-to-chip communication. Our eight-FPGA (field-programmable gate array) cluster can compute a 32,768-spin problem with a high pipeline efficiency of 97.9%. The performance of a 79-FPGA cluster for a 100,000-spin problem, projected using a theoretical performance model validated on smaller experimental clusters, is comparable to that of a state-of-the-art 100,000-spin optical Ising machine.",
      "authors": [
        "Tomoya Kashimata",
        "Masaya Yamasaki",
        "Ryo Hidaka",
        "Kosuke Tatsumura"
      ],
      "published": "2023-11-29T05:49:00Z",
      "updated": "2023-11-29T05:49:00Z",
      "categories": [
        "cs.ET",
        "cs.AR",
        "cs.DC",
        "cs.PF"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.17370v1",
      "landing_url": "https://arxiv.org/abs/2311.17370v1",
      "doi": "https://doi.org/10.1109/ACCESS.2024.3374089"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item targets hardware architecture for simulated bifurcation Ising machines and multi-chip FPGA scaling; it has no content on Spoken Language Models or speech-based modeling, thus it does not meet the inclusion criteria and falls under exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item targets hardware architecture for simulated bifurcation Ising machines and multi-chip FPGA scaling; it has no content on Spoken Language Models or speech-based modeling, thus it does not meet the inclusion criteria and falls under exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an architecture for simulated bifurcation machines and Ising machines, which relates to combinatorial optimization and computer architecture rather than spoken language models or speech-based language modeling. It does not discuss speech or spoken language modeling, synchronization in spoken interactions, or any related evaluation metrics required by the inclusion criteria. Therefore, it falls entirely outside the scope of the inclusion criteria and matches none of the relevant concepts for spoken language models or full-duplex spoken language interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an architecture for simulated bifurcation machines and Ising machines, which relates to combinatorial optimization and computer architecture rather than spoken language models or speech-based language modeling. It does not discuss speech or spoken language modeling, synchronization in spoken interactions, or any related evaluation metrics required by the inclusion criteria. Therefore, it falls entirely outside the scope of the inclusion criteria and matches none of the relevant concepts for spoken language models or full-duplex spoken language interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SimulFlow: Simultaneously Extracting Feature and Identifying Target for Unsupervised Video Object Segmentation",
    "abstract": "Unsupervised video object segmentation (UVOS) aims at detecting the primary objects in a given video sequence without any human interposing. Most existing methods rely on two-stream architectures that separately encode the appearance and motion information before fusing them to identify the target and generate object masks. However, this pipeline is computationally expensive and can lead to suboptimal performance due to the difficulty of fusing the two modalities properly. In this paper, we propose a novel UVOS model called SimulFlow that simultaneously performs feature extraction and target identification, enabling efficient and effective unsupervised video object segmentation. Concretely, we design a novel SimulFlow Attention mechanism to bridege the image and motion by utilizing the flexibility of attention operation, where coarse masks predicted from fused feature at each stage are used to constrain the attention operation within the mask area and exclude the impact of noise. Because of the bidirectional information flow between visual and optical flow features in SimulFlow Attention, no extra hand-designed fusing module is required and we only adopt a light decoder to obtain the final prediction. We evaluate our method on several benchmark datasets and achieve state-of-the-art results. Our proposed approach not only outperforms existing methods but also addresses the computational complexity and fusion difficulties caused by two-stream architectures. Our models achieve 87.4% J & F on DAVIS-16 with the highest speed (63.7 FPS on a 3090) and the lowest parameters (13.7 M). Our SimulFlow also obtains competitive results on video salient object detection datasets.",
    "metadata": {
      "arxiv_id": "2311.18286",
      "title": "SimulFlow: Simultaneously Extracting Feature and Identifying Target for Unsupervised Video Object Segmentation",
      "summary": "Unsupervised video object segmentation (UVOS) aims at detecting the primary objects in a given video sequence without any human interposing. Most existing methods rely on two-stream architectures that separately encode the appearance and motion information before fusing them to identify the target and generate object masks. However, this pipeline is computationally expensive and can lead to suboptimal performance due to the difficulty of fusing the two modalities properly. In this paper, we propose a novel UVOS model called SimulFlow that simultaneously performs feature extraction and target identification, enabling efficient and effective unsupervised video object segmentation. Concretely, we design a novel SimulFlow Attention mechanism to bridege the image and motion by utilizing the flexibility of attention operation, where coarse masks predicted from fused feature at each stage are used to constrain the attention operation within the mask area and exclude the impact of noise. Because of the bidirectional information flow between visual and optical flow features in SimulFlow Attention, no extra hand-designed fusing module is required and we only adopt a light decoder to obtain the final prediction. We evaluate our method on several benchmark datasets and achieve state-of-the-art results. Our proposed approach not only outperforms existing methods but also addresses the computational complexity and fusion difficulties caused by two-stream architectures. Our models achieve 87.4% J & F on DAVIS-16 with the highest speed (63.7 FPS on a 3090) and the lowest parameters (13.7 M). Our SimulFlow also obtains competitive results on video salient object detection datasets.",
      "authors": [
        "Lingyi Hong",
        "Wei Zhang",
        "Shuyong Gao",
        "Hong Lu",
        "WenQiang Zhang"
      ],
      "published": "2023-11-30T06:44:44Z",
      "updated": "2023-11-30T06:44:44Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.18286v1",
      "landing_url": "https://arxiv.org/abs/2311.18286v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.18286"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on unsupervised video object segmentation in computer vision and does not address Spoken Language Models or speech-language modeling, therefore it fails all inclusion criteria and does not trigger any exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on unsupervised video object segmentation in computer vision and does not address Spoken Language Models or speech-language modeling, therefore it fails all inclusion criteria and does not trigger any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on unsupervised video object segmentation using novel model architectures and attention mechanisms related to visual and motion features, without any mention of spoken language models, speech language modeling, or full-duplex spoken interaction capabilities. It does not meet the inclusion criteria centered around spoken language modeling or speech as the main modeling target, nor touches on synchronization or interaction aspects specified in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on unsupervised video object segmentation using novel model architectures and attention mechanisms related to visual and motion features, without any mention of spoken language models, speech language modeling, or full-duplex spoken interaction capabilities. It does not meet the inclusion criteria centered around spoken language modeling or speech as the main modeling target, nor touches on synchronization or interaction aspects specified in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "CAST: Cross-Attention in Space and Time for Video Action Recognition",
    "abstract": "Recognizing human actions in videos requires spatial and temporal understanding. Most existing action recognition models lack a balanced spatio-temporal understanding of videos. In this work, we propose a novel two-stream architecture, called Cross-Attention in Space and Time (CAST), that achieves a balanced spatio-temporal understanding of videos using only RGB input. Our proposed bottleneck cross-attention mechanism enables the spatial and temporal expert models to exchange information and make synergistic predictions, leading to improved performance. We validate the proposed method with extensive experiments on public benchmarks with different characteristics: EPIC-KITCHENS-100, Something-Something-V2, and Kinetics-400. Our method consistently shows favorable performance across these datasets, while the performance of existing methods fluctuates depending on the dataset characteristics.",
    "metadata": {
      "arxiv_id": "2311.18825",
      "title": "CAST: Cross-Attention in Space and Time for Video Action Recognition",
      "summary": "Recognizing human actions in videos requires spatial and temporal understanding. Most existing action recognition models lack a balanced spatio-temporal understanding of videos. In this work, we propose a novel two-stream architecture, called Cross-Attention in Space and Time (CAST), that achieves a balanced spatio-temporal understanding of videos using only RGB input. Our proposed bottleneck cross-attention mechanism enables the spatial and temporal expert models to exchange information and make synergistic predictions, leading to improved performance. We validate the proposed method with extensive experiments on public benchmarks with different characteristics: EPIC-KITCHENS-100, Something-Something-V2, and Kinetics-400. Our method consistently shows favorable performance across these datasets, while the performance of existing methods fluctuates depending on the dataset characteristics.",
      "authors": [
        "Dongho Lee",
        "Jongseo Lee",
        "Jinwoo Choi"
      ],
      "published": "2023-11-30T18:58:51Z",
      "updated": "2024-09-03T08:16:32Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.18825v2",
      "landing_url": "https://arxiv.org/abs/2311.18825v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.18825"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets video action recognition with cross-attention in space and time and uses RGB input; it does not address Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets video action recognition with cross-attention in space and time and uses RGB input; it does not address Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on video action recognition using visual input (RGB video), not on spoken language models or speech/audio as the core modeling object. It does not mention spoken language modeling, speech-based language models, or synchronous voice interaction aspects required by the inclusion criteria. Therefore, it fails to meet the subject definition and focuses on a different domain (video analysis) rather than speech or spoken language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on video action recognition using visual input (RGB video), not on spoken language models or speech/audio as the core modeling object. It does not mention spoken language modeling, speech-based language models, or synchronous voice interaction aspects required by the inclusion criteria. Therefore, it fails to meet the subject definition and focuses on a different domain (video analysis) rather than speech or spoken language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "FLea: Addressing Data Scarcity and Label Skew in Federated Learning via Privacy-preserving Feature Augmentation",
    "abstract": "Federated Learning (FL) enables model development by leveraging data distributed across numerous edge devices without transferring local data to a central server. However, existing FL methods still face challenges when dealing with scarce and label-skewed data across devices, resulting in local model overfitting and drift, consequently hindering the performance of the global model. In response to these challenges, we propose a pioneering framework called \\textit{FLea}, incorporating the following key components: \\textit{i)} A global feature buffer that stores activation-target pairs shared from multiple clients to support local training. This design mitigates local model drift caused by the absence of certain classes; \\textit{ii)} A feature augmentation approach based on local and global activation mix-ups for local training. This strategy enlarges the training samples, thereby reducing the risk of local overfitting; \\textit{iii)} An obfuscation method to minimize the correlation between intermediate activations and the source data, enhancing the privacy of shared features. To verify the superiority of \\textit{FLea}, we conduct extensive experiments using a wide range of data modalities, simulating different levels of local data scarcity and label skew. The results demonstrate that \\textit{FLea} consistently outperforms state-of-the-art FL counterparts (among 13 of the experimented 18 settings, the improvement is over $5\\%$) while concurrently mitigating the privacy vulnerabilities associated with shared features. Code is available at https://github.com/XTxiatong/FLea.git",
    "metadata": {
      "arxiv_id": "2312.02327",
      "title": "FLea: Addressing Data Scarcity and Label Skew in Federated Learning via Privacy-preserving Feature Augmentation",
      "summary": "Federated Learning (FL) enables model development by leveraging data distributed across numerous edge devices without transferring local data to a central server. However, existing FL methods still face challenges when dealing with scarce and label-skewed data across devices, resulting in local model overfitting and drift, consequently hindering the performance of the global model. In response to these challenges, we propose a pioneering framework called \\textit{FLea}, incorporating the following key components: \\textit{i)} A global feature buffer that stores activation-target pairs shared from multiple clients to support local training. This design mitigates local model drift caused by the absence of certain classes; \\textit{ii)} A feature augmentation approach based on local and global activation mix-ups for local training. This strategy enlarges the training samples, thereby reducing the risk of local overfitting; \\textit{iii)} An obfuscation method to minimize the correlation between intermediate activations and the source data, enhancing the privacy of shared features. To verify the superiority of \\textit{FLea}, we conduct extensive experiments using a wide range of data modalities, simulating different levels of local data scarcity and label skew. The results demonstrate that \\textit{FLea} consistently outperforms state-of-the-art FL counterparts (among 13 of the experimented 18 settings, the improvement is over $5\\%$) while concurrently mitigating the privacy vulnerabilities associated with shared features. Code is available at https://github.com/XTxiatong/FLea.git",
      "authors": [
        "Tong Xia",
        "Abhirup Ghosh",
        "Xinchi Qiu",
        "Cecilia Mascolo"
      ],
      "published": "2023-12-04T20:24:09Z",
      "updated": "2024-07-02T01:37:34Z",
      "categories": [
        "cs.LG",
        "cs.CR",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.02327v2",
      "landing_url": "https://arxiv.org/abs/2312.02327v2",
      "doi": "https://doi.org/10.1145/3637528.3671899"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes a federated learning framework for data scarcity and privacy-preserving feature augmentation, with no focus on Spoken Language Models or speech-language modeling concepts; it thus fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes a federated learning framework for data scarcity and privacy-preserving feature augmentation, with no focus on Spoken Language Models or speech-language modeling concepts; it thus fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The submission focuses on federated learning methods addressing data scarcity and label skew by leveraging privacy-preserving feature augmentation, without addressing spoken language models, speech language modeling, or full-duplex spoken interaction; the core modeling and evaluation targets are not on speech or spoken language modeling but on federated learning techniques applicable to generic data modalities. Therefore, it does not meet the inclusion criteria centered on spoken language models and related speech-based modeling and evaluation, nor does it touch upon full-duplex or synchronous speech interaction aspects.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The submission focuses on federated learning methods addressing data scarcity and label skew by leveraging privacy-preserving feature augmentation, without addressing spoken language models, speech language modeling, or full-duplex spoken interaction; the core modeling and evaluation targets are not on speech or spoken language modeling but on federated learning techniques applicable to generic data modalities. Therefore, it does not meet the inclusion criteria centered on spoken language models and related speech-based modeling and evaluation, nor does it touch upon full-duplex or synchronous speech interaction aspects.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SELM: Speech Enhancement Using Discrete Tokens and Language Models",
    "abstract": "Language models (LMs) have shown superior performances in various speech generation tasks recently, demonstrating their powerful ability for semantic context modeling. Given the intrinsic similarity between speech generation and speech enhancement, harnessing semantic information holds potential advantages for speech enhancement tasks. In light of this, we propose SELM, a novel paradigm for speech enhancement, which integrates discrete tokens and leverages language models. SELM comprises three stages: encoding, modeling, and decoding. We transform continuous waveform signals into discrete tokens using pre-trained self-supervised learning (SSL) models and a k-means tokenizer. Language models then capture comprehensive contextual information within these tokens. Finally, a detokenizer and HiFi-GAN restore them into enhanced speech. Experimental results demonstrate that SELM achieves comparable performance in objective metrics alongside superior results in subjective perception. Our demos are available https://honee-w.github.io/SELM/.",
    "metadata": {
      "arxiv_id": "2312.09747",
      "title": "SELM: Speech Enhancement Using Discrete Tokens and Language Models",
      "summary": "Language models (LMs) have shown superior performances in various speech generation tasks recently, demonstrating their powerful ability for semantic context modeling. Given the intrinsic similarity between speech generation and speech enhancement, harnessing semantic information holds potential advantages for speech enhancement tasks. In light of this, we propose SELM, a novel paradigm for speech enhancement, which integrates discrete tokens and leverages language models. SELM comprises three stages: encoding, modeling, and decoding. We transform continuous waveform signals into discrete tokens using pre-trained self-supervised learning (SSL) models and a k-means tokenizer. Language models then capture comprehensive contextual information within these tokens. Finally, a detokenizer and HiFi-GAN restore them into enhanced speech. Experimental results demonstrate that SELM achieves comparable performance in objective metrics alongside superior results in subjective perception. Our demos are available https://honee-w.github.io/SELM/.",
      "authors": [
        "Ziqian Wang",
        "Xinfa Zhu",
        "Zihan Zhang",
        "YuanJun Lv",
        "Ning Jiang",
        "Guoqing Zhao",
        "Lei Xie"
      ],
      "published": "2023-12-15T12:36:05Z",
      "updated": "2024-01-07T09:02:52Z",
      "categories": [
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.09747v2",
      "landing_url": "https://arxiv.org/abs/2312.09747v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.09747"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although it centers on language-model-based processing of discrete speech tokens for enhancement and presents architecture and evaluation within the last 3 years, it focuses on enhancement rather than interactive spoken dialogue modeling, so it should be included but not as a strong exemplar of full SLM/FD-SLM interactive systems.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "Although it centers on language-model-based processing of discrete speech tokens for enhancement and presents architecture and evaluation within the last 3 years, it focuses on enhancement rather than interactive spoken dialogue modeling, so it should be included but not as a strong exemplar of full SLM/FD-SLM interactive systems.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on speech enhancement using discrete tokens and language models, which involves language modeling of speech signals. However, the research primarily addresses speech enhancement (signal quality improvement) rather than spoken language models for end-to-end modeling or synchronization in full-duplex spoken interaction scenarios. It does not discuss architectures or evaluations related to turn-taking, cognitive parallelism, or simultaneous interactive behavior, and the metrics are mainly objective and subjective speech quality without real-time interaction or overlap management. Thus, it does not meet the inclusion criteria related to the scope of spoken language models or full-duplex interaction, nor does it address the more complex dialogue or synchronization aspects required.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The article focuses on speech enhancement using discrete tokens and language models, which involves language modeling of speech signals. However, the research primarily addresses speech enhancement (signal quality improvement) rather than spoken language models for end-to-end modeling or synchronization in full-duplex spoken interaction scenarios. It does not discuss architectures or evaluations related to turn-taking, cognitive parallelism, or simultaneous interactive behavior, and the metrics are mainly objective and subjective speech quality without real-time interaction or overlap management. Thus, it does not meet the inclusion criteria related to the scope of spoken language models or full-duplex interaction, nor does it address the more complex dialogue or synchronization aspects required.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed (one recommended inclusion, one recommended exclusion). The paper uses language models over discrete acoustic tokens for speech enhancement, so it does apply language modeling to speech representations, but its primary objective and evaluations are single-task speech enhancement (objective/subjective audio quality) and it contains no discussion of Spoken Language Models/FD‑SLMs, turn-taking/full‑duplex interaction, cognitive parallelism, low‑latency realtime constraints, or behavior arbitration—requirements in the inclusion criteria. Therefore it fails the topic scope and meets the exclusion conditions (focus on enhancement rather than SLM/FD‑SLM interactive modeling). 1-sentence explanation: Although it models speech tokens with LMs, the work targets speech enhancement only and lacks the spoken-language/full‑duplex interaction aspects required, so it should be excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed (one recommended inclusion, one recommended exclusion). The paper uses language models over discrete acoustic tokens for speech enhancement, so it does apply language modeling to speech representations, but its primary objective and evaluations are single-task speech enhancement (objective/subjective audio quality) and it contains no discussion of Spoken Language Models/FD‑SLMs, turn-taking/full‑duplex interaction, cognitive parallelism, low‑latency realtime constraints, or behavior arbitration—requirements in the inclusion criteria. Therefore it fails the topic scope and meets the exclusion conditions (focus on enhancement rather than SLM/FD‑SLM interactive modeling). 1-sentence explanation: Although it models speech tokens with LMs, the work targets speech enhancement only and lacks the spoken-language/full‑duplex interaction aspects required, so it should be excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "StreamDiffusion: A Pipeline-level Solution for Real-time Interactive Generation",
    "abstract": "We introduce StreamDiffusion, a real-time diffusion pipeline designed for interactive image generation. Existing diffusion models are adept at creating images from text or image prompts, yet they often fall short in real-time interaction. This limitation becomes particularly evident in scenarios involving continuous input, such as Metaverse, live video streaming, and broadcasting, where high throughput is imperative. To address this, we present a novel approach that transforms the original sequential denoising into the batching denoising process. Stream Batch eliminates the conventional wait-and-interact approach and enables fluid and high throughput streams. To handle the frequency disparity between data input and model throughput, we design a novel input-output queue for parallelizing the streaming process. Moreover, the existing diffusion pipeline uses classifier-free guidance(CFG), which requires additional U-Net computation. To mitigate the redundant computations, we propose a novel residual classifier-free guidance (RCFG) algorithm that reduces the number of negative conditional denoising steps to only one or even zero. Besides, we introduce a stochastic similarity filter(SSF) to optimize power consumption. Our Stream Batch achieves around 1.5x speedup compared to the sequential denoising method at different denoising levels. The proposed RCFG leads to speeds up to 2.05x higher than the conventional CFG. Combining the proposed strategies and existing mature acceleration tools makes the image-to-image generation achieve up-to 91.07fps on one RTX4090, improving the throughputs of AutoPipline developed by Diffusers over 59.56x. Furthermore, our proposed StreamDiffusion also significantly reduces the energy consumption by 2.39x on one RTX3060 and 1.99x on one RTX4090, respectively.",
    "metadata": {
      "arxiv_id": "2312.12491",
      "title": "StreamDiffusion: A Pipeline-level Solution for Real-time Interactive Generation",
      "summary": "We introduce StreamDiffusion, a real-time diffusion pipeline designed for interactive image generation. Existing diffusion models are adept at creating images from text or image prompts, yet they often fall short in real-time interaction. This limitation becomes particularly evident in scenarios involving continuous input, such as Metaverse, live video streaming, and broadcasting, where high throughput is imperative. To address this, we present a novel approach that transforms the original sequential denoising into the batching denoising process. Stream Batch eliminates the conventional wait-and-interact approach and enables fluid and high throughput streams. To handle the frequency disparity between data input and model throughput, we design a novel input-output queue for parallelizing the streaming process. Moreover, the existing diffusion pipeline uses classifier-free guidance(CFG), which requires additional U-Net computation. To mitigate the redundant computations, we propose a novel residual classifier-free guidance (RCFG) algorithm that reduces the number of negative conditional denoising steps to only one or even zero. Besides, we introduce a stochastic similarity filter(SSF) to optimize power consumption. Our Stream Batch achieves around 1.5x speedup compared to the sequential denoising method at different denoising levels. The proposed RCFG leads to speeds up to 2.05x higher than the conventional CFG. Combining the proposed strategies and existing mature acceleration tools makes the image-to-image generation achieve up-to 91.07fps on one RTX4090, improving the throughputs of AutoPipline developed by Diffusers over 59.56x. Furthermore, our proposed StreamDiffusion also significantly reduces the energy consumption by 2.39x on one RTX3060 and 1.99x on one RTX4090, respectively.",
      "authors": [
        "Akio Kodaira",
        "Chenfeng Xu",
        "Toshiki Hazama",
        "Takanori Yoshimoto",
        "Kohei Ohno",
        "Shogo Mitsuhori",
        "Soichi Sugano",
        "Hanying Cho",
        "Zhijian Liu",
        "Masayoshi Tomizuka",
        "Kurt Keutzer"
      ],
      "published": "2023-12-19T18:18:33Z",
      "updated": "2025-07-08T17:45:49Z",
      "categories": [
        "cs.CV",
        "cs.GR",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.12491v2",
      "landing_url": "https://arxiv.org/abs/2312.12491v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.12491"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item concerns a real-time diffusion pipeline for image generation and does not address Spoken Language Models or speech-language modeling, hence it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item concerns a real-time diffusion pipeline for image generation and does not address Spoken Language Models or speech-language modeling, hence it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a diffusion pipeline for real-time interactive image generation and does not address spoken language models, speech language modeling, or speech processing as core research; it lacks discussion on speech sequences, full-duplex spoken interaction, or synchronization of speech-based models, thus not meeting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a diffusion pipeline for real-time interactive image generation and does not address spoken language models, speech language modeling, or speech processing as core research; it lacks discussion on speech sequences, full-duplex spoken interaction, or synchronization of speech-based models, thus not meeting the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Meta-control of Dialogue Systems Using Large Language Models",
    "abstract": "Utilizing Large Language Models (LLMs) facilitates the creation of flexible and natural dialogues, a task that has been challenging with traditional rule-based dialogue systems. However, LLMs also have the potential to produce unexpected responses, which may not align with the intentions of dialogue system designers. To address this issue, this paper introduces a meta-control method that employs LLMs to develop more stable and adaptable dialogue systems. The method includes dialogue flow control to ensure that utterances conform to predefined scenarios and turn-taking control to foster natural dialogues. Furthermore, we have implemented a dialogue system that utilizes this meta-control strategy and verified that the dialogue system utilizing meta-control operates as intended.",
    "metadata": {
      "arxiv_id": "2312.13715",
      "title": "Meta-control of Dialogue Systems Using Large Language Models",
      "summary": "Utilizing Large Language Models (LLMs) facilitates the creation of flexible and natural dialogues, a task that has been challenging with traditional rule-based dialogue systems. However, LLMs also have the potential to produce unexpected responses, which may not align with the intentions of dialogue system designers. To address this issue, this paper introduces a meta-control method that employs LLMs to develop more stable and adaptable dialogue systems. The method includes dialogue flow control to ensure that utterances conform to predefined scenarios and turn-taking control to foster natural dialogues. Furthermore, we have implemented a dialogue system that utilizes this meta-control strategy and verified that the dialogue system utilizing meta-control operates as intended.",
      "authors": [
        "Kotaro Shukuri",
        "Ryoma Ishigaki",
        "Jundai Suzuki",
        "Tsubasa Naganuma",
        "Takuma Fujimoto",
        "Daisuke Kawakubo",
        "Masaki Shuzo",
        "Eisaku Maeda"
      ],
      "published": "2023-12-21T10:28:18Z",
      "updated": "2023-12-21T10:28:18Z",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.13715v1",
      "landing_url": "https://arxiv.org/abs/2312.13715v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.13715"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on meta-control of dialogue systems using LLMs in a text-based context and does not treat Spoken Language Models or speech as the core modeling target (no emphasis on speech, audio signals, or FD-SLM concepts), so it fails the core inclusion criteria despite temporal relevance.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on meta-control of dialogue systems using LLMs in a text-based context and does not treat Spoken Language Models or speech as the core modeling target (no emphasis on speech, audio signals, or FD-SLM concepts), so it fails the core inclusion criteria despite temporal relevance.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on utilizing LLMs for meta-control in dialogue systems to ensure dialogue stability and adaptiveness, but it does not explicitly address spoken language models or speech-level modeling as a core modeling object, nor does it mention full-duplex or real-time speech interaction aspects; it centers more on dialogue flow and turn-taking control at a conceptual level without detailing speech-based language modeling or relevant evaluation metrics, making it misaligned with the inclusion criteria and closely related exclusion points.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on utilizing LLMs for meta-control in dialogue systems to ensure dialogue stability and adaptiveness, but it does not explicitly address spoken language models or speech-level modeling as a core modeling object, nor does it mention full-duplex or real-time speech interaction aspects; it centers more on dialogue flow and turn-taking control at a conceptual level without detailing speech-based language modeling or relevant evaluation metrics, making it misaligned with the inclusion criteria and closely related exclusion points.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Team Flow at DRC2023: Building Common Ground and Text-based Turn-taking in a Travel Agent Spoken Dialogue System",
    "abstract": "At the Dialogue Robot Competition 2023 (DRC2023), which was held to improve the capability of dialogue robots, our team developed a system that could build common ground and take more natural turns based on user utterance texts. Our system generated queries for sightseeing spot searches using the common ground and engaged in dialogue while waiting for user comprehension.",
    "metadata": {
      "arxiv_id": "2312.13816",
      "title": "Team Flow at DRC2023: Building Common Ground and Text-based Turn-taking in a Travel Agent Spoken Dialogue System",
      "summary": "At the Dialogue Robot Competition 2023 (DRC2023), which was held to improve the capability of dialogue robots, our team developed a system that could build common ground and take more natural turns based on user utterance texts. Our system generated queries for sightseeing spot searches using the common ground and engaged in dialogue while waiting for user comprehension.",
      "authors": [
        "Ryu Hirai",
        "Shinya Iizuka",
        "Haruhisa Iseno",
        "Ao Guo",
        "Jingjing Jiang",
        "Atsumoto Ohashi",
        "Ryuichiro Higashinaka"
      ],
      "published": "2023-12-21T13:08:09Z",
      "updated": "2023-12-21T13:08:09Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.13816v1",
      "landing_url": "https://arxiv.org/abs/2312.13816v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.13816"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on text-based turn-taking within a spoken dialogue system and mentions using user utterance texts, but it does not clearly center on Spoken Language Models or speech-focused modeling of acoustic signals, nor discuss end-to-end Speech-LLM/FDSLM architecture or real-time speech interaction evaluations; thus it does not meet the inclusion criteria and aligns with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on text-based turn-taking within a spoken dialogue system and mentions using user utterance texts, but it does not clearly center on Spoken Language Models or speech-focused modeling of acoustic signals, nor discuss end-to-end Speech-LLM/FDSLM architecture or real-time speech interaction evaluations; thus it does not meet the inclusion criteria and aligns with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a dialogue system developing common ground and turn-taking based on user utterance texts, but it lacks explicit mention of spoken language models, speech-level modeling, full-duplex interaction, or evaluations related to speech modeling rather than text-based dialogue management; thus, it does not meet the core inclusion criteria focusing on speech-based language modeling and full-duplex spoken language models.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a dialogue system developing common ground and turn-taking based on user utterance texts, but it lacks explicit mention of spoken language models, speech-level modeling, full-duplex interaction, or evaluations related to speech modeling rather than text-based dialogue management; thus, it does not meet the core inclusion criteria focusing on speech-based language modeling and full-duplex spoken language models.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Measuring the Concentration of Control in Contemporary Ethereum",
    "abstract": "Ethereum is undergoing significant changes to its architecture as it evolves. These changes include its switch to PoS consensus and the introduction of significant infrastructural changes that do not require a change to the core protocol, but that fundamentally affect the way users interact with the network. These changes represent an evolution toward a more modular architecture, in which there exists new exogenous vectors for centralization. This paper builds on previous studies of decentralization of Ethereum to reflect these recent significant changes, and Ethereum's new modular paradigm.",
    "metadata": {
      "arxiv_id": "2312.14562",
      "title": "Measuring the Concentration of Control in Contemporary Ethereum",
      "summary": "Ethereum is undergoing significant changes to its architecture as it evolves. These changes include its switch to PoS consensus and the introduction of significant infrastructural changes that do not require a change to the core protocol, but that fundamentally affect the way users interact with the network. These changes represent an evolution toward a more modular architecture, in which there exists new exogenous vectors for centralization. This paper builds on previous studies of decentralization of Ethereum to reflect these recent significant changes, and Ethereum's new modular paradigm.",
      "authors": [
        "Simon Brown"
      ],
      "published": "2023-12-22T09:47:52Z",
      "updated": "2023-12-22T09:47:52Z",
      "categories": [
        "econ.TH"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.14562v1",
      "landing_url": "https://arxiv.org/abs/2312.14562v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.14562"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on Ethereum decentralization and architectural changes, with no focus on Spoken Language Models (SLMs) or speech-language modeling; it fails all inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on Ethereum decentralization and architectural changes, with no focus on Spoken Language Models (SLMs) or speech-language modeling; it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on Ethereum's network architecture changes and decentralization, which are unrelated to Spoken Language Models or speech-based modeling as required by the inclusion criteria. There is no mention of speech, language modeling of speech, full-duplex interaction, synchronization, or relevant evaluation metrics. Therefore, it does not meet the inclusion criteria and matches exclusion criteria regarding topic relevance.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on Ethereum's network architecture changes and decentralization, which are unrelated to Spoken Language Models or speech-based modeling as required by the inclusion criteria. There is no mention of speech, language modeling of speech, full-duplex interaction, synchronization, or relevant evaluation metrics. Therefore, it does not meet the inclusion criteria and matches exclusion criteria regarding topic relevance.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Unraveling the Temporal Dynamics of the Unet in Diffusion Models",
    "abstract": "Diffusion models have garnered significant attention since they can effectively learn complex multivariate Gaussian distributions, resulting in diverse, high-quality outcomes. They introduce Gaussian noise into training data and reconstruct the original data iteratively. Central to this iterative process is a single Unet, adapting across time steps to facilitate generation. Recent work revealed the presence of composition and denoising phases in this generation process, raising questions about the Unets' varying roles. Our study dives into the dynamic behavior of Unets within denoising diffusion probabilistic models (DDPM), focusing on (de)convolutional blocks and skip connections across time steps. We propose an analytical method to systematically assess the impact of time steps and core Unet components on the final output. This method eliminates components to study causal relations and investigate their influence on output changes. The main purpose is to understand the temporal dynamics and identify potential shortcuts during inference. Our findings provide valuable insights into the various generation phases during inference and shed light on the Unets' usage patterns across these phases. Leveraging these insights, we identify redundancies in GLIDE (an improved DDPM) and improve inference time by ~27% with minimal degradation in output quality. Our ultimate goal is to guide more informed optimization strategies for inference and influence new model designs.",
    "metadata": {
      "arxiv_id": "2312.14965",
      "title": "Unraveling the Temporal Dynamics of the Unet in Diffusion Models",
      "summary": "Diffusion models have garnered significant attention since they can effectively learn complex multivariate Gaussian distributions, resulting in diverse, high-quality outcomes. They introduce Gaussian noise into training data and reconstruct the original data iteratively. Central to this iterative process is a single Unet, adapting across time steps to facilitate generation. Recent work revealed the presence of composition and denoising phases in this generation process, raising questions about the Unets' varying roles. Our study dives into the dynamic behavior of Unets within denoising diffusion probabilistic models (DDPM), focusing on (de)convolutional blocks and skip connections across time steps. We propose an analytical method to systematically assess the impact of time steps and core Unet components on the final output. This method eliminates components to study causal relations and investigate their influence on output changes. The main purpose is to understand the temporal dynamics and identify potential shortcuts during inference. Our findings provide valuable insights into the various generation phases during inference and shed light on the Unets' usage patterns across these phases. Leveraging these insights, we identify redundancies in GLIDE (an improved DDPM) and improve inference time by ~27% with minimal degradation in output quality. Our ultimate goal is to guide more informed optimization strategies for inference and influence new model designs.",
      "authors": [
        "Vidya Prasad",
        "Chen Zhu-Tian",
        "Anna Vilanova",
        "Hanspeter Pfister",
        "Nicola Pezzotti",
        "Hendrik Strobelt"
      ],
      "published": "2023-12-17T04:40:33Z",
      "updated": "2023-12-17T04:40:33Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.14965v1",
      "landing_url": "https://arxiv.org/abs/2312.14965v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.14965"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item analyzes the temporal dynamics and optimization of Unets in diffusion models for image/signal generation, with no focus on Spoken Language Models, speech modeling, or speech interaction, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item analyzes the temporal dynamics and optimization of Unets in diffusion models for image/signal generation, with no focus on Spoken Language Models, speech modeling, or speech interaction, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on analyzing the Unet component in diffusion models related to image or data generation, without any mention of spoken language models, speech-level language modeling, or full-duplex spoken interaction capabilities. It does not address speech as a core modeling or inference object, nor does it cover synchronization, cognitive parallelism, or real-time spoken interaction aspects. Therefore, it does not meet the inclusion criteria focused on spoken language models and is unrelated to the core topic of the review task.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on analyzing the Unet component in diffusion models related to image or data generation, without any mention of spoken language models, speech-level language modeling, or full-duplex spoken interaction capabilities. It does not address speech as a core modeling or inference object, nor does it cover synchronization, cognitive parallelism, or real-time spoken interaction aspects. Therefore, it does not meet the inclusion criteria focused on spoken language models and is unrelated to the core topic of the review task.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Discrete Messages Improve Communication Efficiency among Isolated Intelligent Agents",
    "abstract": "Individuals, despite having varied life experiences and learning processes, can communicate effectively through languages. This study aims to explore the efficiency of language as a communication medium. We put forth two specific hypotheses: First, discrete messages are more effective than continuous ones when agents have diverse personal experiences. Second, communications using multiple discrete tokens are more advantageous than those using a single token. To valdate these hypotheses, we designed multi-agent machine learning experiments to assess communication efficiency using various information transmission methods between speakers and listeners. Our empirical findings indicate that, in scenarios where agents are exposed to different data, communicating through sentences composed of discrete tokens offers the best inter-agent communication efficiency. The limitations of our finding include lack of systematic advantages over other more sophisticated encoder-decoder model such as variational autoencoder and lack of evluation on non-image dataset, which we will leave for future studies.",
    "metadata": {
      "arxiv_id": "2312.15985",
      "title": "Discrete Messages Improve Communication Efficiency among Isolated Intelligent Agents",
      "summary": "Individuals, despite having varied life experiences and learning processes, can communicate effectively through languages. This study aims to explore the efficiency of language as a communication medium. We put forth two specific hypotheses: First, discrete messages are more effective than continuous ones when agents have diverse personal experiences. Second, communications using multiple discrete tokens are more advantageous than those using a single token. To valdate these hypotheses, we designed multi-agent machine learning experiments to assess communication efficiency using various information transmission methods between speakers and listeners. Our empirical findings indicate that, in scenarios where agents are exposed to different data, communicating through sentences composed of discrete tokens offers the best inter-agent communication efficiency. The limitations of our finding include lack of systematic advantages over other more sophisticated encoder-decoder model such as variational autoencoder and lack of evluation on non-image dataset, which we will leave for future studies.",
      "authors": [
        "Hang Chen",
        "Yuchuan Jang",
        "Weijie Zhou",
        "Cristian Meo",
        "Ziwei Chen",
        "Dianbo Liu"
      ],
      "published": "2023-12-26T10:30:05Z",
      "updated": "2024-10-18T02:22:19Z",
      "categories": [
        "cs.LG",
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.15985v3",
      "landing_url": "https://arxiv.org/abs/2312.15985v3",
      "doi": "https://doi.org/10.48550/arXiv.2312.15985"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on discrete vs. continuous token communication in multi-agent systems and does not treat Spoken Language Models or speech as the core modeling objects, failing the inclusion criteria for speech-language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on discrete vs. continuous token communication in multi-agent systems and does not treat Spoken Language Models or speech as the core modeling objects, failing the inclusion criteria for speech-language modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on communication efficiency among isolated intelligent agents using discrete messages, primarily within a multi-agent machine learning framework. However, it does not explicitly address spoken language models, speech language modeling, or the treatment of speech as a primary modeling object, nor does it discuss full-duplex or real-time interactive speech systems with synchronization aspects; thus, it fails to meet the core inclusion criteria centered on spoken language modeling. Additionally, there is no mention of evaluation on speech data or speech-specific model architectures, making it unsuitable based on provided criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on communication efficiency among isolated intelligent agents using discrete messages, primarily within a multi-agent machine learning framework. However, it does not explicitly address spoken language models, speech language modeling, or the treatment of speech as a primary modeling object, nor does it discuss full-duplex or real-time interactive speech systems with synchronization aspects; thus, it fails to meet the core inclusion criteria centered on spoken language modeling. Additionally, there is no mention of evaluation on speech data or speech-specific model architectures, making it unsuitable based on provided criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Shrinking Your TimeStep: Towards Low-Latency Neuromorphic Object Recognition with Spiking Neural Network",
    "abstract": "Neuromorphic object recognition with spiking neural networks (SNNs) is the cornerstone of low-power neuromorphic computing. However, existing SNNs suffer from significant latency, utilizing 10 to 40 timesteps or more, to recognize neuromorphic objects. At low latencies, the performance of existing SNNs is drastically degraded. In this work, we propose the Shrinking SNN (SSNN) to achieve low-latency neuromorphic object recognition without reducing performance. Concretely, we alleviate the temporal redundancy in SNNs by dividing SNNs into multiple stages with progressively shrinking timesteps, which significantly reduces the inference latency. During timestep shrinkage, the temporal transformer smoothly transforms the temporal scale and preserves the information maximally. Moreover, we add multiple early classifiers to the SNN during training to mitigate the mismatch between the surrogate gradient and the true gradient, as well as the gradient vanishing/exploding, thus eliminating the performance degradation at low latency. Extensive experiments on neuromorphic datasets, CIFAR10-DVS, N-Caltech101, and DVS-Gesture have revealed that SSNN is able to improve the baseline accuracy by 6.55% ~ 21.41%. With only 5 average timesteps and without any data augmentation, SSNN is able to achieve an accuracy of 73.63% on CIFAR10-DVS. This work presents a heterogeneous temporal scale SNN and provides valuable insights into the development of high-performance, low-latency SNNs.",
    "metadata": {
      "arxiv_id": "2401.01912",
      "title": "Shrinking Your TimeStep: Towards Low-Latency Neuromorphic Object Recognition with Spiking Neural Network",
      "summary": "Neuromorphic object recognition with spiking neural networks (SNNs) is the cornerstone of low-power neuromorphic computing. However, existing SNNs suffer from significant latency, utilizing 10 to 40 timesteps or more, to recognize neuromorphic objects. At low latencies, the performance of existing SNNs is drastically degraded. In this work, we propose the Shrinking SNN (SSNN) to achieve low-latency neuromorphic object recognition without reducing performance. Concretely, we alleviate the temporal redundancy in SNNs by dividing SNNs into multiple stages with progressively shrinking timesteps, which significantly reduces the inference latency. During timestep shrinkage, the temporal transformer smoothly transforms the temporal scale and preserves the information maximally. Moreover, we add multiple early classifiers to the SNN during training to mitigate the mismatch between the surrogate gradient and the true gradient, as well as the gradient vanishing/exploding, thus eliminating the performance degradation at low latency. Extensive experiments on neuromorphic datasets, CIFAR10-DVS, N-Caltech101, and DVS-Gesture have revealed that SSNN is able to improve the baseline accuracy by 6.55% ~ 21.41%. With only 5 average timesteps and without any data augmentation, SSNN is able to achieve an accuracy of 73.63% on CIFAR10-DVS. This work presents a heterogeneous temporal scale SNN and provides valuable insights into the development of high-performance, low-latency SNNs.",
      "authors": [
        "Yongqi Ding",
        "Lin Zuo",
        "Mengmeng Jing",
        "Pei He",
        "Yongjun Xiao"
      ],
      "published": "2024-01-02T02:05:05Z",
      "updated": "2024-01-02T02:05:05Z",
      "categories": [
        "cs.CV",
        "cs.LG",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.01912v1",
      "landing_url": "https://arxiv.org/abs/2401.01912v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.01912"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper targets neuromorphic SNNs for low-latency object recognition, not Spoken Language Models or speech-language modeling guidelines, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper targets neuromorphic SNNs for low-latency object recognition, not Spoken Language Models or speech-language modeling guidelines, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on low-latency neuromorphic object recognition using spiking neural networks, which does not involve spoken language models or speech language modeling, nor does it address synchronous speech interactions or linguistic modeling of speech sequences as required by the inclusion criteria; therefore, it does not meet the core thematic requirements and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on low-latency neuromorphic object recognition using spiking neural networks, which does not involve spoken language models or speech language modeling, nor does it address synchronous speech interactions or linguistic modeling of speech sequences as required by the inclusion criteria; therefore, it does not meet the core thematic requirements and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "StreamVC: Real-Time Low-Latency Voice Conversion",
    "abstract": "We present StreamVC, a streaming voice conversion solution that preserves the content and prosody of any source speech while matching the voice timbre from any target speech. Unlike previous approaches, StreamVC produces the resulting waveform at low latency from the input signal even on a mobile platform, making it applicable to real-time communication scenarios like calls and video conferencing, and addressing use cases such as voice anonymization in these scenarios. Our design leverages the architecture and training strategy of the SoundStream neural audio codec for lightweight high-quality speech synthesis. We demonstrate the feasibility of learning soft speech units causally, as well as the effectiveness of supplying whitened fundamental frequency information to improve pitch stability without leaking the source timbre information.",
    "metadata": {
      "arxiv_id": "2401.03078",
      "title": "StreamVC: Real-Time Low-Latency Voice Conversion",
      "summary": "We present StreamVC, a streaming voice conversion solution that preserves the content and prosody of any source speech while matching the voice timbre from any target speech. Unlike previous approaches, StreamVC produces the resulting waveform at low latency from the input signal even on a mobile platform, making it applicable to real-time communication scenarios like calls and video conferencing, and addressing use cases such as voice anonymization in these scenarios. Our design leverages the architecture and training strategy of the SoundStream neural audio codec for lightweight high-quality speech synthesis. We demonstrate the feasibility of learning soft speech units causally, as well as the effectiveness of supplying whitened fundamental frequency information to improve pitch stability without leaking the source timbre information.",
      "authors": [
        "Yang Yang",
        "Yury Kartynnik",
        "Yunpeng Li",
        "Jiuqiang Tang",
        "Xing Li",
        "George Sung",
        "Matthias Grundmann"
      ],
      "published": "2024-01-05T22:37:26Z",
      "updated": "2024-01-05T22:37:26Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.03078v1",
      "landing_url": "https://arxiv.org/abs/2401.03078v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.03078"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項聚焦於語音轉換與實時處理，並未以 Spoken Language Models / Speech Language Models 為核心語言建模對象，也缺乏明確的語音語言建模架構、訓練與評估內容，與評估對話同步、低延遲互動等核心納入條件不符，因此應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項聚焦於語音轉換與實時處理，並未以 Spoken Language Models / Speech Language Models 為核心語言建模對象，也缺乏明確的語音語言建模架構、訓練與評估內容，與評估對話同步、低延遲互動等核心納入條件不符，因此應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a low-latency, real-time voice conversion system primarily aimed at preserving voice timbre and enabling applications like voice anonymization in communication scenarios; however, it does not explicitly discuss Spoken Language Models (SLMs), language modeling of speech, or full-duplex interaction capabilities, nor does it address model architecture or evaluation metrics related to synchronous spoken dialogue systems, which are essential per the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a low-latency, real-time voice conversion system primarily aimed at preserving voice timbre and enabling applications like voice anonymization in communication scenarios; however, it does not explicitly discuss Spoken Language Models (SLMs), language modeling of speech, or full-duplex interaction capabilities, nor does it address model architecture or evaluation metrics related to synchronous spoken dialogue systems, which are essential per the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "NovelGym: A Flexible Ecosystem for Hybrid Planning and Learning Agents Designed for Open Worlds",
    "abstract": "As AI agents leave the lab and venture into the real world as autonomous vehicles, delivery robots, and cooking robots, it is increasingly necessary to design and comprehensively evaluate algorithms that tackle the ``open-world''. To this end, we introduce NovelGym, a flexible and adaptable ecosystem designed to simulate gridworld environments, serving as a robust platform for benchmarking reinforcement learning (RL) and hybrid planning and learning agents in open-world contexts. The modular architecture of NovelGym facilitates rapid creation and modification of task environments, including multi-agent scenarios, with multiple environment transformations, thus providing a dynamic testbed for researchers to develop open-world AI agents.",
    "metadata": {
      "arxiv_id": "2401.03546",
      "title": "NovelGym: A Flexible Ecosystem for Hybrid Planning and Learning Agents Designed for Open Worlds",
      "summary": "As AI agents leave the lab and venture into the real world as autonomous vehicles, delivery robots, and cooking robots, it is increasingly necessary to design and comprehensively evaluate algorithms that tackle the ``open-world''. To this end, we introduce NovelGym, a flexible and adaptable ecosystem designed to simulate gridworld environments, serving as a robust platform for benchmarking reinforcement learning (RL) and hybrid planning and learning agents in open-world contexts. The modular architecture of NovelGym facilitates rapid creation and modification of task environments, including multi-agent scenarios, with multiple environment transformations, thus providing a dynamic testbed for researchers to develop open-world AI agents.",
      "authors": [
        "Shivam Goel",
        "Yichen Wei",
        "Panagiotis Lymperopoulos",
        "Klara Chura",
        "Matthias Scheutz",
        "Jivko Sinapov"
      ],
      "published": "2024-01-07T17:13:28Z",
      "updated": "2024-01-07T17:13:28Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.03546v1",
      "landing_url": "https://arxiv.org/abs/2401.03546v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.03546"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on a reinforcement learning/open-world benchmarking platform (NovelGym) for hybrid planning and learning agents, not on Spoken Language Models or speech-language modeling, so it fails the inclusion criteria and does not meet the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on a reinforcement learning/open-world benchmarking platform (NovelGym) for hybrid planning and learning agents, not on Spoken Language Models or speech-language modeling, so it fails the inclusion criteria and does not meet the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper titled 'NovelGym: A Flexible Ecosystem for Hybrid Planning and Learning Agents Designed for Open Worlds' focuses on AI agents in open-world gridworld environments and reinforcement learning frameworks, without discussing spoken language models, speech language modeling, or full-duplex spoken interaction paradigms. It does not meet the core inclusion criteria centered on speech-based language modeling or full-duplex spoken language modeling and lacks any mention of relevant keywords or concepts from the inclusion list, thus failing to match the required thematic scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper titled 'NovelGym: A Flexible Ecosystem for Hybrid Planning and Learning Agents Designed for Open Worlds' focuses on AI agents in open-world gridworld environments and reinforcement learning frameworks, without discussing spoken language models, speech language modeling, or full-duplex spoken interaction paradigms. It does not meet the core inclusion criteria centered on speech-based language modeling or full-duplex spoken language modeling and lacks any mention of relevant keywords or concepts from the inclusion list, thus failing to match the required thematic scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Masked Audio Generation using a Single Non-Autoregressive Transformer",
    "abstract": "We introduce MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, we predict spans of masked tokens obtained from a masking scheduler, while during inference we gradually construct the output sequence using several decoding steps. To further enhance the quality of the generated audio, we introduce a novel rescoring method in which, we leverage an external pre-trained model to rescore and rank predictions from MAGNeT, which will be then used for later decoding steps. Lastly, we explore a hybrid version of MAGNeT, in which we fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner while the rest of the sequence is being decoded in parallel. We demonstrate the efficiency of MAGNeT for the task of text-to-music and text-to-audio generation and conduct an extensive empirical evaluation, considering both objective metrics and human studies. The proposed approach is comparable to the evaluated baselines, while being significantly faster (x7 faster than the autoregressive baseline). Through ablation studies and analysis, we shed light on the importance of each of the components comprising MAGNeT, together with pointing to the trade-offs between autoregressive and non-autoregressive modeling, considering latency, throughput, and generation quality. Samples are available on our demo page https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT.",
    "metadata": {
      "arxiv_id": "2401.04577",
      "title": "Masked Audio Generation using a Single Non-Autoregressive Transformer",
      "summary": "We introduce MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, we predict spans of masked tokens obtained from a masking scheduler, while during inference we gradually construct the output sequence using several decoding steps. To further enhance the quality of the generated audio, we introduce a novel rescoring method in which, we leverage an external pre-trained model to rescore and rank predictions from MAGNeT, which will be then used for later decoding steps. Lastly, we explore a hybrid version of MAGNeT, in which we fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner while the rest of the sequence is being decoded in parallel. We demonstrate the efficiency of MAGNeT for the task of text-to-music and text-to-audio generation and conduct an extensive empirical evaluation, considering both objective metrics and human studies. The proposed approach is comparable to the evaluated baselines, while being significantly faster (x7 faster than the autoregressive baseline). Through ablation studies and analysis, we shed light on the importance of each of the components comprising MAGNeT, together with pointing to the trade-offs between autoregressive and non-autoregressive modeling, considering latency, throughput, and generation quality. Samples are available on our demo page https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT.",
      "authors": [
        "Alon Ziv",
        "Itai Gat",
        "Gael Le Lan",
        "Tal Remez",
        "Felix Kreuk",
        "Alexandre Défossez",
        "Jade Copet",
        "Gabriel Synnaeve",
        "Yossi Adi"
      ],
      "published": "2024-01-09T14:29:39Z",
      "updated": "2024-03-05T09:12:35Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.04577v2",
      "landing_url": "https://arxiv.org/abs/2401.04577v2",
      "doi": "https://doi.org/10.48550/arXiv.2401.04577"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on masked audio generation with a non-autoregressive transformer for text-to-audio/music; however, it does not treat spoken language models or speech as the core modeling target, nor discuss interaction/real-time speech aspects, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work centers on masked audio generation with a non-autoregressive transformer for text-to-audio/music; however, it does not treat spoken language models or speech as the core modeling target, nor discuss interaction/real-time speech aspects, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on masked audio generation using a non-autoregressive transformer to generate audio sequences, primarily for text-to-music and text-to-audio tasks, without explicit mention or emphasis on spoken language models (SLMs), full-duplex spoken language modeling, synchronous interactive behaviors, or related evaluation criteria such as cognitive parallelism or sub-200ms latency; thus, it does not meet the core inclusion criteria centered on SLMs as the main modeling object and interactive spoken communication capabilities.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on masked audio generation using a non-autoregressive transformer to generate audio sequences, primarily for text-to-music and text-to-audio tasks, without explicit mention or emphasis on spoken language models (SLMs), full-duplex spoken language modeling, synchronous interactive behaviors, or related evaluation criteria such as cognitive parallelism or sub-200ms latency; thus, it does not meet the core inclusion criteria centered on SLMs as the main modeling object and interactive spoken communication capabilities.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Real-time and Continuous Turn-taking Prediction Using Voice Activity Projection",
    "abstract": "A demonstration of a real-time and continuous turn-taking prediction system is presented. The system is based on a voice activity projection (VAP) model, which directly maps dialogue stereo audio to future voice activities. The VAP model includes contrastive predictive coding (CPC) and self-attention transformers, followed by a cross-attention transformer. We examine the effect of the input context audio length and demonstrate that the proposed system can operate in real-time with CPU settings, with minimal performance degradation.",
    "metadata": {
      "arxiv_id": "2401.04868",
      "title": "Real-time and Continuous Turn-taking Prediction Using Voice Activity Projection",
      "summary": "A demonstration of a real-time and continuous turn-taking prediction system is presented. The system is based on a voice activity projection (VAP) model, which directly maps dialogue stereo audio to future voice activities. The VAP model includes contrastive predictive coding (CPC) and self-attention transformers, followed by a cross-attention transformer. We examine the effect of the input context audio length and demonstrate that the proposed system can operate in real-time with CPU settings, with minimal performance degradation.",
      "authors": [
        "Koji Inoue",
        "Bing'er Jiang",
        "Erik Ekstedt",
        "Tatsuya Kawahara",
        "Gabriel Skantze"
      ],
      "published": "2024-01-10T01:09:55Z",
      "updated": "2024-01-10T01:09:55Z",
      "categories": [
        "cs.CL",
        "cs.HC",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.04868v1",
      "landing_url": "https://arxiv.org/abs/2401.04868v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.04868"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on real-time turn-taking prediction via a voice activity projection model, but the title/abstract do not explicitly frame the work as Spoken Language Models or Full-Duplex SLMs, nor mention SLM-related terminology in title/abstract/keywords, which is required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item centers on real-time turn-taking prediction via a voice activity projection model, but the title/abstract do not explicitly frame the work as Spoken Language Models or Full-Duplex SLMs, nor mention SLM-related terminology in title/abstract/keywords, which is required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on real-time turn-taking prediction using voice activity projection, which involves modeling voice activities directly from dialogue audio, suggesting speech-level modeling. However, the abstract does not explicitly mention spoken language models or speech language models, nor does it discuss model architecture in terms of SLMs or FD-SLMs explicitly. Despite addressing real-time interaction and turn-taking, it is unclear if it covers full-duplex or learned synchronization, or broader linguistic modeling as required. Therefore, it meets some inclusion criteria but lacks explicit connection to spoken language modeling frameworks and detailed multi-dimensional evaluation as per inclusion criteria.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on real-time turn-taking prediction using voice activity projection, which involves modeling voice activities directly from dialogue audio, suggesting speech-level modeling. However, the abstract does not explicitly mention spoken language models or speech language models, nor does it discuss model architecture in terms of SLMs or FD-SLMs explicitly. Despite addressing real-time interaction and turn-taking, it is unclear if it covers full-duplex or learned synchronization, or broader linguistic modeling as required. Therefore, it meets some inclusion criteria but lacks explicit connection to spoken language modeling frameworks and detailed multi-dimensional evaluation as per inclusion criteria.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers correctly noted that the paper focuses on real-time turn-taking prediction using a voice activity projection (VAP) model (CPC + transformers) but the title/abstract do not mention \"spoken language model(s)\" or equivalent phrasing, nor do they claim speech-level language modeling or full-duplex synchronization capabilities. The work appears to target half-duplex turn-taking prediction (voice-activity forecasting) rather than end-to-end SLMs/FD-SLMs, and it does not address learned synchronization, overlap handling, cognitive parallelism, or the multidimensional evaluations required by the inclusion criteria—so it fails the explicit textual requirement and falls under the exclusion of traditional half-duplex turn-taking systems. Score explanation: Better to exclude because the abstract lacks explicit SLM/speech-language-model framing and focuses on turn-taking (half-duplex) VAP rather than the SLM/FD-SLM scope required.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers correctly noted that the paper focuses on real-time turn-taking prediction using a voice activity projection (VAP) model (CPC + transformers) but the title/abstract do not mention \"spoken language model(s)\" or equivalent phrasing, nor do they claim speech-level language modeling or full-duplex synchronization capabilities. The work appears to target half-duplex turn-taking prediction (voice-activity forecasting) rather than end-to-end SLMs/FD-SLMs, and it does not address learned synchronization, overlap handling, cognitive parallelism, or the multidimensional evaluations required by the inclusion criteria—so it fails the explicit textual requirement and falls under the exclusion of traditional half-duplex turn-taking systems. Score explanation: Better to exclude because the abstract lacks explicit SLM/speech-language-model framing and focuses on turn-taking (half-duplex) VAP rather than the SLM/FD-SLM scope required.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering",
    "abstract": "The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups. Audio samples are available at https://ereboas.github.io/ELLAV/.",
    "metadata": {
      "arxiv_id": "2401.07333",
      "title": "ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering",
      "summary": "The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups. Audio samples are available at https://ereboas.github.io/ELLAV/.",
      "authors": [
        "Yakun Song",
        "Zhuo Chen",
        "Xiaofei Wang",
        "Ziyang Ma",
        "Xie Chen"
      ],
      "published": "2024-01-14T17:43:55Z",
      "updated": "2024-01-14T17:43:55Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.07333v1",
      "landing_url": "https://arxiv.org/abs/2401.07333v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.07333"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on Spoken Language Modeling for speech generation (LM-based TTS with acoustic and phoneme tokens) and discusses model architecture/evaluation within the last 3 years, explicitly addressing speech-language modeling rather than treating speech merely as input to NLP, thus meeting inclusion and not falling under exclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on Spoken Language Modeling for speech generation (LM-based TTS with acoustic and phoneme tokens) and discusses model architecture/evaluation within the last 3 years, explicitly addressing speech-language modeling rather than treating speech merely as input to NLP, thus meeting inclusion and not falling under exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a language model for text-to-speech generation emphasizing fine-grained acoustic-phoneme alignment and synthesis quality improvements. However, it primarily addresses zero-shot TTS and modeling speech outputs rather than spoken language models or full-duplex spoken language modeling involving synchronous interactive dialogue, cognitive parallelism, or behavior arbitration; also it lacks discussion on interaction latency, overlapped speech management, or synchronization mechanisms. Furthermore, the abstract does not mention spoken language model terminology or evaluation criteria related to real-time interaction or turn-taking. Hence, it fails to meet the inclusion criteria and matches several exclusion points relating to lack of dialogue interaction and core speech language modeling focus.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a language model for text-to-speech generation emphasizing fine-grained acoustic-phoneme alignment and synthesis quality improvements. However, it primarily addresses zero-shot TTS and modeling speech outputs rather than spoken language models or full-duplex spoken language modeling involving synchronous interactive dialogue, cognitive parallelism, or behavior arbitration; also it lacks discussion on interaction latency, overlapped speech management, or synchronization mechanisms. Furthermore, the abstract does not mention spoken language model terminology or evaluation criteria related to real-time interaction or turn-taking. Hence, it fails to meet the inclusion criteria and matches several exclusion points relating to lack of dialogue interaction and core speech language modeling focus.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed both junior reviewers' comments: JuniorNano argued inclusion because the work uses acoustic and phoneme tokens with an LM, while JuniorMini correctly noted this is a zero-shot TTS paper focusing on synthesis quality and alignment rather than Spoken Language Models or interactive/full‑duplex capabilities. The abstract and title frame the work as an LM-based TTS system (ELLA-V) addressing repetitions, alignment, and decoding stability; it does not explicitly claim or evaluate \"spoken language models\"/\"speech language models\" nor discuss full-duplex interaction, turn-taking, overlap management, low-latency real-time behavior, or related evaluation benchmarks required by the inclusion criteria. Therefore it meets the exclusion criteria (primarily a TTS-focused work) and fails the requirement that the paper explicitly center on SLMs/FD‑SLMs or equivalent speech-language modeling topics. One-sentence explanation for the score: Exclude because this is a TTS/modeling-for-synthesis paper without explicit spoken-language-model framing or any full‑duplex/interaction or synchronization contributions required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "I reviewed both junior reviewers' comments: JuniorNano argued inclusion because the work uses acoustic and phoneme tokens with an LM, while JuniorMini correctly noted this is a zero-shot TTS paper focusing on synthesis quality and alignment rather than Spoken Language Models or interactive/full‑duplex capabilities. The abstract and title frame the work as an LM-based TTS system (ELLA-V) addressing repetitions, alignment, and decoding stability; it does not explicitly claim or evaluate \"spoken language models\"/\"speech language models\" nor discuss full-duplex interaction, turn-taking, overlap management, low-latency real-time behavior, or related evaluation benchmarks required by the inclusion criteria. Therefore it meets the exclusion criteria (primarily a TTS-focused work) and fails the requirement that the paper explicitly center on SLMs/FD‑SLMs or equivalent speech-language modeling topics. One-sentence explanation for the score: Exclude because this is a TTS/modeling-for-synthesis paper without explicit spoken-language-model framing or any full‑duplex/interaction or synchronization contributions required by the inclusion criteria.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DatUS^2: Data-driven Unsupervised Semantic Segmentation with Pre-trained Self-supervised Vision Transformer",
    "abstract": "Successive proposals of several self-supervised training schemes continue to emerge, taking one step closer to developing a universal foundation model. In this process, the unsupervised downstream tasks are recognized as one of the evaluation methods to validate the quality of visual features learned with a self-supervised training scheme. However, unsupervised dense semantic segmentation has not been explored as a downstream task, which can utilize and evaluate the quality of semantic information introduced in patch-level feature representations during self-supervised training of a vision transformer. Therefore, this paper proposes a novel data-driven approach for unsupervised semantic segmentation (DatUS^2) as a downstream task. DatUS^2 generates semantically consistent and dense pseudo annotate segmentation masks for the unlabeled image dataset without using any visual-prior or synchronized data. We compare these pseudo-annotated segmentation masks with ground truth masks for evaluating recent self-supervised training schemes to learn shared semantic properties at the patch level and discriminative semantic properties at the segment level. Finally, we evaluate existing state-of-the-art self-supervised training schemes with our proposed downstream task, i.e., DatUS^2. Also, the best version of DatUS^2 outperforms the existing state-of-the-art method for the unsupervised dense semantic segmentation task with 15.02% MiOU and 21.47% Pixel accuracy on the SUIM dataset. It also achieves a competitive level of accuracy for a large-scale and complex dataset, i.e., the COCO dataset.",
    "metadata": {
      "arxiv_id": "2401.12820",
      "title": "DatUS^2: Data-driven Unsupervised Semantic Segmentation with Pre-trained Self-supervised Vision Transformer",
      "summary": "Successive proposals of several self-supervised training schemes continue to emerge, taking one step closer to developing a universal foundation model. In this process, the unsupervised downstream tasks are recognized as one of the evaluation methods to validate the quality of visual features learned with a self-supervised training scheme. However, unsupervised dense semantic segmentation has not been explored as a downstream task, which can utilize and evaluate the quality of semantic information introduced in patch-level feature representations during self-supervised training of a vision transformer. Therefore, this paper proposes a novel data-driven approach for unsupervised semantic segmentation (DatUS^2) as a downstream task. DatUS^2 generates semantically consistent and dense pseudo annotate segmentation masks for the unlabeled image dataset without using any visual-prior or synchronized data. We compare these pseudo-annotated segmentation masks with ground truth masks for evaluating recent self-supervised training schemes to learn shared semantic properties at the patch level and discriminative semantic properties at the segment level. Finally, we evaluate existing state-of-the-art self-supervised training schemes with our proposed downstream task, i.e., DatUS^2. Also, the best version of DatUS^2 outperforms the existing state-of-the-art method for the unsupervised dense semantic segmentation task with 15.02% MiOU and 21.47% Pixel accuracy on the SUIM dataset. It also achieves a competitive level of accuracy for a large-scale and complex dataset, i.e., the COCO dataset.",
      "authors": [
        "Sonal Kumar",
        "Arijit Sur",
        "Rashmi Dutta Baruah"
      ],
      "published": "2024-01-23T14:53:32Z",
      "updated": "2024-01-23T14:53:32Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.12820v1",
      "landing_url": "https://arxiv.org/abs/2401.12820v1",
      "doi": "https://doi.org/10.1109/TCDS.2024.3383952"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper targets unsupervised semantic segmentation with vision transformers (CV domain) and does not involve spoken language models or speech-language modeling, failing the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper targets unsupervised semantic segmentation with vision transformers (CV domain) and does not involve spoken language models or speech-language modeling, failing the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on unsupervised semantic segmentation using pre-trained self-supervised vision transformers in computer vision, without addressing spoken language modeling, speech as a core modeling object, full-duplex spoken language models, or related interactive speech-based phenomena required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on unsupervised semantic segmentation using pre-trained self-supervised vision transformers in computer vision, without addressing spoken language modeling, speech as a core modeling object, full-duplex spoken language models, or related interactive speech-based phenomena required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "IndiText Boost: Text Augmentation for Low Resource India Languages",
    "abstract": "Text Augmentation is an important task for low-resource languages. It helps deal with the problem of data scarcity. A data augmentation strategy is used to deal with the problem of data scarcity. Through the years, much work has been done on data augmentation for the English language. In contrast, very less work has been done on Indian languages. This is contrary to the fact that data augmentation is used to deal with data scarcity. In this work, we focus on implementing techniques like Easy Data Augmentation, Back Translation, Paraphrasing, Text Generation using LLMs, and Text Expansion using LLMs for text classification on different languages. We focus on 6 Indian languages namely: Sindhi, Marathi, Hindi, Gujarati, Telugu, and Sanskrit. According to our knowledge, no such work exists for text augmentation on Indian languages. We carry out binary as well as multi-class text classification to make our results more comparable. We get surprising results as basic data augmentation techniques surpass LLMs.",
    "metadata": {
      "arxiv_id": "2401.13085",
      "title": "IndiText Boost: Text Augmentation for Low Resource India Languages",
      "summary": "Text Augmentation is an important task for low-resource languages. It helps deal with the problem of data scarcity. A data augmentation strategy is used to deal with the problem of data scarcity. Through the years, much work has been done on data augmentation for the English language. In contrast, very less work has been done on Indian languages. This is contrary to the fact that data augmentation is used to deal with data scarcity. In this work, we focus on implementing techniques like Easy Data Augmentation, Back Translation, Paraphrasing, Text Generation using LLMs, and Text Expansion using LLMs for text classification on different languages. We focus on 6 Indian languages namely: Sindhi, Marathi, Hindi, Gujarati, Telugu, and Sanskrit. According to our knowledge, no such work exists for text augmentation on Indian languages. We carry out binary as well as multi-class text classification to make our results more comparable. We get surprising results as basic data augmentation techniques surpass LLMs.",
      "authors": [
        "Onkar Litake",
        "Niraj Yagnik",
        "Shreyas Labhsetwar"
      ],
      "published": "2024-01-23T20:54:40Z",
      "updated": "2024-01-23T20:54:40Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.13085v1",
      "landing_url": "https://arxiv.org/abs/2401.13085v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.13085"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on text augmentation for NLP in Indian languages and does not address Spoken Language Models or any speech-language modeling concepts, thus failing the core inclusion criteria and alignment with the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on text augmentation for NLP in Indian languages and does not address Spoken Language Models or any speech-language modeling concepts, thus failing the core inclusion criteria and alignment with the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on text augmentation for low-resource Indian languages, specifically for text classification tasks, and makes no mention of speech or spoken language models, speech as a primary modeling object, full-duplex spoken language interaction, or any related synchronization or behavioral arbitration concepts. It also does not discuss model architectures or evaluations pertaining to speech language modeling or real-time interactive speech processing, thus failing all key inclusion criteria related to spoken language models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on text augmentation for low-resource Indian languages, specifically for text classification tasks, and makes no mention of speech or spoken language models, speech as a primary modeling object, full-duplex spoken language interaction, or any related synchronization or behavioral arbitration concepts. It also does not discuss model architectures or evaluations pertaining to speech language modeling or real-time interactive speech processing, thus failing all key inclusion criteria related to spoken language models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A modular architecture for IMU-based data gloves",
    "abstract": "The flexibility and range of motion in human hands play a crucial role in human interaction with the environment and have been studied across different fields. Researchers explored various technological solutions for gathering information from the hands. These solutions include tracking hand motion through cameras or wearable sensors and using wearable sensors to measure the position and pressure of contact points. Data gloves can collect both types of information by utilizing inertial measurement units, flex sensors, magnetic trackers for motion tracking, and force resistors or touch sensors for contact measurement. Although there are commercially available data gloves, researchers often create custom data gloves to achieve the desired flexibility and control over the hardware. However, the existing literature lacks standardization and the reuse of previously designed data gloves. As a result, many gloves with unclear characteristics exist, which makes replication challenging and negatively impacts the reproducibility of studies. This work proposes a modular, open hardware and software architecture for creating customized data gloves based on IMU technology. We also provide an architecture implementation along with an experimental protocol to evaluate device performance.",
    "metadata": {
      "arxiv_id": "2401.13254",
      "title": "A modular architecture for IMU-based data gloves",
      "summary": "The flexibility and range of motion in human hands play a crucial role in human interaction with the environment and have been studied across different fields. Researchers explored various technological solutions for gathering information from the hands. These solutions include tracking hand motion through cameras or wearable sensors and using wearable sensors to measure the position and pressure of contact points. Data gloves can collect both types of information by utilizing inertial measurement units, flex sensors, magnetic trackers for motion tracking, and force resistors or touch sensors for contact measurement. Although there are commercially available data gloves, researchers often create custom data gloves to achieve the desired flexibility and control over the hardware. However, the existing literature lacks standardization and the reuse of previously designed data gloves. As a result, many gloves with unclear characteristics exist, which makes replication challenging and negatively impacts the reproducibility of studies. This work proposes a modular, open hardware and software architecture for creating customized data gloves based on IMU technology. We also provide an architecture implementation along with an experimental protocol to evaluate device performance.",
      "authors": [
        "Alessandro Carfì",
        "Mohamad Alameh",
        "Valerio Belcamino",
        "Fulvio Mastrogiovanni"
      ],
      "published": "2024-01-24T06:34:06Z",
      "updated": "2024-01-24T06:34:06Z",
      "categories": [
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.13254v1",
      "landing_url": "https://arxiv.org/abs/2401.13254v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.13254"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article describes a modular IMU-based data glove hardware architecture and replication concerns, with no focus on Spoken Language Models or speech-based synchronization/evaluation, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article describes a modular IMU-based data glove hardware architecture and replication concerns, with no focus on Spoken Language Models or speech-based synchronization/evaluation, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a modular architecture for IMU-based data gloves, which pertains to hardware development for hand motion tracking and human-computer interaction, rather than spoken language models or speech-based modeling and interaction. There is no mention of spoken language models, speech language modeling, full-duplex interaction, or any related speech-language modeling frameworks in the title or abstract. Therefore, it does not meet the core inclusion criteria centered on spoken language models and speech-based modeling, nor does it address synchronous speech interaction or dialogue management aspects.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a modular architecture for IMU-based data gloves, which pertains to hardware development for hand motion tracking and human-computer interaction, rather than spoken language models or speech-based modeling and interaction. There is no mention of spoken language models, speech language modeling, full-duplex interaction, or any related speech-language modeling frameworks in the title or abstract. Therefore, it does not meet the core inclusion criteria centered on spoken language models and speech-based modeling, nor does it address synchronous speech interaction or dialogue management aspects.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Modular Approach to Automatic Cyber Threat Attribution using Opinion Pools",
    "abstract": "Cyber threat attribution can play an important role in increasing resilience against digital threats. Recent research focuses on automating the threat attribution process and on integrating it with other efforts, such as threat hunting. To support increasing automation of the cyber threat attribution process, this paper proposes a modular architecture as an alternative to current monolithic automated approaches. The modular architecture can utilize opinion pools to combine the output of concrete attributors. The proposed solution increases the tractability of the threat attribution problem and offers increased usability and interpretability, as opposed to monolithic alternatives. In addition, a Pairing Aggregator is proposed as an aggregation method that forms pairs of attributors based on distinct features to produce intermediary results before finally producing a single Probability Mass Function (PMF) as output. The Pairing Aggregator sequentially applies both the logarithmic opinion pool and the linear opinion pool. An experimental validation suggests that the modular approach does not result in decreased performance and can even enhance precision and recall compared to monolithic alternatives. The results also suggest that the Pairing Aggregator can improve precision over the linear and logarithmic opinion pools. Furthermore, the improved k-accuracy in the experiment suggests that forensic experts can leverage the resulting PMF during their manual attribution processes to enhance their efficiency.",
    "metadata": {
      "arxiv_id": "2401.14090",
      "title": "A Modular Approach to Automatic Cyber Threat Attribution using Opinion Pools",
      "summary": "Cyber threat attribution can play an important role in increasing resilience against digital threats. Recent research focuses on automating the threat attribution process and on integrating it with other efforts, such as threat hunting. To support increasing automation of the cyber threat attribution process, this paper proposes a modular architecture as an alternative to current monolithic automated approaches. The modular architecture can utilize opinion pools to combine the output of concrete attributors. The proposed solution increases the tractability of the threat attribution problem and offers increased usability and interpretability, as opposed to monolithic alternatives. In addition, a Pairing Aggregator is proposed as an aggregation method that forms pairs of attributors based on distinct features to produce intermediary results before finally producing a single Probability Mass Function (PMF) as output. The Pairing Aggregator sequentially applies both the logarithmic opinion pool and the linear opinion pool. An experimental validation suggests that the modular approach does not result in decreased performance and can even enhance precision and recall compared to monolithic alternatives. The results also suggest that the Pairing Aggregator can improve precision over the linear and logarithmic opinion pools. Furthermore, the improved k-accuracy in the experiment suggests that forensic experts can leverage the resulting PMF during their manual attribution processes to enhance their efficiency.",
      "authors": [
        "Koen T. W. Teuwen"
      ],
      "published": "2024-01-25T11:12:16Z",
      "updated": "2024-01-25T11:12:16Z",
      "categories": [
        "cs.CR",
        "cs.LG",
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.14090v1",
      "landing_url": "https://arxiv.org/abs/2401.14090v1",
      "doi": "https://doi.org/10.1109/BigData59044.2023.10386708"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Topic is cyber threat attribution; no Spoken Language Models (SLMs) or speech modeling are addressed; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Topic is cyber threat attribution; no Spoken Language Models (SLMs) or speech modeling are addressed; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on automatic cyber threat attribution using a modular approach and opinion pools, with no mention of spoken language models, speech language modeling, full-duplex or half-duplex speech interactions, nor does it address any speech signal processing or speech-based model training/evaluation. It is unrelated to the inclusion criteria concerning spoken language or speech models and misses all core key concepts specified. Therefore, it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on automatic cyber threat attribution using a modular approach and opinion pools, with no mention of spoken language models, speech language modeling, full-duplex or half-duplex speech interactions, nor does it address any speech signal processing or speech-based model training/evaluation. It is unrelated to the inclusion criteria concerning spoken language or speech models and misses all core key concepts specified. Therefore, it should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Turn-taking and Backchannel Prediction with Acoustic and Large Language Model Fusion",
    "abstract": "We propose an approach for continuous prediction of turn-taking and backchanneling locations in spoken dialogue by fusing a neural acoustic model with a large language model (LLM). Experiments on the Switchboard human-human conversation dataset demonstrate that our approach consistently outperforms the baseline models with single modality. We also develop a novel multi-task instruction fine-tuning strategy to further benefit from LLM-encoded knowledge for understanding the tasks and conversational contexts, leading to additional improvements. Our approach demonstrates the potential of combined LLMs and acoustic models for a more natural and conversational interaction between humans and speech-enabled AI agents.",
    "metadata": {
      "arxiv_id": "2401.14717",
      "title": "Turn-taking and Backchannel Prediction with Acoustic and Large Language Model Fusion",
      "summary": "We propose an approach for continuous prediction of turn-taking and backchanneling locations in spoken dialogue by fusing a neural acoustic model with a large language model (LLM). Experiments on the Switchboard human-human conversation dataset demonstrate that our approach consistently outperforms the baseline models with single modality. We also develop a novel multi-task instruction fine-tuning strategy to further benefit from LLM-encoded knowledge for understanding the tasks and conversational contexts, leading to additional improvements. Our approach demonstrates the potential of combined LLMs and acoustic models for a more natural and conversational interaction between humans and speech-enabled AI agents.",
      "authors": [
        "Jinhan Wang",
        "Long Chen",
        "Aparna Khare",
        "Anirudh Raju",
        "Pranav Dheram",
        "Di He",
        "Minhua Wu",
        "Andreas Stolcke",
        "Venkatesh Ravichandran"
      ],
      "published": "2024-01-26T08:59:07Z",
      "updated": "2024-01-26T08:59:07Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.14717v1",
      "landing_url": "https://arxiv.org/abs/2401.14717v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.14717"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本研究以語音序列為核心、融合聲學模型與大語言模型來預測轉話與回饋，屬於 Spoken Language Models / FD-SLMs 的端到端或混合架構，並在 Switchboard 上進行實證評估，符合近3年內發表且聚焦語音語言建模與對話互動的條件。",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "本研究以語音序列為核心、融合聲學模型與大語言模型來預測轉話與回饋，屬於 Spoken Language Models / FD-SLMs 的端到端或混合架構，並在 Switchboard 上進行實證評估，符合近3年內發表且聚焦語音語言建模與對話互動的條件。",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study clearly addresses turn-taking and backchanneling in spoken dialogue using a fusion of acoustic models and large language models, which aligns well with the core focus on spoken language models incorporating speech as the primary modeling target. The approach emphasizes modeling conversational interactions beyond text-only processing, including synchronization and multi-task instruction fine-tuning, suggesting an end-to-end or hybrid system. The study does not appear to be limited to traditional half-duplex systems or isolated ASR tasks and involves new evaluation strategies beyond standard recognition metrics. Moreover, the abstract references spoken dialogue and conversational contexts consistent with the inclusion criteria. However, the abstract does not explicitly mention \"spoken language model(s)\" or exact equivalents, though it strongly implies speech language modeling; inclusion depends partly on that semantic equivalence, so there is minor ambiguity warranting some caution.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The study clearly addresses turn-taking and backchanneling in spoken dialogue using a fusion of acoustic models and large language models, which aligns well with the core focus on spoken language models incorporating speech as the primary modeling target. The approach emphasizes modeling conversational interactions beyond text-only processing, including synchronization and multi-task instruction fine-tuning, suggesting an end-to-end or hybrid system. The study does not appear to be limited to traditional half-duplex systems or isolated ASR tasks and involves new evaluation strategies beyond standard recognition metrics. Moreover, the abstract references spoken dialogue and conversational contexts consistent with the inclusion criteria. However, the abstract does not explicitly mention \"spoken language model(s)\" or exact equivalents, though it strongly implies speech language modeling; inclusion depends partly on that semantic equivalence, so there is minor ambiguity warranting some caution.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Optimal Quality and Efficiency in Adaptive Live Streaming with JND-Aware Low latency Encoding",
    "abstract": "In HTTP adaptive live streaming applications, video segments are encoded at a fixed set of bitrate-resolution pairs known as bitrate ladder. Live encoders use the fastest available encoding configuration, referred to as preset, to ensure the minimum possible latency in video encoding. However, an optimized preset and optimized number of CPU threads for each encoding instance may result in (i) increased quality and (ii) efficient CPU utilization while encoding. For low latency live encoders, the encoding speed is expected to be more than or equal to the video framerate. To this light, this paper introduces a Just Noticeable Difference (JND)-Aware Low latency Encoding Scheme (JALE), which uses random forest-based models to jointly determine the optimized encoder preset and thread count for each representation, based on video complexity features, the target encoding speed, the total number of available CPU threads, and the target encoder. Experimental results show that, on average, JALE yield a quality improvement of 1.32 dB PSNR and 5.38 VMAF points with the same bitrate, compared to the fastest preset encoding of the HTTP Live Streaming (HLS) bitrate ladder using x265 HEVC open-source encoder with eight CPU threads used for each representation. These enhancements are achieved while maintaining the desired encoding speed. Furthermore, on average, JALE results in an overall storage reduction of 72.70 %, a reduction in the total number of CPU threads used by 63.83 %, and a 37.87 % reduction in the overall encoding time, considering a JND of six VMAF points.",
    "metadata": {
      "arxiv_id": "2401.15343",
      "title": "Optimal Quality and Efficiency in Adaptive Live Streaming with JND-Aware Low latency Encoding",
      "summary": "In HTTP adaptive live streaming applications, video segments are encoded at a fixed set of bitrate-resolution pairs known as bitrate ladder. Live encoders use the fastest available encoding configuration, referred to as preset, to ensure the minimum possible latency in video encoding. However, an optimized preset and optimized number of CPU threads for each encoding instance may result in (i) increased quality and (ii) efficient CPU utilization while encoding. For low latency live encoders, the encoding speed is expected to be more than or equal to the video framerate. To this light, this paper introduces a Just Noticeable Difference (JND)-Aware Low latency Encoding Scheme (JALE), which uses random forest-based models to jointly determine the optimized encoder preset and thread count for each representation, based on video complexity features, the target encoding speed, the total number of available CPU threads, and the target encoder. Experimental results show that, on average, JALE yield a quality improvement of 1.32 dB PSNR and 5.38 VMAF points with the same bitrate, compared to the fastest preset encoding of the HTTP Live Streaming (HLS) bitrate ladder using x265 HEVC open-source encoder with eight CPU threads used for each representation. These enhancements are achieved while maintaining the desired encoding speed. Furthermore, on average, JALE results in an overall storage reduction of 72.70 %, a reduction in the total number of CPU threads used by 63.83 %, and a 37.87 % reduction in the overall encoding time, considering a JND of six VMAF points.",
      "authors": [
        "Vignesh V Menon",
        "Jingwen Zhu",
        "Prajit T Rajendran",
        "Samira Afzal",
        "Klaus Schoeffmann",
        "Patrick Le Callet",
        "Christian Timmerer"
      ],
      "published": "2024-01-27T08:23:00Z",
      "updated": "2024-01-27T08:23:00Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.15343v1",
      "landing_url": "https://arxiv.org/abs/2401.15343v1",
      "doi": "https://doi.org/10.1145/3638036.3640807"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on random-forest-based encoder tuning for low-latency video streaming and does not address Spoken Language Models, speech modeling architectures, or speech-based evaluation, hence it fails the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on random-forest-based encoder tuning for low-latency video streaming and does not address Spoken Language Models, speech modeling architectures, or speech-based evaluation, hence it fails the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on optimizing video encoding for adaptive live streaming using JND-aware low latency encoding, which centers on video quality and efficiency rather than speech or spoken language modeling. It does not discuss spoken language models, speech as a core modeling object, or interactive speech behaviors such as full-duplex conversation, cognitive parallelism, or low-latency speech interaction. Hence, it does not meet the inclusion criteria focused on spoken language models and low-latency interactive speech systems, nor does it violate any exclusion criteria specifically, but it is clearly outside the scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on optimizing video encoding for adaptive live streaming using JND-aware low latency encoding, which centers on video quality and efficiency rather than speech or spoken language modeling. It does not discuss spoken language models, speech as a core modeling object, or interactive speech behaviors such as full-duplex conversation, cognitive parallelism, or low-latency speech interaction. Hence, it does not meet the inclusion criteria focused on spoken language models and low-latency interactive speech systems, nor does it violate any exclusion criteria specifically, but it is clearly outside the scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SpeechBERTScore: Reference-Aware Automatic Evaluation of Speech Generation Leveraging NLP Evaluation Metrics",
    "abstract": "While subjective assessments have been the gold standard for evaluating speech generation, there is a growing need for objective metrics that are highly correlated with human subjective judgments due to their cost efficiency. This paper proposes reference-aware automatic evaluation methods for speech generation inspired by evaluation metrics in natural language processing. The proposed SpeechBERTScore computes the BERTScore for self-supervised dense speech features of the generated and reference speech, which can have different sequential lengths. We also propose SpeechBLEU and SpeechTokenDistance, which are computed on speech discrete tokens. The evaluations on synthesized speech show that our method correlates better with human subjective ratings than mel cepstral distortion and a recent mean opinion score prediction model. Also, they are effective in noisy speech evaluation and have cross-lingual applicability.",
    "metadata": {
      "arxiv_id": "2401.16812",
      "title": "SpeechBERTScore: Reference-Aware Automatic Evaluation of Speech Generation Leveraging NLP Evaluation Metrics",
      "summary": "While subjective assessments have been the gold standard for evaluating speech generation, there is a growing need for objective metrics that are highly correlated with human subjective judgments due to their cost efficiency. This paper proposes reference-aware automatic evaluation methods for speech generation inspired by evaluation metrics in natural language processing. The proposed SpeechBERTScore computes the BERTScore for self-supervised dense speech features of the generated and reference speech, which can have different sequential lengths. We also propose SpeechBLEU and SpeechTokenDistance, which are computed on speech discrete tokens. The evaluations on synthesized speech show that our method correlates better with human subjective ratings than mel cepstral distortion and a recent mean opinion score prediction model. Also, they are effective in noisy speech evaluation and have cross-lingual applicability.",
      "authors": [
        "Takaaki Saeki",
        "Soumi Maiti",
        "Shinnosuke Takamichi",
        "Shinji Watanabe",
        "Hiroshi Saruwatari"
      ],
      "published": "2024-01-30T08:26:28Z",
      "updated": "2024-09-01T14:34:27Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.16812v3",
      "landing_url": "https://arxiv.org/abs/2401.16812v3",
      "doi": "https://doi.org/10.48550/arXiv.2401.16812"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on reference-aware evaluation metrics for speech generation (SpeechBERTScore) rather than developing or evaluating Spoken Language Models or full-duplex interactive speech systems; it treats speech mainly as input to NLP-like evaluation metrics and does not center on SLMs, architecture, or multi-dimensional interactive speech capabilities, thus failing the core inclusion criteria despite being within the 3-year window.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on reference-aware evaluation metrics for speech generation (SpeechBERTScore) rather than developing or evaluating Spoken Language Models or full-duplex interactive speech systems; it treats speech mainly as input to NLP-like evaluation metrics and does not center on SLMs, architecture, or multi-dimensional interactive speech capabilities, thus failing the core inclusion criteria despite being within the 3-year window.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on developing objective evaluation metrics for speech generation using NLP-inspired methods, but it does not explicitly center on Spoken Language Models or full-duplex interactive spoken language architectures; it rather addresses evaluation metrics without discussing model architecture, training, or full-duplex interaction capabilities, nor does it mention key concepts like cognitive parallelism or simultaneous interaction behaviors. Additionally, the abstract does not mention spoken language modeling or speech language models explicitly, which is a core inclusion criterion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on developing objective evaluation metrics for speech generation using NLP-inspired methods, but it does not explicitly center on Spoken Language Models or full-duplex interactive spoken language architectures; it rather addresses evaluation metrics without discussing model architecture, training, or full-duplex interaction capabilities, nor does it mention key concepts like cognitive parallelism or simultaneous interaction behaviors. Additionally, the abstract does not mention spoken language modeling or speech language models explicitly, which is a core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "An Intra-BRNN and GB-RVQ Based END-TO-END Neural Audio Codec",
    "abstract": "Recently, neural networks have proven to be effective in performing speech coding task at low bitrates. However, under-utilization of intra-frame correlations and the error of quantizer specifically degrade the reconstructed audio quality. To improve the coding quality, we present an end-to-end neural speech codec, namely CBRC (Convolutional and Bidirectional Recurrent neural Codec). An interleaved structure using 1D-CNN and Intra-BRNN is designed to exploit the intra-frame correlations more efficiently. Furthermore, Group-wise and Beam-search Residual Vector Quantizer (GB-RVQ) is used to reduce the quantization noise. CBRC encodes audio every 20ms with no additional latency, which is suitable for real-time communication. Experimental results demonstrate the superiority of the proposed codec when comparing CBRC at 3kbps with Opus at 12kbps.",
    "metadata": {
      "arxiv_id": "2402.01271",
      "title": "An Intra-BRNN and GB-RVQ Based END-TO-END Neural Audio Codec",
      "summary": "Recently, neural networks have proven to be effective in performing speech coding task at low bitrates. However, under-utilization of intra-frame correlations and the error of quantizer specifically degrade the reconstructed audio quality. To improve the coding quality, we present an end-to-end neural speech codec, namely CBRC (Convolutional and Bidirectional Recurrent neural Codec). An interleaved structure using 1D-CNN and Intra-BRNN is designed to exploit the intra-frame correlations more efficiently. Furthermore, Group-wise and Beam-search Residual Vector Quantizer (GB-RVQ) is used to reduce the quantization noise. CBRC encodes audio every 20ms with no additional latency, which is suitable for real-time communication. Experimental results demonstrate the superiority of the proposed codec when comparing CBRC at 3kbps with Opus at 12kbps.",
      "authors": [
        "Linping Xu",
        "Jiawei Jiang",
        "Dejun Zhang",
        "Xianjun Xia",
        "Li Chen",
        "Yijian Xiao",
        "Piao Ding",
        "Shenyi Song",
        "Sixing Yin",
        "Ferdous Sohel"
      ],
      "published": "2024-02-02T09:55:15Z",
      "updated": "2024-02-02T09:55:15Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01271v1",
      "landing_url": "https://arxiv.org/abs/2402.01271v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.01271"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper centers on neural audio coding and codec design for speech compression, not on Spoken Language Models or speech-language modeling (FD-SLMs), and it does not address dialogue synchronization or conversational behavior, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper centers on neural audio coding and codec design for speech compression, not on Spoken Language Models or speech-language modeling (FD-SLMs), and it does not address dialogue synchronization or conversational behavior, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents an end-to-end neural audio codec focused on speech coding quality improvement and real-time communication efficiency, but it does not mention Spoken Language Models, language modeling of speech, full-duplex interaction, synchronization, or behavioral arbitration aspects required by the inclusion criteria. It focuses on speech coding rather than modeling or reasoning over speech sequences for conversational or interactive systems. Therefore, it fails to meet the core inclusion criteria and also does not align with the evaluation or behavioral interaction requirements, hence should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents an end-to-end neural audio codec focused on speech coding quality improvement and real-time communication efficiency, but it does not mention Spoken Language Models, language modeling of speech, full-duplex interaction, synchronization, or behavioral arbitration aspects required by the inclusion criteria. It focuses on speech coding rather than modeling or reasoning over speech sequences for conversational or interactive systems. Therefore, it fails to meet the core inclusion criteria and also does not align with the evaluation or behavioral interaction requirements, hence should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Full-Duplex Millimeter Wave MIMO Channel Estimation: A Neural Network Approach",
    "abstract": "Millimeter wave (mmWave) multiple-input-multi-output (MIMO) is now a reality with great potential for further improvement. We study full-duplex transmissions as an effective way to improve mmWave MIMO systems. Compared to half-duplex systems, full-duplex transmissions may offer higher data rates and lower latency. However, full-duplex transmission is hindered by self-interference (SI) at the receive antennas, and SI channel estimation becomes a crucial step to make the full-duplex systems feasible. In this paper, we address the problem of channel estimation in full-duplex mmWave MIMO systems using neural networks (NNs). Our approach involves sharing pilot resources between user equipments (UEs) and transmit antennas at the base station (BS), aiming to reduce the pilot overhead in full-duplex systems and to achieve a comparable level to that of a half-duplex system. Additionally, in the case of separate antenna configurations in a full-duplex BS, providing channel estimates of transmit antenna (TX) arrays to the downlink UEs poses another challenge, as the TX arrays are not capable of receiving pilot signals. To address this, we employ an NN to map the channel from the downlink UEs to the receive antenna (RX) arrays to the channel from the TX arrays to the downlink UEs. We further elaborate on how NNs perform the estimation with different architectures, (e.g., different numbers of hidden layers), the introduction of non-linear distortion (e.g., with a 1-bit analog-to-digital converter (ADC)), and different channel conditions (e.g., low-correlated and high-correlated channels). Our work provides novel insights into NN-based channel estimators.",
    "metadata": {
      "arxiv_id": "2402.03886",
      "title": "Full-Duplex Millimeter Wave MIMO Channel Estimation: A Neural Network Approach",
      "summary": "Millimeter wave (mmWave) multiple-input-multi-output (MIMO) is now a reality with great potential for further improvement. We study full-duplex transmissions as an effective way to improve mmWave MIMO systems. Compared to half-duplex systems, full-duplex transmissions may offer higher data rates and lower latency. However, full-duplex transmission is hindered by self-interference (SI) at the receive antennas, and SI channel estimation becomes a crucial step to make the full-duplex systems feasible. In this paper, we address the problem of channel estimation in full-duplex mmWave MIMO systems using neural networks (NNs). Our approach involves sharing pilot resources between user equipments (UEs) and transmit antennas at the base station (BS), aiming to reduce the pilot overhead in full-duplex systems and to achieve a comparable level to that of a half-duplex system. Additionally, in the case of separate antenna configurations in a full-duplex BS, providing channel estimates of transmit antenna (TX) arrays to the downlink UEs poses another challenge, as the TX arrays are not capable of receiving pilot signals. To address this, we employ an NN to map the channel from the downlink UEs to the receive antenna (RX) arrays to the channel from the TX arrays to the downlink UEs. We further elaborate on how NNs perform the estimation with different architectures, (e.g., different numbers of hidden layers), the introduction of non-linear distortion (e.g., with a 1-bit analog-to-digital converter (ADC)), and different channel conditions (e.g., low-correlated and high-correlated channels). Our work provides novel insights into NN-based channel estimators.",
      "authors": [
        "Mehdi Sattari",
        "Hao Guo",
        "Deniz Gündüz",
        "Ashkan Panahi",
        "Tommy Svensson"
      ],
      "published": "2024-02-06T10:48:59Z",
      "updated": "2024-06-18T12:34:43Z",
      "categories": [
        "cs.IT",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.03886v2",
      "landing_url": "https://arxiv.org/abs/2402.03886v2",
      "doi": "https://doi.org/10.1109/TMLCN.2024.3432865"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item targets full-duplex mmWave MIMO channel estimation with neural networks for wireless communications, which is not about Spoken Language Models or speech-language modeling nor related to speech interaction/overlapping dynamics; it does not meet the inclusion criteria and clearly falls under exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item targets full-duplex mmWave MIMO channel estimation with neural networks for wireless communications, which is not about Spoken Language Models or speech-language modeling nor related to speech interaction/overlapping dynamics; it does not meet the inclusion criteria and clearly falls under exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on full-duplex millimeter wave MIMO channel estimation using neural networks, which is related to wireless communication and signal processing rather than modeling or analysis of spoken language or speech-based language models. It does not address spoken language modeling, speech as a core modeling object, or issues such as cognitive parallelism, voice overlap, or real-time interaction in speech. Therefore, it does not meet the inclusion criteria centered on spoken language models or full-duplex spoken language models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on full-duplex millimeter wave MIMO channel estimation using neural networks, which is related to wireless communication and signal processing rather than modeling or analysis of spoken language or speech-based language models. It does not address spoken language modeling, speech as a core modeling object, or issues such as cognitive parallelism, voice overlap, or real-time interaction in speech. Therefore, it does not meet the inclusion criteria centered on spoken language models or full-duplex spoken language models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MusicRL: Aligning Music Generation to Human Preferences",
    "abstract": "We propose MusicRL, the first music generation system finetuned from human feedback. Appreciation of text-to-music models is particularly subjective since the concept of musicality as well as the specific intention behind a caption are user-dependent (e.g. a caption such as \"upbeat work-out music\" can map to a retro guitar solo or a techno pop beat). Not only this makes supervised training of such models challenging, but it also calls for integrating continuous human feedback in their post-deployment finetuning. MusicRL is a pretrained autoregressive MusicLM (Agostinelli et al., 2023) model of discrete audio tokens finetuned with reinforcement learning to maximise sequence-level rewards. We design reward functions related specifically to text-adherence and audio quality with the help from selected raters, and use those to finetune MusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial dataset comprising 300,000 pairwise preferences. Using Reinforcement Learning from Human Feedback (RLHF), we train MusicRL-U, the first text-to-music model that incorporates human feedback at scale. Human evaluations show that both MusicRL-R and MusicRL-U are preferred to the baseline. Ultimately, MusicRL-RU combines the two approaches and results in the best model according to human raters. Ablation studies shed light on the musical attributes influencing human preferences, indicating that text adherence and quality only account for a part of it. This underscores the prevalence of subjectivity in musical appreciation and calls for further involvement of human listeners in the finetuning of music generation models.",
    "metadata": {
      "arxiv_id": "2402.04229",
      "title": "MusicRL: Aligning Music Generation to Human Preferences",
      "summary": "We propose MusicRL, the first music generation system finetuned from human feedback. Appreciation of text-to-music models is particularly subjective since the concept of musicality as well as the specific intention behind a caption are user-dependent (e.g. a caption such as \"upbeat work-out music\" can map to a retro guitar solo or a techno pop beat). Not only this makes supervised training of such models challenging, but it also calls for integrating continuous human feedback in their post-deployment finetuning. MusicRL is a pretrained autoregressive MusicLM (Agostinelli et al., 2023) model of discrete audio tokens finetuned with reinforcement learning to maximise sequence-level rewards. We design reward functions related specifically to text-adherence and audio quality with the help from selected raters, and use those to finetune MusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial dataset comprising 300,000 pairwise preferences. Using Reinforcement Learning from Human Feedback (RLHF), we train MusicRL-U, the first text-to-music model that incorporates human feedback at scale. Human evaluations show that both MusicRL-R and MusicRL-U are preferred to the baseline. Ultimately, MusicRL-RU combines the two approaches and results in the best model according to human raters. Ablation studies shed light on the musical attributes influencing human preferences, indicating that text adherence and quality only account for a part of it. This underscores the prevalence of subjectivity in musical appreciation and calls for further involvement of human listeners in the finetuning of music generation models.",
      "authors": [
        "Geoffrey Cideron",
        "Sertan Girgin",
        "Mauro Verzetti",
        "Damien Vincent",
        "Matej Kastelic",
        "Zalán Borsos",
        "Brian McWilliams",
        "Victor Ungureanu",
        "Olivier Bachem",
        "Olivier Pietquin",
        "Matthieu Geist",
        "Léonard Hussenot",
        "Neil Zeghidour",
        "Andrea Agostinelli"
      ],
      "published": "2024-02-06T18:36:52Z",
      "updated": "2024-02-06T18:36:52Z",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.04229v1",
      "landing_url": "https://arxiv.org/abs/2402.04229v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.04229"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本文關於音樂生成系統的 RLHF 微調，與語音序列/語音模型無直接關聯，未以 Spoken Language Models 或等價概念作為核心，亦未討論對話互動或同步行為，因此不符合納入標準。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本文關於音樂生成系統的 RLHF 微調，與語音序列/語音模型無直接關聯，未以 Spoken Language Models 或等價概念作為核心，亦未討論對話互動或同步行為，因此不符合納入標準。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on music generation using reinforcement learning with human feedback, centered on audio token modeling and subjective musical preferences, but it does not discuss spoken language models, speech language modeling, full-duplex spoken interaction, or related speech interaction challenges such as cognitive parallelism or low-latency synchronization. The core research subject is music generation, not spoken language modeling or speech interaction, thus failing to meet the inclusion criteria and aligning with exclusion criteria about not covering speech language modeling or synchronous speech interactions.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on music generation using reinforcement learning with human feedback, centered on audio token modeling and subjective musical preferences, but it does not discuss spoken language models, speech language modeling, full-duplex spoken interaction, or related speech interaction challenges such as cognitive parallelism or low-latency synchronization. The core research subject is music generation, not spoken language modeling or speech interaction, thus failing to meet the inclusion criteria and aligning with exclusion criteria about not covering speech language modeling or synchronous speech interactions.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DiffsFormer: A Diffusion Transformer on Stock Factor Augmentation",
    "abstract": "Machine learning models have demonstrated remarkable efficacy and efficiency in a wide range of stock forecasting tasks. However, the inherent challenges of data scarcity, including low signal-to-noise ratio (SNR) and data homogeneity, pose significant obstacles to accurate forecasting. To address this issue, we propose a novel approach that utilizes artificial intelligence-generated samples (AIGS) to enhance the training procedures. In our work, we introduce the Diffusion Model to generate stock factors with Transformer architecture (DiffsFormer). DiffsFormer is initially trained on a large-scale source domain, incorporating conditional guidance so as to capture global joint distribution. When presented with a specific downstream task, we employ DiffsFormer to augment the training procedure by editing existing samples. This editing step allows us to control the strength of the editing process, determining the extent to which the generated data deviates from the target domain. To evaluate the effectiveness of DiffsFormer augmented training, we conduct experiments on the CSI300 and CSI800 datasets, employing eight commonly used machine learning models. The proposed method achieves relative improvements of 7.2% and 27.8% in annualized return ratio for the respective datasets. Furthermore, we perform extensive experiments to gain insights into the functionality of DiffsFormer and its constituent components, elucidating how they address the challenges of data scarcity and enhance the overall model performance. Our research demonstrates the efficacy of leveraging AIGS and the DiffsFormer architecture to mitigate data scarcity in stock forecasting tasks.",
    "metadata": {
      "arxiv_id": "2402.06656",
      "title": "DiffsFormer: A Diffusion Transformer on Stock Factor Augmentation",
      "summary": "Machine learning models have demonstrated remarkable efficacy and efficiency in a wide range of stock forecasting tasks. However, the inherent challenges of data scarcity, including low signal-to-noise ratio (SNR) and data homogeneity, pose significant obstacles to accurate forecasting. To address this issue, we propose a novel approach that utilizes artificial intelligence-generated samples (AIGS) to enhance the training procedures. In our work, we introduce the Diffusion Model to generate stock factors with Transformer architecture (DiffsFormer). DiffsFormer is initially trained on a large-scale source domain, incorporating conditional guidance so as to capture global joint distribution. When presented with a specific downstream task, we employ DiffsFormer to augment the training procedure by editing existing samples. This editing step allows us to control the strength of the editing process, determining the extent to which the generated data deviates from the target domain. To evaluate the effectiveness of DiffsFormer augmented training, we conduct experiments on the CSI300 and CSI800 datasets, employing eight commonly used machine learning models. The proposed method achieves relative improvements of 7.2% and 27.8% in annualized return ratio for the respective datasets. Furthermore, we perform extensive experiments to gain insights into the functionality of DiffsFormer and its constituent components, elucidating how they address the challenges of data scarcity and enhance the overall model performance. Our research demonstrates the efficacy of leveraging AIGS and the DiffsFormer architecture to mitigate data scarcity in stock forecasting tasks.",
      "authors": [
        "Yuan Gao",
        "Haokun Chen",
        "Xiang Wang",
        "Zhicai Wang",
        "Xue Wang",
        "Jinyang Gao",
        "Bolin Ding"
      ],
      "published": "2024-02-05T03:54:36Z",
      "updated": "2024-02-05T03:54:36Z",
      "categories": [
        "q-fin.ST",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.06656v1",
      "landing_url": "https://arxiv.org/abs/2402.06656v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.06656"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本研究聚焦股市因子生成與 diffusion/transformer 在金融預測上的資料擴增，未涉及 Spoken Language Models、語音建模或任何同步互動與語音評估，因此不符合包含條件，且屬於排除範疇。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本研究聚焦股市因子生成與 diffusion/transformer 在金融預測上的資料擴增，未涉及 Spoken Language Models、語音建模或任何同步互動與語音評估，因此不符合包含條件，且屬於排除範疇。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on machine learning for stock forecasting using diffusion transformers and augmenting training data, with no indication of spoken language models, speech language modeling, or full-duplex spoken interaction features as required by the inclusion criteria; thus, it does not address the core topics of the inclusion criteria and falls outside the scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on machine learning for stock forecasting using diffusion transformers and augmenting training data, with no indication of spoken language models, speech language modeling, or full-duplex spoken interaction features as required by the inclusion criteria; thus, it does not address the core topics of the inclusion criteria and falls outside the scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "BiasEye: A Bias-Aware Real-time Interactive Material Screening System for Impartial Candidate Assessment",
    "abstract": "In the process of evaluating competencies for job or student recruitment through material screening, decision-makers can be influenced by inherent cognitive biases, such as the screening order or anchoring information, leading to inconsistent outcomes. To tackle this challenge, we conducted interviews with seven experts to understand their challenges and needs for support in the screening process. Building on their insights, we introduce BiasEye, a bias-aware real-time interactive material screening visualization system. BiasEye enhances awareness of cognitive biases by improving information accessibility and transparency. It also aids users in identifying and mitigating biases through a machine learning (ML) approach that models individual screening preferences. Findings from a mixed-design user study with 20 participants demonstrate that, compared to a baseline system lacking our bias-aware features, BiasEye increases participants' bias awareness and boosts their confidence in making final decisions. At last, we discuss the potential of ML and visualization in mitigating biases during human decision-making tasks.",
    "metadata": {
      "arxiv_id": "2402.09148",
      "title": "BiasEye: A Bias-Aware Real-time Interactive Material Screening System for Impartial Candidate Assessment",
      "summary": "In the process of evaluating competencies for job or student recruitment through material screening, decision-makers can be influenced by inherent cognitive biases, such as the screening order or anchoring information, leading to inconsistent outcomes. To tackle this challenge, we conducted interviews with seven experts to understand their challenges and needs for support in the screening process. Building on their insights, we introduce BiasEye, a bias-aware real-time interactive material screening visualization system. BiasEye enhances awareness of cognitive biases by improving information accessibility and transparency. It also aids users in identifying and mitigating biases through a machine learning (ML) approach that models individual screening preferences. Findings from a mixed-design user study with 20 participants demonstrate that, compared to a baseline system lacking our bias-aware features, BiasEye increases participants' bias awareness and boosts their confidence in making final decisions. At last, we discuss the potential of ML and visualization in mitigating biases during human decision-making tasks.",
      "authors": [
        "Qianyu Liu",
        "Haoran Jiang",
        "Zihao Pan",
        "Qiushi Han",
        "Zhenhui Peng",
        "Quan Li"
      ],
      "published": "2024-02-14T12:57:26Z",
      "updated": "2024-02-14T12:57:26Z",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.09148v1",
      "landing_url": "https://arxiv.org/abs/2402.09148v1",
      "doi": "https://doi.org/10.1145/3640543.3645166"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item does not center on Spoken Language Models or any form of speech-language modeling; it focuses on bias-aware decision support and visualization for material screening in recruitment, with no mention of speech data, model architectures for SLMs/FD-SLMs, or related evaluation metrics, thus it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item does not center on Spoken Language Models or any form of speech-language modeling; it focuses on bias-aware decision support and visualization for material screening in recruitment, with no mention of speech data, model architectures for SLMs/FD-SLMs, or related evaluation metrics, thus it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study described focuses on a bias-aware real-time interactive system for material screening in recruitment, involving visualization and machine learning for bias mitigation. However, it does not address Spoken Language Models (SLMs), full-duplex spoken language modeling, speech as the main modeling object, or any relevant training/evaluation of speech or language modeling architectures as required by the inclusion criteria. It also lacks mention of speech language modeling, real-time speech interaction, or evaluation related to speech turn-taking, interruption, or low latency. Therefore, it does not meet the inclusion criteria and falls outside the research scope defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study described focuses on a bias-aware real-time interactive system for material screening in recruitment, involving visualization and machine learning for bias mitigation. However, it does not address Spoken Language Models (SLMs), full-duplex spoken language modeling, speech as the main modeling object, or any relevant training/evaluation of speech or language modeling architectures as required by the inclusion criteria. It also lacks mention of speech language modeling, real-time speech interaction, or evaluation related to speech turn-taking, interruption, or low latency. Therefore, it does not meet the inclusion criteria and falls outside the research scope defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "iMove: Exploring Bio-impedance Sensing for Fitness Activity Recognition",
    "abstract": "Automatic and precise fitness activity recognition can be beneficial in aspects from promoting a healthy lifestyle to personalized preventative healthcare. While IMUs are currently the prominent fitness tracking modality, through iMove, we show bio-impedence can help improve IMU-based fitness tracking through sensor fusion and contrastive learning.To evaluate our methods, we conducted an experiment including six upper body fitness activities performed by ten subjects over five days to collect synchronized data from bio-impedance across two wrists and IMU on the left wrist.The contrastive learning framework uses the two modalities to train a better IMU-only classification model, where bio-impedance is only required at the training phase, by which the average Macro F1 score with the input of a single IMU was improved by 3.22 \\% reaching 84.71 \\% compared to the 81.49 \\% of the IMU baseline model. We have also shown how bio-impedance can improve human activity recognition (HAR) directly through sensor fusion, reaching an average Macro F1 score of 89.57 \\% (two modalities required for both training and inference) even if Bio-impedance alone has an average macro F1 score of 75.36 \\%, which is outperformed by IMU alone. In addition, similar results were obtained in an extended study on lower body fitness activity classification, demonstrating the generalisability of our approach.Our findings underscore the potential of sensor fusion and contrastive learning as valuable tools for advancing fitness activity recognition, with bio-impedance playing a pivotal role in augmenting the capabilities of IMU-based systems.",
    "metadata": {
      "arxiv_id": "2402.09445",
      "title": "iMove: Exploring Bio-impedance Sensing for Fitness Activity Recognition",
      "summary": "Automatic and precise fitness activity recognition can be beneficial in aspects from promoting a healthy lifestyle to personalized preventative healthcare. While IMUs are currently the prominent fitness tracking modality, through iMove, we show bio-impedence can help improve IMU-based fitness tracking through sensor fusion and contrastive learning.To evaluate our methods, we conducted an experiment including six upper body fitness activities performed by ten subjects over five days to collect synchronized data from bio-impedance across two wrists and IMU on the left wrist.The contrastive learning framework uses the two modalities to train a better IMU-only classification model, where bio-impedance is only required at the training phase, by which the average Macro F1 score with the input of a single IMU was improved by 3.22 \\% reaching 84.71 \\% compared to the 81.49 \\% of the IMU baseline model. We have also shown how bio-impedance can improve human activity recognition (HAR) directly through sensor fusion, reaching an average Macro F1 score of 89.57 \\% (two modalities required for both training and inference) even if Bio-impedance alone has an average macro F1 score of 75.36 \\%, which is outperformed by IMU alone. In addition, similar results were obtained in an extended study on lower body fitness activity classification, demonstrating the generalisability of our approach.Our findings underscore the potential of sensor fusion and contrastive learning as valuable tools for advancing fitness activity recognition, with bio-impedance playing a pivotal role in augmenting the capabilities of IMU-based systems.",
      "authors": [
        "Mengxi Liu",
        "Vitor Fortes Rey",
        "Yu Zhang",
        "Lala Shakti Swarup Ray",
        "Bo Zhou",
        "Paul Lukowicz"
      ],
      "published": "2024-01-31T16:53:50Z",
      "updated": "2024-06-03T12:42:50Z",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.09445v2",
      "landing_url": "https://arxiv.org/abs/2402.09445v2",
      "doi": "https://doi.org/10.1109/PerCom59722.2024.10494489"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on bio-impedance sensing for fitness activity recognition and sensor fusion, not on Spoken Language Models or speech-language modeling concepts; it does not address language modeling, speech, dialogue, or related evaluation metrics, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on bio-impedance sensing for fitness activity recognition and sensor fusion, not on Spoken Language Models or speech-language modeling concepts; it does not address language modeling, speech, dialogue, or related evaluation metrics, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on bio-impedance and IMU-based fitness activity recognition, with no mention of Spoken Language Models, speech as the core modeling object, full-duplex speech interaction, or related synchronization and evaluation concepts outlined in the inclusion criteria; thus, it does not meet the thematic requirements and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on bio-impedance and IMU-based fitness activity recognition, with no mention of Spoken Language Models, speech as the core modeling object, full-duplex speech interaction, or related synchronization and evaluation concepts outlined in the inclusion criteria; thus, it does not meet the thematic requirements and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Learning Causal Domain-Invariant Temporal Dynamics for Few-Shot Action Recognition",
    "abstract": "Few-shot action recognition aims at quickly adapting a pre-trained model to the novel data with a distribution shift using only a limited number of samples. Key challenges include how to identify and leverage the transferable knowledge learned by the pre-trained model. We therefore propose CDTD, or Causal Domain-Invariant Temporal Dynamics for knowledge transfer. To identify the temporally invariant and variant representations, we employ the causal representation learning methods for unsupervised pertaining, and then tune the classifier with supervisions in next stage. Specifically, we assume the domain information can be well estimated and the pre-trained image decoder and transition models can be well transferred. During adaptation, we fix the transferable temporal dynamics and update the image encoder and domain estimator. The efficacy of our approach is revealed by the superior accuracy of CDTD over leading alternatives across standard few-shot action recognition datasets.",
    "metadata": {
      "arxiv_id": "2402.12706",
      "title": "Learning Causal Domain-Invariant Temporal Dynamics for Few-Shot Action Recognition",
      "summary": "Few-shot action recognition aims at quickly adapting a pre-trained model to the novel data with a distribution shift using only a limited number of samples. Key challenges include how to identify and leverage the transferable knowledge learned by the pre-trained model. We therefore propose CDTD, or Causal Domain-Invariant Temporal Dynamics for knowledge transfer. To identify the temporally invariant and variant representations, we employ the causal representation learning methods for unsupervised pertaining, and then tune the classifier with supervisions in next stage. Specifically, we assume the domain information can be well estimated and the pre-trained image decoder and transition models can be well transferred. During adaptation, we fix the transferable temporal dynamics and update the image encoder and domain estimator. The efficacy of our approach is revealed by the superior accuracy of CDTD over leading alternatives across standard few-shot action recognition datasets.",
      "authors": [
        "Yuke Li",
        "Guangyi Chen",
        "Ben Abramowitz",
        "Stefano Anzellott",
        "Donglai Wei"
      ],
      "published": "2024-02-20T04:09:58Z",
      "updated": "2024-06-04T18:08:31Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.12706v2",
      "landing_url": "https://arxiv.org/abs/2402.12706v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.12706"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on few-shot action recognition from video using causal domain-invariant temporal dynamics, not on Spoken Language Models or speech-language modeling, so it does not meet inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on few-shot action recognition from video using causal domain-invariant temporal dynamics, not on Spoken Language Models or speech-language modeling, so it does not meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on few-shot action recognition using causal domain-invariant temporal dynamics with no mention of spoken language models, speech language modeling, or full-duplex spoken interaction capabilities. It does not address speech or acoustic signal modeling, synchronization, or interactive conversational behaviors as required by the inclusion criteria; hence it does not meet the essential criteria of focusing on spoken language models or speech modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on few-shot action recognition using causal domain-invariant temporal dynamics with no mention of spoken language models, speech language modeling, or full-duplex spoken interaction capabilities. It does not address speech or acoustic signal modeling, synchronization, or interactive conversational behaviors as required by the inclusion criteria; hence it does not meet the essential criteria of focusing on spoken language models or speech modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "GDTM: An Indoor Geospatial Tracking Dataset with Distributed Multimodal Sensors",
    "abstract": "Constantly locating moving objects, i.e., geospatial tracking, is essential for autonomous building infrastructure. Accurate and robust geospatial tracking often leverages multimodal sensor fusion algorithms, which require large datasets with time-aligned, synchronized data from various sensor types. However, such datasets are not readily available. Hence, we propose GDTM, a nine-hour dataset for multimodal object tracking with distributed multimodal sensors and reconfigurable sensor node placements. Our dataset enables the exploration of several research problems, such as optimizing architectures for processing multimodal data, and investigating models' robustness to adverse sensing conditions and sensor placement variances. A GitHub repository containing the code, sample data, and checkpoints of this work is available at https://github.com/nesl/GDTM.",
    "metadata": {
      "arxiv_id": "2402.14136",
      "title": "GDTM: An Indoor Geospatial Tracking Dataset with Distributed Multimodal Sensors",
      "summary": "Constantly locating moving objects, i.e., geospatial tracking, is essential for autonomous building infrastructure. Accurate and robust geospatial tracking often leverages multimodal sensor fusion algorithms, which require large datasets with time-aligned, synchronized data from various sensor types. However, such datasets are not readily available. Hence, we propose GDTM, a nine-hour dataset for multimodal object tracking with distributed multimodal sensors and reconfigurable sensor node placements. Our dataset enables the exploration of several research problems, such as optimizing architectures for processing multimodal data, and investigating models' robustness to adverse sensing conditions and sensor placement variances. A GitHub repository containing the code, sample data, and checkpoints of this work is available at https://github.com/nesl/GDTM.",
      "authors": [
        "Ho Lyun Jeong",
        "Ziqi Wang",
        "Colin Samplawski",
        "Jason Wu",
        "Shiwei Fang",
        "Lance M. Kaplan",
        "Deepak Ganesan",
        "Benjamin Marlin",
        "Mani Srivastava"
      ],
      "published": "2024-02-21T21:24:57Z",
      "updated": "2024-02-21T21:24:57Z",
      "categories": [
        "cs.RO",
        "cs.LG",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.14136v1",
      "landing_url": "https://arxiv.org/abs/2402.14136v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.14136"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about an indoor geospatial tracking dataset with multimodal sensors and sensor placement, not about Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria for SLM/FD-SLM focus and related evaluation framework.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about an indoor geospatial tracking dataset with multimodal sensors and sensor placement, not about Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria for SLM/FD-SLM focus and related evaluation framework.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an indoor geospatial tracking dataset with multimodal sensors but does not mention Spoken Language Models, speech language modeling, or related core concepts detailed in the inclusion criteria. The study does not discuss speech as the main modeling target, nor does it address full-duplex spoken language modeling or synchronous speech interactions. Therefore, it fails to meet the inclusion criteria and is irrelevant to the research domain specified.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an indoor geospatial tracking dataset with multimodal sensors but does not mention Spoken Language Models, speech language modeling, or related core concepts detailed in the inclusion criteria. The study does not discuss speech as the main modeling target, nor does it address full-duplex spoken language modeling or synchronous speech interactions. Therefore, it fails to meet the inclusion criteria and is irrelevant to the research domain specified.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Topic-to-essay generation with knowledge-based content selection",
    "abstract": "The topic-to-essay generation task is a challenging natural language generation task that aims to generate paragraph-level text with high semantic coherence based on a given set of topic words. Previous work has focused on the introduction of external knowledge, ignoring the insufficient generated text diversity. In order to improve the generation diversity, we propose a novel copy mechanism model with a content selection module that integrates rich semantic knowledge from the language model into the decoder. Furthermore, we introduce the improved prefix tuning method to train the model, enabling it to adapt to varying input complexities. In addition, we have contributed a new Chinese dataset for TEG tasks. Experimental results demonstrate that the proposed model can improve the generated text diversity by 35\\% to 59\\% compared to the state-of-the-art method, while maintaining a high level of topic consistency.",
    "metadata": {
      "arxiv_id": "2402.16248",
      "title": "Topic-to-essay generation with knowledge-based content selection",
      "summary": "The topic-to-essay generation task is a challenging natural language generation task that aims to generate paragraph-level text with high semantic coherence based on a given set of topic words. Previous work has focused on the introduction of external knowledge, ignoring the insufficient generated text diversity. In order to improve the generation diversity, we propose a novel copy mechanism model with a content selection module that integrates rich semantic knowledge from the language model into the decoder. Furthermore, we introduce the improved prefix tuning method to train the model, enabling it to adapt to varying input complexities. In addition, we have contributed a new Chinese dataset for TEG tasks. Experimental results demonstrate that the proposed model can improve the generated text diversity by 35\\% to 59\\% compared to the state-of-the-art method, while maintaining a high level of topic consistency.",
      "authors": [
        "Jieyong Wang",
        "Chunyao Song",
        "Yihao Wu"
      ],
      "published": "2024-02-26T02:14:42Z",
      "updated": "2024-02-26T02:14:42Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.16248v1",
      "landing_url": "https://arxiv.org/abs/2402.16248v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.16248"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on text-only topic-to-essay generation with knowledge-based content selection and novel tuning/data; it does not treat Spoken Language Models or speech-language modeling as the core object, nor discuss real-time speech interaction or multimodal speech phenomena.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on text-only topic-to-essay generation with knowledge-based content selection and novel tuning/data; it does not treat Spoken Language Models or speech-language modeling as the core object, nor discuss real-time speech interaction or multimodal speech phenomena.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on topic-to-essay generation using a natural language generation approach that operates on topic words and text, with no mention of spoken language modeling, speech-level modeling, or full-duplex spoken language models. It does not discuss speech as a core modeling object or synchronization/interactivity aspects relevant to speech-based models. Therefore, it does not meet the inclusion criteria and aligns with exclusion criteria focusing on text rather than speech modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on topic-to-essay generation using a natural language generation approach that operates on topic words and text, with no mention of spoken language modeling, speech-level modeling, or full-duplex spoken language models. It does not discuss speech as a core modeling object or synchronization/interactivity aspects relevant to speech-based models. Therefore, it does not meet the inclusion criteria and aligns with exclusion criteria focusing on text rather than speech modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models",
    "abstract": "Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans. Achieving both the detectability of inserted watermarks and the semantic quality of generated texts is challenging. While current watermarking algorithms have made promising progress in this direction, there remains significant scope for improvement. To address these challenges, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that our method outperforms current watermarking techniques in enhancing the detectability of texts generated by LLMs while maintaining their semantic coherence. Our code is available at https://github.com/mignonjia/TS_watermark.",
    "metadata": {
      "arxiv_id": "2402.18059",
      "title": "Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models",
      "summary": "Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans. Achieving both the detectability of inserted watermarks and the semantic quality of generated texts is challenging. While current watermarking algorithms have made promising progress in this direction, there remains significant scope for improvement. To address these challenges, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that our method outperforms current watermarking techniques in enhancing the detectability of texts generated by LLMs while maintaining their semantic coherence. Our code is available at https://github.com/mignonjia/TS_watermark.",
      "authors": [
        "Mingjia Huo",
        "Sai Ashish Somayajula",
        "Youwei Liang",
        "Ruisi Zhang",
        "Farinaz Koushanfar",
        "Pengtao Xie"
      ],
      "published": "2024-02-28T05:43:22Z",
      "updated": "2024-06-06T04:49:37Z",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.CR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.18059v3",
      "landing_url": "https://arxiv.org/abs/2402.18059v3",
      "doi": "https://doi.org/10.48550/arXiv.2402.18059"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item studies watermarking for large language models in text, with no focus on Spoken Language Models or speech-based modeling/interactions, failing the inclusion criteria for SLM/FD-SLM-centered research.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item studies watermarking for large language models in text, with no focus on Spoken Language Models or speech-based modeling/interactions, failing the inclusion criteria for SLM/FD-SLM-centered research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on watermarking techniques for large language model text outputs, without discussing spoken language models, speech-level language modeling, or full-duplex spoken interaction, which are core criteria of inclusion; it neither addresses speech signals nor spoken language modeling, so it does not meet any of the inclusion requirements and falls outside the scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on watermarking techniques for large language model text outputs, without discussing spoken language models, speech-level language modeling, or full-duplex spoken interaction, which are core criteria of inclusion; it neither addresses speech signals nor spoken language modeling, so it does not meet any of the inclusion requirements and falls outside the scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Saving the legacy of Hero Ibash: Evaluating Four Language Models for Aminoacian",
    "abstract": "This study assesses four cutting-edge language models in the underexplored Aminoacian language. Through evaluation, it scrutinizes their adaptability, effectiveness, and limitations in text generation, semantic coherence, and contextual understanding. Uncovering insights into these models' performance in a low-resourced language, this research pioneers pathways to bridge linguistic gaps. By offering benchmarks and understanding challenges, it lays groundwork for future advancements in natural language processing, aiming to elevate the applicability of language models in similar linguistic landscapes, marking a significant step toward inclusivity and progress in language technology.",
    "metadata": {
      "arxiv_id": "2402.18121",
      "title": "Saving the legacy of Hero Ibash: Evaluating Four Language Models for Aminoacian",
      "summary": "This study assesses four cutting-edge language models in the underexplored Aminoacian language. Through evaluation, it scrutinizes their adaptability, effectiveness, and limitations in text generation, semantic coherence, and contextual understanding. Uncovering insights into these models' performance in a low-resourced language, this research pioneers pathways to bridge linguistic gaps. By offering benchmarks and understanding challenges, it lays groundwork for future advancements in natural language processing, aiming to elevate the applicability of language models in similar linguistic landscapes, marking a significant step toward inclusivity and progress in language technology.",
      "authors": [
        "Yunze Xiao",
        "Yiyang Pan"
      ],
      "published": "2024-02-28T07:22:13Z",
      "updated": "2024-02-28T07:22:13Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.18121v1",
      "landing_url": "https://arxiv.org/abs/2402.18121v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.18121"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item analyzes general text-language models for a low-resource language with no focus on spoken language modeling or audio/speech components, so it fails the Spoken Language Model criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item analyzes general text-language models for a low-resource language with no focus on spoken language modeling or audio/speech components, so it fails the Spoken Language Model criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study evaluates language models focusing on an underexplored language at the text generation and semantic level, without mention of speech or spoken language models, speech as a primary modeling object, full-duplex interaction, or relevant evaluation metrics as required by the inclusion criteria; thus, it does not meet the essential criteria for spoken language modeling or speech-based evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study evaluates language models focusing on an underexplored language at the text generation and semantic level, without mention of speech or spoken language models, speech as a primary modeling object, full-duplex interaction, or relevant evaluation metrics as required by the inclusion criteria; thus, it does not meet the essential criteria for spoken language modeling or speech-based evaluation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Enhancing Steganographic Text Extraction: Evaluating the Impact of NLP Models on Accuracy and Semantic Coherence",
    "abstract": "This study discusses a new method combining image steganography technology with Natural Language Processing (NLP) large models, aimed at improving the accuracy and robustness of extracting steganographic text. Traditional Least Significant Bit (LSB) steganography techniques face challenges in accuracy and robustness of information extraction when dealing with complex character encoding, such as Chinese characters. To address this issue, this study proposes an innovative LSB-NLP hybrid framework. This framework integrates the advanced capabilities of NLP large models, such as error detection, correction, and semantic consistency analysis, as well as information reconstruction techniques, thereby significantly enhancing the robustness of steganographic text extraction. Experimental results show that the LSB-NLP hybrid framework excels in improving the extraction accuracy of steganographic text, especially in handling Chinese characters. The findings of this study not only confirm the effectiveness of combining image steganography technology and NLP large models but also propose new ideas for research and application in the field of information hiding. The successful implementation of this interdisciplinary approach demonstrates the great potential of integrating image steganography technology with natural language processing technology in solving complex information processing problems.",
    "metadata": {
      "arxiv_id": "2402.18849",
      "title": "Enhancing Steganographic Text Extraction: Evaluating the Impact of NLP Models on Accuracy and Semantic Coherence",
      "summary": "This study discusses a new method combining image steganography technology with Natural Language Processing (NLP) large models, aimed at improving the accuracy and robustness of extracting steganographic text. Traditional Least Significant Bit (LSB) steganography techniques face challenges in accuracy and robustness of information extraction when dealing with complex character encoding, such as Chinese characters. To address this issue, this study proposes an innovative LSB-NLP hybrid framework. This framework integrates the advanced capabilities of NLP large models, such as error detection, correction, and semantic consistency analysis, as well as information reconstruction techniques, thereby significantly enhancing the robustness of steganographic text extraction. Experimental results show that the LSB-NLP hybrid framework excels in improving the extraction accuracy of steganographic text, especially in handling Chinese characters. The findings of this study not only confirm the effectiveness of combining image steganography technology and NLP large models but also propose new ideas for research and application in the field of information hiding. The successful implementation of this interdisciplinary approach demonstrates the great potential of integrating image steganography technology with natural language processing technology in solving complex information processing problems.",
      "authors": [
        "Mingyang Li",
        "Maoqin Yuan",
        "Luyao Li",
        "Han Pengsihua"
      ],
      "published": "2024-02-29T04:53:06Z",
      "updated": "2024-02-29T04:53:06Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.18849v1",
      "landing_url": "https://arxiv.org/abs/2402.18849v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.18849"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on image steganography and NLP-based text extraction, with no focus on Spoken Language Models or speech-language modeling; it does not meet inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on image steganography and NLP-based text extraction, with no focus on Spoken Language Models or speech-language modeling; it does not meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on combining image steganography with NLP large models for text extraction accuracy, with no mention of spoken language models, speech modeling, or real-time interactive speech systems, thus it does not meet the inclusion criteria centered on spoken language models and speech-based architectures.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on combining image steganography with NLP large models for text extraction accuracy, with no mention of spoken language models, speech modeling, or real-time interactive speech systems, thus it does not meet the inclusion criteria centered on spoken language models and speech-based architectures.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "HoloVIC: Large-scale Dataset and Benchmark for Multi-Sensor Holographic Intersection and Vehicle-Infrastructure Cooperative",
    "abstract": "Vehicle-to-everything (V2X) is a popular topic in the field of Autonomous Driving in recent years. Vehicle-infrastructure cooperation (VIC) becomes one of the important research area. Due to the complexity of traffic conditions such as blind spots and occlusion, it greatly limits the perception capabilities of single-view roadside sensing systems. To further enhance the accuracy of roadside perception and provide better information to the vehicle side, in this paper, we constructed holographic intersections with various layouts to build a large-scale multi-sensor holographic vehicle-infrastructure cooperation dataset, called HoloVIC. Our dataset includes 3 different types of sensors (Camera, Lidar, Fisheye) and employs 4 sensor-layouts based on the different intersections. Each intersection is equipped with 6-18 sensors to capture synchronous data. While autonomous vehicles pass through these intersections for collecting VIC data. HoloVIC contains in total on 100k+ synchronous frames from different sensors. Additionally, we annotated 3D bounding boxes based on Camera, Fisheye, and Lidar. We also associate the IDs of the same objects across different devices and consecutive frames in sequence. Based on HoloVIC, we formulated four tasks to facilitate the development of related research. We also provide benchmarks for these tasks.",
    "metadata": {
      "arxiv_id": "2403.02640",
      "title": "HoloVIC: Large-scale Dataset and Benchmark for Multi-Sensor Holographic Intersection and Vehicle-Infrastructure Cooperative",
      "summary": "Vehicle-to-everything (V2X) is a popular topic in the field of Autonomous Driving in recent years. Vehicle-infrastructure cooperation (VIC) becomes one of the important research area. Due to the complexity of traffic conditions such as blind spots and occlusion, it greatly limits the perception capabilities of single-view roadside sensing systems. To further enhance the accuracy of roadside perception and provide better information to the vehicle side, in this paper, we constructed holographic intersections with various layouts to build a large-scale multi-sensor holographic vehicle-infrastructure cooperation dataset, called HoloVIC. Our dataset includes 3 different types of sensors (Camera, Lidar, Fisheye) and employs 4 sensor-layouts based on the different intersections. Each intersection is equipped with 6-18 sensors to capture synchronous data. While autonomous vehicles pass through these intersections for collecting VIC data. HoloVIC contains in total on 100k+ synchronous frames from different sensors. Additionally, we annotated 3D bounding boxes based on Camera, Fisheye, and Lidar. We also associate the IDs of the same objects across different devices and consecutive frames in sequence. Based on HoloVIC, we formulated four tasks to facilitate the development of related research. We also provide benchmarks for these tasks.",
      "authors": [
        "Cong Ma",
        "Lei Qiao",
        "Chengkai Zhu",
        "Kai Liu",
        "Zelong Kong",
        "Qing Li",
        "Xueqi Zhou",
        "Yuheng Kan",
        "Wei Wu"
      ],
      "published": "2024-03-05T04:08:19Z",
      "updated": "2024-03-26T17:14:14Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.02640v3",
      "landing_url": "https://arxiv.org/abs/2403.02640v3",
      "doi": "https://doi.org/10.48550/arXiv.2403.02640"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on a multi-sensor holographic V2X dataset for vehicle-infrastructure collaboration and does not involve Spoken Language Models or speech-language modeling, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on a multi-sensor holographic V2X dataset for vehicle-infrastructure collaboration and does not involve Spoken Language Models or speech-language modeling, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on Vehicle-Infrastructure Cooperative datasets and sensors for autonomous driving, which is unrelated to Spoken Language Models, speech language modeling, or full-duplex spoken interaction as defined in the inclusion criteria. It does not address voice as the core modeling object nor any relevant synchronization or interactive speech modeling concepts. Therefore, it fails to meet the core subject requirements and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on Vehicle-Infrastructure Cooperative datasets and sensors for autonomous driving, which is unrelated to Spoken Language Models, speech language modeling, or full-duplex spoken interaction as defined in the inclusion criteria. It does not address voice as the core modeling object nor any relevant synchronization or interactive speech modeling concepts. Therefore, it fails to meet the core subject requirements and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models",
    "abstract": "While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can effectively and efficiently model intricate speech with disentangled subspaces in a divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the state-of-the-art TTS systems on quality, similarity, prosody, and intelligibility, and achieves on-par quality with human recordings. Furthermore, we achieve better performance by scaling to 1B parameters and 200K hours of training data.",
    "metadata": {
      "arxiv_id": "2403.03100",
      "title": "NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models",
      "summary": "While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can effectively and efficiently model intricate speech with disentangled subspaces in a divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the state-of-the-art TTS systems on quality, similarity, prosody, and intelligibility, and achieves on-par quality with human recordings. Furthermore, we achieve better performance by scaling to 1B parameters and 200K hours of training data.",
      "authors": [
        "Zeqian Ju",
        "Yuancheng Wang",
        "Kai Shen",
        "Xu Tan",
        "Detai Xin",
        "Dongchao Yang",
        "Yanqing Liu",
        "Yichong Leng",
        "Kaitao Song",
        "Siliang Tang",
        "Zhizheng Wu",
        "Tao Qin",
        "Xiang-Yang Li",
        "Wei Ye",
        "Shikun Zhang",
        "Jiang Bian",
        "Lei He",
        "Jinyu Li",
        "Sheng Zhao"
      ],
      "published": "2024-03-05T16:35:25Z",
      "updated": "2024-04-23T08:38:03Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.03100v3",
      "landing_url": "https://arxiv.org/abs/2403.03100v3",
      "doi": "https://doi.org/10.48550/arXiv.2403.03100"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item presents a factorized diffusion-based TTS system, but it does not frame speech as a Spoken Language Model (SLM) or equivalent core concept in the title/abstract, so it fails the core inclusion criterion despite recency.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item presents a factorized diffusion-based TTS system, but it does not frame speech as a Spoken Language Model (SLM) or equivalent core concept in the title/abstract, so it fails the core inclusion criterion despite recency.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a text-to-speech (TTS) system that factorizes speech attributes for improved synthesis quality, but it does not discuss spoken language models (SLMs) or full-duplex spoken language modeling, nor does it address interactive or synchronous speech behaviors such as turn-taking, backchanneling, or low-latency interaction; therefore, it does not meet the inclusion criteria and aligns with several exclusion criteria targeting single-task TTS without speech language modeling or interactive behavior arbitration discussion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a text-to-speech (TTS) system that factorizes speech attributes for improved synthesis quality, but it does not discuss spoken language models (SLMs) or full-duplex spoken language modeling, nor does it address interactive or synchronous speech behaviors such as turn-taking, backchanneling, or low-latency interaction; therefore, it does not meet the inclusion criteria and aligns with several exclusion criteria targeting single-task TTS without speech language modeling or interactive behavior arbitration discussion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Multilingual Turn-taking Prediction Using Voice Activity Projection",
    "abstract": "This paper investigates the application of voice activity projection (VAP), a predictive turn-taking model for spoken dialogue, on multilingual data, encompassing English, Mandarin, and Japanese. The VAP model continuously predicts the upcoming voice activities of participants in dyadic dialogue, leveraging a cross-attention Transformer to capture the dynamic interplay between participants. The results show that a monolingual VAP model trained on one language does not make good predictions when applied to other languages. However, a multilingual model, trained on all three languages, demonstrates predictive performance on par with monolingual models across all languages. Further analyses show that the multilingual model has learned to discern the language of the input signal. We also analyze the sensitivity to pitch, a prosodic cue that is thought to be important for turn-taking. Finally, we compare two different audio encoders, contrastive predictive coding (CPC) pre-trained on English, with a recent model based on multilingual wav2vec 2.0 (MMS).",
    "metadata": {
      "arxiv_id": "2403.06487",
      "title": "Multilingual Turn-taking Prediction Using Voice Activity Projection",
      "summary": "This paper investigates the application of voice activity projection (VAP), a predictive turn-taking model for spoken dialogue, on multilingual data, encompassing English, Mandarin, and Japanese. The VAP model continuously predicts the upcoming voice activities of participants in dyadic dialogue, leveraging a cross-attention Transformer to capture the dynamic interplay between participants. The results show that a monolingual VAP model trained on one language does not make good predictions when applied to other languages. However, a multilingual model, trained on all three languages, demonstrates predictive performance on par with monolingual models across all languages. Further analyses show that the multilingual model has learned to discern the language of the input signal. We also analyze the sensitivity to pitch, a prosodic cue that is thought to be important for turn-taking. Finally, we compare two different audio encoders, contrastive predictive coding (CPC) pre-trained on English, with a recent model based on multilingual wav2vec 2.0 (MMS).",
      "authors": [
        "Koji Inoue",
        "Bing'er Jiang",
        "Erik Ekstedt",
        "Tatsuya Kawahara",
        "Gabriel Skantze"
      ],
      "published": "2024-03-11T07:50:29Z",
      "updated": "2024-03-14T23:59:59Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.06487v3",
      "landing_url": "https://arxiv.org/abs/2403.06487v3",
      "doi": "https://doi.org/10.48550/arXiv.2403.06487"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on predicting voice activity for turn-taking in multilingual dialogue, but it does not explicitly mention spoken language model or speech language model in the title/abstract, failing the explicit inclusion criterion, though it aligns with the broader topic.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper centers on predicting voice activity for turn-taking in multilingual dialogue, but it does not explicitly mention spoken language model or speech language model in the title/abstract, failing the explicit inclusion criterion, though it aligns with the broader topic.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multilingual turn-taking prediction using voice activity projection, which involves modeling speech sequences for turn-taking behavior, including cross-attention Transformers to capture dialogue dynamics, aligning with spoken language modeling and synchronization in conversation. The abstract discusses model training, multilingual performance, and pitch sensitivity, indicating modeling and evaluation aspects beyond just ASR or TTS, fitting the inclusion of speech language models for real-time interactive tasks. However, it's not explicitly stated if full-duplex capabilities or sub-200ms latency are evaluated, but the focus on turn-taking prediction and behavioral arbitration fits within the topic. Hence, the paper meets most inclusion criteria and does not fall under exclusion criteria as it does not focus solely on half-duplex turn-taking or ASR/TTS.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multilingual turn-taking prediction using voice activity projection, which involves modeling speech sequences for turn-taking behavior, including cross-attention Transformers to capture dialogue dynamics, aligning with spoken language modeling and synchronization in conversation. The abstract discusses model training, multilingual performance, and pitch sensitivity, indicating modeling and evaluation aspects beyond just ASR or TTS, fitting the inclusion of speech language models for real-time interactive tasks. However, it's not explicitly stated if full-duplex capabilities or sub-200ms latency are evaluated, but the focus on turn-taking prediction and behavioral arbitration fits within the topic. Hence, the paper meets most inclusion criteria and does not fall under exclusion criteria as it does not focus solely on half-duplex turn-taking or ASR/TTS.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted relevant points: JuniorNano flagged the absence of an explicit “spoken language model” / “speech language model” label in the title/abstract (eval=2), while JuniorMini argued the work models speech sequences for turn-taking and could be seen as speech-level modeling (eval=4). After integrating their views, I conclude the paper focuses on voice-activity projection for turn-taking (a half‑duplex predictive task) and does not explicitly frame itself as a Spoken Language Model or as addressing full‑duplex/real‑time synchronous interaction (low‑latency overlap handling, cognitive parallelism, or arbitration), so it fails the inclusion requirement for explicit SLM-equivalent description and falls under the exclusion of studies that only treat traditional turn‑taking; therefore it is better to exclude.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted relevant points: JuniorNano flagged the absence of an explicit “spoken language model” / “speech language model” label in the title/abstract (eval=2), while JuniorMini argued the work models speech sequences for turn-taking and could be seen as speech-level modeling (eval=4). After integrating their views, I conclude the paper focuses on voice-activity projection for turn-taking (a half‑duplex predictive task) and does not explicitly frame itself as a Spoken Language Model or as addressing full‑duplex/real‑time synchronous interaction (low‑latency overlap handling, cognitive parallelism, or arbitration), so it fails the inclusion requirement for explicit SLM-equivalent description and falls under the exclusion of studies that only treat traditional turn‑taking; therefore it is better to exclude.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Leveraging Internal Representations of Model for Magnetic Image Classification",
    "abstract": "Data generated by edge devices has the potential to train intelligent autonomous systems across various domains. Despite the emergence of diverse machine learning approaches addressing privacy concerns and utilizing distributed data, security issues persist due to the sensitive storage of data shards in disparate locations. This paper introduces a potentially groundbreaking paradigm for machine learning model training, specifically designed for scenarios with only a single magnetic image and its corresponding label image available. We harness the capabilities of Deep Learning to generate concise yet informative samples, aiming to overcome data scarcity. Through the utilization of deep learning's internal representations, our objective is to efficiently address data scarcity issues and produce meaningful results. This methodology presents a promising avenue for training machine learning models with minimal data.",
    "metadata": {
      "arxiv_id": "2403.06797",
      "title": "Leveraging Internal Representations of Model for Magnetic Image Classification",
      "summary": "Data generated by edge devices has the potential to train intelligent autonomous systems across various domains. Despite the emergence of diverse machine learning approaches addressing privacy concerns and utilizing distributed data, security issues persist due to the sensitive storage of data shards in disparate locations. This paper introduces a potentially groundbreaking paradigm for machine learning model training, specifically designed for scenarios with only a single magnetic image and its corresponding label image available. We harness the capabilities of Deep Learning to generate concise yet informative samples, aiming to overcome data scarcity. Through the utilization of deep learning's internal representations, our objective is to efficiently address data scarcity issues and produce meaningful results. This methodology presents a promising avenue for training machine learning models with minimal data.",
      "authors": [
        "Adarsh N L",
        "Arun P",
        "Alok Porwal",
        "Malcolm Aranha"
      ],
      "published": "2024-03-11T15:15:50Z",
      "updated": "2024-03-11T15:15:50Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.06797v1",
      "landing_url": "https://arxiv.org/abs/2403.06797v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.06797"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "該論文題名與摘要聚焦於磁性影像分類與少樣本訓練，與 Spoken Language Models 或語音語言建模無关，因此不符合包含條件，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "該論文題名與摘要聚焦於磁性影像分類與少樣本訓練，與 Spoken Language Models 或語音語言建模無关，因此不符合包含條件，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on leveraging deep learning internal representations for magnetic image classification, which does not relate to spoken language models or speech language modeling; it neither discusses speech sequences, full-duplex interaction, nor any of the inclusion criteria related to speech modeling, and thus does not meet the required topic scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on leveraging deep learning internal representations for magnetic image classification, which does not relate to spoken language models or speech language modeling; it neither discusses speech sequences, full-duplex interaction, nor any of the inclusion criteria related to speech modeling, and thus does not meet the required topic scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Empowering Sequential Recommendation from Collaborative Signals and Semantic Relatedness",
    "abstract": "Sequential recommender systems (SRS) could capture dynamic user preferences by modeling historical behaviors ordered in time. Despite effectiveness, focusing only on the \\textit{collaborative signals} from behaviors does not fully grasp user interests. It is also significant to model the \\textit{semantic relatedness} reflected in content features, e.g., images and text. Towards that end, in this paper, we aim to enhance the SRS tasks by effectively unifying collaborative signals and semantic relatedness together. Notably, we empirically point out that it is nontrivial to achieve such a goal due to semantic gap issues. Thus, we propose an end-to-end two-stream architecture for sequential recommendation, named TSSR, to learn user preferences from ID-based and content-based sequence. Specifically, we first present novel hierarchical contrasting module, including coarse user-grained and fine item-grained terms, to align the representations of inter-modality. Furthermore, we also design a two-stream architecture to learn the dependence of intra-modality sequence and the complex interactions of inter-modality sequence, which can yield more expressive capacity in understanding user interests. We conduct extensive experiments on five public datasets. The experimental results show that the TSSR could yield superior performance than competitive baselines. We also make our experimental codes publicly available at https://github.com/Mingyue-Cheng/TSSR.",
    "metadata": {
      "arxiv_id": "2403.07623",
      "title": "Empowering Sequential Recommendation from Collaborative Signals and Semantic Relatedness",
      "summary": "Sequential recommender systems (SRS) could capture dynamic user preferences by modeling historical behaviors ordered in time. Despite effectiveness, focusing only on the \\textit{collaborative signals} from behaviors does not fully grasp user interests. It is also significant to model the \\textit{semantic relatedness} reflected in content features, e.g., images and text. Towards that end, in this paper, we aim to enhance the SRS tasks by effectively unifying collaborative signals and semantic relatedness together. Notably, we empirically point out that it is nontrivial to achieve such a goal due to semantic gap issues. Thus, we propose an end-to-end two-stream architecture for sequential recommendation, named TSSR, to learn user preferences from ID-based and content-based sequence. Specifically, we first present novel hierarchical contrasting module, including coarse user-grained and fine item-grained terms, to align the representations of inter-modality. Furthermore, we also design a two-stream architecture to learn the dependence of intra-modality sequence and the complex interactions of inter-modality sequence, which can yield more expressive capacity in understanding user interests. We conduct extensive experiments on five public datasets. The experimental results show that the TSSR could yield superior performance than competitive baselines. We also make our experimental codes publicly available at https://github.com/Mingyue-Cheng/TSSR.",
      "authors": [
        "Mingyue Cheng",
        "Hao Zhang",
        "Qi Liu",
        "Fajie Yuan",
        "Zhi Li",
        "Zhenya Huang",
        "Enhong Chen",
        "Jun Zhou",
        "Longfei Li"
      ],
      "published": "2024-03-12T13:06:31Z",
      "updated": "2024-09-19T07:43:43Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.07623v2",
      "landing_url": "https://arxiv.org/abs/2403.07623v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.07623"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe a sequential recommender system using collaborative signals and semantic content, with no focus on Spoken Language Models or speech-centric modeling, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe a sequential recommender system using collaborative signals and semantic content, with no focus on Spoken Language Models or speech-centric modeling, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sequential recommendation using collaborative signals and semantic content, without addressing spoken language models or speech-level language modeling, nor does it discuss full-duplex spoken interactions, speech signal modeling, or real-time dialog synchronization; hence, it does not meet the inclusion criteria centered on spoken language modeling and associated synchronous interactive capabilities.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sequential recommendation using collaborative signals and semantic content, without addressing spoken language models or speech-level language modeling, nor does it discuss full-duplex spoken interactions, speech signal modeling, or real-time dialog synchronization; hence, it does not meet the inclusion criteria centered on spoken language modeling and associated synchronous interactive capabilities.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models",
    "abstract": "We introduce SemanticDraw, a new paradigm of interactive content creation where high-quality images are generated in near real-time from given multiple hand-drawn regions, each encoding prescribed semantic meaning. In order to maximize the productivity of content creators and to fully realize their artistic imagination, it requires both quick interactive interfaces and fine-grained regional controls in their tools. Despite astonishing generation quality from recent diffusion models, we find that existing approaches for regional controllability are very slow (52 seconds for $512 \\times 512$ image) while not compatible with acceleration methods such as LCM, blocking their huge potential in interactive content creation. From this observation, we build our solution for interactive content creation in two steps: (1) we establish compatibility between region-based controls and acceleration techniques for diffusion models, maintaining high fidelity of multi-prompt image generation with $\\times 10$ reduced number of inference steps, (2) we increase the generation throughput with our new multi-prompt stream batch pipeline, enabling low-latency generation from multiple, region-based text prompts on a single RTX 2080 Ti GPU. Our proposed framework is generalizable to any existing diffusion models and acceleration schedulers, allowing sub-second (0.64 seconds) image content creation application upon well-established image diffusion models. Our project page is: https://jaerinlee.com/research/semantic-draw",
    "metadata": {
      "arxiv_id": "2403.09055",
      "title": "SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models",
      "summary": "We introduce SemanticDraw, a new paradigm of interactive content creation where high-quality images are generated in near real-time from given multiple hand-drawn regions, each encoding prescribed semantic meaning. In order to maximize the productivity of content creators and to fully realize their artistic imagination, it requires both quick interactive interfaces and fine-grained regional controls in their tools. Despite astonishing generation quality from recent diffusion models, we find that existing approaches for regional controllability are very slow (52 seconds for $512 \\times 512$ image) while not compatible with acceleration methods such as LCM, blocking their huge potential in interactive content creation. From this observation, we build our solution for interactive content creation in two steps: (1) we establish compatibility between region-based controls and acceleration techniques for diffusion models, maintaining high fidelity of multi-prompt image generation with $\\times 10$ reduced number of inference steps, (2) we increase the generation throughput with our new multi-prompt stream batch pipeline, enabling low-latency generation from multiple, region-based text prompts on a single RTX 2080 Ti GPU. Our proposed framework is generalizable to any existing diffusion models and acceleration schedulers, allowing sub-second (0.64 seconds) image content creation application upon well-established image diffusion models. Our project page is: https://jaerinlee.com/research/semantic-draw",
      "authors": [
        "Jaerin Lee",
        "Daniel Sungho Jung",
        "Kanggeon Lee",
        "Kyoung Mu Lee"
      ],
      "published": "2024-03-14T02:51:01Z",
      "updated": "2025-06-01T14:29:54Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.09055v4",
      "landing_url": "https://arxiv.org/abs/2403.09055v4",
      "doi": "https://doi.org/10.48550/arXiv.2403.09055"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on image diffusion with region-based control for visual content creation and does not discuss Spoken Language Models or any form of speech-language modeling, hence it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on image diffusion with region-based control for visual content creation and does not discuss Spoken Language Models or any form of speech-language modeling, hence it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper titled 'SemanticDraw' centers on interactive content creation using image diffusion models, with no mention of spoken language models, speech language modeling, or related speech interaction concepts that are core to the inclusion criteria. It focuses on image generation and acceleration techniques rather than speech or full-duplex spoken language modeling, and thus fails to meet the inclusion criteria entirely.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper titled 'SemanticDraw' centers on interactive content creation using image diffusion models, with no mention of spoken language models, speech language modeling, or related speech interaction concepts that are core to the inclusion criteria. It focuses on image generation and acceleration techniques rather than speech or full-duplex spoken language modeling, and thus fails to meet the inclusion criteria entirely.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Semantic-Enhanced Representation Learning for Road Networks with Temporal Dynamics",
    "abstract": "In this study, we introduce a novel framework called Toast for learning general-purpose representations of road networks, along with its advanced counterpart DyToast, designed to enhance the integration of temporal dynamics to boost the performance of various time-sensitive downstream tasks. Specifically, we propose to encode two pivotal semantic characteristics intrinsic to road networks: traffic patterns and traveling semantics. To achieve this, we refine the skip-gram module by incorporating auxiliary objectives aimed at predicting the traffic context associated with a target road segment. Moreover, we leverage trajectory data and design pre-training strategies based on Transformer to distill traveling semantics on road networks. DyToast further augments this framework by employing unified trigonometric functions characterized by their beneficial properties, enabling the capture of temporal evolution and dynamic nature of road networks more effectively. With these proposed techniques, we can obtain representations that encode multi-faceted aspects of knowledge within road networks, applicable across both road segment-based applications and trajectory-based applications. Extensive experiments on two real-world datasets across three tasks demonstrate that our proposed framework consistently outperforms the state-of-the-art baselines by a significant margin.",
    "metadata": {
      "arxiv_id": "2403.11495",
      "title": "Semantic-Enhanced Representation Learning for Road Networks with Temporal Dynamics",
      "summary": "In this study, we introduce a novel framework called Toast for learning general-purpose representations of road networks, along with its advanced counterpart DyToast, designed to enhance the integration of temporal dynamics to boost the performance of various time-sensitive downstream tasks. Specifically, we propose to encode two pivotal semantic characteristics intrinsic to road networks: traffic patterns and traveling semantics. To achieve this, we refine the skip-gram module by incorporating auxiliary objectives aimed at predicting the traffic context associated with a target road segment. Moreover, we leverage trajectory data and design pre-training strategies based on Transformer to distill traveling semantics on road networks. DyToast further augments this framework by employing unified trigonometric functions characterized by their beneficial properties, enabling the capture of temporal evolution and dynamic nature of road networks more effectively. With these proposed techniques, we can obtain representations that encode multi-faceted aspects of knowledge within road networks, applicable across both road segment-based applications and trajectory-based applications. Extensive experiments on two real-world datasets across three tasks demonstrate that our proposed framework consistently outperforms the state-of-the-art baselines by a significant margin.",
      "authors": [
        "Yile Chen",
        "Xiucheng Li",
        "Gao Cong",
        "Zhifeng Bao",
        "Cheng Long"
      ],
      "published": "2024-03-18T05:59:56Z",
      "updated": "2024-03-18T05:59:56Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.11495v1",
      "landing_url": "https://arxiv.org/abs/2403.11495v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.11495"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about semantic-enhanced representation learning for road networks with temporal dynamics, which does not involve Spoken Language Models or speech-language modeling as the core object, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about semantic-enhanced representation learning for road networks with temporal dynamics, which does not involve Spoken Language Models or speech-language modeling as the core object, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on semantic representation learning for road networks with temporal dynamics, which is unrelated to spoken language models or speech-based language modeling as required by the inclusion criteria; it does not address speech as the core modeling object, nor does it discuss full-duplex interaction, synchronization, or speech-related evaluation metrics, therefore it does not meet any inclusion criteria and falls entirely outside the scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on semantic representation learning for road networks with temporal dynamics, which is unrelated to spoken language models or speech-based language modeling as required by the inclusion criteria; it does not address speech as the core modeling object, nor does it discuss full-duplex interaction, synchronization, or speech-related evaluation metrics, therefore it does not meet any inclusion criteria and falls entirely outside the scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "NaturalTurn: A Method to Segment Speech into Psychologically Meaningful Conversational Turns",
    "abstract": "Conversation is a subject of increasing interest in the social, cognitive, and computational sciences. Yet as conversational datasets continue to increase in size and complexity, researchers lack scalable methods to segment speech-to-text transcripts into conversational \"turns\"-the basic building blocks of social interaction. We discuss this challenge and then introduce \"NaturalTurn,\" a turn-segmentation algorithm designed to accurately capture the dynamics of conversational exchange. NaturalTurn operates by distinguishing speakers' primary conversational turns from listeners' secondary utterances, such as backchannels, brief interjections, and other forms of parallel speech that characterize human conversation. Using data from a large conversation corpus, we show that NaturalTurn captures conversational turns more accurately than a baseline model. For example, it produces turns with durations and gaps that match empirical literature, reveals stronger linguistic alignment patterns between speakers, and uncovers otherwise hidden relationships between turn-taking and affective outcomes. NaturalTurn thus represents a pragmatic development in machine-generated transcript-processing methods, or \"turn models\", that will enable researchers to link turn-taking dynamics with important outcomes of social interaction, a central goal of conversation science.",
    "metadata": {
      "arxiv_id": "2403.15615",
      "title": "NaturalTurn: A Method to Segment Speech into Psychologically Meaningful Conversational Turns",
      "summary": "Conversation is a subject of increasing interest in the social, cognitive, and computational sciences. Yet as conversational datasets continue to increase in size and complexity, researchers lack scalable methods to segment speech-to-text transcripts into conversational \"turns\"-the basic building blocks of social interaction. We discuss this challenge and then introduce \"NaturalTurn,\" a turn-segmentation algorithm designed to accurately capture the dynamics of conversational exchange. NaturalTurn operates by distinguishing speakers' primary conversational turns from listeners' secondary utterances, such as backchannels, brief interjections, and other forms of parallel speech that characterize human conversation. Using data from a large conversation corpus, we show that NaturalTurn captures conversational turns more accurately than a baseline model. For example, it produces turns with durations and gaps that match empirical literature, reveals stronger linguistic alignment patterns between speakers, and uncovers otherwise hidden relationships between turn-taking and affective outcomes. NaturalTurn thus represents a pragmatic development in machine-generated transcript-processing methods, or \"turn models\", that will enable researchers to link turn-taking dynamics with important outcomes of social interaction, a central goal of conversation science.",
      "authors": [
        "Gus Cooney",
        "Andrew Reece"
      ],
      "published": "2024-03-22T21:05:54Z",
      "updated": "2025-11-12T14:27:00Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.15615v4",
      "landing_url": "https://arxiv.org/abs/2403.15615v4",
      "doi": "https://doi.org/10.1038/s41598-025-24381-1"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on a turn-segmentation algorithm for transcripts rather than treating speech as the primary object in a Spoken Language Model; it does not clearly discuss SLM/FD-SLM architecture, training, or evaluation, though it addresses conversational dynamics.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on a turn-segmentation algorithm for transcripts rather than treating speech as the primary object in a Spoken Language Model; it does not clearly discuss SLM/FD-SLM architecture, training, or evaluation, though it addresses conversational dynamics.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on segmenting speech into conversational turns using an algorithm, but it does not explicitly present a Spoken Language Model or speech language modeling architecture, training, or evaluation as core topics; further, it seems to emphasize speech processing at transcript level rather than end-to-end speech modeling, and does not mention full-duplex or synchronous interaction models, or low latency real-time interaction as required, thus it likely falls outside the inclusion criteria despite its relevance to turn-taking and speech-based interaction.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on segmenting speech into conversational turns using an algorithm, but it does not explicitly present a Spoken Language Model or speech language modeling architecture, training, or evaluation as core topics; further, it seems to emphasize speech processing at transcript level rather than end-to-end speech modeling, and does not mention full-duplex or synchronous interaction models, or low latency real-time interaction as required, thus it likely falls outside the inclusion criteria despite its relevance to turn-taking and speech-based interaction.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated Synthesis",
    "abstract": "While replacing Gaussian decoders with a conditional diffusion model enhances the perceptual quality of reconstructions in neural image compression, their lack of inductive bias for image data restricts their ability to achieve state-of-the-art perceptual levels. To address this limitation, we adopt a non-isotropic diffusion model at the decoder side. This model imposes an inductive bias aimed at distinguishing between frequency contents, thereby facilitating the generation of high-quality images. Moreover, our framework is equipped with a novel entropy model that accurately models the probability distribution of latent representation by exploiting spatio-channel correlations in latent space, while accelerating the entropy decoding step. This channel-wise entropy model leverages both local and global spatial contexts within each channel chunk. The global spatial context is built upon the Transformer, which is specifically designed for image compression tasks. The designed Transformer employs a Laplacian-shaped positional encoding, the learnable parameters of which are adaptively adjusted for each channel cluster. Our experiments demonstrate that our proposed framework yields better perceptual quality compared to cutting-edge generative-based codecs, and the proposed entropy model contributes to notable bitrate savings.",
    "metadata": {
      "arxiv_id": "2403.16258",
      "title": "Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated Synthesis",
      "summary": "While replacing Gaussian decoders with a conditional diffusion model enhances the perceptual quality of reconstructions in neural image compression, their lack of inductive bias for image data restricts their ability to achieve state-of-the-art perceptual levels. To address this limitation, we adopt a non-isotropic diffusion model at the decoder side. This model imposes an inductive bias aimed at distinguishing between frequency contents, thereby facilitating the generation of high-quality images. Moreover, our framework is equipped with a novel entropy model that accurately models the probability distribution of latent representation by exploiting spatio-channel correlations in latent space, while accelerating the entropy decoding step. This channel-wise entropy model leverages both local and global spatial contexts within each channel chunk. The global spatial context is built upon the Transformer, which is specifically designed for image compression tasks. The designed Transformer employs a Laplacian-shaped positional encoding, the learnable parameters of which are adaptively adjusted for each channel cluster. Our experiments demonstrate that our proposed framework yields better perceptual quality compared to cutting-edge generative-based codecs, and the proposed entropy model contributes to notable bitrate savings.",
      "authors": [
        "Atefeh Khoshkhahtinat",
        "Ali Zafari",
        "Piyush M. Mehta",
        "Nasser M. Nasrabadi"
      ],
      "published": "2024-03-24T18:33:16Z",
      "updated": "2024-03-24T18:33:16Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.IT",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.16258v1",
      "landing_url": "https://arxiv.org/abs/2403.16258v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.16258"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item deals with neural image compression using diffusion-based decoders and a Transformer-based entropy model, none of which relate to Spoken Language Models or speech-language modeling; it does not address speech data, synchronous interaction, or spoken language modeling concepts, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item deals with neural image compression using diffusion-based decoders and a Transformer-based entropy model, none of which relate to Spoken Language Models or speech-language modeling; it does not address speech data, synchronous interaction, or spoken language modeling concepts, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on neural image compression using a Laplacian-guided entropy model and diffusion models, with no mention of spoken language modeling, speech sequence modeling, or full-duplex spoken language interactions. It does not align with the inclusion criteria which requires the core of the study to be Spoken Language Models or speech language-related architectures and interactions, nor does it address turn-taking or synchronization interactions in speech communication. Therefore, it does not meet the thematic or methodological requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on neural image compression using a Laplacian-guided entropy model and diffusion models, with no mention of spoken language modeling, speech sequence modeling, or full-duplex spoken language interactions. It does not align with the inclusion criteria which requires the core of the study to be Spoken Language Models or speech language-related architectures and interactions, nor does it address turn-taking or synchronization interactions in speech communication. Therefore, it does not meet the thematic or methodological requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Encoding of lexical tone in self-supervised models of spoken language",
    "abstract": "Interpretability research has shown that self-supervised Spoken Language Models (SLMs) encode a wide variety of features in human speech from the acoustic, phonetic, phonological, syntactic and semantic levels, to speaker characteristics. The bulk of prior research on representations of phonology has focused on segmental features such as phonemes; the encoding of suprasegmental phonology (such as tone and stress patterns) in SLMs is not yet well understood. Tone is a suprasegmental feature that is present in more than half of the world's languages. This paper aims to analyze the tone encoding capabilities of SLMs, using Mandarin and Vietnamese as case studies. We show that SLMs encode lexical tone to a significant degree even when they are trained on data from non-tonal languages. We further find that SLMs behave similarly to native and non-native human participants in tone and consonant perception studies, but they do not follow the same developmental trajectory.",
    "metadata": {
      "arxiv_id": "2403.16865",
      "title": "Encoding of lexical tone in self-supervised models of spoken language",
      "summary": "Interpretability research has shown that self-supervised Spoken Language Models (SLMs) encode a wide variety of features in human speech from the acoustic, phonetic, phonological, syntactic and semantic levels, to speaker characteristics. The bulk of prior research on representations of phonology has focused on segmental features such as phonemes; the encoding of suprasegmental phonology (such as tone and stress patterns) in SLMs is not yet well understood. Tone is a suprasegmental feature that is present in more than half of the world's languages. This paper aims to analyze the tone encoding capabilities of SLMs, using Mandarin and Vietnamese as case studies. We show that SLMs encode lexical tone to a significant degree even when they are trained on data from non-tonal languages. We further find that SLMs behave similarly to native and non-native human participants in tone and consonant perception studies, but they do not follow the same developmental trajectory.",
      "authors": [
        "Gaofei Shen",
        "Michaela Watkins",
        "Afra Alishahi",
        "Arianna Bisazza",
        "Grzegorz Chrupała"
      ],
      "published": "2024-03-25T15:28:38Z",
      "updated": "2024-04-03T12:59:20Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.16865v2",
      "landing_url": "https://arxiv.org/abs/2403.16865v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.16865"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此論文以自監督的 Spoken Language Models（SLMs）為核心，探討語音層級的 lexical tone 表徵並包含對模型評估的實證內容，且在近三年內發布、摘要含有 spoken language model 關鍵字，符合納入條件。",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "此論文以自監督的 Spoken Language Models（SLMs）為核心，探討語音層級的 lexical tone 表徵並包含對模型評估的實證內容，且在近三年內發布、摘要含有 spoken language model 關鍵字，符合納入條件。",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study analyzes tone encoding in self-supervised Spoken Language Models (SLMs), which aligns with the core topic of speech-level language modeling and discusses model behavior and representations, but it does not explicitly mention full-duplex interaction, engineered or learned synchronization, or multi-dimensional real-time interaction evaluation which are key inclusion criteria. Although it is relevant to SLMs and the modeling of speech features, it may lack explicit coverage of full-duplex architectures or interaction synchronization aspects.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study analyzes tone encoding in self-supervised Spoken Language Models (SLMs), which aligns with the core topic of speech-level language modeling and discusses model behavior and representations, but it does not explicitly mention full-duplex interaction, engineered or learned synchronization, or multi-dimensional real-time interaction evaluation which are key inclusion criteria. Although it is relevant to SLMs and the modeling of speech features, it may lack explicit coverage of full-duplex architectures or interaction synchronization aspects.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: JuniorNano recommends inclusion (5), noting the paper explicitly studies self-supervised Spoken Language Models and includes empirical evaluation; JuniorMini was uncertain (3), noting the paper does not address full-duplex interaction or synchronization. My judgement: the paper explicitly centers on SLMs, analyzes internal representations (tone encoding) and provides model evaluation, so it satisfies the core inclusion requirement of treating speech as the primary modeling object and discussing model evaluation/analysis; its lack of full-duplex/real-time interaction content does not trigger the exclusion criteria (which mainly rule out papers solely about ASR/TTS or purely turn-taking/interaction engineering). Therefore it should be included. (Brief justification: SLM-focused empirical analysis of speech representations meets inclusion requirements even though it does not address full-duplex interaction specifics.)",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: JuniorNano recommends inclusion (5), noting the paper explicitly studies self-supervised Spoken Language Models and includes empirical evaluation; JuniorMini was uncertain (3), noting the paper does not address full-duplex interaction or synchronization. My judgement: the paper explicitly centers on SLMs, analyzes internal representations (tone encoding) and provides model evaluation, so it satisfies the core inclusion requirement of treating speech as the primary modeling object and discussing model evaluation/analysis; its lack of full-duplex/real-time interaction content does not trigger the exclusion criteria (which mainly rule out papers solely about ASR/TTS or purely turn-taking/interaction engineering). Therefore it should be included. (Brief justification: SLM-focused empirical analysis of speech representations meets inclusion requirements even though it does not address full-duplex interaction specifics.)",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild",
    "abstract": "We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models and real recordings. In particular, for speech editing evaluation, we introduce a high quality, challenging, and realistic dataset named RealEdit. We encourage readers to listen to the demos at https://jasonppy.github.io/VoiceCraft_web.",
    "metadata": {
      "arxiv_id": "2403.16973",
      "title": "VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild",
      "summary": "We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models and real recordings. In particular, for speech editing evaluation, we introduce a high quality, challenging, and realistic dataset named RealEdit. We encourage readers to listen to the demos at https://jasonppy.github.io/VoiceCraft_web.",
      "authors": [
        "Puyuan Peng",
        "Po-Yao Huang",
        "Shang-Wen Li",
        "Abdelrahman Mohamed",
        "David Harwath"
      ],
      "published": "2024-03-25T17:38:32Z",
      "updated": "2024-06-14T00:29:46Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.16973v3",
      "landing_url": "https://arxiv.org/abs/2403.16973v3",
      "doi": "https://doi.org/10.48550/arXiv.2403.16973"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on a speech-focused language model for speech editing and zero-shot TTS, meeting the Spoken Language Model/speech-language modeling core and recent timeframe criteria, with appropriate speech-centric evaluation rather than single-task ASR/TTS metrics alone, and it appears to align with the equivalent of language modeling of speech rather than being solely NLP-on-text.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on a speech-focused language model for speech editing and zero-shot TTS, meeting the Spoken Language Model/speech-language modeling core and recent timeframe criteria, with appropriate speech-centric evaluation rather than single-task ASR/TTS metrics alone, and it appears to align with the equivalent of language modeling of speech rather than being solely NLP-on-text.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces VoiceCraft, a model for speech editing and zero-shot TTS, focusing on generation and editing of speech sequences using a token-based neural codec language model. However, the abstract does not explicitly mention \"spoken language models\", \"speech language models\", or equivalent terms related to language modeling of speech, nor does it address full-duplex interaction, synchronized multiparty conversation, or behavior arbitration such as turn-taking with cognitive parallelism. Also, the focus is mainly on TTS and speech editing quality and naturalness without mention of real-time interactive capabilities or synchronization mechanisms. Therefore, it does not meet the core inclusion criteria of focusing on SLMs or FD-SLMs with interactive speech modeling. It may be better to exclude based on these criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study introduces VoiceCraft, a model for speech editing and zero-shot TTS, focusing on generation and editing of speech sequences using a token-based neural codec language model. However, the abstract does not explicitly mention \"spoken language models\", \"speech language models\", or equivalent terms related to language modeling of speech, nor does it address full-duplex interaction, synchronized multiparty conversation, or behavior arbitration such as turn-taking with cognitive parallelism. Also, the focus is mainly on TTS and speech editing quality and naturalness without mention of real-time interactive capabilities or synchronization mechanisms. Therefore, it does not meet the core inclusion criteria of focusing on SLMs or FD-SLMs with interactive speech modeling. It may be better to exclude based on these criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "The paper presents VoiceCraft, a neural codec language model that performs token-level language modeling and generation directly on speech tokens for editing and zero-shot TTS, describes model architecture (Transformer decoder, token rearrangement) and evaluation on realistic speech datasets — therefore it centers on language modeling of speech (an SLM-equivalent) rather than treating speech only as an ASR/TTS downstream task. I considered the two junior reviews: JuniorNano judged inclusion because it is speech-centric language modeling; JuniorMini recommended exclusion noting the abstract does not literally say “spoken language model” — but the phrase “neural codec language model” and the described token-level infilling/LM behavior satisfy the inclusion requirement for an equivalent description of language modeling of speech. Although the work focuses on generation/editing (not FD-SLM/full-duplex interaction), it nevertheless advances speech-token language modeling and provides architectural and evaluation details, and it does not meet any exclusion criteria (it is not merely ASR/TTS as a downstream, but a speech token LM).",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "The paper presents VoiceCraft, a neural codec language model that performs token-level language modeling and generation directly on speech tokens for editing and zero-shot TTS, describes model architecture (Transformer decoder, token rearrangement) and evaluation on realistic speech datasets — therefore it centers on language modeling of speech (an SLM-equivalent) rather than treating speech only as an ASR/TTS downstream task. I considered the two junior reviews: JuniorNano judged inclusion because it is speech-centric language modeling; JuniorMini recommended exclusion noting the abstract does not literally say “spoken language model” — but the phrase “neural codec language model” and the described token-level infilling/LM behavior satisfy the inclusion requirement for an equivalent description of language modeling of speech. Although the work focuses on generation/editing (not FD-SLM/full-duplex interaction), it nevertheless advances speech-token language modeling and provides architectural and evaluation details, and it does not meet any exclusion criteria (it is not merely ASR/TTS as a downstream, but a speech token LM).",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Low-Latency Neural Stereo Streaming",
    "abstract": "The rise of new video modalities like virtual reality or autonomous driving has increased the demand for efficient multi-view video compression methods, both in terms of rate-distortion (R-D) performance and in terms of delay and runtime. While most recent stereo video compression approaches have shown promising performance, they compress left and right views sequentially, leading to poor parallelization and runtime performance. This work presents Low-Latency neural codec for Stereo video Streaming (LLSS), a novel parallel stereo video coding method designed for fast and efficient low-latency stereo video streaming. Instead of using a sequential cross-view motion compensation like existing methods, LLSS introduces a bidirectional feature shifting module to directly exploit mutual information among views and encode them effectively with a joint cross-view prior model for entropy coding. Thanks to this design, LLSS processes left and right views in parallel, minimizing latency; all while substantially improving R-D performance compared to both existing neural and conventional codecs.",
    "metadata": {
      "arxiv_id": "2403.17879",
      "title": "Low-Latency Neural Stereo Streaming",
      "summary": "The rise of new video modalities like virtual reality or autonomous driving has increased the demand for efficient multi-view video compression methods, both in terms of rate-distortion (R-D) performance and in terms of delay and runtime. While most recent stereo video compression approaches have shown promising performance, they compress left and right views sequentially, leading to poor parallelization and runtime performance. This work presents Low-Latency neural codec for Stereo video Streaming (LLSS), a novel parallel stereo video coding method designed for fast and efficient low-latency stereo video streaming. Instead of using a sequential cross-view motion compensation like existing methods, LLSS introduces a bidirectional feature shifting module to directly exploit mutual information among views and encode them effectively with a joint cross-view prior model for entropy coding. Thanks to this design, LLSS processes left and right views in parallel, minimizing latency; all while substantially improving R-D performance compared to both existing neural and conventional codecs.",
      "authors": [
        "Qiqi Hou",
        "Farzad Farhadzadeh",
        "Amir Said",
        "Guillaume Sautiere",
        "Hoang Le"
      ],
      "published": "2024-03-26T17:11:51Z",
      "updated": "2024-03-26T17:11:51Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.17879v1",
      "landing_url": "https://arxiv.org/abs/2403.17879v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.17879"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item targets neural stereo video streaming (video compression/latency) and does not center Spoken Language Models or speech-based modeling; it lacks the required speech-centric architecture, training, or evaluation frameworks, so it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item targets neural stereo video streaming (video compression/latency) and does not center Spoken Language Models or speech-based modeling; it lacks the required speech-centric architecture, training, or evaluation frameworks, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on low-latency neural stereo streaming for video compression and do not discuss Spoken Language Models, speech-language modeling, full-duplex interaction, or any speech-centric modeling or evaluation; hence, the study falls outside the scope of the specified inclusion criteria centered on spoken language modeling and synchronous speech interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on low-latency neural stereo streaming for video compression and do not discuss Spoken Language Models, speech-language modeling, full-duplex interaction, or any speech-centric modeling or evaluation; hence, the study falls outside the scope of the specified inclusion criteria centered on spoken language modeling and synchronous speech interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SMOF: Streaming Modern CNNs on FPGAs with Smart Off-Chip Eviction",
    "abstract": "Convolutional Neural Networks (CNNs) have demonstrated their effectiveness in numerous vision tasks. However, their high processing requirements necessitate efficient hardware acceleration to meet the application's performance targets. In the space of FPGAs, streaming-based dataflow architectures are often adopted by users, as significant performance gains can be achieved through layer-wise pipelining and reduced off-chip memory access by retaining data on-chip. However, modern topologies, such as the UNet, YOLO, and X3D models, utilise long skip connections, requiring significant on-chip storage and thus limiting the performance achieved by such system architectures. The paper addresses the above limitation by introducing weight and activation eviction mechanisms to off-chip memory along the computational pipeline, taking into account the available compute and memory resources. The proposed mechanism is incorporated into an existing toolflow, expanding the design space by utilising off-chip memory as a buffer. This enables the mapping of such modern CNNs to devices with limited on-chip memory, under the streaming architecture design approach. SMOF has demonstrated the capacity to deliver competitive and, in some cases, state-of-the-art performance across a spectrum of computer vision tasks, achieving up to 10.65 X throughput improvement compared to previous works.",
    "metadata": {
      "arxiv_id": "2403.18921",
      "title": "SMOF: Streaming Modern CNNs on FPGAs with Smart Off-Chip Eviction",
      "summary": "Convolutional Neural Networks (CNNs) have demonstrated their effectiveness in numerous vision tasks. However, their high processing requirements necessitate efficient hardware acceleration to meet the application's performance targets. In the space of FPGAs, streaming-based dataflow architectures are often adopted by users, as significant performance gains can be achieved through layer-wise pipelining and reduced off-chip memory access by retaining data on-chip. However, modern topologies, such as the UNet, YOLO, and X3D models, utilise long skip connections, requiring significant on-chip storage and thus limiting the performance achieved by such system architectures. The paper addresses the above limitation by introducing weight and activation eviction mechanisms to off-chip memory along the computational pipeline, taking into account the available compute and memory resources. The proposed mechanism is incorporated into an existing toolflow, expanding the design space by utilising off-chip memory as a buffer. This enables the mapping of such modern CNNs to devices with limited on-chip memory, under the streaming architecture design approach. SMOF has demonstrated the capacity to deliver competitive and, in some cases, state-of-the-art performance across a spectrum of computer vision tasks, achieving up to 10.65 X throughput improvement compared to previous works.",
      "authors": [
        "Petros Toupas",
        "Zhewen Yu",
        "Christos-Savvas Bouganis",
        "Dimitrios Tzovaras"
      ],
      "published": "2024-03-27T18:12:24Z",
      "updated": "2024-03-27T18:12:24Z",
      "categories": [
        "cs.AR",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.18921v1",
      "landing_url": "https://arxiv.org/abs/2403.18921v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.18921"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on hardware acceleration of CNNs on FPGAs with off-chip eviction; there is no discussion of Spoken Language Models or speech modeling, thus it does not meet inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on hardware acceleration of CNNs on FPGAs with off-chip eviction; there is no discussion of Spoken Language Models or speech modeling, thus it does not meet inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on hardware acceleration of CNNs on FPGAs for computer vision tasks, with no mention of Spoken Language Models, speech-based language modeling, or full-duplex spoken interaction capabilities. It does not meet any inclusion criteria related to speech or spoken language modeling, nor does it discuss synchronous interaction or related evaluation metrics.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on hardware acceleration of CNNs on FPGAs for computer vision tasks, with no mention of Spoken Language Models, speech-based language modeling, or full-duplex spoken interaction capabilities. It does not meet any inclusion criteria related to speech or spoken language modeling, nor does it discuss synchronous interaction or related evaluation metrics.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Rethinking Interactive Image Segmentation with Low Latency, High Quality, and Diverse Prompts",
    "abstract": "The goal of interactive image segmentation is to delineate specific regions within an image via visual or language prompts. Low-latency and high-quality interactive segmentation with diverse prompts remain challenging for existing specialist and generalist models. Specialist models, with their limited prompts and task-specific designs, experience high latency because the image must be recomputed every time the prompt is updated, due to the joint encoding of image and visual prompts. Generalist models, exemplified by the Segment Anything Model (SAM), have recently excelled in prompt diversity and efficiency, lifting image segmentation to the foundation model era. However, for high-quality segmentations, SAM still lags behind state-of-the-art specialist models despite SAM being trained with x100 more segmentation masks. In this work, we delve deep into the architectural differences between the two types of models. We observe that dense representation and fusion of visual prompts are the key design choices contributing to the high segmentation quality of specialist models. In light of this, we reintroduce this dense design into the generalist models, to facilitate the development of generalist models with high segmentation quality. To densely represent diverse visual prompts, we propose to use a dense map to capture five types: clicks, boxes, polygons, scribbles, and masks. Thus, we propose SegNext, a next-generation interactive segmentation approach offering low latency, high quality, and diverse prompt support. Our method outperforms current state-of-the-art methods on HQSeg-44K and DAVIS, both quantitatively and qualitatively.",
    "metadata": {
      "arxiv_id": "2404.00741",
      "title": "Rethinking Interactive Image Segmentation with Low Latency, High Quality, and Diverse Prompts",
      "summary": "The goal of interactive image segmentation is to delineate specific regions within an image via visual or language prompts. Low-latency and high-quality interactive segmentation with diverse prompts remain challenging for existing specialist and generalist models. Specialist models, with their limited prompts and task-specific designs, experience high latency because the image must be recomputed every time the prompt is updated, due to the joint encoding of image and visual prompts. Generalist models, exemplified by the Segment Anything Model (SAM), have recently excelled in prompt diversity and efficiency, lifting image segmentation to the foundation model era. However, for high-quality segmentations, SAM still lags behind state-of-the-art specialist models despite SAM being trained with x100 more segmentation masks. In this work, we delve deep into the architectural differences between the two types of models. We observe that dense representation and fusion of visual prompts are the key design choices contributing to the high segmentation quality of specialist models. In light of this, we reintroduce this dense design into the generalist models, to facilitate the development of generalist models with high segmentation quality. To densely represent diverse visual prompts, we propose to use a dense map to capture five types: clicks, boxes, polygons, scribbles, and masks. Thus, we propose SegNext, a next-generation interactive segmentation approach offering low latency, high quality, and diverse prompt support. Our method outperforms current state-of-the-art methods on HQSeg-44K and DAVIS, both quantitatively and qualitatively.",
      "authors": [
        "Qin Liu",
        "Jaemin Cho",
        "Mohit Bansal",
        "Marc Niethammer"
      ],
      "published": "2024-03-31T17:02:24Z",
      "updated": "2024-03-31T17:02:24Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.00741v1",
      "landing_url": "https://arxiv.org/abs/2404.00741v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.00741"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe interactive image segmentation with prompts and model architectures for CV; there is no discussion of Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe interactive image segmentation with prompts and model architectures for CV; there is no discussion of Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on interactive image segmentation with visual prompts and architectural model design improvements; it does not address spoken language models, speech as core modeling objects, or related full-duplex spoken interaction capabilities required by the inclusion criteria. Hence, the study is irrelevant to the defined topic and fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on interactive image segmentation with visual prompts and architectural model design improvements; it does not address spoken language models, speech as core modeling objects, or related full-duplex spoken interaction capabilities required by the inclusion criteria. Hence, the study is irrelevant to the defined topic and fails the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "JRDB-PanoTrack: An Open-world Panoptic Segmentation and Tracking Robotic Dataset in Crowded Human Environments",
    "abstract": "Autonomous robot systems have attracted increasing research attention in recent years, where environment understanding is a crucial step for robot navigation, human-robot interaction, and decision. Real-world robot systems usually collect visual data from multiple sensors and are required to recognize numerous objects and their movements in complex human-crowded settings. Traditional benchmarks, with their reliance on single sensors and limited object classes and scenarios, fail to provide the comprehensive environmental understanding robots need for accurate navigation, interaction, and decision-making. As an extension of JRDB dataset, we unveil JRDB-PanoTrack, a novel open-world panoptic segmentation and tracking benchmark, towards more comprehensive environmental perception. JRDB-PanoTrack includes (1) various data involving indoor and outdoor crowded scenes, as well as comprehensive 2D and 3D synchronized data modalities; (2) high-quality 2D spatial panoptic segmentation and temporal tracking annotations, with additional 3D label projections for further spatial understanding; (3) diverse object classes for closed- and open-world recognition benchmarks, with OSPA-based metrics for evaluation. Extensive evaluation of leading methods shows significant challenges posed by our dataset.",
    "metadata": {
      "arxiv_id": "2404.01686",
      "title": "JRDB-PanoTrack: An Open-world Panoptic Segmentation and Tracking Robotic Dataset in Crowded Human Environments",
      "summary": "Autonomous robot systems have attracted increasing research attention in recent years, where environment understanding is a crucial step for robot navigation, human-robot interaction, and decision. Real-world robot systems usually collect visual data from multiple sensors and are required to recognize numerous objects and their movements in complex human-crowded settings. Traditional benchmarks, with their reliance on single sensors and limited object classes and scenarios, fail to provide the comprehensive environmental understanding robots need for accurate navigation, interaction, and decision-making. As an extension of JRDB dataset, we unveil JRDB-PanoTrack, a novel open-world panoptic segmentation and tracking benchmark, towards more comprehensive environmental perception. JRDB-PanoTrack includes (1) various data involving indoor and outdoor crowded scenes, as well as comprehensive 2D and 3D synchronized data modalities; (2) high-quality 2D spatial panoptic segmentation and temporal tracking annotations, with additional 3D label projections for further spatial understanding; (3) diverse object classes for closed- and open-world recognition benchmarks, with OSPA-based metrics for evaluation. Extensive evaluation of leading methods shows significant challenges posed by our dataset.",
      "authors": [
        "Duy-Tho Le",
        "Chenhui Gou",
        "Stavya Datta",
        "Hengcan Shi",
        "Ian Reid",
        "Jianfei Cai",
        "Hamid Rezatofighi"
      ],
      "published": "2024-04-02T06:43:22Z",
      "updated": "2024-04-02T06:43:22Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.01686v1",
      "landing_url": "https://arxiv.org/abs/2404.01686v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.01686"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper describes a vision-based robotic dataset for panoptic segmentation and tracking in crowded environments, with no focus on Spoken Language Models or speech-language modeling, hence it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper describes a vision-based robotic dataset for panoptic segmentation and tracking in crowded environments, with no focus on Spoken Language Models or speech-language modeling, hence it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on robotic panoptic segmentation and tracking in crowded environments without mentioning spoken language models, speech language modeling, full-duplex interaction, or any related concepts required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on robotic panoptic segmentation and tracking in crowded environments without mentioning spoken language models, speech language modeling, full-duplex interaction, or any related concepts required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech",
    "abstract": "With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis. Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences. To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed. In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances.",
    "metadata": {
      "arxiv_id": "2404.02781",
      "title": "CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech",
      "summary": "With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis. Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences. To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed. In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances.",
      "authors": [
        "Jaehyeon Kim",
        "Keon Lee",
        "Seungjun Chung",
        "Jaewoong Cho"
      ],
      "published": "2024-04-03T14:52:20Z",
      "updated": "2024-04-03T14:52:20Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.02781v1",
      "landing_url": "https://arxiv.org/abs/2404.02781v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.02781"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on a language-model-based approach to text-to-speech using neural codec language models, treating speech tokens as the modeling target and evaluating speech-related outputs, which aligns with the spoken-language-model (SLM) perspective and falls within the last 3 years, though it focuses on TTS rather than interactive dialogue behaviors.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "This item centers on a language-model-based approach to text-to-speech using neural codec language models, treating speech tokens as the modeling target and evaluating speech-related outputs, which aligns with the spoken-language-model (SLM) perspective and falls within the last 3 years, though it focuses on TTS rather than interactive dialogue behaviors.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving a neural codec language model for zero-shot TTS, which is primarily a single speech synthesis task rather than an end-to-end spoken language model that integrates synchronous full-duplex interaction, cognitive parallelism, or behavior arbitration as required; it also does not explicitly mention spoken language models or speech language models in the title or abstract, nor discussion of synchronous interaction or related behaviors, thus not fully aligning with the inclusion criteria and meeting some exclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving a neural codec language model for zero-shot TTS, which is primarily a single speech synthesis task rather than an end-to-end spoken language model that integrates synchronous full-duplex interaction, cognitive parallelism, or behavior arbitration as required; it also does not explicitly mention spoken language models or speech language models in the title or abstract, nor discussion of synchronous interaction or related behaviors, thus not fully aligning with the inclusion criteria and meeting some exclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers disagreed: one noted it touches language-modeling of audio tokens while the other correctly pointed out it is a single-task TTS paper; the abstract describes improving neural-codec language models for zero-shot TTS (speech synthesis) but does not explicitly claim Spoken Language Models or language modeling of speech as the core research, nor does it address full‑duplex interaction, synchronization, overlap/interrupt handling, or related evaluations—therefore it meets the exclusion criteria (single-task TTS) and should not be included. Score: 2.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers disagreed: one noted it touches language-modeling of audio tokens while the other correctly pointed out it is a single-task TTS paper; the abstract describes improving neural-codec language models for zero-shot TTS (speech synthesis) but does not explicitly claim Spoken Language Models or language modeling of speech as the core research, nor does it address full‑duplex interaction, synchronization, overlap/interrupt handling, or related evaluations—therefore it meets the exclusion criteria (single-task TTS) and should not be included. Score: 2.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Cross-Domain Audio Deepfake Detection: Dataset and Analysis",
    "abstract": "Audio deepfake detection (ADD) is essential for preventing the misuse of synthetic voices that may infringe on personal rights and privacy. Recent zero-shot text-to-speech (TTS) models pose higher risks as they can clone voices with a single utterance. However, the existing ADD datasets are outdated, leading to suboptimal generalization of detection models. In this paper, we construct a new cross-domain ADD dataset comprising over 300 hours of speech data that is generated by five advanced zero-shot TTS models. To simulate real-world scenarios, we employ diverse attack methods and audio prompts from different datasets. Experiments show that, through novel attack-augmented training, the Wav2Vec2-large and Whisper-medium models achieve equal error rates of 4.1\\% and 6.5\\% respectively. Additionally, we demonstrate our models' outstanding few-shot ADD ability by fine-tuning with just one minute of target-domain data. Nonetheless, neural codec compressors greatly affect the detection accuracy, necessitating further research.",
    "metadata": {
      "arxiv_id": "2404.04904",
      "title": "Cross-Domain Audio Deepfake Detection: Dataset and Analysis",
      "summary": "Audio deepfake detection (ADD) is essential for preventing the misuse of synthetic voices that may infringe on personal rights and privacy. Recent zero-shot text-to-speech (TTS) models pose higher risks as they can clone voices with a single utterance. However, the existing ADD datasets are outdated, leading to suboptimal generalization of detection models. In this paper, we construct a new cross-domain ADD dataset comprising over 300 hours of speech data that is generated by five advanced zero-shot TTS models. To simulate real-world scenarios, we employ diverse attack methods and audio prompts from different datasets. Experiments show that, through novel attack-augmented training, the Wav2Vec2-large and Whisper-medium models achieve equal error rates of 4.1\\% and 6.5\\% respectively. Additionally, we demonstrate our models' outstanding few-shot ADD ability by fine-tuning with just one minute of target-domain data. Nonetheless, neural codec compressors greatly affect the detection accuracy, necessitating further research.",
      "authors": [
        "Yuang Li",
        "Min Zhang",
        "Mengxin Ren",
        "Miaomiao Ma",
        "Daimeng Wei",
        "Hao Yang"
      ],
      "published": "2024-04-07T10:10:15Z",
      "updated": "2024-09-20T08:08:53Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.04904v2",
      "landing_url": "https://arxiv.org/abs/2404.04904v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.04904"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on cross-domain audio deepfake detection and datasets; it does not treat Spoken Language Models / Speech Language Models as the core modeling/inference objects (it uses ASR-like detectors but not SLM-centric architecture or evaluation), so it does not meet the inclusion criteria despite being within the time window.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on cross-domain audio deepfake detection and datasets; it does not treat Spoken Language Models / Speech Language Models as the core modeling/inference objects (it uses ASR-like detectors but not SLM-centric architecture or evaluation), so it does not meet the inclusion criteria despite being within the time window.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on audio deepfake detection using deep learning models, which is related to speech data and processing but does not address spoken language modeling or full-duplex spoken language systems as central themes. It lacks discussion on model architectures or evaluation frameworks targeting turn-taking, cognitive parallelism, or real-time interactive aspects emphasized in the inclusion criteria; it also does not mention spoken language models or equivalent terms explicitly, and it is more aligned with security and detection rather than interactive dialogue modeling or speech language modeling per se.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The article focuses on audio deepfake detection using deep learning models, which is related to speech data and processing but does not address spoken language modeling or full-duplex spoken language systems as central themes. It lacks discussion on model architectures or evaluation frameworks targeting turn-taking, cognitive parallelism, or real-time interactive aspects emphasized in the inclusion criteria; it also does not mention spoken language models or equivalent terms explicitly, and it is more aligned with security and detection rather than interactive dialogue modeling or speech language modeling per se.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality Novel-view Synthesis",
    "abstract": "Neural Radiance Fields (NeRF) have achieved huge success in effectively capturing and representing 3D objects and scenes. However, to establish a ubiquitous presence in everyday media formats, such as images and videos, we need to fulfill three key objectives: 1. fast encoding and decoding time, 2. compact model sizes, and 3. high-quality renderings. Despite recent advancements, a comprehensive algorithm that adequately addresses all objectives has yet to be fully realized. In this work, we present CodecNeRF, a neural codec for NeRF representations, consisting of an encoder and decoder architecture that can generate a NeRF representation in a single forward pass. Furthermore, inspired by the recent parameter-efficient finetuning approaches, we propose a finetuning method to efficiently adapt the generated NeRF representations to a new test instance, leading to high-quality image renderings and compact code sizes. The proposed CodecNeRF, a newly suggested encoding-decoding-finetuning pipeline for NeRF, achieved unprecedented compression performance of more than 100x and remarkable reduction in encoding time while maintaining (or improving) the image quality on widely used 3D object datasets.",
    "metadata": {
      "arxiv_id": "2404.04913",
      "title": "CodecNeRF: Toward Fast Encoding and Decoding, Compact, and High-quality Novel-view Synthesis",
      "summary": "Neural Radiance Fields (NeRF) have achieved huge success in effectively capturing and representing 3D objects and scenes. However, to establish a ubiquitous presence in everyday media formats, such as images and videos, we need to fulfill three key objectives: 1. fast encoding and decoding time, 2. compact model sizes, and 3. high-quality renderings. Despite recent advancements, a comprehensive algorithm that adequately addresses all objectives has yet to be fully realized. In this work, we present CodecNeRF, a neural codec for NeRF representations, consisting of an encoder and decoder architecture that can generate a NeRF representation in a single forward pass. Furthermore, inspired by the recent parameter-efficient finetuning approaches, we propose a finetuning method to efficiently adapt the generated NeRF representations to a new test instance, leading to high-quality image renderings and compact code sizes. The proposed CodecNeRF, a newly suggested encoding-decoding-finetuning pipeline for NeRF, achieved unprecedented compression performance of more than 100x and remarkable reduction in encoding time while maintaining (or improving) the image quality on widely used 3D object datasets.",
      "authors": [
        "Gyeongjin Kang",
        "Younggeun Lee",
        "Seungjun Oh",
        "Eunbyung Park"
      ],
      "published": "2024-04-07T10:49:59Z",
      "updated": "2024-09-25T07:16:41Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.04913v3",
      "landing_url": "https://arxiv.org/abs/2404.04913v3",
      "doi": "https://doi.org/10.48550/arXiv.2404.04913"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item targets CodecNeRF for fast NeRF encoding/decoding and 3D view synthesis, not Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item targets CodecNeRF for fast NeRF encoding/decoding and 3D view synthesis, not Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on Neural Radiance Fields (NeRF) for 3D object and scene representation, emphasizing fast encoding/decoding, compactness, and high-quality rendering. It does not address Spoken Language Models or full-duplex spoken language interaction, nor does it relate to speech modeling or speech-language model architectures, training, or evaluation as required by the inclusion criteria. Therefore, it fails to meet the core topical requirements of the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on Neural Radiance Fields (NeRF) for 3D object and scene representation, emphasizing fast encoding/decoding, compactness, and high-quality rendering. It does not address Spoken Language Models or full-duplex spoken language interaction, nor does it relate to speech modeling or speech-language model architectures, training, or evaluation as required by the inclusion criteria. Therefore, it fails to meet the core topical requirements of the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SpeechAlign: Aligning Speech Generation to Human Preferences",
    "abstract": "Speech language models have significantly advanced in generating realistic speech, with neural codec language models standing out. However, the integration of human feedback to align speech outputs to human preferences is often neglected. This paper addresses this gap by first analyzing the distribution gap in codec language models, highlighting how it leads to discrepancies between the training and inference phases, which negatively affects performance. Then we explore leveraging learning from human feedback to bridge the distribution gap. We introduce SpeechAlign, an iterative self-improvement strategy that aligns speech language models to human preferences. SpeechAlign involves constructing a preference codec dataset contrasting golden codec tokens against synthetic tokens, followed by preference optimization to improve the codec language model. This cycle of improvement is carried out iteratively to steadily convert weak models to strong ones. Through both subjective and objective evaluations, we show that SpeechAlign can bridge the distribution gap and facilitating continuous self-improvement of the speech language model. Moreover, SpeechAlign exhibits robust generalization capabilities and works for smaller models. Code and models will be available at https://github.com/0nutation/SpeechGPT.",
    "metadata": {
      "arxiv_id": "2404.05600",
      "title": "SpeechAlign: Aligning Speech Generation to Human Preferences",
      "summary": "Speech language models have significantly advanced in generating realistic speech, with neural codec language models standing out. However, the integration of human feedback to align speech outputs to human preferences is often neglected. This paper addresses this gap by first analyzing the distribution gap in codec language models, highlighting how it leads to discrepancies between the training and inference phases, which negatively affects performance. Then we explore leveraging learning from human feedback to bridge the distribution gap. We introduce SpeechAlign, an iterative self-improvement strategy that aligns speech language models to human preferences. SpeechAlign involves constructing a preference codec dataset contrasting golden codec tokens against synthetic tokens, followed by preference optimization to improve the codec language model. This cycle of improvement is carried out iteratively to steadily convert weak models to strong ones. Through both subjective and objective evaluations, we show that SpeechAlign can bridge the distribution gap and facilitating continuous self-improvement of the speech language model. Moreover, SpeechAlign exhibits robust generalization capabilities and works for smaller models. Code and models will be available at https://github.com/0nutation/SpeechGPT.",
      "authors": [
        "Dong Zhang",
        "Zhaowei Li",
        "Shimin Li",
        "Xin Zhang",
        "Pengyu Wang",
        "Yaqian Zhou",
        "Xipeng Qiu"
      ],
      "published": "2024-04-08T15:21:17Z",
      "updated": "2024-04-08T15:21:17Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.05600v1",
      "landing_url": "https://arxiv.org/abs/2404.05600v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.05600"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此論文以語音語言模型為核心，探討利用人類反饋對齊語音輸出，並提出可訓練/評估的自我改進迭代策略，符合近3年的 spoken language model 研究範疇，且標題/摘要皆含有 speech language model 的描述，涵蓋架構/訓練/評估等實務面向。",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "此論文以語音語言模型為核心，探討利用人類反饋對齊語音輸出，並提出可訓練/評估的自我改進迭代策略，符合近3年的 spoken language model 研究範疇，且標題/摘要皆含有 speech language model 的描述，涵蓋架構/訓練/評估等實務面向。",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on aligning speech generation models to human preferences, dealing with codec language models as the core, which aligns with the inclusion criteria emphasizing speech language models and modeling of speech tokens. It also involves training and evaluation with objective and subjective metrics. However, it does not explicitly mention full-duplex spoken language models, synchronous interaction, turn-taking, or low-latency real-time interaction features emphasized in the inclusion criteria. The study appears more focused on speech generation quality improvement rather than full-duplex or interactive speech dialogue modeling. Therefore, while it is relevant to speech language models, it may lack the focus on interactive or full-duplex spoken language modeling required for this topic.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on aligning speech generation models to human preferences, dealing with codec language models as the core, which aligns with the inclusion criteria emphasizing speech language models and modeling of speech tokens. It also involves training and evaluation with objective and subjective metrics. However, it does not explicitly mention full-duplex spoken language models, synchronous interaction, turn-taking, or low-latency real-time interaction features emphasized in the inclusion criteria. The study appears more focused on speech generation quality improvement rather than full-duplex or interactive speech dialogue modeling. Therefore, while it is relevant to speech language models, it may lack the focus on interactive or full-duplex spoken language modeling required for this topic.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the paper centers on codec/speech language models and training/evaluation (JuniorNano rated include), while JuniorMini worried it lacks interactive/full‑duplex focus; reading the abstract confirms it explicitly targets \"Speech language models\" and proposes an iterative training/preference‑optimization method with objective and subjective evaluations (satisfying the inclusion requirement that speech language models are the core and that training/evaluation are discussed). However, the work does not address full‑duplex synchronous interaction, turn‑taking, overlap handling, low‑latency interaction, or behavior arbitration — so it does trigger the exclusion concern about papers that focus only on single‑direction generation/quality improvements rather than interactive FD‑SLM behaviors. Weighing these, the paper is clearly valid SLM research (architecture/training/evaluation of speech LMs) but does not contribute to the FD‑SLM/real‑time interaction subtopic; therefore I recommend inclusion for the broader SLM scope but with the caveat that it does not address full‑duplex interaction. One-sentence explanation: Score 4 because the abstract meets the SLM inclusion criteria (explicit speech language model focus and training/evaluation contributions) but lacks full‑duplex/interactive behavior coverage required for FD‑SLM specificity.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the paper centers on codec/speech language models and training/evaluation (JuniorNano rated include), while JuniorMini worried it lacks interactive/full‑duplex focus; reading the abstract confirms it explicitly targets \"Speech language models\" and proposes an iterative training/preference‑optimization method with objective and subjective evaluations (satisfying the inclusion requirement that speech language models are the core and that training/evaluation are discussed). However, the work does not address full‑duplex synchronous interaction, turn‑taking, overlap handling, low‑latency interaction, or behavior arbitration — so it does trigger the exclusion concern about papers that focus only on single‑direction generation/quality improvements rather than interactive FD‑SLM behaviors. Weighing these, the paper is clearly valid SLM research (architecture/training/evaluation of speech LMs) but does not contribute to the FD‑SLM/real‑time interaction subtopic; therefore I recommend inclusion for the broader SLM scope but with the caveat that it does not address full‑duplex interaction. One-sentence explanation: Score 4 because the abstract meets the SLM inclusion criteria (explicit speech language model focus and training/evaluation contributions) but lacks full‑duplex/interactive behavior coverage required for FD‑SLM specificity.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Interference Management for Full-Duplex ISAC in B5G/6G Networks: Architectures, Challenges, and Solutions",
    "abstract": "Integrated sensing and communications (ISAC) has been visioned as a key technique for B5G/6G networks. To support monostatic sensing, a full-duplex radio is indispensable to extract echo signals from targets. Such a radio can also greatly improve network capacity via full-duplex communications. However, full-duplex radios in existing ISAC designs are mainly focused on wireless sensing, while the ability of full-duplex communications is usually ignored. In this article, we provide an overview of full-duplex ISAC (FD-ISAC), where a full-duplex radio is used for both wireless sensing and full-duplex communications in B5G/6G networks, with a focus on the fundamental interference management problem in such networks. First, different ISAC architectures are introduced, considering different full-duplex communication modes and wireless sensing modes. Next, the challenging issues of link-level interference and network-level interference are analyzed, illustrating a critical demand on interference management for FD-ISAC. Potential solutions to interference management are then reviewed from the perspective of radio architecture design, beamforming, mode selection, and resource allocation. The corresponding open problems are also highlighted.",
    "metadata": {
      "arxiv_id": "2404.05984",
      "title": "Interference Management for Full-Duplex ISAC in B5G/6G Networks: Architectures, Challenges, and Solutions",
      "summary": "Integrated sensing and communications (ISAC) has been visioned as a key technique for B5G/6G networks. To support monostatic sensing, a full-duplex radio is indispensable to extract echo signals from targets. Such a radio can also greatly improve network capacity via full-duplex communications. However, full-duplex radios in existing ISAC designs are mainly focused on wireless sensing, while the ability of full-duplex communications is usually ignored. In this article, we provide an overview of full-duplex ISAC (FD-ISAC), where a full-duplex radio is used for both wireless sensing and full-duplex communications in B5G/6G networks, with a focus on the fundamental interference management problem in such networks. First, different ISAC architectures are introduced, considering different full-duplex communication modes and wireless sensing modes. Next, the challenging issues of link-level interference and network-level interference are analyzed, illustrating a critical demand on interference management for FD-ISAC. Potential solutions to interference management are then reviewed from the perspective of radio architecture design, beamforming, mode selection, and resource allocation. The corresponding open problems are also highlighted.",
      "authors": [
        "Aimin Tang",
        "Xudong Wang",
        "J. Andrew Zhang"
      ],
      "published": "2024-04-09T03:35:14Z",
      "updated": "2024-04-09T03:35:14Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.05984v1",
      "landing_url": "https://arxiv.org/abs/2404.05984v1",
      "doi": "https://doi.org/10.1109/MCOM.001.2300654"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on interference management for full-duplex ISAC in wireless networks and does not address Spoken Language Models or speech-language modeling, failing all inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on interference management for full-duplex ISAC in wireless networks and does not address Spoken Language Models or speech-language modeling, failing all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on full-duplex radio interference management in integrated sensing and communication for B5G/6G networks, with no mention of spoken language models, speech-based modeling, or interaction at the language or acoustic signal level as required; it is unrelated to spoken language models or full-duplex spoken language interaction topics specified in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on full-duplex radio interference management in integrated sensing and communication for B5G/6G networks, with no mention of spoken language models, speech-based modeling, or interaction at the language or acoustic signal level as required; it is unrelated to spoken language models or full-duplex spoken language interaction topics specified in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Spatio-Temporal Dynamics of Nucleo-Cytoplasmic Transport",
    "abstract": "Nucleocytoplasmic transport is essential for cellular function, presenting a canonical example of rapid molecular sorting inside cells. It consists of a coordinated interplay between import/export of molecules in/out the cell nucleus. Here, we investigate the role of spatio-temporal dynamics of the nucleocytoplasmic transport and its regulation. We develop a biophysical model that captures the main features of the nucleocytoplasmic transport, in particular, its regulation through the Ran cycle. Our model yields steady-state profiles for the molecular components of the Ran cycle, their relaxation times, as well as the nuclear-to-cytoplasmic molecule ratio. We show that these quantities are affected by their spatial dynamics and heterogeneity within the nucleus. Specifically, we find that the spatial nonuniformity of Ran Guanine Exchange Factor (RanGEF) - particularly its proximity to the nuclear envelope - increases the Ran content in the nucleus. We further show that RanGEF's accumulation near the nuclear envelope results from its intrinsic dynamics as a nuclear cargo, transported by the Ran cycle itself. Overall, our work highlights the critical role of molecular spatial dynamics in cellular processes, and proposes new avenues for theoretical and experimental inquiries into the nucleocytoplasmic transport.",
    "metadata": {
      "arxiv_id": "2404.06688",
      "title": "Spatio-Temporal Dynamics of Nucleo-Cytoplasmic Transport",
      "summary": "Nucleocytoplasmic transport is essential for cellular function, presenting a canonical example of rapid molecular sorting inside cells. It consists of a coordinated interplay between import/export of molecules in/out the cell nucleus. Here, we investigate the role of spatio-temporal dynamics of the nucleocytoplasmic transport and its regulation. We develop a biophysical model that captures the main features of the nucleocytoplasmic transport, in particular, its regulation through the Ran cycle. Our model yields steady-state profiles for the molecular components of the Ran cycle, their relaxation times, as well as the nuclear-to-cytoplasmic molecule ratio. We show that these quantities are affected by their spatial dynamics and heterogeneity within the nucleus. Specifically, we find that the spatial nonuniformity of Ran Guanine Exchange Factor (RanGEF) - particularly its proximity to the nuclear envelope - increases the Ran content in the nucleus. We further show that RanGEF's accumulation near the nuclear envelope results from its intrinsic dynamics as a nuclear cargo, transported by the Ran cycle itself. Overall, our work highlights the critical role of molecular spatial dynamics in cellular processes, and proposes new avenues for theoretical and experimental inquiries into the nucleocytoplasmic transport.",
      "authors": [
        "S. Alex Rautu",
        "Alexandra Zidovska",
        "Michael J. Shelley"
      ],
      "published": "2024-04-10T02:24:22Z",
      "updated": "2024-09-25T17:13:49Z",
      "categories": [
        "physics.bio-ph",
        "cond-mat.soft",
        "q-bio.SC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.06688v2",
      "landing_url": "https://arxiv.org/abs/2404.06688v2",
      "doi": "https://doi.org/10.1103/PhysRevResearch.6.043022"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on nucleocytoplasmic transport and biophysical modeling, with no discussion of Spoken Language Models or speech-language modeling, thus it fails all inclusion criteria and does not fit the exclusion criteria (not about synchronous speech interaction).",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on nucleocytoplasmic transport and biophysical modeling, with no discussion of Spoken Language Models or speech-language modeling, thus it fails all inclusion criteria and does not fit the exclusion criteria (not about synchronous speech interaction).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the biophysical modeling of nucleocytoplasmic transport, which is a cellular molecular biology topic unrelated to spoken language models, speech language modeling, or full-duplex spoken language interactions; it lacks any mention of speech, language models, or relevant architectures or evaluations described in the inclusion criteria and is outside the stated thematic scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the biophysical modeling of nucleocytoplasmic transport, which is a cellular molecular biology topic unrelated to spoken language models, speech language modeling, or full-duplex spoken language interactions; it lacks any mention of speech, language models, or relevant architectures or evaluations described in the inclusion criteria and is outside the stated thematic scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Cramer-Rao Bounds for Near-Field Sensing: A Generic Modular Architecture",
    "abstract": "A generic modular array architecture is proposed, featuring uniform/non-uniform subarray layouts that allows for flexible deployment. The bistatic near-field sensing system is considered, where the target is located in the near-field of the whole modular array and the far-field of each subarray. Then, the closed-form expressions of Cramer-Rao bounds (CRBs) for range and angle estimations are derived based on the hybrid spherical and planar wave model (HSPM). Simulation results validate the accuracy of the derived closed-form CRBs and demonstrate that: i) The HSPM with varying angles of arrival (AoAs) between subarrays can reduce the CRB for range estimation compared to the traditional HSPM with shared AoA; and ii) The proposed generic modular architecture with subarrays positioned closer to the edges can significantly reduce the CRBs compared to the traditional modular architecture with uniform subarray layout, when the array aperture is fixed.",
    "metadata": {
      "arxiv_id": "2404.07472",
      "title": "Cramer-Rao Bounds for Near-Field Sensing: A Generic Modular Architecture",
      "summary": "A generic modular array architecture is proposed, featuring uniform/non-uniform subarray layouts that allows for flexible deployment. The bistatic near-field sensing system is considered, where the target is located in the near-field of the whole modular array and the far-field of each subarray. Then, the closed-form expressions of Cramer-Rao bounds (CRBs) for range and angle estimations are derived based on the hybrid spherical and planar wave model (HSPM). Simulation results validate the accuracy of the derived closed-form CRBs and demonstrate that: i) The HSPM with varying angles of arrival (AoAs) between subarrays can reduce the CRB for range estimation compared to the traditional HSPM with shared AoA; and ii) The proposed generic modular architecture with subarrays positioned closer to the edges can significantly reduce the CRBs compared to the traditional modular architecture with uniform subarray layout, when the array aperture is fixed.",
      "authors": [
        "Chunwei Meng",
        "Dingyou Ma",
        "Xu Chen",
        "Zhiyong Feng",
        "Yuanwei Liu"
      ],
      "published": "2024-04-11T04:42:01Z",
      "updated": "2024-04-11T04:42:01Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.07472v1",
      "landing_url": "https://arxiv.org/abs/2404.07472v1",
      "doi": "https://doi.org/10.1109/LWC.2024.3406577"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about near-field sensing and Cramer-Rao bounds for a modular antenna array, not about Spoken Language Models or speech-language modeling, so it does not meet the core inclusion criteria (SLMs/FD-SLMs, end-to-end or mixed speech-language architectures) and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about near-field sensing and Cramer-Rao bounds for a modular antenna array, not about Spoken Language Models or speech-language modeling, so it does not meet the core inclusion criteria (SLMs/FD-SLMs, end-to-end or mixed speech-language architectures) and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on near-field sensing and Cramer-Rao bounds in array architectures, with no mention of spoken language models, speech language models, full-duplex spoken interaction, or any speech modeling and interaction criteria outlined in the inclusion criteria; therefore, the study does not align with the central thematic focus of spoken language model research or related evaluation aspects and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on near-field sensing and Cramer-Rao bounds in array architectures, with no mention of spoken language models, speech language models, full-duplex spoken interaction, or any speech modeling and interaction criteria outlined in the inclusion criteria; therefore, the study does not align with the central thematic focus of spoken language model research or related evaluation aspects and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "An Effective Automated Speaking Assessment Approach to Mitigating Data Scarcity and Imbalanced Distribution",
    "abstract": "Automated speaking assessment (ASA) typically involves automatic speech recognition (ASR) and hand-crafted feature extraction from the ASR transcript of a learner's speech. Recently, self-supervised learning (SSL) has shown stellar performance compared to traditional methods. However, SSL-based ASA systems are faced with at least three data-related challenges: limited annotated data, uneven distribution of learner proficiency levels and non-uniform score intervals between different CEFR proficiency levels. To address these challenges, we explore the use of two novel modeling strategies: metric-based classification and loss reweighting, leveraging distinct SSL-based embedding features. Extensive experimental results on the ICNALE benchmark dataset suggest that our approach can outperform existing strong baselines by a sizable margin, achieving a significant improvement of more than 10% in CEFR prediction accuracy.",
    "metadata": {
      "arxiv_id": "2404.07575",
      "title": "An Effective Automated Speaking Assessment Approach to Mitigating Data Scarcity and Imbalanced Distribution",
      "summary": "Automated speaking assessment (ASA) typically involves automatic speech recognition (ASR) and hand-crafted feature extraction from the ASR transcript of a learner's speech. Recently, self-supervised learning (SSL) has shown stellar performance compared to traditional methods. However, SSL-based ASA systems are faced with at least three data-related challenges: limited annotated data, uneven distribution of learner proficiency levels and non-uniform score intervals between different CEFR proficiency levels. To address these challenges, we explore the use of two novel modeling strategies: metric-based classification and loss reweighting, leveraging distinct SSL-based embedding features. Extensive experimental results on the ICNALE benchmark dataset suggest that our approach can outperform existing strong baselines by a sizable margin, achieving a significant improvement of more than 10% in CEFR prediction accuracy.",
      "authors": [
        "Tien-Hong Lo",
        "Fu-An Chao",
        "Tzu-I Wu",
        "Yao-Ting Sung",
        "Berlin Chen"
      ],
      "published": "2024-04-11T09:06:49Z",
      "updated": "2025-03-02T13:55:52Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.07575v4",
      "landing_url": "https://arxiv.org/abs/2404.07575v4",
      "doi": "https://doi.org/10.48550/arXiv.2404.07575"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item does not center on Spoken Language Models or full-duplex speech interaction; it focuses on ASR-based feature extraction and SSL embeddings for CEFR prediction from transcripts, not on speech-language modeling architecture, synthesis, or interactive dialogue, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item does not center on Spoken Language Models or full-duplex speech interaction; it focuses on ASR-based feature extraction and SSL embeddings for CEFR prediction from transcripts, not on speech-language modeling architecture, synthesis, or interactive dialogue, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses primarily on automated speaking assessment using SSL-based methods for learner speech evaluation, emphasizing CEFR proficiency prediction accuracy improvements; however, it does not discuss spoken language models or full-duplex spoken language models, nor does it address end-to-end modeling of speech sequences, interactive dialogue behaviors, or synchronization topics required by the inclusion criteria, therefore it falls outside the targeted research scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses primarily on automated speaking assessment using SSL-based methods for learner speech evaluation, emphasizing CEFR proficiency prediction accuracy improvements; however, it does not discuss spoken language models or full-duplex spoken language models, nor does it address end-to-end modeling of speech sequences, interactive dialogue behaviors, or synchronization topics required by the inclusion criteria, therefore it falls outside the targeted research scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Full-Duplex Beyond Self-Interference: The Unlimited Sensing Way",
    "abstract": "The success of full-stack full-duplex communication systems depends on how effectively one can achieve digital self-interference cancellation (SIC). Towards this end, in this paper, we consider unlimited sensing framework (USF) enabled full-duplex system. We show that by injecting folding non-linearities in the sensing pipeline, one can not only suppress self-interference but also recover the signal of interest (SoI). This approach leads to novel design of the receiver architecture that is complemented by a modulo-domain channel estimation method. We then demonstrate the advantages of modulo ADC by analyzing the relationship between quantization noise, quantization bits, and dynamic range. Numerical experiments show that the USF enabled receiver structure can achieve up to 40 dB digital SIC by using as few as 4-bits per sample. Our method outperforms the previous approach based on adaptive filters when it comes to SoI reconstruction, detection, and digital SIC performance.",
    "metadata": {
      "arxiv_id": "2404.08610",
      "title": "Full-Duplex Beyond Self-Interference: The Unlimited Sensing Way",
      "summary": "The success of full-stack full-duplex communication systems depends on how effectively one can achieve digital self-interference cancellation (SIC). Towards this end, in this paper, we consider unlimited sensing framework (USF) enabled full-duplex system. We show that by injecting folding non-linearities in the sensing pipeline, one can not only suppress self-interference but also recover the signal of interest (SoI). This approach leads to novel design of the receiver architecture that is complemented by a modulo-domain channel estimation method. We then demonstrate the advantages of modulo ADC by analyzing the relationship between quantization noise, quantization bits, and dynamic range. Numerical experiments show that the USF enabled receiver structure can achieve up to 40 dB digital SIC by using as few as 4-bits per sample. Our method outperforms the previous approach based on adaptive filters when it comes to SoI reconstruction, detection, and digital SIC performance.",
      "authors": [
        "Ziang Liu",
        "Ayush Bhandari",
        "Bruno Clerckx"
      ],
      "published": "2024-04-12T17:15:49Z",
      "updated": "2024-11-20T19:11:41Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.08610v3",
      "landing_url": "https://arxiv.org/abs/2404.08610v3",
      "doi": "https://doi.org/10.48550/arXiv.2404.08610"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "該論文著眼於全雙工通訊系統的自干擾抑制與感知，未以 Spoken Language Models 或語音語言建模為核心，也未涉及語音互動或對話行為評估，符合排除標準。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "該論文著眼於全雙工通訊系統的自干擾抑制與感知，未以 Spoken Language Models 或語音語言建模為核心，也未涉及語音互動或對話行為評估，符合排除標準。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses a full-duplex communication system focusing on digital self-interference cancellation and receiver design, with no mention of spoken language models, speech language modeling, or end-to-end speech-focused architectures; it is focused on signal processing rather than modeling or evaluating spoken language interactions, synchronization, or conversational behaviors, thus failing to meet the inclusion criteria and engaging none of the concepts related to SLMs or FD-SLMs.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses a full-duplex communication system focusing on digital self-interference cancellation and receiver design, with no mention of spoken language models, speech language modeling, or end-to-end speech-focused architectures; it is focused on signal processing rather than modeling or evaluating spoken language interactions, synchronization, or conversational behaviors, thus failing to meet the inclusion criteria and engaging none of the concepts related to SLMs or FD-SLMs.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Tri-modal Confluence with Temporal Dynamics for Scene Graph Generation in Operating Rooms",
    "abstract": "A comprehensive understanding of surgical scenes allows for monitoring of the surgical process, reducing the occurrence of accidents and enhancing efficiency for medical professionals. Semantic modeling within operating rooms, as a scene graph generation (SGG) task, is challenging since it involves consecutive recognition of subtle surgical actions over prolonged periods. To address this challenge, we propose a Tri-modal (i.e., images, point clouds, and language) confluence with Temporal dynamics framework, termed TriTemp-OR. Diverging from previous approaches that integrated temporal information via memory graphs, our method embraces two advantages: 1) we directly exploit bi-modal temporal information from the video streaming for hierarchical feature interaction, and 2) the prior knowledge from Large Language Models (LLMs) is embedded to alleviate the class-imbalance problem in the operating theatre. Specifically, our model performs temporal interactions across 2D frames and 3D point clouds, including a scale-adaptive multi-view temporal interaction (ViewTemp) and a geometric-temporal point aggregation (PointTemp). Furthermore, we transfer knowledge from the biomedical LLM, LLaVA-Med, to deepen the comprehension of intraoperative relations. The proposed TriTemp-OR enables the aggregation of tri-modal features through relation-aware unification to predict relations so as to generate scene graphs. Experimental results on the 4D-OR benchmark demonstrate the superior performance of our model for long-term OR streaming.",
    "metadata": {
      "arxiv_id": "2404.09231",
      "title": "Tri-modal Confluence with Temporal Dynamics for Scene Graph Generation in Operating Rooms",
      "summary": "A comprehensive understanding of surgical scenes allows for monitoring of the surgical process, reducing the occurrence of accidents and enhancing efficiency for medical professionals. Semantic modeling within operating rooms, as a scene graph generation (SGG) task, is challenging since it involves consecutive recognition of subtle surgical actions over prolonged periods. To address this challenge, we propose a Tri-modal (i.e., images, point clouds, and language) confluence with Temporal dynamics framework, termed TriTemp-OR. Diverging from previous approaches that integrated temporal information via memory graphs, our method embraces two advantages: 1) we directly exploit bi-modal temporal information from the video streaming for hierarchical feature interaction, and 2) the prior knowledge from Large Language Models (LLMs) is embedded to alleviate the class-imbalance problem in the operating theatre. Specifically, our model performs temporal interactions across 2D frames and 3D point clouds, including a scale-adaptive multi-view temporal interaction (ViewTemp) and a geometric-temporal point aggregation (PointTemp). Furthermore, we transfer knowledge from the biomedical LLM, LLaVA-Med, to deepen the comprehension of intraoperative relations. The proposed TriTemp-OR enables the aggregation of tri-modal features through relation-aware unification to predict relations so as to generate scene graphs. Experimental results on the 4D-OR benchmark demonstrate the superior performance of our model for long-term OR streaming.",
      "authors": [
        "Diandian Guo",
        "Manxi Lin",
        "Jialun Pei",
        "He Tang",
        "Yueming Jin",
        "Pheng-Ann Heng"
      ],
      "published": "2024-04-14T12:19:16Z",
      "updated": "2024-04-14T12:19:16Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.09231v1",
      "landing_url": "https://arxiv.org/abs/2404.09231v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.09231"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此研究聚焦於在手術場景中以三模態（影像、點雲、語言）與時間動態生成場景圖，核心是視覺-語言-幾何信息的融合與推理，雖使用語言模型作為知識嵌入，但並非以 Spoken Language Models 或等價概念為核心的語音序列建模與實時語音互動，因此不符合以語音/語言模型為核心的納入條件，亦不明確聚焦於語音的端到端或混合式語音語言建模與評估。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此研究聚焦於在手術場景中以三模態（影像、點雲、語言）與時間動態生成場景圖，核心是視覺-語言-幾何信息的融合與推理，雖使用語言模型作為知識嵌入，但並非以 Spoken Language Models 或等價概念為核心的語音序列建模與實時語音互動，因此不符合以語音/語言模型為核心的納入條件，亦不明確聚焦於語音的端到端或混合式語音語言建模與評估。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on scene graph generation in operating rooms using multi-modal data including images, point clouds, and language, but it does not address Spoken Language Models or Speech Language Models as core modeling elements, nor does it involve speech as the primary focus for modeling or inference; thus, it does not meet the inclusion criteria centered on spoken language modeling, full-duplex spoken interaction, or related evaluation aspects.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on scene graph generation in operating rooms using multi-modal data including images, point clouds, and language, but it does not address Spoken Language Models or Speech Language Models as core modeling elements, nor does it involve speech as the primary focus for modeling or inference; thus, it does not meet the inclusion criteria centered on spoken language modeling, full-duplex spoken interaction, or related evaluation aspects.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Video2Game: Real-time, Interactive, Realistic and Browser-Compatible Environment from a Single Video",
    "abstract": "Creating high-quality and interactive virtual environments, such as games and simulators, often involves complex and costly manual modeling processes. In this paper, we present Video2Game, a novel approach that automatically converts videos of real-world scenes into realistic and interactive game environments. At the heart of our system are three core components:(i) a neural radiance fields (NeRF) module that effectively captures the geometry and visual appearance of the scene; (ii) a mesh module that distills the knowledge from NeRF for faster rendering; and (iii) a physics module that models the interactions and physical dynamics among the objects. By following the carefully designed pipeline, one can construct an interactable and actionable digital replica of the real world. We benchmark our system on both indoor and large-scale outdoor scenes. We show that we can not only produce highly-realistic renderings in real-time, but also build interactive games on top.",
    "metadata": {
      "arxiv_id": "2404.09833",
      "title": "Video2Game: Real-time, Interactive, Realistic and Browser-Compatible Environment from a Single Video",
      "summary": "Creating high-quality and interactive virtual environments, such as games and simulators, often involves complex and costly manual modeling processes. In this paper, we present Video2Game, a novel approach that automatically converts videos of real-world scenes into realistic and interactive game environments. At the heart of our system are three core components:(i) a neural radiance fields (NeRF) module that effectively captures the geometry and visual appearance of the scene; (ii) a mesh module that distills the knowledge from NeRF for faster rendering; and (iii) a physics module that models the interactions and physical dynamics among the objects. By following the carefully designed pipeline, one can construct an interactable and actionable digital replica of the real world. We benchmark our system on both indoor and large-scale outdoor scenes. We show that we can not only produce highly-realistic renderings in real-time, but also build interactive games on top.",
      "authors": [
        "Hongchi Xia",
        "Zhi-Hao Lin",
        "Wei-Chiu Ma",
        "Shenlong Wang"
      ],
      "published": "2024-04-15T14:32:32Z",
      "updated": "2024-04-15T14:32:32Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.09833v1",
      "landing_url": "https://arxiv.org/abs/2404.09833v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.09833"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on converting videos to interactive game environments using NeRF, mesh, and physics, with no emphasis on spoken language models or speech-language modeling, thus not meeting the inclusion criteria and satisfying an exclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on converting videos to interactive game environments using NeRF, mesh, and physics, with no emphasis on spoken language models or speech-language modeling, thus not meeting the inclusion criteria and satisfying an exclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on converting videos into interactive gaming environments using neural rendering and physics modules, without any mention of spoken language models, speech language modeling, or related concepts. It does not address speech as a core modeling target nor discuss model architectures or evaluations within the spoken language modeling domain. Therefore, it does not meet the inclusion criteria focused on spoken language models and full-duplex spoken interactions.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on converting videos into interactive gaming environments using neural rendering and physics modules, without any mention of spoken language models, speech language modeling, or related concepts. It does not address speech as a core modeling target nor discuss model architectures or evaluations within the spoken language modeling domain. Therefore, it does not meet the inclusion criteria focused on spoken language models and full-duplex spoken interactions.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MAD Speech: Measures of Acoustic Diversity of Speech",
    "abstract": "Generative spoken language models produce speech in a wide range of voices, prosody, and recording conditions, seemingly approaching the diversity of natural speech. However, the extent to which generated speech is acoustically diverse remains unclear due to a lack of appropriate metrics. We address this gap by developing lightweight metrics of acoustic diversity, which we collectively refer to as MAD Speech. We focus on measuring five facets of acoustic diversity: voice, gender, emotion, accent, and background noise. We construct the metrics as a composition of specialized, per-facet embedding models and an aggregation function that measures diversity within the embedding space. Next, we build a series of datasets with a priori known diversity preferences for each facet. Using these datasets, we demonstrate that our proposed metrics achieve a stronger agreement with the ground-truth diversity than baselines. Finally, we showcase the applicability of our proposed metrics across several real-life evaluation scenarios. MAD Speech is made publicly accessible.",
    "metadata": {
      "arxiv_id": "2404.10419",
      "title": "MAD Speech: Measures of Acoustic Diversity of Speech",
      "summary": "Generative spoken language models produce speech in a wide range of voices, prosody, and recording conditions, seemingly approaching the diversity of natural speech. However, the extent to which generated speech is acoustically diverse remains unclear due to a lack of appropriate metrics. We address this gap by developing lightweight metrics of acoustic diversity, which we collectively refer to as MAD Speech. We focus on measuring five facets of acoustic diversity: voice, gender, emotion, accent, and background noise. We construct the metrics as a composition of specialized, per-facet embedding models and an aggregation function that measures diversity within the embedding space. Next, we build a series of datasets with a priori known diversity preferences for each facet. Using these datasets, we demonstrate that our proposed metrics achieve a stronger agreement with the ground-truth diversity than baselines. Finally, we showcase the applicability of our proposed metrics across several real-life evaluation scenarios. MAD Speech is made publicly accessible.",
      "authors": [
        "Matthieu Futeral",
        "Andrea Agostinelli",
        "Marco Tagliasacchi",
        "Neil Zeghidour",
        "Eugene Kharitonov"
      ],
      "published": "2024-04-16T09:35:27Z",
      "updated": "2025-03-11T12:02:06Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.10419v2",
      "landing_url": "https://arxiv.org/abs/2404.10419v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.10419"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item directly discusses generative spoken language models and evaluates acoustic diversity in speech, aligning with inclusion criteria for SLMs and speech modeling, published within the last 3 years, and not limited to ASR/TTS or single-task evaluation.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item directly discusses generative spoken language models and evaluates acoustic diversity in speech, aligning with inclusion criteria for SLMs and speech modeling, published within the last 3 years, and not limited to ASR/TTS or single-task evaluation.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on developing acoustic diversity metrics for speech generated by spoken language models, which aligns with spoken language modeling as it treats speech as a core modeling object; however, it does not explicitly mention model architecture, training, or end-to-end interaction capabilities such as full-duplex or synchronous turn-taking, and mainly addresses evaluation metrics rather than full dialogue behavior or interactive systems, thus missing some key aspects of inclusion criteria related to synchronous interaction and architecture.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on developing acoustic diversity metrics for speech generated by spoken language models, which aligns with spoken language modeling as it treats speech as a core modeling object; however, it does not explicitly mention model architecture, training, or end-to-end interaction capabilities such as full-duplex or synchronous turn-taking, and mainly addresses evaluation metrics rather than full dialogue behavior or interactive systems, thus missing some key aspects of inclusion criteria related to synchronous interaction and architecture.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "兩位 junior reviewer 意見分歧（一人給 5、一人給 3）；該摘要明確出現 “generative spoken language models” 並聚焦於對生成語音的聲學多樣性進行度量與評估（提出指標、嵌入模型、數據集及驗證），因此滿足以 SLM 為核心且討論評估面的納入要件；該工作屬於評估/分析導向，未涉及全雙工或同步互動設計，但也不僅為單一下游任務或純 ASR/TTS，故建議收錄（較傾向納入但非絕對） 。",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "兩位 junior reviewer 意見分歧（一人給 5、一人給 3）；該摘要明確出現 “generative spoken language models” 並聚焦於對生成語音的聲學多樣性進行度量與評估（提出指標、嵌入模型、數據集及驗證），因此滿足以 SLM 為核心且討論評估面的納入要件；該工作屬於評估/分析導向，未涉及全雙工或同步互動設計，但也不僅為單一下游任務或純 ASR/TTS，故建議收錄（較傾向納入但非絕對） 。",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Select and Reorder: A Novel Approach for Neural Sign Language Production",
    "abstract": "Sign languages, often categorised as low-resource languages, face significant challenges in achieving accurate translation due to the scarcity of parallel annotated datasets. This paper introduces Select and Reorder (S&R), a novel approach that addresses data scarcity by breaking down the translation process into two distinct steps: Gloss Selection (GS) and Gloss Reordering (GR). Our method leverages large spoken language models and the substantial lexical overlap between source spoken languages and target sign languages to establish an initial alignment. Both steps make use of Non-AutoRegressive (NAR) decoding for reduced computation and faster inference speeds. Through this disentanglement of tasks, we achieve state-of-the-art BLEU and Rouge scores on the Meine DGS Annotated (mDGS) dataset, demonstrating a substantial BLUE-1 improvement of 37.88% in Text to Gloss (T2G) Translation. This innovative approach paves the way for more effective translation models for sign languages, even in resource-constrained settings.",
    "metadata": {
      "arxiv_id": "2404.11532",
      "title": "Select and Reorder: A Novel Approach for Neural Sign Language Production",
      "summary": "Sign languages, often categorised as low-resource languages, face significant challenges in achieving accurate translation due to the scarcity of parallel annotated datasets. This paper introduces Select and Reorder (S&R), a novel approach that addresses data scarcity by breaking down the translation process into two distinct steps: Gloss Selection (GS) and Gloss Reordering (GR). Our method leverages large spoken language models and the substantial lexical overlap between source spoken languages and target sign languages to establish an initial alignment. Both steps make use of Non-AutoRegressive (NAR) decoding for reduced computation and faster inference speeds. Through this disentanglement of tasks, we achieve state-of-the-art BLEU and Rouge scores on the Meine DGS Annotated (mDGS) dataset, demonstrating a substantial BLUE-1 improvement of 37.88% in Text to Gloss (T2G) Translation. This innovative approach paves the way for more effective translation models for sign languages, even in resource-constrained settings.",
      "authors": [
        "Harry Walsh",
        "Ben Saunders",
        "Richard Bowden"
      ],
      "published": "2024-04-17T16:25:19Z",
      "updated": "2024-04-17T16:25:19Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.11532v1",
      "landing_url": "https://arxiv.org/abs/2404.11532v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.11532"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "該論文聚焦於手語翻譯與產生，利用大語言模型協助對齊，但核心研究並非以 Spoken Language Models（或等價概念）作為主導的語音建模、訓練與評估，而是文本到手語的翻譯流程與手語語言單元（gloss）的分步處理，因此不符合核心納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "該論文聚焦於手語翻譯與產生，利用大語言模型協助對齊，但核心研究並非以 Spoken Language Models（或等價概念）作為主導的語音建模、訓練與評估，而是文本到手語的翻譯流程與手語語言單元（gloss）的分步處理，因此不符合核心納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sign language translation using methods leveraging spoken language models and gloss alignment. However, it centers on text and gloss translation rather than speech or spoken language as the primary modeling target, and does not explicitly address full-duplex spoken language modeling, speech-level language modeling, synchronization, or real-time interactive behaviors. It does not meet the core inclusion criteria which require speech to be the main modeling and inference object and discussion on spoken language models or equivalent. Therefore, it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sign language translation using methods leveraging spoken language models and gloss alignment. However, it centers on text and gloss translation rather than speech or spoken language as the primary modeling target, and does not explicitly address full-duplex spoken language modeling, speech-level language modeling, synchronization, or real-time interactive behaviors. It does not meet the core inclusion criteria which require speech to be the main modeling and inference object and discussion on spoken language models or equivalent. Therefore, it should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "AudioRepInceptionNeXt: A lightweight single-stream architecture for efficient audio recognition",
    "abstract": "Recent research has successfully adapted vision-based convolutional neural network (CNN) architectures for audio recognition tasks using Mel-Spectrograms. However, these CNNs have high computational costs and memory requirements, limiting their deployment on low-end edge devices. Motivated by the success of efficient vision models like InceptionNeXt and ConvNeXt, we propose AudioRepInceptionNeXt, a single-stream architecture. Its basic building block breaks down the parallel multi-branch depth-wise convolutions with descending scales of k x k kernels into a cascade of two multi-branch depth-wise convolutions. The first multi-branch consists of parallel multi-scale 1 x k depth-wise convolutional layers followed by a similar multi-branch employing parallel multi-scale k x 1 depth-wise convolutional layers. This reduces computational and memory footprint while separating time and frequency processing of Mel-Spectrograms. The large kernels capture global frequencies and long activities, while small kernels get local frequencies and short activities. We also reparameterize the multi-branch design during inference to further boost speed without losing accuracy. Experiments show that AudioRepInceptionNeXt reduces parameters and computations by 50%+ and improves inference speed 1.28x over state-of-the-art CNNs like the Slow-Fast while maintaining comparable accuracy. It also learns robustly across a variety of audio recognition tasks. Codes are available at https://github.com/StevenLauHKHK/AudioRepInceptionNeXt.",
    "metadata": {
      "arxiv_id": "2404.13551",
      "title": "AudioRepInceptionNeXt: A lightweight single-stream architecture for efficient audio recognition",
      "summary": "Recent research has successfully adapted vision-based convolutional neural network (CNN) architectures for audio recognition tasks using Mel-Spectrograms. However, these CNNs have high computational costs and memory requirements, limiting their deployment on low-end edge devices. Motivated by the success of efficient vision models like InceptionNeXt and ConvNeXt, we propose AudioRepInceptionNeXt, a single-stream architecture. Its basic building block breaks down the parallel multi-branch depth-wise convolutions with descending scales of k x k kernels into a cascade of two multi-branch depth-wise convolutions. The first multi-branch consists of parallel multi-scale 1 x k depth-wise convolutional layers followed by a similar multi-branch employing parallel multi-scale k x 1 depth-wise convolutional layers. This reduces computational and memory footprint while separating time and frequency processing of Mel-Spectrograms. The large kernels capture global frequencies and long activities, while small kernels get local frequencies and short activities. We also reparameterize the multi-branch design during inference to further boost speed without losing accuracy. Experiments show that AudioRepInceptionNeXt reduces parameters and computations by 50%+ and improves inference speed 1.28x over state-of-the-art CNNs like the Slow-Fast while maintaining comparable accuracy. It also learns robustly across a variety of audio recognition tasks. Codes are available at https://github.com/StevenLauHKHK/AudioRepInceptionNeXt.",
      "authors": [
        "Kin Wai Lau",
        "Yasar Abbas Ur Rehman",
        "Lai-Man Po"
      ],
      "published": "2024-04-21T06:33:04Z",
      "updated": "2024-04-21T06:33:04Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.13551v1",
      "landing_url": "https://arxiv.org/abs/2404.13551v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.13551"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work targets efficient audio recognition architectures for Mel-spectrogram inputs and compares CNN-based models, but it does not center on Spoken Language Models or speech-language modeling, nor on dialogue synchronization or full-duplex interactions, so it fails the core inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This work targets efficient audio recognition architectures for Mel-spectrogram inputs and compares CNN-based models, but it does not center on Spoken Language Models or speech-language modeling, nor on dialogue synchronization or full-duplex interactions, so it fails the core inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on developing an efficient CNN architecture for audio recognition using Mel-Spectrograms but does not mention Spoken Language Models (SLMs), Full-Duplex Spoken Language Models, or language modeling of speech, nor does it address synchronization, turn-taking, or real-time interactive behaviors that are key for the inclusion criteria. It primarily addresses audio processing and model efficiency rather than end-to-end spoken language modeling or interactive speech behavior, and lacks discussion of behavioral arbitration or multi-dimensional evaluation relevant to the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on developing an efficient CNN architecture for audio recognition using Mel-Spectrograms but does not mention Spoken Language Models (SLMs), Full-Duplex Spoken Language Models, or language modeling of speech, nor does it address synchronization, turn-taking, or real-time interactive behaviors that are key for the inclusion criteria. It primarily addresses audio processing and model efficiency rather than end-to-end spoken language modeling or interactive speech behavior, and lacks discussion of behavioral arbitration or multi-dimensional evaluation relevant to the criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Predicting the Temporal Dynamics of Prosthetic Vision",
    "abstract": "Retinal implants are a promising treatment option for degenerative retinal disease. While numerous models have been developed to simulate the appearance of elicited visual percepts (\"phosphenes\"), these models often either focus solely on spatial characteristics or inadequately capture the complex temporal dynamics observed in clinical trials, which vary heavily across implant technologies, subjects, and stimulus conditions. Here we introduce two computational models designed to accurately predict phosphene fading and persistence under varying stimulus conditions, cross-validated on behavioral data reported by nine users of the Argus II Retinal Prosthesis System. Both models segment the time course of phosphene perception into discrete intervals, decomposing phosphene fading and persistence into either sinusoidal or exponential components. Our spectral model demonstrates state-of-the-art predictions of phosphene intensity over time (r = 0.7 across all participants). Overall, this study lays the groundwork for enhancing prosthetic vision by improving our understanding of phosphene temporal dynamics.",
    "metadata": {
      "arxiv_id": "2404.14591",
      "title": "Predicting the Temporal Dynamics of Prosthetic Vision",
      "summary": "Retinal implants are a promising treatment option for degenerative retinal disease. While numerous models have been developed to simulate the appearance of elicited visual percepts (\"phosphenes\"), these models often either focus solely on spatial characteristics or inadequately capture the complex temporal dynamics observed in clinical trials, which vary heavily across implant technologies, subjects, and stimulus conditions. Here we introduce two computational models designed to accurately predict phosphene fading and persistence under varying stimulus conditions, cross-validated on behavioral data reported by nine users of the Argus II Retinal Prosthesis System. Both models segment the time course of phosphene perception into discrete intervals, decomposing phosphene fading and persistence into either sinusoidal or exponential components. Our spectral model demonstrates state-of-the-art predictions of phosphene intensity over time (r = 0.7 across all participants). Overall, this study lays the groundwork for enhancing prosthetic vision by improving our understanding of phosphene temporal dynamics.",
      "authors": [
        "Yuchen Hou",
        "Laya Pullela",
        "Jiaxin Su",
        "Sriya Aluru",
        "Shivani Sista",
        "Xiankun Lu",
        "Michael Beyeler"
      ],
      "published": "2024-04-22T21:35:30Z",
      "updated": "2024-05-02T00:56:00Z",
      "categories": [
        "cs.CE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.14591v3",
      "landing_url": "https://arxiv.org/abs/2404.14591v3",
      "doi": "https://doi.org/10.48550/arXiv.2404.14591"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The manuscript analyzes temporal dynamics of phosphene perception in retinal prostheses, not Spoken Language Models or speech-language modeling as required by the inclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The manuscript analyzes temporal dynamics of phosphene perception in retinal prostheses, not Spoken Language Models or speech-language modeling as required by the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on modeling temporal dynamics in prosthetic vision, which is a biomedical engineering topic unrelated to Spoken Language Models, speech modeling, or full-duplex spoken interaction systems. There is no mention of speech language models, spoken language modeling, or related concepts central to the inclusion criteria, nor is there discussion of end-to-end training, synchronization, or real-time speech interaction evaluation. Thus, this study falls outside the defined scope and inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on modeling temporal dynamics in prosthetic vision, which is a biomedical engineering topic unrelated to Spoken Language Models, speech modeling, or full-duplex spoken interaction systems. There is no mention of speech language models, spoken language modeling, or related concepts central to the inclusion criteria, nor is there discussion of end-to-end training, synchronization, or real-time speech interaction evaluation. Thus, this study falls outside the defined scope and inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Prompting Towards Alleviating Code-Switched Data Scarcity in Under-Resourced Languages with GPT as a Pivot",
    "abstract": "Many multilingual communities, including numerous in Africa, frequently engage in code-switching during conversations. This behaviour stresses the need for natural language processing technologies adept at processing code-switched text. However, data scarcity, particularly in African languages, poses a significant challenge, as many are low-resourced and under-represented. In this study, we prompted GPT 3.5 to generate Afrikaans--English and Yoruba--English code-switched sentences, enhancing diversity using topic-keyword pairs, linguistic guidelines, and few-shot examples. Our findings indicate that the quality of generated sentences for languages using non-Latin scripts, like Yoruba, is considerably lower when compared with the high Afrikaans-English success rate. There is therefore a notable opportunity to refine prompting guidelines to yield sentences suitable for the fine-tuning of language models. We propose a framework for augmenting the diversity of synthetically generated code-switched data using GPT and propose leveraging this technology to mitigate data scarcity in low-resourced languages, underscoring the essential role of native speakers in this process.",
    "metadata": {
      "arxiv_id": "2404.17216",
      "title": "Prompting Towards Alleviating Code-Switched Data Scarcity in Under-Resourced Languages with GPT as a Pivot",
      "summary": "Many multilingual communities, including numerous in Africa, frequently engage in code-switching during conversations. This behaviour stresses the need for natural language processing technologies adept at processing code-switched text. However, data scarcity, particularly in African languages, poses a significant challenge, as many are low-resourced and under-represented. In this study, we prompted GPT 3.5 to generate Afrikaans--English and Yoruba--English code-switched sentences, enhancing diversity using topic-keyword pairs, linguistic guidelines, and few-shot examples. Our findings indicate that the quality of generated sentences for languages using non-Latin scripts, like Yoruba, is considerably lower when compared with the high Afrikaans-English success rate. There is therefore a notable opportunity to refine prompting guidelines to yield sentences suitable for the fine-tuning of language models. We propose a framework for augmenting the diversity of synthetically generated code-switched data using GPT and propose leveraging this technology to mitigate data scarcity in low-resourced languages, underscoring the essential role of native speakers in this process.",
      "authors": [
        "Michelle Terblanche",
        "Kayode Olaleye",
        "Vukosi Marivate"
      ],
      "published": "2024-04-26T07:44:44Z",
      "updated": "2024-04-26T07:44:44Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.17216v1",
      "landing_url": "https://arxiv.org/abs/2404.17216v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.17216"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on generating code-switched text for Afrikaans-English and Yoruba-English using GPT prompts to augment NLP data, which is text-centric and does not address Spoken Language Models or speech-based modeling, architecture, training, or real-time interaction.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on generating code-switched text for Afrikaans-English and Yoruba-English using GPT prompts to augment NLP data, which is text-centric and does not address Spoken Language Models or speech-based modeling, architecture, training, or real-time interaction.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on generating code-switched text data using GPT for under-resourced languages, which is primarily a text-based NLP task rather than a core investigation into spoken language models or speech-level modeling. The abstract does not mention modeling or reasoning over speech or acoustic signals, full-duplex interactions, cognitive parallelism, real-time multi-task synchronization, or any of the key inclusion criteria related to spoken language models or full-duplex speech interaction frameworks. There is also no discussion of architecture, training, or evaluation related to speech as the primary modeling target. Therefore, the study does not meet the core inclusion criteria and is better excluded.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on generating code-switched text data using GPT for under-resourced languages, which is primarily a text-based NLP task rather than a core investigation into spoken language models or speech-level modeling. The abstract does not mention modeling or reasoning over speech or acoustic signals, full-duplex interactions, cognitive parallelism, real-time multi-task synchronization, or any of the key inclusion criteria related to spoken language models or full-duplex speech interaction frameworks. There is also no discussion of architecture, training, or evaluation related to speech as the primary modeling target. Therefore, the study does not meet the core inclusion criteria and is better excluded.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Logistic Map Pseudo Random Number Generator in FPGA",
    "abstract": "This project develops a pseudo-random number generator (PRNG) using the logistic map, implemented in Verilog HDL on an FPGA and processes its output through a Central Limit Theorem (CLT) function to achieve a Gaussian distribution. The system integrates additional FPGA modules for real-time interaction and visualisation, including a clock generator, UART interface, XADC, and a 7-segment display driver. These components facilitate the direct display of PRNG values on the FPGA and the transmission of data to a laptop for histogram analysis, verifying the Gaussian nature of the output. This approach demonstrates the practical application of chaotic systems for generating Gaussian-distributed pseudo-random numbers in digital hardware, highlighting the logistic map's potential in PRNG design.",
    "metadata": {
      "arxiv_id": "2404.19246",
      "title": "Logistic Map Pseudo Random Number Generator in FPGA",
      "summary": "This project develops a pseudo-random number generator (PRNG) using the logistic map, implemented in Verilog HDL on an FPGA and processes its output through a Central Limit Theorem (CLT) function to achieve a Gaussian distribution. The system integrates additional FPGA modules for real-time interaction and visualisation, including a clock generator, UART interface, XADC, and a 7-segment display driver. These components facilitate the direct display of PRNG values on the FPGA and the transmission of data to a laptop for histogram analysis, verifying the Gaussian nature of the output. This approach demonstrates the practical application of chaotic systems for generating Gaussian-distributed pseudo-random numbers in digital hardware, highlighting the logistic map's potential in PRNG design.",
      "authors": [
        "Mateo Jalen Andrew Calderon",
        "Lee Jun Lei Lucas",
        "Syarifuddin Azhar Bin Rosli",
        "Stephanie See Hui Ying",
        "Jarell Lim En Yu",
        "Maoyang Xiang",
        "T. Hui Teo"
      ],
      "published": "2024-04-30T04:03:31Z",
      "updated": "2024-04-30T04:03:31Z",
      "categories": [
        "cs.CR",
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.19246v1",
      "landing_url": "https://arxiv.org/abs/2404.19246v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.19246"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item analyzes a logistic-map PRNG on FPGA with Gaussian output and hardware interfaces, which has no focus on Spoken Language Models or speech-based modeling, nor on interactive speech evaluation; thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item analyzes a logistic-map PRNG on FPGA with Gaussian output and hardware interfaces, which has no focus on Spoken Language Models or speech-based modeling, nor on interactive speech evaluation; thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a pseudo-random number generator implemented on FPGA using logistic maps and do not mention any aspect related to Spoken Language Models (SLMs), speech language modeling, or any full-duplex spoken interaction capabilities. There is no indication that speech is the primary modeling object or that the work addresses synchronous interactive speech behavior or relevant evaluation metrics, which are core to the inclusion criteria. Therefore, this study clearly fails to meet the inclusion requirements and does not fit the research topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a pseudo-random number generator implemented on FPGA using logistic maps and do not mention any aspect related to Spoken Language Models (SLMs), speech language modeling, or any full-duplex spoken interaction capabilities. There is no indication that speech is the primary modeling object or that the work addresses synchronous interactive speech behavior or relevant evaluation metrics, which are core to the inclusion criteria. Therefore, this study clearly fails to meet the inclusion requirements and does not fit the research topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Temporal Dynamics of Emotion and Cognition in Human Translation: Integrating the Task Segment Framework and the HOF Taxonomy",
    "abstract": "The article develops a generative model of the human translating mind, grounded in empirical translation process data. It posits that three embedded processing layers unfold concurrently in the human mind, and their traces are detectable in behavioral data: sequences of routinized/automated processes are observable in fluent translation production, cognitive/reflective thoughts lead to longer keystroke pauses, while affective/emotional states may be identified through characteristic typing and gazing patterns. Utilizing data from the CRITT Translation Process Research Database (TPR-DB), the article illustrates how the temporal structure of keystroke and gaze data can be related to the three assumed hidden mental processing strata. The article relates this embedded generative model to various theoretical frameworks, dual-process theories and Robinson's (2023) ideosomatic theory of translation, opening exciting new theoretical horizons for Cognitive Translation Studies, grounded in empirical data and evaluation.",
    "metadata": {
      "arxiv_id": "2405.03111",
      "title": "Temporal Dynamics of Emotion and Cognition in Human Translation: Integrating the Task Segment Framework and the HOF Taxonomy",
      "summary": "The article develops a generative model of the human translating mind, grounded in empirical translation process data. It posits that three embedded processing layers unfold concurrently in the human mind, and their traces are detectable in behavioral data: sequences of routinized/automated processes are observable in fluent translation production, cognitive/reflective thoughts lead to longer keystroke pauses, while affective/emotional states may be identified through characteristic typing and gazing patterns. Utilizing data from the CRITT Translation Process Research Database (TPR-DB), the article illustrates how the temporal structure of keystroke and gaze data can be related to the three assumed hidden mental processing strata. The article relates this embedded generative model to various theoretical frameworks, dual-process theories and Robinson's (2023) ideosomatic theory of translation, opening exciting new theoretical horizons for Cognitive Translation Studies, grounded in empirical data and evaluation.",
      "authors": [
        "Michael Carl"
      ],
      "published": "2024-05-06T02:07:13Z",
      "updated": "2025-05-23T06:48:19Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.03111v4",
      "landing_url": "https://arxiv.org/abs/2405.03111v4",
      "doi": "https://doi.org/10.1515/dsll-2025-0002"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article centers on cognitive processing in translation (keystroke/gaze data) with no focus on Spoken Language Models or speech-language modeling, nor on end-to-end speech-anchored architectures or real-time spoken interaction.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article centers on cognitive processing in translation (keystroke/gaze data) with no focus on Spoken Language Models or speech-language modeling, nor on end-to-end speech-anchored architectures or real-time spoken interaction.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on cognitive and emotional processes during human translation, analyzing behavioral data such as keystrokes and gaze patterns. However, it does not center on spoken language models or speech as the primary modeling object, nor does it discuss model architectures, training, or evaluation related to full-duplex spoken language interaction or synchronous speech-based turn-taking. It lacks mention of key inclusion terms such as spoken language models or speech language models and does not address speech-LLM hybrid models or synchronization issues relevant to the defined scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on cognitive and emotional processes during human translation, analyzing behavioral data such as keystrokes and gaze patterns. However, it does not center on spoken language models or speech as the primary modeling object, nor does it discuss model architectures, training, or evaluation related to full-duplex spoken language interaction or synchronous speech-based turn-taking. It lacks mention of key inclusion terms such as spoken language models or speech language models and does not address speech-LLM hybrid models or synchronization issues relevant to the defined scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "CRA5: Extreme Compression of ERA5 for Portable Global Climate and Weather Research via an Efficient Variational Transformer",
    "abstract": "The advent of data-driven weather forecasting models, which learn from hundreds of terabytes (TB) of reanalysis data, has significantly advanced forecasting capabilities. However, the substantial costs associated with data storage and transmission present a major challenge for data providers and users, affecting resource-constrained researchers and limiting their accessibility to participate in AI-based meteorological research. To mitigate this issue, we introduce an efficient neural codec, the Variational Autoencoder Transformer (VAEformer), for extreme compression of climate data to significantly reduce data storage cost, making AI-based meteorological research portable to researchers. Our approach diverges from recent complex neural codecs by utilizing a low-complexity Auto-Encoder transformer. This encoder produces a quantized latent representation through variance inference, which reparameterizes the latent space as a Gaussian distribution. This method improves the estimation of distributions for cross-entropy coding. Extensive experiments demonstrate that our VAEformer outperforms existing state-of-the-art compression methods in the context of climate data. By applying our VAEformer, we compressed the most popular ERA5 climate dataset (226 TB) into a new dataset, CRA5 (0.7 TB). This translates to a compression ratio of over 300 while retaining the dataset's utility for accurate scientific analysis. Further, downstream experiments show that global weather forecasting models trained on the compact CRA5 dataset achieve forecasting accuracy comparable to the model trained on the original dataset. Code, the CRA5 dataset, and the pre-trained model are available at https://github.com/taohan10200/CRA5.",
    "metadata": {
      "arxiv_id": "2405.03376",
      "title": "CRA5: Extreme Compression of ERA5 for Portable Global Climate and Weather Research via an Efficient Variational Transformer",
      "summary": "The advent of data-driven weather forecasting models, which learn from hundreds of terabytes (TB) of reanalysis data, has significantly advanced forecasting capabilities. However, the substantial costs associated with data storage and transmission present a major challenge for data providers and users, affecting resource-constrained researchers and limiting their accessibility to participate in AI-based meteorological research. To mitigate this issue, we introduce an efficient neural codec, the Variational Autoencoder Transformer (VAEformer), for extreme compression of climate data to significantly reduce data storage cost, making AI-based meteorological research portable to researchers. Our approach diverges from recent complex neural codecs by utilizing a low-complexity Auto-Encoder transformer. This encoder produces a quantized latent representation through variance inference, which reparameterizes the latent space as a Gaussian distribution. This method improves the estimation of distributions for cross-entropy coding. Extensive experiments demonstrate that our VAEformer outperforms existing state-of-the-art compression methods in the context of climate data. By applying our VAEformer, we compressed the most popular ERA5 climate dataset (226 TB) into a new dataset, CRA5 (0.7 TB). This translates to a compression ratio of over 300 while retaining the dataset's utility for accurate scientific analysis. Further, downstream experiments show that global weather forecasting models trained on the compact CRA5 dataset achieve forecasting accuracy comparable to the model trained on the original dataset. Code, the CRA5 dataset, and the pre-trained model are available at https://github.com/taohan10200/CRA5.",
      "authors": [
        "Tao Han",
        "Zhenghao Chen",
        "Song Guo",
        "Wanghan Xu",
        "Lei Bai"
      ],
      "published": "2024-05-06T11:30:55Z",
      "updated": "2024-05-08T03:27:04Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.03376v2",
      "landing_url": "https://arxiv.org/abs/2405.03376v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.03376"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on Extreme compression of ERA5 climate data using a neural codec (VAEformer) for portable climate research and does not involve Spoken Language Models or speech-language modeling concepts, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on Extreme compression of ERA5 climate data using a neural codec (VAEformer) for portable climate research and does not involve Spoken Language Models or speech-language modeling concepts, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on compressing global climate data for AI-based weather forecasting using a variational transformer model, which is unrelated to Spoken Language Models or speech modeling; it neither addresses speech as a primary modeling target nor discusses any aspects of spoken language modeling architectures, training, or evaluation, and thus does not meet any inclusion criteria related to spoken language models or their synchronization and interaction capabilities.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on compressing global climate data for AI-based weather forecasting using a variational transformer model, which is unrelated to Spoken Language Models or speech modeling; it neither addresses speech as a primary modeling target nor discusses any aspects of spoken language modeling architectures, training, or evaluation, and thus does not meet any inclusion criteria related to spoken language models or their synchronization and interaction capabilities.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Group-aware Parameter-efficient Updating for Content-Adaptive Neural Video Compression",
    "abstract": "Content-adaptive compression is crucial for enhancing the adaptability of the pre-trained neural codec for various contents. Although these methods have been very practical in neural image compression (NIC), their application in neural video compression (NVC) is still limited due to two main aspects: 1), video compression relies heavily on temporal redundancy, therefore updating just one or a few frames can lead to significant errors accumulating over time; 2), NVC frameworks are generally more complex, with many large components that are not easy to update quickly during encoding. To address the previously mentioned challenges, we have developed a content-adaptive NVC technique called Group-aware Parameter-Efficient Updating (GPU). Initially, to minimize error accumulation, we adopt a group-aware approach for updating encoder parameters. This involves adopting a patch-based Group of Pictures (GoP) training strategy to segment a video into patch-based GoPs, which will be updated to facilitate a globally optimized domain-transferable solution. Subsequently, we introduce a parameter-efficient delta-tuning strategy, which is achieved by integrating several light-weight adapters into each coding component of the encoding process by both serial and parallel configuration. Such architecture-agnostic modules stimulate the components with large parameters, thereby reducing both the update cost and the encoding time. We incorporate our GPU into the latest NVC framework and conduct comprehensive experiments, whose results showcase outstanding video compression efficiency across four video benchmarks and adaptability of one medical image benchmark.",
    "metadata": {
      "arxiv_id": "2405.04274",
      "title": "Group-aware Parameter-efficient Updating for Content-Adaptive Neural Video Compression",
      "summary": "Content-adaptive compression is crucial for enhancing the adaptability of the pre-trained neural codec for various contents. Although these methods have been very practical in neural image compression (NIC), their application in neural video compression (NVC) is still limited due to two main aspects: 1), video compression relies heavily on temporal redundancy, therefore updating just one or a few frames can lead to significant errors accumulating over time; 2), NVC frameworks are generally more complex, with many large components that are not easy to update quickly during encoding. To address the previously mentioned challenges, we have developed a content-adaptive NVC technique called Group-aware Parameter-Efficient Updating (GPU). Initially, to minimize error accumulation, we adopt a group-aware approach for updating encoder parameters. This involves adopting a patch-based Group of Pictures (GoP) training strategy to segment a video into patch-based GoPs, which will be updated to facilitate a globally optimized domain-transferable solution. Subsequently, we introduce a parameter-efficient delta-tuning strategy, which is achieved by integrating several light-weight adapters into each coding component of the encoding process by both serial and parallel configuration. Such architecture-agnostic modules stimulate the components with large parameters, thereby reducing both the update cost and the encoding time. We incorporate our GPU into the latest NVC framework and conduct comprehensive experiments, whose results showcase outstanding video compression efficiency across four video benchmarks and adaptability of one medical image benchmark.",
      "authors": [
        "Zhenghao Chen",
        "Luping Zhou",
        "Zhihao Hu",
        "Dong Xu"
      ],
      "published": "2024-05-07T12:42:23Z",
      "updated": "2024-09-04T05:24:25Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.04274v2",
      "landing_url": "https://arxiv.org/abs/2405.04274v2",
      "doi": "https://doi.org/10.1145/3664647.3680943"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on neural video compression and parameter-efficient updating, with no mention of spoken language models, speech modeling, or interactive dialogue aspects.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on neural video compression and parameter-efficient updating, with no mention of spoken language models, speech modeling, or interactive dialogue aspects.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "This study focuses on content-adaptive neural video compression, addressing technical challenges in neural video codecs by introducing parameter-efficient updates; it does not relate to spoken language models, speech language modeling, full-duplex spoken language models, or any related speech interaction or language modeling topics specified in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "This study focuses on content-adaptive neural video compression, addressing technical challenges in neural video codecs by introducing parameter-efficient updates; it does not relate to spoken language models, speech language modeling, full-duplex spoken language models, or any related speech interaction or language modeling topics specified in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The Codecfake Dataset and Countermeasures for the Universally Detection of Deepfake Audio",
    "abstract": "With the proliferation of Audio Language Model (ALM) based deepfake audio, there is an urgent need for generalized detection methods. ALM-based deepfake audio currently exhibits widespread, high deception, and type versatility, posing a significant challenge to current audio deepfake detection (ADD) models trained solely on vocoded data. To effectively detect ALM-based deepfake audio, we focus on the mechanism of the ALM-based audio generation method, the conversion from neural codec to waveform. We initially constructed the Codecfake dataset, an open-source, large-scale collection comprising over 1 million audio samples in both English and Chinese, focus on ALM-based audio detection. As countermeasure, to achieve universal detection of deepfake audio and tackle domain ascent bias issue of original sharpness aware minimization (SAM), we propose the CSAM strategy to learn a domain balanced and generalized minima. In our experiments, we first demonstrate that ADD model training with the Codecfake dataset can effectively detects ALM-based audio. Furthermore, our proposed generalization countermeasure yields the lowest average equal error rate (EER) of 0.616% across all test conditions compared to baseline models. The dataset and associated code are available online.",
    "metadata": {
      "arxiv_id": "2405.04880",
      "title": "The Codecfake Dataset and Countermeasures for the Universally Detection of Deepfake Audio",
      "summary": "With the proliferation of Audio Language Model (ALM) based deepfake audio, there is an urgent need for generalized detection methods. ALM-based deepfake audio currently exhibits widespread, high deception, and type versatility, posing a significant challenge to current audio deepfake detection (ADD) models trained solely on vocoded data. To effectively detect ALM-based deepfake audio, we focus on the mechanism of the ALM-based audio generation method, the conversion from neural codec to waveform. We initially constructed the Codecfake dataset, an open-source, large-scale collection comprising over 1 million audio samples in both English and Chinese, focus on ALM-based audio detection. As countermeasure, to achieve universal detection of deepfake audio and tackle domain ascent bias issue of original sharpness aware minimization (SAM), we propose the CSAM strategy to learn a domain balanced and generalized minima. In our experiments, we first demonstrate that ADD model training with the Codecfake dataset can effectively detects ALM-based audio. Furthermore, our proposed generalization countermeasure yields the lowest average equal error rate (EER) of 0.616% across all test conditions compared to baseline models. The dataset and associated code are available online.",
      "authors": [
        "Yuankun Xie",
        "Yi Lu",
        "Ruibo Fu",
        "Zhengqi Wen",
        "Zhiyong Wang",
        "Jianhua Tao",
        "Xin Qi",
        "Xiaopeng Wang",
        "Yukun Liu",
        "Haonan Cheng",
        "Long Ye",
        "Yi Sun"
      ],
      "published": "2024-05-08T08:28:40Z",
      "updated": "2024-12-25T07:30:50Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.04880v3",
      "landing_url": "https://arxiv.org/abs/2405.04880v3",
      "doi": "https://doi.org/10.48550/arXiv.2405.04880"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on ALM-based deepfake audio detection and a large dataset, not on Spoken Language Models or speech-language modeling as the core object, so it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on ALM-based deepfake audio detection and a large dataset, not on Spoken Language Models or speech-language modeling as the core object, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on detection of ALM-based deepfake audio using a dataset and countermeasures, concentrating on audio signals and audio deepfake detection, but it does not explicitly involve Spoken Language Models or Full-Duplex Spoken Language Models, nor addresses speech language modeling or synchronous interaction capabilities. The research is centered on audio deepfake detection rather than model architectures, training, or evaluation of spoken language models, and lacks mention of real-time interaction, speech overlap management, or cognitive parallelism, all required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on detection of ALM-based deepfake audio using a dataset and countermeasures, concentrating on audio signals and audio deepfake detection, but it does not explicitly involve Spoken Language Models or Full-Duplex Spoken Language Models, nor addresses speech language modeling or synchronous interaction capabilities. The research is centered on audio deepfake detection rather than model architectures, training, or evaluation of spoken language models, and lacks mention of real-time interaction, speech overlap management, or cognitive parallelism, all required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Natural Language Processing RELIES on Linguistics",
    "abstract": "Large Language Models (LLMs) have become capable of generating highly fluent text in certain languages, without modules specially designed to capture grammar or semantic coherence. What does this mean for the future of linguistic expertise in NLP? We highlight several aspects in which NLP (still) relies on linguistics, or where linguistic thinking can illuminate new directions. We argue our case around the acronym RELIES that encapsulates six major facets where linguistics contributes to NLP: Resources, Evaluation, Low-resource settings, Interpretability, Explanation, and the Study of language. This list is not exhaustive, nor is linguistics the main point of reference for every effort under these themes; but at a macro level, these facets highlight the enduring importance of studying machine systems vis-à-vis systems of human language.",
    "metadata": {
      "arxiv_id": "2405.05966",
      "title": "Natural Language Processing RELIES on Linguistics",
      "summary": "Large Language Models (LLMs) have become capable of generating highly fluent text in certain languages, without modules specially designed to capture grammar or semantic coherence. What does this mean for the future of linguistic expertise in NLP? We highlight several aspects in which NLP (still) relies on linguistics, or where linguistic thinking can illuminate new directions. We argue our case around the acronym RELIES that encapsulates six major facets where linguistics contributes to NLP: Resources, Evaluation, Low-resource settings, Interpretability, Explanation, and the Study of language. This list is not exhaustive, nor is linguistics the main point of reference for every effort under these themes; but at a macro level, these facets highlight the enduring importance of studying machine systems vis-à-vis systems of human language.",
      "authors": [
        "Juri Opitz",
        "Shira Wein",
        "Nathan Schneider"
      ],
      "published": "2024-05-09T17:59:32Z",
      "updated": "2025-10-16T11:35:21Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.05966v5",
      "landing_url": "https://arxiv.org/abs/2405.05966v5",
      "doi": "https://doi.org/10.1162/coli_a_00560"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on linguistic contributions to NLP and general language modeling, not on Spoken Language Models or speech-based modeling, and it does not emphasize speech-centric architectures or evaluations, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on linguistic contributions to NLP and general language modeling, not on Spoken Language Models or speech-based modeling, and it does not emphasize speech-centric architectures or evaluations, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on the role of linguistics in NLP, especially regarding Large Language Models, but do not specifically discuss spoken language models or speech as a primary modeling object, nor do they mention architectures, training, or evaluation related to spoken language or full-duplex spoken interaction. Therefore, the study does not meet the core inclusion criteria related to spoken language models and related interactive or multiplex features.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on the role of linguistics in NLP, especially regarding Large Language Models, but do not specifically discuss spoken language models or speech as a primary modeling object, nor do they mention architectures, training, or evaluation related to spoken language or full-duplex spoken interaction. Therefore, the study does not meet the core inclusion criteria related to spoken language models and related interactive or multiplex features.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "AquaIntellect: A Semantic Self-learning Framework for Underwater Internet of Things Connectivity",
    "abstract": "The emerging paradigm of Non-Conventional Internet of Things (NC IoT), which is focused on the usefulness of information as opposed to the notion of high volume data collection and transmission, will be an important and dominant part of human life in the near future. This paper proposes a novel semantic-based approach for addressing the unique challenges posed by underwater NC IoT. We present an intelligent sensing strategy for exploring the semantics of the underwater environment by judiciously selecting the data to transmit, thereby minimizing redundancy for utmost relevant data transmission. We introduce an evolutionary function for the selection of the semantic-empowered messages relevant to the specific task within a minimum Age of Information (AoI), a freshness metric of the collected information, and for monitoring the underwater environment for performance optimization. A DNN-empowered Bayesian integrated with an adaptive surrogate model optimization will determine the optimal placement strategy of the sensors and the uncertainty level of the underwater landscape. An Adaptive Expected Improvement (AEI) mechanism is introduced to predict the optimal arrival rate for enabling a synchronized data sensing and transmission ecosystem, ensuring efficiency and timeliness. Simulation results show that the proposed solution outperforms conventional approaches.",
    "metadata": {
      "arxiv_id": "2405.07342",
      "title": "AquaIntellect: A Semantic Self-learning Framework for Underwater Internet of Things Connectivity",
      "summary": "The emerging paradigm of Non-Conventional Internet of Things (NC IoT), which is focused on the usefulness of information as opposed to the notion of high volume data collection and transmission, will be an important and dominant part of human life in the near future. This paper proposes a novel semantic-based approach for addressing the unique challenges posed by underwater NC IoT. We present an intelligent sensing strategy for exploring the semantics of the underwater environment by judiciously selecting the data to transmit, thereby minimizing redundancy for utmost relevant data transmission. We introduce an evolutionary function for the selection of the semantic-empowered messages relevant to the specific task within a minimum Age of Information (AoI), a freshness metric of the collected information, and for monitoring the underwater environment for performance optimization. A DNN-empowered Bayesian integrated with an adaptive surrogate model optimization will determine the optimal placement strategy of the sensors and the uncertainty level of the underwater landscape. An Adaptive Expected Improvement (AEI) mechanism is introduced to predict the optimal arrival rate for enabling a synchronized data sensing and transmission ecosystem, ensuring efficiency and timeliness. Simulation results show that the proposed solution outperforms conventional approaches.",
      "authors": [
        "Ananya Hazarika",
        "Mehdi Rahmati"
      ],
      "published": "2024-05-12T17:39:09Z",
      "updated": "2024-05-12T17:39:09Z",
      "categories": [
        "eess.SP",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.07342v1",
      "landing_url": "https://arxiv.org/abs/2405.07342v1",
      "doi": "https://doi.org/10.1109/VCC60689.2023.10474835"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on underwater semantic self-learning for IoT data transmission and sensor optimization, with no reference to Spoken Language Models or speech-language modeling, hence it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on underwater semantic self-learning for IoT data transmission and sensor optimization, with no reference to Spoken Language Models or speech-language modeling, hence it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a semantic self-learning framework for underwater Internet of Things connectivity, involving intelligent sensing and data transmission strategies without any mention of Spoken Language Models or models related to speech or voice-based language modeling. They do not address any aspects of spoken language modeling, full-duplex interaction, speech-based modeling structures, or the synchronization of voice interaction as required by the inclusion criteria, nor do they discuss related model architectures or evaluations. Instead, the topic centers on IoT semantic data transmission and underwater sensor networks, which is outside the scope of the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a semantic self-learning framework for underwater Internet of Things connectivity, involving intelligent sensing and data transmission strategies without any mention of Spoken Language Models or models related to speech or voice-based language modeling. They do not address any aspects of spoken language modeling, full-duplex interaction, speech-based modeling structures, or the synchronization of voice interaction as required by the inclusion criteria, nor do they discuss related model architectures or evaluations. Instead, the topic centers on IoT semantic data transmission and underwater sensor networks, which is outside the scope of the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A predictive learning model can simulate temporal dynamics and context effects found in neural representations of continuous speech",
    "abstract": "Speech perception involves storing and integrating sequentially presented items. Recent work in cognitive neuroscience has identified temporal and contextual characteristics in humans' neural encoding of speech that may facilitate this temporal processing. In this study, we simulated similar analyses with representations extracted from a computational model that was trained on unlabelled speech with the learning objective of predicting upcoming acoustics. Our simulations revealed temporal dynamics similar to those in brain signals, implying that these properties can arise without linguistic knowledge. Another property shared between brains and the model is that the encoding patterns of phonemes support some degree of cross-context generalization. However, we found evidence that the effectiveness of these generalizations depends on the specific contexts, which suggests that this analysis alone is insufficient to support the presence of context-invariant encoding.",
    "metadata": {
      "arxiv_id": "2405.08237",
      "title": "A predictive learning model can simulate temporal dynamics and context effects found in neural representations of continuous speech",
      "summary": "Speech perception involves storing and integrating sequentially presented items. Recent work in cognitive neuroscience has identified temporal and contextual characteristics in humans' neural encoding of speech that may facilitate this temporal processing. In this study, we simulated similar analyses with representations extracted from a computational model that was trained on unlabelled speech with the learning objective of predicting upcoming acoustics. Our simulations revealed temporal dynamics similar to those in brain signals, implying that these properties can arise without linguistic knowledge. Another property shared between brains and the model is that the encoding patterns of phonemes support some degree of cross-context generalization. However, we found evidence that the effectiveness of these generalizations depends on the specific contexts, which suggests that this analysis alone is insufficient to support the presence of context-invariant encoding.",
      "authors": [
        "Oli Danyi Liu",
        "Hao Tang",
        "Naomi Feldman",
        "Sharon Goldwater"
      ],
      "published": "2024-05-13T23:36:19Z",
      "updated": "2024-05-13T23:36:19Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.08237v1",
      "landing_url": "https://arxiv.org/abs/2405.08237v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.08237"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "雖然涉及以語音為核心的預測性學習模型與時序/上下文表徵，但標題/摘要未明確以 Spoken Language Models 或等價的 language modeling of speech 為核心，因此不符合明確的納入條件，儘管具有時效性。",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "雖然涉及以語音為核心的預測性學習模型與時序/上下文表徵，但標題/摘要未明確以 Spoken Language Models 或等價的 language modeling of speech 為核心，因此不符合明確的納入條件，儘管具有時效性。",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on simulating temporal and contextual neural representations of continuous speech using a predictive learning model trained on unlabelled speech to predict upcoming acoustics. However, the abstract does not explicitly mention 'Spoken Language Models' or equivalent terminology, nor does it detail a specific model architecture, training, or evaluation from the Spoken Language Model perspective. It also seems more cognitive neuroscience oriented rather than discussing full-duplex or synchronized spoken language interaction models. Therefore, it does not clearly satisfy the inclusion criteria regarding focus on Spoken Language Models or FD-SLMs and related synchronization or interaction features.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on simulating temporal and contextual neural representations of continuous speech using a predictive learning model trained on unlabelled speech to predict upcoming acoustics. However, the abstract does not explicitly mention 'Spoken Language Models' or equivalent terminology, nor does it detail a specific model architecture, training, or evaluation from the Spoken Language Model perspective. It also seems more cognitive neuroscience oriented rather than discussing full-duplex or synchronized spoken language interaction models. Therefore, it does not clearly satisfy the inclusion criteria regarding focus on Spoken Language Models or FD-SLMs and related synchronization or interaction features.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Neural Speech Coding for Real-time Communications using Constant Bitrate Scalar Quantization",
    "abstract": "Neural audio coding has emerged as a vivid research direction by promising good audio quality at very low bitrates unachievable by classical coding techniques. Here, end-to-end trainable autoencoder-like models represent the state of the art, where a discrete representation in the bottleneck of the autoencoder is learned. This allows for efficient transmission of the input audio signal. The learned discrete representation of neural codecs is typically generated by applying a quantizer to the output of the neural encoder. In almost all state-of-the-art neural audio coding approaches, this quantizer is realized as a Vector Quantizer (VQ) and a lot of effort has been spent to alleviate drawbacks of this quantization technique when used together with a neural audio coder. In this paper, we propose and analyze simple alternatives to VQ, which are based on projected Scalar Quantization (SQ). These quantization techniques do not need any additional losses, scheduling parameters or codebook storage thereby simplifying the training of neural audio codecs. For real-time speech communication applications, these neural codecs are required to operate at low complexity, low latency and at low bitrates. We address those challenges by proposing a new causal network architecture that is based on SQ and a Short-Time Fourier Transform (STFT) representation. The proposed method performs particularly well in the very low complexity and low bitrate regime.",
    "metadata": {
      "arxiv_id": "2405.08417",
      "title": "Neural Speech Coding for Real-time Communications using Constant Bitrate Scalar Quantization",
      "summary": "Neural audio coding has emerged as a vivid research direction by promising good audio quality at very low bitrates unachievable by classical coding techniques. Here, end-to-end trainable autoencoder-like models represent the state of the art, where a discrete representation in the bottleneck of the autoencoder is learned. This allows for efficient transmission of the input audio signal. The learned discrete representation of neural codecs is typically generated by applying a quantizer to the output of the neural encoder. In almost all state-of-the-art neural audio coding approaches, this quantizer is realized as a Vector Quantizer (VQ) and a lot of effort has been spent to alleviate drawbacks of this quantization technique when used together with a neural audio coder. In this paper, we propose and analyze simple alternatives to VQ, which are based on projected Scalar Quantization (SQ). These quantization techniques do not need any additional losses, scheduling parameters or codebook storage thereby simplifying the training of neural audio codecs. For real-time speech communication applications, these neural codecs are required to operate at low complexity, low latency and at low bitrates. We address those challenges by proposing a new causal network architecture that is based on SQ and a Short-Time Fourier Transform (STFT) representation. The proposed method performs particularly well in the very low complexity and low bitrate regime.",
      "authors": [
        "Andreas Brendel",
        "Nicola Pia",
        "Kishan Gupta",
        "Lyonel Behringer",
        "Guillaume Fuchs",
        "Markus Multrus"
      ],
      "published": "2024-05-14T08:23:30Z",
      "updated": "2024-09-19T12:02:07Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.08417v2",
      "landing_url": "https://arxiv.org/abs/2405.08417v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.08417"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本研究聚焦神經音頻編碼與量化方法，核心是音頻信號的編碼與實時傳輸，不涉及 Spoken Language Models / 語音語言建模的架構訓練與評估，也不討論同步互動或對話行為仲裁，因此不符合納入條件，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本研究聚焦神經音頻編碼與量化方法，核心是音頻信號的編碼與實時傳輸，不涉及 Spoken Language Models / 語音語言建模的架構訓練與評估，也不討論同步互動或對話行為仲裁，因此不符合納入條件，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on neural speech coding for real-time communications, emphasizing audio quality, low bitrate coding, and neural network architectures, but it does not explicitly address spoken language models or full-duplex spoken language interaction, nor does it discuss speech language modeling, synchronization, or interactive conversational behavior as required by the inclusion criteria. Additionally, it lacks discussion on model training, inference, or evaluation from an SLM perspective and does not mention terms like spoken language model(s) or speech language model(s). Therefore, it does not meet the core inclusion criteria and aligns more with neural codec technology than the specified research domain.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on neural speech coding for real-time communications, emphasizing audio quality, low bitrate coding, and neural network architectures, but it does not explicitly address spoken language models or full-duplex spoken language interaction, nor does it discuss speech language modeling, synchronization, or interactive conversational behavior as required by the inclusion criteria. Additionally, it lacks discussion on model training, inference, or evaluation from an SLM perspective and does not mention terms like spoken language model(s) or speech language model(s). Therefore, it does not meet the core inclusion criteria and aligns more with neural codec technology than the specified research domain.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model",
    "abstract": "Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.",
    "metadata": {
      "arxiv_id": "2405.09768",
      "title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model",
      "summary": "Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.",
      "authors": [
        "Siyang Wang",
        "Éva Székely"
      ],
      "published": "2024-05-16T02:18:41Z",
      "updated": "2024-05-16T02:18:41Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.09768v1",
      "landing_url": "https://arxiv.org/abs/2405.09768v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.09768"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Item explicitly centers on Speech Language Models for TTS and discusses SLM-based architecture and evaluation in the title/abstract, and falls within the required 3-year window, so it should be included.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "Item explicitly centers on Speech Language Models for TTS and discusses SLM-based architecture and evaluation in the title/abstract, and falls within the required 3-year window, so it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on text-to-speech synthesis from a discrete token-based speech language model, discussing model evaluation across multiple dimensions, including prosody and speaker consistency. It centers on a speech language model with direct modeling of discrete speech tokens and evaluates the model's performance, aligning with the inclusion criteria concerning spoken language models as core objects of modeling and evaluation. However, the study appears to address a single downstream task (TTS) rather than comprehensive full-duplex spoken language modeling or synchronous interactive dialogue behaviors, and it does not mention cognitive parallelism, low latency interaction, or behavior arbitration. It also lacks discussion on synchronous interaction or duplex communication capabilities; hence it misses key inclusion features related to full-duplex or multi-turn interaction capabilities. Still, it is closely related to the research landscape of speech language models and includes model evaluation beyond simple metrics such as intelligibility and naturalness (though no mention of interaction or behavior arbitration). Thus, it moderately satisfies inclusion but partially meets exclusion criteria related to lack of duplex or dialogue behavior support and focus on a single downstream task. The time frame (recent advances) presumably meets the 3-year criterion but is not explicitly dated. Given these balanced pros and cons, the study merits a cautious position on inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on text-to-speech synthesis from a discrete token-based speech language model, discussing model evaluation across multiple dimensions, including prosody and speaker consistency. It centers on a speech language model with direct modeling of discrete speech tokens and evaluates the model's performance, aligning with the inclusion criteria concerning spoken language models as core objects of modeling and evaluation. However, the study appears to address a single downstream task (TTS) rather than comprehensive full-duplex spoken language modeling or synchronous interactive dialogue behaviors, and it does not mention cognitive parallelism, low latency interaction, or behavior arbitration. It also lacks discussion on synchronous interaction or duplex communication capabilities; hence it misses key inclusion features related to full-duplex or multi-turn interaction capabilities. Still, it is closely related to the research landscape of speech language models and includes model evaluation beyond simple metrics such as intelligibility and naturalness (though no mention of interaction or behavior arbitration). Thus, it moderately satisfies inclusion but partially meets exclusion criteria related to lack of duplex or dialogue behavior support and focus on a single downstream task. The time frame (recent advances) presumably meets the 3-year criterion but is not explicitly dated. Given these balanced pros and cons, the study merits a cautious position on inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviews disagreed: one (JuniorNano) included the paper as it explicitly centers on SLMs; the other (JuniorMini) was cautious, noting it focuses on a single downstream TTS task and lacks discussion of full‑duplex/synchronous interaction. My judgement: although the paper explicitly studies a discrete token-based Speech Language Model and provides detailed evaluation, it only addresses TTS as a single downstream task and does not cover full‑duplex interaction, cognitive parallelism, overlap/interrupt handling, or low-latency synchronous behaviors required by the topic—therefore it meets the exclusion criteria. Brief explanation (1 sentence): Relevant to SLMs but limited to single-task TTS evaluation without any full‑duplex/synchronous interaction or behavior‑arbitration analysis, so better to exclude.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviews disagreed: one (JuniorNano) included the paper as it explicitly centers on SLMs; the other (JuniorMini) was cautious, noting it focuses on a single downstream TTS task and lacks discussion of full‑duplex/synchronous interaction. My judgement: although the paper explicitly studies a discrete token-based Speech Language Model and provides detailed evaluation, it only addresses TTS as a single downstream task and does not cover full‑duplex interaction, cognitive parallelism, overlap/interrupt handling, or low-latency synchronous behaviors required by the topic—therefore it meets the exclusion criteria. Brief explanation (1 sentence): Relevant to SLMs but limited to single-task TTS evaluation without any full‑duplex/synchronous interaction or behavior‑arbitration analysis, so better to exclude.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Generating Coherent Sequences of Visual Illustrations for Real-World Manual Tasks",
    "abstract": "Multistep instructions, such as recipes and how-to guides, greatly benefit from visual aids, such as a series of images that accompany the instruction steps. While Large Language Models (LLMs) have become adept at generating coherent textual steps, Large Vision/Language Models (LVLMs) are less capable of generating accompanying image sequences. The most challenging aspect is that each generated image needs to adhere to the relevant textual step instruction, as well as be visually consistent with earlier images in the sequence. To address this problem, we propose an approach for generating consistent image sequences, which integrates a Latent Diffusion Model (LDM) with an LLM to transform the sequence into a caption to maintain the semantic coherence of the sequence. In addition, to maintain the visual coherence of the image sequence, we introduce a copy mechanism to initialise reverse diffusion processes with a latent vector iteration from a previously generated image from a relevant step. Both strategies will condition the reverse diffusion process on the sequence of instruction steps and tie the contents of the current image to previous instruction steps and corresponding images. Experiments show that the proposed approach is preferred by humans in 46.6% of the cases against 26.6% for the second best method. In addition, automatic metrics showed that the proposed method maintains semantic coherence and visual consistency across steps in both domains.",
    "metadata": {
      "arxiv_id": "2405.10122",
      "title": "Generating Coherent Sequences of Visual Illustrations for Real-World Manual Tasks",
      "summary": "Multistep instructions, such as recipes and how-to guides, greatly benefit from visual aids, such as a series of images that accompany the instruction steps. While Large Language Models (LLMs) have become adept at generating coherent textual steps, Large Vision/Language Models (LVLMs) are less capable of generating accompanying image sequences. The most challenging aspect is that each generated image needs to adhere to the relevant textual step instruction, as well as be visually consistent with earlier images in the sequence. To address this problem, we propose an approach for generating consistent image sequences, which integrates a Latent Diffusion Model (LDM) with an LLM to transform the sequence into a caption to maintain the semantic coherence of the sequence. In addition, to maintain the visual coherence of the image sequence, we introduce a copy mechanism to initialise reverse diffusion processes with a latent vector iteration from a previously generated image from a relevant step. Both strategies will condition the reverse diffusion process on the sequence of instruction steps and tie the contents of the current image to previous instruction steps and corresponding images. Experiments show that the proposed approach is preferred by humans in 46.6% of the cases against 26.6% for the second best method. In addition, automatic metrics showed that the proposed method maintains semantic coherence and visual consistency across steps in both domains.",
      "authors": [
        "João Bordalo",
        "Vasco Ramos",
        "Rodrigo Valério",
        "Diogo Glória-Silva",
        "Yonatan Bitton",
        "Michal Yarom",
        "Idan Szpektor",
        "Joao Magalhaes"
      ],
      "published": "2024-05-16T14:22:20Z",
      "updated": "2024-05-16T14:22:20Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.10122v1",
      "landing_url": "https://arxiv.org/abs/2405.10122v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.10122"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses visual sequence generation for manual tasks using LVLMs, not Spoken Language Models / Speech Language Models or speech-centric modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses visual sequence generation for manual tasks using LVLMs, not Spoken Language Models / Speech Language Models or speech-centric modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on generating image sequences for visual aids in multistep instructions using large language and vision models. It does not concern spoken language models, speech as a modeling target, or full-duplex spoken interaction, nor does it address speech-language modeling architectures or evaluations as required. Therefore, it does not meet the inclusion criteria centered on spoken language models and related concepts, and falls under exclusion criteria by focusing on visual-language modeling rather than speech modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on generating image sequences for visual aids in multistep instructions using large language and vision models. It does not concern spoken language models, speech as a modeling target, or full-duplex spoken interaction, nor does it address speech-language modeling architectures or evaluations as required. Therefore, it does not meet the inclusion criteria centered on spoken language models and related concepts, and falls under exclusion criteria by focusing on visual-language modeling rather than speech modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Modeling Real-Time Interactive Conversations as Timed Diarized Transcripts",
    "abstract": "Chatbots built upon language models have exploded in popularity, but they have largely been limited to synchronous, turn-by-turn dialogues. In this paper we present a simple yet general method to simulate real-time interactive conversations using pretrained text-only language models, by modeling timed diarized transcripts and decoding them with causal rejection sampling. We demonstrate the promise of this method with two case studies: instant messenger dialogues and spoken conversations, which require generation at about 30 tok/s and 20 tok/s respectively to maintain real-time interactivity. These capabilities can be added into language models using relatively little data and run on commodity hardware.",
    "metadata": {
      "arxiv_id": "2405.13203",
      "title": "Modeling Real-Time Interactive Conversations as Timed Diarized Transcripts",
      "summary": "Chatbots built upon language models have exploded in popularity, but they have largely been limited to synchronous, turn-by-turn dialogues. In this paper we present a simple yet general method to simulate real-time interactive conversations using pretrained text-only language models, by modeling timed diarized transcripts and decoding them with causal rejection sampling. We demonstrate the promise of this method with two case studies: instant messenger dialogues and spoken conversations, which require generation at about 30 tok/s and 20 tok/s respectively to maintain real-time interactivity. These capabilities can be added into language models using relatively little data and run on commodity hardware.",
      "authors": [
        "Garrett Tanzer",
        "Gustaf Ahdritz",
        "Luke Melas-Kyriazi"
      ],
      "published": "2024-05-21T21:14:31Z",
      "updated": "2024-05-21T21:14:31Z",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.13203v1",
      "landing_url": "https://arxiv.org/abs/2405.13203v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.13203"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目以文字為核心的實時互動對話模型為主，未把語音作為核心建模對象，亦未討論語音語言建模架構、訓練或評估等，因此不符合「Spoken Language Models / FD-SLMs 為核心且以語音為主要建模對象」的 inclusion 條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目以文字為核心的實時互動對話模型為主，未把語音作為核心建模對象，亦未討論語音語言建模架構、訓練或評估等，因此不符合「Spoken Language Models / FD-SLMs 為核心且以語音為主要建模對象」的 inclusion 條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on simulating real-time interactive conversations using pretrained text-only language models and diarized transcripts, centering on text-based modeling rather than spoken language modeling or acoustic signal processing; it does not address spoken language models or full-duplex spoken interaction with speech data as the core modeling object, nor does it discuss speech-level language modeling or evaluation related to speech overlap, interruption, or low-latency interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on simulating real-time interactive conversations using pretrained text-only language models and diarized transcripts, centering on text-based modeling rather than spoken language modeling or acoustic signal processing; it does not address spoken language models or full-duplex spoken interaction with speech data as the core modeling object, nor does it discuss speech-level language modeling or evaluation related to speech overlap, interruption, or low-latency interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Half-duplex communication complexity with adversary can be less than the classical communication complexity",
    "abstract": "Half-duplex communication complexity with adversary was defined in [Hoover, K., Impagliazzo, R., Mihajlin, I., Smal, A. V. Half-Duplex Communication Complexity, ISAAC 2018.] Half-duplex communication protocols generalize classical protocols defined by Andrew Yao in [Yao, A. C.-C. Some Complexity Questions Related to Distributive Computing (Preliminary Report), STOC 1979]. It has been unknown so far whether the communication complexities defined by these models are different or not. In the present paper we answer this question: we exhibit a function whose half-duplex communication complexity with adversary is strictly less than its classical communication complexity.",
    "metadata": {
      "arxiv_id": "2405.16881",
      "title": "Half-duplex communication complexity with adversary can be less than the classical communication complexity",
      "summary": "Half-duplex communication complexity with adversary was defined in [Hoover, K., Impagliazzo, R., Mihajlin, I., Smal, A. V. Half-Duplex Communication Complexity, ISAAC 2018.] Half-duplex communication protocols generalize classical protocols defined by Andrew Yao in [Yao, A. C.-C. Some Complexity Questions Related to Distributive Computing (Preliminary Report), STOC 1979]. It has been unknown so far whether the communication complexities defined by these models are different or not. In the present paper we answer this question: we exhibit a function whose half-duplex communication complexity with adversary is strictly less than its classical communication complexity.",
      "authors": [
        "Mikhail Dektiarev",
        "Nikolay Vereshchagin"
      ],
      "published": "2024-05-27T06:53:46Z",
      "updated": "2024-05-27T06:53:46Z",
      "categories": [
        "cs.CC",
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.16881v1",
      "landing_url": "https://arxiv.org/abs/2405.16881v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.16881"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on half-duplex communication complexity in theoretical CS and does not center Spoken Language Models or speech-language modeling, thus it fails the inclusion criteria and meets the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on half-duplex communication complexity in theoretical CS and does not center Spoken Language Models or speech-language modeling, thus it fails the inclusion criteria and meets the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on communication complexity theory and half-duplex communication protocols in a computational complexity context, which is unrelated to Spoken Language Models or speech language modeling; there is no mention of spoken language models, speech data modeling, full-duplex capabilities, or any of the inclusion criteria related to speech modeling and synchronization interaction. It does not meet the inclusion criteria and fits into exclusion categories such as focusing on traditional half-duplex communication without dialogue behavior arbitration or synchronization relevant to spoken language models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on communication complexity theory and half-duplex communication protocols in a computational complexity context, which is unrelated to Spoken Language Models or speech language modeling; there is no mention of spoken language models, speech data modeling, full-duplex capabilities, or any of the inclusion criteria related to speech modeling and synchronization interaction. It does not meet the inclusion criteria and fits into exclusion categories such as focusing on traditional half-duplex communication without dialogue behavior arbitration or synchronization relevant to spoken language models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SCaRL- A Synthetic Multi-Modal Dataset for Autonomous Driving",
    "abstract": "We present a novel synthetically generated multi-modal dataset, SCaRL, to enable the training and validation of autonomous driving solutions. Multi-modal datasets are essential to attain the robustness and high accuracy required by autonomous systems in applications such as autonomous driving. As deep learning-based solutions are becoming more prevalent for object detection, classification, and tracking tasks, there is great demand for datasets combining camera, lidar, and radar sensors. Existing real/synthetic datasets for autonomous driving lack synchronized data collection from a complete sensor suite. SCaRL provides synchronized Synthetic data from RGB, semantic/instance, and depth Cameras; Range-Doppler-Azimuth/Elevation maps and raw data from Radar; and 3D point clouds/2D maps of semantic, depth and Doppler data from coherent Lidar. SCaRL is a large dataset based on the CARLA Simulator, which provides data for diverse, dynamic scenarios and traffic conditions. SCaRL is the first dataset to include synthetic synchronized data from coherent Lidar and MIMO radar sensors. The dataset can be accessed here: https://fhr-ihs-sva.pages.fraunhofer.de/asp/scarl/",
    "metadata": {
      "arxiv_id": "2405.17030",
      "title": "SCaRL- A Synthetic Multi-Modal Dataset for Autonomous Driving",
      "summary": "We present a novel synthetically generated multi-modal dataset, SCaRL, to enable the training and validation of autonomous driving solutions. Multi-modal datasets are essential to attain the robustness and high accuracy required by autonomous systems in applications such as autonomous driving. As deep learning-based solutions are becoming more prevalent for object detection, classification, and tracking tasks, there is great demand for datasets combining camera, lidar, and radar sensors. Existing real/synthetic datasets for autonomous driving lack synchronized data collection from a complete sensor suite. SCaRL provides synchronized Synthetic data from RGB, semantic/instance, and depth Cameras; Range-Doppler-Azimuth/Elevation maps and raw data from Radar; and 3D point clouds/2D maps of semantic, depth and Doppler data from coherent Lidar. SCaRL is a large dataset based on the CARLA Simulator, which provides data for diverse, dynamic scenarios and traffic conditions. SCaRL is the first dataset to include synthetic synchronized data from coherent Lidar and MIMO radar sensors.\n  The dataset can be accessed here: https://fhr-ihs-sva.pages.fraunhofer.de/asp/scarl/",
      "authors": [
        "Avinash Nittur Ramesh",
        "Aitor Correas-Serrano",
        "María González-Huici"
      ],
      "published": "2024-05-27T10:31:26Z",
      "updated": "2024-05-27T10:31:26Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.17030v1",
      "landing_url": "https://arxiv.org/abs/2405.17030v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.17030"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes a synthetic multi-modal autonomous driving dataset and does not center on Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria; exclusion criteria related to speech interaction do not apply here beyond not being relevant, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes a synthetic multi-modal autonomous driving dataset and does not center on Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria; exclusion criteria related to speech interaction do not apply here beyond not being relevant, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article presents a synthetic multi-modal dataset for autonomous driving focusing on sensor data (camera, lidar, radar) and does not address Spoken Language Models, speech-based modeling, or full-duplex speech interactions as specified in the inclusion criteria. Therefore, it does not meet the core research focus required for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article presents a synthetic multi-modal dataset for autonomous driving focusing on sensor data (camera, lidar, radar) and does not address Spoken Language Models, speech-based modeling, or full-duplex speech interactions as specified in the inclusion criteria. Therefore, it does not meet the core research focus required for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model",
    "abstract": "Recent advancements in large language models (LLMs) have highlighted the importance of extending context lengths for handling complex tasks. While traditional methods for training on long contexts often use filtered long documents, these approaches lead to domain imbalances, limiting model performance. To address this, techniques like random document concatenation (Standard) and similarity-based methods (KNN, ICLM) have been developed. However, they either sacrifice semantic coherence or diversity. To balance both aspects, we introduce Quest, a query-centric data synthesis method aggregating semantically relevant yet diverse documents. Quest uses a generative model to predict potential queries for each document, grouping documents with similar queries and keywords. Extensive experiments demonstrate Quest's superior performance on long-context tasks, achieving remarkable results with context lengths of up to 1M tokens and confirming its scalability across various model sizes.",
    "metadata": {
      "arxiv_id": "2405.19846",
      "title": "Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model",
      "summary": "Recent advancements in large language models (LLMs) have highlighted the importance of extending context lengths for handling complex tasks. While traditional methods for training on long contexts often use filtered long documents, these approaches lead to domain imbalances, limiting model performance. To address this, techniques like random document concatenation (Standard) and similarity-based methods (KNN, ICLM) have been developed. However, they either sacrifice semantic coherence or diversity. To balance both aspects, we introduce Quest, a query-centric data synthesis method aggregating semantically relevant yet diverse documents. Quest uses a generative model to predict potential queries for each document, grouping documents with similar queries and keywords. Extensive experiments demonstrate Quest's superior performance on long-context tasks, achieving remarkable results with context lengths of up to 1M tokens and confirming its scalability across various model sizes.",
      "authors": [
        "Chaochen Gao",
        "Xing Wu",
        "Qi Fu",
        "Songlin Hu"
      ],
      "published": "2024-05-30T08:50:55Z",
      "updated": "2025-02-11T06:22:30Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.19846v7",
      "landing_url": "https://arxiv.org/abs/2405.19846v7",
      "doi": "https://doi.org/10.48550/arXiv.2405.19846"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此論文聚焦於長上下文的資料合成以擴展大語言模型，並未以 Spoken Language Models / 語音互動為核心，也未討論語音建模或語音–文字混合架構，因此不符合本研究的語音語言建模相關收錄條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此論文聚焦於長上下文的資料合成以擴展大語言模型，並未以 Spoken Language Models / 語音互動為核心，也未討論語音建模或語音–文字混合架構，因此不符合本研究的語音語言建模相關收錄條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on data synthesis approaches for long-context scaling of large language models, addressing text-based semantic coherence and scalability, without any mention of spoken language models, speech-specific modeling, full-duplex interaction, or speech-language model architectures that are the core inclusion criteria. Therefore, it does not align with the inclusion requirements related to speech as a primary modeling object or synchronous spoken interaction capabilities, and it fits the exclusion of studies focusing solely on text-based NLP without speech modeling innovation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on data synthesis approaches for long-context scaling of large language models, addressing text-based semantic coherence and scalability, without any mention of spoken language models, speech-specific modeling, full-duplex interaction, or speech-language model architectures that are the core inclusion criteria. Therefore, it does not align with the inclusion requirements related to speech as a primary modeling object or synchronous spoken interaction capabilities, and it fits the exclusion of studies focusing solely on text-based NLP without speech modeling innovation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "RapVerse: Coherent Vocals and Whole-Body Motions Generations from Text",
    "abstract": "In this work, we introduce a challenging task for simultaneously generating 3D holistic body motions and singing vocals directly from textual lyrics inputs, advancing beyond existing works that typically address these two modalities in isolation. To facilitate this, we first collect the RapVerse dataset, a large dataset containing synchronous rapping vocals, lyrics, and high-quality 3D holistic body meshes. With the RapVerse dataset, we investigate the extent to which scaling autoregressive multimodal transformers across language, audio, and motion can enhance the coherent and realistic generation of vocals and whole-body human motions. For modality unification, a vector-quantized variational autoencoder is employed to encode whole-body motion sequences into discrete motion tokens, while a vocal-to-unit model is leveraged to obtain quantized audio tokens preserving content, prosodic information and singer identity. By jointly performing transformer modeling on these three modalities in a unified way, our framework ensures a seamless and realistic blend of vocals and human motions. Extensive experiments demonstrate that our unified generation framework not only produces coherent and realistic singing vocals alongside human motions directly from textual inputs, but also rivals the performance of specialized single-modality generation systems, establishing new benchmarks for joint vocal-motion generation.",
    "metadata": {
      "arxiv_id": "2405.20336",
      "title": "RapVerse: Coherent Vocals and Whole-Body Motions Generations from Text",
      "summary": "In this work, we introduce a challenging task for simultaneously generating 3D holistic body motions and singing vocals directly from textual lyrics inputs, advancing beyond existing works that typically address these two modalities in isolation. To facilitate this, we first collect the RapVerse dataset, a large dataset containing synchronous rapping vocals, lyrics, and high-quality 3D holistic body meshes. With the RapVerse dataset, we investigate the extent to which scaling autoregressive multimodal transformers across language, audio, and motion can enhance the coherent and realistic generation of vocals and whole-body human motions. For modality unification, a vector-quantized variational autoencoder is employed to encode whole-body motion sequences into discrete motion tokens, while a vocal-to-unit model is leveraged to obtain quantized audio tokens preserving content, prosodic information and singer identity. By jointly performing transformer modeling on these three modalities in a unified way, our framework ensures a seamless and realistic blend of vocals and human motions. Extensive experiments demonstrate that our unified generation framework not only produces coherent and realistic singing vocals alongside human motions directly from textual inputs, but also rivals the performance of specialized single-modality generation systems, establishing new benchmarks for joint vocal-motion generation.",
      "authors": [
        "Jiaben Chen",
        "Xin Yan",
        "Yihang Chen",
        "Siyuan Cen",
        "Zixin Wang",
        "Qinwei Ma",
        "Haoyu Zhen",
        "Kaizhi Qian",
        "Lie Lu",
        "Chuang Gan"
      ],
      "published": "2024-05-30T17:59:39Z",
      "updated": "2025-12-14T23:06:36Z",
      "categories": [
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.20336v3",
      "landing_url": "https://arxiv.org/abs/2405.20336v3",
      "doi": "https://doi.org/10.48550/arXiv.2405.20336"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the work enables joint generation of vocals and full-body motions from text with multimodal transformers, it does not clearly center Spoken Language Models or an equivalent speech-language modeling core, and lacks explicit focus on speech-language modeling tasks (beyond using audio as a modality), so it fails the inclusion criteria despite being recent and scholarly.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the work enables joint generation of vocals and full-body motions from text with multimodal transformers, it does not clearly center Spoken Language Models or an equivalent speech-language modeling core, and lacks explicit focus on speech-language modeling tasks (beyond using audio as a modality), so it fails the inclusion criteria despite being recent and scholarly.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces a novel multimodal generation task combining 3D body motions and singing vocals from textual lyrics, but it does not clearly focus on spoken language modeling or speech language models as core objects, nor does it address full-duplex spoken language models, turn-taking, or synchronous spoken interaction behaviors. Additionally, the paper's focus is more on multimodal generation than on speech language modeling or dialogue interaction, and there is no mention of evaluation oriented towards real-time interactive speech behaviors or language modeling of speech tokens as defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study introduces a novel multimodal generation task combining 3D body motions and singing vocals from textual lyrics, but it does not clearly focus on spoken language modeling or speech language models as core objects, nor does it address full-duplex spoken language models, turn-taking, or synchronous spoken interaction behaviors. Additionally, the paper's focus is more on multimodal generation than on speech language modeling or dialogue interaction, and there is no mention of evaluation oriented towards real-time interactive speech behaviors or language modeling of speech tokens as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MaskSR: Masked Language Model for Full-band Speech Restoration",
    "abstract": "Speech restoration aims at restoring high quality speech in the presence of a diverse set of distortions. Although several deep learning paradigms have been studied for this task, the power of the recently emerging language models has not been fully explored. In this paper, we propose MaskSR, a masked language model capable of restoring full-band 44.1 kHz speech jointly considering noise, reverb, clipping, and low bandwidth. MaskSR works with discrete acoustic tokens extracted using a pre-trained neural codec. During training, MaskSR is optimized to predict randomly masked tokens extracted from the high quality target speech, conditioned on the corrupted speech with various distortions. During inference, MaskSR reconstructs the target speech tokens with efficient iterative sampling. Extensive experiments show that MaskSR obtains competitive results on both the full-band speech restoration task and also on sub-tasks compared with a wide range of models.",
    "metadata": {
      "arxiv_id": "2406.02092",
      "title": "MaskSR: Masked Language Model for Full-band Speech Restoration",
      "summary": "Speech restoration aims at restoring high quality speech in the presence of a diverse set of distortions. Although several deep learning paradigms have been studied for this task, the power of the recently emerging language models has not been fully explored. In this paper, we propose MaskSR, a masked language model capable of restoring full-band 44.1 kHz speech jointly considering noise, reverb, clipping, and low bandwidth. MaskSR works with discrete acoustic tokens extracted using a pre-trained neural codec. During training, MaskSR is optimized to predict randomly masked tokens extracted from the high quality target speech, conditioned on the corrupted speech with various distortions. During inference, MaskSR reconstructs the target speech tokens with efficient iterative sampling. Extensive experiments show that MaskSR obtains competitive results on both the full-band speech restoration task and also on sub-tasks compared with a wide range of models.",
      "authors": [
        "Xu Li",
        "Qirui Wang",
        "Xiaoyu Liu"
      ],
      "published": "2024-06-04T08:23:57Z",
      "updated": "2024-06-04T08:23:57Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02092v1",
      "landing_url": "https://arxiv.org/abs/2406.02092v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02092"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此論文以離散聲學 token 的 masked language model 進行全頻語音修復，將語音視為主要建模與推理對象，著重於模型架構、訓練與評估，且符合近三年期間及有 spoken language model / speech language model 的描述，因此符合納入條件（非僅 ASR/TTS、非半雙工等排除條件）。",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "此論文以離散聲學 token 的 masked language model 進行全頻語音修復，將語音視為主要建模與推理對象，著重於模型架構、訓練與評估，且符合近三年期間及有 spoken language model / speech language model 的描述，因此符合納入條件（非僅 ASR/TTS、非半雙工等排除條件）。",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on speech restoration using a masked language model with discrete acoustic tokens, which is related to speech processing but does not explicitly position itself as a Spoken Language Model (SLM) or Full-Duplex Spoken Language Model (FD-SLM), nor does it address synchronous interaction capabilities, cognitive parallelism, or real-time conversational behaviors required by the inclusion criteria. The abstract lacks mention of modeling or evaluation of spoken language interaction or synchronous interaction behaviors, and it centers on speech restoration quality rather than dialogue or language modeling aspects. Therefore, it does not meet key inclusion criteria nor violate exclusion criteria explicitly, but overall it does not fit the target scope well, suggesting exclusion is preferable.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on speech restoration using a masked language model with discrete acoustic tokens, which is related to speech processing but does not explicitly position itself as a Spoken Language Model (SLM) or Full-Duplex Spoken Language Model (FD-SLM), nor does it address synchronous interaction capabilities, cognitive parallelism, or real-time conversational behaviors required by the inclusion criteria. The abstract lacks mention of modeling or evaluation of spoken language interaction or synchronous interaction behaviors, and it centers on speech restoration quality rather than dialogue or language modeling aspects. Therefore, it does not meet key inclusion criteria nor violate exclusion criteria explicitly, but overall it does not fit the target scope well, suggesting exclusion is preferable.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviews correctly note that the paper uses a masked language model over discrete acoustic tokens (JuniorNano) but the manuscript is focused on a speech-restoration single-task and the abstract lacks any mention of Spoken Language Models / Full‑Duplex interaction capabilities, synchronous interaction behaviors, or evaluation of dialogue/real-time interaction (JuniorMini). Because the inclusion criteria emphasize work that explicitly positions itself as an SLM or equivalent and/or addresses interactive/full‑duplex spoken language modeling (beyond task-specific restoration), this paper does not meet the core topical scope despite using a speech-token LM approach, so it is better to exclude.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviews correctly note that the paper uses a masked language model over discrete acoustic tokens (JuniorNano) but the manuscript is focused on a speech-restoration single-task and the abstract lacks any mention of Spoken Language Models / Full‑Duplex interaction capabilities, synchronous interaction behaviors, or evaluation of dialogue/real-time interaction (JuniorMini). Because the inclusion criteria emphasize work that explicitly positions itself as an SLM or equivalent and/or addresses interactive/full‑duplex spoken language modeling (beyond task-specific restoration), this paper does not meet the core topical scope despite using a speech-token LM approach, so it is better to exclude.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "On The Statistical Representation Properties Of The Perturb-Softmax And The Perturb-Argmax Probability Distributions",
    "abstract": "The Gumbel-Softmax probability distribution allows learning discrete tokens in generative learning, while the Gumbel-Argmax probability distribution is useful in learning discrete structures in discriminative learning. Despite the efforts invested in optimizing these probability models, their statistical properties are under-explored. In this work, we investigate their representation properties and determine for which families of parameters these probability distributions are complete, i.e., can represent any probability distribution, and minimal, i.e., can represent a probability distribution uniquely. We rely on convexity and differentiability to determine these statistical conditions and extend this framework to general probability models, such as Gaussian-Softmax and Gaussian-Argmax. We experimentally validate the qualities of these extensions, which enjoy a faster convergence rate. We conclude the analysis by identifying two sets of parameters that satisfy these assumptions and thus admit a complete and minimal representation. Our contribution is theoretical with supporting practical evaluation.",
    "metadata": {
      "arxiv_id": "2406.02180",
      "title": "On The Statistical Representation Properties Of The Perturb-Softmax And The Perturb-Argmax Probability Distributions",
      "summary": "The Gumbel-Softmax probability distribution allows learning discrete tokens in generative learning, while the Gumbel-Argmax probability distribution is useful in learning discrete structures in discriminative learning. Despite the efforts invested in optimizing these probability models, their statistical properties are under-explored. In this work, we investigate their representation properties and determine for which families of parameters these probability distributions are complete, i.e., can represent any probability distribution, and minimal, i.e., can represent a probability distribution uniquely. We rely on convexity and differentiability to determine these statistical conditions and extend this framework to general probability models, such as Gaussian-Softmax and Gaussian-Argmax. We experimentally validate the qualities of these extensions, which enjoy a faster convergence rate. We conclude the analysis by identifying two sets of parameters that satisfy these assumptions and thus admit a complete and minimal representation. Our contribution is theoretical with supporting practical evaluation.",
      "authors": [
        "Hedda Cohen Indelman",
        "Tamir Hazan"
      ],
      "published": "2024-06-04T10:22:12Z",
      "updated": "2024-06-04T10:22:12Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02180v1",
      "landing_url": "https://arxiv.org/abs/2406.02180v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02180"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article investigates statistical properties of perturb-softmax/perturb-argmax distributions without addressing Spoken Language Models, speech interaction, or any audio/speech-specific evaluation, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article investigates statistical properties of perturb-softmax/perturb-argmax distributions without addressing Spoken Language Models, speech interaction, or any audio/speech-specific evaluation, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on theoretical and experimental analysis of Perturb-Softmax and Perturb-Argmax probability distributions and their statistical properties in generative and discriminative learning models, without any mention or discussion of spoken language models, speech-based modeling, full-duplex interaction, or related speech/dialogue paradigms described in the inclusion criteria. It does not address speech as a core modeling object or evaluate synchronization or interactive speech behaviors.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on theoretical and experimental analysis of Perturb-Softmax and Perturb-Argmax probability distributions and their statistical properties in generative and discriminative learning models, without any mention or discussion of spoken language models, speech-based modeling, full-duplex interaction, or related speech/dialogue paradigms described in the inclusion criteria. It does not address speech as a core modeling object or evaluate synchronization or interactive speech behaviors.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Multi-Stage Speech Bandwidth Extension with Flexible Sampling Rate Control",
    "abstract": "The majority of existing speech bandwidth extension (BWE) methods operate under the constraint of fixed source and target sampling rates, which limits their flexibility in practical applications. In this paper, we propose a multi-stage speech BWE model named MS-BWE, which can handle a set of source and target sampling rate pairs and achieve flexible extensions of frequency bandwidth. The proposed MS-BWE model comprises a cascade of BWE blocks, with each block featuring a dual-stream architecture to realize amplitude and phase extension, progressively painting the speech frequency bands stage by stage. The teacher-forcing strategy is employed to mitigate the discrepancy between training and inference. Experimental results demonstrate that our proposed MS-BWE is comparable to state-of-the-art speech BWE methods in speech quality. Regarding generation efficiency, the one-stage generation of MS-BWE can achieve over one thousand times real-time on GPU and about sixty times on CPU.",
    "metadata": {
      "arxiv_id": "2406.02250",
      "title": "Multi-Stage Speech Bandwidth Extension with Flexible Sampling Rate Control",
      "summary": "The majority of existing speech bandwidth extension (BWE) methods operate under the constraint of fixed source and target sampling rates, which limits their flexibility in practical applications. In this paper, we propose a multi-stage speech BWE model named MS-BWE, which can handle a set of source and target sampling rate pairs and achieve flexible extensions of frequency bandwidth. The proposed MS-BWE model comprises a cascade of BWE blocks, with each block featuring a dual-stream architecture to realize amplitude and phase extension, progressively painting the speech frequency bands stage by stage. The teacher-forcing strategy is employed to mitigate the discrepancy between training and inference. Experimental results demonstrate that our proposed MS-BWE is comparable to state-of-the-art speech BWE methods in speech quality. Regarding generation efficiency, the one-stage generation of MS-BWE can achieve over one thousand times real-time on GPU and about sixty times on CPU.",
      "authors": [
        "Ye-Xin Lu",
        "Yang Ai",
        "Zheng-Yan Sheng",
        "Zhen-Hua Ling"
      ],
      "published": "2024-06-04T12:17:11Z",
      "updated": "2024-06-04T12:17:11Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02250v1",
      "landing_url": "https://arxiv.org/abs/2406.02250v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02250"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on speech bandwidth extension and sampling-rate-flexible waveform processing, not on Spoken Language Models or full-duplex speech interaction, so it does not meet the core SLM/FD-SLM criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on speech bandwidth extension and sampling-rate-flexible waveform processing, not on Spoken Language Models or full-duplex speech interaction, so it does not meet the core SLM/FD-SLM criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on speech bandwidth extension (BWE), which is more about speech signal enhancement and does not involve Spoken Language Models or full-duplex spoken interaction modeling. It does not mention any language modeling of speech or address synchronization, cognitive parallelism, or dialog interaction aspects. Its evaluation centers on speech quality and generation efficiency, without discussing real-time interaction, speech overlap management, or behavioral arbitration. Therefore, it does not meet the inclusion criteria centered on spoken language models or full-duplex interaction capabilities.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on speech bandwidth extension (BWE), which is more about speech signal enhancement and does not involve Spoken Language Models or full-duplex spoken interaction modeling. It does not mention any language modeling of speech or address synchronization, cognitive parallelism, or dialog interaction aspects. Its evaluation centers on speech quality and generation efficiency, without discussing real-time interaction, speech overlap management, or behavioral arbitration. Therefore, it does not meet the inclusion criteria centered on spoken language models or full-duplex interaction capabilities.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "An Independence-promoting Loss for Music Generation with Language Models",
    "abstract": "Music generation schemes using language modeling rely on a vocabulary of audio tokens, generally provided as codes in a discrete latent space learnt by an auto-encoder. Multi-stage quantizers are often employed to produce these tokens, therefore the decoding strategy used for token prediction must be adapted to account for multiple codebooks: either it should model the joint distribution over all codebooks, or fit the product of the codebook marginal distributions. Modelling the joint distribution requires a costly increase in the number of auto-regressive steps, while fitting the product of the marginals yields an inexact model unless the codebooks are mutually independent. In this work, we introduce an independence-promoting loss to regularize the auto-encoder used as the tokenizer in language models for music generation. The proposed loss is a proxy for mutual information based on the maximum mean discrepancy principle, applied in reproducible kernel Hilbert spaces. Our criterion is simple to implement and train, and it is generalizable to other multi-stream codecs. We show that it reduces the statistical dependence between codebooks during auto-encoding. This leads to an increase in the generated music quality when modelling the product of the marginal distributions, while generating audio much faster than the joint distribution model.",
    "metadata": {
      "arxiv_id": "2406.02315",
      "title": "An Independence-promoting Loss for Music Generation with Language Models",
      "summary": "Music generation schemes using language modeling rely on a vocabulary of audio tokens, generally provided as codes in a discrete latent space learnt by an auto-encoder. Multi-stage quantizers are often employed to produce these tokens, therefore the decoding strategy used for token prediction must be adapted to account for multiple codebooks: either it should model the joint distribution over all codebooks, or fit the product of the codebook marginal distributions. Modelling the joint distribution requires a costly increase in the number of auto-regressive steps, while fitting the product of the marginals yields an inexact model unless the codebooks are mutually independent. In this work, we introduce an independence-promoting loss to regularize the auto-encoder used as the tokenizer in language models for music generation. The proposed loss is a proxy for mutual information based on the maximum mean discrepancy principle, applied in reproducible kernel Hilbert spaces. Our criterion is simple to implement and train, and it is generalizable to other multi-stream codecs. We show that it reduces the statistical dependence between codebooks during auto-encoding. This leads to an increase in the generated music quality when modelling the product of the marginal distributions, while generating audio much faster than the joint distribution model.",
      "authors": [
        "Jean-Marie Lemercier",
        "Simon Rouard",
        "Jade Copet",
        "Yossi Adi",
        "Alexandre Défossez"
      ],
      "published": "2024-06-04T13:44:39Z",
      "updated": "2024-06-09T17:55:51Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02315v2",
      "landing_url": "https://arxiv.org/abs/2406.02315v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.02315"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on music generation with language models and codebook independence, not on Spoken Language Models or speech-language modeling with interactive dialogue, thus failing the core inclusion criteria and aligning with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on music generation with language models and codebook independence, not on Spoken Language Models or speech-language modeling with interactive dialogue, thus failing the core inclusion criteria and aligning with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on music generation using language models with audio tokens from auto-encoders, not on spoken language models or speech language modeling as core topics. It does not address end-to-end spoken language models, full-duplex spoken interaction, or related synchronization, cognitive parallelism, or real-time interaction issues that are key in the inclusion criteria. The abstract also lacks mention of spoken language modeling, speech token LM, or related terminology required for inclusion, and centers around discrete audio token modeling for music rather than speech. Therefore, it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on music generation using language models with audio tokens from auto-encoders, not on spoken language models or speech language modeling as core topics. It does not address end-to-end spoken language models, full-duplex spoken interaction, or related synchronization, cognitive parallelism, or real-time interaction issues that are key in the inclusion criteria. The abstract also lacks mention of spoken language modeling, speech token LM, or related terminology required for inclusion, and centers around discrete audio token modeling for music rather than speech. Therefore, it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Self-Supervised Singing Voice Pre-Training towards Speech-to-Singing Conversion",
    "abstract": "Speech-to-singing voice conversion (STS) task always suffers from data scarcity, because it requires paired speech and singing data. Compounding this issue are the challenges of content-pitch alignment and the suboptimal quality of generated outputs, presenting significant hurdles in STS research. This paper presents SVPT, an STS approach boosted by a self-supervised singing voice pre-training model. We leverage spoken language model techniques to tackle the rhythm alignment problem and the in-context learning capability to achieve zero-shot conversion. We adopt discrete-unit random resampling and pitch corruption strategies, enabling training with unpaired singing data and thus mitigating the issue of data scarcity. SVPT also serves as an effective backbone for singing voice synthesis (SVS), offering insights into scaling up SVS models. Experimental results indicate that SVPT delivers notable improvements in both STS and SVS endeavors. Audio samples are available at https://speech2sing.github.io.",
    "metadata": {
      "arxiv_id": "2406.02429",
      "title": "Self-Supervised Singing Voice Pre-Training towards Speech-to-Singing Conversion",
      "summary": "Speech-to-singing voice conversion (STS) task always suffers from data scarcity, because it requires paired speech and singing data. Compounding this issue are the challenges of content-pitch alignment and the suboptimal quality of generated outputs, presenting significant hurdles in STS research. This paper presents SVPT, an STS approach boosted by a self-supervised singing voice pre-training model. We leverage spoken language model techniques to tackle the rhythm alignment problem and the in-context learning capability to achieve zero-shot conversion. We adopt discrete-unit random resampling and pitch corruption strategies, enabling training with unpaired singing data and thus mitigating the issue of data scarcity. SVPT also serves as an effective backbone for singing voice synthesis (SVS), offering insights into scaling up SVS models. Experimental results indicate that SVPT delivers notable improvements in both STS and SVS endeavors. Audio samples are available at https://speech2sing.github.io.",
      "authors": [
        "Ruiqi Li",
        "Rongjie Huang",
        "Yongqi Wang",
        "Zhiqing Hong",
        "Zhou Zhao"
      ],
      "published": "2024-06-04T15:47:59Z",
      "updated": "2024-06-04T15:47:59Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02429v1",
      "landing_url": "https://arxiv.org/abs/2406.02429v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02429"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly uses spoken language model techniques for speech-related tasks within a speech-to-singing framework, discusses model architecture/training/evaluation, and is from 2025 (within the last 3 years), meeting inclusion criteria and not falling under the stated exclusions.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly uses spoken language model techniques for speech-related tasks within a speech-to-singing framework, discusses model architecture/training/evaluation, and is from 2025 (within the last 3 years), meeting inclusion criteria and not falling under the stated exclusions.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on speech-to-singing conversion using a self-supervised singing voice pre-training model, which addresses a specific speech-related task but does not explicitly discuss spoken language models, speech language models, full-duplex interaction, behavioral arbitration, or the modeling and reasoning of speech sequences as required by the inclusion criteria; the core is voice conversion rather than spoken language modeling or real-time interactive spoken language processing, thus it does not meet the critical inclusion requirements and aligns more with conventional speech synthesis/conversion tasks without addressing the synchronization, low-latency, or dialogue interaction aspects.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on speech-to-singing conversion using a self-supervised singing voice pre-training model, which addresses a specific speech-related task but does not explicitly discuss spoken language models, speech language models, full-duplex interaction, behavioral arbitration, or the modeling and reasoning of speech sequences as required by the inclusion criteria; the core is voice conversion rather than spoken language modeling or real-time interactive spoken language processing, thus it does not meet the critical inclusion requirements and aligns more with conventional speech synthesis/conversion tasks without addressing the synchronization, low-latency, or dialogue interaction aspects.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers raised valid but opposing points: JuniorNano recommended inclusion (4) because the abstract explicitly mentions leveraging “spoken language model techniques” and discusses self-supervised pre‑training, discrete-unit strategies, and experimental evaluation; JuniorMini recommended exclusion (2) arguing the paper is primarily an application (speech-to-singing / SVS) rather than a work that centers Spoken Language Models or addresses full‑duplex interactive capabilities. I agree more with the latter: although the abstract name‑checks “spoken language model techniques” and describes speech‑centric modeling/training tricks (discrete units, pretraining, pitch corruption) and evaluation for STS/SVS, the core contribution and evaluation target is a specific downstream task (speech→singing conversion and singing synthesis) rather than advancing Spoken Language Models (SLMs) as the central research theme or addressing FD‑SLM concerns such as synchronous full‑duplex interaction, overlap/interrupt handling, low‑latency turn arbitration, or broader speech–LLM architectures and benchmarks. Therefore it does not meet the inclusion requirement that SLMs (or equivalent) be the clear central focus, nor does it cover the full‑duplex / interaction/synchronization evaluation axes required by this topic.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers raised valid but opposing points: JuniorNano recommended inclusion (4) because the abstract explicitly mentions leveraging “spoken language model techniques” and discusses self-supervised pre‑training, discrete-unit strategies, and experimental evaluation; JuniorMini recommended exclusion (2) arguing the paper is primarily an application (speech-to-singing / SVS) rather than a work that centers Spoken Language Models or addresses full‑duplex interactive capabilities. I agree more with the latter: although the abstract name‑checks “spoken language model techniques” and describes speech‑centric modeling/training tricks (discrete units, pretraining, pitch corruption) and evaluation for STS/SVS, the core contribution and evaluation target is a specific downstream task (speech→singing conversion and singing synthesis) rather than advancing Spoken Language Models (SLMs) as the central research theme or addressing FD‑SLM concerns such as synchronous full‑duplex interaction, overlap/interrupt handling, low‑latency turn arbitration, or broader speech–LLM architectures and benchmarks. Therefore it does not meet the inclusion requirement that SLMs (or equivalent) be the clear central focus, nor does it cover the full‑duplex / interaction/synchronization evaluation axes required by this topic.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LiveSpeech: Low-Latency Zero-shot Text-to-Speech via Autoregressive Modeling of Audio Discrete Codes",
    "abstract": "Prior works have demonstrated zero-shot text-to-speech by using a generative language model on audio tokens obtained via a neural audio codec. It is still challenging, however, to adapt them to low-latency scenarios. In this paper, we present LiveSpeech - a fully autoregressive language model-based approach for zero-shot text-to-speech, enabling low-latency streaming of the output audio. To allow multiple token prediction within a single decoding step, we propose (1) using adaptive codebook loss weights that consider codebook contribution in each frame and focus on hard instances, and (2) grouping codebooks and processing groups in parallel. Experiments show our proposed models achieve competitive results to state-of-the-art baselines in terms of content accuracy, speaker similarity, audio quality, and inference speed while being suitable for low-latency streaming applications.",
    "metadata": {
      "arxiv_id": "2406.02897",
      "title": "LiveSpeech: Low-Latency Zero-shot Text-to-Speech via Autoregressive Modeling of Audio Discrete Codes",
      "summary": "Prior works have demonstrated zero-shot text-to-speech by using a generative language model on audio tokens obtained via a neural audio codec. It is still challenging, however, to adapt them to low-latency scenarios. In this paper, we present LiveSpeech - a fully autoregressive language model-based approach for zero-shot text-to-speech, enabling low-latency streaming of the output audio. To allow multiple token prediction within a single decoding step, we propose (1) using adaptive codebook loss weights that consider codebook contribution in each frame and focus on hard instances, and (2) grouping codebooks and processing groups in parallel. Experiments show our proposed models achieve competitive results to state-of-the-art baselines in terms of content accuracy, speaker similarity, audio quality, and inference speed while being suitable for low-latency streaming applications.",
      "authors": [
        "Trung Dang",
        "David Aponte",
        "Dung Tran",
        "Kazuhito Koishida"
      ],
      "published": "2024-06-05T03:36:11Z",
      "updated": "2024-06-10T05:50:53Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02897v2",
      "landing_url": "https://arxiv.org/abs/2406.02897v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.02897"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "It clearly centers on speech as the modeling target (audio-token LM for TTS) with architecture/training/evaluation, and is within 3 years, fitting the Spoken Language Model scope.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "It clearly centers on speech as the modeling target (audio-token LM for TTS) with architecture/training/evaluation, and is within 3 years, fitting the Spoken Language Model scope.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on zero-shot text-to-speech via autoregressive modeling of audio discrete codes, emphasizing low-latency streaming suitable for real-time applications, which relates to low-latency spoken language generation. However, the abstract does not explicitly state the use of Spoken Language Models or equivalent language modeling of speech, nor does it discuss full-duplex spoken language models, cognitive parallelism, or behavior arbitration in conversational interaction as required by inclusion criteria. It primarily addresses speech synthesis (TTS) technology with evaluation metrics such as audio quality and inference speed, lacking explicit discussion of interactive dialogue behaviors or synchronization, leading to a partial mismatch with the inclusion and exclusion rules.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on zero-shot text-to-speech via autoregressive modeling of audio discrete codes, emphasizing low-latency streaming suitable for real-time applications, which relates to low-latency spoken language generation. However, the abstract does not explicitly state the use of Spoken Language Models or equivalent language modeling of speech, nor does it discuss full-duplex spoken language models, cognitive parallelism, or behavior arbitration in conversational interaction as required by inclusion criteria. It primarily addresses speech synthesis (TTS) technology with evaluation metrics such as audio quality and inference speed, lacking explicit discussion of interactive dialogue behaviors or synchronization, leading to a partial mismatch with the inclusion and exclusion rules.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted relevant points: JuniorNano highlighted that the work centers on language modeling of audio tokens and low-latency streaming (supporting inclusion), while JuniorMini pointed out it is primarily a TTS paper without explicit SLM/FD-SLM interactive behaviors. Although the abstract does describe \"autoregressive language model\" on audio discrete codes (i.e., language modeling of speech tokens), the study focuses on single-direction zero-shot TTS and does not address full-duplex interaction, overlap/interrupt handling, or behavior arbitration required by the inclusion criteria—thus it meets exclusion criteria for being a TTS-only system rather than an SLM/FD-SLM interactive study. Score explanation: better to exclude because it is a TTS-focused, single-direction work lacking the interactive spoken-language modeling aspects required.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted relevant points: JuniorNano highlighted that the work centers on language modeling of audio tokens and low-latency streaming (supporting inclusion), while JuniorMini pointed out it is primarily a TTS paper without explicit SLM/FD-SLM interactive behaviors. Although the abstract does describe \"autoregressive language model\" on audio discrete codes (i.e., language modeling of speech tokens), the study focuses on single-direction zero-shot TTS and does not address full-duplex interaction, overlap/interrupt handling, or behavior arbitration required by the inclusion criteria—thus it meets exclusion criteria for being a TTS-only system rather than an SLM/FD-SLM interactive study. Score explanation: better to exclude because it is a TTS-focused, single-direction work lacking the interactive spoken-language modeling aspects required.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Confabulation: The Surprising Value of Large Language Model Hallucinations",
    "abstract": "This paper presents a systematic defense of large language model (LLM) hallucinations or 'confabulations' as a potential resource instead of a categorically negative pitfall. The standard view is that confabulations are inherently problematic and AI research should eliminate this flaw. In this paper, we argue and empirically demonstrate that measurable semantic characteristics of LLM confabulations mirror a human propensity to utilize increased narrativity as a cognitive resource for sense-making and communication. In other words, it has potential value. Specifically, we analyze popular hallucination benchmarks and reveal that hallucinated outputs display increased levels of narrativity and semantic coherence relative to veridical outputs. This finding reveals a tension in our usually dismissive understandings of confabulation. It suggests, counter-intuitively, that the tendency for LLMs to confabulate may be intimately associated with a positive capacity for coherent narrative-text generation.",
    "metadata": {
      "arxiv_id": "2406.04175",
      "title": "Confabulation: The Surprising Value of Large Language Model Hallucinations",
      "summary": "This paper presents a systematic defense of large language model (LLM) hallucinations or 'confabulations' as a potential resource instead of a categorically negative pitfall. The standard view is that confabulations are inherently problematic and AI research should eliminate this flaw. In this paper, we argue and empirically demonstrate that measurable semantic characteristics of LLM confabulations mirror a human propensity to utilize increased narrativity as a cognitive resource for sense-making and communication. In other words, it has potential value. Specifically, we analyze popular hallucination benchmarks and reveal that hallucinated outputs display increased levels of narrativity and semantic coherence relative to veridical outputs. This finding reveals a tension in our usually dismissive understandings of confabulation. It suggests, counter-intuitively, that the tendency for LLMs to confabulate may be intimately associated with a positive capacity for coherent narrative-text generation.",
      "authors": [
        "Peiqi Sui",
        "Eamon Duede",
        "Sophie Wu",
        "Richard Jean So"
      ],
      "published": "2024-06-06T15:32:29Z",
      "updated": "2024-06-25T18:37:19Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04175v2",
      "landing_url": "https://arxiv.org/abs/2406.04175v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.04175"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses LLM hallucinations in text generation and does not focus on Spoken Language Models or speech-language modeling, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses LLM hallucinations in text generation and does not focus on Spoken Language Models or speech-language modeling, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses large language model hallucinations in terms of semantic narrativity and coherence but does not address spoken language models, speech as a primary modeling object, or related aspects such as full-duplex interaction or speech-LLM architectures; it also lacks mention of spoken language modeling or synchronous speech interaction features required by the inclusion criteria and does not meet the specified scope centered on speech modeling and evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses large language model hallucinations in terms of semantic narrativity and coherence but does not address spoken language models, speech as a primary modeling object, or related aspects such as full-duplex interaction or speech-LLM architectures; it also lacks mention of spoken language modeling or synchronous speech interaction features required by the inclusion criteria and does not meet the specified scope centered on speech modeling and evaluation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Neural Codec-based Adversarial Sample Detection for Speaker Verification",
    "abstract": "Automatic Speaker Verification (ASV), increasingly used in security-critical applications, faces vulnerabilities from rising adversarial attacks, with few effective defenses available. In this paper, we propose a neural codec-based adversarial sample detection method for ASV. The approach leverages the codec's ability to discard redundant perturbations and retain essential information. Specifically, we distinguish between genuine and adversarial samples by comparing ASV score differences between original and re-synthesized audio (by codec models). This comprehensive study explores all open-source neural codecs and their variant models for experiments. The Descript-audio-codec model stands out by delivering the highest detection rate among 15 neural codecs and surpassing seven prior state-of-the-art (SOTA) detection methods. Note that, our single-model method even outperforms a SOTA ensemble method by a large margin.",
    "metadata": {
      "arxiv_id": "2406.04582",
      "title": "Neural Codec-based Adversarial Sample Detection for Speaker Verification",
      "summary": "Automatic Speaker Verification (ASV), increasingly used in security-critical applications, faces vulnerabilities from rising adversarial attacks, with few effective defenses available. In this paper, we propose a neural codec-based adversarial sample detection method for ASV. The approach leverages the codec's ability to discard redundant perturbations and retain essential information. Specifically, we distinguish between genuine and adversarial samples by comparing ASV score differences between original and re-synthesized audio (by codec models). This comprehensive study explores all open-source neural codecs and their variant models for experiments. The Descript-audio-codec model stands out by delivering the highest detection rate among 15 neural codecs and surpassing seven prior state-of-the-art (SOTA) detection methods. Note that, our single-model method even outperforms a SOTA ensemble method by a large margin.",
      "authors": [
        "Xuanjun Chen",
        "Jiawei Du",
        "Haibin Wu",
        "Jyh-Shing Roger Jang",
        "Hung-yi Lee"
      ],
      "published": "2024-06-07T02:03:27Z",
      "updated": "2024-06-07T02:03:27Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04582v1",
      "landing_url": "https://arxiv.org/abs/2406.04582v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.04582"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper targets adversarial detection for speaker verification using neural codecs and does not center on Spoken Language Models (SLMs) or full-duplex speech-language modeling; it lacks SLM-centric modeling/training/evaluation and reads as a single-task ASV defense, meeting exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper targets adversarial detection for speaker verification using neural codecs and does not center on Spoken Language Models (SLMs) or full-duplex speech-language modeling; it lacks SLM-centric modeling/training/evaluation and reads as a single-task ASV defense, meeting exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on adversarial sample detection for Automatic Speaker Verification (ASV) using neural codecs, and does not discuss Spoken Language Models or full-duplex spoken language interactions; it lacks focus on language modeling of speech or related model architectures and evaluation from the viewpoint of speech–language modeling or dialogue behavior arbitration, thus not meeting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on adversarial sample detection for Automatic Speaker Verification (ASV) using neural codecs, and does not discuss Spoken Language Models or full-duplex spoken language interactions; it lacks focus on language modeling of speech or related model architectures and evaluation from the viewpoint of speech–language modeling or dialogue behavior arbitration, thus not meeting the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Graph Mining under Data scarcity",
    "abstract": "Multitude of deep learning models have been proposed for node classification in graphs. However, they tend to perform poorly under labeled-data scarcity. Although Few-shot learning for graphs has been introduced to overcome this problem, the existing models are not easily adaptable for generic graph learning frameworks like Graph Neural Networks (GNNs). Our work proposes an Uncertainty Estimator framework that can be applied on top of any generic GNN backbone network (which are typically designed for supervised/semi-supervised node classification) to improve the node classification performance. A neural network is used to model the Uncertainty Estimator as a probability distribution rather than probabilistic discrete scalar values. We train these models under the classic episodic learning paradigm in the $n$-way, $k$-shot fashion, in an end-to-end setting. Our work demonstrates that implementation of the uncertainty estimator on a GNN backbone network improves the classification accuracy under Few-shot setting without any meta-learning specific architecture. We conduct experiments on multiple datasets under different Few-shot settings and different GNN-based backbone networks. Our method outperforms the baselines, which demonstrates the efficacy of the Uncertainty Estimator for Few-shot node classification on graphs with a GNN.",
    "metadata": {
      "arxiv_id": "2406.04825",
      "title": "Graph Mining under Data scarcity",
      "summary": "Multitude of deep learning models have been proposed for node classification in graphs. However, they tend to perform poorly under labeled-data scarcity. Although Few-shot learning for graphs has been introduced to overcome this problem, the existing models are not easily adaptable for generic graph learning frameworks like Graph Neural Networks (GNNs). Our work proposes an Uncertainty Estimator framework that can be applied on top of any generic GNN backbone network (which are typically designed for supervised/semi-supervised node classification) to improve the node classification performance. A neural network is used to model the Uncertainty Estimator as a probability distribution rather than probabilistic discrete scalar values. We train these models under the classic episodic learning paradigm in the $n$-way, $k$-shot fashion, in an end-to-end setting. Our work demonstrates that implementation of the uncertainty estimator on a GNN backbone network improves the classification accuracy under Few-shot setting without any meta-learning specific architecture. We conduct experiments on multiple datasets under different Few-shot settings and different GNN-based backbone networks. Our method outperforms the baselines, which demonstrates the efficacy of the Uncertainty Estimator for Few-shot node classification on graphs with a GNN.",
      "authors": [
        "Appan Rakaraddi",
        "Lam Siew-Kei",
        "Mahardhika Pratama",
        "Marcus de Carvalho"
      ],
      "published": "2024-06-07T10:50:03Z",
      "updated": "2024-06-11T13:33:16Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04825v2",
      "landing_url": "https://arxiv.org/abs/2406.04825v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.04825"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on graph neural networks and few-shot node classification, with no emphasis on Spoken Language Models, speech modeling, or conversational interaction, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on graph neural networks and few-shot node classification, with no emphasis on Spoken Language Models, speech modeling, or conversational interaction, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The research focuses on graph mining and few-shot node classification using Graph Neural Networks, which is unrelated to the core topic of Spoken Language Models or full-duplex speech interaction as defined by the inclusion criteria. Additionally, it neither involves speech modeling, speech-based architectures, nor discusses any relevant evaluation for spoken language interaction or synchronization, so it does not meet the subject definition.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The research focuses on graph mining and few-shot node classification using Graph Neural Networks, which is unrelated to the core topic of Spoken Language Models or full-duplex speech interaction as defined by the inclusion criteria. Additionally, it neither involves speech modeling, speech-based architectures, nor discusses any relevant evaluation for spoken language interaction or synchronization, so it does not meet the subject definition.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Spectral Codecs: Improving Non-Autoregressive Speech Synthesis with Spectrogram-Based Audio Codecs",
    "abstract": "Historically, most speech models in machine-learning have used the mel-spectrogram as a speech representation. Recently, discrete audio tokens produced by neural audio codecs have become a popular alternate speech representation for speech synthesis tasks such as text-to-speech (TTS). However, the data distribution produced by such codecs is too complex for some TTS models to predict, typically requiring large autoregressive models to get good quality. Most existing audio codecs use Residual Vector Quantization (RVQ) to compress and reconstruct the time-domain audio signal. We propose a spectral codec which uses Finite Scalar Quantization (FSQ) to compress the mel-spectrogram and reconstruct the time-domain audio signal. A study of objective audio quality metrics and subjective listening tests suggests that our spectral codec has comparable perceptual quality to equivalent audio codecs. We show that FSQ, and the use of spectral speech representations, can both improve the performance of parallel TTS models.",
    "metadata": {
      "arxiv_id": "2406.05298",
      "title": "Spectral Codecs: Improving Non-Autoregressive Speech Synthesis with Spectrogram-Based Audio Codecs",
      "summary": "Historically, most speech models in machine-learning have used the mel-spectrogram as a speech representation. Recently, discrete audio tokens produced by neural audio codecs have become a popular alternate speech representation for speech synthesis tasks such as text-to-speech (TTS). However, the data distribution produced by such codecs is too complex for some TTS models to predict, typically requiring large autoregressive models to get good quality. Most existing audio codecs use Residual Vector Quantization (RVQ) to compress and reconstruct the time-domain audio signal. We propose a spectral codec which uses Finite Scalar Quantization (FSQ) to compress the mel-spectrogram and reconstruct the time-domain audio signal. A study of objective audio quality metrics and subjective listening tests suggests that our spectral codec has comparable perceptual quality to equivalent audio codecs. We show that FSQ, and the use of spectral speech representations, can both improve the performance of parallel TTS models.",
      "authors": [
        "Ryan Langman",
        "Ante Jukić",
        "Kunal Dhawan",
        "Nithin Rao Koluguri",
        "Jason Li"
      ],
      "published": "2024-06-07T23:47:51Z",
      "updated": "2025-06-04T16:25:54Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05298v2",
      "landing_url": "https://arxiv.org/abs/2406.05298v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.05298"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the paper discusses speech synthesis modeling, it centers on spectrogram-based audio codecs and TTS rather than explicitly treating Spoken Language Models or full-duplex interactive speech modeling; it lacks the interactive speech-language modeling framing and evaluation emphasized by the criteria, so it should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the paper discusses speech synthesis modeling, it centers on spectrogram-based audio codecs and TTS rather than explicitly treating Spoken Language Models or full-duplex interactive speech modeling; it lacks the interactive speech-language modeling framing and evaluation emphasized by the criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving speech synthesis quality using spectral codecs but does not mention spoken language models, full-duplex interaction, cognitive parallelism, or simultaneous speech interaction, nor does it discuss end-to-end or hybrid SLM architectures or behavioral arbitration. It primarily centers on TTS quality improvement via spectral representations without addressing the core modeling or reasoning of speech sequences fundamental to spoken language models as defined in the inclusion criteria. Therefore, it fails to meet the main thematic inclusion requirements and aligns more with single-task speech technology research rather than a comprehensive spoken language model framework with interaction capabilities.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving speech synthesis quality using spectral codecs but does not mention spoken language models, full-duplex interaction, cognitive parallelism, or simultaneous speech interaction, nor does it discuss end-to-end or hybrid SLM architectures or behavioral arbitration. It primarily centers on TTS quality improvement via spectral representations without addressing the core modeling or reasoning of speech sequences fundamental to spoken language models as defined in the inclusion criteria. Therefore, it fails to meet the main thematic inclusion requirements and aligns more with single-task speech technology research rather than a comprehensive spoken language model framework with interaction capabilities.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers",
    "abstract": "This paper introduces VALL-E 2, the latest advancement in neural codec language models that marks a milestone in zero-shot text-to-speech synthesis (TTS), achieving human parity for the first time. Based on its predecessor, VALL-E, the new iteration introduces two significant enhancements: Repetition Aware Sampling refines the original nucleus sampling process by accounting for token repetition in the decoding history. It not only stabilizes the decoding but also circumvents the infinite loop issue. Grouped Code Modeling organizes codec codes into groups to effectively shorten the sequence length, which not only boosts inference speed but also addresses the challenges of long sequence modeling. Our experiments on the LibriSpeech and VCTK datasets show that VALL-E 2 surpasses previous systems in speech robustness, naturalness, and speaker similarity. It is the first of its kind to reach human parity on these benchmarks. Moreover, VALL-E 2 consistently synthesizes high-quality speech, even for sentences that are traditionally challenging due to their complexity or repetitive phrases. The advantages of this work could contribute to valuable endeavors, such as generating speech for individuals with aphasia or people with amyotrophic lateral sclerosis. See https://aka.ms/valle2 for demos of VALL-E 2.",
    "metadata": {
      "arxiv_id": "2406.05370",
      "title": "VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers",
      "summary": "This paper introduces VALL-E 2, the latest advancement in neural codec language models that marks a milestone in zero-shot text-to-speech synthesis (TTS), achieving human parity for the first time. Based on its predecessor, VALL-E, the new iteration introduces two significant enhancements: Repetition Aware Sampling refines the original nucleus sampling process by accounting for token repetition in the decoding history. It not only stabilizes the decoding but also circumvents the infinite loop issue. Grouped Code Modeling organizes codec codes into groups to effectively shorten the sequence length, which not only boosts inference speed but also addresses the challenges of long sequence modeling. Our experiments on the LibriSpeech and VCTK datasets show that VALL-E 2 surpasses previous systems in speech robustness, naturalness, and speaker similarity. It is the first of its kind to reach human parity on these benchmarks. Moreover, VALL-E 2 consistently synthesizes high-quality speech, even for sentences that are traditionally challenging due to their complexity or repetitive phrases. The advantages of this work could contribute to valuable endeavors, such as generating speech for individuals with aphasia or people with amyotrophic lateral sclerosis. See https://aka.ms/valle2 for demos of VALL-E 2.",
      "authors": [
        "Sanyuan Chen",
        "Shujie Liu",
        "Long Zhou",
        "Yanqing Liu",
        "Xu Tan",
        "Jinyu Li",
        "Sheng Zhao",
        "Yao Qian",
        "Furu Wei"
      ],
      "published": "2024-06-08T06:31:03Z",
      "updated": "2024-06-17T04:39:08Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05370v2",
      "landing_url": "https://arxiv.org/abs/2406.05370v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.05370"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on speech synthesis using neural codec language models and presents architectural innovations with training/evaluation on standard speech datasets, aligning with spoken-language-modeling of speech rather than treating speech solely as input; it’s within the 3-year window, though it doesn’t discuss interactive speech-duplex dialogue.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item focuses on speech synthesis using neural codec language models and presents architectural innovations with training/evaluation on standard speech datasets, aligning with spoken-language-modeling of speech rather than treating speech solely as input; it’s within the 3-year window, though it doesn’t discuss interactive speech-duplex dialogue.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on zero-shot text-to-speech synthesis (TTS), enhancing speech synthesis quality and robustness, but does not discuss Spoken Language Models or speech language modeling from a modeling, training, or evaluation perspective, nor does it address full-duplex spoken language models, synchronous interaction, or dialog arbitration. It primarily targets TTS performance and does not include metrics or frameworks related to real-time interaction or overlapping speech, which are required. Thus, it does not meet the inclusion criteria related to spoken language models and synchronous interaction, but aligns with exclusion criteria regarding focus on single downstream tasks (TTS) without broader SLM context.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on zero-shot text-to-speech synthesis (TTS), enhancing speech synthesis quality and robustness, but does not discuss Spoken Language Models or speech language modeling from a modeling, training, or evaluation perspective, nor does it address full-duplex spoken language models, synchronous interaction, or dialog arbitration. It primarily targets TTS performance and does not include metrics or frameworks related to real-time interaction or overlapping speech, which are required. Thus, it does not meet the inclusion criteria related to spoken language models and synchronous interaction, but aligns with exclusion criteria regarding focus on single downstream tasks (TTS) without broader SLM context.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers examined the abstract: JuniorNano rated it 4 believing it aligned with spoken-language modeling because it focuses on speech synthesis and codec language models; JuniorMini rated it 1 pointing out it is a TTS-focused work that does not address Spoken Language Models, full‑duplex interaction, or the required speech-language modeling perspectives. My assessment: although VALL‑E 2 advances zero‑shot TTS and codec modeling, the title/abstract does not use “spoken language model(s)” or equivalent, does not treat speech as the core language‑modeling/reasoning object in the sense required by the inclusion criteria, and it focuses on a single downstream task (TTS) without discussion of full‑duplex interaction, synchronization, overlap handling, or multi‑turn/behavioral arbitration—thus it meets the exclusion criteria. 1-sentence explanation: Exclude because the paper is a TTS/codec modeling advancement and does not meet the SLM/FD‑SLM framing, objectives, or evaluation requirements in the inclusion criteria.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers examined the abstract: JuniorNano rated it 4 believing it aligned with spoken-language modeling because it focuses on speech synthesis and codec language models; JuniorMini rated it 1 pointing out it is a TTS-focused work that does not address Spoken Language Models, full‑duplex interaction, or the required speech-language modeling perspectives. My assessment: although VALL‑E 2 advances zero‑shot TTS and codec modeling, the title/abstract does not use “spoken language model(s)” or equivalent, does not treat speech as the core language‑modeling/reasoning object in the sense required by the inclusion criteria, and it focuses on a single downstream task (TTS) without discussion of full‑duplex interaction, synchronization, overlap handling, or multi‑turn/behavioral arbitration—thus it meets the exclusion criteria. 1-sentence explanation: Exclude because the paper is a TTS/codec modeling advancement and does not meet the SLM/FD‑SLM framing, objectives, or evaluation requirements in the inclusion criteria.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Autoregressive Diffusion Transformer for Text-to-Speech Synthesis",
    "abstract": "Audio language models have recently emerged as a promising approach for various audio generation tasks, relying on audio tokenizers to encode waveforms into sequences of discrete symbols. Audio tokenization often poses a necessary compromise between code bitrate and reconstruction accuracy. When dealing with low-bitrate audio codes, language models are constrained to process only a subset of the information embedded in the audio, which in turn restricts their generative capabilities. To circumvent these issues, we propose encoding audio as vector sequences in continuous space $\\mathbb R^d$ and autoregressively generating these sequences using a decoder-only diffusion transformer (ARDiT). Our findings indicate that ARDiT excels in zero-shot text-to-speech and exhibits performance that compares to or even surpasses that of state-of-the-art models. High-bitrate continuous speech representation enables almost flawless reconstruction, allowing our model to achieve nearly perfect speech editing. Our experiments reveal that employing Integral Kullback-Leibler (IKL) divergence for distillation at each autoregressive step significantly boosts the perceived quality of the samples. Simultaneously, it condenses the iterative sampling process of the diffusion model into a single step. Furthermore, ARDiT can be trained to predict several continuous vectors in one step, significantly reducing latency during sampling. Impressively, one of our models can generate $170$ ms of $24$ kHz speech per evaluation step with minimal degradation in performance. Audio samples are available at http://ardit-tts.github.io/ .",
    "metadata": {
      "arxiv_id": "2406.05551",
      "title": "Autoregressive Diffusion Transformer for Text-to-Speech Synthesis",
      "summary": "Audio language models have recently emerged as a promising approach for various audio generation tasks, relying on audio tokenizers to encode waveforms into sequences of discrete symbols. Audio tokenization often poses a necessary compromise between code bitrate and reconstruction accuracy. When dealing with low-bitrate audio codes, language models are constrained to process only a subset of the information embedded in the audio, which in turn restricts their generative capabilities. To circumvent these issues, we propose encoding audio as vector sequences in continuous space $\\mathbb R^d$ and autoregressively generating these sequences using a decoder-only diffusion transformer (ARDiT). Our findings indicate that ARDiT excels in zero-shot text-to-speech and exhibits performance that compares to or even surpasses that of state-of-the-art models. High-bitrate continuous speech representation enables almost flawless reconstruction, allowing our model to achieve nearly perfect speech editing. Our experiments reveal that employing Integral Kullback-Leibler (IKL) divergence for distillation at each autoregressive step significantly boosts the perceived quality of the samples. Simultaneously, it condenses the iterative sampling process of the diffusion model into a single step. Furthermore, ARDiT can be trained to predict several continuous vectors in one step, significantly reducing latency during sampling. Impressively, one of our models can generate $170$ ms of $24$ kHz speech per evaluation step with minimal degradation in performance. Audio samples are available at http://ardit-tts.github.io/ .",
      "authors": [
        "Zhijun Liu",
        "Shuai Wang",
        "Sho Inoue",
        "Qibing Bai",
        "Haizhou Li"
      ],
      "published": "2024-06-08T18:57:13Z",
      "updated": "2024-06-08T18:57:13Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05551v1",
      "landing_url": "https://arxiv.org/abs/2406.05551v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.05551"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on a diffusion-based autoregressive model for text-to-speech, i.e., speech-synthesis modeling with architecture/training/evaluation discussions, fitting the Spoken/Speech Language Modeling domain and within the last 3 years, even though the abstract does not explicitly brand it as a 'speech language model' in phrasing.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on a diffusion-based autoregressive model for text-to-speech, i.e., speech-synthesis modeling with architecture/training/evaluation discussions, fitting the Spoken/Speech Language Modeling domain and within the last 3 years, even though the abstract does not explicitly brand it as a 'speech language model' in phrasing.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on an autoregressive diffusion transformer for text-to-speech synthesis, emphasizing audio modeling in continuous vector space. However, it primarily addresses TTS with no indication of spoken language models or synchronous full-duplex conversational modeling, nor does it mention language modeling of speech or interactive dialogue capabilities like cognitive parallelism or turn-taking. The evaluation metrics focus on speech quality and reconstruction accuracy, without discussing real-time interaction or multi-turn dialogue behavior arbitration. Thus, it does not fully meet the inclusion criteria centered on spoken language models or full-duplex interaction nor avoids the exclusion criteria related to single downstream speech tasks and lack of interactive dialogue modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on an autoregressive diffusion transformer for text-to-speech synthesis, emphasizing audio modeling in continuous vector space. However, it primarily addresses TTS with no indication of spoken language models or synchronous full-duplex conversational modeling, nor does it mention language modeling of speech or interactive dialogue capabilities like cognitive parallelism or turn-taking. The evaluation metrics focus on speech quality and reconstruction accuracy, without discussing real-time interaction or multi-turn dialogue behavior arbitration. Thus, it does not fully meet the inclusion criteria centered on spoken language models or full-duplex interaction nor avoids the exclusion criteria related to single downstream speech tasks and lack of interactive dialogue modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "I considered the two junior reviewers' split opinions (4 vs 2) and conclude that although the paper uses \"audio language models\" and continuous autoregressive generation, it is primarily a TTS/speech-editing system focused on synthesis quality and sampling efficiency without addressing spoken-language-modeling for interactive/full‑duplex dialogue, turn-taking, or relevant evaluations, thus matching the exclusion criteria; final score: 2.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I considered the two junior reviewers' split opinions (4 vs 2) and conclude that although the paper uses \"audio language models\" and continuous autoregressive generation, it is primarily a TTS/speech-editing system focused on synthesis quality and sampling efficiency without addressing spoken-language-modeling for interactive/full‑duplex dialogue, turn-taking, or relevant evaluations, thus matching the exclusion criteria; final score: 2.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Few-Shot Load Forecasting Under Data Scarcity in Smart Grids: A Meta-Learning Approach",
    "abstract": "Despite the rapid expansion of smart grids and large volumes of data at the individual consumer level, there are still various cases where adequate data collection to train accurate load forecasting models is challenging or even impossible. This paper proposes adapting an established model-agnostic meta-learning algorithm for short-term load forecasting in the context of few-shot learning. Specifically, the proposed method can rapidly adapt and generalize within any unknown load time series of arbitrary length using only minimal training samples. In this context, the meta-learning model learns an optimal set of initial parameters for a base-level learner recurrent neural network. The proposed model is evaluated using a dataset of historical load consumption data from real-world consumers. Despite the examined load series' short length, it produces accurate forecasts outperforming transfer learning and task-specific machine learning methods by $12.5\\%$. To enhance robustness and fairness during model evaluation, a novel metric, mean average log percentage error, is proposed that alleviates the bias introduced by the commonly used MAPE metric. Finally, a series of studies to evaluate the model's robustness under different hyperparameters and time series lengths is also conducted, demonstrating that the proposed approach consistently outperforms all other models.",
    "metadata": {
      "arxiv_id": "2406.05887",
      "title": "Few-Shot Load Forecasting Under Data Scarcity in Smart Grids: A Meta-Learning Approach",
      "summary": "Despite the rapid expansion of smart grids and large volumes of data at the individual consumer level, there are still various cases where adequate data collection to train accurate load forecasting models is challenging or even impossible. This paper proposes adapting an established model-agnostic meta-learning algorithm for short-term load forecasting in the context of few-shot learning. Specifically, the proposed method can rapidly adapt and generalize within any unknown load time series of arbitrary length using only minimal training samples. In this context, the meta-learning model learns an optimal set of initial parameters for a base-level learner recurrent neural network. The proposed model is evaluated using a dataset of historical load consumption data from real-world consumers. Despite the examined load series' short length, it produces accurate forecasts outperforming transfer learning and task-specific machine learning methods by $12.5\\%$. To enhance robustness and fairness during model evaluation, a novel metric, mean average log percentage error, is proposed that alleviates the bias introduced by the commonly used MAPE metric. Finally, a series of studies to evaluate the model's robustness under different hyperparameters and time series lengths is also conducted, demonstrating that the proposed approach consistently outperforms all other models.",
      "authors": [
        "Georgios Tsoumplekas",
        "Christos L. Athanasiadis",
        "Dimitrios I. Doukas",
        "Antonios Chrysopoulos",
        "Pericles A. Mitkas"
      ],
      "published": "2024-06-09T18:59:08Z",
      "updated": "2024-06-09T18:59:08Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05887v1",
      "landing_url": "https://arxiv.org/abs/2406.05887v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.05887"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns few-shot load forecasting in smart grids with meta-learning for time-series data and does not address Spoken Language Models or speech-language modeling concepts, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns few-shot load forecasting in smart grids with meta-learning for time-series data and does not address Spoken Language Models or speech-language modeling concepts, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on load forecasting in smart grids using meta-learning applied to time series data of electrical consumption; it does not address spoken language models, speech language modeling, or full-duplex spoken interaction as required by the inclusion criteria. It also does not discuss any aspect of speech modeling, synchronous interaction, or the specific evaluation frameworks mentioned. Therefore, it is not aligned with the core thematic requirements and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on load forecasting in smart grids using meta-learning applied to time series data of electrical consumption; it does not address spoken language models, speech language modeling, or full-duplex spoken interaction as required by the inclusion criteria. It also does not discuss any aspect of speech modeling, synchronous interaction, or the specific evaluation frameworks mentioned. Therefore, it is not aligned with the core thematic requirements and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio",
    "abstract": "With the proliferation of Large Language Model (LLM) based deepfake audio, there is an urgent need for effective detection methods. Previous deepfake audio generation methods typically involve a multi-step generation process, with the final step using a vocoder to predict the waveform from handcrafted features. However, LLM-based audio is directly generated from discrete neural codecs in an end-to-end generation process, skipping the final step of vocoder processing. This poses a significant challenge for current audio deepfake detection (ADD) models based on vocoder artifacts. To effectively detect LLM-based deepfake audio, we focus on the core of the generation process, the conversion from neural codec to waveform. We propose Codecfake dataset, which is generated by seven representative neural codec methods. Experiment results show that codec-trained ADD models exhibit a 41.406% reduction in average equal error rate compared to vocoder-trained ADD models on the Codecfake test set.",
    "metadata": {
      "arxiv_id": "2406.08112",
      "title": "Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio",
      "summary": "With the proliferation of Large Language Model (LLM) based deepfake audio, there is an urgent need for effective detection methods. Previous deepfake audio generation methods typically involve a multi-step generation process, with the final step using a vocoder to predict the waveform from handcrafted features. However, LLM-based audio is directly generated from discrete neural codecs in an end-to-end generation process, skipping the final step of vocoder processing. This poses a significant challenge for current audio deepfake detection (ADD) models based on vocoder artifacts. To effectively detect LLM-based deepfake audio, we focus on the core of the generation process, the conversion from neural codec to waveform. We propose Codecfake dataset, which is generated by seven representative neural codec methods. Experiment results show that codec-trained ADD models exhibit a 41.406% reduction in average equal error rate compared to vocoder-trained ADD models on the Codecfake test set.",
      "authors": [
        "Yi Lu",
        "Yuankun Xie",
        "Ruibo Fu",
        "Zhengqi Wen",
        "Jianhua Tao",
        "Zhiyong Wang",
        "Xin Qi",
        "Xuefei Liu",
        "Yongwei Li",
        "Yukun Liu",
        "Xiaopeng Wang",
        "Shuchen Shi"
      ],
      "published": "2024-06-12T11:47:23Z",
      "updated": "2024-06-12T11:47:23Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08112v1",
      "landing_url": "https://arxiv.org/abs/2406.08112v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.08112"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about a dataset and methods for detecting LLM-based deepfake audio and does not center on Spoken Language Models or speech-language modeling as the core object, nor discuss end-to-end SLM architecture or real-time interaction, thus it does not satisfy the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "This item is about a dataset and methods for detecting LLM-based deepfake audio and does not center on Spoken Language Models or speech-language modeling as the core object, nor discuss end-to-end SLM architecture or real-time interaction, thus it does not satisfy the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on detecting LLM-based deepfake audio by analyzing the generation process from neural codecs to waveform, which pertains more to audio signal authenticity detection rather than modeling or reasoning on speech sequences or acoustic signals in spoken language models. It does not explicitly involve Spoken Language Models, full-duplex interaction, or speech language modeling architectures/training/evaluation as required by the inclusion criteria, nor does it discuss synchronization, cognitive parallelism, or multi-dimensional evaluation related to interactive spoken language systems.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on detecting LLM-based deepfake audio by analyzing the generation process from neural codecs to waveform, which pertains more to audio signal authenticity detection rather than modeling or reasoning on speech sequences or acoustic signals in spoken language models. It does not explicitly involve Spoken Language Models, full-duplex interaction, or speech language modeling architectures/training/evaluation as required by the inclusion criteria, nor does it discuss synchronization, cognitive parallelism, or multi-dimensional evaluation related to interactive spoken language systems.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "CoLM-DSR: Leveraging Neural Codec Language Modeling for Multi-Modal Dysarthric Speech Reconstruction",
    "abstract": "Dysarthric speech reconstruction (DSR) aims to transform dysarthric speech into normal speech. It still suffers from low speaker similarity and poor prosody naturalness. In this paper, we propose a multi-modal DSR model by leveraging neural codec language modeling to improve the reconstruction results, especially for the speaker similarity and prosody naturalness. Our proposed model consists of: (i) a multi-modal content encoder to extract robust phoneme embeddings from dysarthric speech with auxiliary visual inputs; (ii) a speaker codec encoder to extract and normalize the speaker-aware codecs from the dysarthric speech, in order to provide original timbre and normal prosody; (iii) a codec language model based speech decoder to reconstruct the speech based on the extracted phoneme embeddings and normalized codecs. Evaluations on the commonly used UASpeech corpus show that our proposed model can achieve significant improvements in terms of speaker similarity and prosody naturalness.",
    "metadata": {
      "arxiv_id": "2406.08336",
      "title": "CoLM-DSR: Leveraging Neural Codec Language Modeling for Multi-Modal Dysarthric Speech Reconstruction",
      "summary": "Dysarthric speech reconstruction (DSR) aims to transform dysarthric speech into normal speech. It still suffers from low speaker similarity and poor prosody naturalness. In this paper, we propose a multi-modal DSR model by leveraging neural codec language modeling to improve the reconstruction results, especially for the speaker similarity and prosody naturalness. Our proposed model consists of: (i) a multi-modal content encoder to extract robust phoneme embeddings from dysarthric speech with auxiliary visual inputs; (ii) a speaker codec encoder to extract and normalize the speaker-aware codecs from the dysarthric speech, in order to provide original timbre and normal prosody; (iii) a codec language model based speech decoder to reconstruct the speech based on the extracted phoneme embeddings and normalized codecs. Evaluations on the commonly used UASpeech corpus show that our proposed model can achieve significant improvements in terms of speaker similarity and prosody naturalness.",
      "authors": [
        "Xueyuan Chen",
        "Dongchao Yang",
        "Dingdong Wang",
        "Xixin Wu",
        "Zhiyong Wu",
        "Helen Meng"
      ],
      "published": "2024-06-12T15:42:21Z",
      "updated": "2024-06-24T06:09:42Z",
      "categories": [
        "cs.SD",
        "cs.CV",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08336v2",
      "landing_url": "https://arxiv.org/abs/2406.08336v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.08336"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on speech signals and uses a language-modeling approach (neural codec language modeling) for dysarthric speech reconstruction, aligning with core speech-language modeling concepts and falls within the 3-year window; while the abstract does not explicitly use the term 'spoken language model', it describes language modeling of speech via a codecLM framework, which can be considered an equivalent concept.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item centers on speech signals and uses a language-modeling approach (neural codec language modeling) for dysarthric speech reconstruction, aligning with core speech-language modeling concepts and falls within the 3-year window; while the abstract does not explicitly use the term 'spoken language model', it describes language modeling of speech via a codecLM framework, which can be considered an equivalent concept.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on dysarthric speech reconstruction leveraging neural codec language modeling, but it does not explicitly address core aspects of Spoken Language Models or Full-Duplex Spoken Language Models such as turn-taking, real-time interaction, cognitive parallelism, or synchronization in conversational contexts. The abstract emphasizes speech signal processing for reconstruction rather than spoken language modeling or dialogue interaction, and does not mention multi-modal dialogue behavior arbitration or low-latency synchronous communication features.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on dysarthric speech reconstruction leveraging neural codec language modeling, but it does not explicitly address core aspects of Spoken Language Models or Full-Duplex Spoken Language Models such as turn-taking, real-time interaction, cognitive parallelism, or synchronization in conversational contexts. The abstract emphasizes speech signal processing for reconstruction rather than spoken language modeling or dialogue interaction, and does not mention multi-modal dialogue behavior arbitration or low-latency synchronous communication features.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted relevant points: one flagged that the paper explicitly uses neural codec language modeling (an equivalent of language modeling of speech) and describes model architecture and evaluation, while the other correctly observed it does not address dialogue/full‑duplex interaction. According to the inclusion/exclusion rules, the paper explicitly contains an equivalent description ('neural codec language modeling') treating speech as the primary modeling object and details model components and evaluation, so it satisfies the inclusion criteria even though it is a single-task reconstruction study and does not cover interactive/full‑duplex behaviors; therefore I recommend inclusion but not as an unequivocal top priority.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted relevant points: one flagged that the paper explicitly uses neural codec language modeling (an equivalent of language modeling of speech) and describes model architecture and evaluation, while the other correctly observed it does not address dialogue/full‑duplex interaction. According to the inclusion/exclusion rules, the paper explicitly contains an equivalent description ('neural codec language modeling') treating speech as the primary modeling object and details model components and evaluation, so it satisfies the inclusion criteria even though it is a single-task reconstruction study and does not cover interactive/full‑duplex behaviors; therefore I recommend inclusion but not as an unequivocal top priority.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "TokSing: Singing Voice Synthesis based on Discrete Tokens",
    "abstract": "Recent advancements in speech synthesis witness significant benefits by leveraging discrete tokens extracted from self-supervised learning (SSL) models. Discrete tokens offer higher storage efficiency and greater operability in intermediate representations compared to traditional continuous Mel spectrograms. However, when it comes to singing voice synthesis(SVS), achieving higher levels of melody expression poses a great challenge for utilizing discrete tokens. In this paper, we introduce TokSing, a discrete-based SVS system equipped with a token formulator that offers flexible token blendings. We observe a melody degradation during discretization, prompting us to integrate a melody signal with the discrete token and incorporate a specially-designed melody enhancement strategy in the musical encoder. Extensive experiments demonstrate that our TokSing achieves better performance against the Mel spectrogram baselines while offering advantages in intermediate representation space cost and convergence speed.",
    "metadata": {
      "arxiv_id": "2406.08416",
      "title": "TokSing: Singing Voice Synthesis based on Discrete Tokens",
      "summary": "Recent advancements in speech synthesis witness significant benefits by leveraging discrete tokens extracted from self-supervised learning (SSL) models. Discrete tokens offer higher storage efficiency and greater operability in intermediate representations compared to traditional continuous Mel spectrograms. However, when it comes to singing voice synthesis(SVS), achieving higher levels of melody expression poses a great challenge for utilizing discrete tokens. In this paper, we introduce TokSing, a discrete-based SVS system equipped with a token formulator that offers flexible token blendings. We observe a melody degradation during discretization, prompting us to integrate a melody signal with the discrete token and incorporate a specially-designed melody enhancement strategy in the musical encoder. Extensive experiments demonstrate that our TokSing achieves better performance against the Mel spectrogram baselines while offering advantages in intermediate representation space cost and convergence speed.",
      "authors": [
        "Yuning Wu",
        "Chunlei zhang",
        "Jiatong Shi",
        "Yuxun Tang",
        "Shan Yang",
        "Qin Jin"
      ],
      "published": "2024-06-12T16:59:24Z",
      "updated": "2024-06-20T16:46:14Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08416v2",
      "landing_url": "https://arxiv.org/abs/2406.08416v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.08416"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on singing voice synthesis via discrete tokens and token formulation, but does not treat Spoken Language Models or their equivalents as the core modeling paradigm (voice modeling is treated as SVS/TTs, not speech-language modeling with dialogue interaction or turn-taking), thus it should be excluded under the stated criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work centers on singing voice synthesis via discrete tokens and token formulation, but does not treat Spoken Language Models or their equivalents as the core modeling paradigm (voice modeling is treated as SVS/TTs, not speech-language modeling with dialogue interaction or turn-taking), thus it should be excluded under the stated criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on singing voice synthesis using discrete tokens and does not address spoken language models or full-duplex spoken language models nor aspects like turn-taking, cognitive parallelism, or real-time interaction synchronization, which are key inclusion criteria; it also centers on a single speech technology (Singing Voice Synthesis) without discussions on interaction, arbitration, or comprehensive spoken language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on singing voice synthesis using discrete tokens and does not address spoken language models or full-duplex spoken language models nor aspects like turn-taking, cognitive parallelism, or real-time interaction synchronization, which are key inclusion criteria; it also centers on a single speech technology (Singing Voice Synthesis) without discussions on interaction, arbitration, or comprehensive spoken language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SingOMD: Singing Oriented Multi-resolution Discrete Representation Construction from Speech Models",
    "abstract": "Discrete representation has shown advantages in speech generation tasks, wherein discrete tokens are derived by discretizing hidden features from self-supervised learning (SSL) pre-trained models. However, the direct application of speech SSL models to singing generation encounters domain gaps between speech and singing. Furthermore, singing generation necessitates a more refined representation than typical speech. To address these challenges, we introduce SingOMD, a novel method to extract singing-oriented multi-resolution discrete representations from speech SSL models. Specifically, we first adapt the features from speech SSL through a resynthesis task and incorporate multi-resolution modules based on resampling to better serve singing generation. These adapted multi-resolution features are then discretized via clustering. Extensive experiments demonstrate the robustness, efficiency, and effectiveness of these representations in singing vocoders and singing voice synthesis.",
    "metadata": {
      "arxiv_id": "2406.08905",
      "title": "SingOMD: Singing Oriented Multi-resolution Discrete Representation Construction from Speech Models",
      "summary": "Discrete representation has shown advantages in speech generation tasks, wherein discrete tokens are derived by discretizing hidden features from self-supervised learning (SSL) pre-trained models. However, the direct application of speech SSL models to singing generation encounters domain gaps between speech and singing. Furthermore, singing generation necessitates a more refined representation than typical speech. To address these challenges, we introduce SingOMD, a novel method to extract singing-oriented multi-resolution discrete representations from speech SSL models. Specifically, we first adapt the features from speech SSL through a resynthesis task and incorporate multi-resolution modules based on resampling to better serve singing generation. These adapted multi-resolution features are then discretized via clustering. Extensive experiments demonstrate the robustness, efficiency, and effectiveness of these representations in singing vocoders and singing voice synthesis.",
      "authors": [
        "Yuxun Tang",
        "Yuning Wu",
        "Jiatong Shi",
        "Qin Jin"
      ],
      "published": "2024-06-13T08:00:25Z",
      "updated": "2024-06-20T11:01:14Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08905v2",
      "landing_url": "https://arxiv.org/abs/2406.08905v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.08905"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "While the work leverages speech SSL representations for singing generation and is within the last 3 years, it does not center on Spoken Language Models or equivalent language-modeling concepts as the core focus; it targets singing generation/synthesis and discrete representation from SSL rather than LMs for speech, thus it fails the core inclusion criterion (SLMs/FD-SLMs as main object).",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "While the work leverages speech SSL representations for singing generation and is within the last 3 years, it does not center on Spoken Language Models or equivalent language-modeling concepts as the core focus; it targets singing generation/synthesis and discrete representation from SSL rather than LMs for speech, thus it fails the core inclusion criterion (SLMs/FD-SLMs as main object).",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on extracting discrete representations from speech models tailored for singing voice synthesis, but it does not explicitly discuss spoken language modeling, full-duplex spoken language models, or interactive speech interactions; it primarily deals with discrete representation for singing synthesis rather than general speech language modeling and interactive behaviors required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on extracting discrete representations from speech models tailored for singing voice synthesis, but it does not explicitly discuss spoken language modeling, full-duplex spoken language models, or interactive speech interactions; it primarily deals with discrete representation for singing synthesis rather than general speech language modeling and interactive behaviors required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "UniAudio 1.5: Large Language Model-driven Audio Codec is A Few-shot Audio Task Learner",
    "abstract": "The Large Language models (LLMs) have demonstrated supreme capabilities in text understanding and generation, but cannot be directly applied to cross-modal tasks without fine-tuning. This paper proposes a cross-modal in-context learning approach, empowering the frozen LLMs to achieve multiple audio tasks in a few-shot style without any parameter update. Specifically, we propose a novel and LLMs-driven audio codec model, LLM-Codec, to transfer the audio modality into the textual space, \\textit{i.e.} representing audio tokens with words or sub-words in the vocabulary of LLMs, while keeping high audio reconstruction quality. The key idea is to reduce the modality heterogeneity between text and audio by compressing the audio modality into a well-trained LLMs token space. Thus, the audio representation can be viewed as a new \\textit{foreign language}, and LLMs can learn the new \\textit{foreign language} with several demonstrations. In experiments, we investigate the performance of the proposed approach across multiple audio understanding and generation tasks, \\textit{e.g.} speech emotion classification, audio classification, text-to-speech generation, speech enhancement, etc. The experimental results demonstrate that the LLMs equipped with the proposed LLM-Codec, named as UniAudio 1.5, prompted by only a few examples, can achieve the expected functions in simple scenarios. It validates the feasibility and effectiveness of the proposed cross-modal in-context learning approach. To facilitate research on few-shot audio task learning and multi-modal LLMs, we have open-sourced the LLM-Codec model.",
    "metadata": {
      "arxiv_id": "2406.10056",
      "title": "UniAudio 1.5: Large Language Model-driven Audio Codec is A Few-shot Audio Task Learner",
      "summary": "The Large Language models (LLMs) have demonstrated supreme capabilities in text understanding and generation, but cannot be directly applied to cross-modal tasks without fine-tuning. This paper proposes a cross-modal in-context learning approach, empowering the frozen LLMs to achieve multiple audio tasks in a few-shot style without any parameter update. Specifically, we propose a novel and LLMs-driven audio codec model, LLM-Codec, to transfer the audio modality into the textual space, \\textit{i.e.} representing audio tokens with words or sub-words in the vocabulary of LLMs, while keeping high audio reconstruction quality. The key idea is to reduce the modality heterogeneity between text and audio by compressing the audio modality into a well-trained LLMs token space. Thus, the audio representation can be viewed as a new \\textit{foreign language}, and LLMs can learn the new \\textit{foreign language} with several demonstrations. In experiments, we investigate the performance of the proposed approach across multiple audio understanding and generation tasks, \\textit{e.g.} speech emotion classification, audio classification, text-to-speech generation, speech enhancement, etc. The experimental results demonstrate that the LLMs equipped with the proposed LLM-Codec, named as UniAudio 1.5, prompted by only a few examples, can achieve the expected functions in simple scenarios. It validates the feasibility and effectiveness of the proposed cross-modal in-context learning approach. To facilitate research on few-shot audio task learning and multi-modal LLMs, we have open-sourced the LLM-Codec model.",
      "authors": [
        "Dongchao Yang",
        "Haohan Guo",
        "Yuanyuan Wang",
        "Rongjie Huang",
        "Xiang Li",
        "Xu Tan",
        "Xixin Wu",
        "Helen Meng"
      ],
      "published": "2024-06-14T14:13:18Z",
      "updated": "2024-06-14T14:13:18Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10056v1",
      "landing_url": "https://arxiv.org/abs/2406.10056v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10056"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the work uses LLMs for audio tasks and treats audio as a modality processed by language models, the abstract does not explicitly mention spoken language models or language modeling of speech in the title/abstract/keywords, failing the explicit keyword criterion.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the work uses LLMs for audio tasks and treats audio as a modality processed by language models, the abstract does not explicitly mention spoken language models or language modeling of speech in the title/abstract/keywords, failing the explicit keyword criterion.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a novel audio codec model enabling frozen LLMs to perform multiple audio tasks like speech emotion classification, audio classification, text-to-speech generation, and speech enhancement in a few-shot learning manner. However, the paper does not explicitly focus on Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) as core modeling objects, nor does it address issues such as synchronous interaction, cognitive parallelism, or real-time low-latency interaction. Moreover, the model transforms audio into textual token space rather than emphasizing direct speech-level language modeling or full-duplex interactions, and evaluation focuses on simple scenarios rather than multi-dimensional or dynamic behavioral assessments. Therefore, it does not fully meet the inclusion criteria centered on SLMs or FD-SLMs and synchronization aspects, and partially touches on speech-related modeling without the interactive/dialogue behavior focus required.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes a novel audio codec model enabling frozen LLMs to perform multiple audio tasks like speech emotion classification, audio classification, text-to-speech generation, and speech enhancement in a few-shot learning manner. However, the paper does not explicitly focus on Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) as core modeling objects, nor does it address issues such as synchronous interaction, cognitive parallelism, or real-time low-latency interaction. Moreover, the model transforms audio into textual token space rather than emphasizing direct speech-level language modeling or full-duplex interactions, and evaluation focuses on simple scenarios rather than multi-dimensional or dynamic behavioral assessments. Therefore, it does not fully meet the inclusion criteria centered on SLMs or FD-SLMs and synchronization aspects, and partially touches on speech-related modeling without the interactive/dialogue behavior focus required.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Detecting the terminality of speech-turn boundary for spoken interactions in French TV and Radio content",
    "abstract": "Transition Relevance Places are defined as the end of an utterance where the interlocutor may take the floor without interrupting the current speaker --i.e., a place where the turn is terminal. Analyzing turn terminality is useful to study the dynamic of turn-taking in spontaneous conversations. This paper presents an automatic classification of spoken utterances as Terminal or Non-Terminal in multi-speaker settings. We compared audio, text, and fusions of both approaches on a French corpus of TV and Radio extracts annotated with turn-terminality information at each speaker change. Our models are based on pre-trained self-supervised representations. We report results for different fusion strategies and varying context sizes. This study also questions the problem of performance variability by analyzing the differences in results for multiple training runs with random initialization. The measured accuracy would allow the use of these models for large-scale analysis of turn-taking.",
    "metadata": {
      "arxiv_id": "2406.10073",
      "title": "Detecting the terminality of speech-turn boundary for spoken interactions in French TV and Radio content",
      "summary": "Transition Relevance Places are defined as the end of an utterance where the interlocutor may take the floor without interrupting the current speaker --i.e., a place where the turn is terminal. Analyzing turn terminality is useful to study the dynamic of turn-taking in spontaneous conversations. This paper presents an automatic classification of spoken utterances as Terminal or Non-Terminal in multi-speaker settings. We compared audio, text, and fusions of both approaches on a French corpus of TV and Radio extracts annotated with turn-terminality information at each speaker change. Our models are based on pre-trained self-supervised representations. We report results for different fusion strategies and varying context sizes. This study also questions the problem of performance variability by analyzing the differences in results for multiple training runs with random initialization. The measured accuracy would allow the use of these models for large-scale analysis of turn-taking.",
      "authors": [
        "Rémi Uro",
        "Marie Tahon",
        "David Doukhan",
        "Antoine Laurent",
        "Albert Rilliard"
      ],
      "published": "2024-06-14T14:28:06Z",
      "updated": "2024-06-14T14:28:06Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.HC",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10073v1",
      "landing_url": "https://arxiv.org/abs/2406.10073v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10073"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on automatic classification of turn-terminal vs. non-terminal utterances for turn-taking in multi-speaker French content, using pre-trained SSL representations; it does not center on Spoken Language Models or full-duplex, speech-centric modeling, nor discuss architectures/training/evaluation within an SLM/FD-SLM framework, so it fails the core inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on automatic classification of turn-terminal vs. non-terminal utterances for turn-taking in multi-speaker French content, using pre-trained SSL representations; it does not center on Spoken Language Models or full-duplex, speech-centric modeling, nor discuss architectures/training/evaluation within an SLM/FD-SLM framework, so it fails the core inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on automatic classification of spoken utterances as terminal or non-terminal in multi-speaker French TV and radio content using pre-trained self-supervised representations, which aligns with spoken language modeling aspects involving acoustic signal-based analysis and turn-taking dynamics. However, the abstract lacks explicit mention of 'Spoken Language Models' or related terminology and does not clearly discuss model architecture or training related to end-to-end spoken language models or full-duplex interaction capabilities as specified in the inclusion criteria. Moreover, it focuses on half-duplex turn-taking detection rather than full-duplex or synchronous interaction management. Therefore, it does not fully meet the inclusion criteria and partially overlaps with exclusion criteria related to half-duplex discussions and the lack of clear speech-LLM modeling focus.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on automatic classification of spoken utterances as terminal or non-terminal in multi-speaker French TV and radio content using pre-trained self-supervised representations, which aligns with spoken language modeling aspects involving acoustic signal-based analysis and turn-taking dynamics. However, the abstract lacks explicit mention of 'Spoken Language Models' or related terminology and does not clearly discuss model architecture or training related to end-to-end spoken language models or full-duplex interaction capabilities as specified in the inclusion criteria. Moreover, it focuses on half-duplex turn-taking detection rather than full-duplex or synchronous interaction management. Therefore, it does not fully meet the inclusion criteria and partially overlaps with exclusion criteria related to half-duplex discussions and the lack of clear speech-LLM modeling focus.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Let the Poem Hit the Rhythm: Using a Byte-Based Transformer for Beat-Aligned Poetry Generation",
    "abstract": "The intersection between poetry and music provides an interesting case for computational creativity, yet remains relatively unexplored. This paper explores the integration of poetry and music through the lens of beat patterns, investigating whether a byte-based language model can generate words that fit specific beat patterns within the context of poetry. Drawing on earlier studies, we developed a method to train a byte-based transformer model, ByT5, to align poems with beat patterns. The results demonstrate a high level of beat alignment while maintaining semantic coherence. Future work will aim to improve the model's ability to create complete beat-aligned poems.",
    "metadata": {
      "arxiv_id": "2406.10174",
      "title": "Let the Poem Hit the Rhythm: Using a Byte-Based Transformer for Beat-Aligned Poetry Generation",
      "summary": "The intersection between poetry and music provides an interesting case for computational creativity, yet remains relatively unexplored. This paper explores the integration of poetry and music through the lens of beat patterns, investigating whether a byte-based language model can generate words that fit specific beat patterns within the context of poetry. Drawing on earlier studies, we developed a method to train a byte-based transformer model, ByT5, to align poems with beat patterns. The results demonstrate a high level of beat alignment while maintaining semantic coherence. Future work will aim to improve the model's ability to create complete beat-aligned poems.",
      "authors": [
        "Mohamad Elzohbi",
        "Richard Zhao"
      ],
      "published": "2024-06-14T16:54:48Z",
      "updated": "2024-06-14T16:54:48Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10174v1",
      "landing_url": "https://arxiv.org/abs/2406.10174v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10174"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper centers on text-based beat-aligned poetry generation and does not treat Spoken Language Models or speech as the core modeling target, thus it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper centers on text-based beat-aligned poetry generation and does not treat Spoken Language Models or speech as the core modeling target, thus it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using a byte-based transformer for poetry generation aligned with beats, which concerns text generation and computational creativity, without mention of spoken language models, speech signals as core objects, or speech-based modeling; it does not meet the inclusion criteria centered on spoken language models and speech-level modeling, nor does it address full-duplex spoken interaction, asynchronous behavior arbitration, or low-latency speech interaction. Therefore, it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using a byte-based transformer for poetry generation aligned with beats, which concerns text generation and computational creativity, without mention of spoken language models, speech signals as core objects, or speech-based modeling; it does not meet the inclusion criteria centered on spoken language models and speech-level modeling, nor does it address full-duplex spoken interaction, asynchronous behavior arbitration, or low-latency speech interaction. Therefore, it should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "TokenRec: Learning to Tokenize ID for LLM-based Generative Recommendation",
    "abstract": "There is a growing interest in utilizing large-scale language models (LLMs) to advance next-generation Recommender Systems (RecSys), driven by their outstanding language understanding and in-context learning capabilities. In this scenario, tokenizing (i.e., indexing) users and items becomes essential for ensuring a seamless alignment of LLMs with recommendations. While several studies have made progress in representing users and items through textual contents or latent representations, challenges remain in efficiently capturing high-order collaborative knowledge into discrete tokens that are compatible with LLMs. Additionally, the majority of existing tokenization approaches often face difficulties in generalizing effectively to new/unseen users or items that were not in the training corpus. To address these challenges, we propose a novel framework called TokenRec, which introduces not only an effective ID tokenization strategy but also an efficient retrieval paradigm for LLM-based recommendations. Specifically, our tokenization strategy, Masked Vector-Quantized (MQ) Tokenizer, involves quantizing the masked user/item representations learned from collaborative filtering into discrete tokens, thus achieving a smooth incorporation of high-order collaborative knowledge and a generalizable tokenization of users and items for LLM-based RecSys. Meanwhile, our generative retrieval paradigm is designed to efficiently recommend top-$K$ items for users to eliminate the need for the time-consuming auto-regressive decoding and beam search processes used by LLMs, thus significantly reducing inference time. Comprehensive experiments validate the effectiveness of the proposed methods, demonstrating that TokenRec outperforms competitive benchmarks, including both traditional recommender systems and emerging LLM-based recommender systems.",
    "metadata": {
      "arxiv_id": "2406.10450",
      "title": "TokenRec: Learning to Tokenize ID for LLM-based Generative Recommendation",
      "summary": "There is a growing interest in utilizing large-scale language models (LLMs) to advance next-generation Recommender Systems (RecSys), driven by their outstanding language understanding and in-context learning capabilities. In this scenario, tokenizing (i.e., indexing) users and items becomes essential for ensuring a seamless alignment of LLMs with recommendations. While several studies have made progress in representing users and items through textual contents or latent representations, challenges remain in efficiently capturing high-order collaborative knowledge into discrete tokens that are compatible with LLMs. Additionally, the majority of existing tokenization approaches often face difficulties in generalizing effectively to new/unseen users or items that were not in the training corpus. To address these challenges, we propose a novel framework called TokenRec, which introduces not only an effective ID tokenization strategy but also an efficient retrieval paradigm for LLM-based recommendations. Specifically, our tokenization strategy, Masked Vector-Quantized (MQ) Tokenizer, involves quantizing the masked user/item representations learned from collaborative filtering into discrete tokens, thus achieving a smooth incorporation of high-order collaborative knowledge and a generalizable tokenization of users and items for LLM-based RecSys. Meanwhile, our generative retrieval paradigm is designed to efficiently recommend top-$K$ items for users to eliminate the need for the time-consuming auto-regressive decoding and beam search processes used by LLMs, thus significantly reducing inference time. Comprehensive experiments validate the effectiveness of the proposed methods, demonstrating that TokenRec outperforms competitive benchmarks, including both traditional recommender systems and emerging LLM-based recommender systems.",
      "authors": [
        "Haohao Qu",
        "Wenqi Fan",
        "Zihuai Zhao",
        "Qing Li"
      ],
      "published": "2024-06-15T00:07:44Z",
      "updated": "2025-08-15T05:34:06Z",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10450v3",
      "landing_url": "https://arxiv.org/abs/2406.10450v3",
      "doi": "https://doi.org/10.48550/arXiv.2406.10450"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on tokenizing IDs for LLM-based recommendations and does not center on Spoken Language Models or any speech-language modeling aspects, thus it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on tokenizing IDs for LLM-based recommendations and does not center on Spoken Language Models or any speech-language modeling aspects, thus it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on tokenization strategies for large language model-based recommender systems, with no mention of spoken language models, speech data modeling, full-duplex interaction, or speech-language model architectures; it primarily addresses recommendation and collaborative filtering rather than speech or spoken language modeling, hence it does not meet the inclusion criteria focused on speech-centric language modeling research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on tokenization strategies for large language model-based recommender systems, with no mention of spoken language models, speech data modeling, full-duplex interaction, or speech-language model architectures; it primarily addresses recommendation and collaborative filtering rather than speech or spoken language modeling, hence it does not meet the inclusion criteria focused on speech-centric language modeling research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?",
    "abstract": "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.",
    "metadata": {
      "arxiv_id": "2406.10735",
      "title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?",
      "summary": "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.",
      "authors": [
        "Pooneh Mousavi",
        "Jarod Duret",
        "Salah Zaiem",
        "Luca Della Libera",
        "Artem Ploujnikov",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "published": "2024-06-15T20:43:07Z",
      "updated": "2024-06-15T20:43:07Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10735v1",
      "landing_url": "https://arxiv.org/abs/2406.10735v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10735"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on discrete audio token extraction from SSL models, not on Spoken Language Models or end-to-end speech-language architectures, and lacks explicit framing around SLM/FD-SLM concepts or conversational interaction; thus it does not meet the inclusion criteria (and may violate exclusion criteria by not addressing interactive speech-language modeling).",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on discrete audio token extraction from SSL models, not on Spoken Language Models or end-to-end speech-language architectures, and lacks explicit framing around SLM/FD-SLM concepts or conversational interaction; thus it does not meet the inclusion criteria (and may violate exclusion criteria by not addressing interactive speech-language modeling).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on extracting discrete audio tokens from self-supervised models, concentrating on audio representation and tokenization methods rather than on spoken language models used for full-duplex or synchronized interactive dialogue systems; it lacks explicit mention of 'spoken language models' or related terms and does not discuss real-time interaction, turn-taking, or cognitive parallelism, thus not fully meeting the inclusion criteria and partially aligning with exclusion criteria related to lack of dialogue or real-time interaction focus.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on extracting discrete audio tokens from self-supervised models, concentrating on audio representation and tokenization methods rather than on spoken language models used for full-duplex or synchronized interactive dialogue systems; it lacks explicit mention of 'spoken language models' or related terms and does not discuss real-time interaction, turn-taking, or cognitive parallelism, thus not fully meeting the inclusion criteria and partially aligning with exclusion criteria related to lack of dialogue or real-time interaction focus.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "NAST: Noise Aware Speech Tokenization for Speech Language Models",
    "abstract": "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.",
    "metadata": {
      "arxiv_id": "2406.11037",
      "title": "NAST: Noise Aware Speech Tokenization for Speech Language Models",
      "summary": "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.",
      "authors": [
        "Shoval Messica",
        "Yossi Adi"
      ],
      "published": "2024-06-16T18:20:45Z",
      "updated": "2024-06-16T18:20:45Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11037v1",
      "landing_url": "https://arxiv.org/abs/2406.11037v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.11037"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on Noise Aware Speech Tokenization as a component of Speech Language Models with speech as the core modeling unit, discusses architecture and evaluation for SLMs, and is from a 2025 publication, meeting all inclusion criteria and none of the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on Noise Aware Speech Tokenization as a component of Speech Language Models with speech as the core modeling unit, discusses architecture and evaluation for SLMs, and is from a 2025 publication, meeting all inclusion criteria and none of the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on noise-aware speech tokenization as a core task for speech language models (SLMs), explicitly addressing speech tokenization which serves as the basis for speech language models, thus aligning with the key concept of speech-level language modeling. The abstract mentions evaluation on spoken language modeling tasks, indicating involvement with model training and evaluation. However, the study appears to concentrate on tokenization and robustness to noise rather than exploring full-duplex spoken language modeling, synchronization for interactive behaviors like turn-taking or backchanneling, or real-time interaction aspects emphasized by the inclusion criteria. It also lacks mention of cognitive parallelism or low-latency interaction evaluations. While it is relevant to speech language models research, it does not fully meet the detailed inclusion criteria nor does it trigger any explicit exclusion conditions, placing it between unsure and mild inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on noise-aware speech tokenization as a core task for speech language models (SLMs), explicitly addressing speech tokenization which serves as the basis for speech language models, thus aligning with the key concept of speech-level language modeling. The abstract mentions evaluation on spoken language modeling tasks, indicating involvement with model training and evaluation. However, the study appears to concentrate on tokenization and robustness to noise rather than exploring full-duplex spoken language modeling, synchronization for interactive behaviors like turn-taking or backchanneling, or real-time interaction aspects emphasized by the inclusion criteria. It also lacks mention of cognitive parallelism or low-latency interaction evaluations. While it is relevant to speech language models research, it does not fully meet the detailed inclusion criteria nor does it trigger any explicit exclusion conditions, placing it between unsure and mild inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The paper explicitly addresses speech tokenization for Speech Language Models (the title and abstract contain \"Speech Language Models\"), describes a concrete model architecture (predictor, residual encoder, decoder), reports evaluation on spoken language modeling tasks, and analyzes robustness—therefore it treats speech as the core modeling object and discusses model/ evaluation details required by the inclusion criteria; it does not fall under exclusions (it is not merely ASR/TTS or only WER-focused nor solely about half-duplex interaction). I considered the two junior reviews: one recommended inclusion (5) for these same reasons; the other was cautious (3) because the work focuses on tokenization and not on full-duplex interactive behaviors, but full-duplex requirements are not mandatory for inclusion. Score explanation: Because this work is directly about speech-level modeling for SLMs with architecture and evaluation, it should be included.",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "The paper explicitly addresses speech tokenization for Speech Language Models (the title and abstract contain \"Speech Language Models\"), describes a concrete model architecture (predictor, residual encoder, decoder), reports evaluation on spoken language modeling tasks, and analyzes robustness—therefore it treats speech as the core modeling object and discusses model/ evaluation details required by the inclusion criteria; it does not fall under exclusions (it is not merely ASR/TTS or only WER-focused nor solely about half-duplex interaction). I considered the two junior reviews: one recommended inclusion (5) for these same reasons; the other was cautious (3) because the work focuses on tokenization and not on full-duplex interactive behaviors, but full-duplex requirements are not mandatory for inclusion. Score explanation: Because this work is directly about speech-level modeling for SLMs with architecture and evaluation, it should be included.",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MASAI: Modular Architecture for Software-engineering AI Agents",
    "abstract": "A common method to solve complex problems in software engineering, is to divide the problem into multiple sub-problems. Inspired by this, we propose a Modular Architecture for Software-engineering AI (MASAI) agents, where different LLM-powered sub-agents are instantiated with well-defined objectives and strategies tuned to achieve those objectives. Our modular architecture offers several advantages: (1) employing and tuning different problem-solving strategies across sub-agents, (2) enabling sub-agents to gather information from different sources scattered throughout a repository, and (3) avoiding unnecessarily long trajectories which inflate costs and add extraneous context. MASAI enabled us to achieve the highest performance (28.33% resolution rate) on the popular and highly challenging SWE-bench Lite dataset consisting of 300 GitHub issues from 11 Python repositories. We conduct a comprehensive evaluation of MASAI relative to other agentic methods and analyze the effects of our design decisions and their contribution to the success of MASAI.",
    "metadata": {
      "arxiv_id": "2406.11638",
      "title": "MASAI: Modular Architecture for Software-engineering AI Agents",
      "summary": "A common method to solve complex problems in software engineering, is to divide the problem into multiple sub-problems. Inspired by this, we propose a Modular Architecture for Software-engineering AI (MASAI) agents, where different LLM-powered sub-agents are instantiated with well-defined objectives and strategies tuned to achieve those objectives. Our modular architecture offers several advantages: (1) employing and tuning different problem-solving strategies across sub-agents, (2) enabling sub-agents to gather information from different sources scattered throughout a repository, and (3) avoiding unnecessarily long trajectories which inflate costs and add extraneous context. MASAI enabled us to achieve the highest performance (28.33% resolution rate) on the popular and highly challenging SWE-bench Lite dataset consisting of 300 GitHub issues from 11 Python repositories. We conduct a comprehensive evaluation of MASAI relative to other agentic methods and analyze the effects of our design decisions and their contribution to the success of MASAI.",
      "authors": [
        "Daman Arora",
        "Atharv Sonwane",
        "Nalin Wadhwa",
        "Abhav Mehrotra",
        "Saiteja Utpala",
        "Ramakrishna Bairi",
        "Aditya Kanade",
        "Nagarajan Natarajan"
      ],
      "published": "2024-06-17T15:19:51Z",
      "updated": "2024-06-17T15:19:51Z",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11638v1",
      "landing_url": "https://arxiv.org/abs/2406.11638v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.11638"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item addresses MASAI: Modular Architecture for Software-engineering AI Agents and evaluates SWE-bench; it centers on software-engineering agents and does not discuss Spoken Language Models (SLMs) or speech-language modeling, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item addresses MASAI: Modular Architecture for Software-engineering AI Agents and evaluates SWE-bench; it centers on software-engineering agents and does not discuss Spoken Language Models (SLMs) or speech-language modeling, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a modular architecture for software engineering AI agents focused on subdividing software engineering problems and improving agentic LLM methods, with no mention of spoken language models, speech as primary modeling input, or synchronous full-duplex spoken interaction characteristics; thus, it fails to meet the core inclusion criteria focusing on speech-centric language modeling and full-duplex spoken interaction, making it irrelevant to the topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a modular architecture for software engineering AI agents focused on subdividing software engineering problems and improving agentic LLM methods, with no mention of spoken language models, speech as primary modeling input, or synchronous full-duplex spoken interaction characteristics; thus, it fails to meet the core inclusion criteria focusing on speech-centric language modeling and full-duplex spoken interaction, making it irrelevant to the topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SyncVSR: Data-Efficient Visual Speech Recognition with End-to-End Crossmodal Audio Token Synchronization",
    "abstract": "Visual Speech Recognition (VSR) stands at the intersection of computer vision and speech recognition, aiming to interpret spoken content from visual cues. A prominent challenge in VSR is the presence of homophenes-visually similar lip gestures that represent different phonemes. Prior approaches have sought to distinguish fine-grained visemes by aligning visual and auditory semantics, but often fell short of full synchronization. To address this, we present SyncVSR, an end-to-end learning framework that leverages quantized audio for frame-level crossmodal supervision. By integrating a projection layer that synchronizes visual representation with acoustic data, our encoder learns to generate discrete audio tokens from a video sequence in a non-autoregressive manner. SyncVSR shows versatility across tasks, languages, and modalities at the cost of a forward pass. Our empirical evaluations show that it not only achieves state-of-the-art results but also reduces data usage by up to ninefold.",
    "metadata": {
      "arxiv_id": "2406.12233",
      "title": "SyncVSR: Data-Efficient Visual Speech Recognition with End-to-End Crossmodal Audio Token Synchronization",
      "summary": "Visual Speech Recognition (VSR) stands at the intersection of computer vision and speech recognition, aiming to interpret spoken content from visual cues. A prominent challenge in VSR is the presence of homophenes-visually similar lip gestures that represent different phonemes. Prior approaches have sought to distinguish fine-grained visemes by aligning visual and auditory semantics, but often fell short of full synchronization. To address this, we present SyncVSR, an end-to-end learning framework that leverages quantized audio for frame-level crossmodal supervision. By integrating a projection layer that synchronizes visual representation with acoustic data, our encoder learns to generate discrete audio tokens from a video sequence in a non-autoregressive manner. SyncVSR shows versatility across tasks, languages, and modalities at the cost of a forward pass. Our empirical evaluations show that it not only achieves state-of-the-art results but also reduces data usage by up to ninefold.",
      "authors": [
        "Young Jin Ahn",
        "Jungwoo Park",
        "Sangha Park",
        "Jonghyun Choi",
        "Kee-Eung Kim"
      ],
      "published": "2024-06-18T03:14:22Z",
      "updated": "2024-06-18T03:14:22Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.12233v1",
      "landing_url": "https://arxiv.org/abs/2406.12233v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.12233"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This entry centers on visual speech recognition with crossmodal audio token synchronization, but it does not treat speech as the core modeling target (no explicit spoken language model/FD-SLM framework) and lacks emphasis on full-duplex interactive speech/dialogue or dedicated SLM evaluation; thus it does not meet the stated inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "This entry centers on visual speech recognition with crossmodal audio token synchronization, but it does not treat speech as the core modeling target (no explicit spoken language model/FD-SLM framework) and lacks emphasis on full-duplex interactive speech/dialogue or dedicated SLM evaluation; thus it does not meet the stated inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Visual Speech Recognition, leveraging audio-visual synchronization in an end-to-end learning framework to improve lip-reading tasks. However, it does not explicitly address Spoken Language Models or Full-Duplex Spoken Language Models, nor does it discuss turn-taking, interactive dialogue behavior, or speech language modeling as the core model. The study emphasizes speech recognition accuracy and data efficiency rather than interactive or full-duplex speech communication or modeling; as such, it fails to meet the explicit inclusion criteria regarding spoken language model focus and interactive speech behavior modeling and evaluation.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Visual Speech Recognition, leveraging audio-visual synchronization in an end-to-end learning framework to improve lip-reading tasks. However, it does not explicitly address Spoken Language Models or Full-Duplex Spoken Language Models, nor does it discuss turn-taking, interactive dialogue behavior, or speech language modeling as the core model. The study emphasizes speech recognition accuracy and data efficiency rather than interactive or full-duplex speech communication or modeling; as such, it fails to meet the explicit inclusion criteria regarding spoken language model focus and interactive speech behavior modeling and evaluation.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "RITA: A Real-time Interactive Talking Avatars Framework",
    "abstract": "RITA presents a high-quality real-time interactive framework built upon generative models, designed with practical applications in mind. Our framework enables the transformation of user-uploaded photos into digital avatars that can engage in real-time dialogue interactions. By leveraging the latest advancements in generative modeling, we have developed a versatile platform that not only enhances the user experience through dynamic conversational avatars but also opens new avenues for applications in virtual reality, online education, and interactive gaming. This work showcases the potential of integrating computer vision and natural language processing technologies to create immersive and interactive digital personas, pushing the boundaries of how we interact with digital content.",
    "metadata": {
      "arxiv_id": "2406.13093",
      "title": "RITA: A Real-time Interactive Talking Avatars Framework",
      "summary": "RITA presents a high-quality real-time interactive framework built upon generative models, designed with practical applications in mind. Our framework enables the transformation of user-uploaded photos into digital avatars that can engage in real-time dialogue interactions. By leveraging the latest advancements in generative modeling, we have developed a versatile platform that not only enhances the user experience through dynamic conversational avatars but also opens new avenues for applications in virtual reality, online education, and interactive gaming. This work showcases the potential of integrating computer vision and natural language processing technologies to create immersive and interactive digital personas, pushing the boundaries of how we interact with digital content.",
      "authors": [
        "Wuxinlin Cheng",
        "Cheng Wan",
        "Yupeng Cao",
        "Sihan Chen"
      ],
      "published": "2024-06-18T22:53:15Z",
      "updated": "2024-06-18T22:53:15Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13093v1",
      "landing_url": "https://arxiv.org/abs/2406.13093v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.13093"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on real-time talking avatars and cross-modal dialog, but does not center on Spoken Language Models or speech-language modeling with architectural/training/evaluation emphasis, nor clearly addresses full-duplex speech interaction as defined by the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on real-time talking avatars and cross-modal dialog, but does not center on Spoken Language Models or speech-language modeling with architectural/training/evaluation emphasis, nor clearly addresses full-duplex speech interaction as defined by the criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes a system that creates digital avatars capable of real-time dialogue using generative models, but it does not explicitly discuss spoken language models (SLMs) or speech-level language modeling as core components, nor does it mention full-duplex interaction, low latency, or speech-based model architectures and evaluation as required by the inclusion criteria. The focus seems more on generative multimodal avatars and interactive dialogue rather than on speech language modeling or full-duplex spoken interaction with detailed evaluation. Therefore, it does not meet the inclusion criteria related to spoken language modeling and synchronous speech interaction behavior arbitration.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The abstract describes a system that creates digital avatars capable of real-time dialogue using generative models, but it does not explicitly discuss spoken language models (SLMs) or speech-level language modeling as core components, nor does it mention full-duplex interaction, low latency, or speech-based model architectures and evaluation as required by the inclusion criteria. The focus seems more on generative multimodal avatars and interactive dialogue rather than on speech language modeling or full-duplex spoken interaction with detailed evaluation. Therefore, it does not meet the inclusion criteria related to spoken language modeling and synchronous speech interaction behavior arbitration.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Low Latency Visual Inertial Odometry with On-Sensor Accelerated Optical Flow for Resource-Constrained UAVs",
    "abstract": "Visual Inertial Odometry (VIO) is the task of estimating the movement trajectory of an agent from an onboard camera stream fused with additional Inertial Measurement Unit (IMU) measurements. A crucial subtask within VIO is the tracking of features, which can be achieved through Optical Flow (OF). As the calculation of OF is a resource-demanding task in terms of computational load and memory footprint, which needs to be executed at low latency, especially in robotic applications, OF estimation is today performed on powerful CPUs or GPUs. This restricts its use in a broad spectrum of applications where the deployment of such powerful, power-hungry processors is unfeasible due to constraints related to cost, size, and power consumption. On-sensor hardware acceleration is a promising approach to enable low latency VIO even on resource-constrained devices such as nano drones. This paper assesses the speed-up in a VIO sensor system exploiting a compact OF sensor consisting of a global shutter camera and an Application Specific Integrated Circuit (ASIC). By replacing the feature tracking logic of the VINS-Mono pipeline with data from this OF camera, we demonstrate a 49.4% reduction in latency and a 53.7% reduction of compute load of the VIO pipeline over the original VINS-Mono implementation, allowing VINS-Mono operation up to 50 FPS instead of 20 FPS on the quad-core ARM Cortex-A72 processor of a Raspberry Pi Compute Module 4.",
    "metadata": {
      "arxiv_id": "2406.13345",
      "title": "Low Latency Visual Inertial Odometry with On-Sensor Accelerated Optical Flow for Resource-Constrained UAVs",
      "summary": "Visual Inertial Odometry (VIO) is the task of estimating the movement trajectory of an agent from an onboard camera stream fused with additional Inertial Measurement Unit (IMU) measurements. A crucial subtask within VIO is the tracking of features, which can be achieved through Optical Flow (OF). As the calculation of OF is a resource-demanding task in terms of computational load and memory footprint, which needs to be executed at low latency, especially in robotic applications, OF estimation is today performed on powerful CPUs or GPUs. This restricts its use in a broad spectrum of applications where the deployment of such powerful, power-hungry processors is unfeasible due to constraints related to cost, size, and power consumption. On-sensor hardware acceleration is a promising approach to enable low latency VIO even on resource-constrained devices such as nano drones. This paper assesses the speed-up in a VIO sensor system exploiting a compact OF sensor consisting of a global shutter camera and an Application Specific Integrated Circuit (ASIC). By replacing the feature tracking logic of the VINS-Mono pipeline with data from this OF camera, we demonstrate a 49.4% reduction in latency and a 53.7% reduction of compute load of the VIO pipeline over the original VINS-Mono implementation, allowing VINS-Mono operation up to 50 FPS instead of 20 FPS on the quad-core ARM Cortex-A72 processor of a Raspberry Pi Compute Module 4.",
      "authors": [
        "Jonas Kühne",
        "Michele Magno",
        "Luca Benini"
      ],
      "published": "2024-06-19T08:51:19Z",
      "updated": "2024-06-19T08:51:19Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13345v1",
      "landing_url": "https://arxiv.org/abs/2406.13345v1",
      "doi": "https://doi.org/10.1109/JSEN.2024.3406948"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on visual-inertial odometry and on-sensor optical-flow acceleration for UAVs, not on Spoken Language Models or any form of speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on visual-inertial odometry and on-sensor optical-flow acceleration for UAVs, not on Spoken Language Models or any form of speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on visual inertial odometry and hardware acceleration in UAVs, with no mention of spoken language models, speech language modeling, or any focus on speech signals, full-duplex interaction, or speech-related model training and evaluation, thus it does not meet the inclusion criteria and falls outside the research scope defined by the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on visual inertial odometry and hardware acceleration in UAVs, with no mention of spoken language models, speech language modeling, or any focus on speech signals, full-duplex interaction, or speech-related model training and evaluation, thus it does not meet the inclusion criteria and falls outside the research scope defined by the criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Children's Speech Recognition through Discrete Token Enhancement",
    "abstract": "Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters.",
    "metadata": {
      "arxiv_id": "2406.13431",
      "title": "Children's Speech Recognition through Discrete Token Enhancement",
      "summary": "Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters.",
      "authors": [
        "Vrunda N. Sukhadia",
        "Shammur Absar Chowdhury"
      ],
      "published": "2024-06-19T10:45:12Z",
      "updated": "2024-06-24T15:31:59Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13431v2",
      "landing_url": "https://arxiv.org/abs/2406.13431v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.13431"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study centers on transforming children’s speech into discrete tokens for ASR and parameter reduction, but it does not position Speech Language Models / Spoken Language Models as the core object nor discuss full-duplex interaction, joint speech-language modeling, or broader SLM evaluation; thus it fails to meet the inclusion criteria which require SLM-centric modeling and evaluation beyond basic ASR tasks.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The study centers on transforming children’s speech into discrete tokens for ASR and parameter reduction, but it does not position Speech Language Models / Spoken Language Models as the core object nor discuss full-duplex interaction, joint speech-language modeling, or broader SLM evaluation; thus it fails to meet the inclusion criteria which require SLM-centric modeling and evaluation beyond basic ASR tasks.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on children's speech recognition using discrete token enhancement mainly targeting ASR performance, without addressing broader spoken language model architectures, full-duplex interaction capabilities, synchronization, or multi-dimensional evaluation criteria specified in the inclusion criteria. It essentially addresses a single downstream task (children's ASR) and does not discuss speech language modeling from a modeling or interactive system perspective, which falls under exclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on children's speech recognition using discrete token enhancement mainly targeting ASR performance, without addressing broader spoken language model architectures, full-duplex interaction capabilities, synchronization, or multi-dimensional evaluation criteria specified in the inclusion criteria. It essentially addresses a single downstream task (children's ASR) and does not discuss speech language modeling from a modeling or interactive system perspective, which falls under exclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DASB - Discrete Audio and Speech Benchmark",
    "abstract": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.",
    "metadata": {
      "arxiv_id": "2406.14294",
      "title": "DASB - Discrete Audio and Speech Benchmark",
      "summary": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.",
      "authors": [
        "Pooneh Mousavi",
        "Luca Della Libera",
        "Jarod Duret",
        "Artem Ploujnikov",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "published": "2024-06-20T13:23:27Z",
      "updated": "2024-06-21T17:07:17Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14294v2",
      "landing_url": "https://arxiv.org/abs/2406.14294v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.14294"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This DASB work benchmarks discrete audio tokens for a range of speech-related tasks within multimodal LLM frameworks, aligning with the goal of evaluating speech-language modeling approaches over the past 3 years, though it does not explicitly use the term 'spoken language model' in the abstract.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "This DASB work benchmarks discrete audio tokens for a range of speech-related tasks within multimodal LLM frameworks, aligning with the goal of evaluating speech-language modeling approaches over the past 3 years, though it does not explicitly use the term 'spoken language model' in the abstract.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a benchmark for discrete audio tokens addressing multiple speech-related tasks, including recognition and generation, but it does not explicitly mention Spoken Language Models (SLMs), language modeling of speech, or full-duplex spoken language interaction as core concepts. It focuses on evaluating audio tokenizers rather than modeling end-to-end spoken language understanding or interaction with synchronous behaviors, and lacks discussion on cognitive parallelism or low-latency interaction required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study presents a benchmark for discrete audio tokens addressing multiple speech-related tasks, including recognition and generation, but it does not explicitly mention Spoken Language Models (SLMs), language modeling of speech, or full-duplex spoken language interaction as core concepts. It focuses on evaluating audio tokenizers rather than modeling end-to-end spoken language understanding or interaction with synchronous behaviors, and lacks discussion on cognitive parallelism or low-latency interaction required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "The abstract describes a benchmark of discrete audio tokens for downstream discriminative and generative speech tasks and tokenizers for multimodal LLMs, but it does not explicitly position the work as a Spoken Language Model (SLM) or language modeling of speech, nor does it address full-duplex interaction, cognitive parallelism, low-latency synchronous behaviors, or evaluation of dialog/overlap/turn-taking—thus failing the explicit inclusion requirement; junior reviewers were split (one leaning include for relevance to speech-language modeling, the other exclude for focus on tokenizers), but the absence of explicit SLM/language-modeling-of-speech phrasing and lack of synchronous interaction content means this should be excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "The abstract describes a benchmark of discrete audio tokens for downstream discriminative and generative speech tasks and tokenizers for multimodal LLMs, but it does not explicitly position the work as a Spoken Language Model (SLM) or language modeling of speech, nor does it address full-duplex interaction, cognitive parallelism, low-latency synchronous behaviors, or evaluation of dialog/overlap/turn-taking—thus failing the explicit inclusion requirement; junior reviewers were split (one leaning include for relevance to speech-language modeling, the other exclude for focus on tokenizers), but the absence of explicit SLM/language-modeling-of-speech phrasing and lack of synchronous interaction content means this should be excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Model-Based Learning for Network Clock Synchronization in Half-Duplex TDMA Networks",
    "abstract": "Supporting increasingly higher rates in wireless networks requires highly accurate clock synchronization across the nodes. Motivated by this need, in this work we consider distributed clock synchronization for half-duplex (HD) TDMA wireless networks. We focus on pulse-coupling (PC)-based synchronization as it is practically advantageous for high-speed networks using low-power nodes. Previous works on PC-based synchronization for TDMA networks assumed full-duplex communications, and focused on correcting the clock phase at each node, without synchronizing clocks' frequencies. However, as in the HD regime corrections are temporally sparse, uncompensated clock frequency differences between the nodes result in large phase drifts between updates. Moreover, as the clocks determine the processing rates at the nodes, leaving the clocks' frequencies unsynchronized results in processing rates mismatch between the nodes, leading to a throughput reduction. Our goal in this work is to synchronize both clock frequency and clock phase across the clocks in HD TDMA networks, via distributed processing. The key challenges are the coupling between frequency correction and phase correction, and the lack of a computationally efficient analytical framework for determining the optimal correction signal at the nodes. We address these challenges via a DNN-aided nested loop structure in which the DNN are used for generating the weights applied to the loop input for computing the correction signal. This loop is operated in a sequential manner which decouples frequency and phase compensations, thereby facilitating synchronization of both parameters. Performance evaluation shows that the proposed scheme significantly improves synchronization accuracy compared to the conventional approaches.",
    "metadata": {
      "arxiv_id": "2406.15258",
      "title": "Model-Based Learning for Network Clock Synchronization in Half-Duplex TDMA Networks",
      "summary": "Supporting increasingly higher rates in wireless networks requires highly accurate clock synchronization across the nodes. Motivated by this need, in this work we consider distributed clock synchronization for half-duplex (HD) TDMA wireless networks. We focus on pulse-coupling (PC)-based synchronization as it is practically advantageous for high-speed networks using low-power nodes. Previous works on PC-based synchronization for TDMA networks assumed full-duplex communications, and focused on correcting the clock phase at each node, without synchronizing clocks' frequencies. However, as in the HD regime corrections are temporally sparse, uncompensated clock frequency differences between the nodes result in large phase drifts between updates. Moreover, as the clocks determine the processing rates at the nodes, leaving the clocks' frequencies unsynchronized results in processing rates mismatch between the nodes, leading to a throughput reduction. Our goal in this work is to synchronize both clock frequency and clock phase across the clocks in HD TDMA networks, via distributed processing. The key challenges are the coupling between frequency correction and phase correction, and the lack of a computationally efficient analytical framework for determining the optimal correction signal at the nodes. We address these challenges via a DNN-aided nested loop structure in which the DNN are used for generating the weights applied to the loop input for computing the correction signal. This loop is operated in a sequential manner which decouples frequency and phase compensations, thereby facilitating synchronization of both parameters. Performance evaluation shows that the proposed scheme significantly improves synchronization accuracy compared to the conventional approaches.",
      "authors": [
        "Itay Zino",
        "Ron Dabora",
        "H. Vincent Poor"
      ],
      "published": "2024-06-21T15:47:50Z",
      "updated": "2024-06-21T15:47:50Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.15258v1",
      "landing_url": "https://arxiv.org/abs/2406.15258v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.15258"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on DNN-aided clock synchronization for half-duplex TDMA wireless networks, with no focus on Spoken Language Models or speech-language processing, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper centers on DNN-aided clock synchronization for half-duplex TDMA wireless networks, with no focus on Spoken Language Models or speech-language processing, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on clock synchronization in half-duplex TDMA wireless networks and does not address Spoken Language Models (SLMs) or their modeling, training, or evaluation, nor does it discuss speech-based interactions, full-duplex spoken language modeling, or related behavioral arbitration topics central to the inclusion criteria. Therefore, it does not meet the inclusion criteria and falls into an unrelated domain of network synchronization rather than speech language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on clock synchronization in half-duplex TDMA wireless networks and does not address Spoken Language Models (SLMs) or their modeling, training, or evaluation, nor does it discuss speech-based interactions, full-duplex spoken language modeling, or related behavioral arbitration topics central to the inclusion criteria. Therefore, it does not meet the inclusion criteria and falls into an unrelated domain of network synchronization rather than speech language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "TacoLM: GaTed Attention Equipped Codec Language Model are Efficient Zero-Shot Text to Speech Synthesizers",
    "abstract": "Neural codec language model (LM) has demonstrated strong capability in zero-shot text-to-speech (TTS) synthesis. However, the codec LM often suffers from limitations in inference speed and stability, due to its auto-regressive nature and implicit alignment between text and audio. In this work, to handle these challenges, we introduce a new variant of neural codec LM, namely TacoLM. Specifically, TacoLM introduces a gated attention mechanism to improve the training and inference efficiency and reduce the model size. Meanwhile, an additional gated cross-attention layer is included for each decoder layer, which improves the efficiency and content accuracy of the synthesized speech. In the evaluation of the Librispeech corpus, the proposed TacoLM achieves a better word error rate, speaker similarity, and mean opinion score, with 90% fewer parameters and 5.2 times speed up, compared with VALL-E. Demo and code is available at https://ereboas.github.io/TacoLM/.",
    "metadata": {
      "arxiv_id": "2406.15752",
      "title": "TacoLM: GaTed Attention Equipped Codec Language Model are Efficient Zero-Shot Text to Speech Synthesizers",
      "summary": "Neural codec language model (LM) has demonstrated strong capability in zero-shot text-to-speech (TTS) synthesis. However, the codec LM often suffers from limitations in inference speed and stability, due to its auto-regressive nature and implicit alignment between text and audio. In this work, to handle these challenges, we introduce a new variant of neural codec LM, namely TacoLM. Specifically, TacoLM introduces a gated attention mechanism to improve the training and inference efficiency and reduce the model size. Meanwhile, an additional gated cross-attention layer is included for each decoder layer, which improves the efficiency and content accuracy of the synthesized speech. In the evaluation of the Librispeech corpus, the proposed TacoLM achieves a better word error rate, speaker similarity, and mean opinion score, with 90% fewer parameters and 5.2 times speed up, compared with VALL-E. Demo and code is available at https://ereboas.github.io/TacoLM/.",
      "authors": [
        "Yakun Song",
        "Zhuo Chen",
        "Xiaofei Wang",
        "Ziyang Ma",
        "Guanrou Yang",
        "Xie Chen"
      ],
      "published": "2024-06-22T06:39:52Z",
      "updated": "2024-06-22T06:39:52Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.15752v1",
      "landing_url": "https://arxiv.org/abs/2406.15752v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.15752"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "TacoLM uses a neural codec language model for zero-shot TTS, treating speech as the primary modeling target with explicit architectural changes and evaluation on speech data, and it falls within the past 3 years, aligning with the spoken-language modeling/core FD-SLM perspective without focusing solely on a single input-output voice task.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "TacoLM uses a neural codec language model for zero-shot TTS, treating speech as the primary modeling target with explicit architectural changes and evaluation on speech data, and it falls within the past 3 years, aligning with the spoken-language modeling/core FD-SLM perspective without focusing solely on a single input-output voice task.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes TacoLM, a neural codec language model designed for zero-shot text-to-speech synthesis, focusing on speech synthesis efficiency and quality improvements. However, it centers on TTS as a single downstream application without addressing spoken language modeling as a core focus, synchronous interaction behaviors, dialog arbitration, or full-duplex capabilities. The evaluation metrics reported (WER, speaker similarity, MOS) do not include real-time interaction, overlap handling, or dialog management assessments. Although it involves a speech-oriented model, it treats speech primarily as output rather than the core modeling and inference subject with synchronous interaction capabilities required by inclusion criteria. Therefore, it fails to meet the comprehensive criteria for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes TacoLM, a neural codec language model designed for zero-shot text-to-speech synthesis, focusing on speech synthesis efficiency and quality improvements. However, it centers on TTS as a single downstream application without addressing spoken language modeling as a core focus, synchronous interaction behaviors, dialog arbitration, or full-duplex capabilities. The evaluation metrics reported (WER, speaker similarity, MOS) do not include real-time interaction, overlap handling, or dialog management assessments. Although it involves a speech-oriented model, it treats speech primarily as output rather than the core modeling and inference subject with synchronous interaction capabilities required by inclusion criteria. Therefore, it fails to meet the comprehensive criteria for inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "The paper describes TacoLM, a neural codec language model applied to zero-shot TTS and reports improvements in efficiency and audio quality (WER, speaker similarity, MOS), but it is clearly focused on a single downstream task (TTS) and engineering/model-efficiency gains rather than on Spoken Language Models as the core research theme (e.g., full‑duplex interaction, synchronous overlap handling, low‑latency real‑time interaction, or dialog arbitration). Junior reviewer A argued inclusion noting it is a codec LM over speech tokens, while Junior reviewer B correctly noted it centers on TTS without SLM/FD‑SLM interaction concerns; I agree with the latter—this meets the exclusion criteria of being a single-task TTS study and lacks the required evaluation/claims about spoken‑language modeling or synchronous interaction—so it should be excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "The paper describes TacoLM, a neural codec language model applied to zero-shot TTS and reports improvements in efficiency and audio quality (WER, speaker similarity, MOS), but it is clearly focused on a single downstream task (TTS) and engineering/model-efficiency gains rather than on Spoken Language Models as the core research theme (e.g., full‑duplex interaction, synchronous overlap handling, low‑latency real‑time interaction, or dialog arbitration). Junior reviewer A argued inclusion noting it is a codec LM over speech tokens, while Junior reviewer B correctly noted it centers on TTS without SLM/FD‑SLM interaction concerns; I agree with the latter—this meets the exclusion criteria of being a single-task TTS study and lacks the required evaluation/claims about spoken‑language modeling or synchronous interaction—so it should be excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model",
    "abstract": "We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity.",
    "metadata": {
      "arxiv_id": "2406.17310",
      "title": "High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model",
      "summary": "We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity.",
      "authors": [
        "Joun Yeop Lee",
        "Myeonghun Jeong",
        "Minchan Kim",
        "Ji-Hyun Lee",
        "Hoon-Young Cho",
        "Nam Soo Kim"
      ],
      "published": "2024-06-25T06:46:47Z",
      "updated": "2024-06-25T06:46:47Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17310v1",
      "landing_url": "https://arxiv.org/abs/2406.17310v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.17310"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Treats speech as the central modeling target and uses a language-modeling component (G-MLM) for speech tokens within a TTS framework, within the last 3 years, satisfying the spoken-language-modeling orientation, though focused on TTS rather than interactive dialogue.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "Treats speech as the central modeling target and uses a language-modeling component (G-MLM) for speech tokens within a TTS framework, within the last 3 years, satisfying the spoken-language-modeling orientation, though focused on TTS rather than interactive dialogue.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a novel text-to-speech (TTS) framework focusing on high-fidelity speech synthesis using discrete semantic and acoustic tokens. However, it primarily centers on TTS architecture and quality improvement without explicitly addressing Spoken Language Models (SLMs), full-duplex interaction, synchronous dialogue behavior arbitration, or language modeling of speech sequences as required. The study also does not mention synchronization, cognitive parallelism, or real-time interaction evaluation, thus not fully aligning with the inclusion criteria and touching on some exclusion criteria such as focusing only on single speech technology (TTS) without dialogue or interaction aspects.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes a novel text-to-speech (TTS) framework focusing on high-fidelity speech synthesis using discrete semantic and acoustic tokens. However, it primarily centers on TTS architecture and quality improvement without explicitly addressing Spoken Language Models (SLMs), full-duplex interaction, synchronous dialogue behavior arbitration, or language modeling of speech sequences as required. The study also does not mention synchronization, cognitive parallelism, or real-time interaction evaluation, thus not fully aligning with the inclusion criteria and touching on some exclusion criteria such as focusing only on single speech technology (TTS) without dialogue or interaction aspects.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers’ notes were considered: JuniorNano leaned toward inclusion by interpreting the use of discrete speech tokens and a masked language model as speech-level language modeling, but JuniorMini correctly highlighted that the paper focuses on a TTS architecture and quality improvements without addressing Spoken Language Models (SLMs), full-duplex interaction, synchronization, or explicit language modeling of speech sequences. The title/abstract do not mention “spoken language model(s)” or equivalent, do not target end-to-end speech-centric modeling for interactive/full‑duplex dialogue, and instead describe a two-stage TTS system (semantic→acoustic tokens) and zero-shot voice synthesis—therefore it fails the inclusion criteria and meets exclusion criteria (single-task TTS focus). 1-sentence explanation: Exclude (score 2) — the work is a TTS architecture paper using discrete tokens and MLM components but does not present spoken-language-model research, full‑duplex interaction, or speech-centric language modeling required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers’ notes were considered: JuniorNano leaned toward inclusion by interpreting the use of discrete speech tokens and a masked language model as speech-level language modeling, but JuniorMini correctly highlighted that the paper focuses on a TTS architecture and quality improvements without addressing Spoken Language Models (SLMs), full-duplex interaction, synchronization, or explicit language modeling of speech sequences. The title/abstract do not mention “spoken language model(s)” or equivalent, do not target end-to-end speech-centric modeling for interactive/full‑duplex dialogue, and instead describe a two-stage TTS system (semantic→acoustic tokens) and zero-shot voice synthesis—therefore it fails the inclusion criteria and meets exclusion criteria (single-task TTS focus). 1-sentence explanation: Exclude (score 2) — the work is a TTS architecture paper using discrete tokens and MLM components but does not present spoken-language-model research, full‑duplex interaction, or speech-centric language modeling required by the inclusion criteria.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Temporal Dynamics beyond the Exceptional Point in the Ikeda Map with Balanced Gain and Loss",
    "abstract": "We investigate the temporal dynamics of the Ikeda Map with Balanced Gain and Loss and in the presence of feedback loops with saturation nonlinearity. From the bifurcation analysis, we find that the temporal evolution of optical power undergoes period quadrupling at the exceptional point (EP) of the system and beyond that, chaotic dynamics emerge in the system and this has been further corroborated from the Largest Lyapunov Exponent (LLE) of the model. For a closer inspection, we analyzed the parameter basin of the system, which further leads to our inference that the Ikeda Map with Balanced Gain and Loss exhibits the emergence of chaotic dynamics beyond the exceptional point (EP). Furthermore, we find that the temporal dynamics beyond the EP regime leads to the onset of Extreme Events (EE) in this system via attractor merging crisis.",
    "metadata": {
      "arxiv_id": "2406.17783",
      "title": "Temporal Dynamics beyond the Exceptional Point in the Ikeda Map with Balanced Gain and Loss",
      "summary": "We investigate the temporal dynamics of the Ikeda Map with Balanced Gain and Loss and in the presence of feedback loops with saturation nonlinearity. From the bifurcation analysis, we find that the temporal evolution of optical power undergoes period quadrupling at the exceptional point (EP) of the system and beyond that, chaotic dynamics emerge in the system and this has been further corroborated from the Largest Lyapunov Exponent (LLE) of the model. For a closer inspection, we analyzed the parameter basin of the system, which further leads to our inference that the Ikeda Map with Balanced Gain and Loss exhibits the emergence of chaotic dynamics beyond the exceptional point (EP). Furthermore, we find that the temporal dynamics beyond the EP regime leads to the onset of Extreme Events (EE) in this system via attractor merging crisis.",
      "authors": [
        "Jyoti Prasad Deka",
        "Amarendra K. Sarma"
      ],
      "published": "2024-05-13T04:40:39Z",
      "updated": "2024-05-13T04:40:39Z",
      "categories": [
        "eess.SP",
        "nlin.CD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17783v1",
      "landing_url": "https://arxiv.org/abs/2406.17783v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.17783"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item does not address Spoken Language Models or any speech-language modeling concepts; it studies nonlinear optical dynamics (Ikeda map) and exceptional points, with no discussion of turn-taking, real-time interaction, or speech modeling—therefore should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item does not address Spoken Language Models or any speech-language modeling concepts; it studies nonlinear optical dynamics (Ikeda map) and exceptional points, with no discussion of turn-taking, real-time interaction, or speech modeling—therefore should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on the temporal dynamics of the Ikeda Map in a physical system involving balanced gain and loss and nonlinear feedback, with no mention of spoken language models, speech language modeling, or any speech-related modeling concepts. There is no indication that this research relates to speech, language modeling, or interactive spoken dialogue systems as defined in the inclusion criteria. Therefore, it does not meet the inclusion criteria and does not address any synchronous interaction or voice modeling aspects required, thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on the temporal dynamics of the Ikeda Map in a physical system involving balanced gain and loss and nonlinear feedback, with no mention of spoken language models, speech language modeling, or any speech-related modeling concepts. There is no indication that this research relates to speech, language modeling, or interactive spoken dialogue systems as defined in the inclusion criteria. Therefore, it does not meet the inclusion criteria and does not address any synchronous interaction or voice modeling aspects required, thus it should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "FoldToken2: Learning compact, invariant and generative protein structure language",
    "abstract": "The equivalent nature of 3D coordinates has posed long term challenges in protein structure representation learning, alignment, and generation. Can we create a compact and invariant language that equivalently represents protein structures? Towards this goal, we propose FoldToken2 to transfer equivariant structures into discrete tokens, while maintaining the recoverability of the original structures. From FoldToken1 to FoldToken2, we improve three key components: (1) invariant structure encoder, (2) vector-quantized compressor, and (3) equivalent structure decoder. We evaluate FoldToken2 on the protein structure reconstruction task and show that it outperforms previous FoldToken1 by 20\\% in TMScore and 81\\% in RMSD. FoldToken2 probably be the first method that works well on both single-chain and multi-chain protein structures quantization. We believe that FoldToken2 will inspire further improvement in protein structure representation learning, structure alignment, and structure generation tasks.",
    "metadata": {
      "arxiv_id": "2407.00050",
      "title": "FoldToken2: Learning compact, invariant and generative protein structure language",
      "summary": "The equivalent nature of 3D coordinates has posed long term challenges in protein structure representation learning, alignment, and generation. Can we create a compact and invariant language that equivalently represents protein structures? Towards this goal, we propose FoldToken2 to transfer equivariant structures into discrete tokens, while maintaining the recoverability of the original structures. From FoldToken1 to FoldToken2, we improve three key components: (1) invariant structure encoder, (2) vector-quantized compressor, and (3) equivalent structure decoder. We evaluate FoldToken2 on the protein structure reconstruction task and show that it outperforms previous FoldToken1 by 20\\% in TMScore and 81\\% in RMSD. FoldToken2 probably be the first method that works well on both single-chain and multi-chain protein structures quantization. We believe that FoldToken2 will inspire further improvement in protein structure representation learning, structure alignment, and structure generation tasks.",
      "authors": [
        "Zhangyang Gao",
        "Cheng Tan",
        "Stan Z. Li"
      ],
      "published": "2024-06-11T09:24:51Z",
      "updated": "2024-06-11T09:24:51Z",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.00050v1",
      "landing_url": "https://arxiv.org/abs/2407.00050v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.00050"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Not about Spoken Language Models; focuses on protein structure language representations (FoldToken2) rather than speech-language modeling, thus does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Not about Spoken Language Models; focuses on protein structure language representations (FoldToken2) rather than speech-language modeling, thus does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on protein structure representation learning and does not address spoken language models, speech language modeling, or any speech-related modeling or interaction tasks defined in the inclusion criteria; thus it fails to meet the core topic requirements and is unrelated to speech-based language modeling or full-duplex spoken interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on protein structure representation learning and does not address spoken language models, speech language modeling, or any speech-related modeling or interaction tasks defined in the inclusion criteria; thus it fails to meet the core topic requirements and is unrelated to speech-based language modeling or full-duplex spoken interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Collocation-based Method for Addressing Challenges in Word-level Metric Differential Privacy",
    "abstract": "Applications of Differential Privacy (DP) in NLP must distinguish between the syntactic level on which a proposed mechanism operates, often taking the form of $\\textit{word-level}$ or $\\textit{document-level}$ privatization. Recently, several word-level $\\textit{Metric}$ Differential Privacy approaches have been proposed, which rely on this generalized DP notion for operating in word embedding spaces. These approaches, however, often fail to produce semantically coherent textual outputs, and their application at the sentence- or document-level is only possible by a basic composition of word perturbations. In this work, we strive to address these challenges by operating $\\textit{between}$ the word and sentence levels, namely with $\\textit{collocations}$. By perturbing n-grams rather than single words, we devise a method where composed privatized outputs have higher semantic coherence and variable length. This is accomplished by constructing an embedding model based on frequently occurring word groups, in which unigram words co-exist with bi- and trigram collocations. We evaluate our method in utility and privacy tests, which make a clear case for tokenization strategies beyond the word level.",
    "metadata": {
      "arxiv_id": "2407.00638",
      "title": "A Collocation-based Method for Addressing Challenges in Word-level Metric Differential Privacy",
      "summary": "Applications of Differential Privacy (DP) in NLP must distinguish between the syntactic level on which a proposed mechanism operates, often taking the form of $\\textit{word-level}$ or $\\textit{document-level}$ privatization. Recently, several word-level $\\textit{Metric}$ Differential Privacy approaches have been proposed, which rely on this generalized DP notion for operating in word embedding spaces. These approaches, however, often fail to produce semantically coherent textual outputs, and their application at the sentence- or document-level is only possible by a basic composition of word perturbations. In this work, we strive to address these challenges by operating $\\textit{between}$ the word and sentence levels, namely with $\\textit{collocations}$. By perturbing n-grams rather than single words, we devise a method where composed privatized outputs have higher semantic coherence and variable length. This is accomplished by constructing an embedding model based on frequently occurring word groups, in which unigram words co-exist with bi- and trigram collocations. We evaluate our method in utility and privacy tests, which make a clear case for tokenization strategies beyond the word level.",
      "authors": [
        "Stephen Meisenbacher",
        "Maulik Chevli",
        "Florian Matthes"
      ],
      "published": "2024-06-30T09:37:34Z",
      "updated": "2024-06-30T09:37:34Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.00638v1",
      "landing_url": "https://arxiv.org/abs/2407.00638v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.00638"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此論文聚焦於在 NLP 的 word-level metric differential privacy，著重於語義連貫性與 Collocation 的私密性處理，未以 Spoken Language Models 或語音作為核心建模與推理對象，也未討論語音層面的評估或對話互動，因此不符合包含條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此論文聚焦於在 NLP 的 word-level metric differential privacy，著重於語義連貫性與 Collocation 的私密性處理，未以 Spoken Language Models 或語音作為核心建模與推理對象，也未討論語音層面的評估或對話互動，因此不符合包含條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a differential privacy method for word-level metric privacy in NLP, specifically operating on collocations in text embedding spaces. It does not address Spoken Language Models or speech-level modeling, and does not discuss full-duplex spoken language models, speech language modeling, or real-time speech interaction features required by the inclusion criteria. Therefore, it does not meet the inclusion criteria which require speech as the core modeling object and the study of spoken language models or equivalent architectures.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a differential privacy method for word-level metric privacy in NLP, specifically operating on collocations in text embedding spaces. It does not address Spoken Language Models or speech-level modeling, and does not discuss full-duplex spoken language models, speech language modeling, or real-time speech interaction features required by the inclusion criteria. Therefore, it does not meet the inclusion criteria which require speech as the core modeling object and the study of spoken language models or equivalent architectures.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Investigating the Effects of Large-Scale Pseudo-Stereo Data and Different Speech Foundation Model on Dialogue Generative Spoken Language Model",
    "abstract": "Recent efforts in Spoken Dialogue Modeling aim to synthesize spoken dialogue without the need for direct transcription, thereby preserving the wealth of non-textual information inherent in speech. However, this approach faces a challenge when speakers talk simultaneously, requiring stereo dialogue data with speakers recorded on separate channels, a notably scarce resource. To address this, we have developed an innovative pipeline capable of transforming single-channel dialogue data into pseudo-stereo data. This expanded our training dataset from a mere 2,000 to an impressive 17,600 hours, significantly enriching the diversity and quality of the training examples available. The inclusion of this pseudo-stereo data has proven to be effective in improving the performance of spoken dialogue language models. Additionally, we explored the use of discrete units of different speech foundation models for spoken dialogue generation.",
    "metadata": {
      "arxiv_id": "2407.01911",
      "title": "Investigating the Effects of Large-Scale Pseudo-Stereo Data and Different Speech Foundation Model on Dialogue Generative Spoken Language Model",
      "summary": "Recent efforts in Spoken Dialogue Modeling aim to synthesize spoken dialogue without the need for direct transcription, thereby preserving the wealth of non-textual information inherent in speech. However, this approach faces a challenge when speakers talk simultaneously, requiring stereo dialogue data with speakers recorded on separate channels, a notably scarce resource. To address this, we have developed an innovative pipeline capable of transforming single-channel dialogue data into pseudo-stereo data. This expanded our training dataset from a mere 2,000 to an impressive 17,600 hours, significantly enriching the diversity and quality of the training examples available. The inclusion of this pseudo-stereo data has proven to be effective in improving the performance of spoken dialogue language models. Additionally, we explored the use of discrete units of different speech foundation models for spoken dialogue generation.",
      "authors": [
        "Yu-Kuan Fu",
        "Cheng-Kuang Lee",
        "Hsiu-Hsuan Wang",
        "Hung-yi Lee"
      ],
      "published": "2024-07-02T03:22:41Z",
      "updated": "2024-07-02T03:22:41Z",
      "categories": [
        "cs.CL",
        "cs.HC",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.01911v1",
      "landing_url": "https://arxiv.org/abs/2407.01911v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.01911"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article explicitly targets Spoken Language Models / dialogue-generative SLMs, discusses architecture/training/evaluation using speech data (pseudo-stereo data augmentation) and speech foundation models, is within the 3-year window, and uses terms like spoken language model(s) / speech language model(s); it goes beyond ASR/TTS-only by focusing on dialogue generation and cross-channel interactivity implied by a dialogue generative SLM.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The article explicitly targets Spoken Language Models / dialogue-generative SLMs, discusses architecture/training/evaluation using speech data (pseudo-stereo data augmentation) and speech foundation models, is within the 3-year window, and uses terms like spoken language model(s) / speech language model(s); it goes beyond ASR/TTS-only by focusing on dialogue generation and cross-channel interactivity implied by a dialogue generative SLM.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on spoken dialogue modeling using pseudo-stereo data to improve spoken dialogue language models, explicitly involving speech as the primary modeling target (not just as text input), discussing model training with expanded and novel data, and exploring different speech foundation models; however, the abstract does not clearly indicate addressing full-duplex capabilities such as cognitive parallelism, overlap management, low latency, or multi-dimensional evaluations beyond performance improvement. Given it involves spoken dialogue generative models and speech data manipulation beyond ASR/TTS, it partially aligns with the inclusion criteria but lacks explicit mention of key full-duplex features and detailed evaluation metrics related to interaction or concurrency.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on spoken dialogue modeling using pseudo-stereo data to improve spoken dialogue language models, explicitly involving speech as the primary modeling target (not just as text input), discussing model training with expanded and novel data, and exploring different speech foundation models; however, the abstract does not clearly indicate addressing full-duplex capabilities such as cognitive parallelism, overlap management, low latency, or multi-dimensional evaluations beyond performance improvement. Given it involves spoken dialogue generative models and speech data manipulation beyond ASR/TTS, it partially aligns with the inclusion criteria but lacks explicit mention of key full-duplex features and detailed evaluation metrics related to interaction or concurrency.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed both junior reviewers' notes and the abstract: the paper explicitly targets Spoken Language Models (title uses \"Dialogue Generative Spoken Language Model\") and treats speech as the primary modeling object (synthesizing spoken dialogue without transcription), and it directly addresses simultaneous-speaker/overlap issues by creating large-scale pseudo-stereo data and evaluating effects on spoken dialogue generation—these strongly align with the SLM/FD-SLM scope; however, the abstract lacks explicit details on evaluation metrics, benchmarks, or other full-duplex aspects (e.g., cognitive parallelism, latency, backchanneling, engineered vs learned synchronization). Integrating the juniors' views, this is borderline but overall relevant to the topic and not excluded. 1-sentence explanation: The study should be included (score 4) because it centers on spoken language modeling and explicitly tackles multi-speaker/overlap via pseudo-stereo augmentation, though it lacks explicit full-duplex evaluation details and therefore is not an unequivocal include.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "I reviewed both junior reviewers' notes and the abstract: the paper explicitly targets Spoken Language Models (title uses \"Dialogue Generative Spoken Language Model\") and treats speech as the primary modeling object (synthesizing spoken dialogue without transcription), and it directly addresses simultaneous-speaker/overlap issues by creating large-scale pseudo-stereo data and evaluating effects on spoken dialogue generation—these strongly align with the SLM/FD-SLM scope; however, the abstract lacks explicit details on evaluation metrics, benchmarks, or other full-duplex aspects (e.g., cognitive parallelism, latency, backchanneling, engineered vs learned synchronization). Integrating the juniors' views, this is borderline but overall relevant to the topic and not excluded. 1-sentence explanation: The study should be included (score 4) because it centers on spoken language modeling and explicitly tackles multi-speaker/overlap via pseudo-stereo augmentation, though it lacks explicit full-duplex evaluation details and therefore is not an unequivocal include.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "QSync: Quantization-Minimized Synchronous Distributed Training Across Hybrid Devices",
    "abstract": "A number of production deep learning clusters have attempted to explore inference hardware for DNN training, at the off-peak serving hours with many inference GPUs idling. Conducting DNN training with a combination of heterogeneous training and inference GPUs, known as hybrid device training, presents considerable challenges due to disparities in compute capability and significant differences in memory capacity. We propose QSync, a training system that enables efficient synchronous data-parallel DNN training over hybrid devices by strategically exploiting quantized operators. According to each device's available resource capacity, QSync selects a quantization-minimized setting for operators in the distributed DNN training graph, minimizing model accuracy degradation but keeping the training efficiency brought by quantization. We carefully design a predictor with a bi-directional mixed-precision indicator to reflect the sensitivity of DNN layers on fixed-point and floating-point low-precision operators, a replayer with a neighborhood-aware cost mapper to accurately estimate the latency of distributed hybrid mixed-precision training, and then an allocator that efficiently synchronizes workers with minimized model accuracy degradation. QSync bridges the computational graph on PyTorch to an optimized backend for quantization kernel performance and flexible support for various GPU architectures. Extensive experiments show that QSync's predictor can accurately simulate distributed mixed-precision training with <5% error, with a consistent 0.27-1.03% accuracy improvement over the from-scratch training tasks compared to uniform precision.",
    "metadata": {
      "arxiv_id": "2407.02327",
      "title": "QSync: Quantization-Minimized Synchronous Distributed Training Across Hybrid Devices",
      "summary": "A number of production deep learning clusters have attempted to explore inference hardware for DNN training, at the off-peak serving hours with many inference GPUs idling. Conducting DNN training with a combination of heterogeneous training and inference GPUs, known as hybrid device training, presents considerable challenges due to disparities in compute capability and significant differences in memory capacity. We propose QSync, a training system that enables efficient synchronous data-parallel DNN training over hybrid devices by strategically exploiting quantized operators. According to each device's available resource capacity, QSync selects a quantization-minimized setting for operators in the distributed DNN training graph, minimizing model accuracy degradation but keeping the training efficiency brought by quantization. We carefully design a predictor with a bi-directional mixed-precision indicator to reflect the sensitivity of DNN layers on fixed-point and floating-point low-precision operators, a replayer with a neighborhood-aware cost mapper to accurately estimate the latency of distributed hybrid mixed-precision training, and then an allocator that efficiently synchronizes workers with minimized model accuracy degradation. QSync bridges the computational graph on PyTorch to an optimized backend for quantization kernel performance and flexible support for various GPU architectures. Extensive experiments show that QSync's predictor can accurately simulate distributed mixed-precision training with <5% error, with a consistent 0.27-1.03% accuracy improvement over the from-scratch training tasks compared to uniform precision.",
      "authors": [
        "Juntao Zhao",
        "Borui Wan",
        "Yanghua Peng",
        "Haibin Lin",
        "Yibo Zhu",
        "Chuan Wu"
      ],
      "published": "2024-07-02T14:56:47Z",
      "updated": "2024-07-02T14:56:47Z",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.02327v1",
      "landing_url": "https://arxiv.org/abs/2407.02327v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.02327"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on quantization-based synchronous distributed training for hybrid devices and makes no reference to Spoken Language Models or speech modeling, thus failing the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on quantization-based synchronous distributed training for hybrid devices and makes no reference to Spoken Language Models or speech modeling, thus failing the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on distributed training systems for deep neural networks using quantization techniques on heterogeneous GPUs, which is unrelated to the research topic of Spoken Language Models or full-duplex spoken language interaction; it neither involves speech as a core modeling object nor discusses related model architectures, training, evaluation, or synchronous spoken interaction behaviors specified in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on distributed training systems for deep neural networks using quantization techniques on heterogeneous GPUs, which is unrelated to the research topic of Spoken Language Models or full-duplex spoken language interaction; it neither involves speech as a core modeling object nor discusses related model architectures, training, evaluation, or synchronous spoken interaction behaviors specified in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Predicting and Understanding Turn-Taking Behavior in Open-Ended Group Activities in Virtual Reality",
    "abstract": "In networked virtual reality (VR), user behaviors, individual differences, and group dynamics can serve as important signals into future speech behaviors, such as who the next speaker will be and the timing of turn-taking behaviors. The ability to predict and understand these behaviors offers opportunities to provide adaptive and personalized assistance, for example helping users with varying sensory abilities navigate complex social scenes and instantiating virtual moderators with natural behaviors. In this work, we predict turn-taking behaviors using features extracted based on social dynamics literature. We discuss results from a large-scale VR classroom dataset consisting of 77 sessions and 1660 minutes of small-group social interactions collected over four weeks. In our evaluation, gradient boosting classifiers achieved the best performance, with accuracies of 0.71--0.78 AUC (area under the ROC curve) across three tasks concerning the \"what\", \"who\", and \"when\" of turn-taking behaviors. In interpreting these models, we found that group size, listener personality, speech-related behavior (e.g., time elapsed since the listener's last speech event), group gaze (e.g., how much the group looks at the speaker), as well as the listener's and previous speaker's head pitch, head y-axis position, and left hand y-axis position more saliently influenced predictions. Results suggested that these features remain reliable indicators in novel social VR settings, as prediction performance is robust over time and with groups and activities not used in the training dataset. We discuss theoretical and practical implications of the work.",
    "metadata": {
      "arxiv_id": "2407.02896",
      "title": "Predicting and Understanding Turn-Taking Behavior in Open-Ended Group Activities in Virtual Reality",
      "summary": "In networked virtual reality (VR), user behaviors, individual differences, and group dynamics can serve as important signals into future speech behaviors, such as who the next speaker will be and the timing of turn-taking behaviors. The ability to predict and understand these behaviors offers opportunities to provide adaptive and personalized assistance, for example helping users with varying sensory abilities navigate complex social scenes and instantiating virtual moderators with natural behaviors. In this work, we predict turn-taking behaviors using features extracted based on social dynamics literature. We discuss results from a large-scale VR classroom dataset consisting of 77 sessions and 1660 minutes of small-group social interactions collected over four weeks. In our evaluation, gradient boosting classifiers achieved the best performance, with accuracies of 0.71--0.78 AUC (area under the ROC curve) across three tasks concerning the \"what\", \"who\", and \"when\" of turn-taking behaviors. In interpreting these models, we found that group size, listener personality, speech-related behavior (e.g., time elapsed since the listener's last speech event), group gaze (e.g., how much the group looks at the speaker), as well as the listener's and previous speaker's head pitch, head y-axis position, and left hand y-axis position more saliently influenced predictions. Results suggested that these features remain reliable indicators in novel social VR settings, as prediction performance is robust over time and with groups and activities not used in the training dataset. We discuss theoretical and practical implications of the work.",
      "authors": [
        "Portia Wang",
        "Eugy Han",
        "Anna C. M. Queiroz",
        "Cyan DeVeaux",
        "Jeremy N. Bailenson"
      ],
      "published": "2024-07-03T08:17:59Z",
      "updated": "2025-04-25T20:57:25Z",
      "categories": [
        "cs.HC",
        "cs.CY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.02896v2",
      "landing_url": "https://arxiv.org/abs/2407.02896v2",
      "doi": "https://doi.org/10.1145/3757498"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on predicting turn-taking in VR using non-linguistic social-dynamics features, with no emphasis on Spoken Language Models (SLMs) or Full-Duplex SLMs, no discussion of speech-language modeling architectures/training/evaluation, and no explicit consideration of sub-200 ms synchronization or speech-language modeling aspects; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The study focuses on predicting turn-taking in VR using non-linguistic social-dynamics features, with no emphasis on Spoken Language Models (SLMs) or Full-Duplex SLMs, no discussion of speech-language modeling architectures/training/evaluation, and no explicit consideration of sub-200 ms synchronization or speech-language modeling aspects; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on predicting and understanding turn-taking behavior in virtual reality using features from social dynamics and includes a thorough evaluation with gradient boosting classifiers. However, it does not explicitly address spoken language models or speech language models as core objects of study nor discuss model architectures, training, or evaluation related to speech modeling; it centers on behavioral prediction rather than speech modeling or end-to-end synchronization. Therefore, it does not meet the inclusion criteria emphasizing core spoken language model research.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on predicting and understanding turn-taking behavior in virtual reality using features from social dynamics and includes a thorough evaluation with gradient boosting classifiers. However, it does not explicitly address spoken language models or speech language models as core objects of study nor discuss model architectures, training, or evaluation related to speech modeling; it centers on behavioral prediction rather than speech modeling or end-to-end synchronization. Therefore, it does not meet the inclusion criteria emphasizing core spoken language model research.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Learning Video Temporal Dynamics with Cross-Modal Attention for Robust Audio-Visual Speech Recognition",
    "abstract": "Audio-visual speech recognition (AVSR) aims to transcribe human speech using both audio and video modalities. In practical environments with noise-corrupted audio, the role of video information becomes crucial. However, prior works have primarily focused on enhancing audio features in AVSR, overlooking the importance of video features. In this study, we strengthen the video features by learning three temporal dynamics in video data: context order, playback direction, and the speed of video frames. Cross-modal attention modules are introduced to enrich video features with audio information so that speech variability can be taken into account when training on the video temporal dynamics. Based on our approach, we achieve the state-of-the-art performance on the LRS2 and LRS3 AVSR benchmarks for the noise-dominant settings. Our approach excels in scenarios especially for babble and speech noise, indicating the ability to distinguish the speech signal that should be recognized from lip movements in the video modality. We support the validity of our methodology by offering the ablation experiments for the temporal dynamics losses and the cross-modal attention architecture design.",
    "metadata": {
      "arxiv_id": "2407.03563",
      "title": "Learning Video Temporal Dynamics with Cross-Modal Attention for Robust Audio-Visual Speech Recognition",
      "summary": "Audio-visual speech recognition (AVSR) aims to transcribe human speech using both audio and video modalities. In practical environments with noise-corrupted audio, the role of video information becomes crucial. However, prior works have primarily focused on enhancing audio features in AVSR, overlooking the importance of video features. In this study, we strengthen the video features by learning three temporal dynamics in video data: context order, playback direction, and the speed of video frames. Cross-modal attention modules are introduced to enrich video features with audio information so that speech variability can be taken into account when training on the video temporal dynamics. Based on our approach, we achieve the state-of-the-art performance on the LRS2 and LRS3 AVSR benchmarks for the noise-dominant settings. Our approach excels in scenarios especially for babble and speech noise, indicating the ability to distinguish the speech signal that should be recognized from lip movements in the video modality. We support the validity of our methodology by offering the ablation experiments for the temporal dynamics losses and the cross-modal attention architecture design.",
      "authors": [
        "Sungnyun Kim",
        "Kangwook Jang",
        "Sangmin Bae",
        "Hoirin Kim",
        "Se-Young Yun"
      ],
      "published": "2024-07-04T01:25:20Z",
      "updated": "2024-10-14T07:22:29Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.03563v3",
      "landing_url": "https://arxiv.org/abs/2407.03563v3",
      "doi": "https://doi.org/10.48550/arXiv.2407.03563"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on audio-visual speech recognition with cross-modal attention and video temporal dynamics, but it does not center on Spoken Language Models or an equivalent speech-language modeling paradigm in title/abstract, thus it fails the key inclusion criterion requiring SLM/FD-SLM framing.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on audio-visual speech recognition with cross-modal attention and video temporal dynamics, but it does not center on Spoken Language Models or an equivalent speech-language modeling paradigm in title/abstract, thus it fails the key inclusion criterion requiring SLM/FD-SLM framing.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on audio-visual speech recognition (AVSR), particularly enhancing video features for speech variability in noisy environments, but it does not discuss Spoken Language Models (SLMs), full-duplex spoken language modeling, or synchronization behaviors such as cognitive parallelism or real-time interaction as specified in the inclusion criteria. The research primarily addresses improving ASR performance rather than spoken language modeling architecture or evaluation involving interactive or full-duplex dialogue, thus it does not meet the core inclusion criteria and falls under exclusion for focusing on a specific ASR task without spoken language modeling perspectives.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on audio-visual speech recognition (AVSR), particularly enhancing video features for speech variability in noisy environments, but it does not discuss Spoken Language Models (SLMs), full-duplex spoken language modeling, or synchronization behaviors such as cognitive parallelism or real-time interaction as specified in the inclusion criteria. The research primarily addresses improving ASR performance rather than spoken language modeling architecture or evaluation involving interactive or full-duplex dialogue, thus it does not meet the core inclusion criteria and falls under exclusion for focusing on a specific ASR task without spoken language modeling perspectives.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Low-latency machine learning FPGA accelerator for multi-qubit-state discrimination",
    "abstract": "Measuring a qubit state is a fundamental yet error-prone operation in quantum computing. These errors can arise from various sources, such as crosstalk, spontaneous state transitions, and excitations caused by the readout pulse. Here, we utilize an integrated approach to deploy neural networks onto field-programmable gate arrays (FPGA). We demonstrate that implementing a fully connected neural network accelerator for multi-qubit readout is advantageous, balancing computational complexity with low latency requirements without significant loss in accuracy. The neural network is implemented by quantizing weights, activation functions, and inputs. The hardware accelerator performs frequency-multiplexed readout of five superconducting qubits in less than 50 ns on a radio frequency system on chip (RFSoC) ZCU111 FPGA, marking the advent of RFSoC-based low-latency multi-qubit readout using neural networks. These modules can be implemented and integrated into existing quantum control and readout platforms, making the RFSoC ZCU111 ready for experimental deployment.",
    "metadata": {
      "arxiv_id": "2407.03852",
      "title": "Low-latency machine learning FPGA accelerator for multi-qubit-state discrimination",
      "summary": "Measuring a qubit state is a fundamental yet error-prone operation in quantum computing. These errors can arise from various sources, such as crosstalk, spontaneous state transitions, and excitations caused by the readout pulse. Here, we utilize an integrated approach to deploy neural networks onto field-programmable gate arrays (FPGA). We demonstrate that implementing a fully connected neural network accelerator for multi-qubit readout is advantageous, balancing computational complexity with low latency requirements without significant loss in accuracy. The neural network is implemented by quantizing weights, activation functions, and inputs. The hardware accelerator performs frequency-multiplexed readout of five superconducting qubits in less than 50 ns on a radio frequency system on chip (RFSoC) ZCU111 FPGA, marking the advent of RFSoC-based low-latency multi-qubit readout using neural networks. These modules can be implemented and integrated into existing quantum control and readout platforms, making the RFSoC ZCU111 ready for experimental deployment.",
      "authors": [
        "Pradeep Kumar Gautam",
        "Shantharam Kalipatnapu",
        "Shankaranarayanan H",
        "Ujjawal Singhal",
        "Benjamin Lienhard",
        "Vibhor Singh",
        "Chetan Singh Thakur"
      ],
      "published": "2024-07-04T11:34:43Z",
      "updated": "2024-08-14T18:00:57Z",
      "categories": [
        "quant-ph",
        "cs.AR",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.03852v2",
      "landing_url": "https://arxiv.org/abs/2407.03852v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.03852"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article concerns quantum qubit readout and FPGA-based neural network acceleration, not Spoken Language Models or speech-based modeling, thus it fails the inclusion criteria for SLM/FD-SLM topics and is not relevant to the exclusion scope.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article concerns quantum qubit readout and FPGA-based neural network acceleration, not Spoken Language Models or speech-based modeling, thus it fails the inclusion criteria for SLM/FD-SLM topics and is not relevant to the exclusion scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on a low-latency FPGA accelerator for multi-qubit-state discrimination in quantum computing, which is unrelated to spoken language models, speech language modeling, or full-duplex spoken language interaction, and does not mention any relevant concepts such as spoken language modeling, cognitive parallelism, or speech-based interaction architectures; therefore, it does not meet any inclusion criteria and falls outside the research scope defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on a low-latency FPGA accelerator for multi-qubit-state discrimination in quantum computing, which is unrelated to spoken language models, speech language modeling, or full-duplex spoken language interaction, and does not mention any relevant concepts such as spoken language modeling, cognitive parallelism, or speech-based interaction architectures; therefore, it does not meet any inclusion criteria and falls outside the research scope defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "On the Effectiveness of Acoustic BPE in Decoder-Only TTS",
    "abstract": "Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS.",
    "metadata": {
      "arxiv_id": "2407.03892",
      "title": "On the Effectiveness of Acoustic BPE in Decoder-Only TTS",
      "summary": "Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS.",
      "authors": [
        "Bohan Li",
        "Feiyu Shen",
        "Yiwei Guo",
        "Shuai Wang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-07-04T12:35:32Z",
      "updated": "2024-07-04T12:35:32Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.03892v1",
      "landing_url": "https://arxiv.org/abs/2407.03892v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.03892"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the abstract references Spoken Language Models (SLM) and is recent, the core focus is decoder-only TTS with acoustic BPE rather than a study centered on SLM/FD-SLM under the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the abstract references Spoken Language Models (SLM) and is recent, the core focus is decoder-only TTS with acoustic BPE rather than a study centered on SLM/FD-SLM under the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating acoustic BPE's effectiveness in decoder-only TTS models, which is a single speech technology (TTS) rather than a broader spoken language model or full-duplex spoken language interaction system. It does not mention spoken language modeling, full-duplex interaction, cognitive parallelism, engineered or learned synchronization, or real-time interaction capabilities, and it only reports on speech synthesis quality and diversity rather than interactive behavior or arbitration metrics. Therefore, it does not meet the inclusion criteria that require explicit emphasis on spoken language modeling or full-duplex interactive capabilities and appears to fall under the exclusion criteria for single-task speech technology without dialogue interaction or synchronization aspects.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating acoustic BPE's effectiveness in decoder-only TTS models, which is a single speech technology (TTS) rather than a broader spoken language model or full-duplex spoken language interaction system. It does not mention spoken language modeling, full-duplex interaction, cognitive parallelism, engineered or learned synchronization, or real-time interaction capabilities, and it only reports on speech synthesis quality and diversity rather than interactive behavior or arbitration metrics. Therefore, it does not meet the inclusion criteria that require explicit emphasis on spoken language modeling or full-duplex interactive capabilities and appears to fall under the exclusion criteria for single-task speech technology without dialogue interaction or synchronization aspects.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Robust and Explainable Framework to Address Data Scarcity in Diagnostic Imaging",
    "abstract": "Deep learning has significantly advanced automatic medical diagnostics and released the occupation of human resources to reduce clinical pressure, yet the persistent challenge of data scarcity in this area hampers its further improvements and applications. To address this gap, we introduce a novel ensemble framework called `Efficient Transfer and Self-supervised Learning based Ensemble Framework' (ETSEF). ETSEF leverages features from multiple pre-trained deep learning models to efficiently learn powerful representations from a limited number of data samples. To the best of our knowledge, ETSEF is the first strategy that combines two pre-training methodologies (Transfer Learning and Self-supervised Learning) with ensemble learning approaches. Various data enhancement techniques, including data augmentation, feature fusion, feature selection, and decision fusion, have also been deployed to maximise the efficiency and robustness of the ETSEF model. Five independent medical imaging tasks, including endoscopy, breast cancer, monkeypox, brain tumour, and glaucoma detection, were tested to demonstrate ETSEF's effectiveness and robustness. Facing limited sample numbers and challenging medical tasks, ETSEF has proved its effectiveness by improving diagnostics accuracies from 10\\% to 13.3\\% when compared to strong ensemble baseline models and up to 14.4\\% improvements compared with published state-of-the-art methods. Moreover, we emphasise the robustness and trustworthiness of the ETSEF method through various vision-explainable artificial intelligence techniques, including Grad-CAM, SHAP, and t-SNE. Compared to those large-scale deep learning models, ETSEF can be deployed flexibly and maintain superior performance for challenging medical imaging tasks, showing the potential to be applied to more areas that lack training data",
    "metadata": {
      "arxiv_id": "2407.06566",
      "title": "Robust and Explainable Framework to Address Data Scarcity in Diagnostic Imaging",
      "summary": "Deep learning has significantly advanced automatic medical diagnostics and released the occupation of human resources to reduce clinical pressure, yet the persistent challenge of data scarcity in this area hampers its further improvements and applications. To address this gap, we introduce a novel ensemble framework called `Efficient Transfer and Self-supervised Learning based Ensemble Framework' (ETSEF). ETSEF leverages features from multiple pre-trained deep learning models to efficiently learn powerful representations from a limited number of data samples. To the best of our knowledge, ETSEF is the first strategy that combines two pre-training methodologies (Transfer Learning and Self-supervised Learning) with ensemble learning approaches. Various data enhancement techniques, including data augmentation, feature fusion, feature selection, and decision fusion, have also been deployed to maximise the efficiency and robustness of the ETSEF model. Five independent medical imaging tasks, including endoscopy, breast cancer, monkeypox, brain tumour, and glaucoma detection, were tested to demonstrate ETSEF's effectiveness and robustness. Facing limited sample numbers and challenging medical tasks, ETSEF has proved its effectiveness by improving diagnostics accuracies from 10\\% to 13.3\\% when compared to strong ensemble baseline models and up to 14.4\\% improvements compared with published state-of-the-art methods. Moreover, we emphasise the robustness and trustworthiness of the ETSEF method through various vision-explainable artificial intelligence techniques, including Grad-CAM, SHAP, and t-SNE. Compared to those large-scale deep learning models, ETSEF can be deployed flexibly and maintain superior performance for challenging medical imaging tasks, showing the potential to be applied to more areas that lack training data",
      "authors": [
        "Zehui Zhao",
        "Laith Alzubaidi",
        "Jinglan Zhang",
        "Ye Duan",
        "Usman Naseem",
        "Yuantong Gu"
      ],
      "published": "2024-07-09T05:48:45Z",
      "updated": "2024-07-09T05:48:45Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.06566v1",
      "landing_url": "https://arxiv.org/abs/2407.06566v1",
      "doi": "https://doi.org/10.1016/j.compbiomed.2025.111052"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item targets diagnostic imaging data scarcity with image-focused transfer and self-supervised/enhanced ensembles, not Spoken Language Models or speech-based modeling as required by the inclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item targets diagnostic imaging data scarcity with image-focused transfer and self-supervised/enhanced ensembles, not Spoken Language Models or speech-based modeling as required by the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a deep learning framework for medical diagnostic imaging with an emphasis on handling data scarcity, and it does not discuss spoken language models, speech-language modeling, or related interactive speech communication concepts outlined in the inclusion criteria. The study neither centers on speech as the main modeling object nor addresses full-duplex spoken language models or synchronization in speech sessions, which are key themes for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a deep learning framework for medical diagnostic imaging with an emphasis on handling data scarcity, and it does not discuss spoken language models, speech-language modeling, or related interactive speech communication concepts outlined in the inclusion criteria. The study neither centers on speech as the main modeling object nor addresses full-duplex spoken language models or synchronization in speech sessions, which are key themes for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "CoopScenes: Multi-Scene Infrastructure and Vehicle Data for Advancing Collective Perception in Autonomous Driving",
    "abstract": "The increasing complexity of urban environments has underscored the potential of effective collective perception systems. To address these challenges, we present the CoopScenes dataset, a large-scale, multi-scene dataset that provides synchronized sensor data from both the ego-vehicle and the supporting infrastructure.The dataset provides 104 minutes of spatially and temporally synchronized data at 10 Hz, resulting in 62,000 frames. It achieves competitive synchronization with a mean deviation of only 2.3 ms. Additionally the dataset includes a novel procedure for precise registration of point cloud data from the ego-vehicle and infrastructure sensors, automated annotation pipelines, and an open-source anonymization pipeline for faces and license plates. Covering nine diverse scenes with 100 maneuvers, the dataset features scenarios such as public transport hubs, city construction sites, and high-speed rural roads across three cities in the Stuttgart region, Germany. The full dataset amounts to 527 GB of data and is provided in the .4mse format, making it easily accessible through our comprehensive development kit. By providing precise, large-scale data, CoopScenes facilitates research in collective perception, real-time sensor registration, and cooperative intelligent systems for urban mobility, including machine learning-based approaches.",
    "metadata": {
      "arxiv_id": "2407.08261",
      "title": "CoopScenes: Multi-Scene Infrastructure and Vehicle Data for Advancing Collective Perception in Autonomous Driving",
      "summary": "The increasing complexity of urban environments has underscored the potential of effective collective perception systems. To address these challenges, we present the CoopScenes dataset, a large-scale, multi-scene dataset that provides synchronized sensor data from both the ego-vehicle and the supporting infrastructure.The dataset provides 104 minutes of spatially and temporally synchronized data at 10 Hz, resulting in 62,000 frames. It achieves competitive synchronization with a mean deviation of only 2.3 ms. Additionally the dataset includes a novel procedure for precise registration of point cloud data from the ego-vehicle and infrastructure sensors, automated annotation pipelines, and an open-source anonymization pipeline for faces and license plates. Covering nine diverse scenes with 100 maneuvers, the dataset features scenarios such as public transport hubs, city construction sites, and high-speed rural roads across three cities in the Stuttgart region, Germany. The full dataset amounts to 527 GB of data and is provided in the .4mse format, making it easily accessible through our comprehensive development kit. By providing precise, large-scale data, CoopScenes facilitates research in collective perception, real-time sensor registration, and cooperative intelligent systems for urban mobility, including machine learning-based approaches.",
      "authors": [
        "Marcel Vosshans",
        "Alexander Baumann",
        "Matthias Drueppel",
        "Omar Ait-Aider",
        "Youcef Mezouar",
        "Thao Dang",
        "Markus Enzweiler"
      ],
      "published": "2024-07-11T08:00:46Z",
      "updated": "2025-04-30T12:09:45Z",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08261v3",
      "landing_url": "https://arxiv.org/abs/2407.08261v3",
      "doi": "https://doi.org/10.1109/IV64158.2025.11097591"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes a multi-scene dataset for cooperative perception in autonomous driving and does not involve Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes a multi-scene dataset for cooperative perception in autonomous driving and does not involve Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a dataset focused on autonomous driving and collective perception using sensor data from vehicles and infrastructure; there is no mention of spoken language models, speech language modeling, full-duplex spoken language models, or related speech-based interactive modeling concepts. The work does not meet the inclusion criteria focused on speech language modeling and related synchronous interactive speech processing, nor does it address the exclusion criteria related to speech-only systems because it is outside the speech domain entirely.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a dataset focused on autonomous driving and collective perception using sensor data from vehicles and infrastructure; there is no mention of spoken language models, speech language modeling, full-duplex spoken language models, or related speech-based interactive modeling concepts. The work does not meet the inclusion criteria focused on speech language modeling and related synchronous interactive speech processing, nor does it address the exclusion criteria related to speech-only systems because it is outside the speech domain entirely.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Explore the Potential of CLIP for Training-Free Open Vocabulary Semantic Segmentation",
    "abstract": "CLIP, as a vision-language model, has significantly advanced Open-Vocabulary Semantic Segmentation (OVSS) with its zero-shot capabilities. Despite its success, its application to OVSS faces challenges due to its initial image-level alignment training, which affects its performance in tasks requiring detailed local context. Our study delves into the impact of CLIP's [CLS] token on patch feature correlations, revealing a dominance of \"global\" patches that hinders local feature discrimination. To overcome this, we propose CLIPtrase, a novel training-free semantic segmentation strategy that enhances local feature awareness through recalibrated self-correlation among patches. This approach demonstrates notable improvements in segmentation accuracy and the ability to maintain semantic coherence across objects.Experiments show that we are 22.3% ahead of CLIP on average on 9 segmentation benchmarks, outperforming existing state-of-the-art training-free methods.The code are made publicly available at: https://github.com/leaves162/CLIPtrase.",
    "metadata": {
      "arxiv_id": "2407.08268",
      "title": "Explore the Potential of CLIP for Training-Free Open Vocabulary Semantic Segmentation",
      "summary": "CLIP, as a vision-language model, has significantly advanced Open-Vocabulary Semantic Segmentation (OVSS) with its zero-shot capabilities. Despite its success, its application to OVSS faces challenges due to its initial image-level alignment training, which affects its performance in tasks requiring detailed local context. Our study delves into the impact of CLIP's [CLS] token on patch feature correlations, revealing a dominance of \"global\" patches that hinders local feature discrimination. To overcome this, we propose CLIPtrase, a novel training-free semantic segmentation strategy that enhances local feature awareness through recalibrated self-correlation among patches. This approach demonstrates notable improvements in segmentation accuracy and the ability to maintain semantic coherence across objects.Experiments show that we are 22.3% ahead of CLIP on average on 9 segmentation benchmarks, outperforming existing state-of-the-art training-free methods.The code are made publicly available at: https://github.com/leaves162/CLIPtrase.",
      "authors": [
        "Tong Shao",
        "Zhuotao Tian",
        "Hang Zhao",
        "Jingyong Su"
      ],
      "published": "2024-07-11T08:12:16Z",
      "updated": "2024-07-11T08:12:16Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08268v1",
      "landing_url": "https://arxiv.org/abs/2407.08268v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.08268"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目聚焦於 CLIP 等視覺–語言模型在開放字彙語義分割的訓練自由方法，核心為視覺任務與語言-視覺對齊，未以 Spoken Language Models / 語音語言模型為核心，且未涉及語音互動或語音建模，因此不符合本主題的包含條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目聚焦於 CLIP 等視覺–語言模型在開放字彙語義分割的訓練自由方法，核心為視覺任務與語言-視覺對齊，未以 Spoken Language Models / 語音語言模型為核心，且未涉及語音互動或語音建模，因此不符合本主題的包含條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a vision-language model (CLIP) for open vocabulary semantic segmentation, which is unrelated to spoken language models or modeling speech as a core. The topic does not concern spoken language modeling, full-duplex speech interaction, or relevant speech-specific modeling/training/evaluation as required by inclusion criteria. Therefore, it fails to meet any inclusion criterion and matches exclusion criteria (no speech modeling, no speech interaction).",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a vision-language model (CLIP) for open vocabulary semantic segmentation, which is unrelated to spoken language models or modeling speech as a core. The topic does not concern spoken language modeling, full-duplex speech interaction, or relevant speech-specific modeling/training/evaluation as required by inclusion criteria. Therefore, it fails to meet any inclusion criterion and matches exclusion criteria (no speech modeling, no speech interaction).",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "On the Role of Discrete Tokenization in Visual Representation Learning",
    "abstract": "In the realm of self-supervised learning (SSL), masked image modeling (MIM) has gained popularity alongside contrastive learning methods. MIM involves reconstructing masked regions of input images using their unmasked portions. A notable subset of MIM methodologies employs discrete tokens as the reconstruction target, but the theoretical underpinnings of this choice remain underexplored. In this paper, we explore the role of these discrete tokens, aiming to unravel their benefits and limitations. Building upon the connection between MIM and contrastive learning, we provide a comprehensive theoretical understanding on how discrete tokenization affects the model's generalization capabilities. Furthermore, we propose a novel metric named TCAS, which is specifically designed to assess the effectiveness of discrete tokens within the MIM framework. Inspired by this metric, we contribute an innovative tokenizer design and propose a corresponding MIM method named ClusterMIM. It demonstrates superior performance on a variety of benchmark datasets and ViT backbones. Code is available at https://github.com/PKU-ML/ClusterMIM.",
    "metadata": {
      "arxiv_id": "2407.09087",
      "title": "On the Role of Discrete Tokenization in Visual Representation Learning",
      "summary": "In the realm of self-supervised learning (SSL), masked image modeling (MIM) has gained popularity alongside contrastive learning methods. MIM involves reconstructing masked regions of input images using their unmasked portions. A notable subset of MIM methodologies employs discrete tokens as the reconstruction target, but the theoretical underpinnings of this choice remain underexplored. In this paper, we explore the role of these discrete tokens, aiming to unravel their benefits and limitations. Building upon the connection between MIM and contrastive learning, we provide a comprehensive theoretical understanding on how discrete tokenization affects the model's generalization capabilities. Furthermore, we propose a novel metric named TCAS, which is specifically designed to assess the effectiveness of discrete tokens within the MIM framework. Inspired by this metric, we contribute an innovative tokenizer design and propose a corresponding MIM method named ClusterMIM. It demonstrates superior performance on a variety of benchmark datasets and ViT backbones. Code is available at https://github.com/PKU-ML/ClusterMIM.",
      "authors": [
        "Tianqi Du",
        "Yifei Wang",
        "Yisen Wang"
      ],
      "published": "2024-07-12T08:25:31Z",
      "updated": "2024-07-12T08:25:31Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.09087v1",
      "landing_url": "https://arxiv.org/abs/2407.09087v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.09087"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item investigates discrete tokenization in visual representation learning for self-supervised image modeling, not Spoken Language Models or speech-centric architectures, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item investigates discrete tokenization in visual representation learning for self-supervised image modeling, not Spoken Language Models or speech-centric architectures, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on visual representation learning via discrete tokenization and masked image modeling, without mentioning any aspects related to spoken language models, speech language models, or full-duplex spoken language interactions; thus, it does not meet the inclusion criteria related to speech-based modeling and synchronization for real-time spoken interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on visual representation learning via discrete tokenization and masked image modeling, without mentioning any aspects related to spoken language models, speech language models, or full-duplex spoken language interactions; thus, it does not meet the inclusion criteria related to speech-based modeling and synchronization for real-time spoken interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Stream State-tying for Sign Language Recognition",
    "abstract": "In this paper, a novel approach to sign language recognition based on state tying in each of data streams is presented. In this framework, it is assumed that hand gesture signal is represented in terms of six synchronous data streams, i.e., the left/right hand position, left/right hand orientation and left/right handshape. This approach offers a very accurate representation of the sign space and keeps the number of parameters reasonably small in favor of a fast decoding. Experiments were carried out for 5177 Chinese signs. The real time isolated recognition rate is 94.8%. For continuous sign recognition, the word correct rate is 91.4%. Keywords: Sign language recognition; Automatic sign language translation; Hand gesture recognition; Hidden Markov models; State-tying; Multimodal user interface; Virtual reality; Man-machine systems.",
    "metadata": {
      "arxiv_id": "2407.10975",
      "title": "Stream State-tying for Sign Language Recognition",
      "summary": "In this paper, a novel approach to sign language recognition based on state tying in each of data streams is presented. In this framework, it is assumed that hand gesture signal is represented in terms of six synchronous data streams, i.e., the left/right hand position, left/right hand orientation and left/right handshape. This approach offers a very accurate representation of the sign space and keeps the number of parameters reasonably small in favor of a fast decoding. Experiments were carried out for 5177 Chinese signs. The real time isolated recognition rate is 94.8%. For continuous sign recognition, the word correct rate is 91.4%. Keywords: Sign language recognition; Automatic sign language translation; Hand gesture recognition; Hidden Markov models; State-tying; Multimodal user interface; Virtual reality; Man-machine systems.",
      "authors": [
        "Jiyong Ma",
        "Wen Gao",
        "Chunli Wang"
      ],
      "published": "2024-04-21T23:21:52Z",
      "updated": "2024-04-21T23:21:52Z",
      "categories": [
        "cs.OH",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.10975v1",
      "landing_url": "https://arxiv.org/abs/2407.10975v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.10975"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item targets sign language recognition using state-tying and HMMs, not Spoken Language Models (SLMs) or any speech-centric modeling; it fails the core inclusion criterion requiring SLM/FD-SLM focus, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item targets sign language recognition using state-tying and HMMs, not Spoken Language Models (SLMs) or any speech-centric modeling; it fails the core inclusion criterion requiring SLM/FD-SLM focus, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on sign language recognition using hand gesture signals without addressing spoken language models, speech language modeling, or full-duplex spoken interaction frameworks; it neither discusses model architecture involving spoken language models nor evaluation related to synchronous spoken interaction, thus it does not meet the inclusion criteria centered on spoken language modeling and full-duplex interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on sign language recognition using hand gesture signals without addressing spoken language models, speech language modeling, or full-duplex spoken interaction frameworks; it neither discusses model architecture involving spoken language models nor evaluation related to synchronous spoken interaction, thus it does not meet the inclusion criteria centered on spoken language modeling and full-duplex interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Target conversation extraction: Source separation using turn-taking dynamics",
    "abstract": "Extracting the speech of participants in a conversation amidst interfering speakers and noise presents a challenging problem. In this paper, we introduce the novel task of target conversation extraction, where the goal is to extract the audio of a target conversation based on the speaker embedding of one of its participants. To accomplish this, we propose leveraging temporal patterns inherent in human conversations, particularly turn-taking dynamics, which uniquely characterize speakers engaged in conversation and distinguish them from interfering speakers and noise. Using neural networks, we show the feasibility of our approach on English and Mandarin conversation datasets. In the presence of interfering speakers, our results show an 8.19 dB improvement in signal-to-noise ratio for 2-speaker conversations and a 7.92 dB improvement for 2-4-speaker conversations. Code, dataset available at https://github.com/chentuochao/Target-Conversation-Extraction.",
    "metadata": {
      "arxiv_id": "2407.11277",
      "title": "Target conversation extraction: Source separation using turn-taking dynamics",
      "summary": "Extracting the speech of participants in a conversation amidst interfering speakers and noise presents a challenging problem. In this paper, we introduce the novel task of target conversation extraction, where the goal is to extract the audio of a target conversation based on the speaker embedding of one of its participants. To accomplish this, we propose leveraging temporal patterns inherent in human conversations, particularly turn-taking dynamics, which uniquely characterize speakers engaged in conversation and distinguish them from interfering speakers and noise. Using neural networks, we show the feasibility of our approach on English and Mandarin conversation datasets. In the presence of interfering speakers, our results show an 8.19 dB improvement in signal-to-noise ratio for 2-speaker conversations and a 7.92 dB improvement for 2-4-speaker conversations. Code, dataset available at https://github.com/chentuochao/Target-Conversation-Extraction.",
      "authors": [
        "Tuochao Chen",
        "Qirui Wang",
        "Bohan Wu",
        "Malek Itani",
        "Sefik Emre Eskimez",
        "Takuya Yoshioka",
        "Shyamnath Gollakota"
      ],
      "published": "2024-07-15T22:55:27Z",
      "updated": "2024-07-29T19:35:36Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11277v2",
      "landing_url": "https://arxiv.org/abs/2407.11277v2",
      "doi": "https://doi.org/10.21437/Interspeech.2024-225"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item does not center on Spoken Language Models or language modeling of speech as the core object; it focuses on target speech extraction via turn-taking cues for speech separation, not on SLM/FD-SLM architectures or evaluation frameworks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item does not center on Spoken Language Models or language modeling of speech as the core object; it focuses on target speech extraction via turn-taking cues for speech separation, not on SLM/FD-SLM architectures or evaluation frameworks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on extracting speech from conversations leveraging turn-taking dynamics and uses neural networks to improve signal separation. However, while it discusses turn-taking and conversational dynamics, it does not explicitly address spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs) as core modeling paradigms, nor does it mention spoken language modeling architectures, training, or evaluation. The abstract lacks keywords such as spoken language model(s) or speech language model(s), and it appears to focus more on audio source separation rather than speech language modeling or interactive full-duplex systems. Hence, it does not fully meet the inclusion criteria requiring explicit focus on spoken language models or related architectures and evaluation; also, it risks falling under exclusion criteria since it only discusses half-duplex conversation dynamics and audio separation rather than real-time or full-duplex interaction with modeling aspects.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on extracting speech from conversations leveraging turn-taking dynamics and uses neural networks to improve signal separation. However, while it discusses turn-taking and conversational dynamics, it does not explicitly address spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs) as core modeling paradigms, nor does it mention spoken language modeling architectures, training, or evaluation. The abstract lacks keywords such as spoken language model(s) or speech language model(s), and it appears to focus more on audio source separation rather than speech language modeling or interactive full-duplex systems. Hence, it does not fully meet the inclusion criteria requiring explicit focus on spoken language models or related architectures and evaluation; also, it risks falling under exclusion criteria since it only discusses half-duplex conversation dynamics and audio separation rather than real-time or full-duplex interaction with modeling aspects.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Pilot Study of GSLM-based Simulation of Foreign Accentuation Only Using Native Speech Corpora",
    "abstract": "We propose a method of simulating the human process of foreign accentuation using Generative Spoken Language Model (GSLM) only with native speech corpora. When one listens to spoken words of a foreign language and repeats them, the repeated speech is often with the accent of that listener's L1. This is said to be because the spoken words are mentally represented as a sequence of phonological units of the L1, and those units are used for oral reproduction. We simulate this process by inputting speech of language A into GSLM of language B to add B's accent onto the input speech. The process of running ASR of the L1 for foreign input speech and giving the ASR result to TTS of the L1 can be viewed as a naive implementation of this approach. The results of our experiments show that the synthesized accent of the output speech is highly natural, compared to real samples of A generated by speakers whose L1 is B, and that the degree of accentuation is controllable.",
    "metadata": {
      "arxiv_id": "2407.11370",
      "title": "A Pilot Study of GSLM-based Simulation of Foreign Accentuation Only Using Native Speech Corpora",
      "summary": "We propose a method of simulating the human process of foreign accentuation using Generative Spoken Language Model (GSLM) only with native speech corpora. When one listens to spoken words of a foreign language and repeats them, the repeated speech is often with the accent of that listener's L1. This is said to be because the spoken words are mentally represented as a sequence of phonological units of the L1, and those units are used for oral reproduction. We simulate this process by inputting speech of language A into GSLM of language B to add B's accent onto the input speech. The process of running ASR of the L1 for foreign input speech and giving the ASR result to TTS of the L1 can be viewed as a naive implementation of this approach. The results of our experiments show that the synthesized accent of the output speech is highly natural, compared to real samples of A generated by speakers whose L1 is B, and that the degree of accentuation is controllable.",
      "authors": [
        "Kentaro Onda",
        "Joonyong Park",
        "Nobuaki Minematsu",
        "Daisuke Saito"
      ],
      "published": "2024-07-16T04:29:00Z",
      "updated": "2024-07-16T04:29:00Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11370v1",
      "landing_url": "https://arxiv.org/abs/2407.11370v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.11370"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study presents a one-way speech transformation using a Generative Spoken Language Model to simulate foreign accent and does not address full-duplex interactive spoken language modeling or dialog arbitration, thus meeting an exclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study presents a one-way speech transformation using a Generative Spoken Language Model to simulate foreign accent and does not address full-duplex interactive spoken language modeling or dialog arbitration, thus meeting an exclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on simulating foreign accentuation using a Generative Spoken Language Model (GSLM) with native speech corpora, which involves spoken language modeling; however, it does not explicitly address full-duplex spoken language models, synchronized interactive behavior, cognitive parallelism, or multimodal speech-LLM architectures as required. The evaluation and discussion center around accent synthesis and control rather than interactive capabilities or synchronization mechanisms, and there is no mention of real-time interaction, behavioral arbitration, or comprehensive spoken language modeling frameworks. Therefore, it partially meets the inclusion criteria but lacks essential elements related to full-duplex interaction and advanced synchronization aspects, suggesting an uncertain inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on simulating foreign accentuation using a Generative Spoken Language Model (GSLM) with native speech corpora, which involves spoken language modeling; however, it does not explicitly address full-duplex spoken language models, synchronized interactive behavior, cognitive parallelism, or multimodal speech-LLM architectures as required. The evaluation and discussion center around accent synthesis and control rather than interactive capabilities or synchronization mechanisms, and there is no mention of real-time interaction, behavioral arbitration, or comprehensive spoken language modeling frameworks. Therefore, it partially meets the inclusion criteria but lacks essential elements related to full-duplex interaction and advanced synchronization aspects, suggesting an uncertain inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the paper uses a Generative Spoken Language Model (GSLM) but focuses on single-direction accent simulation (ASR→TTS-like process) rather than on full‑duplex spoken language modeling or synchronization; I agree. The abstract describes accent synthesis and controllability but contains no discussion or evaluation of FD‑SLM capabilities (cognitive parallelism, overlap/interrupt handling, low-latency interactive behavior, engineered/learned synchronization, or relevant benchmarks), and thus meets the exclusion criteria for being a one-way speech transformation rather than a study of spoken-language models for synchronous interaction. One-sentence explanation: Exclude because it is an accent-synthesis/GSLM transformation study without any FD‑SLM/synchronization or interactive evaluation required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the paper uses a Generative Spoken Language Model (GSLM) but focuses on single-direction accent simulation (ASR→TTS-like process) rather than on full‑duplex spoken language modeling or synchronization; I agree. The abstract describes accent synthesis and controllability but contains no discussion or evaluation of FD‑SLM capabilities (cognitive parallelism, overlap/interrupt handling, low-latency interactive behavior, engineered/learned synchronization, or relevant benchmarks), and thus meets the exclusion criteria for being a one-way speech transformation rather than a study of spoken-language models for synchronous interaction. One-sentence explanation: Exclude because it is an accent-synthesis/GSLM transformation study without any FD‑SLM/synchronization or interactive evaluation required by the inclusion criteria.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "AlphaPEM: an open-source dynamic 1D physics-based PEM fuel cell model for embedded applications",
    "abstract": "The urgency of the energy transition requires improving the performance and longevity of hydrogen technologies. AlphaPEM is a dynamic one-dimensional (1D) physics-based PEM fuel cell system simulator, programmed in Python and experimentally validated. It offers a good balance between accuracy and execution speed. The modular architecture allows for addition of new features, and it has a user-friendly graphical interface. An automatic calibration method is proposed to match the model to the studied machine. The software provides information on the internal states of the system in response to any current density and can produce polarization and EIS curves. AlphaPEM facilitates the use of a model in embedded conditions, allowing real-time modification of the fuel cell's operating conditions.",
    "metadata": {
      "arxiv_id": "2407.12373",
      "title": "AlphaPEM: an open-source dynamic 1D physics-based PEM fuel cell model for embedded applications",
      "summary": "The urgency of the energy transition requires improving the performance and longevity of hydrogen technologies. AlphaPEM is a dynamic one-dimensional (1D) physics-based PEM fuel cell system simulator, programmed in Python and experimentally validated. It offers a good balance between accuracy and execution speed. The modular architecture allows for addition of new features, and it has a user-friendly graphical interface. An automatic calibration method is proposed to match the model to the studied machine. The software provides information on the internal states of the system in response to any current density and can produce polarization and EIS curves. AlphaPEM facilitates the use of a model in embedded conditions, allowing real-time modification of the fuel cell's operating conditions.",
      "authors": [
        "Raphaël Gass",
        "Zhongliang Li",
        "Samir Jemeï",
        "Rachid Outbib",
        "Daniel Hissel"
      ],
      "published": "2024-07-17T07:52:22Z",
      "updated": "2024-07-17T07:52:22Z",
      "categories": [
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12373v1",
      "landing_url": "https://arxiv.org/abs/2407.12373v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.12373"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about a physics-based PEM fuel cell model; there is no content on Spoken Language Models or speech-language modeling, so it does not satisfy the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about a physics-based PEM fuel cell model; there is no content on Spoken Language Models or speech-language modeling, so it does not satisfy the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a physics-based PEM fuel cell model and simulator, which is unrelated to spoken language models or speech language modeling; it does not discuss speech as a core modeling or inference object, nor any aspect of full-duplex spoken interaction or synchronization, therefore it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a physics-based PEM fuel cell model and simulator, which is unrelated to spoken language models or speech language modeling; it does not discuss speech as a core modeling or inference object, nor any aspect of full-duplex spoken interaction or synchronization, therefore it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Werewolf Arena: A Case Study in LLM Evaluation via Social Deduction",
    "abstract": "This paper introduces Werewolf Arena, a novel framework for evaluating large language models (LLMs) through the lens of the classic social deduction game, Werewolf. In Werewolf Arena, LLMs compete against each other, navigating the game's complex dynamics of deception, deduction, and persuasion. The framework introduces a dynamic turn-taking system based on bidding, mirroring real-world discussions where individuals strategically choose when to speak. We demonstrate the framework's utility through an arena-style tournament featuring Gemini and GPT models. Our results reveal distinct strengths and weaknesses in the models' strategic reasoning and communication. These findings highlight Werewolf Arena's potential as a challenging and scalable LLM benchmark.",
    "metadata": {
      "arxiv_id": "2407.13943",
      "title": "Werewolf Arena: A Case Study in LLM Evaluation via Social Deduction",
      "summary": "This paper introduces Werewolf Arena, a novel framework for evaluating large language models (LLMs) through the lens of the classic social deduction game, Werewolf. In Werewolf Arena, LLMs compete against each other, navigating the game's complex dynamics of deception, deduction, and persuasion. The framework introduces a dynamic turn-taking system based on bidding, mirroring real-world discussions where individuals strategically choose when to speak. We demonstrate the framework's utility through an arena-style tournament featuring Gemini and GPT models. Our results reveal distinct strengths and weaknesses in the models' strategic reasoning and communication. These findings highlight Werewolf Arena's potential as a challenging and scalable LLM benchmark.",
      "authors": [
        "Suma Bailis",
        "Jane Friedhoff",
        "Feiyang Chen"
      ],
      "published": "2024-07-18T23:41:05Z",
      "updated": "2024-07-18T23:41:05Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.13943v1",
      "landing_url": "https://arxiv.org/abs/2407.13943v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.13943"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on evaluating LLMs through a text-based Werewolf game, with no focus on Spoken Language Models or speech-language modeling concepts (no speech input/output, no turn-taking in spoken dialogue per se), so it fails the inclusion criteria and does meet the exclusion but not required since it's not relevant to speech modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper centers on evaluating LLMs through a text-based Werewolf game, with no focus on Spoken Language Models or speech-language modeling concepts (no speech input/output, no turn-taking in spoken dialogue per se), so it fails the inclusion criteria and does meet the exclusion but not required since it's not relevant to speech modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces a framework for evaluating large language models through a social deduction game, focusing on strategic reasoning and communication of textual language models like Gemini and GPT, without mentioning any use or modeling of spoken language, speech signals, or full-duplex spoken language models, nor discussing speech-layer language modeling or synchronous speech interaction; thus, it does not meet the inclusion criteria centered on spoken language models or speech-based modeling and interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper introduces a framework for evaluating large language models through a social deduction game, focusing on strategic reasoning and communication of textual language models like Gemini and GPT, without mentioning any use or modeling of spoken language, speech signals, or full-duplex spoken language models, nor discussing speech-layer language modeling or synchronous speech interaction; thus, it does not meet the inclusion criteria centered on spoken language models or speech-based modeling and interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "EMO-Codec: An In-Depth Look at Emotion Preservation capacity of Legacy and Neural Codec Models With Subjective and Objective Evaluations",
    "abstract": "The neural codec model reduces speech data transmission delay and serves as the foundational tokenizer for speech language models (speech LMs). Preserving emotional information in codecs is crucial for effective communication and context understanding. However, there is a lack of studies on emotion loss in existing codecs. This paper evaluates neural and legacy codecs using subjective and objective methods on emotion datasets like IEMOCAP. Our study identifies which codecs best preserve emotional information under various bitrate scenarios. We found that training codec models with both English and Chinese data had limited success in retaining emotional information in Chinese. Additionally, resynthesizing speech through these codecs degrades the performance of speech emotion recognition (SER), particularly for emotions like sadness, depression, fear, and disgust. Human listening tests confirmed these findings. This work guides future speech technology developments to ensure new codecs maintain the integrity of emotional information in speech.",
    "metadata": {
      "arxiv_id": "2407.15458",
      "title": "EMO-Codec: An In-Depth Look at Emotion Preservation capacity of Legacy and Neural Codec Models With Subjective and Objective Evaluations",
      "summary": "The neural codec model reduces speech data transmission delay and serves as the foundational tokenizer for speech language models (speech LMs). Preserving emotional information in codecs is crucial for effective communication and context understanding. However, there is a lack of studies on emotion loss in existing codecs. This paper evaluates neural and legacy codecs using subjective and objective methods on emotion datasets like IEMOCAP. Our study identifies which codecs best preserve emotional information under various bitrate scenarios. We found that training codec models with both English and Chinese data had limited success in retaining emotional information in Chinese. Additionally, resynthesizing speech through these codecs degrades the performance of speech emotion recognition (SER), particularly for emotions like sadness, depression, fear, and disgust. Human listening tests confirmed these findings. This work guides future speech technology developments to ensure new codecs maintain the integrity of emotional information in speech.",
      "authors": [
        "Wenze Ren",
        "Yi-Cheng Lin",
        "Huang-Cheng Chou",
        "Haibin Wu",
        "Yi-Chiao Wu",
        "Chi-Chun Lee",
        "Hung-yi Lee",
        "Yu Tsao"
      ],
      "published": "2024-07-22T08:14:16Z",
      "updated": "2024-07-30T12:37:35Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.15458v4",
      "landing_url": "https://arxiv.org/abs/2407.15458v4",
      "doi": "https://doi.org/10.48550/arXiv.2407.15458"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此研究聚焦於語音編解碼器在情感保留上的性能評估，核心是編碼/解碼與情感辨識測試，而非以 Spoken Language Models（SLMs）或全雙工語音互動的模型架構與評估，因此不符合主題與評估框架的核心條件，應予以排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此研究聚焦於語音編解碼器在情感保留上的性能評估，核心是編碼/解碼與情感辨識測試，而非以 Spoken Language Models（SLMs）或全雙工語音互動的模型架構與評估，因此不符合主題與評估框架的核心條件，應予以排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on evaluating codecs for preserving emotional information in speech, rather than on spoken language models or full-duplex spoken interaction models; it lacks discussion on modeling or inference architectures related to SLMs or FD-SLMs, synchronization, or interactive real-time dialogue behaviors, and does not mention spoken language modeling explicitly in the title or abstract, thus it does not meet the inclusion criteria and falls under exclusion criteria regarding focus and scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on evaluating codecs for preserving emotional information in speech, rather than on spoken language models or full-duplex spoken interaction models; it lacks discussion on modeling or inference architectures related to SLMs or FD-SLMs, synchronization, or interactive real-time dialogue behaviors, and does not mention spoken language modeling explicitly in the title or abstract, thus it does not meet the inclusion criteria and falls under exclusion criteria regarding focus and scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "J-CHAT: Japanese Large-scale Spoken Dialogue Corpus for Spoken Dialogue Language Modeling",
    "abstract": "Spoken dialogue plays a crucial role in human-AI interactions, necessitating dialogue-oriented spoken language models (SLMs). To develop versatile SLMs, large-scale and diverse speech datasets are essential. Additionally, to ensure hiqh-quality speech generation, the data must be spontaneous like in-wild data and must be acoustically clean with noise removed. Despite the critical need, no open-source corpus meeting all these criteria has been available. This study addresses this gap by constructing and releasing a large-scale spoken dialogue corpus, named Japanese Corpus for Human-AI Talks (J-CHAT), which is publicly accessible. Furthermore, this paper presents a language-independent method for corpus construction and describes experiments on dialogue generation using SLMs trained on J-CHAT. Experimental results indicate that the collected data from multiple domains by our method improve the naturalness and meaningfulness of dialogue generation.",
    "metadata": {
      "arxiv_id": "2407.15828",
      "title": "J-CHAT: Japanese Large-scale Spoken Dialogue Corpus for Spoken Dialogue Language Modeling",
      "summary": "Spoken dialogue plays a crucial role in human-AI interactions, necessitating dialogue-oriented spoken language models (SLMs). To develop versatile SLMs, large-scale and diverse speech datasets are essential. Additionally, to ensure hiqh-quality speech generation, the data must be spontaneous like in-wild data and must be acoustically clean with noise removed. Despite the critical need, no open-source corpus meeting all these criteria has been available. This study addresses this gap by constructing and releasing a large-scale spoken dialogue corpus, named Japanese Corpus for Human-AI Talks (J-CHAT), which is publicly accessible. Furthermore, this paper presents a language-independent method for corpus construction and describes experiments on dialogue generation using SLMs trained on J-CHAT. Experimental results indicate that the collected data from multiple domains by our method improve the naturalness and meaningfulness of dialogue generation.",
      "authors": [
        "Wataru Nakata",
        "Kentaro Seki",
        "Hitomi Yanaka",
        "Yuki Saito",
        "Shinnosuke Takamichi",
        "Hiroshi Saruwatari"
      ],
      "published": "2024-07-22T17:46:50Z",
      "updated": "2024-07-22T17:46:50Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.15828v1",
      "landing_url": "https://arxiv.org/abs/2407.15828v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.15828"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item clearly centers on Spoken Language Models (SLMs) and dialogue-oriented modeling, discusses a large-scale spoken dialogue corpus for training/evaluating SLMs, is recent (2025), and mentions SLMs in the title/abstract, with no exclusion criteria clearly applying.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item clearly centers on Spoken Language Models (SLMs) and dialogue-oriented modeling, discusses a large-scale spoken dialogue corpus for training/evaluating SLMs, is recent (2025), and mentions SLMs in the title/abstract, with no exclusion criteria clearly applying.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The article presents a large-scale spoken dialogue corpus (J-CHAT) for spoken dialogue language modeling, addressing the need for spontaneous, acoustically clean data to develop speech language models (SLMs). The abstract focuses on corpus construction and experiments on dialogue generation using SLMs trained on this data. However, the abstract does not explicitly discuss full-duplex spoken language models (FD-SLMs), cognitive parallelism, synchronous turn-taking, or low-latency interactions, nor does it mention detailed evaluations aligned with the inclusion criteria's advanced synchronization or behavioral arbitration concepts. Additionally, the study centers on dataset construction and general SLM usage rather than novel model architectures or explicit speech-language modeling innovations beyond language modeling. Therefore, it partially meets the inclusion criteria but does not fully satisfy the focus on FD-SLM or advanced synchronization aspects, nor does it demonstrate comprehensive assessments described in the exclusion criteria.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The article presents a large-scale spoken dialogue corpus (J-CHAT) for spoken dialogue language modeling, addressing the need for spontaneous, acoustically clean data to develop speech language models (SLMs). The abstract focuses on corpus construction and experiments on dialogue generation using SLMs trained on this data. However, the abstract does not explicitly discuss full-duplex spoken language models (FD-SLMs), cognitive parallelism, synchronous turn-taking, or low-latency interactions, nor does it mention detailed evaluations aligned with the inclusion criteria's advanced synchronization or behavioral arbitration concepts. Additionally, the study centers on dataset construction and general SLM usage rather than novel model architectures or explicit speech-language modeling innovations beyond language modeling. Therefore, it partially meets the inclusion criteria but does not fully satisfy the focus on FD-SLM or advanced synchronization aspects, nor does it demonstrate comprehensive assessments described in the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "整合兩位初評意見：一位給予高度肯定（5分），認為本文明確以 Spoken Language Models (SLMs) 為核心並提供可公開的大規模口語對話語料；另一位則較保守（3分），指出摘要未觸及 full‑duplex、同步化（turn‑taking、重疊語音、低延遲等）或行為仲裁的細節。評估原文：標題與摘要明確以 “spoken dialogue language modeling”／SLMs 為主題，描述了大規模、自然且乾淨的口語對話語料庫（J‑CHAT）的建構、公開釋出，並報告了用該語料訓練 SLMs 進行對話生成的實驗，顯示資料能提升生成之自然性與意義性。依據納入準則，研究必須以 SLM 或等價概念為核心並將語音視為主要建模對象——本條件在標題與摘要中被滿足；排除準則主要針對僅為傳統 ASR/TTS、僅為半雙工/單向系統、或僅以 WER/音質為評估指標之研究，而本工作不是僅討論 ASR/TTS，且著重資料建構與以語音為核心的語言模型實驗，因此不落入明確排除類別。缺點是本文摘要未報告關於 full‑duplex 能力、認知並行、重疊語音處理或低延遲互動之具體架構或評估，因此若研究範圍嚴格限定為 FD‑SLM 與同步化技術的話，可能不足；但就廣義的 Spoken Language Models 與語音層級的 language modeling 研究主題而言，本項目符合納入要件。 1‑sentence 評分說明：給予 4 分（較傾向納入），因為標題與摘要明確以 SLM 為核心並提供可用於訓練/評估 SLM 的大規模口語對話語料，雖然未涉及 full‑duplex/同步化的深入技術與評估細節。",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "整合兩位初評意見：一位給予高度肯定（5分），認為本文明確以 Spoken Language Models (SLMs) 為核心並提供可公開的大規模口語對話語料；另一位則較保守（3分），指出摘要未觸及 full‑duplex、同步化（turn‑taking、重疊語音、低延遲等）或行為仲裁的細節。評估原文：標題與摘要明確以 “spoken dialogue language modeling”／SLMs 為主題，描述了大規模、自然且乾淨的口語對話語料庫（J‑CHAT）的建構、公開釋出，並報告了用該語料訓練 SLMs 進行對話生成的實驗，顯示資料能提升生成之自然性與意義性。依據納入準則，研究必須以 SLM 或等價概念為核心並將語音視為主要建模對象——本條件在標題與摘要中被滿足；排除準則主要針對僅為傳統 ASR/TTS、僅為半雙工/單向系統、或僅以 WER/音質為評估指標之研究，而本工作不是僅討論 ASR/TTS，且著重資料建構與以語音為核心的語言模型實驗，因此不落入明確排除類別。缺點是本文摘要未報告關於 full‑duplex 能力、認知並行、重疊語音處理或低延遲互動之具體架構或評估，因此若研究範圍嚴格限定為 FD‑SLM 與同步化技術的話，可能不足；但就廣義的 Spoken Language Models 與語音層級的 language modeling 研究主題而言，本項目符合納入要件。 1‑sentence 評分說明：給予 4 分（較傾向納入），因為標題與摘要明確以 SLM 為核心並提供可用於訓練/評估 SLM 的大規模口語對話語料，雖然未涉及 full‑duplex/同步化的深入技術與評估細節。",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Real-Time Interactions Between Human Controllers and Remote Devices in Metaverse",
    "abstract": "Supporting real-time interactions between human controllers and remote devices remains a challenging goal in the Metaverse due to the stringent requirements on computing workload, communication throughput, and round-trip latency. In this paper, we establish a novel framework for real-time interactions through the virtual models in the Metaverse. Specifically, we jointly predict the motion of the human controller for 1) proactive rendering in the Metaverse and 2) generating control commands to the real-world remote device in advance. The virtual model is decoupled into two components for rendering and control, respectively. To dynamically adjust the prediction horizons for rendering and control, we develop a two-step human-in-the-loop continuous reinforcement learning approach and use an expert policy to improve the training efficiency. An experimental prototype is built to verify our algorithm with different communication latencies. Compared with the baseline policy without prediction, our proposed method can reduce 1) the Motion-To-Photon (MTP) latency between human motion and rendering feedback and 2) the root mean squared error (RMSE) between human motion and real-world remote devices significantly.",
    "metadata": {
      "arxiv_id": "2407.16591",
      "title": "Real-Time Interactions Between Human Controllers and Remote Devices in Metaverse",
      "summary": "Supporting real-time interactions between human controllers and remote devices remains a challenging goal in the Metaverse due to the stringent requirements on computing workload, communication throughput, and round-trip latency. In this paper, we establish a novel framework for real-time interactions through the virtual models in the Metaverse. Specifically, we jointly predict the motion of the human controller for 1) proactive rendering in the Metaverse and 2) generating control commands to the real-world remote device in advance. The virtual model is decoupled into two components for rendering and control, respectively. To dynamically adjust the prediction horizons for rendering and control, we develop a two-step human-in-the-loop continuous reinforcement learning approach and use an expert policy to improve the training efficiency. An experimental prototype is built to verify our algorithm with different communication latencies. Compared with the baseline policy without prediction, our proposed method can reduce 1) the Motion-To-Photon (MTP) latency between human motion and rendering feedback and 2) the root mean squared error (RMSE) between human motion and real-world remote devices significantly.",
      "authors": [
        "Kan Chen",
        "Zhen Meng",
        "Xiangmin Xu",
        "Changyang She",
        "Philip G. Zhao"
      ],
      "published": "2024-07-23T15:48:36Z",
      "updated": "2024-07-23T15:48:36Z",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.16591v1",
      "landing_url": "https://arxiv.org/abs/2407.16591v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.16591"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on real-time interaction between humans and remote devices in the Metaverse and RL-based prediction for rendering/control, with no focus on Spoken Language Models or speech modeling, thus not meeting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on real-time interaction between humans and remote devices in the Metaverse and RL-based prediction for rendering/control, with no focus on Spoken Language Models or speech modeling, thus not meeting the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on real-time interaction between human controllers and remote devices in the Metaverse and emphasizes motion prediction and low-latency control; however, it does not address Spoken Language Models or speech language modeling as its core subject, nor does it discuss model architectures or evaluations related to speech or spoken language, which are central to the inclusion criteria. Therefore, it does not meet the inclusion criteria focused on speech modeling and language models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on real-time interaction between human controllers and remote devices in the Metaverse and emphasizes motion prediction and low-latency control; however, it does not address Spoken Language Models or speech language modeling as its core subject, nor does it discuss model architectures or evaluations related to speech or spoken language, which are central to the inclusion criteria. Therefore, it does not meet the inclusion criteria focused on speech modeling and language models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A high-efficiency plug-and-play superconducting qubit network",
    "abstract": "Modular architectures are a promising approach to scale quantum devices to the point of fault tolerance and utility. Modularity is particularly appealing for superconducting qubits, as monolithically manufactured devices are limited in both system size and quality. Constructing complex quantum systems as networks of interchangeable modules can overcome this challenge through `Lego-like' assembly, reconfiguration, and expansion, in a spirit similar to modern classical computers. First prototypical superconducting quantum device networks have been demonstrated. Interfaces that simultaneously permit interchangeability and high-fidelity operations remain a crucial challenge, however. Here, we demonstrate a high-efficiency interconnect based on a detachable cable between superconducting qubit devices. We overcome the inevitable loss in a detachable connection through a fast pump scheme, enabling inter-module SWAP efficiencies at the 99%-level in less than 100 ns. We use this scheme to generate high-fidelity entanglement and operate a distributed logical dual-rail qubit. At the observed ~1% error rate, operations through the interconnect are at the threshold for fault-tolerance. These results introduce a modular architecture for scaling quantum processors with reconfigurable and expandable networks.",
    "metadata": {
      "arxiv_id": "2407.16743",
      "title": "A high-efficiency plug-and-play superconducting qubit network",
      "summary": "Modular architectures are a promising approach to scale quantum devices to the point of fault tolerance and utility. Modularity is particularly appealing for superconducting qubits, as monolithically manufactured devices are limited in both system size and quality. Constructing complex quantum systems as networks of interchangeable modules can overcome this challenge through `Lego-like' assembly, reconfiguration, and expansion, in a spirit similar to modern classical computers. First prototypical superconducting quantum device networks have been demonstrated. Interfaces that simultaneously permit interchangeability and high-fidelity operations remain a crucial challenge, however. Here, we demonstrate a high-efficiency interconnect based on a detachable cable between superconducting qubit devices. We overcome the inevitable loss in a detachable connection through a fast pump scheme, enabling inter-module SWAP efficiencies at the 99%-level in less than 100 ns. We use this scheme to generate high-fidelity entanglement and operate a distributed logical dual-rail qubit. At the observed ~1% error rate, operations through the interconnect are at the threshold for fault-tolerance. These results introduce a modular architecture for scaling quantum processors with reconfigurable and expandable networks.",
      "authors": [
        "Michael Mollenhauer",
        "Abdullah Irfan",
        "Xi Cao",
        "Supriya Mandal",
        "Wolfgang Pfaff"
      ],
      "published": "2024-07-23T17:58:59Z",
      "updated": "2024-07-23T17:58:59Z",
      "categories": [
        "quant-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.16743v1",
      "landing_url": "https://arxiv.org/abs/2407.16743v1",
      "doi": "https://doi.org/10.1038/s41928-025-01404-3"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about superconducting qubit networks in quantum computing, unrelated to Spoken Language Models or speech-language modeling; it fails all inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about superconducting qubit networks in quantum computing, unrelated to Spoken Language Models or speech-language modeling; it fails all inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a high-efficiency modular superconducting qubit network and quantum processor scaling, which is unrelated to spoken language models or speech-based language modeling as required by the inclusion criteria. There is no mention or implication of speech, spoken language models, full-duplex interaction, or related concepts. Therefore, the study does not meet the inclusion criteria and is unrelated to the review topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a high-efficiency modular superconducting qubit network and quantum processor scaling, which is unrelated to spoken language models or speech-based language modeling as required by the inclusion criteria. There is no mention or implication of speech, spoken language models, full-duplex interaction, or related concepts. Therefore, the study does not meet the inclusion criteria and is unrelated to the review topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Graph Neural Networks for Virtual Sensing in Complex Systems: Addressing Heterogeneous Temporal Dynamics",
    "abstract": "Real-time condition monitoring is crucial for the reliable and efficient operation of complex systems. However, relying solely on physical sensors can be limited due to their cost, placement constraints, or inability to directly measure certain critical parameters. Virtual sensing addresses these limitations by leveraging readily available sensor data and system knowledge to estimate inaccessible parameters or infer system states. The increasing complexity of industrial systems necessitates deployments of sensors with diverse modalities to provide a comprehensive understanding of system states. These sensors capture data at varying frequencies to monitor both rapid and slowly varying system dynamics, as well as local and global state evolutions of the systems. This leads to heterogeneous temporal dynamics, which, particularly under varying operational end environmental conditions, pose a significant challenge for accurate virtual sensing. To address this, we propose a Heterogeneous Temporal Graph Neural Network (HTGNN) framework. HTGNN explicitly models signals from diverse sensors and integrates operating conditions into the model architecture. We evaluate HTGNN using two newly released datasets: a bearing dataset with diverse load conditions for bearing load prediction and a year-long simulated dataset for predicting bridge live loads. Our results demonstrate that HTGNN significantly outperforms established baseline methods in both tasks, particularly under highly varying operating conditions. These results highlight HTGNN's potential as a robust and accurate virtual sensing approach for complex systems, paving the way for improved monitoring, predictive maintenance, and enhanced system performance. Our code and data are available under https://github.com/EPFL-IMOS/htgnn.",
    "metadata": {
      "arxiv_id": "2407.18691",
      "title": "Graph Neural Networks for Virtual Sensing in Complex Systems: Addressing Heterogeneous Temporal Dynamics",
      "summary": "Real-time condition monitoring is crucial for the reliable and efficient operation of complex systems. However, relying solely on physical sensors can be limited due to their cost, placement constraints, or inability to directly measure certain critical parameters. Virtual sensing addresses these limitations by leveraging readily available sensor data and system knowledge to estimate inaccessible parameters or infer system states. The increasing complexity of industrial systems necessitates deployments of sensors with diverse modalities to provide a comprehensive understanding of system states. These sensors capture data at varying frequencies to monitor both rapid and slowly varying system dynamics, as well as local and global state evolutions of the systems. This leads to heterogeneous temporal dynamics, which, particularly under varying operational end environmental conditions, pose a significant challenge for accurate virtual sensing. To address this, we propose a Heterogeneous Temporal Graph Neural Network (HTGNN) framework. HTGNN explicitly models signals from diverse sensors and integrates operating conditions into the model architecture. We evaluate HTGNN using two newly released datasets: a bearing dataset with diverse load conditions for bearing load prediction and a year-long simulated dataset for predicting bridge live loads. Our results demonstrate that HTGNN significantly outperforms established baseline methods in both tasks, particularly under highly varying operating conditions. These results highlight HTGNN's potential as a robust and accurate virtual sensing approach for complex systems, paving the way for improved monitoring, predictive maintenance, and enhanced system performance. Our code and data are available under https://github.com/EPFL-IMOS/htgnn.",
      "authors": [
        "Mengjie Zhao",
        "Cees Taal",
        "Stephan Baggerohr",
        "Olga Fink"
      ],
      "published": "2024-07-26T12:16:53Z",
      "updated": "2025-03-06T15:47:01Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.18691v2",
      "landing_url": "https://arxiv.org/abs/2407.18691v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.18691"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item deals with Graph Neural Networks for virtual sensing in engineering and has no focus on Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria and aligns with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item deals with Graph Neural Networks for virtual sensing in engineering and has no focus on Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria and aligns with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on graph neural networks for virtual sensing in industrial systems, addressing heterogeneous temporal dynamics in sensor signals, with no mention of spoken language models, speech language modeling, full-duplex spoken language interaction, or related evaluation criteria specified in the inclusion criteria. The research neither centers on speech as the primary modeling object nor discusses full-duplex spoken language models or cognitive parallelism, and thus does not meet the inclusion criteria focused on spoken language models and full-duplex speech interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on graph neural networks for virtual sensing in industrial systems, addressing heterogeneous temporal dynamics in sensor signals, with no mention of spoken language models, speech language modeling, full-duplex spoken language interaction, or related evaluation criteria specified in the inclusion criteria. The research neither centers on speech as the primary modeling object nor discusses full-duplex spoken language models or cognitive parallelism, and thus does not meet the inclusion criteria focused on spoken language models and full-duplex speech interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A High-Throughput FPGA Accelerator for Lightweight CNNs With Balanced Dataflow",
    "abstract": "FPGA accelerators for lightweight neural convolutional networks (LWCNNs) have recently attracted significant attention. Most existing LWCNN accelerators focus on single-Computing-Engine (CE) architecture with local optimization. However, these designs typically suffer from high on-chip/off-chip memory overhead and low computational efficiency due to their layer-by-layer dataflow and unified resource mapping mechanisms. To tackle these issues, a novel multi-CE-based accelerator with balanced dataflow is proposed to efficiently accelerate LWCNN through memory-oriented and computing-oriented optimizations. Firstly, a streaming architecture with hybrid CEs is designed to minimize off-chip memory access while maintaining a low cost of on-chip buffer size. Secondly, a balanced dataflow strategy is introduced for streaming architectures to enhance computational efficiency by improving efficient resource mapping and mitigating data congestion. Furthermore, a resource-aware memory and parallelism allocation methodology is proposed, based on a performance model, to achieve better performance and scalability. The proposed accelerator is evaluated on Xilinx ZC706 platform using MobileNetV2 and ShuffleNetV2.Implementation results demonstrate that the proposed accelerator can save up to 68.3% of on-chip memory size with reduced off-chip memory access compared to the reference design. It achieves an impressive performance of up to 2092.4 FPS and a state-of-the-art MAC efficiency of up to 94.58%, while maintaining a high DSP utilization of 95%, thus significantly outperforming current LWCNN accelerators.",
    "metadata": {
      "arxiv_id": "2407.19449",
      "title": "A High-Throughput FPGA Accelerator for Lightweight CNNs With Balanced Dataflow",
      "summary": "FPGA accelerators for lightweight neural convolutional networks (LWCNNs) have recently attracted significant attention. Most existing LWCNN accelerators focus on single-Computing-Engine (CE) architecture with local optimization. However, these designs typically suffer from high on-chip/off-chip memory overhead and low computational efficiency due to their layer-by-layer dataflow and unified resource mapping mechanisms. To tackle these issues, a novel multi-CE-based accelerator with balanced dataflow is proposed to efficiently accelerate LWCNN through memory-oriented and computing-oriented optimizations. Firstly, a streaming architecture with hybrid CEs is designed to minimize off-chip memory access while maintaining a low cost of on-chip buffer size. Secondly, a balanced dataflow strategy is introduced for streaming architectures to enhance computational efficiency by improving efficient resource mapping and mitigating data congestion. Furthermore, a resource-aware memory and parallelism allocation methodology is proposed, based on a performance model, to achieve better performance and scalability. The proposed accelerator is evaluated on Xilinx ZC706 platform using MobileNetV2 and ShuffleNetV2.Implementation results demonstrate that the proposed accelerator can save up to 68.3% of on-chip memory size with reduced off-chip memory access compared to the reference design. It achieves an impressive performance of up to 2092.4 FPS and a state-of-the-art MAC efficiency of up to 94.58%, while maintaining a high DSP utilization of 95%, thus significantly outperforming current LWCNN accelerators.",
      "authors": [
        "Zhiyuan Zhao",
        "Yihao Chen",
        "Pengcheng Feng",
        "Jixing Li",
        "Gang Chen",
        "Rongxuan Shen",
        "Huaxiang Lu"
      ],
      "published": "2024-07-28T09:53:03Z",
      "updated": "2024-12-16T12:22:03Z",
      "categories": [
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.19449v4",
      "landing_url": "https://arxiv.org/abs/2407.19449v4",
      "doi": "https://doi.org/10.48550/arXiv.2407.19449"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes a hardware accelerator for lightweight CNNs and contains no discussion of Spoken Language Models, speech modeling, or dialogue synchronization; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes a hardware accelerator for lightweight CNNs and contains no discussion of Spoken Language Models, speech modeling, or dialogue synchronization; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on FPGA acceleration for lightweight CNNs, specifically targeting hardware performance optimizations. There is no mention of Spoken Language Models, speech language modeling, full-duplex spoken language interaction, or any of the inclusion criteria related to speech modeling and real-time speech interaction. The core topic is hardware acceleration for neural networks rather than spoken language modeling or speech interaction systems, thus failing to meet the key inclusion criteria while indirectly matching some exclusion points like missing spoken language models and focus on speech interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on FPGA acceleration for lightweight CNNs, specifically targeting hardware performance optimizations. There is no mention of Spoken Language Models, speech language modeling, full-duplex spoken language interaction, or any of the inclusion criteria related to speech modeling and real-time speech interaction. The core topic is hardware acceleration for neural networks rather than spoken language modeling or speech interaction systems, thus failing to meet the key inclusion criteria while indirectly matching some exclusion points like missing spoken language models and focus on speech interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Interpreting and learning voice commands with a Large Language Model for a robot system",
    "abstract": "Robots are increasingly common in industry and daily life, such as in nursing homes where they can assist staff. A key challenge is developing intuitive interfaces for easy communication. The use of Large Language Models (LLMs) like GPT-4 has enhanced robot capabilities, allowing for real-time interaction and decision-making. This integration improves robots' adaptability and functionality. This project focuses on merging LLMs with databases to improve decision-making and enable knowledge acquisition for request interpretation problems.",
    "metadata": {
      "arxiv_id": "2407.21512",
      "title": "Interpreting and learning voice commands with a Large Language Model for a robot system",
      "summary": "Robots are increasingly common in industry and daily life, such as in nursing homes where they can assist staff. A key challenge is developing intuitive interfaces for easy communication. The use of Large Language Models (LLMs) like GPT-4 has enhanced robot capabilities, allowing for real-time interaction and decision-making. This integration improves robots' adaptability and functionality. This project focuses on merging LLMs with databases to improve decision-making and enable knowledge acquisition for request interpretation problems.",
      "authors": [
        "Stanislau Stankevich",
        "Wojciech Dudek"
      ],
      "published": "2024-07-31T10:30:31Z",
      "updated": "2024-07-31T10:30:31Z",
      "categories": [
        "cs.RO",
        "cs.CL",
        "cs.NE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.21512v1",
      "landing_url": "https://arxiv.org/abs/2407.21512v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.21512"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on using LLMs to interpret voice commands in a robot system but does not clearly position Spoken Language Models (or FD-SLMs) as the core modeling object with speech-level modeling, training, or end-to-end speech-LLM architectures and evaluation typically required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item centers on using LLMs to interpret voice commands in a robot system but does not clearly position Spoken Language Models (or FD-SLMs) as the core modeling object with speech-level modeling, training, or end-to-end speech-LLM architectures and evaluation typically required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on integrating Large Language Models with robot systems for interpreting voice commands, but it primarily discusses LLM capabilities and decision-making improvement without explicitly centering on spoken language modeling or full-duplex spoken interaction paradigms. There is no clear evidence of research on spoken language models or low-latency, real-time synchronous speech processing as required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on integrating Large Language Models with robot systems for interpreting voice commands, but it primarily discusses LLM capabilities and decision-making improvement without explicitly centering on spoken language modeling or full-duplex spoken interaction paradigms. There is no clear evidence of research on spoken language models or low-latency, real-time synchronous speech processing as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Nested Music Transformer: Sequentially Decoding Compound Tokens in Symbolic Music and Audio Generation",
    "abstract": "Representing symbolic music with compound tokens, where each token consists of several different sub-tokens representing a distinct musical feature or attribute, offers the advantage of reducing sequence length. While previous research has validated the efficacy of compound tokens in music sequence modeling, predicting all sub-tokens simultaneously can lead to suboptimal results as it may not fully capture the interdependencies between them. We introduce the Nested Music Transformer (NMT), an architecture tailored for decoding compound tokens autoregressively, similar to processing flattened tokens, but with low memory usage. The NMT consists of two transformers: the main decoder that models a sequence of compound tokens and the sub-decoder for modeling sub-tokens of each compound token. The experiment results showed that applying the NMT to compound tokens can enhance the performance in terms of better perplexity in processing various symbolic music datasets and discrete audio tokens from the MAESTRO dataset.",
    "metadata": {
      "arxiv_id": "2408.01180",
      "title": "Nested Music Transformer: Sequentially Decoding Compound Tokens in Symbolic Music and Audio Generation",
      "summary": "Representing symbolic music with compound tokens, where each token consists of several different sub-tokens representing a distinct musical feature or attribute, offers the advantage of reducing sequence length. While previous research has validated the efficacy of compound tokens in music sequence modeling, predicting all sub-tokens simultaneously can lead to suboptimal results as it may not fully capture the interdependencies between them. We introduce the Nested Music Transformer (NMT), an architecture tailored for decoding compound tokens autoregressively, similar to processing flattened tokens, but with low memory usage. The NMT consists of two transformers: the main decoder that models a sequence of compound tokens and the sub-decoder for modeling sub-tokens of each compound token. The experiment results showed that applying the NMT to compound tokens can enhance the performance in terms of better perplexity in processing various symbolic music datasets and discrete audio tokens from the MAESTRO dataset.",
      "authors": [
        "Jiwoo Ryu",
        "Hao-Wen Dong",
        "Jongmin Jung",
        "Dasaem Jeong"
      ],
      "published": "2024-08-02T11:02:38Z",
      "updated": "2024-08-02T11:02:38Z",
      "categories": [
        "cs.SD",
        "cs.IR",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.01180v1",
      "landing_url": "https://arxiv.org/abs/2408.01180v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.01180"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on symbolic music tokens and music generation, not on Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on symbolic music tokens and music generation, not on Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on symbolic music and audio generation using compound tokens and transformers but does not address spoken language models or speech language modeling, nor does it discuss full-duplex spoken language interaction, speech-related model architectures, training, or evaluation relevant to the specified inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on symbolic music and audio generation using compound tokens and transformers but does not address spoken language models or speech language modeling, nor does it discuss full-duplex spoken language interaction, speech-related model architectures, training, or evaluation relevant to the specified inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SiamMo: Siamese Motion-Centric 3D Object Tracking",
    "abstract": "Current 3D single object tracking methods primarily rely on the Siamese matching-based paradigm, which struggles with textureless and incomplete LiDAR point clouds. Conversely, the motion-centric paradigm avoids appearance matching, thus overcoming these issues. However, its complex multi-stage pipeline and the limited temporal modeling capability of a single-stream architecture constrain its potential. In this paper, we introduce SiamMo, a novel and simple Siamese motion-centric tracking approach. Unlike the traditional single-stream architecture, we employ Siamese feature extraction for motion-centric tracking. This decouples feature extraction from temporal fusion, significantly enhancing tracking performance. Additionally, we design a Spatio-Temporal Feature Aggregation module to integrate Siamese features at multiple scales, capturing motion information effectively. We also introduce a Box-aware Feature Encoding module to encode object size priors into motion estimation. SiamMo is a purely motion-centric tracker that eliminates the need for additional processes like segmentation and box refinement. Without whistles and bells, SiamMo not only surpasses state-of-the-art methods across multiple benchmarks but also demonstrates exceptional robustness in challenging scenarios. SiamMo sets a new record on the KITTI tracking benchmark with 90.1\\% precision while maintaining a high inference speed of 108 FPS. The code will be released at https://github.com/HDU-VRLab/SiamMo.",
    "metadata": {
      "arxiv_id": "2408.01688",
      "title": "SiamMo: Siamese Motion-Centric 3D Object Tracking",
      "summary": "Current 3D single object tracking methods primarily rely on the Siamese matching-based paradigm, which struggles with textureless and incomplete LiDAR point clouds. Conversely, the motion-centric paradigm avoids appearance matching, thus overcoming these issues. However, its complex multi-stage pipeline and the limited temporal modeling capability of a single-stream architecture constrain its potential. In this paper, we introduce SiamMo, a novel and simple Siamese motion-centric tracking approach. Unlike the traditional single-stream architecture, we employ Siamese feature extraction for motion-centric tracking. This decouples feature extraction from temporal fusion, significantly enhancing tracking performance. Additionally, we design a Spatio-Temporal Feature Aggregation module to integrate Siamese features at multiple scales, capturing motion information effectively. We also introduce a Box-aware Feature Encoding module to encode object size priors into motion estimation. SiamMo is a purely motion-centric tracker that eliminates the need for additional processes like segmentation and box refinement. Without whistles and bells, SiamMo not only surpasses state-of-the-art methods across multiple benchmarks but also demonstrates exceptional robustness in challenging scenarios. SiamMo sets a new record on the KITTI tracking benchmark with 90.1\\% precision while maintaining a high inference speed of 108 FPS. The code will be released at https://github.com/HDU-VRLab/SiamMo.",
      "authors": [
        "Yuxiang Yang",
        "Yingqi Deng",
        "Jing Zhang",
        "Hongjie Gu",
        "Zhekang Dong"
      ],
      "published": "2024-08-03T07:02:01Z",
      "updated": "2024-09-09T05:24:27Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.01688v2",
      "landing_url": "https://arxiv.org/abs/2408.01688v2",
      "doi": "https://doi.org/10.48550/arXiv.2408.01688"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on 3D object tracking and motion-centric LiDAR methods, not on Spoken Language Models or speech-language modeling; it fails the SLM core criterion and aligns with non-SLM topics, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on 3D object tracking and motion-centric LiDAR methods, not on Spoken Language Models or speech-language modeling; it fails the SLM core criterion and aligns with non-SLM topics, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on 3D object tracking using motion-centric Siamese networks with LiDAR data, which is unrelated to Spoken Language Models or speech language modeling. It does not address speech as a core modeling target nor discuss end-to-end training or evaluation related to spoken language models. Therefore, it does not meet the inclusion criteria centered on speech and spoken language modeling, and falls outside the research scope defined for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on 3D object tracking using motion-centric Siamese networks with LiDAR data, which is unrelated to Spoken Language Models or speech language modeling. It does not address speech as a core modeling target nor discuss end-to-end training or evaluation related to spoken language models. Therefore, it does not meet the inclusion criteria centered on speech and spoken language modeling, and falls outside the research scope defined for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Language Model Can Listen While Speaking",
    "abstract": "Dialogue serves as the most natural manner of human-computer interaction (HCI). Recent advancements in speech language models (SLM) have significantly enhanced speech-based conversational AI. However, these models are limited to turn-based conversation, lacking the ability to interact with humans in real-time spoken scenarios, for example, being interrupted when the generated content is not satisfactory. To address these limitations, we explore full duplex modeling (FDM) in interactive speech language models (iSLM), focusing on enhancing real-time interaction and, more explicitly, exploring the quintessential ability of interruption. We introduce a novel model design, namely listening-while-speaking language model (LSLM), an end-to-end system equipped with both listening and speaking channels. Our LSLM employs a token-based decoder-only TTS for speech generation and a streaming self-supervised learning (SSL) encoder for real-time audio input. LSLM fuses both channels for autoregressive generation and detects turn-taking in real time. Three fusion strategies -- early fusion, middle fusion, and late fusion -- are explored, with middle fusion achieving an optimal balance between speech generation and real-time interaction. Two experimental settings, command-based FDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivity to diverse instructions. Our results highlight LSLM's capability to achieve duplex communication with minimal impact on existing systems. This study aims to advance the development of interactive speech dialogue systems, enhancing their applicability in real-world contexts.",
    "metadata": {
      "arxiv_id": "2408.02622",
      "title": "Language Model Can Listen While Speaking",
      "summary": "Dialogue serves as the most natural manner of human-computer interaction (HCI). Recent advancements in speech language models (SLM) have significantly enhanced speech-based conversational AI. However, these models are limited to turn-based conversation, lacking the ability to interact with humans in real-time spoken scenarios, for example, being interrupted when the generated content is not satisfactory. To address these limitations, we explore full duplex modeling (FDM) in interactive speech language models (iSLM), focusing on enhancing real-time interaction and, more explicitly, exploring the quintessential ability of interruption. We introduce a novel model design, namely listening-while-speaking language model (LSLM), an end-to-end system equipped with both listening and speaking channels. Our LSLM employs a token-based decoder-only TTS for speech generation and a streaming self-supervised learning (SSL) encoder for real-time audio input. LSLM fuses both channels for autoregressive generation and detects turn-taking in real time. Three fusion strategies -- early fusion, middle fusion, and late fusion -- are explored, with middle fusion achieving an optimal balance between speech generation and real-time interaction. Two experimental settings, command-based FDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivity to diverse instructions. Our results highlight LSLM's capability to achieve duplex communication with minimal impact on existing systems. This study aims to advance the development of interactive speech dialogue systems, enhancing their applicability in real-world contexts.",
      "authors": [
        "Ziyang Ma",
        "Yakun Song",
        "Chenpeng Du",
        "Jian Cong",
        "Zhuo Chen",
        "Yuping Wang",
        "Yuxuan Wang",
        "Xie Chen"
      ],
      "published": "2024-08-05T16:47:22Z",
      "updated": "2024-08-05T16:47:22Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.02622v1",
      "landing_url": "https://arxiv.org/abs/2408.02622v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.02622"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on Spoken Language Models with real-time full-duplex listening and speaking, discusses architecture and evaluation, is within the 3-year window, and matches the inclusion criteria while not triggering any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on Spoken Language Models with real-time full-duplex listening and speaking, discusses architecture and evaluation, is within the 3-year window, and matches the inclusion criteria while not triggering any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study clearly focuses on Spoken Language Models with an emphasis on full-duplex interaction (listening-while-speaking) and real-time interruption capabilities, discussing model design, training, fusion strategies, and evaluation of turn-taking behavior, which matches the inclusion criteria; it also avoids all exclusion criteria such as limiting to half-duplex turn-taking or single speech technologies without interaction modeling.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study clearly focuses on Spoken Language Models with an emphasis on full-duplex interaction (listening-while-speaking) and real-time interruption capabilities, discussing model design, training, fusion strategies, and evaluation of turn-taking behavior, which matches the inclusion criteria; it also avoids all exclusion criteria such as limiting to half-duplex turn-taking or single speech technologies without interaction modeling.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Modular Architectures and Entanglement Schemes for Error-Corrected Distributed Quantum Computation",
    "abstract": "Connecting multiple smaller qubit modules by generating high-fidelity entangled states is a promising path for scaling quantum computing hardware. The performance of such a modular quantum computer is highly dependent on the quality and rate of entanglement generation. However, the optimal architectures and entanglement generation schemes are not yet established. Focusing on modular quantum computers with solid-state quantum hardware, we investigate a distributed surface code's error-correcting threshold and logical failure rate. We consider both emission-based and scattering-based entanglement generation schemes for the measurement of non-local stabilizers. Through quantum optical modeling, we link the performance of the quantum error correction code to the parameters of the underlying physical hardware and identify the necessary parameter regime for fault-tolerant modular quantum computation. In addition, we compare modular architectures with one or two data qubits per module. We find that the performance of the code depends significantly on the choice of entanglement generation scheme, while the two modular architectures have similar error-correcting thresholds. For some schemes, thresholds nearing the thresholds of non-distributed implementations ($\\sim0.4 \\%$) appear feasible with future parameters.",
    "metadata": {
      "arxiv_id": "2408.02837",
      "title": "Modular Architectures and Entanglement Schemes for Error-Corrected Distributed Quantum Computation",
      "summary": "Connecting multiple smaller qubit modules by generating high-fidelity entangled states is a promising path for scaling quantum computing hardware. The performance of such a modular quantum computer is highly dependent on the quality and rate of entanglement generation. However, the optimal architectures and entanglement generation schemes are not yet established. Focusing on modular quantum computers with solid-state quantum hardware, we investigate a distributed surface code's error-correcting threshold and logical failure rate. We consider both emission-based and scattering-based entanglement generation schemes for the measurement of non-local stabilizers. Through quantum optical modeling, we link the performance of the quantum error correction code to the parameters of the underlying physical hardware and identify the necessary parameter regime for fault-tolerant modular quantum computation. In addition, we compare modular architectures with one or two data qubits per module. We find that the performance of the code depends significantly on the choice of entanglement generation scheme, while the two modular architectures have similar error-correcting thresholds. For some schemes, thresholds nearing the thresholds of non-distributed implementations ($\\sim0.4 \\%$) appear feasible with future parameters.",
      "authors": [
        "Siddhant Singh",
        "Fenglei Gu",
        "Sébastian de Bone",
        "Eduardo Villaseñor",
        "David Elkouss",
        "Johannes Borregaard"
      ],
      "published": "2024-08-05T21:20:03Z",
      "updated": "2025-12-22T10:18:26Z",
      "categories": [
        "quant-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.02837v2",
      "landing_url": "https://arxiv.org/abs/2408.02837v2",
      "doi": "https://doi.org/10.1038/s41534-025-01146-2"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on modular quantum computing architectures and entanglement protocols, which is unrelated to Spoken Language Models or speech-based language modeling, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on modular quantum computing architectures and entanglement protocols, which is unrelated to Spoken Language Models or speech-based language modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on quantum computing architectures and entanglement schemes, which is unrelated to the research topic of Spoken Language Models or full-duplex spoken language interaction as specified in the inclusion criteria. There are no mentions of speech language modeling, spoken language models, or related concepts in the title or abstract, thus it does not meet the inclusion criteria and falls outside the domain defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on quantum computing architectures and entanglement schemes, which is unrelated to the research topic of Spoken Language Models or full-duplex spoken language interaction as specified in the inclusion criteria. There are no mentions of speech language modeling, spoken language models, or related concepts in the title or abstract, thus it does not meet the inclusion criteria and falls outside the domain defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Potamoi: Accelerating Neural Rendering via a Unified Streaming Architecture",
    "abstract": "Neural Radiance Field (NeRF) has emerged as a promising alternative for photorealistic rendering. Despite recent algorithmic advancements, achieving real-time performance on today's resource-constrained devices remains challenging. In this paper, we identify the primary bottlenecks in current NeRF algorithms and introduce a unified algorithm-architecture co-design, Potamoi, designed to accommodate various NeRF algorithms. Specifically, we introduce a runtime system featuring a plug-and-play algorithm, SpaRW, which significantly reduces the per-frame computational workload and alleviates compute inefficiencies. Furthermore, our unified streaming pipeline coupled with customized hardware support effectively tames both SRAM and DRAM inefficiencies by minimizing repetitive DRAM access and completely eliminating SRAM bank conflicts. When evaluated against a baseline utilizing a dedicated DNN accelerator, our framework demonstrates a speed-up and energy reduction of 53.1$\\times$ and 67.7$\\times$, respectively, all while maintaining high visual quality with less than a 1.0 dB reduction in peak signal-to-noise ratio.",
    "metadata": {
      "arxiv_id": "2408.06608",
      "title": "Potamoi: Accelerating Neural Rendering via a Unified Streaming Architecture",
      "summary": "Neural Radiance Field (NeRF) has emerged as a promising alternative for photorealistic rendering. Despite recent algorithmic advancements, achieving real-time performance on today's resource-constrained devices remains challenging. In this paper, we identify the primary bottlenecks in current NeRF algorithms and introduce a unified algorithm-architecture co-design, Potamoi, designed to accommodate various NeRF algorithms. Specifically, we introduce a runtime system featuring a plug-and-play algorithm, SpaRW, which significantly reduces the per-frame computational workload and alleviates compute inefficiencies. Furthermore, our unified streaming pipeline coupled with customized hardware support effectively tames both SRAM and DRAM inefficiencies by minimizing repetitive DRAM access and completely eliminating SRAM bank conflicts. When evaluated against a baseline utilizing a dedicated DNN accelerator, our framework demonstrates a speed-up and energy reduction of 53.1$\\times$ and 67.7$\\times$, respectively, all while maintaining high visual quality with less than a 1.0 dB reduction in peak signal-to-noise ratio.",
      "authors": [
        "Yu Feng",
        "Weikai Lin",
        "Zihan Liu",
        "Jingwen Leng",
        "Minyi Guo",
        "Han Zhao",
        "Xiaofeng Hou",
        "Jieru Zhao",
        "Yuhao Zhu"
      ],
      "published": "2024-08-13T03:39:44Z",
      "updated": "2024-08-13T03:39:44Z",
      "categories": [
        "cs.AR",
        "cs.GR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.06608v1",
      "landing_url": "https://arxiv.org/abs/2408.06608v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.06608"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item targets neural rendering acceleration for NeRF and hardware/software co-design, not Spoken Language Models or speech-based modeling. It does not discuss speech, dialogue, synchronization, or speech-language modeling concepts, so it fails all inclusion criteria and meets at least one exclusion criterion (non-SLM/robust interactive speech focus). Therefore, it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item targets neural rendering acceleration for NeRF and hardware/software co-design, not Spoken Language Models or speech-based modeling. It does not discuss speech, dialogue, synchronization, or speech-language modeling concepts, so it fails all inclusion criteria and meets at least one exclusion criterion (non-SLM/robust interactive speech focus). Therefore, it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on accelerating neural rendering using algorithm-architecture co-design, which is centered on visual rendering technologies (Neural Radiance Field) rather than spoken language models or speech language modeling. It does not address spoken language model architectures, training, inference or evaluation, nor does it discuss synchronous interactive speech behavior, full-duplex systems, or speech language modeling topics stipulated in the inclusion criteria. The abstract and title do not mention any spoken language model-related concepts or speech-centric modeling, and the paper is unrelated to speech sequence modeling or full-duplex spoken language systems. Therefore, it does not meet the inclusion criteria and falls under exclusion as it is unrelated to speech modeling or interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on accelerating neural rendering using algorithm-architecture co-design, which is centered on visual rendering technologies (Neural Radiance Field) rather than spoken language models or speech language modeling. It does not address spoken language model architectures, training, inference or evaluation, nor does it discuss synchronous interactive speech behavior, full-duplex systems, or speech language modeling topics stipulated in the inclusion criteria. The abstract and title do not mention any spoken language model-related concepts or speech-centric modeling, and the paper is unrelated to speech sequence modeling or full-duplex spoken language systems. Therefore, it does not meet the inclusion criteria and falls under exclusion as it is unrelated to speech modeling or interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DPSNN: Spiking Neural Network for Low-Latency Streaming Speech Enhancement",
    "abstract": "Speech enhancement (SE) improves communication in noisy environments, affecting areas such as automatic speech recognition, hearing aids, and telecommunications. With these domains typically being power-constrained and event-based while requiring low latency, neuromorphic algorithms in the form of spiking neural networks (SNNs) have great potential. Yet, current effective SNN solutions require a contextual sampling window imposing substantial latency, typically around 32ms, too long for many applications. Inspired by Dual-Path Spiking Neural Networks (DPSNNs) in classical neural networks, we develop a two-phase time-domain streaming SNN framework -- the Dual-Path Spiking Neural Network (DPSNN). In the DPSNN, the first phase uses Spiking Convolutional Neural Networks (SCNNs) to capture global contextual information, while the second phase uses Spiking Recurrent Neural Networks (SRNNs) to focus on frequency-related features. In addition, the regularizer suppresses activation to further enhance energy efficiency of our DPSNNs. Evaluating on the VCTK and Intel DNS Datasets, we demonstrate that our approach achieves the very low latency (approximately 5ms) required for applications like hearing aids, while demonstrating excellent signal-to-noise ratio (SNR), perceptual quality, and energy efficiency.",
    "metadata": {
      "arxiv_id": "2408.07388",
      "title": "DPSNN: Spiking Neural Network for Low-Latency Streaming Speech Enhancement",
      "summary": "Speech enhancement (SE) improves communication in noisy environments, affecting areas such as automatic speech recognition, hearing aids, and telecommunications. With these domains typically being power-constrained and event-based while requiring low latency, neuromorphic algorithms in the form of spiking neural networks (SNNs) have great potential. Yet, current effective SNN solutions require a contextual sampling window imposing substantial latency, typically around 32ms, too long for many applications. Inspired by Dual-Path Spiking Neural Networks (DPSNNs) in classical neural networks, we develop a two-phase time-domain streaming SNN framework -- the Dual-Path Spiking Neural Network (DPSNN). In the DPSNN, the first phase uses Spiking Convolutional Neural Networks (SCNNs) to capture global contextual information, while the second phase uses Spiking Recurrent Neural Networks (SRNNs) to focus on frequency-related features. In addition, the regularizer suppresses activation to further enhance energy efficiency of our DPSNNs. Evaluating on the VCTK and Intel DNS Datasets, we demonstrate that our approach achieves the very low latency (approximately 5ms) required for applications like hearing aids, while demonstrating excellent signal-to-noise ratio (SNR), perceptual quality, and energy efficiency.",
      "authors": [
        "Tao Sun",
        "Sander Bohté"
      ],
      "published": "2024-08-14T09:08:43Z",
      "updated": "2024-08-14T09:08:43Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "cs.NE",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.07388v1",
      "landing_url": "https://arxiv.org/abs/2408.07388v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.07388"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on low-latency speech enhancement using spiking neural networks, but it does not treat speech as a language modeling object nor discuss Spoken Language Models or FD-SLMs; therefore it does not meet the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on low-latency speech enhancement using spiking neural networks, but it does not treat speech as a language modeling object nor discuss Spoken Language Models or FD-SLMs; therefore it does not meet the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on spiking neural networks for low-latency speech enhancement, primarily addressing signal processing improvements such as noise reduction, latency, and energy efficiency. While it discusses low-latency processing in speech signals, it does not explicitly focus on Spoken Language Models (SLMs), general speech language modeling, or full-duplex spoken interaction capabilities as defined in the inclusion criteria. It lacks discussion on model architectures for speech language modeling or end-to-end training associated with spoken language models, or synchronization and interactive dialogue behavior arbitration. The evaluation criteria focus on SNR and perceptual quality, without addressing language modeling, dialogue interaction, or multi-dimensional evaluation metrics related to full-duplex speech language models. Therefore, it does not meet the core requirement of emphasizing spoken language modeling or full-duplex spoken language interactions, nor does it address the specific evaluation or architectural components required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on spiking neural networks for low-latency speech enhancement, primarily addressing signal processing improvements such as noise reduction, latency, and energy efficiency. While it discusses low-latency processing in speech signals, it does not explicitly focus on Spoken Language Models (SLMs), general speech language modeling, or full-duplex spoken interaction capabilities as defined in the inclusion criteria. It lacks discussion on model architectures for speech language modeling or end-to-end training associated with spoken language models, or synchronization and interactive dialogue behavior arbitration. The evaluation criteria focus on SNR and perceptual quality, without addressing language modeling, dialogue interaction, or multi-dimensional evaluation metrics related to full-duplex speech language models. Therefore, it does not meet the core requirement of emphasizing spoken language modeling or full-duplex spoken language interactions, nor does it address the specific evaluation or architectural components required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Navigating Data Scarcity using Foundation Models: A Benchmark of Few-Shot and Zero-Shot Learning Approaches in Medical Imaging",
    "abstract": "Data scarcity is a major limiting factor for applying modern machine learning techniques to clinical tasks. Although sufficient data exists for some well-studied medical tasks, there remains a long tail of clinically relevant tasks with poor data availability. Recently, numerous foundation models have demonstrated high suitability for few-shot learning (FSL) and zero-shot learning (ZSL), potentially making them more accessible to practitioners. However, it remains unclear which foundation model performs best on FSL medical image analysis tasks and what the optimal methods are for learning from limited data. We conducted a comprehensive benchmark study of ZSL and FSL using 16 pretrained foundation models on 19 diverse medical imaging datasets. Our results indicate that BiomedCLIP, a model pretrained exclusively on medical data, performs best on average for very small training set sizes, while very large CLIP models pretrained on LAION-2B perform best with slightly more training samples. However, simply fine-tuning a ResNet-18 pretrained on ImageNet performs similarly with more than five training examples per class. Our findings also highlight the need for further research on foundation models specifically tailored for medical applications and the collection of more datasets to train these models.",
    "metadata": {
      "arxiv_id": "2408.08058",
      "title": "Navigating Data Scarcity using Foundation Models: A Benchmark of Few-Shot and Zero-Shot Learning Approaches in Medical Imaging",
      "summary": "Data scarcity is a major limiting factor for applying modern machine learning techniques to clinical tasks. Although sufficient data exists for some well-studied medical tasks, there remains a long tail of clinically relevant tasks with poor data availability. Recently, numerous foundation models have demonstrated high suitability for few-shot learning (FSL) and zero-shot learning (ZSL), potentially making them more accessible to practitioners. However, it remains unclear which foundation model performs best on FSL medical image analysis tasks and what the optimal methods are for learning from limited data. We conducted a comprehensive benchmark study of ZSL and FSL using 16 pretrained foundation models on 19 diverse medical imaging datasets. Our results indicate that BiomedCLIP, a model pretrained exclusively on medical data, performs best on average for very small training set sizes, while very large CLIP models pretrained on LAION-2B perform best with slightly more training samples. However, simply fine-tuning a ResNet-18 pretrained on ImageNet performs similarly with more than five training examples per class. Our findings also highlight the need for further research on foundation models specifically tailored for medical applications and the collection of more datasets to train these models.",
      "authors": [
        "Stefano Woerner",
        "Christian F. Baumgartner"
      ],
      "published": "2024-08-15T09:55:51Z",
      "updated": "2024-08-15T09:55:51Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.08058v1",
      "landing_url": "https://arxiv.org/abs/2408.08058v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.08058"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on foundation models for medical imaging (FSL/ZSL) and benchmarking in vision tasks, with no reference to Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on foundation models for medical imaging (FSL/ZSL) and benchmarking in vision tasks, with no reference to Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on foundation models applied to medical imaging with an emphasis on few-shot and zero-shot learning, which is unrelated to Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs). It does not address speech sequences, acoustic signal modeling, real-time interactive speech behavior, or relevant model architectures centered on speech as outlined in the inclusion criteria. It also does not include terminology or concepts related to spoken language models in its title or abstract, and thus does not meet any inclusion criteria while clearly mismatching the thematic focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on foundation models applied to medical imaging with an emphasis on few-shot and zero-shot learning, which is unrelated to Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs). It does not address speech sequences, acoustic signal modeling, real-time interactive speech behavior, or relevant model architectures centered on speech as outlined in the inclusion criteria. It also does not include terminology or concepts related to spoken language models in its title or abstract, and thus does not meet any inclusion criteria while clearly mismatching the thematic focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Comparative Analysis of Generative Models: Enhancing Image Synthesis with VAEs, GANs, and Stable Diffusion",
    "abstract": "This paper examines three major generative modelling frameworks: Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Stable Diffusion models. VAEs are effective at learning latent representations but frequently yield blurry results. GANs can generate realistic images but face issues such as mode collapse. Stable Diffusion models, while producing high-quality images with strong semantic coherence, are demanding in terms of computational resources. Additionally, the paper explores how incorporating Grounding DINO and Grounded SAM with Stable Diffusion improves image accuracy by utilising sophisticated segmentation and inpainting techniques. The analysis guides on selecting suitable models for various applications and highlights areas for further research.",
    "metadata": {
      "arxiv_id": "2408.08751",
      "title": "Comparative Analysis of Generative Models: Enhancing Image Synthesis with VAEs, GANs, and Stable Diffusion",
      "summary": "This paper examines three major generative modelling frameworks: Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Stable Diffusion models. VAEs are effective at learning latent representations but frequently yield blurry results. GANs can generate realistic images but face issues such as mode collapse. Stable Diffusion models, while producing high-quality images with strong semantic coherence, are demanding in terms of computational resources. Additionally, the paper explores how incorporating Grounding DINO and Grounded SAM with Stable Diffusion improves image accuracy by utilising sophisticated segmentation and inpainting techniques. The analysis guides on selecting suitable models for various applications and highlights areas for further research.",
      "authors": [
        "Sanchayan Vivekananthan"
      ],
      "published": "2024-08-16T13:50:50Z",
      "updated": "2024-08-16T13:50:50Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.08751v1",
      "landing_url": "https://arxiv.org/abs/2408.08751v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.08751"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本研究聚焦影像生成模型（VAEs、GANs、Stable Diffusion）及其分割/修補增強，未涉及語音/ Spoken Language Models 或語音語言建模；因此不符合主題核心，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本研究聚焦影像生成模型（VAEs、GANs、Stable Diffusion）及其分割/修補增強，未涉及語音/ Spoken Language Models 或語音語言建模；因此不符合主題核心，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on generative image models such as VAEs, GANs, and Stable Diffusion, without any mention of spoken language models, speech language modeling, full-duplex spoken language interaction, or the other inclusion criteria related to speech and spoken language processing as specified. The study neither addresses speech-based modeling nor synchronization or interaction in speech streams, so it does not meet any inclusion criteria and instead falls fully outside the defined research scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on generative image models such as VAEs, GANs, and Stable Diffusion, without any mention of spoken language models, speech language modeling, full-duplex spoken language interaction, or the other inclusion criteria related to speech and spoken language processing as specified. The study neither addresses speech-based modeling nor synchronization or interaction in speech streams, so it does not meet any inclusion criteria and instead falls fully outside the defined research scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Efficient Autoregressive Audio Modeling via Next-Scale Prediction",
    "abstract": "Audio generation has achieved remarkable progress with the advance of sophisticated generative models, such as diffusion models (DMs) and autoregressive (AR) models. However, due to the naturally significant sequence length of audio, the efficiency of audio generation remains an essential issue to be addressed, especially for AR models that are incorporated in large language models (LLMs). In this paper, we analyze the token length of audio tokenization and propose a novel \\textbf{S}cale-level \\textbf{A}udio \\textbf{T}okenizer (SAT), with improved residual quantization. Based on SAT, a scale-level \\textbf{A}coustic \\textbf{A}uto\\textbf{R}egressive (AAR) modeling framework is further proposed, which shifts the next-token AR prediction to next-scale AR prediction, significantly reducing the training cost and inference time. To validate the effectiveness of the proposed approach, we comprehensively analyze design choices and demonstrate the proposed AAR framework achieves a remarkable \\textbf{35}$\\times$ faster inference speed and +\\textbf{1.33} Fréchet Audio Distance (FAD) against baselines on the AudioSet benchmark. Code: \\url{https://github.com/qiuk2/AAR}.",
    "metadata": {
      "arxiv_id": "2408.09027",
      "title": "Efficient Autoregressive Audio Modeling via Next-Scale Prediction",
      "summary": "Audio generation has achieved remarkable progress with the advance of sophisticated generative models, such as diffusion models (DMs) and autoregressive (AR) models. However, due to the naturally significant sequence length of audio, the efficiency of audio generation remains an essential issue to be addressed, especially for AR models that are incorporated in large language models (LLMs). In this paper, we analyze the token length of audio tokenization and propose a novel \\textbf{S}cale-level \\textbf{A}udio \\textbf{T}okenizer (SAT), with improved residual quantization. Based on SAT, a scale-level \\textbf{A}coustic \\textbf{A}uto\\textbf{R}egressive (AAR) modeling framework is further proposed, which shifts the next-token AR prediction to next-scale AR prediction, significantly reducing the training cost and inference time. To validate the effectiveness of the proposed approach, we comprehensively analyze design choices and demonstrate the proposed AAR framework achieves a remarkable \\textbf{35}$\\times$ faster inference speed and +\\textbf{1.33} Fréchet Audio Distance (FAD) against baselines on the AudioSet benchmark. Code: \\url{https://github.com/qiuk2/AAR}.",
      "authors": [
        "Kai Qiu",
        "Xiang Li",
        "Hao Chen",
        "Jie Sun",
        "Jinglu Wang",
        "Zhe Lin",
        "Marios Savvides",
        "Bhiksha Raj"
      ],
      "published": "2024-08-16T21:48:53Z",
      "updated": "2024-12-16T21:50:56Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.09027v2",
      "landing_url": "https://arxiv.org/abs/2408.09027v2",
      "doi": "https://doi.org/10.48550/arXiv.2408.09027"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on scalable autoregressive audio generation and tokenization (SAT/AAR) with speed improvements on AudioSet, but it does not present Spoken Language Models or a core focus on speech as the modeling object, nor does it address full-duplex conversational interaction, synchronization, or dialogue-level evaluation required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on scalable autoregressive audio generation and tokenization (SAT/AAR) with speed improvements on AudioSet, but it does not present Spoken Language Models or a core focus on speech as the modeling object, nor does it address full-duplex conversational interaction, synchronization, or dialogue-level evaluation required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on autoregressive audio modeling with a novel tokenizer and modeling framework aimed at efficient audio token prediction and generation, demonstrating technical advancements in audio sequence modeling and generation efficiency. However, it does not explicitly discuss Spoken Language Models (SLMs) or full-duplex spoken language interactive systems, nor does it mention language modeling of speech or synchronous dialogue behaviors as defined in the inclusion criteria. The focus appears more on general audio generation and efficiency rather than on speech-centric language modeling or interactive behaviors relevant to full-duplex spoken language models. Therefore, it does not meet the core inclusion criteria of focusing on spoken language modeling or related interactive multi-turn dialogue capabilities.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on autoregressive audio modeling with a novel tokenizer and modeling framework aimed at efficient audio token prediction and generation, demonstrating technical advancements in audio sequence modeling and generation efficiency. However, it does not explicitly discuss Spoken Language Models (SLMs) or full-duplex spoken language interactive systems, nor does it mention language modeling of speech or synchronous dialogue behaviors as defined in the inclusion criteria. The focus appears more on general audio generation and efficiency rather than on speech-centric language modeling or interactive behaviors relevant to full-duplex spoken language models. Therefore, it does not meet the core inclusion criteria of focusing on spoken language modeling or related interactive multi-turn dialogue capabilities.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "GSTran: Joint Geometric and Semantic Coherence for Point Cloud Segmentation",
    "abstract": "Learning meaningful local and global information remains a challenge in point cloud segmentation tasks. When utilizing local information, prior studies indiscriminately aggregates neighbor information from different classes to update query points, potentially compromising the distinctive feature of query points. In parallel, inaccurate modeling of long-distance contextual dependencies when utilizing global information can also impact model performance. To address these issues, we propose GSTran, a novel transformer network tailored for the segmentation task. The proposed network mainly consists of two principal components: a local geometric transformer and a global semantic transformer. In the local geometric transformer module, we explicitly calculate the geometric disparity within the local region. This enables amplifying the affinity with geometrically similar neighbor points while suppressing the association with other neighbors. In the global semantic transformer module, we design a multi-head voting strategy. This strategy evaluates semantic similarity across the entire spatial range, facilitating the precise capture of contextual dependencies. Experiments on ShapeNetPart and S3DIS benchmarks demonstrate the effectiveness of the proposed method, showing its superiority over other algorithms. The code is available at https://github.com/LAB123-tech/GSTran.",
    "metadata": {
      "arxiv_id": "2408.11558",
      "title": "GSTran: Joint Geometric and Semantic Coherence for Point Cloud Segmentation",
      "summary": "Learning meaningful local and global information remains a challenge in point cloud segmentation tasks. When utilizing local information, prior studies indiscriminately aggregates neighbor information from different classes to update query points, potentially compromising the distinctive feature of query points. In parallel, inaccurate modeling of long-distance contextual dependencies when utilizing global information can also impact model performance. To address these issues, we propose GSTran, a novel transformer network tailored for the segmentation task. The proposed network mainly consists of two principal components: a local geometric transformer and a global semantic transformer. In the local geometric transformer module, we explicitly calculate the geometric disparity within the local region. This enables amplifying the affinity with geometrically similar neighbor points while suppressing the association with other neighbors. In the global semantic transformer module, we design a multi-head voting strategy. This strategy evaluates semantic similarity across the entire spatial range, facilitating the precise capture of contextual dependencies. Experiments on ShapeNetPart and S3DIS benchmarks demonstrate the effectiveness of the proposed method, showing its superiority over other algorithms. The code is available at https://github.com/LAB123-tech/GSTran.",
      "authors": [
        "Abiao Li",
        "Chenlei Lv",
        "Guofeng Mei",
        "Yifan Zuo",
        "Jian Zhang",
        "Yuming Fang"
      ],
      "published": "2024-08-21T12:12:37Z",
      "updated": "2024-08-21T12:12:37Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.11558v1",
      "landing_url": "https://arxiv.org/abs/2408.11558v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.11558"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on a transformer-based method for 3D point cloud segmentation (GSTran) and does not involve Spoken Language Models or speech-language modeling, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on a transformer-based method for 3D point cloud segmentation (GSTran) and does not involve Spoken Language Models or speech-language modeling, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on point cloud segmentation using transformers and does not address spoken language models, speech language modeling, or full-duplex spoken interaction systems as defined by the inclusion criteria. It lacks any mention of speech signals, linguistic modeling of speech, or related evaluation metrics relevant to synchronous speech interaction, which are core requirements for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on point cloud segmentation using transformers and does not address spoken language models, speech language modeling, or full-duplex spoken interaction systems as defined by the inclusion criteria. It lacks any mention of speech signals, linguistic modeling of speech, or related evaluation metrics relevant to synchronous speech interaction, which are core requirements for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Non-Causal to Causal SSL-Supported Transfer Learning: Towards a High-Performance Low-Latency Speech Vocoder",
    "abstract": "Recently, BigVGAN has emerged as high-performance speech vocoder. Its sequence-to-sequence-based synthesis, however, prohibits usage in low-latency conversational applications. Our work addresses this shortcoming in three steps. First, we introduce low latency into BigVGAN via implementing causal convolutions, yielding decreased performance. Second, to regain performance, we propose a teacher-student transfer learning scheme to distill the high-delay non-causal BigVGAN into our low-latency causal vocoder. Third, taking advantage of a self-supervised learning (SSL) model, in our case wav2vec 2.0, we align its encoder speech representations extracted from our low-latency causal vocoder to the ground truth ones. In speaker-independent settings, both proposed training schemes notably elevate the performance of our low-latency vocoder, closing up to the original high-delay BigVGAN. At only 21% higher complexity, our best small causal vocoder achieves 3.96 PESQ and 1.25 MCD, excelling even the original small non-causal BigVGAN (3.64 PESQ) by 0.32 PESQ and 0.1 MCD points, respectively.",
    "metadata": {
      "arxiv_id": "2408.11842",
      "title": "Non-Causal to Causal SSL-Supported Transfer Learning: Towards a High-Performance Low-Latency Speech Vocoder",
      "summary": "Recently, BigVGAN has emerged as high-performance speech vocoder. Its sequence-to-sequence-based synthesis, however, prohibits usage in low-latency conversational applications. Our work addresses this shortcoming in three steps. First, we introduce low latency into BigVGAN via implementing causal convolutions, yielding decreased performance. Second, to regain performance, we propose a teacher-student transfer learning scheme to distill the high-delay non-causal BigVGAN into our low-latency causal vocoder. Third, taking advantage of a self-supervised learning (SSL) model, in our case wav2vec 2.0, we align its encoder speech representations extracted from our low-latency causal vocoder to the ground truth ones. In speaker-independent settings, both proposed training schemes notably elevate the performance of our low-latency vocoder, closing up to the original high-delay BigVGAN. At only 21% higher complexity, our best small causal vocoder achieves 3.96 PESQ and 1.25 MCD, excelling even the original small non-causal BigVGAN (3.64 PESQ) by 0.32 PESQ and 0.1 MCD points, respectively.",
      "authors": [
        "Renzheng Shi",
        "Andreas Bär",
        "Marvin Sach",
        "Wouter Tirry",
        "Tim Fingscheidt"
      ],
      "published": "2024-08-07T12:49:40Z",
      "updated": "2024-08-26T12:01:07Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.11842v2",
      "landing_url": "https://arxiv.org/abs/2408.11842v2",
      "doi": "https://doi.org/10.48550/arXiv.2408.11842"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on a low-latency speech vocoder with SSL-based transfer learning, not on Spoken Language Models or full-duplex interactive speech modeling; it centers on vocoder architectures and objective speech quality metrics (PESQ/MCD) rather than speech-language modeling tasks or turn-taking interactivity, thus it does not meet the core inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work focuses on a low-latency speech vocoder with SSL-based transfer learning, not on Spoken Language Models or full-duplex interactive speech modeling; it centers on vocoder architectures and objective speech quality metrics (PESQ/MCD) rather than speech-language modeling tasks or turn-taking interactivity, thus it does not meet the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving a speech vocoder with an emphasis on low-latency performance, which is relevant to real-time speech synthesis and conversational applications; however, it does not explicitly involve spoken language models, speech language modeling, or full-duplex spoken language modeling architectures as required by the inclusion criteria. It mainly addresses a speech synthesis component (vocoder) without discussing general spoken language modeling, synchronization, or multi-turn interaction paradigms, and does not include evaluation related to turn-taking, cognitive parallelism, or speech overlap management. Therefore, it fails to meet the key inclusion requirements and partially falls within exclusion criteria focusing solely on a single speech technology (speech vocoder) without integrating language modeling or dialog interaction aspects.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving a speech vocoder with an emphasis on low-latency performance, which is relevant to real-time speech synthesis and conversational applications; however, it does not explicitly involve spoken language models, speech language modeling, or full-duplex spoken language modeling architectures as required by the inclusion criteria. It mainly addresses a speech synthesis component (vocoder) without discussing general spoken language modeling, synchronization, or multi-turn interaction paradigms, and does not include evaluation related to turn-taking, cognitive parallelism, or speech overlap management. Therefore, it fails to meet the key inclusion requirements and partially falls within exclusion criteria focusing solely on a single speech technology (speech vocoder) without integrating language modeling or dialog interaction aspects.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Long-term Pre-training for Temporal Action Detection with Transformers",
    "abstract": "Temporal action detection (TAD) is challenging, yet fundamental for real-world video applications. Recently, DETR-based models for TAD have been prevailing thanks to their unique benefits. However, transformers demand a huge dataset, and unfortunately data scarcity in TAD causes a severe degeneration. In this paper, we identify two crucial problems from data scarcity: attention collapse and imbalanced performance. To this end, we propose a new pre-training strategy, Long-Term Pre-training (LTP), tailored for transformers. LTP has two main components: 1) class-wise synthesis, 2) long-term pretext tasks. Firstly, we synthesize long-form video features by merging video snippets of a target class and non-target classes. They are analogous to untrimmed data used in TAD, despite being created from trimmed data. In addition, we devise two types of long-term pretext tasks to learn long-term dependency. They impose long-term conditions such as finding second-to-fourth or short-duration actions. Our extensive experiments show state-of-the-art performances in DETR-based methods on ActivityNet-v1.3 and THUMOS14 by a large margin. Moreover, we demonstrate that LTP significantly relieves the data scarcity issues in TAD.",
    "metadata": {
      "arxiv_id": "2408.13152",
      "title": "Long-term Pre-training for Temporal Action Detection with Transformers",
      "summary": "Temporal action detection (TAD) is challenging, yet fundamental for real-world video applications. Recently, DETR-based models for TAD have been prevailing thanks to their unique benefits. However, transformers demand a huge dataset, and unfortunately data scarcity in TAD causes a severe degeneration. In this paper, we identify two crucial problems from data scarcity: attention collapse and imbalanced performance. To this end, we propose a new pre-training strategy, Long-Term Pre-training (LTP), tailored for transformers. LTP has two main components: 1) class-wise synthesis, 2) long-term pretext tasks. Firstly, we synthesize long-form video features by merging video snippets of a target class and non-target classes. They are analogous to untrimmed data used in TAD, despite being created from trimmed data. In addition, we devise two types of long-term pretext tasks to learn long-term dependency. They impose long-term conditions such as finding second-to-fourth or short-duration actions. Our extensive experiments show state-of-the-art performances in DETR-based methods on ActivityNet-v1.3 and THUMOS14 by a large margin. Moreover, we demonstrate that LTP significantly relieves the data scarcity issues in TAD.",
      "authors": [
        "Jihwan Kim",
        "Miso Lee",
        "Jae-Pil Heo"
      ],
      "published": "2024-08-23T15:20:53Z",
      "updated": "2024-09-09T16:16:19Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.13152v2",
      "landing_url": "https://arxiv.org/abs/2408.13152v2",
      "doi": "https://doi.org/10.48550/arXiv.2408.13152"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper addresses video temporal action detection and pre-training for transformers, not Spoken Language Models or speech-centric modeling, so it does not meet the inclusion criteria focused on speech-language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper addresses video temporal action detection and pre-training for transformers, not Spoken Language Models or speech-centric modeling, so it does not meet the inclusion criteria focused on speech-language modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on temporal action detection in videos using transformers and does not address spoken language models, speech language modeling, or full-duplex spoken language interaction. The core of the research is video-based temporal action detection rather than modeling or reasoning about speech signals, and it lacks discussion about model architectures or evaluation metrics pertinent to spoken language or synchronised speech interaction as specified in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on temporal action detection in videos using transformers and does not address spoken language models, speech language modeling, or full-duplex spoken language interaction. The core of the research is video-based temporal action detection rather than modeling or reasoning about speech signals, and it lacks discussion about model architectures or evaluation metrics pertinent to spoken language or synchronised speech interaction as specified in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "StreamAAD: Decoding Spatial Auditory Attention with a Streaming Architecture",
    "abstract": "In this paper, we present our approach for the Track 1 of the Chinese Auditory Attention Decoding (Chinese AAD) Challenge at ISCSLP 2024. Most existing spatial auditory attention decoding (Sp-AAD) methods employ an isolated window architecture, focusing solely on global invariant features without considering relationships between different decision windows, which can lead to suboptimal performance. To address this issue, we propose a novel streaming decoding architecture, termed StreamAAD. In StreamAAD, decision windows are input to the network as a sequential stream and decoded in order, allowing for the modeling of inter-window relationships. Additionally, we employ a model ensemble strategy, achieving significant better performance than the baseline, ranking First in the challenge.",
    "metadata": {
      "arxiv_id": "2408.13522",
      "title": "StreamAAD: Decoding Spatial Auditory Attention with a Streaming Architecture",
      "summary": "In this paper, we present our approach for the Track 1 of the Chinese Auditory Attention Decoding (Chinese AAD) Challenge at ISCSLP 2024. Most existing spatial auditory attention decoding (Sp-AAD) methods employ an isolated window architecture, focusing solely on global invariant features without considering relationships between different decision windows, which can lead to suboptimal performance. To address this issue, we propose a novel streaming decoding architecture, termed StreamAAD. In StreamAAD, decision windows are input to the network as a sequential stream and decoded in order, allowing for the modeling of inter-window relationships. Additionally, we employ a model ensemble strategy, achieving significant better performance than the baseline, ranking First in the challenge.",
      "authors": [
        "Zelin Qiu",
        "Dingding Yao",
        "Junfeng Li"
      ],
      "published": "2024-08-24T08:54:26Z",
      "updated": "2024-08-24T08:54:26Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.13522v1",
      "landing_url": "https://arxiv.org/abs/2408.13522v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.13522"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item targets spatial auditory attention decoding in a streaming audio processing context and does not center on Spoken Language Models or speech-language modeling (no LM-based speech modeling, end-to-end speech-LLM, or dialogue arbitration), so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item targets spatial auditory attention decoding in a streaming audio processing context and does not center on Spoken Language Models or speech-language modeling (no LM-based speech modeling, end-to-end speech-LLM, or dialogue arbitration), so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on spatial auditory attention decoding with a novel streaming architecture, which is primarily about decoding auditory attention signals rather than spoken language modeling, full-duplex spoken language interaction, or language modeling of speech sequences as required. It does not mention spoken language models, speech language models, or related architectures focusing on real-time turn-taking or cognitive parallelism, nor does it address synchronization, low latency interaction, or behavior arbitration. Hence, it does not meet the inclusion criteria centered on spoken language models and related research themes, nor does it explicitly satisfy the evaluation or architecture requirements specified.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on spatial auditory attention decoding with a novel streaming architecture, which is primarily about decoding auditory attention signals rather than spoken language modeling, full-duplex spoken language interaction, or language modeling of speech sequences as required. It does not mention spoken language models, speech language models, or related architectures focusing on real-time turn-taking or cognitive parallelism, nor does it address synchronization, low latency interaction, or behavior arbitration. Hence, it does not meet the inclusion criteria centered on spoken language models and related research themes, nor does it explicitly satisfy the evaluation or architecture requirements specified.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "RoboSense: Large-scale Dataset and Benchmark for Egocentric Robot Perception and Navigation in Crowded and Unstructured Environments",
    "abstract": "Reliable embodied perception from an egocentric perspective is challenging yet essential for autonomous navigation technology of intelligent mobile agents. With the growing demand of social robotics, near-field scene understanding becomes an important research topic in the areas of egocentric perceptual tasks related to navigation in both crowded and unstructured environments. Due to the complexity of environmental conditions and difficulty of surrounding obstacles owing to truncation and occlusion, the perception capability under this circumstance is still inferior. To further enhance the intelligence of mobile robots, in this paper, we setup an egocentric multi-sensor data collection platform based on 3 main types of sensors (Camera, LiDAR and Fisheye), which supports flexible sensor configurations to enable dynamic sight of view from ego-perspective, capturing either near or farther areas. Meanwhile, a large-scale multimodal dataset is constructed, named RoboSense, to facilitate egocentric robot perception. Specifically, RoboSense contains more than 133K synchronized data with 1.4M 3D bounding box and IDs annotated in the full $360^{\\circ}$ view, forming 216K trajectories across 7.6K temporal sequences. It has $270\\times$ and $18\\times$ as many annotations of surrounding obstacles within near ranges as the previous datasets collected for autonomous driving scenarios such as KITTI and nuScenes. Moreover, we define a novel matching criterion for near-field 3D perception and prediction metrics. Based on RoboSense, we formulate 6 popular tasks to facilitate the future research development, where the detailed analysis as well as benchmarks are also provided accordingly. Data desensitization measures have been conducted for privacy protection.",
    "metadata": {
      "arxiv_id": "2408.15503",
      "title": "RoboSense: Large-scale Dataset and Benchmark for Egocentric Robot Perception and Navigation in Crowded and Unstructured Environments",
      "summary": "Reliable embodied perception from an egocentric perspective is challenging yet essential for autonomous navigation technology of intelligent mobile agents. With the growing demand of social robotics, near-field scene understanding becomes an important research topic in the areas of egocentric perceptual tasks related to navigation in both crowded and unstructured environments. Due to the complexity of environmental conditions and difficulty of surrounding obstacles owing to truncation and occlusion, the perception capability under this circumstance is still inferior. To further enhance the intelligence of mobile robots, in this paper, we setup an egocentric multi-sensor data collection platform based on 3 main types of sensors (Camera, LiDAR and Fisheye), which supports flexible sensor configurations to enable dynamic sight of view from ego-perspective, capturing either near or farther areas. Meanwhile, a large-scale multimodal dataset is constructed, named RoboSense, to facilitate egocentric robot perception. Specifically, RoboSense contains more than 133K synchronized data with 1.4M 3D bounding box and IDs annotated in the full $360^{\\circ}$ view, forming 216K trajectories across 7.6K temporal sequences. It has $270\\times$ and $18\\times$ as many annotations of surrounding obstacles within near ranges as the previous datasets collected for autonomous driving scenarios such as KITTI and nuScenes. Moreover, we define a novel matching criterion for near-field 3D perception and prediction metrics. Based on RoboSense, we formulate 6 popular tasks to facilitate the future research development, where the detailed analysis as well as benchmarks are also provided accordingly. Data desensitization measures have been conducted for privacy protection.",
      "authors": [
        "Haisheng Su",
        "Feixiang Song",
        "Cong Ma",
        "Wei Wu",
        "Junchi Yan"
      ],
      "published": "2024-08-28T03:17:40Z",
      "updated": "2025-03-05T05:14:34Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.15503v5",
      "landing_url": "https://arxiv.org/abs/2408.15503v5",
      "doi": "https://doi.org/10.48550/arXiv.2408.15503"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about a large-scale dataset for egocentric robot perception and navigation, not about Spoken Language Models or any speech-language modeling concepts, thus does not meet the inclusion criteria and clearly falls under exclusion criteria for topics centered on speech interaction or language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about a large-scale dataset for egocentric robot perception and navigation, not about Spoken Language Models or any speech-language modeling concepts, thus does not meet the inclusion criteria and clearly falls under exclusion criteria for topics centered on speech interaction or language modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a large-scale dataset and benchmark for egocentric robot perception and navigation, which is unrelated to Spoken Language Models or speech language modeling; it neither discusses speech-based modeling, full-duplex speech interaction, nor related evaluation metrics, thus it does not meet the inclusion criteria and falls under exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a large-scale dataset and benchmark for egocentric robot perception and navigation, which is unrelated to Spoken Language Models or speech language modeling; it neither discusses speech-based modeling, full-duplex speech interaction, nor related evaluation metrics, thus it does not meet the inclusion criteria and falls under exclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Merging and Splitting Diffusion Paths for Semantically Coherent Panoramas",
    "abstract": "Diffusion models have become the State-of-the-Art for text-to-image generation, and increasing research effort has been dedicated to adapting the inference process of pretrained diffusion models to achieve zero-shot capabilities. An example is the generation of panorama images, which has been tackled in recent works by combining independent diffusion paths over overlapping latent features, which is referred to as joint diffusion, obtaining perceptually aligned panoramas. However, these methods often yield semantically incoherent outputs and trade-off diversity for uniformity. To overcome this limitation, we propose the Merge-Attend-Diffuse operator, which can be plugged into different types of pretrained diffusion models used in a joint diffusion setting to improve the perceptual and semantical coherence of the generated panorama images. Specifically, we merge the diffusion paths, reprogramming self- and cross-attention to operate on the aggregated latent space. Extensive quantitative and qualitative experimental analysis, together with a user study, demonstrate that our method maintains compatibility with the input prompt and visual quality of the generated images while increasing their semantic coherence. We release the code at https://github.com/aimagelab/MAD.",
    "metadata": {
      "arxiv_id": "2408.15660",
      "title": "Merging and Splitting Diffusion Paths for Semantically Coherent Panoramas",
      "summary": "Diffusion models have become the State-of-the-Art for text-to-image generation, and increasing research effort has been dedicated to adapting the inference process of pretrained diffusion models to achieve zero-shot capabilities. An example is the generation of panorama images, which has been tackled in recent works by combining independent diffusion paths over overlapping latent features, which is referred to as joint diffusion, obtaining perceptually aligned panoramas. However, these methods often yield semantically incoherent outputs and trade-off diversity for uniformity. To overcome this limitation, we propose the Merge-Attend-Diffuse operator, which can be plugged into different types of pretrained diffusion models used in a joint diffusion setting to improve the perceptual and semantical coherence of the generated panorama images. Specifically, we merge the diffusion paths, reprogramming self- and cross-attention to operate on the aggregated latent space. Extensive quantitative and qualitative experimental analysis, together with a user study, demonstrate that our method maintains compatibility with the input prompt and visual quality of the generated images while increasing their semantic coherence. We release the code at https://github.com/aimagelab/MAD.",
      "authors": [
        "Fabio Quattrini",
        "Vittorio Pippi",
        "Silvia Cascianelli",
        "Rita Cucchiara"
      ],
      "published": "2024-08-28T09:22:32Z",
      "updated": "2024-08-28T09:22:32Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.15660v1",
      "landing_url": "https://arxiv.org/abs/2408.15660v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.15660"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item deals with diffusion-based image panorama generation and does not involve Spoken Language Models, speech, or dialogue synchronization, hence it does not meet the inclusion criteria for speech-language modeling projects.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item deals with diffusion-based image panorama generation and does not involve Spoken Language Models, speech, or dialogue synchronization, hence it does not meet the inclusion criteria for speech-language modeling projects.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on diffusion models for generating panorama images and improving semantic coherence in visual outputs, with no mention of spoken language models, speech-based modeling, full-duplex interaction, or spoken language understanding/training/evaluation. The core research is on image generation, unrelated to speech or spoken language modeling as required by the inclusion criteria, and none of the inclusion criteria about spoken language models or synchronous speech interactions are met. Therefore, this study falls outside the scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on diffusion models for generating panorama images and improving semantic coherence in visual outputs, with no mention of spoken language models, speech-based modeling, full-duplex interaction, or spoken language understanding/training/evaluation. The core research is on image generation, unrelated to speech or spoken language modeling as required by the inclusion criteria, and none of the inclusion criteria about spoken language models or synchronous speech interactions are met. Therefore, this study falls outside the scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Enabling Beam Search for Language Model-Based Text-to-Speech Synthesis",
    "abstract": "Tokenising continuous speech into sequences of discrete tokens and modelling them with language models (LMs) has led to significant success in text-to-speech (TTS) synthesis. Although these models can generate speech with high quality and naturalness, their synthesised samples can still suffer from artefacts, mispronunciation, word repeating, etc. In this paper, we argue these undesirable properties could partly be caused by the randomness of sampling-based strategies during the autoregressive decoding of LMs. Therefore, we look at maximisation-based decoding approaches and propose Temporal Repetition Aware Diverse Beam Search (TRAD-BS) to find the most probable sequences of the generated speech tokens. Experiments with two state-of-the-art LM-based TTS models demonstrate that our proposed maximisation-based decoding strategy generates speech with fewer mispronunciations and improved speaker consistency.",
    "metadata": {
      "arxiv_id": "2408.16373",
      "title": "Enabling Beam Search for Language Model-Based Text-to-Speech Synthesis",
      "summary": "Tokenising continuous speech into sequences of discrete tokens and modelling them with language models (LMs) has led to significant success in text-to-speech (TTS) synthesis. Although these models can generate speech with high quality and naturalness, their synthesised samples can still suffer from artefacts, mispronunciation, word repeating, etc. In this paper, we argue these undesirable properties could partly be caused by the randomness of sampling-based strategies during the autoregressive decoding of LMs. Therefore, we look at maximisation-based decoding approaches and propose Temporal Repetition Aware Diverse Beam Search (TRAD-BS) to find the most probable sequences of the generated speech tokens. Experiments with two state-of-the-art LM-based TTS models demonstrate that our proposed maximisation-based decoding strategy generates speech with fewer mispronunciations and improved speaker consistency.",
      "authors": [
        "Zehai Tu",
        "Guangyan Zhang",
        "Yiting Lu",
        "Adaeze Adigwe",
        "Simon King",
        "Yiwen Guo"
      ],
      "published": "2024-08-29T09:31:06Z",
      "updated": "2024-08-29T09:31:06Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.16373v1",
      "landing_url": "https://arxiv.org/abs/2408.16373v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.16373"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "符合包含條件：核心聚焦於 Spoken Language Models / Speech Language Models 的語音建模，將語音視為主要建模對象，且在標題/摘要中明示使用 language model 於 TTS；時間範圍在近三年內（2025 年的 arXiv 條目），且未涉及排除項如僅限單向/ASR、僅文本 NLP 等。故應納入。",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "符合包含條件：核心聚焦於 Spoken Language Models / Speech Language Models 的語音建模，將語音視為主要建模對象，且在標題/摘要中明示使用 language model 於 TTS；時間範圍在近三年內（2025 年的 arXiv 條目），且未涉及排除項如僅限單向/ASR、僅文本 NLP 等。故應納入。",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving decoding strategies in language model-based text-to-speech synthesis, primarily targeting sound quality issues like mispronunciation and repetition, without explicitly discussing Spoken Language Models (SLMs), full-duplex interaction, or synchronization aspects central to the inclusion criteria; it also does not address interaction, behavior arbitration, or multi-dimensional evaluation in the context of full-duplex spoken language modeling, thus failing to meet key inclusion points and matching several exclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving decoding strategies in language model-based text-to-speech synthesis, primarily targeting sound quality issues like mispronunciation and repetition, without explicitly discussing Spoken Language Models (SLMs), full-duplex interaction, or synchronization aspects central to the inclusion criteria; it also does not address interaction, behavior arbitration, or multi-dimensional evaluation in the context of full-duplex spoken language modeling, thus failing to meet key inclusion points and matching several exclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers disagree: JuniorNano argued inclusion because the paper models speech as discrete tokens with language models for TTS, while JuniorMini argued exclusion because the work focuses on TTS decoding and does not address spoken-language modeling for interactive/full‑duplex behavior or synchronization. After review, although the paper uses language models over speech tokens (language modeling of speech) it is narrowly focused on TTS decoding (beam search) and audio-quality/artifact reduction, and does not address core inclusion requirements such as Spoken Language Models for interactive/full‑duplex behavior, synchronization, or multi-dimensional evaluation; this matches the exclusion criterion that studies focusing solely on TTS (a single downstream speech task) without covering interaction/synchronization/SLM perspectives should be excluded. Therefore I score it as better to exclude.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers disagree: JuniorNano argued inclusion because the paper models speech as discrete tokens with language models for TTS, while JuniorMini argued exclusion because the work focuses on TTS decoding and does not address spoken-language modeling for interactive/full‑duplex behavior or synchronization. After review, although the paper uses language models over speech tokens (language modeling of speech) it is narrowly focused on TTS decoding (beam search) and audio-quality/artifact reduction, and does not address core inclusion requirements such as Spoken Language Models for interactive/full‑duplex behavior, synchronization, or multi-dimensional evaluation; this matches the exclusion criterion that studies focusing solely on TTS (a single downstream speech task) without covering interaction/synchronization/SLM perspectives should be excluded. Therefore I score it as better to exclude.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming",
    "abstract": "Recent advances in language models have achieved significant progress. GPT-4o, as a new milestone, has enabled real-time conversations with humans, demonstrating near-human natural fluency. Such human-computer interaction necessitates models with the capability to perform reasoning directly with the audio modality and generate output in streaming. However, this remains beyond the reach of current academic models, as they typically depend on extra TTS systems for speech synthesis, resulting in undesirable latency. This paper introduces the Mini-Omni, an audio-based end-to-end conversational model, capable of real-time speech interaction. To achieve this capability, we propose a text-instructed speech generation method, along with batch-parallel strategies during inference to further boost the performance. Our method also helps to retain the original model's language capabilities with minimal degradation, enabling other works to establish real-time interaction capabilities. We call this training method \"Any Model Can Talk\". We also introduce the VoiceAssistant-400K dataset to fine-tune models optimized for speech output. To our best knowledge, Mini-Omni is the first fully end-to-end, open-source model for real-time speech interaction, offering valuable potential for future research.",
    "metadata": {
      "arxiv_id": "2408.16725",
      "title": "Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming",
      "summary": "Recent advances in language models have achieved significant progress. GPT-4o, as a new milestone, has enabled real-time conversations with humans, demonstrating near-human natural fluency. Such human-computer interaction necessitates models with the capability to perform reasoning directly with the audio modality and generate output in streaming. However, this remains beyond the reach of current academic models, as they typically depend on extra TTS systems for speech synthesis, resulting in undesirable latency. This paper introduces the Mini-Omni, an audio-based end-to-end conversational model, capable of real-time speech interaction. To achieve this capability, we propose a text-instructed speech generation method, along with batch-parallel strategies during inference to further boost the performance. Our method also helps to retain the original model's language capabilities with minimal degradation, enabling other works to establish real-time interaction capabilities. We call this training method \"Any Model Can Talk\". We also introduce the VoiceAssistant-400K dataset to fine-tune models optimized for speech output. To our best knowledge, Mini-Omni is the first fully end-to-end, open-source model for real-time speech interaction, offering valuable potential for future research.",
      "authors": [
        "Zhifei Xie",
        "Changqiao Wu"
      ],
      "published": "2024-08-29T17:18:53Z",
      "updated": "2024-11-05T02:24:18Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.16725v3",
      "landing_url": "https://arxiv.org/abs/2408.16725v3",
      "doi": "https://doi.org/10.48550/arXiv.2408.16725"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item describes Mini-Omni, an end-to-end audio-based conversational model capable of real-time streaming speech input/output and reasoning over audio, aligning with the Spoken Language Model/FD-SLM focus, and it discusses architecture/training/inference rather than isolated ASR/TTS, within the past 3 years.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This item describes Mini-Omni, an end-to-end audio-based conversational model capable of real-time streaming speech input/output and reasoning over audio, aligning with the Spoken Language Model/FD-SLM focus, and it discusses architecture/training/inference rather than isolated ASR/TTS, within the past 3 years.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents Mini-Omni, an end-to-end conversational model capable of real-time speech interaction, focusing on streaming speech generation and simultaneous hearing and talking, aligning well with full-duplex spoken language models and learned synchronization; it proposes new training methods and datasets, focusing on real-time speech output, but does not explicitly mention terms like \"spoken language model\" or \"speech language model,\" though it is clearly centered on speech as a core modeling and reasoning modality beyond just text. The abstract indicates low-latency and real-time, streaming capabilities, addressing full-duplex interaction; it also includes model architecture, training methods, and evaluation perspectives, which satisfy inclusion criteria. Therefore, it shows strong relevance to the inclusion criteria without indicating the exclusion criteria conditions.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The paper presents Mini-Omni, an end-to-end conversational model capable of real-time speech interaction, focusing on streaming speech generation and simultaneous hearing and talking, aligning well with full-duplex spoken language models and learned synchronization; it proposes new training methods and datasets, focusing on real-time speech output, but does not explicitly mention terms like \"spoken language model\" or \"speech language model,\" though it is clearly centered on speech as a core modeling and reasoning modality beyond just text. The abstract indicates low-latency and real-time, streaming capabilities, addressing full-duplex interaction; it also includes model architecture, training methods, and evaluation perspectives, which satisfy inclusion criteria. Therefore, it shows strong relevance to the inclusion criteria without indicating the exclusion criteria conditions.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model",
    "abstract": "Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: https://x-codec-audio.github.io Code: https://github.com/zhenye234/xcodec)",
    "metadata": {
      "arxiv_id": "2408.17175",
      "title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model",
      "summary": "Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: https://x-codec-audio.github.io Code: https://github.com/zhenye234/xcodec)",
      "authors": [
        "Zhen Ye",
        "Peiwen Sun",
        "Jiahe Lei",
        "Hongzhan Lin",
        "Xu Tan",
        "Zheqi Dai",
        "Qiuqiang Kong",
        "Jianyi Chen",
        "Jiahao Pan",
        "Qifeng Liu",
        "Yike Guo",
        "Wei Xue"
      ],
      "published": "2024-08-30T10:24:07Z",
      "updated": "2024-11-27T11:47:45Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.17175v3",
      "landing_url": "https://arxiv.org/abs/2408.17175v3",
      "doi": "https://doi.org/10.48550/arXiv.2408.17175"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on audio language modeling for speech, proposes a semantic-enhanced codec, and evaluates on multi-task speech-related outcomes (TTS, music/sound generation) within the last 3 years, aligning with the Spoken/Speech Language Model focus and not fitting the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item focuses on audio language modeling for speech, proposes a semantic-enhanced codec, and evaluates on multi-task speech-related outcomes (TTS, music/sound generation) within the last 3 years, aligning with the Spoken/Speech Language Model focus and not fitting the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving codecs for audio language models by enhancing semantic representations to reduce word error rates and improve audio generation quality across speech and non-speech domains. However, it does not clearly address key aspects such as full-duplex spoken language models, synchronous interaction behaviors (e.g., turn-taking, interruptions, backchanneling), or low-latency real-time interaction; its primary evaluation metric is WER rather than multi-dimensional assessments involving interaction synchronization or behavior arbitration. Additionally, the paper centers on codec enhancement for audio modeling but lacks explicit discussion on modeling architectures or training strategies for spoken language models in the context of synchronous dialogue or cognitive parallelism. Therefore, it aligns with some characteristics but does not fully meet the inclusion criteria emphasizing full-duplex and interaction-focused spoken language modeling, nor does it violate exclusion criteria of being purely ASR-TTS focused, yet it lacks sufficient evidence to be confidently included.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving codecs for audio language models by enhancing semantic representations to reduce word error rates and improve audio generation quality across speech and non-speech domains. However, it does not clearly address key aspects such as full-duplex spoken language models, synchronous interaction behaviors (e.g., turn-taking, interruptions, backchanneling), or low-latency real-time interaction; its primary evaluation metric is WER rather than multi-dimensional assessments involving interaction synchronization or behavior arbitration. Additionally, the paper centers on codec enhancement for audio modeling but lacks explicit discussion on modeling architectures or training strategies for spoken language models in the context of synchronous dialogue or cognitive parallelism. Therefore, it aligns with some characteristics but does not fully meet the inclusion criteria emphasizing full-duplex and interaction-focused spoken language modeling, nor does it violate exclusion criteria of being purely ASR-TTS focused, yet it lacks sufficient evidence to be confidently included.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers' notes were considered: Junior A recommended inclusion, noting the paper targets audio language modeling and codec improvements; Junior B recommended exclusion, highlighting the lack of full‑duplex/interaction focus and reliance on WER as the primary metric. The abstract indeed targets \"audio language models / audio LLMs\" and proposes a semantic-aware codec (X-Codec) that improves generation WER and quality across TTS, music, and text-to-sound. However, it does not present work on Spoken Language Models with interactive/full‑duplex capabilities, nor on synchronization, overlap/interrupt handling, or low‑latency real‑time interaction—requirements of the inclusion criteria. Its evaluations are mainly WER and generation quality, which fall under the exclusion criteria (papers that focus on ASR/TTS or single-task metrics without addressing SLM/FD‑SLM modeling or interaction). Therefore it should be excluded from this SLM/FD‑SLM collection.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers' notes were considered: Junior A recommended inclusion, noting the paper targets audio language modeling and codec improvements; Junior B recommended exclusion, highlighting the lack of full‑duplex/interaction focus and reliance on WER as the primary metric. The abstract indeed targets \"audio language models / audio LLMs\" and proposes a semantic-aware codec (X-Codec) that improves generation WER and quality across TTS, music, and text-to-sound. However, it does not present work on Spoken Language Models with interactive/full‑duplex capabilities, nor on synchronization, overlap/interrupt handling, or low‑latency real‑time interaction—requirements of the inclusion criteria. Its evaluations are mainly WER and generation quality, which fall under the exclusion criteria (papers that focus on ASR/TTS or single-task metrics without addressing SLM/FD‑SLM modeling or interaction). Therefore it should be excluded from this SLM/FD‑SLM collection.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Coordinated Half-Duplex/Full-Duplex Cooperative Rate-Splitting Multiple Access in Multi-Cell Networks",
    "abstract": "This paper explores downlink Cooperative Rate-Splitting Multiple Access (C-RSMA) in a multi-cell wireless network with the assistance of Joint-Transmission Coordinated Multipoint (JT-CoMP). In this network, each cell consists of a base station (BS) equipped with multiple antennas, one or more cell-center users (CCU), and multiple cell-edge users (CEU) located at the edge of the cells. Through JT-CoMP, all the BSs collaborate to simultaneously transmit the data to all the users including the CCUs and CEUs. To enhance the signal quality for the CEUs, CCUs relay the common stream to the CEUs by operating in either half-duplex (HD) or full-duplex (FD) decode-and-forward (DF) relaying mode. In this setup, we aim to jointly optimize the beamforming vectors at the BS, the allocation of common stream rates, the transmit power at relaying users, i.e., CCUs, and the time slot fraction, aiming to maximize the minimum achievable data rate. However, the formulated optimization problem is non-convex and is challenging to solve directly. To address this challenge, we employ change-of-variables, first-order Taylor approximations, and a low-complexity algorithm based on Successive Convex Approximation (SCA). We demonstrate through simulation results the efficacy of the proposed scheme, in terms of average achievable data rate, and we compare its performance to that of four baseline schemes, including HD/FD cooperative non-orthogonal multiple access (C-NOMA), NOMA, and RSMA without user cooperation. The results show that the proposed FD C-RSMA can achieve 25% over FD C-NOMA and the proposed HD C-RSMA can achieve 19% over HD C-NOMA respectively, when the BS transmit power is 20 dBm.",
    "metadata": {
      "arxiv_id": "2409.01263",
      "title": "Coordinated Half-Duplex/Full-Duplex Cooperative Rate-Splitting Multiple Access in Multi-Cell Networks",
      "summary": "This paper explores downlink Cooperative Rate-Splitting Multiple Access (C-RSMA) in a multi-cell wireless network with the assistance of Joint-Transmission Coordinated Multipoint (JT-CoMP). In this network, each cell consists of a base station (BS) equipped with multiple antennas, one or more cell-center users (CCU), and multiple cell-edge users (CEU) located at the edge of the cells. Through JT-CoMP, all the BSs collaborate to simultaneously transmit the data to all the users including the CCUs and CEUs. To enhance the signal quality for the CEUs, CCUs relay the common stream to the CEUs by operating in either half-duplex (HD) or full-duplex (FD) decode-and-forward (DF) relaying mode. In this setup, we aim to jointly optimize the beamforming vectors at the BS, the allocation of common stream rates, the transmit power at relaying users, i.e., CCUs, and the time slot fraction, aiming to maximize the minimum achievable data rate. However, the formulated optimization problem is non-convex and is challenging to solve directly. To address this challenge, we employ change-of-variables, first-order Taylor approximations, and a low-complexity algorithm based on Successive Convex Approximation (SCA). We demonstrate through simulation results the efficacy of the proposed scheme, in terms of average achievable data rate, and we compare its performance to that of four baseline schemes, including HD/FD cooperative non-orthogonal multiple access (C-NOMA), NOMA, and RSMA without user cooperation. The results show that the proposed FD C-RSMA can achieve 25% over FD C-NOMA and the proposed HD C-RSMA can achieve 19% over HD C-NOMA respectively, when the BS transmit power is 20 dBm.",
      "authors": [
        "Mohamed Elhattab",
        "Shreya Khisa",
        "Chadi Assi",
        "Ali Ghrayeb",
        "Marwa Qaraqe",
        "Georges Kaddoum"
      ],
      "published": "2024-09-02T13:55:45Z",
      "updated": "2024-09-02T13:55:45Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.01263v1",
      "landing_url": "https://arxiv.org/abs/2409.01263v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.01263"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on downlink C-RSMA, JT-CoMP, HD/FD relaying in multi-cell wireless networks and beamforming optimization, which does not address Spoken Language Models or any speech-language modeling concepts.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on downlink C-RSMA, JT-CoMP, HD/FD relaying in multi-cell wireless networks and beamforming optimization, which does not address Spoken Language Models or any speech-language modeling concepts.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on cooperative rate-splitting multiple access in multi-cell wireless networks related to communication systems, with no mention of spoken language models, speech language modeling, or any related concepts focused on modeling or reasoning of speech signals, synchronization, or conversational interaction as required by the inclusion criteria. Therefore, it falls outside the scope defined for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on cooperative rate-splitting multiple access in multi-cell wireless networks related to communication systems, with no mention of spoken language models, speech language modeling, or any related concepts focused on modeling or reasoning of speech signals, synchronization, or conversational interaction as required by the inclusion criteria. Therefore, it falls outside the scope defined for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "vec2wav 2.0: Advancing Voice Conversion via Discrete Token Vocoders",
    "abstract": "We propose a new speech discrete token vocoder, vec2wav 2.0, which advances voice conversion (VC). We use discrete tokens from speech self-supervised models as the content features of source speech, and treat VC as a prompted vocoding task. To amend the loss of speaker timbre in the content tokens, vec2wav 2.0 utilizes the WavLM features to provide strong timbre-dependent information. A novel adaptive Snake activation function is proposed to better incorporate timbre into the waveform reconstruction process. In this way, vec2wav 2.0 learns to alter the speaker timbre appropriately given different reference prompts. Also, no supervised data is required for vec2wav 2.0 to be effectively trained. Experimental results demonstrate that vec2wav 2.0 outperforms all other baselines to a considerable margin in terms of audio quality and speaker similarity in any-to-any VC. Ablation studies verify the effects made by the proposed techniques. Moreover, vec2wav 2.0 achieves competitive cross-lingual VC even only trained on monolingual corpus. Thus, vec2wav 2.0 shows timbre can potentially be manipulated only by speech token vocoders, pushing the frontiers of VC and speech synthesis.",
    "metadata": {
      "arxiv_id": "2409.01995",
      "title": "vec2wav 2.0: Advancing Voice Conversion via Discrete Token Vocoders",
      "summary": "We propose a new speech discrete token vocoder, vec2wav 2.0, which advances voice conversion (VC). We use discrete tokens from speech self-supervised models as the content features of source speech, and treat VC as a prompted vocoding task. To amend the loss of speaker timbre in the content tokens, vec2wav 2.0 utilizes the WavLM features to provide strong timbre-dependent information. A novel adaptive Snake activation function is proposed to better incorporate timbre into the waveform reconstruction process. In this way, vec2wav 2.0 learns to alter the speaker timbre appropriately given different reference prompts. Also, no supervised data is required for vec2wav 2.0 to be effectively trained. Experimental results demonstrate that vec2wav 2.0 outperforms all other baselines to a considerable margin in terms of audio quality and speaker similarity in any-to-any VC. Ablation studies verify the effects made by the proposed techniques. Moreover, vec2wav 2.0 achieves competitive cross-lingual VC even only trained on monolingual corpus. Thus, vec2wav 2.0 shows timbre can potentially be manipulated only by speech token vocoders, pushing the frontiers of VC and speech synthesis.",
      "authors": [
        "Yiwei Guo",
        "Zhihan Li",
        "Junjie Li",
        "Chenpeng Du",
        "Hankun Wang",
        "Shuai Wang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-09-03T15:41:07Z",
      "updated": "2025-05-24T13:50:34Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.01995v4",
      "landing_url": "https://arxiv.org/abs/2409.01995v4",
      "doi": "https://doi.org/10.48550/arXiv.2409.01995"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work focuses on a voice-conversion vocoder using discrete tokens and self-supervised speech representations; it does not clearly center on Spoken Language Models or full-duplex speech–language modeling with synchronous interaction/ dialogue behavior, which is a core criterion for inclusion, so it should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "This work focuses on a voice-conversion vocoder using discrete tokens and self-supervised speech representations; it does not clearly center on Spoken Language Models or full-duplex speech–language modeling with synchronous interaction/ dialogue behavior, which is a core criterion for inclusion, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on voice conversion using discrete token vocoders and speaker timbre manipulation, but it does not explicitly discuss Spoken Language Models, full-duplex interaction, or synchronization mechanisms central to the inclusion criteria. It centers on speech signal processing and synthesis rather than on speech language modeling or real-time interactive dialogue behaviors, which are required for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on voice conversion using discrete token vocoders and speaker timbre manipulation, but it does not explicitly discuss Spoken Language Models, full-duplex interaction, or synchronization mechanisms central to the inclusion criteria. It centers on speech signal processing and synthesis rather than on speech language modeling or real-time interactive dialogue behaviors, which are required for inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Coral Model Generation from Single Images for Virtual Reality Applications",
    "abstract": "With the rapid development of VR technology, the demand for high-quality 3D models is increasing. Traditional methods struggle with efficiency and quality in large-scale customization. This paper introduces a deep-learning framework that generates high-precision 3D coral models from a single image. Using the Coral dataset, the framework extracts geometric and texture features, performs 3D reconstruction, and optimizes design and material blending. Advanced optimization and polygon count control ensure shape accuracy, detail retention, and flexible output for various complexities, catering to high-quality rendering and real-time interaction needs.The project incorporates Explainable AI (XAI) to transform AI-generated models into interactive \"artworks,\" best viewed in VR and XR. This enhances model interpretability and human-machine collaboration. Real-time feedback in VR interactions displays information like coral species and habitat, enriching user experience. The generated models surpass traditional methods in detail, visual quality, and efficiency. This research offers an intelligent approach to 3D content creation for VR, lowering production barriers, and promoting widespread VR applications. Additionally, integrating XAI provides new insights into AI-generated visual content and advances research in 3D vision interpretability.",
    "metadata": {
      "arxiv_id": "2409.02376",
      "title": "Coral Model Generation from Single Images for Virtual Reality Applications",
      "summary": "With the rapid development of VR technology, the demand for high-quality 3D models is increasing. Traditional methods struggle with efficiency and quality in large-scale customization. This paper introduces a deep-learning framework that generates high-precision 3D coral models from a single image. Using the Coral dataset, the framework extracts geometric and texture features, performs 3D reconstruction, and optimizes design and material blending. Advanced optimization and polygon count control ensure shape accuracy, detail retention, and flexible output for various complexities, catering to high-quality rendering and real-time interaction needs.The project incorporates Explainable AI (XAI) to transform AI-generated models into interactive \"artworks,\" best viewed in VR and XR. This enhances model interpretability and human-machine collaboration. Real-time feedback in VR interactions displays information like coral species and habitat, enriching user experience. The generated models surpass traditional methods in detail, visual quality, and efficiency. This research offers an intelligent approach to 3D content creation for VR, lowering production barriers, and promoting widespread VR applications. Additionally, integrating XAI provides new insights into AI-generated visual content and advances research in 3D vision interpretability.",
      "authors": [
        "Jie Fu",
        "Shun Fu",
        "Mick Grierson"
      ],
      "published": "2024-09-04T01:54:20Z",
      "updated": "2024-09-04T01:54:20Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.HC",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.02376v1",
      "landing_url": "https://arxiv.org/abs/2409.02376v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.02376"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper addresses 3D coral model generation from a single image for VR, which is unrelated to Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper addresses 3D coral model generation from a single image for VR, which is unrelated to Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on generating 3D coral models from single images for VR applications and involves deep learning and Explainable AI, but it does not address spoken language models, speech-based modeling, full-duplex interaction, or any speech-related modeling or evaluation as required by the inclusion criteria; therefore, it falls outside the scope of spoken language model research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on generating 3D coral models from single images for VR applications and involves deep learning and Explainable AI, but it does not address spoken language models, speech-based modeling, full-duplex interaction, or any speech-related modeling or evaluation as required by the inclusion criteria; therefore, it falls outside the scope of spoken language model research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LAST: Language Model Aware Speech Tokenization",
    "abstract": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.",
    "metadata": {
      "arxiv_id": "2409.03701",
      "title": "LAST: Language Model Aware Speech Tokenization",
      "summary": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.",
      "authors": [
        "Arnon Turetzky",
        "Yossi Adi"
      ],
      "published": "2024-09-05T16:57:39Z",
      "updated": "2024-09-10T14:45:15Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.03701v2",
      "landing_url": "https://arxiv.org/abs/2409.03701v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.03701"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work explicitly targets Spoken Language Models and LM-aware speech tokenization, discussing architecture/training/evaluation to improve speech-LM and speech-to-text performance, within the last 3 years, and is not limited to traditional ASR or TTS alone.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work explicitly targets Spoken Language Models and LM-aware speech tokenization, discussing architecture/training/evaluation to improve speech-LM and speech-to-text performance, within the last 3 years, and is not limited to traditional ASR or TTS alone.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving speech tokenization incorporating language model objectives, aiming to better integrate speech representations within language models; however, it primarily addresses speech tokenization in relation to speech language modeling performance, without explicit discussion of full-duplex spoken language models, synchronous interaction, turn-taking, or real-time interactive behavior, and mainly evaluates metrics like spoken language modeling and speech-to-text accuracy, without mention of low-latency, cognitive parallelism, or behavioral arbitration; thus, it does not fully meet the inclusion criteria emphasizing full-duplex interaction and multi-dimensional evaluation of synchronous spoken language models.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving speech tokenization incorporating language model objectives, aiming to better integrate speech representations within language models; however, it primarily addresses speech tokenization in relation to speech language modeling performance, without explicit discussion of full-duplex spoken language models, synchronous interaction, turn-taking, or real-time interactive behavior, and mainly evaluates metrics like spoken language modeling and speech-to-text accuracy, without mention of low-latency, cognitive parallelism, or behavioral arbitration; thus, it does not fully meet the inclusion criteria emphasizing full-duplex interaction and multi-dimensional evaluation of synchronous spoken language models.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "The paper explicitly targets speech language modeling by proposing LM-aware speech tokenization and evaluates spoken language modeling and speech-to-text, satisfying the requirement that spoken language models/speech language modeling appear as core in the abstract; JuniorNano correctly favored inclusion for that reason, while JuniorMini flagged the absence of full-duplex/turn-taking or real-time interaction evaluation — which is true but not a mandatory exclusion (the study focuses on core SLM/tokenization innovation rather than FD-SLM interaction behaviors). Therefore it meets the core inclusion criteria (SLM-centric innovation) and does not fall under exclusion rules, so it should be included.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "The paper explicitly targets speech language modeling by proposing LM-aware speech tokenization and evaluates spoken language modeling and speech-to-text, satisfying the requirement that spoken language models/speech language modeling appear as core in the abstract; JuniorNano correctly favored inclusion for that reason, while JuniorMini flagged the absence of full-duplex/turn-taking or real-time interaction evaluation — which is true but not a mandatory exclusion (the study focuses on core SLM/tokenization innovation rather than FD-SLM interaction behaviors). Therefore it meets the core inclusion criteria (SLM-centric innovation) and does not fall under exclusion rules, so it should be included.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Investigating Neural Audio Codecs for Speech Language Model-Based Speech Generation",
    "abstract": "Neural audio codec tokens serve as the fundamental building blocks for speech language model (SLM)-based speech generation. However, there is no systematic understanding on how the codec system affects the speech generation performance of the SLM. In this work, we examine codec tokens within SLM framework for speech generation to provide insights for effective codec design. We retrain existing high-performing neural codec models on the same data set and loss functions to compare their performance in a uniform setting. We integrate codec tokens into two SLM systems: masked-based parallel speech generation system and an auto-regressive (AR) plus non-auto-regressive (NAR) model-based system. Our findings indicate that better speech reconstruction in codec systems does not guarantee improved speech generation in SLM. A high-quality codec decoder is crucial for natural speech production in SLM, while speech intelligibility depends more on quantization mechanism.",
    "metadata": {
      "arxiv_id": "2409.04016",
      "title": "Investigating Neural Audio Codecs for Speech Language Model-Based Speech Generation",
      "summary": "Neural audio codec tokens serve as the fundamental building blocks for speech language model (SLM)-based speech generation. However, there is no systematic understanding on how the codec system affects the speech generation performance of the SLM. In this work, we examine codec tokens within SLM framework for speech generation to provide insights for effective codec design. We retrain existing high-performing neural codec models on the same data set and loss functions to compare their performance in a uniform setting. We integrate codec tokens into two SLM systems: masked-based parallel speech generation system and an auto-regressive (AR) plus non-auto-regressive (NAR) model-based system. Our findings indicate that better speech reconstruction in codec systems does not guarantee improved speech generation in SLM. A high-quality codec decoder is crucial for natural speech production in SLM, while speech intelligibility depends more on quantization mechanism.",
      "authors": [
        "Jiaqi Li",
        "Dongmei Wang",
        "Xiaofei Wang",
        "Yao Qian",
        "Long Zhou",
        "Shujie Liu",
        "Midia Yousefi",
        "Canrun Li",
        "Chung-Hsien Tsai",
        "Zhen Xiao",
        "Yanqing Liu",
        "Junkun Chen",
        "Sheng Zhao",
        "Jinyu Li",
        "Zhizheng Wu",
        "Michael Zeng"
      ],
      "published": "2024-09-06T04:06:50Z",
      "updated": "2024-09-06T04:06:50Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.04016v1",
      "landing_url": "https://arxiv.org/abs/2409.04016v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.04016"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Based on inclusion criteria focusing on Spoken Language Models with speech as the core modeling object, recent within the last 3 years, and explicit discussion of model architecture, training/evaluation in a speech-language modeling context; this item examines neural audio codecs within SLM-based speech generation, aligning with SLM ideas rather than pure ASR/TTS, and includes method/evaluation details.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "Based on inclusion criteria focusing on Spoken Language Models with speech as the core modeling object, recent within the last 3 years, and explicit discussion of model architecture, training/evaluation in a speech-language modeling context; this item examines neural audio codecs within SLM-based speech generation, aligning with SLM ideas rather than pure ASR/TTS, and includes method/evaluation details.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on neural audio codecs for Speech Language Model (SLM)-based speech generation, clearly involving speech as the primary modeling object and discussing model architecture and evaluation. It features the term 'speech language model' explicitly in its title and abstract and is within the scope of exploring SLM frameworks. However, it does not address full-duplex or real-time interactive behaviors, cognitive parallelism, or synchronization in multi-turn dialogue, nor does it discuss evaluation of interactive features such as turn-taking, backchanneling, or low-latency speech interaction. The evaluation focuses on reconstruction quality and speech intelligibility rather than multi-dimensional assessments involving behavioral arbitration or real-time interaction metrics. Hence, while it aligns with some inclusion criteria, it misses key aspects about full-duplex interaction and synchronous communication, and does not tackle conversational behavior arbitration or real-time multi-turn interaction, leading to uncertainty about inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on neural audio codecs for Speech Language Model (SLM)-based speech generation, clearly involving speech as the primary modeling object and discussing model architecture and evaluation. It features the term 'speech language model' explicitly in its title and abstract and is within the scope of exploring SLM frameworks. However, it does not address full-duplex or real-time interactive behaviors, cognitive parallelism, or synchronization in multi-turn dialogue, nor does it discuss evaluation of interactive features such as turn-taking, backchanneling, or low-latency speech interaction. The evaluation focuses on reconstruction quality and speech intelligibility rather than multi-dimensional assessments involving behavioral arbitration or real-time interaction metrics. Hence, while it aligns with some inclusion criteria, it misses key aspects about full-duplex interaction and synchronous communication, and does not tackle conversational behavior arbitration or real-time multi-turn interaction, leading to uncertainty about inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "本篇在標題與摘要明確以 “speech language model (SLM)” 為核心，且針對 codec token 與 SLM 中的生成性能做實驗與分析，符合對於語音層級 language modeling 與 SLM 架構/評估關切（JuniorNano 給分 5）。但論文並未討論 full‑duplex 或同步互動、語音重疊管理、低延遲或行為仲裁等即時互動面向，僅聚焦於重建品質與生成可懂度，落入排除條件中針對僅討論單向合成/half‑duplex 類型研究的範疇（JuniorMini 給分 3，指出相同疑慮）。因此雖然屬於 SLM 研究，但違反「不得僅為單向/非同步互動」的排除準則，不應納入。",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "本篇在標題與摘要明確以 “speech language model (SLM)” 為核心，且針對 codec token 與 SLM 中的生成性能做實驗與分析，符合對於語音層級 language modeling 與 SLM 架構/評估關切（JuniorNano 給分 5）。但論文並未討論 full‑duplex 或同步互動、語音重疊管理、低延遲或行為仲裁等即時互動面向，僅聚焦於重建品質與生成可懂度，落入排除條件中針對僅討論單向合成/half‑duplex 類型研究的範疇（JuniorMini 給分 3，指出相同疑慮）。因此雖然屬於 SLM 研究，但違反「不得僅為單向/非同步互動」的排除準則，不應納入。",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "NPU-NTU System for Voice Privacy 2024 Challenge",
    "abstract": "Speaker anonymization is an effective privacy protection solution that conceals the speaker's identity while preserving the linguistic content and paralinguistic information of the original speech. To establish a fair benchmark and facilitate comparison of speaker anonymization systems, the VoicePrivacy Challenge (VPC) was held in 2020 and 2022, with a new edition planned for 2024. In this paper, we describe our proposed speaker anonymization system for VPC 2024. Our system employs a disentangled neural codec architecture and a serial disentanglement strategy to gradually disentangle the global speaker identity and time-variant linguistic content and paralinguistic information. We introduce multiple distillation methods to disentangle linguistic content, speaker identity, and emotion. These methods include semantic distillation, supervised speaker distillation, and frame-level emotion distillation. Based on these distillations, we anonymize the original speaker identity using a weighted sum of a set of candidate speaker identities and a randomly generated speaker identity. Our system achieves the best trade-off of privacy protection and emotion preservation in VPC 2024.",
    "metadata": {
      "arxiv_id": "2409.04173",
      "title": "NPU-NTU System for Voice Privacy 2024 Challenge",
      "summary": "Speaker anonymization is an effective privacy protection solution that conceals the speaker's identity while preserving the linguistic content and paralinguistic information of the original speech. To establish a fair benchmark and facilitate comparison of speaker anonymization systems, the VoicePrivacy Challenge (VPC) was held in 2020 and 2022, with a new edition planned for 2024. In this paper, we describe our proposed speaker anonymization system for VPC 2024. Our system employs a disentangled neural codec architecture and a serial disentanglement strategy to gradually disentangle the global speaker identity and time-variant linguistic content and paralinguistic information. We introduce multiple distillation methods to disentangle linguistic content, speaker identity, and emotion. These methods include semantic distillation, supervised speaker distillation, and frame-level emotion distillation. Based on these distillations, we anonymize the original speaker identity using a weighted sum of a set of candidate speaker identities and a randomly generated speaker identity. Our system achieves the best trade-off of privacy protection and emotion preservation in VPC 2024.",
      "authors": [
        "Jixun Yao",
        "Nikita Kuzmin",
        "Qing Wang",
        "Pengcheng Guo",
        "Ziqian Ning",
        "Dake Guo",
        "Kong Aik Lee",
        "Eng-Siong Chng",
        "Lei Xie"
      ],
      "published": "2024-09-06T10:32:42Z",
      "updated": "2025-02-04T08:43:31Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.04173v2",
      "landing_url": "https://arxiv.org/abs/2409.04173v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.04173"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on speaker anonymization and benchmarking for privacy, not on Spoken Language Models or core speech-language modeling architectures, thus it does not meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item focuses on speaker anonymization and benchmarking for privacy, not on Spoken Language Models or core speech-language modeling architectures, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speaker anonymization technology for a voice privacy challenge, centered on disentangling speaker identity from speech signals; however, it does not discuss spoken language models, speech language models, full-duplex interaction, cognitive parallelism, or any synchonous interactive dialogue modeling. The paper also lacks discussion of model architecture related to Spoken Language Models or evaluations involving low-latency or multi-dimensional behavioral arbitration relevant to full-duplex spoken interaction. It is thus primarily a privacy/anonymization system rather than a study aligned with the inclusion criteria targeting spoken language modeling and full-duplex interaction capabilities.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speaker anonymization technology for a voice privacy challenge, centered on disentangling speaker identity from speech signals; however, it does not discuss spoken language models, speech language models, full-duplex interaction, cognitive parallelism, or any synchonous interactive dialogue modeling. The paper also lacks discussion of model architecture related to Spoken Language Models or evaluations involving low-latency or multi-dimensional behavioral arbitration relevant to full-duplex spoken interaction. It is thus primarily a privacy/anonymization system rather than a study aligned with the inclusion criteria targeting spoken language modeling and full-duplex interaction capabilities.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Vector Quantized Diffusion Model Based Speech Bandwidth Extension",
    "abstract": "Recent advancements in neural audio codec (NAC) unlock new potential in audio signal processing. Studies have increasingly explored leveraging the latent features of NAC for various speech signal processing tasks. This paper introduces the first approach to speech bandwidth extension (BWE) that utilizes the discrete features obtained from NAC. By restoring high-frequency details within highly compressed discrete tokens, this approach enhances speech intelligibility and naturalness. Based on Vector Quantized Diffusion, the proposed framework combines the strengths of advanced NAC, diffusion models, and Mamba-2 to reconstruct high-frequency speech components. Extensive experiments demonstrate that this method exhibits superior performance across both log-spectral distance and ViSQOL, significantly improving speech quality.",
    "metadata": {
      "arxiv_id": "2409.05784",
      "title": "Vector Quantized Diffusion Model Based Speech Bandwidth Extension",
      "summary": "Recent advancements in neural audio codec (NAC) unlock new potential in audio signal processing. Studies have increasingly explored leveraging the latent features of NAC for various speech signal processing tasks. This paper introduces the first approach to speech bandwidth extension (BWE) that utilizes the discrete features obtained from NAC. By restoring high-frequency details within highly compressed discrete tokens, this approach enhances speech intelligibility and naturalness. Based on Vector Quantized Diffusion, the proposed framework combines the strengths of advanced NAC, diffusion models, and Mamba-2 to reconstruct high-frequency speech components. Extensive experiments demonstrate that this method exhibits superior performance across both log-spectral distance and ViSQOL, significantly improving speech quality.",
      "authors": [
        "Yuan Fang",
        "Jinglin Bai",
        "Jiajie Wang",
        "Xueliang Zhang"
      ],
      "published": "2024-09-09T16:46:54Z",
      "updated": "2024-09-14T14:25:52Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.05784v2",
      "landing_url": "https://arxiv.org/abs/2409.05784v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.05784"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on speech bandwidth extension using neural audio codecs and diffusion models, treating speech as a signal to reconstruct rather than a Spoken Language Model/FD-SLM core, with no emphasis on interactive dialogue or speech-language modeling, so it fails the inclusion criteria and meets at least one exclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on speech bandwidth extension using neural audio codecs and diffusion models, treating speech as a signal to reconstruct rather than a Spoken Language Model/FD-SLM core, with no emphasis on interactive dialogue or speech-language modeling, so it fails the inclusion criteria and meets at least one exclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on using neural audio codec latent features and Vector Quantized Diffusion models for speech bandwidth extension, which is an audio enhancement task. However, it does not mention spoken language models, speech language modeling, full-duplex interaction, or synchronization necessary for the inclusion criteria. It also lacks discussion of modeling architectures or evaluation related to spoken language models or interactive speech tasks, and no keywords like spoken language model or speech language model appear. Therefore, it does not meet the inclusion criteria and falls under exclusion for focusing on a single speech enhancement task without addressing speech language modeling or interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on using neural audio codec latent features and Vector Quantized Diffusion models for speech bandwidth extension, which is an audio enhancement task. However, it does not mention spoken language models, speech language modeling, full-duplex interaction, or synchronization necessary for the inclusion criteria. It also lacks discussion of modeling architectures or evaluation related to spoken language models or interactive speech tasks, and no keywords like spoken language model or speech language model appear. Therefore, it does not meet the inclusion criteria and falls under exclusion for focusing on a single speech enhancement task without addressing speech language modeling or interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech Editing and Synthesis",
    "abstract": "In this paper, we introduce SSR-Speech, a neural codec autoregressive model designed for stable, safe, and robust zero-shot textbased speech editing and text-to-speech synthesis. SSR-Speech is built on a Transformer decoder and incorporates classifier-free guidance to enhance the stability of the generation process. A watermark Encodec is proposed to embed frame-level watermarks into the edited regions of the speech so that which parts were edited can be detected. In addition, the waveform reconstruction leverages the original unedited speech segments, providing superior recovery compared to the Encodec model. Our approach achieves state-of-the-art performance in the RealEdit speech editing task and the LibriTTS text-to-speech task, surpassing previous methods. Furthermore, SSR-Speech excels in multi-span speech editing and also demonstrates remarkable robustness to background sounds. The source code and demos are released.",
    "metadata": {
      "arxiv_id": "2409.07556",
      "title": "SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech Editing and Synthesis",
      "summary": "In this paper, we introduce SSR-Speech, a neural codec autoregressive model designed for stable, safe, and robust zero-shot textbased speech editing and text-to-speech synthesis. SSR-Speech is built on a Transformer decoder and incorporates classifier-free guidance to enhance the stability of the generation process. A watermark Encodec is proposed to embed frame-level watermarks into the edited regions of the speech so that which parts were edited can be detected. In addition, the waveform reconstruction leverages the original unedited speech segments, providing superior recovery compared to the Encodec model. Our approach achieves state-of-the-art performance in the RealEdit speech editing task and the LibriTTS text-to-speech task, surpassing previous methods. Furthermore, SSR-Speech excels in multi-span speech editing and also demonstrates remarkable robustness to background sounds. The source code and demos are released.",
      "authors": [
        "Helin Wang",
        "Meng Yu",
        "Jiarui Hai",
        "Chen Chen",
        "Yuchen Hu",
        "Rilin Chen",
        "Najim Dehak",
        "Dong Yu"
      ],
      "published": "2024-09-11T18:24:07Z",
      "updated": "2025-01-02T03:07:29Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.07556v2",
      "landing_url": "https://arxiv.org/abs/2409.07556v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.07556"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "這篇論文以零-shot文本驅動的語音編輯與語音合成為核心，對語音作為建模與推理對象，且在過去3年內發表，涵蓋語音模型的架構與評估，未僅限於單一語音技術，因此應該列入。",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "這篇論文以零-shot文本驅動的語音編輯與語音合成為核心，對語音作為建模與推理對象，且在過去3年內發表，涵蓋語音模型的架構與評估，未僅限於單一語音技術，因此應該列入。",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a neural codec autoregressive model for zero-shot text-based speech editing and synthesis, focusing on speech generation quality and robustness. However, it does not explicitly address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models, nor does it discuss synchronous interaction, dialogue arbitration, or cognitive parallelism. It mainly emphasizes text-to-speech and speech editing tasks without covering modeling from a spoken language modeling perspective or interaction synchronization, which are central to the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study presents a neural codec autoregressive model for zero-shot text-based speech editing and synthesis, focusing on speech generation quality and robustness. However, it does not explicitly address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models, nor does it discuss synchronous interaction, dialogue arbitration, or cognitive parallelism. It mainly emphasizes text-to-speech and speech editing tasks without covering modeling from a spoken language modeling perspective or interaction synchronization, which are central to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "The abstract describes a neural codec autoregressive model for zero-shot text-based speech editing and text-to-speech (SSR-Speech) focusing on generation stability, watermarking edited regions, waveform reconstruction, and robustness to background sounds — it does not present itself as a Spoken Language Model (SLM) or discuss language modeling of speech, full-duplex interaction, synchronization, cognitive parallelism, or other SLM/FD-SLM concerns required by the inclusion criteria. I note the two junior reviews: one (round-A_JuniorNano) recommended inclusion likely because it treats speech modeling broadly, while the other (round-A_JuniorMini) correctly judged exclusion because the work centers on TTS/speech editing rather than SLMs; after integrating these opinions, the manuscript fails the explicit requirement that the title/abstract mention spoken language models or equivalent and fails to address synchronous/full-duplex interaction or speech-level language modeling. Score explanation: This paper should be excluded because it focuses on TTS and speech editing tasks rather than on Spoken Language Models or full-duplex speech language modeling required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "The abstract describes a neural codec autoregressive model for zero-shot text-based speech editing and text-to-speech (SSR-Speech) focusing on generation stability, watermarking edited regions, waveform reconstruction, and robustness to background sounds — it does not present itself as a Spoken Language Model (SLM) or discuss language modeling of speech, full-duplex interaction, synchronization, cognitive parallelism, or other SLM/FD-SLM concerns required by the inclusion criteria. I note the two junior reviews: one (round-A_JuniorNano) recommended inclusion likely because it treats speech modeling broadly, while the other (round-A_JuniorMini) correctly judged exclusion because the work centers on TTS/speech editing rather than SLMs; after integrating these opinions, the manuscript fails the explicit requirement that the title/abstract mention spoken language models or equivalent and fails to address synchronous/full-duplex interaction or speech-level language modeling. Score explanation: This paper should be excluded because it focuses on TTS and speech editing tasks rather than on Spoken Language Models or full-duplex speech language modeling required by the inclusion criteria.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "TSELM: Target Speaker Extraction using Discrete Tokens and Language Models",
    "abstract": "We propose TSELM, a novel target speaker extraction network that leverages discrete tokens and language models. TSELM utilizes multiple discretized layers from WavLM as input tokens and incorporates cross-attention mechanisms to integrate target speaker information. Language models are employed to capture the sequence dependencies, while a scalable HiFi-GAN is used to reconstruct the audio from the tokens. By applying a cross-entropy loss, TSELM models the probability distribution of output tokens, thus converting the complex regression problem of audio generation into a classification task. Experimental results show that TSELM achieves excellent results in speech quality and comparable results in speech intelligibility.",
    "metadata": {
      "arxiv_id": "2409.07841",
      "title": "TSELM: Target Speaker Extraction using Discrete Tokens and Language Models",
      "summary": "We propose TSELM, a novel target speaker extraction network that leverages discrete tokens and language models. TSELM utilizes multiple discretized layers from WavLM as input tokens and incorporates cross-attention mechanisms to integrate target speaker information. Language models are employed to capture the sequence dependencies, while a scalable HiFi-GAN is used to reconstruct the audio from the tokens. By applying a cross-entropy loss, TSELM models the probability distribution of output tokens, thus converting the complex regression problem of audio generation into a classification task. Experimental results show that TSELM achieves excellent results in speech quality and comparable results in speech intelligibility.",
      "authors": [
        "Beilong Tang",
        "Bang Zeng",
        "Ming Li"
      ],
      "published": "2024-09-12T08:41:07Z",
      "updated": "2024-09-17T01:41:32Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.07841v3",
      "landing_url": "https://arxiv.org/abs/2409.07841v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.07841"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on target speaker extraction using discrete tokens and language models, leveraging speech representations (WavLM) and language models to model speech sequences, which aligns with the Spoken Language Models / FD-SLMs focus. It discusses model architecture, training, and evaluation and is within the last 3 years (arXiv: 2504.*). It is not limited to ASR/TTS nor confined to unilateral turn-taking or non-interactive tasks, and it explicitly handles speech as the modeling target rather than treating it merely as input to NLP.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This item centers on target speaker extraction using discrete tokens and language models, leveraging speech representations (WavLM) and language models to model speech sequences, which aligns with the Spoken Language Models / FD-SLMs focus. It discusses model architecture, training, and evaluation and is within the last 3 years (arXiv: 2504.*). It is not limited to ASR/TTS nor confined to unilateral turn-taking or non-interactive tasks, and it explicitly handles speech as the modeling target rather than treating it merely as input to NLP.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The work proposes TSELM, a target speaker extraction model using discrete tokens and language models, focusing on speech token modeling and audio generation. However, it does not explicitly discuss 'Spoken Language Models' or 'Speech Language Models' as core concepts, nor does it address full-duplex interaction, turn-taking, synchronized interaction, or related multi-dimensional evaluations such as interruption management or cognitive parallelism. The evaluation also focuses on speech quality and intelligibility without addressing interaction or behavioral arbitration, which are required by inclusion criteria. Therefore, while the paper deals with speech modeling, it lacks the explicit alignment with the specified inclusion criteria centered on spoken language modeling for full-duplex spoken dialogue systems.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The work proposes TSELM, a target speaker extraction model using discrete tokens and language models, focusing on speech token modeling and audio generation. However, it does not explicitly discuss 'Spoken Language Models' or 'Speech Language Models' as core concepts, nor does it address full-duplex interaction, turn-taking, synchronized interaction, or related multi-dimensional evaluations such as interruption management or cognitive parallelism. The evaluation also focuses on speech quality and intelligibility without addressing interaction or behavioral arbitration, which are required by inclusion criteria. Therefore, while the paper deals with speech modeling, it lacks the explicit alignment with the specified inclusion criteria centered on spoken language modeling for full-duplex spoken dialogue systems.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviews: one recommended inclusion noting the paper explicitly uses discrete speech tokens and language models for speech generation/modeling; the other recommended exclusion arguing it lacks any FD-SLM/full‑duplex interaction focus and only evaluates speech quality/intelligibility. My judgment: the title and abstract explicitly describe language modeling of speech tokens (\"discrete tokens and language models\", modeling token probability distributions) and present architecture/training/evaluation details, which satisfies the core inclusion requirement that the work centers on spoken‑language (speech token) modeling rather than mere ASR/TTS preprocessing. It does not, however, address full‑duplex interaction, synchronization, or real‑time/behavioral arbitration — so it does not cover the FD‑SLM subtopic—but that absence alone does not violate the stated inclusion rules (which require explicit speech language modeling). Therefore this item should be included but with lower confidence due to missing interaction/FD evaluation aspects.\nScore (one‑sentence explanation): 4 — better to include: the paper explicitly develops language modeling over discrete speech tokens and describes model/training/evaluation, meeting the SLM-focused inclusion requirement, though it lacks FD/interaction evaluation.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Junior reviews: one recommended inclusion noting the paper explicitly uses discrete speech tokens and language models for speech generation/modeling; the other recommended exclusion arguing it lacks any FD-SLM/full‑duplex interaction focus and only evaluates speech quality/intelligibility. My judgment: the title and abstract explicitly describe language modeling of speech tokens (\"discrete tokens and language models\", modeling token probability distributions) and present architecture/training/evaluation details, which satisfies the core inclusion requirement that the work centers on spoken‑language (speech token) modeling rather than mere ASR/TTS preprocessing. It does not, however, address full‑duplex interaction, synchronization, or real‑time/behavioral arbitration — so it does not cover the FD‑SLM subtopic—but that absence alone does not violate the stated inclusion rules (which require explicit speech language modeling). Therefore this item should be included but with lower confidence due to missing interaction/FD evaluation aspects.\nScore (one‑sentence explanation): 4 — better to include: the paper explicitly develops language modeling over discrete speech tokens and describes model/training/evaluation, meeting the SLM-focused inclusion requirement, though it lacks FD/interaction evaluation.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Exploring SSL Discrete Speech Features for Zipformer-based Contextual ASR",
    "abstract": "Self-supervised learning (SSL) based discrete speech representations are highly compact and domain adaptable. In this paper, SSL discrete speech features extracted from WavLM models are used as additional cross-utterance acoustic context features in Zipformer-Transducer ASR systems. The efficacy of replacing Fbank features with discrete token features for modelling either cross-utterance contexts (from preceding and future segments), or current utterance's internal contexts alone, or both at the same time, are demonstrated thoroughly on the Gigaspeech 1000-hr corpus. The best Zipformer-Transducer system using discrete tokens based cross-utterance context features outperforms the baseline using utterance internal context only with statistically significant word error rate (WER) reductions of 0.32% to 0.41% absolute (2.78% to 3.54% relative) on the dev and test data. The lowest published WER of 11.15% and 11.14% were obtained on the dev and test sets. Our work is open-source and publicly available at https://github.com/open-creator/icefall/tree/master/egs/gigaspeech/Context\\_ASR.",
    "metadata": {
      "arxiv_id": "2409.08797",
      "title": "Exploring SSL Discrete Speech Features for Zipformer-based Contextual ASR",
      "summary": "Self-supervised learning (SSL) based discrete speech representations are highly compact and domain adaptable. In this paper, SSL discrete speech features extracted from WavLM models are used as additional cross-utterance acoustic context features in Zipformer-Transducer ASR systems. The efficacy of replacing Fbank features with discrete token features for modelling either cross-utterance contexts (from preceding and future segments), or current utterance's internal contexts alone, or both at the same time, are demonstrated thoroughly on the Gigaspeech 1000-hr corpus. The best Zipformer-Transducer system using discrete tokens based cross-utterance context features outperforms the baseline using utterance internal context only with statistically significant word error rate (WER) reductions of 0.32% to 0.41% absolute (2.78% to 3.54% relative) on the dev and test data. The lowest published WER of 11.15% and 11.14% were obtained on the dev and test sets. Our work is open-source and publicly available at https://github.com/open-creator/icefall/tree/master/egs/gigaspeech/Context\\_ASR.",
      "authors": [
        "Mingyu Cui",
        "Yifan Yang",
        "Jiajun Deng",
        "Jiawen Kang",
        "Shujie Hu",
        "Tianzi Wang",
        "Zhaoqing Li",
        "Shiliang Zhang",
        "Xie Chen",
        "Xunying Liu"
      ],
      "published": "2024-09-13T13:01:09Z",
      "updated": "2025-06-10T03:12:23Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.08797v2",
      "landing_url": "https://arxiv.org/abs/2409.08797v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.08797"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "理由：雖然是語音表示學習與跨語句上下文的 ASR 研究，但未在標題/摘要中以 Spoken Language Models 或等價概念為核心，且聚焦於語音特徵與 ASR 的關聯性而非真正的語音語言建模，因此不符合納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "理由：雖然是語音表示學習與跨語句上下文的 ASR 研究，但未在標題/摘要中以 Spoken Language Models 或等價概念為核心，且聚焦於語音特徵與 ASR 的關聯性而非真正的語音語言建模，因此不符合納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using self-supervised discrete speech features within an ASR system, emphasizing acoustic context modeling and performance improvements on the Gigaspeech corpus. However, it primarily addresses ASR performance (WER reduction) without discussing spoken language models, full-duplex interaction, cognitive parallelism, or synchronization aspects critical to spoken language modeling or full-duplex spoken language models. The abstract lacks any mention of language modeling of speech or architectural/training aspects related to spoken language models or learned synchronization. Thus, it aligns more with traditional speech recognition evaluation rather than the inclusive criteria emphasizing advanced spoken language modeling and interactive synchronous dialogue capabilities.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using self-supervised discrete speech features within an ASR system, emphasizing acoustic context modeling and performance improvements on the Gigaspeech corpus. However, it primarily addresses ASR performance (WER reduction) without discussing spoken language models, full-duplex interaction, cognitive parallelism, or synchronization aspects critical to spoken language modeling or full-duplex spoken language models. The abstract lacks any mention of language modeling of speech or architectural/training aspects related to spoken language models or learned synchronization. Thus, it aligns more with traditional speech recognition evaluation rather than the inclusive criteria emphasizing advanced spoken language modeling and interactive synchronous dialogue capabilities.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Exploring SSL Discrete Tokens for Multilingual ASR",
    "abstract": "With the advancement of Self-supervised Learning (SSL) in speech-related tasks, there has been growing interest in utilizing discrete tokens generated by SSL for automatic speech recognition (ASR), as they offer faster processing techniques. However, previous studies primarily focused on multilingual ASR with Fbank features or English ASR with discrete tokens, leaving a gap in adapting discrete tokens for multilingual ASR scenarios. This study presents a comprehensive comparison of discrete tokens generated by various leading SSL models across multiple language domains. We aim to explore the performance and efficiency of speech discrete tokens across multiple language domains for both monolingual and multilingual ASR scenarios. Experimental results demonstrate that discrete tokens achieve comparable results against systems trained on Fbank features in ASR tasks across seven language domains with an average word error rate (WER) reduction of 0.31% and 1.76% absolute (2.80% and 15.70% relative) on dev and test sets respectively, with particularly WER reduction of 6.82% absolute (41.48% relative) on the Polish test set.",
    "metadata": {
      "arxiv_id": "2409.08805",
      "title": "Exploring SSL Discrete Tokens for Multilingual ASR",
      "summary": "With the advancement of Self-supervised Learning (SSL) in speech-related tasks, there has been growing interest in utilizing discrete tokens generated by SSL for automatic speech recognition (ASR), as they offer faster processing techniques. However, previous studies primarily focused on multilingual ASR with Fbank features or English ASR with discrete tokens, leaving a gap in adapting discrete tokens for multilingual ASR scenarios. This study presents a comprehensive comparison of discrete tokens generated by various leading SSL models across multiple language domains. We aim to explore the performance and efficiency of speech discrete tokens across multiple language domains for both monolingual and multilingual ASR scenarios. Experimental results demonstrate that discrete tokens achieve comparable results against systems trained on Fbank features in ASR tasks across seven language domains with an average word error rate (WER) reduction of 0.31% and 1.76% absolute (2.80% and 15.70% relative) on dev and test sets respectively, with particularly WER reduction of 6.82% absolute (41.48% relative) on the Polish test set.",
      "authors": [
        "Mingyu Cui",
        "Daxin Tan",
        "Yifan Yang",
        "Dingdong Wang",
        "Huimeng Wang",
        "Xiao Chen",
        "Xie Chen",
        "Xunying Liu"
      ],
      "published": "2024-09-13T13:13:39Z",
      "updated": "2024-09-13T13:13:39Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.08805v1",
      "landing_url": "https://arxiv.org/abs/2409.08805v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.08805"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項聚焦於以 SSL 產生的離散語音標記在多語言 ASR 的性能比較，未明確以 Spoken Language Model/語音語言模組為核心的模型架構與推理，且標題與摘要未出現相關的 spoken language model 或等價描述，屬於單一語音辨識技術的研究，符合排除條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項聚焦於以 SSL 產生的離散語音標記在多語言 ASR 的性能比較，未明確以 Spoken Language Model/語音語言模組為核心的模型架構與推理，且標題與摘要未出現相關的 spoken language model 或等價描述，屬於單一語音辨識技術的研究，符合排除條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using discrete tokens from self-supervised learning for multilingual ASR, emphasizing speech recognition performance improvements; however, it does not address spoken language modeling, full-duplex spoken interaction, synchronization, cognitive parallelism, or any dynamic interactive behavior, instead focusing primarily on ASR accuracy metrics (WER). It also lacks discussion on model architecture concerning spoken language models or synchronization mechanisms, failing to meet the core inclusion criteria, and aligns with several exclusion criteria such as focusing only on ASR and evaluation metrics limited to WER without interactive aspects.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using discrete tokens from self-supervised learning for multilingual ASR, emphasizing speech recognition performance improvements; however, it does not address spoken language modeling, full-duplex spoken interaction, synchronization, cognitive parallelism, or any dynamic interactive behavior, instead focusing primarily on ASR accuracy metrics (WER). It also lacks discussion on model architecture concerning spoken language models or synchronization mechanisms, failing to meet the core inclusion criteria, and aligns with several exclusion criteria such as focusing only on ASR and evaluation metrics limited to WER without interactive aspects.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Reputation-Driven Peer-to-Peer Live Streaming Architecture for Preventing Free-Riding",
    "abstract": "We present a peer-to-peer (P2P) live-streaming architecture designed to address challenges such as free-riding, malicious peers, churn, and network instability through the integration of a reputation system. The proposed algorithm incentivizes active peer participation while discouraging opportunistic behaviors, with a reputation mechanism that rewards altruistic peers and penalizes free riders and malicious actors. To manage peer dynamics, the algorithm continuously updates the strategies and adjusts to changing neighbors. It also implements a request-to-join mechanism for flash crowd scenarios, allowing the source node to delegate requests to child nodes, forming an interconnected tree structure that efficiently handles high demand and maintains system stability. The decentralized reputation mechanism promotes long-term sustainability in the P2P live streaming system.",
    "metadata": {
      "arxiv_id": "2409.09329",
      "title": "Reputation-Driven Peer-to-Peer Live Streaming Architecture for Preventing Free-Riding",
      "summary": "We present a peer-to-peer (P2P) live-streaming architecture designed to address challenges such as free-riding, malicious peers, churn, and network instability through the integration of a reputation system. The proposed algorithm incentivizes active peer participation while discouraging opportunistic behaviors, with a reputation mechanism that rewards altruistic peers and penalizes free riders and malicious actors. To manage peer dynamics, the algorithm continuously updates the strategies and adjusts to changing neighbors. It also implements a request-to-join mechanism for flash crowd scenarios, allowing the source node to delegate requests to child nodes, forming an interconnected tree structure that efficiently handles high demand and maintains system stability. The decentralized reputation mechanism promotes long-term sustainability in the P2P live streaming system.",
      "authors": [
        "Rashmi Kushwaha",
        "Rahul Bhattacharyya",
        "Yatindra Nath Singh"
      ],
      "published": "2024-09-14T06:16:15Z",
      "updated": "2024-09-14T06:16:15Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.09329v1",
      "landing_url": "https://arxiv.org/abs/2409.09329v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.09329"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on a P2P live streaming and reputation mechanism, not on Spoken Language Models or speech-language modeling; it lacks core SLM/FD-SLM topics, and does not address end-to-end speech modeling or real-time spoken interaction, thus it does not meet the inclusion criteria and aligns with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on a P2P live streaming and reputation mechanism, not on Spoken Language Models or speech-language modeling; it lacks core SLM/FD-SLM topics, and does not address end-to-end speech modeling or real-time spoken interaction, thus it does not meet the inclusion criteria and aligns with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a peer-to-peer live streaming architecture addressing free-riding and network instability, without mention of spoken language models, speech language modeling, or any related architecture focused on speech or voice-based modeling, training, or evaluation. It does not discuss full-duplex spoken language models, speech-LLM, or any speech-based interactive or cognitive parallelism aspects required by the inclusion criteria, and thus does not meet the core thematic requirements for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a peer-to-peer live streaming architecture addressing free-riding and network instability, without mention of spoken language models, speech language modeling, or any related architecture focused on speech or voice-based modeling, training, or evaluation. It does not discuss full-duplex spoken language models, speech-LLM, or any speech-based interactive or cognitive parallelism aspects required by the inclusion criteria, and thus does not meet the core thematic requirements for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Leveraging Self-Supervised Learning for Speaker Diarization",
    "abstract": "End-to-end neural diarization has evolved considerably over the past few years, but data scarcity is still a major obstacle for further improvements. Self-supervised learning methods such as WavLM have shown promising performance on several downstream tasks, but their application on speaker diarization is somehow limited. In this work, we explore using WavLM to alleviate the problem of data scarcity for neural diarization training. We use the same pipeline as Pyannote and improve the local end-to-end neural diarization with WavLM and Conformer. Experiments on far-field AMI, AISHELL-4, and AliMeeting datasets show that our method substantially outperforms the Pyannote baseline and achieves new state-of-the-art results on AMI and AISHELL-4, respectively. In addition, by analyzing the system performance under different data quantity scenarios, we show that WavLM representations are much more robust against data scarcity than filterbank features, enabling less data hungry training strategies. Furthermore, we found that simulated data, usually used to train endto-end diarization models, does not help when using WavLM in our experiments. Additionally, we also evaluate our model on the recent CHiME8 NOTSOFAR-1 task where it achieves better performance than the Pyannote baseline. Our source code is publicly available at https://github.com/BUTSpeechFIT/DiariZen.",
    "metadata": {
      "arxiv_id": "2409.09408",
      "title": "Leveraging Self-Supervised Learning for Speaker Diarization",
      "summary": "End-to-end neural diarization has evolved considerably over the past few years, but data scarcity is still a major obstacle for further improvements. Self-supervised learning methods such as WavLM have shown promising performance on several downstream tasks, but their application on speaker diarization is somehow limited. In this work, we explore using WavLM to alleviate the problem of data scarcity for neural diarization training. We use the same pipeline as Pyannote and improve the local end-to-end neural diarization with WavLM and Conformer. Experiments on far-field AMI, AISHELL-4, and AliMeeting datasets show that our method substantially outperforms the Pyannote baseline and achieves new state-of-the-art results on AMI and AISHELL-4, respectively. In addition, by analyzing the system performance under different data quantity scenarios, we show that WavLM representations are much more robust against data scarcity than filterbank features, enabling less data hungry training strategies. Furthermore, we found that simulated data, usually used to train endto-end diarization models, does not help when using WavLM in our experiments. Additionally, we also evaluate our model on the recent CHiME8 NOTSOFAR-1 task where it achieves better performance than the Pyannote baseline. Our source code is publicly available at https://github.com/BUTSpeechFIT/DiariZen.",
      "authors": [
        "Jiangyu Han",
        "Federico Landini",
        "Johan Rohdin",
        "Anna Silnova",
        "Mireia Diez",
        "Lukas Burget"
      ],
      "published": "2024-09-14T10:49:06Z",
      "updated": "2024-10-21T07:46:22Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.09408v3",
      "landing_url": "https://arxiv.org/abs/2409.09408v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.09408"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on speaker diarization using self-supervised speech representations (WavLM) rather than framing the work around Spoken Language Models or speech-language modeling as the core, which fails the explicit inclusion criterion requiring SLM/FD-SLM emphasis.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on speaker diarization using self-supervised speech representations (WavLM) rather than framing the work around Spoken Language Models or speech-language modeling as the core, which fails the explicit inclusion criterion requiring SLM/FD-SLM emphasis.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving speaker diarization using self-supervised learning representations (WavLM) but does not explicitly address Spoken Language Models or full-duplex spoken language modeling; it centers on diarization, which is a speech processing task rather than speech language modeling, and does not discuss modelling or evaluation of full-duplex interaction, synchronous turn-taking, or spoken language model architectures. Thus, it does not meet the core inclusion criteria focusing on speech language models or full-duplex spoken language modeling despite being a recent well-conducted research with evaluation.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving speaker diarization using self-supervised learning representations (WavLM) but does not explicitly address Spoken Language Models or full-duplex spoken language modeling; it centers on diarization, which is a speech processing task rather than speech language modeling, and does not discuss modelling or evaluation of full-duplex interaction, synchronous turn-taking, or spoken language model architectures. Thus, it does not meet the core inclusion criteria focusing on speech language models or full-duplex spoken language modeling despite being a recent well-conducted research with evaluation.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "StyleTTS-ZS: Efficient High-Quality Zero-Shot Text-to-Speech Synthesis with Distilled Time-Varying Style Diffusion",
    "abstract": "The rapid development of large-scale text-to-speech (TTS) models has led to significant advancements in modeling diverse speaker prosody and voices. However, these models often face issues such as slow inference speeds, reliance on complex pre-trained neural codec representations, and difficulties in achieving naturalness and high similarity to reference speakers. To address these challenges, this work introduces StyleTTS-ZS, an efficient zero-shot TTS model that leverages distilled time-varying style diffusion to capture diverse speaker identities and prosodies. We propose a novel approach that represents human speech using input text and fixed-length time-varying discrete style codes to capture diverse prosodic variations, trained adversarially with multi-modal discriminators. A diffusion model is then built to sample this time-varying style code for efficient latent diffusion. Using classifier-free guidance, StyleTTS-ZS achieves high similarity to the reference speaker in the style diffusion process. Furthermore, to expedite sampling, the style diffusion model is distilled with perceptual loss using only 10k samples, maintaining speech quality and similarity while reducing inference speed by 90%. Our model surpasses previous state-of-the-art large-scale zero-shot TTS models in both naturalness and similarity, offering a 10-20 faster sampling speed, making it an attractive alternative for efficient large-scale zero-shot TTS systems. The audio demo, code and models are available at https://styletts-zs.github.io/.",
    "metadata": {
      "arxiv_id": "2409.10058",
      "title": "StyleTTS-ZS: Efficient High-Quality Zero-Shot Text-to-Speech Synthesis with Distilled Time-Varying Style Diffusion",
      "summary": "The rapid development of large-scale text-to-speech (TTS) models has led to significant advancements in modeling diverse speaker prosody and voices. However, these models often face issues such as slow inference speeds, reliance on complex pre-trained neural codec representations, and difficulties in achieving naturalness and high similarity to reference speakers. To address these challenges, this work introduces StyleTTS-ZS, an efficient zero-shot TTS model that leverages distilled time-varying style diffusion to capture diverse speaker identities and prosodies. We propose a novel approach that represents human speech using input text and fixed-length time-varying discrete style codes to capture diverse prosodic variations, trained adversarially with multi-modal discriminators. A diffusion model is then built to sample this time-varying style code for efficient latent diffusion. Using classifier-free guidance, StyleTTS-ZS achieves high similarity to the reference speaker in the style diffusion process. Furthermore, to expedite sampling, the style diffusion model is distilled with perceptual loss using only 10k samples, maintaining speech quality and similarity while reducing inference speed by 90%. Our model surpasses previous state-of-the-art large-scale zero-shot TTS models in both naturalness and similarity, offering a 10-20 faster sampling speed, making it an attractive alternative for efficient large-scale zero-shot TTS systems. The audio demo, code and models are available at https://styletts-zs.github.io/.",
      "authors": [
        "Yinghao Aaron Li",
        "Xilin Jiang",
        "Cong Han",
        "Nima Mesgarani"
      ],
      "published": "2024-09-16T07:39:58Z",
      "updated": "2024-09-16T07:39:58Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.10058v1",
      "landing_url": "https://arxiv.org/abs/2409.10058v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.10058"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article is about StyleTTS-ZS, a zero-shot TTS model, and while it targets speech synthesis, it does not explicitly discuss Spoken Language Models or speech-language modeling, nor conversational synchronization aspects, so it does not meet inclusion criteria and aligns with exclusion criteria as a single-task TTS system.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article is about StyleTTS-ZS, a zero-shot TTS model, and while it targets speech synthesis, it does not explicitly discuss Spoken Language Models or speech-language modeling, nor conversational synchronization aspects, so it does not meet inclusion criteria and aligns with exclusion criteria as a single-task TTS system.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses primarily on a zero-shot text-to-speech synthesis model (StyleTTS-ZS) that improves the efficiency and quality of TTS by using novel style diffusion techniques; however, it does not mention spoken language models, full-duplex or synchronous interactive capabilities, or modeling speech as a core inference object, rather it centers on a speech synthesis architecture with primary goals of prosody and speaker similarity enhancement. Therefore, it does not meet the core inclusion criteria centered on Spoken Language Models or full-duplex speech interaction modeling, nor does it address cognitive parallelism, behavioral arbitration, or low-latency interactive synchronization. The focus on TTS alone and the lack of discussion on language modeling of speech or synchronous interactive behavior places it outside the scope of the inclusion criteria, and it aligns with exclusion criteria regarding single-task TTS focus without modeling or interaction considerations.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses primarily on a zero-shot text-to-speech synthesis model (StyleTTS-ZS) that improves the efficiency and quality of TTS by using novel style diffusion techniques; however, it does not mention spoken language models, full-duplex or synchronous interactive capabilities, or modeling speech as a core inference object, rather it centers on a speech synthesis architecture with primary goals of prosody and speaker similarity enhancement. Therefore, it does not meet the core inclusion criteria centered on Spoken Language Models or full-duplex speech interaction modeling, nor does it address cognitive parallelism, behavioral arbitration, or low-latency interactive synchronization. The focus on TTS alone and the lack of discussion on language modeling of speech or synchronous interactive behavior places it outside the scope of the inclusion criteria, and it aligns with exclusion criteria regarding single-task TTS focus without modeling or interaction considerations.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Ultra-Low Latency Speech Enhancement - A Comprehensive Study",
    "abstract": "Speech enhancement models should meet very low latency requirements typically smaller than 5 ms for hearing assistive devices. While various low-latency techniques have been proposed, comparing these methods in a controlled setup using DNNs remains blank. Previous papers have variations in task, training data, scripts, and evaluation settings, which make fair comparison impossible. Moreover, all methods are tested on small, simulated datasets, making it difficult to fairly assess their performance in real-world conditions, which could impact the reliability of scientific findings. To address these issues, we comprehensively investigate various low-latency techniques using consistent training on large-scale data and evaluate with more relevant metrics on real-world data. Specifically, we explore the effectiveness of asymmetric windows, learnable windows, adaptive time domain filterbanks, and the future-frame prediction technique. Additionally, we examine whether increasing the model size can compensate for the reduced window size, as well as the novel Mamba architecture in low-latency environments.",
    "metadata": {
      "arxiv_id": "2409.10358",
      "title": "Ultra-Low Latency Speech Enhancement - A Comprehensive Study",
      "summary": "Speech enhancement models should meet very low latency requirements typically smaller than 5 ms for hearing assistive devices. While various low-latency techniques have been proposed, comparing these methods in a controlled setup using DNNs remains blank. Previous papers have variations in task, training data, scripts, and evaluation settings, which make fair comparison impossible. Moreover, all methods are tested on small, simulated datasets, making it difficult to fairly assess their performance in real-world conditions, which could impact the reliability of scientific findings. To address these issues, we comprehensively investigate various low-latency techniques using consistent training on large-scale data and evaluate with more relevant metrics on real-world data. Specifically, we explore the effectiveness of asymmetric windows, learnable windows, adaptive time domain filterbanks, and the future-frame prediction technique. Additionally, we examine whether increasing the model size can compensate for the reduced window size, as well as the novel Mamba architecture in low-latency environments.",
      "authors": [
        "Haibin Wu",
        "Sebastian Braun"
      ],
      "published": "2024-09-16T15:06:47Z",
      "updated": "2024-09-16T15:06:47Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.10358v1",
      "landing_url": "https://arxiv.org/abs/2409.10358v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.10358"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "該研究聚焦於低延遲語音增強技術與模型架構，核心並非以Spoken Language Models/Full-Duplex SLMs的語音建模與端到端/混合架構為中心，也未提及同步互動、語音重疊處理等語音語言建模觀點，且缺乏明確為近三年的研究與相關評估框架的說明，因此不符合條件中的主題與時效要求。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "該研究聚焦於低延遲語音增強技術與模型架構，核心並非以Spoken Language Models/Full-Duplex SLMs的語音建模與端到端/混合架構為中心，也未提及同步互動、語音重疊處理等語音語言建模觀點，且缺乏明確為近三年的研究與相關評估框架的說明，因此不符合條件中的主題與時效要求。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on low-latency speech enhancement techniques relevant for hearing assistive devices, emphasizing engineering methods and evaluation on real-world data. However, it does not explicitly discuss Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models, nor does it address speech language modeling, turn-taking, or interactive dialogue behavior arbitration. The core subject revolves around acoustic enhancement and latency reduction rather than language modeling or dialogue synchronization, which falls outside the specified inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on low-latency speech enhancement techniques relevant for hearing assistive devices, emphasizing engineering methods and evaluation on real-world data. However, it does not explicitly discuss Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models, nor does it address speech language modeling, turn-taking, or interactive dialogue behavior arbitration. The core subject revolves around acoustic enhancement and latency reduction rather than language modeling or dialogue synchronization, which falls outside the specified inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation",
    "abstract": "Audio token modeling has become a powerful framework for speech synthesis, with two-stage approaches employing semantic tokens remaining prevalent. In this paper, we aim to simplify this process by introducing a semantic knowledge distillation method that enables high-quality speech generation in a single stage. Our proposed model improves speech quality, intelligibility, and speaker similarity compared to a single-stage baseline. Although two-stage systems still lead in intelligibility, our model significantly narrows the gap while delivering comparable speech quality. These findings showcase the potential of single-stage models to achieve efficient, high-quality TTS with a more compact and streamlined architecture.",
    "metadata": {
      "arxiv_id": "2409.11003",
      "title": "Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation",
      "summary": "Audio token modeling has become a powerful framework for speech synthesis, with two-stage approaches employing semantic tokens remaining prevalent. In this paper, we aim to simplify this process by introducing a semantic knowledge distillation method that enables high-quality speech generation in a single stage. Our proposed model improves speech quality, intelligibility, and speaker similarity compared to a single-stage baseline. Although two-stage systems still lead in intelligibility, our model significantly narrows the gap while delivering comparable speech quality. These findings showcase the potential of single-stage models to achieve efficient, high-quality TTS with a more compact and streamlined architecture.",
      "authors": [
        "Gerard I. Gállego",
        "Roy Fejgin",
        "Chunghsin Yeh",
        "Xiaoyu Liu",
        "Gautam Bhattacharya"
      ],
      "published": "2024-09-17T09:08:43Z",
      "updated": "2024-09-17T09:08:43Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11003v1",
      "landing_url": "https://arxiv.org/abs/2409.11003v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.11003"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Abstract centers on single-stage TTS with audio token modeling and semantic distillation, but it does not explicitly frame the work under Spoken Language Models or Speech Language Models, nor does it use the required terminology in the abstract/keywords, so it fails the core inclusion criterion despite recent publication and relevant topic.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Abstract centers on single-stage TTS with audio token modeling and semantic distillation, but it does not explicitly frame the work under Spoken Language Models or Speech Language Models, nor does it use the required terminology in the abstract/keywords, so it fails the core inclusion criterion despite recent publication and relevant topic.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on single-stage text-to-speech (TTS) synthesis using masked audio token modeling and semantic knowledge distillation but does not clearly discuss spoken language models or language modeling of speech as a core topic; it concentrates primarily on speech synthesis quality improvements rather than synchronization, dialogue behavior arbitration, or full-duplex spoken language models, and it lacks mention of interaction aspects such as cognitive parallelism, low latency, or turn-taking relevant to the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on single-stage text-to-speech (TTS) synthesis using masked audio token modeling and semantic knowledge distillation but does not clearly discuss spoken language models or language modeling of speech as a core topic; it concentrates primarily on speech synthesis quality improvements rather than synchronization, dialogue behavior arbitration, or full-duplex spoken language models, and it lacks mention of interaction aspects such as cognitive parallelism, low latency, or turn-taking relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Learning Source Disentanglement in Neural Audio Codec",
    "abstract": "Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.",
    "metadata": {
      "arxiv_id": "2409.11228",
      "title": "Learning Source Disentanglement in Neural Audio Codec",
      "summary": "Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.",
      "authors": [
        "Xiaoyu Bie",
        "Xubo Liu",
        "Gaël Richard"
      ],
      "published": "2024-09-17T14:21:02Z",
      "updated": "2025-02-11T10:35:04Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11228v2",
      "landing_url": "https://arxiv.org/abs/2409.11228v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.11228"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on neural audio codecs and source separation, not on Spoken Language Models or speech-language modeling as the core target, nor on interactive dialogue or FD-SLM concepts; therefore it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on neural audio codecs and source separation, not on Spoken Language Models or speech-language modeling as the core target, nor on interactive dialogue or FD-SLM concepts; therefore it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a neural audio codec for audio compression and source disentanglement across sound domains but does not mention spoken language models, speech language models, or language modeling of speech, nor does it address full-duplex spoken interaction, synchronization, or relevant behaviors such as turn-taking or overlapping speech; it also lacks discussion of model training or evaluation in the context of spoken language modeling, making it outside the defined inclusion scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a neural audio codec for audio compression and source disentanglement across sound domains but does not mention spoken language models, speech language models, or language modeling of speech, nor does it address full-duplex spoken interaction, synchronization, or relevant behaviors such as turn-taking or overlapping speech; it also lacks discussion of model training or evaluation in the context of spoken language modeling, making it outside the defined inclusion scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Speaking from Coarse to Fine: Improving Neural Codec Language Model via Multi-Scale Speech Coding and Generation",
    "abstract": "The neural codec language model (CLM) has demonstrated remarkable performance in text-to-speech (TTS) synthesis. However, troubled by ``recency bias\", CLM lacks sufficient attention to coarse-grained information at a higher temporal scale, often producing unnatural or even unintelligible speech. This work proposes CoFi-Speech, a coarse-to-fine CLM-TTS approach, employing multi-scale speech coding and generation to address this issue. We train a multi-scale neural codec, CoFi-Codec, to encode speech into a multi-scale discrete representation, comprising multiple token sequences with different time resolutions. Then, we propose CoFi-LM that can generate this representation in two modes: the single-LM-based chain-of-scale generation and the multiple-LM-based stack-of-scale generation. In experiments, CoFi-Speech significantly outperforms single-scale baseline systems on naturalness and speaker similarity in zero-shot TTS. The analysis of multi-scale coding demonstrates the effectiveness of CoFi-Codec in learning multi-scale discrete speech representations while keeping high-quality speech reconstruction. The coarse-to-fine multi-scale generation, especially for the stack-of-scale approach, is also validated as a crucial approach in pursuing a high-quality neural codec language model for TTS.",
    "metadata": {
      "arxiv_id": "2409.11630",
      "title": "Speaking from Coarse to Fine: Improving Neural Codec Language Model via Multi-Scale Speech Coding and Generation",
      "summary": "The neural codec language model (CLM) has demonstrated remarkable performance in text-to-speech (TTS) synthesis. However, troubled by ``recency bias\", CLM lacks sufficient attention to coarse-grained information at a higher temporal scale, often producing unnatural or even unintelligible speech. This work proposes CoFi-Speech, a coarse-to-fine CLM-TTS approach, employing multi-scale speech coding and generation to address this issue. We train a multi-scale neural codec, CoFi-Codec, to encode speech into a multi-scale discrete representation, comprising multiple token sequences with different time resolutions. Then, we propose CoFi-LM that can generate this representation in two modes: the single-LM-based chain-of-scale generation and the multiple-LM-based stack-of-scale generation. In experiments, CoFi-Speech significantly outperforms single-scale baseline systems on naturalness and speaker similarity in zero-shot TTS. The analysis of multi-scale coding demonstrates the effectiveness of CoFi-Codec in learning multi-scale discrete speech representations while keeping high-quality speech reconstruction. The coarse-to-fine multi-scale generation, especially for the stack-of-scale approach, is also validated as a crucial approach in pursuing a high-quality neural codec language model for TTS.",
      "authors": [
        "Haohan Guo",
        "Fenglong Xie",
        "Dongchao Yang",
        "Xixin Wu",
        "Helen Meng"
      ],
      "published": "2024-09-18T01:31:19Z",
      "updated": "2024-09-18T01:31:19Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11630v1",
      "landing_url": "https://arxiv.org/abs/2409.11630v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.11630"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work treats speech as the core modeling target with a multi-scale speech-token LM (CoFi-LM) for TTS, aligning with Spoken Language Models/SLM concepts, using multi-resolution speech representations, is within the 3-year window, and reports speech-quality metrics rather than only ASR/WER, satisfying the inclusion criteria.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "This work treats speech as the core modeling target with a multi-scale speech-token LM (CoFi-LM) for TTS, aligning with Spoken Language Models/SLM concepts, using multi-resolution speech representations, is within the 3-year window, and reports speech-quality metrics rather than only ASR/WER, satisfying the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a neural codec language model for text-to-speech synthesis, emphasizing multi-scale discrete speech representations and speech generation quality. However, it does not explicitly address spoken language models (SLMs) or full-duplex spoken language models, nor does it discuss synchronous interactive capabilities such as cognitive parallelism, speech overlap, backchanneling, or low-latency interaction as required by the inclusion criteria. The work primarily targets TTS quality improvements without broader spoken language modeling or synchronous conversational interaction perspectives. Therefore, it does not meet all inclusion criteria and overlaps with exclusion criteria focusing mainly on single speech technology without interactive or modeling evaluation.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a neural codec language model for text-to-speech synthesis, emphasizing multi-scale discrete speech representations and speech generation quality. However, it does not explicitly address spoken language models (SLMs) or full-duplex spoken language models, nor does it discuss synchronous interactive capabilities such as cognitive parallelism, speech overlap, backchanneling, or low-latency interaction as required by the inclusion criteria. The work primarily targets TTS quality improvements without broader spoken language modeling or synchronous conversational interaction perspectives. Therefore, it does not meet all inclusion criteria and overlaps with exclusion criteria focusing mainly on single speech technology without interactive or modeling evaluation.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers' opinions were considered: JuniorNano favored inclusion noting this is a speech-token neural codec language model, while JuniorMini recommended exclusion noting the work is TTS-focused and lacks interactive/full‑duplex considerations. The paper indeed proposes a neural codec language model and multi-scale discrete speech token generation (which is related to speech-level language modeling), but its scope and evaluation are squarely on TTS quality (zero-shot TTS naturalness and speaker similarity) and do not address spoken-dialogue, synchronous/full‑duplex interaction, low-latency overlap/interruption handling, or any behavior arbitration benchmarks required by the topic; this places it into the exclusion category “only focuses on ASR/TTS or single speech technology.” Therefore I recommend excluding. 1-sentence explanation: Although it uses a speech-token language model for TTS, it is a single-task TTS study without discussion or evaluation of spoken-language modeling for interactive/full‑duplex scenarios, so it should be excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers' opinions were considered: JuniorNano favored inclusion noting this is a speech-token neural codec language model, while JuniorMini recommended exclusion noting the work is TTS-focused and lacks interactive/full‑duplex considerations. The paper indeed proposes a neural codec language model and multi-scale discrete speech token generation (which is related to speech-level language modeling), but its scope and evaluation are squarely on TTS quality (zero-shot TTS naturalness and speaker similarity) and do not address spoken-dialogue, synchronous/full‑duplex interaction, low-latency overlap/interruption handling, or any behavior arbitration benchmarks required by the topic; this places it into the exclusion category “only focuses on ASR/TTS or single speech technology.” Therefore I recommend excluding. 1-sentence explanation: Although it uses a speech-token language model for TTS, it is a single-task TTS study without discussion or evaluation of spoken-language modeling for interactive/full‑duplex scenarios, so it should be excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Enabling Real-Time Conversations with Minimal Training Costs",
    "abstract": "Large language models (LLMs) have demonstrated the ability to improve human efficiency through conversational interactions. Conventional LLM-powered dialogue systems, operating on a turn-based paradigm, preclude real-time interaction during response generation. To address this limitation, researchers have proposed duplex models. These models can dynamically adapt to user input, facilitating real-time interactive feedback. However, these methods typically require substantial computational resources to acquire the ability. To reduce overhead, this paper presents a new duplex decoding approach that enhances LLMs with duplex ability, requiring minimal additional training. Specifically, our method employs parallel decoding of queries and responses in conversations, effectively implementing a channel-division-multiplexing decoding strategy. Experimental results indicate that our proposed method significantly enhances the naturalness and human-likeness of user-AI interactions with minimal training costs.",
    "metadata": {
      "arxiv_id": "2409.11727",
      "title": "Enabling Real-Time Conversations with Minimal Training Costs",
      "summary": "Large language models (LLMs) have demonstrated the ability to improve human efficiency through conversational interactions. Conventional LLM-powered dialogue systems, operating on a turn-based paradigm, preclude real-time interaction during response generation. To address this limitation, researchers have proposed duplex models. These models can dynamically adapt to user input, facilitating real-time interactive feedback. However, these methods typically require substantial computational resources to acquire the ability. To reduce overhead, this paper presents a new duplex decoding approach that enhances LLMs with duplex ability, requiring minimal additional training. Specifically, our method employs parallel decoding of queries and responses in conversations, effectively implementing a channel-division-multiplexing decoding strategy. Experimental results indicate that our proposed method significantly enhances the naturalness and human-likeness of user-AI interactions with minimal training costs.",
      "authors": [
        "Wang Xu",
        "Shuo Wang",
        "Weilin Zhao",
        "Xu Han",
        "Yukun Yan",
        "Yudi Zhang",
        "Zhe Tao",
        "Zhiyuan Liu",
        "Wanxiang Che"
      ],
      "published": "2024-09-18T06:27:26Z",
      "updated": "2024-09-18T06:27:26Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11727v1",
      "landing_url": "https://arxiv.org/abs/2409.11727v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.11727"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses enabling real-time conversations with minimal training costs in LLMs, but it does not center on Spoken Language Models or speech as the primary modeling target, nor does it address speech-oriented architecture, training, or evaluation; thus it fails the core inclusion criteria for speech-language modeling despite the recent timeframe.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item discusses enabling real-time conversations with minimal training costs in LLMs, but it does not center on Spoken Language Models or speech as the primary modeling target, nor does it address speech-oriented architecture, training, or evaluation; thus it fails the core inclusion criteria for speech-language modeling despite the recent timeframe.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on real-time conversations and duplex decoding in large language models, but it does not explicitly state that it models or processes speech or acoustic signals as core elements, nor does it mention spoken language models or speech-level language modeling. The approach seems to focus on text-based LLM interactive dialogue rather than spoken language modeling or full-duplex spoken interactions involving speech signals, which is the main inclusion criterion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on real-time conversations and duplex decoding in large language models, but it does not explicitly state that it models or processes speech or acoustic signals as core elements, nor does it mention spoken language models or speech-level language modeling. The approach seems to focus on text-based LLM interactive dialogue rather than spoken language modeling or full-duplex spoken interactions involving speech signals, which is the main inclusion criterion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference",
    "abstract": "Large language models (LLMs) have significantly advanced audio processing through audio codecs that convert audio into discrete tokens, enabling the application of language modeling techniques to audio data. However, audio codecs often operate at high frame rates, resulting in slow training and inference, especially for autoregressive models. To address this challenge, we present the Low Frame-rate Speech Codec (LFSC): a neural audio codec that leverages finite scalar quantization and adversarial training with large speech language models to achieve high-quality audio compression with a 1.89 kbps bitrate and 21.5 frames per second. We demonstrate that our novel codec can make the inference of LLM-based text-to-speech models around three times faster while improving intelligibility and producing quality comparable to previous models.",
    "metadata": {
      "arxiv_id": "2409.12117",
      "title": "Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference",
      "summary": "Large language models (LLMs) have significantly advanced audio processing through audio codecs that convert audio into discrete tokens, enabling the application of language modeling techniques to audio data. However, audio codecs often operate at high frame rates, resulting in slow training and inference, especially for autoregressive models. To address this challenge, we present the Low Frame-rate Speech Codec (LFSC): a neural audio codec that leverages finite scalar quantization and adversarial training with large speech language models to achieve high-quality audio compression with a 1.89 kbps bitrate and 21.5 frames per second. We demonstrate that our novel codec can make the inference of LLM-based text-to-speech models around three times faster while improving intelligibility and producing quality comparable to previous models.",
      "authors": [
        "Edresson Casanova",
        "Ryan Langman",
        "Paarth Neekhara",
        "Shehzeen Hussain",
        "Jason Li",
        "Subhankar Ghosh",
        "Ante Jukić",
        "Sang-gil Lee"
      ],
      "published": "2024-09-18T16:39:10Z",
      "updated": "2024-09-18T16:39:10Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.12117v1",
      "landing_url": "https://arxiv.org/abs/2409.12117v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.12117"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item explicitly centers on speech-language modeling within the context of LLMs (mentions ‘large speech language models’ and LLM-based TTS) and discusses architecture/training/evaluation aspects; it's a recent (2025) study and uses speech as the modeling object rather than treating it only as preprocessing. While it focuses on a codec to speed up LLM-based speech tasks and not on interactive dialogue, it still aligns with the spoken language model scope and evaluation beyond mere ASR/TTS metrics.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item explicitly centers on speech-language modeling within the context of LLMs (mentions ‘large speech language models’ and LLM-based TTS) and discusses architecture/training/evaluation aspects; it's a recent (2025) study and uses speech as the modeling object rather than treating it only as preprocessing. While it focuses on a codec to speed up LLM-based speech tasks and not on interactive dialogue, it still aligns with the spoken language model scope and evaluation beyond mere ASR/TTS metrics.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a low frame-rate speech codec for fast, high-quality speech LLM training and inference, which addresses improvements in speech tokenization efficiency for language modeling. However, it does not explicitly discuss Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) as core concepts, nor does it mention synchronous interaction capabilities, cognitive parallelism, or behavioral arbitration related to multi-turn or full-duplex conversational models. It also primarily emphasizes audio codec quality and inference speed rather than modeling architectures, evaluation of turn-taking, or interaction dynamics, thus failing to meet several inclusion criteria and possibly falling under exclusion points regarding single speech technologies without dialogue interaction or modeling focus.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a low frame-rate speech codec for fast, high-quality speech LLM training and inference, which addresses improvements in speech tokenization efficiency for language modeling. However, it does not explicitly discuss Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) as core concepts, nor does it mention synchronous interaction capabilities, cognitive parallelism, or behavioral arbitration related to multi-turn or full-duplex conversational models. It also primarily emphasizes audio codec quality and inference speed rather than modeling architectures, evaluation of turn-taking, or interaction dynamics, thus failing to meet several inclusion criteria and possibly falling under exclusion points regarding single speech technologies without dialogue interaction or modeling focus.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers split: one (round-A_JuniorNano) favored inclusion noting the paper mentions “large speech language models” and addresses tokenization/efficiency for speech LLMs; the other (round-A_JuniorMini) recommended exclusion because the work centers on a codec/TTS speedup and lacks discussion of Spoken Language Model architectures, interaction dynamics, or full‑duplex/turn‑taking evaluation. My assessment: although the abstract refers to “large speech language models” and language modeling of audio, the primary contribution is an engineering codec for lower frame-rate compression and faster LLM‑based TTS inference (single‑task/ASR–TTS style focus), without explicit SLM/FD‑SLM modeling, interaction/synchronization, or the required evaluation of real‑time conversational behaviors — therefore it meets the exclusion criteria (focus on single speech technology) and does not satisfy the inclusion requirements. Score rationale (one sentence): 2 — Better to exclude: relevant to SLM tooling but not a study whose core is Spoken Language Models or real-time/full‑duplex interaction or their evaluations.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers split: one (round-A_JuniorNano) favored inclusion noting the paper mentions “large speech language models” and addresses tokenization/efficiency for speech LLMs; the other (round-A_JuniorMini) recommended exclusion because the work centers on a codec/TTS speedup and lacks discussion of Spoken Language Model architectures, interaction dynamics, or full‑duplex/turn‑taking evaluation. My assessment: although the abstract refers to “large speech language models” and language modeling of audio, the primary contribution is an engineering codec for lower frame-rate compression and faster LLM‑based TTS inference (single‑task/ASR–TTS style focus), without explicit SLM/FD‑SLM modeling, interaction/synchronization, or the required evaluation of real‑time conversational behaviors — therefore it meets the exclusion criteria (focus on single speech technology) and does not satisfy the inclusion requirements. Score rationale (one sentence): 2 — Better to exclude: relevant to SLM tooling but not a study whose core is Spoken Language Models or real-time/full‑duplex interaction or their evaluations.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Takin: A Cohort of Superior Quality Zero-shot Speech Generation Models",
    "abstract": "With the advent of the big data and large language model era, zero-shot personalized rapid customization has emerged as a significant trend. In this report, we introduce Takin AudioLLM, a series of techniques and models, mainly including Takin TTS, Takin VC, and Takin Morphing, specifically designed for audiobook production. These models are capable of zero-shot speech production, generating high-quality speech that is nearly indistinguishable from real human speech and facilitating individuals to customize the speech content according to their own needs. Specifically, we first introduce Takin TTS, a neural codec language model that builds upon an enhanced neural speech codec and a multi-task training framework, capable of generating high-fidelity natural speech in a zero-shot way. For Takin VC, we advocate an effective content and timbre joint modeling approach to improve the speaker similarity, while advocating for a conditional flow matching based decoder to further enhance its naturalness and expressiveness. Last, we propose the Takin Morphing system with highly decoupled and advanced timbre and prosody modeling approaches, which enables individuals to customize speech production with their preferred timbre and prosody in a precise and controllable manner. Extensive experiments validate the effectiveness and robustness of our Takin AudioLLM series models. For detailed demos, please refer to https://everest-ai.github.io/takinaudiollm/.",
    "metadata": {
      "arxiv_id": "2409.12139",
      "title": "Takin: A Cohort of Superior Quality Zero-shot Speech Generation Models",
      "summary": "With the advent of the big data and large language model era, zero-shot personalized rapid customization has emerged as a significant trend. In this report, we introduce Takin AudioLLM, a series of techniques and models, mainly including Takin TTS, Takin VC, and Takin Morphing, specifically designed for audiobook production. These models are capable of zero-shot speech production, generating high-quality speech that is nearly indistinguishable from real human speech and facilitating individuals to customize the speech content according to their own needs. Specifically, we first introduce Takin TTS, a neural codec language model that builds upon an enhanced neural speech codec and a multi-task training framework, capable of generating high-fidelity natural speech in a zero-shot way. For Takin VC, we advocate an effective content and timbre joint modeling approach to improve the speaker similarity, while advocating for a conditional flow matching based decoder to further enhance its naturalness and expressiveness. Last, we propose the Takin Morphing system with highly decoupled and advanced timbre and prosody modeling approaches, which enables individuals to customize speech production with their preferred timbre and prosody in a precise and controllable manner. Extensive experiments validate the effectiveness and robustness of our Takin AudioLLM series models. For detailed demos, please refer to https://everest-ai.github.io/takinaudiollm/.",
      "authors": [
        "Sijing Chen",
        "Yuan Feng",
        "Laipeng He",
        "Tianwei He",
        "Wendi He",
        "Yanni Hu",
        "Bin Lin",
        "Yiting Lin",
        "Yu Pan",
        "Pengfei Tan",
        "Chengwei Tian",
        "Chen Wang",
        "Zhicheng Wang",
        "Ruoye Xie",
        "Jixun Yao",
        "Quanlei Yan",
        "Yuguang Yang",
        "Jianhao Ye",
        "Jingjing Yin",
        "Yanzhen Yu",
        "Huimin Zhang",
        "Xiang Zhang",
        "Guangcheng Zhao",
        "Hongbin Zhou",
        "Pengpeng Zou"
      ],
      "published": "2024-09-18T17:03:12Z",
      "updated": "2024-09-24T02:00:54Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.12139v3",
      "landing_url": "https://arxiv.org/abs/2409.12139v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.12139"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on zero-shot speech generation with language-model concepts (Takin AudioLLM, Takin TTS/VC/Morphing), discusses model architecture and training/evaluation, and uses a title/abstract that align with speech-language modeling concepts; it is within the last 3 years and goes beyond a single ASR/TTS task, meeting the inclusion criteria and not falling into the stated excludes (no requirement for full duplex interaction). Therefore, include.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item centers on zero-shot speech generation with language-model concepts (Takin AudioLLM, Takin TTS/VC/Morphing), discusses model architecture and training/evaluation, and uses a title/abstract that align with speech-language modeling concepts; it is within the last 3 years and goes beyond a single ASR/TTS task, meeting the inclusion criteria and not falling into the stated excludes (no requirement for full duplex interaction). Therefore, include.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces zero-shot speech generation models focused on high-quality speech synthesis for audiobook production, emphasizing neural codec language models and timbre/prosody modeling, but it does not explicitly address Spoken Language Models or full-duplex spoken language modeling with dialog interaction, synchronization, or multi-turn conversational capabilities. The core discussion is on speech synthesis technology rather than on spoken language modeling or interactive dialogue systems, and no evaluation involving real-time interaction or cognitive parallelism is provided, which are key inclusion criteria. Therefore, this study does not meet the inclusion criteria around Spoken Language Models and full-duplex interactive features, nor does it deal with speech-language modeling in the context required.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study introduces zero-shot speech generation models focused on high-quality speech synthesis for audiobook production, emphasizing neural codec language models and timbre/prosody modeling, but it does not explicitly address Spoken Language Models or full-duplex spoken language modeling with dialog interaction, synchronization, or multi-turn conversational capabilities. The core discussion is on speech synthesis technology rather than on spoken language modeling or interactive dialogue systems, and no evaluation involving real-time interaction or cognitive parallelism is provided, which are key inclusion criteria. Therefore, this study does not meet the inclusion criteria around Spoken Language Models and full-duplex interactive features, nor does it deal with speech-language modeling in the context required.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers were consulted: JuniorNano favored inclusion based on language-model framing, while JuniorMini correctly noted the paper focuses on speech synthesis rather than Spoken Language Models or full‑duplex interactive capabilities. The abstract describes zero‑shot TTS/VC/Morphing for audiobook production and neural codec LMs, but it does not use the term \"spoken language model(s)\" nor discuss spoken‑level language modeling, full‑duplex interaction, overlap/interrupt handling, low‑latency synchronization, or relevant interaction benchmarks — all required by the inclusion criteria. It therefore falls squarely into the exclusion criteria (single‑task TTS/VC focus without SLM/FD‑SLM contributions). Score explanation: This should be excluded (rating 2) because it does not meet the SLM/FD‑SLM inclusion requirements and instead addresses standalone speech synthesis.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers were consulted: JuniorNano favored inclusion based on language-model framing, while JuniorMini correctly noted the paper focuses on speech synthesis rather than Spoken Language Models or full‑duplex interactive capabilities. The abstract describes zero‑shot TTS/VC/Morphing for audiobook production and neural codec LMs, but it does not use the term \"spoken language model(s)\" nor discuss spoken‑level language modeling, full‑duplex interaction, overlap/interrupt handling, low‑latency synchronization, or relevant interaction benchmarks — all required by the inclusion criteria. It therefore falls squarely into the exclusion criteria (single‑task TTS/VC focus without SLM/FD‑SLM contributions). Score explanation: This should be excluded (rating 2) because it does not meet the SLM/FD‑SLM inclusion requirements and instead addresses standalone speech synthesis.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Large Language Models are Strong Audio-Visual Speech Recognition Learners",
    "abstract": "Multimodal large language models (MLLMs) have recently become a focal point of research due to their formidable multimodal understanding capabilities. For example, in the audio and speech domains, an LLM can be equipped with (automatic) speech recognition (ASR) abilities by just concatenating the audio tokens, computed with an audio encoder, and the text tokens to achieve state-of-the-art results. On the contrary, tasks like visual and audio-visual speech recognition (VSR/AVSR), which also exploit noise-invariant lip movement information, have received little or no attention. To bridge this gap, we propose Llama-AVSR, a new MLLM with strong audio-visual speech recognition capabilities. It leverages pre-trained audio and video encoders to produce modality-specific tokens which, together with the text tokens, are processed by a pre-trained LLM (e.g., Llama3.1-8B) to yield the resulting response in an auto-regressive fashion. Llama-AVSR requires a small number of trainable parameters as only modality-specific projectors and LoRA modules are trained whereas the multi-modal encoders and LLM are kept frozen. We evaluate our proposed approach on LRS3, the largest public AVSR benchmark, and we achieve new state-of-the-art results for the tasks of ASR and AVSR with a WER of 0.79% and 0.77%, respectively. To bolster our results, we investigate the key factors that underpin the effectiveness of Llama-AVSR: the choice of the pre-trained encoders and LLM, the efficient integration of LoRA modules, and the optimal performance-efficiency trade-off obtained via modality-aware compression rates.",
    "metadata": {
      "arxiv_id": "2409.12319",
      "title": "Large Language Models are Strong Audio-Visual Speech Recognition Learners",
      "summary": "Multimodal large language models (MLLMs) have recently become a focal point of research due to their formidable multimodal understanding capabilities. For example, in the audio and speech domains, an LLM can be equipped with (automatic) speech recognition (ASR) abilities by just concatenating the audio tokens, computed with an audio encoder, and the text tokens to achieve state-of-the-art results. On the contrary, tasks like visual and audio-visual speech recognition (VSR/AVSR), which also exploit noise-invariant lip movement information, have received little or no attention. To bridge this gap, we propose Llama-AVSR, a new MLLM with strong audio-visual speech recognition capabilities. It leverages pre-trained audio and video encoders to produce modality-specific tokens which, together with the text tokens, are processed by a pre-trained LLM (e.g., Llama3.1-8B) to yield the resulting response in an auto-regressive fashion. Llama-AVSR requires a small number of trainable parameters as only modality-specific projectors and LoRA modules are trained whereas the multi-modal encoders and LLM are kept frozen. We evaluate our proposed approach on LRS3, the largest public AVSR benchmark, and we achieve new state-of-the-art results for the tasks of ASR and AVSR with a WER of 0.79% and 0.77%, respectively. To bolster our results, we investigate the key factors that underpin the effectiveness of Llama-AVSR: the choice of the pre-trained encoders and LLM, the efficient integration of LoRA modules, and the optimal performance-efficiency trade-off obtained via modality-aware compression rates.",
      "authors": [
        "Umberto Cappellazzo",
        "Minsu Kim",
        "Honglie Chen",
        "Pingchuan Ma",
        "Stavros Petridis",
        "Daniele Falavigna",
        "Alessio Brutti",
        "Maja Pantic"
      ],
      "published": "2024-09-18T21:17:27Z",
      "updated": "2025-03-07T09:30:16Z",
      "categories": [
        "cs.CV",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.12319v2",
      "landing_url": "https://arxiv.org/abs/2409.12319v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.12319"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on spoken language modeling via audio-visual speech recognition within an LLM framework, discusses architecture and evaluation, is within the 3-year window, and does not appear to be limited to single-turn or pure ASR, so it should be included.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This item centers on spoken language modeling via audio-visual speech recognition within an LLM framework, discusses architecture and evaluation, is within the 3-year window, and does not appear to be limited to single-turn or pure ASR, so it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on audio-visual speech recognition using large language models, emphasizing integration of audio and visual tokens with LLMs to achieve state-of-the-art ASR and AVSR performance. However, it lacks explicit discussion or mention of Spoken Language Models or Speech Language Models as core modeling objects; also, it mainly centers on ASR and AVSR tasks with WER metric and does not address full-duplex synchronous interaction, cognitive parallelism, real-time feedback, or multi-dimensional evaluation aspects required for inclusion. The research does not demonstrate innovations in speech modeling beyond established ASR capabilities and does not satisfy inclusion criteria regarding spoken language modeling or interactive speech capabilities.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on audio-visual speech recognition using large language models, emphasizing integration of audio and visual tokens with LLMs to achieve state-of-the-art ASR and AVSR performance. However, it lacks explicit discussion or mention of Spoken Language Models or Speech Language Models as core modeling objects; also, it mainly centers on ASR and AVSR tasks with WER metric and does not address full-duplex synchronous interaction, cognitive parallelism, real-time feedback, or multi-dimensional evaluation aspects required for inclusion. The research does not demonstrate innovations in speech modeling beyond established ASR capabilities and does not satisfy inclusion criteria regarding spoken language modeling or interactive speech capabilities.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Reasoning: The paper describes Llama-AVSR, a multimodal LLM engineered for ASR and AVSR (audio+visual speech recognition) with strong WER performance; it treats speech as an input modality for recognition and does not mention Spoken Language Models or equivalent speech-level language modeling, nor does it address full‑duplex interaction, cognitive parallelism, overlap management, low-latency real‑time feedback, or synchronization — and its evaluation is limited to WER. Junior reviewers: one recommended inclusion (seems to have conflated strong AVSR performance with spoken-language-model research), the other correctly recommended exclusion. Final assessment: this study fails the inclusion criteria and triggers multiple exclusion criteria (focus on ASR/AVSR only, WER-only evaluation, speech used primarily as input to an LLM rather than as the core spoken-language-modeling object). One-sentence explanation: Exclude — the work is an ASR/AVSR engineering study and does not constitute research on Spoken Language Models or full‑duplex spoken interaction.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Reasoning: The paper describes Llama-AVSR, a multimodal LLM engineered for ASR and AVSR (audio+visual speech recognition) with strong WER performance; it treats speech as an input modality for recognition and does not mention Spoken Language Models or equivalent speech-level language modeling, nor does it address full‑duplex interaction, cognitive parallelism, overlap management, low-latency real‑time feedback, or synchronization — and its evaluation is limited to WER. Junior reviewers: one recommended inclusion (seems to have conflated strong AVSR performance with spoken-language-model research), the other correctly recommended exclusion. Final assessment: this study fails the inclusion criteria and triggers multiple exclusion criteria (focus on ASR/AVSR only, WER-only evaluation, speech used primarily as input to an LLM rather than as the core spoken-language-modeling object). One-sentence explanation: Exclude — the work is an ASR/AVSR engineering study and does not constitute research on Spoken Language Models or full‑duplex spoken interaction.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural audio codec models",
    "abstract": "Neural audio codec models are becoming increasingly important as they serve as tokenizers for audio, enabling efficient transmission or facilitating speech language modeling. The ideal neural audio codec should maintain content, paralinguistics, speaker characteristics, and audio information even at low bitrates. Recently, numerous advanced neural codec models have been proposed. However, codec models are often tested under varying experimental conditions. As a result, we introduce the Codec-SUPERB challenge at SLT 2024, designed to facilitate fair and lightweight comparisons among existing codec models and inspire advancements in the field. This challenge brings together representative speech applications and objective metrics, and carefully selects license-free datasets, sampling them into small sets to reduce evaluation computation costs. This paper presents the challenge's rules, datasets, five participant systems, results, and findings.",
    "metadata": {
      "arxiv_id": "2409.14085",
      "title": "Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural audio codec models",
      "summary": "Neural audio codec models are becoming increasingly important as they serve as tokenizers for audio, enabling efficient transmission or facilitating speech language modeling. The ideal neural audio codec should maintain content, paralinguistics, speaker characteristics, and audio information even at low bitrates. Recently, numerous advanced neural codec models have been proposed. However, codec models are often tested under varying experimental conditions. As a result, we introduce the Codec-SUPERB challenge at SLT 2024, designed to facilitate fair and lightweight comparisons among existing codec models and inspire advancements in the field. This challenge brings together representative speech applications and objective metrics, and carefully selects license-free datasets, sampling them into small sets to reduce evaluation computation costs. This paper presents the challenge's rules, datasets, five participant systems, results, and findings.",
      "authors": [
        "Haibin Wu",
        "Xuanjun Chen",
        "Yi-Cheng Lin",
        "Kaiwei Chang",
        "Jiawei Du",
        "Ke-Han Lu",
        "Alexander H. Liu",
        "Ho-Lam Chung",
        "Yuan-Kuei Wu",
        "Dongchao Yang",
        "Songxiang Liu",
        "Yi-Chiao Wu",
        "Xu Tan",
        "James Glass",
        "Shinji Watanabe",
        "Hung-yi Lee"
      ],
      "published": "2024-09-21T09:39:36Z",
      "updated": "2024-09-21T09:39:36Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.14085v1",
      "landing_url": "https://arxiv.org/abs/2409.14085v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.14085"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on a lightweight benchmark for neural audio codec models (Codec-SUPERB). It centers on audio codecs as signal processing tools and benchmarking datasets, not on Spoken Language Models or speech-language modeling of audio as the core object, nor on synchronous spoken interaction evaluation. Therefore it fails the inclusion criteria (core focus on SLM/FD-SLM) and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on a lightweight benchmark for neural audio codec models (Codec-SUPERB). It centers on audio codecs as signal processing tools and benchmarking datasets, not on Spoken Language Models or speech-language modeling of audio as the core object, nor on synchronous spoken interaction evaluation. Therefore it fails the inclusion criteria (core focus on SLM/FD-SLM) and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on benchmarking neural audio codec models, which are tokenizers for audio, primarily targeting audio compression and codec evaluation rather than spoken language modeling or full-duplex spoken interaction; it does not explicitly address spoken language models, speech language models, or full-duplex interaction capabilities, nor does it discuss model architectures focused on spoken language processing or synchronization as per inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on benchmarking neural audio codec models, which are tokenizers for audio, primarily targeting audio compression and codec evaluation rather than spoken language modeling or full-duplex spoken interaction; it does not explicitly address spoken language models, speech language models, or full-duplex interaction capabilities, nor does it discuss model architectures focused on spoken language processing or synchronization as per inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs",
    "abstract": "Large Language Models (LLMs) have excelled in various tasks but are still vulnerable to jailbreaking attacks, where attackers create jailbreak prompts to mislead the model to produce harmful or offensive content. Current jailbreak methods either rely heavily on manually crafted templates, which pose challenges in scalability and adaptability, or struggle to generate semantically coherent prompts, making them easy to detect. Additionally, most existing approaches involve lengthy prompts, leading to higher query costs. In this paper, to remedy these challenges, we introduce a novel jailbreaking attack framework called PAPILLON, which is an automated, black-box jailbreaking attack framework that adapts the black-box fuzz testing approach with a series of customized designs. Instead of relying on manually crafted templates,PAPILLON starts with an empty seed pool, removing the need to search for any related jailbreaking templates. We also develop three novel question-dependent mutation strategies using an LLM helper to generate prompts that maintain semantic coherence while significantly reducing their length. Additionally, we implement a two-level judge module to accurately detect genuine successful jailbreaks. We evaluated PAPILLON on 7 representative LLMs and compared it with 5 state-of-the-art jailbreaking attack strategies. For proprietary LLM APIs, such as GPT-3.5 turbo, GPT-4, and Gemini-Pro, PAPILLONs achieves attack success rates of over 90%, 80%, and 74%, respectively, exceeding existing baselines by more than 60\\%. Additionally, PAPILLON can maintain high semantic coherence while significantly reducing the length of jailbreak prompts. When targeting GPT-4, PAPILLON can achieve over 78% attack success rate even with 100 tokens. Moreover, PAPILLON demonstrates transferability and is robust to state-of-the-art defenses. Code: https://github.com/aaFrostnova/Papillon",
    "metadata": {
      "arxiv_id": "2409.14866",
      "title": "PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs",
      "summary": "Large Language Models (LLMs) have excelled in various tasks but are still vulnerable to jailbreaking attacks, where attackers create jailbreak prompts to mislead the model to produce harmful or offensive content. Current jailbreak methods either rely heavily on manually crafted templates, which pose challenges in scalability and adaptability, or struggle to generate semantically coherent prompts, making them easy to detect. Additionally, most existing approaches involve lengthy prompts, leading to higher query costs. In this paper, to remedy these challenges, we introduce a novel jailbreaking attack framework called PAPILLON, which is an automated, black-box jailbreaking attack framework that adapts the black-box fuzz testing approach with a series of customized designs. Instead of relying on manually crafted templates,PAPILLON starts with an empty seed pool, removing the need to search for any related jailbreaking templates. We also develop three novel question-dependent mutation strategies using an LLM helper to generate prompts that maintain semantic coherence while significantly reducing their length. Additionally, we implement a two-level judge module to accurately detect genuine successful jailbreaks. We evaluated PAPILLON on 7 representative LLMs and compared it with 5 state-of-the-art jailbreaking attack strategies. For proprietary LLM APIs, such as GPT-3.5 turbo, GPT-4, and Gemini-Pro, PAPILLONs achieves attack success rates of over 90%, 80%, and 74%, respectively, exceeding existing baselines by more than 60\\%. Additionally, PAPILLON can maintain high semantic coherence while significantly reducing the length of jailbreak prompts. When targeting GPT-4, PAPILLON can achieve over 78% attack success rate even with 100 tokens. Moreover, PAPILLON demonstrates transferability and is robust to state-of-the-art defenses. Code: https://github.com/aaFrostnova/Papillon",
      "authors": [
        "Xueluan Gong",
        "Mingzhe Li",
        "Yilin Zhang",
        "Fengyuan Ran",
        "Chen Chen",
        "Yanjiao Chen",
        "Qian Wang",
        "Kwok-Yan Lam"
      ],
      "published": "2024-09-23T10:03:09Z",
      "updated": "2025-03-03T07:25:21Z",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.14866v5",
      "landing_url": "https://arxiv.org/abs/2409.14866v5",
      "doi": "https://doi.org/10.48550/arXiv.2409.14866"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about jailbreaking LLMs and prompt design, with no focus on Spoken Language Models or speech-language modeling, nor on real-time interactive synchronous speech tasks; thus it does not meet the core inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about jailbreaking LLMs and prompt design, with no focus on Spoken Language Models or speech-language modeling, nor on real-time interactive synchronous speech tasks; thus it does not meet the core inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on jailbreaking attacks on Large Language Models (LLMs) via prompt generation and testing, which is centered on text-based language models rather than spoken language models or speech-level modeling. It does not specifically address speech signals, spoken language model architectures, full-duplex spoken interaction, or speech-related synchronization, which are critical for the inclusion criteria. Therefore, the research content does not meet the core inclusion criteria based on spoken language and speech modeling topics.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on jailbreaking attacks on Large Language Models (LLMs) via prompt generation and testing, which is centered on text-based language models rather than spoken language models or speech-level modeling. It does not specifically address speech signals, spoken language model architectures, full-duplex spoken interaction, or speech-related synchronization, which are critical for the inclusion criteria. Therefore, the research content does not meet the core inclusion criteria based on spoken language and speech modeling topics.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents",
    "abstract": "Despite broad interest in modeling spoken dialogue agents, most approaches are inherently \"half-duplex\" -- restricted to turn-based interaction with responses requiring explicit prompting by the user or implicit tracking of interruption or silence events. Human dialogue, by contrast, is \"full-duplex\" allowing for rich synchronicity in the form of quick and dynamic turn-taking, overlapping speech, and backchanneling. Technically, the challenge of achieving full-duplex dialogue with LLMs lies in modeling synchrony as pre-trained LLMs do not have a sense of \"time\". To bridge this gap, we propose Synchronous LLMs for full-duplex spoken dialogue modeling. We design a novel mechanism to integrate time information into Llama3-8b so that they run synchronously with the real-world clock. We also introduce a training recipe that uses 212k hours of synthetic spoken dialogue data generated from text dialogue data to create a model that generates meaningful and natural spoken dialogue, with just 2k hours of real-world spoken dialogue data. Synchronous LLMs outperform state-of-the-art in dialogue meaningfulness while maintaining naturalness. Finally, we demonstrate the model's ability to participate in full-duplex dialogue by simulating interaction between two agents trained on different datasets, while considering Internet-scale latencies of up to 240 ms. Webpage: https://syncllm.cs.washington.edu/.",
    "metadata": {
      "arxiv_id": "2409.15594",
      "title": "Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents",
      "summary": "Despite broad interest in modeling spoken dialogue agents, most approaches are inherently \"half-duplex\" -- restricted to turn-based interaction with responses requiring explicit prompting by the user or implicit tracking of interruption or silence events. Human dialogue, by contrast, is \"full-duplex\" allowing for rich synchronicity in the form of quick and dynamic turn-taking, overlapping speech, and backchanneling. Technically, the challenge of achieving full-duplex dialogue with LLMs lies in modeling synchrony as pre-trained LLMs do not have a sense of \"time\". To bridge this gap, we propose Synchronous LLMs for full-duplex spoken dialogue modeling. We design a novel mechanism to integrate time information into Llama3-8b so that they run synchronously with the real-world clock. We also introduce a training recipe that uses 212k hours of synthetic spoken dialogue data generated from text dialogue data to create a model that generates meaningful and natural spoken dialogue, with just 2k hours of real-world spoken dialogue data. Synchronous LLMs outperform state-of-the-art in dialogue meaningfulness while maintaining naturalness. Finally, we demonstrate the model's ability to participate in full-duplex dialogue by simulating interaction between two agents trained on different datasets, while considering Internet-scale latencies of up to 240 ms. Webpage: https://syncllm.cs.washington.edu/.",
      "authors": [
        "Bandhav Veluri",
        "Benjamin N Peloquin",
        "Bokai Yu",
        "Hongyu Gong",
        "Shyamnath Gollakota"
      ],
      "published": "2024-09-23T23:01:31Z",
      "updated": "2024-09-23T23:01:31Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.15594v1",
      "landing_url": "https://arxiv.org/abs/2409.15594v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.15594"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on Spoken Language Models / full-duplex spoken dialogue with synchronization, discusses architecture/training/evaluation for SLMs, and is within the past 3 years, with spoken-language concepts explicitly in the title/abstract, thus meeting inclusion and not meeting any exclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on Spoken Language Models / full-duplex spoken dialogue with synchronization, discusses architecture/training/evaluation for SLMs, and is within the past 3 years, with spoken-language concepts explicitly in the title/abstract, thus meeting inclusion and not meeting any exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly focuses on full-duplex spoken dialogue modeling using synchronous LLMs, integrating time information and training on large amounts of synthetic and real spoken dialogue data. It addresses spoken language modeling with emphasis on full-duplex capabilities, dynamic turn-taking, overlapping speech, and low-latency interaction, aligning well with the inclusion criteria's focus on spoken language models, speech-level modeling, and full-duplex dialogues. The work goes beyond turn-based (half-duplex) and includes evaluation of dialogue meaningfulness and naturalness, consistent with required evaluation dimensions. Therefore, it meets all inclusion and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly focuses on full-duplex spoken dialogue modeling using synchronous LLMs, integrating time information and training on large amounts of synthetic and real spoken dialogue data. It addresses spoken language modeling with emphasis on full-duplex capabilities, dynamic turn-taking, overlapping speech, and low-latency interaction, aligning well with the inclusion criteria's focus on spoken language models, speech-level modeling, and full-duplex dialogues. The work goes beyond turn-based (half-duplex) and includes evaluation of dialogue meaningfulness and naturalness, consistent with required evaluation dimensions. Therefore, it meets all inclusion and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ESPnet-Codec: Comprehensive Training and Evaluation of Neural Codecs for Audio, Music, and Speech",
    "abstract": "Neural codecs have become crucial to recent speech and audio generation research. In addition to signal compression capabilities, discrete codecs have also been found to enhance downstream training efficiency and compatibility with autoregressive language models. However, as extensive downstream applications are investigated, challenges have arisen in ensuring fair comparisons across diverse applications. To address these issues, we present a new open-source platform ESPnet-Codec, which is built on ESPnet and focuses on neural codec training and evaluation. ESPnet-Codec offers various recipes in audio, music, and speech for training and evaluation using several widely adopted codec models. Together with ESPnet-Codec, we present VERSA, a standalone evaluation toolkit, which provides a comprehensive evaluation of codec performance over 20 audio evaluation metrics. Notably, we demonstrate that ESPnet-Codec can be integrated into six ESPnet tasks, supporting diverse applications.",
    "metadata": {
      "arxiv_id": "2409.15897",
      "title": "ESPnet-Codec: Comprehensive Training and Evaluation of Neural Codecs for Audio, Music, and Speech",
      "summary": "Neural codecs have become crucial to recent speech and audio generation research. In addition to signal compression capabilities, discrete codecs have also been found to enhance downstream training efficiency and compatibility with autoregressive language models. However, as extensive downstream applications are investigated, challenges have arisen in ensuring fair comparisons across diverse applications. To address these issues, we present a new open-source platform ESPnet-Codec, which is built on ESPnet and focuses on neural codec training and evaluation. ESPnet-Codec offers various recipes in audio, music, and speech for training and evaluation using several widely adopted codec models. Together with ESPnet-Codec, we present VERSA, a standalone evaluation toolkit, which provides a comprehensive evaluation of codec performance over 20 audio evaluation metrics. Notably, we demonstrate that ESPnet-Codec can be integrated into six ESPnet tasks, supporting diverse applications.",
      "authors": [
        "Jiatong Shi",
        "Jinchuan Tian",
        "Yihan Wu",
        "Jee-weon Jung",
        "Jia Qi Yip",
        "Yoshiki Masuyama",
        "William Chen",
        "Yuning Wu",
        "Yuxun Tang",
        "Massa Baali",
        "Dareen Alharhi",
        "Dong Zhang",
        "Ruifan Deng",
        "Tejes Srivastava",
        "Haibin Wu",
        "Alexander H. Liu",
        "Bhiksha Raj",
        "Qin Jin",
        "Ruihua Song",
        "Shinji Watanabe"
      ],
      "published": "2024-09-24T09:16:11Z",
      "updated": "2025-02-24T18:34:41Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.15897v2",
      "landing_url": "https://arxiv.org/abs/2409.15897v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.15897"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the item discusses recent audio codecs and ESPnet-Codec, it does not center on Spoken Language Models or speech-language modeling as the core focus, instead focusing on neural codecs and evaluation, so it fails the inclusion criteria regardless of timeframe.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although the item discusses recent audio codecs and ESPnet-Codec, it does not center on Spoken Language Models or speech-language modeling as the core focus, instead focusing on neural codecs and evaluation, so it fails the inclusion criteria regardless of timeframe.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents ESPnet-Codec, a platform focused on neural codec training and evaluation for audio, music, and speech, but it mainly addresses codec compression and evaluation rather than spoken language modeling or full-duplex spoken language models; it lacks discussion on language modeling of speech or synchronous interactive behaviors as required by the inclusion criteria, and no mention of spoken language models or equivalent terms is found in title or abstract.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents ESPnet-Codec, a platform focused on neural codec training and evaluation for audio, music, and speech, but it mainly addresses codec compression and evaluation rather than spoken language modeling or full-duplex spoken language models; it lacks discussion on language modeling of speech or synchronous interactive behaviors as required by the inclusion criteria, and no mention of spoken language models or equivalent terms is found in title or abstract.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Enhancing IoT based Plant Health Monitoring through Advanced Human Plant Interaction using Large Language Models and Mobile Applications",
    "abstract": "This paper presents the development of a novel plant communication application that allows plants to \"talk\" to humans using real-time sensor data and AI-powered language models. Utilizing soil sensors that track moisture, temperature, and nutrient levels, the system feeds this data into the Gemini API, where it is processed and transformed into natural language insights about the plant's health and \"mood.\" Developed using Flutter, Firebase, and ThingSpeak, the app offers a seamless user experience with real-time interaction capabilities. By fostering human-plant connectivity, this system enhances plant care practices, promotes sustainability, and introduces innovative applications for AI and IoT technologies in both personal and agricultural contexts. The paper explores the technical architecture, system integration, and broader implications of AI-driven plant communication.",
    "metadata": {
      "arxiv_id": "2409.15910",
      "title": "Enhancing IoT based Plant Health Monitoring through Advanced Human Plant Interaction using Large Language Models and Mobile Applications",
      "summary": "This paper presents the development of a novel plant communication application that allows plants to \"talk\" to humans using real-time sensor data and AI-powered language models. Utilizing soil sensors that track moisture, temperature, and nutrient levels, the system feeds this data into the Gemini API, where it is processed and transformed into natural language insights about the plant's health and \"mood.\" Developed using Flutter, Firebase, and ThingSpeak, the app offers a seamless user experience with real-time interaction capabilities. By fostering human-plant connectivity, this system enhances plant care practices, promotes sustainability, and introduces innovative applications for AI and IoT technologies in both personal and agricultural contexts. The paper explores the technical architecture, system integration, and broader implications of AI-driven plant communication.",
      "authors": [
        "Kriti Agarwal",
        "Samhruth Ananthanarayanan",
        "Srinitish Srinivasan",
        "Abirami S"
      ],
      "published": "2024-09-24T09:26:47Z",
      "updated": "2025-01-05T18:21:23Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.15910v2",
      "landing_url": "https://arxiv.org/abs/2409.15910v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.15910"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although it uses AI language models to generate textual plant health insights from sensor data, the work does not center on Spoken Language Models or speech-language modeling (no speech-LLM architecture or full-duplex spoken interaction), so it fails the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although it uses AI language models to generate textual plant health insights from sensor data, the work does not center on Spoken Language Models or speech-language modeling (no speech-LLM architecture or full-duplex spoken interaction), so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an IoT and AI application for plant health monitoring using natural language insights generated from sensor data. It does not focus on spoken language models or speech-based modeling as the core research; instead, it centers around text-based natural language outputs and AI interpretation of sensor data. There is no mention of speech sequence modeling, full-duplex spoken language interaction, or synchronous real-time speech communication, which are required by the inclusion criteria. Therefore, it does not meet the inclusion criteria related to spoken language models and their interaction paradigms.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an IoT and AI application for plant health monitoring using natural language insights generated from sensor data. It does not focus on spoken language models or speech-based modeling as the core research; instead, it centers around text-based natural language outputs and AI interpretation of sensor data. There is no mention of speech sequence modeling, full-duplex spoken language interaction, or synchronous real-time speech communication, which are required by the inclusion criteria. Therefore, it does not meet the inclusion criteria related to spoken language models and their interaction paradigms.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "OffRIPP: Offline RL-based Informative Path Planning",
    "abstract": "Informative path planning (IPP) is a crucial task in robotics, where agents must design paths to gather valuable information about a target environment while adhering to resource constraints. Reinforcement learning (RL) has been shown to be effective for IPP, however, it requires environment interactions, which are risky and expensive in practice. To address this problem, we propose an offline RL-based IPP framework that optimizes information gain without requiring real-time interaction during training, offering safety and cost-efficiency by avoiding interaction, as well as superior performance and fast computation during execution -- key advantages of RL. Our framework leverages batch-constrained reinforcement learning to mitigate extrapolation errors, enabling the agent to learn from pre-collected datasets generated by arbitrary algorithms. We validate the framework through extensive simulations and real-world experiments. The numerical results show that our framework outperforms the baselines, demonstrating the effectiveness of the proposed approach.",
    "metadata": {
      "arxiv_id": "2409.16830",
      "title": "OffRIPP: Offline RL-based Informative Path Planning",
      "summary": "Informative path planning (IPP) is a crucial task in robotics, where agents must design paths to gather valuable information about a target environment while adhering to resource constraints. Reinforcement learning (RL) has been shown to be effective for IPP, however, it requires environment interactions, which are risky and expensive in practice. To address this problem, we propose an offline RL-based IPP framework that optimizes information gain without requiring real-time interaction during training, offering safety and cost-efficiency by avoiding interaction, as well as superior performance and fast computation during execution -- key advantages of RL. Our framework leverages batch-constrained reinforcement learning to mitigate extrapolation errors, enabling the agent to learn from pre-collected datasets generated by arbitrary algorithms. We validate the framework through extensive simulations and real-world experiments. The numerical results show that our framework outperforms the baselines, demonstrating the effectiveness of the proposed approach.",
      "authors": [
        "Srikar Babu Gadipudi",
        "Srujan Deolasee",
        "Siva Kailas",
        "Wenhao Luo",
        "Katia Sycara",
        "Woojun Kim"
      ],
      "published": "2024-09-25T11:30:59Z",
      "updated": "2024-09-25T11:30:59Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.16830v1",
      "landing_url": "https://arxiv.org/abs/2409.16830v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.16830"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on offline RL for informative path planning in robotics and discusses no spoken language models or speech-language architectures, therefore it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on offline RL for informative path planning in robotics and discusses no spoken language models or speech-language architectures, therefore it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on reinforcement learning for informative path planning in robotics and does not address spoken language models, speech language modeling, full-duplex spoken language interactions, or related speech processing and synchronization topics specified in the inclusion criteria. No mention of spoken language modeling or speech-centric modeling in the title or abstract is present, and the research is unrelated to the core domain of speech language models or full-duplex interaction. Therefore, it fails to meet the inclusion criteria and matches exclusion criteria implicitly by focusing on a different domain.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on reinforcement learning for informative path planning in robotics and does not address spoken language models, speech language modeling, full-duplex spoken language interactions, or related speech processing and synchronization topics specified in the inclusion criteria. No mention of spoken language modeling or speech-centric modeling in the title or abstract is present, and the research is unrelated to the core domain of speech language models or full-duplex interaction. Therefore, it fails to meet the inclusion criteria and matches exclusion criteria implicitly by focusing on a different domain.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "From Vision to Audio and Beyond: A Unified Model for Audio-Visual Representation and Generation",
    "abstract": "Video encompasses both visual and auditory data, creating a perceptually rich experience where these two modalities complement each other. As such, videos are a valuable type of media for the investigation of the interplay between audio and visual elements. Previous studies of audio-visual modalities primarily focused on either audio-visual representation learning or generative modeling of a modality conditioned on the other, creating a disconnect between these two branches. A unified framework that learns representation and generates modalities has not been developed yet. In this work, we introduce a novel framework called Vision to Audio and Beyond (VAB) to bridge the gap between audio-visual representation learning and vision-to-audio generation. The key approach of VAB is that rather than working with raw video frames and audio data, VAB performs representation learning and generative modeling within latent spaces. In particular, VAB uses a pre-trained audio tokenizer and an image encoder to obtain audio tokens and visual features, respectively. It then performs the pre-training task of visual-conditioned masked audio token prediction. This training strategy enables the model to engage in contextual learning and simultaneous video-to-audio generation. After the pre-training phase, VAB employs the iterative-decoding approach to rapidly generate audio tokens conditioned on visual features. Since VAB is a unified model, its backbone can be fine-tuned for various audio-visual downstream tasks. Our experiments showcase the efficiency of VAB in producing high-quality audio from video, and its capability to acquire semantic audio-visual features, leading to competitive results in audio-visual retrieval and classification.",
    "metadata": {
      "arxiv_id": "2409.19132",
      "title": "From Vision to Audio and Beyond: A Unified Model for Audio-Visual Representation and Generation",
      "summary": "Video encompasses both visual and auditory data, creating a perceptually rich experience where these two modalities complement each other. As such, videos are a valuable type of media for the investigation of the interplay between audio and visual elements. Previous studies of audio-visual modalities primarily focused on either audio-visual representation learning or generative modeling of a modality conditioned on the other, creating a disconnect between these two branches. A unified framework that learns representation and generates modalities has not been developed yet. In this work, we introduce a novel framework called Vision to Audio and Beyond (VAB) to bridge the gap between audio-visual representation learning and vision-to-audio generation. The key approach of VAB is that rather than working with raw video frames and audio data, VAB performs representation learning and generative modeling within latent spaces. In particular, VAB uses a pre-trained audio tokenizer and an image encoder to obtain audio tokens and visual features, respectively. It then performs the pre-training task of visual-conditioned masked audio token prediction. This training strategy enables the model to engage in contextual learning and simultaneous video-to-audio generation. After the pre-training phase, VAB employs the iterative-decoding approach to rapidly generate audio tokens conditioned on visual features. Since VAB is a unified model, its backbone can be fine-tuned for various audio-visual downstream tasks. Our experiments showcase the efficiency of VAB in producing high-quality audio from video, and its capability to acquire semantic audio-visual features, leading to competitive results in audio-visual retrieval and classification.",
      "authors": [
        "Kun Su",
        "Xiulong Liu",
        "Eli Shlizerman"
      ],
      "published": "2024-09-27T20:26:34Z",
      "updated": "2024-09-27T20:26:34Z",
      "categories": [
        "cs.MM",
        "cs.CV",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.19132v1",
      "landing_url": "https://arxiv.org/abs/2409.19132v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.19132"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on a unified audio-visual representation and generation framework rather than spoken-language-modeling of speech as the core object, so it does not meet the inclusion criteria that require SLM/FD-SLM emphasis and speech-centric modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on a unified audio-visual representation and generation framework rather than spoken-language-modeling of speech as the core object, so it does not meet the inclusion criteria that require SLM/FD-SLM emphasis and speech-centric modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on audio-visual representation and generation, specifically transforming visual features into audio tokens within latent spaces. However, it does not focus on Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) as core research subjects, nor does it address speech as a primary modeling object or discuss synchronous interaction, low-latency real-time processing, or cognitive parallelism. The research largely involves cross-modal generation rather than end-to-end speech language modeling or spoken dialogue behavior arbitration, and the key terms required by inclusion criteria (e.g., spoken language model(s), speech language model(s)) do not appear in the title or abstract. Thus, it does not meet the inclusion criteria and falls under exclusion for lacking core speech language modeling focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on audio-visual representation and generation, specifically transforming visual features into audio tokens within latent spaces. However, it does not focus on Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) as core research subjects, nor does it address speech as a primary modeling object or discuss synchronous interaction, low-latency real-time processing, or cognitive parallelism. The research largely involves cross-modal generation rather than end-to-end speech language modeling or spoken dialogue behavior arbitration, and the key terms required by inclusion criteria (e.g., spoken language model(s), speech language model(s)) do not appear in the title or abstract. Thus, it does not meet the inclusion criteria and falls under exclusion for lacking core speech language modeling focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Analyzing and Mitigating Inconsistency in Discrete Audio Tokens for Neural Codec Language Models",
    "abstract": "Building upon advancements in Large Language Models (LLMs), the field of audio processing has seen increased interest in training audio generation tasks with discrete audio token sequences. However, directly discretizing audio by neural audio codecs often results in sequences that fundamentally differ from text sequences. Unlike text, where text token sequences are deterministic, discrete audio tokens can exhibit significant variability based on contextual factors, while still producing perceptually identical audio segments. We refer to this phenomenon as \\textbf{Discrete Representation Inconsistency (DRI)}. This inconsistency can lead to a single audio segment being represented by multiple divergent sequences, which creates confusion in neural codec language models and results in omissions and repetitions during speech generation. In this paper, we quantitatively analyze the DRI phenomenon within popular audio tokenizers such as EnCodec. Our approach effectively mitigates the DRI phenomenon of the neural audio codec. Furthermore, extensive experiments on the neural codec language model over LibriTTS and large-scale MLS datases (44,000 hours) demonstrate the effectiveness and generality of our method. The demo of audio samples is available online~\\footnote{\\url{https://consistencyinneuralcodec.github.io}}.",
    "metadata": {
      "arxiv_id": "2409.19283",
      "title": "Analyzing and Mitigating Inconsistency in Discrete Audio Tokens for Neural Codec Language Models",
      "summary": "Building upon advancements in Large Language Models (LLMs), the field of audio processing has seen increased interest in training audio generation tasks with discrete audio token sequences. However, directly discretizing audio by neural audio codecs often results in sequences that fundamentally differ from text sequences. Unlike text, where text token sequences are deterministic, discrete audio tokens can exhibit significant variability based on contextual factors, while still producing perceptually identical audio segments. We refer to this phenomenon as \\textbf{Discrete Representation Inconsistency (DRI)}. This inconsistency can lead to a single audio segment being represented by multiple divergent sequences, which creates confusion in neural codec language models and results in omissions and repetitions during speech generation. In this paper, we quantitatively analyze the DRI phenomenon within popular audio tokenizers such as EnCodec. Our approach effectively mitigates the DRI phenomenon of the neural audio codec. Furthermore, extensive experiments on the neural codec language model over LibriTTS and large-scale MLS datases (44,000 hours) demonstrate the effectiveness and generality of our method. The demo of audio samples is available online~\\footnote{\\url{https://consistencyinneuralcodec.github.io}}.",
      "authors": [
        "Wenrui Liu",
        "Zhifang Guo",
        "Jin Xu",
        "Yuanjun Lv",
        "Yunfei Chu",
        "Zhou Zhao",
        "Junyang Lin"
      ],
      "published": "2024-09-28T08:36:44Z",
      "updated": "2024-10-04T22:34:38Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.19283v2",
      "landing_url": "https://arxiv.org/abs/2409.19283v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.19283"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "理由：此研究以語音為核心的語言模型（對離散語音 token 的建模與評估，稱為 neural codec language model）並在 LibriTTS/MLS 等資料集上驗證，屬於語音層級的語言建模，符合近年以 Spoken/Speech Language Models 等價描述的範疇，且討論架構與評估，符合納入與不排除的條件。",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "理由：此研究以語音為核心的語言模型（對離散語音 token 的建模與評估，稱為 neural codec language model）並在 LibriTTS/MLS 等資料集上驗證，屬於語音層級的語言建模，符合近年以 Spoken/Speech Language Models 等價描述的範疇，且討論架構與評估，符合納入與不排除的條件。",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on analyzing and mitigating inconsistency in discrete audio tokens for neural codec language models, which relates to language modeling of speech at the audio token level; however, it does not explicitly address spoken language models, full-duplex interaction, synchronization, or real-time interactive behavior as required by the inclusion criteria, and it lacks discussion on cognitive parallelism or multi-dimensional evaluation for interactive speech systems.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on analyzing and mitigating inconsistency in discrete audio tokens for neural codec language models, which relates to language modeling of speech at the audio token level; however, it does not explicitly address spoken language models, full-duplex interaction, synchronization, or real-time interactive behavior as required by the inclusion criteria, and it lacks discussion on cognitive parallelism or multi-dimensional evaluation for interactive speech systems.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior reviewers' comments (JuniorNano: include, JuniorMini: exclude because it lacks interaction/synchronization focus). The paper explicitly studies neural codec language models and language modeling of discrete audio tokens (mentions “neural codec language model” and analyzes token-level LM issues), so it directly addresses the key concept of speech-layer language modeling required by the inclusion criteria; it is not merely ASR/TTS or text-centric, and contains method and empirical evaluation on speech datasets, therefore it meets the inclusion requirements—while it does not target full-duplex interaction, the inclusion criteria accept works centered on speech-level language modeling (not every paper must cover FD-SLM specifics). Given this, I judge the paper should be included.",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior reviewers' comments (JuniorNano: include, JuniorMini: exclude because it lacks interaction/synchronization focus). The paper explicitly studies neural codec language models and language modeling of discrete audio tokens (mentions “neural codec language model” and analyzes token-level LM issues), so it directly addresses the key concept of speech-layer language modeling required by the inclusion criteria; it is not merely ASR/TTS or text-centric, and contains method and empirical evaluation on speech datasets, therefore it meets the inclusion requirements—while it does not target full-duplex interaction, the inclusion criteria accept works centered on speech-level language modeling (not every paper must cover FD-SLM specifics). Given this, I judge the paper should be included.",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Improving Spoken Language Modeling with Phoneme Classification: A Simple Fine-tuning Approach",
    "abstract": "Recent progress in Spoken Language Modeling has shown that learning language directly from speech is feasible. Generating speech through a pipeline that operates at the text level typically loses nuances, intonations, and non-verbal vocalizations. Modeling directly from speech opens up the path to more natural and expressive systems. On the other hand, speech-only systems require up to three orders of magnitude more data to catch up to their text-based counterparts in terms of their semantic abilities. We show that fine-tuning speech representation models on phoneme classification leads to more context-invariant representations, and language models trained on these units achieve comparable lexical comprehension to ones trained on hundred times more data.",
    "metadata": {
      "arxiv_id": "2410.00025",
      "title": "Improving Spoken Language Modeling with Phoneme Classification: A Simple Fine-tuning Approach",
      "summary": "Recent progress in Spoken Language Modeling has shown that learning language directly from speech is feasible. Generating speech through a pipeline that operates at the text level typically loses nuances, intonations, and non-verbal vocalizations. Modeling directly from speech opens up the path to more natural and expressive systems. On the other hand, speech-only systems require up to three orders of magnitude more data to catch up to their text-based counterparts in terms of their semantic abilities. We show that fine-tuning speech representation models on phoneme classification leads to more context-invariant representations, and language models trained on these units achieve comparable lexical comprehension to ones trained on hundred times more data.",
      "authors": [
        "Maxime Poli",
        "Emmanuel Chemla",
        "Emmanuel Dupoux"
      ],
      "published": "2024-09-16T10:29:15Z",
      "updated": "2024-10-30T17:46:22Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00025v2",
      "landing_url": "https://arxiv.org/abs/2410.00025v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.00025"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "該研究以語音為主要建模對象的 Spoken Language Modeling 為核心，標題含 spoken language modeling，且發表於近三年、內容涵蓋語音層面的模型訓練/評估，未僅限於 ASR/TTS 或單向任務，符合納入條件。",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "該研究以語音為主要建模對象的 Spoken Language Modeling 為核心，標題含 spoken language modeling，且發表於近三年、內容涵蓋語音層面的模型訓練/評估，未僅限於 ASR/TTS 或單向任務，符合納入條件。",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on improving spoken language modeling by fine-tuning speech representation models on phoneme classification, which clearly treats speech as the core modeling object rather than just pre-processing for text. The research emphasizes spoken language models and language modeling of speech, aligns with the concept of speech-level modeling, and discusses training and evaluation aspects. However, it lacks direct mention of full-duplex interaction, synchronous turn-taking, or evaluation involving real-time interaction or behavior arbitration, which are key parts of the inclusion criteria. The study falls short of addressing multi-dimensional dynamic evaluation or synchronicity aspects, leaning more towards improved speech representation and language modeling effectiveness rather than the interaction/behavior arbitration dimension. Hence, it meets some but not all criteria, and lacks full coverage of the interaction and real-time synchronization focus required.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The article focuses on improving spoken language modeling by fine-tuning speech representation models on phoneme classification, which clearly treats speech as the core modeling object rather than just pre-processing for text. The research emphasizes spoken language models and language modeling of speech, aligns with the concept of speech-level modeling, and discusses training and evaluation aspects. However, it lacks direct mention of full-duplex interaction, synchronous turn-taking, or evaluation involving real-time interaction or behavior arbitration, which are key parts of the inclusion criteria. The study falls short of addressing multi-dimensional dynamic evaluation or synchronicity aspects, leaning more towards improved speech representation and language modeling effectiveness rather than the interaction/behavior arbitration dimension. Hence, it meets some but not all criteria, and lacks full coverage of the interaction and real-time synchronization focus required.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the paper is centered on spoken-language modeling rather than ASR/TTS pipelines; the title and abstract explicitly mention \"Spoken Language Modeling\" and describe fine-tuning speech representation models (via phoneme classification) and training language models on speech-derived units, which satisfies the core inclusion requirements that speech be the primary modeling object and that training/evaluation are discussed. It does not, however, address full-duplex interaction, synchronization, or real-time turn-taking behaviors emphasized in the topic's special focus, but none of the exclusion criteria (being solely ASR/TTS, limited to WER/audio quality, or purely text-core research) are triggered. Brief explanation (1 sentence): The study should be included because it explicitly investigates spoken-language modeling and speech-centric representation/training (meeting the main inclusion criteria), despite not covering full-duplex interaction specifics required by the narrower special-focus aspects.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the paper is centered on spoken-language modeling rather than ASR/TTS pipelines; the title and abstract explicitly mention \"Spoken Language Modeling\" and describe fine-tuning speech representation models (via phoneme classification) and training language models on speech-derived units, which satisfies the core inclusion requirements that speech be the primary modeling object and that training/evaluation are discussed. It does not, however, address full-duplex interaction, synchronization, or real-time turn-taking behaviors emphasized in the topic's special focus, but none of the exclusion criteria (being solely ASR/TTS, limited to WER/audio quality, or purely text-core research) are triggered. Brief explanation (1 sentence): The study should be included because it explicitly investigates spoken-language modeling and speech-centric representation/training (meeting the main inclusion criteria), despite not covering full-duplex interaction specifics required by the narrower special-focus aspects.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Moshi: a speech-text foundation model for real-time dialogue",
    "abstract": "We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \"Inner Monologue\" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.",
    "metadata": {
      "arxiv_id": "2410.00037",
      "title": "Moshi: a speech-text foundation model for real-time dialogue",
      "summary": "We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \"Inner Monologue\" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.",
      "authors": [
        "Alexandre Défossez",
        "Laurent Mazaré",
        "Manu Orsini",
        "Amélie Royer",
        "Patrick Pérez",
        "Hervé Jégou",
        "Edouard Grave",
        "Neil Zeghidour"
      ],
      "published": "2024-09-17T17:55:39Z",
      "updated": "2024-10-02T09:11:45Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00037v2",
      "landing_url": "https://arxiv.org/abs/2410.00037v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.00037"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper Moshi focuses on a full-duplex spoken dialogue model with speech as the core modality, detailing architecture (speech-to-speech generation), real-time streaming recognition/TTS, and evaluation of latency and dialogue dynamics, aligning with FD-SLM and speech-language modeling criteria and not falling into exclusion categories.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper Moshi focuses on a full-duplex spoken dialogue model with speech as the core modality, detailing architecture (speech-to-speech generation), real-time streaming recognition/TTS, and evaluation of latency and dialogue dynamics, aligning with FD-SLM and speech-language modeling criteria and not falling into exclusion categories.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces Moshi, a speech-text foundation model addressing full-duplex spoken dialogue with low latency and overlapping speech handling, aligning well with the inclusion criteria focusing on spoken language models that emphasize speech as the core modeling object and advance full-duplex real-time interactions; it includes architecture and evaluation details and is recent work, thus meeting all inclusion and no exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study introduces Moshi, a speech-text foundation model addressing full-duplex spoken dialogue with low latency and overlapping speech handling, aligning well with the inclusion criteria focusing on spoken language models that emphasize speech as the core modeling object and advance full-duplex real-time interactions; it includes architecture and evaluation details and is recent work, thus meeting all inclusion and no exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Multi-Target Cross-Lingual Summarization: a novel task and a language-neutral approach",
    "abstract": "Cross-lingual summarization aims to bridge language barriers by summarizing documents in different languages. However, ensuring semantic coherence across languages is an overlooked challenge and can be critical in several contexts. To fill this gap, we introduce multi-target cross-lingual summarization as the task of summarizing a document into multiple target languages while ensuring that the produced summaries are semantically similar. We propose a principled re-ranking approach to this problem and a multi-criteria evaluation protocol to assess semantic coherence across target languages, marking a first step that will hopefully stimulate further research on this problem.",
    "metadata": {
      "arxiv_id": "2410.00502",
      "title": "Multi-Target Cross-Lingual Summarization: a novel task and a language-neutral approach",
      "summary": "Cross-lingual summarization aims to bridge language barriers by summarizing documents in different languages. However, ensuring semantic coherence across languages is an overlooked challenge and can be critical in several contexts. To fill this gap, we introduce multi-target cross-lingual summarization as the task of summarizing a document into multiple target languages while ensuring that the produced summaries are semantically similar. We propose a principled re-ranking approach to this problem and a multi-criteria evaluation protocol to assess semantic coherence across target languages, marking a first step that will hopefully stimulate further research on this problem.",
      "authors": [
        "Diogo Pernes",
        "Gonçalo M. Correia",
        "Afonso Mendes"
      ],
      "published": "2024-10-01T08:33:57Z",
      "updated": "2024-10-01T08:33:57Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00502v1",
      "landing_url": "https://arxiv.org/abs/2410.00502v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.00502"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "這篇論文聚焦於跨語言文字摘要與語義一致性，未以語音語言模型（Spoken Language Models / FD-SLMs）為核心，也未討論語音建模、語音互動或同步機制與評估，因此不符合 inclusion 條件且應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "這篇論文聚焦於跨語言文字摘要與語義一致性，未以語音語言模型（Spoken Language Models / FD-SLMs）為核心，也未討論語音建模、語音互動或同步機制與評估，因此不符合 inclusion 條件且應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multi-target cross-lingual summarization, which deals with text summarization across languages, and does not address spoken language models, speech as a core modeling object, or full-duplex spoken language interaction aspects required by the inclusion criteria; it also lacks any mention of speech-based modeling, real-time interaction, or speech-based synchronization mechanisms.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multi-target cross-lingual summarization, which deals with text summarization across languages, and does not address spoken language models, speech as a core modeling object, or full-duplex spoken language interaction aspects required by the inclusion criteria; it also lacks any mention of speech-based modeling, real-time interaction, or speech-based synchronization mechanisms.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "VHASR: A Multimodal Speech Recognition System With Vision Hotwords",
    "abstract": "The image-based multimodal automatic speech recognition (ASR) model enhances speech recognition performance by incorporating audio-related image. However, some works suggest that introducing image information to model does not help improving ASR performance. In this paper, we propose a novel approach effectively utilizing audio-related image information and set up VHASR, a multimodal speech recognition system that uses vision as hotwords to strengthen the model's speech recognition capability. Our system utilizes a dual-stream architecture, which firstly transcribes the text on the two streams separately, and then combines the outputs. We evaluate the proposed model on four datasets: Flickr8k, ADE20k, COCO, and OpenImages. The experimental results show that VHASR can effectively utilize key information in images to enhance the model's speech recognition ability. Its performance not only surpasses unimodal ASR, but also achieves SOTA among existing image-based multimodal ASR.",
    "metadata": {
      "arxiv_id": "2410.00822",
      "title": "VHASR: A Multimodal Speech Recognition System With Vision Hotwords",
      "summary": "The image-based multimodal automatic speech recognition (ASR) model enhances speech recognition performance by incorporating audio-related image. However, some works suggest that introducing image information to model does not help improving ASR performance. In this paper, we propose a novel approach effectively utilizing audio-related image information and set up VHASR, a multimodal speech recognition system that uses vision as hotwords to strengthen the model's speech recognition capability. Our system utilizes a dual-stream architecture, which firstly transcribes the text on the two streams separately, and then combines the outputs. We evaluate the proposed model on four datasets: Flickr8k, ADE20k, COCO, and OpenImages. The experimental results show that VHASR can effectively utilize key information in images to enhance the model's speech recognition ability. Its performance not only surpasses unimodal ASR, but also achieves SOTA among existing image-based multimodal ASR.",
      "authors": [
        "Jiliang Hu",
        "Zuchao Li",
        "Ping Wang",
        "Haojun Ai",
        "Lefei Zhang",
        "Hai Zhao"
      ],
      "published": "2024-10-01T16:06:02Z",
      "updated": "2024-10-04T18:30:06Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00822v2",
      "landing_url": "https://arxiv.org/abs/2410.00822v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.00822"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on image-augmented multimodal ASR and does not treat speech as the core object of a Spoken Language Model/FD-SLM; it lacks discussion of SLM architecture, training, or evaluation beyond ASR, thus fails inclusion and aligns with exclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The study focuses on image-augmented multimodal ASR and does not treat speech as the core object of a Spoken Language Model/FD-SLM; it lacks discussion of SLM architecture, training, or evaluation beyond ASR, thus fails inclusion and aligns with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a multimodal automatic speech recognition system that integrates vision to improve ASR performance; it centers on ASR, discusses model architecture and evaluation but does not explicitly address Spoken Language Models or full-duplex spoken interaction capabilities; nor does it mention speech language modeling or synchronous interaction, cognitive parallelism, or real-time dialogue arbitration as required by inclusion criteria, and its evaluation is limited to recognition performance rather than interactive aspects.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a multimodal automatic speech recognition system that integrates vision to improve ASR performance; it centers on ASR, discusses model architecture and evaluation but does not explicitly address Spoken Language Models or full-duplex spoken interaction capabilities; nor does it mention speech language modeling or synchronous interaction, cognitive parallelism, or real-time dialogue arbitration as required by inclusion criteria, and its evaluation is limited to recognition performance rather than interactive aspects.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Open-source shape optimization for isogeometric shells using FEniCS and OpenMDAO",
    "abstract": "We present an open-source Python framework for the shape optimization of complex shell structures using isogeometric analysis (IGA). IGA seamlessly integrates computer-aided design (CAD) and analysis models by employing non-uniform rational B-splines (NURBS) as basis functions, enabling the natural implementation of the Kirchhoff--Love shell model due to their higher order of continuity. We leverage the recently developed FEniCS-based analysis framework, PENGoLINS, for the direct structural analysis of shell structures consisting of a collection of NURBS patches through a penalty-based formulation. This contribution introduces the open-source implementation of gradient-based shape optimization for isogeometric Kirchhoff--Love shells with a modular architecture. Complex shell structures with non-matching intersections are handled using a free-form deformation (FFD) approach and a moving intersections formulation. The symbolic differentiation and code generation capabilities in FEniCS are utilized to compute the analytical derivatives. By integrating FEniCS with OpenMDAO, we build modular components that facilitate gradient-based shape optimization of shell structures. The modular architecture in this work supports future extensions and integration with other disciplines and solvers, making it highly customizable and suitable for a wide range of applications. We validate the design-analysis-optimization workflow through several benchmark problems and demonstrate its application to aircraft wing design optimization. The framework is implemented in a Python library named GOLDFISH (Gradient-based Optimization and Large-scale Design Framework for Isogeometric SHells) and the source code will be maintained at https://github.com/hanzhao2020/GOLDFISH.",
    "metadata": {
      "arxiv_id": "2410.02225",
      "title": "Open-source shape optimization for isogeometric shells using FEniCS and OpenMDAO",
      "summary": "We present an open-source Python framework for the shape optimization of complex shell structures using isogeometric analysis (IGA). IGA seamlessly integrates computer-aided design (CAD) and analysis models by employing non-uniform rational B-splines (NURBS) as basis functions, enabling the natural implementation of the Kirchhoff--Love shell model due to their higher order of continuity. We leverage the recently developed FEniCS-based analysis framework, PENGoLINS, for the direct structural analysis of shell structures consisting of a collection of NURBS patches through a penalty-based formulation. This contribution introduces the open-source implementation of gradient-based shape optimization for isogeometric Kirchhoff--Love shells with a modular architecture. Complex shell structures with non-matching intersections are handled using a free-form deformation (FFD) approach and a moving intersections formulation. The symbolic differentiation and code generation capabilities in FEniCS are utilized to compute the analytical derivatives. By integrating FEniCS with OpenMDAO, we build modular components that facilitate gradient-based shape optimization of shell structures. The modular architecture in this work supports future extensions and integration with other disciplines and solvers, making it highly customizable and suitable for a wide range of applications. We validate the design-analysis-optimization workflow through several benchmark problems and demonstrate its application to aircraft wing design optimization. The framework is implemented in a Python library named GOLDFISH (Gradient-based Optimization and Large-scale Design Framework for Isogeometric SHells) and the source code will be maintained at https://github.com/hanzhao2020/GOLDFISH.",
      "authors": [
        "Han Zhao",
        "John T. Hwang",
        "Jiun-Shyan Chen"
      ],
      "published": "2024-10-03T05:45:26Z",
      "updated": "2025-02-04T19:21:53Z",
      "categories": [
        "math.OC",
        "physics.comp-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.02225v2",
      "landing_url": "https://arxiv.org/abs/2410.02225v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.02225"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The topic is about isogeometric shell optimization (FEniCS/OpenMDAO) and not about Spoken Language Models or speech-language modeling per the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The topic is about isogeometric shell optimization (FEniCS/OpenMDAO) and not about Spoken Language Models or speech-language modeling per the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on an open-source framework for shape optimization of shell structures using isogeometric analysis; it does not discuss spoken language models, speech language modeling, or related concepts involving speech as the core modeling object nor addresses any synchronization or real-time interaction aspects required in the inclusion criteria. Therefore, it does not meet the inclusion criteria related to speech language models and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on an open-source framework for shape optimization of shell structures using isogeometric analysis; it does not discuss spoken language models, speech language modeling, or related concepts involving speech as the core modeling object nor addresses any synchronization or real-time interaction aspects required in the inclusion criteria. Therefore, it does not meet the inclusion criteria related to speech language models and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "CAX: Cellular Automata Accelerated in JAX",
    "abstract": "Cellular automata have become a cornerstone for investigating emergence and self-organization across diverse scientific disciplines. However, the absence of a hardware-accelerated cellular automata library limits the exploration of new research directions, hinders collaboration, and impedes reproducibility. In this work, we introduce CAX (Cellular Automata Accelerated in JAX), a high-performance and flexible open-source library designed to accelerate cellular automata research. CAX delivers cutting-edge performance through hardware acceleration while maintaining flexibility through its modular architecture, intuitive API, and support for both discrete and continuous cellular automata in arbitrary dimensions. We demonstrate CAX's performance and flexibility through a wide range of benchmarks and applications. From classic models like elementary cellular automata and Conway's Game of Life to advanced applications such as growing neural cellular automata and self-classifying MNIST digits, CAX speeds up simulations up to 2,000 times faster. Furthermore, we demonstrate CAX's potential to accelerate research by presenting a collection of three novel cellular automata experiments, each implemented in just a few lines of code thanks to the library's modular architecture. Notably, we show that a simple one-dimensional cellular automaton can outperform GPT-4 on the 1D-ARC challenge.",
    "metadata": {
      "arxiv_id": "2410.02651",
      "title": "CAX: Cellular Automata Accelerated in JAX",
      "summary": "Cellular automata have become a cornerstone for investigating emergence and self-organization across diverse scientific disciplines. However, the absence of a hardware-accelerated cellular automata library limits the exploration of new research directions, hinders collaboration, and impedes reproducibility. In this work, we introduce CAX (Cellular Automata Accelerated in JAX), a high-performance and flexible open-source library designed to accelerate cellular automata research. CAX delivers cutting-edge performance through hardware acceleration while maintaining flexibility through its modular architecture, intuitive API, and support for both discrete and continuous cellular automata in arbitrary dimensions. We demonstrate CAX's performance and flexibility through a wide range of benchmarks and applications. From classic models like elementary cellular automata and Conway's Game of Life to advanced applications such as growing neural cellular automata and self-classifying MNIST digits, CAX speeds up simulations up to 2,000 times faster. Furthermore, we demonstrate CAX's potential to accelerate research by presenting a collection of three novel cellular automata experiments, each implemented in just a few lines of code thanks to the library's modular architecture. Notably, we show that a simple one-dimensional cellular automaton can outperform GPT-4 on the 1D-ARC challenge.",
      "authors": [
        "Maxence Faldor",
        "Antoine Cully"
      ],
      "published": "2024-10-03T16:36:05Z",
      "updated": "2025-03-11T11:34:10Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.02651v2",
      "landing_url": "https://arxiv.org/abs/2410.02651v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.02651"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item concerns a cellular automata library (CAX) and hardware-accelerated simulation, not Spoken Language Models or speech-language modeling concepts; it fails all inclusion criteria and aligns with exclusion criteria focused on speech-language interaction and dialogue aspects.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item concerns a cellular automata library (CAX) and hardware-accelerated simulation, not Spoken Language Models or speech-language modeling concepts; it fails all inclusion criteria and aligns with exclusion criteria focused on speech-language interaction and dialogue aspects.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on cellular automata acceleration technology, with no mention of spoken language models, speech processing, full-duplex spoken interaction, or relevant architecture and evaluation metrics related to speech or language modeling; thus, it completely falls outside the inclusion criteria centered on spoken language models and related concepts.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on cellular automata acceleration technology, with no mention of spoken language models, speech processing, full-duplex spoken interaction, or relevant architecture and evaluation metrics related to speech or language modeling; thus, it completely falls outside the inclusion criteria centered on spoken language models and related concepts.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SyllableLM: Learning Coarse Semantic Units for Speech Language Models",
    "abstract": "Language models require tokenized inputs. However, tokenization strategies for continuous data like audio and vision are often based on simple heuristics such as fixed sized convolutions or discrete clustering, which do not necessarily align with the semantic structure of the data. For speech in particular, the high resolution of waveforms (16,000 samples/second or more) presents a significant challenge as speech-based language models have had to use several times more tokens per word than text-based language models. In this work, we introduce a controllable self-supervised technique to merge speech representations into coarser syllable-like units while still preserving semantic information. We do this by 1) extracting noisy boundaries through analyzing correlations in pretrained encoder losses and 2) iteratively improving model representations with a novel distillation technique. Our method produces controllable-rate semantic units at as low as 5Hz and 60bps and achieves SotA in syllabic segmentation and clustering. Using these coarse tokens, we successfully train SyllableLM, a Speech Language Model (SpeechLM) that matches or outperforms current SotA SpeechLMs on a range of spoken language modeling tasks. SyllableLM also achieves significant improvements in efficiency with a 30x reduction in training compute and a 4x wall-clock inference speedup.",
    "metadata": {
      "arxiv_id": "2410.04029",
      "title": "SyllableLM: Learning Coarse Semantic Units for Speech Language Models",
      "summary": "Language models require tokenized inputs. However, tokenization strategies for continuous data like audio and vision are often based on simple heuristics such as fixed sized convolutions or discrete clustering, which do not necessarily align with the semantic structure of the data. For speech in particular, the high resolution of waveforms (16,000 samples/second or more) presents a significant challenge as speech-based language models have had to use several times more tokens per word than text-based language models. In this work, we introduce a controllable self-supervised technique to merge speech representations into coarser syllable-like units while still preserving semantic information. We do this by 1) extracting noisy boundaries through analyzing correlations in pretrained encoder losses and 2) iteratively improving model representations with a novel distillation technique. Our method produces controllable-rate semantic units at as low as 5Hz and 60bps and achieves SotA in syllabic segmentation and clustering. Using these coarse tokens, we successfully train SyllableLM, a Speech Language Model (SpeechLM) that matches or outperforms current SotA SpeechLMs on a range of spoken language modeling tasks. SyllableLM also achieves significant improvements in efficiency with a 30x reduction in training compute and a 4x wall-clock inference speedup.",
      "authors": [
        "Alan Baade",
        "Puyuan Peng",
        "David Harwath"
      ],
      "published": "2024-10-05T04:29:55Z",
      "updated": "2024-10-05T04:29:55Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.04029v1",
      "landing_url": "https://arxiv.org/abs/2410.04029v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.04029"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item clearly centers on Spoken Language Models / Speech Language Models by modeling speech data directly with coarse syllable-like units and training a Speech Language Model (SyllableLM), including architecture, training, and evaluation, is within the last 3 years, and is not limited to ASR/TTS or non-interactive settings.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This item clearly centers on Spoken Language Models / Speech Language Models by modeling speech data directly with coarse syllable-like units and training a Speech Language Model (SyllableLM), including architecture, training, and evaluation, is within the last 3 years, and is not limited to ASR/TTS or non-interactive settings.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study centers on a Speech Language Model (SyllableLM) that handles speech data as primary modeling objects rather than text preprocessing; it presents a novel method for semantic tokenization of speech and includes evaluations across spoken language modeling tasks, aligning with inclusion criteria concerning spoken language models and their architecture and evaluation. However, the abstract does not explicitly mention full-duplex interaction, synchronous interaction management such as turn-taking or backchanneling, or behavioral arbitration, which are core aspects of the topic. There is no direct evidence about real-time interaction capabilities or multi-dimensional evaluation related to speech interaction behaviors. Thus, while it meets fundamental criteria for spoken language modeling, it lacks explicit coverage of advanced interactive aspects emphasized in the inclusion criteria, leading to a rating between uncertain and moderate inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study centers on a Speech Language Model (SyllableLM) that handles speech data as primary modeling objects rather than text preprocessing; it presents a novel method for semantic tokenization of speech and includes evaluations across spoken language modeling tasks, aligning with inclusion criteria concerning spoken language models and their architecture and evaluation. However, the abstract does not explicitly mention full-duplex interaction, synchronous interaction management such as turn-taking or backchanneling, or behavioral arbitration, which are core aspects of the topic. There is no direct evidence about real-time interaction capabilities or multi-dimensional evaluation related to speech interaction behaviors. Thus, while it meets fundamental criteria for spoken language modeling, it lacks explicit coverage of advanced interactive aspects emphasized in the inclusion criteria, leading to a rating between uncertain and moderate inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The abstract explicitly presents a Speech Language Model (SyllableLM) and a novel method for learning discrete/coarse speech tokens, describing model architecture/training and evaluations on spoken language modeling tasks (SoTA in segmentation/clustering and downstream SpeechLM performance); this satisfies the inclusion requirement that speech is the primary modeling object and that model/training/evaluation are discussed. I note the two junior reviews: JuniorNano rated 5 (inclusion) while JuniorMini rated 3, concerned about missing full-duplex/interaction details — however the inclusion criteria do not require full-duplex interaction features, only that the work focuses on spoken language models rather than only ASR/TTS, which this paper does. No exclusion criteria (ASR/TTS-only, text-centered work, or lack of method/evaluation) apply. Therefore I recommend inclusion.",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "The abstract explicitly presents a Speech Language Model (SyllableLM) and a novel method for learning discrete/coarse speech tokens, describing model architecture/training and evaluations on spoken language modeling tasks (SoTA in segmentation/clustering and downstream SpeechLM performance); this satisfies the inclusion requirement that speech is the primary modeling object and that model/training/evaluation are discussed. I note the two junior reviews: JuniorNano rated 5 (inclusion) while JuniorMini rated 3, concerned about missing full-duplex/interaction details — however the inclusion criteria do not require full-duplex interaction features, only that the work focuses on spoken language models rather than only ASR/TTS, which this paper does. No exclusion criteria (ASR/TTS-only, text-centered work, or lack of method/evaluation) apply. Therefore I recommend inclusion.",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis",
    "abstract": "Recently, Text-to-speech (TTS) models based on large language models (LLMs) that translate natural language text into sequences of discrete audio tokens have gained great research attention, with advances in neural audio codec (NAC) models using residual vector quantization (RVQ). However, long-form speech synthesis remains a significant challenge due to the high frame rate, which increases the length of audio tokens and makes it difficult for autoregressive language models to generate audio tokens for even a minute of speech. To address this challenge, this paper introduces two novel post-training approaches: 1) Multi-Resolution Requantization (MReQ) and 2) HALL-E. MReQ is a framework to reduce the frame rate of pre-trained NAC models. Specifically, it incorporates multi-resolution residual vector quantization (MRVQ) module that hierarchically reorganizes discrete audio tokens through teacher-student distillation. HALL-E is an LLM-based TTS model designed to predict hierarchical tokens of MReQ. Specifically, it incorporates the technique of using MRVQ sub-modules and continues training from a pre-trained LLM-based TTS model. Furthermore, to promote TTS research, we create MinutesSpeech, a new benchmark dataset consisting of 40k hours of filtered speech data for training and evaluating speech synthesis ranging from 3s up to 180s. In experiments, we demonstrated the effectiveness of our approaches by applying our post-training framework to VALL-E. We achieved the frame rate down to as low as 8 Hz, enabling the stable minitue-long speech synthesis in a single inference step. Audio samples, dataset, codes and pre-trained models are available at https://yutonishimura-v2.github.io/HALL-E_DEMO/.",
    "metadata": {
      "arxiv_id": "2410.04380",
      "title": "HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis",
      "summary": "Recently, Text-to-speech (TTS) models based on large language models (LLMs) that translate natural language text into sequences of discrete audio tokens have gained great research attention, with advances in neural audio codec (NAC) models using residual vector quantization (RVQ). However, long-form speech synthesis remains a significant challenge due to the high frame rate, which increases the length of audio tokens and makes it difficult for autoregressive language models to generate audio tokens for even a minute of speech. To address this challenge, this paper introduces two novel post-training approaches: 1) Multi-Resolution Requantization (MReQ) and 2) HALL-E. MReQ is a framework to reduce the frame rate of pre-trained NAC models. Specifically, it incorporates multi-resolution residual vector quantization (MRVQ) module that hierarchically reorganizes discrete audio tokens through teacher-student distillation. HALL-E is an LLM-based TTS model designed to predict hierarchical tokens of MReQ. Specifically, it incorporates the technique of using MRVQ sub-modules and continues training from a pre-trained LLM-based TTS model. Furthermore, to promote TTS research, we create MinutesSpeech, a new benchmark dataset consisting of 40k hours of filtered speech data for training and evaluating speech synthesis ranging from 3s up to 180s. In experiments, we demonstrated the effectiveness of our approaches by applying our post-training framework to VALL-E. We achieved the frame rate down to as low as 8 Hz, enabling the stable minitue-long speech synthesis in a single inference step. Audio samples, dataset, codes and pre-trained models are available at https://yutonishimura-v2.github.io/HALL-E_DEMO/.",
      "authors": [
        "Yuto Nishimura",
        "Takumi Hirose",
        "Masanari Ohi",
        "Hideki Nakayama",
        "Nakamasa Inoue"
      ],
      "published": "2024-10-06T07:20:58Z",
      "updated": "2024-10-06T07:20:58Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.04380v1",
      "landing_url": "https://arxiv.org/abs/2410.04380v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.04380"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work treats speech as the main modeling object with LLM-based TTS (HALL-E) using hierarchical audio tokens, discusses architecture, training, and evaluation, and references minute-long synthesis and a speech-focused benchmark, aligning with spoken-language/modeling criteria within the last 3 years.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work treats speech as the main modeling object with LLM-based TTS (HALL-E) using hierarchical audio tokens, discusses architecture, training, and evaluation, and references minute-long synthesis and a speech-focused benchmark, aligning with spoken-language/modeling criteria within the last 3 years.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on text-to-speech (TTS) synthesis using large language models to produce minute-long audio, but it primarily addresses the improvement of TTS quality and efficiency rather than Spoken Language Models where speech is the main modeling and inference object; it lacks discussion on full-duplex spoken interaction, cognitive parallelism, or behavioral arbitration, and does not explicitly state 'spoken language model(s)' or equivalent terms in the abstract or title, thus it mainly focuses on single-direction speech synthesis rather than synchronous spoken language modeling or multimodal interaction.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on text-to-speech (TTS) synthesis using large language models to produce minute-long audio, but it primarily addresses the improvement of TTS quality and efficiency rather than Spoken Language Models where speech is the main modeling and inference object; it lacks discussion on full-duplex spoken interaction, cognitive parallelism, or behavioral arbitration, and does not explicitly state 'spoken language model(s)' or equivalent terms in the abstract or title, thus it mainly focuses on single-direction speech synthesis rather than synchronous spoken language modeling or multimodal interaction.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "The paper explicitly treats speech as the primary modeling/inference object by using large language models to produce discrete audio tokens (i.e., language modeling of speech), describes model architecture/training (MReQ, MRVQ, HALL-E) and provides a large benchmark (MinutesSpeech), so it meets the inclusion requirement of speech-level language modeling despite not using the exact phrase “spoken language model(s)”; however it focuses on single-direction long-form TTS and does not address full‑duplex interaction or dialogue synchronization, so it is a relevant SLM paper but not a FD‑SLM study—thus I recommend inclusion but with moderate confidence.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "The paper explicitly treats speech as the primary modeling/inference object by using large language models to produce discrete audio tokens (i.e., language modeling of speech), describes model architecture/training (MReQ, MRVQ, HALL-E) and provides a large benchmark (MinutesSpeech), so it meets the inclusion requirement of speech-level language modeling despite not using the exact phrase “spoken language model(s)”; however it focuses on single-direction long-form TTS and does not address full‑duplex interaction or dialogue synchronization, so it is a relevant SLM paper but not a FD‑SLM study—thus I recommend inclusion but with moderate confidence.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "UniMuMo: Unified Text, Music and Motion Generation",
    "abstract": "We introduce UniMuMo, a unified multimodal model capable of taking arbitrary text, music, and motion data as input conditions to generate outputs across all three modalities. To address the lack of time-synchronized data, we align unpaired music and motion data based on rhythmic patterns to leverage existing large-scale music-only and motion-only datasets. By converting music, motion, and text into token-based representation, our model bridges these modalities through a unified encoder-decoder transformer architecture. To support multiple generation tasks within a single framework, we introduce several architectural improvements. We propose encoding motion with a music codebook, mapping motion into the same feature space as music. We introduce a music-motion parallel generation scheme that unifies all music and motion generation tasks into a single transformer decoder architecture with a single training task of music-motion joint generation. Moreover, the model is designed by fine-tuning existing pre-trained single-modality models, significantly reducing computational demands. Extensive experiments demonstrate that UniMuMo achieves competitive results on all unidirectional generation benchmarks across music, motion, and text modalities. Quantitative results are available in the \\href{https://hanyangclarence.github.io/unimumo_demo/}{project page}.",
    "metadata": {
      "arxiv_id": "2410.04534",
      "title": "UniMuMo: Unified Text, Music and Motion Generation",
      "summary": "We introduce UniMuMo, a unified multimodal model capable of taking arbitrary text, music, and motion data as input conditions to generate outputs across all three modalities. To address the lack of time-synchronized data, we align unpaired music and motion data based on rhythmic patterns to leverage existing large-scale music-only and motion-only datasets. By converting music, motion, and text into token-based representation, our model bridges these modalities through a unified encoder-decoder transformer architecture. To support multiple generation tasks within a single framework, we introduce several architectural improvements. We propose encoding motion with a music codebook, mapping motion into the same feature space as music. We introduce a music-motion parallel generation scheme that unifies all music and motion generation tasks into a single transformer decoder architecture with a single training task of music-motion joint generation. Moreover, the model is designed by fine-tuning existing pre-trained single-modality models, significantly reducing computational demands. Extensive experiments demonstrate that UniMuMo achieves competitive results on all unidirectional generation benchmarks across music, motion, and text modalities. Quantitative results are available in the \\href{https://hanyangclarence.github.io/unimumo_demo/}{project page}.",
      "authors": [
        "Han Yang",
        "Kun Su",
        "Yutong Zhang",
        "Jiaben Chen",
        "Kaizhi Qian",
        "Gaowen Liu",
        "Chuang Gan"
      ],
      "published": "2024-10-06T16:04:05Z",
      "updated": "2024-10-06T16:04:05Z",
      "categories": [
        "cs.SD",
        "cs.CV",
        "cs.GR",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.04534v1",
      "landing_url": "https://arxiv.org/abs/2410.04534v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.04534"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本文 UniMuMo 是跨模態的文本、音樂與運動生成，核心並非以語音模型或語音-語言建模為主，且未涵蓋語音互動/語音重疊等評估，因此不符合 Spoken Language Models 的核心要求，應排除。",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "本文 UniMuMo 是跨模態的文本、音樂與運動生成，核心並非以語音模型或語音-語言建模為主，且未涵蓋語音互動/語音重疊等評估，因此不符合 Spoken Language Models 的核心要求，應排除。",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a unified model for generating text, music, and motion data, with no emphasis on spoken language models or speech as the primary modeling object, nor does it address full-duplex spoken language modeling, real-time interaction, or speech-specific cognitive or behavioral arbitration as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a unified model for generating text, music, and motion data, with no emphasis on spoken language models or speech as the primary modeling object, nor does it address full-duplex spoken language modeling, real-time interaction, or speech-specific cognitive or behavioral arbitration as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Asynchronous Stochastic Gradient Descent with Decoupled Backpropagation and Layer-Wise Updates",
    "abstract": "The increasing size of deep learning models has made distributed training across multiple devices essential. However, current methods such as distributed data-parallel training suffer from large communication and synchronization overheads when training across devices, leading to longer training times as a result of suboptimal hardware utilization. Asynchronous stochastic gradient descent (ASGD) methods can improve training speed, but are sensitive to delays due to both communication and differences throughput. Moreover, the backpropagation algorithm used within ASGD workers is bottlenecked by the interlocking between its forward and backward passes. Current methods also do not take advantage of the large differences in the computation required for the forward and backward passes. Therefore, we propose an extension to ASGD called Partial Decoupled ASGD (PD-ASGD) that addresses these issues. PD-ASGD uses separate threads for the forward and backward passes, decoupling the updates and allowing for a higher ratio of forward to backward threads than the usual 1:1 ratio, leading to higher throughput. PD-ASGD also performs layer-wise (partial) model updates concurrently across multiple threads. This reduces parameter staleness and consequently improves robustness to delays. Our approach yields close to state-of-the-art results while running up to $5.95\\times$ faster than synchronous data parallelism in the presence of delays, and up to $2.14\\times$ times faster than comparable ASGD algorithms by achieving higher model flops utilization. We mathematically describe the gradient bias introduced by our method, establish an upper bound, and prove convergence.",
    "metadata": {
      "arxiv_id": "2410.05985",
      "title": "Asynchronous Stochastic Gradient Descent with Decoupled Backpropagation and Layer-Wise Updates",
      "summary": "The increasing size of deep learning models has made distributed training across multiple devices essential. However, current methods such as distributed data-parallel training suffer from large communication and synchronization overheads when training across devices, leading to longer training times as a result of suboptimal hardware utilization. Asynchronous stochastic gradient descent (ASGD) methods can improve training speed, but are sensitive to delays due to both communication and differences throughput. Moreover, the backpropagation algorithm used within ASGD workers is bottlenecked by the interlocking between its forward and backward passes. Current methods also do not take advantage of the large differences in the computation required for the forward and backward passes. Therefore, we propose an extension to ASGD called Partial Decoupled ASGD (PD-ASGD) that addresses these issues. PD-ASGD uses separate threads for the forward and backward passes, decoupling the updates and allowing for a higher ratio of forward to backward threads than the usual 1:1 ratio, leading to higher throughput. PD-ASGD also performs layer-wise (partial) model updates concurrently across multiple threads. This reduces parameter staleness and consequently improves robustness to delays. Our approach yields close to state-of-the-art results while running up to $5.95\\times$ faster than synchronous data parallelism in the presence of delays, and up to $2.14\\times$ times faster than comparable ASGD algorithms by achieving higher model flops utilization. We mathematically describe the gradient bias introduced by our method, establish an upper bound, and prove convergence.",
      "authors": [
        "Cabrel Teguemne Fokam",
        "Khaleelulla Khan Nazeer",
        "Lukas König",
        "David Kappel",
        "Anand Subramoney"
      ],
      "published": "2024-10-08T12:32:36Z",
      "updated": "2025-02-07T13:33:12Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05985v3",
      "landing_url": "https://arxiv.org/abs/2410.05985v3",
      "doi": "https://doi.org/10.48550/arXiv.2410.05985"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本項目聚焦分布式訓練中的 ASGD、分離前向/反向傳播等技術，與 Spoken Language Models 或語音語言建模無關，亦未涵蓋語音互動、同步處理或語音評估等相關議題，因此不符合納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本項目聚焦分布式訓練中的 ASGD、分離前向/反向傳播等技術，與 Spoken Language Models 或語音語言建模無關，亦未涵蓋語音互動、同步處理或語音評估等相關議題，因此不符合納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on asynchronous stochastic gradient descent methods to improve distributed training efficiency for deep learning models, without any mention of spoken language models, speech language models, or related speech-centric modeling or interaction paradigms such as full-duplex spoken language models, speech overlap, or turn-taking; it does not discuss spoken language model architectures, training, or evaluation, nor does it focus on speech data as the core modeling object. Therefore, it clearly does not meet the inclusion criteria requiring research to be centered on spoken language models or speech representations for interactive spoken language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on asynchronous stochastic gradient descent methods to improve distributed training efficiency for deep learning models, without any mention of spoken language models, speech language models, or related speech-centric modeling or interaction paradigms such as full-duplex spoken language models, speech overlap, or turn-taking; it does not discuss spoken language model architectures, training, or evaluation, nor does it focus on speech data as the core modeling object. Therefore, it clearly does not meet the inclusion criteria requiring research to be centered on spoken language models or speech representations for interactive spoken language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "An Eye for an Ear: Zero-shot Audio Description Leveraging an Image Captioner using Audiovisual Distribution Alignment",
    "abstract": "Multimodal large language models have fueled progress in image captioning. These models, fine-tuned on vast image datasets, exhibit a deep understanding of semantic concepts. In this work, we show that this ability can be re-purposed for audio captioning, where the joint image-language decoder can be leveraged to describe auditory content associated with image sequences within videos featuring audiovisual content. This can be achieved via multimodal alignment. Yet, this multimodal alignment task is non-trivial due to the inherent disparity between audible and visible elements in real-world videos. Moreover, multimodal representation learning often relies on contrastive learning, facing the challenge of the so-called modality gap which hinders smooth integration between modalities. In this work, we introduce a novel methodology for bridging the audiovisual modality gap by matching the distributions of tokens produced by an audio backbone and those of an image captioner. Our approach aligns the audio token distribution with that of the image tokens, enabling the model to perform zero-shot audio captioning in an unsupervised fashion while keeping the initial image captioning component unaltered. This alignment allows for the use of either audio or audiovisual input by combining or substituting the image encoder with the aligned audio encoder. Our method achieves significantly improved performances in zero-shot audio captioning, compared to existing approaches.",
    "metadata": {
      "arxiv_id": "2410.05997",
      "title": "An Eye for an Ear: Zero-shot Audio Description Leveraging an Image Captioner using Audiovisual Distribution Alignment",
      "summary": "Multimodal large language models have fueled progress in image captioning. These models, fine-tuned on vast image datasets, exhibit a deep understanding of semantic concepts. In this work, we show that this ability can be re-purposed for audio captioning, where the joint image-language decoder can be leveraged to describe auditory content associated with image sequences within videos featuring audiovisual content. This can be achieved via multimodal alignment. Yet, this multimodal alignment task is non-trivial due to the inherent disparity between audible and visible elements in real-world videos. Moreover, multimodal representation learning often relies on contrastive learning, facing the challenge of the so-called modality gap which hinders smooth integration between modalities. In this work, we introduce a novel methodology for bridging the audiovisual modality gap by matching the distributions of tokens produced by an audio backbone and those of an image captioner. Our approach aligns the audio token distribution with that of the image tokens, enabling the model to perform zero-shot audio captioning in an unsupervised fashion while keeping the initial image captioning component unaltered. This alignment allows for the use of either audio or audiovisual input by combining or substituting the image encoder with the aligned audio encoder. Our method achieves significantly improved performances in zero-shot audio captioning, compared to existing approaches.",
      "authors": [
        "Hugo Malard",
        "Michel Olvera",
        "Stéphane Lathuiliere",
        "Slim Essid"
      ],
      "published": "2024-10-08T12:52:48Z",
      "updated": "2024-10-08T12:52:48Z",
      "categories": [
        "eess.AS",
        "cs.CV",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05997v1",
      "landing_url": "https://arxiv.org/abs/2410.05997v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.05997"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the paper discusses zero-shot audio captioning using an image captioner, it does not treat speech as the central object of modeling (no explicit Spoken Language Model/FD-SLM core in the title/abstract) and it focuses on cross-modal audio-to-captioning rather than interactive, full-duplex speech systems; thus it fails the core inclusion criteria despite being recent.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the paper discusses zero-shot audio captioning using an image captioner, it does not treat speech as the central object of modeling (no explicit Spoken Language Model/FD-SLM core in the title/abstract) and it focuses on cross-modal audio-to-captioning rather than interactive, full-duplex speech systems; thus it fails the core inclusion criteria despite being recent.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on zero-shot audio captioning with multimodal alignment between audio and image tokens but does not mention spoken language models, speech language models, or core modeling of speech sequences; it lacks focus on full-duplex spoken language modeling, low-latency interactive speech modeling, or related evaluation metrics, thus not meeting the inclusion criteria focused on SLMs and FD-SLMs.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on zero-shot audio captioning with multimodal alignment between audio and image tokens but does not mention spoken language models, speech language models, or core modeling of speech sequences; it lacks focus on full-duplex spoken language modeling, low-latency interactive speech modeling, or related evaluation metrics, thus not meeting the inclusion criteria focused on SLMs and FD-SLMs.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and Computation Offloading Policies in Vehicular Edge Networks",
    "abstract": "Computer simulation platforms offer an alternative solution by emulating complex systems in a controlled manner. However, existing Edge Computing (EC) simulators, as well as general-purpose vehicular network simulators, are not tailored for VEC and lack dedicated support for modeling the distinct access pattern, entity mobility trajectory and other unique characteristics of VEC networks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation platform for in-depth evaluation and analysis of various service caching and computation offloading policies in VEC networks. VEC-Sim incorporates realistic mechanisms to replicate real-world access patterns, including service feature vector, vehicle mobility modeling, evolving service popularity, new service upload and user preference shifts, etc. Moreover, its modular architecture and extensive Application Programming Interfaces (APIs) allow seamless integration of customized scheduling policies and user-defined metrics. A comprehensive evaluation of VEC-Sim's capabilities is undertaken in comparison to real-world ground truths. Results prove it to be accurate in reproducing classical scheduling algorithms and extremely effective in conducting case studies.",
    "metadata": {
      "arxiv_id": "2410.06934",
      "title": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and Computation Offloading Policies in Vehicular Edge Networks",
      "summary": "Computer simulation platforms offer an alternative solution by emulating complex systems in a controlled manner. However, existing Edge Computing (EC) simulators, as well as general-purpose vehicular network simulators, are not tailored for VEC and lack dedicated support for modeling the distinct access pattern, entity mobility trajectory and other unique characteristics of VEC networks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation platform for in-depth evaluation and analysis of various service caching and computation offloading policies in VEC networks. VEC-Sim incorporates realistic mechanisms to replicate real-world access patterns, including service feature vector, vehicle mobility modeling, evolving service popularity, new service upload and user preference shifts, etc. Moreover, its modular architecture and extensive Application Programming Interfaces (APIs) allow seamless integration of customized scheduling policies and user-defined metrics. A comprehensive evaluation of VEC-Sim's capabilities is undertaken in comparison to real-world ground truths. Results prove it to be accurate in reproducing classical scheduling algorithms and extremely effective in conducting case studies.",
      "authors": [
        "Fan Wu",
        "Xiaolong Xu",
        "Muhammad Bilal",
        "Xiangwei Wang",
        "Hao Cheng",
        "Siyu Wu"
      ],
      "published": "2024-10-09T14:28:59Z",
      "updated": "2024-10-09T14:28:59Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06934v1",
      "landing_url": "https://arxiv.org/abs/2410.06934v1",
      "doi": "https://doi.org/10.1016/j.comnet.2024.110985"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on a VEC simulation platform for caching/offloading in vehicular networks and does not address Spoken Language Models or any speech-language modeling, thus it fails inclusion criteria and meets exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on a VEC simulation platform for caching/offloading in vehicular networks and does not address Spoken Language Models or any speech-language modeling, thus it fails inclusion criteria and meets exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a simulation platform for vehicular edge computing networks with service caching and computation offloading policies, without any mention of spoken language modeling, speech language models, or related concepts; therefore, it does not meet the inclusion criteria centered around spoken language models and speech-oriented research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a simulation platform for vehicular edge computing networks with service caching and computation offloading policies, without any mention of spoken language modeling, speech language models, or related concepts; therefore, it does not meet the inclusion criteria centered around spoken language models and speech-oriented research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Sylber: Syllabic Embedding Representation of Speech from Raw Audio",
    "abstract": "Syllables are compositional units of spoken language that efficiently structure human speech perception and production. However, current neural speech representations lack such structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised learning (SSL) framework that bootstraps syllabic embeddings by distilling from its own initial unsupervised syllabic segmentation. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) novel phonological units suited for efficient spoken language modeling. Our proposed segmentation method is highly robust and generalizes to out-of-domain data and unseen languages without any tuning. By training token-to-speech generative models, fully intelligible speech can be reconstructed from Sylber tokens with a significantly lower bitrate than baseline SSL tokens. This suggests that our model effectively compresses speech into a compact sequence of tokens with minimal information loss. Lastly, we demonstrate that categorical perception-a linguistic phenomenon in speech perception-emerges naturally in Sylber, making the embedding space more categorical and sparse than previous speech features and thus supporting the high efficiency of our tokenization. Together, we present a novel SSL approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling.",
    "metadata": {
      "arxiv_id": "2410.07168",
      "title": "Sylber: Syllabic Embedding Representation of Speech from Raw Audio",
      "summary": "Syllables are compositional units of spoken language that efficiently structure human speech perception and production. However, current neural speech representations lack such structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised learning (SSL) framework that bootstraps syllabic embeddings by distilling from its own initial unsupervised syllabic segmentation. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) novel phonological units suited for efficient spoken language modeling. Our proposed segmentation method is highly robust and generalizes to out-of-domain data and unseen languages without any tuning. By training token-to-speech generative models, fully intelligible speech can be reconstructed from Sylber tokens with a significantly lower bitrate than baseline SSL tokens. This suggests that our model effectively compresses speech into a compact sequence of tokens with minimal information loss. Lastly, we demonstrate that categorical perception-a linguistic phenomenon in speech perception-emerges naturally in Sylber, making the embedding space more categorical and sparse than previous speech features and thus supporting the high efficiency of our tokenization. Together, we present a novel SSL approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling.",
      "authors": [
        "Cheol Jun Cho",
        "Nicholas Lee",
        "Akshat Gupta",
        "Dhruv Agarwal",
        "Ethan Chen",
        "Alan W Black",
        "Gopala K. Anumanchipalli"
      ],
      "published": "2024-10-09T17:59:04Z",
      "updated": "2025-03-02T09:16:05Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07168v2",
      "landing_url": "https://arxiv.org/abs/2410.07168v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.07168"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on a spoken language modeling approach (syllabic embeddings and token-to-speech generation) and discusses end-to-end speech representations for efficient speech modeling, aligning with the core SLM/FD-SLM focus and recent literature timeframe (within last 3 years).",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on a spoken language modeling approach (syllabic embeddings and token-to-speech generation) and discusses end-to-end speech representations for efficient speech modeling, aligning with the core SLM/FD-SLM focus and recent literature timeframe (within last 3 years).",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on developing a new self-supervised speech representation model (Sylber) that structures speech at the syllabic level, aiming at efficient spoken language modeling and compression of speech tokens. However, although it deals with speech representation and modeling and mentions modeling benefits, it does not explicitly discuss Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it address synchronous interaction, turn-taking, or behavioral arbitration as required. The work is not focused on the real-time interactive dimensions, delay, or multi-modal synchronization. Therefore, it meets some but not all of the inclusion criteria and does not show evidence to exclude based on the exclusion criteria, placing it in an uncertain zone regarding inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on developing a new self-supervised speech representation model (Sylber) that structures speech at the syllabic level, aiming at efficient spoken language modeling and compression of speech tokens. However, although it deals with speech representation and modeling and mentions modeling benefits, it does not explicitly discuss Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it address synchronous interaction, turn-taking, or behavioral arbitration as required. The work is not focused on the real-time interactive dimensions, delay, or multi-modal synchronization. Therefore, it meets some but not all of the inclusion criteria and does not show evidence to exclude based on the exclusion criteria, placing it in an uncertain zone regarding inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: JuniorNano rated include (5) noting alignment with SLM/FD‑SLM focus; JuniorMini rated unsure (3) because the abstract lacks explicit discussion of interactive/full‑duplex behaviors. My assessment: the abstract explicitly uses the phrase “spoken language modeling” and centers on speech‑centric representations (syllabic tokenization, compact tokens, and token‑to‑speech generative modeling), so it treats speech as the primary modeling object and advances components relevant to Spoken Language Models rather than being purely ASR/TTS; it does not, however, address full‑duplex interaction or turn‑taking evaluation. One‑sentence explanation for score: include (4) — the paper clearly targets spoken language modeling and introduces speech‑centric tokenization useful for SLMs, but it lacks FD‑SLM/interactive evaluation so I recommend inclusion with moderate confidence.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: JuniorNano rated include (5) noting alignment with SLM/FD‑SLM focus; JuniorMini rated unsure (3) because the abstract lacks explicit discussion of interactive/full‑duplex behaviors. My assessment: the abstract explicitly uses the phrase “spoken language modeling” and centers on speech‑centric representations (syllabic tokenization, compact tokens, and token‑to‑speech generative modeling), so it treats speech as the primary modeling object and advances components relevant to Spoken Language Models rather than being purely ASR/TTS; it does not, however, address full‑duplex interaction or turn‑taking evaluation. One‑sentence explanation for score: include (4) — the paper clearly targets spoken language modeling and introduces speech‑centric tokenization useful for SLMs, but it lacks FD‑SLM/interactive evaluation so I recommend inclusion with moderate confidence.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer",
    "abstract": "Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0",
    "metadata": {
      "arxiv_id": "2410.08325",
      "title": "Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer",
      "summary": "Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0",
      "authors": [
        "Slava Shechtman",
        "Avihu Dekel"
      ],
      "published": "2024-10-10T19:29:05Z",
      "updated": "2024-10-10T19:29:05Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08325v1",
      "landing_url": "https://arxiv.org/abs/2410.08325v1",
      "doi": "https://doi.org/10.21437/Interspeech.2024-2366"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on discrete speech tokenizers for audio and their use with LLMs, but it does not clearly treat Spoken Language Models / FD-SLMs as the core object nor discuss architecture, training, or evaluation of SLMs; it centers on tokenization/codec design rather than speech-language modeling per se, and lacks explicit inclusion of SLM in title/abstract/keywords.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on discrete speech tokenizers for audio and their use with LLMs, but it does not clearly treat Spoken Language Models / FD-SLMs as the core object nor discuss architecture, training, or evaluation of SLMs; it centers on tokenization/codec design rather than speech-language modeling per se, and lacks explicit inclusion of SLM in title/abstract/keywords.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on fine-tuning a discrete audio tokenizer (RVQGAN) for speech reconstruction at low bitrates. However, it does not explicitly discuss spoken language models, full-duplex interaction, speech language modeling as a core focus, or synchronous interactive behaviors like cognitive parallelism or low-latency feedback. The evaluation focuses on speech reconstruction quality rather than interactive behaviors or language modeling. Therefore, it does not meet the inclusion criteria centered on spoken language models and synchronous interaction, and meets some exclusion points related to focusing purely on audio codec/tokenizer without speech language modeling or interactive dialogue aspects.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on fine-tuning a discrete audio tokenizer (RVQGAN) for speech reconstruction at low bitrates. However, it does not explicitly discuss spoken language models, full-duplex interaction, speech language modeling as a core focus, or synchronous interactive behaviors like cognitive parallelism or low-latency feedback. The evaluation focuses on speech reconstruction quality rather than interactive behaviors or language modeling. Therefore, it does not meet the inclusion criteria centered on spoken language models and synchronous interaction, and meets some exclusion points related to focusing purely on audio codec/tokenizer without speech language modeling or interactive dialogue aspects.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Graph Neural Alchemist: An innovative fully modular architecture for time series-to-graph classification",
    "abstract": "This paper introduces a novel Graph Neural Network (GNN) architecture for time series classification, based on visibility graph representations. Traditional time series classification methods often struggle with high computational complexity and inadequate capture of spatio-temporal dynamics. By representing time series as visibility graphs, it is possible to encode both spatial and temporal dependencies inherent to time series data, while being computationally efficient. Our architecture is fully modular, enabling flexible experimentation with different models and representations. We employ directed visibility graphs encoded with in-degree and PageRank features to improve the representation of time series, ensuring efficient computation while enhancing the model's ability to capture long-range dependencies in the data. We show the robustness and generalization capability of the proposed architecture across a diverse set of classification tasks and against a traditional model. Our work represents a significant advancement in the application of GNNs for time series analysis, offering a powerful and flexible framework for future research and practical implementations.",
    "metadata": {
      "arxiv_id": "2410.09307",
      "title": "Graph Neural Alchemist: An innovative fully modular architecture for time series-to-graph classification",
      "summary": "This paper introduces a novel Graph Neural Network (GNN) architecture for time series classification, based on visibility graph representations. Traditional time series classification methods often struggle with high computational complexity and inadequate capture of spatio-temporal dynamics. By representing time series as visibility graphs, it is possible to encode both spatial and temporal dependencies inherent to time series data, while being computationally efficient. Our architecture is fully modular, enabling flexible experimentation with different models and representations. We employ directed visibility graphs encoded with in-degree and PageRank features to improve the representation of time series, ensuring efficient computation while enhancing the model's ability to capture long-range dependencies in the data. We show the robustness and generalization capability of the proposed architecture across a diverse set of classification tasks and against a traditional model. Our work represents a significant advancement in the application of GNNs for time series analysis, offering a powerful and flexible framework for future research and practical implementations.",
      "authors": [
        "Paulo Coelho",
        "Raul Araju",
        "Luís Ramos",
        "Samir Saliba",
        "Renato Vimieiro"
      ],
      "published": "2024-10-12T00:03:40Z",
      "updated": "2024-10-12T00:03:40Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.09307v1",
      "landing_url": "https://arxiv.org/abs/2410.09307v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.09307"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on time series-to-graph classification using GNNs and visibility graphs, not on Spoken Language Models or speech-based synchronous interaction, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on time series-to-graph classification using GNNs and visibility graphs, not on Spoken Language Models or speech-based synchronous interaction, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a Graph Neural Network architecture for time series classification, specifically representing time series as visibility graphs, without any mention of spoken language models, speech language modeling, or full-duplex spoken language interaction. It does not address speech or acoustic signal modeling, synchronization, or related evaluation criteria from the inclusion criteria, and instead deals with a generic time series classification task.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a Graph Neural Network architecture for time series classification, specifically representing time series as visibility graphs, without any mention of spoken language models, speech language modeling, or full-duplex spoken language interaction. It does not address speech or acoustic signal modeling, synchronization, or related evaluation criteria from the inclusion criteria, and instead deals with a generic time series classification task.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "HART: Efficient Visual Generation with Hybrid Autoregressive Transformer",
    "abstract": "We introduce Hybrid Autoregressive Transformer (HART), an autoregressive (AR) visual generation model capable of directly generating 1024x1024 images, rivaling diffusion models in image generation quality. Existing AR models face limitations due to the poor image reconstruction quality of their discrete tokenizers and the prohibitive training costs associated with generating 1024px images. To address these challenges, we present the hybrid tokenizer, which decomposes the continuous latents from the autoencoder into two components: discrete tokens representing the big picture and continuous tokens representing the residual components that cannot be represented by the discrete tokens. The discrete component is modeled by a scalable-resolution discrete AR model, while the continuous component is learned with a lightweight residual diffusion module with only 37M parameters. Compared with the discrete-only VAR tokenizer, our hybrid approach improves reconstruction FID from 2.11 to 0.30 on MJHQ-30K, leading to a 31% generation FID improvement from 7.85 to 5.38. HART also outperforms state-of-the-art diffusion models in both FID and CLIP score, with 4.5-7.7x higher throughput and 6.9-13.4x lower MACs. Our code is open sourced at https://github.com/mit-han-lab/hart.",
    "metadata": {
      "arxiv_id": "2410.10812",
      "title": "HART: Efficient Visual Generation with Hybrid Autoregressive Transformer",
      "summary": "We introduce Hybrid Autoregressive Transformer (HART), an autoregressive (AR) visual generation model capable of directly generating 1024x1024 images, rivaling diffusion models in image generation quality. Existing AR models face limitations due to the poor image reconstruction quality of their discrete tokenizers and the prohibitive training costs associated with generating 1024px images. To address these challenges, we present the hybrid tokenizer, which decomposes the continuous latents from the autoencoder into two components: discrete tokens representing the big picture and continuous tokens representing the residual components that cannot be represented by the discrete tokens. The discrete component is modeled by a scalable-resolution discrete AR model, while the continuous component is learned with a lightweight residual diffusion module with only 37M parameters. Compared with the discrete-only VAR tokenizer, our hybrid approach improves reconstruction FID from 2.11 to 0.30 on MJHQ-30K, leading to a 31% generation FID improvement from 7.85 to 5.38. HART also outperforms state-of-the-art diffusion models in both FID and CLIP score, with 4.5-7.7x higher throughput and 6.9-13.4x lower MACs. Our code is open sourced at https://github.com/mit-han-lab/hart.",
      "authors": [
        "Haotian Tang",
        "Yecheng Wu",
        "Shang Yang",
        "Enze Xie",
        "Junsong Chen",
        "Junyu Chen",
        "Zhuoyang Zhang",
        "Han Cai",
        "Yao Lu",
        "Song Han"
      ],
      "published": "2024-10-14T17:59:42Z",
      "updated": "2024-10-14T17:59:42Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.10812v1",
      "landing_url": "https://arxiv.org/abs/2410.10812v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.10812"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on visual generation with a Hybrid Autoregressive Transformer and a hybrid tokenizer, with no emphasis on spoken language modeling, speech interaction, or dialogue synchronization, so it does not meet the spoken-language modeling inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on visual generation with a Hybrid Autoregressive Transformer and a hybrid tokenizer, with no emphasis on spoken language modeling, speech interaction, or dialogue synchronization, so it does not meet the spoken-language modeling inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "This paper centers on visual generation models and does not discuss spoken language models, speech language modeling, or any associated architectures, training, or evaluation related to speech sequences as primary objects; there is no mention of full-duplex spoken interaction capabilities, synchronization, or other key criteria outlined for inclusion. It is therefore unrelated to the defined research scope and inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "This paper centers on visual generation models and does not discuss spoken language models, speech language modeling, or any associated architectures, training, or evaluation related to speech sequences as primary objects; there is no mention of full-duplex spoken interaction capabilities, synchronization, or other key criteria outlined for inclusion. It is therefore unrelated to the defined research scope and inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Code Drift: Towards Idempotent Neural Audio Codecs",
    "abstract": "Neural codecs have demonstrated strong performance in high-fidelity compression of audio signals at low bitrates. The token-based representations produced by these codecs have proven particularly useful for generative modeling. While much research has focused on improvements in compression ratio and perceptual transparency, recent works have largely overlooked another desirable codec property -- idempotence, the stability of compressed outputs under multiple rounds of encoding. We find that state-of-the-art neural codecs exhibit varied degrees of idempotence, with some degrading audio outputs significantly after as few as three encodings. We investigate possible causes of low idempotence and devise a method for improving idempotence through fine-tuning a codec model. We then examine the effect of idempotence on a simple conditional generative modeling task, and find that increased idempotence can be achieved without negatively impacting downstream modeling performance -- potentially extending the usefulness of neural codecs for practical file compression and iterative generative modeling workflows.",
    "metadata": {
      "arxiv_id": "2410.11025",
      "title": "Code Drift: Towards Idempotent Neural Audio Codecs",
      "summary": "Neural codecs have demonstrated strong performance in high-fidelity compression of audio signals at low bitrates. The token-based representations produced by these codecs have proven particularly useful for generative modeling. While much research has focused on improvements in compression ratio and perceptual transparency, recent works have largely overlooked another desirable codec property -- idempotence, the stability of compressed outputs under multiple rounds of encoding. We find that state-of-the-art neural codecs exhibit varied degrees of idempotence, with some degrading audio outputs significantly after as few as three encodings. We investigate possible causes of low idempotence and devise a method for improving idempotence through fine-tuning a codec model. We then examine the effect of idempotence on a simple conditional generative modeling task, and find that increased idempotence can be achieved without negatively impacting downstream modeling performance -- potentially extending the usefulness of neural codecs for practical file compression and iterative generative modeling workflows.",
      "authors": [
        "Patrick O'Reilly",
        "Prem Seetharaman",
        "Jiaqi Su",
        "Zeyu Jin",
        "Bryan Pardo"
      ],
      "published": "2024-10-14T19:21:28Z",
      "updated": "2025-04-14T23:07:19Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.11025v2",
      "landing_url": "https://arxiv.org/abs/2410.11025v2",
      "doi": "https://doi.org/10.1109/ICASSP49660.2025.10890096"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on neural audio codecs and idempotence rather than Spoken Language Models or speech-language modeling; it lacks core emphasis on SLM/FD-SLM architectures, turn-taking, synchronization, or interactive speech tasks, and does not explicitly use the required keywords in title/abstract/keywords or target the past 3 years in the SLM sense.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on neural audio codecs and idempotence rather than Spoken Language Models or speech-language modeling; it lacks core emphasis on SLM/FD-SLM architectures, turn-taking, synchronization, or interactive speech tasks, and does not explicitly use the required keywords in title/abstract/keywords or target the past 3 years in the SLM sense.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on neural audio codecs and their idempotence properties for compression and generative modeling, but do not address spoken language models or full-duplex spoken interaction involving speech-based language modeling, synchronization, or real-time interaction aspects required by the inclusion criteria; moreover, the study does not mention speech language modeling or related architectures explicitly, nor does it discuss cognitive parallelism or low-latency interaction, which leads to exclusion based on the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on neural audio codecs and their idempotence properties for compression and generative modeling, but do not address spoken language models or full-duplex spoken interaction involving speech-based language modeling, synchronization, or real-time interaction aspects required by the inclusion criteria; moreover, the study does not mention speech language modeling or related architectures explicitly, nor does it discuss cognitive parallelism or low-latency interaction, which leads to exclusion based on the criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "EEG-based 90-Degree Turn Intention Detection for Brain-Computer Interface",
    "abstract": "Electroencephalography (EEG)--based turn intention prediction for lower limb movement is important to build an efficient brain-computer interface (BCI) system. This study investigates the feasibility of intention detection of left-turn, right-turn, and straight walk by utilizing EEG signals obtained before the event occurrence. Synchronous data was collected using 31-channel EEG and IMU-based motion capture systems for nine healthy participants while performing left-turn, right-turn, and straight walk movements. EEG data was preprocessed with steps including Artifact Subspace Reconstruction (ASR), re-referencing, and Independent Component Analysis (ICA) to remove data noise. Feature extraction from the preprocessed EEG data involved computing various statistical measures (mean, median, standard deviation, skew, and kurtosis), and Hjorth parameters (activity, mobility, and complexity). Further, the feature selection was performed using the Random forest algorithm for the dimensionality reduction. The feature set obtained was utilized for 3-class classification using XG boost, gradient boosting, and support vector machine (SVM) with RBF kernel classifiers in a five-fold cross-validation scheme. Using the proposed intention detection methodology, the SVM classifier using an EEG window of 1.5 s and 0 s time-lag has the best decoding performance with mean accuracy, precision, and recall of 81.23%, 85.35%, and 83.92%, respectively, across the nine participants. The decoding analysis shows the feasibility of turn intention prediction for lower limb movement using the EEG signal before the event onset.",
    "metadata": {
      "arxiv_id": "2410.11339",
      "title": "EEG-based 90-Degree Turn Intention Detection for Brain-Computer Interface",
      "summary": "Electroencephalography (EEG)--based turn intention prediction for lower limb movement is important to build an efficient brain-computer interface (BCI) system. This study investigates the feasibility of intention detection of left-turn, right-turn, and straight walk by utilizing EEG signals obtained before the event occurrence. Synchronous data was collected using 31-channel EEG and IMU-based motion capture systems for nine healthy participants while performing left-turn, right-turn, and straight walk movements. EEG data was preprocessed with steps including Artifact Subspace Reconstruction (ASR), re-referencing, and Independent Component Analysis (ICA) to remove data noise. Feature extraction from the preprocessed EEG data involved computing various statistical measures (mean, median, standard deviation, skew, and kurtosis), and Hjorth parameters (activity, mobility, and complexity). Further, the feature selection was performed using the Random forest algorithm for the dimensionality reduction. The feature set obtained was utilized for 3-class classification using XG boost, gradient boosting, and support vector machine (SVM) with RBF kernel classifiers in a five-fold cross-validation scheme. Using the proposed intention detection methodology, the SVM classifier using an EEG window of 1.5 s and 0 s time-lag has the best decoding performance with mean accuracy, precision, and recall of 81.23%, 85.35%, and 83.92%, respectively, across the nine participants. The decoding analysis shows the feasibility of turn intention prediction for lower limb movement using the EEG signal before the event onset.",
      "authors": [
        "Pradyot Anand",
        "Anant Jain",
        "Suriya Prakash Muthukrishnan",
        "Shubhendu Bhasin",
        "Sitikantha Roy",
        "Lalan Kumar"
      ],
      "published": "2024-10-15T07:12:06Z",
      "updated": "2025-02-04T09:45:10Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.11339v2",
      "landing_url": "https://arxiv.org/abs/2410.11339v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.11339"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on EEG-based turn-intention detection for BCI, not on Spoken Language Models or speech-language modeling; it does not discuss SLM/FD-SLM architectures, training, or evaluation, so it fails the inclusion criteria and meets exclusion conditions.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on EEG-based turn-intention detection for BCI, not on Spoken Language Models or speech-language modeling; it does not discuss SLM/FD-SLM architectures, training, or evaluation, so it fails the inclusion criteria and meets exclusion conditions.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on EEG-based intention detection related to lower limb movement, which does not address spoken language models, speech language modeling, full-duplex spoken language interaction, or any of the key topics related to speech or language modeling as defined in the inclusion criteria. There is no discussion of spoken language modeling architectures, training, evaluation, or synchronous interactive speech behaviors. Therefore, it does not meet the inclusion criteria and conflicts with several exclusion points, such as not involving spoken language models or speech interaction modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on EEG-based intention detection related to lower limb movement, which does not address spoken language models, speech language modeling, full-duplex spoken language interaction, or any of the key topics related to speech or language modeling as defined in the inclusion criteria. There is no discussion of spoken language modeling architectures, training, evaluation, or synchronous interactive speech behaviors. Therefore, it does not meet the inclusion criteria and conflicts with several exclusion points, such as not involving spoken language models or speech interaction modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "From promise to practice: realizing high-performance decentralized training",
    "abstract": "Decentralized training of deep neural networks has attracted significant attention for its theoretically superior scalability over synchronous data-parallel methods like All-Reduce. However, realizing this potential in multi-node training is challenging due to the complex design space that involves communication topologies, computation patterns, and optimization algorithms. This paper identifies three key factors that can lead to speedups over All-Reduce training and constructs a runtime model to determine when, how, and to what degree decentralization can yield shorter per-iteration runtimes. Furthermore, to support the decentralized training of transformer-based models, we study a decentralized Adam algorithm that allows for overlapping communications and computations, prove its convergence, and propose an accumulation technique to mitigate the high variance caused by small local batch sizes. We deploy the proposed approach in clusters with up to 64 GPUs and demonstrate its practicality and advantages in both runtime and generalization performance under a fixed iteration budget.",
    "metadata": {
      "arxiv_id": "2410.11998",
      "title": "From promise to practice: realizing high-performance decentralized training",
      "summary": "Decentralized training of deep neural networks has attracted significant attention for its theoretically superior scalability over synchronous data-parallel methods like All-Reduce. However, realizing this potential in multi-node training is challenging due to the complex design space that involves communication topologies, computation patterns, and optimization algorithms. This paper identifies three key factors that can lead to speedups over All-Reduce training and constructs a runtime model to determine when, how, and to what degree decentralization can yield shorter per-iteration runtimes. Furthermore, to support the decentralized training of transformer-based models, we study a decentralized Adam algorithm that allows for overlapping communications and computations, prove its convergence, and propose an accumulation technique to mitigate the high variance caused by small local batch sizes. We deploy the proposed approach in clusters with up to 64 GPUs and demonstrate its practicality and advantages in both runtime and generalization performance under a fixed iteration budget.",
      "authors": [
        "Zesen Wang",
        "Jiaojiao Zhang",
        "Xuyang Wu",
        "Mikael Johansson"
      ],
      "published": "2024-10-15T19:04:56Z",
      "updated": "2024-10-15T19:04:56Z",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.11998v1",
      "landing_url": "https://arxiv.org/abs/2410.11998v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.11998"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The article centers on decentralized training of transformer models and runtime considerations, with no focus on Spoken Language Models or speech-based modeling or interactive speech evaluation, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The article centers on decentralized training of transformer models and runtime considerations, with no focus on Spoken Language Models or speech-based modeling or interactive speech evaluation, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on decentralized training of deep neural networks for transformer-based models and their runtime and generalization performance; it does not mention spoken language models, speech language models, or any speech-specific modeling as the core subject, nor does it discuss speech as the main modeling target, synchronous speech interaction, or dialogue behavior arbitration, which are essential inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on decentralized training of deep neural networks for transformer-based models and their runtime and generalization performance; it does not mention spoken language models, speech language models, or any speech-specific modeling as the core subject, nor does it discuss speech as the main modeling target, synchronous speech interaction, or dialogue behavior arbitration, which are essential inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Learning Graph Quantized Tokenizers",
    "abstract": "Transformers serve as the backbone architectures of Foundational Models, where domain-specific tokenizers allow them to adapt to various domains. Graph Transformers (GTs) have recently emerged as leading models in geometric deep learning, outperforming Graph Neural Networks (GNNs) in various graph learning tasks. However, the development of tokenizers for graphs has lagged behind other modalities. To address this, we introduce GQT (\\textbf{G}raph \\textbf{Q}uantized \\textbf{T}okenizer), which decouples tokenizer training from Transformer training by leveraging multi-task graph self-supervised learning, yielding robust and generalizable graph tokens. Furthermore, the GQT utilizes Residual Vector Quantization (RVQ) to learn hierarchical discrete tokens, resulting in significantly reduced memory requirements and improved generalization capabilities. By combining the GQT with token modulation, a Transformer encoder achieves state-of-the-art performance on 20 out of 22 benchmarks, including large-scale homophilic and heterophilic datasets.",
    "metadata": {
      "arxiv_id": "2410.13798",
      "title": "Learning Graph Quantized Tokenizers",
      "summary": "Transformers serve as the backbone architectures of Foundational Models, where domain-specific tokenizers allow them to adapt to various domains. Graph Transformers (GTs) have recently emerged as leading models in geometric deep learning, outperforming Graph Neural Networks (GNNs) in various graph learning tasks. However, the development of tokenizers for graphs has lagged behind other modalities. To address this, we introduce GQT (\\textbf{G}raph \\textbf{Q}uantized \\textbf{T}okenizer), which decouples tokenizer training from Transformer training by leveraging multi-task graph self-supervised learning, yielding robust and generalizable graph tokens. Furthermore, the GQT utilizes Residual Vector Quantization (RVQ) to learn hierarchical discrete tokens, resulting in significantly reduced memory requirements and improved generalization capabilities. By combining the GQT with token modulation, a Transformer encoder achieves state-of-the-art performance on 20 out of 22 benchmarks, including large-scale homophilic and heterophilic datasets.",
      "authors": [
        "Limei Wang",
        "Kaveh Hassani",
        "Si Zhang",
        "Dongqi Fu",
        "Baichuan Yuan",
        "Weilin Cong",
        "Zhigang Hua",
        "Hao Wu",
        "Ning Yao",
        "Bo Long"
      ],
      "published": "2024-10-17T17:38:24Z",
      "updated": "2025-04-02T03:04:44Z",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.13798v2",
      "landing_url": "https://arxiv.org/abs/2410.13798v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.13798"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on graph tokenizers and Graph Transformers, not Spoken Language Models or speech processes; it lacks speech signals, dialogue, or speech-language modeling aspects required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on graph tokenizers and Graph Transformers, not Spoken Language Models or speech processes; it lacks speech signals, dialogue, or speech-language modeling aspects required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Graph Transformers and tokenizers for graph learning tasks, with no mention of spoken language models, speech language modeling, or any speech-related processing or interaction, which are core to the inclusion criteria. It does not address speech or spoken language modeling aspects, synchronization, full-duplex interaction, or any evaluation related to speech or immediate interaction, thus failing to meet the inclusion criteria and aligning with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Graph Transformers and tokenizers for graph learning tasks, with no mention of spoken language models, speech language modeling, or any speech-related processing or interaction, which are core to the inclusion criteria. It does not address speech or spoken language modeling aspects, synchronization, full-duplex interaction, or any evaluation related to speech or immediate interaction, thus failing to meet the inclusion criteria and aligning with exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Large Language Models Know What To Say But Not When To Speak",
    "abstract": "Turn-taking is a fundamental mechanism in human communication that ensures smooth and coherent verbal interactions. Recent advances in Large Language Models (LLMs) have motivated their use in improving the turn-taking capabilities of Spoken Dialogue Systems (SDS), such as their ability to respond at appropriate times. However, existing models often struggle to predict opportunities for speaking -- called Transition Relevance Places (TRPs) -- in natural, unscripted conversations, focusing only on turn-final TRPs and not within-turn TRPs. To address these limitations, we introduce a novel dataset of participant-labeled within-turn TRPs and use it to evaluate the performance of state-of-the-art LLMs in predicting opportunities for speaking. Our experiments reveal the current limitations of LLMs in modeling unscripted spoken interactions, highlighting areas for improvement and paving the way for more naturalistic dialogue systems.",
    "metadata": {
      "arxiv_id": "2410.16044",
      "title": "Large Language Models Know What To Say But Not When To Speak",
      "summary": "Turn-taking is a fundamental mechanism in human communication that ensures smooth and coherent verbal interactions. Recent advances in Large Language Models (LLMs) have motivated their use in improving the turn-taking capabilities of Spoken Dialogue Systems (SDS), such as their ability to respond at appropriate times. However, existing models often struggle to predict opportunities for speaking -- called Transition Relevance Places (TRPs) -- in natural, unscripted conversations, focusing only on turn-final TRPs and not within-turn TRPs. To address these limitations, we introduce a novel dataset of participant-labeled within-turn TRPs and use it to evaluate the performance of state-of-the-art LLMs in predicting opportunities for speaking. Our experiments reveal the current limitations of LLMs in modeling unscripted spoken interactions, highlighting areas for improvement and paving the way for more naturalistic dialogue systems.",
      "authors": [
        "Muhammad Umair",
        "Vasanth Sarathy",
        "JP de Ruiter"
      ],
      "published": "2024-10-21T14:20:25Z",
      "updated": "2024-10-21T14:20:25Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.16044v1",
      "landing_url": "https://arxiv.org/abs/2410.16044v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.16044"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on predicting speaking opportunities in turn-taking using Large Language Models, but it does not clearly position Speech/Spoken Language Models as the core modeling object nor explicitly discuss speech-based architecture, training, or evaluation as required by the inclusion criteria; it mentions Spoken Dialogue Systems but not SLM/FD-SLM with speech as primary modeling target, so it likely does not meet all criteria despite relevance.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on predicting speaking opportunities in turn-taking using Large Language Models, but it does not clearly position Speech/Spoken Language Models as the core modeling object nor explicitly discuss speech-based architecture, training, or evaluation as required by the inclusion criteria; it mentions Spoken Dialogue Systems but not SLM/FD-SLM with speech as primary modeling target, so it likely does not meet all criteria despite relevance.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on predicting turn-taking points in natural spoken dialogue using Large Language Models, which relates to spoken dialogue systems but the abstract does not clearly indicate that it treats speech signals as the core modeling object or that it involves speech language modeling at the acoustic or speech token level; it primarily analyzes transition relevance places in conversation timing rather than full-duplex spoken language models or end-to-end speech modeling architectures. Thus, it does not fully meet the inclusion criteria, nor does it clearly violate all exclusion criteria, resulting in ambiguity on inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on predicting turn-taking points in natural spoken dialogue using Large Language Models, which relates to spoken dialogue systems but the abstract does not clearly indicate that it treats speech signals as the core modeling object or that it involves speech language modeling at the acoustic or speech token level; it primarily analyzes transition relevance places in conversation timing rather than full-duplex spoken language models or end-to-end speech modeling architectures. Thus, it does not fully meet the inclusion criteria, nor does it clearly violate all exclusion criteria, resulting in ambiguity on inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers correctly observed that the paper studies turn-taking/TRP prediction using LLMs for Spoken Dialogue Systems but the abstract contains no indication that speech/audio is the core modeling object (no mention of Spoken Language Models, speech tokens/representations, Speech–LLM architectures, or acoustic-level modeling/metrics); it instead appears to analyze timing/annotations of spoken interaction at a higher/linguistic level (half‑duplex TRPs) rather than proposing end-to-end or mixed speech-centered SLM/FD‑SLM architectures or low-latency/full-duplex synchronization methods. Therefore it fails the inclusion requirement that speech be treated as the primary modeling target and falls into the exclusion categories (focus on half‑duplex turn-taking and likely text/annotation-level modeling). 1-sentence explanation: I rate this a 2 (better to exclude) because the abstract does not explicitly present spoken-language/speech-centric modeling, architectures, or acoustic-level evaluation required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers correctly observed that the paper studies turn-taking/TRP prediction using LLMs for Spoken Dialogue Systems but the abstract contains no indication that speech/audio is the core modeling object (no mention of Spoken Language Models, speech tokens/representations, Speech–LLM architectures, or acoustic-level modeling/metrics); it instead appears to analyze timing/annotations of spoken interaction at a higher/linguistic level (half‑duplex TRPs) rather than proposing end-to-end or mixed speech-centered SLM/FD‑SLM architectures or low-latency/full-duplex synchronization methods. Therefore it fails the inclusion requirement that speech be treated as the primary modeling target and falls into the exclusion categories (focus on half‑duplex turn-taking and likely text/annotation-level modeling). 1-sentence explanation: I rate this a 2 (better to exclude) because the abstract does not explicitly present spoken-language/speech-centric modeling, architectures, or acoustic-level evaluation required by the inclusion criteria.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Deep learning for model correction of dynamical systems with data scarcity",
    "abstract": "We present a deep learning framework for correcting existing dynamical system models utilizing only a scarce high-fidelity data set. In many practical situations, one has a low-fidelity model that can capture the dynamics reasonably well but lacks high resolution, due to the inherent limitation of the model and the complexity of the underlying physics. When high resolution data become available, it is natural to seek model correction to improve the resolution of the model predictions. We focus on the case when the amount of high-fidelity data is so small that most of the existing data driven modeling methods cannot be applied. In this paper, we address these challenges with a model-correction method which only requires a scarce high-fidelity data set. Our method first seeks a deep neural network (DNN) model to approximate the existing low-fidelity model. By using the scarce high-fidelity data, the method then corrects the DNN model via transfer learning (TL). After TL, an improved DNN model with high prediction accuracy to the underlying dynamics is obtained. One distinct feature of the propose method is that it does not assume a specific form of the model correction terms. Instead, it offers an inherent correction to the low-fidelity model via TL. A set of numerical examples are presented to demonstrate the effectiveness of the proposed method.",
    "metadata": {
      "arxiv_id": "2410.17913",
      "title": "Deep learning for model correction of dynamical systems with data scarcity",
      "summary": "We present a deep learning framework for correcting existing dynamical system models utilizing only a scarce high-fidelity data set. In many practical situations, one has a low-fidelity model that can capture the dynamics reasonably well but lacks high resolution, due to the inherent limitation of the model and the complexity of the underlying physics. When high resolution data become available, it is natural to seek model correction to improve the resolution of the model predictions. We focus on the case when the amount of high-fidelity data is so small that most of the existing data driven modeling methods cannot be applied. In this paper, we address these challenges with a model-correction method which only requires a scarce high-fidelity data set. Our method first seeks a deep neural network (DNN) model to approximate the existing low-fidelity model. By using the scarce high-fidelity data, the method then corrects the DNN model via transfer learning (TL). After TL, an improved DNN model with high prediction accuracy to the underlying dynamics is obtained. One distinct feature of the propose method is that it does not assume a specific form of the model correction terms. Instead, it offers an inherent correction to the low-fidelity model via TL. A set of numerical examples are presented to demonstrate the effectiveness of the proposed method.",
      "authors": [
        "Caroline Tatsuoka",
        "Dongbin Xiu"
      ],
      "published": "2024-10-23T14:33:11Z",
      "updated": "2024-10-23T14:33:11Z",
      "categories": [
        "cs.LG",
        "math.DS",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17913v1",
      "landing_url": "https://arxiv.org/abs/2410.17913v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.17913"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item targets model correction of dynamical systems with scarce high-fidelity data and transfer learning, not Spoken Language Models or speech-language modeling, nor any evaluation of interactive speech systems; thus it falls outside the stated inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item targets model correction of dynamical systems with scarce high-fidelity data and transfer learning, not Spoken Language Models or speech-language modeling, nor any evaluation of interactive speech systems; thus it falls outside the stated inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on deep learning for correcting dynamical system models using scarce data and does not address spoken language models, speech language modeling, full-duplex interaction, or any speech-based modeling and evaluation criteria outlined in the inclusion criteria. It is purely about model correction in dynamical systems without any connection to speech or spoken language models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on deep learning for correcting dynamical system models using scarce data and does not address spoken language models, speech language modeling, full-duplex interaction, or any speech-based modeling and evaluation criteria outlined in the inclusion criteria. It is purely about model correction in dynamical systems without any connection to speech or spoken language models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Advancing Histopathology with Deep Learning Under Data Scarcity: A Decade in Review",
    "abstract": "Recent years witnessed remarkable progress in computational histopathology, largely fueled by deep learning. This brought the clinical adoption of deep learning-based tools within reach, promising significant benefits to healthcare, offering a valuable second opinion on diagnoses, streamlining complex tasks, and mitigating the risks of inconsistency and bias in clinical decisions. However, a well-known challenge is that deep learning models may contain up to billions of parameters; supervising their training effectively would require vast labeled datasets to achieve reliable generalization and noise resilience. In medical imaging, particularly histopathology, amassing such extensive labeled data collections places additional demands on clinicians and incurs higher costs, which hinders the art's progress. Addressing this challenge, researchers devised various strategies for leveraging deep learning with limited data and annotation availability. In this paper, we present a comprehensive review of deep learning applications in histopathology, with a focus on the challenges posed by data scarcity over the past decade. We systematically categorize and compare various approaches, evaluate their distinct contributions using benchmarking tables, and highlight their respective advantages and limitations. Additionally, we address gaps in existing reviews and identify underexplored research opportunities, underscoring the potential for future advancements in this field.",
    "metadata": {
      "arxiv_id": "2410.19820",
      "title": "Advancing Histopathology with Deep Learning Under Data Scarcity: A Decade in Review",
      "summary": "Recent years witnessed remarkable progress in computational histopathology, largely fueled by deep learning. This brought the clinical adoption of deep learning-based tools within reach, promising significant benefits to healthcare, offering a valuable second opinion on diagnoses, streamlining complex tasks, and mitigating the risks of inconsistency and bias in clinical decisions. However, a well-known challenge is that deep learning models may contain up to billions of parameters; supervising their training effectively would require vast labeled datasets to achieve reliable generalization and noise resilience. In medical imaging, particularly histopathology, amassing such extensive labeled data collections places additional demands on clinicians and incurs higher costs, which hinders the art's progress. Addressing this challenge, researchers devised various strategies for leveraging deep learning with limited data and annotation availability. In this paper, we present a comprehensive review of deep learning applications in histopathology, with a focus on the challenges posed by data scarcity over the past decade. We systematically categorize and compare various approaches, evaluate their distinct contributions using benchmarking tables, and highlight their respective advantages and limitations. Additionally, we address gaps in existing reviews and identify underexplored research opportunities, underscoring the potential for future advancements in this field.",
      "authors": [
        "Ahmad Obeid",
        "Said Boumaraf",
        "Anabia Sohail",
        "Taimur Hassan",
        "Sajid Javed",
        "Jorge Dias",
        "Mohammed Bennamoun",
        "Naoufel Werghi"
      ],
      "published": "2024-10-18T07:29:48Z",
      "updated": "2024-10-18T07:29:48Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.19820v1",
      "landing_url": "https://arxiv.org/abs/2410.19820v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.19820"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about histopathology deep learning under data scarcity and does not address Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria focused on SLM/FD-SLM topics.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about histopathology deep learning under data scarcity and does not address Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria focused on SLM/FD-SLM topics.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on deep learning in histopathology under data scarcity, which is unrelated to Spoken Language Models or speech-based language modeling; it lacks any mention of speech, spoken language models, full-duplex interaction, or related evaluation metrics, and thus does not meet the inclusion criteria centered on speech-based modeling and synchronous spoken interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on deep learning in histopathology under data scarcity, which is unrelated to Spoken Language Models or speech-based language modeling; it lacks any mention of speech, spoken language models, full-duplex interaction, or related evaluation metrics, and thus does not meet the inclusion criteria centered on speech-based modeling and synchronous spoken interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "That was not what I was aiming at! Differentiating human intent and outcome in a physically dynamic throwing task",
    "abstract": "Recognising intent in collaborative human robot tasks can improve team performance and human perception of robots. Intent can differ from the observed outcome in the presence of mistakes which are likely in physically dynamic tasks. We created a dataset of 1227 throws of a ball at a target from 10 participants and observed that 47% of throws were mistakes with 16% completely missing the target. Our research leverages facial images capturing the person's reaction to the outcome of a throw to predict when the resulting throw is a mistake and then we determine the actual intent of the throw. The approach we propose for outcome prediction performs 38% better than the two-stream architecture used previously for this task on front-on videos. In addition, we propose a 1-D CNN model which is used in conjunction with priors learned from the frequency of mistakes to provide an end-to-end pipeline for outcome and intent recognition in this throwing task.",
    "metadata": {
      "arxiv_id": "2410.20256",
      "title": "That was not what I was aiming at! Differentiating human intent and outcome in a physically dynamic throwing task",
      "summary": "Recognising intent in collaborative human robot tasks can improve team performance and human perception of robots. Intent can differ from the observed outcome in the presence of mistakes which are likely in physically dynamic tasks. We created a dataset of 1227 throws of a ball at a target from 10 participants and observed that 47% of throws were mistakes with 16% completely missing the target. Our research leverages facial images capturing the person's reaction to the outcome of a throw to predict when the resulting throw is a mistake and then we determine the actual intent of the throw. The approach we propose for outcome prediction performs 38% better than the two-stream architecture used previously for this task on front-on videos. In addition, we propose a 1-D CNN model which is used in conjunction with priors learned from the frequency of mistakes to provide an end-to-end pipeline for outcome and intent recognition in this throwing task.",
      "authors": [
        "Vidullan Surendran",
        "Alan R. Wagner"
      ],
      "published": "2024-10-26T19:24:46Z",
      "updated": "2024-10-26T19:24:46Z",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.20256v1",
      "landing_url": "https://arxiv.org/abs/2410.20256v1",
      "doi": "https://doi.org/10.1007/s10514-022-10074-5"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on predicting intent vs outcome in a dynamic throwing task using video/facial data; it does not involve Spoken Language Models or speech-language modeling as the core, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on predicting intent vs outcome in a dynamic throwing task using video/facial data; it does not involve Spoken Language Models or speech-language modeling as the core, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on recognizing human intent and outcomes in a physically dynamic throwing task using facial images and video, without addressing spoken language models, speech language modeling, or full-duplex spoken interaction, which are core to the inclusion criteria. It also lacks mention of speech as a primary modeling object, relevant model architectures or evaluations related to speech or real-time spoken interactions, and does not fit the thematic scope required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on recognizing human intent and outcomes in a physically dynamic throwing task using facial images and video, without addressing spoken language models, speech language modeling, or full-duplex spoken interaction, which are core to the inclusion criteria. It also lacks mention of speech as a primary modeling object, relevant model architectures or evaluations related to speech or real-time spoken interactions, and does not fit the thematic scope required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior",
    "abstract": "We present LARP, a novel video tokenizer designed to overcome limitations in current video tokenization methods for autoregressive (AR) generative models. Unlike traditional patchwise tokenizers that directly encode local visual patches into discrete tokens, LARP introduces a holistic tokenization scheme that gathers information from the visual content using a set of learned holistic queries. This design allows LARP to capture more global and semantic representations, rather than being limited to local patch-level information. Furthermore, it offers flexibility by supporting an arbitrary number of discrete tokens, enabling adaptive and efficient tokenization based on the specific requirements of the task. To align the discrete token space with downstream AR generation tasks, LARP integrates a lightweight AR transformer as a training-time prior model that predicts the next token on its discrete latent space. By incorporating the prior model during training, LARP learns a latent space that is not only optimized for video reconstruction but is also structured in a way that is more conducive to autoregressive generation. Moreover, this process defines a sequential order for the discrete tokens, progressively pushing them toward an optimal configuration during training, ensuring smoother and more accurate AR generation at inference time. Comprehensive experiments demonstrate LARP's strong performance, achieving state-of-the-art FVD on the UCF101 class-conditional video generation benchmark. LARP enhances the compatibility of AR models with videos and opens up the potential to build unified high-fidelity multimodal large language models (MLLMs).",
    "metadata": {
      "arxiv_id": "2410.21264",
      "title": "LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior",
      "summary": "We present LARP, a novel video tokenizer designed to overcome limitations in current video tokenization methods for autoregressive (AR) generative models. Unlike traditional patchwise tokenizers that directly encode local visual patches into discrete tokens, LARP introduces a holistic tokenization scheme that gathers information from the visual content using a set of learned holistic queries. This design allows LARP to capture more global and semantic representations, rather than being limited to local patch-level information. Furthermore, it offers flexibility by supporting an arbitrary number of discrete tokens, enabling adaptive and efficient tokenization based on the specific requirements of the task. To align the discrete token space with downstream AR generation tasks, LARP integrates a lightweight AR transformer as a training-time prior model that predicts the next token on its discrete latent space. By incorporating the prior model during training, LARP learns a latent space that is not only optimized for video reconstruction but is also structured in a way that is more conducive to autoregressive generation. Moreover, this process defines a sequential order for the discrete tokens, progressively pushing them toward an optimal configuration during training, ensuring smoother and more accurate AR generation at inference time. Comprehensive experiments demonstrate LARP's strong performance, achieving state-of-the-art FVD on the UCF101 class-conditional video generation benchmark. LARP enhances the compatibility of AR models with videos and opens up the potential to build unified high-fidelity multimodal large language models (MLLMs).",
      "authors": [
        "Hanyu Wang",
        "Saksham Suri",
        "Yixuan Ren",
        "Hao Chen",
        "Abhinav Shrivastava"
      ],
      "published": "2024-10-28T17:57:07Z",
      "updated": "2025-06-16T22:06:19Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.21264v2",
      "landing_url": "https://arxiv.org/abs/2410.21264v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.21264"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on video tokenization and autoregressive video generation and does not address spoken language models or speech-centric modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on video tokenization and autoregressive video generation and does not address spoken language models or speech-centric modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on video tokenization for autoregressive generative models and does not discuss spoken language models, speech language modeling, or synchronous duplex spoken interaction, which are the core inclusion criteria; thus, it falls out of scope for the specified topic focused on speech-centric language modeling and interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on video tokenization for autoregressive generative models and does not discuss spoken language models, speech language modeling, or synchronous duplex spoken interaction, which are the core inclusion criteria; thus, it falls out of scope for the specified topic focused on speech-centric language modeling and interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "TraderTalk: An LLM Behavioural ABM applied to Simulating Human Bilateral Trading Interactions",
    "abstract": "We introduce a novel hybrid approach that augments Agent-Based Models (ABMs) with behaviors generated by Large Language Models (LLMs) to simulate human trading interactions. We call our model TraderTalk. Leveraging LLMs trained on extensive human-authored text, we capture detailed and nuanced representations of bilateral conversations in financial trading. Applying this Generative Agent-Based Model (GABM) to government bond markets, we replicate trading decisions between two stylised virtual humans. Our method addresses both structural challenges, such as coordinating turn-taking between realistic LLM-based agents, and design challenges, including the interpretation of LLM outputs by the agent model. By exploring prompt design opportunistically rather than systematically, we enhance the realism of agent interactions without exhaustive overfitting or model reliance. Our approach successfully replicates trade-to-order volume ratios observed in related asset markets, demonstrating the potential of LLM-augmented ABMs in financial simulations",
    "metadata": {
      "arxiv_id": "2410.21280",
      "title": "TraderTalk: An LLM Behavioural ABM applied to Simulating Human Bilateral Trading Interactions",
      "summary": "We introduce a novel hybrid approach that augments Agent-Based Models (ABMs) with behaviors generated by Large Language Models (LLMs) to simulate human trading interactions. We call our model TraderTalk. Leveraging LLMs trained on extensive human-authored text, we capture detailed and nuanced representations of bilateral conversations in financial trading. Applying this Generative Agent-Based Model (GABM) to government bond markets, we replicate trading decisions between two stylised virtual humans. Our method addresses both structural challenges, such as coordinating turn-taking between realistic LLM-based agents, and design challenges, including the interpretation of LLM outputs by the agent model. By exploring prompt design opportunistically rather than systematically, we enhance the realism of agent interactions without exhaustive overfitting or model reliance. Our approach successfully replicates trade-to-order volume ratios observed in related asset markets, demonstrating the potential of LLM-augmented ABMs in financial simulations",
      "authors": [
        "Alicia Vidler",
        "Toby Walsh"
      ],
      "published": "2024-10-10T23:58:07Z",
      "updated": "2024-10-10T23:58:07Z",
      "categories": [
        "q-fin.TR",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.21280v1",
      "landing_url": "https://arxiv.org/abs/2410.21280v1",
      "doi": "https://doi.org/10.1109/ICA63002.2024.00042"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article focuses on LLM-based ABMs for simulating bilateral trading conversations in text form and does not center on Spoken Language Models or speech-language modeling with full-duplex interaction; thus it does not meet inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article focuses on LLM-based ABMs for simulating bilateral trading conversations in text form and does not center on Spoken Language Models or speech-language modeling with full-duplex interaction; thus it does not meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on augmenting agent-based models with large language models to simulate human bilateral trading conversations, but does not emphasize spoken language models, speech/audio as core modeling objects, or full-duplex spoken interaction capabilities; it mainly deals with text-based dialogue modeling rather than speech sequence modeling or speech language models, and does not address the required aspects of synchronous speech interaction, low-latency voice overlap management, or explicit spoken language model architectures as per inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on augmenting agent-based models with large language models to simulate human bilateral trading conversations, but does not emphasize spoken language models, speech/audio as core modeling objects, or full-duplex spoken interaction capabilities; it mainly deals with text-based dialogue modeling rather than speech sequence modeling or speech language models, and does not address the required aspects of synchronous speech interaction, low-latency voice overlap management, or explicit spoken language model architectures as per inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Closer Look at Neural Codec Resynthesis: Bridging the Gap between Codec and Waveform Generation",
    "abstract": "Neural Audio Codecs, initially designed as a compression technique, have gained more attention recently for speech generation. Codec models represent each audio frame as a sequence of tokens, i.e., discrete embeddings. The discrete and low-frequency nature of neural codecs introduced a new way to generate speech with token-based models. As these tokens encode information at various levels of granularity, from coarse to fine, most existing works focus on how to better generate the coarse tokens. In this paper, we focus on an equally important but often overlooked question: How can we better resynthesize the waveform from coarse tokens? We point out that both the choice of learning target and resynthesis approach have a dramatic impact on the generated audio quality. Specifically, we study two different strategies based on token prediction and regression, and introduce a new method based on Schrödinger Bridge. We examine how different design choices affect machine and human perception.",
    "metadata": {
      "arxiv_id": "2410.22448",
      "title": "A Closer Look at Neural Codec Resynthesis: Bridging the Gap between Codec and Waveform Generation",
      "summary": "Neural Audio Codecs, initially designed as a compression technique, have gained more attention recently for speech generation. Codec models represent each audio frame as a sequence of tokens, i.e., discrete embeddings. The discrete and low-frequency nature of neural codecs introduced a new way to generate speech with token-based models. As these tokens encode information at various levels of granularity, from coarse to fine, most existing works focus on how to better generate the coarse tokens. In this paper, we focus on an equally important but often overlooked question: How can we better resynthesize the waveform from coarse tokens? We point out that both the choice of learning target and resynthesis approach have a dramatic impact on the generated audio quality. Specifically, we study two different strategies based on token prediction and regression, and introduce a new method based on Schrödinger Bridge. We examine how different design choices affect machine and human perception.",
      "authors": [
        "Alexander H. Liu",
        "Qirui Wang",
        "Yuan Gong",
        "James Glass"
      ],
      "published": "2024-10-29T18:29:39Z",
      "updated": "2024-10-29T18:29:39Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.22448v1",
      "landing_url": "https://arxiv.org/abs/2410.22448v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.22448"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on neural audio codecs and waveform resynthesis rather than Spoken Language Models or speech-language modeling as the core object, lacking discussion of SLM/FD-SLM architectures, training, or evaluation and therefore does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on neural audio codecs and waveform resynthesis rather than Spoken Language Models or speech-language modeling as the core object, lacking discussion of SLM/FD-SLM architectures, training, or evaluation and therefore does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on neural audio codecs and waveform resynthesis, discussing token-based speech generation and model design choices. However, it does not explicitly address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it discuss turn-taking, synchronization, or interactive dialogue behaviors as required. It mainly treats speech as a signal for generation rather than as the main object of language modeling or interactive communication. Therefore, it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on neural audio codecs and waveform resynthesis, discussing token-based speech generation and model design choices. However, it does not explicitly address Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor does it discuss turn-taking, synchronization, or interactive dialogue behaviors as required. It mainly treats speech as a signal for generation rather than as the main object of language modeling or interactive communication. Therefore, it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Lina-Speech: Gated Linear Attention and Initial-State Tuning for Multi-Sample Prompting Text-To-Speech Synthesis",
    "abstract": "Neural codec language models, built on transformer architecture, have revolutionized text-to-speech (TTS) synthesis, excelling in voice cloning by treating it as a prefix continuation task. However, their limited context length hinders their effectiveness to short speech samples. As a result, the voice cloning ability is restricted to a limited coverage and diversity of the speaker's prosody and style. Besides, adapting prosody, accent, or appropriate emotion from a short prefix remains a challenging task. Finally, the quadratic complexity of self-attention limits inference throughput. In this work, we introduce Lina-Speech, a TTS model with Gated Linear Attention (GLA) to replace standard self-attention as a principled backbone, improving inference throughput while matching state-of-the-art performance. Leveraging the stateful property of recurrent architecture, we introduce an Initial-State Tuning (IST) strategy that unlocks the possibility of multiple speech sample conditioning of arbitrary numbers and lengths and provides a comprehensive and efficient strategy for voice cloning and out-of-domain speaking style and emotion adaptation. We demonstrate the effectiveness of this approach for controlling fine-grained characteristics such as prosody and emotion. Code, checkpoints, and demo are freely available: https://github.com/theodorblackbird/lina-speech",
    "metadata": {
      "arxiv_id": "2410.23320",
      "title": "Lina-Speech: Gated Linear Attention and Initial-State Tuning for Multi-Sample Prompting Text-To-Speech Synthesis",
      "summary": "Neural codec language models, built on transformer architecture, have revolutionized text-to-speech (TTS) synthesis, excelling in voice cloning by treating it as a prefix continuation task. However, their limited context length hinders their effectiveness to short speech samples. As a result, the voice cloning ability is restricted to a limited coverage and diversity of the speaker's prosody and style. Besides, adapting prosody, accent, or appropriate emotion from a short prefix remains a challenging task. Finally, the quadratic complexity of self-attention limits inference throughput. In this work, we introduce Lina-Speech, a TTS model with Gated Linear Attention (GLA) to replace standard self-attention as a principled backbone, improving inference throughput while matching state-of-the-art performance. Leveraging the stateful property of recurrent architecture, we introduce an Initial-State Tuning (IST) strategy that unlocks the possibility of multiple speech sample conditioning of arbitrary numbers and lengths and provides a comprehensive and efficient strategy for voice cloning and out-of-domain speaking style and emotion adaptation. We demonstrate the effectiveness of this approach for controlling fine-grained characteristics such as prosody and emotion. Code, checkpoints, and demo are freely available: https://github.com/theodorblackbird/lina-speech",
      "authors": [
        "Théodor Lemerle",
        "Téo Guichoux",
        "Axel Roebel",
        "Nicolas Obin"
      ],
      "published": "2024-10-30T04:50:40Z",
      "updated": "2025-11-15T14:30:03Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.23320v2",
      "landing_url": "https://arxiv.org/abs/2410.23320v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.23320"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "It clearly fits the Spoken Language Models/ Speech Language Modeling scope by modelling speech sequences in an end-to-end TTS framework with multi-sample prompting and voice emotion control, includes architectural/training/evaluation details, is within the last 3 years, and is not merely a narrow single-task ASR/TTS paper, so it should be included.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "It clearly fits the Spoken Language Models/ Speech Language Modeling scope by modelling speech sequences in an end-to-end TTS framework with multi-sample prompting and voice emotion control, includes architectural/training/evaluation details, is within the last 3 years, and is not merely a narrow single-task ASR/TTS paper, so it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on a text-to-speech (TTS) synthesis model improving voice cloning with gated linear attention and initial-state tuning, primarily addressing acoustic modeling and inference efficiency. However, it does not discuss Spoken Language Models or full-duplex spoken interaction, nor does it address synchronization, cognitive parallelism, dialogue behavior arbitration, or low-latency interactive speech modeling. It also lacks mention of speech language modeling or speech-centric language models as defined in the inclusion criteria. Therefore, it does not meet the core inclusion criteria and falls under several exclusion criteria, particularly focusing solely on single speech technology (TTS) without interaction or modeling of speech communication behavior.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on a text-to-speech (TTS) synthesis model improving voice cloning with gated linear attention and initial-state tuning, primarily addressing acoustic modeling and inference efficiency. However, it does not discuss Spoken Language Models or full-duplex spoken interaction, nor does it address synchronization, cognitive parallelism, dialogue behavior arbitration, or low-latency interactive speech modeling. It also lacks mention of speech language modeling or speech-centric language models as defined in the inclusion criteria. Therefore, it does not meet the core inclusion criteria and falls under several exclusion criteria, particularly focusing solely on single speech technology (TTS) without interaction or modeling of speech communication behavior.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers disagreed (one included, one excluded); after review I exclude this paper: the work is a TTS/neural-codec language model focused on voice cloning, inference throughput, and prosody/emotion adaptation, but the abstract lacks explicit framing as a \"spoken language model\" or \"language modeling of speech\" and it is a single-task TTS study rather than research on Spoken Language Models, full‑duplex interaction, or synchronous conversational behavior—therefore it meets the exclusion criteria. 1-sentence explanation: I score 1 because it fails the core inclusion requirement of centering on Spoken Language Models/SLMs (or equivalent) and is limited to single-task TTS.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers disagreed (one included, one excluded); after review I exclude this paper: the work is a TTS/neural-codec language model focused on voice cloning, inference throughput, and prosody/emotion adaptation, but the abstract lacks explicit framing as a \"spoken language model\" or \"language modeling of speech\" and it is a single-task TTS study rather than research on Spoken Language Models, full‑duplex interaction, or synchronous conversational behavior—therefore it meets the exclusion criteria. 1-sentence explanation: I score 1 because it fails the core inclusion requirement of centering on Spoken Language Models/SLMs (or equivalent) and is limited to single-task TTS.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Transferable & Stealthy Ensemble Attacks: A Black-Box Jailbreaking Framework for Large Language Models",
    "abstract": "We present a novel black-box jailbreaking framework that integrates multiple LLM-as-Attacker strategies to deliver highly transferable and effective attacks. The framework is grounded in three key insights from prior jailbreaking research and practice: ensemble approaches outperform single methods in exposing aligned LLM vulnerabilities, malicious instructions vary in jailbreaking difficulty requiring tailored optimization, and disrupting semantic coherence of malicious prompts can manipulate their embeddings to boost success rates. Validated in the Competition for LLM and Agent Safety 2024, our solution achieved top rankings in the Jailbreaking Attack Track.",
    "metadata": {
      "arxiv_id": "2410.23558",
      "title": "Transferable & Stealthy Ensemble Attacks: A Black-Box Jailbreaking Framework for Large Language Models",
      "summary": "We present a novel black-box jailbreaking framework that integrates multiple LLM-as-Attacker strategies to deliver highly transferable and effective attacks. The framework is grounded in three key insights from prior jailbreaking research and practice: ensemble approaches outperform single methods in exposing aligned LLM vulnerabilities, malicious instructions vary in jailbreaking difficulty requiring tailored optimization, and disrupting semantic coherence of malicious prompts can manipulate their embeddings to boost success rates. Validated in the Competition for LLM and Agent Safety 2024, our solution achieved top rankings in the Jailbreaking Attack Track.",
      "authors": [
        "Yiqi Yang",
        "Hongye Fu"
      ],
      "published": "2024-10-31T01:55:33Z",
      "updated": "2025-11-06T07:56:24Z",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.23558v3",
      "landing_url": "https://arxiv.org/abs/2410.23558v3",
      "doi": "https://doi.org/10.48550/arXiv.2410.23558"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The article centers on black-box jailbreaking of large language models (text-based) with ensemble attack strategies, and has no focus on Spoken Language Models, speech-language modeling, or interactive speech evaluation, so it does not meet the stipulated inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The article centers on black-box jailbreaking of large language models (text-based) with ensemble attack strategies, and has no focus on Spoken Language Models, speech-language modeling, or interactive speech evaluation, so it does not meet the stipulated inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on black-box jailbreaking attacks on large language models, without any mention or indication of spoken language modeling, speech-level language modeling, full-duplex spoken language models, or any speech-based model architecture, training, or evaluation. It also does not cover real-time interaction, speech overlap, backchanneling, or behavioral arbitration, and there is no mention of speech as the core modeling object. Therefore, it does not meet the inclusion criteria focused on Spoken Language Models or Full-Duplex Spoken Language Models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on black-box jailbreaking attacks on large language models, without any mention or indication of spoken language modeling, speech-level language modeling, full-duplex spoken language models, or any speech-based model architecture, training, or evaluation. It also does not cover real-time interaction, speech overlap, backchanneling, or behavioral arbitration, and there is no mention of speech as the core modeling object. Therefore, it does not meet the inclusion criteria focused on Spoken Language Models or Full-Duplex Spoken Language Models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Posture-Informed Muscular Force Learning for Robust Hand Pressure Estimation",
    "abstract": "We present PiMForce, a novel framework that enhances hand pressure estimation by leveraging 3D hand posture information to augment forearm surface electromyography (sEMG) signals. Our approach utilizes detailed spatial information from 3D hand poses in conjunction with dynamic muscle activity from sEMG to enable accurate and robust whole-hand pressure measurements under diverse hand-object interactions. We also developed a multimodal data collection system that combines a pressure glove, an sEMG armband, and a markerless finger-tracking module. We created a comprehensive dataset from 21 participants, capturing synchronized data of hand posture, sEMG signals, and exerted hand pressure across various hand postures and hand-object interaction scenarios using our collection system. Our framework enables precise hand pressure estimation in complex and natural interaction scenarios. Our approach substantially mitigates the limitations of traditional sEMG-based or vision-based methods by integrating 3D hand posture information with sEMG signals. Video demos, data, and code are available online.",
    "metadata": {
      "arxiv_id": "2410.23629",
      "title": "Posture-Informed Muscular Force Learning for Robust Hand Pressure Estimation",
      "summary": "We present PiMForce, a novel framework that enhances hand pressure estimation by leveraging 3D hand posture information to augment forearm surface electromyography (sEMG) signals. Our approach utilizes detailed spatial information from 3D hand poses in conjunction with dynamic muscle activity from sEMG to enable accurate and robust whole-hand pressure measurements under diverse hand-object interactions. We also developed a multimodal data collection system that combines a pressure glove, an sEMG armband, and a markerless finger-tracking module. We created a comprehensive dataset from 21 participants, capturing synchronized data of hand posture, sEMG signals, and exerted hand pressure across various hand postures and hand-object interaction scenarios using our collection system. Our framework enables precise hand pressure estimation in complex and natural interaction scenarios. Our approach substantially mitigates the limitations of traditional sEMG-based or vision-based methods by integrating 3D hand posture information with sEMG signals. Video demos, data, and code are available online.",
      "authors": [
        "Kyungjin Seo",
        "Junghoon Seo",
        "Hanseok Jeong",
        "Sangpil Kim",
        "Sang Ho Yoon"
      ],
      "published": "2024-10-31T04:42:43Z",
      "updated": "2024-11-01T08:38:21Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.23629v2",
      "landing_url": "https://arxiv.org/abs/2410.23629v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.23629"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on hand posture, sEMG signals, and hand pressure estimation, with no focus on Spoken Language Models or speech-language modeling concepts, thus failing the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on hand posture, sEMG signals, and hand pressure estimation, with no focus on Spoken Language Models or speech-language modeling concepts, thus failing the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on enhancing hand pressure estimation using 3D hand posture and sEMG signals, without any mention or discussion of Spoken Language Models, speech language modeling, or related concepts such as full-duplex spoken language interaction or cognitive parallelism. It does not address speech as a primary modeling target or discuss model architecture, training, or evaluation in the speech domain; therefore, it does not meet any inclusion criterion and does not fit the research scope on speech language models or full-duplex spoken language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on enhancing hand pressure estimation using 3D hand posture and sEMG signals, without any mention or discussion of Spoken Language Models, speech language modeling, or related concepts such as full-duplex spoken language interaction or cognitive parallelism. It does not address speech as a primary modeling target or discuss model architecture, training, or evaluation in the speech domain; therefore, it does not meet any inclusion criterion and does not fit the research scope on speech language models or full-duplex spoken language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Improbable Bigrams Expose Vulnerabilities of Incomplete Tokens in Byte-Level Tokenizers",
    "abstract": "Tokenization is a crucial step that bridges human-readable text with model-readable discrete tokens. However, recent studies have revealed that tokenizers can be exploited to elicit unwanted model behaviors. In this work, we investigate incomplete tokens, i.e., undecodable tokens with stray bytes resulting from byte-level byte-pair encoding (BPE) tokenization. We hypothesize that such tokens are heavily reliant on their adjacent tokens and are fragile when paired with unfamiliar tokens. To demonstrate this vulnerability, we introduce improbable bigrams: out-of-distribution combinations of incomplete tokens designed to exploit their dependency. Our experiments show that improbable bigrams are significantly prone to hallucinatory behaviors. Surprisingly, the same phrases have drastically lower rates of hallucination (90% reduction in Llama3.1) when an alternative tokenization is used. We caution against the potential vulnerabilities introduced by byte-level BPE tokenizers, which may introduce blind spots to language models.",
    "metadata": {
      "arxiv_id": "2410.23684",
      "title": "Improbable Bigrams Expose Vulnerabilities of Incomplete Tokens in Byte-Level Tokenizers",
      "summary": "Tokenization is a crucial step that bridges human-readable text with model-readable discrete tokens. However, recent studies have revealed that tokenizers can be exploited to elicit unwanted model behaviors. In this work, we investigate incomplete tokens, i.e., undecodable tokens with stray bytes resulting from byte-level byte-pair encoding (BPE) tokenization. We hypothesize that such tokens are heavily reliant on their adjacent tokens and are fragile when paired with unfamiliar tokens. To demonstrate this vulnerability, we introduce improbable bigrams: out-of-distribution combinations of incomplete tokens designed to exploit their dependency. Our experiments show that improbable bigrams are significantly prone to hallucinatory behaviors. Surprisingly, the same phrases have drastically lower rates of hallucination (90% reduction in Llama3.1) when an alternative tokenization is used. We caution against the potential vulnerabilities introduced by byte-level BPE tokenizers, which may introduce blind spots to language models.",
      "authors": [
        "Eugene Jang",
        "Kimin Lee",
        "Jin-Woo Chung",
        "Keuntae Park",
        "Seungwon Shin"
      ],
      "published": "2024-10-31T07:19:44Z",
      "updated": "2025-10-10T15:55:08Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.23684v2",
      "landing_url": "https://arxiv.org/abs/2410.23684v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.23684"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item analyzes tokenization vulnerabilities in byte-level tokenizers for language models in text, not about Spoken Language Models or speech-based interaction; it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item analyzes tokenization vulnerabilities in byte-level tokenizers for language models in text, not about Spoken Language Models or speech-based interaction; it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on tokenization vulnerabilities in byte-level tokenizers and their impact on language models, with no mention of Spoken Language Models, speech as a core modeling object, full-duplex interaction, or synchronization aspects central to the inclusion criteria. The work is centered around byte-pair encoding at the text tokenization level rather than speech or spoken language models, and does not discuss speech modeling or real-time speech interaction, which are mandatory inclusion criteria. Therefore, it does not meet the required inclusion criteria and falls into exclusion due to focusing on text-level tokenization vulnerabilities rather than speech or spoken language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on tokenization vulnerabilities in byte-level tokenizers and their impact on language models, with no mention of Spoken Language Models, speech as a core modeling object, full-duplex interaction, or synchronization aspects central to the inclusion criteria. The work is centered around byte-pair encoding at the text tokenization level rather than speech or spoken language models, and does not discuss speech modeling or real-time speech interaction, which are mandatory inclusion criteria. Therefore, it does not meet the required inclusion criteria and falls into exclusion due to focusing on text-level tokenization vulnerabilities rather than speech or spoken language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Asynchronous Jump Testing and Estimation in High Dimensions Under Complex Temporal Dynamics",
    "abstract": "Most high dimensional changepoint detection methods assume the error process is stationary and changepoints occur synchronously across dimensions. The violation of these assumptions, which in applied settings is increasingly likely as the dimensionality of the time series being analyzed grows, can dramatically curtail the sensitivity or the accuracy of these methods. We propose AJDN (Asynchronous Jump Detection under Nonstationary noise). AJDN is a high dimensional multiscale jump detection method that tests and estimates jumps in an otherwise smoothly varying mean function for high dimensional time series with nonstationary noise where the jumps across dimensions may not occur at the same time. AJDN is correct in the sense that it detects the correct number of jumps with a prescribed probability asymptotically and its accuracy in estimating the locations of the jumps is asymptotically nearly optimal under the asynchronous jump assumption. Through a simulation study we demonstrate AJDN's robustness across a wide variety of stationary and nonstationary high dimensional time series, and we show its strong performance relative to some existing high dimensional changepoint detection methods. We apply AJDN to a seismic time series to demonstrate its ability to accurately detect jumps in real-world high dimensional time series with complex temporal dynamics.",
    "metadata": {
      "arxiv_id": "2410.23706",
      "title": "Asynchronous Jump Testing and Estimation in High Dimensions Under Complex Temporal Dynamics",
      "summary": "Most high dimensional changepoint detection methods assume the error process is stationary and changepoints occur synchronously across dimensions. The violation of these assumptions, which in applied settings is increasingly likely as the dimensionality of the time series being analyzed grows, can dramatically curtail the sensitivity or the accuracy of these methods. We propose AJDN (Asynchronous Jump Detection under Nonstationary noise). AJDN is a high dimensional multiscale jump detection method that tests and estimates jumps in an otherwise smoothly varying mean function for high dimensional time series with nonstationary noise where the jumps across dimensions may not occur at the same time. AJDN is correct in the sense that it detects the correct number of jumps with a prescribed probability asymptotically and its accuracy in estimating the locations of the jumps is asymptotically nearly optimal under the asynchronous jump assumption. Through a simulation study we demonstrate AJDN's robustness across a wide variety of stationary and nonstationary high dimensional time series, and we show its strong performance relative to some existing high dimensional changepoint detection methods. We apply AJDN to a seismic time series to demonstrate its ability to accurately detect jumps in real-world high dimensional time series with complex temporal dynamics.",
      "authors": [
        "Weichi Wu",
        "David Veitch",
        "Zhou Zhou"
      ],
      "published": "2024-10-31T07:51:49Z",
      "updated": "2024-11-23T04:40:55Z",
      "categories": [
        "stat.ME"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.23706v2",
      "landing_url": "https://arxiv.org/abs/2410.23706v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.23706"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about asynchronous jump detection in high-dimensional time series with nonstationary noise, not about Spoken Language Models or speech-language modeling; it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about asynchronous jump detection in high-dimensional time series with nonstationary noise, not about Spoken Language Models or speech-language modeling; it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on asynchronous jump detection in high dimensional time series, particularly for seismic data, without addressing spoken language modeling, speech language models, or full-duplex spoken language models. The core topic does not align with the inclusion criteria that require explicit coverage of speech-based language modeling or full-duplex interaction capabilities, and no mention of speech or related modeling can be found in the title or abstract. Therefore, it does not meet the inclusion criteria and falls outside the scope defined for selection.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on asynchronous jump detection in high dimensional time series, particularly for seismic data, without addressing spoken language modeling, speech language models, or full-duplex spoken language models. The core topic does not align with the inclusion criteria that require explicit coverage of speech-based language modeling or full-duplex interaction capabilities, and no mention of speech or related modeling can be found in the title or abstract. Therefore, it does not meet the inclusion criteria and falls outside the scope defined for selection.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models",
    "abstract": "Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",
    "metadata": {
      "arxiv_id": "2410.24177",
      "title": "DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models",
      "summary": "Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",
      "authors": [
        "Heng-Jui Chang",
        "Hongyu Gong",
        "Changhan Wang",
        "James Glass",
        "Yu-An Chung"
      ],
      "published": "2024-10-31T17:43:13Z",
      "updated": "2024-10-31T17:43:13Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.24177v1",
      "landing_url": "https://arxiv.org/abs/2410.24177v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.24177"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work explicitly centers on Spoken Language Models and speech-language modeling, introducing a speaker-invariant speech tokenizer to improve SLM tokenization and downstream tasks, within the last three years, and discusses model architecture and evaluation rather than being solely ASR/TTS or a non-academic source.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work explicitly centers on Spoken Language Models and speech-language modeling, introducing a speaker-invariant speech tokenizer to improve SLM tokenization and downstream tasks, within the last three years, and discusses model architecture and evaluation rather than being solely ASR/TTS or a non-academic source.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on Spoken Language Models (SLMs) and proposes a method (DC-Spin) to improve speech tokenization for SLMs, directly dealing with speech as the core modeling object and discussing model architectures and evaluations, which aligns well with the inclusion criteria. However, it does not explicitly mention addressing full-duplex interaction, synchronous behavior arbitration, or cognitive parallelism, which are emphasized in the inclusion criteria, nor does it provide evidence of multi-dimensional evaluation related to real-time interaction or speech overlap management. Thus, while it fits many inclusion points, it lacks key elements related to full-duplex spoken language interaction and multi-dimensional behavioral assessment.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on Spoken Language Models (SLMs) and proposes a method (DC-Spin) to improve speech tokenization for SLMs, directly dealing with speech as the core modeling object and discussing model architectures and evaluations, which aligns well with the inclusion criteria. However, it does not explicitly mention addressing full-duplex interaction, synchronous behavior arbitration, or cognitive parallelism, which are emphasized in the inclusion criteria, nor does it provide evidence of multi-dimensional evaluation related to real-time interaction or speech overlap management. Thus, while it fits many inclusion points, it lacks key elements related to full-duplex spoken language interaction and multi-dimensional behavioral assessment.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The paper explicitly targets Spoken Language Models (title and abstract mention SLMs), proposes a novel speech-tokenization method (DC-Spin) that treats speech as the primary modeling object, and reports evaluations and downstream SLM tasks—so it meets the inclusion requirements and does not fall under exclusion (it is not merely ASR/TTS or a text-centric study); junior reviewers split (5 vs 3) mainly because the paper does not address full‑duplex/real‑time interaction specifics, but those are not mandatory exclusion criteria here, so I recommend inclusion.",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "The paper explicitly targets Spoken Language Models (title and abstract mention SLMs), proposes a novel speech-tokenization method (DC-Spin) that treats speech as the primary modeling object, and reports evaluations and downstream SLM tasks—so it meets the inclusion requirements and does not fall under exclusion (it is not merely ASR/TTS or a text-centric study); junior reviewers split (5 vs 3) mainly because the paper does not address full‑duplex/real‑time interaction specifics, but those are not mandatory exclusion criteria here, so I recommend inclusion.",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Cross-Fundus Transformer for Multi-modal Diabetic Retinopathy Grading with Cataract",
    "abstract": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide and a common complication of diabetes. As two different imaging tools for DR grading, color fundus photography (CFP) and infrared fundus photography (IFP) are highly-correlated and complementary in clinical applications. To the best of our knowledge, this is the first study that explores a novel multi-modal deep learning framework to fuse the information from CFP and IFP towards more accurate DR grading. Specifically, we construct a dual-stream architecture Cross-Fundus Transformer (CFT) to fuse the ViT-based features of two fundus image modalities. In particular, a meticulously engineered Cross-Fundus Attention (CFA) module is introduced to capture the correspondence between CFP and IFP images. Moreover, we adopt both the single-modality and multi-modality supervisions to maximize the overall performance for DR grading. Extensive experiments on a clinical dataset consisting of 1,713 pairs of multi-modal fundus images demonstrate the superiority of our proposed method. Our code will be released for public access.",
    "metadata": {
      "arxiv_id": "2411.00726",
      "title": "Cross-Fundus Transformer for Multi-modal Diabetic Retinopathy Grading with Cataract",
      "summary": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide and a common complication of diabetes. As two different imaging tools for DR grading, color fundus photography (CFP) and infrared fundus photography (IFP) are highly-correlated and complementary in clinical applications. To the best of our knowledge, this is the first study that explores a novel multi-modal deep learning framework to fuse the information from CFP and IFP towards more accurate DR grading. Specifically, we construct a dual-stream architecture Cross-Fundus Transformer (CFT) to fuse the ViT-based features of two fundus image modalities. In particular, a meticulously engineered Cross-Fundus Attention (CFA) module is introduced to capture the correspondence between CFP and IFP images. Moreover, we adopt both the single-modality and multi-modality supervisions to maximize the overall performance for DR grading. Extensive experiments on a clinical dataset consisting of 1,713 pairs of multi-modal fundus images demonstrate the superiority of our proposed method. Our code will be released for public access.",
      "authors": [
        "Fan Xiao",
        "Junlin Hou",
        "Ruiwei Zhao",
        "Rui Feng",
        "Haidong Zou",
        "Lina Lu",
        "Yi Xu",
        "Juzhao Zhang"
      ],
      "published": "2024-11-01T16:38:49Z",
      "updated": "2024-11-01T16:38:49Z",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00726v1",
      "landing_url": "https://arxiv.org/abs/2411.00726v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.00726"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is a vision/medical imaging paper on multi-modal fundus imaging for diabetic retinopathy grading and does not address Spoken Language Models or speech-language modeling, failing all inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is a vision/medical imaging paper on multi-modal fundus imaging for diabetic retinopathy grading and does not address Spoken Language Models or speech-language modeling, failing all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on diabetic retinopathy grading using multi-modal fundus imaging and deep learning, with no involvement of spoken language models, speech language modeling, or real-time full-duplex spoken interaction modeling; it does not meet any inclusion criteria related to spoken language models and is unrelated to the specified topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on diabetic retinopathy grading using multi-modal fundus imaging and deep learning, with no involvement of spoken language models, speech language modeling, or real-time full-duplex spoken interaction modeling; it does not meet any inclusion criteria related to spoken language models and is unrelated to the specified topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Personality Analysis from Online Short Video Platforms with Multi-domain Adaptation",
    "abstract": "Personality analysis from online short videos has gained prominence due to its applications in personalized recommendation systems, sentiment analysis, and human-computer interaction. Traditional assessment methods, such as questionnaires based on the Big Five Personality Framework, are limited by self-report biases and are impractical for large-scale or real-time analysis. Leveraging the rich, multi-modal data present in short videos offers a promising alternative for more accurate personality inference. However, integrating these diverse and asynchronous modalities poses significant challenges, particularly in aligning time-varying data and ensuring models generalize well to new domains with limited labeled data. In this paper, we propose a novel multi-modal personality analysis framework that addresses these challenges by synchronizing and integrating features from multiple modalities and enhancing model generalization through domain adaptation. We introduce a timestamp-based modality alignment mechanism that synchronizes data based on spoken word timestamps, ensuring accurate correspondence across modalities and facilitating effective feature integration. To capture temporal dependencies and inter-modal interactions, we employ Bidirectional Long Short-Term Memory networks and self-attention mechanisms, allowing the model to focus on the most informative features for personality prediction. Furthermore, we develop a gradient-based domain adaptation method that transfers knowledge from multiple source domains to improve performance in target domains with scarce labeled data. Extensive experiments on real-world datasets demonstrate that our framework significantly outperforms existing methods in personality prediction tasks, highlighting its effectiveness in capturing complex behavioral cues and robustness in adapting to new domains.",
    "metadata": {
      "arxiv_id": "2411.00813",
      "title": "Personality Analysis from Online Short Video Platforms with Multi-domain Adaptation",
      "summary": "Personality analysis from online short videos has gained prominence due to its applications in personalized recommendation systems, sentiment analysis, and human-computer interaction. Traditional assessment methods, such as questionnaires based on the Big Five Personality Framework, are limited by self-report biases and are impractical for large-scale or real-time analysis. Leveraging the rich, multi-modal data present in short videos offers a promising alternative for more accurate personality inference. However, integrating these diverse and asynchronous modalities poses significant challenges, particularly in aligning time-varying data and ensuring models generalize well to new domains with limited labeled data. In this paper, we propose a novel multi-modal personality analysis framework that addresses these challenges by synchronizing and integrating features from multiple modalities and enhancing model generalization through domain adaptation. We introduce a timestamp-based modality alignment mechanism that synchronizes data based on spoken word timestamps, ensuring accurate correspondence across modalities and facilitating effective feature integration. To capture temporal dependencies and inter-modal interactions, we employ Bidirectional Long Short-Term Memory networks and self-attention mechanisms, allowing the model to focus on the most informative features for personality prediction. Furthermore, we develop a gradient-based domain adaptation method that transfers knowledge from multiple source domains to improve performance in target domains with scarce labeled data. Extensive experiments on real-world datasets demonstrate that our framework significantly outperforms existing methods in personality prediction tasks, highlighting its effectiveness in capturing complex behavioral cues and robustness in adapting to new domains.",
      "authors": [
        "Sixu An",
        "Xiangguo Sun",
        "Yicong Li",
        "Yu Yang",
        "Guandong Xu"
      ],
      "published": "2024-10-26T03:29:32Z",
      "updated": "2024-10-26T03:29:32Z",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.CY",
        "cs.LG",
        "cs.SI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00813v1",
      "landing_url": "https://arxiv.org/abs/2411.00813v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.00813"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "While the abstract mentions audio timestamps, the work targets multi-modal personality prediction and is not centered on Spoken Language Models or speech-language modeling, lacking core architecture/training/evaluation for SLMs or FD-SLMs, so it should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "While the abstract mentions audio timestamps, the work targets multi-modal personality prediction and is not centered on Spoken Language Models or speech-language modeling, lacking core architecture/training/evaluation for SLMs or FD-SLMs, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on personality analysis from online short videos using multi-modal data, but it does not mention spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs) as core modeling objects, nor does it discuss speech-level language modeling or end-to-end architectures centered on speech. It primarily emphasizes synchronizing spoken word timestamps for multimodal integration and domain adaptation for personality prediction, which aligns more with behavioral analysis than speech language modeling research. Therefore, it does not meet the inclusion criteria centered on SLMs and FD-SLMs, nor does it engage with synchronous interaction or the detailed spoken language modeling concepts required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on personality analysis from online short videos using multi-modal data, but it does not mention spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs) as core modeling objects, nor does it discuss speech-level language modeling or end-to-end architectures centered on speech. It primarily emphasizes synchronizing spoken word timestamps for multimodal integration and domain adaptation for personality prediction, which aligns more with behavioral analysis than speech language modeling research. Therefore, it does not meet the inclusion criteria centered on SLMs and FD-SLMs, nor does it engage with synchronous interaction or the detailed spoken language modeling concepts required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback",
    "abstract": "While textless Spoken Language Models (SLMs) have shown potential in end-to-end speech-to-speech modeling, they still lag behind text-based Large Language Models (LLMs) in terms of semantic coherence and relevance. This work introduces the Align-SLM framework, which leverages preference optimization inspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the semantic understanding of SLMs. Our approach generates multiple speech continuations from a given prompt and uses semantic metrics to create preference data for Direct Preference Optimization (DPO). We evaluate the framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling, the spoken version of the StoryCloze dataset for semantic coherence, and other speech generation metrics, including the GPT4-o score and human evaluation. Experimental results show that our method achieves state-of-the-art performance for SLMs on most benchmarks, highlighting the importance of preference optimization to improve the semantics of SLMs.",
    "metadata": {
      "arxiv_id": "2411.01834",
      "title": "Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback",
      "summary": "While textless Spoken Language Models (SLMs) have shown potential in end-to-end speech-to-speech modeling, they still lag behind text-based Large Language Models (LLMs) in terms of semantic coherence and relevance. This work introduces the Align-SLM framework, which leverages preference optimization inspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the semantic understanding of SLMs. Our approach generates multiple speech continuations from a given prompt and uses semantic metrics to create preference data for Direct Preference Optimization (DPO). We evaluate the framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling, the spoken version of the StoryCloze dataset for semantic coherence, and other speech generation metrics, including the GPT4-o score and human evaluation. Experimental results show that our method achieves state-of-the-art performance for SLMs on most benchmarks, highlighting the importance of preference optimization to improve the semantics of SLMs.",
      "authors": [
        "Guan-Ting Lin",
        "Prashanth Gurunath Shivakumar",
        "Aditya Gourav",
        "Yile Gu",
        "Ankur Gandhe",
        "Hung-yi Lee",
        "Ivan Bulyko"
      ],
      "published": "2024-11-04T06:07:53Z",
      "updated": "2025-05-27T16:17:52Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.01834v2",
      "landing_url": "https://arxiv.org/abs/2411.01834v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.01834"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item explicitly centers on Spoken Language Models (textless SLMs), uses speech as the core modeling target with end-to-end speech-to-speech framing, discusses model architecture/training/evaluation, is within the last 3 years, and uses SLM terminology in the title/abstract, thus meeting all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This item explicitly centers on Spoken Language Models (textless SLMs), uses speech as the core modeling target with end-to-end speech-to-speech framing, discusses model architecture/training/evaluation, is within the last 3 years, and uses SLM terminology in the title/abstract, thus meeting all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Spoken Language Models (SLMs) with an emphasis on improving semantic understanding using reinforcement learning from AI feedback, clearly situating speech as the core modeling and reasoning object rather than just preprocessing for text. It involves training and evaluation using multiple benchmarks including lexical, syntactic, and semantic coherence at the speech level, aligning well with the inclusion criteria. However, the abstract does not explicitly mention full-duplex interaction features such as cognitive parallelism, speech overlap management, or real-time interactive aspects, which are key inclusion points; despite this, it presents state-of-the-art SLM results and multi-dimensional evaluations beyond just ASR or TTS tasks, indicating a broad modeling perspective. Given its focus on SLM architecture and evaluation within recent research, the paper is more aligned with inclusion than exclusion criteria but lacks some specific mentions of synchronous dialogue behaviors or low latency interaction required for full-duplex classification.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Spoken Language Models (SLMs) with an emphasis on improving semantic understanding using reinforcement learning from AI feedback, clearly situating speech as the core modeling and reasoning object rather than just preprocessing for text. It involves training and evaluation using multiple benchmarks including lexical, syntactic, and semantic coherence at the speech level, aligning well with the inclusion criteria. However, the abstract does not explicitly mention full-duplex interaction features such as cognitive parallelism, speech overlap management, or real-time interactive aspects, which are key inclusion points; despite this, it presents state-of-the-art SLM results and multi-dimensional evaluations beyond just ASR or TTS tasks, indicating a broad modeling perspective. Given its focus on SLM architecture and evaluation within recent research, the paper is more aligned with inclusion than exclusion criteria but lacks some specific mentions of synchronous dialogue behaviors or low latency interaction required for full-duplex classification.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Study of Data Augmentation Techniques to Overcome Data Scarcity in Wound Classification using Deep Learning",
    "abstract": "Chronic wounds are a significant burden on individuals and the healthcare system, affecting millions of people and incurring high costs. Wound classification using deep learning techniques is a promising approach for faster diagnosis and treatment initiation. However, lack of high quality data to train the ML models is a major challenge to realize the potential of ML in wound care. In fact, data limitations are the biggest challenge in studies using medical or forensic imaging today. We study data augmentation techniques that can be used to overcome the data scarcity limitations and unlock the potential of deep learning based solutions. In our study we explore a range of data augmentation techniques from geometric transformations of wound images to advanced GANs, to enrich and expand datasets. Using the Keras, Tensorflow, and Pandas libraries, we implemented the data augmentation techniques that can generate realistic wound images. We show that geometric data augmentation can improve classification performance, F1 scores, by up to 11% on top of state-of-the-art models, across several key classes of wounds. Our experiments with GAN based augmentation prove the viability of using DE-GANs to generate wound images with richer variations. Our study and results show that data augmentation is a valuable privacy-preserving tool with huge potential to overcome the data scarcity limitations and we believe it will be part of any real-world ML-based wound care system.",
    "metadata": {
      "arxiv_id": "2411.02456",
      "title": "A Study of Data Augmentation Techniques to Overcome Data Scarcity in Wound Classification using Deep Learning",
      "summary": "Chronic wounds are a significant burden on individuals and the healthcare system, affecting millions of people and incurring high costs. Wound classification using deep learning techniques is a promising approach for faster diagnosis and treatment initiation. However, lack of high quality data to train the ML models is a major challenge to realize the potential of ML in wound care. In fact, data limitations are the biggest challenge in studies using medical or forensic imaging today. We study data augmentation techniques that can be used to overcome the data scarcity limitations and unlock the potential of deep learning based solutions. In our study we explore a range of data augmentation techniques from geometric transformations of wound images to advanced GANs, to enrich and expand datasets. Using the Keras, Tensorflow, and Pandas libraries, we implemented the data augmentation techniques that can generate realistic wound images. We show that geometric data augmentation can improve classification performance, F1 scores, by up to 11% on top of state-of-the-art models, across several key classes of wounds. Our experiments with GAN based augmentation prove the viability of using DE-GANs to generate wound images with richer variations. Our study and results show that data augmentation is a valuable privacy-preserving tool with huge potential to overcome the data scarcity limitations and we believe it will be part of any real-world ML-based wound care system.",
      "authors": [
        "Harini Narayanan",
        "Sindhu Ghanta"
      ],
      "published": "2024-11-04T00:24:50Z",
      "updated": "2024-11-04T00:24:50Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.02456v1",
      "landing_url": "https://arxiv.org/abs/2411.02456v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.02456"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses data augmentation for wound classification using deep learning and does not involve Spoken Language Models or speech-centric modeling, training, or evaluation. It thus fails the inclusion criteria and meets exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses data augmentation for wound classification using deep learning and does not involve Spoken Language Models or speech-centric modeling, training, or evaluation. It thus fails the inclusion criteria and meets exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on data augmentation techniques in the medical imaging domain, specifically wound classification using deep learning, without any mention of spoken language models, speech language modeling, full-duplex interaction, or related concepts of voice-based modeling as required by the inclusion criteria. It does not address core topics such as speech sequence modeling, full-duplex spoken language modeling, or synchronization in speech interactions, thus failing to meet the thematic relevance. Therefore, it falls outside the scope of the inclusion criteria and matches none of the specific key concepts or required model focuses.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on data augmentation techniques in the medical imaging domain, specifically wound classification using deep learning, without any mention of spoken language models, speech language modeling, full-duplex interaction, or related concepts of voice-based modeling as required by the inclusion criteria. It does not address core topics such as speech sequence modeling, full-duplex spoken language modeling, or synchronization in speech interactions, thus failing to meet the thematic relevance. Therefore, it falls outside the scope of the inclusion criteria and matches none of the specific key concepts or required model focuses.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SynthSet: Generative Diffusion Model for Semantic Segmentation in Precision Agriculture",
    "abstract": "This paper introduces a methodology for generating synthetic annotated data to address data scarcity in semantic segmentation tasks within the precision agriculture domain. Utilizing Denoising Diffusion Probabilistic Models (DDPMs) and Generative Adversarial Networks (GANs), we propose a dual diffusion model architecture for synthesizing realistic annotated agricultural data, without any human intervention. We employ super-resolution to enhance the phenotypic characteristics of the synthesized images and their coherence with the corresponding generated masks. We showcase the utility of the proposed method for wheat head segmentation. The high quality of synthesized data underscores the effectiveness of the proposed methodology in generating image-mask pairs. Furthermore, models trained on our generated data exhibit promising performance when tested on an external, diverse dataset of real wheat fields. The results show the efficacy of the proposed methodology for addressing data scarcity for semantic segmentation tasks. Moreover, the proposed approach can be readily adapted for various segmentation tasks in precision agriculture and beyond.",
    "metadata": {
      "arxiv_id": "2411.03505",
      "title": "SynthSet: Generative Diffusion Model for Semantic Segmentation in Precision Agriculture",
      "summary": "This paper introduces a methodology for generating synthetic annotated data to address data scarcity in semantic segmentation tasks within the precision agriculture domain. Utilizing Denoising Diffusion Probabilistic Models (DDPMs) and Generative Adversarial Networks (GANs), we propose a dual diffusion model architecture for synthesizing realistic annotated agricultural data, without any human intervention. We employ super-resolution to enhance the phenotypic characteristics of the synthesized images and their coherence with the corresponding generated masks. We showcase the utility of the proposed method for wheat head segmentation. The high quality of synthesized data underscores the effectiveness of the proposed methodology in generating image-mask pairs. Furthermore, models trained on our generated data exhibit promising performance when tested on an external, diverse dataset of real wheat fields. The results show the efficacy of the proposed methodology for addressing data scarcity for semantic segmentation tasks. Moreover, the proposed approach can be readily adapted for various segmentation tasks in precision agriculture and beyond.",
      "authors": [
        "Andrew Heschl",
        "Mauricio Murillo",
        "Keyhan Najafian",
        "Farhad Maleki"
      ],
      "published": "2024-11-05T20:42:23Z",
      "updated": "2024-11-05T20:42:23Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.03505v1",
      "landing_url": "https://arxiv.org/abs/2411.03505v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.03505"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not address Spoken Language Models or any speech-language modeling; topic is semantic segmentation in agriculture (vision), which falls outside the inclusion criteria focused on SLM/FD-SLM and speech interaction features.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Does not address Spoken Language Models or any speech-language modeling; topic is semantic segmentation in agriculture (vision), which falls outside the inclusion criteria focused on SLM/FD-SLM and speech interaction features.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on semantic segmentation in precision agriculture using synthetic data generation methods and does not address spoken language models, speech language modeling, or related synchronous interactive speech behaviors. It does not fit the inclusion criteria centered on Spoken Language Models or Full-Duplex interaction models and is therefore unrelated to the defined research scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on semantic segmentation in precision agriculture using synthetic data generation methods and does not address spoken language models, speech language modeling, or related synchronous interactive speech behaviors. It does not fit the inclusion criteria centered on Spoken Language Models or Full-Duplex interaction models and is therefore unrelated to the defined research scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Beam Tracking for Full-Duplex User Terminals in Low Earth Orbit Satellite Communication Systems",
    "abstract": "This paper introduces a novel beam tracking scheme for full-duplex ground user terminals aiming to transmit uplink and receive downlink from two low Earth orbit (LEO) satellites at the same time and same frequency. Our proposed technique leverages observed phenomena from a recent measurement campaign to strategically select transmit and receive beams which couple low self-interference across the satellites' trajectories, thereby enabling in-band full-duplex operation. Our scheme takes a measurement-driven approach, meaning it does not rely on explicit knowledge of the self-interference channel and can inherently account for hardware impairments or other nonidealities. We show that our proposed scheme reliably selects beams which spatially cancel self-interference to below the noise floor, circumventing the need for digital/analog cancellation. Simulation results using satellite and orbital parameters published in 3GPP and FCC filings show that this substantial reduction in self-interference does not prohibitively compromise beamforming gain, allowing the user terminal to attain near-maximal SINRs, thus unlocking full-duplex operation.",
    "metadata": {
      "arxiv_id": "2411.03606",
      "title": "Beam Tracking for Full-Duplex User Terminals in Low Earth Orbit Satellite Communication Systems",
      "summary": "This paper introduces a novel beam tracking scheme for full-duplex ground user terminals aiming to transmit uplink and receive downlink from two low Earth orbit (LEO) satellites at the same time and same frequency. Our proposed technique leverages observed phenomena from a recent measurement campaign to strategically select transmit and receive beams which couple low self-interference across the satellites' trajectories, thereby enabling in-band full-duplex operation. Our scheme takes a measurement-driven approach, meaning it does not rely on explicit knowledge of the self-interference channel and can inherently account for hardware impairments or other nonidealities. We show that our proposed scheme reliably selects beams which spatially cancel self-interference to below the noise floor, circumventing the need for digital/analog cancellation. Simulation results using satellite and orbital parameters published in 3GPP and FCC filings show that this substantial reduction in self-interference does not prohibitively compromise beamforming gain, allowing the user terminal to attain near-maximal SINRs, thus unlocking full-duplex operation.",
      "authors": [
        "Chaeyeon Kim",
        "Joohyun Son",
        "Daesik Hong",
        "Ian P. Roberts"
      ],
      "published": "2024-11-06T02:00:48Z",
      "updated": "2024-11-13T23:39:07Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.03606v2",
      "landing_url": "https://arxiv.org/abs/2411.03606v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.03606"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on beam tracking for full-duplex satellite communications and self-interference with no reference to Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on beam tracking for full-duplex satellite communications and self-interference with no reference to Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on beam tracking for full-duplex communication in satellite user terminals, which is unrelated to Spoken Language Models or speech language modeling. There is no mention of speech as a modeling object or any aspect of synchronous spoken interaction or full-duplex spoken language models, thus not meeting the inclusion criteria focused on speech-language modeling and full-duplex spoken interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on beam tracking for full-duplex communication in satellite user terminals, which is unrelated to Spoken Language Models or speech language modeling. There is no mention of speech as a modeling object or any aspect of synchronous spoken interaction or full-duplex spoken language models, thus not meeting the inclusion criteria focused on speech-language modeling and full-duplex spoken interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Analog Beamforming Codebooks for Wideband Full-Duplex Millimeter-Wave Systems",
    "abstract": "In full-duplex millimeter-wave (mmWave) systems, the effects of beam squint and the frequency-selectivity of self-interference exacerbate over wide bandwidths. This complicates the use of beamforming to cancel self-interference when communicating over bandwidths on the order of gigahertz. In this work, we present the first analog beamforming codebooks tailored to wideband full-duplex mmWave systems, designed to both combat beam squint and cancel frequency-selective self-interference. Our proposed design constructs such codebooks by minimizing self-interference across the entire band of interest while constraining the coverage provided by these codebooks across that same band. Simulation results using computational electromagnetics to model self-interference suggest that a full-duplex 60 GHz system with our design enjoys lower self-interference and delivers better coverage across bandwidths as wide as 6 GHz, when compared to similar codebook designs that ignore beam squint and/or frequency-selectivity. This allows our design to sustain higher SINRs and spectral efficiencies across wide bandwidths, unlocking the potentials of wideband full-duplex mmWave systems.",
    "metadata": {
      "arxiv_id": "2411.03691",
      "title": "Analog Beamforming Codebooks for Wideband Full-Duplex Millimeter-Wave Systems",
      "summary": "In full-duplex millimeter-wave (mmWave) systems, the effects of beam squint and the frequency-selectivity of self-interference exacerbate over wide bandwidths. This complicates the use of beamforming to cancel self-interference when communicating over bandwidths on the order of gigahertz. In this work, we present the first analog beamforming codebooks tailored to wideband full-duplex mmWave systems, designed to both combat beam squint and cancel frequency-selective self-interference. Our proposed design constructs such codebooks by minimizing self-interference across the entire band of interest while constraining the coverage provided by these codebooks across that same band. Simulation results using computational electromagnetics to model self-interference suggest that a full-duplex 60 GHz system with our design enjoys lower self-interference and delivers better coverage across bandwidths as wide as 6 GHz, when compared to similar codebook designs that ignore beam squint and/or frequency-selectivity. This allows our design to sustain higher SINRs and spectral efficiencies across wide bandwidths, unlocking the potentials of wideband full-duplex mmWave systems.",
      "authors": [
        "Sungho Cho",
        "Ian P. Roberts"
      ],
      "published": "2024-11-06T06:19:07Z",
      "updated": "2024-11-06T06:19:07Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.03691v1",
      "landing_url": "https://arxiv.org/abs/2411.03691v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.03691"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item concerns analog beamforming for wideband full-duplex mmWave wireless systems and self-interference, not Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item concerns analog beamforming for wideband full-duplex mmWave wireless systems and self-interference, not Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on analog beamforming codebooks for millimeter-wave full-duplex systems, which are related to wireless communication hardware and signal processing, not spoken language models or speech language modeling. There is no mention of speech-focused language modeling, synchronization for spoken dialogue, or related evaluation metrics, and the study does not address full-duplex spoken language models or speech-based modeling as defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on analog beamforming codebooks for millimeter-wave full-duplex systems, which are related to wireless communication hardware and signal processing, not spoken language models or speech language modeling. There is no mention of speech-focused language modeling, synchronization for spoken dialogue, or related evaluation metrics, and the study does not address full-duplex spoken language models or speech-based modeling as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Unified Pathological Speech Analysis with Prompt Tuning",
    "abstract": "Pathological speech analysis has been of interest in the detection of certain diseases like depression and Alzheimer's disease and attracts much interest from researchers. However, previous pathological speech analysis models are commonly designed for a specific disease while overlooking the connection between diseases, which may constrain performance and lower training efficiency. Instead of fine-tuning deep models for different tasks, prompt tuning is a much more efficient training paradigm. We thus propose a unified pathological speech analysis system for as many as three diseases with the prompt tuning technique. This system uses prompt tuning to adjust only a small part of the parameters to detect different diseases from speeches of possible patients. Our system leverages a pre-trained spoken language model and demonstrates strong performance across multiple disorders while only fine-tuning a fraction of the parameters. This efficient training approach leads to faster convergence and improved F1 scores by allowing knowledge to be shared across tasks. Our experiments on Alzheimer's disease, Depression, and Parkinson's disease show competitive results, highlighting the effectiveness of our method in pathological speech analysis.",
    "metadata": {
      "arxiv_id": "2411.04142",
      "title": "Unified Pathological Speech Analysis with Prompt Tuning",
      "summary": "Pathological speech analysis has been of interest in the detection of certain diseases like depression and Alzheimer's disease and attracts much interest from researchers. However, previous pathological speech analysis models are commonly designed for a specific disease while overlooking the connection between diseases, which may constrain performance and lower training efficiency. Instead of fine-tuning deep models for different tasks, prompt tuning is a much more efficient training paradigm. We thus propose a unified pathological speech analysis system for as many as three diseases with the prompt tuning technique. This system uses prompt tuning to adjust only a small part of the parameters to detect different diseases from speeches of possible patients. Our system leverages a pre-trained spoken language model and demonstrates strong performance across multiple disorders while only fine-tuning a fraction of the parameters. This efficient training approach leads to faster convergence and improved F1 scores by allowing knowledge to be shared across tasks. Our experiments on Alzheimer's disease, Depression, and Parkinson's disease show competitive results, highlighting the effectiveness of our method in pathological speech analysis.",
      "authors": [
        "Fei Yang",
        "Xuenan Xu",
        "Mengyue Wu",
        "Kai Yu"
      ],
      "published": "2024-11-05T06:47:26Z",
      "updated": "2024-11-05T06:47:26Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04142v1",
      "landing_url": "https://arxiv.org/abs/2411.04142v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.04142"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "該研究以預訓練的 spoken language model 為核心，提出以 prompt tuning 為主的跨疾病語音分析系統，涵蓋模型架構、訓練與評估，且在過去三年公開，且摘要中出現 spoken language model(s)，符合 inclusion 條件，亦非屬排除條件。",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "該研究以預訓練的 spoken language model 為核心，提出以 prompt tuning 為主的跨疾病語音分析系統，涵蓋模型架構、訓練與評估，且在過去三年公開，且摘要中出現 spoken language model(s)，符合 inclusion 條件，亦非屬排除條件。",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on pathological speech analysis using prompt tuning on a pre-trained spoken language model but does not explicitly discuss full-duplex spoken language models, engineered synchronization, cognitive parallelism, or low latency interactions as specified in the inclusion criteria. The abstract emphasizes disease detection tasks rather than modeling of speech sequences or end-to-end speech language modeling architectures central to conversational dynamics and synchronous interaction. Thus, it lacks core concepts like synchrony, dialogue arbitration, and evaluation metrics related to real-time interactive speech processing required by the criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on pathological speech analysis using prompt tuning on a pre-trained spoken language model but does not explicitly discuss full-duplex spoken language models, engineered synchronization, cognitive parallelism, or low latency interactions as specified in the inclusion criteria. The abstract emphasizes disease detection tasks rather than modeling of speech sequences or end-to-end speech language modeling architectures central to conversational dynamics and synchronous interaction. Thus, it lacks core concepts like synchrony, dialogue arbitration, and evaluation metrics related to real-time interactive speech processing required by the criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "JuniorNano recommended inclusion because the abstract explicitly mentions a “pre-trained spoken language model” and fits the literal inclusion token requirement; JuniorMini recommended exclusion noting the paper focuses on disease-detection tasks and lacks any discussion of full‑duplex interaction, synchronization, or speech‑LM architectural/benchmark contributions. After integrating both opinions and checking the inclusion/exclusion rules, this work is a downstream pathological speech classification study that leverages a pre-trained spoken-language model but does not develop or evaluate spoken-language modeling itself, nor does it address full‑duplex interaction, learned/engineered synchronization, cognitive parallelism, low‑latency interactive benchmarks, or multi-turn overlap/behavior arbitration — therefore it fails the required scope and meets exclusion criteria.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "JuniorNano recommended inclusion because the abstract explicitly mentions a “pre-trained spoken language model” and fits the literal inclusion token requirement; JuniorMini recommended exclusion noting the paper focuses on disease-detection tasks and lacks any discussion of full‑duplex interaction, synchronization, or speech‑LM architectural/benchmark contributions. After integrating both opinions and checking the inclusion/exclusion rules, this work is a downstream pathological speech classification study that leverages a pre-trained spoken-language model but does not develop or evaluate spoken-language modeling itself, nor does it address full‑duplex interaction, learned/engineered synchronization, cognitive parallelism, low‑latency interactive benchmarks, or multi-turn overlap/behavior arbitration — therefore it fails the required scope and meets exclusion criteria.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Analyzing The Language of Visual Tokens",
    "abstract": "With the introduction of transformer-based models for vision and language tasks, such as LLaVA and Chameleon, there has been renewed interest in the discrete tokenized representation of images. These models often treat image patches as discrete tokens, analogous to words in natural language, learning joint alignments between visual and human languages. However, little is known about the statistical behavior of these visual languages - whether they follow similar frequency distributions, grammatical structures, or topologies as natural languages. In this paper, we take a natural-language-centric approach to analyzing discrete visual languages and uncover striking similarities and fundamental differences. We demonstrate that, although visual languages adhere to Zipfian distributions, higher token innovation drives greater entropy and lower compression, with tokens predominantly representing object parts, indicating intermediate granularity. We also show that visual languages lack cohesive grammatical structures, leading to higher perplexity and weaker hierarchical organization compared to natural languages. Finally, we demonstrate that, while vision models align more closely with natural languages than other models, this alignment remains significantly weaker than the cohesion found within natural languages. Through these experiments, we demonstrate how understanding the statistical properties of discrete visual languages can inform the design of more effective computer vision models.",
    "metadata": {
      "arxiv_id": "2411.05001",
      "title": "Analyzing The Language of Visual Tokens",
      "summary": "With the introduction of transformer-based models for vision and language tasks, such as LLaVA and Chameleon, there has been renewed interest in the discrete tokenized representation of images. These models often treat image patches as discrete tokens, analogous to words in natural language, learning joint alignments between visual and human languages. However, little is known about the statistical behavior of these visual languages - whether they follow similar frequency distributions, grammatical structures, or topologies as natural languages. In this paper, we take a natural-language-centric approach to analyzing discrete visual languages and uncover striking similarities and fundamental differences. We demonstrate that, although visual languages adhere to Zipfian distributions, higher token innovation drives greater entropy and lower compression, with tokens predominantly representing object parts, indicating intermediate granularity. We also show that visual languages lack cohesive grammatical structures, leading to higher perplexity and weaker hierarchical organization compared to natural languages. Finally, we demonstrate that, while vision models align more closely with natural languages than other models, this alignment remains significantly weaker than the cohesion found within natural languages. Through these experiments, we demonstrate how understanding the statistical properties of discrete visual languages can inform the design of more effective computer vision models.",
      "authors": [
        "David M. Chan",
        "Rodolfo Corona",
        "Joonyong Park",
        "Cheol Jun Cho",
        "Yutong Bai",
        "Trevor Darrell"
      ],
      "published": "2024-11-07T18:59:28Z",
      "updated": "2024-11-07T18:59:28Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.05001v1",
      "landing_url": "https://arxiv.org/abs/2411.05001v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.05001"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本研究聚焦視覺語言的離散語詞統計與模型對齊，並未以 Spoken Language Models（語音語言模型）為核心，亦未論及語音-文字混合、同步互動或語音語言建模評估，因此不符合包含條件，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本研究聚焦視覺語言的離散語詞統計與模型對齊，並未以 Spoken Language Models（語音語言模型）為核心，亦未論及語音-文字混合、同步互動或語音語言建模評估，因此不符合包含條件，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on analyzing visual token languages in vision-language transformer models, not on spoken language models or speech as a core modeling object, which is the main inclusion criterion. It does not discuss speech modeling, full-duplex spoken language interaction, or relevant architectures, training, and evaluation within speech language models. Therefore, it does not meet the thematic and conceptual requirements of the inclusion criteria and falls under exclusion for lacking speech language modeling focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on analyzing visual token languages in vision-language transformer models, not on spoken language models or speech as a core modeling object, which is the main inclusion criterion. It does not discuss speech modeling, full-duplex spoken language interaction, or relevant architectures, training, and evaluation within speech language models. Therefore, it does not meet the thematic and conceptual requirements of the inclusion criteria and falls under exclusion for lacking speech language modeling focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks",
    "abstract": "Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized human-machine interactions by seamlessly integrating various forms of data. Developing a universal spoken language model that comprehends a wide range of natural language instructions is critical for bridging communication gaps and facilitating more intuitive interactions. However, the absence of a comprehensive evaluation benchmark poses a significant challenge. We present Dynamic-SUPERB Phase-2, an open and evolving benchmark for the comprehensive evaluation of instruction-based universal speech models. Building upon the first generation, this second version incorporates 125 new tasks contributed collaboratively by the global research community, expanding the benchmark to a total of 180 tasks, making it the largest benchmark for speech and audio evaluation. While the first generation of Dynamic-SUPERB was limited to classification tasks, Dynamic-SUPERB Phase-2 broadens its evaluation capabilities by introducing a wide array of novel and diverse tasks, including regression and sequence generation, across speech, music, and environmental audio. Evaluation results show that no model performed well universally. SALMONN-13B excelled in English ASR and Qwen2-Audio-7B-Instruct showed high accuracy in emotion recognition, but current models still require further innovations to handle a broader range of tasks. We open-source all task data and the evaluation pipeline at https://github.com/dynamic-superb/dynamic-superb.",
    "metadata": {
      "arxiv_id": "2411.05361",
      "title": "Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks",
      "summary": "Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized human-machine interactions by seamlessly integrating various forms of data. Developing a universal spoken language model that comprehends a wide range of natural language instructions is critical for bridging communication gaps and facilitating more intuitive interactions. However, the absence of a comprehensive evaluation benchmark poses a significant challenge. We present Dynamic-SUPERB Phase-2, an open and evolving benchmark for the comprehensive evaluation of instruction-based universal speech models. Building upon the first generation, this second version incorporates 125 new tasks contributed collaboratively by the global research community, expanding the benchmark to a total of 180 tasks, making it the largest benchmark for speech and audio evaluation. While the first generation of Dynamic-SUPERB was limited to classification tasks, Dynamic-SUPERB Phase-2 broadens its evaluation capabilities by introducing a wide array of novel and diverse tasks, including regression and sequence generation, across speech, music, and environmental audio. Evaluation results show that no model performed well universally. SALMONN-13B excelled in English ASR and Qwen2-Audio-7B-Instruct showed high accuracy in emotion recognition, but current models still require further innovations to handle a broader range of tasks. We open-source all task data and the evaluation pipeline at https://github.com/dynamic-superb/dynamic-superb.",
      "authors": [
        "Chien-yu Huang",
        "Wei-Chih Chen",
        "Shu-wen Yang",
        "Andy T. Liu",
        "Chen-An Li",
        "Yu-Xiang Lin",
        "Wei-Cheng Tseng",
        "Anuj Diwan",
        "Yi-Jen Shih",
        "Jiatong Shi",
        "William Chen",
        "Chih-Kai Yang",
        "Wenze Ren",
        "Xuanjun Chen",
        "Chi-Yuan Hsiao",
        "Puyuan Peng",
        "Shih-Heng Wang",
        "Chun-Yi Kuan",
        "Ke-Han Lu",
        "Kai-Wei Chang",
        "Fabian Ritter-Gutierrez",
        "Kuan-Po Huang",
        "Siddhant Arora",
        "You-Kuan Lin",
        "Ming To Chuang",
        "Eunjung Yeo",
        "Kalvin Chang",
        "Chung-Ming Chien",
        "Kwanghee Choi",
        "Jun-You Wang",
        "Cheng-Hsiu Hsieh",
        "Yi-Cheng Lin",
        "Chee-En Yu",
        "I-Hsiang Chiu",
        "Heitor R. Guimarães",
        "Jionghao Han",
        "Tzu-Quan Lin",
        "Tzu-Yuan Lin",
        "Homu Chang",
        "Ting-Wu Chang",
        "Chun Wei Chen",
        "Shou-Jen Chen",
        "Yu-Hua Chen",
        "Hsi-Chun Cheng",
        "Kunal Dhawan",
        "Jia-Lin Fang",
        "Shi-Xin Fang",
        "Kuan-Yu Fang Chiang",
        "Chi An Fu",
        "Hsien-Fu Hsiao",
        "Ching Yu Hsu",
        "Shao-Syuan Huang",
        "Lee Chen Wei",
        "Hsi-Che Lin",
        "Hsuan-Hao Lin",
        "Hsuan-Ting Lin",
        "Jian-Ren Lin",
        "Ting-Chun Liu",
        "Li-Chun Lu",
        "Tsung-Min Pai",
        "Ankita Pasad",
        "Shih-Yun Shan Kuan",
        "Suwon Shon",
        "Yuxun Tang",
        "Yun-Shao Tsai",
        "Jui-Chiang Wei",
        "Tzu-Chieh Wei",
        "Chengxi Wu",
        "Dien-Ruei Wu",
        "Chao-Han Huck Yang",
        "Chieh-Chi Yang",
        "Jia Qi Yip",
        "Shao-Xiang Yuan",
        "Vahid Noroozi",
        "Zhehuai Chen",
        "Haibin Wu",
        "Karen Livescu",
        "David Harwath",
        "Shinji Watanabe",
        "Hung-yi Lee"
      ],
      "published": "2024-11-08T06:33:22Z",
      "updated": "2025-06-09T16:36:12Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.05361v2",
      "landing_url": "https://arxiv.org/abs/2411.05361v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.05361"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本研究以 Spoken Language Models 为核心，聚焦对话式、跨任务的语音模型评估基准（Dynamic-SUPERB Phase-2），标题/摘要明确提及 SLMs，涵盖多任务、跨模态评估，非仅 ASR/TTS，且属于近三年公开研究，故符合纳入标准且应纳入。",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "本研究以 Spoken Language Models 为核心，聚焦对话式、跨任务的语音模型评估基准（Dynamic-SUPERB Phase-2），标题/摘要明确提及 SLMs，涵盖多任务、跨模态评估，非仅 ASR/TTS，且属于近三年公开研究，故符合纳入标准且应纳入。",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents Dynamic-SUPERB Phase-2 as a large collaborative benchmark focused on evaluating universal spoken language models with diverse tasks including regression and sequence generation over speech and audio data, indicating a comprehensive evaluation framework beyond just ASR or TTS. However, the abstract does not specifically mention full-duplex spoken language models, engineered or learned synchronization, or real-time interactive features like cognitive parallelism or low latency, which are crucial inclusion criteria. While it addresses spoken language models and comprehensive evaluation, it lacks explicit focus on synchronous full-duplex interaction or behavioral arbitration. Given it meets some but not all key inclusion criteria and does not clearly hit exclusion criteria, the evaluation lands in an uncertain zone regarding relevance.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study presents Dynamic-SUPERB Phase-2 as a large collaborative benchmark focused on evaluating universal spoken language models with diverse tasks including regression and sequence generation over speech and audio data, indicating a comprehensive evaluation framework beyond just ASR or TTS. However, the abstract does not specifically mention full-duplex spoken language models, engineered or learned synchronization, or real-time interactive features like cognitive parallelism or low latency, which are crucial inclusion criteria. While it addresses spoken language models and comprehensive evaluation, it lacks explicit focus on synchronous full-duplex interaction or behavioral arbitration. Given it meets some but not all key inclusion criteria and does not clearly hit exclusion criteria, the evaluation lands in an uncertain zone regarding relevance.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviews: Reviewer A recommends inclusion (5) noting the work centers on spoken language model evaluation; Reviewer B was uncertain (3) because the abstract lacks explicit focus on full‑duplex/synchronization/real‑time interactive behaviors. The paper clearly treats speech/SLMs as the core and provides a comprehensive evaluation framework (meets the requirement to discuss evaluation of SLMs and is recent), but it does not address full‑duplex interaction, overlap handling, or low‑latency synchronized behaviors emphasized in the topic—therefore I lean to include it as relevant to SLM research but note it does not cover the full‑duplex synchronization aspects required for stricter inclusion.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Junior reviews: Reviewer A recommends inclusion (5) noting the work centers on spoken language model evaluation; Reviewer B was uncertain (3) because the abstract lacks explicit focus on full‑duplex/synchronization/real‑time interactive behaviors. The paper clearly treats speech/SLMs as the core and provides a comprehensive evaluation framework (meets the requirement to discuss evaluation of SLMs and is recent), but it does not address full‑duplex interaction, overlap handling, or low‑latency synchronized behaviors emphasized in the topic—therefore I lean to include it as relevant to SLM research but note it does not cover the full‑duplex synchronization aspects required for stricter inclusion.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Tell What You Hear From What You See -- Video to Audio Generation Through Text",
    "abstract": "The content of visual and audio scenes is multi-faceted such that a video can be paired with various audio and vice-versa. Thereby, in video-to-audio generation task, it is imperative to introduce steering approaches for controlling the generated audio. While Video-to-Audio generation is a well-established generative task, existing methods lack such controllability. In this work, we propose VATT, a multi-modal generative framework that takes a video and an optional text prompt as input, and generates audio and optional textual description of the audio. Such a framework has two advantages: i) Video-to-Audio generation process can be refined and controlled via text which complements the context of visual information, and ii) The model can suggest what audio to generate for the video by generating audio captions. VATT consists of two key modules: VATT Converter, a LLM that is fine-tuned for instructions and includes a projection layer that maps video features to the LLM vector space; and VATT Audio, a transformer that generates audio tokens from visual frames and from optional text prompt using iterative parallel decoding. The audio tokens are converted to a waveform by pretrained neural codec. Experiments show that when VATT is compared to existing video-to-audio generation methods in objective metrics, it achieves competitive performance when the audio caption is not provided. When the audio caption is provided as a prompt, VATT achieves even more refined performance (lowest KLD score of 1.41). Furthermore, subjective studies show that VATT Audio has been chosen as preferred generated audio than audio generated by existing methods. VATT enables controllable video-to-audio generation through text as well as suggesting text prompts for videos through audio captions, unlocking novel applications such as text-guided video-to-audio generation and video-to-audio captioning.",
    "metadata": {
      "arxiv_id": "2411.05679",
      "title": "Tell What You Hear From What You See -- Video to Audio Generation Through Text",
      "summary": "The content of visual and audio scenes is multi-faceted such that a video can be paired with various audio and vice-versa. Thereby, in video-to-audio generation task, it is imperative to introduce steering approaches for controlling the generated audio. While Video-to-Audio generation is a well-established generative task, existing methods lack such controllability. In this work, we propose VATT, a multi-modal generative framework that takes a video and an optional text prompt as input, and generates audio and optional textual description of the audio. Such a framework has two advantages: i) Video-to-Audio generation process can be refined and controlled via text which complements the context of visual information, and ii) The model can suggest what audio to generate for the video by generating audio captions. VATT consists of two key modules: VATT Converter, a LLM that is fine-tuned for instructions and includes a projection layer that maps video features to the LLM vector space; and VATT Audio, a transformer that generates audio tokens from visual frames and from optional text prompt using iterative parallel decoding. The audio tokens are converted to a waveform by pretrained neural codec. Experiments show that when VATT is compared to existing video-to-audio generation methods in objective metrics, it achieves competitive performance when the audio caption is not provided. When the audio caption is provided as a prompt, VATT achieves even more refined performance (lowest KLD score of 1.41). Furthermore, subjective studies show that VATT Audio has been chosen as preferred generated audio than audio generated by existing methods. VATT enables controllable video-to-audio generation through text as well as suggesting text prompts for videos through audio captions, unlocking novel applications such as text-guided video-to-audio generation and video-to-audio captioning.",
      "authors": [
        "Xiulong Liu",
        "Kun Su",
        "Eli Shlizerman"
      ],
      "published": "2024-11-08T16:29:07Z",
      "updated": "2025-04-04T21:50:29Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.05679v3",
      "landing_url": "https://arxiv.org/abs/2411.05679v3",
      "doi": "https://doi.org/10.48550/arXiv.2411.05679"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on video-to-audio generation and audio captioning in a multimodal setting, not on core Spoken Language Models or speech-language modeling; there is no emphasis on SLM/FD-SLM architecture, training, or evaluation of speech as the primary modeling object, thus it does not meet the core inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on video-to-audio generation and audio captioning in a multimodal setting, not on core Spoken Language Models or speech-language modeling; there is no emphasis on SLM/FD-SLM architecture, training, or evaluation of speech as the primary modeling object, thus it does not meet the core inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on video-to-audio generation controlled by text prompts and audio captioning, but it does not center on Spoken Language Models or the modeling and reasoning of speech as a core object, nor does it discuss multi-turn or full-duplex spoken language interactions, synchronization, or relevant evaluations; thus, it does not meet the inclusion criteria regarding SLMs or FD-SLMs, nor does it fit the exclusion criteria specifically related to speech-language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on video-to-audio generation controlled by text prompts and audio captioning, but it does not center on Spoken Language Models or the modeling and reasoning of speech as a core object, nor does it discuss multi-turn or full-duplex spoken language interactions, synchronization, or relevant evaluations; thus, it does not meet the inclusion criteria regarding SLMs or FD-SLMs, nor does it fit the exclusion criteria specifically related to speech-language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ajdmom: A Python Package for Deriving Moment Formulas of Affine Jump Diffusion Processes",
    "abstract": "We introduce ajdmom, a Python package designed for automatically deriving moment formulae for the well-established affine jump diffusion processes with state-independent jump intensities. ajdmom can produce explicit closed-form expressions for conditional and unconditional moments of any order, significantly enhancing the usability of these models. Additionally, ajdmom can compute partial derivatives of these moments with respect to the model parameters, offering a valuable tool for sensitivity analysis. The package's modular architecture makes it easy for adaptation and extension by researchers. ajdmom is open-source and readily available for installation from GitHub or the Python package index (PyPI).",
    "metadata": {
      "arxiv_id": "2411.06484",
      "title": "ajdmom: A Python Package for Deriving Moment Formulas of Affine Jump Diffusion Processes",
      "summary": "We introduce ajdmom, a Python package designed for automatically deriving moment formulae for the well-established affine jump diffusion processes with state-independent jump intensities. ajdmom can produce explicit closed-form expressions for conditional and unconditional moments of any order, significantly enhancing the usability of these models. Additionally, ajdmom can compute partial derivatives of these moments with respect to the model parameters, offering a valuable tool for sensitivity analysis. The package's modular architecture makes it easy for adaptation and extension by researchers. ajdmom is open-source and readily available for installation from GitHub or the Python package index (PyPI).",
      "authors": [
        "Yan-Feng Wu",
        "Jian-Qiang Hu"
      ],
      "published": "2024-11-10T14:55:10Z",
      "updated": "2025-04-07T02:27:58Z",
      "categories": [
        "q-fin.MF"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.06484v2",
      "landing_url": "https://arxiv.org/abs/2411.06484v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.06484"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item concerns a Python package for deriving moment formulas for affine jump diffusion processes in quantitative finance, not about Spoken Language Models or speech-language modeling; it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item concerns a Python package for deriving moment formulas for affine jump diffusion processes in quantitative finance, not about Spoken Language Models or speech-language modeling; it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on a Python package for affine jump diffusion processes moment formula derivation, which is unrelated to Spoken Language Models or full-duplex spoken language modeling as required; it does not discuss speech as the primary modeling object, nor any model architecture, training, or evaluation related to speech or interaction; thus, it does not meet inclusion criteria and matches exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on a Python package for affine jump diffusion processes moment formula derivation, which is unrelated to Spoken Language Models or full-duplex spoken language modeling as required; it does not discuss speech as the primary modeling object, nor any model architecture, training, or evaluation related to speech or interaction; thus, it does not meet inclusion criteria and matches exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Building a Taiwanese Mandarin Spoken Language Model: A First Attempt",
    "abstract": "This technical report presents our initial attempt to build a spoken large language model (LLM) for Taiwanese Mandarin, specifically tailored to enable real-time, speech-to-speech interaction in multi-turn conversations. Our end-to-end model incorporates a decoder-only transformer architecture and aims to achieve seamless interaction while preserving the conversational flow, including full-duplex capabilities allowing simultaneous speaking and listening. The paper also details the training process, including data preparation with synthesized dialogues and adjustments for real-time interaction. We also developed a platform to evaluate conversational fluency and response coherence in multi-turn dialogues. We hope the release of the report can contribute to the future development of spoken LLMs in Taiwanese Mandarin.",
    "metadata": {
      "arxiv_id": "2411.07111",
      "title": "Building a Taiwanese Mandarin Spoken Language Model: A First Attempt",
      "summary": "This technical report presents our initial attempt to build a spoken large language model (LLM) for Taiwanese Mandarin, specifically tailored to enable real-time, speech-to-speech interaction in multi-turn conversations. Our end-to-end model incorporates a decoder-only transformer architecture and aims to achieve seamless interaction while preserving the conversational flow, including full-duplex capabilities allowing simultaneous speaking and listening. The paper also details the training process, including data preparation with synthesized dialogues and adjustments for real-time interaction. We also developed a platform to evaluate conversational fluency and response coherence in multi-turn dialogues. We hope the release of the report can contribute to the future development of spoken LLMs in Taiwanese Mandarin.",
      "authors": [
        "Chih-Kai Yang",
        "Yu-Kuan Fu",
        "Chen-An Li",
        "Yi-Cheng Lin",
        "Yu-Xiang Lin",
        "Wei-Chih Chen",
        "Ho Lam Chung",
        "Chun-Yi Kuan",
        "Wei-Ping Huang",
        "Ke-Han Lu",
        "Tzu-Quan Lin",
        "Hsiu-Hsuan Wang",
        "En-Pei Hu",
        "Chan-Jan Hsu",
        "Liang-Hsuan Tseng",
        "I-Hsiang Chiu",
        "Ulin Sanga",
        "Xuanjun Chen",
        "Po-chun Hsu",
        "Shu-wen Yang",
        "Hung-yi Lee"
      ],
      "published": "2024-11-11T16:37:40Z",
      "updated": "2024-12-27T07:29:19Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.07111v2",
      "landing_url": "https://arxiv.org/abs/2411.07111v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.07111"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item explicitly centers on Spoken Language Models and full-duplex, real-time speech-to-speech interaction, describes end-to-end training and evaluation, and is within the last 3 years, meeting all inclusion criteria and none of the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This item explicitly centers on Spoken Language Models and full-duplex, real-time speech-to-speech interaction, describes end-to-end training and evaluation, and is within the last 3 years, meeting all inclusion criteria and none of the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper clearly focuses on spoken large language models with an end-to-end transformer architecture aimed at real-time, full-duplex speech interaction, which aligns well with the inclusion criteria that emphasize spoken language modeling with synchronization and multi-turn dialogue evaluation. It also explicitly addresses full-duplex capabilities, conversational flow, and real-time interaction evaluation, which excludes it from criteria related to half-duplex or only ASR/TTS tasks. Although the abstract does not detail the evaluation metrics extensively, it mentions a platform to evaluate conversational fluency and response coherence, satisfying the multidimensional evaluation aspect. The year of publication is not specified but we assume recency given the 'first attempt' phrasing and technical report context. Therefore, it meets the inclusion criteria and avoids the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper clearly focuses on spoken large language models with an end-to-end transformer architecture aimed at real-time, full-duplex speech interaction, which aligns well with the inclusion criteria that emphasize spoken language modeling with synchronization and multi-turn dialogue evaluation. It also explicitly addresses full-duplex capabilities, conversational flow, and real-time interaction evaluation, which excludes it from criteria related to half-duplex or only ASR/TTS tasks. Although the abstract does not detail the evaluation metrics extensively, it mentions a platform to evaluate conversational fluency and response coherence, satisfying the multidimensional evaluation aspect. The year of publication is not specified but we assume recency given the 'first attempt' phrasing and technical report context. Therefore, it meets the inclusion criteria and avoids the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Tracing the Roots: Leveraging Temporal Dynamics in Diffusion Trajectories for Origin Attribution",
    "abstract": "Diffusion models have transformed image synthesis through iterative denoising, by defining trajectories from noise to coherent data. While their capabilities are widely celebrated, a critical challenge remains unaddressed: ensuring responsible use by verifying whether an image originates from a model's training set, its novel generations or external sources. We introduce a framework that analyzes diffusion trajectories for this purpose. Specifically, we demonstrate that temporal dynamics across the entire trajectory allow for more robust classification and challenge the widely-adopted \"Goldilocks zone\" conjecture, which posits that membership inference is effective only within narrow denoising stages. More fundamentally, we expose critical flaws in current membership inference practices by showing that representative methods fail under distribution shifts or when model-generated data is present. For model attribution, we demonstrate a first white-box approach directly applicable to diffusion. Ultimately, we propose the unification of data provenance into a single, cohesive framework tailored to modern generative systems.",
    "metadata": {
      "arxiv_id": "2411.07449",
      "title": "Tracing the Roots: Leveraging Temporal Dynamics in Diffusion Trajectories for Origin Attribution",
      "summary": "Diffusion models have transformed image synthesis through iterative denoising, by defining trajectories from noise to coherent data. While their capabilities are widely celebrated, a critical challenge remains unaddressed: ensuring responsible use by verifying whether an image originates from a model's training set, its novel generations or external sources. We introduce a framework that analyzes diffusion trajectories for this purpose. Specifically, we demonstrate that temporal dynamics across the entire trajectory allow for more robust classification and challenge the widely-adopted \"Goldilocks zone\" conjecture, which posits that membership inference is effective only within narrow denoising stages. More fundamentally, we expose critical flaws in current membership inference practices by showing that representative methods fail under distribution shifts or when model-generated data is present. For model attribution, we demonstrate a first white-box approach directly applicable to diffusion. Ultimately, we propose the unification of data provenance into a single, cohesive framework tailored to modern generative systems.",
      "authors": [
        "Andreas Floros",
        "Seyed-Mohsen Moosavi-Dezfooli",
        "Pier Luigi Dragotti"
      ],
      "published": "2024-11-12T00:20:11Z",
      "updated": "2025-12-18T22:55:10Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.07449v3",
      "landing_url": "https://arxiv.org/abs/2411.07449v3",
      "doi": "https://doi.org/10.48550/arXiv.2411.07449"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses diffusion models for image provenance, not Spoken Language Models or speech-language modeling, thus it fails the core inclusion criteria oriented around SLM/FD-SLM concepts and does not address audio or dialogue analyses.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses diffusion models for image provenance, not Spoken Language Models or speech-language modeling, thus it fails the core inclusion criteria oriented around SLM/FD-SLM concepts and does not address audio or dialogue analyses.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on diffusion models for image synthesis and membership inference, which is unrelated to Spoken Language Models or speech language modeling; it does not address any aspects of speech sequences, full-duplex speech interaction, spoken language model architectures, or relevant evaluations within the last 3 years as per inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on diffusion models for image synthesis and membership inference, which is unrelated to Spoken Language Models or speech language modeling; it does not address any aspects of speech sequences, full-duplex speech interaction, spoken language model architectures, or relevant evaluations within the last 3 years as per inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models",
    "abstract": "With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs.",
    "metadata": {
      "arxiv_id": "2411.08742",
      "title": "A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models",
      "summary": "With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs.",
      "authors": [
        "Dingdong Wang",
        "Mingyu Cui",
        "Dongchao Yang",
        "Xueyuan Chen",
        "Helen Meng"
      ],
      "published": "2024-11-13T16:20:20Z",
      "updated": "2024-11-13T16:20:20Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.08742v1",
      "landing_url": "https://arxiv.org/abs/2411.08742v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.08742"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study explicitly centers on Spoken Language Models / Speech LLMs, treats speech representations (discrete vs continuous tokens) as core modeling objects with architecture/evaluation in semantic tasks, is within the last three years, and is not limited to ASR/TTS or single-task evaluations, meeting inclusion criteria and not triggering exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The study explicitly centers on Spoken Language Models / Speech LLMs, treats speech representations (discrete vs continuous tokens) as core modeling objects with architecture/evaluation in semantic tasks, is within the last three years, and is not limited to ASR/TTS or single-task evaluations, meeting inclusion criteria and not triggering exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on comparing discrete speech tokens and continuous features using a lightweight Speech Large Language Model, thus dealing with speech language modeling and semantic-related tasks. However, the abstract does not mention full-duplex interaction, synchronous turn-taking, cognitive parallelism, or low-latency real-time interaction, nor does it discuss engineered or learned synchronization mechanisms or multi-dimensional dynamic evaluations relevant to interactive dialogue behaviors. Therefore, while it fits the theme of speech language modeling, it lacks the focus on full-duplex interaction or detailed synchronization and behavior arbitration critical for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on comparing discrete speech tokens and continuous features using a lightweight Speech Large Language Model, thus dealing with speech language modeling and semantic-related tasks. However, the abstract does not mention full-duplex interaction, synchronous turn-taking, cognitive parallelism, or low-latency real-time interaction, nor does it discuss engineered or learned synchronization mechanisms or multi-dimensional dynamic evaluations relevant to interactive dialogue behaviors. Therefore, while it fits the theme of speech language modeling, it lacks the focus on full-duplex interaction or detailed synchronization and behavior arbitration critical for inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Integrated reasoning: JuniorNano recommended inclusion, correctly noting the paper explicitly centers on Speech Large Language Models and speech-token representations; JuniorMini recommended exclusion due to lack of full‑duplex / interaction/synchronization focus. Per the inclusion criteria the key requirements are explicit Speech/Spoken Language Model framing and analysis of speech-level modeling/architecture/evaluation, not necessarily full‑duplex interaction coverage. The abstract explicitly states \"Speech Large Language Models (Speech LLMs)\", compares discrete vs continuous speech representations, and analyzes reasons for performance differences and potential improvements—so it focuses on speech modeling and evaluation and does not fall into the exclusion categories (not merely ASR/TTS or text‑only ASR→LLM piping, nor limited to single downstream task). Therefore it meets all inclusion criteria and none of the exclusions.\nOne‑sentence explanation: Score 5 because the study explicitly treats speech tokens and Speech LLMs as the core modeling objects with comparative analysis of representations and evaluation, satisfying inclusion and not triggering exclusions.",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "Integrated reasoning: JuniorNano recommended inclusion, correctly noting the paper explicitly centers on Speech Large Language Models and speech-token representations; JuniorMini recommended exclusion due to lack of full‑duplex / interaction/synchronization focus. Per the inclusion criteria the key requirements are explicit Speech/Spoken Language Model framing and analysis of speech-level modeling/architecture/evaluation, not necessarily full‑duplex interaction coverage. The abstract explicitly states \"Speech Large Language Models (Speech LLMs)\", compares discrete vs continuous speech representations, and analyzes reasons for performance differences and potential improvements—so it focuses on speech modeling and evaluation and does not fall into the exclusion categories (not merely ASR/TTS or text‑only ASR→LLM piping, nor limited to single downstream task). Therefore it meets all inclusion criteria and none of the exclusions.\nOne‑sentence explanation: Score 5 because the study explicitly treats speech tokens and Speech LLMs as the core modeling objects with comparative analysis of representations and evaluation, satisfying inclusion and not triggering exclusions.",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Back to Supervision: Boosting Word Boundary Detection through Frame Classification",
    "abstract": "Speech segmentation at both word and phoneme levels is crucial for various speech processing tasks. It significantly aids in extracting meaningful units from an utterance, thus enabling the generation of discrete elements. In this work we propose a model-agnostic framework to perform word boundary detection in a supervised manner also employing a labels augmentation technique and an output-frame selection strategy. We trained and tested on the Buckeye dataset and only tested on TIMIT one, using state-of-the-art encoder models, including pre-trained solutions (Wav2Vec 2.0 and HuBERT), as well as convolutional and convolutional recurrent networks. Our method, with the HuBERT encoder, surpasses the performance of other state-of-the-art architectures, whether trained in supervised or self-supervised settings on the same datasets. Specifically, we achieved F-values of 0.8427 on the Buckeye dataset and 0.7436 on the TIMIT dataset, along with R-values of 0.8489 and 0.7807, respectively. These results establish a new state-of-the-art for both datasets. Beyond the immediate task, our approach offers a robust and efficient preprocessing method for future research in audio tokenization.",
    "metadata": {
      "arxiv_id": "2411.10423",
      "title": "Back to Supervision: Boosting Word Boundary Detection through Frame Classification",
      "summary": "Speech segmentation at both word and phoneme levels is crucial for various speech processing tasks. It significantly aids in extracting meaningful units from an utterance, thus enabling the generation of discrete elements. In this work we propose a model-agnostic framework to perform word boundary detection in a supervised manner also employing a labels augmentation technique and an output-frame selection strategy. We trained and tested on the Buckeye dataset and only tested on TIMIT one, using state-of-the-art encoder models, including pre-trained solutions (Wav2Vec 2.0 and HuBERT), as well as convolutional and convolutional recurrent networks. Our method, with the HuBERT encoder, surpasses the performance of other state-of-the-art architectures, whether trained in supervised or self-supervised settings on the same datasets. Specifically, we achieved F-values of 0.8427 on the Buckeye dataset and 0.7436 on the TIMIT dataset, along with R-values of 0.8489 and 0.7807, respectively. These results establish a new state-of-the-art for both datasets. Beyond the immediate task, our approach offers a robust and efficient preprocessing method for future research in audio tokenization.",
      "authors": [
        "Simone Carnemolla",
        "Salvatore Calcagno",
        "Simone Palazzo",
        "Daniela Giordano"
      ],
      "published": "2024-11-15T18:43:29Z",
      "updated": "2024-11-15T18:43:29Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.10423v1",
      "landing_url": "https://arxiv.org/abs/2411.10423v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.10423"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項聚焦於語音邊界偵測與分段，使用編碼器模型在 Buckeye/TIMIT 上的監督訓練與評估，並未以 Spoken Language Models / Speech Language Models 為核心的語音語言建模或對話互動框架，亦未涵蓋同步互動或語音重疊處理等 FD-SLM 的重點，因此不符合納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項聚焦於語音邊界偵測與分段，使用編碼器模型在 Buckeye/TIMIT 上的監督訓練與評估，並未以 Spoken Language Models / Speech Language Models 為核心的語音語言建模或對話互動框架，亦未涵蓋同步互動或語音重疊處理等 FD-SLM 的重點，因此不符合納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on word boundary detection as a preprocessing step in speech processing, without explicitly addressing spoken language models (SLMs), speech language modeling, or full-duplex interactive spoken language modeling, nor discussing model architectures or evaluation in those terms. It also lacks mention of synchronous interaction capabilities, cognitive parallelism, or any form of learned or engineered synchronization relevant to full-duplex spoken language models. The keywords such as \"spoken language model(s)\" or equivalents do not appear in the title or abstract, and the primary focus appears to be on acoustic segmentation rather than end-to-end speech language model training, inference, or evaluation.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on word boundary detection as a preprocessing step in speech processing, without explicitly addressing spoken language models (SLMs), speech language modeling, or full-duplex interactive spoken language modeling, nor discussing model architectures or evaluation in those terms. It also lacks mention of synchronous interaction capabilities, cognitive parallelism, or any form of learned or engineered synchronization relevant to full-duplex spoken language models. The keywords such as \"spoken language model(s)\" or equivalents do not appear in the title or abstract, and the primary focus appears to be on acoustic segmentation rather than end-to-end speech language model training, inference, or evaluation.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Pitch-and-Spectrum-Aware Singing Quality Assessment with Bias Correction and Model Fusion",
    "abstract": "We participated in track 2 of the VoiceMOS Challenge 2024, which aimed to predict the mean opinion score (MOS) of singing samples. Our submission secured the first place among all participating teams, excluding the official baseline. In this paper, we further improve our submission and propose a novel Pitch-and-Spectrum-aware Singing Quality Assessment (PS-SQA) method. The PS-SQA is designed based on the self-supervised-learning (SSL) MOS predictor, incorporating singing pitch and spectral information, which are extracted using pitch histogram and non-quantized neural codec, respectively. Additionally, the PS-SQA introduces a bias correction strategy to address prediction biases caused by low-resource training samples, and employs model fusion technology to further enhance prediction accuracy. Experimental results confirm that our proposed PS-SQA significantly outperforms all competing systems across all system-level metrics, confirming its strong sing quality assessment capabilities.",
    "metadata": {
      "arxiv_id": "2411.11123",
      "title": "Pitch-and-Spectrum-Aware Singing Quality Assessment with Bias Correction and Model Fusion",
      "summary": "We participated in track 2 of the VoiceMOS Challenge 2024, which aimed to predict the mean opinion score (MOS) of singing samples. Our submission secured the first place among all participating teams, excluding the official baseline. In this paper, we further improve our submission and propose a novel Pitch-and-Spectrum-aware Singing Quality Assessment (PS-SQA) method. The PS-SQA is designed based on the self-supervised-learning (SSL) MOS predictor, incorporating singing pitch and spectral information, which are extracted using pitch histogram and non-quantized neural codec, respectively. Additionally, the PS-SQA introduces a bias correction strategy to address prediction biases caused by low-resource training samples, and employs model fusion technology to further enhance prediction accuracy. Experimental results confirm that our proposed PS-SQA significantly outperforms all competing systems across all system-level metrics, confirming its strong sing quality assessment capabilities.",
      "authors": [
        "Yu-Fei Shi",
        "Yang Ai",
        "Ye-Xin Lu",
        "Hui-Peng Du",
        "Zhen-Hua Ling"
      ],
      "published": "2024-11-17T16:53:39Z",
      "updated": "2024-12-23T12:42:03Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.11123v3",
      "landing_url": "https://arxiv.org/abs/2411.11123v3",
      "doi": "https://doi.org/10.48550/arXiv.2411.11123"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on predicting MOS for singing audio (VoiceMOS) and does not center on Spoken Language Models or FD-SLMs, nor on interactive dialogue/synchronization aspects, thus it fails the core inclusion criteria and aligns with exclusion criteria for lacking speech-language modeling focus.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on predicting MOS for singing audio (VoiceMOS) and does not center on Spoken Language Models or FD-SLMs, nor on interactive dialogue/synchronization aspects, thus it fails the core inclusion criteria and aligns with exclusion criteria for lacking speech-language modeling focus.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on singing quality assessment using self-supervised learning and model fusion techniques, but it neither discusses spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs) nor addresses end-to-end speech language modeling, real-time interactive behavior, or synchronization aspects required by the inclusion criteria. The task relates to quality scoring of singing voice samples rather than modeling or interactive dialogue behaviors, thus it falls outside the defined research scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on singing quality assessment using self-supervised learning and model fusion techniques, but it neither discusses spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs) nor addresses end-to-end speech language modeling, real-time interactive behavior, or synchronization aspects required by the inclusion criteria. The task relates to quality scoring of singing voice samples rather than modeling or interactive dialogue behaviors, thus it falls outside the defined research scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "An Efficient Multicast Addressing Encoding Scheme for Multi-Core Neuromorphic Processors",
    "abstract": "Multi-core neuromorphic processors are becoming increasingly significant due to their energy-efficient local computing and scalable modular architecture, particularly for event-based processing applications. However, minimizing the cost of inter-core communication, which accounts for the majority of energy usage, remains a challenging issue. Beyond optimizing circuit design at lower abstraction levels, an efficient multicast addressing scheme is crucial. We propose a hierarchical bit string encoding scheme that largely expands the addressing capability of state-of-the-art symbol-based schemes for the same number of routing bits. When put at work with a real neuromorphic task, this hierarchical bit string encoding achieves a reduction in area cost by approximately 29% and decreases energy consumption by about 50%.",
    "metadata": {
      "arxiv_id": "2411.11545",
      "title": "An Efficient Multicast Addressing Encoding Scheme for Multi-Core Neuromorphic Processors",
      "summary": "Multi-core neuromorphic processors are becoming increasingly significant due to their energy-efficient local computing and scalable modular architecture, particularly for event-based processing applications. However, minimizing the cost of inter-core communication, which accounts for the majority of energy usage, remains a challenging issue. Beyond optimizing circuit design at lower abstraction levels, an efficient multicast addressing scheme is crucial. We propose a hierarchical bit string encoding scheme that largely expands the addressing capability of state-of-the-art symbol-based schemes for the same number of routing bits. When put at work with a real neuromorphic task, this hierarchical bit string encoding achieves a reduction in area cost by approximately 29% and decreases energy consumption by about 50%.",
      "authors": [
        "Zhe Su",
        "Aron Bencsik",
        "Giacomo Indiveri",
        "Davide Bertozzi"
      ],
      "published": "2024-11-18T13:04:38Z",
      "updated": "2024-11-18T13:04:38Z",
      "categories": [
        "cs.AR",
        "cs.NE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.11545v1",
      "landing_url": "https://arxiv.org/abs/2411.11545v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.11545"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "該論文聚焦於多核心神經形態處理器的位址編碼與能源效率，與 Spoken Language Models/語音語言建模無關，且未涉及語音對話互動或語音建模評估，因此不符合包含標準。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "該論文聚焦於多核心神經形態處理器的位址編碼與能源效率，與 Spoken Language Models/語音語言建模無關，且未涉及語音對話互動或語音建模評估，因此不符合包含標準。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on hardware and communication efficiency in multi-core neuromorphic processors without discussing Spoken Language Models, speech language modeling, or full-duplex spoken language interactions as required by the inclusion criteria. It also lacks any mention of speech-based model architectures, training, evaluation related to spoken language, or dialogue behavior arbitration, indicating it falls outside the defined research scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on hardware and communication efficiency in multi-core neuromorphic processors without discussing Spoken Language Models, speech language modeling, or full-duplex spoken language interactions as required by the inclusion criteria. It also lacks any mention of speech-based model architectures, training, evaluation related to spoken language, or dialogue behavior arbitration, indicating it falls outside the defined research scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SMT-Layout: A MaxSMT-based Approach Supporting Real-time Interaction of Real-world GUI Layout",
    "abstract": "Leveraging the flexible expressive ability of (Max)SMT and the powerful solving ability of SMT solvers, we propose a novel layout model named SMT-Layout. SMT-Layout is the first constraint-based layout model that can support real-time interaction for real-world GUI layout adapting to various screen sizes with only one specification. Previous works neglect the hierarchy information among widgets and thus cannot exploit the reasoning ability of solvers. For the first time, we introduce Boolean variables to encode the hierarchy relationship, boosting the reasoning ability of SMT solvers. The workflow is divided into two stages. At the development end, two novel preprocessing methods are proposed to simplify constraints and extract useful information in advance, easing the solving burden. After deploying constraints to the terminal end, SMT solvers are applied to solve constraints incrementally. Besides mainstream SMT solvers, a local search solver is customized to this scenario. Experiments show that SMT-Layout can support millisecond-level interaction for real-world layouts, even on devices with low computing power and rigorous memory limitations.",
    "metadata": {
      "arxiv_id": "2411.12271",
      "title": "SMT-Layout: A MaxSMT-based Approach Supporting Real-time Interaction of Real-world GUI Layout",
      "summary": "Leveraging the flexible expressive ability of (Max)SMT and the powerful solving ability of SMT solvers, we propose a novel layout model named SMT-Layout. SMT-Layout is the first constraint-based layout model that can support real-time interaction for real-world GUI layout adapting to various screen sizes with only one specification. Previous works neglect the hierarchy information among widgets and thus cannot exploit the reasoning ability of solvers. For the first time, we introduce Boolean variables to encode the hierarchy relationship, boosting the reasoning ability of SMT solvers. The workflow is divided into two stages. At the development end, two novel preprocessing methods are proposed to simplify constraints and extract useful information in advance, easing the solving burden. After deploying constraints to the terminal end, SMT solvers are applied to solve constraints incrementally. Besides mainstream SMT solvers, a local search solver is customized to this scenario. Experiments show that SMT-Layout can support millisecond-level interaction for real-world layouts, even on devices with low computing power and rigorous memory limitations.",
      "authors": [
        "Bohan Li",
        "Dawei Li",
        "Ming Fu",
        "Shaowei Cai"
      ],
      "published": "2024-11-19T06:48:53Z",
      "updated": "2024-11-19T06:48:53Z",
      "categories": [
        "cs.LO",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.12271v1",
      "landing_url": "https://arxiv.org/abs/2411.12271v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.12271"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article SMT-Layout focuses on MaxSMT-based GUI layout and real-time interaction; it does not address Spoken Language Models or speech-language modeling, thus does not meet the inclusion criteria and clearly aligns with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article SMT-Layout focuses on MaxSMT-based GUI layout and real-time interaction; it does not address Spoken Language Models or speech-language modeling, thus does not meet the inclusion criteria and clearly aligns with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a MaxSMT-based approach for GUI layout interaction and do not pertain to spoken language models, speech language modeling, or real-time spoken interactive capabilities like full-duplex spoken language models as defined by the inclusion criteria. They do not discuss speech or voice as the object of modeling or inference, nor do they mention relevant evaluation or synchronization aspects tied to spoken language models. Therefore, it does not meet the inclusion criteria and falls under exclusion based on topic mismatch.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a MaxSMT-based approach for GUI layout interaction and do not pertain to spoken language models, speech language modeling, or real-time spoken interactive capabilities like full-duplex spoken language models as defined by the inclusion criteria. They do not discuss speech or voice as the object of modeling or inference, nor do they mention relevant evaluation or synchronization aspects tied to spoken language models. Therefore, it does not meet the inclusion criteria and falls under exclusion based on topic mismatch.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Leveraging Virtual Reality and AI Tutoring for Language Learning: A Case Study of a Virtual Campus Environment with OpenAI GPT Integration with Unity 3D",
    "abstract": "This paper presents a new approach to multiple language learning, with Hindi the language to be learnt in our case, by using the integration of virtual reality environments and AI enabled tutoring systems using OpenAIs GPT api calls. We have developed a scenario which has a virtual campus environment using Unity which focuses on a detailed representation of our universitys buildings 11th floor, where most of the cultural and technological activities take place. Within this virtual environment that we have created, we have an AI tutor powered by OpenAI's GPT model which was called using an api which moves around with the user. This provided language learning support in Hindi, as GPT is able to take care of language translation. Our approach mainly involves utilising speech to text, text to text conversion and text to speech capabilities to facilitate real time interaction between users and the AI tutor in the presence of internet. This research demonstrates the use of combining VR technology with AI tutoring for immersive language learning experiences and provides interaction.",
    "metadata": {
      "arxiv_id": "2411.12619",
      "title": "Leveraging Virtual Reality and AI Tutoring for Language Learning: A Case Study of a Virtual Campus Environment with OpenAI GPT Integration with Unity 3D",
      "summary": "This paper presents a new approach to multiple language learning, with Hindi the language to be learnt in our case, by using the integration of virtual reality environments and AI enabled tutoring systems using OpenAIs GPT api calls. We have developed a scenario which has a virtual campus environment using Unity which focuses on a detailed representation of our universitys buildings 11th floor, where most of the cultural and technological activities take place. Within this virtual environment that we have created, we have an AI tutor powered by OpenAI's GPT model which was called using an api which moves around with the user. This provided language learning support in Hindi, as GPT is able to take care of language translation. Our approach mainly involves utilising speech to text, text to text conversion and text to speech capabilities to facilitate real time interaction between users and the AI tutor in the presence of internet. This research demonstrates the use of combining VR technology with AI tutoring for immersive language learning experiences and provides interaction.",
      "authors": [
        "Adithya TG",
        "Abhinavaram N",
        "Gowri Srinivasa"
      ],
      "published": "2024-11-19T16:26:19Z",
      "updated": "2024-11-19T16:26:19Z",
      "categories": [
        "cs.HC",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.12619v1",
      "landing_url": "https://arxiv.org/abs/2411.12619v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.12619"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item uses speech components but does not treat Spoken Language Models as the core research object; it centers on VR-based language tutoring with GPT, lacking explicit SLM/FD-SLM architecture, training, or evaluation discussions, and does not emphasize speech-centric model development or evaluation beyond basic ASR/TTS interfaces.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item uses speech components but does not treat Spoken Language Models as the core research object; it centers on VR-based language tutoring with GPT, lacking explicit SLM/FD-SLM architecture, training, or evaluation discussions, and does not emphasize speech-centric model development or evaluation beyond basic ASR/TTS interfaces.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on using virtual reality and an AI tutor leveraging OpenAI's GPT model for language learning, primarily utilizing speech-to-text and text-to-speech capabilities for interaction in Hindi learning. However, it does not mention the core aspects required by the inclusion criteria such as Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), speech-level language modeling, learned or engineered synchronization, or advanced real-time interactive features like low-latency or speech overlap management. The study appears to treat speech mainly as an input/output modality for a GPT-based tutoring system, with no discussion on speech modeling architecture, training, or evaluation, and no mention of multi-turn simultaneous spoken interaction or related metrics. Therefore, it does not meet the inclusion criteria and overlaps somewhat with the exclusion criteria regarding no discussion of speech language modeling or synchronous dialogue behavior.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on using virtual reality and an AI tutor leveraging OpenAI's GPT model for language learning, primarily utilizing speech-to-text and text-to-speech capabilities for interaction in Hindi learning. However, it does not mention the core aspects required by the inclusion criteria such as Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), speech-level language modeling, learned or engineered synchronization, or advanced real-time interactive features like low-latency or speech overlap management. The study appears to treat speech mainly as an input/output modality for a GPT-based tutoring system, with no discussion on speech modeling architecture, training, or evaluation, and no mention of multi-turn simultaneous spoken interaction or related metrics. Therefore, it does not meet the inclusion criteria and overlaps somewhat with the exclusion criteria regarding no discussion of speech language modeling or synchronous dialogue behavior.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ClickTrack: Towards Real-time Interactive Single Object Tracking",
    "abstract": "Single object tracking(SOT) relies on precise object bounding box initialization. In this paper, we reconsidered the deficiencies in the current approaches to initializing single object trackers and propose a new paradigm for single object tracking algorithms, ClickTrack, a new paradigm using clicking interaction for real-time scenarios. Moreover, click as an input type inherently lack hierarchical information. To address ambiguity in certain special scenarios, we designed the Guided Click Refiner(GCR), which accepts point and optional textual information as inputs, transforming the point into the bounding box expected by the operator. The bounding box will be used as input of single object trackers. Experiments on LaSOT and GOT-10k benchmarks show that tracker combined with GCR achieves stable performance in real-time interactive scenarios. Furthermore, we explored the integration of GCR into the Segment Anything model(SAM), significantly reducing ambiguity issues when SAM receives point inputs.",
    "metadata": {
      "arxiv_id": "2411.13183",
      "title": "ClickTrack: Towards Real-time Interactive Single Object Tracking",
      "summary": "Single object tracking(SOT) relies on precise object bounding box initialization. In this paper, we reconsidered the deficiencies in the current approaches to initializing single object trackers and propose a new paradigm for single object tracking algorithms, ClickTrack, a new paradigm using clicking interaction for real-time scenarios. Moreover, click as an input type inherently lack hierarchical information. To address ambiguity in certain special scenarios, we designed the Guided Click Refiner(GCR), which accepts point and optional textual information as inputs, transforming the point into the bounding box expected by the operator. The bounding box will be used as input of single object trackers. Experiments on LaSOT and GOT-10k benchmarks show that tracker combined with GCR achieves stable performance in real-time interactive scenarios. Furthermore, we explored the integration of GCR into the Segment Anything model(SAM), significantly reducing ambiguity issues when SAM receives point inputs.",
      "authors": [
        "Kuiran Wang",
        "Xuehui Yu",
        "Wenwen Yu",
        "Guorong Li",
        "Xiangyuan Lan",
        "Qixiang Ye",
        "Jianbin Jiao",
        "Zhenjun Han"
      ],
      "published": "2024-11-20T10:30:33Z",
      "updated": "2024-11-24T14:35:40Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.13183v2",
      "landing_url": "https://arxiv.org/abs/2411.13183v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.13183"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns computer vision SOT with click-based interaction, not Spoken Language Models or speech-language modeling; it does not meet the inclusion criteria and does not involve audio/speech-based modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns computer vision SOT with click-based interaction, not Spoken Language Models or speech-language modeling; it does not meet the inclusion criteria and does not involve audio/speech-based modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on interactive single object tracking using clicking interaction and bounding box refinement techniques, without any mention or discussion of spoken language models, speech-level language modeling, or full-duplex spoken language interaction; thus, it does not meet the inclusion criteria centered on speech-language modeling research nor addresses the specific themes such as low-latency full-duplex models or related behavioral arbitration in speech interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on interactive single object tracking using clicking interaction and bounding box refinement techniques, without any mention or discussion of spoken language models, speech-level language modeling, or full-duplex spoken language interaction; thus, it does not meet the inclusion criteria centered on speech-language modeling research nor addresses the specific themes such as low-latency full-duplex models or related behavioral arbitration in speech interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SpikEmo: Enhancing Emotion Recognition With Spiking Temporal Dynamics in Conversations",
    "abstract": "In affective computing, the task of Emotion Recognition in Conversations (ERC) has emerged as a focal area of research. The primary objective of this task is to predict emotional states within conversations by analyzing multimodal data including text, audio, and video. While existing studies have progressed in extracting and fusing representations from multimodal data, they often overlook the temporal dynamics in the data during conversations. To address this challenge, we have developed the SpikEmo framework, which is based on spiking neurons and employs a Semantic & Dynamic Two-stage Modeling approach to more precisely capture the complex temporal features of multimodal emotional data. Additionally, to tackle the class imbalance and emotional semantic similarity problems in the ERC tasks, we have devised an innovative combination of loss functions that significantly enhances the model's performance when dealing with ERC data characterized by long-tail distributions. Extensive experiments conducted on multiple ERC benchmark datasets demonstrate that SpikEmo significantly outperforms existing state-of-the-art methods in ERC tasks. Our code is available at https://github.com/Yu-xm/SpikEmo.git.",
    "metadata": {
      "arxiv_id": "2411.13917",
      "title": "SpikEmo: Enhancing Emotion Recognition With Spiking Temporal Dynamics in Conversations",
      "summary": "In affective computing, the task of Emotion Recognition in Conversations (ERC) has emerged as a focal area of research. The primary objective of this task is to predict emotional states within conversations by analyzing multimodal data including text, audio, and video. While existing studies have progressed in extracting and fusing representations from multimodal data, they often overlook the temporal dynamics in the data during conversations. To address this challenge, we have developed the SpikEmo framework, which is based on spiking neurons and employs a Semantic & Dynamic Two-stage Modeling approach to more precisely capture the complex temporal features of multimodal emotional data. Additionally, to tackle the class imbalance and emotional semantic similarity problems in the ERC tasks, we have devised an innovative combination of loss functions that significantly enhances the model's performance when dealing with ERC data characterized by long-tail distributions. Extensive experiments conducted on multiple ERC benchmark datasets demonstrate that SpikEmo significantly outperforms existing state-of-the-art methods in ERC tasks. Our code is available at https://github.com/Yu-xm/SpikEmo.git.",
      "authors": [
        "Xiaomin Yu",
        "Feiyang Wang",
        "Ziyue Qiao"
      ],
      "published": "2024-11-21T08:07:26Z",
      "updated": "2024-11-21T08:07:26Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.13917v1",
      "landing_url": "https://arxiv.org/abs/2411.13917v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.13917"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on emotion recognition in conversations using multimodal data and spiking temporal dynamics, but it does not treat speech as the core modeling object nor discuss Spoken Language Models/FD-SLM architectures, training, or evaluation, thus failing the inclusion criteria and meeting an exclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on emotion recognition in conversations using multimodal data and spiking temporal dynamics, but it does not treat speech as the core modeling object nor discuss Spoken Language Models/FD-SLM architectures, training, or evaluation, thus failing the inclusion criteria and meeting an exclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Emotion Recognition in Conversations using multimodal data (text, audio, video) but does not center on Spoken Language Models or Full-Duplex Spoken Language Models with speech as the core modeling object; it also lacks explicit discussion of spoken language model architectures or synchronization mechanisms relating to full-duplex interaction, low latency, or behavior arbitration. Thus, it does not meet the inclusion criteria nor fully address the exclusion criteria regarding synchronous dialog modeling or speech-language model focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Emotion Recognition in Conversations using multimodal data (text, audio, video) but does not center on Spoken Language Models or Full-Duplex Spoken Language Models with speech as the core modeling object; it also lacks explicit discussion of spoken language model architectures or synchronization mechanisms relating to full-duplex interaction, low latency, or behavior arbitration. Thus, it does not meet the inclusion criteria nor fully address the exclusion criteria regarding synchronous dialog modeling or speech-language model focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ML-SPEAK: A Theory-Guided Machine Learning Method for Studying and Predicting Conversational Turn-taking Patterns",
    "abstract": "Predicting team dynamics from personality traits remains a fundamental challenge for the psychological sciences and team-based organizations. Understanding how team composition generates team processes can significantly advance team-based research along with providing practical guidelines for team staffing and training. Although the Input-Process-Output (IPO) model has been useful for studying these connections, the complex nature of team member interactions demands a more dynamic approach. We develop a computational model of conversational turn-taking within self-organized teams that can provide insight into the relationships between team member personality traits and team communication dynamics. We focus on turn-taking patterns between team members, independent of content, which can significantly influence team emergent states and outcomes while being objectively measurable and quantifiable. As our model is trained on conversational data from teams of given trait compositions, it can learn the relationships between individual traits and speaking behaviors and predict group-wide patterns of communication based on team trait composition alone. We first evaluate the performance of our model using simulated data and then apply it to real-world data collected from self-organized student teams. In comparison to baselines, our model is more accurate at predicting speaking turn sequences and can reveal new relationships between team member traits and their communication patterns. Our approach offers a more data-driven and dynamic understanding of team processes. By bridging the gap between individual personality traits and team communication patterns, our model has the potential to inform theories of team processes and provide powerful insights into optimizing team staffing and training.",
    "metadata": {
      "arxiv_id": "2411.15405",
      "title": "ML-SPEAK: A Theory-Guided Machine Learning Method for Studying and Predicting Conversational Turn-taking Patterns",
      "summary": "Predicting team dynamics from personality traits remains a fundamental challenge for the psychological sciences and team-based organizations. Understanding how team composition generates team processes can significantly advance team-based research along with providing practical guidelines for team staffing and training. Although the Input-Process-Output (IPO) model has been useful for studying these connections, the complex nature of team member interactions demands a more dynamic approach. We develop a computational model of conversational turn-taking within self-organized teams that can provide insight into the relationships between team member personality traits and team communication dynamics. We focus on turn-taking patterns between team members, independent of content, which can significantly influence team emergent states and outcomes while being objectively measurable and quantifiable. As our model is trained on conversational data from teams of given trait compositions, it can learn the relationships between individual traits and speaking behaviors and predict group-wide patterns of communication based on team trait composition alone. We first evaluate the performance of our model using simulated data and then apply it to real-world data collected from self-organized student teams. In comparison to baselines, our model is more accurate at predicting speaking turn sequences and can reveal new relationships between team member traits and their communication patterns. Our approach offers a more data-driven and dynamic understanding of team processes. By bridging the gap between individual personality traits and team communication patterns, our model has the potential to inform theories of team processes and provide powerful insights into optimizing team staffing and training.",
      "authors": [
        "Lisa R. O'Bryan",
        "Madeline Navarro",
        "Juan Segundo Hevia",
        "Santiago Segarra"
      ],
      "published": "2024-11-23T01:27:01Z",
      "updated": "2024-11-23T01:27:01Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.15405v1",
      "landing_url": "https://arxiv.org/abs/2411.15405v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.15405"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: This item does not center on Spoken Language Models (SLMs) or Speech-Language Modeling; it focuses on psychological turn-taking dynamics and personality traits with no explicit model architecture, training, or evaluation of SLMs or audio signals. Therefore it does not meet the inclusion criteria; conclusion: exclude.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: This item does not center on Spoken Language Models (SLMs) or Speech-Language Modeling; it focuses on psychological turn-taking dynamics and personality traits with no explicit model architecture, training, or evaluation of SLMs or audio signals. Therefore it does not meet the inclusion criteria; conclusion: exclude.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a machine learning model to analyze conversational turn-taking patterns based on team members' personality traits, but it primarily models turn-taking behavior at a higher behavioral or social level rather than focusing on speech as the primary modeling object or using spoken language models based on acoustic or speech token representations. It focuses on turn sequences, not on spoken language modeling, speech token prediction, or full-duplex speech interaction capabilities like cognitive parallelism or low-latency speech overlap handling. The abstract lacks mention of spoken language models, speech language models, or speech-level language modeling, nor does it discuss specific model architectures, training, and evaluation of spoken language models as defined in the inclusion criteria. There is no evidence of involvement of speech modality modeling beyond turn-taking patterns as behavioral signals, so the core research is not on spoken language modeling. Therefore, this study does not meet the inclusion criteria and aligns more with sociotechnical or psychological modeling rather than computational spoken language modeling as required.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes a machine learning model to analyze conversational turn-taking patterns based on team members' personality traits, but it primarily models turn-taking behavior at a higher behavioral or social level rather than focusing on speech as the primary modeling object or using spoken language models based on acoustic or speech token representations. It focuses on turn sequences, not on spoken language modeling, speech token prediction, or full-duplex speech interaction capabilities like cognitive parallelism or low-latency speech overlap handling. The abstract lacks mention of spoken language models, speech language models, or speech-level language modeling, nor does it discuss specific model architectures, training, and evaluation of spoken language models as defined in the inclusion criteria. There is no evidence of involvement of speech modality modeling beyond turn-taking patterns as behavioral signals, so the core research is not on spoken language modeling. Therefore, this study does not meet the inclusion criteria and aligns more with sociotechnical or psychological modeling rather than computational spoken language modeling as required.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and Generation",
    "abstract": "Full-duplex multimodal large language models (LLMs) provide a unified framework for addressing diverse speech understanding and generation tasks, enabling more natural and seamless human-machine conversations. Unlike traditional modularised conversational AI systems, which separate speech recognition, understanding, and text-to-speech generation into distinct components, multimodal LLMs operate as single end-to-end models. This streamlined design eliminates error propagation across components and fully leverages the rich non-verbal information embedded in input speech signals. We introduce SALMONN-omni, a codec-free, full-duplex speech understanding and generation model capable of simultaneously listening to its own generated speech and background sounds while speaking. To support this capability, we propose a novel duplex spoken dialogue framework incorporating a ``thinking'' mechanism that facilitates asynchronous text and speech generation relying on embeddings instead of codecs (quantized speech and audio tokens). Experimental results demonstrate SALMONN-omni's versatility across a broad range of streaming speech tasks, including speech recognition, speech enhancement, and spoken question answering. Additionally, SALMONN-omni excels at managing turn-taking, barge-in, and echo cancellation scenarios, establishing its potential as a robust prototype for full-duplex conversational AI systems. To the best of our knowledge, SALMONN-omni is the first codec-free model of its kind. A full technical report along with model checkpoints will be released soon.",
    "metadata": {
      "arxiv_id": "2411.18138",
      "title": "SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and Generation",
      "summary": "Full-duplex multimodal large language models (LLMs) provide a unified framework for addressing diverse speech understanding and generation tasks, enabling more natural and seamless human-machine conversations. Unlike traditional modularised conversational AI systems, which separate speech recognition, understanding, and text-to-speech generation into distinct components, multimodal LLMs operate as single end-to-end models. This streamlined design eliminates error propagation across components and fully leverages the rich non-verbal information embedded in input speech signals. We introduce SALMONN-omni, a codec-free, full-duplex speech understanding and generation model capable of simultaneously listening to its own generated speech and background sounds while speaking. To support this capability, we propose a novel duplex spoken dialogue framework incorporating a ``thinking'' mechanism that facilitates asynchronous text and speech generation relying on embeddings instead of codecs (quantized speech and audio tokens). Experimental results demonstrate SALMONN-omni's versatility across a broad range of streaming speech tasks, including speech recognition, speech enhancement, and spoken question answering. Additionally, SALMONN-omni excels at managing turn-taking, barge-in, and echo cancellation scenarios, establishing its potential as a robust prototype for full-duplex conversational AI systems. To the best of our knowledge, SALMONN-omni is the first codec-free model of its kind. A full technical report along with model checkpoints will be released soon.",
      "authors": [
        "Wenyi Yu",
        "Siyin Wang",
        "Xiaoyu Yang",
        "Xianzhao Chen",
        "Xiaohai Tian",
        "Jun Zhang",
        "Guangzhi Sun",
        "Lu Lu",
        "Yuxuan Wang",
        "Chao Zhang"
      ],
      "published": "2024-11-27T08:38:57Z",
      "updated": "2024-11-27T08:38:57Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.18138v1",
      "landing_url": "https://arxiv.org/abs/2411.18138v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.18138"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe a codec-free, full-duplex spoken language model framework with end-to-end speech understanding and generation, covering architecture and multi-task evaluation, within 3 years, and aligning with the core focus on Spoken Language Models / Speech Language Models rather than only ASR/TTS.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe a codec-free, full-duplex spoken language model framework with end-to-end speech understanding and generation, covering architecture and multi-task evaluation, within 3 years, and aligning with the core focus on Spoken Language Models / Speech Language Models rather than only ASR/TTS.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces SALMONN-omni, a codec-free full-duplex spoken language model that simultaneously handles speech understanding and generation with mechanisms for turn-taking, barge-in, and echo cancellation, which aligns well with the inclusion criteria focusing on full-duplex spoken language models, end-to-end architectures, and multi-faceted evaluation beyond traditional speech recognition or synthesis tasks; it presents a novel duplex dialogue framework and addresses real-time, synchronous interaction challenges, thus meeting the thematic and methodological requirements specified.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study introduces SALMONN-omni, a codec-free full-duplex spoken language model that simultaneously handles speech understanding and generation with mechanisms for turn-taking, barge-in, and echo cancellation, which aligns well with the inclusion criteria focusing on full-duplex spoken language models, end-to-end architectures, and multi-faceted evaluation beyond traditional speech recognition or synthesis tasks; it presents a novel duplex dialogue framework and addresses real-time, synchronous interaction challenges, thus meeting the thematic and methodological requirements specified.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Continuous Autoregressive Models with Noise Augmentation Avoid Error Accumulation",
    "abstract": "Autoregressive models are typically applied to sequences of discrete tokens, but recent research indicates that generating sequences of continuous embeddings in an autoregressive manner is also feasible. However, such Continuous Autoregressive Models (CAMs) can suffer from a decline in generation quality over extended sequences due to error accumulation during inference. We introduce a novel method to address this issue by injecting random noise into the input embeddings during training. This procedure makes the model robust against varying error levels at inference. We further reduce error accumulation through an inference procedure that introduces low-level noise. Experiments on musical audio generation show that CAM substantially outperforms existing autoregressive and non-autoregressive approaches while preserving audio quality over extended sequences. This work paves the way for generating continuous embeddings in a purely autoregressive setting, opening new possibilities for real-time and interactive generative applications.",
    "metadata": {
      "arxiv_id": "2411.18447",
      "title": "Continuous Autoregressive Models with Noise Augmentation Avoid Error Accumulation",
      "summary": "Autoregressive models are typically applied to sequences of discrete tokens, but recent research indicates that generating sequences of continuous embeddings in an autoregressive manner is also feasible. However, such Continuous Autoregressive Models (CAMs) can suffer from a decline in generation quality over extended sequences due to error accumulation during inference. We introduce a novel method to address this issue by injecting random noise into the input embeddings during training. This procedure makes the model robust against varying error levels at inference. We further reduce error accumulation through an inference procedure that introduces low-level noise. Experiments on musical audio generation show that CAM substantially outperforms existing autoregressive and non-autoregressive approaches while preserving audio quality over extended sequences. This work paves the way for generating continuous embeddings in a purely autoregressive setting, opening new possibilities for real-time and interactive generative applications.",
      "authors": [
        "Marco Pasini",
        "Javier Nistal",
        "Stefan Lattner",
        "George Fazekas"
      ],
      "published": "2024-11-27T15:38:20Z",
      "updated": "2024-11-27T15:38:20Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.18447v1",
      "landing_url": "https://arxiv.org/abs/2411.18447v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.18447"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on continuous autoregressive models for audio generation with noise augmentation, not on Spoken Language Models or speech-centric dialogue/interactions, and lacks emphasis on speech as a core modeling objective or full-duplex speech exchange aspects, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on continuous autoregressive models for audio generation with noise augmentation, not on Spoken Language Models or speech-centric dialogue/interactions, and lacks emphasis on speech as a core modeling objective or full-duplex speech exchange aspects, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on continuous autoregressive models for sequence generation, applied to musical audio generation, without mentioning spoken language models, full-duplex interaction, speech-language modeling, or related synchronization or interaction aspects specified by the inclusion criteria. It does not address core spoken language modeling or interactive speech turn-taking, thus it does not meet the inclusion criteria and falls within exclusion criteria related to lacking spoken language interaction focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on continuous autoregressive models for sequence generation, applied to musical audio generation, without mentioning spoken language models, full-duplex interaction, speech-language modeling, or related synchronization or interaction aspects specified by the inclusion criteria. It does not address core spoken language modeling or interactive speech turn-taking, thus it does not meet the inclusion criteria and falls within exclusion criteria related to lacking spoken language interaction focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos",
    "abstract": "We introduce HOT3D, a publicly available dataset for egocentric hand and object tracking in 3D. The dataset offers over 833 minutes (3.7M+ images) of recordings that feature 19 subjects interacting with 33 diverse rigid objects. In addition to simple pick-up, observe, and put-down actions, the subjects perform actions typical for a kitchen, office, and living room environment. The recordings include multiple synchronized data streams containing egocentric multi-view RGB/monochrome images, eye gaze signal, scene point clouds, and 3D poses of cameras, hands, and objects. The dataset is recorded with two headsets from Meta: Project Aria, which is a research prototype of AI glasses, and Quest 3, a virtual-reality headset that has shipped millions of units. Ground-truth poses were obtained by a motion-capture system using small optical markers attached to hands and objects. Hand annotations are provided in the UmeTrack and MANO formats, and objects are represented by 3D meshes with PBR materials obtained by an in-house scanner. In our experiments, we demonstrate the effectiveness of multi-view egocentric data for three popular tasks: 3D hand tracking, model-based 6DoF object pose estimation, and 3D lifting of unknown in-hand objects. The evaluated multi-view methods, whose benchmarking is uniquely enabled by HOT3D, significantly outperform their single-view counterparts.",
    "metadata": {
      "arxiv_id": "2411.19167",
      "title": "HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos",
      "summary": "We introduce HOT3D, a publicly available dataset for egocentric hand and object tracking in 3D. The dataset offers over 833 minutes (3.7M+ images) of recordings that feature 19 subjects interacting with 33 diverse rigid objects. In addition to simple pick-up, observe, and put-down actions, the subjects perform actions typical for a kitchen, office, and living room environment. The recordings include multiple synchronized data streams containing egocentric multi-view RGB/monochrome images, eye gaze signal, scene point clouds, and 3D poses of cameras, hands, and objects. The dataset is recorded with two headsets from Meta: Project Aria, which is a research prototype of AI glasses, and Quest 3, a virtual-reality headset that has shipped millions of units. Ground-truth poses were obtained by a motion-capture system using small optical markers attached to hands and objects. Hand annotations are provided in the UmeTrack and MANO formats, and objects are represented by 3D meshes with PBR materials obtained by an in-house scanner. In our experiments, we demonstrate the effectiveness of multi-view egocentric data for three popular tasks: 3D hand tracking, model-based 6DoF object pose estimation, and 3D lifting of unknown in-hand objects. The evaluated multi-view methods, whose benchmarking is uniquely enabled by HOT3D, significantly outperform their single-view counterparts.",
      "authors": [
        "Prithviraj Banerjee",
        "Sindi Shkodrani",
        "Pierre Moulon",
        "Shreyas Hampali",
        "Shangchen Han",
        "Fan Zhang",
        "Linguang Zhang",
        "Jade Fountain",
        "Edward Miller",
        "Selen Basol",
        "Richard Newcombe",
        "Robert Wang",
        "Jakob Julian Engel",
        "Tomas Hodan"
      ],
      "published": "2024-11-28T14:09:42Z",
      "updated": "2025-04-30T13:32:06Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.19167v2",
      "landing_url": "https://arxiv.org/abs/2411.19167v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.19167"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is a computer-vision dataset paper on egocentric hand/object tracking in 3D, with no focus on Spoken Language Models or speech-language modeling, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is a computer-vision dataset paper on egocentric hand/object tracking in 3D, with no focus on Spoken Language Models or speech-language modeling, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a dataset and methods for 3D hand and object tracking from egocentric multi-view videos, which pertains to computer vision and human-computer interaction rather than spoken language models or speech-based modeling. There is no mention of spoken language modeling, full-duplex spoken language interaction, speech language models, or relevant topics in the inclusion criteria. Therefore, this study does not meet the inclusion criteria centered on spoken language models and related speech interaction capabilities.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a dataset and methods for 3D hand and object tracking from egocentric multi-view videos, which pertains to computer vision and human-computer interaction rather than spoken language models or speech-based modeling. There is no mention of spoken language modeling, full-duplex spoken language interaction, speech language models, or relevant topics in the inclusion criteria. Therefore, this study does not meet the inclusion criteria centered on spoken language models and related speech interaction capabilities.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding",
    "abstract": "Human motion is inherently continuous and dynamic, posing significant challenges for generative models. While discrete generation methods are widely used, they suffer from limited expressiveness and frame-wise noise artifacts. In contrast, continuous approaches produce smoother, more natural motion but often struggle to adhere to conditioning signals due to high-dimensional complexity and limited training data. To resolve this 'discord' between discrete and continuous representations we introduce DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding, a novel method that leverages rectified flow to decode discrete motion tokens in the continuous, raw motion space. Our core idea is to frame token decoding as a conditional generation task, ensuring that DisCoRD captures fine-grained dynamics and achieves smoother, more natural motions. Compatible with any discrete-based framework, our method enhances naturalness without compromising faithfulness to the conditioning signals on diverse settings. Extensive evaluations demonstrate that DisCoRD achieves state-of-the-art performance, with FID of 0.032 on HumanML3D and 0.169 on KIT-ML. These results establish DisCoRD as a robust solution for bridging the divide between discrete efficiency and continuous realism. Project website: https://whwjdqls.github.io/discord-motion/",
    "metadata": {
      "arxiv_id": "2411.19527",
      "title": "DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding",
      "summary": "Human motion is inherently continuous and dynamic, posing significant challenges for generative models. While discrete generation methods are widely used, they suffer from limited expressiveness and frame-wise noise artifacts. In contrast, continuous approaches produce smoother, more natural motion but often struggle to adhere to conditioning signals due to high-dimensional complexity and limited training data. To resolve this 'discord' between discrete and continuous representations we introduce DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding, a novel method that leverages rectified flow to decode discrete motion tokens in the continuous, raw motion space. Our core idea is to frame token decoding as a conditional generation task, ensuring that DisCoRD captures fine-grained dynamics and achieves smoother, more natural motions. Compatible with any discrete-based framework, our method enhances naturalness without compromising faithfulness to the conditioning signals on diverse settings. Extensive evaluations demonstrate that DisCoRD achieves state-of-the-art performance, with FID of 0.032 on HumanML3D and 0.169 on KIT-ML. These results establish DisCoRD as a robust solution for bridging the divide between discrete efficiency and continuous realism. Project website: https://whwjdqls.github.io/discord-motion/",
      "authors": [
        "Jungbin Cho",
        "Junwan Kim",
        "Jisoo Kim",
        "Minseo Kim",
        "Mingu Kang",
        "Sungeun Hong",
        "Tae-Hyun Oh",
        "Youngjae Yu"
      ],
      "published": "2024-11-29T07:54:56Z",
      "updated": "2025-08-07T08:03:29Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.19527v4",
      "landing_url": "https://arxiv.org/abs/2411.19527v4",
      "doi": "https://doi.org/10.48550/arXiv.2411.19527"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on discrete-to-continuous motion generation for human motion and does not address Spoken Language Models or speech-language modeling, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on discrete-to-continuous motion generation for human motion and does not address Spoken Language Models or speech-language modeling, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on generative modeling of human motion using discrete and continuous representations, without any mention of spoken language models, speech language modeling, end-to-end speech architectures, or related interactive behavior such as full-duplex spoken interaction or turn-taking; it is unrelated to the inclusion criteria centered on spoken language modeling or spoken interaction systems, and does not meet any of the inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on generative modeling of human motion using discrete and continuous representations, without any mention of spoken language models, speech language modeling, end-to-end speech architectures, or related interactive behavior such as full-duplex spoken interaction or turn-taking; it is unrelated to the inclusion criteria centered on spoken language modeling or spoken interaction systems, and does not meet any of the inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Playable Game Generation",
    "abstract": "In recent years, Artificial Intelligence Generated Content (AIGC) has advanced from text-to-image generation to text-to-video and multimodal video synthesis. However, generating playable games presents significant challenges due to the stringent requirements for real-time interaction, high visual quality, and accurate simulation of game mechanics. Existing approaches often fall short, either lacking real-time capabilities or failing to accurately simulate interactive mechanics. To tackle the playability issue, we propose a novel method called \\emph{PlayGen}, which encompasses game data generation, an autoregressive DiT-based diffusion model, and a comprehensive playability-based evaluation framework. Validated on well-known 2D and 3D games, PlayGen achieves real-time interaction, ensures sufficient visual quality, and provides accurate interactive mechanics simulation. Notably, these results are sustained even after over 1000 frames of gameplay on an NVIDIA RTX 2060 GPU. Our code is publicly available: https://github.com/GreatX3/Playable-Game-Generation. Our playable demo generated by AI is: http://124.156.151.207.",
    "metadata": {
      "arxiv_id": "2412.00887",
      "title": "Playable Game Generation",
      "summary": "In recent years, Artificial Intelligence Generated Content (AIGC) has advanced from text-to-image generation to text-to-video and multimodal video synthesis. However, generating playable games presents significant challenges due to the stringent requirements for real-time interaction, high visual quality, and accurate simulation of game mechanics. Existing approaches often fall short, either lacking real-time capabilities or failing to accurately simulate interactive mechanics. To tackle the playability issue, we propose a novel method called \\emph{PlayGen}, which encompasses game data generation, an autoregressive DiT-based diffusion model, and a comprehensive playability-based evaluation framework. Validated on well-known 2D and 3D games, PlayGen achieves real-time interaction, ensures sufficient visual quality, and provides accurate interactive mechanics simulation. Notably, these results are sustained even after over 1000 frames of gameplay on an NVIDIA RTX 2060 GPU. Our code is publicly available: https://github.com/GreatX3/Playable-Game-Generation. Our playable demo generated by AI is: http://124.156.151.207.",
      "authors": [
        "Mingyu Yang",
        "Junyou Li",
        "Zhongbin Fang",
        "Sheng Chen",
        "Yangbin Yu",
        "Qiang Fu",
        "Wei Yang",
        "Deheng Ye"
      ],
      "published": "2024-12-01T16:53:02Z",
      "updated": "2024-12-01T16:53:02Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.00887v1",
      "landing_url": "https://arxiv.org/abs/2412.00887v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.00887"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on AI-generated playable game content (PlayGen) using diffusion models and real-time game playability; it does not center on Spoken Language Models or speech-language modeling as core objects, and does not discuss SLM/FD-SLM concepts, thus fails the inclusion criteria and meets at least one exclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on AI-generated playable game content (PlayGen) using diffusion models and real-time game playability; it does not center on Spoken Language Models or speech-language modeling as core objects, and does not discuss SLM/FD-SLM concepts, thus fails the inclusion criteria and meets at least one exclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on AI-generated playable games and does not address Spoken Language Models or related speech modeling concepts central to the inclusion criteria; it also lacks any discussion on speech-based modeling, architectures, or evaluations, making it irrelevant to the defined scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on AI-generated playable games and does not address Spoken Language Models or related speech modeling concepts central to the inclusion criteria; it also lacks any discussion on speech-based modeling, architectures, or evaluations, making it irrelevant to the defined scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Advancing Speech Language Models by Scaling Supervised Fine-Tuning with Over 60,000 Hours of Synthetic Speech Dialogue Data",
    "abstract": "The GPT-4o represents a significant milestone in enabling real-time interaction with large language models (LLMs) through speech, its remarkable low latency and high fluency not only capture attention but also stimulate research interest in the field. This real-time speech interaction is particularly valuable in scenarios requiring rapid feedback and immediate responses, dramatically enhancing user experience. However, there is a notable lack of research focused on real-time large speech language models, particularly for Chinese. In this work, we present KE-Omni, a seamless large speech language model built upon Ke-SpeechChat, a large-scale high-quality synthetic speech interaction dataset consisting of 7 million Chinese and English conversations, featuring 42,002 speakers, and totaling over 60,000 hours, This contributes significantly to the advancement of research and development in this field. The demos can be accessed at \\url{https://huggingface.co/spaces/KE-Team/KE-Omni}.",
    "metadata": {
      "arxiv_id": "2412.01078",
      "title": "Advancing Speech Language Models by Scaling Supervised Fine-Tuning with Over 60,000 Hours of Synthetic Speech Dialogue Data",
      "summary": "The GPT-4o represents a significant milestone in enabling real-time interaction with large language models (LLMs) through speech, its remarkable low latency and high fluency not only capture attention but also stimulate research interest in the field. This real-time speech interaction is particularly valuable in scenarios requiring rapid feedback and immediate responses, dramatically enhancing user experience. However, there is a notable lack of research focused on real-time large speech language models, particularly for Chinese. In this work, we present KE-Omni, a seamless large speech language model built upon Ke-SpeechChat, a large-scale high-quality synthetic speech interaction dataset consisting of 7 million Chinese and English conversations, featuring 42,002 speakers, and totaling over 60,000 hours, This contributes significantly to the advancement of research and development in this field. The demos can be accessed at \\url{https://huggingface.co/spaces/KE-Team/KE-Omni}.",
      "authors": [
        "Shuaijiang Zhao",
        "Tingwei Guo",
        "Bajian Xiang",
        "Tongtang Wan",
        "Qiang Niu",
        "Wei Zou",
        "Xiangang Li"
      ],
      "published": "2024-12-02T03:31:46Z",
      "updated": "2024-12-03T02:59:43Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.01078v2",
      "landing_url": "https://arxiv.org/abs/2412.01078v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.01078"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on Spoken Language Models / full-duplex spoken interaction, discusses model architecture, training, and evaluation (KE-Omni built on a large synthetic speech dialogue dataset), and addresses real-time speech with LLMs. It mentions a substantial multi-language dataset and is within the last three years, not limited to ASR/TTS or non-academic sources.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on Spoken Language Models / full-duplex spoken interaction, discusses model architecture, training, and evaluation (KE-Omni built on a large synthetic speech dialogue dataset), and addresses real-time speech with LLMs. It mentions a substantial multi-language dataset and is within the last three years, not limited to ASR/TTS or non-academic sources.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a large speech language model (KE-Omni) involving extensive synthetic speech dialogue data and aims to advance real-time interaction with speech in multiple languages including Chinese, which aligns with the Spoken Language Models (SLMs) topic. However, the abstract does not explicitly mention details on full-duplex interaction, cognitive parallelism, turn-taking mechanisms beyond half-duplex, or the evaluation frameworks related to synchronous interaction or low latency metrics such as sub-200ms delay. The model is built on a synthetic speech interaction dataset and targets real-time speech interaction but lacks explicit descriptions of engineered or learned synchronization, interruption handling, or detailed multi-dimensional evaluation. Therefore, it partly meets inclusion criteria but does not provide enough evidence to confirm all critical aspects, especially around full-duplex interaction and evaluation, making inclusion uncertain.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a large speech language model (KE-Omni) involving extensive synthetic speech dialogue data and aims to advance real-time interaction with speech in multiple languages including Chinese, which aligns with the Spoken Language Models (SLMs) topic. However, the abstract does not explicitly mention details on full-duplex interaction, cognitive parallelism, turn-taking mechanisms beyond half-duplex, or the evaluation frameworks related to synchronous interaction or low latency metrics such as sub-200ms delay. The model is built on a synthetic speech interaction dataset and targets real-time speech interaction but lacks explicit descriptions of engineered or learned synchronization, interruption handling, or detailed multi-dimensional evaluation. Therefore, it partly meets inclusion criteria but does not provide enough evidence to confirm all critical aspects, especially around full-duplex interaction and evaluation, making inclusion uncertain.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: one recommended inclusion (5) citing SLM focus and dataset; the other was uncertain (3) due to lack of explicit full‑duplex/synchronization/evaluation details. The title explicitly contains “Speech Language Models” and the abstract describes KE‑Omni as a large speech language model trained via scaled supervised fine‑tuning on 60,000+ hours of synthetic speech dialogue (thus treating speech as the core modeling object and describing training/data), so it satisfies the inclusion requirements (even if it lacks detailed full‑duplex metrics). Verdict explanation (1 sentence): Include (5) — the work clearly centers on speech language modeling and training at scale, meeting the core inclusion criteria despite limited detail on full‑duplex evaluation.",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: one recommended inclusion (5) citing SLM focus and dataset; the other was uncertain (3) due to lack of explicit full‑duplex/synchronization/evaluation details. The title explicitly contains “Speech Language Models” and the abstract describes KE‑Omni as a large speech language model trained via scaled supervised fine‑tuning on 60,000+ hours of synthetic speech dialogue (thus treating speech as the core modeling object and describing training/data), so it satisfies the inclusion requirements (even if it lacks detailed full‑duplex metrics). Verdict explanation (1 sentence): Include (5) — the work clearly centers on speech language modeling and training at scale, meeting the core inclusion criteria despite limited detail on full‑duplex evaluation.",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion",
    "abstract": "We propose 4Real-Video, a novel framework for generating 4D videos, organized as a grid of video frames with both time and viewpoint axes. In this grid, each row contains frames sharing the same timestep, while each column contains frames from the same viewpoint. We propose a novel two-stream architecture. One stream performs viewpoint updates on columns, and the other stream performs temporal updates on rows. After each diffusion transformer layer, a synchronization layer exchanges information between the two token streams. We propose two implementations of the synchronization layer, using either hard or soft synchronization. This feedforward architecture improves upon previous work in three ways: higher inference speed, enhanced visual quality (measured by FVD, CLIP, and VideoScore), and improved temporal and viewpoint consistency (measured by VideoScore and Dust3R-Confidence).",
    "metadata": {
      "arxiv_id": "2412.04462",
      "title": "4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion",
      "summary": "We propose 4Real-Video, a novel framework for generating 4D videos, organized as a grid of video frames with both time and viewpoint axes. In this grid, each row contains frames sharing the same timestep, while each column contains frames from the same viewpoint. We propose a novel two-stream architecture. One stream performs viewpoint updates on columns, and the other stream performs temporal updates on rows. After each diffusion transformer layer, a synchronization layer exchanges information between the two token streams. We propose two implementations of the synchronization layer, using either hard or soft synchronization. This feedforward architecture improves upon previous work in three ways: higher inference speed, enhanced visual quality (measured by FVD, CLIP, and VideoScore), and improved temporal and viewpoint consistency (measured by VideoScore and Dust3R-Confidence).",
      "authors": [
        "Chaoyang Wang",
        "Peiye Zhuang",
        "Tuan Duc Ngo",
        "Willi Menapace",
        "Aliaksandr Siarohin",
        "Michael Vasilkovsky",
        "Ivan Skorokhodov",
        "Sergey Tulyakov",
        "Peter Wonka",
        "Hsin-Ying Lee"
      ],
      "published": "2024-12-05T18:59:41Z",
      "updated": "2024-12-05T18:59:41Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.04462v1",
      "landing_url": "https://arxiv.org/abs/2412.04462v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.04462"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on 4D video diffusion with a diffusion transformer architecture and video-oriented evaluation, not on Spoken Language Models or speech-language modeling as the core object, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on 4D video diffusion with a diffusion transformer architecture and video-oriented evaluation, not on Spoken Language Models or speech-language modeling as the core object, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses a novel framework for generating 4D videos focusing on visual content modeling and does not address spoken language modeling, speech language models, or speech as a core modeling object; it also lacks any mention of full-duplex spoken language interaction, synchronization in speech, or related evaluation metrics. Therefore, it does not meet the inclusion criteria that focus on spoken language models and synchronous speech interaction and falls outside the scope of the defined topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses a novel framework for generating 4D videos focusing on visual content modeling and does not address spoken language modeling, speech language models, or speech as a core modeling object; it also lacks any mention of full-duplex spoken language interaction, synchronization in speech, or related evaluation metrics. Therefore, it does not meet the inclusion criteria that focus on spoken language models and synchronous speech interaction and falls outside the scope of the defined topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Who Speaks Next? Multi-party AI Discussion Leveraging the Systematics of Turn-taking in Murder Mystery Games",
    "abstract": "Multi-agent systems utilizing large language models (LLMs) have shown great promise in achieving natural dialogue. However, smooth dialogue control and autonomous decision making among agents still remain challenges. In this study, we focus on conversational norms such as adjacency pairs and turn-taking found in conversation analysis and propose a new framework called \"Murder Mystery Agents\" that applies these norms to AI agents' dialogue control. As an evaluation target, we employed the \"Murder Mystery\" game, a reasoning-type table-top role-playing game that requires complex social reasoning and information manipulation. In this game, players need to unravel the truth of the case based on fragmentary information through cooperation and bargaining. The proposed framework integrates next speaker selection based on adjacency pairs and a self-selection mechanism that takes agents' internal states into account to achieve more natural and strategic dialogue. To verify the effectiveness of this new approach, we analyzed utterances that led to dialogue breakdowns and conducted automatic evaluation using LLMs, as well as human evaluation using evaluation criteria developed for the Murder Mystery game. Experimental results showed that the implementation of the next speaker selection mechanism significantly reduced dialogue breakdowns and improved the ability of agents to share information and perform logical reasoning. The results of this study demonstrate that the systematics of turn-taking in human conversation are also effective in controlling dialogue among AI agents, and provide design guidelines for more advanced multi-agent dialogue systems.",
    "metadata": {
      "arxiv_id": "2412.04937",
      "title": "Who Speaks Next? Multi-party AI Discussion Leveraging the Systematics of Turn-taking in Murder Mystery Games",
      "summary": "Multi-agent systems utilizing large language models (LLMs) have shown great promise in achieving natural dialogue. However, smooth dialogue control and autonomous decision making among agents still remain challenges. In this study, we focus on conversational norms such as adjacency pairs and turn-taking found in conversation analysis and propose a new framework called \"Murder Mystery Agents\" that applies these norms to AI agents' dialogue control. As an evaluation target, we employed the \"Murder Mystery\" game, a reasoning-type table-top role-playing game that requires complex social reasoning and information manipulation. In this game, players need to unravel the truth of the case based on fragmentary information through cooperation and bargaining. The proposed framework integrates next speaker selection based on adjacency pairs and a self-selection mechanism that takes agents' internal states into account to achieve more natural and strategic dialogue. To verify the effectiveness of this new approach, we analyzed utterances that led to dialogue breakdowns and conducted automatic evaluation using LLMs, as well as human evaluation using evaluation criteria developed for the Murder Mystery game. Experimental results showed that the implementation of the next speaker selection mechanism significantly reduced dialogue breakdowns and improved the ability of agents to share information and perform logical reasoning. The results of this study demonstrate that the systematics of turn-taking in human conversation are also effective in controlling dialogue among AI agents, and provide design guidelines for more advanced multi-agent dialogue systems.",
      "authors": [
        "Ryota Nonomura",
        "Hiroki Mori"
      ],
      "published": "2024-12-06T10:45:54Z",
      "updated": "2025-02-21T04:08:06Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.04937v2",
      "landing_url": "https://arxiv.org/abs/2412.04937v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.04937"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on turn-taking and dialogue control in multi-agent AI using text-based interactions, not on spoken language models or audio--centric modeling, thus it does not meet the core SLM/FD-SLM criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on turn-taking and dialogue control in multi-agent AI using text-based interactions, not on spoken language models or audio--centric modeling, thus it does not meet the core SLM/FD-SLM criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multi-agent dialogue and turn-taking in text-based murder mystery games using large language models, without any indication of modeling or reasoning over spoken language signals or Speech Language Models. It does not discuss speech-level language modeling, speech-LLM hybrid architectures, full-duplex spoken interaction, or relevant synchronization/overlap/backchanneling phenomena relevant to spoken dialogue systems. The core research is on text-based dialogue control rather than spoken language modeling, and the evaluation does not cover speech-based metrics or real-time voice interaction. Therefore, it does not satisfy the inclusion criteria centered on speech language models and full-duplex spoken interaction, nor does it meet the exclusion criteria for solely text or turn-taking in conversation analysis without speech modeling. Hence, it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multi-agent dialogue and turn-taking in text-based murder mystery games using large language models, without any indication of modeling or reasoning over spoken language signals or Speech Language Models. It does not discuss speech-level language modeling, speech-LLM hybrid architectures, full-duplex spoken interaction, or relevant synchronization/overlap/backchanneling phenomena relevant to spoken dialogue systems. The core research is on text-based dialogue control rather than spoken language modeling, and the evaluation does not cover speech-based metrics or real-time voice interaction. Therefore, it does not satisfy the inclusion criteria centered on speech language models and full-duplex spoken interaction, nor does it meet the exclusion criteria for solely text or turn-taking in conversation analysis without speech modeling. Hence, it should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch",
    "abstract": "It is well known that LLM-based systems are data-hungry. Recent LLM-based TTS works typically employ complex data processing pipelines to obtain high-quality training data. These sophisticated pipelines require excellent models at each stage (e.g., speech denoising, speech enhancement, speaker diarization, and punctuation models), which themselves demand high-quality training data and are rarely open-sourced. Even with state-of-the-art models, issues persist, such as incomplete background noise removal and misalignment between punctuation and actual speech pauses. Moreover, the stringent filtering strategies often retain only 10-30\\% of the original data, significantly impeding data scaling efforts. In this work, we leverage a noise-robust audio tokenizer (S3Tokenizer) to design a simplified yet effective TTS data processing pipeline that maintains data quality while substantially reducing data acquisition costs, achieving a data retention rate of over 50\\%. Beyond data scaling challenges, LLM-based TTS systems also incur higher deployment costs compared to conventional approaches. Current systems typically use LLMs solely for text-to-token generation, while requiring separate models (e.g., flow matching models) for token-to-waveform generation, which cannot be directly executed by LLM inference engines, further complicating deployment. To address these challenges, we eliminate redundant modules in both LLM and flow components, replacing the flow model backbone with an LLM architecture. Building upon this simplified flow backbone, we propose a unified architecture for both streaming and non-streaming inference, significantly reducing deployment costs. Finally, we explore the feasibility of unifying TTS and ASR tasks using the same data for training, thanks to the simplified pipeline and the S3Tokenizer that reduces the quality requirements for TTS training data.",
    "metadata": {
      "arxiv_id": "2412.08237",
      "title": "TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch",
      "summary": "It is well known that LLM-based systems are data-hungry. Recent LLM-based TTS works typically employ complex data processing pipelines to obtain high-quality training data. These sophisticated pipelines require excellent models at each stage (e.g., speech denoising, speech enhancement, speaker diarization, and punctuation models), which themselves demand high-quality training data and are rarely open-sourced. Even with state-of-the-art models, issues persist, such as incomplete background noise removal and misalignment between punctuation and actual speech pauses. Moreover, the stringent filtering strategies often retain only 10-30\\% of the original data, significantly impeding data scaling efforts. In this work, we leverage a noise-robust audio tokenizer (S3Tokenizer) to design a simplified yet effective TTS data processing pipeline that maintains data quality while substantially reducing data acquisition costs, achieving a data retention rate of over 50\\%. Beyond data scaling challenges, LLM-based TTS systems also incur higher deployment costs compared to conventional approaches. Current systems typically use LLMs solely for text-to-token generation, while requiring separate models (e.g., flow matching models) for token-to-waveform generation, which cannot be directly executed by LLM inference engines, further complicating deployment. To address these challenges, we eliminate redundant modules in both LLM and flow components, replacing the flow model backbone with an LLM architecture. Building upon this simplified flow backbone, we propose a unified architecture for both streaming and non-streaming inference, significantly reducing deployment costs. Finally, we explore the feasibility of unifying TTS and ASR tasks using the same data for training, thanks to the simplified pipeline and the S3Tokenizer that reduces the quality requirements for TTS training data.",
      "authors": [
        "Xingchen Song",
        "Mengtao Xing",
        "Changwei Ma",
        "Shengqiang Li",
        "Di Wu",
        "Binbin Zhang",
        "Fuping Pan",
        "Dinghao Zhou",
        "Yuekai Zhang",
        "Shun Lei",
        "Zhendong Peng",
        "Zhiyong Wu"
      ],
      "published": "2024-12-11T09:38:50Z",
      "updated": "2024-12-12T10:01:11Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.08237v2",
      "landing_url": "https://arxiv.org/abs/2412.08237v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.08237"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目主要聚焦於簡化 TTS 數據處理流程與在 LLM 架構下的語音到文字整合，並未將語音本身作為核心建模對象（亦未明確涵蓋全雙工互動與即時語音協調的評估），因此不符合以 Spoken Language Models 或 FD-SLM 為核心的納入標準。",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "此項目主要聚焦於簡化 TTS 數據處理流程與在 LLM 架構下的語音到文字整合，並未將語音本身作為核心建模對象（亦未明確涵蓋全雙工互動與即時語音協調的評估），因此不符合以 Spoken Language Models 或 FD-SLM 為核心的納入標準。",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a TTS framework that simplifies data processing and deployment by leveraging an LLM architecture but is centered mainly on TTS (speech synthesis) rather than broader spoken language modeling or full-duplex interactive spoken language models; it lacks discussion on speech language modeling concepts, synchronous interaction, or multi-dimensional evaluation relevant to FD-SLMs as required by the inclusion criteria and appears to represent a single downstream task rather than general spoken language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a TTS framework that simplifies data processing and deployment by leveraging an LLM architecture but is centered mainly on TTS (speech synthesis) rather than broader spoken language modeling or full-duplex interactive spoken language models; it lacks discussion on speech language modeling concepts, synchronous interaction, or multi-dimensional evaluation relevant to FD-SLMs as required by the inclusion criteria and appears to represent a single downstream task rather than general spoken language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Align, Generate, Learn: A Novel Closed-Loop Framework for Cross-Lingual In-Context Learning",
    "abstract": "Cross-lingual in-context learning (XICL) has emerged as a transformative paradigm for leveraging large language models (LLMs) to tackle multilingual tasks, especially for low-resource languages. However, existing approaches often rely on external retrievers or task-specific fine-tuning, limiting their scalability and generalizability. In this paper, we propose a novel self-supervised framework that harnesses the generative capabilities of LLMs to internally select and utilize task-relevant examples. Our method introduces two key objectives: a retrieval-generation alignment loss to optimize the quality of selected examples and a semantic coherence loss to ensure cross-lingual consistency. Through extensive experiments on multilingual benchmarks, our approach achieves state-of-the-art performance, significantly outperforming existing baselines. Further analysis highlights its robustness across diverse language families and its ability to generalize to unseen tasks. Human evaluations confirm the superior fluency, relevance, and semantic correctness of outputs generated by our method. This work provides a scalable, effective, and generalizable solution for cross-lingual in-context learning.",
    "metadata": {
      "arxiv_id": "2412.08955",
      "title": "Align, Generate, Learn: A Novel Closed-Loop Framework for Cross-Lingual In-Context Learning",
      "summary": "Cross-lingual in-context learning (XICL) has emerged as a transformative paradigm for leveraging large language models (LLMs) to tackle multilingual tasks, especially for low-resource languages. However, existing approaches often rely on external retrievers or task-specific fine-tuning, limiting their scalability and generalizability. In this paper, we propose a novel self-supervised framework that harnesses the generative capabilities of LLMs to internally select and utilize task-relevant examples. Our method introduces two key objectives: a retrieval-generation alignment loss to optimize the quality of selected examples and a semantic coherence loss to ensure cross-lingual consistency. Through extensive experiments on multilingual benchmarks, our approach achieves state-of-the-art performance, significantly outperforming existing baselines. Further analysis highlights its robustness across diverse language families and its ability to generalize to unseen tasks. Human evaluations confirm the superior fluency, relevance, and semantic correctness of outputs generated by our method. This work provides a scalable, effective, and generalizable solution for cross-lingual in-context learning.",
      "authors": [
        "Mateo Alejandro Rojas",
        "Rafael Carranza"
      ],
      "published": "2024-12-12T05:36:51Z",
      "updated": "2024-12-12T05:36:51Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.08955v1",
      "landing_url": "https://arxiv.org/abs/2412.08955v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.08955"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper centers on cross-lingual in-context learning with large language models and retrieval-generation alignment, not on Spoken Language Models or speech-based modeling; it does not address speech as the core modeling object, so it does not meet inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper centers on cross-lingual in-context learning with large language models and retrieval-generation alignment, not on Spoken Language Models or speech-based modeling; it does not address speech as the core modeling object, so it does not meet inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on cross-lingual in-context learning with large language models, emphasizing multilingual text-based tasks and generative model alignment, without any mention of speech modeling, spoken language models, or speech sequence processing; thus, it does not meet the core inclusion criteria centered on spoken language modeling or full-duplex spoken interaction capabilities.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on cross-lingual in-context learning with large language models, emphasizing multilingual text-based tasks and generative model alignment, without any mention of speech modeling, spoken language models, or speech sequence processing; thus, it does not meet the core inclusion criteria centered on spoken language modeling or full-duplex spoken interaction capabilities.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Learned Compression for Compressed Learning",
    "abstract": "Modern sensors produce increasingly rich streams of high-resolution data. Due to resource constraints, machine learning systems discard the vast majority of this information via resolution reduction. Compressed-domain learning allows models to operate on compact latent representations, allowing higher effective resolution for the same budget. However, existing compression systems are not ideal for compressed learning. Linear transform coding and end-to-end learned compression systems reduce bitrate, but do not uniformly reduce dimensionality; thus, they do not meaningfully increase efficiency. Generative autoencoders reduce dimensionality, but their adversarial or perceptual objectives lead to significant information loss. To address these limitations, we introduce WaLLoC (Wavelet Learned Lossy Compression), a neural codec architecture that combines linear transform coding with nonlinear dimensionality-reducing autoencoders. WaLLoC sandwiches a shallow, asymmetric autoencoder and entropy bottleneck between an invertible wavelet packet transform. Across several key metrics, WaLLoC outperforms the autoencoders used in state-of-the-art latent diffusion models. WaLLoC does not require perceptual or adversarial losses to represent high-frequency detail, providing compatibility with modalities beyond RGB images and stereo audio. WaLLoC's encoder consists almost entirely of linear operations, making it exceptionally efficient and suitable for mobile computing, remote sensing, and learning directly from compressed data. We demonstrate WaLLoC's capability for compressed-domain learning across several tasks, including image classification, colorization, document understanding, and music source separation. Our code, experiments, and pre-trained audio and image codecs are available at https://ut-sysml.org/walloc",
    "metadata": {
      "arxiv_id": "2412.09405",
      "title": "Learned Compression for Compressed Learning",
      "summary": "Modern sensors produce increasingly rich streams of high-resolution data. Due to resource constraints, machine learning systems discard the vast majority of this information via resolution reduction. Compressed-domain learning allows models to operate on compact latent representations, allowing higher effective resolution for the same budget. However, existing compression systems are not ideal for compressed learning. Linear transform coding and end-to-end learned compression systems reduce bitrate, but do not uniformly reduce dimensionality; thus, they do not meaningfully increase efficiency. Generative autoencoders reduce dimensionality, but their adversarial or perceptual objectives lead to significant information loss. To address these limitations, we introduce WaLLoC (Wavelet Learned Lossy Compression), a neural codec architecture that combines linear transform coding with nonlinear dimensionality-reducing autoencoders. WaLLoC sandwiches a shallow, asymmetric autoencoder and entropy bottleneck between an invertible wavelet packet transform. Across several key metrics, WaLLoC outperforms the autoencoders used in state-of-the-art latent diffusion models. WaLLoC does not require perceptual or adversarial losses to represent high-frequency detail, providing compatibility with modalities beyond RGB images and stereo audio. WaLLoC's encoder consists almost entirely of linear operations, making it exceptionally efficient and suitable for mobile computing, remote sensing, and learning directly from compressed data. We demonstrate WaLLoC's capability for compressed-domain learning across several tasks, including image classification, colorization, document understanding, and music source separation. Our code, experiments, and pre-trained audio and image codecs are available at https://ut-sysml.org/walloc",
      "authors": [
        "Dan Jacobellis",
        "Neeraja J. Yadwadkar"
      ],
      "published": "2024-12-12T16:09:57Z",
      "updated": "2024-12-12T16:09:57Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.09405v1",
      "landing_url": "https://arxiv.org/abs/2412.09405v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.09405"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on general learned compression and compressed-domain learning (image/music data) rather than Spoken Language Models or full-duplex speech interaction, hence it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on general learned compression and compressed-domain learning (image/music data) rather than Spoken Language Models or full-duplex speech interaction, hence it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on learned compression methods for compressed-domain learning tasks such as image classification, colorization, document understanding, and music source separation, with no mention of spoken language models, speech language modeling, or related concepts in the abstract or title. It does not address core modeling or reasoning over speech sequences, full-duplex spoken language models, or any part of the specified inclusion criteria related to speech-based language modeling and interaction. Therefore, it fails to meet the inclusion criteria and is irrelevant to the given research theme.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on learned compression methods for compressed-domain learning tasks such as image classification, colorization, document understanding, and music source separation, with no mention of spoken language models, speech language modeling, or related concepts in the abstract or title. It does not address core modeling or reasoning over speech sequences, full-duplex spoken language models, or any part of the specified inclusion criteria related to speech-based language modeling and interaction. Therefore, it fails to meet the inclusion criteria and is irrelevant to the given research theme.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "HiTZ at VarDial 2025 NorSID: Overcoming Data Scarcity with Language Transfer and Automatic Data Annotation",
    "abstract": "In this paper we present our submission for the NorSID Shared Task as part of the 2025 VarDial Workshop (Scherrer et al., 2025), consisting of three tasks: Intent Detection, Slot Filling and Dialect Identification, evaluated using data in different dialects of the Norwegian language. For Intent Detection and Slot Filling, we have fine-tuned a multitask model in a cross-lingual setting, to leverage the xSID dataset available in 17 languages. In the case of Dialect Identification, our final submission consists of a model fine-tuned on the provided development set, which has obtained the highest scores within our experiments. Our final results on the test set show that our models do not drop in performance compared to the development set, likely due to the domain-specificity of the dataset and the similar distribution of both subsets. Finally, we also report an in-depth analysis of the provided datasets and their artifacts, as well as other sets of experiments that have been carried out but did not yield the best results. Additionally, we present an analysis on the reasons why some methods have been more successful than others; mainly the impact of the combination of languages and domain-specificity of the training data on the results.",
    "metadata": {
      "arxiv_id": "2412.10095",
      "title": "HiTZ at VarDial 2025 NorSID: Overcoming Data Scarcity with Language Transfer and Automatic Data Annotation",
      "summary": "In this paper we present our submission for the NorSID Shared Task as part of the 2025 VarDial Workshop (Scherrer et al., 2025), consisting of three tasks: Intent Detection, Slot Filling and Dialect Identification, evaluated using data in different dialects of the Norwegian language. For Intent Detection and Slot Filling, we have fine-tuned a multitask model in a cross-lingual setting, to leverage the xSID dataset available in 17 languages. In the case of Dialect Identification, our final submission consists of a model fine-tuned on the provided development set, which has obtained the highest scores within our experiments. Our final results on the test set show that our models do not drop in performance compared to the development set, likely due to the domain-specificity of the dataset and the similar distribution of both subsets. Finally, we also report an in-depth analysis of the provided datasets and their artifacts, as well as other sets of experiments that have been carried out but did not yield the best results. Additionally, we present an analysis on the reasons why some methods have been more successful than others; mainly the impact of the combination of languages and domain-specificity of the training data on the results.",
      "authors": [
        "Jaione Bengoetxea",
        "Mikel Zubillaga",
        "Ekhi Azurmendi",
        "Maite Heredia",
        "Julen Etxaniz",
        "Markel Ferro",
        "Jeremy Barnes"
      ],
      "published": "2024-12-13T12:31:06Z",
      "updated": "2025-01-09T09:09:32Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.10095v2",
      "landing_url": "https://arxiv.org/abs/2412.10095v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.10095"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目未聚焦於語音/語音語言模型核心，而是針對跨語言文本任務（意圖識別、槽位填充、方言識別），缺乏語音建模的核心內容與評估，符合排除條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目未聚焦於語音/語音語言模型核心，而是針對跨語言文本任務（意圖識別、槽位填充、方言識別），缺乏語音建模的核心內容與評估，符合排除條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on intent detection, slot filling, and dialect identification in Norwegian dialects using multitask and cross-lingual models, but there is no indication that it treats speech or acoustic signals as the core modeling objects, nor does it discuss Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) or features like synchronous interaction or cognitive parallelism. The abstract does not mention speech language modeling, speech token language modeling, or specific aspects of spoken language models according to the inclusion criteria; instead, it mainly discusses NLP tasks commonly based on text representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on intent detection, slot filling, and dialect identification in Norwegian dialects using multitask and cross-lingual models, but there is no indication that it treats speech or acoustic signals as the core modeling objects, nor does it discuss Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) or features like synchronous interaction or cognitive parallelism. The abstract does not mention speech language modeling, speech token language modeling, or specific aspects of spoken language models according to the inclusion criteria; instead, it mainly discusses NLP tasks commonly based on text representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "PSGraph: Differentially Private Streaming Graph Synthesis by Considering Temporal Dynamics",
    "abstract": "Streaming graphs are ubiquitous in daily life, such as evolving social networks and dynamic communication systems. Due to the sensitive information contained in the graph, directly sharing the streaming graphs poses significant privacy risks. Differential privacy, offering strict theoretical guarantees, has emerged as a standard approach for private graph data synthesis. However, existing methods predominantly focus on static graph publishing, neglecting the intrinsic relationship between adjacent graphs, thereby resulting in limited performance in streaming data publishing scenarios. To address this gap, we propose PSGraph, the first differentially private streaming graph synthesis framework that integrates temporal dynamics. PSGraph adaptively adjusts the privacy budget allocation mechanism by analyzing the variations in the current graph compared to the previous one for conserving the privacy budget. Moreover, PSGraph aggregates information across various timestamps and adopts crucial post-processing techniques to enhance the synthetic streaming graphs. We conduct extensive experiments on four real-world datasets under five commonly used metrics. The experimental results demonstrate the superiority of our proposed PSGraph.",
    "metadata": {
      "arxiv_id": "2412.11369",
      "title": "PSGraph: Differentially Private Streaming Graph Synthesis by Considering Temporal Dynamics",
      "summary": "Streaming graphs are ubiquitous in daily life, such as evolving social networks and dynamic communication systems. Due to the sensitive information contained in the graph, directly sharing the streaming graphs poses significant privacy risks. Differential privacy, offering strict theoretical guarantees, has emerged as a standard approach for private graph data synthesis. However, existing methods predominantly focus on static graph publishing, neglecting the intrinsic relationship between adjacent graphs, thereby resulting in limited performance in streaming data publishing scenarios. To address this gap, we propose PSGraph, the first differentially private streaming graph synthesis framework that integrates temporal dynamics. PSGraph adaptively adjusts the privacy budget allocation mechanism by analyzing the variations in the current graph compared to the previous one for conserving the privacy budget. Moreover, PSGraph aggregates information across various timestamps and adopts crucial post-processing techniques to enhance the synthetic streaming graphs. We conduct extensive experiments on four real-world datasets under five commonly used metrics. The experimental results demonstrate the superiority of our proposed PSGraph.",
      "authors": [
        "Quan Yuan",
        "Zhikun Zhang",
        "Linkang Du",
        "Min Chen",
        "Mingyang Sun",
        "Yunjun Gao",
        "Michael Backes",
        "Shibo He",
        "Jiming Chen"
      ],
      "published": "2024-12-16T01:56:32Z",
      "updated": "2025-05-30T14:28:15Z",
      "categories": [
        "cs.CR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.11369v2",
      "landing_url": "https://arxiv.org/abs/2412.11369v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.11369"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on differential privacy for streaming graphs and has no connection to Spoken Language Models or speech-language modeling, thus fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on differential privacy for streaming graphs and has no connection to Spoken Language Models or speech-language modeling, thus fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on differentially private streaming graph synthesis, which is unrelated to spoken language models or speech-language modeling, and does not mention any core concepts such as speech modeling, full-duplex interaction, or synchronous speech interaction. Therefore, it fails to meet the inclusion criteria and is irrelevant to the thematic focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on differentially private streaming graph synthesis, which is unrelated to spoken language models or speech-language modeling, and does not mention any core concepts such as speech modeling, full-duplex interaction, or synchronous speech interaction. Therefore, it fails to meet the inclusion criteria and is irrelevant to the thematic focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Whisper-GPT: A Hybrid Representation Audio Large Language Model",
    "abstract": "We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music.",
    "metadata": {
      "arxiv_id": "2412.11449",
      "title": "Whisper-GPT: A Hybrid Representation Audio Large Language Model",
      "summary": "We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music.",
      "authors": [
        "Prateek Verma"
      ],
      "published": "2024-12-16T05:03:48Z",
      "updated": "2024-12-16T05:03:48Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.11449v1",
      "landing_url": "https://arxiv.org/abs/2412.11449v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.11449"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item presents an audio-based language model (Whisper-GPT) for speech and music, explicitly framing it as language modeling of speech with a hybrid continuous/discrete representation, aligned with Spoken Language Models concepts, within the last 3 years, and not limited to ASR/TTS or single-task evaluation.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item presents an audio-based language model (Whisper-GPT) for speech and music, explicitly framing it as language modeling of speech with a hybrid continuous/discrete representation, aligned with Spoken Language Models concepts, within the last 3 years, and not limited to ASR/TTS or single-task evaluation.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes Whisper-GPT, a generative large language model that jointly models continuous audio representations and discrete tokens for speech and music, advancing language modeling from audio signals themselves rather than from text alone. However, the abstract lacks mention of synchronized full-duplex spoken interaction, turn-taking, cognitive parallelism, low-latency real-time interactions, or explicit evaluation on these interaction metrics, which are key inclusion aspects. It focuses mainly on improving token prediction perplexity and log-likelihood for speech/music audio, without addressing dialogue behavior arbitration or interactive evaluation. Thus, while it aligns with spoken language modeling at the audio representation level, it does not clearly fulfill full-duplex or synchronized conversational interaction criteria or explicit evaluation frameworks, making inclusion uncertain.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper proposes Whisper-GPT, a generative large language model that jointly models continuous audio representations and discrete tokens for speech and music, advancing language modeling from audio signals themselves rather than from text alone. However, the abstract lacks mention of synchronized full-duplex spoken interaction, turn-taking, cognitive parallelism, low-latency real-time interactions, or explicit evaluation on these interaction metrics, which are key inclusion aspects. It focuses mainly on improving token prediction perplexity and log-likelihood for speech/music audio, without addressing dialogue behavior arbitration or interactive evaluation. Thus, while it aligns with spoken language modeling at the audio representation level, it does not clearly fulfill full-duplex or synchronized conversational interaction criteria or explicit evaluation frameworks, making inclusion uncertain.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the paper is a speech-focused large language model that jointly models continuous and discrete audio representations and reports language-modeling metrics (perplexity, NLL), which satisfies the core inclusion requirement that the work treats speech as the primary modeling object and discusses model architecture/evaluation; JuniorNano recommended inclusion (4) while JuniorMini flagged the absence of full‑duplex/synchronous interaction evaluation but still acknowledged it as speech LM research (3). The abstract explicitly frames the work as an LLM “for speech and music” and discusses hybrid audio representations and next-token prediction—thus it meets the inclusion criteria (spoken-language-model / language modeling of speech present, architecture and evaluation discussed) and does not meet any exclusion criteria (it is not solely ASR/TTS, nor purely text-based, nor only WER/audio-quality focused). However, it does not address full‑duplex interaction, low-latency or conversational arbitration, which are desirable but not mandatory per the stated inclusion rules. 1-sentence explanation for the score: The paper should be included because it clearly advances spoken-language modeling with a hybrid continuous/discrete audio LLM and reports relevant model-level evaluations, despite lacking full-duplex interaction experiments.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the paper is a speech-focused large language model that jointly models continuous and discrete audio representations and reports language-modeling metrics (perplexity, NLL), which satisfies the core inclusion requirement that the work treats speech as the primary modeling object and discusses model architecture/evaluation; JuniorNano recommended inclusion (4) while JuniorMini flagged the absence of full‑duplex/synchronous interaction evaluation but still acknowledged it as speech LM research (3). The abstract explicitly frames the work as an LLM “for speech and music” and discusses hybrid audio representations and next-token prediction—thus it meets the inclusion criteria (spoken-language-model / language modeling of speech present, architecture and evaluation discussed) and does not meet any exclusion criteria (it is not solely ASR/TTS, nor purely text-based, nor only WER/audio-quality focused). However, it does not address full‑duplex interaction, low-latency or conversational arbitration, which are desirable but not mandatory per the stated inclusion rules. 1-sentence explanation for the score: The paper should be included because it clearly advances spoken-language modeling with a hybrid continuous/discrete audio LLM and reports relevant model-level evaluations, despite lacking full-duplex interaction experiments.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DSGram: Dynamic Weighting Sub-Metrics for Grammatical Error Correction in the Era of Large Language Models",
    "abstract": "Evaluating the performance of Grammatical Error Correction (GEC) models has become increasingly challenging, as large language model (LLM)-based GEC systems often produce corrections that diverge from provided gold references. This discrepancy undermines the reliability of traditional reference-based evaluation metrics. In this study, we propose a novel evaluation framework for GEC models, DSGram, integrating Semantic Coherence, Edit Level, and Fluency, and utilizing a dynamic weighting mechanism. Our framework employs the Analytic Hierarchy Process (AHP) in conjunction with large language models to ascertain the relative importance of various evaluation criteria. Additionally, we develop a dataset incorporating human annotations and LLM-simulated sentences to validate our algorithms and fine-tune more cost-effective models. Experimental results indicate that our proposed approach enhances the effectiveness of GEC model evaluations.",
    "metadata": {
      "arxiv_id": "2412.12832",
      "title": "DSGram: Dynamic Weighting Sub-Metrics for Grammatical Error Correction in the Era of Large Language Models",
      "summary": "Evaluating the performance of Grammatical Error Correction (GEC) models has become increasingly challenging, as large language model (LLM)-based GEC systems often produce corrections that diverge from provided gold references. This discrepancy undermines the reliability of traditional reference-based evaluation metrics. In this study, we propose a novel evaluation framework for GEC models, DSGram, integrating Semantic Coherence, Edit Level, and Fluency, and utilizing a dynamic weighting mechanism. Our framework employs the Analytic Hierarchy Process (AHP) in conjunction with large language models to ascertain the relative importance of various evaluation criteria. Additionally, we develop a dataset incorporating human annotations and LLM-simulated sentences to validate our algorithms and fine-tune more cost-effective models. Experimental results indicate that our proposed approach enhances the effectiveness of GEC model evaluations.",
      "authors": [
        "Jinxiang Xie",
        "Yilin Li",
        "Xunjian Yin",
        "Xiaojun Wan"
      ],
      "published": "2024-12-17T11:54:16Z",
      "updated": "2025-06-22T05:51:54Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.12832v2",
      "landing_url": "https://arxiv.org/abs/2412.12832v2",
      "doi": "https://doi.org/10.1609/aaai.v39i24.34746"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article addresses GEC evaluation in LLMs and uses a framework for evaluating text corrections; it does not center on Spoken Language Models or speech-language modeling, nor does it cover speech interaction or multi-turn dialogue features.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article addresses GEC evaluation in LLMs and uses a framework for evaluating text corrections; it does not center on Spoken Language Models or speech-language modeling, nor does it cover speech interaction or multi-turn dialogue features.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating grammatical error correction models using language models, but it does not address spoken language models, speech-level modeling, full-duplex interaction, or synchronization aspects involving speech signals. It is centered on text-based evaluation metrics rather than speech or speech-language modeling frameworks, which fails to meet the core inclusion criteria requiring speech as the primary modeling and reasoning object and related architectural or evaluation discussions.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating grammatical error correction models using language models, but it does not address spoken language models, speech-level modeling, full-duplex interaction, or synchronization aspects involving speech signals. It is centered on text-based evaluation metrics rather than speech or speech-language modeling frameworks, which fails to meet the core inclusion criteria requiring speech as the primary modeling and reasoning object and related architectural or evaluation discussions.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Associative memory inspires improvements for in-context learning using a novel attention residual stream architecture",
    "abstract": "Large language models (LLMs) demonstrate an impressive ability to utilise information within the context of their input sequences to appropriately respond to data unseen by the LLM during its training procedure. This ability is known as in-context learning (ICL). Humans and non-human animals demonstrate similar abilities, however their neural architectures differ substantially from LLMs. Despite this, a critical component within LLMs, the attention mechanism, resembles modern associative memory models, widely used in and influenced by the computational neuroscience community to model biological memory systems. Using this connection, we introduce an associative memory model capable of performing ICL. We use this as inspiration for a novel residual stream architecture which allows information to directly flow between attention heads. We test this architecture during training within a two-layer Transformer and show its ICL abilities manifest more quickly than without this modification. We then apply our architecture in small language models with 8 million and 1 billion parameters, focusing on attention head values, with results also indicating improved performance at these larger and more naturalistic scales.",
    "metadata": {
      "arxiv_id": "2412.15113",
      "title": "Associative memory inspires improvements for in-context learning using a novel attention residual stream architecture",
      "summary": "Large language models (LLMs) demonstrate an impressive ability to utilise information within the context of their input sequences to appropriately respond to data unseen by the LLM during its training procedure. This ability is known as in-context learning (ICL). Humans and non-human animals demonstrate similar abilities, however their neural architectures differ substantially from LLMs. Despite this, a critical component within LLMs, the attention mechanism, resembles modern associative memory models, widely used in and influenced by the computational neuroscience community to model biological memory systems. Using this connection, we introduce an associative memory model capable of performing ICL. We use this as inspiration for a novel residual stream architecture which allows information to directly flow between attention heads. We test this architecture during training within a two-layer Transformer and show its ICL abilities manifest more quickly than without this modification. We then apply our architecture in small language models with 8 million and 1 billion parameters, focusing on attention head values, with results also indicating improved performance at these larger and more naturalistic scales.",
      "authors": [
        "Thomas F Burns",
        "Tomoki Fukai",
        "Christopher J Earls"
      ],
      "published": "2024-12-19T17:55:42Z",
      "updated": "2025-08-04T12:51:56Z",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.15113v2",
      "landing_url": "https://arxiv.org/abs/2412.15113v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.15113"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on in-context learning and attention/associative-memory ideas for large language models, not on Spoken Language Models or speech-based modeling, so it does not meet the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on in-context learning and attention/associative-memory ideas for large language models, not on Spoken Language Models or speech-based modeling, so it does not meet the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on associative memory models and improvements in in-context learning for language models but does not address spoken language modeling, speech as a core modeling target, or full-duplex spoken language interaction aspects as required by the inclusion criteria. It lacks mention of speech or spoken language models specifically and focuses on text-based transformer architectures rather than speech or speech-LLM hybrid models, thus failing to meet the core requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on associative memory models and improvements in in-context learning for language models but does not address spoken language modeling, speech as a core modeling target, or full-duplex spoken language interaction aspects as required by the inclusion criteria. It lacks mention of speech or spoken language models specifically and focuses on text-based transformer architectures rather than speech or speech-LLM hybrid models, thus failing to meet the core requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training",
    "abstract": "Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality. In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training. SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder. By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference. Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data. Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks. Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.",
    "metadata": {
      "arxiv_id": "2412.15649",
      "title": "SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training",
      "summary": "Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality. In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training. SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder. By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference. Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data. Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks. Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.",
      "authors": [
        "Wenxi Chen",
        "Ziyang Ma",
        "Ruiqi Yan",
        "Yuzhe Liang",
        "Xiquan Li",
        "Ruiyang Xu",
        "Zhikang Niu",
        "Yanqiao Zhu",
        "Yifan Yang",
        "Zhanxun Liu",
        "Kai Yu",
        "Yuxuan Hu",
        "Jinyu Li",
        "Yan Lu",
        "Shujie Liu",
        "Xie Chen"
      ],
      "published": "2024-12-20T08:05:55Z",
      "updated": "2024-12-20T08:05:55Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.15649v1",
      "landing_url": "https://arxiv.org/abs/2412.15649v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.15649"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper describes an end-to-end, timbre-controllable spoken voice interaction system with single-stage training, using semantic tokens for speech modeling and supporting multi-turn, real-time dialogue, aligning with Spok en Language Models/FD-SLMs and explicit evaluation of speech interaction rather than pure ASR/TTS; it also spans recent years (arXiv 2504.*).",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper describes an end-to-end, timbre-controllable spoken voice interaction system with single-stage training, using semantic tokens for speech modeling and supporting multi-turn, real-time dialogue, aligning with Spok en Language Models/FD-SLMs and explicit evaluation of speech interaction rather than pure ASR/TTS; it also spans recent years (arXiv 2504.*).",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents an end-to-end spoken dialogue system with timbre-controllable voice interaction and single-stage training, which models spoken language with semantic tokens and addresses multi-turn dialogue, suggesting a focus on speech language modeling and synchronization for dialogue. However, the abstract does not explicitly mention spoken language models or full-duplex spoken language models terminology, nor does it clearly discuss full-duplex interaction aspects such as real-time interruption, backchanneling, or low-latency under 200ms. The evaluation claims comprehensive metrics and multilingual capabilities but does not detail metrics related to low-latency or cognitive parallelism. Thus, while it is closely related to the spoken language modeling domain and end-to-end speech interaction, it partially meets inclusion criteria but lacks explicit focus on synchronization and evaluation dimensions requested in the inclusion criteria.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper presents an end-to-end spoken dialogue system with timbre-controllable voice interaction and single-stage training, which models spoken language with semantic tokens and addresses multi-turn dialogue, suggesting a focus on speech language modeling and synchronization for dialogue. However, the abstract does not explicitly mention spoken language models or full-duplex spoken language models terminology, nor does it clearly discuss full-duplex interaction aspects such as real-time interruption, backchanneling, or low-latency under 200ms. The evaluation claims comprehensive metrics and multilingual capabilities but does not detail metrics related to low-latency or cognitive parallelism. Thus, while it is closely related to the spoken language modeling domain and end-to-end speech interaction, it partially meets inclusion criteria but lacks explicit focus on synchronization and evaluation dimensions requested in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The paper explicitly treats speech as the core modeling object—describing “modeling spoken language with semantic tokens,” a single-stage end-to-end spoken dialogue system, architectural/training details (grouped token prediction, single-stage training, training compute/time) and comprehensive evaluations—so it meets the requirement to center on Spoken Language Modeling and to report model/training/evaluation aspects; although it does not explicitly claim full-duplex/overlap/backchannel capabilities, it is not merely an ASR/TTS work nor solely half-duplex command recognition, and therefore does not hit the exclusion criteria; the two junior reviewers were split (one supportive, one cautious about full‑duplex claims) but overall the abstract satisfies the inclusion conditions (and is within the 3-year window).",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "The paper explicitly treats speech as the core modeling object—describing “modeling spoken language with semantic tokens,” a single-stage end-to-end spoken dialogue system, architectural/training details (grouped token prediction, single-stage training, training compute/time) and comprehensive evaluations—so it meets the requirement to center on Spoken Language Modeling and to report model/training/evaluation aspects; although it does not explicitly claim full-duplex/overlap/backchannel capabilities, it is not merely an ASR/TTS work nor solely half-duplex command recognition, and therefore does not hit the exclusion criteria; the two junior reviewers were split (one supportive, one cautious about full‑duplex claims) but overall the abstract satisfies the inclusion conditions (and is within the 3-year window).",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Efficiently Solving Turn-Taking Stochastic Games with Extensive-Form Correlation",
    "abstract": "We study equilibrium computation with extensive-form correlation in two-player turn-taking stochastic games. Our main results are two-fold: (1) We give an algorithm for computing a Stackelberg extensive-form correlated equilibrium (SEFCE), which runs in time polynomial in the size of the game, as well as the number of bits required to encode each input number. (2) We give an efficient algorithm for approximately computing an optimal extensive-form correlated equilibrium (EFCE) up to machine precision, i.e., the algorithm achieves approximation error $\\varepsilon$ in time polynomial in the size of the game, as well as $\\log(1 / \\varepsilon)$. Our algorithm for SEFCE is the first polynomial-time algorithm for equilibrium computation with commitment in such a general class of stochastic games. Existing algorithms for SEFCE typically make stronger assumptions such as no chance moves, and are designed for extensive-form games in the less succinct tree form. Our algorithm for approximately optimal EFCE is, to our knowledge, the first algorithm that achieves 3 desiderata simultaneously: approximate optimality, polylogarithmic dependency on the approximation error, and compatibility with stochastic games in the more succinct graph form. Existing algorithms achieve at most 2 of these desiderata, often also relying on additional technical assumptions.",
    "metadata": {
      "arxiv_id": "2412.16934",
      "title": "Efficiently Solving Turn-Taking Stochastic Games with Extensive-Form Correlation",
      "summary": "We study equilibrium computation with extensive-form correlation in two-player turn-taking stochastic games. Our main results are two-fold: (1) We give an algorithm for computing a Stackelberg extensive-form correlated equilibrium (SEFCE), which runs in time polynomial in the size of the game, as well as the number of bits required to encode each input number. (2) We give an efficient algorithm for approximately computing an optimal extensive-form correlated equilibrium (EFCE) up to machine precision, i.e., the algorithm achieves approximation error $\\varepsilon$ in time polynomial in the size of the game, as well as $\\log(1 / \\varepsilon)$.\n  Our algorithm for SEFCE is the first polynomial-time algorithm for equilibrium computation with commitment in such a general class of stochastic games. Existing algorithms for SEFCE typically make stronger assumptions such as no chance moves, and are designed for extensive-form games in the less succinct tree form. Our algorithm for approximately optimal EFCE is, to our knowledge, the first algorithm that achieves 3 desiderata simultaneously: approximate optimality, polylogarithmic dependency on the approximation error, and compatibility with stochastic games in the more succinct graph form. Existing algorithms achieve at most 2 of these desiderata, often also relying on additional technical assumptions.",
      "authors": [
        "Hanrui Zhang",
        "Yu Cheng",
        "Vincent Conitzer"
      ],
      "published": "2024-12-22T09:12:05Z",
      "updated": "2024-12-22T09:12:05Z",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.DS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.16934v1",
      "landing_url": "https://arxiv.org/abs/2412.16934v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.16934"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on algorithms for equilibrium computation in two-player turn-taking stochastic games and does not center on Spoken Language Models or speech-language modeling for interactive, full-duplex dialogue.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on algorithms for equilibrium computation in two-player turn-taking stochastic games and does not center on Spoken Language Models or speech-language modeling for interactive, full-duplex dialogue.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on computational algorithms for equilibria in two-player turn-taking stochastic games, which appears unrelated to the domain of Spoken Language Models or full-duplex spoken interaction systems; it neither discusses speech as a core modeling object nor addresses relevant architectures, training, evaluation, or real-time speech interaction capabilities required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on computational algorithms for equilibria in two-player turn-taking stochastic games, which appears unrelated to the domain of Spoken Language Models or full-duplex spoken interaction systems; it neither discusses speech as a core modeling object nor addresses relevant architectures, training, evaluation, or real-time speech interaction capabilities required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Why Do Speech Language Models Fail to Generate Semantically Coherent Outputs? A Modality Evolving Perspective",
    "abstract": "Although text-based large language models exhibit human-level writing ability and remarkable intelligence, speech language models (SLMs) still struggle to generate semantically coherent outputs. There are several potential reasons for this performance degradation: (A) speech tokens mainly provide phonetic information rather than semantic information, (B) the length of speech sequences is much longer than that of text sequences, and (C) paralinguistic information, such as prosody, introduces additional complexity and variability. In this paper, we explore the influence of three key factors separately by transiting the modality from text to speech in an evolving manner. Our findings reveal that the impact of the three factors varies. Factor A has a relatively minor impact, factor B influences syntactical and semantic modeling more obviously, and factor C exerts the most significant impact, particularly in the basic lexical modeling. Based on these findings, we provide insights into the unique challenges of training SLMs and highlight pathways to develop more effective end-to-end SLMs.",
    "metadata": {
      "arxiv_id": "2412.17048",
      "title": "Why Do Speech Language Models Fail to Generate Semantically Coherent Outputs? A Modality Evolving Perspective",
      "summary": "Although text-based large language models exhibit human-level writing ability and remarkable intelligence, speech language models (SLMs) still struggle to generate semantically coherent outputs. There are several potential reasons for this performance degradation: (A) speech tokens mainly provide phonetic information rather than semantic information, (B) the length of speech sequences is much longer than that of text sequences, and (C) paralinguistic information, such as prosody, introduces additional complexity and variability. In this paper, we explore the influence of three key factors separately by transiting the modality from text to speech in an evolving manner. Our findings reveal that the impact of the three factors varies. Factor A has a relatively minor impact, factor B influences syntactical and semantic modeling more obviously, and factor C exerts the most significant impact, particularly in the basic lexical modeling. Based on these findings, we provide insights into the unique challenges of training SLMs and highlight pathways to develop more effective end-to-end SLMs.",
      "authors": [
        "Hankun Wang",
        "Haoran Wang",
        "Yiwei Guo",
        "Zhihan Li",
        "Chenpeng Du",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-12-22T14:59:19Z",
      "updated": "2024-12-22T14:59:19Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.17048v1",
      "landing_url": "https://arxiv.org/abs/2412.17048v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.17048"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract center on Spoken Language Models (SLMs) with emphasis on architecture/training/evaluation of end-to-end SLMs, include 'speech language model(s)' in wording, appear within the last 3 years, and are not restricted to ASR/TTS or single-turn tasks, thus meeting inclusion criteria and not triggering exclusions.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The title/abstract center on Spoken Language Models (SLMs) with emphasis on architecture/training/evaluation of end-to-end SLMs, include 'speech language model(s)' in wording, appear within the last 3 years, and are not restricted to ASR/TTS or single-turn tasks, thus meeting inclusion criteria and not triggering exclusions.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on speech language models (SLMs) and explores challenges in semantic coherence related to speech token modeling and modality transitions; it addresses the core modeling of speech sequences rather than treating speech merely as input for text, includes a discussion on model training and underlying factors, and uses specific terminology matching the inclusion criteria. However, it does not explicitly mention full-duplex interaction, synchronization, or handling of turn-taking and speech overlap, nor does it describe any evaluation framework related to real-time interaction or behavior arbitration. Given that it addresses SLMs and their challenges in semantic modeling, it partially matches the topic scope but lacks coverage of the critical aspects of full-duplex or synchronized interaction and detailed evaluation frameworks as stipulated, leading to a somewhat ambiguous inclusion status.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The article focuses on speech language models (SLMs) and explores challenges in semantic coherence related to speech token modeling and modality transitions; it addresses the core modeling of speech sequences rather than treating speech merely as input for text, includes a discussion on model training and underlying factors, and uses specific terminology matching the inclusion criteria. However, it does not explicitly mention full-duplex interaction, synchronization, or handling of turn-taking and speech overlap, nor does it describe any evaluation framework related to real-time interaction or behavior arbitration. Given that it addresses SLMs and their challenges in semantic modeling, it partially matches the topic scope but lacks coverage of the critical aspects of full-duplex or synchronized interaction and detailed evaluation frameworks as stipulated, leading to a somewhat ambiguous inclusion status.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior reviewers' comments (one gave 4 noting the SLM focus; the other gave 3 expressing ambiguity about full‑duplex/interaction aspects). The title and abstract explicitly mention \"speech language models (SLMs)\", analyze modality transitions from text to speech, study core modeling/training issues (phonetic vs semantic tokens, long speech sequences, paralinguistic/prosody effects), and discuss directions for end-to-end SLMs — indicating speech is the primary modeling object and that architecture/training/evaluation concerns are addressed rather than being a simple ASR/TTS or text-first study. It does not trigger any exclusion (not limited to ASR/TTS, not only using speech as input for text, and discusses model-level challenges), so it should be included. One-sentence explanation: This paper clearly meets the inclusion criteria by explicitly addressing end-to-end speech language models and their training/analysis and does not fall under any exclusion.",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior reviewers' comments (one gave 4 noting the SLM focus; the other gave 3 expressing ambiguity about full‑duplex/interaction aspects). The title and abstract explicitly mention \"speech language models (SLMs)\", analyze modality transitions from text to speech, study core modeling/training issues (phonetic vs semantic tokens, long speech sequences, paralinguistic/prosody effects), and discuss directions for end-to-end SLMs — indicating speech is the primary modeling object and that architecture/training/evaluation concerns are addressed rather than being a simple ASR/TTS or text-first study. It does not trigger any exclusion (not limited to ASR/TTS, not only using speech as input for text, and discusses model-level challenges), so it should be included. One-sentence explanation: This paper clearly meets the inclusion criteria by explicitly addressing end-to-end speech language models and their training/analysis and does not fall under any exclusion.",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Analysis of Speech Temporal Dynamics in the Context of Speaker Verification and Voice Anonymization",
    "abstract": "In this paper, we investigate the impact of speech temporal dynamics in application to automatic speaker verification and speaker voice anonymization tasks. We propose several metrics to perform automatic speaker verification based only on phoneme durations. Experimental results demonstrate that phoneme durations leak some speaker information and can reveal speaker identity from both original and anonymized speech. Thus, this work emphasizes the importance of taking into account the speaker's speech rate and, more importantly, the speaker's phonetic duration characteristics, as well as the need to modify them in order to develop anonymization systems with strong privacy protection capacity.",
    "metadata": {
      "arxiv_id": "2412.17164",
      "title": "Analysis of Speech Temporal Dynamics in the Context of Speaker Verification and Voice Anonymization",
      "summary": "In this paper, we investigate the impact of speech temporal dynamics in application to automatic speaker verification and speaker voice anonymization tasks. We propose several metrics to perform automatic speaker verification based only on phoneme durations. Experimental results demonstrate that phoneme durations leak some speaker information and can reveal speaker identity from both original and anonymized speech. Thus, this work emphasizes the importance of taking into account the speaker's speech rate and, more importantly, the speaker's phonetic duration characteristics, as well as the need to modify them in order to develop anonymization systems with strong privacy protection capacity.",
      "authors": [
        "Natalia Tomashenko",
        "Emmanuel Vincent",
        "Marc Tommasi"
      ],
      "published": "2024-12-22T21:18:08Z",
      "updated": "2024-12-22T21:18:08Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.17164v1",
      "landing_url": "https://arxiv.org/abs/2412.17164v1",
      "doi": "https://doi.org/10.1109/ICASSP49660.2025.10887896"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not address Spoken Language Models or full-duplex interactive speech modeling; it's about ASV/voice anonymization metrics, not SLM-based architectures or conversational synchronization, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Does not address Spoken Language Models or full-duplex interactive speech modeling; it's about ASV/voice anonymization metrics, not SLM-based architectures or conversational synchronization, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on analyzing speech temporal dynamics for speaker verification and voice anonymization, without discussing Spoken Language Models or full-duplex interactive speech modeling architectures. It also lacks mention of model training, inference, or evaluation related to spoken language models, and does not address issues such as synchronous interaction, cognitive parallelism, or low-latency multi-turn dialogue capabilities, which are core to the inclusion criteria. Hence, it does not meet the necessary inclusion conditions and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on analyzing speech temporal dynamics for speaker verification and voice anonymization, without discussing Spoken Language Models or full-duplex interactive speech modeling architectures. It also lacks mention of model training, inference, or evaluation related to spoken language models, and does not address issues such as synchronous interaction, cognitive parallelism, or low-latency multi-turn dialogue capabilities, which are core to the inclusion criteria. Hence, it does not meet the necessary inclusion conditions and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "HPCNeuroNet: A Neuromorphic Approach Merging SNN Temporal Dynamics with Transformer Attention for FPGA-based Particle Physics",
    "abstract": "This paper presents the innovative HPCNeuroNet model, a pioneering fusion of Spiking Neural Networks (SNNs), Transformers, and high-performance computing tailored for particle physics, particularly in particle identification from detector responses. Our approach leverages SNNs' intrinsic temporal dynamics and Transformers' robust attention mechanisms to enhance performance when discerning intricate particle interactions. At the heart of HPCNeuroNet lies the integration of the sequential dynamism inherent in SNNs with the context-aware attention capabilities of Transformers, enabling the model to precisely decode and interpret complex detector data. HPCNeuroNet is realized through the HLS4ML framework and optimized for deployment in FPGA environments. The model accuracy and scalability are also enhanced by this architectural choice. Benchmarked against machine learning models, HPCNeuroNet showcases better performance metrics, underlining its transformative potential in high-energy physics. We demonstrate that the combination of SNNs, Transformers, and FPGA-based high-performance computing in particle physics signifies a significant step forward and provides a strong foundation for future research.",
    "metadata": {
      "arxiv_id": "2412.17571",
      "title": "HPCNeuroNet: A Neuromorphic Approach Merging SNN Temporal Dynamics with Transformer Attention for FPGA-based Particle Physics",
      "summary": "This paper presents the innovative HPCNeuroNet model, a pioneering fusion of Spiking Neural Networks (SNNs), Transformers, and high-performance computing tailored for particle physics, particularly in particle identification from detector responses. Our approach leverages SNNs' intrinsic temporal dynamics and Transformers' robust attention mechanisms to enhance performance when discerning intricate particle interactions. At the heart of HPCNeuroNet lies the integration of the sequential dynamism inherent in SNNs with the context-aware attention capabilities of Transformers, enabling the model to precisely decode and interpret complex detector data. HPCNeuroNet is realized through the HLS4ML framework and optimized for deployment in FPGA environments. The model accuracy and scalability are also enhanced by this architectural choice. Benchmarked against machine learning models, HPCNeuroNet showcases better performance metrics, underlining its transformative potential in high-energy physics. We demonstrate that the combination of SNNs, Transformers, and FPGA-based high-performance computing in particle physics signifies a significant step forward and provides a strong foundation for future research.",
      "authors": [
        "Murat Isik",
        "Hiruna Vishwamith",
        "Jonathan Naoukin",
        "I. Can Dikmen"
      ],
      "published": "2024-12-23T13:44:29Z",
      "updated": "2024-12-23T13:44:29Z",
      "categories": [
        "cs.LG",
        "cs.AR",
        "physics.comp-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.17571v1",
      "landing_url": "https://arxiv.org/abs/2412.17571v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.17571"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on neuromorphic SNN/Transformer fusion for FPGA-based particle physics; it does not involve Spoken Language Models or speech-language modeling, and does not discuss dialogue or real-time speech interaction.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on neuromorphic SNN/Transformer fusion for FPGA-based particle physics; it does not involve Spoken Language Models or speech-language modeling, and does not discuss dialogue or real-time speech interaction.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a neuromorphic model combining SNNs and Transformers for particle physics data analysis on FPGA, with no mention of spoken language models, speech as a core modeling object, full-duplex spoken interaction, or speech-language modeling aspects required by the inclusion criteria; thus, it does not meet the inclusion criteria and meets exclusion criteria of no relevance to spoken language or speech modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a neuromorphic model combining SNNs and Transformers for particle physics data analysis on FPGA, with no mention of spoken language models, speech as a core modeling object, full-duplex spoken interaction, or speech-language modeling aspects required by the inclusion criteria; thus, it does not meet the inclusion criteria and meets exclusion criteria of no relevance to spoken language or speech modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Lla-VAP: LSTM Ensemble of Llama and VAP for Turn-Taking Prediction",
    "abstract": "Turn-taking prediction is the task of anticipating when the speaker in a conversation will yield their turn to another speaker to begin speaking. This project expands on existing strategies for turn-taking prediction by employing a multi-modal ensemble approach that integrates large language models (LLMs) and voice activity projection (VAP) models. By combining the linguistic capabilities of LLMs with the temporal precision of VAP models, we aim to improve the accuracy and efficiency of identifying TRPs in both scripted and unscripted conversational scenarios. Our methods are evaluated on the In-Conversation Corpus (ICC) and Coached Conversational Preference Elicitation (CCPE) datasets, highlighting the strengths and limitations of current models while proposing a potentially more robust framework for enhanced prediction.",
    "metadata": {
      "arxiv_id": "2412.18061",
      "title": "Lla-VAP: LSTM Ensemble of Llama and VAP for Turn-Taking Prediction",
      "summary": "Turn-taking prediction is the task of anticipating when the speaker in a conversation will yield their turn to another speaker to begin speaking. This project expands on existing strategies for turn-taking prediction by employing a multi-modal ensemble approach that integrates large language models (LLMs) and voice activity projection (VAP) models. By combining the linguistic capabilities of LLMs with the temporal precision of VAP models, we aim to improve the accuracy and efficiency of identifying TRPs in both scripted and unscripted conversational scenarios. Our methods are evaluated on the In-Conversation Corpus (ICC) and Coached Conversational Preference Elicitation (CCPE) datasets, highlighting the strengths and limitations of current models while proposing a potentially more robust framework for enhanced prediction.",
      "authors": [
        "Hyunbae Jeon",
        "Frederic Guintu",
        "Rayvant Sahni"
      ],
      "published": "2024-12-24T00:20:38Z",
      "updated": "2024-12-24T00:20:38Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.HC",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.18061v1",
      "landing_url": "https://arxiv.org/abs/2412.18061v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.18061"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on turn-taking prediction using speech-related modeling (LLMs + VAP), aligning with the speech-language modeling theme and recent timeframe, but it does not explicitly mention 'spoken language model' or 'speech language model' in the title/abstract/keywords as required, so it fails the keyword-based inclusion criterion; hence exclusion.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item centers on turn-taking prediction using speech-related modeling (LLMs + VAP), aligning with the speech-language modeling theme and recent timeframe, but it does not explicitly mention 'spoken language model' or 'speech language model' in the title/abstract/keywords as required, so it fails the keyword-based inclusion criterion; hence exclusion.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on turn-taking prediction by integrating LLMs and voice activity projection models, involving both linguistic and voice activity information, which suggests a multi-modal approach related to spoken language interaction. However, the abstract does not explicitly mention spoken language models or speech language models as a core modeling target, nor does it clearly state that it addresses full-duplex spoken language models or synchronization mechanisms beyond turn-taking prediction. The evaluation datasets and the task relate to turn-taking but do not clarify if the work addresses real-time interaction aspects like cognitive parallelism, overlap, or low latency. Also, the approach appears more about a multi-modal ensemble rather than end-to-end or hybrid speech-language model architectures focusing on speech as the core modeling unit. Hence, it only partially fits the inclusion criteria and may be better excluded unless more details prove the core spoken language model aspect.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on turn-taking prediction by integrating LLMs and voice activity projection models, involving both linguistic and voice activity information, which suggests a multi-modal approach related to spoken language interaction. However, the abstract does not explicitly mention spoken language models or speech language models as a core modeling target, nor does it clearly state that it addresses full-duplex spoken language models or synchronization mechanisms beyond turn-taking prediction. The evaluation datasets and the task relate to turn-taking but do not clarify if the work addresses real-time interaction aspects like cognitive parallelism, overlap, or low latency. Also, the approach appears more about a multi-modal ensemble rather than end-to-end or hybrid speech-language model architectures focusing on speech as the core modeling unit. Hence, it only partially fits the inclusion criteria and may be better excluded unless more details prove the core spoken language model aspect.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "RDPM: Solve Diffusion Probabilistic Models via Recurrent Token Prediction",
    "abstract": "Diffusion Probabilistic Models (DPMs) have emerged as the de facto approach for high-fidelity image synthesis, operating diffusion processes on continuous VAE latent, which significantly differ from the text generation methods employed by Large Language Models (LLMs). In this paper, we introduce a novel generative framework, the Recurrent Diffusion Probabilistic Model (RDPM), which enhances the diffusion process through a recurrent token prediction mechanism, thereby pioneering the field of Discrete Diffusion. By progressively introducing Gaussian noise into the latent representations of images and encoding them into vector-quantized tokens in a recurrent manner, RDPM facilitates a unique diffusion process on discrete-value domains. This process iteratively predicts the token codes for subsequent timesteps, transforming the initial standard Gaussian noise into the source data distribution, aligning with GPT-style models in terms of the loss function. RDPM demonstrates superior performance while benefiting from the speed advantage of requiring only a few inference steps. This model not only leverages the diffusion process to ensure high-quality generation but also converts continuous signals into a series of high-fidelity discrete tokens, thereby maintaining a unified optimization strategy with other discrete tokens, such as text. We anticipate that this work will contribute to the development of a unified model for multimodal generation, specifically by integrating continuous signal domains such as images, videos, and audio with text. We will release the code and model weights to the open-source community.",
    "metadata": {
      "arxiv_id": "2412.18390",
      "title": "RDPM: Solve Diffusion Probabilistic Models via Recurrent Token Prediction",
      "summary": "Diffusion Probabilistic Models (DPMs) have emerged as the de facto approach for high-fidelity image synthesis, operating diffusion processes on continuous VAE latent, which significantly differ from the text generation methods employed by Large Language Models (LLMs). In this paper, we introduce a novel generative framework, the Recurrent Diffusion Probabilistic Model (RDPM), which enhances the diffusion process through a recurrent token prediction mechanism, thereby pioneering the field of Discrete Diffusion. By progressively introducing Gaussian noise into the latent representations of images and encoding them into vector-quantized tokens in a recurrent manner, RDPM facilitates a unique diffusion process on discrete-value domains. This process iteratively predicts the token codes for subsequent timesteps, transforming the initial standard Gaussian noise into the source data distribution, aligning with GPT-style models in terms of the loss function. RDPM demonstrates superior performance while benefiting from the speed advantage of requiring only a few inference steps. This model not only leverages the diffusion process to ensure high-quality generation but also converts continuous signals into a series of high-fidelity discrete tokens, thereby maintaining a unified optimization strategy with other discrete tokens, such as text. We anticipate that this work will contribute to the development of a unified model for multimodal generation, specifically by integrating continuous signal domains such as images, videos, and audio with text. We will release the code and model weights to the open-source community.",
      "authors": [
        "Xiaoping Wu",
        "Jie Hu",
        "Xiaoming Wei"
      ],
      "published": "2024-12-24T12:28:19Z",
      "updated": "2024-12-25T12:12:10Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.18390v2",
      "landing_url": "https://arxiv.org/abs/2412.18390v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.18390"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets image diffusion and discrete token generation rather than Spoken Language Models or speech as the modeling target, with no focus on speech interactions or evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets image diffusion and discrete token generation rather than Spoken Language Models or speech as the modeling target, with no focus on speech interactions or evaluation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on diffusion probabilistic models for image synthesis using discrete tokens and recurrent token prediction without any mention of spoken language modeling, speech-based language modeling, or synchronization of full-duplex spoken interaction; it does not meet the inclusion criteria of focusing on spoken language models with speech as the core modeling object, nor addressing full-duplex speech interaction or related evaluation metrics.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on diffusion probabilistic models for image synthesis using discrete tokens and recurrent token prediction without any mention of spoken language modeling, speech-based language modeling, or synchronization of full-duplex spoken interaction; it does not meet the inclusion criteria of focusing on spoken language models with speech as the core modeling object, nor addressing full-duplex speech interaction or related evaluation metrics.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Long-Form Speech Generation with Spoken Language Models",
    "abstract": "We consider the generative modeling of speech over multiple minutes, a requirement for long-form multimedia generation and audio-native voice assistants. However, textless spoken language models struggle to generate plausible speech past tens of seconds, due to high temporal resolution of speech tokens causing loss of coherence, architectural issues with long-sequence training or extrapolation, and memory costs at inference time. From these considerations we derive SpeechSSM, the first speech language model family to learn from and sample long-form spoken audio (e.g., 16 minutes of read or extemporaneous speech) in a single decoding session without text intermediates. SpeechSSMs leverage recent advances in linear-time sequence modeling to greatly surpass current Transformer spoken LMs in coherence and efficiency on multi-minute generations while still matching them at the utterance level. As we found current spoken language evaluations uninformative, especially in this new long-form setting, we also introduce: LibriSpeech-Long, a benchmark for long-form speech evaluation; new embedding-based and LLM-judged metrics; and quality measurements over length and time. Speech samples, the LibriSpeech-Long dataset, and any future code or model releases can be found at https://google.github.io/tacotron/publications/speechssm/.",
    "metadata": {
      "arxiv_id": "2412.18603",
      "title": "Long-Form Speech Generation with Spoken Language Models",
      "summary": "We consider the generative modeling of speech over multiple minutes, a requirement for long-form multimedia generation and audio-native voice assistants. However, textless spoken language models struggle to generate plausible speech past tens of seconds, due to high temporal resolution of speech tokens causing loss of coherence, architectural issues with long-sequence training or extrapolation, and memory costs at inference time. From these considerations we derive SpeechSSM, the first speech language model family to learn from and sample long-form spoken audio (e.g., 16 minutes of read or extemporaneous speech) in a single decoding session without text intermediates. SpeechSSMs leverage recent advances in linear-time sequence modeling to greatly surpass current Transformer spoken LMs in coherence and efficiency on multi-minute generations while still matching them at the utterance level. As we found current spoken language evaluations uninformative, especially in this new long-form setting, we also introduce: LibriSpeech-Long, a benchmark for long-form speech evaluation; new embedding-based and LLM-judged metrics; and quality measurements over length and time. Speech samples, the LibriSpeech-Long dataset, and any future code or model releases can be found at https://google.github.io/tacotron/publications/speechssm/.",
      "authors": [
        "Se Jin Park",
        "Julian Salazar",
        "Aren Jansen",
        "Keisuke Kinoshita",
        "Yong Man Ro",
        "RJ Skerry-Ryan"
      ],
      "published": "2024-12-24T18:56:46Z",
      "updated": "2025-07-10T17:52:43Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.18603v2",
      "landing_url": "https://arxiv.org/abs/2412.18603v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.18603"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此项标题与摘要明确以 Spoken Language Models/Speech Language Models 为核心，讨论长时域语音生成、无文本中介、架构与评估，并提出新的基准和评估指标，且在过去3年公开，符合包含条件且非排除项。",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "此项标题与摘要明确以 Spoken Language Models/Speech Language Models 为核心，讨论长时域语音生成、无文本中介、架构与评估，并提出新的基准和评估指标，且在过去3年公开，符合包含条件且非排除项。",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on spoken language models that generate long-form speech directly from speech tokens rather than text intermediates, addressing architectural and modeling challenges at speech token level, which aligns with core spoken language modeling research. However, it does not explicitly discuss full-duplex interaction capabilities, cognitive parallelism, or multi-dimensional behavioral arbitration such as turn-taking, interruption, or low-latency interaction, which are emphasized in the inclusion criteria. Moreover, the evaluation introduces new benchmarks and metrics beyond ASR and audio quality, indicating some advancement in evaluating speech modeling coherence. Given the clear relevance to speech language modeling but partial alignment with the interactive or synchronization focus, it is better to include the study for its contribution to speech token-based language modeling and long-form generation, despite lacking coverage of full-duplex or real-time interaction behaviors.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The study focuses on spoken language models that generate long-form speech directly from speech tokens rather than text intermediates, addressing architectural and modeling challenges at speech token level, which aligns with core spoken language modeling research. However, it does not explicitly discuss full-duplex interaction capabilities, cognitive parallelism, or multi-dimensional behavioral arbitration such as turn-taking, interruption, or low-latency interaction, which are emphasized in the inclusion criteria. Moreover, the evaluation introduces new benchmarks and metrics beyond ASR and audio quality, indicating some advancement in evaluating speech modeling coherence. Given the clear relevance to speech language modeling but partial alignment with the interactive or synchronization focus, it is better to include the study for its contribution to speech token-based language modeling and long-form generation, despite lacking coverage of full-duplex or real-time interaction behaviors.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Speech Recognition With LLMs Adapted to Disordered Speech Using Reinforcement Learning",
    "abstract": "We introduce a large language model (LLM) capable of processing speech inputs and show that tuning it further with reinforcement learning on human preference (RLHF) enables it to adapt better to disordered speech than traditional fine-tuning. Our method replaces low-frequency text tokens in an LLM's vocabulary with audio tokens and enables the model to recognize speech by fine-tuning it on speech with transcripts. We then use RL with rewards based on syntactic and semantic accuracy measures generalizing the LLM further to recognize disordered speech. While the resulting LLM does not outperform existing systems for speech recognition, we find that tuning with reinforcement learning using custom rewards leads to substantially better performance than supervised fine-tuning of the language model, specifically when adapting to speech in a different setting. This presents a compelling alternative tuning strategy for speech recognition using large language models.",
    "metadata": {
      "arxiv_id": "2501.00039",
      "title": "Speech Recognition With LLMs Adapted to Disordered Speech Using Reinforcement Learning",
      "summary": "We introduce a large language model (LLM) capable of processing speech inputs and show that tuning it further with reinforcement learning on human preference (RLHF) enables it to adapt better to disordered speech than traditional fine-tuning. Our method replaces low-frequency text tokens in an LLM's vocabulary with audio tokens and enables the model to recognize speech by fine-tuning it on speech with transcripts. We then use RL with rewards based on syntactic and semantic accuracy measures generalizing the LLM further to recognize disordered speech. While the resulting LLM does not outperform existing systems for speech recognition, we find that tuning with reinforcement learning using custom rewards leads to substantially better performance than supervised fine-tuning of the language model, specifically when adapting to speech in a different setting. This presents a compelling alternative tuning strategy for speech recognition using large language models.",
      "authors": [
        "Chirag Nagpal",
        "Subhashini Venugopalan",
        "Jimmy Tobin",
        "Marilyn Ladewig",
        "Katherine Heller",
        "Katrin Tomanek"
      ],
      "published": "2024-12-25T00:16:22Z",
      "updated": "2024-12-25T00:16:22Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.00039v1",
      "landing_url": "https://arxiv.org/abs/2501.00039v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.00039"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on a Spoken Language Model that processes speech inputs via audio tokens, employs RLHF training to adapt to disordered speech, and discusses architecture/training/evaluation beyond pure ASR, within the last 3 years, thus meeting inclusion and not falling under exclusion.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "This item centers on a Spoken Language Model that processes speech inputs via audio tokens, employs RLHF training to adapt to disordered speech, and discusses architecture/training/evaluation beyond pure ASR, within the last 3 years, thus meeting inclusion and not falling under exclusion.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on adapting large language models to disordered speech using reinforcement learning and involves replacing low-frequency text tokens with audio tokens, which indicates some level of speech-level modeling; however, the abstract does not explicitly mention full-duplex interactive capabilities, cognitive parallelism, or synchronized multi-dimensional evaluation as specified in the inclusion criteria. Furthermore, the emphasis appears to be on ASR improvement via LLM fine-tuning rather than on spoken language models as a core modeling and reasoning object or on dialogue behavior arbitration and synchronous interaction. Therefore, while it partially fits the topic, it lacks key aspects required for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on adapting large language models to disordered speech using reinforcement learning and involves replacing low-frequency text tokens with audio tokens, which indicates some level of speech-level modeling; however, the abstract does not explicitly mention full-duplex interactive capabilities, cognitive parallelism, or synchronized multi-dimensional evaluation as specified in the inclusion criteria. Furthermore, the emphasis appears to be on ASR improvement via LLM fine-tuning rather than on spoken language models as a core modeling and reasoning object or on dialogue behavior arbitration and synchronous interaction. Therefore, while it partially fits the topic, it lacks key aspects required for inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior reviewers' comments and the abstract: the paper adapts an LLM to accept audio tokens and uses RLHF to improve recognition of disordered speech, which shows some speech-level modeling innovation (audio tokens) and adaptation experiments; however the abstract does not frame the work as a Spoken Language Model or speech-language modeling research in the sense required by the inclusion criteria, nor does it address full‑duplex interaction, synchronization/real‑time behavior, cognitive parallelism, or multi-dimensional evaluation—instead it reads as an ASR-focused adaptation study that uses an LLM for recognition and reports improvements over supervised fine‑tuning. Given the exclusion rules (studies that are simply ASR-focused or that merely use speech as input while keeping the core NLP focus on text should be excluded), this item fails to meet the requirement of centering SLM/FD‑SLM research and synchronous interaction aspects. Therefore I agree more with the cautious junior review: better to exclude.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior reviewers' comments and the abstract: the paper adapts an LLM to accept audio tokens and uses RLHF to improve recognition of disordered speech, which shows some speech-level modeling innovation (audio tokens) and adaptation experiments; however the abstract does not frame the work as a Spoken Language Model or speech-language modeling research in the sense required by the inclusion criteria, nor does it address full‑duplex interaction, synchronization/real‑time behavior, cognitive parallelism, or multi-dimensional evaluation—instead it reads as an ASR-focused adaptation study that uses an LLM for recognition and reports improvements over supervised fine‑tuning. Given the exclusion rules (studies that are simply ASR-focused or that merely use speech as input while keeping the core NLP focus on text should be excluded), this item fails to meet the requirement of centering SLM/FD‑SLM research and synchronous interaction aspects. Therefore I agree more with the cautious junior review: better to exclude.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "\"Generative Models for Financial Time Series Data: Enhancing Signal-to-Noise Ratio and Addressing Data Scarcity in A-Share Market",
    "abstract": "The financial industry is increasingly seeking robust methods to address the challenges posed by data scarcity and low signal-to-noise ratios, which limit the application of deep learning techniques in stock market analysis. This paper presents two innovative generative model-based approaches to synthesize stock data, specifically tailored for different scenarios within the A-share market in China. The first method, a sector-based synthesis approach, enhances the signal-to-noise ratio of stock data by classifying the characteristics of stocks from various sectors in China's A-share market. This method employs an Approximate Non-Local Total Variation algorithm to smooth the generated data, a bandpass filtering method based on Fourier Transform to eliminate noise, and Denoising Diffusion Implicit Models to accelerate sampling speed. The second method, a recursive stock data synthesis approach based on pattern recognition, is designed to synthesize data for stocks with short listing periods and limited comparable companies. It leverages pattern recognition techniques and Markov models to learn and generate variable-length stock sequences, while introducing a sub-time-level data augmentation method to alleviate data scarcity issues.We validate the effectiveness of these methods through extensive experiments on various datasets, including those from the main board, STAR Market, Growth Enterprise Market Board, Beijing Stock Exchange, NASDAQ, NYSE, and AMEX. The results demonstrate that our synthesized data not only improve the performance of predictive models but also enhance the signal-to-noise ratio of individual stock signals in price trading strategies. Furthermore, the introduction of sub-time-level data significantly improves the quality of synthesized data.",
    "metadata": {
      "arxiv_id": "2501.00063",
      "title": "\"Generative Models for Financial Time Series Data: Enhancing Signal-to-Noise Ratio and Addressing Data Scarcity in A-Share Market",
      "summary": "The financial industry is increasingly seeking robust methods to address the challenges posed by data scarcity and low signal-to-noise ratios, which limit the application of deep learning techniques in stock market analysis. This paper presents two innovative generative model-based approaches to synthesize stock data, specifically tailored for different scenarios within the A-share market in China. The first method, a sector-based synthesis approach, enhances the signal-to-noise ratio of stock data by classifying the characteristics of stocks from various sectors in China's A-share market. This method employs an Approximate Non-Local Total Variation algorithm to smooth the generated data, a bandpass filtering method based on Fourier Transform to eliminate noise, and Denoising Diffusion Implicit Models to accelerate sampling speed. The second method, a recursive stock data synthesis approach based on pattern recognition, is designed to synthesize data for stocks with short listing periods and limited comparable companies. It leverages pattern recognition techniques and Markov models to learn and generate variable-length stock sequences, while introducing a sub-time-level data augmentation method to alleviate data scarcity issues.We validate the effectiveness of these methods through extensive experiments on various datasets, including those from the main board, STAR Market, Growth Enterprise Market Board, Beijing Stock Exchange, NASDAQ, NYSE, and AMEX. The results demonstrate that our synthesized data not only improve the performance of predictive models but also enhance the signal-to-noise ratio of individual stock signals in price trading strategies. Furthermore, the introduction of sub-time-level data significantly improves the quality of synthesized data.",
      "authors": [
        "Guangming Che"
      ],
      "published": "2024-12-29T09:35:23Z",
      "updated": "2024-12-29T09:35:23Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.00063v1",
      "landing_url": "https://arxiv.org/abs/2501.00063v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.00063"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on generative models for financial time series data in stock markets and does not address Spoken Language Models or any speech-based modeling, thus it fails to meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on generative models for financial time series data in stock markets and does not address Spoken Language Models or any speech-based modeling, thus it fails to meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on generative models for financial time series data in stock markets, which does not relate to Spoken Language Models or speech as a core modeling target, nor discusses model architectures or evaluations in the speech domain relevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on generative models for financial time series data in stock markets, which does not relate to Spoken Language Models or speech as a core modeling target, nor discusses model architectures or evaluations in the speech domain relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Proactive Conversational Agents with Inner Thoughts",
    "abstract": "One of the long-standing aspirations in conversational AI is to allow them to autonomously take initiatives in conversations, i.e., being proactive. This is especially challenging for multi-party conversations. Prior NLP research focused mainly on predicting the next speaker from contexts like preceding conversations. In this paper, we demonstrate the limitations of such methods and rethink what it means for AI to be proactive in multi-party, human-AI conversations. We propose that just like humans, rather than merely reacting to turn-taking cues, a proactive AI formulates its own inner thoughts during a conversation, and seeks the right moment to contribute. Through a formative study with 24 participants and inspiration from linguistics and cognitive psychology, we introduce the Inner Thoughts framework. Our framework equips AI with a continuous, covert train of thoughts in parallel to the overt communication process, which enables it to proactively engage by modeling its intrinsic motivation to express these thoughts. We instantiated this framework into two real-time systems: an AI playground web app and a chatbot. Through a technical evaluation and user studies with human participants, our framework significantly surpasses existing baselines on aspects like anthropomorphism, coherence, intelligence, and turn-taking appropriateness.",
    "metadata": {
      "arxiv_id": "2501.00383",
      "title": "Proactive Conversational Agents with Inner Thoughts",
      "summary": "One of the long-standing aspirations in conversational AI is to allow them to autonomously take initiatives in conversations, i.e., being proactive. This is especially challenging for multi-party conversations. Prior NLP research focused mainly on predicting the next speaker from contexts like preceding conversations. In this paper, we demonstrate the limitations of such methods and rethink what it means for AI to be proactive in multi-party, human-AI conversations. We propose that just like humans, rather than merely reacting to turn-taking cues, a proactive AI formulates its own inner thoughts during a conversation, and seeks the right moment to contribute. Through a formative study with 24 participants and inspiration from linguistics and cognitive psychology, we introduce the Inner Thoughts framework. Our framework equips AI with a continuous, covert train of thoughts in parallel to the overt communication process, which enables it to proactively engage by modeling its intrinsic motivation to express these thoughts. We instantiated this framework into two real-time systems: an AI playground web app and a chatbot. Through a technical evaluation and user studies with human participants, our framework significantly surpasses existing baselines on aspects like anthropomorphism, coherence, intelligence, and turn-taking appropriateness.",
      "authors": [
        "Xingyu Bruce Liu",
        "Shitao Fang",
        "Weiyan Shi",
        "Chien-Sheng Wu",
        "Takeo Igarashi",
        "Xiang Anthony Chen"
      ],
      "published": "2024-12-31T10:41:56Z",
      "updated": "2025-02-18T08:53:06Z",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.00383v2",
      "landing_url": "https://arxiv.org/abs/2501.00383v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.00383"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes proactive conversational agents with inner thoughts but does not center on Spoken Language Models or speech-based modeling; it appears text-based dialogue rather than SLM/FD-SLM with speech, cognitive parallelism, or speech-specific evaluation, thus failing the core inclusion criteria despite being recent.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item describes proactive conversational agents with inner thoughts but does not center on Spoken Language Models or speech-based modeling; it appears text-based dialogue rather than SLM/FD-SLM with speech, cognitive parallelism, or speech-specific evaluation, thus failing the core inclusion criteria despite being recent.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on proactive conversational agents in multi-party human-AI dialogues but does not emphasize spoken language models or modeling acoustic speech signals as core components; it does not discuss speech-level modeling, full-duplex interaction, or low-latency speech overlapped interaction, nor does it mention speech language models or equivalent terms in the title or abstract. The system seems more centered on dialog management and inner thoughts generation rather than speech signal modeling or end-to-end spoken language modeling architectures.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on proactive conversational agents in multi-party human-AI dialogues but does not emphasize spoken language models or modeling acoustic speech signals as core components; it does not discuss speech-level modeling, full-duplex interaction, or low-latency speech overlapped interaction, nor does it mention speech language models or equivalent terms in the title or abstract. The system seems more centered on dialog management and inner thoughts generation rather than speech signal modeling or end-to-end spoken language modeling architectures.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SLIDE: Integrating Speech Language Model with LLM for Spontaneous Spoken Dialogue Generation",
    "abstract": "Recently, ``textless\" speech language models (SLMs) based on speech units have made huge progress in generating naturalistic speech, including non-verbal vocalizations. However, the generated speech samples often lack semantic coherence. In this paper, we propose SLM and LLM Integration for spontaneous spoken Dialogue gEneration (SLIDE). Specifically, we first utilize an LLM to generate the textual content of spoken dialogue. Next, we convert the textual dialogues into phoneme sequences and use a two-tower transformer-based duration predictor to predict the duration of each phoneme. Finally, an SLM conditioned on the spoken phoneme sequences is used to vocalize the textual dialogue. Experimental results on the Fisher dataset demonstrate that our system can generate naturalistic spoken dialogue while maintaining high semantic coherence.",
    "metadata": {
      "arxiv_id": "2501.00805",
      "title": "SLIDE: Integrating Speech Language Model with LLM for Spontaneous Spoken Dialogue Generation",
      "summary": "Recently, ``textless\" speech language models (SLMs) based on speech units have made huge progress in generating naturalistic speech, including non-verbal vocalizations. However, the generated speech samples often lack semantic coherence. In this paper, we propose SLM and LLM Integration for spontaneous spoken Dialogue gEneration (SLIDE). Specifically, we first utilize an LLM to generate the textual content of spoken dialogue. Next, we convert the textual dialogues into phoneme sequences and use a two-tower transformer-based duration predictor to predict the duration of each phoneme. Finally, an SLM conditioned on the spoken phoneme sequences is used to vocalize the textual dialogue. Experimental results on the Fisher dataset demonstrate that our system can generate naturalistic spoken dialogue while maintaining high semantic coherence.",
      "authors": [
        "Haitian Lu",
        "Gaofeng Cheng",
        "Liuping Luo",
        "Leying Zhang",
        "Yanmin Qian",
        "Pengyuan Zhang"
      ],
      "published": "2025-01-01T11:11:07Z",
      "updated": "2025-01-01T11:11:07Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.00805v1",
      "landing_url": "https://arxiv.org/abs/2501.00805v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.00805"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "該論文以 Spoken Language Model (SLM) 為核心，結合大型語言模型 (LLM) 進行語音對話產生，透過音素序列與時長預測等構成以語音層級為主的混合式語言-語音模型架構，並在 Fisher 資料集上進行評估，標題/摘要含有 spoken language model(s)/speech language model(s)，且發表/公開時間在近三年內，符合「以語音為主要建模對象、可被明確評估」的 inclusion 條件，且未呈現僅聚焦單一語音技術或僅文本處理的排除要素，因此很適合納入初篩。",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "該論文以 Spoken Language Model (SLM) 為核心，結合大型語言模型 (LLM) 進行語音對話產生，透過音素序列與時長預測等構成以語音層級為主的混合式語言-語音模型架構，並在 Fisher 資料集上進行評估，標題/摘要含有 spoken language model(s)/speech language model(s)，且發表/公開時間在近三年內，符合「以語音為主要建模對象、可被明確評估」的 inclusion 條件，且未呈現僅聚焦單一語音技術或僅文本處理的排除要素，因此很適合納入初篩。",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study clearly focuses on Spoken Language Models (SLMs) integrating speech and language models, addressing spoken dialogue generation which involves speech sequence modeling and semantic coherence. However, it does not explicitly mention full-duplex capabilities, cognitive parallelism, low latency, or synchronization mechanisms crucial for synchronous spoken dialogue interaction. The evaluation is on naturalistic spoken dialogue generation quality rather than real-time interaction metrics or synchronization. It meets many inclusion criteria but lacks explicit coverage of full-duplex interactive dialogue or synchronous turn-taking behavior arbitration, which are central to the topic's scope.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study clearly focuses on Spoken Language Models (SLMs) integrating speech and language models, addressing spoken dialogue generation which involves speech sequence modeling and semantic coherence. However, it does not explicitly mention full-duplex capabilities, cognitive parallelism, low latency, or synchronization mechanisms crucial for synchronous spoken dialogue interaction. The evaluation is on naturalistic spoken dialogue generation quality rather than real-time interaction metrics or synchronization. It meets many inclusion criteria but lacks explicit coverage of full-duplex interactive dialogue or synchronous turn-taking behavior arbitration, which are central to the topic's scope.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers split: one judged it fully includable (5) because the work explicitly centers on Spoken/Speech Language Models and evaluates a speech-centric generation pipeline; the other was cautious (3) noting the abstract lacks any mention of full‑duplex/synchronous interaction, overlap handling, or latency measures. My assessment: the title/abstract explicitly mention speech language models (SLMs), describe speech‑centric modeling (speech units, phoneme sequences, duration predictor) and a hybrid Speech–LLM pipeline, and report evaluation on the Fisher dataset — so it meets the core inclusion requirements that the paper treat speech as the primary modeling object and present methods and evaluation. However, the abstract does not address full‑duplex interaction, synchronization, overlap/interruption handling, or low‑latency/real‑time evaluation; while that does not violate the inclusion criteria (which require SLM focus), it means the paper does not cover the FD‑SLM / synchronization aspects emphasized in the topic. Therefore I recommend inclusion for the SLM-focused collection but flag it as not addressing full‑duplex/synchronization research questions. Score justification (one sentence): I give a 4 because the study clearly meets the core SLM and speech‑modeling inclusion criteria but does not tackle full‑duplex/synchronous interaction aspects.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers split: one judged it fully includable (5) because the work explicitly centers on Spoken/Speech Language Models and evaluates a speech-centric generation pipeline; the other was cautious (3) noting the abstract lacks any mention of full‑duplex/synchronous interaction, overlap handling, or latency measures. My assessment: the title/abstract explicitly mention speech language models (SLMs), describe speech‑centric modeling (speech units, phoneme sequences, duration predictor) and a hybrid Speech–LLM pipeline, and report evaluation on the Fisher dataset — so it meets the core inclusion requirements that the paper treat speech as the primary modeling object and present methods and evaluation. However, the abstract does not address full‑duplex interaction, synchronization, overlap/interruption handling, or low‑latency/real‑time evaluation; while that does not violate the inclusion criteria (which require SLM focus), it means the paper does not cover the FD‑SLM / synchronization aspects emphasized in the topic. Therefore I recommend inclusion for the SLM-focused collection but flag it as not addressing full‑duplex/synchronization research questions. Score justification (one sentence): I give a 4 because the study clearly meets the core SLM and speech‑modeling inclusion criteria but does not tackle full‑duplex/synchronous interaction aspects.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Decoupling Knowledge and Reasoning in Transformers: A Modular Architecture with Generalized Cross-Attention",
    "abstract": "Transformers have achieved remarkable success across diverse domains, but their monolithic architecture presents challenges in interpretability, adaptability, and scalability. This paper introduces a novel modular Transformer architecture that explicitly decouples knowledge and reasoning through a generalized cross-attention mechanism to a globally shared knowledge base with layer-specific transformations, specifically designed for effective knowledge retrieval. Critically, we provide a rigorous mathematical derivation demonstrating that the Feed-Forward Network (FFN) in a standard Transformer is a specialized case (a closure) of this generalized cross-attention, revealing its role in implicit knowledge retrieval and validating our design. This theoretical framework provides a new lens for understanding FFNs and lays the foundation for future research exploring enhanced interpretability, adaptability, and scalability, enabling richer interplay with external knowledge bases and other systems.",
    "metadata": {
      "arxiv_id": "2501.00823",
      "title": "Decoupling Knowledge and Reasoning in Transformers: A Modular Architecture with Generalized Cross-Attention",
      "summary": "Transformers have achieved remarkable success across diverse domains, but their monolithic architecture presents challenges in interpretability, adaptability, and scalability. This paper introduces a novel modular Transformer architecture that explicitly decouples knowledge and reasoning through a generalized cross-attention mechanism to a globally shared knowledge base with layer-specific transformations, specifically designed for effective knowledge retrieval. Critically, we provide a rigorous mathematical derivation demonstrating that the Feed-Forward Network (FFN) in a standard Transformer is a specialized case (a closure) of this generalized cross-attention, revealing its role in implicit knowledge retrieval and validating our design. This theoretical framework provides a new lens for understanding FFNs and lays the foundation for future research exploring enhanced interpretability, adaptability, and scalability, enabling richer interplay with external knowledge bases and other systems.",
      "authors": [
        "Zhenyu Guo",
        "Wenguang Chen"
      ],
      "published": "2025-01-01T12:55:57Z",
      "updated": "2025-01-06T14:26:41Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.00823v2",
      "landing_url": "https://arxiv.org/abs/2501.00823v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.00823"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses a modular Transformer architecture for knowledge retrieval, not Spoken Language Models or speech-centric modeling; it lacks a focus on speech as the modeling object or on dialogue/interactive speech evaluation, so it fails the core inclusion criteria and meets an exclusion based on topic scope.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses a modular Transformer architecture for knowledge retrieval, not Spoken Language Models or speech-centric modeling; it lacks a focus on speech as the modeling object or on dialogue/interactive speech evaluation, so it fails the core inclusion criteria and meets an exclusion based on topic scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a novel modular Transformer architecture for decoupling knowledge and reasoning, with emphasis on theoretical framework and Transformer internals, but do not discuss spoken language models, speech as core data, speech-language modeling, full-duplex interaction, or other inclusion criteria related to spoken language and interactive speech modeling; thus, it does not meet the inclusion criteria and misses the core topic focused on speech modeling or dialogue synchronization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a novel modular Transformer architecture for decoupling knowledge and reasoning, with emphasis on theoretical framework and Transformer internals, but do not discuss spoken language models, speech as core data, speech-language modeling, full-duplex interaction, or other inclusion criteria related to spoken language and interactive speech modeling; thus, it does not meet the inclusion criteria and misses the core topic focused on speech modeling or dialogue synchronization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MuQ: Self-Supervised Music Representation Learning with Mel Residual Vector Quantization",
    "abstract": "Recent years have witnessed the success of foundation models pre-trained with self-supervised learning (SSL) in various music informatics understanding tasks, including music tagging, instrument classification, key detection, and more. In this paper, we propose a self-supervised music representation learning model for music understanding. Distinguished from previous studies adopting random projection or existing neural codec, the proposed model, named MuQ, is trained to predict tokens generated by Mel Residual Vector Quantization (Mel-RVQ). Our Mel-RVQ utilizes residual linear projection structure for Mel spectrum quantization to enhance the stability and efficiency of target extraction and lead to better performance. Experiments in a large variety of downstream tasks demonstrate that MuQ outperforms previous self-supervised music representation models with only 0.9K hours of open-source pre-training data. Scaling up the data to over 160K hours and adopting iterative training consistently improve the model performance. To further validate the strength of our model, we present MuQ-MuLan, a joint music-text embedding model based on contrastive learning, which achieves state-of-the-art performance in the zero-shot music tagging task on the MagnaTagATune dataset. Code and checkpoints are open source in https://github.com/tencent-ailab/MuQ.",
    "metadata": {
      "arxiv_id": "2501.01108",
      "title": "MuQ: Self-Supervised Music Representation Learning with Mel Residual Vector Quantization",
      "summary": "Recent years have witnessed the success of foundation models pre-trained with self-supervised learning (SSL) in various music informatics understanding tasks, including music tagging, instrument classification, key detection, and more. In this paper, we propose a self-supervised music representation learning model for music understanding. Distinguished from previous studies adopting random projection or existing neural codec, the proposed model, named MuQ, is trained to predict tokens generated by Mel Residual Vector Quantization (Mel-RVQ). Our Mel-RVQ utilizes residual linear projection structure for Mel spectrum quantization to enhance the stability and efficiency of target extraction and lead to better performance. Experiments in a large variety of downstream tasks demonstrate that MuQ outperforms previous self-supervised music representation models with only 0.9K hours of open-source pre-training data. Scaling up the data to over 160K hours and adopting iterative training consistently improve the model performance. To further validate the strength of our model, we present MuQ-MuLan, a joint music-text embedding model based on contrastive learning, which achieves state-of-the-art performance in the zero-shot music tagging task on the MagnaTagATune dataset. Code and checkpoints are open source in https://github.com/tencent-ailab/MuQ.",
      "authors": [
        "Haina Zhu",
        "Yizhi Zhou",
        "Hangting Chen",
        "Jianwei Yu",
        "Ziyang Ma",
        "Rongzhi Gu",
        "Yi Luo",
        "Wei Tan",
        "Xie Chen"
      ],
      "published": "2025-01-02T07:08:29Z",
      "updated": "2025-01-03T08:35:34Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.01108v2",
      "landing_url": "https://arxiv.org/abs/2501.01108v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.01108"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on self-supervised music representation (MuQ) and Mel-RVQ, not on Spoken Language Models or speech-centered modeling, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on self-supervised music representation (MuQ) and Mel-RVQ, not on Spoken Language Models or speech-centered modeling, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised music representation learning, specifically for music understanding tasks, and does not address spoken language models or full-duplex spoken language interaction involving speech sequence modeling as defined in the inclusion criteria. It lacks discussion on spoken language model architectures, training, or evaluation for synchronous speech interaction or cognitive parallelism, and does not align with the focused theme of spoken language modeling and interaction specified in the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised music representation learning, specifically for music understanding tasks, and does not address spoken language models or full-duplex spoken language interaction involving speech sequence modeling as defined in the inclusion criteria. It lacks discussion on spoken language model architectures, training, or evaluation for synchronous speech interaction or cognitive parallelism, and does not align with the focused theme of spoken language modeling and interaction specified in the criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Exploiting Latent Properties to Optimize Neural Codecs",
    "abstract": "End-to-end image and video codecs are becoming increasingly competitive, compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques, such as their straightforward adaptation to perceptual distortion metrics and high performance in specific fields thanks to their learning ability. However, current state-of-the-art neural codecs do not fully exploit the benefits of vector quantization and the existence of the entropy gradient in decoding devices. In this paper, we propose to leverage these two properties (vector quantization and entropy gradient) to improve the performance of off-the-shelf codecs. Firstly, we demonstrate that using non-uniform scalar quantization cannot improve performance over uniform quantization. We thus suggest using predefined optimal uniform vector quantization to improve performance. Secondly, we show that the entropy gradient, available at the decoder, is correlated with the reconstruction error gradient, which is not available at the decoder. We therefore use the former as a proxy to enhance compression performance. Our experimental results show that these approaches save between 1 to 3% of the rate for the same quality across various pretrained methods. In addition, the entropy gradient based solution improves traditional codec performance significantly as well.",
    "metadata": {
      "arxiv_id": "2501.01231",
      "title": "Exploiting Latent Properties to Optimize Neural Codecs",
      "summary": "End-to-end image and video codecs are becoming increasingly competitive, compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques, such as their straightforward adaptation to perceptual distortion metrics and high performance in specific fields thanks to their learning ability. However, current state-of-the-art neural codecs do not fully exploit the benefits of vector quantization and the existence of the entropy gradient in decoding devices. In this paper, we propose to leverage these two properties (vector quantization and entropy gradient) to improve the performance of off-the-shelf codecs. Firstly, we demonstrate that using non-uniform scalar quantization cannot improve performance over uniform quantization. We thus suggest using predefined optimal uniform vector quantization to improve performance. Secondly, we show that the entropy gradient, available at the decoder, is correlated with the reconstruction error gradient, which is not available at the decoder. We therefore use the former as a proxy to enhance compression performance. Our experimental results show that these approaches save between 1 to 3% of the rate for the same quality across various pretrained methods. In addition, the entropy gradient based solution improves traditional codec performance significantly as well.",
      "authors": [
        "Muhammet Balcilar",
        "Bharath Bhushan Damodaran",
        "Karam Naser",
        "Franck Galpin",
        "Pierre Hellier"
      ],
      "published": "2025-01-02T12:45:31Z",
      "updated": "2025-01-02T12:45:31Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.01231v1",
      "landing_url": "https://arxiv.org/abs/2501.01231v1",
      "doi": "https://doi.org/10.1109/TIP.2024.352281"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on neural video/image codecs and vector quantization; no relation to Spoken Language Models, speech modeling, or interactive speech tasks defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on neural video/image codecs and vector quantization; no relation to Spoken Language Models, speech modeling, or interactive speech tasks defined in the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on image and video codecs optimization through neural networks, without addressing spoken language models, speech sequence modeling, or full-duplex spoken language interactions, which are the central inclusion criteria; it also does not mention speech-based modeling or low-latency synchronous speech interaction, thus not fitting the defined topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on image and video codecs optimization through neural networks, without addressing spoken language models, speech sequence modeling, or full-duplex spoken language interactions, which are the central inclusion criteria; it also does not mention speech-based modeling or low-latency synchronous speech interaction, thus not fitting the defined topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction",
    "abstract": "Active Real-time interaction with video LLMs introduces a new paradigm for human-computer interaction, where the model not only understands user intent but also responds while continuously processing streaming video on the fly. Unlike offline video LLMs, which analyze the entire video before answering questions, active real-time interaction requires three capabilities: 1) Perception: real-time video monitoring and interaction capturing. 2) Decision: raising proactive interaction in proper situations, 3) Reaction: continuous interaction with users. However, inherent conflicts exist among the desired capabilities. The Decision and Reaction require a contrary Perception scale and grain, and the autoregressive decoding blocks the real-time Perception and Decision during the Reaction. To unify the conflicted capabilities within a harmonious system, we present Dispider, a system that disentangles Perception, Decision, and Reaction. Dispider features a lightweight proactive streaming video processing module that tracks the video stream and identifies optimal moments for interaction. Once the interaction is triggered, an asynchronous interaction module provides detailed responses, while the processing module continues to monitor the video in the meantime. Our disentangled and asynchronous design ensures timely, contextually accurate, and computationally efficient responses, making Dispider ideal for active real-time interaction for long-duration video streams. Experiments show that Dispider not only maintains strong performance in conventional video QA tasks, but also significantly surpasses previous online models in streaming scenario responses, thereby validating the effectiveness of our architecture. The code and model are released at \\url{https://github.com/Mark12Ding/Dispider}.",
    "metadata": {
      "arxiv_id": "2501.03218",
      "title": "Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction",
      "summary": "Active Real-time interaction with video LLMs introduces a new paradigm for human-computer interaction, where the model not only understands user intent but also responds while continuously processing streaming video on the fly. Unlike offline video LLMs, which analyze the entire video before answering questions, active real-time interaction requires three capabilities: 1) Perception: real-time video monitoring and interaction capturing. 2) Decision: raising proactive interaction in proper situations, 3) Reaction: continuous interaction with users. However, inherent conflicts exist among the desired capabilities. The Decision and Reaction require a contrary Perception scale and grain, and the autoregressive decoding blocks the real-time Perception and Decision during the Reaction. To unify the conflicted capabilities within a harmonious system, we present Dispider, a system that disentangles Perception, Decision, and Reaction. Dispider features a lightweight proactive streaming video processing module that tracks the video stream and identifies optimal moments for interaction. Once the interaction is triggered, an asynchronous interaction module provides detailed responses, while the processing module continues to monitor the video in the meantime. Our disentangled and asynchronous design ensures timely, contextually accurate, and computationally efficient responses, making Dispider ideal for active real-time interaction for long-duration video streams. Experiments show that Dispider not only maintains strong performance in conventional video QA tasks, but also significantly surpasses previous online models in streaming scenario responses, thereby validating the effectiveness of our architecture. The code and model are released at \\url{https://github.com/Mark12Ding/Dispider}.",
      "authors": [
        "Rui Qian",
        "Shuangrui Ding",
        "Xiaoyi Dong",
        "Pan Zhang",
        "Yuhang Zang",
        "Yuhang Cao",
        "Dahua Lin",
        "Jiaqi Wang"
      ],
      "published": "2025-01-06T18:55:10Z",
      "updated": "2025-01-06T18:55:10Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.03218v1",
      "landing_url": "https://arxiv.org/abs/2501.03218v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.03218"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本研究的核心聚焦於實時視頻互動的 Dispider 框架，並非以 Spoken Language Models 或 FD-SLMs 為核心，亦未聚焦語音層級建模、語音同步或相關評估，因此不符合包含條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本研究的核心聚焦於實時視頻互動的 Dispider 框架，並非以 Spoken Language Models 或 FD-SLMs 為核心，亦未聚焦語音層級建模、語音同步或相關評估，因此不符合包含條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a system for video language models that enable active real-time interaction by disentangling perception, decision, and reaction for streaming video. However, the research does not explicitly address spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs) with speech as the core modeling and reasoning object, nor does it mention speech language modeling or related core concepts. It lacks discussion of speech-level language modeling, low-latency spoken interaction, or evaluation on speech-centric benchmarks. Therefore, the study does not meet the inclusion criteria focused on spoken language models and speech language modeling, nor does it align with the exclusion criteria explicitly but fails to satisfy the core inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a system for video language models that enable active real-time interaction by disentangling perception, decision, and reaction for streaming video. However, the research does not explicitly address spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs) with speech as the core modeling and reasoning object, nor does it mention speech language modeling or related core concepts. It lacks discussion of speech-level language modeling, low-latency spoken interaction, or evaluation on speech-centric benchmarks. Therefore, the study does not meet the inclusion criteria focused on spoken language models and speech language modeling, nor does it align with the exclusion criteria explicitly but fails to satisfy the core inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Phone-purity Guided Discrete Tokens for Dysarthric Speech Recognition",
    "abstract": "Discrete tokens extracted provide efficient and domain adaptable speech features. Their application to disordered speech that exhibits articulation imprecision and large mismatch against normal voice remains unexplored. To improve their phonetic discrimination that is weakened during unsupervised K-means or vector quantization of continuous features, this paper proposes novel phone-purity guided (PPG) discrete tokens for dysarthric speech recognition. Phonetic label supervision is used to regularize maximum likelihood and reconstruction error costs used in standard K-means and VAE-VQ based discrete token extraction. Experiments conducted on the UASpeech corpus suggest that the proposed PPG discrete token features extracted from HuBERT consistently outperform hybrid TDNN and End-to-End (E2E) Conformer systems using non-PPG based K-means or VAE-VQ tokens across varying codebook sizes by statistically significant word error rate (WER) reductions up to 0.99\\% and 1.77\\% absolute (3.21\\% and 4.82\\% relative) respectively on the UASpeech test set of 16 dysarthric speakers. The lowest WER of 23.25\\% was obtained by combining systems using different token features. Consistent improvements on the phone purity metric were also achieved. T-SNE visualization further demonstrates sharper decision boundaries were produced between K-means/VAE-VQ clusters after introducing phone-purity guidance.",
    "metadata": {
      "arxiv_id": "2501.04379",
      "title": "Phone-purity Guided Discrete Tokens for Dysarthric Speech Recognition",
      "summary": "Discrete tokens extracted provide efficient and domain adaptable speech features. Their application to disordered speech that exhibits articulation imprecision and large mismatch against normal voice remains unexplored. To improve their phonetic discrimination that is weakened during unsupervised K-means or vector quantization of continuous features, this paper proposes novel phone-purity guided (PPG) discrete tokens for dysarthric speech recognition. Phonetic label supervision is used to regularize maximum likelihood and reconstruction error costs used in standard K-means and VAE-VQ based discrete token extraction. Experiments conducted on the UASpeech corpus suggest that the proposed PPG discrete token features extracted from HuBERT consistently outperform hybrid TDNN and End-to-End (E2E) Conformer systems using non-PPG based K-means or VAE-VQ tokens across varying codebook sizes by statistically significant word error rate (WER) reductions up to 0.99\\% and 1.77\\% absolute (3.21\\% and 4.82\\% relative) respectively on the UASpeech test set of 16 dysarthric speakers. The lowest WER of 23.25\\% was obtained by combining systems using different token features. Consistent improvements on the phone purity metric were also achieved. T-SNE visualization further demonstrates sharper decision boundaries were produced between K-means/VAE-VQ clusters after introducing phone-purity guidance.",
      "authors": [
        "Huimeng Wang",
        "Xurong Xie",
        "Mengzhe Geng",
        "Shujie Hu",
        "Haoning Xu",
        "Youjun Chen",
        "Zhaoqing Li",
        "Jiajun Deng",
        "Xunying Liu"
      ],
      "published": "2025-01-08T09:45:14Z",
      "updated": "2025-01-08T09:45:14Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.04379v1",
      "landing_url": "https://arxiv.org/abs/2501.04379v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.04379"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on dysarthric speech recognition using phone-purity guided discrete tokens and token-based ASR features; it does not center on Spoken Language Models or full-duplex speech-language modeling, nor discuss interactive dialogue, and thus does not meet the stated inclusion criteria for SLM/FD-SLM emphasis.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on dysarthric speech recognition using phone-purity guided discrete tokens and token-based ASR features; it does not center on Spoken Language Models or full-duplex speech-language modeling, nor discuss interactive dialogue, and thus does not meet the stated inclusion criteria for SLM/FD-SLM emphasis.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on extracting discrete tokens for dysarthric speech recognition, optimizing phonetic feature representations and improving ASR performance, but it does not discuss spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs) as core modeling objects nor does it address issues like synchronous interaction, cognitive parallelism, or multidimensional evaluations beyond WER. The focus remains on ASR feature engineering and model performance without explicit discussion of language modeling of speech or interactive/dialogue behaviors, thus it does not meet the inclusion criteria or avoids the exclusion criteria related to interaction and modeling scope.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on extracting discrete tokens for dysarthric speech recognition, optimizing phonetic feature representations and improving ASR performance, but it does not discuss spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs) as core modeling objects nor does it address issues like synchronous interaction, cognitive parallelism, or multidimensional evaluations beyond WER. The focus remains on ASR feature engineering and model performance without explicit discussion of language modeling of speech or interactive/dialogue behaviors, thus it does not meet the inclusion criteria or avoids the exclusion criteria related to interaction and modeling scope.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Real-Time Textless Dialogue Generation",
    "abstract": "Recent advancements in large language models (LLMs) have led to significant progress in text-based dialogue systems. These systems can now generate high-quality responses that are accurate and coherent across a wide range of topics and tasks. However, spoken dialogue systems still lag behind in terms of naturalness. They tend to produce robotic interactions, with issues such as slow response times, overly generic or cautious replies, and a lack of natural rhythm and fluid turn-taking. This shortcoming is largely due to the over-reliance on the traditional cascaded design, which involve separate, sequential components, as well as the use of text as an intermediate representation. This paper propose a real-time, textless spoken dialogue generation model (RTTL-DG) that aims to overcome these challenges. Our system enables fluid turn-taking and generates responses with minimal delay by processing streaming spoken conversation directly. Additionally, our model incorporates backchannels, filters, laughter, and other paralinguistic signals, which are often absent in cascaded dialogue systems, to create more natural and human-like interactions. The implementations and generated samples are available in our repository: https://github.com/mailong25/rts2s-dg",
    "metadata": {
      "arxiv_id": "2501.04877",
      "title": "Real-Time Textless Dialogue Generation",
      "summary": "Recent advancements in large language models (LLMs) have led to significant progress in text-based dialogue systems. These systems can now generate high-quality responses that are accurate and coherent across a wide range of topics and tasks. However, spoken dialogue systems still lag behind in terms of naturalness. They tend to produce robotic interactions, with issues such as slow response times, overly generic or cautious replies, and a lack of natural rhythm and fluid turn-taking. This shortcoming is largely due to the over-reliance on the traditional cascaded design, which involve separate, sequential components, as well as the use of text as an intermediate representation. This paper propose a real-time, textless spoken dialogue generation model (RTTL-DG) that aims to overcome these challenges. Our system enables fluid turn-taking and generates responses with minimal delay by processing streaming spoken conversation directly. Additionally, our model incorporates backchannels, filters, laughter, and other paralinguistic signals, which are often absent in cascaded dialogue systems, to create more natural and human-like interactions. The implementations and generated samples are available in our repository: https://github.com/mailong25/rts2s-dg",
      "authors": [
        "Long Mai",
        "Julie Carson-Berndsen"
      ],
      "published": "2025-01-08T23:21:43Z",
      "updated": "2025-01-08T23:21:43Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.04877v1",
      "landing_url": "https://arxiv.org/abs/2501.04877v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.04877"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on spoken language modeling with a full-duplex, real-time, textless dialogue system that processes streaming speech (not just ASR/TTS) and discusses architecture and evaluation; published in 2025, within the 3-year window, and aligns with the inclusion criteria without triggering any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on spoken language modeling with a full-duplex, real-time, textless dialogue system that processes streaming speech (not just ASR/TTS) and discusses architecture and evaluation; published in 2025, within the 3-year window, and aligns with the inclusion criteria without triggering any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a real-time spoken dialogue generation model that processes streaming spoken conversation directly and incorporates paralinguistic signals for natural interactions, aligning well with topics of spoken language models, real-time processing, turn-taking, and backchanneling. However, the abstract does not explicitly mention \"spoken language model(s)\" or equivalent terms, nor does it clearly state the use of full-duplex modeling or detailed model architecture/training aspects; there is also no explicit mention of evaluation frameworks or benchmarks. Given the focus on speech as the core and inclusion of backchannels and minimal delay, it leans towards the inclusion criteria but lacks explicit terminology and comprehensive evaluation details required by the criteria.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The paper presents a real-time spoken dialogue generation model that processes streaming spoken conversation directly and incorporates paralinguistic signals for natural interactions, aligning well with topics of spoken language models, real-time processing, turn-taking, and backchanneling. However, the abstract does not explicitly mention \"spoken language model(s)\" or equivalent terms, nor does it clearly state the use of full-duplex modeling or detailed model architecture/training aspects; there is also no explicit mention of evaluation frameworks or benchmarks. Given the focus on speech as the core and inclusion of backchannels and minimal delay, it leans towards the inclusion criteria but lacks explicit terminology and comprehensive evaluation details required by the criteria.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "VoxEval: Benchmarking the Knowledge Understanding Capabilities of End-to-End Spoken Language Models",
    "abstract": "With the rising need for speech-based interaction models, end-to-end Spoken Language Models (SLMs) have emerged as a promising solution. While these models require comprehensive world knowledge for meaningful and reliable human interactions, existing question-answering (QA) benchmarks fall short in evaluating SLMs' knowledge understanding due to their inability to support end-to-end speech evaluation and account for varied input audio conditions. To address these limitations, we present VoxEval, a novel SpeechQA benchmark that assesses SLMs' knowledge understanding through pure speech interactions. Our benchmark 1) uniquely maintains speech format for both inputs and outputs, 2) evaluates model robustness across diverse input audio conditions, and 3) pioneers the assessment of complex tasks like mathematical reasoning in spoken format. Systematic evaluation demonstrates that VoxEval presents significant challenges to current SLMs, revealing their sensitivity to varying audio conditions and highlighting the need to enhance reasoning capabilities in future development. We hope this benchmark could guide the advancement of more sophisticated and reliable SLMs. VoxEval dataset is available at: https://github.com/dreamtheater123/VoxEval",
    "metadata": {
      "arxiv_id": "2501.04962",
      "title": "VoxEval: Benchmarking the Knowledge Understanding Capabilities of End-to-End Spoken Language Models",
      "summary": "With the rising need for speech-based interaction models, end-to-end Spoken Language Models (SLMs) have emerged as a promising solution. While these models require comprehensive world knowledge for meaningful and reliable human interactions, existing question-answering (QA) benchmarks fall short in evaluating SLMs' knowledge understanding due to their inability to support end-to-end speech evaluation and account for varied input audio conditions. To address these limitations, we present VoxEval, a novel SpeechQA benchmark that assesses SLMs' knowledge understanding through pure speech interactions. Our benchmark 1) uniquely maintains speech format for both inputs and outputs, 2) evaluates model robustness across diverse input audio conditions, and 3) pioneers the assessment of complex tasks like mathematical reasoning in spoken format. Systematic evaluation demonstrates that VoxEval presents significant challenges to current SLMs, revealing their sensitivity to varying audio conditions and highlighting the need to enhance reasoning capabilities in future development. We hope this benchmark could guide the advancement of more sophisticated and reliable SLMs. VoxEval dataset is available at: https://github.com/dreamtheater123/VoxEval",
      "authors": [
        "Wenqian Cui",
        "Xiaoqi Jiao",
        "Ziqiao Meng",
        "Irwin King"
      ],
      "published": "2025-01-09T04:30:12Z",
      "updated": "2025-05-27T16:14:30Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.04962v4",
      "landing_url": "https://arxiv.org/abs/2501.04962v4",
      "doi": "https://doi.org/10.48550/arXiv.2501.04962"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item clearly centers on end-to-end Spoken Language Models with speech as the core modality, evaluates knowledge understanding via a SpeechQA benchmark using pure speech interactions (likely multi-turn), covers complex reasoning in spoken form, is within the past 3 years, and explicitly mentions spoken language models in the title/abstract, while not being limited to single-turn ASR or non-academic/weak-methodology concerns.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item clearly centers on end-to-end Spoken Language Models with speech as the core modality, evaluates knowledge understanding via a SpeechQA benchmark using pure speech interactions (likely multi-turn), covers complex reasoning in spoken form, is within the past 3 years, and explicitly mentions spoken language models in the title/abstract, while not being limited to single-turn ASR or non-academic/weak-methodology concerns.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents VoxEval, a benchmark specifically designed for end-to-end Spoken Language Models (SLMs) that maintain speech as the core input and output, evaluating robustness and reasoning capabilities in various speech conditions. However, it seems focused on assessment of knowledge understanding through speech QA tasks rather than explicitly addressing full-duplex interaction capabilities, cognitive parallelism, or synchronization mechanisms detailed in the inclusion criteria. It does involve end-to-end speech modeling and evaluation but does not explicitly mention full-duplex interaction or multi-dimensional evaluation aspects such as voice overlap, interruption, or real-time behavioral arbitration. Hence, it partially meets inclusion criteria but lacks clarity on some key points, leading to some uncertainty about full alignment with the tightly defined thematic scope.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study presents VoxEval, a benchmark specifically designed for end-to-end Spoken Language Models (SLMs) that maintain speech as the core input and output, evaluating robustness and reasoning capabilities in various speech conditions. However, it seems focused on assessment of knowledge understanding through speech QA tasks rather than explicitly addressing full-duplex interaction capabilities, cognitive parallelism, or synchronization mechanisms detailed in the inclusion criteria. It does involve end-to-end speech modeling and evaluation but does not explicitly mention full-duplex interaction or multi-dimensional evaluation aspects such as voice overlap, interruption, or real-time behavioral arbitration. Hence, it partially meets inclusion criteria but lacks clarity on some key points, leading to some uncertainty about full alignment with the tightly defined thematic scope.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The paper clearly targets end-to-end Spoken Language Models (SLMs) and introduces a SpeechQA benchmark (VoxEval) that keeps speech as both input and output and evaluates robustness and reasoning — satisfying the core inclusion requirements about speech-centric SLMs and evaluation — but the abstract does not mention full-duplex/synchronous interaction capabilities (cognitive parallelism, overlap handling, low-latency arbitration) emphasized by the topical scope, so it may fall into the exclusion clause if interpreted as only a traditional half-duplex QA benchmark; junior reviewers were split (one recommended include for meeting SLM and evaluation criteria, the other was uncertain due to lack of full-duplex detail). Given this ambiguity, I rate it as uncertain.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "The paper clearly targets end-to-end Spoken Language Models (SLMs) and introduces a SpeechQA benchmark (VoxEval) that keeps speech as both input and output and evaluates robustness and reasoning — satisfying the core inclusion requirements about speech-centric SLMs and evaluation — but the abstract does not mention full-duplex/synchronous interaction capabilities (cognitive parallelism, overlap handling, low-latency arbitration) emphasized by the topical scope, so it may fall into the exclusion clause if interpreted as only a traditional half-duplex QA benchmark; junior reviewers were split (one recommended include for meeting SLM and evaluation criteria, the other was uncertain due to lack of full-duplex detail). Given this ambiguity, I rate it as uncertain.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Video-Conferencing Beyond Screen-Sharing and Thumbnail Webcam Videos: Gesture-Aware Augmented Reality Video for Data-Rich Remote Presentations",
    "abstract": "Synchronous data-rich conversations are commonplace within enterprise organizations, taking place at varying degrees of formality between stakeholders at different levels of data literacy. In these conversations, representations of data are used to analyze past decisions, inform future course of action, as well as persuade customers, investors, and executives. However, it is difficult to conduct these conversations between remote stakeholders due to poor support for presenting data when video-conferencing, resulting in disappointing audience experiences. In this position statement, I reflect on our recent work incorporating multimodal interaction and augmented reality video, suggesting that video-conferencing does not need to be limited to screen-sharing and relegating a speaker's video to a separate thumbnail view. I also comment on future research directions and collaboration opportunities.",
    "metadata": {
      "arxiv_id": "2501.05345",
      "title": "Video-Conferencing Beyond Screen-Sharing and Thumbnail Webcam Videos: Gesture-Aware Augmented Reality Video for Data-Rich Remote Presentations",
      "summary": "Synchronous data-rich conversations are commonplace within enterprise organizations, taking place at varying degrees of formality between stakeholders at different levels of data literacy. In these conversations, representations of data are used to analyze past decisions, inform future course of action, as well as persuade customers, investors, and executives. However, it is difficult to conduct these conversations between remote stakeholders due to poor support for presenting data when video-conferencing, resulting in disappointing audience experiences. In this position statement, I reflect on our recent work incorporating multimodal interaction and augmented reality video, suggesting that video-conferencing does not need to be limited to screen-sharing and relegating a speaker's video to a separate thumbnail view. I also comment on future research directions and collaboration opportunities.",
      "authors": [
        "Matthew Brehmer"
      ],
      "published": "2025-01-09T16:21:12Z",
      "updated": "2025-01-09T16:21:12Z",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.05345v1",
      "landing_url": "https://arxiv.org/abs/2501.05345v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.05345"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on gesture-aware AR video conferencing and multimodal data presentation, not on Spoken Language Models or speech-language modeling (no core SLM/FD-SLM focus, architecture, training, or evaluation discussed).",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on gesture-aware AR video conferencing and multimodal data presentation, not on Spoken Language Models or speech-language modeling (no core SLM/FD-SLM focus, architecture, training, or evaluation discussed).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on augmented reality video and multimodal interactions in video-conferencing, rather than on spoken language models or speech language modeling architectures; there is no mention of spoken language model training, inference, evaluation, or full-duplex speech interaction features outlined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on augmented reality video and multimodal interactions in video-conferencing, rather than on spoken language models or speech language modeling architectures; there is no mention of spoken language model training, inference, evaluation, or full-duplex speech interaction features outlined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model",
    "abstract": "Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities. However, they often struggle with more expressive references or complex text inputs. We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent improvements in spoken language modelling. Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality. We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality. This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger. We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability. Project page: https://camb-ai.github.io/mars6-turbo/",
    "metadata": {
      "arxiv_id": "2501.05787",
      "title": "MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model",
      "summary": "Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities. However, they often struggle with more expressive references or complex text inputs. We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent improvements in spoken language modelling. Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality. We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality. This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger. We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability. Project page: https://camb-ai.github.io/mars6-turbo/",
      "authors": [
        "Matthew Baas",
        "Pieter Scholtz",
        "Arnav Mehta",
        "Elliott Dyson",
        "Akshat Prakash",
        "Herman Kamper"
      ],
      "published": "2025-01-10T08:41:42Z",
      "updated": "2025-01-10T08:41:42Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.05787v1",
      "landing_url": "https://arxiv.org/abs/2501.05787v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.05787"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the abstract references spoken language modelling, the work centers on a TTS model without interactive dialogue/synchronization or explicit speech–language modeling as the core, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although the abstract references spoken language modelling, the work centers on a TTS model without interactive dialogue/synchronization or explicit speech–language modeling as the core, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a text-to-speech model (MARS6) with hierarchical and codec-based architecture, emphasizing fast and expressive speech synthesis. However, it primarily addresses TTS quality, voice cloning, and efficiency improvements without explicit discussion of spoken language models as the core, synchronous or full-duplex spoken interaction capabilities, cognitive parallelism, or real-time interaction features such as turn-taking or backchanneling. The abstract lacks mention of model architectures or training explicitly framed as spoken language modeling, nor does it discuss evaluations related to synchronous interaction or behavior arbitration. Therefore, while it is related to speech technology, it does not meet the inclusion criteria centered on spoken language models or full-duplex interaction research, and partially meets the exclusion criteria focusing only on TTS without dialogue or real-time interaction components.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a text-to-speech model (MARS6) with hierarchical and codec-based architecture, emphasizing fast and expressive speech synthesis. However, it primarily addresses TTS quality, voice cloning, and efficiency improvements without explicit discussion of spoken language models as the core, synchronous or full-duplex spoken interaction capabilities, cognitive parallelism, or real-time interaction features such as turn-taking or backchanneling. The abstract lacks mention of model architectures or training explicitly framed as spoken language modeling, nor does it discuss evaluations related to synchronous interaction or behavior arbitration. Therefore, while it is related to speech technology, it does not meet the inclusion criteria centered on spoken language models or full-duplex interaction research, and partially meets the exclusion criteria focusing only on TTS without dialogue or real-time interaction components.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Neural Codec Source Tracing: Toward Comprehensive Attribution in Open-Set Condition",
    "abstract": "Current research in audio deepfake detection is gradually transitioning from binary classification to multi-class tasks, referred as audio deepfake source tracing task. However, existing studies on source tracing consider only closed-set scenarios and have not considered the challenges posed by open-set conditions. In this paper, we define the Neural Codec Source Tracing (NCST) task, which is capable of performing open-set neural codec classification and interpretable ALM detection. Specifically, we constructed the ST-Codecfake dataset for the NCST task, which includes bilingual audio samples generated by 11 state-of-the-art neural codec methods and ALM-based out-ofdistribution (OOD) test samples. Furthermore, we establish a comprehensive source tracing benchmark to assess NCST models in open-set conditions. The experimental results reveal that although the NCST models perform well in in-distribution (ID) classification and OOD detection, they lack robustness in classifying unseen real audio. The ST-codecfake dataset and code are available.",
    "metadata": {
      "arxiv_id": "2501.06514",
      "title": "Neural Codec Source Tracing: Toward Comprehensive Attribution in Open-Set Condition",
      "summary": "Current research in audio deepfake detection is gradually transitioning from binary classification to multi-class tasks, referred as audio deepfake source tracing task. However, existing studies on source tracing consider only closed-set scenarios and have not considered the challenges posed by open-set conditions. In this paper, we define the Neural Codec Source Tracing (NCST) task, which is capable of performing open-set neural codec classification and interpretable ALM detection. Specifically, we constructed the ST-Codecfake dataset for the NCST task, which includes bilingual audio samples generated by 11 state-of-the-art neural codec methods and ALM-based out-ofdistribution (OOD) test samples. Furthermore, we establish a comprehensive source tracing benchmark to assess NCST models in open-set conditions. The experimental results reveal that although the NCST models perform well in in-distribution (ID) classification and OOD detection, they lack robustness in classifying unseen real audio. The ST-codecfake dataset and code are available.",
      "authors": [
        "Yuankun Xie",
        "Xiaopeng Wang",
        "Zhiyong Wang",
        "Ruibo Fu",
        "Zhengqi Wen",
        "Songjun Cao",
        "Long Ma",
        "Chenxing Li",
        "Haonnan Cheng",
        "Long Ye"
      ],
      "published": "2025-01-11T11:15:58Z",
      "updated": "2025-01-11T11:15:58Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.06514v1",
      "landing_url": "https://arxiv.org/abs/2501.06514v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.06514"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on open-set neural codec source tracing for audio deepfake detection, not on Spoken Language Models or speech-language modeling; it does not discuss SLM architecture, training, or evaluation, so it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on open-set neural codec source tracing for audio deepfake detection, not on Spoken Language Models or speech-language modeling; it does not discuss SLM architecture, training, or evaluation, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on audio deepfake source tracing and neural codec classification, which centers on audio signal analysis and classification related to neural codecs and open-set conditions. However, it does not explicitly focus on Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor discusses speech language modeling, synchronization for full-duplex interaction, cognitive parallelism, or real-time interaction aspects as required by the inclusion criteria. It is more about classification and detection of audio deepfakes and neural codec attribution than modeling speech sequences or real-time spoken language interaction. Therefore, it does not meet the core inclusion criteria for research on spoken language models or full-duplex spoken interaction modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on audio deepfake source tracing and neural codec classification, which centers on audio signal analysis and classification related to neural codecs and open-set conditions. However, it does not explicitly focus on Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs), nor discusses speech language modeling, synchronization for full-duplex interaction, cognitive parallelism, or real-time interaction aspects as required by the inclusion criteria. It is more about classification and detection of audio deepfakes and neural codec attribution than modeling speech sequences or real-time spoken language interaction. Therefore, it does not meet the core inclusion criteria for research on spoken language models or full-duplex spoken interaction modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Consistency of Responses and Continuations Generated by Large Language Models on Social Media",
    "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities in text generation, yet their emotional consistency and semantic coherence in social media contexts remain insufficiently understood. This study investigates how LLMs handle emotional content and maintain semantic relationships through continuation and response tasks using three open-source models: Gemma, Llama3 and Llama3.3 and one commercial Model:Claude. By analyzing climate change discussions from Twitter and Reddit, we examine emotional transitions, intensity patterns, and semantic consistency between human-authored and LLM-generated content. Our findings reveal that while both models maintain high semantic coherence, they exhibit distinct emotional patterns: these models show a strong tendency to moderate negative emotions. When the input text carries negative emotions such as anger, disgust, fear, or sadness, LLM tends to generate content with more neutral emotions, or even convert them into positive emotions such as joy or surprise. At the same time, we compared the LLM-generated content with human-authored content. The four models systematically generated responses with reduced emotional intensity and showed a preference for neutral rational emotions in the response task. In addition, these models all maintained a high semantic similarity with the original text, although their performance in the continuation task and the response task was different. These findings provide deep insights into the emotion and semantic processing capabilities of LLM, which are of great significance for its deployment in social media environments and human-computer interaction design.",
    "metadata": {
      "arxiv_id": "2501.08102",
      "title": "Consistency of Responses and Continuations Generated by Large Language Models on Social Media",
      "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text generation, yet their emotional consistency and semantic coherence in social media contexts remain insufficiently understood. This study investigates how LLMs handle emotional content and maintain semantic relationships through continuation and response tasks using three open-source models: Gemma, Llama3 and Llama3.3 and one commercial Model:Claude. By analyzing climate change discussions from Twitter and Reddit, we examine emotional transitions, intensity patterns, and semantic consistency between human-authored and LLM-generated content. Our findings reveal that while both models maintain high semantic coherence, they exhibit distinct emotional patterns: these models show a strong tendency to moderate negative emotions. When the input text carries negative emotions such as anger, disgust, fear, or sadness, LLM tends to generate content with more neutral emotions, or even convert them into positive emotions such as joy or surprise. At the same time, we compared the LLM-generated content with human-authored content. The four models systematically generated responses with reduced emotional intensity and showed a preference for neutral rational emotions in the response task. In addition, these models all maintained a high semantic similarity with the original text, although their performance in the continuation task and the response task was different. These findings provide deep insights into the emotion and semantic processing capabilities of LLM, which are of great significance for its deployment in social media environments and human-computer interaction design.",
      "authors": [
        "Wentao Xu",
        "Wenlu Fan",
        "Yuqi Zhu",
        "Bin Wang"
      ],
      "published": "2025-01-14T13:19:47Z",
      "updated": "2025-11-04T02:32:20Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.08102v6",
      "landing_url": "https://arxiv.org/abs/2501.08102v6",
      "doi": "https://doi.org/10.48550/arXiv.2501.08102"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item analyzes text-based LLM emotion and semantic coherence in social media, not Spoken Language Models or Speech-LM; it lacks speech/voice modeling focus, turn-taking, and speech-centric evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item analyzes text-based LLM emotion and semantic coherence in social media, not Spoken Language Models or Speech-LM; it lacks speech/voice modeling focus, turn-taking, and speech-centric evaluation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on large language models' emotional and semantic consistency in social media text generation, without involving spoken language models or speech as the core modeling object. It does not address speech-level language modeling, full-duplex spoken language interaction, nor synchronization or acoustic modeling aspects required by the inclusion criteria. Hence, it does not meet the inclusion requirements centered on spoken language models and speech-based end-to-end architectures.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on large language models' emotional and semantic consistency in social media text generation, without involving spoken language models or speech as the core modeling object. It does not address speech-level language modeling, full-duplex spoken language interaction, nor synchronization or acoustic modeling aspects required by the inclusion criteria. Hence, it does not meet the inclusion requirements centered on spoken language models and speech-based end-to-end architectures.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "RouteNet-Gauss: Hardware-Enhanced Network Modeling with Machine Learning",
    "abstract": "Network simulation is pivotal in network modeling, assisting with tasks ranging from capacity planning to performance estimation. Traditional approaches such as Discrete Event Simulation (DES) face limitations in terms of computational cost and accuracy. This paper introduces RouteNet-Gauss, a novel integration of a testbed network with a Machine Learning (ML) model to address these challenges. By using the testbed as a hardware accelerator, RouteNet-Gauss generates training datasets rapidly and simulates network scenarios with high fidelity to real-world conditions. Experimental results show that RouteNet-Gauss significantly reduces prediction errors by up to 95% and achieves a 488x speedup in inference time compared to state-of-the-art DES-based methods. RouteNet-Gauss's modular architecture is dynamically constructed based on the specific characteristics of the network scenario, such as topology and routing. This enables it to understand and generalize to different network configurations beyond those seen during training, including networks up to 10x larger. Additionally, it supports Temporal Aggregated Performance Estimation (TAPE), providing configurable temporal granularity and maintaining high accuracy in flow performance metrics. This approach shows promise in improving both simulation efficiency and accuracy, offering a valuable tool for network operators.",
    "metadata": {
      "arxiv_id": "2501.08848",
      "title": "RouteNet-Gauss: Hardware-Enhanced Network Modeling with Machine Learning",
      "summary": "Network simulation is pivotal in network modeling, assisting with tasks ranging from capacity planning to performance estimation. Traditional approaches such as Discrete Event Simulation (DES) face limitations in terms of computational cost and accuracy. This paper introduces RouteNet-Gauss, a novel integration of a testbed network with a Machine Learning (ML) model to address these challenges. By using the testbed as a hardware accelerator, RouteNet-Gauss generates training datasets rapidly and simulates network scenarios with high fidelity to real-world conditions. Experimental results show that RouteNet-Gauss significantly reduces prediction errors by up to 95% and achieves a 488x speedup in inference time compared to state-of-the-art DES-based methods. RouteNet-Gauss's modular architecture is dynamically constructed based on the specific characteristics of the network scenario, such as topology and routing. This enables it to understand and generalize to different network configurations beyond those seen during training, including networks up to 10x larger. Additionally, it supports Temporal Aggregated Performance Estimation (TAPE), providing configurable temporal granularity and maintaining high accuracy in flow performance metrics. This approach shows promise in improving both simulation efficiency and accuracy, offering a valuable tool for network operators.",
      "authors": [
        "Carlos Güemes-Palau",
        "Miquel Ferriol-Galmés",
        "Jordi Paillisse-Vilanova",
        "Albert López-Brescó",
        "Pere Barlet-Ros",
        "Albert Cabellos-Aparicio"
      ],
      "published": "2025-01-15T15:00:11Z",
      "updated": "2025-09-03T16:19:20Z",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.08848v2",
      "landing_url": "https://arxiv.org/abs/2501.08848v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.08848"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on hardware-enhanced network modeling with ML, not on Spoken Language Models or speech-centric modeling, thus it does not meet the inclusion criteria focused on SLM/FD-SLM and related speech interaction aspects.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on hardware-enhanced network modeling with ML, not on Spoken Language Models or speech-centric modeling, thus it does not meet the inclusion criteria focused on SLM/FD-SLM and related speech interaction aspects.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on network simulation using machine learning and hardware acceleration, with no mention of spoken language models, speech-language modeling, or related concepts central to the inclusion criteria. It does not address speech-based modeling, full-duplex spoken interaction, or any speech modeling architecture, training, or evaluation, thus failing to meet any inclusion criterion and instead falling outside the research domain defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on network simulation using machine learning and hardware acceleration, with no mention of spoken language models, speech-language modeling, or related concepts central to the inclusion criteria. It does not address speech-based modeling, full-duplex spoken interaction, or any speech modeling architecture, training, or evaluation, thus failing to meet any inclusion criterion and instead falling outside the research domain defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Applying General Turn-taking Models to Conversational Human-Robot Interaction",
    "abstract": "Turn-taking is a fundamental aspect of conversation, but current Human-Robot Interaction (HRI) systems often rely on simplistic, silence-based models, leading to unnatural pauses and interruptions. This paper investigates, for the first time, the application of general turn-taking models, specifically TurnGPT and Voice Activity Projection (VAP), to improve conversational dynamics in HRI. These models are trained on human-human dialogue data using self-supervised learning objectives, without requiring domain-specific fine-tuning. We propose methods for using these models in tandem to predict when a robot should begin preparing responses, take turns, and handle potential interruptions. We evaluated the proposed system in a within-subject study against a traditional baseline system, using the Furhat robot with 39 adults in a conversational setting, in combination with a large language model for autonomous response generation. The results show that participants significantly prefer the proposed system, and it significantly reduces response delays and interruptions.",
    "metadata": {
      "arxiv_id": "2501.08946",
      "title": "Applying General Turn-taking Models to Conversational Human-Robot Interaction",
      "summary": "Turn-taking is a fundamental aspect of conversation, but current Human-Robot Interaction (HRI) systems often rely on simplistic, silence-based models, leading to unnatural pauses and interruptions. This paper investigates, for the first time, the application of general turn-taking models, specifically TurnGPT and Voice Activity Projection (VAP), to improve conversational dynamics in HRI. These models are trained on human-human dialogue data using self-supervised learning objectives, without requiring domain-specific fine-tuning. We propose methods for using these models in tandem to predict when a robot should begin preparing responses, take turns, and handle potential interruptions. We evaluated the proposed system in a within-subject study against a traditional baseline system, using the Furhat robot with 39 adults in a conversational setting, in combination with a large language model for autonomous response generation. The results show that participants significantly prefer the proposed system, and it significantly reduces response delays and interruptions.",
      "authors": [
        "Gabriel Skantze",
        "Bahar Irfan"
      ],
      "published": "2025-01-15T16:49:22Z",
      "updated": "2025-01-15T16:49:22Z",
      "categories": [
        "cs.CL",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.08946v1",
      "landing_url": "https://arxiv.org/abs/2501.08946v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.08946"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item does not position Spoken Language Models (SLMs) or Full-Duplex SLMs as the core, instead focusing on general turn-taking models in HRI (TurnGPT/VAP) with LLM-based response generation; no explicit framing around speech-centric modeling, architecture, training, or evaluation of SLMs in the title/abstract, thus it fails the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item does not position Spoken Language Models (SLMs) or Full-Duplex SLMs as the core, instead focusing on general turn-taking models in HRI (TurnGPT/VAP) with LLM-based response generation; no explicit framing around speech-centric modeling, architecture, training, or evaluation of SLMs in the title/abstract, thus it fails the core inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study addresses conversational turn-taking within Human-Robot Interaction using pretrained models that focus on dialogue dynamics, but does not explicitly frame its core modeling as Spoken Language Models or Speech Language Models; the abstract lacks mention of speech-level modeling, full-duplex capabilities, or detailed speech-based model architectures and evaluation fulfilling the core inclusion criteria, and mostly discusses dialogue turn-taking in a human-robot context rather than speech language modeling per se.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study addresses conversational turn-taking within Human-Robot Interaction using pretrained models that focus on dialogue dynamics, but does not explicitly frame its core modeling as Spoken Language Models or Speech Language Models; the abstract lacks mention of speech-level modeling, full-duplex capabilities, or detailed speech-based model architectures and evaluation fulfilling the core inclusion criteria, and mostly discusses dialogue turn-taking in a human-robot context rather than speech language modeling per se.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "VECT-GAN: A variationally encoded generative model for overcoming data scarcity in pharmaceutical science",
    "abstract": "Data scarcity in pharmaceutical research has led to reliance on labour-intensive trial-and-error approaches for development rather than data-driven methods. While Machine Learning offers a solution, existing datasets are often small and noisy, limiting their utility. To address this, we developed a Variationally Encoded Conditional Tabular Generative Adversarial Network (VECT-GAN), a novel generative model specifically designed for augmenting small, noisy datasets. We introduce a pipeline where data is augmented before regression model development and demonstrate that this consistently and significantly improves performance over other state-of-the-art tabular generative models. We apply this pipeline across six pharmaceutical datasets, and highlight its real-world applicability by developing novel polymers with medically desirable mucoadhesive properties, which we made and experimentally characterised. Additionally, we pre-train the model on the ChEMBL database of drug-like molecules, leveraging knowledge distillation to enhance its generalisability, making it readily available for use on pharmaceutical datasets containing small molecules, an extremely common pharmaceutical task. We demonstrate the power of synthetic data for regularising small tabular datasets, highlighting its potential to become standard practice in pharmaceutical model development, and make our method, including VECT-GAN pre-trained on ChEMBL available as a pip package.",
    "metadata": {
      "arxiv_id": "2501.08995",
      "title": "VECT-GAN: A variationally encoded generative model for overcoming data scarcity in pharmaceutical science",
      "summary": "Data scarcity in pharmaceutical research has led to reliance on labour-intensive trial-and-error approaches for development rather than data-driven methods. While Machine Learning offers a solution, existing datasets are often small and noisy, limiting their utility. To address this, we developed a Variationally Encoded Conditional Tabular Generative Adversarial Network (VECT-GAN), a novel generative model specifically designed for augmenting small, noisy datasets. We introduce a pipeline where data is augmented before regression model development and demonstrate that this consistently and significantly improves performance over other state-of-the-art tabular generative models. We apply this pipeline across six pharmaceutical datasets, and highlight its real-world applicability by developing novel polymers with medically desirable mucoadhesive properties, which we made and experimentally characterised. Additionally, we pre-train the model on the ChEMBL database of drug-like molecules, leveraging knowledge distillation to enhance its generalisability, making it readily available for use on pharmaceutical datasets containing small molecules, an extremely common pharmaceutical task. We demonstrate the power of synthetic data for regularising small tabular datasets, highlighting its potential to become standard practice in pharmaceutical model development, and make our method, including VECT-GAN pre-trained on ChEMBL available as a pip package.",
      "authors": [
        "Youssef Abdalla",
        "Marrisa Taub",
        "Eleanor Hilton",
        "Priya Akkaraju",
        "Alexander Milanovic",
        "Mine Orlu",
        "Abdul W. Basit",
        "Michael T Cook",
        "Tapabrata Chakraborti",
        "David Shorthouse"
      ],
      "published": "2025-01-15T18:23:33Z",
      "updated": "2025-01-17T08:58:48Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.08995v2",
      "landing_url": "https://arxiv.org/abs/2501.08995v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.08995"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on a variationally encoded GAN for augmenting small pharmaceutical tabular datasets; it does not involve Spoken Language Models or any speech-language modeling concepts, thus fails inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper centers on a variationally encoded GAN for augmenting small pharmaceutical tabular datasets; it does not involve Spoken Language Models or any speech-language modeling concepts, thus fails inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a generative model for pharmaceutical data augmentation and do not mention spoken language models, speech language modeling, or any form of speech-related modeling or full-duplex spoken interaction. The core subject is pharmaceutical science data modeling, thus failing to meet the inclusion criteria centered on spoken language models or related topics involving speech sequence modeling and synchronous interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a generative model for pharmaceutical data augmentation and do not mention spoken language models, speech language modeling, or any form of speech-related modeling or full-duplex spoken interaction. The core subject is pharmaceutical science data modeling, thus failing to meet the inclusion criteria centered on spoken language models or related topics involving speech sequence modeling and synchronous interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Adaptive Few-Shot Learning (AFSL): Tackling Data Scarcity with Stability, Robustness, and Versatility",
    "abstract": "Few-shot learning (FSL) enables machine learning models to generalize effectively with minimal labeled data, making it crucial for data-scarce domains such as healthcare, robotics, and natural language processing. Despite its potential, FSL faces challenges including sensitivity to initialization, difficulty in adapting to diverse domains, and vulnerability to noisy datasets. To address these issues, this paper introduces Adaptive Few-Shot Learning (AFSL), a framework that integrates advancements in meta-learning, domain alignment, noise resilience, and multi-modal integration. AFSL consists of four key modules: a Dynamic Stability Module for performance consistency, a Contextual Domain Alignment Module for domain adaptation, a Noise-Adaptive Resilience Module for handling noisy data, and a Multi-Modal Fusion Module for integrating diverse modalities. This work also explores strategies such as task-aware data augmentation, semi-supervised learning, and explainable AI techniques to enhance the applicability and robustness of FSL. AFSL provides scalable, reliable, and impactful solutions for real-world, high-stakes domains.",
    "metadata": {
      "arxiv_id": "2501.13479",
      "title": "Adaptive Few-Shot Learning (AFSL): Tackling Data Scarcity with Stability, Robustness, and Versatility",
      "summary": "Few-shot learning (FSL) enables machine learning models to generalize effectively with minimal labeled data, making it crucial for data-scarce domains such as healthcare, robotics, and natural language processing. Despite its potential, FSL faces challenges including sensitivity to initialization, difficulty in adapting to diverse domains, and vulnerability to noisy datasets. To address these issues, this paper introduces Adaptive Few-Shot Learning (AFSL), a framework that integrates advancements in meta-learning, domain alignment, noise resilience, and multi-modal integration. AFSL consists of four key modules: a Dynamic Stability Module for performance consistency, a Contextual Domain Alignment Module for domain adaptation, a Noise-Adaptive Resilience Module for handling noisy data, and a Multi-Modal Fusion Module for integrating diverse modalities. This work also explores strategies such as task-aware data augmentation, semi-supervised learning, and explainable AI techniques to enhance the applicability and robustness of FSL. AFSL provides scalable, reliable, and impactful solutions for real-world, high-stakes domains.",
      "authors": [
        "Rishabh Agrawal"
      ],
      "published": "2025-01-23T08:51:49Z",
      "updated": "2025-01-23T08:51:49Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.13479v1",
      "landing_url": "https://arxiv.org/abs/2501.13479v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.13479"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on Adaptive Few-Shot Learning in general ML without focusing on Spoken Language Models or full-duplex speech, and does not discuss speech-centric modeling, evaluation, or dialogue synchronization required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on Adaptive Few-Shot Learning in general ML without focusing on Spoken Language Models or full-duplex speech, and does not discuss speech-centric modeling, evaluation, or dialogue synchronization required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on Adaptive Few-Shot Learning addressing challenges like domain adaptation and noisy data across multiple modalities but does not mention Spoken Language Models, speech-level language modeling, or full-duplex spoken interaction capabilities; its core subject is not about speech or spoken language modeling, nor does it discuss end-to-end speech processing architectures or relevant evaluations tied to spoken language interaction. Therefore, it does not meet the inclusion criteria which require explicit focus on spoken language models and speech as the primary modeling object.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on Adaptive Few-Shot Learning addressing challenges like domain adaptation and noisy data across multiple modalities but does not mention Spoken Language Models, speech-level language modeling, or full-duplex spoken interaction capabilities; its core subject is not about speech or spoken language modeling, nor does it discuss end-to-end speech processing architectures or relevant evaluations tied to spoken language interaction. Therefore, it does not meet the inclusion criteria which require explicit focus on spoken language models and speech as the primary modeling object.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "VARFVV: View-Adaptive Real-Time Interactive Free-View Video Streaming with Edge Computing",
    "abstract": "Free-view video (FVV) allows users to explore immersive video content from multiple views. However, delivering FVV poses significant challenges due to the uncertainty in view switching, combined with the substantial bandwidth and computational resources required to transmit and decode multiple video streams, which may result in frequent playback interruptions. Existing approaches, either client-based or cloud-based, struggle to meet high Quality of Experience (QoE) requirements under limited bandwidth and computational resources. To address these issues, we propose VARFVV, a bandwidth- and computationally-efficient system that enables real-time interactive FVV streaming with high QoE and low switching delay. Specifically, VARFVV introduces a low-complexity FVV generation scheme that reassembles multiview video frames at the edge server based on user-selected view tracks, eliminating the need for transcoding and significantly reducing computational overhead. This design makes it well-suited for large-scale, mobile-based UHD FVV experiences. Furthermore, we present a popularity-adaptive bit allocation method, leveraging a graph neural network, that predicts view popularity and dynamically adjusts bit allocation to maximize QoE within bandwidth constraints. We also construct an FVV dataset comprising 330 videos from 10 scenes, including basketball, opera, etc. Extensive experiments show that VARFVV surpasses existing methods in video quality, switching latency, computational efficiency, and bandwidth usage, supporting over 500 users on a single edge server with a switching delay of 71.5ms. Our code and dataset are available at https://github.com/qianghu-huber/VARFVV.",
    "metadata": {
      "arxiv_id": "2501.13630",
      "title": "VARFVV: View-Adaptive Real-Time Interactive Free-View Video Streaming with Edge Computing",
      "summary": "Free-view video (FVV) allows users to explore immersive video content from multiple views. However, delivering FVV poses significant challenges due to the uncertainty in view switching, combined with the substantial bandwidth and computational resources required to transmit and decode multiple video streams, which may result in frequent playback interruptions. Existing approaches, either client-based or cloud-based, struggle to meet high Quality of Experience (QoE) requirements under limited bandwidth and computational resources. To address these issues, we propose VARFVV, a bandwidth- and computationally-efficient system that enables real-time interactive FVV streaming with high QoE and low switching delay. Specifically, VARFVV introduces a low-complexity FVV generation scheme that reassembles multiview video frames at the edge server based on user-selected view tracks, eliminating the need for transcoding and significantly reducing computational overhead. This design makes it well-suited for large-scale, mobile-based UHD FVV experiences. Furthermore, we present a popularity-adaptive bit allocation method, leveraging a graph neural network, that predicts view popularity and dynamically adjusts bit allocation to maximize QoE within bandwidth constraints. We also construct an FVV dataset comprising 330 videos from 10 scenes, including basketball, opera, etc. Extensive experiments show that VARFVV surpasses existing methods in video quality, switching latency, computational efficiency, and bandwidth usage, supporting over 500 users on a single edge server with a switching delay of 71.5ms. Our code and dataset are available at https://github.com/qianghu-huber/VARFVV.",
      "authors": [
        "Qiang Hu",
        "Qihan He",
        "Houqiang Zhong",
        "Guo Lu",
        "Xiaoyun Zhang",
        "Guangtao Zhai",
        "Yanfeng Wang"
      ],
      "published": "2025-01-23T12:58:58Z",
      "updated": "2025-01-23T12:58:58Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.13630v1",
      "landing_url": "https://arxiv.org/abs/2501.13630v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.13630"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article focuses on video streaming with edge computing (VARFVV) and does not address Spoken Language Models or any speech modeling concepts, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article focuses on video streaming with edge computing (VARFVV) and does not address Spoken Language Models or any speech modeling concepts, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a system for free-view video streaming using edge computing and graph neural networks, focusing on bandwidth efficiency and quality of experience. There is no mention of Spoken Language Models, speech-language modeling, or any related concepts such as end-to-end speech modeling, full-duplex spoken interaction, or speech-based cognitive parallelism. The research does not focus on modeling or reasoning over speech signals but rather on video streaming technology. Therefore, it does not meet the inclusion criteria centered on spoken language models and speech interaction modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a system for free-view video streaming using edge computing and graph neural networks, focusing on bandwidth efficiency and quality of experience. There is no mention of Spoken Language Models, speech-language modeling, or any related concepts such as end-to-end speech modeling, full-duplex spoken interaction, or speech-based cognitive parallelism. The research does not focus on modeling or reasoning over speech signals but rather on video streaming technology. Therefore, it does not meet the inclusion criteria centered on spoken language models and speech interaction modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Baichuan-Omni-1.5 Technical Report",
    "abstract": "We introduce Baichuan-Omni-1.5, an omni-modal model that not only has omni-modal understanding capabilities but also provides end-to-end audio generation capabilities. To achieve fluent and high-quality interaction across modalities without compromising the capabilities of any modality, we prioritized optimizing three key aspects. First, we establish a comprehensive data cleaning and synthesis pipeline for multimodal data, obtaining about 500B high-quality data (text, audio, and vision). Second, an audio-tokenizer (Baichuan-Audio-Tokenizer) has been designed to capture both semantic and acoustic information from audio, enabling seamless integration and enhanced compatibility with MLLM. Lastly, we designed a multi-stage training strategy that progressively integrates multimodal alignment and multitask fine-tuning, ensuring effective synergy across all modalities. Baichuan-Omni-1.5 leads contemporary models (including GPT4o-mini and MiniCPM-o 2.6) in terms of comprehensive omni-modal capabilities. Notably, it achieves results comparable to leading models such as Qwen2-VL-72B across various multimodal medical benchmarks.",
    "metadata": {
      "arxiv_id": "2501.15368",
      "title": "Baichuan-Omni-1.5 Technical Report",
      "summary": "We introduce Baichuan-Omni-1.5, an omni-modal model that not only has omni-modal understanding capabilities but also provides end-to-end audio generation capabilities. To achieve fluent and high-quality interaction across modalities without compromising the capabilities of any modality, we prioritized optimizing three key aspects. First, we establish a comprehensive data cleaning and synthesis pipeline for multimodal data, obtaining about 500B high-quality data (text, audio, and vision). Second, an audio-tokenizer (Baichuan-Audio-Tokenizer) has been designed to capture both semantic and acoustic information from audio, enabling seamless integration and enhanced compatibility with MLLM. Lastly, we designed a multi-stage training strategy that progressively integrates multimodal alignment and multitask fine-tuning, ensuring effective synergy across all modalities. Baichuan-Omni-1.5 leads contemporary models (including GPT4o-mini and MiniCPM-o 2.6) in terms of comprehensive omni-modal capabilities. Notably, it achieves results comparable to leading models such as Qwen2-VL-72B across various multimodal medical benchmarks.",
      "authors": [
        "Yadong Li",
        "Jun Liu",
        "Tao Zhang",
        "Tao Zhang",
        "Song Chen",
        "Tianpeng Li",
        "Zehuan Li",
        "Lijun Liu",
        "Lingfeng Ming",
        "Guosheng Dong",
        "Da Pan",
        "Chong Li",
        "Yuanbo Fang",
        "Dongdong Kuang",
        "Mingrui Wang",
        "Chenglin Zhu",
        "Youwei Zhang",
        "Hongyu Guo",
        "Fengyu Zhang",
        "Yuran Wang",
        "Bowen Ding",
        "Wei Song",
        "Xu Li",
        "Yuqi Huo",
        "Zheng Liang",
        "Shusen Zhang",
        "Xin Wu",
        "Shuai Zhao",
        "Linchu Xiong",
        "Yozhen Wu",
        "Jiahui Ye",
        "Wenhao Lu",
        "Bowen Li",
        "Yan Zhang",
        "Yaqi Zhou",
        "Xin Chen",
        "Lei Su",
        "Hongda Zhang",
        "Fuzhong Chen",
        "Xuezhen Dong",
        "Na Nie",
        "Zhiying Wu",
        "Bin Xiao",
        "Ting Li",
        "Shunya Dang",
        "Ping Zhang",
        "Yijia Sun",
        "Jincheng Wu",
        "Jinjie Yang",
        "Xionghai Lin",
        "Zhi Ma",
        "Kegeng Wu",
        "Jia li",
        "Aiyuan Yang",
        "Hui Liu",
        "Jianqiang Zhang",
        "Xiaoxi Chen",
        "Guangwei Ai",
        "Wentao Zhang",
        "Yicong Chen",
        "Xiaoqin Huang",
        "Kun Li",
        "Wenjing Luo",
        "Yifei Duan",
        "Lingling Zhu",
        "Ran Xiao",
        "Zhe Su",
        "Jiani Pu",
        "Dian Wang",
        "Xu Jia",
        "Tianyu Zhang",
        "Mengyu Ai",
        "Mang Wang",
        "Yujing Qiao",
        "Lei Zhang",
        "Yanjun Shen",
        "Fan Yang",
        "Miao Zhen",
        "Yijie Zhou",
        "Mingyang Chen",
        "Fei Li",
        "Chenzheng Zhu",
        "Keer Lu",
        "Yaqi Zhao",
        "Hao Liang",
        "Youquan Li",
        "Yanzhao Qin",
        "Linzhuang Sun",
        "Jianhua Xu",
        "Haoze Sun",
        "Mingan Lin",
        "Zenan Zhou",
        "Weipeng Chen"
      ],
      "published": "2025-01-26T02:19:03Z",
      "updated": "2025-01-26T02:19:03Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.15368v1",
      "landing_url": "https://arxiv.org/abs/2501.15368v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.15368"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract presents an omni-modal model with audio generation but does not center on Spoken Language Models (SLMs) or full-duplex speech-language modeling, nor discuss speech as the primary modeling object, synchronous interaction, or dedicated speech-language evaluation; thus it does not meet the inclusion criteria and aligns with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract presents an omni-modal model with audio generation but does not center on Spoken Language Models (SLMs) or full-duplex speech-language modeling, nor discuss speech as the primary modeling object, synchronous interaction, or dedicated speech-language evaluation; thus it does not meet the inclusion criteria and aligns with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The Baichuan-Omni-1.5 report focuses on an omni-modal model with audio generation and multimodal alignment, but the study does not explicitly center on Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models with speech as the primary modeling object, nor does it discuss full-duplex capabilities, turn-taking, or related interaction dynamics central to the inclusion criteria. The description mainly highlights audio-tokenizer design and multimodal data handling without meeting the detailed requirements regarding synchronous spoken interaction modeling or evaluation metrics related to real-time interaction or behavior arbitration. Hence, it does not meet the full inclusion criteria and aligns more with general multimodal AI modeling rather than focused SLM research.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The Baichuan-Omni-1.5 report focuses on an omni-modal model with audio generation and multimodal alignment, but the study does not explicitly center on Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models with speech as the primary modeling object, nor does it discuss full-duplex capabilities, turn-taking, or related interaction dynamics central to the inclusion criteria. The description mainly highlights audio-tokenizer design and multimodal data handling without meeting the detailed requirements regarding synchronous spoken interaction modeling or evaluation metrics related to real-time interaction or behavior arbitration. Hence, it does not meet the full inclusion criteria and aligns more with general multimodal AI modeling rather than focused SLM research.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Cross-Modal Transfer from Memes to Videos: Addressing Data Scarcity in Hateful Video Detection",
    "abstract": "Detecting hate speech in online content is essential to ensuring safer digital spaces. While significant progress has been made in text and meme modalities, video-based hate speech detection remains under-explored, hindered by a lack of annotated datasets and the high cost of video annotation. This gap is particularly problematic given the growing reliance on large models, which demand substantial amounts of training data. To address this challenge, we leverage meme datasets as both a substitution and an augmentation strategy for training hateful video detection models. Our approach introduces a human-assisted reannotation pipeline to align meme dataset labels with video datasets, ensuring consistency with minimal labeling effort. Using two state-of-the-art vision-language models, we demonstrate that meme data can substitute for video data in resource-scarce scenarios and augment video datasets to achieve further performance gains. Our results consistently outperform state-of-the-art benchmarks, showcasing the potential of cross-modal transfer learning for advancing hateful video detection. Dataset and code are available at https://github.com/Social-AI-Studio/CrossModalTransferLearning.",
    "metadata": {
      "arxiv_id": "2501.15438",
      "title": "Cross-Modal Transfer from Memes to Videos: Addressing Data Scarcity in Hateful Video Detection",
      "summary": "Detecting hate speech in online content is essential to ensuring safer digital spaces. While significant progress has been made in text and meme modalities, video-based hate speech detection remains under-explored, hindered by a lack of annotated datasets and the high cost of video annotation. This gap is particularly problematic given the growing reliance on large models, which demand substantial amounts of training data. To address this challenge, we leverage meme datasets as both a substitution and an augmentation strategy for training hateful video detection models. Our approach introduces a human-assisted reannotation pipeline to align meme dataset labels with video datasets, ensuring consistency with minimal labeling effort. Using two state-of-the-art vision-language models, we demonstrate that meme data can substitute for video data in resource-scarce scenarios and augment video datasets to achieve further performance gains. Our results consistently outperform state-of-the-art benchmarks, showcasing the potential of cross-modal transfer learning for advancing hateful video detection. Dataset and code are available at https://github.com/Social-AI-Studio/CrossModalTransferLearning.",
      "authors": [
        "Han Wang",
        "Rui Yang Tan",
        "Roy Ka-Wei Lee"
      ],
      "published": "2025-01-26T07:50:14Z",
      "updated": "2025-01-26T07:50:14Z",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.15438v1",
      "landing_url": "https://arxiv.org/abs/2501.15438v1",
      "doi": "https://doi.org/10.1145/3696410.3714534"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper addresses cross-modal transfer from memes to videos for hate-speech detection using vision-language models, with no emphasis on Spoken Language Models or speech-centric modeling, so it does not meet the SLM-focused inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper addresses cross-modal transfer from memes to videos for hate-speech detection using vision-language models, with no emphasis on Spoken Language Models or speech-centric modeling, so it does not meet the SLM-focused inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on detecting hate speech in videos using cross-modal transfer learning from memes, with no mentioned focus on Spoken Language Models, speech-level language modeling, or full-duplex spoken interaction capabilities, thus failing to meet the core inclusion criteria related to speech modeling and interactive spoken language systems.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on detecting hate speech in videos using cross-modal transfer learning from memes, with no mentioned focus on Spoken Language Models, speech-level language modeling, or full-duplex spoken interaction capabilities, thus failing to meet the core inclusion criteria related to speech modeling and interactive spoken language systems.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced Polyphone Disambiguation -- Challenges and Insights",
    "abstract": "We present BreezyVoice, a Text-to-Speech (TTS) system specifically adapted for Taiwanese Mandarin, highlighting phonetic control abilities to address the unique challenges of polyphone disambiguation in the language. Building upon CosyVoice, we incorporate a $S^{3}$ tokenizer, a large language model (LLM), an optimal-transport conditional flow matching model (OT-CFM), and a grapheme to phoneme prediction model, to generate realistic speech that closely mimics human utterances. Our evaluation demonstrates BreezyVoice's superior performance in both general and code-switching contexts, highlighting its robustness and effectiveness in generating high-fidelity speech. Additionally, we address the challenges of generalizability in modeling long-tail speakers and polyphone disambiguation. Our approach significantly enhances performance and offers valuable insights into the workings of neural codec TTS systems.",
    "metadata": {
      "arxiv_id": "2501.17790",
      "title": "BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced Polyphone Disambiguation -- Challenges and Insights",
      "summary": "We present BreezyVoice, a Text-to-Speech (TTS) system specifically adapted for Taiwanese Mandarin, highlighting phonetic control abilities to address the unique challenges of polyphone disambiguation in the language. Building upon CosyVoice, we incorporate a $S^{3}$ tokenizer, a large language model (LLM), an optimal-transport conditional flow matching model (OT-CFM), and a grapheme to phoneme prediction model, to generate realistic speech that closely mimics human utterances. Our evaluation demonstrates BreezyVoice's superior performance in both general and code-switching contexts, highlighting its robustness and effectiveness in generating high-fidelity speech. Additionally, we address the challenges of generalizability in modeling long-tail speakers and polyphone disambiguation. Our approach significantly enhances performance and offers valuable insights into the workings of neural codec TTS systems.",
      "authors": [
        "Chan-Jan Hsu",
        "Yi-Cheng Lin",
        "Chia-Chun Lin",
        "Wei-Chih Chen",
        "Ho Lam Chung",
        "Chen-An Li",
        "Yi-Chang Chen",
        "Chien-Yu Yu",
        "Ming-Ji Lee",
        "Chien-Cheng Chen",
        "Ru-Heng Huang",
        "Hung-yi Lee",
        "Da-Shan Shiu"
      ],
      "published": "2025-01-29T17:31:26Z",
      "updated": "2025-01-29T17:31:26Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.17790v1",
      "landing_url": "https://arxiv.org/abs/2501.17790v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.17790"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on a TTS system for Taiwanese Mandarin and polyphone disambiguation rather than on Spoken Language Models (SLMs) or Speech-Language Modeling with interactive, full-duplex capabilities; it does not emphasize SLM/FD-SLM architecture, training, or evaluation in a way that aligns with the stated inclusion criteria and clearly falls under the exclusion scope for single-task TTS without dialogue/interaction modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on a TTS system for Taiwanese Mandarin and polyphone disambiguation rather than on Spoken Language Models (SLMs) or Speech-Language Modeling with interactive, full-duplex capabilities; it does not emphasize SLM/FD-SLM architecture, training, or evaluation in a way that aligns with the stated inclusion criteria and clearly falls under the exclusion scope for single-task TTS without dialogue/interaction modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a TTS system adapting to Taiwanese Mandarin with enhanced polyphone disambiguation and emphasizes phonetic control and realistic speech generation; however, it primarily addresses speech synthesis technology without discussing spoken language models, full-duplex interactions, or synchronization mechanisms relevant to spoken language modeling and interactive dialogue behaviors, and it does not mention speech language modeling clearly in the title or abstract, thus failing core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a TTS system adapting to Taiwanese Mandarin with enhanced polyphone disambiguation and emphasizes phonetic control and realistic speech generation; however, it primarily addresses speech synthesis technology without discussing spoken language models, full-duplex interactions, or synchronization mechanisms relevant to spoken language modeling and interactive dialogue behaviors, and it does not mention speech language modeling clearly in the title or abstract, thus failing core inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Beyond Turn-taking: Introducing Text-based Overlap into Human-LLM Interactions",
    "abstract": "Traditional text-based human-AI interactions often adhere to a strict turn-taking approach. In this research, we propose a novel approach that incorporates overlapping messages, mirroring natural human conversations. Through a formative study, we observed that even in text-based contexts, users instinctively engage in overlapping behaviors like \"A: Today I went to-\" \"B: yeah.\" To capitalize on these insights, we developed OverlapBot, a prototype chatbot where both AI and users can initiate overlapping. Our user study revealed that OverlapBot was perceived as more communicative and immersive than traditional turn-taking chatbot, fostering faster and more natural interactions. Our findings contribute to the understanding of design space for overlapping interactions. We also provide recommendations for implementing overlap-capable AI interactions to enhance the fluidity and engagement of text-based conversations.",
    "metadata": {
      "arxiv_id": "2501.18103",
      "title": "Beyond Turn-taking: Introducing Text-based Overlap into Human-LLM Interactions",
      "summary": "Traditional text-based human-AI interactions often adhere to a strict turn-taking approach. In this research, we propose a novel approach that incorporates overlapping messages, mirroring natural human conversations. Through a formative study, we observed that even in text-based contexts, users instinctively engage in overlapping behaviors like \"A: Today I went to-\" \"B: yeah.\" To capitalize on these insights, we developed OverlapBot, a prototype chatbot where both AI and users can initiate overlapping. Our user study revealed that OverlapBot was perceived as more communicative and immersive than traditional turn-taking chatbot, fostering faster and more natural interactions. Our findings contribute to the understanding of design space for overlapping interactions. We also provide recommendations for implementing overlap-capable AI interactions to enhance the fluidity and engagement of text-based conversations.",
      "authors": [
        "JiWoo Kim",
        "Minsuk Chang",
        "JinYeong Bak"
      ],
      "published": "2025-01-30T03:01:01Z",
      "updated": "2025-01-30T03:01:01Z",
      "categories": [
        "cs.HC",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.18103v1",
      "landing_url": "https://arxiv.org/abs/2501.18103v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.18103"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on text-based overlap in human–LLM dialogue and does not treat Spoken Language Models or any speech-centric modeling as the core object, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on text-based overlap in human–LLM dialogue and does not treat Spoken Language Models or any speech-centric modeling as the core object, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on text-based human-AI interactions and proposes overlapping messages in chatbots, without addressing spoken language modeling, speech signal processing, or full-duplex spoken language systems; thus, it does not meet the inclusion criteria centered on spoken language models or speech-based full-duplex interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on text-based human-AI interactions and proposes overlapping messages in chatbots, without addressing spoken language modeling, speech signal processing, or full-duplex spoken language systems; thus, it does not meet the inclusion criteria centered on spoken language models or speech-based full-duplex interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Adaptive Video Streaming with AI-Based Optimization for Dynamic Network Conditions",
    "abstract": "The increase in video streaming has presented a challenge of handling stream request effectively, especially over networks that are variable. This paper describes a new adaptive video streaming architecture capable of changing the video quality and buffer size depending on the data and latency of streamed video. For video streaming VLC media player was used where network performance data were obtained through Python scripts with very accurate data rate and latency measurement. The collected data is analyzed using Gemini AI, containing characteristics of the machine learning algorithm that recognizes the best resolution of videos and the buffer sizes. Through the features of real-time monitoring and artificial intelligence decision making, the proposed framework improves the user experience by reducing the occurrence of buffering events while at the same time increasing the video quality. Our findings therefore confirm that the proposed solution based on artificial intelligence increases video quality and flexibility. This study advances knowledge of adaptive streaming and offers an argument about how intelligent datadriven approaches and AI may be useful tools for enhancing the delivery of video in practical environments.",
    "metadata": {
      "arxiv_id": "2501.18332",
      "title": "Adaptive Video Streaming with AI-Based Optimization for Dynamic Network Conditions",
      "summary": "The increase in video streaming has presented a challenge of handling stream request effectively, especially over networks that are variable. This paper describes a new adaptive video streaming architecture capable of changing the video quality and buffer size depending on the data and latency of streamed video. For video streaming VLC media player was used where network performance data were obtained through Python scripts with very accurate data rate and latency measurement. The collected data is analyzed using Gemini AI, containing characteristics of the machine learning algorithm that recognizes the best resolution of videos and the buffer sizes. Through the features of real-time monitoring and artificial intelligence decision making, the proposed framework improves the user experience by reducing the occurrence of buffering events while at the same time increasing the video quality. Our findings therefore confirm that the proposed solution based on artificial intelligence increases video quality and flexibility. This study advances knowledge of adaptive streaming and offers an argument about how intelligent datadriven approaches and AI may be useful tools for enhancing the delivery of video in practical environments.",
      "authors": [
        "Mohammad Tarik",
        "Qutaiba Ibrahim"
      ],
      "published": "2025-01-30T13:20:23Z",
      "updated": "2025-01-30T13:20:23Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.18332v1",
      "landing_url": "https://arxiv.org/abs/2501.18332v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.18332"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此論文主題為自適應視訊串流與 AI 優化，核心為視訊品質與緩衝控制，與 Spoken Language Models / 語音語言建模無關，亦非對話互動評估，因此不符合納入條件，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此論文主題為自適應視訊串流與 AI 優化，核心為視訊品質與緩衝控制，與 Spoken Language Models / 語音語言建模無關，亦非對話互動評估，因此不符合納入條件，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on adaptive video streaming optimized by AI, without discussing spoken language models, speech language modeling, or full-duplex spoken interaction systems, which are required by the inclusion criteria; it also does not address synchronization, cognitive parallelism, or related speech-centric model architectures or evaluations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on adaptive video streaming optimized by AI, without discussing spoken language models, speech language modeling, or full-duplex spoken interaction systems, which are required by the inclusion criteria; it also does not address synchronization, cognitive parallelism, or related speech-centric model architectures or evaluations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "STRIELAD -- A Scalable Toolkit for Real-time Interactive Exploration of Large Atmospheric Datasets",
    "abstract": "Technological advances in high performance computing and maturing physical models allow scientists to simulate weather and climate evolutions with an increasing accuracy. While this improved accuracy allows us to explore complex dynamical interactions within such physical systems, inconceivable a few years ago, it also results in grand challenges regarding the data visualization and analytics process. We present STRIELAD, a scalable weather analytics toolkit, which allows for interactive exploration and real-time visualization of such large scale datasets. It combines parallel and distributed feature extraction using high-performance computing resources with smart level-of-detail rendering methods to assure interactivity during the complete analysis process.",
    "metadata": {
      "arxiv_id": "2502.00033",
      "title": "STRIELAD -- A Scalable Toolkit for Real-time Interactive Exploration of Large Atmospheric Datasets",
      "summary": "Technological advances in high performance computing and maturing physical models allow scientists to simulate weather and climate evolutions with an increasing accuracy. While this improved accuracy allows us to explore complex dynamical interactions within such physical systems, inconceivable a few years ago, it also results in grand challenges regarding the data visualization and analytics process. We present STRIELAD, a scalable weather analytics toolkit, which allows for interactive exploration and real-time visualization of such large scale datasets. It combines parallel and distributed feature extraction using high-performance computing resources with smart level-of-detail rendering methods to assure interactivity during the complete analysis process.",
      "authors": [
        "Simon Schneegans",
        "Lori Neary",
        "Markus Flatken",
        "Andreas Gerndt"
      ],
      "published": "2025-01-24T09:31:22Z",
      "updated": "2025-01-24T09:31:22Z",
      "categories": [
        "cs.HC",
        "cs.DC",
        "cs.GR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.00033v1",
      "landing_url": "https://arxiv.org/abs/2502.00033v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.00033"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item STRIELAD focuses on a scalable toolkit for real-time exploration of large atmospheric datasets using HPC and visualization, not on Spoken Language Models or any speech-language modeling concepts, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item STRIELAD focuses on a scalable toolkit for real-time exploration of large atmospheric datasets using HPC and visualization, not on Spoken Language Models or any speech-language modeling concepts, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a scalable toolkit for real-time interactive exploration of large atmospheric datasets, emphasizing data visualization and analytics in weather simulations, and does not address spoken language models, speech language modeling, full-duplex spoken language interactions, or any related speech processing and evaluation aspects relevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a scalable toolkit for real-time interactive exploration of large atmospheric datasets, emphasizing data visualization and analytics in weather simulations, and does not address spoken language models, speech language modeling, full-duplex spoken language interactions, or any related speech processing and evaluation aspects relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "AudioGenX: Explainability on Text-to-Audio Generative Models",
    "abstract": "Text-to-audio generation models (TAG) have achieved significant advances in generating audio conditioned on text descriptions. However, a critical challenge lies in the lack of transparency regarding how each textual input impacts the generated audio. To address this issue, we introduce AudioGenX, an Explainable AI (XAI) method that provides explanations for text-to-audio generation models by highlighting the importance of input tokens. AudioGenX optimizes an Explainer by leveraging factual and counterfactual objective functions to provide faithful explanations at the audio token level. This method offers a detailed and comprehensive understanding of the relationship between text inputs and audio outputs, enhancing both the explainability and trustworthiness of TAG models. Extensive experiments demonstrate the effectiveness of AudioGenX in producing faithful explanations, benchmarked against existing methods using novel evaluation metrics specifically designed for audio generation tasks.",
    "metadata": {
      "arxiv_id": "2502.00459",
      "title": "AudioGenX: Explainability on Text-to-Audio Generative Models",
      "summary": "Text-to-audio generation models (TAG) have achieved significant advances in generating audio conditioned on text descriptions. However, a critical challenge lies in the lack of transparency regarding how each textual input impacts the generated audio. To address this issue, we introduce AudioGenX, an Explainable AI (XAI) method that provides explanations for text-to-audio generation models by highlighting the importance of input tokens. AudioGenX optimizes an Explainer by leveraging factual and counterfactual objective functions to provide faithful explanations at the audio token level. This method offers a detailed and comprehensive understanding of the relationship between text inputs and audio outputs, enhancing both the explainability and trustworthiness of TAG models. Extensive experiments demonstrate the effectiveness of AudioGenX in producing faithful explanations, benchmarked against existing methods using novel evaluation metrics specifically designed for audio generation tasks.",
      "authors": [
        "Hyunju Kang",
        "Geonhee Han",
        "Yoonjae Jeong",
        "Hogun Park"
      ],
      "published": "2025-02-01T15:37:42Z",
      "updated": "2025-02-04T04:00:01Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.00459v2",
      "landing_url": "https://arxiv.org/abs/2502.00459v2",
      "doi": "https://doi.org/10.1609/aaai.v39i17.33950"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the abstract addresses explainability for text-to-audio generation, it does not center on Spoken Language Models or Speech Language Modeling as the core object, nor discuss synchronous dialogue, turn-taking, or audio-centric architecture/train/evaluation in the sense required by the inclusion criteria; it is more about general text-to-audio generation and XAI rather than SLM/FD-SLM with audio as the modeling target.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the abstract addresses explainability for text-to-audio generation, it does not center on Spoken Language Models or Speech Language Modeling as the core object, nor discuss synchronous dialogue, turn-taking, or audio-centric architecture/train/evaluation in the sense required by the inclusion criteria; it is more about general text-to-audio generation and XAI rather than SLM/FD-SLM with audio as the modeling target.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on explainability for text-to-audio generative models, centering on how text input affects generated audio outputs. Although it deals with audio generation conditioned on textual inputs, it does not explicitly focus on Spoken Language Models (SLMs) or Full-Duplex SLMs, nor does it discuss architecture, training, or evaluation in those contexts. The emphasis is on explainability rather than language modeling of speech as the core task. Therefore, it does not meet the inclusion criteria that require modeling speech as the primary object with clear SLM or FD-SLM context, and it may fall under exclusion criteria, particularly because it treats audio generation from text rather than speech language modeling or synchronous interactive speech behaviors.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on explainability for text-to-audio generative models, centering on how text input affects generated audio outputs. Although it deals with audio generation conditioned on textual inputs, it does not explicitly focus on Spoken Language Models (SLMs) or Full-Duplex SLMs, nor does it discuss architecture, training, or evaluation in those contexts. The emphasis is on explainability rather than language modeling of speech as the core task. Therefore, it does not meet the inclusion criteria that require modeling speech as the primary object with clear SLM or FD-SLM context, and it may fall under exclusion criteria, particularly because it treats audio generation from text rather than speech language modeling or synchronous interactive speech behaviors.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling",
    "abstract": "Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \\textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability.",
    "metadata": {
      "arxiv_id": "2502.02942",
      "title": "GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling",
      "summary": "Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \\textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability.",
      "authors": [
        "Jixun Yao",
        "Hexin Liu",
        "Chen Chen",
        "Yuchen Hu",
        "EngSiong Chng",
        "Lei Xie"
      ],
      "published": "2025-02-05T07:14:39Z",
      "updated": "2025-02-05T07:14:39Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.02942v1",
      "landing_url": "https://arxiv.org/abs/2502.02942v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.02942"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work uses language-model-based processing on speech tokens as the primary modeling object (speech-language modeling) within the last 3 years, with no exclusive focus on single-task ASR/TTS or synchronous interaction, so it should be included.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The work uses language-model-based processing on speech tokens as the primary modeling object (speech-language modeling) within the last 3 years, with no exclusive focus on single-task ASR/TTS or synchronous interaction, so it should be included.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a speech enhancement model that leverages language models for semantic information, framing the task as conditional language modeling of speech tokens, which aligns with spoken language modeling concepts. However, it primarily targets speech enhancement quality and does not explicitly address full-duplex interaction capabilities such as turn-taking, cognitive parallelism, or synchronous dialogue behavior; it also lacks explicit discussion of models supporting low-latency real-time interaction or behavioral arbitration. The emphasis is on enhancement, not explicitly on spoken language modeling as a core for synchronous or full-duplex interaction. Therefore, while relevant to speech-language modeling, it partially meets the inclusion criteria but seems to fall short regarding full-duplex synchronous interaction focus and evaluation metrics regarding interaction behaviors. Hence, the inclusion score leans towards uncertain inclusion but not strong enough for definite inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a speech enhancement model that leverages language models for semantic information, framing the task as conditional language modeling of speech tokens, which aligns with spoken language modeling concepts. However, it primarily targets speech enhancement quality and does not explicitly address full-duplex interaction capabilities such as turn-taking, cognitive parallelism, or synchronous dialogue behavior; it also lacks explicit discussion of models supporting low-latency real-time interaction or behavioral arbitration. The emphasis is on enhancement, not explicitly on spoken language modeling as a core for synchronous or full-duplex interaction. Therefore, while relevant to speech-language modeling, it partially meets the inclusion criteria but seems to fall short regarding full-duplex synchronous interaction focus and evaluation metrics regarding interaction behaviors. Hence, the inclusion score leans towards uncertain inclusion but not strong enough for definite inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers observed this work reframes speech enhancement as conditional language modeling over discrete semantic and acoustic tokens (JuniorNano: 4, JuniorMini: 3). The paper explicitly treats speech as the modeling object and uses language-model-based approaches (tokenization, hierarchical LM stages, token prompting), so it meets the core Spoken Language Model criterion; however it does not address full‑duplex interaction, turn‑taking, low‑latency or overlap/behavior arbitration evaluations required for FD‑SLM work. Therefore I recommend inclusion but not as a definitive FD‑SLM contribution.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers observed this work reframes speech enhancement as conditional language modeling over discrete semantic and acoustic tokens (JuniorNano: 4, JuniorMini: 3). The paper explicitly treats speech as the modeling object and uses language-model-based approaches (tokenization, hierarchical LM stages, token prompting), so it meets the core Spoken Language Model criterion; however it does not address full‑duplex interaction, turn‑taking, low‑latency or overlap/behavior arbitration evaluations required for FD‑SLM work. Therefore I recommend inclusion but not as a definitive FD‑SLM contribution.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "High-Fidelity Simultaneous Speech-To-Speech Translation",
    "abstract": "We introduce Hibiki, a decoder-only model for simultaneous speech translation. Hibiki leverages a multistream language model to synchronously process source and target speech, and jointly produces text and audio tokens to perform speech-to-text and speech-to-speech translation. We furthermore address the fundamental challenge of simultaneous interpretation, which unlike its consecutive counterpart, where one waits for the end of the source utterance to start translating, adapts its flow to accumulate just enough context to produce a correct translation in real-time, chunk by chunk. To do so, we introduce a weakly-supervised method that leverages the perplexity of an off-the-shelf text translation system to identify optimal delays on a per-word basis and create aligned synthetic data. After supervised training, Hibiki performs adaptive, simultaneous speech translation with vanilla temperature sampling. On a French-English simultaneous speech translation task, Hibiki demonstrates state-of-the-art performance in translation quality, speaker fidelity and naturalness. Moreover, the simplicity of its inference process makes it compatible with batched translation and even real-time on-device deployment. We provide examples as well as models and inference code.",
    "metadata": {
      "arxiv_id": "2502.03382",
      "title": "High-Fidelity Simultaneous Speech-To-Speech Translation",
      "summary": "We introduce Hibiki, a decoder-only model for simultaneous speech translation. Hibiki leverages a multistream language model to synchronously process source and target speech, and jointly produces text and audio tokens to perform speech-to-text and speech-to-speech translation. We furthermore address the fundamental challenge of simultaneous interpretation, which unlike its consecutive counterpart, where one waits for the end of the source utterance to start translating, adapts its flow to accumulate just enough context to produce a correct translation in real-time, chunk by chunk. To do so, we introduce a weakly-supervised method that leverages the perplexity of an off-the-shelf text translation system to identify optimal delays on a per-word basis and create aligned synthetic data. After supervised training, Hibiki performs adaptive, simultaneous speech translation with vanilla temperature sampling. On a French-English simultaneous speech translation task, Hibiki demonstrates state-of-the-art performance in translation quality, speaker fidelity and naturalness. Moreover, the simplicity of its inference process makes it compatible with batched translation and even real-time on-device deployment. We provide examples as well as models and inference code.",
      "authors": [
        "Tom Labiausse",
        "Laurent Mazaré",
        "Edouard Grave",
        "Patrick Pérez",
        "Alexandre Défossez",
        "Neil Zeghidour"
      ],
      "published": "2025-02-05T17:18:55Z",
      "updated": "2025-02-26T09:31:58Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.03382v2",
      "landing_url": "https://arxiv.org/abs/2502.03382v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.03382"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on a full-duplex spoken-language modeling approach (simultaneous speech translation using a multistream language model that jointly processes source and target speech and outputs text and audio), includes architecture/training/evaluation and reports bilingual task performance, aligning with the Spoken Language Models / FD-SLMs scope within the last 3 years.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This item centers on a full-duplex spoken-language modeling approach (simultaneous speech translation using a multistream language model that jointly processes source and target speech and outputs text and audio), includes architecture/training/evaluation and reports bilingual task performance, aligning with the Spoken Language Models / FD-SLMs scope within the last 3 years.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a simultaneous speech-to-speech translation model that jointly processes source and target speech, addressing the challenge of real-time translation with adaptive delay strategies. However, the abstract does not explicitly mention spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs), nor does it explicitly frame the model as a spoken language model; rather, it emphasizes translation and simultaneous interpretation. Additionally, there is no direct mention of cognitive parallelism, speech overlap management, backchanneling, or detailed evaluation metrics related to interactive, full-duplex spoken interactions. The work revolves around speech translation with text and speech token generation, which aligns partially but does not clearly meet the core inclusion requirement of modeling and reasoning with spoken language models as the main focus. Therefore, it is better to exclude or at least uncertain for inclusion given the explicit criteria requiring SLM/FD-SLM focus and evaluation of synchronization and interaction aspects.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a simultaneous speech-to-speech translation model that jointly processes source and target speech, addressing the challenge of real-time translation with adaptive delay strategies. However, the abstract does not explicitly mention spoken language models (SLMs) or full-duplex spoken language models (FD-SLMs), nor does it explicitly frame the model as a spoken language model; rather, it emphasizes translation and simultaneous interpretation. Additionally, there is no direct mention of cognitive parallelism, speech overlap management, backchanneling, or detailed evaluation metrics related to interactive, full-duplex spoken interactions. The work revolves around speech translation with text and speech token generation, which aligns partially but does not clearly meet the core inclusion requirement of modeling and reasoning with spoken language models as the main focus. Therefore, it is better to exclude or at least uncertain for inclusion given the explicit criteria requiring SLM/FD-SLM focus and evaluation of synchronization and interaction aspects.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "The abstract explicitly presents a decoder-only \"multistream language model\" that jointly processes source and target speech and generates audio tokens for adaptive, simultaneous speech-to-speech translation—constituting end-to-end language modeling of speech with learned synchronization and concrete training/evaluation, so it meets the SLM/FD-SLM inclusion criteria; I note the junior reviewers split (one included, one excluded for wording concerns), but the abstract uses equivalent and specific speech-language-model terminology and functionality, justifying inclusion.",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "The abstract explicitly presents a decoder-only \"multistream language model\" that jointly processes source and target speech and generates audio tokens for adaptive, simultaneous speech-to-speech translation—constituting end-to-end language modeling of speech with learned synchronization and concrete training/evaluation, so it meets the SLM/FD-SLM inclusion criteria; I note the junior reviewers split (one included, one excluded for wording concerns), but the abstract uses equivalent and specific speech-language-model terminology and functionality, justifying inclusion.",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Efficient Evaluation of Quantization-Effects in Neural Codecs",
    "abstract": "Neural codecs, comprising an encoder, quantizer, and decoder, enable signal transmission at exceptionally low bitrates. Training these systems requires techniques like the straight-through estimator, soft-to-hard annealing, or statistical quantizer emulation to allow a non-zero gradient across the quantizer. Evaluating the effect of quantization in neural codecs, like the influence of gradient passing techniques on the whole system, is often costly and time-consuming due to training demands and the lack of affordable and reliable metrics. This paper proposes an efficient evaluation framework for neural codecs using simulated data with a defined number of bits and low-complexity neural encoders/decoders to emulate the non-linear behavior in larger networks. Our system is highly efficient in terms of training time and computational and hardware requirements, allowing us to uncover distinct behaviors in neural codecs. We propose a modification to stabilize training with the straight-through estimator based on our findings. We validate our findings against an internal neural audio codec and against the state-of-the-art descript-audio-codec.",
    "metadata": {
      "arxiv_id": "2502.04770",
      "title": "Efficient Evaluation of Quantization-Effects in Neural Codecs",
      "summary": "Neural codecs, comprising an encoder, quantizer, and decoder, enable signal transmission at exceptionally low bitrates. Training these systems requires techniques like the straight-through estimator, soft-to-hard annealing, or statistical quantizer emulation to allow a non-zero gradient across the quantizer. Evaluating the effect of quantization in neural codecs, like the influence of gradient passing techniques on the whole system, is often costly and time-consuming due to training demands and the lack of affordable and reliable metrics. This paper proposes an efficient evaluation framework for neural codecs using simulated data with a defined number of bits and low-complexity neural encoders/decoders to emulate the non-linear behavior in larger networks. Our system is highly efficient in terms of training time and computational and hardware requirements, allowing us to uncover distinct behaviors in neural codecs. We propose a modification to stabilize training with the straight-through estimator based on our findings. We validate our findings against an internal neural audio codec and against the state-of-the-art descript-audio-codec.",
      "authors": [
        "Wolfgang Mack",
        "Ahmed Mustafa",
        "Rafał Łaganowski",
        "Samer Hijazy"
      ],
      "published": "2025-02-07T09:11:19Z",
      "updated": "2025-02-07T09:11:19Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.04770v1",
      "landing_url": "https://arxiv.org/abs/2502.04770v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.04770"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此題聚焦神經編碼器的量化效應評估，核心是語音語言模型與端到端的語音建模，但文中未以 Spoken Language Models / speech language models 為核心，亦未討論語音互動、對話行為仲裁等與語音語言建模直接相關的評估指標，故不符合納入條件，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此題聚焦神經編碼器的量化效應評估，核心是語音語言模型與端到端的語音建模，但文中未以 Spoken Language Models / speech language models 為核心，亦未討論語音互動、對話行為仲裁等與語音語言建模直接相關的評估指標，故不符合納入條件，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on neural codecs and efficient evaluation of quantization effects, which are technical aspects related to signal compression and transmission rather than spoken language modeling or full-duplex spoken language interactive systems. It does not mention spoken language models, end-to-end speech language model architectures, full-duplex interaction, cognitive parallelism, or related evaluation metrics focusing on synchronous speech interaction. Hence it does not meet the inclusion criteria centered on spoken language models and related synchronization or interactive capabilities, nor does it violate an explicit exclusion criterion, but it is outside the targeted research scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on neural codecs and efficient evaluation of quantization effects, which are technical aspects related to signal compression and transmission rather than spoken language modeling or full-duplex spoken language interactive systems. It does not mention spoken language models, end-to-end speech language model architectures, full-duplex interaction, cognitive parallelism, or related evaluation metrics focusing on synchronous speech interaction. Hence it does not meet the inclusion criteria centered on spoken language models and related synchronization or interactive capabilities, nor does it violate an explicit exclusion criterion, but it is outside the targeted research scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "FreeBlend: Advancing Concept Blending with Staged Feedback-Driven Interpolation Diffusion",
    "abstract": "Concept blending is a promising yet underexplored area in generative models. While recent approaches, such as embedding mixing and latent modification based on structural sketches, have been proposed, they often suffer from incompatible semantic information and discrepancies in shape and appearance. In this work, we introduce FreeBlend, an effective, training-free framework designed to address these challenges. To mitigate cross-modal loss and enhance feature detail, we leverage transferred image embeddings as conditional inputs. The framework employs a stepwise increasing interpolation strategy between latents, progressively adjusting the blending ratio to seamlessly integrate auxiliary features. Additionally, we introduce a feedback-driven mechanism that updates the auxiliary latents in reverse order, facilitating global blending and preventing rigid or unnatural outputs. Extensive experiments demonstrate that our method significantly improves both the semantic coherence and visual quality of blended images, yielding compelling and coherent results.",
    "metadata": {
      "arxiv_id": "2502.05606",
      "title": "FreeBlend: Advancing Concept Blending with Staged Feedback-Driven Interpolation Diffusion",
      "summary": "Concept blending is a promising yet underexplored area in generative models. While recent approaches, such as embedding mixing and latent modification based on structural sketches, have been proposed, they often suffer from incompatible semantic information and discrepancies in shape and appearance. In this work, we introduce FreeBlend, an effective, training-free framework designed to address these challenges. To mitigate cross-modal loss and enhance feature detail, we leverage transferred image embeddings as conditional inputs. The framework employs a stepwise increasing interpolation strategy between latents, progressively adjusting the blending ratio to seamlessly integrate auxiliary features. Additionally, we introduce a feedback-driven mechanism that updates the auxiliary latents in reverse order, facilitating global blending and preventing rigid or unnatural outputs. Extensive experiments demonstrate that our method significantly improves both the semantic coherence and visual quality of blended images, yielding compelling and coherent results.",
      "authors": [
        "Yufan Zhou",
        "Haoyu Shen",
        "Huan Wang"
      ],
      "published": "2025-02-08T15:25:03Z",
      "updated": "2025-11-09T11:33:47Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.05606v3",
      "landing_url": "https://arxiv.org/abs/2502.05606v3",
      "doi": "https://doi.org/10.48550/arXiv.2502.05606"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on image generation and concept blending using diffusion, with no discussion of Spoken Language Models, speech, or dialogue synchronization, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on image generation and concept blending using diffusion, with no discussion of Spoken Language Models, speech, or dialogue synchronization, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a framework for concept blending in image generation and does not focus on Spoken Language Models, speech-based modeling, full-duplex spoken interaction, or any of the required speech-related concepts in the inclusion criteria. It neither addresses language modeling of speech nor discusses synchronization or cognitive parallelism in speech interaction; thus, it does not meet any inclusion criteria and matches several exclusion criteria such as lacking speech-centric modeling and interaction focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents a framework for concept blending in image generation and does not focus on Spoken Language Models, speech-based modeling, full-duplex spoken interaction, or any of the required speech-related concepts in the inclusion criteria. It neither addresses language modeling of speech nor discusses synchronization or cognitive parallelism in speech interaction; thus, it does not meet any inclusion criteria and matches several exclusion criteria such as lacking speech-centric modeling and interaction focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "HDT: Hierarchical Discrete Transformer for Multivariate Time Series Forecasting",
    "abstract": "Generative models have gained significant attention in multivariate time series forecasting (MTS), particularly due to their ability to generate high-fidelity samples. Forecasting the probability distribution of multivariate time series is a challenging yet practical task. Although some recent attempts have been made to handle this task, two major challenges persist: 1) some existing generative methods underperform in high-dimensional multivariate time series forecasting, which is hard to scale to higher dimensions; 2) the inherent high-dimensional multivariate attributes constrain the forecasting lengths of existing generative models. In this paper, we point out that discrete token representations can model high-dimensional MTS with faster inference time, and forecasting the target with long-term trends of itself can extend the forecasting length with high accuracy. Motivated by this, we propose a vector quantized framework called Hierarchical Discrete Transformer (HDT) that models time series into discrete token representations with l2 normalization enhanced vector quantized strategy, in which we transform the MTS forecasting into discrete tokens generation. To address the limitations of generative models in long-term forecasting, we propose a hierarchical discrete Transformer. This model captures the discrete long-term trend of the target at the low level and leverages this trend as a condition to generate the discrete representation of the target at the high level that introduces the features of the target itself to extend the forecasting length in high-dimensional MTS. Extensive experiments on five popular MTS datasets verify the effectiveness of our proposed method.",
    "metadata": {
      "arxiv_id": "2502.08302",
      "title": "HDT: Hierarchical Discrete Transformer for Multivariate Time Series Forecasting",
      "summary": "Generative models have gained significant attention in multivariate time series forecasting (MTS), particularly due to their ability to generate high-fidelity samples. Forecasting the probability distribution of multivariate time series is a challenging yet practical task. Although some recent attempts have been made to handle this task, two major challenges persist: 1) some existing generative methods underperform in high-dimensional multivariate time series forecasting, which is hard to scale to higher dimensions; 2) the inherent high-dimensional multivariate attributes constrain the forecasting lengths of existing generative models. In this paper, we point out that discrete token representations can model high-dimensional MTS with faster inference time, and forecasting the target with long-term trends of itself can extend the forecasting length with high accuracy. Motivated by this, we propose a vector quantized framework called Hierarchical Discrete Transformer (HDT) that models time series into discrete token representations with l2 normalization enhanced vector quantized strategy, in which we transform the MTS forecasting into discrete tokens generation. To address the limitations of generative models in long-term forecasting, we propose a hierarchical discrete Transformer. This model captures the discrete long-term trend of the target at the low level and leverages this trend as a condition to generate the discrete representation of the target at the high level that introduces the features of the target itself to extend the forecasting length in high-dimensional MTS. Extensive experiments on five popular MTS datasets verify the effectiveness of our proposed method.",
      "authors": [
        "Shibo Feng",
        "Peilin Zhao",
        "Liu Liu",
        "Pengcheng Wu",
        "Zhiqi Shen"
      ],
      "published": "2025-02-12T11:03:51Z",
      "updated": "2025-02-12T11:03:51Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.08302v1",
      "landing_url": "https://arxiv.org/abs/2502.08302v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.08302"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on hierarchical discrete transformers for multivariate time series forecasting using vector-quantized discrete tokens, which is a time-series modeling method rather than Spoken Language Models or speech-language modeling; it lacks audio/speech inputs, turn-taking synchronization, or speech-focused evaluation, thus it does not meet inclusion criteria and falls under exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on hierarchical discrete transformers for multivariate time series forecasting using vector-quantized discrete tokens, which is a time-series modeling method rather than Spoken Language Models or speech-language modeling; it lacks audio/speech inputs, turn-taking synchronization, or speech-focused evaluation, thus it does not meet inclusion criteria and falls under exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multivariate time series forecasting using a hierarchical discrete transformer model, without addressing spoken language modeling, speech language models, or the core aspects of full-duplex spoken language interaction, synchronization, or related evaluations as required by the inclusion criteria. The paper neither emphasizes speech as a primary object of modeling nor discusses spoken language models or equivalent concepts explicitly, and as such does not meet the thematic or methodological scope of the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multivariate time series forecasting using a hierarchical discrete transformer model, without addressing spoken language modeling, speech language models, or the core aspects of full-duplex spoken language interaction, synchronization, or related evaluations as required by the inclusion criteria. The paper neither emphasizes speech as a primary object of modeling nor discusses spoken language models or equivalent concepts explicitly, and as such does not meet the thematic or methodological scope of the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "TokenSynth: A Token-based Neural Synthesizer for Instrument Cloning and Text-to-Instrument",
    "abstract": "Recent advancements in neural audio codecs have enabled the use of tokenized audio representations in various audio generation tasks, such as text-to-speech, text-to-audio, and text-to-music generation. Leveraging this approach, we propose TokenSynth, a novel neural synthesizer that utilizes a decoder-only transformer to generate desired audio tokens from MIDI tokens and CLAP (Contrastive Language-Audio Pretraining) embedding, which has timbre-related information. Our model is capable of performing instrument cloning, text-to-instrument synthesis, and text-guided timbre manipulation without any fine-tuning. This flexibility enables diverse sound design and intuitive timbre control. We evaluated the quality of the synthesized audio, the timbral similarity between synthesized and target audio/text, and synthesis accuracy (i.e., how accurately it follows the input MIDI) using objective measures. TokenSynth demonstrates the potential of leveraging advanced neural audio codecs and transformers to create powerful and versatile neural synthesizers. The source code, model weights, and audio demos are available at: https://github.com/KyungsuKim42/tokensynth",
    "metadata": {
      "arxiv_id": "2502.08939",
      "title": "TokenSynth: A Token-based Neural Synthesizer for Instrument Cloning and Text-to-Instrument",
      "summary": "Recent advancements in neural audio codecs have enabled the use of tokenized audio representations in various audio generation tasks, such as text-to-speech, text-to-audio, and text-to-music generation. Leveraging this approach, we propose TokenSynth, a novel neural synthesizer that utilizes a decoder-only transformer to generate desired audio tokens from MIDI tokens and CLAP (Contrastive Language-Audio Pretraining) embedding, which has timbre-related information. Our model is capable of performing instrument cloning, text-to-instrument synthesis, and text-guided timbre manipulation without any fine-tuning. This flexibility enables diverse sound design and intuitive timbre control. We evaluated the quality of the synthesized audio, the timbral similarity between synthesized and target audio/text, and synthesis accuracy (i.e., how accurately it follows the input MIDI) using objective measures. TokenSynth demonstrates the potential of leveraging advanced neural audio codecs and transformers to create powerful and versatile neural synthesizers. The source code, model weights, and audio demos are available at: https://github.com/KyungsuKim42/tokensynth",
      "authors": [
        "Kyungsu Kim",
        "Junghyun Koo",
        "Sungho Lee",
        "Haesun Joung",
        "Kyogu Lee"
      ],
      "published": "2025-02-13T03:40:30Z",
      "updated": "2025-02-13T03:40:30Z",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.08939v1",
      "landing_url": "https://arxiv.org/abs/2502.08939v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.08939"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on neural instrument synthesis and text-to-instrument generation for music, not on Spoken Language Models or core speech-language modeling, hence it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on neural instrument synthesis and text-to-instrument generation for music, not on Spoken Language Models or core speech-language modeling, hence it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on neural synthesizers for instrument cloning and audio token generation from MIDI, dealing with audio and timbre but not with spoken language modeling, full-duplex spoken interaction, or real-time speech conversational behavior; it lacks discussion on models centered on speech language modeling or synchronous speech interaction, and the evaluation metrics do not consider interaction or dialogue arbitration, therefore it does not meet the inclusion criteria for spoken language models or full-duplex spoken language interaction research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on neural synthesizers for instrument cloning and audio token generation from MIDI, dealing with audio and timbre but not with spoken language modeling, full-duplex spoken interaction, or real-time speech conversational behavior; it lacks discussion on models centered on speech language modeling or synchronous speech interaction, and the evaluation metrics do not consider interaction or dialogue arbitration, therefore it does not meet the inclusion criteria for spoken language models or full-duplex spoken language interaction research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Masked Modulation: High-Throughput Half-Duplex ISAC Transmission Waveform Design",
    "abstract": "Integrated sensing and communication (ISAC) enables numerous innovative wireless applications. Communication-centric design is a practical choice for the construction of the sixth generation (6G) ISAC networks. Continuous-wave-based ISAC systems, with orthogonal frequency-division multiplexing (OFDM) being a representative example, suffer from the self-interference (SI) problem, and hence are less suitable for long-range sensing. On the other hand, pulse-based half-duplex ISAC systems are free of SI, but are also less favourable for high-throughput communication scenarios. In this treatise, we propose MASked Modulation (MASM), a half-duplex ISAC waveform design scheme, which minimises a range blindness metric, termed as \"mainlobe fluctuation\", given a duty cycle (proportional to communication throughput) constraint. In particular, MASM is capable of supporting high-throughput communication ($\\sim$50% duty cycle) under mild mainlobe fluctuation. Moreover, MASM can be flexibly adapted to frame-level waveform designs by operating on the slow-time scale. In terms of optimal transmit mask design, a set of masks is shown to be ideal in the sense of sidelobe level and mainlobe fluctuation intensity.",
    "metadata": {
      "arxiv_id": "2502.08996",
      "title": "Masked Modulation: High-Throughput Half-Duplex ISAC Transmission Waveform Design",
      "summary": "Integrated sensing and communication (ISAC) enables numerous innovative wireless applications. Communication-centric design is a practical choice for the construction of the sixth generation (6G) ISAC networks. Continuous-wave-based ISAC systems, with orthogonal frequency-division multiplexing (OFDM) being a representative example, suffer from the self-interference (SI) problem, and hence are less suitable for long-range sensing. On the other hand, pulse-based half-duplex ISAC systems are free of SI, but are also less favourable for high-throughput communication scenarios.\n  In this treatise, we propose MASked Modulation (MASM), a half-duplex ISAC waveform design scheme, which minimises a range blindness metric, termed as \"mainlobe fluctuation\", given a duty cycle (proportional to communication throughput) constraint. In particular, MASM is capable of supporting high-throughput communication ($\\sim$50% duty cycle) under mild mainlobe fluctuation. Moreover, MASM can be flexibly adapted to frame-level waveform designs by operating on the slow-time scale. In terms of optimal transmit mask design, a set of masks is shown to be ideal in the sense of sidelobe level and mainlobe fluctuation intensity.",
      "authors": [
        "Yifeng Xiong",
        "Junsheng Mu",
        "Shuangyang Li",
        "Marco Lops",
        "Jianhua Zhang"
      ],
      "published": "2025-02-13T06:20:39Z",
      "updated": "2025-05-25T09:27:30Z",
      "categories": [
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.08996v2",
      "landing_url": "https://arxiv.org/abs/2502.08996v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.08996"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此文專注於 ISAC 波形設計與遮罩調制，核心為無線通道與頻域特性，未以語音/語言模型為核心，亦無語音互動評估，因此不符合納入條件，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此文專注於 ISAC 波形設計與遮罩調制，核心為無線通道與頻域特性，未以語音/語言模型為核心，亦無語音互動評估，因此不符合納入條件，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on integrated sensing and communication waveform design for ISAC systems, which is unrelated to Spoken Language Models, speech language modeling, or synchronous speech interaction behaviors required by the inclusion criteria. It also does not mention language modeling of speech, full-duplex spoken language models, nor any relevant training, inference, or evaluation of speech-based models. Therefore, the paper is clearly outside the core topic defined by the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on integrated sensing and communication waveform design for ISAC systems, which is unrelated to Spoken Language Models, speech language modeling, or synchronous speech interaction behaviors required by the inclusion criteria. It also does not mention language modeling of speech, full-duplex spoken language models, nor any relevant training, inference, or evaluation of speech-based models. Therefore, the paper is clearly outside the core topic defined by the criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MsEdF: A Multi-stream Encoder-decoder Framework for Remote Sensing Image Captioning",
    "abstract": "Remote sensing images contain complex spatial patterns and semantic structures, which makes the captioning model difficult to accurately describe. Encoder-decoder architectures have become the widely used approach for RSIC by translating visual content into descriptive text. However, many existing methods rely on a single-stream architecture, which weakens the model to accurately describe the image. Such single-stream architectures typically struggle to extract diverse spatial features or capture complex semantic relationships, limiting their effectiveness in scenes with high intraclass similarity or contextual ambiguity. In this work, we propose a novel Multi-stream Encoder-decoder Framework (MsEdF) which improves the performance of RSIC by optimizing both the spatial representation and language generation of encoder-decoder architecture. The encoder fuses information from two complementary image encoders, thereby promoting feature diversity through the integration of multiscale and structurally distinct cues. To improve the capture of context-aware descriptions, we refine the input sequence's semantic modeling on the decoder side using a stacked GRU architecture with an element-wise aggregation scheme. Experiments on three benchmark RSIC datasets show that MsEdF outperforms several baseline models.",
    "metadata": {
      "arxiv_id": "2502.09282",
      "title": "MsEdF: A Multi-stream Encoder-decoder Framework for Remote Sensing Image Captioning",
      "summary": "Remote sensing images contain complex spatial patterns and semantic structures, which makes the captioning model difficult to accurately describe. Encoder-decoder architectures have become the widely used approach for RSIC by translating visual content into descriptive text. However, many existing methods rely on a single-stream architecture, which weakens the model to accurately describe the image. Such single-stream architectures typically struggle to extract diverse spatial features or capture complex semantic relationships, limiting their effectiveness in scenes with high intraclass similarity or contextual ambiguity. In this work, we propose a novel Multi-stream Encoder-decoder Framework (MsEdF) which improves the performance of RSIC by optimizing both the spatial representation and language generation of encoder-decoder architecture. The encoder fuses information from two complementary image encoders, thereby promoting feature diversity through the integration of multiscale and structurally distinct cues. To improve the capture of context-aware descriptions, we refine the input sequence's semantic modeling on the decoder side using a stacked GRU architecture with an element-wise aggregation scheme. Experiments on three benchmark RSIC datasets show that MsEdF outperforms several baseline models.",
      "authors": [
        "Swadhin Das",
        "Raksha Sharma"
      ],
      "published": "2025-02-13T12:54:13Z",
      "updated": "2025-10-28T04:40:41Z",
      "categories": [
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.09282v4",
      "landing_url": "https://arxiv.org/abs/2502.09282v4",
      "doi": "https://doi.org/10.48550/arXiv.2502.09282"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on a multi-stream encoder-decoder framework for remote sensing image captioning with visual inputs, not on Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on a multi-stream encoder-decoder framework for remote sensing image captioning with visual inputs, not on Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on remote sensing image captioning using an encoder-decoder framework to improve spatial and semantic representations, with no mention of spoken language models, speech modeling, full-duplex interaction, or relevant speech-related architectures and evaluations as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on remote sensing image captioning using an encoder-decoder framework to improve spatial and semantic representations, with no mention of spoken language models, speech modeling, full-duplex interaction, or relevant speech-related architectures and evaluations as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Polymind: Parallel Visual Diagramming with Large Language Models to Support Prewriting Through Microtasks",
    "abstract": "Prewriting is the process of generating and organising ideas before a first draft. It consists of a combination of informal, iterative, and semi-structured strategies such as visual diagramming, which poses a challenge for collaborating with large language models (LLMs) in a turn-taking conversational manner. We present Polymind, a visual diagramming tool that leverages multiple LLM-powered agents to support prewriting. The system features a parallel collaboration workflow in place of the turn-taking conversational interactions. It defines multiple ``microtasks'' to simulate group collaboration scenarios such as collaborative writing and group brainstorming. Instead of repetitively prompting a chatbot for various purposes, Polymind enables users to orchestrate multiple microtasks simultaneously. Users can configure and delegate customised microtasks, and manage their microtasks by specifying task requirements and toggling visibility and initiative. Our evaluation revealed that, compared to ChatGPT, users had more customizability over collaboration with Polymind, and were thus able to quickly expand personalised writing ideas during prewriting.",
    "metadata": {
      "arxiv_id": "2502.09577",
      "title": "Polymind: Parallel Visual Diagramming with Large Language Models to Support Prewriting Through Microtasks",
      "summary": "Prewriting is the process of generating and organising ideas before a first draft. It consists of a combination of informal, iterative, and semi-structured strategies such as visual diagramming, which poses a challenge for collaborating with large language models (LLMs) in a turn-taking conversational manner. We present Polymind, a visual diagramming tool that leverages multiple LLM-powered agents to support prewriting. The system features a parallel collaboration workflow in place of the turn-taking conversational interactions. It defines multiple ``microtasks'' to simulate group collaboration scenarios such as collaborative writing and group brainstorming. Instead of repetitively prompting a chatbot for various purposes, Polymind enables users to orchestrate multiple microtasks simultaneously. Users can configure and delegate customised microtasks, and manage their microtasks by specifying task requirements and toggling visibility and initiative. Our evaluation revealed that, compared to ChatGPT, users had more customizability over collaboration with Polymind, and were thus able to quickly expand personalised writing ideas during prewriting.",
      "authors": [
        "Qian Wan",
        "Jiannan Li",
        "Huanchen Wang",
        "Zhicong Lu"
      ],
      "published": "2025-02-13T18:34:52Z",
      "updated": "2025-08-12T12:49:59Z",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.09577v2",
      "landing_url": "https://arxiv.org/abs/2502.09577v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.09577"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此研究重點為使用多代理LLM進行前寫作的視覺圖示與並行協作，未涉及 Spoken Language Models / 語音語言建模或同步互動，因此不符合主題核心。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此研究重點為使用多代理LLM進行前寫作的視覺圖示與並行協作，未涉及 Spoken Language Models / 語音語言建模或同步互動，因此不符合主題核心。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a visual diagramming tool leveraging multiple LLM-powered agents for prewriting via microtasks. It does not address spoken language models, speech as a core modeling object, nor full-duplex spoken language model capabilities. It also lacks discussion of architecture, training, or evaluation related to speech-based language modeling, and does not mention synchronization or real-time speech interaction features as required. Therefore, it fails to meet the inclusion criteria and aligns with exclusion criteria as it treats language modeling at the text/dialogue level without a speech modeling focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a visual diagramming tool leveraging multiple LLM-powered agents for prewriting via microtasks. It does not address spoken language models, speech as a core modeling object, nor full-duplex spoken language model capabilities. It also lacks discussion of architecture, training, or evaluation related to speech-based language modeling, and does not mention synchronization or real-time speech interaction features as required. Therefore, it fails to meet the inclusion criteria and aligns with exclusion criteria as it treats language modeling at the text/dialogue level without a speech modeling focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration",
    "abstract": "Agents built on large language models (LLMs) have excelled in turn-by-turn human-AI collaboration but struggle with simultaneous tasks requiring real-time interaction. Latency issues and the challenge of inferring variable human strategies hinder their ability to make autonomous decisions without explicit instructions. Through experiments with current independent System 1 and System 2 methods, we validate the necessity of using Dual Process Theory (DPT) in real-time tasks. We propose DPT-Agent, a novel language agent framework that integrates System 1 and System 2 for efficient real-time simultaneous human-AI collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and code-as-policy for fast, intuitive, and controllable decision-making. DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous reflection to infer human intentions and perform reasoning-based autonomous decisions. We demonstrate the effectiveness of DPT-Agent through further experiments with rule-based agents and human collaborators, showing significant improvements over mainstream LLM-based frameworks. DPT-Agent can effectively help LLMs convert correct slow thinking and reasoning into executable actions, thereby improving performance. To the best of our knowledge, DPT-Agent is the first language agent framework that achieves successful real-time simultaneous human-AI collaboration autonomously. Code of DPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.",
    "metadata": {
      "arxiv_id": "2502.11882",
      "title": "Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration",
      "summary": "Agents built on large language models (LLMs) have excelled in turn-by-turn human-AI collaboration but struggle with simultaneous tasks requiring real-time interaction. Latency issues and the challenge of inferring variable human strategies hinder their ability to make autonomous decisions without explicit instructions. Through experiments with current independent System 1 and System 2 methods, we validate the necessity of using Dual Process Theory (DPT) in real-time tasks. We propose DPT-Agent, a novel language agent framework that integrates System 1 and System 2 for efficient real-time simultaneous human-AI collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and code-as-policy for fast, intuitive, and controllable decision-making. DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous reflection to infer human intentions and perform reasoning-based autonomous decisions. We demonstrate the effectiveness of DPT-Agent through further experiments with rule-based agents and human collaborators, showing significant improvements over mainstream LLM-based frameworks. DPT-Agent can effectively help LLMs convert correct slow thinking and reasoning into executable actions, thereby improving performance. To the best of our knowledge, DPT-Agent is the first language agent framework that achieves successful real-time simultaneous human-AI collaboration autonomously. Code of DPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.",
      "authors": [
        "Shao Zhang",
        "Xihuai Wang",
        "Wenhao Zhang",
        "Chaoran Li",
        "Junru Song",
        "Tingyu Li",
        "Lin Qiu",
        "Xuezhi Cao",
        "Xunliang Cai",
        "Wen Yao",
        "Weinan Zhang",
        "Xinbing Wang",
        "Ying Wen"
      ],
      "published": "2025-02-17T15:09:45Z",
      "updated": "2025-05-28T12:14:14Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.LG",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.11882v5",
      "landing_url": "https://arxiv.org/abs/2502.11882v5",
      "doi": "https://doi.org/10.48550/arXiv.2502.11882"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此研究聚焦於以大語言模型為核心的實時人機協作框架與雙加工理論，未以 Spoken Language Models/ FD-SLM 為核心語音建模對象，亦未討論語音感知、重疊處理或語音層的建模與評估，因此不符合包含條件，且符合排除條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此研究聚焦於以大語言模型為核心的實時人機協作框架與雙加工理論，未以 Spoken Language Models/ FD-SLM 為核心語音建模對象，亦未討論語音感知、重疊處理或語音層的建模與評估，因此不符合包含條件，且符合排除條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on dual-process theory in a language agent framework for real-time human-AI collaboration but does not mention spoken language models, speech language models, or specifically address speech as the core modeling object; it primarily deals with large language models and autonomous decision-making rather than speech-based modeling, full-duplex spoken interaction, or speech signal processing as required by the inclusion criteria. Therefore, it does not meet the key requirements of the inclusion criteria and partially overlaps with exclusion points about focusing on text-based NLP and autonomous agents rather than speech modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on dual-process theory in a language agent framework for real-time human-AI collaboration but does not mention spoken language models, speech language models, or specifically address speech as the core modeling object; it primarily deals with large language models and autonomous decision-making rather than speech-based modeling, full-duplex spoken interaction, or speech signal processing as required by the inclusion criteria. Therefore, it does not meet the key requirements of the inclusion criteria and partially overlaps with exclusion points about focusing on text-based NLP and autonomous agents rather than speech modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarcity",
    "abstract": "Existing LMs struggle with proof-oriented programming due to data scarcity, which manifest in two key ways: (1) a lack of sufficient corpora for proof-oriented programming languages such as F*, and (2) the absence of large-scale, project-level proof-oriented implementations that can teach the model the intricate reasoning process when performing proof-oriented programming. We present the first on synthetic data augmentation for project level proof oriented programming for both generation and repair. Our method addresses data scarcity by synthesizing basic proof-oriented programming problems for proficiency in that language; incorporating diverse coding data for reasoning capability elicitation and creating new proofs and repair data within existing repositories. This approach enables language models to both synthesize and repair proofs for function- and repository-level code. We show that our fine-tuned 14B parameter model, PoPilot, can exceed the performance of the models that outperforms GPT-4o in project-level proof-oriented programming by 64% relative margin, and can improve GPT-4o's performance by 54% by repairing its outputs over GPT-4o's self-repair.",
    "metadata": {
      "arxiv_id": "2502.11901",
      "title": "Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarcity",
      "summary": "Existing LMs struggle with proof-oriented programming due to data scarcity, which manifest in two key ways: (1) a lack of sufficient corpora for proof-oriented programming languages such as F*, and (2) the absence of large-scale, project-level proof-oriented implementations that can teach the model the intricate reasoning process when performing proof-oriented programming. We present the first on synthetic data augmentation for project level proof oriented programming for both generation and repair. Our method addresses data scarcity by synthesizing basic proof-oriented programming problems for proficiency in that language; incorporating diverse coding data for reasoning capability elicitation and creating new proofs and repair data within existing repositories. This approach enables language models to both synthesize and repair proofs for function- and repository-level code. We show that our fine-tuned 14B parameter model, PoPilot, can exceed the performance of the models that outperforms GPT-4o in project-level proof-oriented programming by 64% relative margin, and can improve GPT-4o's performance by 54% by repairing its outputs over GPT-4o's self-repair.",
      "authors": [
        "Dylan Zhang",
        "Justin Wang",
        "Tianran Sun"
      ],
      "published": "2025-02-17T15:24:11Z",
      "updated": "2025-04-13T23:38:44Z",
      "categories": [
        "cs.CL",
        "cs.PL",
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.11901v2",
      "landing_url": "https://arxiv.org/abs/2502.11901v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.11901"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on proof-oriented programming and data-augmentation for code-models, with no emphasis on spoken language models, speech-language modeling, or full-duplex conversational interaction; it does not meet the core topic requirement and fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on proof-oriented programming and data-augmentation for code-models, with no emphasis on spoken language models, speech-language modeling, or full-duplex conversational interaction; it does not meet the core topic requirement and fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving proof-oriented programming models under data scarcity, involving language models for code synthesis and repair, without any mention of spoken language models, speech language modeling, or full-duplex spoken interaction capabilities as required by the inclusion criteria. It does not address speech as the primary modeling object, nor discuss synchronous interaction, cognitive parallelism, or speech-specific evaluation metrics. Hence, it falls outside the thematic scope defined for inclusion, clearly aligning with exclusion criteria related to non-speech-focused research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving proof-oriented programming models under data scarcity, involving language models for code synthesis and repair, without any mention of spoken language models, speech language modeling, or full-duplex spoken interaction capabilities as required by the inclusion criteria. It does not address speech as the primary modeling object, nor discuss synchronous interaction, cognitive parallelism, or speech-specific evaluation metrics. Hence, it falls outside the thematic scope defined for inclusion, clearly aligning with exclusion criteria related to non-speech-focused research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "An a posteriori data-driven method for phase-averaged optical measurements",
    "abstract": "Phase-averaging is a fundamental approach for investigating periodic and non-stationary phenomena. In fluid dynamics, these can be generated by rotating blades such as propellers/turbines or by pulsed jets. Traditional phase-averaging approaches often rely on synchronized data acquisition systems, which might require high-speed cameras, light sources, and precise delay generators and encoders, making them expensive and sometimes unfeasible. This work proposes an a posteriori data-driven approach that reconstructs phase information from randomly acquired uncorrelated photographic frames (snapshots) using the ISOMAP algorithm. The technique enables accurate reordering of snapshots in the phase space and subsequent computation of the phase-averaged flow field without the need for synchronization. The framework was validated through numerical simulations and experimental fluid dynamics datasets from an optical setup featuring single- and multi-propeller configurations. The results demonstrate that the proposed method effectively captures the periodic flow characteristics while addressing the challenges related to synchronization and hardware limitations. Furthermore, the ability to apply this technique to archival datasets extends its applicability to a wide range of experimental fluid dynamics studies. This approach provides a scalable and cost-effective alternative to traditional methods for the analysis of periodic phenomena.",
    "metadata": {
      "arxiv_id": "2502.12369",
      "title": "An a posteriori data-driven method for phase-averaged optical measurements",
      "summary": "Phase-averaging is a fundamental approach for investigating periodic and non-stationary phenomena. In fluid dynamics, these can be generated by rotating blades such as propellers/turbines or by pulsed jets. Traditional phase-averaging approaches often rely on synchronized data acquisition systems, which might require high-speed cameras, light sources, and precise delay generators and encoders, making them expensive and sometimes unfeasible. This work proposes an a posteriori data-driven approach that reconstructs phase information from randomly acquired uncorrelated photographic frames (snapshots) using the ISOMAP algorithm. The technique enables accurate reordering of snapshots in the phase space and subsequent computation of the phase-averaged flow field without the need for synchronization. The framework was validated through numerical simulations and experimental fluid dynamics datasets from an optical setup featuring single- and multi-propeller configurations. The results demonstrate that the proposed method effectively captures the periodic flow characteristics while addressing the challenges related to synchronization and hardware limitations. Furthermore, the ability to apply this technique to archival datasets extends its applicability to a wide range of experimental fluid dynamics studies. This approach provides a scalable and cost-effective alternative to traditional methods for the analysis of periodic phenomena.",
      "authors": [
        "Enrico Amico",
        "Sara Montagner",
        "Jacopo Serpieri",
        "Gioacchino Cafiero"
      ],
      "published": "2025-02-17T23:19:45Z",
      "updated": "2025-02-19T06:07:01Z",
      "categories": [
        "physics.flu-dyn",
        "physics.data-an"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.12369v2",
      "landing_url": "https://arxiv.org/abs/2502.12369v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.12369"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item studies phase-averaged optical measurements in fluid dynamics using data-driven snapshot reconstruction, not related to Spoken Language Models or speech-language modeling, thus not meeting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item studies phase-averaged optical measurements in fluid dynamics using data-driven snapshot reconstruction, not related to Spoken Language Models or speech-language modeling, thus not meeting the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a data-driven method for phase-averaged optical measurements in fluid dynamics, which is unrelated to spoken language models or speech language modeling architectures, training, or evaluation; it does not cover any of the key concepts such as full-duplex spoken language models, engineered or learned synchronization in speech, or speech-related evaluation metrics, and thus does not meet the inclusion criteria of focusing on speech language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a data-driven method for phase-averaged optical measurements in fluid dynamics, which is unrelated to spoken language models or speech language modeling architectures, training, or evaluation; it does not cover any of the key concepts such as full-duplex spoken language models, engineered or learned synchronization in speech, or speech-related evaluation metrics, and thus does not meet the inclusion criteria of focusing on speech language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "FlexDuo: A Pluggable System for Enabling Full-Duplex Capabilities in Speech Dialogue Systems",
    "abstract": "Full-Duplex Speech Dialogue Systems (Full-Duplex SDS) have significantly enhanced the naturalness of human-machine interaction by enabling real-time bidirectional communication. However, existing approaches face challenges such as difficulties in independent module optimization and contextual noise interference due to highly coupled architectural designs and oversimplified binary state modeling. This paper proposes FlexDuo, a flexible full-duplex control module that decouples duplex control from spoken dialogue systems through a plug-and-play architectural design. Furthermore, inspired by human information-filtering mechanisms in conversations, we introduce an explicit Idle state. On one hand, the Idle state filters redundant noise and irrelevant audio to enhance dialogue quality. On the other hand, it establishes a semantic integrity-based buffering mechanism, reducing the risk of mutual interruptions while ensuring accurate response transitions. Experimental results on the Fisher corpus demonstrate that FlexDuo reduces the false interruption rate by 24.9% and improves response accuracy by 7.6% compared to integrated full-duplex dialogue system baselines. It also outperforms voice activity detection (VAD) controlled baseline systems in both Chinese and English dialogue quality. The proposed modular architecture and state-based dialogue model provide a novel technical pathway for building flexible and efficient duplex dialogue systems.",
    "metadata": {
      "arxiv_id": "2502.13472",
      "title": "FlexDuo: A Pluggable System for Enabling Full-Duplex Capabilities in Speech Dialogue Systems",
      "summary": "Full-Duplex Speech Dialogue Systems (Full-Duplex SDS) have significantly enhanced the naturalness of human-machine interaction by enabling real-time bidirectional communication. However, existing approaches face challenges such as difficulties in independent module optimization and contextual noise interference due to highly coupled architectural designs and oversimplified binary state modeling. This paper proposes FlexDuo, a flexible full-duplex control module that decouples duplex control from spoken dialogue systems through a plug-and-play architectural design. Furthermore, inspired by human information-filtering mechanisms in conversations, we introduce an explicit Idle state. On one hand, the Idle state filters redundant noise and irrelevant audio to enhance dialogue quality. On the other hand, it establishes a semantic integrity-based buffering mechanism, reducing the risk of mutual interruptions while ensuring accurate response transitions. Experimental results on the Fisher corpus demonstrate that FlexDuo reduces the false interruption rate by 24.9% and improves response accuracy by 7.6% compared to integrated full-duplex dialogue system baselines. It also outperforms voice activity detection (VAD) controlled baseline systems in both Chinese and English dialogue quality. The proposed modular architecture and state-based dialogue model provide a novel technical pathway for building flexible and efficient duplex dialogue systems.",
      "authors": [
        "Borui Liao",
        "Yulong Xu",
        "Jiao Ou",
        "Kaiyuan Yang",
        "Weihua Jian",
        "Pengfei Wan",
        "Di Zhang"
      ],
      "published": "2025-02-19T06:51:34Z",
      "updated": "2025-05-29T03:32:21Z",
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.13472v2",
      "landing_url": "https://arxiv.org/abs/2502.13472v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.13472"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on a pluggable, modular control system for enabling full-duplex in speech dialogue, emphasizing architectural decoupling and noise filtering rather than core Spoken Language Models or end-to-end speech-language modeling of speech as the modeling target; it lacks emphasis on SLM/FD-SLM core concepts and explicit evaluation of speech-language modeling, thus does not meet the inclusion criteria despite being recent.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on a pluggable, modular control system for enabling full-duplex in speech dialogue, emphasizing architectural decoupling and noise filtering rather than core Spoken Language Models or end-to-end speech-language modeling of speech as the modeling target; it lacks emphasis on SLM/FD-SLM core concepts and explicit evaluation of speech-language modeling, thus does not meet the inclusion criteria despite being recent.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a modular architecture for full-duplex speech dialogue systems, focusing on speech-level modeling and interaction control that handle real-time bidirectional communication, overlapping speech, and interruption management; it includes experimental evaluation on dialogue quality metrics beyond ASR or TTS, aligning with inclusion criteria for FD-SLM research. However, the abstract does not explicitly mention 'spoken language model(s)' or 'speech language model(s)', which is a strict requirement. Despite this, the focus is clearly on speech-level dialogue modeling and engineering synchronization, with evaluations that go beyond conventional speech tech metrics, indicating relevance to the core research theme.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The paper presents a modular architecture for full-duplex speech dialogue systems, focusing on speech-level modeling and interaction control that handle real-time bidirectional communication, overlapping speech, and interruption management; it includes experimental evaluation on dialogue quality metrics beyond ASR or TTS, aligning with inclusion criteria for FD-SLM research. However, the abstract does not explicitly mention 'spoken language model(s)' or 'speech language model(s)', which is a strict requirement. Despite this, the focus is clearly on speech-level dialogue modeling and engineering synchronization, with evaluations that go beyond conventional speech tech metrics, indicating relevance to the core research theme.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted relevance to engineered synchronization for full-duplex dialogue, but the abstract describes a pluggable duplex control module and noise-filtering Idle state rather than any spoken-language-modeling (SLM/FD-SLM) architecture, training, or explicit language-modeling of speech; because the inclusion rules require the title/abstract to explicitly mention spoken/speech language models or equivalent, this item fails that mandatory criterion and should be excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted relevance to engineered synchronization for full-duplex dialogue, but the abstract describes a pluggable duplex control module and noise-filtering Idle state rather than any spoken-language-modeling (SLM/FD-SLM) architecture, training, or explicit language-modeling of speech; because the inclusion rules require the title/abstract to explicitly mention spoken/speech language models or equivalent, this item fails that mandatory criterion and should be excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LLM-Enhanced Dialogue Management for Full-Duplex Spoken Dialogue Systems",
    "abstract": "Achieving full-duplex communication in spoken dialogue systems (SDS) requires real-time coordination between listening, speaking, and thinking. This paper proposes a semantic voice activity detection (VAD) module as a dialogue manager (DM) to efficiently manage turn-taking in full-duplex SDS. Implemented as a lightweight (0.5B) LLM fine-tuned on full-duplex conversation data, the semantic VAD predicts four control tokens to regulate turn-switching and turn-keeping, distinguishing between intentional and unintentional barge-ins while detecting query completion for handling user pauses and hesitations. By processing input speech in short intervals, the semantic VAD enables real-time decision-making, while the core dialogue engine (CDE) is only activated for response generation, reducing computational overhead. This design allows independent DM optimization without retraining the CDE, balancing interaction accuracy and inference efficiency for scalable, next-generation full-duplex SDS.",
    "metadata": {
      "arxiv_id": "2502.14145",
      "title": "LLM-Enhanced Dialogue Management for Full-Duplex Spoken Dialogue Systems",
      "summary": "Achieving full-duplex communication in spoken dialogue systems (SDS) requires real-time coordination between listening, speaking, and thinking. This paper proposes a semantic voice activity detection (VAD) module as a dialogue manager (DM) to efficiently manage turn-taking in full-duplex SDS. Implemented as a lightweight (0.5B) LLM fine-tuned on full-duplex conversation data, the semantic VAD predicts four control tokens to regulate turn-switching and turn-keeping, distinguishing between intentional and unintentional barge-ins while detecting query completion for handling user pauses and hesitations. By processing input speech in short intervals, the semantic VAD enables real-time decision-making, while the core dialogue engine (CDE) is only activated for response generation, reducing computational overhead. This design allows independent DM optimization without retraining the CDE, balancing interaction accuracy and inference efficiency for scalable, next-generation full-duplex SDS.",
      "authors": [
        "Hao Zhang",
        "Weiwei Li",
        "Rilin Chen",
        "Vinay Kothapally",
        "Meng Yu",
        "Dong Yu"
      ],
      "published": "2025-02-19T23:15:13Z",
      "updated": "2025-02-24T19:08:11Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.14145v2",
      "landing_url": "https://arxiv.org/abs/2502.14145v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.14145"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on Spoken Language Models (FD-SLM) for full-duplex dialogue, uses speech as the core modeling object with an LLM-based dialogue manager and semantic VAD, and discusses model architecture/ training/ evaluation within the last 3 years, meeting all inclusion criteria and none of the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper centers on Spoken Language Models (FD-SLM) for full-duplex dialogue, uses speech as the core modeling object with an LLM-based dialogue manager and semantic VAD, and discusses model architecture/ training/ evaluation within the last 3 years, meeting all inclusion criteria and none of the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a semantic voice activity detection module acting as a dialogue manager in full-duplex spoken dialogue systems, which directly involves spoken language models and real-time turn-taking synchronization. It highlights lightweight LLM fine-tuning on full-duplex conversation data, real-time decisions, and modular dialogue engine design, addressing engineered synchronization and interaction efficiency as stipulated in the inclusion criteria. The research goes beyond traditional half-duplex systems, covers synchronous interaction, and is not limited to ASR/TTS or single downstream tasks. It also targets modeling and managing speech sequences during dialogue, aligning well with the inclusion criteria on topic, methodology, and evaluation scope. However, the abstract lacks explicit mention of \"spoken language model(s)\" or exact synonymous terms in title or abstract, which is technically a required textual cue per criteria. Given this, the evaluation is cautious but leans positive due to the strong conceptual fit and partial textual alignment, suggesting better to include rather than exclude.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a semantic voice activity detection module acting as a dialogue manager in full-duplex spoken dialogue systems, which directly involves spoken language models and real-time turn-taking synchronization. It highlights lightweight LLM fine-tuning on full-duplex conversation data, real-time decisions, and modular dialogue engine design, addressing engineered synchronization and interaction efficiency as stipulated in the inclusion criteria. The research goes beyond traditional half-duplex systems, covers synchronous interaction, and is not limited to ASR/TTS or single downstream tasks. It also targets modeling and managing speech sequences during dialogue, aligning well with the inclusion criteria on topic, methodology, and evaluation scope. However, the abstract lacks explicit mention of \"spoken language model(s)\" or exact synonymous terms in title or abstract, which is technically a required textual cue per criteria. Given this, the evaluation is cautious but leans positive due to the strong conceptual fit and partial textual alignment, suggesting better to include rather than exclude.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Mitigating Data Scarcity in Time Series Analysis: A Foundation Model with Series-Symbol Data Generation",
    "abstract": "Foundation models for time series analysis (TSA) have attracted significant attention. However, challenges such as data scarcity and data imbalance continue to hinder their development. To address this, we consider modeling complex systems through symbolic expressions that serve as semantic descriptors of time series. Building on this concept, we introduce a series-symbol (S2) dual-modulity data generation mechanism, enabling the unrestricted creation of high-quality time series data paired with corresponding symbolic representations. Leveraging the S2 dataset, we develop SymTime, a pre-trained foundation model for TSA. SymTime demonstrates competitive performance across five major TSA tasks when fine-tuned with downstream task, rivaling foundation models pre-trained on real-world datasets. This approach underscores the potential of dual-modality data generation and pretraining mechanisms in overcoming data scarcity and enhancing task performance.",
    "metadata": {
      "arxiv_id": "2502.15466",
      "title": "Mitigating Data Scarcity in Time Series Analysis: A Foundation Model with Series-Symbol Data Generation",
      "summary": "Foundation models for time series analysis (TSA) have attracted significant attention. However, challenges such as data scarcity and data imbalance continue to hinder their development. To address this, we consider modeling complex systems through symbolic expressions that serve as semantic descriptors of time series. Building on this concept, we introduce a series-symbol (S2) dual-modulity data generation mechanism, enabling the unrestricted creation of high-quality time series data paired with corresponding symbolic representations. Leveraging the S2 dataset, we develop SymTime, a pre-trained foundation model for TSA. SymTime demonstrates competitive performance across five major TSA tasks when fine-tuned with downstream task, rivaling foundation models pre-trained on real-world datasets. This approach underscores the potential of dual-modality data generation and pretraining mechanisms in overcoming data scarcity and enhancing task performance.",
      "authors": [
        "Wenxuan Wang",
        "Kai Wu",
        "Yujian Betterest Li",
        "Dan Wang",
        "Xiaoyu Zhang",
        "Jing Liu"
      ],
      "published": "2025-02-21T13:43:24Z",
      "updated": "2025-02-21T13:43:24Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.15466v1",
      "landing_url": "https://arxiv.org/abs/2502.15466v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.15466"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about time series foundation models and symbolic data generation, not about Spoken Language Models or speech-language modeling; it lacks core speech/dialogue interaction aspects required by the inclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about time series foundation models and symbolic data generation, not about Spoken Language Models or speech-language modeling; it lacks core speech/dialogue interaction aspects required by the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on time series analysis using foundation models with symbolic data generation, without any direct reference to spoken language models, speech language modeling, or full-duplex spoken language interaction; it also lacks discussion on speech as the main modeling object or on synchronization in speech dialogues, which are central inclusion criteria. Therefore, it does not meet the specified thematic inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on time series analysis using foundation models with symbolic data generation, without any direct reference to spoken language models, speech language modeling, or full-duplex spoken language interaction; it also lacks discussion on speech as the main modeling object or on synchronization in speech dialogues, which are central inclusion criteria. Therefore, it does not meet the specified thematic inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Patch Stitching Data Augmentation for Cancer Classification in Pathology Images",
    "abstract": "Computational pathology, integrating computational methods and digital imaging, has shown to be effective in advancing disease diagnosis and prognosis. In recent years, the development of machine learning and deep learning has greatly bolstered the power of computational pathology. However, there still remains the issue of data scarcity and data imbalance, which can have an adversarial effect on any computational method. In this paper, we introduce an efficient and effective data augmentation strategy to generate new pathology images from the existing pathology images and thus enrich datasets without additional data collection or annotation costs. To evaluate the proposed method, we employed two sets of colorectal cancer datasets and obtained improved classification results, suggesting that the proposed simple approach holds the potential for alleviating the data scarcity and imbalance in computational pathology.",
    "metadata": {
      "arxiv_id": "2502.16162",
      "title": "Patch Stitching Data Augmentation for Cancer Classification in Pathology Images",
      "summary": "Computational pathology, integrating computational methods and digital imaging, has shown to be effective in advancing disease diagnosis and prognosis. In recent years, the development of machine learning and deep learning has greatly bolstered the power of computational pathology. However, there still remains the issue of data scarcity and data imbalance, which can have an adversarial effect on any computational method. In this paper, we introduce an efficient and effective data augmentation strategy to generate new pathology images from the existing pathology images and thus enrich datasets without additional data collection or annotation costs. To evaluate the proposed method, we employed two sets of colorectal cancer datasets and obtained improved classification results, suggesting that the proposed simple approach holds the potential for alleviating the data scarcity and imbalance in computational pathology.",
      "authors": [
        "Jiamu Wang",
        "Chang-Su Kim",
        "Jin Tae Kwak"
      ],
      "published": "2025-02-22T09:34:50Z",
      "updated": "2025-02-22T09:34:50Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16162v1",
      "landing_url": "https://arxiv.org/abs/2502.16162v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.16162"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study centers on data augmentation for colorectal pathology image classification and does not involve Spoken Language Models or speech-language modeling concepts, thus it does not meet the inclusion criteria or align with the exclusion criteria relevant to SLMs/Speech.\n",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study centers on data augmentation for colorectal pathology image classification and does not involve Spoken Language Models or speech-language modeling concepts, thus it does not meet the inclusion criteria or align with the exclusion criteria relevant to SLMs/Speech.\n",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on computational pathology and data augmentation for cancer classification, with no mention of spoken language models, speech-based modeling, or full-duplex interaction; thus, it does not meet the inclusion criteria centered on spoken language models and related speech interaction features.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on computational pathology and data augmentation for cancer classification, with no mention of spoken language models, speech-based modeling, or full-duplex interaction; thus, it does not meet the inclusion criteria centered on spoken language models and related speech interaction features.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Audio-FLAN: A Preliminary Release",
    "abstract": "Recent advancements in audio tokenization have significantly enhanced the integration of audio capabilities into large language models (LLMs). However, audio understanding and generation are often treated as distinct tasks, hindering the development of truly unified audio-language models. While instruction tuning has demonstrated remarkable success in improving generalization and zero-shot learning across text and vision, its application to audio remains largely unexplored. A major obstacle is the lack of comprehensive datasets that unify audio understanding and generation. To address this, we introduce Audio-FLAN, a large-scale instruction-tuning dataset covering 80 diverse tasks across speech, music, and sound domains, with over 100 million instances. Audio-FLAN lays the foundation for unified audio-language models that can seamlessly handle both understanding (e.g., transcription, comprehension) and generation (e.g., speech, music, sound) tasks across a wide range of audio domains in a zero-shot manner. The Audio-FLAN dataset is available on HuggingFace and GitHub and will be continuously updated.",
    "metadata": {
      "arxiv_id": "2502.16584",
      "title": "Audio-FLAN: A Preliminary Release",
      "summary": "Recent advancements in audio tokenization have significantly enhanced the integration of audio capabilities into large language models (LLMs). However, audio understanding and generation are often treated as distinct tasks, hindering the development of truly unified audio-language models. While instruction tuning has demonstrated remarkable success in improving generalization and zero-shot learning across text and vision, its application to audio remains largely unexplored. A major obstacle is the lack of comprehensive datasets that unify audio understanding and generation. To address this, we introduce Audio-FLAN, a large-scale instruction-tuning dataset covering 80 diverse tasks across speech, music, and sound domains, with over 100 million instances. Audio-FLAN lays the foundation for unified audio-language models that can seamlessly handle both understanding (e.g., transcription, comprehension) and generation (e.g., speech, music, sound) tasks across a wide range of audio domains in a zero-shot manner. The Audio-FLAN dataset is available on HuggingFace and GitHub and will be continuously updated.",
      "authors": [
        "Liumeng Xue",
        "Ziya Zhou",
        "Jiahao Pan",
        "Zixuan Li",
        "Shuai Fan",
        "Yinghao Ma",
        "Sitong Cheng",
        "Dongchao Yang",
        "Haohan Guo",
        "Yujia Xiao",
        "Xinsheng Wang",
        "Zixuan Shen",
        "Chuanbo Zhu",
        "Xinshen Zhang",
        "Tianchi Liu",
        "Ruibin Yuan",
        "Zeyue Tian",
        "Haohe Liu",
        "Emmanouil Benetos",
        "Ge Zhang",
        "Yike Guo",
        "Wei Xue"
      ],
      "published": "2025-02-23T14:24:15Z",
      "updated": "2025-02-23T14:24:15Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16584v1",
      "landing_url": "https://arxiv.org/abs/2502.16584v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.16584"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本研究雖未在摘要中直接使用「spoken language model」或「speech language model」字句，但核心聚焦於以音頻作為輸入，進行統一的 audio-language 模型訓練與評估，涵蓋多任務、零-shot、跨語音域的能力，符合 SLM/ FD-SLM 的研究方向與評估需求，且時間範圍在近三年，非僅聚焦於單一語音技術（如 ASR/TTS），因此符合納入條件且具增益性。",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "本研究雖未在摘要中直接使用「spoken language model」或「speech language model」字句，但核心聚焦於以音頻作為輸入，進行統一的 audio-language 模型訓練與評估，涵蓋多任務、零-shot、跨語音域的能力，符合 SLM/ FD-SLM 的研究方向與評估需求，且時間範圍在近三年，非僅聚焦於單一語音技術（如 ASR/TTS），因此符合納入條件且具增益性。",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces Audio-FLAN, a large-scale instruction-tuning dataset for unified audio-language models aimed at both audio understanding and generation tasks across speech, music, and sound domains, but it lacks explicit mention of spoken language models or full-duplex spoken language interactions, detailed model architectures, training or evaluation focused on speech-level language modeling, or real-time interaction capabilities as required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study introduces Audio-FLAN, a large-scale instruction-tuning dataset for unified audio-language models aimed at both audio understanding and generation tasks across speech, music, and sound domains, but it lacks explicit mention of spoken language models or full-duplex spoken language interactions, detailed model architectures, training or evaluation focused on speech-level language modeling, or real-time interaction capabilities as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "綜合兩位初審意見：JuniorNano 認為本研究聚焦於以音頻為核心的統一 audio-language 模型、涵蓋多任務與零-shot 能力，與 SLM/FD-SLM 研究方向相關；但 JuniorMini 指出摘要中未明確出現“spoken language model(s)”或等價描述，也未說明模型架構、訓練細節、即時/全雙工互動或相關評估指標。根據納入準則，本研究必須在標題或摘要明確出現 spoken/speech language model 或等價描述，並聚焦語音層級的 language modeling 與相應的架構/評估；而本摘要雖包含“audio-language models”與資料集細節，但未履行該明確措辭與對 FD-SLM 特徵（如低延遲、重疊處理、同步互動或專門 benchmark）的討論，因此不符合所有納入條件。 結論（1句）：摘要未明確使用「spoken/speech language model」或討論 FD‑SLM 必要的架構與評估要素，故應排除。",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "綜合兩位初審意見：JuniorNano 認為本研究聚焦於以音頻為核心的統一 audio-language 模型、涵蓋多任務與零-shot 能力，與 SLM/FD-SLM 研究方向相關；但 JuniorMini 指出摘要中未明確出現“spoken language model(s)”或等價描述，也未說明模型架構、訓練細節、即時/全雙工互動或相關評估指標。根據納入準則，本研究必須在標題或摘要明確出現 spoken/speech language model 或等價描述，並聚焦語音層級的 language modeling 與相應的架構/評估；而本摘要雖包含“audio-language models”與資料集細節，但未履行該明確措辭與對 FD-SLM 特徵（如低延遲、重疊處理、同步互動或專門 benchmark）的討論，因此不符合所有納入條件。 結論（1句）：摘要未明確使用「spoken/speech language model」或討論 FD‑SLM 必要的架構與評估要素，故應排除。",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Balancing Speech Understanding and Generation Using Continual Pre-training for Codec-based Speech LLM",
    "abstract": "Recent advances in speech language models (LLMs) have extended textual LLMs to the speech domain, but balancing speech understanding and generation remains challenging, especially with codec-based representations. We propose a continual pre-training (CPT) framework that adapts a textual LLM to handle codec-discretized speech, mitigating modality mismatch and preserving linguistic reasoning. Our unified model supports both understanding and generation, achieving strong results across ASR, TTS, S2T-Trans, and S2S-Trans. Notably, we present the first end-to-end, single-pass S2S-Trans system using only neural codec tokens, without intermediate transcriptions, translations, or semantic tokens. CPT proves essential for cross-modal alignment and task generalization, making it a powerful tool for building robust, unified speech LLMs.",
    "metadata": {
      "arxiv_id": "2502.16897",
      "title": "Balancing Speech Understanding and Generation Using Continual Pre-training for Codec-based Speech LLM",
      "summary": "Recent advances in speech language models (LLMs) have extended textual LLMs to the speech domain, but balancing speech understanding and generation remains challenging, especially with codec-based representations. We propose a continual pre-training (CPT) framework that adapts a textual LLM to handle codec-discretized speech, mitigating modality mismatch and preserving linguistic reasoning. Our unified model supports both understanding and generation, achieving strong results across ASR, TTS, S2T-Trans, and S2S-Trans. Notably, we present the first end-to-end, single-pass S2S-Trans system using only neural codec tokens, without intermediate transcriptions, translations, or semantic tokens. CPT proves essential for cross-modal alignment and task generalization, making it a powerful tool for building robust, unified speech LLMs.",
      "authors": [
        "Jiatong Shi",
        "Chunlei Zhang",
        "Jinchuan Tian",
        "Junrui Ni",
        "Hao Zhang",
        "Shinji Watanabe",
        "Dong Yu"
      ],
      "published": "2025-02-24T06:50:40Z",
      "updated": "2025-11-27T18:46:39Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16897v2",
      "landing_url": "https://arxiv.org/abs/2502.16897v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.16897"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此論文以 Spoken Language Models/語音語言模型為核心，將語音作為主要建模對象，涵蓋理解與生成的跨模態任務，並在標題/摘要中明確提及 speech LLM/SLM，符合近3年的研究與多任務評估的纳入條件。",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "此論文以 Spoken Language Models/語音語言模型為核心，將語音作為主要建模對象，涵蓋理解與生成的跨模態任務，並在標題/摘要中明確提及 speech LLM/SLM，符合近3年的研究與多任務評估的纳入條件。",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on balancing speech understanding and generation within a unified speech language model (LLM) framework that handles codec-discretized speech, covering speech language modeling inclusive of both recognition and generation tasks without relying solely on text intermediates. However, the abstract does not explicitly address full-duplex interaction, real-time low latency, cognitive parallelism, or synchronization aspects (e.g., managing turn-taking, interruption, backchanneling) as required by the inclusion criteria. Moreover, the evaluation mainly mentions performance results on ASR, TTS, and translation tasks without clear evidence of behavioral arbitration or dynamic interaction evaluation. Therefore, while the study is relevant to speech language modeling and codec-based speech representation, it lacks explicit coverage of the key criteria related to full-duplex interaction and multi-dimensional evaluation.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on balancing speech understanding and generation within a unified speech language model (LLM) framework that handles codec-discretized speech, covering speech language modeling inclusive of both recognition and generation tasks without relying solely on text intermediates. However, the abstract does not explicitly address full-duplex interaction, real-time low latency, cognitive parallelism, or synchronization aspects (e.g., managing turn-taking, interruption, backchanneling) as required by the inclusion criteria. Moreover, the evaluation mainly mentions performance results on ASR, TTS, and translation tasks without clear evidence of behavioral arbitration or dynamic interaction evaluation. Therefore, while the study is relevant to speech language modeling and codec-based speech representation, it lacks explicit coverage of the key criteria related to full-duplex interaction and multi-dimensional evaluation.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the paper is squarely on speech language modeling and codec-based unified speech LLMs (JuniorNano gave inclusion), but JuniorMini flagged missing evidence for full-duplex interaction and behavioral arbitration; while the abstract does explicitly describe a unified speech LLM handling codec tokens and generation/understanding (ASR, TTS, S2T, S2S) and even end-to-end S2S-Trans, it does not mention full-duplex/synchronous listening–speaking, cognitive parallelism, overlap/interrupt handling, low-latency constraints, or multi-dimensional interaction benchmarks required by the inclusion criteria—thus it should be excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the paper is squarely on speech language modeling and codec-based unified speech LLMs (JuniorNano gave inclusion), but JuniorMini flagged missing evidence for full-duplex interaction and behavioral arbitration; while the abstract does explicitly describe a unified speech LLM handling codec tokens and generation/understanding (ASR, TTS, S2T, S2S) and even end-to-end S2S-Trans, it does not mention full-duplex/synchronous listening–speaking, cognitive parallelism, overlap/interrupt handling, low-latency constraints, or multi-dimensional interaction benchmarks required by the inclusion criteria—thus it should be excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction",
    "abstract": "We introduce Baichuan-Audio, an end-to-end audio large language model that seamlessly integrates audio understanding and generation. It features a text-guided aligned speech generation mechanism, enabling real-time speech interaction with both comprehension and generation capabilities. Baichuan-Audio leverages a pre-trained ASR model, followed by multi-codebook discretization of speech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that speech tokens retain both semantic and acoustic information. To further enhance modeling, an independent audio head is employed to process audio tokens, effectively capturing their unique characteristics. To mitigate the loss of intelligence during pre-training and preserve the original capabilities of the LLM, we propose a two-stage pre-training strategy that maintains language understanding while enhancing audio modeling. Following alignment, the model excels in real-time speech-based conversation and exhibits outstanding question-answering capabilities, demonstrating its versatility and efficiency. The proposed model demonstrates superior performance in real-time spoken dialogue and exhibits strong question-answering abilities. Our code, model and training data are available at https://github.com/baichuan-inc/Baichuan-Audio",
    "metadata": {
      "arxiv_id": "2502.17239",
      "title": "Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction",
      "summary": "We introduce Baichuan-Audio, an end-to-end audio large language model that seamlessly integrates audio understanding and generation. It features a text-guided aligned speech generation mechanism, enabling real-time speech interaction with both comprehension and generation capabilities. Baichuan-Audio leverages a pre-trained ASR model, followed by multi-codebook discretization of speech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that speech tokens retain both semantic and acoustic information. To further enhance modeling, an independent audio head is employed to process audio tokens, effectively capturing their unique characteristics. To mitigate the loss of intelligence during pre-training and preserve the original capabilities of the LLM, we propose a two-stage pre-training strategy that maintains language understanding while enhancing audio modeling. Following alignment, the model excels in real-time speech-based conversation and exhibits outstanding question-answering capabilities, demonstrating its versatility and efficiency. The proposed model demonstrates superior performance in real-time spoken dialogue and exhibits strong question-answering abilities. Our code, model and training data are available at https://github.com/baichuan-inc/Baichuan-Audio",
      "authors": [
        "Tianpeng Li",
        "Jun Liu",
        "Tao Zhang",
        "Yuanbo Fang",
        "Da Pan",
        "Mingrui Wang",
        "Zheng Liang",
        "Zehuan Li",
        "Mingan Lin",
        "Guosheng Dong",
        "Jianhua Xu",
        "Haoze Sun",
        "Zenan Zhou",
        "Weipeng Chen"
      ],
      "published": "2025-02-24T15:16:34Z",
      "updated": "2025-02-24T15:16:34Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.17239v1",
      "landing_url": "https://arxiv.org/abs/2502.17239v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.17239"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item targets end-to-end speech interaction with an audio LLM, enabling speech understanding and generation and real-time, full-duplex interactive capabilities, and discusses model architecture, training, and evaluation; it clearly aligns with Spoken Language Models/FD-SLMs and falls within the past 3 years, with relevant keywords in the title/abstract, and not restricted to ASR/TTS alone or non-interactive tasks.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item targets end-to-end speech interaction with an audio LLM, enabling speech understanding and generation and real-time, full-duplex interactive capabilities, and discusses model architecture, training, and evaluation; it clearly aligns with Spoken Language Models/FD-SLMs and falls within the past 3 years, with relevant keywords in the title/abstract, and not restricted to ASR/TTS alone or non-interactive tasks.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe Baichuan-Audio as an end-to-end audio large language model integrating both speech understanding and generation with real-time speech interaction capabilities, aligning closely with Spoken Language Models (SLMs) and Full-Duplex Spoken Language Models (FD-SLMs). It explicitly mentions multimodal speech token modeling, a two-stage pretraining strategy to preserve language and audio modeling, and excels in real-time spoken dialogue and question-answering, indicating an investigation into model architecture, training, and evaluation beyond simple ASR/TTS tasks. However, the abstract does not explicitly mention full-duplex features like cognitive parallelism, interruption management, or low-latency benchmarks, nor detailed multi-dimensional evaluations such as behavior arbitration or overlap handling. Nevertheless, it goes beyond half-duplex and single-task ASR/TTS systems and proposes an end-to-end spoken language model with speech as a primary modeling object. Given the thematic focus and recency, it largely fits the inclusion criteria but lacks some explicit synchronization and evaluation details mentioned as key concepts. Therefore, I rate it as better to include to encourage further review and validation of its adherence to the full-duplex and evaluation specifics.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe Baichuan-Audio as an end-to-end audio large language model integrating both speech understanding and generation with real-time speech interaction capabilities, aligning closely with Spoken Language Models (SLMs) and Full-Duplex Spoken Language Models (FD-SLMs). It explicitly mentions multimodal speech token modeling, a two-stage pretraining strategy to preserve language and audio modeling, and excels in real-time spoken dialogue and question-answering, indicating an investigation into model architecture, training, and evaluation beyond simple ASR/TTS tasks. However, the abstract does not explicitly mention full-duplex features like cognitive parallelism, interruption management, or low-latency benchmarks, nor detailed multi-dimensional evaluations such as behavior arbitration or overlap handling. Nevertheless, it goes beyond half-duplex and single-task ASR/TTS systems and proposes an end-to-end spoken language model with speech as a primary modeling object. Given the thematic focus and recency, it largely fits the inclusion criteria but lacks some explicit synchronization and evaluation details mentioned as key concepts. Therefore, I rate it as better to include to encourage further review and validation of its adherence to the full-duplex and evaluation specifics.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MDN: Mamba-Driven Dualstream Network For Medical Hyperspectral Image Segmentation",
    "abstract": "Medical Hyperspectral Imaging (MHSI) offers potential for computational pathology and precision medicine. However, existing CNN and Transformer struggle to balance segmentation accuracy and speed due to high spatial-spectral dimensionality. In this study, we leverage Mamba's global context modeling to propose a dual-stream architecture for joint spatial-spectral feature extraction. To address the limitation of Mamba's unidirectional aggregation, we introduce a recurrent spectral sequence representation to capture low-redundancy global spectral features. Experiments on a public Multi-Dimensional Choledoch dataset and a private Cervical Cancer dataset show that our method outperforms state-of-the-art approaches in segmentation accuracy while minimizing resource usage and achieving the fastest inference speed. Our code will be available at https://github.com/DeepMed-Lab-ECNU/MDN.",
    "metadata": {
      "arxiv_id": "2502.17255",
      "title": "MDN: Mamba-Driven Dualstream Network For Medical Hyperspectral Image Segmentation",
      "summary": "Medical Hyperspectral Imaging (MHSI) offers potential for computational pathology and precision medicine. However, existing CNN and Transformer struggle to balance segmentation accuracy and speed due to high spatial-spectral dimensionality. In this study, we leverage Mamba's global context modeling to propose a dual-stream architecture for joint spatial-spectral feature extraction. To address the limitation of Mamba's unidirectional aggregation, we introduce a recurrent spectral sequence representation to capture low-redundancy global spectral features. Experiments on a public Multi-Dimensional Choledoch dataset and a private Cervical Cancer dataset show that our method outperforms state-of-the-art approaches in segmentation accuracy while minimizing resource usage and achieving the fastest inference speed. Our code will be available at https://github.com/DeepMed-Lab-ECNU/MDN.",
      "authors": [
        "Shijie Lin",
        "Boxiang Yun",
        "Wei Shen",
        "Qingli Li",
        "Anqiang Yang",
        "Yan Wang"
      ],
      "published": "2025-02-24T15:35:37Z",
      "updated": "2025-02-24T15:35:37Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.17255v1",
      "landing_url": "https://arxiv.org/abs/2502.17255v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.17255"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets medical hyperspectral image segmentation using vision-centric models and does not involve Spoken Language Models or speech-language modeling concepts, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets medical hyperspectral image segmentation using vision-centric models and does not involve Spoken Language Models or speech-language modeling concepts, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a dual-stream network architecture for medical hyperspectral image segmentation, which is unrelated to spoken language models, speech language modeling, or full-duplex spoken language interaction, and does not address any speech-related modeling, training, or evaluation topics; thus, it fails to meet the inclusion criteria and falls outside the scope of speech language modeling research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a dual-stream network architecture for medical hyperspectral image segmentation, which is unrelated to spoken language models, speech language modeling, or full-duplex spoken language interaction, and does not address any speech-related modeling, training, or evaluation topics; thus, it fails to meet the inclusion criteria and falls outside the scope of speech language modeling research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Intention Recognition in Real-Time Interactive Navigation Maps",
    "abstract": "In this demonstration, we develop IntentRec4Maps, a system to recognise users' intentions in interactive maps for real-world navigation. IntentRec4Maps uses the Google Maps Platform as the real-world interactive map, and a very effective approach for recognising users' intentions in real-time. We showcase the recognition process of IntentRec4Maps using two different Path-Planners and a Large Language Model (LLM). GitHub: https://github.com/PeijieZ/IntentRec4Maps",
    "metadata": {
      "arxiv_id": "2502.17581",
      "title": "Intention Recognition in Real-Time Interactive Navigation Maps",
      "summary": "In this demonstration, we develop IntentRec4Maps, a system to recognise users' intentions in interactive maps for real-world navigation. IntentRec4Maps uses the Google Maps Platform as the real-world interactive map, and a very effective approach for recognising users' intentions in real-time. We showcase the recognition process of IntentRec4Maps using two different Path-Planners and a Large Language Model (LLM).\n  GitHub: https://github.com/PeijieZ/IntentRec4Maps",
      "authors": [
        "Peijie Zhao",
        "Zunayed Arefin",
        "Felipe Meneguzzi",
        "Ramon Fraga Pereira"
      ],
      "published": "2025-02-24T19:04:18Z",
      "updated": "2025-02-24T19:04:18Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.17581v1",
      "landing_url": "https://arxiv.org/abs/2502.17581v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.17581"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on real-time intention recognition in interactive maps using LLMs but does not center on Spoken Language Models or speech as the core modeling target; it lacks speech data modeling or evaluation of speech-language modeling, thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on real-time intention recognition in interactive maps using LLMs but does not center on Spoken Language Models or speech as the core modeling target; it lacks speech data modeling or evaluation of speech-language modeling, thus it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on intention recognition in interactive navigation maps using Google Maps and Large Language Models, with no indication that it addresses spoken language models, speech language modeling, or full-duplex spoken interaction as required by the inclusion criteria. There is also no mention of speech-based modeling, synchronized dialogue interaction, or related evaluation metrics, which makes it not meet the thematic scope and other inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on intention recognition in interactive navigation maps using Google Maps and Large Language Models, with no indication that it addresses spoken language models, speech language modeling, or full-duplex spoken interaction as required by the inclusion criteria. There is also no mention of speech-based modeling, synchronized dialogue interaction, or related evaluation metrics, which makes it not meet the thematic scope and other inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Wireless sensor networks data synchronization using node MCU memory for precision agriculture applications",
    "abstract": "Wireless Sensor Networks have risen as a highly promising technology suitable for precision agriculture implementations, enabling efficient monitoring and control of agricultural processes. In precision agriculture, accurate and synchronized data collection is crucial for effective analysis and decision making. Using principles of information theory, we can define conditions and parameters that influence the efficient transmission and processing of information. Existing technologies have limitations in maintaining consistent time references, handling node failures, and unreliable communication links, leading to inaccurate data readings. Reliable data storage is demanding now-a-days for storing data on local monitoring station as well as in online live server. Sometime internet is not working properly due to congestion and there is frequent packet loss. Current solutions often synchronize records based on database timestamps, leading to record duplication and waste storage. Both databases synchronize each other after internet restoration. By providing synchronization among nodes and data, accuracy and storage will be saved in IoT based WSNs for precision agriculture applications. A prototype Node-MCU internal memory is used as a resource for achieving data synchronization. This proposed work generates record ID from Node MCU EEPROM which helps in records synchronization if there is any packet loss at the local server or at the online server to maintain synchronization accuracy despite unreliable communication links. Experiment shows that for a particular duration Node MCU generated 2364 packets and packet loss at local server was 08 and at online server was 174 packets. Results shows that after synchronization 99.87% packets were synchronized. Using previous technique of timestamp, the redundancy was 70% which reduced to 0% using our proposed technique.",
    "metadata": {
      "arxiv_id": "2502.18671",
      "title": "Wireless sensor networks data synchronization using node MCU memory for precision agriculture applications",
      "summary": "Wireless Sensor Networks have risen as a highly promising technology suitable for precision agriculture implementations, enabling efficient monitoring and control of agricultural processes. In precision agriculture, accurate and synchronized data collection is crucial for effective analysis and decision making. Using principles of information theory, we can define conditions and parameters that influence the efficient transmission and processing of information. Existing technologies have limitations in maintaining consistent time references, handling node failures, and unreliable communication links, leading to inaccurate data readings. Reliable data storage is demanding now-a-days for storing data on local monitoring station as well as in online live server. Sometime internet is not working properly due to congestion and there is frequent packet loss. Current solutions often synchronize records based on database timestamps, leading to record duplication and waste storage. Both databases synchronize each other after internet restoration. By providing synchronization among nodes and data, accuracy and storage will be saved in IoT based WSNs for precision agriculture applications. A prototype Node-MCU internal memory is used as a resource for achieving data synchronization. This proposed work generates record ID from Node MCU EEPROM which helps in records synchronization if there is any packet loss at the local server or at the online server to maintain synchronization accuracy despite unreliable communication links. Experiment shows that for a particular duration Node MCU generated 2364 packets and packet loss at local server was 08 and at online server was 174 packets. Results shows that after synchronization 99.87% packets were synchronized. Using previous technique of timestamp, the redundancy was 70% which reduced to 0% using our proposed technique.",
      "authors": [
        "Kashif Sattar",
        "Muhammad Arslan",
        "Saqib Majeed",
        "Salim Iqbal"
      ],
      "published": "2025-02-25T22:11:14Z",
      "updated": "2025-02-25T22:11:14Z",
      "categories": [
        "cs.NI",
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.18671v1",
      "landing_url": "https://arxiv.org/abs/2502.18671v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.18671"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此篇標題/摘要聚焦於物聯網感測網路的資料同步與 Node-MCU 記憶體使用，未涉及 Spoken Language Models 或語音語言建模的核心主題與評估，因此不符合納入條件，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此篇標題/摘要聚焦於物聯網感測網路的資料同步與 Node-MCU 記憶體使用，未涉及 Spoken Language Models 或語音語言建模的核心主題與評估，因此不符合納入條件，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on synchronization techniques in wireless sensor networks for precision agriculture and does not relate to spoken language models, speech language modeling, or full-duplex spoken language interaction systems as required by the inclusion criteria. It neither discusses model architecture, training, or evaluation related to speech modeling, nor does it address synchronization or interaction in the context of spoken language models. Therefore, it does not meet the inclusion criteria and falls outside the scope defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on synchronization techniques in wireless sensor networks for precision agriculture and does not relate to spoken language models, speech language modeling, or full-duplex spoken language interaction systems as required by the inclusion criteria. It neither discusses model architecture, training, or evaluation related to speech modeling, nor does it address synchronization or interaction in the context of spoken language models. Therefore, it does not meet the inclusion criteria and falls outside the scope defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "BeamVQ: Beam Search with Vector Quantization to Mitigate Data Scarcity in Physical Spatiotemporal Forecasting",
    "abstract": "In practice, physical spatiotemporal forecasting can suffer from data scarcity, because collecting large-scale data is non-trivial, especially for extreme events. Hence, we propose \\method{}, a novel probabilistic framework to realize iterative self-training with new self-ensemble strategies, achieving better physical consistency and generalization on extreme events. Following any base forecasting model, we can encode its deterministic outputs into a latent space and retrieve multiple codebook entries to generate probabilistic outputs. Then BeamVQ extends the beam search from discrete spaces to the continuous state spaces in this field. We can further employ domain-specific metrics (e.g., Critical Success Index for extreme events) to filter out the top-k candidates and develop the new self-ensemble strategy by combining the high-quality candidates. The self-ensemble can not only improve the inference quality and robustness but also iteratively augment the training datasets during continuous self-training. Consequently, BeamVQ realizes the exploration of rare but critical phenomena beyond the original dataset. Comprehensive experiments on different benchmarks and backbones show that BeamVQ consistently reduces forecasting MSE (up to 39%), enhancing extreme events detection and proving its effectiveness in handling data scarcity.",
    "metadata": {
      "arxiv_id": "2502.18925",
      "title": "BeamVQ: Beam Search with Vector Quantization to Mitigate Data Scarcity in Physical Spatiotemporal Forecasting",
      "summary": "In practice, physical spatiotemporal forecasting can suffer from data scarcity, because collecting large-scale data is non-trivial, especially for extreme events. Hence, we propose \\method{}, a novel probabilistic framework to realize iterative self-training with new self-ensemble strategies, achieving better physical consistency and generalization on extreme events. Following any base forecasting model, we can encode its deterministic outputs into a latent space and retrieve multiple codebook entries to generate probabilistic outputs. Then BeamVQ extends the beam search from discrete spaces to the continuous state spaces in this field. We can further employ domain-specific metrics (e.g., Critical Success Index for extreme events) to filter out the top-k candidates and develop the new self-ensemble strategy by combining the high-quality candidates. The self-ensemble can not only improve the inference quality and robustness but also iteratively augment the training datasets during continuous self-training. Consequently, BeamVQ realizes the exploration of rare but critical phenomena beyond the original dataset. Comprehensive experiments on different benchmarks and backbones show that BeamVQ consistently reduces forecasting MSE (up to 39%), enhancing extreme events detection and proving its effectiveness in handling data scarcity.",
      "authors": [
        "Weiyan Wang",
        "Xingjian Shi",
        "Ruiqi Shu",
        "Yuan Gao",
        "Rui Ray Chen",
        "Kun Wang",
        "Fan Xu",
        "Jinbao Xue",
        "Shuaipeng Li",
        "Yangyu Tao",
        "Di Wang",
        "Hao Wu",
        "Xiaomeng Huang"
      ],
      "published": "2025-02-26T08:27:25Z",
      "updated": "2025-02-26T08:27:25Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.18925v1",
      "landing_url": "https://arxiv.org/abs/2502.18925v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.18925"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item deals with BeamVQ for probabilistic physical spatiotemporal forecasting and data scarcity, with no focus on Spoken Language Models or speech-language modeling; it does not meet the inclusion criteria nor align with the exclusion criteria either, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item deals with BeamVQ for probabilistic physical spatiotemporal forecasting and data scarcity, with no focus on Spoken Language Models or speech-language modeling; it does not meet the inclusion criteria nor align with the exclusion criteria either, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on physical spatiotemporal forecasting with data scarcity mitigation, addressing probabilistic modeling for physical phenomena rather than spoken language modeling or speech-based interactive systems. There is no mention of spoken language models, speech language models, or any speech-based modeling or interaction, and it does not discuss synchronization or real-time speech interaction aspects defined in the inclusion criteria. Hence, it does not meet the inclusion criteria of focusing on spoken language modeling or speech-based turn-taking or full-duplex interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on physical spatiotemporal forecasting with data scarcity mitigation, addressing probabilistic modeling for physical phenomena rather than spoken language modeling or speech-based interactive systems. There is no mention of spoken language models, speech language models, or any speech-based modeling or interaction, and it does not discuss synchronization or real-time speech interaction aspects defined in the inclusion criteria. Hence, it does not meet the inclusion criteria of focusing on spoken language modeling or speech-based turn-taking or full-duplex interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Bidirectionalization For The Common People",
    "abstract": "This paper presents an innovative approach to applying bidirectional transformations (BX) in practice. To introduce BX to a wider audience of technologists, engineers, and researchers, we have chosen to use C# to develop Bifrons - a library of BX lenses that replaces domain-specific programming languages (DSL) in practical use. The proposed approach simplifies the implementation effort for two-way transformations by using simple symmetric lenses as the initial design pattern. It ensures correctness within reason by providing a simple lens-testing framework. We demonstrate the usability of BX lenses in a realistic scenario by using Bifrons to perform a case study experiment synchronizing data from two structurally and technologically heterogeneous databases.",
    "metadata": {
      "arxiv_id": "2502.18954",
      "title": "Bidirectionalization For The Common People",
      "summary": "This paper presents an innovative approach to applying bidirectional transformations (BX) in practice. To introduce BX to a wider audience of technologists, engineers, and researchers, we have chosen to use C# to develop Bifrons - a library of BX lenses that replaces domain-specific programming languages (DSL) in practical use. The proposed approach simplifies the implementation effort for two-way transformations by using simple symmetric lenses as the initial design pattern. It ensures correctness within reason by providing a simple lens-testing framework. We demonstrate the usability of BX lenses in a realistic scenario by using Bifrons to perform a case study experiment synchronizing data from two structurally and technologically heterogeneous databases.",
      "authors": [
        "Juraj Dončević",
        "Mario Brčić",
        "Danijel Mlinarić"
      ],
      "published": "2025-02-26T09:05:13Z",
      "updated": "2025-03-06T13:17:02Z",
      "categories": [
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.18954v2",
      "landing_url": "https://arxiv.org/abs/2502.18954v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.18954"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses bidirectional transformations and a C# library; it does not involve Spoken Language Models or speech-language modeling, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses bidirectional transformations and a C# library; it does not involve Spoken Language Models or speech-language modeling, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on bidirectional transformations using a C# library for synchronizing heterogeneous databases, which is unrelated to spoken language models, speech language modeling, or full-duplex spoken language interaction; it does not mention speech, voice modeling, synchronization in conversational speech, or any related evaluation metrics as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on bidirectional transformations using a C# library for synchronizing heterogeneous databases, which is unrelated to spoken language models, speech language modeling, or full-duplex spoken language interaction; it does not mention speech, voice modeling, synchronization in conversational speech, or any related evaluation metrics as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Ev-3DOD: Pushing the Temporal Boundaries of 3D Object Detection with Event Cameras",
    "abstract": "Detecting 3D objects in point clouds plays a crucial role in autonomous driving systems. Recently, advanced multi-modal methods incorporating camera information have achieved notable performance. For a safe and effective autonomous driving system, algorithms that excel not only in accuracy but also in speed and low latency are essential. However, existing algorithms fail to meet these requirements due to the latency and bandwidth limitations of fixed frame rate sensors, e.g., LiDAR and camera. To address this limitation, we introduce asynchronous event cameras into 3D object detection for the first time. We leverage their high temporal resolution and low bandwidth to enable high-speed 3D object detection. Our method enables detection even during inter-frame intervals when synchronized data is unavailable, by retrieving previous 3D information through the event camera. Furthermore, we introduce the first event-based 3D object detection dataset, DSEC-3DOD, which includes ground-truth 3D bounding boxes at 100 FPS, establishing the first benchmark for event-based 3D detectors. The code and dataset are available at https://github.com/mickeykang16/Ev3DOD.",
    "metadata": {
      "arxiv_id": "2502.19630",
      "title": "Ev-3DOD: Pushing the Temporal Boundaries of 3D Object Detection with Event Cameras",
      "summary": "Detecting 3D objects in point clouds plays a crucial role in autonomous driving systems. Recently, advanced multi-modal methods incorporating camera information have achieved notable performance. For a safe and effective autonomous driving system, algorithms that excel not only in accuracy but also in speed and low latency are essential. However, existing algorithms fail to meet these requirements due to the latency and bandwidth limitations of fixed frame rate sensors, e.g., LiDAR and camera. To address this limitation, we introduce asynchronous event cameras into 3D object detection for the first time. We leverage their high temporal resolution and low bandwidth to enable high-speed 3D object detection. Our method enables detection even during inter-frame intervals when synchronized data is unavailable, by retrieving previous 3D information through the event camera. Furthermore, we introduce the first event-based 3D object detection dataset, DSEC-3DOD, which includes ground-truth 3D bounding boxes at 100 FPS, establishing the first benchmark for event-based 3D detectors. The code and dataset are available at https://github.com/mickeykang16/Ev3DOD.",
      "authors": [
        "Hoonhee Cho",
        "Jae-young Kang",
        "Youngho Kim",
        "Kuk-Jin Yoon"
      ],
      "published": "2025-02-26T23:51:25Z",
      "updated": "2025-02-26T23:51:25Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.19630v1",
      "landing_url": "https://arxiv.org/abs/2502.19630v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.19630"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe 3D object detection with event cameras (CV topic) and do not involve Spoken Language Models or speech-language modeling, thus not meeting the inclusion criteria and failing all related requirements.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe 3D object detection with event cameras (CV topic) and do not involve Spoken Language Models or speech-language modeling, thus not meeting the inclusion criteria and failing all related requirements.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on 3D object detection using event cameras for autonomous driving, without any mention of Spoken Language Models, speech-level language modeling, full-duplex interaction, or any speech-related modeling aspects required by the inclusion criteria; hence it does not meet the thematic scope nor the explicit keyword requirement, and should therefore be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on 3D object detection using event cameras for autonomous driving, without any mention of Spoken Language Models, speech-level language modeling, full-duplex interaction, or any speech-related modeling aspects required by the inclusion criteria; hence it does not meet the thematic scope nor the explicit keyword requirement, and should therefore be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Impilict Runge-Kutta based sparse identification of governing equations in biologically motivated systems",
    "abstract": "Identifying governing equations in physical and biological systems from datasets remains a long-standing challenge across various scientific disciplines, providing mechanistic insights into complex system evolution. Common methods like sparse identification of nonlinear dynamics (SINDy) often rely on precise derivative estimations, making them vulnerable to data scarcity and noise. This study presents a novel data-driven framework by integrating high order implicit Runge-Kutta methods (IRKs) with the sparse identification, termed IRK-SINDy. The framework exhibits remarkable robustness to data scarcity and noise by leveraging the lower stepsize constraint of IRKs. Two methods for incorporating IRKs into sparse regression are introduced: one employs iterative schemes for numerically solving nonlinear algebraic system of equations, while the other utilizes deep neural networks to predict stage values of IRKs. The performance of IRK-SINDy is demonstrated through numerical experiments on benchmark problems with varied dynamical behaviors, including linear and nonlinear oscillators, the Lorenz system, and biologically relevant models like predator-prey dynamics, logistic growth, and the FitzHugh-Nagumo model. Results indicate that IRK-SINDy outperforms conventional SINDy and the RK4-SINDy framework, particularly under conditions of extreme data scarcity and noise, yielding interpretable and generalizable models.",
    "metadata": {
      "arxiv_id": "2502.20319",
      "title": "Impilict Runge-Kutta based sparse identification of governing equations in biologically motivated systems",
      "summary": "Identifying governing equations in physical and biological systems from datasets remains a long-standing challenge across various scientific disciplines, providing mechanistic insights into complex system evolution. Common methods like sparse identification of nonlinear dynamics (SINDy) often rely on precise derivative estimations, making them vulnerable to data scarcity and noise. This study presents a novel data-driven framework by integrating high order implicit Runge-Kutta methods (IRKs) with the sparse identification, termed IRK-SINDy. The framework exhibits remarkable robustness to data scarcity and noise by leveraging the lower stepsize constraint of IRKs. Two methods for incorporating IRKs into sparse regression are introduced: one employs iterative schemes for numerically solving nonlinear algebraic system of equations, while the other utilizes deep neural networks to predict stage values of IRKs. The performance of IRK-SINDy is demonstrated through numerical experiments on benchmark problems with varied dynamical behaviors, including linear and nonlinear oscillators, the Lorenz system, and biologically relevant models like predator-prey dynamics, logistic growth, and the FitzHugh-Nagumo model. Results indicate that IRK-SINDy outperforms conventional SINDy and the RK4-SINDy framework, particularly under conditions of extreme data scarcity and noise, yielding interpretable and generalizable models.",
      "authors": [
        "Mehrdad Anvari",
        "Hamidreza Marasi",
        "Hossein Kheiri"
      ],
      "published": "2025-02-27T17:44:37Z",
      "updated": "2025-02-27T17:44:37Z",
      "categories": [
        "math.DS",
        "cs.LG",
        "math.NA",
        "q-bio.QM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.20319v1",
      "landing_url": "https://arxiv.org/abs/2502.20319v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.20319"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses IRK-SINDy for identifying governing equations in biological systems, which is a mathematical dynamical-systems topic and not about Spoken Language Models or speech-language modeling, nor about synchronous interactive speech tasks; thus it does not meet the inclusion criteria focused on SLM/FD-SLM research.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses IRK-SINDy for identifying governing equations in biological systems, which is a mathematical dynamical-systems topic and not about Spoken Language Models or speech-language modeling, nor about synchronous interactive speech tasks; thus it does not meet the inclusion criteria focused on SLM/FD-SLM research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on identifying governing equations in biological and physical systems using a data-driven framework integrating implicit Runge-Kutta and sparse identification methods, without discussing spoken language models, speech language modeling, or related architectures, training, or evaluations involving speech or full-duplex spoken interaction. Hence, it does not meet the inclusion criteria which emphasize speech-level language modeling and interaction aspects, nor does it discuss synchronization or dialog arbitration relevant to FD-SLMs.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on identifying governing equations in biological and physical systems using a data-driven framework integrating implicit Runge-Kutta and sparse identification methods, without discussing spoken language models, speech language modeling, or related architectures, training, or evaluations involving speech or full-duplex spoken interaction. Hence, it does not meet the inclusion criteria which emphasize speech-level language modeling and interaction aspects, nor does it discuss synchronization or dialog arbitration relevant to FD-SLMs.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "InspireMusic: Integrating Super Resolution and Large Language Model for High-Fidelity Long-Form Music Generation",
    "abstract": "We introduce InspireMusic, a framework integrated super resolution and large language model for high-fidelity long-form music generation. A unified framework generates high-fidelity music, songs, and audio, which incorporates an autoregressive transformer with a super-resolution flow-matching model. This framework enables the controllable generation of high-fidelity long-form music at a higher sampling rate from both text and audio prompts. Our model differs from previous approaches, as we utilize an audio tokenizer with one codebook that contains richer semantic information, thereby reducing training costs and enhancing efficiency. This combination enables us to achieve high-quality audio generation with long-form coherence of up to $8$ minutes. Then, an autoregressive transformer model based on Qwen 2.5 predicts audio tokens. Next, we employ a super-resolution flow-matching model to generate high-sampling rate audio with fine-grained details learned from an acoustic codec model. Comprehensive experiments show that the InspireMusic-1.5B-Long model has a comparable performance to recent top-tier open-source systems, including MusicGen and Stable Audio 2.0, on subjective and objective evaluations. The code and pre-trained models are released at https://github.com/FunAudioLLM/InspireMusic.",
    "metadata": {
      "arxiv_id": "2503.00084",
      "title": "InspireMusic: Integrating Super Resolution and Large Language Model for High-Fidelity Long-Form Music Generation",
      "summary": "We introduce InspireMusic, a framework integrated super resolution and large language model for high-fidelity long-form music generation. A unified framework generates high-fidelity music, songs, and audio, which incorporates an autoregressive transformer with a super-resolution flow-matching model. This framework enables the controllable generation of high-fidelity long-form music at a higher sampling rate from both text and audio prompts. Our model differs from previous approaches, as we utilize an audio tokenizer with one codebook that contains richer semantic information, thereby reducing training costs and enhancing efficiency. This combination enables us to achieve high-quality audio generation with long-form coherence of up to $8$ minutes. Then, an autoregressive transformer model based on Qwen 2.5 predicts audio tokens. Next, we employ a super-resolution flow-matching model to generate high-sampling rate audio with fine-grained details learned from an acoustic codec model. Comprehensive experiments show that the InspireMusic-1.5B-Long model has a comparable performance to recent top-tier open-source systems, including MusicGen and Stable Audio 2.0, on subjective and objective evaluations. The code and pre-trained models are released at https://github.com/FunAudioLLM/InspireMusic.",
      "authors": [
        "Chong Zhang",
        "Yukun Ma",
        "Qian Chen",
        "Wen Wang",
        "Shengkui Zhao",
        "Zexu Pan",
        "Hao Wang",
        "Chongjia Ni",
        "Trung Hieu Nguyen",
        "Kun Zhou",
        "Yidi Jiang",
        "Chaohong Tan",
        "Zhifu Gao",
        "Zhihao Du",
        "Bin Ma"
      ],
      "published": "2025-02-28T09:58:25Z",
      "updated": "2025-02-28T09:58:25Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.00084v1",
      "landing_url": "https://arxiv.org/abs/2503.00084v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.00084"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此研究聚焦於高保真長篇音樂生成，採用文本與音頻提示的生成框架，並以音訊編碼與超解析等技術為核心，未以 Spoken Language Models 或語音語言模型為核心進行架構、訓練或評估分析，因此不符合題目所定的語音語言模型相關納入標準。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此研究聚焦於高保真長篇音樂生成，採用文本與音頻提示的生成框架，並以音訊編碼與超解析等技術為核心，未以 Spoken Language Models 或語音語言模型為核心進行架構、訓練或評估分析，因此不符合題目所定的語音語言模型相關納入標準。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a framework for high-fidelity long-form music generation integrating super resolution and large language models, but it does not address spoken language modeling or speech as the core modeling object; furthermore, it lacks discussion on turn-taking, full-duplex interaction, or related spoken interactive phenomena, and does not mention spoken language models or equivalents explicitly, thus failing to meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a framework for high-fidelity long-form music generation integrating super resolution and large language models, but it does not address spoken language modeling or speech as the core modeling object; furthermore, it lacks discussion on turn-taking, full-duplex interaction, or related spoken interactive phenomena, and does not mention spoken language models or equivalents explicitly, thus failing to meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Dynamical Diffusion: Learning Temporal Dynamics with Diffusion Models",
    "abstract": "Diffusion models have emerged as powerful generative frameworks by progressively adding noise to data through a forward process and then reversing this process to generate realistic samples. While these models have achieved strong performance across various tasks and modalities, their application to temporal predictive learning remains underexplored. Existing approaches treat predictive learning as a conditional generation problem, but often fail to fully exploit the temporal dynamics inherent in the data, leading to challenges in generating temporally coherent sequences. To address this, we introduce Dynamical Diffusion (DyDiff), a theoretically sound framework that incorporates temporally aware forward and reverse processes. Dynamical Diffusion explicitly models temporal transitions at each diffusion step, establishing dependencies on preceding states to better capture temporal dynamics. Through the reparameterization trick, Dynamical Diffusion achieves efficient training and inference similar to any standard diffusion model. Extensive experiments across scientific spatiotemporal forecasting, video prediction, and time series forecasting demonstrate that Dynamical Diffusion consistently improves performance in temporal predictive tasks, filling a crucial gap in existing methodologies. Code is available at this repository: https://github.com/thuml/dynamical-diffusion.",
    "metadata": {
      "arxiv_id": "2503.00951",
      "title": "Dynamical Diffusion: Learning Temporal Dynamics with Diffusion Models",
      "summary": "Diffusion models have emerged as powerful generative frameworks by progressively adding noise to data through a forward process and then reversing this process to generate realistic samples. While these models have achieved strong performance across various tasks and modalities, their application to temporal predictive learning remains underexplored. Existing approaches treat predictive learning as a conditional generation problem, but often fail to fully exploit the temporal dynamics inherent in the data, leading to challenges in generating temporally coherent sequences. To address this, we introduce Dynamical Diffusion (DyDiff), a theoretically sound framework that incorporates temporally aware forward and reverse processes. Dynamical Diffusion explicitly models temporal transitions at each diffusion step, establishing dependencies on preceding states to better capture temporal dynamics. Through the reparameterization trick, Dynamical Diffusion achieves efficient training and inference similar to any standard diffusion model. Extensive experiments across scientific spatiotemporal forecasting, video prediction, and time series forecasting demonstrate that Dynamical Diffusion consistently improves performance in temporal predictive tasks, filling a crucial gap in existing methodologies. Code is available at this repository: https://github.com/thuml/dynamical-diffusion.",
      "authors": [
        "Xingzhuo Guo",
        "Yu Zhang",
        "Baixu Chen",
        "Haoran Xu",
        "Jianmin Wang",
        "Mingsheng Long"
      ],
      "published": "2025-03-02T16:10:32Z",
      "updated": "2025-03-02T16:10:32Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.00951v1",
      "landing_url": "https://arxiv.org/abs/2503.00951v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.00951"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on diffusion-based temporal modeling in general and is not about Spoken Language Models or speech-centric language modeling, so it does not meet the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on diffusion-based temporal modeling in general and is not about Spoken Language Models or speech-centric language modeling, so it does not meet the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a diffusion model focused on temporal dynamics in data such as video prediction and time series forecasting, without any mention of spoken language models, speech language modeling, full-duplex spoken language interaction, or related concepts. The study does not satisfy the inclusion criteria which require focus on spoken language models or speech as a core modeling target, nor does it address synchronization, full-duplex communication, or any direct speech-language modeling evaluations. Therefore, it should be excluded based on the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a diffusion model focused on temporal dynamics in data such as video prediction and time series forecasting, without any mention of spoken language models, speech language modeling, full-duplex spoken language interaction, or related concepts. The study does not satisfy the inclusion criteria which require focus on spoken language models or speech as a core modeling target, nor does it address synchronization, full-duplex communication, or any direct speech-language modeling evaluations. Therefore, it should be excluded based on the criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Talking Turns: Benchmarking Audio Foundation Models on Turn-Taking Dynamics",
    "abstract": "The recent wave of audio foundation models (FMs) could provide new capabilities for conversational modeling. However, there have been limited efforts to evaluate these audio FMs comprehensively on their ability to have natural and interactive conversations. To engage in meaningful conversation with the end user, we would want the FMs to additionally perform a fluent succession of turns without too much overlapping speech or long stretches of silence. Inspired by this, we ask whether the recently proposed audio FMs can understand, predict, and perform turn-taking events? To answer this, we propose a novel evaluation protocol that can assess spoken dialog system's turn-taking capabilities using a supervised model as a judge that has been trained to predict turn-taking events in human-human conversations. Using this protocol, we present the first comprehensive user study that evaluates existing spoken dialogue systems on their ability to perform turn-taking events and reveal many interesting insights, such as they sometimes do not understand when to speak up, can interrupt too aggressively and rarely backchannel. We further evaluate multiple open-source and proprietary audio FMs accessible through APIs on carefully curated test benchmarks from Switchboard to measure their ability to understand and predict turn-taking events and identify significant room for improvement. We will open source our evaluation platform to promote the development of advanced conversational AI systems.",
    "metadata": {
      "arxiv_id": "2503.01174",
      "title": "Talking Turns: Benchmarking Audio Foundation Models on Turn-Taking Dynamics",
      "summary": "The recent wave of audio foundation models (FMs) could provide new capabilities for conversational modeling. However, there have been limited efforts to evaluate these audio FMs comprehensively on their ability to have natural and interactive conversations. To engage in meaningful conversation with the end user, we would want the FMs to additionally perform a fluent succession of turns without too much overlapping speech or long stretches of silence. Inspired by this, we ask whether the recently proposed audio FMs can understand, predict, and perform turn-taking events? To answer this, we propose a novel evaluation protocol that can assess spoken dialog system's turn-taking capabilities using a supervised model as a judge that has been trained to predict turn-taking events in human-human conversations. Using this protocol, we present the first comprehensive user study that evaluates existing spoken dialogue systems on their ability to perform turn-taking events and reveal many interesting insights, such as they sometimes do not understand when to speak up, can interrupt too aggressively and rarely backchannel. We further evaluate multiple open-source and proprietary audio FMs accessible through APIs on carefully curated test benchmarks from Switchboard to measure their ability to understand and predict turn-taking events and identify significant room for improvement. We will open source our evaluation platform to promote the development of advanced conversational AI systems.",
      "authors": [
        "Siddhant Arora",
        "Zhiyun Lu",
        "Chung-Cheng Chiu",
        "Ruoming Pang",
        "Shinji Watanabe"
      ],
      "published": "2025-03-03T04:46:04Z",
      "updated": "2025-03-03T04:46:04Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.01174v1",
      "landing_url": "https://arxiv.org/abs/2503.01174v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.01174"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: This item targets spoken language modeling in audio-based turn-taking with evaluation benchmarks (full-duplex interaction), within the last 3 years, thus meeting inclusion criteria and not triggering exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "Reasoning: This item targets spoken language modeling in audio-based turn-taking with evaluation benchmarks (full-duplex interaction), within the last 3 years, thus meeting inclusion criteria and not triggering exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating audio foundation models for turn-taking dynamics, emphasizing spoken dialog systems' ability to perform interactive conversations involving turn-taking, interruptions, backchanneling—key aspects of full-duplex spoken language models and synchronization. It includes a proposed comprehensive evaluation framework with supervised models predicting turn-taking events, aligning with the inclusion criteria of addressing spoken language models with detailed evaluation protocols rather than only ASR or TTS tasks. The abstract mentions benchmarks from Switchboard and discusses limitations and improvements, reflecting an academic, methodical study within recent research. Although it does not explicitly use the exact keywords \"spoken language model(s)\" or \"speech language model(s)\", the content strongly aligns with the narrative and key concepts of modeling speech with an interactive focus beyond half-duplex turn-taking, fitting well within the scope. Therefore, it meets all inclusion criteria and avoids the exclusion criteria about focusing solely on ASR or half-duplex systems or lacking interactive evaluation.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating audio foundation models for turn-taking dynamics, emphasizing spoken dialog systems' ability to perform interactive conversations involving turn-taking, interruptions, backchanneling—key aspects of full-duplex spoken language models and synchronization. It includes a proposed comprehensive evaluation framework with supervised models predicting turn-taking events, aligning with the inclusion criteria of addressing spoken language models with detailed evaluation protocols rather than only ASR or TTS tasks. The abstract mentions benchmarks from Switchboard and discusses limitations and improvements, reflecting an academic, methodical study within recent research. Although it does not explicitly use the exact keywords \"spoken language model(s)\" or \"speech language model(s)\", the content strongly aligns with the narrative and key concepts of modeling speech with an interactive focus beyond half-duplex turn-taking, fitting well within the scope. Therefore, it meets all inclusion criteria and avoids the exclusion criteria about focusing solely on ASR or half-duplex systems or lacking interactive evaluation.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Voice Cloning for Dysarthric Speech Synthesis: Addressing Data Scarcity in Speech-Language Pathology",
    "abstract": "This study explores voice cloning to generate synthetic speech replicating the unique patterns of individuals with dysarthria. Using the TORGO dataset, we address data scarcity and privacy challenges in speech-language pathology. Our contributions include demonstrating that voice cloning preserves dysarthric speech characteristics, analyzing differences between real and synthetic data, and discussing implications for diagnostics, rehabilitation, and communication. We cloned voices from dysarthric and control speakers using a commercial platform, ensuring gender-matched synthetic voices. A licensed speech-language pathologist (SLP) evaluated a subset for dysarthria, speaker gender, and synthetic indicators. The SLP correctly identified dysarthria in all cases and speaker gender in 95% but misclassified 30% of synthetic samples as real, indicating high realism. Our results suggest synthetic speech effectively captures disordered characteristics and that voice cloning has advanced to produce high-quality data resembling real speech, even to trained professionals. This has critical implications for healthcare, where synthetic data can mitigate data scarcity, protect privacy, and enhance AI-driven diagnostics. By enabling the creation of diverse, high-quality speech datasets, voice cloning can improve generalizable models, personalize therapy, and advance assistive technologies for dysarthria. We publicly release our synthetic dataset to foster further research and collaboration, aiming to develop robust models that improve patient outcomes in speech-language pathology.",
    "metadata": {
      "arxiv_id": "2503.01266",
      "title": "Voice Cloning for Dysarthric Speech Synthesis: Addressing Data Scarcity in Speech-Language Pathology",
      "summary": "This study explores voice cloning to generate synthetic speech replicating the unique patterns of individuals with dysarthria. Using the TORGO dataset, we address data scarcity and privacy challenges in speech-language pathology. Our contributions include demonstrating that voice cloning preserves dysarthric speech characteristics, analyzing differences between real and synthetic data, and discussing implications for diagnostics, rehabilitation, and communication. We cloned voices from dysarthric and control speakers using a commercial platform, ensuring gender-matched synthetic voices. A licensed speech-language pathologist (SLP) evaluated a subset for dysarthria, speaker gender, and synthetic indicators. The SLP correctly identified dysarthria in all cases and speaker gender in 95% but misclassified 30% of synthetic samples as real, indicating high realism. Our results suggest synthetic speech effectively captures disordered characteristics and that voice cloning has advanced to produce high-quality data resembling real speech, even to trained professionals. This has critical implications for healthcare, where synthetic data can mitigate data scarcity, protect privacy, and enhance AI-driven diagnostics. By enabling the creation of diverse, high-quality speech datasets, voice cloning can improve generalizable models, personalize therapy, and advance assistive technologies for dysarthria.\n  We publicly release our synthetic dataset to foster further research and collaboration, aiming to develop robust models that improve patient outcomes in speech-language pathology.",
      "authors": [
        "Birger Moell",
        "Fredrik Sand Aronsson"
      ],
      "published": "2025-03-03T07:44:49Z",
      "updated": "2025-03-03T07:44:49Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.01266v1",
      "landing_url": "https://arxiv.org/abs/2503.01266v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.01266"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on voice cloning for dysarthric speech synthesis and synthetic data generation, but it does not center on Spoken Language Models or Full-Duplex Spoken Language Models, nor on interactive synchronization or speech-language modeling architectures, thus it does not meet the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on voice cloning for dysarthric speech synthesis and synthetic data generation, but it does not center on Spoken Language Models or Full-Duplex Spoken Language Models, nor on interactive synchronization or speech-language modeling architectures, thus it does not meet the core inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on voice cloning for dysarthric speech to address data scarcity and privacy in speech-language pathology, primarily targeting synthetic speech generation and evaluation. It does not explicitly address Spoken Language Models or Full-Duplex Spoken Language Models, nor does it discuss model architectures, training, or evaluation related to speech language modeling or real-time interactive behavior such as cognitive parallelism, turn-taking, or synchronization. The research centers on speech synthesis and pathological speech characterization, lacking aspects of speech modeling as defined in the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on voice cloning for dysarthric speech to address data scarcity and privacy in speech-language pathology, primarily targeting synthetic speech generation and evaluation. It does not explicitly address Spoken Language Models or Full-Duplex Spoken Language Models, nor does it discuss model architectures, training, or evaluation related to speech language modeling or real-time interactive behavior such as cognitive parallelism, turn-taking, or synchronization. The research centers on speech synthesis and pathological speech characterization, lacking aspects of speech modeling as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Car-STAGE: Automated framework for large-scale high-dimensional simulated time-series data generation based on user-defined criteria",
    "abstract": "Generating large-scale sensing datasets through photo-realistic simulation is an important aspect of many robotics applications such as autonomous driving. In this paper, we consider the problem of synchronous data collection from the open-source CARLA simulator using multiple sensors attached to vehicle based on user-defined criteria. We propose a novel, one-step framework that we refer to as Car-STAGE, based on CARLA simulator, to generate data using a graphical user interface (GUI) defining configuration parameters to data collection without any user intervention. This framework can utilize the user-defined configuration parameters such as choice of maps, number and configurations of sensors, environmental and lighting conditions etc. to run the simulation in the background, collecting high-dimensional sensor data from diverse sensors such as RGB Camera, LiDAR, Radar, Depth Camera, IMU Sensor, GNSS Sensor, Semantic Segmentation Camera, Instance Segmentation Camera, and Optical Flow Camera along with the ground-truths of the individual actors and storing the sensor data as well as ground-truth labels in a local or cloud-based database. The framework uses multiple threads where a main thread runs the server, a worker thread deals with queue and frame number and the rest of the threads processes the sensor data. The other way we derive speed up over the native implementation is by memory mapping the raw binary data into the disk and then converting the data into known formats at the end of data collection. We show that using these techniques, we gain a significant speed up over frames, under an increasing set of sensors and over the number of spawned objects.",
    "metadata": {
      "arxiv_id": "2503.03100",
      "title": "Car-STAGE: Automated framework for large-scale high-dimensional simulated time-series data generation based on user-defined criteria",
      "summary": "Generating large-scale sensing datasets through photo-realistic simulation is an important aspect of many robotics applications such as autonomous driving. In this paper, we consider the problem of synchronous data collection from the open-source CARLA simulator using multiple sensors attached to vehicle based on user-defined criteria. We propose a novel, one-step framework that we refer to as Car-STAGE, based on CARLA simulator, to generate data using a graphical user interface (GUI) defining configuration parameters to data collection without any user intervention. This framework can utilize the user-defined configuration parameters such as choice of maps, number and configurations of sensors, environmental and lighting conditions etc. to run the simulation in the background, collecting high-dimensional sensor data from diverse sensors such as RGB Camera, LiDAR, Radar, Depth Camera, IMU Sensor, GNSS Sensor, Semantic Segmentation Camera, Instance Segmentation Camera, and Optical Flow Camera along with the ground-truths of the individual actors and storing the sensor data as well as ground-truth labels in a local or cloud-based database. The framework uses multiple threads where a main thread runs the server, a worker thread deals with queue and frame number and the rest of the threads processes the sensor data. The other way we derive speed up over the native implementation is by memory mapping the raw binary data into the disk and then converting the data into known formats at the end of data collection. We show that using these techniques, we gain a significant speed up over frames, under an increasing set of sensors and over the number of spawned objects.",
      "authors": [
        "Asma A. Almutairi",
        "David J. LeBlanc",
        "Arpan Kusari"
      ],
      "published": "2025-03-05T01:32:56Z",
      "updated": "2025-03-05T01:32:56Z",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.03100v1",
      "landing_url": "https://arxiv.org/abs/2503.03100v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.03100"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about automated data generation for multi-sensor time-series in autonomous driving using CARLA, not about Spoken Language Models or speech-language modeling, and it lacks speech interaction or evaluation aspects, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about automated data generation for multi-sensor time-series in autonomous driving using CARLA, not about Spoken Language Models or speech-language modeling, and it lacks speech interaction or evaluation aspects, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on a framework for generating simulated sensor data for robotics applications using the CARLA simulator. It does not address spoken language models or speech language modeling as a core part of its research, nor does it discuss synchronous full-duplex spoken language interaction, modeling, training, or evaluation. Therefore, it does not meet the inclusion criteria related to spoken language models and full-duplex speech interaction and instead focuses on a different research area entirely.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on a framework for generating simulated sensor data for robotics applications using the CARLA simulator. It does not address spoken language models or speech language modeling as a core part of its research, nor does it discuss synchronous full-duplex spoken language interaction, modeling, training, or evaluation. Therefore, it does not meet the inclusion criteria related to spoken language models and full-duplex speech interaction and instead focuses on a different research area entirely.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "On the Relation Between Speech Quality and Quantized Latent Representations of Neural Codecs",
    "abstract": "Neural audio signal codecs have attracted significant attention in recent years. In essence, the impressive low bitrate achieved by such encoders is enabled by learning an abstract representation that captures the properties of encoded signals, e.g., speech. In this work, we investigate the relation between the latent representation of the input signal learned by a neural codec and the quality of speech signals. To do so, we introduce Latent-representation-to-Quantization error Ratio (LQR) measures, which quantify the distance from the idealized neural codec's speech signal model for a given speech signal. We compare the proposed metrics to intrusive measures as well as data-driven supervised methods using two subjective speech quality datasets. This analysis shows that the proposed LQR correlates strongly (up to 0.9 Pearson's correlation) with the subjective quality of speech. Despite being a non-intrusive metric, this yields a competitive performance with, or even better than, other pre-trained and intrusive measures. These results show that LQR is a promising basis for more sophisticated speech quality measures.",
    "metadata": {
      "arxiv_id": "2503.03304",
      "title": "On the Relation Between Speech Quality and Quantized Latent Representations of Neural Codecs",
      "summary": "Neural audio signal codecs have attracted significant attention in recent years. In essence, the impressive low bitrate achieved by such encoders is enabled by learning an abstract representation that captures the properties of encoded signals, e.g., speech. In this work, we investigate the relation between the latent representation of the input signal learned by a neural codec and the quality of speech signals. To do so, we introduce Latent-representation-to-Quantization error Ratio (LQR) measures, which quantify the distance from the idealized neural codec's speech signal model for a given speech signal. We compare the proposed metrics to intrusive measures as well as data-driven supervised methods using two subjective speech quality datasets. This analysis shows that the proposed LQR correlates strongly (up to 0.9 Pearson's correlation) with the subjective quality of speech. Despite being a non-intrusive metric, this yields a competitive performance with, or even better than, other pre-trained and intrusive measures. These results show that LQR is a promising basis for more sophisticated speech quality measures.",
      "authors": [
        "Mhd Modar Halimeh",
        "Matteo Torcoli",
        "Philipp Grundhuber",
        "Emanuël A. P. Habets"
      ],
      "published": "2025-03-05T09:37:14Z",
      "updated": "2025-03-05T09:37:14Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.03304v1",
      "landing_url": "https://arxiv.org/abs/2503.03304v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.03304"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Not about Spoken Language Models (SLMs) or Full-Duplex SLMs; focuses on neural codecs and speech-quality metrics rather than speech modeling or interactive dialogue, thus it does not meet inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Not about Spoken Language Models (SLMs) or Full-Duplex SLMs; focuses on neural codecs and speech-quality metrics rather than speech modeling or interactive dialogue, thus it does not meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on analyzing the relation between latent representations of neural codecs and speech quality, emphasizing speech signal modeling and quality metrics rather than Spoken Language Models or full-duplex spoken interaction systems. It does not address model architectures for spoken language models, training or evaluation of end-to-end or hybrid speech-language models, or interaction paradigms like cognitive parallelism or low-latency full-duplex communication. Therefore, it does not meet the core inclusion criterion of engaging with spoken language modeling or full-duplex interaction, and it primarily centers on speech signal quality assessment instead of modeling or interactive dialogue behaviors.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on analyzing the relation between latent representations of neural codecs and speech quality, emphasizing speech signal modeling and quality metrics rather than Spoken Language Models or full-duplex spoken interaction systems. It does not address model architectures for spoken language models, training or evaluation of end-to-end or hybrid speech-language models, or interaction paradigms like cognitive parallelism or low-latency full-duplex communication. Therefore, it does not meet the core inclusion criterion of engaging with spoken language modeling or full-duplex interaction, and it primarily centers on speech signal quality assessment instead of modeling or interactive dialogue behaviors.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Enhancing Spoken Discourse Modeling in Language Models Using Gestural Cues",
    "abstract": "Research in linguistics shows that non-verbal cues, such as gestures, play a crucial role in spoken discourse. For example, speakers perform hand gestures to indicate topic shifts, helping listeners identify transitions in discourse. In this work, we investigate whether the joint modeling of gestures using human motion sequences and language can improve spoken discourse modeling in language models. To integrate gestures into language models, we first encode 3D human motion sequences into discrete gesture tokens using a VQ-VAE. These gesture token embeddings are then aligned with text embeddings through feature alignment, mapping them into the text embedding space. To evaluate the gesture-aligned language model on spoken discourse, we construct text infilling tasks targeting three key discourse cues grounded in linguistic research: discourse connectives, stance markers, and quantifiers. Results show that incorporating gestures enhances marker prediction accuracy across the three tasks, highlighting the complementary information that gestures can offer in modeling spoken discourse. We view this work as an initial step toward leveraging non-verbal cues to advance spoken language modeling in language models.",
    "metadata": {
      "arxiv_id": "2503.03474",
      "title": "Enhancing Spoken Discourse Modeling in Language Models Using Gestural Cues",
      "summary": "Research in linguistics shows that non-verbal cues, such as gestures, play a crucial role in spoken discourse. For example, speakers perform hand gestures to indicate topic shifts, helping listeners identify transitions in discourse. In this work, we investigate whether the joint modeling of gestures using human motion sequences and language can improve spoken discourse modeling in language models. To integrate gestures into language models, we first encode 3D human motion sequences into discrete gesture tokens using a VQ-VAE. These gesture token embeddings are then aligned with text embeddings through feature alignment, mapping them into the text embedding space. To evaluate the gesture-aligned language model on spoken discourse, we construct text infilling tasks targeting three key discourse cues grounded in linguistic research: discourse connectives, stance markers, and quantifiers. Results show that incorporating gestures enhances marker prediction accuracy across the three tasks, highlighting the complementary information that gestures can offer in modeling spoken discourse. We view this work as an initial step toward leveraging non-verbal cues to advance spoken language modeling in language models.",
      "authors": [
        "Varsha Suresh",
        "M. Hamza Mughal",
        "Christian Theobalt",
        "Vera Demberg"
      ],
      "published": "2025-03-05T13:10:07Z",
      "updated": "2025-03-05T13:10:07Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.03474v1",
      "landing_url": "https://arxiv.org/abs/2503.03474v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.03474"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item explicitly centers on Spoken Language Models (multimodal with gestural cues) and discusses architecture, training, and evaluation; it is within the last 3 years and not limited to ASR/TTS or single-turn interactions, thus meeting inclusion criteria and not triggering exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item explicitly centers on Spoken Language Models (multimodal with gestural cues) and discusses architecture, training, and evaluation; it is within the last 3 years and not limited to ASR/TTS or single-turn interactions, thus meeting inclusion criteria and not triggering exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on integrating gestural cues with language models to improve spoken discourse modeling but does not explicitly treat speech or acoustic signals as core modeling or reasoning objects, nor does it explicitly address spoken language models, full-duplex interaction, speech modeling architecture, or evaluation aligned with the specified spoken language modeling or full-duplex criteria. The work is primarily concerned with text and gesture alignment rather than handling actual speech sequences or speech-based modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on integrating gestural cues with language models to improve spoken discourse modeling but does not explicitly treat speech or acoustic signals as core modeling or reasoning objects, nor does it explicitly address spoken language models, full-duplex interaction, speech modeling architecture, or evaluation aligned with the specified spoken language modeling or full-duplex criteria. The work is primarily concerned with text and gesture alignment rather than handling actual speech sequences or speech-based modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed, but on inspection the paper centers on encoding 3D gestures and aligning them with text embeddings to improve marker prediction in text infilling tasks — it does not treat speech/audio as the core modeling object, nor does it mention spoken language models, full‑duplex interaction, or speech‑level modeling/evaluation; therefore it fails the inclusion criteria. (One‑sentence explanation: The work is multimodal text+gesture research and not a Spoken Language Model or speech‑centric study, so it should be excluded.)",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed, but on inspection the paper centers on encoding 3D gestures and aligning them with text embeddings to improve marker prediction in text infilling tasks — it does not treat speech/audio as the core modeling object, nor does it mention spoken language models, full‑duplex interaction, or speech‑level modeling/evaluation; therefore it fails the inclusion criteria. (One‑sentence explanation: The work is multimodal text+gesture research and not a Spoken Language Model or speech‑centric study, so it should be excluded.)",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities",
    "abstract": "Spoken dialogue modeling poses challenges beyond text-based language modeling, requiring real-time interaction, turn-taking, and backchanneling. While most Spoken Dialogue Models (SDMs) operate in half-duplex mode-processing one turn at a time - emerging full-duplex SDMs can listen and speak simultaneously, enabling more natural conversations. However, current evaluations remain limited, focusing mainly on turn-based metrics or coarse corpus-level analyses. To address this, we introduce Full-Duplex-Bench, a benchmark that systematically evaluates key interactive behaviors: pause handling, backchanneling, turn-taking, and interruption management. Our framework uses automatic metrics for consistent, reproducible assessment and provides a fair, fast evaluation setup. By releasing our benchmark and code, we aim to advance spoken dialogue modeling and foster the development of more natural and engaging SDMs.",
    "metadata": {
      "arxiv_id": "2503.04721",
      "title": "Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities",
      "summary": "Spoken dialogue modeling poses challenges beyond text-based language modeling, requiring real-time interaction, turn-taking, and backchanneling. While most Spoken Dialogue Models (SDMs) operate in half-duplex mode-processing one turn at a time - emerging full-duplex SDMs can listen and speak simultaneously, enabling more natural conversations. However, current evaluations remain limited, focusing mainly on turn-based metrics or coarse corpus-level analyses. To address this, we introduce Full-Duplex-Bench, a benchmark that systematically evaluates key interactive behaviors: pause handling, backchanneling, turn-taking, and interruption management. Our framework uses automatic metrics for consistent, reproducible assessment and provides a fair, fast evaluation setup. By releasing our benchmark and code, we aim to advance spoken dialogue modeling and foster the development of more natural and engaging SDMs.",
      "authors": [
        "Guan-Ting Lin",
        "Jiachen Lian",
        "Tingle Li",
        "Qirui Wang",
        "Gopala Anumanchipalli",
        "Alexander H. Liu",
        "Hung-yi Lee"
      ],
      "published": "2025-03-06T18:59:16Z",
      "updated": "2025-08-16T05:46:19Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.04721v3",
      "landing_url": "https://arxiv.org/abs/2503.04721v3",
      "doi": "https://doi.org/10.48550/arXiv.2503.04721"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on Spoken Language Models with full-duplex dialogue, evaluates turn-taking and interactive behaviors, and proposes a benchmark with architecture/evaluation aspects, published within the last 3 years, aligning with the inclusion criteria and not triggering the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This item centers on Spoken Language Models with full-duplex dialogue, evaluates turn-taking and interactive behaviors, and proposes a benchmark with architecture/evaluation aspects, published within the last 3 years, aligning with the inclusion criteria and not triggering the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract clearly focus on Full-Duplex Spoken Dialogue Models, emphasizing turn-taking, backchanneling, and interruption management with an automatic, reproducible benchmark evaluation, aligning well with spoken language modeling as a core and highlighting synchronous real-time interactions beyond half-duplex systems; the study also involves clear evaluation metrics and benchmark development, fitting the inclusion criteria and excluding none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The title and abstract clearly focus on Full-Duplex Spoken Dialogue Models, emphasizing turn-taking, backchanneling, and interruption management with an automatic, reproducible benchmark evaluation, aligning well with spoken language modeling as a core and highlighting synchronous real-time interactions beyond half-duplex systems; the study also involves clear evaluation metrics and benchmark development, fitting the inclusion criteria and excluding none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Analyzing the temporal dynamics of linguistic features contained in misinformation",
    "abstract": "Consumption of misinformation can lead to negative consequences that impact the individual and society. To help mitigate the influence of misinformation on human beliefs, algorithmic labels providing context about content accuracy and source reliability have been developed. Since the linguistic features used by algorithms to estimate information accuracy can change across time, it is important to understand their temporal dynamics. As a result, this study uses natural language processing to analyze PolitiFact statements spanning between 2010 and 2024 to quantify how the sources and linguistic features of misinformation change between five-year time periods. The results show that statement sentiment has decreased significantly over time, reflecting a generally more negative tone in PolitiFact statements. Moreover, statements associated with misinformation realize significantly lower sentiment than accurate information. Additional analysis shows that recent time periods are dominated by sources from online social networks and other digital forums, such as blogs and viral images, that contain high levels of misinformation containing negative sentiment. In contrast, most statements during early time periods are attributed to individual sources (i.e., politicians) that are relatively balanced in accuracy ratings and contain statements with neutral or positive sentiment. Named-entity recognition was used to identify that presidential incumbents and candidates are relatively more prevalent in statements containing misinformation, while US states tend to be present in accurate information. Finally, entity labels associated with people and organizations are more common in misinformation, while accurate statements are more likely to contain numeric entity labels, such as percentages and dates.",
    "metadata": {
      "arxiv_id": "2503.04786",
      "title": "Analyzing the temporal dynamics of linguistic features contained in misinformation",
      "summary": "Consumption of misinformation can lead to negative consequences that impact the individual and society. To help mitigate the influence of misinformation on human beliefs, algorithmic labels providing context about content accuracy and source reliability have been developed. Since the linguistic features used by algorithms to estimate information accuracy can change across time, it is important to understand their temporal dynamics. As a result, this study uses natural language processing to analyze PolitiFact statements spanning between 2010 and 2024 to quantify how the sources and linguistic features of misinformation change between five-year time periods. The results show that statement sentiment has decreased significantly over time, reflecting a generally more negative tone in PolitiFact statements. Moreover, statements associated with misinformation realize significantly lower sentiment than accurate information. Additional analysis shows that recent time periods are dominated by sources from online social networks and other digital forums, such as blogs and viral images, that contain high levels of misinformation containing negative sentiment. In contrast, most statements during early time periods are attributed to individual sources (i.e., politicians) that are relatively balanced in accuracy ratings and contain statements with neutral or positive sentiment. Named-entity recognition was used to identify that presidential incumbents and candidates are relatively more prevalent in statements containing misinformation, while US states tend to be present in accurate information. Finally, entity labels associated with people and organizations are more common in misinformation, while accurate statements are more likely to contain numeric entity labels, such as percentages and dates.",
      "authors": [
        "Erik J Schlicht"
      ],
      "published": "2025-02-27T14:15:43Z",
      "updated": "2025-03-10T01:43:38Z",
      "categories": [
        "cs.CL",
        "cs.SI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.04786v2",
      "landing_url": "https://arxiv.org/abs/2503.04786v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.04786"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article analyzes linguistic features in misinformation using text-based NLP and does not center on Spoken Language Models or speech-based modeling as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article analyzes linguistic features in misinformation using text-based NLP and does not center on Spoken Language Models or speech-based modeling as required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on analyzing linguistic features in misinformation using natural language processing over a timeline and does not address Spoken Language Models or speech-based modeling, training, or evaluation. There is no indication of speech-level language modeling, full-duplex spoken interactive capabilities, or relevant evaluation metrics, and it primarily works with text-based content from PolitiFact statements. Therefore, it does not meet the inclusion criteria focused on Spoken Language Models and speech as the core modeling object.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on analyzing linguistic features in misinformation using natural language processing over a timeline and does not address Spoken Language Models or speech-based modeling, training, or evaluation. There is no indication of speech-level language modeling, full-duplex spoken interactive capabilities, or relevant evaluation metrics, and it primarily works with text-based content from PolitiFact statements. Therefore, it does not meet the inclusion criteria focused on Spoken Language Models and speech as the core modeling object.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "FALCON: A Framework for Fault Prediction in Open RAN Using Multi-Level Telemetry",
    "abstract": "O-RAN has brought in deployment flexibility and intelligent RAN control for mobile operators through its disaggregated and modular architecture using open interfaces. However, this disaggregation introduces complexities in system integration and network management, as components are often sourced from different vendors. In addition, the operators who are relying on open source and virtualized components -- which are deployed on commodity hardware -- require additional resilient solutions as O-RAN deployments suffer from the risk of failures at multiple levels including infrastructure, platform, and RAN levels. To address these challenges, this paper proposes FALCON, a fault prediction framework for O-RAN, which leverages infrastructure-, platform-, and RAN-level telemetry to predict faults in virtualized O-RAN deployments. By aggregating and analyzing metrics from various components at different levels using AI/ML models, the FALCON framework enables proactive fault management, providing operators with actionable insights to implement timely preventive measures. The FALCON framework, using a Random Forest classifier, outperforms two other classifiers on the predicted telemetry, achieving an average accuracy and F1-score of more than 98%.",
    "metadata": {
      "arxiv_id": "2503.06197",
      "title": "FALCON: A Framework for Fault Prediction in Open RAN Using Multi-Level Telemetry",
      "summary": "O-RAN has brought in deployment flexibility and intelligent RAN control for mobile operators through its disaggregated and modular architecture using open interfaces. However, this disaggregation introduces complexities in system integration and network management, as components are often sourced from different vendors. In addition, the operators who are relying on open source and virtualized components -- which are deployed on commodity hardware -- require additional resilient solutions as O-RAN deployments suffer from the risk of failures at multiple levels including infrastructure, platform, and RAN levels. To address these challenges, this paper proposes FALCON, a fault prediction framework for O-RAN, which leverages infrastructure-, platform-, and RAN-level telemetry to predict faults in virtualized O-RAN deployments. By aggregating and analyzing metrics from various components at different levels using AI/ML models, the FALCON framework enables proactive fault management, providing operators with actionable insights to implement timely preventive measures. The FALCON framework, using a Random Forest classifier, outperforms two other classifiers on the predicted telemetry, achieving an average accuracy and F1-score of more than 98%.",
      "authors": [
        "Yaswanth Kumar LS",
        "Somya Jain",
        "Bheemarjuna Reddy Tamma",
        "Koteswararao Kondepu"
      ],
      "published": "2025-03-08T12:40:55Z",
      "updated": "2025-03-08T12:40:55Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.06197v1",
      "landing_url": "https://arxiv.org/abs/2503.06197v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.06197"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目並未以 Spoken Language Models 或等價概念為核心，焦點在 O-RAN 之故障預測與跨層遙測資料，未討論語音層級建模、語音互動或語音評估，因此不符合納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目並未以 Spoken Language Models 或等價概念為核心，焦點在 O-RAN 之故障預測與跨層遙測資料，未討論語音層級建模、語音互動或語音評估，因此不符合納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a fault prediction framework for Open RAN, which is related to telecommunications infrastructure and AI/ML for fault management, but it does not address Spoken Language Models or speech-based language modeling architectures, training, or evaluation as required by the inclusion criteria. The topic is unrelated to speech modeling, full-duplex spoken interaction, or any of the defined key concepts. Hence, it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a fault prediction framework for Open RAN, which is related to telecommunications infrastructure and AI/ML for fault management, but it does not address Spoken Language Models or speech-based language modeling architectures, training, or evaluation as required by the inclusion criteria. The topic is unrelated to speech modeling, full-duplex spoken interaction, or any of the defined key concepts. Hence, it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Noise-Robust Turn-Taking System for Real-World Dialogue Robots: A Field Experiment",
    "abstract": "Turn-taking is a crucial aspect of human-robot interaction, directly influencing conversational fluidity and user engagement. While previous research has explored turn-taking models in controlled environments, their robustness in real-world settings remains underexplored. In this study, we propose a noise-robust voice activity projection (VAP) model, based on a Transformer architecture, to enhance real-time turn-taking in dialogue robots. To evaluate the effectiveness of the proposed system, we conducted a field experiment in a shopping mall, comparing the VAP system with a conventional cloud-based speech recognition system. Our analysis covered both subjective user evaluations and objective behavioral analysis. The results showed that the proposed system significantly reduced response latency, leading to a more natural conversation where both the robot and users responded faster. The subjective evaluations suggested that faster responses contribute to a better interaction experience.",
    "metadata": {
      "arxiv_id": "2503.06241",
      "title": "A Noise-Robust Turn-Taking System for Real-World Dialogue Robots: A Field Experiment",
      "summary": "Turn-taking is a crucial aspect of human-robot interaction, directly influencing conversational fluidity and user engagement. While previous research has explored turn-taking models in controlled environments, their robustness in real-world settings remains underexplored. In this study, we propose a noise-robust voice activity projection (VAP) model, based on a Transformer architecture, to enhance real-time turn-taking in dialogue robots. To evaluate the effectiveness of the proposed system, we conducted a field experiment in a shopping mall, comparing the VAP system with a conventional cloud-based speech recognition system. Our analysis covered both subjective user evaluations and objective behavioral analysis. The results showed that the proposed system significantly reduced response latency, leading to a more natural conversation where both the robot and users responded faster. The subjective evaluations suggested that faster responses contribute to a better interaction experience.",
      "authors": [
        "Koji Inoue",
        "Yuki Okafuji",
        "Jun Baba",
        "Yoshiki Ohira",
        "Katsuya Hyodo",
        "Tatsuya Kawahara"
      ],
      "published": "2025-03-08T14:53:20Z",
      "updated": "2025-07-14T11:04:50Z",
      "categories": [
        "cs.RO",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.06241v2",
      "landing_url": "https://arxiv.org/abs/2503.06241v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.06241"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on a noise-robust turn-taking system for dialogue robots and compares against an ASR baseline, but does not clearly present Spoken Language Models or full-duplex SLM architecture, training, or evaluation, thus not meeting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on a noise-robust turn-taking system for dialogue robots and compares against an ASR baseline, but does not clearly present Spoken Language Models or full-duplex SLM architecture, training, or evaluation, thus not meeting the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a noise-robust turn-taking system for dialogue robots evaluated in a real-world setting, which addresses real-time interaction and latency improvement, but it does not explicitly mention spoken language models, speech language modeling, or architectural/model training details related to speech language modeling as required by the inclusion criteria; thus, it appears to focus more on engineering a system for turn-taking rather than on SLM or FD-SLM research per se.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a noise-robust turn-taking system for dialogue robots evaluated in a real-world setting, which addresses real-time interaction and latency improvement, but it does not explicitly mention spoken language models, speech language modeling, or architectural/model training details related to speech language modeling as required by the inclusion criteria; thus, it appears to focus more on engineering a system for turn-taking rather than on SLM or FD-SLM research per se.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Global Position Aware Group Choreography using Large Language Model",
    "abstract": "Dance serves as a profound and universal expression of human culture, conveying emotions and stories through movements synchronized with music. Although some current works have achieved satisfactory results in the task of single-person dance generation, the field of multi-person dance generation remains relatively novel. In this work, we present a group choreography framework that leverages recent advancements in Large Language Models (LLM) by modeling the group dance generation problem as a sequence-to-sequence translation task. Our framework consists of a tokenizer that transforms continuous features into discrete tokens, and an LLM that is fine-tuned to predict motion tokens given the audio tokens. We show that by proper tokenization of input modalities and careful design of the LLM training strategies, our framework can generate realistic and diverse group dances while maintaining strong music correlation and dancer-wise consistency. Extensive experiments and evaluations demonstrate that our framework achieves state-of-the-art performance.",
    "metadata": {
      "arxiv_id": "2503.09645",
      "title": "Global Position Aware Group Choreography using Large Language Model",
      "summary": "Dance serves as a profound and universal expression of human culture, conveying emotions and stories through movements synchronized with music. Although some current works have achieved satisfactory results in the task of single-person dance generation, the field of multi-person dance generation remains relatively novel. In this work, we present a group choreography framework that leverages recent advancements in Large Language Models (LLM) by modeling the group dance generation problem as a sequence-to-sequence translation task. Our framework consists of a tokenizer that transforms continuous features into discrete tokens, and an LLM that is fine-tuned to predict motion tokens given the audio tokens. We show that by proper tokenization of input modalities and careful design of the LLM training strategies, our framework can generate realistic and diverse group dances while maintaining strong music correlation and dancer-wise consistency. Extensive experiments and evaluations demonstrate that our framework achieves state-of-the-art performance.",
      "authors": [
        "Haozhou Pang",
        "Tianwei Ding",
        "Lanshan He",
        "Qi Gan"
      ],
      "published": "2025-03-12T07:25:32Z",
      "updated": "2025-03-12T07:25:32Z",
      "categories": [
        "cs.GR",
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.09645v1",
      "landing_url": "https://arxiv.org/abs/2503.09645v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.09645"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on group choreography generation using LLMs with audio tokens to predict motion tokens, which is a multimodal generation task centered on music/motion, not on Spoken Language Models or speech as the core modeling object. It lacks emphasis on speech-based language modeling, turn-taking, real-time interaction, or evaluation frameworks typical of SLM/FD-SLM research, and thus does not meet the inclusion criteria (last 3 years, SLM core) and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on group choreography generation using LLMs with audio tokens to predict motion tokens, which is a multimodal generation task centered on music/motion, not on Spoken Language Models or speech as the core modeling object. It lacks emphasis on speech-based language modeling, turn-taking, real-time interaction, or evaluation frameworks typical of SLM/FD-SLM research, and thus does not meet the inclusion criteria (last 3 years, SLM core) and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on group choreography generation using Large Language Models to generate dance movements from audio tokens, which does not engage with spoken language models or speech as the primary modeling object, nor does it address full-duplex spoken language interactions or related evaluation metrics; it instead centers on multi-person dance generation, thus not meeting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on group choreography generation using Large Language Models to generate dance movements from audio tokens, which does not engage with spoken language models or speech as the primary modeling object, nor does it address full-duplex spoken language interactions or related evaluation metrics; it instead centers on multi-person dance generation, thus not meeting the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Speech-to-Video Synthesis Approach Using Spatio-Temporal Diffusion for Vocal Tract MRI",
    "abstract": "Understanding the relationship between vocal tract motion during speech and the resulting acoustic signal is crucial for aided clinical assessment and developing personalized treatment and rehabilitation strategies. Toward this goal, we introduce an audio-to-video generation framework for creating Real Time/cine-Magnetic Resonance Imaging (RT-/cine-MRI) visuals of the vocal tract from speech signals. Our framework first preprocesses RT-/cine-MRI sequences and speech samples to achieve temporal alignment, ensuring synchronization between visual and audio data. We then employ a modified stable diffusion model, integrating structural and temporal blocks, to effectively capture movement characteristics and temporal dynamics in the synchronized data. This process enables the generation of MRI sequences from new speech inputs, improving the conversion of audio into visual data. We evaluated our framework on healthy controls and tongue cancer patients by analyzing and comparing the vocal tract movements in synthesized videos. Our framework demonstrated adaptability to new speech inputs and effective generalization. In addition, positive human evaluations confirmed its effectiveness, with realistic and accurate visualizations, suggesting its potential for outpatient therapy and personalized simulation of vocal tract visualizations.",
    "metadata": {
      "arxiv_id": "2503.12102",
      "title": "A Speech-to-Video Synthesis Approach Using Spatio-Temporal Diffusion for Vocal Tract MRI",
      "summary": "Understanding the relationship between vocal tract motion during speech and the resulting acoustic signal is crucial for aided clinical assessment and developing personalized treatment and rehabilitation strategies. Toward this goal, we introduce an audio-to-video generation framework for creating Real Time/cine-Magnetic Resonance Imaging (RT-/cine-MRI) visuals of the vocal tract from speech signals. Our framework first preprocesses RT-/cine-MRI sequences and speech samples to achieve temporal alignment, ensuring synchronization between visual and audio data. We then employ a modified stable diffusion model, integrating structural and temporal blocks, to effectively capture movement characteristics and temporal dynamics in the synchronized data. This process enables the generation of MRI sequences from new speech inputs, improving the conversion of audio into visual data. We evaluated our framework on healthy controls and tongue cancer patients by analyzing and comparing the vocal tract movements in synthesized videos. Our framework demonstrated adaptability to new speech inputs and effective generalization. In addition, positive human evaluations confirmed its effectiveness, with realistic and accurate visualizations, suggesting its potential for outpatient therapy and personalized simulation of vocal tract visualizations.",
      "authors": [
        "Paula Andrea Pérez-Toro",
        "Tomás Arias-Vergara",
        "Fangxu Xing",
        "Xiaofeng Liu",
        "Maureen Stone",
        "Jiachen Zhuo",
        "Juan Rafael Orozco-Arroyave",
        "Elmar Nöth",
        "Jana Hutter",
        "Jerry L. Prince",
        "Andreas Maier",
        "Jonghye Woo"
      ],
      "published": "2025-03-15T12:12:50Z",
      "updated": "2025-03-15T12:12:50Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.12102v1",
      "landing_url": "https://arxiv.org/abs/2503.12102v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.12102"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item targets audio-to-video generation of vocal-tract MRI visuals, a cross-modal imaging task, not core Spoken Language Models or speech-language modeling (no emphasis on SLM/FD-SLM architectures, training, or evaluation frameworks for language modeling of speech). It appears within 3 years but fails the inclusion criterion that the core focus be on SLMs or equivalent concepts; thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item targets audio-to-video generation of vocal-tract MRI visuals, a cross-modal imaging task, not core Spoken Language Models or speech-language modeling (no emphasis on SLM/FD-SLM architectures, training, or evaluation frameworks for language modeling of speech). It appears within 3 years but fails the inclusion criterion that the core focus be on SLMs or equivalent concepts; thus it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on synthesizing vocal tract MRI videos from speech signals using a diffusion model, which is about mapping audio to visual representations rather than modeling spoken language itself; it lacks discussion of spoken language models, end-to-end speech language modeling, or full-duplex interactive speech capabilities. The research does not address spoken language model architectures, training, or synchronous interactive behavior arbitration as required by the inclusion criteria, and instead serves clinical assessment and simulation purposes.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on synthesizing vocal tract MRI videos from speech signals using a diffusion model, which is about mapping audio to visual representations rather than modeling spoken language itself; it lacks discussion of spoken language models, end-to-end speech language modeling, or full-duplex interactive speech capabilities. The research does not address spoken language model architectures, training, or synchronous interactive behavior arbitration as required by the inclusion criteria, and instead serves clinical assessment and simulation purposes.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations",
    "abstract": "Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.",
    "metadata": {
      "arxiv_id": "2503.12115",
      "title": "Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations",
      "summary": "Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.",
      "authors": [
        "Xue Jiang",
        "Xiulian Peng",
        "Yuan Zhang",
        "Yan Lu"
      ],
      "published": "2025-03-15T12:50:43Z",
      "updated": "2025-10-15T06:52:30Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.12115v2",
      "landing_url": "https://arxiv.org/abs/2503.12115v2",
      "doi": "https://doi.org/10.1109/JSTSP.2024.3488557"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on universal speech token learning for speech language models, discusses architecture/training (neural codec, self-supervised features) and multilingual evaluation, and is from 2025, aligning with the SLM/FD-SLM core and evaluation scope, while not being limited to ASR/TTS or half-duplex dialogue, so it should be included (though it may not address real-time synchronization in depth).",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on universal speech token learning for speech language models, discusses architecture/training (neural codec, self-supervised features) and multilingual evaluation, and is from 2025, aligning with the SLM/FD-SLM core and evaluation scope, while not being limited to ASR/TTS or half-duplex dialogue, so it should be included (though it may not address real-time synchronization in depth).",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a universal speech token learning method integrating semantic and paralinguistic information, focusing on token representations for speech language models, but it does not explicitly mention 'spoken language model(s)' or 'speech language model(s)' nor does it address full-duplex interaction, multi-turn synchronization, or behavior arbitration; it also lacks explicit evaluation on interaction-related metrics (e.g., low-latency, overlapping speech handling). Hence, it partially touches on speech language modeling but misses key inclusion criteria and possibly some exclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes a universal speech token learning method integrating semantic and paralinguistic information, focusing on token representations for speech language models, but it does not explicitly mention 'spoken language model(s)' or 'speech language model(s)' nor does it address full-duplex interaction, multi-turn synchronization, or behavior arbitration; it also lacks explicit evaluation on interaction-related metrics (e.g., low-latency, overlapping speech handling). Hence, it partially touches on speech language modeling but misses key inclusion criteria and possibly some exclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "The abstract explicitly uses the term \"speech language models\" and proposes a unified token learning method (UniCodec) that targets representations for speech language models, describing model components (low-bitrate neural codec, distillation from self-supervised features), training/objectives, and multilingual evaluations—so it clearly centers on spoken/speech language modeling rather than being merely ASR/TTS preprocessing; however, it does not address full-duplex interaction, overlap/turn-taking, latency, or behavior-arbitration concerns. Junior reviewers were split (one recommended inclusion, one exclusion); after synthesis, I judge it satisfies the core inclusion requirement of being an SLM-focused modeling contribution though it lacks FD-SLM interaction specifics. One-sentence explanation: Include (4) — it explicitly targets speech language models with architectural/training innovations and evaluations, even though it does not cover full-duplex interaction aspects.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "The abstract explicitly uses the term \"speech language models\" and proposes a unified token learning method (UniCodec) that targets representations for speech language models, describing model components (low-bitrate neural codec, distillation from self-supervised features), training/objectives, and multilingual evaluations—so it clearly centers on spoken/speech language modeling rather than being merely ASR/TTS preprocessing; however, it does not address full-duplex interaction, overlap/turn-taking, latency, or behavior-arbitration concerns. Junior reviewers were split (one recommended inclusion, one exclusion); after synthesis, I judge it satisfies the core inclusion requirement of being an SLM-focused modeling contribution though it lacks FD-SLM interaction specifics. One-sentence explanation: Include (4) — it explicitly targets speech language models with architectural/training innovations and evaluations, even though it does not cover full-duplex interaction aspects.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "RENO: Real-Time Neural Compression for 3D LiDAR Point Clouds",
    "abstract": "Despite the substantial advancements demonstrated by learning-based neural models in the LiDAR Point Cloud Compression (LPCC) task, realizing real-time compression - an indispensable criterion for numerous industrial applications - remains a formidable challenge. This paper proposes RENO, the first real-time neural codec for 3D LiDAR point clouds, achieving superior performance with a lightweight model. RENO skips the octree construction and directly builds upon the multiscale sparse tensor representation. Instead of the multi-stage inferring, RENO devises sparse occupancy codes, which exploit cross-scale correlation and derive voxels' occupancy in a one-shot manner, greatly saving processing time. Experimental results demonstrate that the proposed RENO achieves real-time coding speed, 10 fps at 14-bit depth on a desktop platform (e.g., one RTX 3090 GPU) for both encoding and decoding processes, while providing 12.25% and 48.34% bit-rate savings compared to G-PCCv23 and Draco, respectively, at a similar quality. RENO model size is merely 1MB, making it attractive for practical applications. The source code is available at https://github.com/NJUVISION/RENO.",
    "metadata": {
      "arxiv_id": "2503.12382",
      "title": "RENO: Real-Time Neural Compression for 3D LiDAR Point Clouds",
      "summary": "Despite the substantial advancements demonstrated by learning-based neural models in the LiDAR Point Cloud Compression (LPCC) task, realizing real-time compression - an indispensable criterion for numerous industrial applications - remains a formidable challenge. This paper proposes RENO, the first real-time neural codec for 3D LiDAR point clouds, achieving superior performance with a lightweight model. RENO skips the octree construction and directly builds upon the multiscale sparse tensor representation. Instead of the multi-stage inferring, RENO devises sparse occupancy codes, which exploit cross-scale correlation and derive voxels' occupancy in a one-shot manner, greatly saving processing time. Experimental results demonstrate that the proposed RENO achieves real-time coding speed, 10 fps at 14-bit depth on a desktop platform (e.g., one RTX 3090 GPU) for both encoding and decoding processes, while providing 12.25% and 48.34% bit-rate savings compared to G-PCCv23 and Draco, respectively, at a similar quality. RENO model size is merely 1MB, making it attractive for practical applications. The source code is available at https://github.com/NJUVISION/RENO.",
      "authors": [
        "Kang You",
        "Tong Chen",
        "Dandan Ding",
        "M. Salman Asif",
        "Zhan Ma"
      ],
      "published": "2025-03-16T07:03:12Z",
      "updated": "2025-03-16T07:03:12Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.12382v1",
      "landing_url": "https://arxiv.org/abs/2503.12382v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.12382"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此研究項目聚焦於 3D LiDAR 點雲的實時神經編碼壓縮，未涉及語音語言模型、語音–文字混合或全雙工語言互動等核心議題，因而不符合納入條件，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此研究項目聚焦於 3D LiDAR 點雲的實時神經編碼壓縮，未涉及語音語言模型、語音–文字混合或全雙工語言互動等核心議題，因而不符合納入條件，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on real-time neural compression for 3D LiDAR point clouds, which is a topic outside the scope of Spoken Language Models or speech-based language modeling; it neither discusses speech sequences, spoken language modeling, nor full-duplex spoken interaction concepts required by the inclusion criteria. Hence, it does not meet the inclusion criteria and is irrelevant to the specified research domain.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on real-time neural compression for 3D LiDAR point clouds, which is a topic outside the scope of Spoken Language Models or speech-based language modeling; it neither discusses speech sequences, spoken language modeling, nor full-duplex spoken interaction concepts required by the inclusion criteria. Hence, it does not meet the inclusion criteria and is irrelevant to the specified research domain.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Matching Skeleton-based Activity Representations with Heterogeneous Signals for HAR",
    "abstract": "In human activity recognition (HAR), activity labels have typically been encoded in one-hot format, which has a recent shift towards using textual representations to provide contextual knowledge. Here, we argue that HAR should be anchored to physical motion data, as motion forms the basis of activity and applies effectively across sensing systems, whereas text is inherently limited. We propose SKELAR, a novel HAR framework that pretrains activity representations from skeleton data and matches them with heterogeneous HAR signals. Our method addresses two major challenges: (1) capturing core motion knowledge without context-specific details. We achieve this through a self-supervised coarse angle reconstruction task that recovers joint rotation angles, invariant to both users and deployments; (2) adapting the representations to downstream tasks with varying modalities and focuses. To address this, we introduce a self-attention matching module that dynamically prioritizes relevant body parts in a data-driven manner. Given the lack of corresponding labels in existing skeleton data, we establish MASD, a new HAR dataset with IMU, WiFi, and skeleton, collected from 20 subjects performing 27 activities. This is the first broadly applicable HAR dataset with time-synchronized data across three modalities. Experiments show that SKELAR achieves the state-of-the-art performance in both full-shot and few-shot settings. We also demonstrate that SKELAR can effectively leverage synthetic skeleton data to extend its use in scenarios without skeleton collections.",
    "metadata": {
      "arxiv_id": "2503.14547",
      "title": "Matching Skeleton-based Activity Representations with Heterogeneous Signals for HAR",
      "summary": "In human activity recognition (HAR), activity labels have typically been encoded in one-hot format, which has a recent shift towards using textual representations to provide contextual knowledge. Here, we argue that HAR should be anchored to physical motion data, as motion forms the basis of activity and applies effectively across sensing systems, whereas text is inherently limited. We propose SKELAR, a novel HAR framework that pretrains activity representations from skeleton data and matches them with heterogeneous HAR signals. Our method addresses two major challenges: (1) capturing core motion knowledge without context-specific details. We achieve this through a self-supervised coarse angle reconstruction task that recovers joint rotation angles, invariant to both users and deployments; (2) adapting the representations to downstream tasks with varying modalities and focuses. To address this, we introduce a self-attention matching module that dynamically prioritizes relevant body parts in a data-driven manner. Given the lack of corresponding labels in existing skeleton data, we establish MASD, a new HAR dataset with IMU, WiFi, and skeleton, collected from 20 subjects performing 27 activities. This is the first broadly applicable HAR dataset with time-synchronized data across three modalities. Experiments show that SKELAR achieves the state-of-the-art performance in both full-shot and few-shot settings. We also demonstrate that SKELAR can effectively leverage synthetic skeleton data to extend its use in scenarios without skeleton collections.",
      "authors": [
        "Shuheng Li",
        "Jiayun Zhang",
        "Xiaohan Fu",
        "Xiyuan Zhang",
        "Jingbo Shang",
        "Rajesh K. Gupta"
      ],
      "published": "2025-03-17T18:43:06Z",
      "updated": "2025-03-17T18:43:06Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.14547v1",
      "landing_url": "https://arxiv.org/abs/2503.14547v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.14547"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on skeleton-based HAR and multimodal sensing for activity recognition, with no reference to Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on skeleton-based HAR and multimodal sensing for activity recognition, with no reference to Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on human activity recognition using skeleton data and heterogeneous signals, which is unrelated to spoken language models or speech language modeling, synchronization, or real-time spoken interaction. There is no mention of speech or spoken language modeling concepts, architectures, or evaluations as required by the inclusion criteria. Therefore, it does not meet the inclusion criteria relevant to spoken language modeling and full-duplex interaction.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on human activity recognition using skeleton data and heterogeneous signals, which is unrelated to spoken language models or speech language modeling, synchronization, or real-time spoken interaction. There is no mention of speech or spoken language modeling concepts, architectures, or evaluations as required by the inclusion criteria. Therefore, it does not meet the inclusion criteria relevant to spoken language modeling and full-duplex interaction.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SceneEval: Evaluating Semantic Coherence in Text-Conditioned 3D Indoor Scene Synthesis",
    "abstract": "Despite recent advances in text-conditioned 3D indoor scene generation, there remain gaps in the evaluation of these methods. Existing metrics primarily assess the realism of generated scenes by comparing them to a set of ground-truth scenes, often overlooking alignment with the input text - a critical factor in determining how effectively a method meets user requirements. We present SceneEval, an evaluation framework designed to address this limitation. SceneEval includes metrics for both explicit user requirements, such as the presence of specific objects and their attributes described in the input text, and implicit expectations, like the absence of object collisions, providing a comprehensive assessment of scene quality. To facilitate evaluation, we introduce SceneEval-500, a dataset of scene descriptions with annotated ground-truth scene properties. We evaluate recent scene generation methods using SceneEval and demonstrate its ability to provide detailed assessments of the generated scenes, highlighting strengths and areas for improvement across multiple dimensions. Our results show that current methods struggle at generating scenes that meet user requirements, underscoring the need for further research in this direction.",
    "metadata": {
      "arxiv_id": "2503.14756",
      "title": "SceneEval: Evaluating Semantic Coherence in Text-Conditioned 3D Indoor Scene Synthesis",
      "summary": "Despite recent advances in text-conditioned 3D indoor scene generation, there remain gaps in the evaluation of these methods. Existing metrics primarily assess the realism of generated scenes by comparing them to a set of ground-truth scenes, often overlooking alignment with the input text - a critical factor in determining how effectively a method meets user requirements. We present SceneEval, an evaluation framework designed to address this limitation. SceneEval includes metrics for both explicit user requirements, such as the presence of specific objects and their attributes described in the input text, and implicit expectations, like the absence of object collisions, providing a comprehensive assessment of scene quality. To facilitate evaluation, we introduce SceneEval-500, a dataset of scene descriptions with annotated ground-truth scene properties. We evaluate recent scene generation methods using SceneEval and demonstrate its ability to provide detailed assessments of the generated scenes, highlighting strengths and areas for improvement across multiple dimensions. Our results show that current methods struggle at generating scenes that meet user requirements, underscoring the need for further research in this direction.",
      "authors": [
        "Hou In Ivan Tam",
        "Hou In Derek Pun",
        "Austin T. Wang",
        "Angel X. Chang",
        "Manolis Savva"
      ],
      "published": "2025-03-18T22:02:35Z",
      "updated": "2025-06-11T06:42:00Z",
      "categories": [
        "cs.GR",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.14756v2",
      "landing_url": "https://arxiv.org/abs/2503.14756v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.14756"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work targets evaluation metrics for text-conditioned 3D scene generation and scene-description alignment, not speech-language modeling or any synchronized audio-dialogue capabilities; thus it fails all inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work targets evaluation metrics for text-conditioned 3D scene generation and scene-description alignment, not speech-language modeling or any synchronized audio-dialogue capabilities; thus it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on evaluating semantic coherence in text-conditioned 3D indoor scene synthesis, with no mention of spoken language models, speech language modeling, or related concepts such as speech as a main modeling object, full-duplex interaction, or relevant evaluation metrics tied to spoken language processing. It does not meet any inclusion criteria focused on spoken language modeling, nor does it address synchronous speech interaction or related evaluation aspects.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on evaluating semantic coherence in text-conditioned 3D indoor scene synthesis, with no mention of spoken language models, speech language modeling, or related concepts such as speech as a main modeling object, full-duplex interaction, or relevant evaluation metrics tied to spoken language processing. It does not meet any inclusion criteria focused on spoken language modeling, nor does it address synchronous speech interaction or related evaluation aspects.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Multivariate Gaussian Topic Modelling: A novel approach to discover topics with greater semantic coherence",
    "abstract": "An important aspect of text mining involves information retrieval in form of discovery of semantic themes (topics) from documents using topic modelling. While generative topic models like Latent Dirichlet Allocation (LDA) or Latent Semantic Analysis (LSA) elegantly model topics as probability distributions and are useful in identifying latent topics from large document corpora with minimal supervision, they suffer from difficulty in topic interpretability and reduced performance in shorter texts. Here we propose a novel Multivariate Gaussian Topic Model (MGTM). In this approach topics are presented as Multivariate Gaussian Distributions and documents as Gaussian Mixture Models. Applying EM algorithm on a document corpus, the various constituent Multivariate Gaussian distributions corresponding to the latent topics and their respective parameters are identified. Analysis of the parameters of each distribution helps identify the respective topic keywords, and from these key-words topic annotations are carried out. This approach is applied on 20 newsgroups dataset to demonstrate the interpretability benefits vis-`a-vis 4 other benchmark models. The effectiveness of this model in capturing the semantic theme of the topics with high interpretability is examined by calculating the topic coherence and comparing the coherence values with benchmark models. This model achieves a highest mean topic coherence (0.7) and median topic coherence (0.76) vis-`a-vis the benchmark models, demonstrating high effectiveness in identifying interpretable, semantically coherent topics.",
    "metadata": {
      "arxiv_id": "2503.15036",
      "title": "Multivariate Gaussian Topic Modelling: A novel approach to discover topics with greater semantic coherence",
      "summary": "An important aspect of text mining involves information retrieval in form of discovery of semantic themes (topics) from documents using topic modelling. While generative topic models like Latent Dirichlet Allocation (LDA) or Latent Semantic Analysis (LSA) elegantly model topics as probability distributions and are useful in identifying latent topics from large document corpora with minimal supervision, they suffer from difficulty in topic interpretability and reduced performance in shorter texts. Here we propose a novel Multivariate Gaussian Topic Model (MGTM). In this approach topics are presented as Multivariate Gaussian Distributions and documents as Gaussian Mixture Models. Applying EM algorithm on a document corpus, the various constituent Multivariate Gaussian distributions corresponding to the latent topics and their respective parameters are identified. Analysis of the parameters of each distribution helps identify the respective topic keywords, and from these key-words topic annotations are carried out. This approach is applied on 20 newsgroups dataset to demonstrate the interpretability benefits vis-`a-vis 4 other benchmark models. The effectiveness of this model in capturing the semantic theme of the topics with high interpretability is examined by calculating the topic coherence and comparing the coherence values with benchmark models. This model achieves a highest mean topic coherence (0.7) and median topic coherence (0.76) vis-`a-vis the benchmark models, demonstrating high effectiveness in identifying interpretable, semantically coherent topics.",
      "authors": [
        "Satyajeet Sahoo",
        "Jhareswar Maiti"
      ],
      "published": "2025-03-19T09:25:54Z",
      "updated": "2025-11-01T12:16:39Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.15036v2",
      "landing_url": "https://arxiv.org/abs/2503.15036v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.15036"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "該研究聚焦於文字主題建模，未以 Spoken Language Models/語音作為核心建模與推理對象，亦未討論語音相關評估或同步互動，故不符合納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "該研究聚焦於文字主題建模，未以 Spoken Language Models/語音作為核心建模與推理對象，亦未討論語音相關評估或同步互動，故不符合納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on topic modelling of text documents using a novel Multivariate Gaussian Topic Model, dealing primarily with textual semantic themes, and does not address spoken language models or speech-related modeling as required by the inclusion criteria. It neither discusses speech as a core modeling object nor addresses full-duplex spoken interaction or related evaluation metrics; thus, it fails to meet the inclusion criteria and aligns with exclusion due to its focus on NLP text modeling only.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on topic modelling of text documents using a novel Multivariate Gaussian Topic Model, dealing primarily with textual semantic themes, and does not address spoken language models or speech-related modeling as required by the inclusion criteria. It neither discusses speech as a core modeling object nor addresses full-duplex spoken interaction or related evaluation metrics; thus, it fails to meet the inclusion criteria and aligns with exclusion due to its focus on NLP text modeling only.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Blend the Separated: Mixture of Synergistic Experts for Data-Scarcity Drug-Target Interaction Prediction",
    "abstract": "Drug-target interaction prediction (DTI) is essential in various applications including drug discovery and clinical application. There are two perspectives of input data widely used in DTI prediction: Intrinsic data represents how drugs or targets are constructed, and extrinsic data represents how drugs or targets are related to other biological entities. However, any of the two perspectives of input data can be scarce for some drugs or targets, especially for those unpopular or newly discovered. Furthermore, ground-truth labels for specific interaction types can also be scarce. Therefore, we propose the first method to tackle DTI prediction under input data and/or label scarcity. To make our model functional when only one perspective of input data is available, we design two separate experts to process intrinsic and extrinsic data respectively and fuse them adaptively according to different samples. Furthermore, to make the two perspectives complement each other and remedy label scarcity, two experts synergize with each other in a mutually supervised way to exploit the enormous unlabeled data. Extensive experiments on 3 real-world datasets under different extents of input data scarcity and/or label scarcity demonstrate our model outperforms states of the art significantly and steadily, with a maximum improvement of 53.53%. We also test our model without any data scarcity and it still outperforms current methods.",
    "metadata": {
      "arxiv_id": "2503.15796",
      "title": "Blend the Separated: Mixture of Synergistic Experts for Data-Scarcity Drug-Target Interaction Prediction",
      "summary": "Drug-target interaction prediction (DTI) is essential in various applications including drug discovery and clinical application. There are two perspectives of input data widely used in DTI prediction: Intrinsic data represents how drugs or targets are constructed, and extrinsic data represents how drugs or targets are related to other biological entities. However, any of the two perspectives of input data can be scarce for some drugs or targets, especially for those unpopular or newly discovered. Furthermore, ground-truth labels for specific interaction types can also be scarce. Therefore, we propose the first method to tackle DTI prediction under input data and/or label scarcity. To make our model functional when only one perspective of input data is available, we design two separate experts to process intrinsic and extrinsic data respectively and fuse them adaptively according to different samples. Furthermore, to make the two perspectives complement each other and remedy label scarcity, two experts synergize with each other in a mutually supervised way to exploit the enormous unlabeled data. Extensive experiments on 3 real-world datasets under different extents of input data scarcity and/or label scarcity demonstrate our model outperforms states of the art significantly and steadily, with a maximum improvement of 53.53%. We also test our model without any data scarcity and it still outperforms current methods.",
      "authors": [
        "Xinlong Zhai",
        "Chunchen Wang",
        "Ruijia Wang",
        "Jiazheng Kang",
        "Shujie Li",
        "Boyu Chen",
        "Tengfei Ma",
        "Zikai Zhou",
        "Cheng Yang",
        "Chuan Shi"
      ],
      "published": "2025-03-20T02:27:16Z",
      "updated": "2025-03-20T02:27:16Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.15796v1",
      "landing_url": "https://arxiv.org/abs/2503.15796v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.15796"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本研究聚焦於資料稀疏情況下的藥物-靶點相互作用預測，與語音語言模型/語音建模無關，亦未涉及語音互動、同步或語音/語言建模的評估，因此不符合包含條件且符合排除條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本研究聚焦於資料稀疏情況下的藥物-靶點相互作用預測，與語音語言模型/語音建模無關，亦未涉及語音互動、同步或語音/語言建模的評估，因此不符合包含條件且符合排除條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on drug-target interaction prediction using data-driven model approaches, which is unrelated to spoken language models, speech modeling, or full-duplex spoken language interaction as required by the inclusion criteria. The abstract does not mention speech, spoken language modeling, turn-taking, or synchronization aspects, and the methodology focuses on bioinformatics data scarcity issues rather than speech or language modeling frameworks. Thus, it does not meet any of the inclusion criteria and falls outside the research domain of interest.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on drug-target interaction prediction using data-driven model approaches, which is unrelated to spoken language models, speech modeling, or full-duplex spoken language interaction as required by the inclusion criteria. The abstract does not mention speech, spoken language modeling, turn-taking, or synchronization aspects, and the methodology focuses on bioinformatics data scarcity issues rather than speech or language modeling frameworks. Thus, it does not meet any of the inclusion criteria and falls outside the research domain of interest.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "STOP: Integrated Spatial-Temporal Dynamic Prompting for Video Understanding",
    "abstract": "Pre-trained on tremendous image-text pairs, vision-language models like CLIP have demonstrated promising zero-shot generalization across numerous image-based tasks. However, extending these capabilities to video tasks remains challenging due to limited labeled video data and high training costs. Recent video prompting methods attempt to adapt CLIP for video tasks by introducing learnable prompts, but they typically rely on a single static prompt for all video sequences, overlooking the diverse temporal dynamics and spatial variations that exist across frames. This limitation significantly hinders the model's ability to capture essential temporal information for effective video understanding. To address this, we propose an integrated Spatial-TempOral dynamic Prompting (STOP) model which consists of two complementary modules, the intra-frame spatial prompting and inter-frame temporal prompting. Our intra-frame spatial prompts are designed to adaptively highlight discriminative regions within each frame by leveraging intra-frame attention and temporal variation, allowing the model to focus on areas with substantial temporal dynamics and capture fine-grained spatial details. Additionally, to highlight the varying importance of frames for video understanding, we further introduce inter-frame temporal prompts, dynamically inserting prompts between frames with high temporal variance as measured by frame similarity. This enables the model to prioritize key frames and enhances its capacity to understand temporal dependencies across sequences. Extensive experiments on various video benchmarks demonstrate that STOP consistently achieves superior performance against state-of-the-art methods. The code is available at https://github.com/zhoujiahuan1991/CVPR2025-STOP.",
    "metadata": {
      "arxiv_id": "2503.15973",
      "title": "STOP: Integrated Spatial-Temporal Dynamic Prompting for Video Understanding",
      "summary": "Pre-trained on tremendous image-text pairs, vision-language models like CLIP have demonstrated promising zero-shot generalization across numerous image-based tasks. However, extending these capabilities to video tasks remains challenging due to limited labeled video data and high training costs. Recent video prompting methods attempt to adapt CLIP for video tasks by introducing learnable prompts, but they typically rely on a single static prompt for all video sequences, overlooking the diverse temporal dynamics and spatial variations that exist across frames. This limitation significantly hinders the model's ability to capture essential temporal information for effective video understanding. To address this, we propose an integrated Spatial-TempOral dynamic Prompting (STOP) model which consists of two complementary modules, the intra-frame spatial prompting and inter-frame temporal prompting. Our intra-frame spatial prompts are designed to adaptively highlight discriminative regions within each frame by leveraging intra-frame attention and temporal variation, allowing the model to focus on areas with substantial temporal dynamics and capture fine-grained spatial details. Additionally, to highlight the varying importance of frames for video understanding, we further introduce inter-frame temporal prompts, dynamically inserting prompts between frames with high temporal variance as measured by frame similarity. This enables the model to prioritize key frames and enhances its capacity to understand temporal dependencies across sequences. Extensive experiments on various video benchmarks demonstrate that STOP consistently achieves superior performance against state-of-the-art methods. The code is available at https://github.com/zhoujiahuan1991/CVPR2025-STOP.",
      "authors": [
        "Zichen Liu",
        "Kunlun Xu",
        "Bing Su",
        "Xu Zou",
        "Yuxin Peng",
        "Jiahuan Zhou"
      ],
      "published": "2025-03-20T09:16:20Z",
      "updated": "2025-03-25T03:05:36Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.15973v2",
      "landing_url": "https://arxiv.org/abs/2503.15973v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.15973"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on vision-language video understanding with dynamic prompts and does not center on Spoken Language Models or speech-language modeling, thus failing the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on vision-language video understanding with dynamic prompts and does not center on Spoken Language Models or speech-language modeling, thus failing the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on video understanding using vision-language models and dynamic prompting mechanisms, without addressing spoken language models, speech as a primary modeling object, or aspects like full-duplex spoken interaction, cognitive parallelism, or speech-language modeling frameworks. It also lacks discussion on synchronization, real-time interaction, or evaluation metrics relevant to spoken language models; therefore, it does not meet the inclusion criteria which focus on speech-centric language modeling and interaction capabilities.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on video understanding using vision-language models and dynamic prompting mechanisms, without addressing spoken language models, speech as a primary modeling object, or aspects like full-duplex spoken interaction, cognitive parallelism, or speech-language modeling frameworks. It also lacks discussion on synchronization, real-time interaction, or evaluation metrics relevant to spoken language models; therefore, it does not meet the inclusion criteria which focus on speech-centric language modeling and interaction capabilities.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Bridging Continuous and Discrete Tokens for Autoregressive Visual Generation",
    "abstract": "Autoregressive visual generation models typically rely on tokenizers to compress images into tokens that can be predicted sequentially. A fundamental dilemma exists in token representation: discrete tokens enable straightforward modeling with standard cross-entropy loss, but suffer from information loss and tokenizer training instability; continuous tokens better preserve visual details, but require complex distribution modeling, complicating the generation pipeline. In this paper, we propose TokenBridge, which bridges this gap by maintaining the strong representation capacity of continuous tokens while preserving the modeling simplicity of discrete tokens. To achieve this, we decouple discretization from the tokenizer training process through post-training quantization that directly obtains discrete tokens from continuous representations. Specifically, we introduce a dimension-wise quantization strategy that independently discretizes each feature dimension, paired with a lightweight autoregressive prediction mechanism that efficiently model the resulting large token space. Extensive experiments show that our approach achieves reconstruction and generation quality on par with continuous methods while using standard categorical prediction. This work demonstrates that bridging discrete and continuous paradigms can effectively harness the strengths of both approaches, providing a promising direction for high-quality visual generation with simple autoregressive modeling. Project page: https://yuqingwang1029.github.io/TokenBridge.",
    "metadata": {
      "arxiv_id": "2503.16430",
      "title": "Bridging Continuous and Discrete Tokens for Autoregressive Visual Generation",
      "summary": "Autoregressive visual generation models typically rely on tokenizers to compress images into tokens that can be predicted sequentially. A fundamental dilemma exists in token representation: discrete tokens enable straightforward modeling with standard cross-entropy loss, but suffer from information loss and tokenizer training instability; continuous tokens better preserve visual details, but require complex distribution modeling, complicating the generation pipeline. In this paper, we propose TokenBridge, which bridges this gap by maintaining the strong representation capacity of continuous tokens while preserving the modeling simplicity of discrete tokens. To achieve this, we decouple discretization from the tokenizer training process through post-training quantization that directly obtains discrete tokens from continuous representations. Specifically, we introduce a dimension-wise quantization strategy that independently discretizes each feature dimension, paired with a lightweight autoregressive prediction mechanism that efficiently model the resulting large token space. Extensive experiments show that our approach achieves reconstruction and generation quality on par with continuous methods while using standard categorical prediction. This work demonstrates that bridging discrete and continuous paradigms can effectively harness the strengths of both approaches, providing a promising direction for high-quality visual generation with simple autoregressive modeling. Project page: https://yuqingwang1029.github.io/TokenBridge.",
      "authors": [
        "Yuqing Wang",
        "Zhijie Lin",
        "Yao Teng",
        "Yuanzhi Zhu",
        "Shuhuai Ren",
        "Jiashi Feng",
        "Xihui Liu"
      ],
      "published": "2025-03-20T17:59:59Z",
      "updated": "2025-08-29T02:25:47Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.16430v3",
      "landing_url": "https://arxiv.org/abs/2503.16430v3",
      "doi": "https://doi.org/10.48550/arXiv.2503.16430"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on token representations for autoregressive visual generation and does not address Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on token representations for autoregressive visual generation and does not address Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on autoregressive visual generation and token representation in images, without any mention of spoken language models, speech language modeling, or related core topics such as speech signals, full-duplex interaction, or speech-based model architectures. It does not meet the inclusion criteria centered on spoken language models and related speech interaction behaviors, nor does it address evaluation or synchronization aspects relevant to the speech modality. Therefore, it does not fit the scope of the inclusion criteria and falls under exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on autoregressive visual generation and token representation in images, without any mention of spoken language models, speech language modeling, or related core topics such as speech signals, full-duplex interaction, or speech-based model architectures. It does not meet the inclusion criteria centered on spoken language models and related speech interaction behaviors, nor does it address evaluation or synchronization aspects relevant to the speech modality. Therefore, it does not fit the scope of the inclusion criteria and falls under exclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Multimodal Transformer Models for Turn-taking Prediction: Effects on Conversational Dynamics of Human-Agent Interaction during Cooperative Gameplay",
    "abstract": "This study investigates multimodal turn-taking prediction within human-agent interactions (HAI), particularly focusing on cooperative gaming environments. It comprises both model development and subsequent user study, aiming to refine our understanding and improve conversational dynamics in spoken dialogue systems (SDSs). For the modeling phase, we introduce a novel transformer-based deep learning (DL) model that simultaneously integrates multiple modalities - text, vision, audio, and contextual in-game data to predict turn-taking events in real-time. Our model employs a Crossmodal Transformer architecture to effectively fuse information from these diverse modalities, enabling more comprehensive turn-taking predictions. The model demonstrates superior performance compared to baseline models, achieving 87.3% accuracy and 83.0% macro F1 score. A human user study was then conducted to empirically evaluate the turn-taking DL model in an interactive scenario with a virtual avatar while playing the game \"Dont Starve Together\", comparing a control condition without turn-taking prediction (n=20) to an experimental condition with our model deployed (n=40). Both conditions included a mix of English and Korean speakers, since turn-taking cues are known to vary by culture. We then analyzed the interaction quality, examining aspects such as utterance counts, interruption frequency, and participant perceptions of the avatar. Results from the user study suggest that our multimodal turn-taking model not only enhances the fluidity and naturalness of human-agent conversations, but also maintains a balanced conversational dynamic without significantly altering dialogue frequency. The study provides in-depth insights into the influence of turn-taking abilities on user perceptions and interaction quality, underscoring the potential for more contextually adaptive and responsive conversational agents.",
    "metadata": {
      "arxiv_id": "2503.16432",
      "title": "Multimodal Transformer Models for Turn-taking Prediction: Effects on Conversational Dynamics of Human-Agent Interaction during Cooperative Gameplay",
      "summary": "This study investigates multimodal turn-taking prediction within human-agent interactions (HAI), particularly focusing on cooperative gaming environments. It comprises both model development and subsequent user study, aiming to refine our understanding and improve conversational dynamics in spoken dialogue systems (SDSs). For the modeling phase, we introduce a novel transformer-based deep learning (DL) model that simultaneously integrates multiple modalities - text, vision, audio, and contextual in-game data to predict turn-taking events in real-time. Our model employs a Crossmodal Transformer architecture to effectively fuse information from these diverse modalities, enabling more comprehensive turn-taking predictions. The model demonstrates superior performance compared to baseline models, achieving 87.3% accuracy and 83.0% macro F1 score. A human user study was then conducted to empirically evaluate the turn-taking DL model in an interactive scenario with a virtual avatar while playing the game \"Dont Starve Together\", comparing a control condition without turn-taking prediction (n=20) to an experimental condition with our model deployed (n=40). Both conditions included a mix of English and Korean speakers, since turn-taking cues are known to vary by culture. We then analyzed the interaction quality, examining aspects such as utterance counts, interruption frequency, and participant perceptions of the avatar. Results from the user study suggest that our multimodal turn-taking model not only enhances the fluidity and naturalness of human-agent conversations, but also maintains a balanced conversational dynamic without significantly altering dialogue frequency. The study provides in-depth insights into the influence of turn-taking abilities on user perceptions and interaction quality, underscoring the potential for more contextually adaptive and responsive conversational agents.",
      "authors": [
        "Young-Ho Bae",
        "Casey C. Bennett"
      ],
      "published": "2025-02-05T23:00:49Z",
      "updated": "2025-02-05T23:00:49Z",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.16432v1",
      "landing_url": "https://arxiv.org/abs/2503.16432v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.16432"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on multimodal turn-taking prediction in human-agent interactions and evaluates interaction quality, but it does not clearly frame speech/language modeling as the core object nor explicitly address spoken-language-model architectures (FD-SLM/SLM); recency in the past 3 years is also unclear from the provided text, making inclusion uncertain.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on multimodal turn-taking prediction in human-agent interactions and evaluates interaction quality, but it does not clearly frame speech/language modeling as the core object nor explicitly address spoken-language-model architectures (FD-SLM/SLM); recency in the past 3 years is also unclear from the provided text, making inclusion uncertain.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multimodal turn-taking prediction involving speech and other modalities (text, vision, audio, in-game context), uses a transformer-based deep learning model, and evaluates conversational dynamics including turn-taking interruptions. However, while it involves predictive modeling for conversational turn-taking and discusses conversation dynamics, it does not explicitly describe modeling speech sequences or language modeling of speech as a core element, nor does it mention Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) explicitly. The focus on 'turn-taking' and multimodal fusion is promising for inclusion, but the absence of explicit speech language modeling or equivalent terms in the abstract/title is a critical omission according to the inclusion criteria. Therefore, it partly fits but doesn't fully meet the specialized requirement related to speech language modeling as core. Hence, inclusion is uncertain, suggesting a moderate score reflecting this ambiguity.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multimodal turn-taking prediction involving speech and other modalities (text, vision, audio, in-game context), uses a transformer-based deep learning model, and evaluates conversational dynamics including turn-taking interruptions. However, while it involves predictive modeling for conversational turn-taking and discusses conversation dynamics, it does not explicitly describe modeling speech sequences or language modeling of speech as a core element, nor does it mention Spoken Language Models (SLMs) or Full-Duplex Spoken Language Models (FD-SLMs) explicitly. The focus on 'turn-taking' and multimodal fusion is promising for inclusion, but the absence of explicit speech language modeling or equivalent terms in the abstract/title is a critical omission according to the inclusion criteria. Therefore, it partly fits but doesn't fully meet the specialized requirement related to speech language modeling as core. Hence, inclusion is uncertain, suggesting a moderate score reflecting this ambiguity.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers rated this as uncertain (3) and noted the paper focuses on multimodal turn-taking prediction with audio among other modalities, but neither identified explicit Spoken Language Model (SLM) or Full‑Duplex SLM claims; the abstract describes a Crossmodal Transformer that fuses text, vision, audio and game context to predict turn-taking events and reports user study results, yet it treats audio as one input modality rather than centering on speech-as-primary modeling (no mention of speech language modeling, SLM/FD‑SLM terminology, speech‑level LM, or end-to-end speech modeling/evaluation). According to the inclusion criteria, the study must explicitly center on Spoken Language Models or equivalent (speech as the main modeling/inference object) and mention such concepts in the title/abstract; this item fails that requirement and thus should not be included. Brief explanation: Does not meet inclusion — audio is a modality but the core contribution is turn-taking prediction in a multimodal setting, not speech-focused language modeling, so better to exclude.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers rated this as uncertain (3) and noted the paper focuses on multimodal turn-taking prediction with audio among other modalities, but neither identified explicit Spoken Language Model (SLM) or Full‑Duplex SLM claims; the abstract describes a Crossmodal Transformer that fuses text, vision, audio and game context to predict turn-taking events and reports user study results, yet it treats audio as one input modality rather than centering on speech-as-primary modeling (no mention of speech language modeling, SLM/FD‑SLM terminology, speech‑level LM, or end-to-end speech modeling/evaluation). According to the inclusion criteria, the study must explicitly center on Spoken Language Models or equivalent (speech as the main modeling/inference object) and mention such concepts in the title/abstract; this item fails that requirement and thus should not be included. Brief explanation: Does not meet inclusion — audio is a modality but the core contribution is turn-taking prediction in a multimodal setting, not speech-focused language modeling, so better to exclude.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Federated Cross-Domain Click-Through Rate Prediction With Large Language Model Augmentation",
    "abstract": "Accurately predicting click-through rates (CTR) under stringent privacy constraints poses profound challenges, particularly when user-item interactions are sparse and fragmented across domains. Conventional cross-domain CTR (CCTR) methods frequently assume homogeneous feature spaces and rely on centralized data sharing, neglecting complex inter-domain discrepancies and the subtle trade-offs imposed by privacy-preserving protocols. Here, we present Federated Cross-Domain CTR Prediction with Large Language Model Augmentation (FedCCTR-LM), a federated framework engineered to address these limitations by synchronizing data augmentation, representation disentanglement, and adaptive privacy protection. Our approach integrates three core innovations. First, the Privacy-Preserving Augmentation Network (PrivAugNet) employs large language models to enrich user and item representations and expand interaction sequences, mitigating data sparsity and feature incompleteness. Second, the Independent Domain-Specific Transformer with Contrastive Learning (IDST-CL) module disentangles domain-specific and shared user preferences, employing intra-domain representation alignment (IDRA) and crossdomain representation disentanglement (CDRD) to refine the learned embeddings and enhance knowledge transfer across domains. Finally, the Adaptive Local Differential Privacy (AdaLDP) mechanism dynamically calibrates noise injection to achieve an optimal balance between rigorous privacy guarantees and predictive accuracy. Empirical evaluations on four real-world datasets demonstrate that FedCCTR-LM substantially outperforms existing baselines, offering robust, privacy-preserving, and generalizable cross-domain CTR prediction in heterogeneous, federated environments.",
    "metadata": {
      "arxiv_id": "2503.16875",
      "title": "Federated Cross-Domain Click-Through Rate Prediction With Large Language Model Augmentation",
      "summary": "Accurately predicting click-through rates (CTR) under stringent privacy constraints poses profound challenges, particularly when user-item interactions are sparse and fragmented across domains. Conventional cross-domain CTR (CCTR) methods frequently assume homogeneous feature spaces and rely on centralized data sharing, neglecting complex inter-domain discrepancies and the subtle trade-offs imposed by privacy-preserving protocols. Here, we present Federated Cross-Domain CTR Prediction with Large Language Model Augmentation (FedCCTR-LM), a federated framework engineered to address these limitations by synchronizing data augmentation, representation disentanglement, and adaptive privacy protection. Our approach integrates three core innovations. First, the Privacy-Preserving Augmentation Network (PrivAugNet) employs large language models to enrich user and item representations and expand interaction sequences, mitigating data sparsity and feature incompleteness. Second, the Independent Domain-Specific Transformer with Contrastive Learning (IDST-CL) module disentangles domain-specific and shared user preferences, employing intra-domain representation alignment (IDRA) and crossdomain representation disentanglement (CDRD) to refine the learned embeddings and enhance knowledge transfer across domains. Finally, the Adaptive Local Differential Privacy (AdaLDP) mechanism dynamically calibrates noise injection to achieve an optimal balance between rigorous privacy guarantees and predictive accuracy. Empirical evaluations on four real-world datasets demonstrate that FedCCTR-LM substantially outperforms existing baselines, offering robust, privacy-preserving, and generalizable cross-domain CTR prediction in heterogeneous, federated environments.",
      "authors": [
        "Jiangcheng Qin",
        "Xueyuan Zhang",
        "Baisong Liu",
        "Jiangbo Qian",
        "Yangyang Wang"
      ],
      "published": "2025-03-21T06:22:42Z",
      "updated": "2025-03-21T06:22:42Z",
      "categories": [
        "cs.IR",
        "cs.CL",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.16875v1",
      "landing_url": "https://arxiv.org/abs/2503.16875v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.16875"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本研究聚焦於聯邦學習的跨域 CTR 预测與大語言模型增強，用於預測點擊率，與語音語言模型核心議題（以語音為主要建模對象、同步對話、語音重疊等）不符，因此不滿足包含條件，亦無符合排除條件中的語音互動或語音建模相關限制。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本研究聚焦於聯邦學習的跨域 CTR 预测與大語言模型增強，用於預測點擊率，與語音語言模型核心議題（以語音為主要建模對象、同步對話、語音重疊等）不符，因此不滿足包含條件，亦無符合排除條件中的語音互動或語音建模相關限制。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on federated cross-domain click-through rate prediction enhanced by large language models, concentrating on user-item interaction data, privacy, and representation learning; it does not address spoken language models, speech-level language modeling, full-duplex interaction, or any speech-specific modalities or engineering related to real-time spoken interaction as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on federated cross-domain click-through rate prediction enhanced by large language models, concentrating on user-item interaction data, privacy, and representation learning; it does not address spoken language models, speech-level language modeling, full-duplex interaction, or any speech-specific modalities or engineering related to real-time spoken interaction as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "VQToken: Neural Discrete Token Representation Learning for Extreme Token Reduction in Video Large Language Models",
    "abstract": "Token-based video representation has emerged as a promising approach for enabling large language models (LLMs) to interpret video content. However, existing token reduction techniques, such as pruning and merging, often disrupt essential positional embeddings and rely on continuous visual tokens sampled from nearby pixels with similar spatial-temporal locations. By removing only a small fraction of tokens, these methods still produce relatively lengthy continuous sequences, which falls short of the extreme compression required to balance computational efficiency and token count in video LLMs. In this paper, we introduce the novel task of Extreme Short Token Reduction, which aims to represent entire videos using a minimal set of discrete tokens. We propose VQToken, a neural discrete token representation framework that (i) applies adaptive vector quantization to continuous ViT embeddings to learn a compact codebook and (ii) preserves spatial-temporal positions via a token hash function by assigning each grid-level token to its nearest codebook entry. On the Extreme Short Token Reduction task, our VQToken compresses sequences to just 0.07 percent of their original length while incurring only a 0.66 percent drop in accuracy on the NextQA-MC benchmark. It also achieves comparable performance on ActNet-QA, Long Video Bench, and VideoMME. We further introduce the Token Information Density (TokDense) metric and formalize fixed-length and adaptive-length subtasks, achieving state-of-the-art results in both settings. Our approach dramatically lowers theoretical complexity, increases information density, drastically reduces token counts, and enables efficient video LLMs in resource-constrained environments.",
    "metadata": {
      "arxiv_id": "2503.16980",
      "title": "VQToken: Neural Discrete Token Representation Learning for Extreme Token Reduction in Video Large Language Models",
      "summary": "Token-based video representation has emerged as a promising approach for enabling large language models (LLMs) to interpret video content. However, existing token reduction techniques, such as pruning and merging, often disrupt essential positional embeddings and rely on continuous visual tokens sampled from nearby pixels with similar spatial-temporal locations. By removing only a small fraction of tokens, these methods still produce relatively lengthy continuous sequences, which falls short of the extreme compression required to balance computational efficiency and token count in video LLMs. In this paper, we introduce the novel task of Extreme Short Token Reduction, which aims to represent entire videos using a minimal set of discrete tokens. We propose VQToken, a neural discrete token representation framework that (i) applies adaptive vector quantization to continuous ViT embeddings to learn a compact codebook and (ii) preserves spatial-temporal positions via a token hash function by assigning each grid-level token to its nearest codebook entry. On the Extreme Short Token Reduction task, our VQToken compresses sequences to just 0.07 percent of their original length while incurring only a 0.66 percent drop in accuracy on the NextQA-MC benchmark. It also achieves comparable performance on ActNet-QA, Long Video Bench, and VideoMME. We further introduce the Token Information Density (TokDense) metric and formalize fixed-length and adaptive-length subtasks, achieving state-of-the-art results in both settings. Our approach dramatically lowers theoretical complexity, increases information density, drastically reduces token counts, and enables efficient video LLMs in resource-constrained environments.",
      "authors": [
        "Haichao Zhang",
        "Yun Fu"
      ],
      "published": "2025-03-21T09:46:31Z",
      "updated": "2025-09-29T01:09:31Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.16980v6",
      "landing_url": "https://arxiv.org/abs/2503.16980v6",
      "doi": "https://doi.org/10.48550/arXiv.2503.16980"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on extreme token reduction for video LLMs using discrete tokens, not on Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on extreme token reduction for video LLMs using discrete tokens, not on Spoken Language Models or speech-language modeling, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "This study focuses on token reduction techniques in video large language models and does not explicitly involve spoken language models, speech language models, or any speech-centric modeling as required by the inclusion criteria; furthermore, it does not discuss full-duplex spoken language interactions or related benchmarks, hence it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "This study focuses on token reduction techniques in video large language models and does not explicitly involve spoken language models, speech language models, or any speech-centric modeling as required by the inclusion criteria; furthermore, it does not discuss full-duplex spoken language interactions or related benchmarks, hence it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "D2C: Unlocking the Potential of Continuous Autoregressive Image Generation with Discrete Tokens",
    "abstract": "In the domain of image generation, latent-based generative models occupy a dominant status; however, these models rely heavily on image tokenizer. To meet modeling requirements, autoregressive models possessing the characteristics of scalability and flexibility embrace a discrete-valued tokenizer, but face the challenge of poor image generation quality. In contrast, diffusion models take advantage of the continuous-valued tokenizer to achieve better generation quality but are subject to low efficiency and complexity. The existing hybrid models are mainly to compensate for information loss and simplify the diffusion learning process. The potential of merging discrete-valued and continuous-valued tokens in the field of image generation has not yet been explored. In this paper, we propose D2C, a novel two-stage method to enhance model generation capacity. In the first stage, the discrete-valued tokens representing coarse-grained image features are sampled by employing a small discrete-valued generator. Then in the second stage, the continuous-valued tokens representing fine-grained image features are learned conditioned on the discrete token sequence. In addition, we design two kinds of fusion modules for seamless interaction. On the ImageNet-256 benchmark, extensive experiment results validate that our model achieves superior performance compared with several continuous-valued and discrete-valued generative models on the class-conditional image generation tasks.",
    "metadata": {
      "arxiv_id": "2503.17155",
      "title": "D2C: Unlocking the Potential of Continuous Autoregressive Image Generation with Discrete Tokens",
      "summary": "In the domain of image generation, latent-based generative models occupy a dominant status; however, these models rely heavily on image tokenizer. To meet modeling requirements, autoregressive models possessing the characteristics of scalability and flexibility embrace a discrete-valued tokenizer, but face the challenge of poor image generation quality. In contrast, diffusion models take advantage of the continuous-valued tokenizer to achieve better generation quality but are subject to low efficiency and complexity. The existing hybrid models are mainly to compensate for information loss and simplify the diffusion learning process. The potential of merging discrete-valued and continuous-valued tokens in the field of image generation has not yet been explored. In this paper, we propose D2C, a novel two-stage method to enhance model generation capacity. In the first stage, the discrete-valued tokens representing coarse-grained image features are sampled by employing a small discrete-valued generator. Then in the second stage, the continuous-valued tokens representing fine-grained image features are learned conditioned on the discrete token sequence. In addition, we design two kinds of fusion modules for seamless interaction. On the ImageNet-256 benchmark, extensive experiment results validate that our model achieves superior performance compared with several continuous-valued and discrete-valued generative models on the class-conditional image generation tasks.",
      "authors": [
        "Panpan Wang",
        "Liqiang Niu",
        "Fandong Meng",
        "Jinan Xu",
        "Yufeng Chen",
        "Jie Zhou"
      ],
      "published": "2025-03-21T13:58:49Z",
      "updated": "2025-03-21T13:58:49Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.17155v1",
      "landing_url": "https://arxiv.org/abs/2503.17155v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.17155"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item addresses image generation with discrete/continuous tokens and does not center on Spoken Language Models or speech-language modeling; thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item addresses image generation with discrete/continuous tokens and does not center on Spoken Language Models or speech-language modeling; thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus entirely on image generation models that integrate discrete and continuous tokens for improved visual data synthesis, with no mention or focus on spoken language models, speech language modeling, speech signals as modeling objects, or full-duplex spoken language interactions, which are essential inclusion criteria. The study is outside the scope defined by the inclusion criteria, which focus on speech and spoken language modeling rather than image generation or visual data.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus entirely on image generation models that integrate discrete and continuous tokens for improved visual data synthesis, with no mention or focus on spoken language models, speech language modeling, speech signals as modeling objects, or full-duplex spoken language interactions, which are essential inclusion criteria. The study is outside the scope defined by the inclusion criteria, which focus on speech and spoken language modeling rather than image generation or visual data.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "CODA: Repurposing Continuous VAEs for Discrete Tokenization",
    "abstract": "Discrete visual tokenizers transform images into a sequence of tokens, enabling token-based visual generation akin to language models. However, this process is inherently challenging, as it requires both compressing visual signals into a compact representation and discretizing them into a fixed set of codes. Traditional discrete tokenizers typically learn the two tasks jointly, often leading to unstable training, low codebook utilization, and limited reconstruction quality. In this paper, we introduce \\textbf{CODA}(\\textbf{CO}ntinuous-to-\\textbf{D}iscrete \\textbf{A}daptation), a framework that decouples compression and discretization. Instead of training discrete tokenizers from scratch, CODA adapts off-the-shelf continuous VAEs -- already optimized for perceptual compression -- into discrete tokenizers via a carefully designed discretization process. By primarily focusing on discretization, CODA ensures stable and efficient training while retaining the strong visual fidelity of continuous VAEs. Empirically, with $\\mathbf{6 \\times}$ less training budget than standard VQGAN, our approach achieves a remarkable codebook utilization of 100% and notable reconstruction FID (rFID) of $\\mathbf{0.43}$ and $\\mathbf{1.34}$ for $8 \\times$ and $16 \\times$ compression on ImageNet 256$\\times$ 256 benchmark.",
    "metadata": {
      "arxiv_id": "2503.17760",
      "title": "CODA: Repurposing Continuous VAEs for Discrete Tokenization",
      "summary": "Discrete visual tokenizers transform images into a sequence of tokens, enabling token-based visual generation akin to language models. However, this process is inherently challenging, as it requires both compressing visual signals into a compact representation and discretizing them into a fixed set of codes. Traditional discrete tokenizers typically learn the two tasks jointly, often leading to unstable training, low codebook utilization, and limited reconstruction quality. In this paper, we introduce \\textbf{CODA}(\\textbf{CO}ntinuous-to-\\textbf{D}iscrete \\textbf{A}daptation), a framework that decouples compression and discretization. Instead of training discrete tokenizers from scratch, CODA adapts off-the-shelf continuous VAEs -- already optimized for perceptual compression -- into discrete tokenizers via a carefully designed discretization process. By primarily focusing on discretization, CODA ensures stable and efficient training while retaining the strong visual fidelity of continuous VAEs. Empirically, with $\\mathbf{6 \\times}$ less training budget than standard VQGAN, our approach achieves a remarkable codebook utilization of 100% and notable reconstruction FID (rFID) of $\\mathbf{0.43}$ and $\\mathbf{1.34}$ for $8 \\times$ and $16 \\times$ compression on ImageNet 256$\\times$ 256 benchmark.",
      "authors": [
        "Zeyu Liu",
        "Zanlin Ni",
        "Yeguo Hua",
        "Xin Deng",
        "Xiao Ma",
        "Cheng Zhong",
        "Gao Huang"
      ],
      "published": "2025-03-22T12:59:00Z",
      "updated": "2025-09-30T14:36:25Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.17760v2",
      "landing_url": "https://arxiv.org/abs/2503.17760v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.17760"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper targets image tokenization via discretized representations derived from continuous VAEs; it does not address Spoken Language Models or any speech-language modeling tasks, nor interactive dialogue/real-time speech behavior, thus should be excluded per criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper targets image tokenization via discretized representations derived from continuous VAEs; it does not address Spoken Language Models or any speech-language modeling tasks, nor interactive dialogue/real-time speech behavior, thus should be excluded per criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a method for discrete visual tokenization using continuous VAEs, focusing on visual image compression and generation, with no mention of spoken language models, speech language modeling, or full-duplex spoken interaction capabilities. The work does not address speech as a primary modeling object, nor does it discuss synchronous speech interaction or related evaluation frameworks, thus failing all inclusion criteria related to spoken language models and interaction dynamics.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a method for discrete visual tokenization using continuous VAEs, focusing on visual image compression and generation, with no mention of spoken language models, speech language modeling, or full-duplex spoken interaction capabilities. The work does not address speech as a primary modeling object, nor does it discuss synchronous speech interaction or related evaluation frameworks, thus failing all inclusion criteria related to spoken language models and interaction dynamics.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Color Conditional Generation with Sliced Wasserstein Guidance",
    "abstract": "We propose SW-Guidance, a training-free approach for image generation conditioned on the color distribution of a reference image. While it is possible to generate an image with fixed colors by first creating an image from a text prompt and then applying a color style transfer method, this approach often results in semantically meaningless colors in the generated image. Our method solves this problem by modifying the sampling process of a diffusion model to incorporate the differentiable Sliced 1-Wasserstein distance between the color distribution of the generated image and the reference palette. Our method outperforms state-of-the-art techniques for color-conditional generation in terms of color similarity to the reference, producing images that not only match the reference colors but also maintain semantic coherence with the original text prompt. Our source code is available at https://github.com/alobashev/sw-guidance/.",
    "metadata": {
      "arxiv_id": "2503.19034",
      "title": "Color Conditional Generation with Sliced Wasserstein Guidance",
      "summary": "We propose SW-Guidance, a training-free approach for image generation conditioned on the color distribution of a reference image. While it is possible to generate an image with fixed colors by first creating an image from a text prompt and then applying a color style transfer method, this approach often results in semantically meaningless colors in the generated image. Our method solves this problem by modifying the sampling process of a diffusion model to incorporate the differentiable Sliced 1-Wasserstein distance between the color distribution of the generated image and the reference palette. Our method outperforms state-of-the-art techniques for color-conditional generation in terms of color similarity to the reference, producing images that not only match the reference colors but also maintain semantic coherence with the original text prompt. Our source code is available at https://github.com/alobashev/sw-guidance/.",
      "authors": [
        "Alexander Lobashev",
        "Maria Larchenko",
        "Dmitry Guskov"
      ],
      "published": "2025-03-24T18:06:03Z",
      "updated": "2025-03-24T18:06:03Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.19034v1",
      "landing_url": "https://arxiv.org/abs/2503.19034v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.19034"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on image generation conditioned on color distributions and does not address Spoken Language Models or speech-language modeling, so it does not meet the core inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on image generation conditioned on color distributions and does not address Spoken Language Models or speech-language modeling, so it does not meet the core inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on image generation conditioned on color distribution using diffusion models, which is unrelated to spoken language models or speech-based modeling. It does not discuss any spoken language modeling, speech-language models, or full-duplex spoken language interaction, nor does it address any speech or audio-based themes, training, or evaluations as required by the inclusion criteria. Hence, it clearly does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on image generation conditioned on color distribution using diffusion models, which is unrelated to spoken language models or speech-based modeling. It does not discuss any spoken language modeling, speech-language models, or full-duplex spoken language interaction, nor does it address any speech or audio-based themes, training, or evaluations as required by the inclusion criteria. Hence, it clearly does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Analyzable Chain-of-Musical-Thought Prompting for High-Fidelity Music Generation",
    "abstract": "Autoregressive (AR) models have demonstrated impressive capabilities in generating high-fidelity music. However, the conventional next-token prediction paradigm in AR models does not align with the human creative process in music composition, potentially compromising the musicality of generated samples. To overcome this limitation, we introduce MusiCoT, a novel chain-of-thought (CoT) prompting technique tailored for music generation. MusiCoT empowers the AR model to first outline an overall music structure before generating audio tokens, thereby enhancing the coherence and creativity of the resulting compositions. By leveraging the contrastive language-audio pretraining (CLAP) model, we establish a chain of \"musical thoughts\", making MusiCoT scalable and independent of human-labeled data, in contrast to conventional CoT methods. Moreover, MusiCoT allows for in-depth analysis of music structure, such as instrumental arrangements, and supports music referencing -- accepting variable-length audio inputs as optional style references. This innovative approach effectively addresses copying issues, positioning MusiCoT as a vital practical method for music prompting. Our experimental results indicate that MusiCoT consistently achieves superior performance across both objective and subjective metrics, producing music quality that rivals state-of-the-art generation models. Our samples are available at https://MusiCoT.github.io/.",
    "metadata": {
      "arxiv_id": "2503.19611",
      "title": "Analyzable Chain-of-Musical-Thought Prompting for High-Fidelity Music Generation",
      "summary": "Autoregressive (AR) models have demonstrated impressive capabilities in generating high-fidelity music. However, the conventional next-token prediction paradigm in AR models does not align with the human creative process in music composition, potentially compromising the musicality of generated samples. To overcome this limitation, we introduce MusiCoT, a novel chain-of-thought (CoT) prompting technique tailored for music generation. MusiCoT empowers the AR model to first outline an overall music structure before generating audio tokens, thereby enhancing the coherence and creativity of the resulting compositions. By leveraging the contrastive language-audio pretraining (CLAP) model, we establish a chain of \"musical thoughts\", making MusiCoT scalable and independent of human-labeled data, in contrast to conventional CoT methods. Moreover, MusiCoT allows for in-depth analysis of music structure, such as instrumental arrangements, and supports music referencing -- accepting variable-length audio inputs as optional style references. This innovative approach effectively addresses copying issues, positioning MusiCoT as a vital practical method for music prompting. Our experimental results indicate that MusiCoT consistently achieves superior performance across both objective and subjective metrics, producing music quality that rivals state-of-the-art generation models.\n  Our samples are available at https://MusiCoT.github.io/.",
      "authors": [
        "Max W. Y. Lam",
        "Yijin Xing",
        "Weiya You",
        "Jingcheng Wu",
        "Zongyu Yin",
        "Fuqiang Jiang",
        "Hangyu Liu",
        "Feng Liu",
        "Xingda Li",
        "Wei-Tsung Lu",
        "Hanyu Chen",
        "Tong Feng",
        "Tianwei Zhao",
        "Chien-Hung Liu",
        "Xuchen Song",
        "Yang Li",
        "Yahui Zhou"
      ],
      "published": "2025-03-25T12:51:21Z",
      "updated": "2025-03-25T12:51:21Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.19611v1",
      "landing_url": "https://arxiv.org/abs/2503.19611v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.19611"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on chain-of-thought prompting for music generation and does not center on Spoken Language Models or speech-language modeling as core; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on chain-of-thought prompting for music generation and does not center on Spoken Language Models or speech-language modeling as core; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on music generation using chain-of-thought prompting in autoregressive models, centering on audio token generation and music structure, without addressing spoken language models, speech language modeling, or interactive full-duplex spoken communication features emphasized in the inclusion criteria. It does not discuss synchronization, cognitive parallelism, or real-time interaction in spoken dialogue systems, nor does it mention speech language modeling or evaluation metrics related to speech interaction. Hence, it falls outside the defined research scope related to spoken language or speech language models and interactive speech systems.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on music generation using chain-of-thought prompting in autoregressive models, centering on audio token generation and music structure, without addressing spoken language models, speech language modeling, or interactive full-duplex spoken communication features emphasized in the inclusion criteria. It does not discuss synchronization, cognitive parallelism, or real-time interaction in spoken dialogue systems, nor does it mention speech language modeling or evaluation metrics related to speech interaction. Hence, it falls outside the defined research scope related to spoken language or speech language models and interactive speech systems.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Qwen2.5-Omni Technical Report",
    "abstract": "In this report, we present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. To synchronize the timestamps of video inputs with audio, we organize the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose \\textbf{Thinker-Talker} architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni is comparable with the similarly sized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni's performance in end-to-end speech instruction following is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming Talker outperforms most existing streaming and non-streaming alternatives in robustness and naturalness.",
    "metadata": {
      "arxiv_id": "2503.20215",
      "title": "Qwen2.5-Omni Technical Report",
      "summary": "In this report, we present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. To synchronize the timestamps of video inputs with audio, we organize the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose \\textbf{Thinker-Talker} architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni is comparable with the similarly sized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni's performance in end-to-end speech instruction following is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming Talker outperforms most existing streaming and non-streaming alternatives in robustness and naturalness.",
      "authors": [
        "Jin Xu",
        "Zhifang Guo",
        "Jinzheng He",
        "Hangrui Hu",
        "Ting He",
        "Shuai Bai",
        "Keqin Chen",
        "Jialin Wang",
        "Yang Fan",
        "Kai Dang",
        "Bin Zhang",
        "Xiong Wang",
        "Yunfei Chu",
        "Junyang Lin"
      ],
      "published": "2025-03-26T04:17:55Z",
      "updated": "2025-03-26T04:17:55Z",
      "categories": [
        "cs.CL",
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.20215v1",
      "landing_url": "https://arxiv.org/abs/2503.20215v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.20215"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This abstract describes a multimodal model with streaming speech generation, but it does not explicitly frame the work as a Spoken Language Model or Speech Language Model, nor treat speech as the core modeling/inference target, so it likely does not meet the inclusion criteria which require explicit focus on SLM/FD-SLM with speech-centric evaluation.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "This abstract describes a multimodal model with streaming speech generation, but it does not explicitly frame the work as a Spoken Language Model or Speech Language Model, nor treat speech as the core modeling/inference target, so it likely does not meet the inclusion criteria which require explicit focus on SLM/FD-SLM with speech-centric evaluation.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents Qwen2.5-Omni, an end-to-end multimodal model including audio and speech generation capabilities with streaming and low-latency features, but it focuses on multimodal perception and generation broadly without explicitly defining itself as a Spoken Language Model (SLM) or Full-Duplex SLM; it lacks mention of core spoken language modeling, cognitive parallelism, or detailed evaluation on speech turn-taking, overlap, or interaction synchronization specific to FD-SLMs, thus it partially aligns but does not fully meet the inclusion criteria focused explicitly on spoken language modeling and full-duplex interaction, nor does it clearly meet exclusion criteria such as being single-task or ASR/TTS-only, making it borderline relevant but not clearly fitting all criteria.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study presents Qwen2.5-Omni, an end-to-end multimodal model including audio and speech generation capabilities with streaming and low-latency features, but it focuses on multimodal perception and generation broadly without explicitly defining itself as a Spoken Language Model (SLM) or Full-Duplex SLM; it lacks mention of core spoken language modeling, cognitive parallelism, or detailed evaluation on speech turn-taking, overlap, or interaction synchronization specific to FD-SLMs, thus it partially aligns but does not fully meet the inclusion criteria focused explicitly on spoken language modeling and full-duplex interaction, nor does it clearly meet exclusion criteria such as being single-task or ASR/TTS-only, making it borderline relevant but not clearly fitting all criteria.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers flagged this as borderline: one judged it likely excluded for lacking explicit SLM labeling, the other called it partially aligned but not fully fitting FD-SLM specifics. I examined the abstract and found it does treat speech as a core modeling and inference target (autogressive Talker that outputs audio tokens, end-to-end speech instruction following, streaming low-latency decoding and benchmarks), describes model architecture and evaluations, and is recent—so it meets the inclusion requirement of speech-centric modeling (equivalent to “language modeling of speech”) and avoids the exclusion rules (not just ASR/TTS or purely text-centered). It does not, however, explicitly use the term “Spoken Language Model” nor deeply analyze full-duplex behaviors like overlap/backchanneling, so it is relevant but not a perfect match for FD-SLM focus.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers flagged this as borderline: one judged it likely excluded for lacking explicit SLM labeling, the other called it partially aligned but not fully fitting FD-SLM specifics. I examined the abstract and found it does treat speech as a core modeling and inference target (autogressive Talker that outputs audio tokens, end-to-end speech instruction following, streaming low-latency decoding and benchmarks), describes model architecture and evaluations, and is recent—so it meets the inclusion requirement of speech-centric modeling (equivalent to “language modeling of speech”) and avoids the exclusion rules (not just ASR/TTS or purely text-centered). It does not, however, explicitly use the term “Spoken Language Model” nor deeply analyze full-duplex behaviors like overlap/backchanneling, so it is relevant but not a perfect match for FD-SLM focus.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Dewey Long Context Embedding Model: A Technical Report",
    "abstract": "This technical report presents the training methodology and evaluation results of the open-source dewey_en_beta embedding model. The increasing demand for retrieval-augmented generation (RAG) systems and the expanding context window capabilities of large language models (LLMs) have created critical challenges for conventional embedding models. Current approaches often struggle to maintain semantic coherence when processing documents exceeding typical sequence length limitations, significantly impacting retrieval performance in knowledge-intensive applications. This paper presents dewey_en_beta, a novel text embedding model that achieves excellent performance on MTEB (Eng, v2) and LongEmbed benchmark while supporting 128K token sequences. Our technical contribution centers on chunk alignment training, an innovative methodology that enables the simultaneous generation of localized chunk embeddings and global document-level representations through distillation. Information regarding the model release can be found at https://huggingface.co/infgrad/dewey_en_beta.",
    "metadata": {
      "arxiv_id": "2503.20376",
      "title": "Dewey Long Context Embedding Model: A Technical Report",
      "summary": "This technical report presents the training methodology and evaluation results of the open-source dewey_en_beta embedding model. The increasing demand for retrieval-augmented generation (RAG) systems and the expanding context window capabilities of large language models (LLMs) have created critical challenges for conventional embedding models. Current approaches often struggle to maintain semantic coherence when processing documents exceeding typical sequence length limitations, significantly impacting retrieval performance in knowledge-intensive applications. This paper presents dewey_en_beta, a novel text embedding model that achieves excellent performance on MTEB (Eng, v2) and LongEmbed benchmark while supporting 128K token sequences. Our technical contribution centers on chunk alignment training, an innovative methodology that enables the simultaneous generation of localized chunk embeddings and global document-level representations through distillation. Information regarding the model release can be found at https://huggingface.co/infgrad/dewey_en_beta.",
      "authors": [
        "Dun Zhang",
        "Panxiang Zou",
        "Yudong Zhou"
      ],
      "published": "2025-03-26T09:55:00Z",
      "updated": "2025-03-26T09:55:00Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.20376v1",
      "landing_url": "https://arxiv.org/abs/2503.20376v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.20376"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on long-context text embeddings for retrieval (RAG) with no emphasis on spoken language, speech signals, or audio-based modeling, thus it does not meet the Spoken Language Model core criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on long-context text embeddings for retrieval (RAG) with no emphasis on spoken language, speech signals, or audio-based modeling, thus it does not meet the Spoken Language Model core criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a text embedding model for long-context language processing with no mention of spoken language models, speech language modeling, or any speech/acoustic signal processing; the core research is purely on text embeddings and retrieval, which does not meet the inclusion criteria related to spoken language models or full-duplex speech interaction, nor does it address exclusion criteria as it is not related to speech modeling at all.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a text embedding model for long-context language processing with no mention of spoken language models, speech language modeling, or any speech/acoustic signal processing; the core research is purely on text embeddings and retrieval, which does not meet the inclusion criteria related to spoken language models or full-duplex speech interaction, nor does it address exclusion criteria as it is not related to speech modeling at all.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "De Novo Functional Protein Sequence Generation: Overcoming Data Scarcity through Regeneration and Large Models",
    "abstract": "Proteins are essential components of all living organisms and play a critical role in cellular survival. They have a broad range of applications, from clinical treatments to material engineering. This versatility has spurred the development of protein design, with amino acid sequence design being a crucial step in the process. Recent advancements in deep generative models have shown promise for protein sequence design. However, the scarcity of functional protein sequence data for certain types can hinder the training of these models, which often require large datasets. To address this challenge, we propose a hierarchical model named ProteinRG that can generate functional protein sequences using relatively small datasets. ProteinRG begins by generating a representation of a protein sequence, leveraging existing large protein sequence models, before producing a functional protein sequence. We have tested our model on various functional protein sequences and evaluated the results from three perspectives: multiple sequence alignment, t-SNE distribution analysis, and 3D structure prediction. The findings indicate that our generated protein sequences maintain both similarity to the original sequences and consistency with the desired functions. Moreover, our model demonstrates superior performance compared to other generative models for protein sequence generation.",
    "metadata": {
      "arxiv_id": "2503.21123",
      "title": "De Novo Functional Protein Sequence Generation: Overcoming Data Scarcity through Regeneration and Large Models",
      "summary": "Proteins are essential components of all living organisms and play a critical role in cellular survival. They have a broad range of applications, from clinical treatments to material engineering. This versatility has spurred the development of protein design, with amino acid sequence design being a crucial step in the process. Recent advancements in deep generative models have shown promise for protein sequence design. However, the scarcity of functional protein sequence data for certain types can hinder the training of these models, which often require large datasets. To address this challenge, we propose a hierarchical model named ProteinRG that can generate functional protein sequences using relatively small datasets. ProteinRG begins by generating a representation of a protein sequence, leveraging existing large protein sequence models, before producing a functional protein sequence. We have tested our model on various functional protein sequences and evaluated the results from three perspectives: multiple sequence alignment, t-SNE distribution analysis, and 3D structure prediction. The findings indicate that our generated protein sequences maintain both similarity to the original sequences and consistency with the desired functions. Moreover, our model demonstrates superior performance compared to other generative models for protein sequence generation.",
      "authors": [
        "Chenyu Ren",
        "Daihai He",
        "Jian Huang"
      ],
      "published": "2025-03-27T03:25:21Z",
      "updated": "2025-03-27T03:25:21Z",
      "categories": [
        "stat.AP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.21123v1",
      "landing_url": "https://arxiv.org/abs/2503.21123v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.21123"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about de novo functional protein sequence generation using ProteinRG; it does not involve Spoken Language Models, speech modeling, or any full-duplex conversational AI concepts, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about de novo functional protein sequence generation using ProteinRG; it does not involve Spoken Language Models, speech modeling, or any full-duplex conversational AI concepts, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on protein sequence generation using deep generative models and does not involve spoken language models, speech as the core modeling object, or any aspects related to speech language modeling, synchronization in speech interaction, or full-duplex spoken language systems as defined by the inclusion criteria. Therefore, it does not meet the thematic requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on protein sequence generation using deep generative models and does not involve spoken language models, speech as the core modeling object, or any aspects related to speech language modeling, synchronization in speech interaction, or full-duplex spoken language systems as defined by the inclusion criteria. Therefore, it does not meet the thematic requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Make Some Noise: Towards LLM audio reasoning and generation using sound tokens",
    "abstract": "Integrating audio comprehension and generation into large language models (LLMs) remains challenging due to the continuous nature of audio and the resulting high sampling rates. Here, we introduce a novel approach that combines Variational Quantization with Conditional Flow Matching to convert audio into ultra-low bitrate discrete tokens of 0.23kpbs, allowing for seamless integration with text tokens in LLMs. We fine-tuned a pretrained text-based LLM using Low-Rank Adaptation (LoRA) to assess its effectiveness in achieving true multimodal capabilities, i.e., audio comprehension and generation. Our tokenizer outperforms a traditional VQ-VAE across various datasets with diverse acoustic events. Despite the substantial loss of fine-grained details through audio tokenization, our multimodal LLM trained with discrete tokens achieves competitive results in audio comprehension with state-of-the-art methods, though audio generation is poor. Our results highlight the need for larger, more diverse datasets and improved evaluation metrics to advance multimodal LLM performance.",
    "metadata": {
      "arxiv_id": "2503.22275",
      "title": "Make Some Noise: Towards LLM audio reasoning and generation using sound tokens",
      "summary": "Integrating audio comprehension and generation into large language models (LLMs) remains challenging due to the continuous nature of audio and the resulting high sampling rates. Here, we introduce a novel approach that combines Variational Quantization with Conditional Flow Matching to convert audio into ultra-low bitrate discrete tokens of 0.23kpbs, allowing for seamless integration with text tokens in LLMs. We fine-tuned a pretrained text-based LLM using Low-Rank Adaptation (LoRA) to assess its effectiveness in achieving true multimodal capabilities, i.e., audio comprehension and generation. Our tokenizer outperforms a traditional VQ-VAE across various datasets with diverse acoustic events. Despite the substantial loss of fine-grained details through audio tokenization, our multimodal LLM trained with discrete tokens achieves competitive results in audio comprehension with state-of-the-art methods, though audio generation is poor. Our results highlight the need for larger, more diverse datasets and improved evaluation metrics to advance multimodal LLM performance.",
      "authors": [
        "Shivam Mehta",
        "Nebojsa Jojic",
        "Hannes Gamper"
      ],
      "published": "2025-03-28T09:43:47Z",
      "updated": "2025-03-28T09:43:47Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.22275v1",
      "landing_url": "https://arxiv.org/abs/2503.22275v1",
      "doi": "https://doi.org/10.1109/ICASSP49660.2025.10888809"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work treats audio as the main modeling target and integrates audio tokens with an LLM for multimodal speech–language modeling, aligning with the core Spoken Language Model focus and recent research (not a pure ASR/TTS or single-turn task).",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work treats audio as the main modeling target and integrates audio tokens with an LLM for multimodal speech–language modeling, aligning with the core Spoken Language Model focus and recent research (not a pure ASR/TTS or single-turn task).",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on integrating audio comprehension and generation into large language models using audio tokens, which involves language modeling of speech and multimodal LLM architectures; however, it does not explicitly mention spoken language models, full-duplex or synchronous interactive capabilities, nor does it discuss turn-taking, cognitive parallelism, or low-latency interaction, which are key concepts of inclusion. The evaluation is mainly on audio comprehension and generation without emphasis on dialogue behavior arbitration, synchronous interaction, or detailed behavioral dynamics. Thus, while it partially aligns with speech-language modeling themes, it does not fully meet the inclusion criteria related to spoken language model core focus and synchronous interaction features, nor does it hit the exclusion criteria to exclude outright, resulting in uncertainty about inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on integrating audio comprehension and generation into large language models using audio tokens, which involves language modeling of speech and multimodal LLM architectures; however, it does not explicitly mention spoken language models, full-duplex or synchronous interactive capabilities, nor does it discuss turn-taking, cognitive parallelism, or low-latency interaction, which are key concepts of inclusion. The evaluation is mainly on audio comprehension and generation without emphasis on dialogue behavior arbitration, synchronous interaction, or detailed behavioral dynamics. Thus, while it partially aligns with speech-language modeling themes, it does not fully meet the inclusion criteria related to spoken language model core focus and synchronous interaction features, nor does it hit the exclusion criteria to exclude outright, resulting in uncertainty about inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers' notes considered: Junior A rated 5 noting the work treats audio as the main modeling target and integrates audio tokens with an LLM, aligning with SLM themes; Junior B was uncertain (3) because the abstract does not explicitly use the term “spoken language model” nor discuss full‑duplex/synchronous interaction. I judge this paper to be relevant because it clearly makes speech/audio the core modeling object (introducing discrete sound tokens, novel tokenization methods, and fine‑tuning a text LLM for audio comprehension/generation) and discusses model architecture, training, and evaluation — thus addressing Spoken Language Model–style research rather than being a pure ASR/TTS or single-task study — even though it does not explicitly use the exact phrase “spoken language model” and does not tackle full‑duplex interaction. It also does not meet any exclusion criteria (not solely ASR/TTS or only half‑duplex single‑turn task).",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers' notes considered: Junior A rated 5 noting the work treats audio as the main modeling target and integrates audio tokens with an LLM, aligning with SLM themes; Junior B was uncertain (3) because the abstract does not explicitly use the term “spoken language model” nor discuss full‑duplex/synchronous interaction. I judge this paper to be relevant because it clearly makes speech/audio the core modeling object (introducing discrete sound tokens, novel tokenization methods, and fine‑tuning a text LLM for audio comprehension/generation) and discusses model architecture, training, and evaluation — thus addressing Spoken Language Model–style research rather than being a pure ASR/TTS or single-task study — even though it does not explicitly use the exact phrase “spoken language model” and does not tackle full‑duplex interaction. It also does not meet any exclusion criteria (not solely ASR/TTS or only half‑duplex single‑turn task).",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Real-time Video Prediction With Fast Video Interpolation Model and Prediction Training",
    "abstract": "Transmission latency significantly affects users' quality of experience in real-time interaction and actuation. As latency is principally inevitable, video prediction can be utilized to mitigate the latency and ultimately enable zero-latency transmission. However, most of the existing video prediction methods are computationally expensive and impractical for real-time applications. In this work, we therefore propose real-time video prediction towards the zero-latency interaction over networks, called IFRVP (Intermediate Feature Refinement Video Prediction). Firstly, we propose three training methods for video prediction that extend frame interpolation models, where we utilize a simple convolution-only frame interpolation network based on IFRNet. Secondly, we introduce ELAN-based residual blocks into the prediction models to improve both inference speed and accuracy. Our evaluations show that our proposed models perform efficiently and achieve the best trade-off between prediction accuracy and computational speed among the existing video prediction methods. A demonstration movie is also provided at http://bit.ly/IFRVPDemo. The code will be released at https://github.com/FykAikawa/IFRVP.",
    "metadata": {
      "arxiv_id": "2503.23185",
      "title": "Real-time Video Prediction With Fast Video Interpolation Model and Prediction Training",
      "summary": "Transmission latency significantly affects users' quality of experience in real-time interaction and actuation. As latency is principally inevitable, video prediction can be utilized to mitigate the latency and ultimately enable zero-latency transmission. However, most of the existing video prediction methods are computationally expensive and impractical for real-time applications. In this work, we therefore propose real-time video prediction towards the zero-latency interaction over networks, called IFRVP (Intermediate Feature Refinement Video Prediction). Firstly, we propose three training methods for video prediction that extend frame interpolation models, where we utilize a simple convolution-only frame interpolation network based on IFRNet. Secondly, we introduce ELAN-based residual blocks into the prediction models to improve both inference speed and accuracy. Our evaluations show that our proposed models perform efficiently and achieve the best trade-off between prediction accuracy and computational speed among the existing video prediction methods. A demonstration movie is also provided at http://bit.ly/IFRVPDemo. The code will be released at https://github.com/FykAikawa/IFRVP.",
      "authors": [
        "Shota Hirose",
        "Kazuki Kotoyori",
        "Kasidis Arunruangsirilert",
        "Fangzheng Lin",
        "Heming Sun",
        "Jiro Katto"
      ],
      "published": "2025-03-29T18:48:46Z",
      "updated": "2025-04-04T14:29:20Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.23185v2",
      "landing_url": "https://arxiv.org/abs/2503.23185v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.23185"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on real-time video prediction and latency reduction in video transmission, with no mention of Spoken Language Models or speech-language modeling concepts, thus not fitting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on real-time video prediction and latency reduction in video transmission, with no mention of Spoken Language Models or speech-language modeling concepts, thus not fitting the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on real-time video prediction and interpolation models for latency mitigation, which is related to visual data processing rather than speech or spoken language modeling, and does not mention spoken language models, speech language model architectures, or spoken interactions with features like full-duplex communication, cognitive parallelism, or backchanneling; thus, it does not fulfill the inclusion criteria centered on spoken language models and related aspects.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on real-time video prediction and interpolation models for latency mitigation, which is related to visual data processing rather than speech or spoken language modeling, and does not mention spoken language models, speech language model architectures, or spoken interactions with features like full-duplex communication, cognitive parallelism, or backchanneling; thus, it does not fulfill the inclusion criteria centered on spoken language models and related aspects.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Exploring Temporal Dynamics in Event-based Eye Tracker",
    "abstract": "Eye-tracking is a vital technology for human-computer interaction, especially in wearable devices such as AR, VR, and XR. The realization of high-speed and high-precision eye-tracking using frame-based image sensors is constrained by their limited temporal resolution, which impairs the accurate capture of rapid ocular dynamics, such as saccades and blinks. Event cameras, inspired by biological vision systems, are capable of perceiving eye movements with extremely low power consumption and ultra-high temporal resolution. This makes them a promising solution for achieving high-speed, high-precision tracking with rich temporal dynamics. In this paper, we propose TDTracker, an effective eye-tracking framework that captures rapid eye movements by thoroughly modeling temporal dynamics from both implicit and explicit perspectives. TDTracker utilizes 3D convolutional neural networks to capture implicit short-term temporal dynamics and employs a cascaded structure consisting of a Frequency-aware Module, GRU, and Mamba to extract explicit long-term temporal dynamics. Ultimately, a prediction heatmap is used for eye coordinate regression. Experimental results demonstrate that TDTracker achieves state-of-the-art (SOTA) performance on the synthetic SEET dataset and secured Third place in the CVPR event-based eye-tracking challenge 2025. Our code is available at https://github.com/rhwxmx/TDTracker.",
    "metadata": {
      "arxiv_id": "2503.23725",
      "title": "Exploring Temporal Dynamics in Event-based Eye Tracker",
      "summary": "Eye-tracking is a vital technology for human-computer interaction, especially in wearable devices such as AR, VR, and XR. The realization of high-speed and high-precision eye-tracking using frame-based image sensors is constrained by their limited temporal resolution, which impairs the accurate capture of rapid ocular dynamics, such as saccades and blinks. Event cameras, inspired by biological vision systems, are capable of perceiving eye movements with extremely low power consumption and ultra-high temporal resolution. This makes them a promising solution for achieving high-speed, high-precision tracking with rich temporal dynamics. In this paper, we propose TDTracker, an effective eye-tracking framework that captures rapid eye movements by thoroughly modeling temporal dynamics from both implicit and explicit perspectives. TDTracker utilizes 3D convolutional neural networks to capture implicit short-term temporal dynamics and employs a cascaded structure consisting of a Frequency-aware Module, GRU, and Mamba to extract explicit long-term temporal dynamics. Ultimately, a prediction heatmap is used for eye coordinate regression. Experimental results demonstrate that TDTracker achieves state-of-the-art (SOTA) performance on the synthetic SEET dataset and secured Third place in the CVPR event-based eye-tracking challenge 2025. Our code is available at https://github.com/rhwxmx/TDTracker.",
      "authors": [
        "Hongwei Ren",
        "Xiaopeng Lin",
        "Hongxiang Huang",
        "Yue Zhou",
        "Bojun Cheng"
      ],
      "published": "2025-03-31T04:57:13Z",
      "updated": "2025-03-31T04:57:13Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.23725v1",
      "landing_url": "https://arxiv.org/abs/2503.23725v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.23725"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on vision-based eye-tracking with event cameras and temporal dynamics, not on Spoken Language Models or speech-language modeling, so it fails all inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on vision-based eye-tracking with event cameras and temporal dynamics, not on Spoken Language Models or speech-language modeling, so it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on event-based eye tracker technology and temporal dynamics of ocular movements, with no mention of spoken language models, speech language modeling, or related interactive speech processing topics; thus it does not meet the inclusion criteria and is unrelated to the core research theme of SLMs or FD-SLMs.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on event-based eye tracker technology and temporal dynamics of ocular movements, with no mention of spoken language models, speech language modeling, or related interactive speech processing topics; thus it does not meet the inclusion criteria and is unrelated to the core research theme of SLMs or FD-SLMs.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments",
    "abstract": "The era of foundation models has revolutionized AI research, yet Graph Foundation Models (GFMs) remain constrained by the scarcity of large-scale graph corpora. Traditional graph data synthesis techniques primarily focus on simplistic structural operations, lacking the capacity to generate semantically rich nodes with meaningful textual attributes: a critical limitation for real-world applications. While large language models (LLMs) demonstrate exceptional text generation capabilities, their direct application to graph synthesis is impeded by context window limitations, hallucination phenomena, and structural consistency challenges. To address these issues, we introduce GraphMaster, the first multi-agent framework specifically designed for graph data synthesis in data-limited environments. GraphMaster orchestrates four specialized LLM agents (Manager, Perception, Enhancement, and Evaluation) that collaboratively optimize the synthesis process through iterative refinement, ensuring both semantic coherence and structural integrity. To rigorously evaluate our approach, we create new data-limited \"Sub\" variants of six standard graph benchmarks, specifically designed to test synthesis capabilities under realistic constraints. Additionally, we develop a novel interpretability assessment framework that combines human evaluation with a principled Grassmannian manifold-based analysis, providing both qualitative and quantitative measures of semantic coherence. Experimental results demonstrate that GraphMaster significantly outperforms traditional synthesis methods across multiple datasets, establishing a strong foundation for advancing GFMs in data-scarce environments.",
    "metadata": {
      "arxiv_id": "2504.00711",
      "title": "GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments",
      "summary": "The era of foundation models has revolutionized AI research, yet Graph Foundation Models (GFMs) remain constrained by the scarcity of large-scale graph corpora. Traditional graph data synthesis techniques primarily focus on simplistic structural operations, lacking the capacity to generate semantically rich nodes with meaningful textual attributes: a critical limitation for real-world applications. While large language models (LLMs) demonstrate exceptional text generation capabilities, their direct application to graph synthesis is impeded by context window limitations, hallucination phenomena, and structural consistency challenges. To address these issues, we introduce GraphMaster, the first multi-agent framework specifically designed for graph data synthesis in data-limited environments. GraphMaster orchestrates four specialized LLM agents (Manager, Perception, Enhancement, and Evaluation) that collaboratively optimize the synthesis process through iterative refinement, ensuring both semantic coherence and structural integrity. To rigorously evaluate our approach, we create new data-limited \"Sub\" variants of six standard graph benchmarks, specifically designed to test synthesis capabilities under realistic constraints. Additionally, we develop a novel interpretability assessment framework that combines human evaluation with a principled Grassmannian manifold-based analysis, providing both qualitative and quantitative measures of semantic coherence. Experimental results demonstrate that GraphMaster significantly outperforms traditional synthesis methods across multiple datasets, establishing a strong foundation for advancing GFMs in data-scarce environments.",
      "authors": [
        "Enjun Du",
        "Xunkai Li",
        "Tian Jin",
        "Zhihan Zhang",
        "Rong-Hua Li",
        "Guoren Wang"
      ],
      "published": "2025-04-01T12:21:50Z",
      "updated": "2025-05-05T13:57:12Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.00711v2",
      "landing_url": "https://arxiv.org/abs/2504.00711v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.00711"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work targets graph data synthesis with LLMs and multi-agent coordination, not Spoken Language Models or speech-based modeling, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This work targets graph data synthesis with LLMs and multi-agent coordination, not Spoken Language Models or speech-based modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on graph data synthesis using large language model agents in data-limited environments, without any emphasis on spoken language models, speech language modeling, or related full-duplex spoken interaction capabilities. The core modeling target is not speech or spoken language sequences, but graph data synthesis; thus, it does not meet the inclusion criteria centered on speech-based language modeling and related architectures or evaluations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on graph data synthesis using large language model agents in data-limited environments, without any emphasis on spoken language models, speech language modeling, or related full-duplex spoken interaction capabilities. The core modeling target is not speech or spoken language sequences, but graph data synthesis; thus, it does not meet the inclusion criteria centered on speech-based language modeling and related architectures or evaluations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Adapting Knowledge Prompt Tuning for Enhanced Automated Program Repair",
    "abstract": "Automated Program Repair (APR) aims to enhance software reliability by automatically generating bug-fixing patches. Recent work has improved the state-of-the-art of APR by fine-tuning pre-trained large language models (LLMs), such as CodeT5, for APR. However, the effectiveness of fine-tuning becomes weakened in data scarcity scenarios, and data scarcity can be a common issue in practice, limiting fine-tuning performance. To alleviate this limitation, this paper adapts prompt tuning for enhanced APR and conducts a comprehensive study to evaluate its effectiveness in data scarcity scenarios, using three LLMs of different sizes and six diverse datasets across four programming languages. Prompt tuning rewrites the input to a model by adding extra prompt tokens and tunes both the model and the prompts on a small dataset. These tokens provide task-specific knowledge that can improve the model for APR, which is especially critical in data scarcity scenarios. Moreover, domain knowledge has proven crucial in many code intelligence tasks, but existing studies fail to leverage domain knowledge during the prompt tuning for APR. To close this gap, we introduce knowledge prompt tuning, an approach that adapts prompt tuning with six distinct types of code- or bug-related domain knowledge for APR. Our work, to the best of our knowledge, is the first to adapt and evaluate prompt tuning and the effectiveness of code- or bug-related domain knowledge for APR, particularly under data scarcity settings. Our evaluation results demonstrate that prompt tuning with knowledge generally outperforms fine-tuning under various experimental settings, achieving an average improvement of 87.33% over fine-tuning in data scarcity scenarios.",
    "metadata": {
      "arxiv_id": "2504.01523",
      "title": "Adapting Knowledge Prompt Tuning for Enhanced Automated Program Repair",
      "summary": "Automated Program Repair (APR) aims to enhance software reliability by automatically generating bug-fixing patches. Recent work has improved the state-of-the-art of APR by fine-tuning pre-trained large language models (LLMs), such as CodeT5, for APR. However, the effectiveness of fine-tuning becomes weakened in data scarcity scenarios, and data scarcity can be a common issue in practice, limiting fine-tuning performance. To alleviate this limitation, this paper adapts prompt tuning for enhanced APR and conducts a comprehensive study to evaluate its effectiveness in data scarcity scenarios, using three LLMs of different sizes and six diverse datasets across four programming languages. Prompt tuning rewrites the input to a model by adding extra prompt tokens and tunes both the model and the prompts on a small dataset. These tokens provide task-specific knowledge that can improve the model for APR, which is especially critical in data scarcity scenarios. Moreover, domain knowledge has proven crucial in many code intelligence tasks, but existing studies fail to leverage domain knowledge during the prompt tuning for APR. To close this gap, we introduce knowledge prompt tuning, an approach that adapts prompt tuning with six distinct types of code- or bug-related domain knowledge for APR. Our work, to the best of our knowledge, is the first to adapt and evaluate prompt tuning and the effectiveness of code- or bug-related domain knowledge for APR, particularly under data scarcity settings. Our evaluation results demonstrate that prompt tuning with knowledge generally outperforms fine-tuning under various experimental settings, achieving an average improvement of 87.33% over fine-tuning in data scarcity scenarios.",
      "authors": [
        "Xuemeng Cai",
        "Lingxiao Jiang"
      ],
      "published": "2025-04-02T09:10:02Z",
      "updated": "2025-04-02T09:10:02Z",
      "categories": [
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.01523v1",
      "landing_url": "https://arxiv.org/abs/2504.01523v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.01523"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper is about automated program repair and knowledge prompt tuning for code-focused LLMs, with no focus on Spoken Language Models or speech-based modeling; it does not discuss voice as the modeling target or any speech interaction/diagnostics, thus it fails the inclusion criteria and aligns with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper is about automated program repair and knowledge prompt tuning for code-focused LLMs, with no focus on Spoken Language Models or speech-based modeling; it does not discuss voice as the modeling target or any speech interaction/diagnostics, thus it fails the inclusion criteria and aligns with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on automated program repair using prompt tuning of large language models with domain knowledge; it does not address spoken language models, speech modeling, or full-duplex spoken interactions, which are core to the inclusion criteria. There is no mention of speech data, speech-language modeling, or synchronization for spoken dialogue systems.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on automated program repair using prompt tuning of large language models with domain knowledge; it does not address spoken language models, speech modeling, or full-duplex spoken interactions, which are core to the inclusion criteria. There is no mention of speech data, speech-language modeling, or synchronization for spoken dialogue systems.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "VoiceCraft-Dub: Automated Video Dubbing with Neural Codec Language Models",
    "abstract": "We present VoiceCraft-Dub, a novel approach for automated video dubbing that synthesizes high-quality speech from text and facial cues. This task has broad applications in filmmaking, multimedia creation, and assisting voice-impaired individuals. Building on the success of Neural Codec Language Models (NCLMs) for speech synthesis, our method extends their capabilities by incorporating video features, ensuring that synthesized speech is time-synchronized and expressively aligned with facial movements while preserving natural prosody. To inject visual cues, we design adapters to align facial features with the NCLM token space and introduce audio-visual fusion layers to merge audio-visual information within the NCLM framework. Additionally, we curate CelebV-Dub, a new dataset of expressive, real-world videos specifically designed for automated video dubbing. Extensive experiments show that our model achieves high-quality, intelligible, and natural speech synthesis with accurate lip synchronization, outperforming existing methods in human perception and performing favorably in objective evaluations. We also adapt VoiceCraft-Dub for the video-to-speech task, demonstrating its versatility for various applications.",
    "metadata": {
      "arxiv_id": "2504.02386",
      "title": "VoiceCraft-Dub: Automated Video Dubbing with Neural Codec Language Models",
      "summary": "We present VoiceCraft-Dub, a novel approach for automated video dubbing that synthesizes high-quality speech from text and facial cues. This task has broad applications in filmmaking, multimedia creation, and assisting voice-impaired individuals. Building on the success of Neural Codec Language Models (NCLMs) for speech synthesis, our method extends their capabilities by incorporating video features, ensuring that synthesized speech is time-synchronized and expressively aligned with facial movements while preserving natural prosody. To inject visual cues, we design adapters to align facial features with the NCLM token space and introduce audio-visual fusion layers to merge audio-visual information within the NCLM framework. Additionally, we curate CelebV-Dub, a new dataset of expressive, real-world videos specifically designed for automated video dubbing. Extensive experiments show that our model achieves high-quality, intelligible, and natural speech synthesis with accurate lip synchronization, outperforming existing methods in human perception and performing favorably in objective evaluations. We also adapt VoiceCraft-Dub for the video-to-speech task, demonstrating its versatility for various applications.",
      "authors": [
        "Kim Sung-Bin",
        "Jeongsoo Choi",
        "Puyuan Peng",
        "Joon Son Chung",
        "Tae-Hyun Oh",
        "David Harwath"
      ],
      "published": "2025-04-03T08:24:47Z",
      "updated": "2025-04-03T08:24:47Z",
      "categories": [
        "cs.CV",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.02386v1",
      "landing_url": "https://arxiv.org/abs/2504.02386v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.02386"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on spoken language modeling via Neural Codec Language Models for speech synthesis with audio-visual lip synchronization, discusses model architecture and evaluation, introduces a dataset, and is within the 3-year window, while addressing synchronization rather than a single-task ASR/TTS, so it meets all inclusion criteria and avoids exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work centers on spoken language modeling via Neural Codec Language Models for speech synthesis with audio-visual lip synchronization, discusses model architecture and evaluation, introduces a dataset, and is within the 3-year window, while addressing synchronization rather than a single-task ASR/TTS, so it meets all inclusion criteria and avoids exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on automated speech synthesis from text and facial cues for video dubbing, extending Neural Codec Language Models with visual synchronization to improve speech naturalness and lip synchronization; however, it does not explicitly position itself as research on Spoken Language Models (SLMs) or full-duplex spoken interaction models, nor does it address turn-taking, cognitive parallelism, real-time interaction, or behavioral arbitration, which are essential inclusion criteria. It is primarily about speech synthesis quality and audiovisual alignment, rather than speech language modeling or interactive dialogue systems, and does not mention key concepts like end-to-end speech modeling for full-duplex interaction or multi-dimensional evaluation involving cognitive parallelism or low-latency interaction.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on automated speech synthesis from text and facial cues for video dubbing, extending Neural Codec Language Models with visual synchronization to improve speech naturalness and lip synchronization; however, it does not explicitly position itself as research on Spoken Language Models (SLMs) or full-duplex spoken interaction models, nor does it address turn-taking, cognitive parallelism, real-time interaction, or behavioral arbitration, which are essential inclusion criteria. It is primarily about speech synthesis quality and audiovisual alignment, rather than speech language modeling or interactive dialogue systems, and does not mention key concepts like end-to-end speech modeling for full-duplex interaction or multi-dimensional evaluation involving cognitive parallelism or low-latency interaction.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "I recommend excluding this paper: while it leverages Neural Codec Language Models for speech synthesis and proposes adapters and fusion layers for audio-visual dubbing (and one junior reviewer interpreted that as speech-language-model related), the work is essentially a single-task TTS/video-dubbing system focused on speech quality and lip synchronization with a new dataset, and it does not center on Spoken Language Models/FD‑SLMs, nor does it address full‑duplex interaction, cognitive parallelism, overlap management, low‑latency interaction, or behavior arbitration required by the inclusion criteria—therefore it meets exclusion criteria (single-task TTS) despite technical novelty. (One-sentence explanation: Primary focus is TTS/video dubbing, not SLM/FD‑SLM research, so exclude.)",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I recommend excluding this paper: while it leverages Neural Codec Language Models for speech synthesis and proposes adapters and fusion layers for audio-visual dubbing (and one junior reviewer interpreted that as speech-language-model related), the work is essentially a single-task TTS/video-dubbing system focused on speech quality and lip synchronization with a new dataset, and it does not center on Spoken Language Models/FD‑SLMs, nor does it address full‑duplex interaction, cognitive parallelism, overlap management, low‑latency interaction, or behavior arbitration required by the inclusion criteria—therefore it meets exclusion criteria (single-task TTS) despite technical novelty. (One-sentence explanation: Primary focus is TTS/video dubbing, not SLM/FD‑SLM research, so exclude.)",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Electromyography-Based Gesture Recognition: Hierarchical Feature Extraction for Enhanced Spatial-Temporal Dynamics",
    "abstract": "Hand gesture recognition using multichannel surface electromyography (sEMG) is challenging due to unstable predictions and inefficient time-varying feature enhancement. To overcome the lack of signal based time-varying feature problems, we propose a lightweight squeeze-excitation deep learning-based multi stream spatial temporal dynamics time-varying feature extraction approach to build an effective sEMG-based hand gesture recognition system. Each branch of the proposed model was designed to extract hierarchical features, capturing both global and detailed spatial-temporal relationships to ensure feature effectiveness. The first branch, utilizing a Bidirectional-TCN (Bi-TCN), focuses on capturing long-term temporal dependencies by modelling past and future temporal contexts, providing a holistic view of gesture dynamics. The second branch, incorporating a 1D Convolutional layer, separable CNN, and Squeeze-and-Excitation (SE) block, efficiently extracts spatial-temporal features while emphasizing critical feature channels, enhancing feature relevance. The third branch, combining a Temporal Convolutional Network (TCN) and Bidirectional LSTM (BiLSTM), captures bidirectional temporal relationships and time-varying patterns. Outputs from all branches are fused using concatenation to capture subtle variations in the data and then refined with a channel attention module, selectively focusing on the most informative features while improving computational efficiency. The proposed model was tested on the Ninapro DB2, DB4, and DB5 datasets, achieving accuracy rates of 96.41%, 92.40%, and 93.34%, respectively. These results demonstrate the capability of the system to handle complex sEMG dynamics, offering advancements in prosthetic limb control and human-machine interface technologies with significant implications for assistive technologies.",
    "metadata": {
      "arxiv_id": "2504.03221",
      "title": "Electromyography-Based Gesture Recognition: Hierarchical Feature Extraction for Enhanced Spatial-Temporal Dynamics",
      "summary": "Hand gesture recognition using multichannel surface electromyography (sEMG) is challenging due to unstable predictions and inefficient time-varying feature enhancement. To overcome the lack of signal based time-varying feature problems, we propose a lightweight squeeze-excitation deep learning-based multi stream spatial temporal dynamics time-varying feature extraction approach to build an effective sEMG-based hand gesture recognition system. Each branch of the proposed model was designed to extract hierarchical features, capturing both global and detailed spatial-temporal relationships to ensure feature effectiveness. The first branch, utilizing a Bidirectional-TCN (Bi-TCN), focuses on capturing long-term temporal dependencies by modelling past and future temporal contexts, providing a holistic view of gesture dynamics. The second branch, incorporating a 1D Convolutional layer, separable CNN, and Squeeze-and-Excitation (SE) block, efficiently extracts spatial-temporal features while emphasizing critical feature channels, enhancing feature relevance. The third branch, combining a Temporal Convolutional Network (TCN) and Bidirectional LSTM (BiLSTM), captures bidirectional temporal relationships and time-varying patterns. Outputs from all branches are fused using concatenation to capture subtle variations in the data and then refined with a channel attention module, selectively focusing on the most informative features while improving computational efficiency. The proposed model was tested on the Ninapro DB2, DB4, and DB5 datasets, achieving accuracy rates of 96.41%, 92.40%, and 93.34%, respectively. These results demonstrate the capability of the system to handle complex sEMG dynamics, offering advancements in prosthetic limb control and human-machine interface technologies with significant implications for assistive technologies.",
      "authors": [
        "Jungpil Shin",
        "Abu Saleh Musa Miah",
        "Sota Konnai",
        "Shu Hoshitaka",
        "Pankoo Kim"
      ],
      "published": "2025-04-04T07:11:12Z",
      "updated": "2025-04-04T07:11:12Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.03221v1",
      "landing_url": "https://arxiv.org/abs/2504.03221v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.03221"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about electromyography-based gesture recognition with deep learning architectures, not about Spoken Language Models or speech-language modeling, thus it fails the core topic and timeframe criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about electromyography-based gesture recognition with deep learning architectures, not about Spoken Language Models or speech-language modeling, thus it fails the core topic and timeframe criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on hand gesture recognition using electromyography signals and deep learning models, not on Spoken Language Models (SLMs) or speech language modeling. It does not address spoken language, full-duplex interaction, synchronous turn-taking, or related aspects of speech-based language modeling. Therefore, it does not meet any of the inclusion criteria centered on spoken language models and full-duplex spoken language modeling, nor does it meet exclusion criteria based on domain relevance, but it is outside the scope of the requested topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on hand gesture recognition using electromyography signals and deep learning models, not on Spoken Language Models (SLMs) or speech language modeling. It does not address spoken language, full-duplex interaction, synchronous turn-taking, or related aspects of speech-based language modeling. Therefore, it does not meet any of the inclusion criteria centered on spoken language models and full-duplex spoken language modeling, nor does it meet exclusion criteria based on domain relevance, but it is outside the scope of the requested topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "CAMINO: Cloud-native Autonomous Management and Intent-based Orchestrator",
    "abstract": "This paper introduces CAMINO, a Cloud-native Autonomous Management and Intent-based Orchestrator designed to address the challenges of scalable, declarative, and cloud-native service management and orchestration. CAMINO leverages a modular architecture, the Configuration-as-Data (CaD) paradigm, and real-time resource monitoring to facilitate zero-touch provisioning across multi-edge infrastructure. By incorporating intent-driven orchestration and observability capabilities, CAMINO enables automated lifecycle management of network functions, ensuring optimized resource utilisation. The proposed solution abstracts complex configurations into high-level intents, offering a scalable approach to orchestrating services in distributed cloud-native infrastructures. This paper details CAMINO's system architecture, implementation, and key benefits, highlighting its effectiveness in cloud-native telecommunications environments.",
    "metadata": {
      "arxiv_id": "2504.03586",
      "title": "CAMINO: Cloud-native Autonomous Management and Intent-based Orchestrator",
      "summary": "This paper introduces CAMINO, a Cloud-native Autonomous Management and Intent-based Orchestrator designed to address the challenges of scalable, declarative, and cloud-native service management and orchestration. CAMINO leverages a modular architecture, the Configuration-as-Data (CaD) paradigm, and real-time resource monitoring to facilitate zero-touch provisioning across multi-edge infrastructure. By incorporating intent-driven orchestration and observability capabilities, CAMINO enables automated lifecycle management of network functions, ensuring optimized resource utilisation. The proposed solution abstracts complex configurations into high-level intents, offering a scalable approach to orchestrating services in distributed cloud-native infrastructures. This paper details CAMINO's system architecture, implementation, and key benefits, highlighting its effectiveness in cloud-native telecommunications environments.",
      "authors": [
        "Konstantinos Antonakoglou",
        "Ioannis Mavromatis",
        "Saptarshi Ghosh",
        "Mark Rouse",
        "Konstantinos Katsaros"
      ],
      "published": "2025-04-04T16:55:23Z",
      "updated": "2025-04-04T16:55:23Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.03586v1",
      "landing_url": "https://arxiv.org/abs/2504.03586v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.03586"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "CAMINO targets cloud-native management/orchestration for network services; the abstract has no discussion of Spoken Language Models, speech processing, or language modeling of speech, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "CAMINO targets cloud-native management/orchestration for network services; the abstract has no discussion of Spoken Language Models, speech processing, or language modeling of speech, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on cloud-native orchestration and management of network functions in telecommunications, without any indication of spoken language models, speech-level language modeling, or full-duplex spoken interaction capabilities; it does not align with the core inclusion criteria centered on Speech Language Models or FD-SLMs, nor does it mention relevant architectures, modeling, or evaluations in the speech domain.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on cloud-native orchestration and management of network functions in telecommunications, without any indication of spoken language models, speech-level language modeling, or full-duplex spoken interaction capabilities; it does not align with the core inclusion criteria centered on Speech Language Models or FD-SLMs, nor does it mention relevant architectures, modeling, or evaluations in the speech domain.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "TangibleNet: Synchronous Network Data Storytelling through Tangible Interactions in Augmented Reality",
    "abstract": "Synchronous data-driven storytelling with network visualizations presents significant challenges due to the complexity of real-time manipulation of network components. While existing research addresses asynchronous scenarios, there is a lack of effective tools for live presentations. To address this gap, we developed TangibleNet, a projector-based AR prototype that allows presenters to interact with node-link diagrams using double-sided magnets during live presentations. The design process was informed by interviews with professionals experienced in synchronous data storytelling and workshops with 14 HCI/VIS researchers. Insights from the interviews helped identify key design considerations for integrating physical objects as interactive tools in presentation contexts. The workshops contributed to the development of a design space mapping user actions to interaction commands for node-link diagrams. Evaluation with 12 participants confirmed that TangibleNet supports intuitive interactions and enhances presenter autonomy, demonstrating its effectiveness for synchronous network-based data storytelling.",
    "metadata": {
      "arxiv_id": "2504.04710",
      "title": "TangibleNet: Synchronous Network Data Storytelling through Tangible Interactions in Augmented Reality",
      "summary": "Synchronous data-driven storytelling with network visualizations presents significant challenges due to the complexity of real-time manipulation of network components. While existing research addresses asynchronous scenarios, there is a lack of effective tools for live presentations. To address this gap, we developed TangibleNet, a projector-based AR prototype that allows presenters to interact with node-link diagrams using double-sided magnets during live presentations. The design process was informed by interviews with professionals experienced in synchronous data storytelling and workshops with 14 HCI/VIS researchers. Insights from the interviews helped identify key design considerations for integrating physical objects as interactive tools in presentation contexts. The workshops contributed to the development of a design space mapping user actions to interaction commands for node-link diagrams. Evaluation with 12 participants confirmed that TangibleNet supports intuitive interactions and enhances presenter autonomy, demonstrating its effectiveness for synchronous network-based data storytelling.",
      "authors": [
        "Kentaro Takahira",
        "Wong Kam-Kwai",
        "Leni Yang",
        "Xian Xu",
        "Takanori Fujiwara",
        "Huamin Qu"
      ],
      "published": "2025-04-07T03:46:46Z",
      "updated": "2025-04-07T03:46:46Z",
      "categories": [
        "cs.HC",
        "cs.GR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.04710v1",
      "landing_url": "https://arxiv.org/abs/2504.04710v1",
      "doi": "https://doi.org/10.1145/3706598.3714265"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on a tangible AR system for live data storytelling and interaction, not on Spoken Language Models or speech-language modeling, thus fails the core inclusion criteria; despite being recent, it does not address speech synchronization, overlap handling, or linguistic modeling concepts required for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on a tangible AR system for live data storytelling and interaction, not on Spoken Language Models or speech-language modeling, thus fails the core inclusion criteria; despite being recent, it does not address speech synchronization, overlap handling, or linguistic modeling concepts required for inclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article TangibleNet focuses on augmented reality interactions for synchronous data storytelling via network visualizations, but it does not address spoken language models, speech language modeling, full-duplex spoken language interaction, or related speech processing architectures and evaluations. It mainly concerns network visualization technology rather than speech or language model research, thus it does not meet the inclusion criteria and falls under exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article TangibleNet focuses on augmented reality interactions for synchronous data storytelling via network visualizations, but it does not address spoken language models, speech language modeling, full-duplex spoken language interaction, or related speech processing architectures and evaluations. It mainly concerns network visualization technology rather than speech or language model research, thus it does not meet the inclusion criteria and falls under exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LLM-Alignment Live-Streaming Recommendation",
    "abstract": "In recent years, integrated short-video and live-streaming platforms have gained massive global adoption, offering dynamic content creation and consumption. Unlike pre-recorded short videos, live-streaming enables real-time interaction between authors and users, fostering deeper engagement. However, this dynamic nature introduces a critical challenge for recommendation systems (RecSys): the same live-streaming vastly different experiences depending on when a user watching. To optimize recommendations, a RecSys must accurately interpret the real-time semantics of live content and align them with user preferences.",
    "metadata": {
      "arxiv_id": "2504.05217",
      "title": "LLM-Alignment Live-Streaming Recommendation",
      "summary": "In recent years, integrated short-video and live-streaming platforms have gained massive global adoption, offering dynamic content creation and consumption. Unlike pre-recorded short videos, live-streaming enables real-time interaction between authors and users, fostering deeper engagement. However, this dynamic nature introduces a critical challenge for recommendation systems (RecSys): the same live-streaming vastly different experiences depending on when a user watching. To optimize recommendations, a RecSys must accurately interpret the real-time semantics of live content and align them with user preferences.",
      "authors": [
        "Yueyang Liu",
        "Jiangxia Cao",
        "Shen Wang",
        "Shuang Wen",
        "Xiang Chen",
        "Xiangyu Wu",
        "Shuang Yang",
        "Zhaojie Liu",
        "Kun Gai",
        "Guorui Zhou"
      ],
      "published": "2025-04-07T16:04:00Z",
      "updated": "2025-04-07T16:04:00Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.05217v1",
      "landing_url": "https://arxiv.org/abs/2504.05217v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.05217"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本論文主題聚焦於直播推薦系統中與實時語義理解之對齊，並未以 Spoken Language Models（SLMs）或全雙工語音模型（FD-SLMs）為核心建模推理，因此不符合納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本論文主題聚焦於直播推薦系統中與實時語義理解之對齊，並未以 Spoken Language Models（SLMs）或全雙工語音模型（FD-SLMs）為核心建模推理，因此不符合納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on recommendation systems for live-streaming platforms and real-time interaction, with no mention of Spoken Language Models, speech-based language modeling, full-duplex interaction capabilities, or evaluation of speech-specific interaction metrics that align with the inclusion criteria. The study appears unrelated to speech modeling or synchronous spoken dialogue systems as required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on recommendation systems for live-streaming platforms and real-time interaction, with no mention of Spoken Language Models, speech-based language modeling, full-duplex interaction capabilities, or evaluation of speech-specific interaction metrics that align with the inclusion criteria. The study appears unrelated to speech modeling or synchronous spoken dialogue systems as required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Accessible and Pedagogically-Grounded Explainability for Human-Robot Interaction: A Framework Based on UDL and Symbolic Interfaces",
    "abstract": "This paper presents a novel framework for accessible and pedagogically-grounded robot explainability, designed to support human-robot interaction (HRI) with users who have diverse cognitive, communicative, or learning needs. We combine principles from Universal Design for Learning (UDL) and Universal Design (UD) with symbolic communication strategies to facilitate the alignment of mental models between humans and robots. Our approach employs Asterics Grid and ARASAAC pictograms as a multimodal, interpretable front-end, integrated with a lightweight HTTP-to-ROS 2 bridge that enables real-time interaction and explanation triggering. We emphasize that explainability is not a one-way function but a bidirectional process, where human understanding and robot transparency must co-evolve. We further argue that in educational or assistive contexts, the role of a human mediator (e.g., a teacher) may be essential to support shared understanding. We validate our framework with examples of multimodal explanation boards and discuss how it can be extended to different scenarios in education, assistive robotics, and inclusive AI.",
    "metadata": {
      "arxiv_id": "2504.06189",
      "title": "Accessible and Pedagogically-Grounded Explainability for Human-Robot Interaction: A Framework Based on UDL and Symbolic Interfaces",
      "summary": "This paper presents a novel framework for accessible and pedagogically-grounded robot explainability, designed to support human-robot interaction (HRI) with users who have diverse cognitive, communicative, or learning needs. We combine principles from Universal Design for Learning (UDL) and Universal Design (UD) with symbolic communication strategies to facilitate the alignment of mental models between humans and robots. Our approach employs Asterics Grid and ARASAAC pictograms as a multimodal, interpretable front-end, integrated with a lightweight HTTP-to-ROS 2 bridge that enables real-time interaction and explanation triggering. We emphasize that explainability is not a one-way function but a bidirectional process, where human understanding and robot transparency must co-evolve. We further argue that in educational or assistive contexts, the role of a human mediator (e.g., a teacher) may be essential to support shared understanding. We validate our framework with examples of multimodal explanation boards and discuss how it can be extended to different scenarios in education, assistive robotics, and inclusive AI.",
      "authors": [
        "Francisco J. Rodríguez Lera",
        "Raquel Fernández Hernández",
        "Sonia Lopez González",
        "Miguel Angel González-Santamarta",
        "Francisco Jesús Rodríguez Sedano",
        "Camino Fernandez Llamas"
      ],
      "published": "2025-04-08T16:33:52Z",
      "updated": "2025-04-08T16:33:52Z",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.06189v1",
      "landing_url": "https://arxiv.org/abs/2504.06189v1",
      "doi": "https://doi.org/10.1109/RO-MAN63969.2025.11217773"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item addresses accessible explainability for human-robot interaction using visual/pictogram interfaces and pedagogy rather than focusing on Spoken Language Models or speech-language modeling, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item addresses accessible explainability for human-robot interaction using visual/pictogram interfaces and pedagogy rather than focusing on Spoken Language Models or speech-language modeling, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on accessible robot explainability in human-robot interaction using symbolic communication and universal design principles, without explicit mention or focus on spoken language models, speech-level language modeling, full-duplex spoken interaction capabilities, or related speech modeling architectures or evaluations as required by the inclusion criteria. It does not address spoken language modeling or speech-based end-to-end modeling nor discusses relevant engineering or learned synchronization for real-time speech interaction or evaluation metrics aligned with spoken language models. Thus, it falls outside the specified inclusion scope focused on spoken language models and related technologies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on accessible robot explainability in human-robot interaction using symbolic communication and universal design principles, without explicit mention or focus on spoken language models, speech-level language modeling, full-duplex spoken interaction capabilities, or related speech modeling architectures or evaluations as required by the inclusion criteria. It does not address spoken language modeling or speech-based end-to-end modeling nor discusses relevant engineering or learned synchronization for real-time speech interaction or evaluation metrics aligned with spoken language models. Thus, it falls outside the specified inclusion scope focused on spoken language models and related technologies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Temporal dynamics of GHz acoustic waves in chipscale phononic integrated circuits",
    "abstract": "Phononic integrated circuits, which manipulate GHz-frequency acoustic fields in μm-scale waveguides, provide new degrees of freedom for routing and manipulation of microwaves in deeply sub-wavelength geometries with associated implications for chipscale sensing and signal processing. The combination of low propagation loss, long interaction lengths and slow speed of sound put together with the large measurement bandwidths and high frequency resolution available from modern vector network analyzers (VNA) makes it feasible to visualize the temporal dynamics of propagating acoustic fields in these devices and see the device in action. Two representative examples we discuss here are pulse circulation and ringdown in an acoustic microring resonator, and the observation of (parasitic) multipath interference effects in waveguide resonator geometries. In the absence of fast 3D acoustic field imaging modalities, such time domain reflectometry based methods provide the best alternative for mapping interface reflection and loss, which becomes increasingly critical as these devices start to scale in complexity.",
    "metadata": {
      "arxiv_id": "2504.06959",
      "title": "Temporal dynamics of GHz acoustic waves in chipscale phononic integrated circuits",
      "summary": "Phononic integrated circuits, which manipulate GHz-frequency acoustic fields in μm-scale waveguides, provide new degrees of freedom for routing and manipulation of microwaves in deeply sub-wavelength geometries with associated implications for chipscale sensing and signal processing. The combination of low propagation loss, long interaction lengths and slow speed of sound put together with the large measurement bandwidths and high frequency resolution available from modern vector network analyzers (VNA) makes it feasible to visualize the temporal dynamics of propagating acoustic fields in these devices and see the device in action. Two representative examples we discuss here are pulse circulation and ringdown in an acoustic microring resonator, and the observation of (parasitic) multipath interference effects in waveguide resonator geometries. In the absence of fast 3D acoustic field imaging modalities, such time domain reflectometry based methods provide the best alternative for mapping interface reflection and loss, which becomes increasingly critical as these devices start to scale in complexity.",
      "authors": [
        "A. Fahad Malik",
        "Mahmut Bicer",
        "Krishna C. Balram"
      ],
      "published": "2025-04-09T15:09:58Z",
      "updated": "2025-04-09T15:09:58Z",
      "categories": [
        "physics.app-ph",
        "physics.optics"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.06959v1",
      "landing_url": "https://arxiv.org/abs/2504.06959v1",
      "doi": "https://doi.org/10.1109/JMEMS.2025.3593384"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article discusses GHz acoustic wave dynamics in chip-scale phononic integrated circuits, which is not about Spoken Language Models or any speech-language modeling concepts, thus it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article discusses GHz acoustic wave dynamics in chip-scale phononic integrated circuits, which is not about Spoken Language Models or any speech-language modeling concepts, thus it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on GHz acoustic waves in phononic integrated circuits, with no mention of Spoken Language Models, speech language modeling, full-duplex interaction, or any speech modeling architecture or evaluation, which are required by the inclusion criteria. Therefore, it does not meet the inclusion criteria relevant to speech processing or language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on GHz acoustic waves in phononic integrated circuits, with no mention of Spoken Language Models, speech language modeling, full-duplex interaction, or any speech modeling architecture or evaluation, which are required by the inclusion criteria. Therefore, it does not meet the inclusion criteria relevant to speech processing or language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling",
    "abstract": "Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io.",
    "metadata": {
      "arxiv_id": "2504.07053",
      "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling",
      "summary": "Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io.",
      "authors": [
        "Liang-Hsuan Tseng",
        "Yi-Chang Chen",
        "Kuan-Yi Lee",
        "Da-Shan Shiu",
        "Hung-yi Lee"
      ],
      "published": "2025-04-09T17:14:33Z",
      "updated": "2025-05-22T14:49:03Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.07053v2",
      "landing_url": "https://arxiv.org/abs/2504.07053v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.07053"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: It directly addresses Spoken Language Models with a joint speech-text modeling approach and explicit focus on speech-language modeling architecture/training/evaluation, within the last three years, so it meets inclusion and not exclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "Reasoning: It directly addresses Spoken Language Models with a joint speech-text modeling approach and explicit focus on speech-language modeling architecture/training/evaluation, within the last three years, so it meets inclusion and not exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study centers on Spoken Language Models (SLMs) that jointly model speech and text, directly aligning speech tokens with text transcription for spoken language modeling. It introduces a novel method (TASTE) that addresses speech tokenization and embedding for speech-based language modeling rather than just text-based NLP, aligns with the inclusion criteria of covering SLMs with speech as the primary modeling object, including architecture, training, and evaluation. The abstract mentions evaluation beyond simple ASR or TTS metrics, including subjective and objective evaluations related to speech continuation. While the abstract does not explicitly mention full-duplex, cognitive parallelism, or low latency, it fits the broader scope of SLM research with speech-text joint modeling and relevant model and evaluation components. The publication timing is indicated by the arXiv link, which is recent (2025). Thus, it meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study centers on Spoken Language Models (SLMs) that jointly model speech and text, directly aligning speech tokens with text transcription for spoken language modeling. It introduces a novel method (TASTE) that addresses speech tokenization and embedding for speech-based language modeling rather than just text-based NLP, aligns with the inclusion criteria of covering SLMs with speech as the primary modeling object, including architecture, training, and evaluation. The abstract mentions evaluation beyond simple ASR or TTS metrics, including subjective and objective evaluations related to speech continuation. While the abstract does not explicitly mention full-duplex, cognitive parallelism, or low latency, it fits the broader scope of SLM research with speech-text joint modeling and relevant model and evaluation components. The publication timing is indicated by the arXiv link, which is recent (2025). Thus, it meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Reward Generation via Large Vision-Language Model in Offline Reinforcement Learning",
    "abstract": "In offline reinforcement learning (RL), learning from fixed datasets presents a promising solution for domains where real-time interaction with the environment is expensive or risky. However, designing dense reward signals for offline dataset requires significant human effort and domain expertise. Reinforcement learning with human feedback (RLHF) has emerged as an alternative, but it remains costly due to the human-in-the-loop process, prompting interest in automated reward generation models. To address this, we propose Reward Generation via Large Vision-Language Models (RG-VLM), which leverages the reasoning capabilities of LVLMs to generate rewards from offline data without human involvement. RG-VLM improves generalization in long-horizon tasks and can be seamlessly integrated with the sparse reward signals to enhance task performance, demonstrating its potential as an auxiliary reward signal.",
    "metadata": {
      "arxiv_id": "2504.08772",
      "title": "Reward Generation via Large Vision-Language Model in Offline Reinforcement Learning",
      "summary": "In offline reinforcement learning (RL), learning from fixed datasets presents a promising solution for domains where real-time interaction with the environment is expensive or risky. However, designing dense reward signals for offline dataset requires significant human effort and domain expertise. Reinforcement learning with human feedback (RLHF) has emerged as an alternative, but it remains costly due to the human-in-the-loop process, prompting interest in automated reward generation models. To address this, we propose Reward Generation via Large Vision-Language Models (RG-VLM), which leverages the reasoning capabilities of LVLMs to generate rewards from offline data without human involvement. RG-VLM improves generalization in long-horizon tasks and can be seamlessly integrated with the sparse reward signals to enhance task performance, demonstrating its potential as an auxiliary reward signal.",
      "authors": [
        "Younghwan Lee",
        "Tung M. Luu",
        "Donghoon Lee",
        "Chang D. Yoo"
      ],
      "published": "2025-04-03T07:11:18Z",
      "updated": "2025-04-03T07:11:18Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.08772v1",
      "landing_url": "https://arxiv.org/abs/2504.08772v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.08772"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item targets Vision-Language models for reward generation in offline RL, not Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item targets Vision-Language models for reward generation in offline RL, not Spoken Language Models or speech-language modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on offline reinforcement learning using large vision-language models to generate rewards without human involvement, with no mention of spoken language models, speech language modeling, or any of the core topics such as full-duplex spoken language interaction, speech-level modeling, or synchronous speech interaction. It does not meet the inclusion criteria centered on spoken language modeling and synchronous speech interactions, nor does it address the domain-specific modeling, training, or evaluation requirements related to speech. Therefore, this paper falls outside the specified research scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on offline reinforcement learning using large vision-language models to generate rewards without human involvement, with no mention of spoken language models, speech language modeling, or any of the core topics such as full-duplex spoken language interaction, speech-level modeling, or synchronous speech interaction. It does not meet the inclusion criteria centered on spoken language modeling and synchronous speech interactions, nor does it address the domain-specific modeling, training, or evaluation requirements related to speech. Therefore, this paper falls outside the specified research scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "AUTONAV: A Toolfor Autonomous Navigation of Robots",
    "abstract": "We present a tool AUTONAV that automates the mapping, localization, and path-planning tasks for autonomous navigation of robots. The modular architecture allows easy integration of various algorithms for these tasks for comparison. We present the generated maps and path-plans by AUTONAV in indoor simulation scenarios.",
    "metadata": {
      "arxiv_id": "2504.12318",
      "title": "AUTONAV: A Toolfor Autonomous Navigation of Robots",
      "summary": "We present a tool AUTONAV that automates the mapping, localization, and path-planning tasks for autonomous navigation of robots. The modular architecture allows easy integration of various algorithms for these tasks for comparison. We present the generated maps and path-plans by AUTONAV in indoor simulation scenarios.",
      "authors": [
        "Mir Md Sajid Sarwar",
        "Sudip Samanta",
        "Rajarshi Ray"
      ],
      "published": "2025-04-10T16:37:30Z",
      "updated": "2025-04-10T16:37:30Z",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.12318v1",
      "landing_url": "https://arxiv.org/abs/2504.12318v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.12318"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item concentrates on autonomous robot navigation (mapping, localization, path planning) without any emphasis on spoken language models or speech-language modeling, so it does not meet the inclusion criteria and does not trigger the exclusion criteria related to speech-language processing.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item concentrates on autonomous robot navigation (mapping, localization, path planning) without any emphasis on spoken language models or speech-language modeling, so it does not meet the inclusion criteria and does not trigger the exclusion criteria related to speech-language processing.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a tool for autonomous navigation of robots, which involves mapping, localization, and path-planning, with no mention of spoken language models, speech language modeling, or relevant concepts such as full-duplex spoken language models or speech-based model architectures. It does not meet any of the inclusion criteria related to spoken language modeling or speech-based interactive systems, and thus should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a tool for autonomous navigation of robots, which involves mapping, localization, and path-planning, with no mention of spoken language models, speech language modeling, or relevant concepts such as full-duplex spoken language models or speech-based model architectures. It does not meet any of the inclusion criteria related to spoken language modeling or speech-based interactive systems, and thus should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": null
  }
]