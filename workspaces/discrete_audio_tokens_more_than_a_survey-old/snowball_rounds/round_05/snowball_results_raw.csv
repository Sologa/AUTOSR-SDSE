openalex_id,doi,title,abstract,referenced_works,publication_date
https://openalex.org/W3166440012,https://doi.org/10.48550/arxiv.2106.05933,"PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition","Self-supervised speech representation learning (speech SSL) has demonstrated the benefit of scale in learning rich representations for Automatic Speech Recognition (ASR) with limited paired data, such as wav2vec 2.0. We investigate the existence of sparse subnetworks in pre-trained speech SSL models that achieve even better low-resource ASR results. However, directly applying widely adopted pruning methods such as the Lottery Ticket Hypothesis (LTH) is suboptimal in the computational cost needed. Moreover, we show that the discovered subnetworks yield minimal performance gain compared to the original dense network. We present Prune-Adjust-Re-Prune (PARP), which discovers and finetunes subnetworks for much better performance, while only requiring a single downstream ASR finetuning run. PARP is inspired by our surprising observation that subnetworks pruned for pre-training tasks need merely a slight adjustment to achieve a sizeable performance boost in downstream ASR tasks. Extensive experiments on low-resource ASR verify (1) sparse subnetworks exist in mono-lingual/multi-lingual pre-trained speech SSL, and (2) the computational advantage and performance gain of PARP over baseline pruning methods. In particular, on the 10min Librispeech split without LM decoding, PARP discovers subnetworks from wav2vec 2.0 with an absolute 10.9%/12.6% WER decrease compared to the full model. We further demonstrate the effectiveness of PARP via: cross-lingual pruning without any phone recognition degradation, the discovery of a multi-lingual subnetwork for 10 spoken languages in 1 finetuning run, and its applicability to pre-trained BERT/XLNet for natural language tasks.","['https://openalex.org/W3163842642', 'https://openalex.org/W3143035657', 'https://openalex.org/W3160399536', 'https://openalex.org/W3148001440', 'https://openalex.org/W2802201485', 'https://openalex.org/W2982223350', 'https://openalex.org/W2963340922', 'https://openalex.org/W3025165719', 'https://openalex.org/W2971237698', 'https://openalex.org/W2294543795', 'https://openalex.org/W3093579165', 'https://openalex.org/W3098680936', 'https://openalex.org/W3099782249', 'https://openalex.org/W3162665866', 'https://openalex.org/W3178203035', 'https://openalex.org/W3198217962', 'https://openalex.org/W3160405885', 'https://openalex.org/W3035615218', 'https://openalex.org/W1974205368', 'https://openalex.org/W3205032693', 'https://openalex.org/W3112034174', 'https://openalex.org/W2996383576', 'https://openalex.org/W2962965870', 'https://openalex.org/W2963813662', 'https://openalex.org/W3162391496', 'https://openalex.org/W2768501777', 'https://openalex.org/W3163600291', 'https://openalex.org/W3207300132', 'https://openalex.org/W2948130861', 'https://openalex.org/W2608554408', 'https://openalex.org/W2730658205', 'https://openalex.org/W3152519008', 'https://openalex.org/W3021469861', 'https://openalex.org/W3178584664', 'https://openalex.org/W3202278141', 'https://openalex.org/W3147962056', 'https://openalex.org/W2930682606', 'https://openalex.org/W3161223924', 'https://openalex.org/W3157923770', 'https://openalex.org/W3204123830', 'https://openalex.org/W3205533980', 'https://openalex.org/W3104263050', 'https://openalex.org/W2963310665', 'https://openalex.org/W3104136798', 'https://openalex.org/W2975044525', 'https://openalex.org/W3180374548', 'https://openalex.org/W2963981420', 'https://openalex.org/W2972808286', 'https://openalex.org/W2114766824', 'https://openalex.org/W3129009457', 'https://openalex.org/W2972943112', 'https://openalex.org/W3206559778', 'https://openalex.org/W2963674932', 'https://openalex.org/W3040454670', 'https://openalex.org/W3034487470', 'https://openalex.org/W2043422002', 'https://openalex.org/W2888867175', 'https://openalex.org/W3204696009', 'https://openalex.org/W2084910356', 'https://openalex.org/W2125389748', 'https://openalex.org/W3111265704', 'https://openalex.org/W2965862774', 'https://openalex.org/W3167207712', 'https://openalex.org/W3037057938', 'https://openalex.org/W2407115099', 'https://openalex.org/W3197974236', 'https://openalex.org/W3081179222', 'https://openalex.org/W3003875258', 'https://openalex.org/W3166035876', 'https://openalex.org/W3035081900', 'https://openalex.org/W104222852', 'https://openalex.org/W3165666670', 'https://openalex.org/W2963247446', 'https://openalex.org/W3206252155', 'https://openalex.org/W2951569836', 'https://openalex.org/W3144173820', 'https://openalex.org/W2894835365', 'https://openalex.org/W3094550259', 'https://openalex.org/W3162309234', 'https://openalex.org/W3204224625', 'https://openalex.org/W1515156256', 'https://openalex.org/W3104350794', 'https://openalex.org/W3095292526', 'https://openalex.org/W2936481169', 'https://openalex.org/W3193521535', 'https://openalex.org/W3207558756', 'https://openalex.org/W3162649911', 'https://openalex.org/W2963400886', 'https://openalex.org/W3173970713', 'https://openalex.org/W3169320628', 'https://openalex.org/W2988736778', 'https://openalex.org/W3197278374', 'https://openalex.org/W2112984492', 'https://openalex.org/W3140429000', 'https://openalex.org/W3147414526', 'https://openalex.org/W3152884768', 'https://openalex.org/W3038041907', 'https://openalex.org/W3111921445', 'https://openalex.org/W3128478537', 'https://openalex.org/W3160525311', 'https://openalex.org/W3157697407', 'https://openalex.org/W2962813140', 'https://openalex.org/W3041561163', 'https://openalex.org/W3205644108', 'https://openalex.org/W2963503967', 'https://openalex.org/W3001899777', 'https://openalex.org/W2915589364', 'https://openalex.org/W3034234149', 'https://openalex.org/W3022969335', 'https://openalex.org/W2842511635', 'https://openalex.org/W2156700117', 'https://openalex.org/W2291975472', 'https://openalex.org/W3139918052', 'https://openalex.org/W3205710300', 'https://openalex.org/W3087835661', 'https://openalex.org/W2995816250', 'https://openalex.org/W3162868000', 'https://openalex.org/W3024171804', 'https://openalex.org/W3162133897', 'https://openalex.org/W2970120757', 'https://openalex.org/W3096587983', 'https://openalex.org/W3016181583', 'https://openalex.org/W2292087804', 'https://openalex.org/W3093346109']",2021-06-10
https://openalex.org/W3206649123,,Deep Clustering For General-Purpose Audio Representations,"We introduce DECAR, a self-supervised pre-training approach for learning general-purpose audio representations. Our system is based on clustering: it utilizes an offline clustering step to provide target labels that act as pseudo-labels for solving a prediction task. We develop on top of recent advances in self-supervised learning for computer vision and design a lightweight, easy-to-use self-supervised pre-training scheme. We pre-train DECAR embeddings on a balanced subset of the large-scale Audioset dataset and transfer those representations to 9 downstream classification tasks, including speech, music, animal sounds, and acoustic scenes. Furthermore, we conduct ablation studies identifying key design choices and also make all our code and pre-trained models publicly available.","['https://openalex.org/W2606176153', 'https://openalex.org/W2097820631', 'https://openalex.org/W2947454875', 'https://openalex.org/W3162391496', 'https://openalex.org/W2963341956', 'https://openalex.org/W2767754137', 'https://openalex.org/W3099782249', 'https://openalex.org/W2982223350', 'https://openalex.org/W3036224891', 'https://openalex.org/W2883725317', 'https://openalex.org/W3169320628', 'https://openalex.org/W2936774411', 'https://openalex.org/W2926827382', 'https://openalex.org/W3178584664', 'https://openalex.org/W2797583228', 'https://openalex.org/W3006926732', 'https://openalex.org/W2951535099', 'https://openalex.org/W3034978746', 'https://openalex.org/W1494198834', 'https://openalex.org/W2726515241', 'https://openalex.org/W2593116425', 'https://openalex.org/W2883595988', 'https://openalex.org/W2146334809']",2021-10-17
https://openalex.org/W4408346014,https://doi.org/10.1109/icassp49660.2025.10889108,Enhancing Expressive Voice Conversion with Discrete Pitch-Conditioned Flow Matching Model,,"['https://openalex.org/W6762533536', 'https://openalex.org/W6804125026', 'https://openalex.org/W2972659941', 'https://openalex.org/W3163475957', 'https://openalex.org/W6776390925', 'https://openalex.org/W6803547063', 'https://openalex.org/W4372337821', 'https://openalex.org/W6839738141', 'https://openalex.org/W4385822372', 'https://openalex.org/W3210530853', 'https://openalex.org/W4392903591', 'https://openalex.org/W4385823264', 'https://openalex.org/W4402670354', 'https://openalex.org/W3209059054', 'https://openalex.org/W6780218876', 'https://openalex.org/W3213029956', 'https://openalex.org/W4224916568', 'https://openalex.org/W4402111360', 'https://openalex.org/W3015826515', 'https://openalex.org/W4392902857', 'https://openalex.org/W4401980889', 'https://openalex.org/W4312732823', 'https://openalex.org/W6783867762', 'https://openalex.org/W3140429000', 'https://openalex.org/W6871686990', 'https://openalex.org/W6870277322', 'https://openalex.org/W6852870047', 'https://openalex.org/W3024869864', 'https://openalex.org/W6752307458', 'https://openalex.org/W6846539466', 'https://openalex.org/W6870225445', 'https://openalex.org/W2972359262', 'https://openalex.org/W1494198834', 'https://openalex.org/W3163573274', 'https://openalex.org/W4392931276', 'https://openalex.org/W4402669711', 'https://openalex.org/W3015805741', 'https://openalex.org/W3198217962']",2025-03-12
https://openalex.org/W4404965706,https://doi.org/10.1007/978-3-031-78312-8_26,Comparative Analysis of Voice Conversion in German,,"['https://openalex.org/W4385823432', 'https://openalex.org/W3209984917', 'https://openalex.org/W6607045254', 'https://openalex.org/W3209059054', 'https://openalex.org/W2531638282', 'https://openalex.org/W4372260053', 'https://openalex.org/W4392902857', 'https://openalex.org/W4392969497', 'https://openalex.org/W3083423753', 'https://openalex.org/W4392904717', 'https://openalex.org/W4392909504', 'https://openalex.org/W4392910523', 'https://openalex.org/W4392903256', 'https://openalex.org/W1494198834', 'https://openalex.org/W4392904630', 'https://openalex.org/W4225956675', 'https://openalex.org/W4322629454', 'https://openalex.org/W4392903903', 'https://openalex.org/W3201506562', 'https://openalex.org/W4392909950', 'https://openalex.org/W4392903591', 'https://openalex.org/W4392903127', 'https://openalex.org/W2972359262', 'https://openalex.org/W6600140940', 'https://openalex.org/W3165478005']",2024-12-03
https://openalex.org/W4406461642,https://doi.org/10.1109/slt61566.2024.10832210,Hierarchical Multi-Path and Multi-Model Selection For Fake Speech Detection,,"['https://openalex.org/W6802527329', 'https://openalex.org/W4392903591', 'https://openalex.org/W6848482659', 'https://openalex.org/W6859230650', 'https://openalex.org/W2061278248', 'https://openalex.org/W3095259706', 'https://openalex.org/W3024869864', 'https://openalex.org/W4221154745', 'https://openalex.org/W6769178842', 'https://openalex.org/W4321780088', 'https://openalex.org/W4313590886', 'https://openalex.org/W2916301830', 'https://openalex.org/W2150962366', 'https://openalex.org/W2807550049', 'https://openalex.org/W2303197844', 'https://openalex.org/W3196837826', 'https://openalex.org/W3024920698', 'https://openalex.org/W3197014136', 'https://openalex.org/W3201773091', 'https://openalex.org/W3163596559', 'https://openalex.org/W3146945401', 'https://openalex.org/W2964052309', 'https://openalex.org/W4225527248', 'https://openalex.org/W3026777299', 'https://openalex.org/W3127781933', 'https://openalex.org/W3211424380', 'https://openalex.org/W3162784934', 'https://openalex.org/W4317181815', 'https://openalex.org/W3147454823', 'https://openalex.org/W3151295995', 'https://openalex.org/W4319862427', 'https://openalex.org/W4297841826', 'https://openalex.org/W3213029956', 'https://openalex.org/W2194775991', 'https://openalex.org/W4389471275', 'https://openalex.org/W4288091954', 'https://openalex.org/W3158663310', 'https://openalex.org/W4313447020']",2024-12-02
https://openalex.org/W4407692478,https://doi.org/10.1109/lsp.2025.3543456,UnitDiff: A Unit-Diffusion Model for Code-Switching Speech Synthesis,,"['https://openalex.org/W2963609956', 'https://openalex.org/W2964243274', 'https://openalex.org/W6778823374', 'https://openalex.org/W2936832667', 'https://openalex.org/W3016139610', 'https://openalex.org/W2525997790', 'https://openalex.org/W2973084242', 'https://openalex.org/W4372260486', 'https://openalex.org/W3095873922', 'https://openalex.org/W2996952407', 'https://openalex.org/W4402979375', 'https://openalex.org/W3205154814', 'https://openalex.org/W4392903591', 'https://openalex.org/W3210530853', 'https://openalex.org/W6679045638', 'https://openalex.org/W6779823529', 'https://openalex.org/W6783713337', 'https://openalex.org/W6795261426', 'https://openalex.org/W4304099317', 'https://openalex.org/W4385823402', 'https://openalex.org/W4392904694', 'https://openalex.org/W4391615163', 'https://openalex.org/W6809884996', 'https://openalex.org/W6917585676', 'https://openalex.org/W6783867762', 'https://openalex.org/W6805710207', 'https://openalex.org/W6847363464', 'https://openalex.org/W6777694618']",2025-01-01
https://openalex.org/W4408345987,https://doi.org/10.1109/icassp49660.2025.10890000,ExVC: Leveraging Mixture of Experts Models for Efficient Zero-shot Voice Conversion,,"['https://openalex.org/W3098557217', 'https://openalex.org/W6802527329', 'https://openalex.org/W4372267192', 'https://openalex.org/W2997354053', 'https://openalex.org/W4386057728', 'https://openalex.org/W4386132131', 'https://openalex.org/W2937579788', 'https://openalex.org/W6762533536', 'https://openalex.org/W4392902857', 'https://openalex.org/W4402112067', 'https://openalex.org/W3196584150', 'https://openalex.org/W6805710207', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W4402112452', 'https://openalex.org/W4392903591', 'https://openalex.org/W4372260053', 'https://openalex.org/W4385823432', 'https://openalex.org/W3097777922', 'https://openalex.org/W6844796666', 'https://openalex.org/W2760103357', 'https://openalex.org/W3083423753', 'https://openalex.org/W6843384352', 'https://openalex.org/W6732520560', 'https://openalex.org/W6788811087', 'https://openalex.org/W4372349677', 'https://openalex.org/W4390873360', 'https://openalex.org/W3134077887', 'https://openalex.org/W6869382384', 'https://openalex.org/W4385823191', 'https://openalex.org/W6796464841', 'https://openalex.org/W2972394484']",2025-03-12
https://openalex.org/W4411556324,https://doi.org/10.1007/978-981-96-7005-5_30,Constructing Multi-detector Decision Forest for Fake Speech Detection,,"['https://openalex.org/W2150962366', 'https://openalex.org/W3024920698', 'https://openalex.org/W2973164265', 'https://openalex.org/W3196837826', 'https://openalex.org/W4313590886', 'https://openalex.org/W4206147850', 'https://openalex.org/W3099891349', 'https://openalex.org/W3209059054', 'https://openalex.org/W3201773091', 'https://openalex.org/W4297841850', 'https://openalex.org/W2972811785', 'https://openalex.org/W4206289720', 'https://openalex.org/W4388630478', 'https://openalex.org/W4317181815', 'https://openalex.org/W2964052309', 'https://openalex.org/W3197014136', 'https://openalex.org/W3200167423', 'https://openalex.org/W3163596559', 'https://openalex.org/W4225527248', 'https://openalex.org/W2505121225', 'https://openalex.org/W2987137416', 'https://openalex.org/W3026777299', 'https://openalex.org/W4392903591', 'https://openalex.org/W4312120800']",2025-01-01
https://openalex.org/W4413221907,https://doi.org/10.1145/3749644,Enhanced Prosody Modeling and Character Voice Controlling for Audiobook Speech Synthesis,"Conventional speech synthesis techniques have made significant strides towards achieving human-like performance. However, the domain of audiobook speech synthesis still presents notable challenges. On one hand, the speech in audiobooks exhibits rich prosodic expressiveness, posing substantial difficulties in prosody modeling. On the other hand, the reader of audiobooks uses different voices to perform dialogues of different characters, which has been inadequately explored in existing speech synthesis methods. To address the first challenge, we integrate discourse-scale prosody modeling into the conventional autoencoder-based framework and introduce generative adversarial networks (GANs) for phoneme-level prosody code prediction. Regarding the second challenge, we further explore a character voice encoder based on the pretrained speaker verification model, integrating it into our proposed method. Experimental results validate that the proposed method enhances the prosodic expressiveness of synthesized audiobook speech. Moreover, it demonstrates the capacity to produce distinctive voices for different audiobook characters without compromising the naturalness of the synthesized speech.","['https://openalex.org/W2964243274', 'https://openalex.org/W2903739847', 'https://openalex.org/W3161296985', 'https://openalex.org/W4385823163', 'https://openalex.org/W4391020683', 'https://openalex.org/W3161113899', 'https://openalex.org/W2963341956', 'https://openalex.org/W3161732385', 'https://openalex.org/W3097795905', 'https://openalex.org/W3081488690', 'https://openalex.org/W3163339651', 'https://openalex.org/W3193700177', 'https://openalex.org/W3196027980', 'https://openalex.org/W4372341043', 'https://openalex.org/W4385329631', 'https://openalex.org/W4385993905', 'https://openalex.org/W4391021635', 'https://openalex.org/W4402112210', 'https://openalex.org/W4224928640', 'https://openalex.org/W3198123658', 'https://openalex.org/W4375869257', 'https://openalex.org/W4398152753', 'https://openalex.org/W4385822787', 'https://openalex.org/W4392903591', 'https://openalex.org/W3216941316', 'https://openalex.org/W3015338123', 'https://openalex.org/W4372346850', 'https://openalex.org/W2972951102', 'https://openalex.org/W4375868902', 'https://openalex.org/W3096086473', 'https://openalex.org/W3024869864', 'https://openalex.org/W2890964092', 'https://openalex.org/W3162746464', 'https://openalex.org/W3150572638', 'https://openalex.org/W2962691331', 'https://openalex.org/W4392904630', 'https://openalex.org/W3158374895', 'https://openalex.org/W3151450932', 'https://openalex.org/W3015440759', 'https://openalex.org/W3094917204', 'https://openalex.org/W4396542467', 'https://openalex.org/W2904459034', 'https://openalex.org/W2885800352', 'https://openalex.org/W3095545636', 'https://openalex.org/W4319586596', 'https://openalex.org/W4296068776', 'https://openalex.org/W3097777922', 'https://openalex.org/W2593414223', 'https://openalex.org/W4285218396', 'https://openalex.org/W4395961599', 'https://openalex.org/W2471520273', 'https://openalex.org/W2808631503', 'https://openalex.org/W4406794071', 'https://openalex.org/W4402112010']",2025-08-11
https://openalex.org/W4415708059,https://doi.org/10.1109/icme59968.2025.11208979,Fitted-Singer: Singing Voice Synthesis with Style Control and Rhythm Control,,"['https://openalex.org/W4402979888', 'https://openalex.org/W3097514409', 'https://openalex.org/W4385571362', 'https://openalex.org/W3158762648', 'https://openalex.org/W4392903591', 'https://openalex.org/W4385822787', 'https://openalex.org/W4392908903', 'https://openalex.org/W4398152753', 'https://openalex.org/W4375869257', 'https://openalex.org/W4401042666', 'https://openalex.org/W4402982072', 'https://openalex.org/W3196584150', 'https://openalex.org/W4393160753', 'https://openalex.org/W4221167708', 'https://openalex.org/W4384028538', 'https://openalex.org/W4391021559', 'https://openalex.org/W2884225676', 'https://openalex.org/W4200556575', 'https://openalex.org/W4392909919']",2025-06-30
https://openalex.org/W4416017203,https://doi.org/10.1145/3746252.3761614,S2Cap: A Benchmark and a Baseline for Singing Style Captioning,,"['https://openalex.org/W4402112231', 'https://openalex.org/W4375869156', 'https://openalex.org/W3015591594', 'https://openalex.org/W2067709094', 'https://openalex.org/W3160173020', 'https://openalex.org/W4285345683', 'https://openalex.org/W4372340819', 'https://openalex.org/W2949376505', 'https://openalex.org/W4375928773', 'https://openalex.org/W3016243847', 'https://openalex.org/W2101105183', 'https://openalex.org/W2970641574', 'https://openalex.org/W2914699769', 'https://openalex.org/W4372260250', 'https://openalex.org/W2886641317', 'https://openalex.org/W4401042666', 'https://openalex.org/W2425121537', 'https://openalex.org/W4392903700', 'https://openalex.org/W4392903591', 'https://openalex.org/W2185175083', 'https://openalex.org/W4393160753', 'https://openalex.org/W4391021679', 'https://openalex.org/W3205860970', 'https://openalex.org/W4403780871']",2025-11-08
https://openalex.org/W4416798711,https://doi.org/10.1109/apsipaasc65261.2025.11249275,CycleSiFiNF-VC: Controllable Non-Parallel Voice Conversion by Neural Formant Manipulation with Improved Cycle-Consistency Loss,,"['https://openalex.org/W3162512456', 'https://openalex.org/W3196667132', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W4385245566', 'https://openalex.org/W3197763626', 'https://openalex.org/W4372260053', 'https://openalex.org/W4392903591', 'https://openalex.org/W4402111357', 'https://openalex.org/W2937579788', 'https://openalex.org/W2145892079', 'https://openalex.org/W4385823333', 'https://openalex.org/W2962793481', 'https://openalex.org/W2902070858', 'https://openalex.org/W2476548250', 'https://openalex.org/W4293363567', 'https://openalex.org/W4372262501', 'https://openalex.org/W4225956675']",2025-10-22
https://openalex.org/W4417297300,https://doi.org/10.1038/s41598-025-25815-6,The Hierarchical Disentangled Information framework enables the discovery of meaningful structures,,"['https://openalex.org/W2163922914', 'https://openalex.org/W4385799550', 'https://openalex.org/W4385988305', 'https://openalex.org/W4297536219', 'https://openalex.org/W4391020683', 'https://openalex.org/W2747680751', 'https://openalex.org/W4392849937', 'https://openalex.org/W4386075604', 'https://openalex.org/W4392940145', 'https://openalex.org/W4391021724', 'https://openalex.org/W4372260053', 'https://openalex.org/W4386132131', 'https://openalex.org/W4385822372', 'https://openalex.org/W4392903591', 'https://openalex.org/W3138516171', 'https://openalex.org/W3175906703', 'https://openalex.org/W4283731195', 'https://openalex.org/W4393156269', 'https://openalex.org/W2234589100', 'https://openalex.org/W4323891848', 'https://openalex.org/W4375850690', 'https://openalex.org/W2902070858', 'https://openalex.org/W4403022196', 'https://openalex.org/W2603777577', 'https://openalex.org/W2903141607', 'https://openalex.org/W4390874265', 'https://openalex.org/W2112796928']",2025-12-11
https://openalex.org/W7114777046,https://doi.org/10.1038/s41598-025-25815-6,The Hierarchical Disentangled Information framework enables the discovery of meaningful structures,,"['https://openalex.org/W2163922914', 'https://openalex.org/W4385799550', 'https://openalex.org/W4385988305', 'https://openalex.org/W4297536219', 'https://openalex.org/W4391020683', 'https://openalex.org/W2747680751', 'https://openalex.org/W4392849937', 'https://openalex.org/W4386075604', 'https://openalex.org/W4392940145', 'https://openalex.org/W4391021724', 'https://openalex.org/W4372260053', 'https://openalex.org/W4386132131', 'https://openalex.org/W4385822372', 'https://openalex.org/W4392903591', 'https://openalex.org/W3138516171', 'https://openalex.org/W3175906703', 'https://openalex.org/W4283731195', 'https://openalex.org/W4393156269', 'https://openalex.org/W2234589100', 'https://openalex.org/W4323891848', 'https://openalex.org/W4375850690', 'https://openalex.org/W2902070858', 'https://openalex.org/W4403022196', 'https://openalex.org/W2603777577', 'https://openalex.org/W2903141607', 'https://openalex.org/W4390874265', 'https://openalex.org/W2112796928']",2025-12-11
https://openalex.org/W4382918397,https://doi.org/10.1111/cogs.13307,Introducing Meta‐analysis in the Evaluation of Computational Models of Infant Language Development,"Abstract Computational models of child language development can help us understand the cognitive underpinnings of the language learning process, which occurs along several linguistic levels at once (e.g., prosodic and phonological). However, in light of the replication crisis, modelers face the challenge of selecting representative and consolidated infant data. Thus, it is desirable to have evaluation methodologies that could account for robust empirical reference data, across multiple infant capabilities. Moreover, there is a need for practices that can compare developmental trajectories of infants to those of models as a function of language experience and development. The present study aims to take concrete steps to address these needs by introducing the concept of comparing models with large‐scale cumulative empirical data from infants, as quantified by meta‐analyses conducted across a large number of individual behavioral studies. We formalize the connection between measurable model and human behavior, and then present a conceptual framework for meta‐analytic evaluation of computational models. We exemplify the meta‐analytic model evaluation approach with two modeling experiments on infant‐directed speech preference and native/non‐native vowel discrimination.","['https://openalex.org/W2137295896', 'https://openalex.org/W3177829661', 'https://openalex.org/W4213058750', 'https://openalex.org/W4283258029', 'https://openalex.org/W2772732614', 'https://openalex.org/W4212863985', 'https://openalex.org/W6629510986', 'https://openalex.org/W2054289822', 'https://openalex.org/W3130490927', 'https://openalex.org/W3165112351', 'https://openalex.org/W2067191956', 'https://openalex.org/W6667443654', 'https://openalex.org/W3017025049', 'https://openalex.org/W3035750922', 'https://openalex.org/W2926827382', 'https://openalex.org/W2972943112', 'https://openalex.org/W4244103449', 'https://openalex.org/W6652776558', 'https://openalex.org/W2253063626', 'https://openalex.org/W3082376609', 'https://openalex.org/W3023889965', 'https://openalex.org/W2940544976', 'https://openalex.org/W2963620343', 'https://openalex.org/W6697293080', 'https://openalex.org/W2483390977', 'https://openalex.org/W2130890537', 'https://openalex.org/W4299555621', 'https://openalex.org/W2157823046', 'https://openalex.org/W3145811386', 'https://openalex.org/W3196459653', 'https://openalex.org/W2079408923', 'https://openalex.org/W6670098071', 'https://openalex.org/W2123292690', 'https://openalex.org/W2782905495', 'https://openalex.org/W2595479191', 'https://openalex.org/W2141038596', 'https://openalex.org/W3119754616', 'https://openalex.org/W2148764920', 'https://openalex.org/W3084297320', 'https://openalex.org/W4253971549', 'https://openalex.org/W2191828469', 'https://openalex.org/W3119802905', 'https://openalex.org/W3123354371', 'https://openalex.org/W7071764571', 'https://openalex.org/W2070696251', 'https://openalex.org/W4285726357', 'https://openalex.org/W4295309037', 'https://openalex.org/W3114436296', 'https://openalex.org/W3174311593', 'https://openalex.org/W2466796873', 'https://openalex.org/W2144981148', 'https://openalex.org/W135984148', 'https://openalex.org/W4206039057', 'https://openalex.org/W3164946614', 'https://openalex.org/W4230640548', 'https://openalex.org/W2057256606', 'https://openalex.org/W4225079082', 'https://openalex.org/W1990351858', 'https://openalex.org/W2103091632', 'https://openalex.org/W2031445901', 'https://openalex.org/W4253566955', 'https://openalex.org/W3161374022', 'https://openalex.org/W2785183465', 'https://openalex.org/W6618980683', 'https://openalex.org/W3081675162', 'https://openalex.org/W2031880432', 'https://openalex.org/W6658466740', 'https://openalex.org/W1576931943', 'https://openalex.org/W2002572909', 'https://openalex.org/W3023172065', 'https://openalex.org/W1965305729', 'https://openalex.org/W6641383474', 'https://openalex.org/W2024579455', 'https://openalex.org/W2091143423', 'https://openalex.org/W2615578810', 'https://openalex.org/W3110458199', 'https://openalex.org/W3163184902', 'https://openalex.org/W1897139626', 'https://openalex.org/W2610253745', 'https://openalex.org/W1494198834', 'https://openalex.org/W2268773773', 'https://openalex.org/W2140386792', 'https://openalex.org/W6680931182', 'https://openalex.org/W3033724742', 'https://openalex.org/W2119885245', 'https://openalex.org/W2010188467', 'https://openalex.org/W4376140200', 'https://openalex.org/W2415378728', 'https://openalex.org/W3023533951', 'https://openalex.org/W3035507081', 'https://openalex.org/W3129957462', 'https://openalex.org/W2768381684', 'https://openalex.org/W3125087428', 'https://openalex.org/W2395899413', 'https://openalex.org/W2406349064', 'https://openalex.org/W3102519966', 'https://openalex.org/W4303629135', 'https://openalex.org/W2154600605', 'https://openalex.org/W3084747096', 'https://openalex.org/W2092929651', 'https://openalex.org/W2041173449', 'https://openalex.org/W2077382402', 'https://openalex.org/W6669981445', 'https://openalex.org/W2032476212', 'https://openalex.org/W2114831903', 'https://openalex.org/W7073679870', 'https://openalex.org/W1925965306', 'https://openalex.org/W6640227979', 'https://openalex.org/W2786608204', 'https://openalex.org/W6973666849', 'https://openalex.org/W2089883580', 'https://openalex.org/W3157923770', 'https://openalex.org/W3197580070', 'https://openalex.org/W3117111924', 'https://openalex.org/W4211209158', 'https://openalex.org/W2284729062', 'https://openalex.org/W2129168188', 'https://openalex.org/W2973026522', 'https://openalex.org/W612372977', 'https://openalex.org/W2482374272']",2023-07-01
https://openalex.org/W2787447541,https://doi.org/10.1109/asru.2017.8269011,Feature optimized DPGMM clustering for unsupervised subword modeling: A contribution to zerospeech 2017,"This paper describes our unsupervised subword modeling pipeline for the zero resource speech challenge (ZeroSpeech) 2017. Our approach is built around the Dirichlet process Gaussian mixture model (DPGMM) that we use to cluster speech feature vectors into a dynamically sized set of classes. By considering each class an acoustic unit, speech can be represented as sequence of class posteriorgrams. We enhance this method by automatically optimizing the DPGMM sampler's input features in a multi-stage clustering framework, where we unsupervisedly learn transformations using LDA, MLLT and (basis) fMLLR to reduce variance in the features. We show that this optimization considerably boosts the subword modeling quality, according to the performance on the ABX phone discriminability task. For the first time, we apply inferred subword models to previously unseen data from a new set of speakers. We demonstrate our method's good generalization and the effectiveness of its blind speaker adaptation in extensive experiments on a multitude of datasets. Our pipeline has very little need for hyper-parameter adjustment and is entirely unsupervised, i.e., it only takes raw audio recordings as input, without requiring any pre-defined segmentation, explicit speaker IDs or other meta data.","['https://openalex.org/W6678947187', 'https://openalex.org/W2001619934', 'https://openalex.org/W2124629003', 'https://openalex.org/W2106554350', 'https://openalex.org/W1599512239', 'https://openalex.org/W2002342963', 'https://openalex.org/W2019042707', 'https://openalex.org/W2078769636', 'https://openalex.org/W6712444837', 'https://openalex.org/W6712202099', 'https://openalex.org/W6638159135', 'https://openalex.org/W6712553779', 'https://openalex.org/W6713256719', 'https://openalex.org/W2509930204', 'https://openalex.org/W6704305767', 'https://openalex.org/W2963620343', 'https://openalex.org/W6973666849', 'https://openalex.org/W2586754519', 'https://openalex.org/W6631362777', 'https://openalex.org/W2113641473', 'https://openalex.org/W1631260214', 'https://openalex.org/W2395899413', 'https://openalex.org/W2396043527', 'https://openalex.org/W2404799143', 'https://openalex.org/W2128032727', 'https://openalex.org/W1796128977', 'https://openalex.org/W2786608204', 'https://openalex.org/W1524333225', 'https://openalex.org/W2399576818', 'https://openalex.org/W2345811097']",2017-12-01
https://openalex.org/W2767122664,https://doi.org/10.1109/jstsp.2017.2764270,Multitask Feature Learning for Low-Resource Query-by-Example Spoken Term Detection,"We propose a novel technique that learns a low-dimensional feature representation from unlabeled data of a target language, and labeled data from a nontarget language. The technique is studied as a solution to query-by-example spoken term detection (QbE-STD) for a low-resource language. We extract low-dimensional features from a bottle-neck layer of a multitask deep neural network, which is jointly trained with speech data from the low-resource target language and resource-rich nontarget language. The proposed feature learning technique aims to extract acoustic features that offer phonetic discriminability. It explores a new way of leveraging cross-lingual speech data to overcome the resource limitation in the target language. We conduct QbE-STD experiments using the dynamic time warping distance of the multitask bottle-neck features between the query and the search database. The QbE-STD process does not rely on an automatic speech recognition pipeline of the target language. We validate the effectiveness of multitask feature learning through a series of comparative experiments.","['https://openalex.org/W2962980711', 'https://openalex.org/W1577418252', 'https://openalex.org/W2592866267', 'https://openalex.org/W2094035326', 'https://openalex.org/W6744261651', 'https://openalex.org/W6713745070', 'https://openalex.org/W6712444837', 'https://openalex.org/W3127686677', 'https://openalex.org/W6607149552', 'https://openalex.org/W2566925314', 'https://openalex.org/W2162888803', 'https://openalex.org/W2171019095', 'https://openalex.org/W2962903176', 'https://openalex.org/W2126203737', 'https://openalex.org/W2518166530', 'https://openalex.org/W2290269503', 'https://openalex.org/W1580290522', 'https://openalex.org/W6712625807', 'https://openalex.org/W2913340405', 'https://openalex.org/W2511774920', 'https://openalex.org/W6719357382', 'https://openalex.org/W6629208684', 'https://openalex.org/W2186319236', 'https://openalex.org/W1984076147', 'https://openalex.org/W6638159135', 'https://openalex.org/W2404799143', 'https://openalex.org/W2963571336', 'https://openalex.org/W2963684067', 'https://openalex.org/W2511733680', 'https://openalex.org/W2137044559', 'https://openalex.org/W2513125788', 'https://openalex.org/W1997505733', 'https://openalex.org/W2509930204', 'https://openalex.org/W6631362777', 'https://openalex.org/W1539935047', 'https://openalex.org/W2106284094', 'https://openalex.org/W2347098582', 'https://openalex.org/W6678007500', 'https://openalex.org/W2586754519', 'https://openalex.org/W2154085905', 'https://openalex.org/W2114347655', 'https://openalex.org/W6636469245', 'https://openalex.org/W6712553779', 'https://openalex.org/W1967924372', 'https://openalex.org/W6675022971', 'https://openalex.org/W2110589736', 'https://openalex.org/W2154093685', 'https://openalex.org/W2020607164', 'https://openalex.org/W1545920196', 'https://openalex.org/W6884732673', 'https://openalex.org/W6629386585', 'https://openalex.org/W2402401665', 'https://openalex.org/W6713061419', 'https://openalex.org/W6602705600', 'https://openalex.org/W2094721296', 'https://openalex.org/W6677734967', 'https://openalex.org/W6713287686', 'https://openalex.org/W1499332833', 'https://openalex.org/W2137630452', 'https://openalex.org/W6713823255', 'https://openalex.org/W2025198378', 'https://openalex.org/W1545083717', 'https://openalex.org/W1606350000', 'https://openalex.org/W2395899413', 'https://openalex.org/W2406349064', 'https://openalex.org/W2120636621', 'https://openalex.org/W3104240813', 'https://openalex.org/W2119187236', 'https://openalex.org/W2398352857', 'https://openalex.org/W66167291', 'https://openalex.org/W1490960657', 'https://openalex.org/W2407793339', 'https://openalex.org/W2464234964', 'https://openalex.org/W2128032727', 'https://openalex.org/W2036043322', 'https://openalex.org/W2916450621', 'https://openalex.org/W2915397826', 'https://openalex.org/W2916959077', 'https://openalex.org/W2401805659', 'https://openalex.org/W2962695963', 'https://openalex.org/W1524333225', 'https://openalex.org/W2406778302', 'https://openalex.org/W2399576818', 'https://openalex.org/W1796128977', 'https://openalex.org/W2755891984', 'https://openalex.org/W173856587', 'https://openalex.org/W1490911310', 'https://openalex.org/W2100768664']",2017-10-18
https://openalex.org/W4211116738,https://doi.org/10.1016/j.csl.2022.101358,Discovering phonetic inventories with crosslingual automatic speech recognition,,"['https://openalex.org/W2787223168', 'https://openalex.org/W2785860501', 'https://openalex.org/W4249993336', 'https://openalex.org/W1964917299', 'https://openalex.org/W2402366697', 'https://openalex.org/W2108776455', 'https://openalex.org/W2412593044', 'https://openalex.org/W2747192917', 'https://openalex.org/W2749648329', 'https://openalex.org/W6638728282', 'https://openalex.org/W6623517193', 'https://openalex.org/W6748978592', 'https://openalex.org/W2896457183', 'https://openalex.org/W2940544976', 'https://openalex.org/W2750248772', 'https://openalex.org/W2156483112', 'https://openalex.org/W6784479336', 'https://openalex.org/W3196459653', 'https://openalex.org/W6672028191', 'https://openalex.org/W6649207683', 'https://openalex.org/W2127141656', 'https://openalex.org/W6783126531', 'https://openalex.org/W2586754519', 'https://openalex.org/W2509930204', 'https://openalex.org/W6704305767', 'https://openalex.org/W6748342566', 'https://openalex.org/W2791647162', 'https://openalex.org/W3016368932', 'https://openalex.org/W2739883972', 'https://openalex.org/W6779984486', 'https://openalex.org/W6786495357', 'https://openalex.org/W2025198378', 'https://openalex.org/W6756399367', 'https://openalex.org/W6792432218', 'https://openalex.org/W1436754857', 'https://openalex.org/W6648710200', 'https://openalex.org/W2971840980', 'https://openalex.org/W6768009688', 'https://openalex.org/W1993799394', 'https://openalex.org/W3204007566', 'https://openalex.org/W6728030952', 'https://openalex.org/W2079641914', 'https://openalex.org/W6648602273', 'https://openalex.org/W2332939288', 'https://openalex.org/W2100768664', 'https://openalex.org/W2077804127', 'https://openalex.org/W1778492285', 'https://openalex.org/W6680297918', 'https://openalex.org/W1577418252', 'https://openalex.org/W3015877095', 'https://openalex.org/W2041252889', 'https://openalex.org/W2164505566', 'https://openalex.org/W2889326414', 'https://openalex.org/W2972764223', 'https://openalex.org/W2046932483', 'https://openalex.org/W2668277942', 'https://openalex.org/W6679569544', 'https://openalex.org/W2295676751', 'https://openalex.org/W2511875134', 'https://openalex.org/W1968147312', 'https://openalex.org/W6704752648', 'https://openalex.org/W2927191280', 'https://openalex.org/W4240842827', 'https://openalex.org/W6754581696', 'https://openalex.org/W2888867175', 'https://openalex.org/W2514741789', 'https://openalex.org/W1796128977', 'https://openalex.org/W2754396191', 'https://openalex.org/W2079623482', 'https://openalex.org/W2321675883', 'https://openalex.org/W3005578234', 'https://openalex.org/W6602682705', 'https://openalex.org/W47568227', 'https://openalex.org/W2033436836', 'https://openalex.org/W1978352171', 'https://openalex.org/W2787426069', 'https://openalex.org/W308497914', 'https://openalex.org/W2078769636', 'https://openalex.org/W1631260214', 'https://openalex.org/W2319623048', 'https://openalex.org/W2120209245', 'https://openalex.org/W2972374322', 'https://openalex.org/W2891816510', 'https://openalex.org/W6746524143', 'https://openalex.org/W2317002935', 'https://openalex.org/W2317579269', 'https://openalex.org/W2899157532', 'https://openalex.org/W2826003142', 'https://openalex.org/W6739901393', 'https://openalex.org/W2786608204', 'https://openalex.org/W1970890968', 'https://openalex.org/W2401430117', 'https://openalex.org/W6748640972', 'https://openalex.org/W2962780374', 'https://openalex.org/W3007826012', 'https://openalex.org/W2006617198', 'https://openalex.org/W6754889592', 'https://openalex.org/W6682420859', 'https://openalex.org/W2400517318', 'https://openalex.org/W6785385881', 'https://openalex.org/W3095732712', 'https://openalex.org/W3016010032', 'https://openalex.org/W2973026522', 'https://openalex.org/W2513440303', 'https://openalex.org/W4287173589', 'https://openalex.org/W2769437540', 'https://openalex.org/W3145668479', 'https://openalex.org/W2295297373', 'https://openalex.org/W2559655401', 'https://openalex.org/W3105242324', 'https://openalex.org/W2727795052', 'https://openalex.org/W4252531498', 'https://openalex.org/W2316803017', 'https://openalex.org/W2612690371', 'https://openalex.org/W630728710', 'https://openalex.org/W2188703032', 'https://openalex.org/W2972574141', 'https://openalex.org/W3152218910']",2022-02-09
https://openalex.org/W2895297209,https://doi.org/10.21437/sltu.2018-1,Optimizing DPGMM Clustering in Zero Resource Setting Based on Functional Load,,"['https://openalex.org/W2089381378', 'https://openalex.org/W2017339534', 'https://openalex.org/W2399576818', 'https://openalex.org/W2394934205', 'https://openalex.org/W2010548701', 'https://openalex.org/W2506406911', 'https://openalex.org/W1995875735', 'https://openalex.org/W1524333225', 'https://openalex.org/W1796128977', 'https://openalex.org/W2586754519', 'https://openalex.org/W2128032727', 'https://openalex.org/W2786608204', 'https://openalex.org/W2054496875', 'https://openalex.org/W1545920196', 'https://openalex.org/W2151926297', 'https://openalex.org/W2395899413', 'https://openalex.org/W2065223519', 'https://openalex.org/W2345811097']",2018-08-29
https://openalex.org/W2971041032,https://doi.org/10.1109/taslp.2019.2937953,Exploiting Cross-Lingual Speaker and Phonetic Diversity for Unsupervised Subword Modeling,"This research addresses the problem of acoustic modeling of low-resource\nlanguages for which transcribed training data is absent. The goal is to learn\nrobust frame-level feature representations that can be used to identify and\ndistinguish subword-level speech units. The proposed feature representations\ncomprise various types of multilingual bottleneck features (BNFs) that are\nobtained via multi-task learning of deep neural networks (MTL-DNN). One of the\nkey problems is how to acquire high-quality frame labels for untranscribed\ntraining data to facilitate supervised DNN training. It is shown that learning\nof robust BNF representations can be achieved by effectively leveraging\ntranscribed speech data and well-trained automatic speech recognition (ASR)\nsystems from one or more out-of-domain (resource-rich) languages. Out-of-domain\nASR systems can be applied to perform speaker adaptation with untranscribed\ntraining data of the target language, and to decode the training speech into\nframe-level labels for DNN training. It is also found that better frame labels\ncan be generated by considering temporal dependency in speech when performing\nframe clustering. The proposed methods of feature learning are evaluated on the\nstandard task of unsupervised subword modeling in Track 1 of the ZeroSpeech\n2017 Challenge. The best performance achieved by our system is $9.7\\%$ in terms\nof across-speaker triphone minimal-pair ABX error rate, which is comparable to\nthe best systems reported recently. Lastly, our investigation reveals that the\ncloseness between target languages and out-of-domain languages and the amount\nof available training data for individual target languages could have\nsignificant impact on the goodness of learned features.\n","['https://openalex.org/W2747192917', 'https://openalex.org/W1975728937', 'https://openalex.org/W6677803786', 'https://openalex.org/W2785860501', 'https://openalex.org/W2826003142', 'https://openalex.org/W2767122664', 'https://openalex.org/W2167845555', 'https://openalex.org/W6640828828', 'https://openalex.org/W2921843068', 'https://openalex.org/W2594951208', 'https://openalex.org/W3100270690', 'https://openalex.org/W6638159135', 'https://openalex.org/W2785415724', 'https://openalex.org/W2627092829', 'https://openalex.org/W2963266252', 'https://openalex.org/W2787447541', 'https://openalex.org/W2586754519', 'https://openalex.org/W2759889345', 'https://openalex.org/W2895297209', 'https://openalex.org/W2810166208', 'https://openalex.org/W2509930204', 'https://openalex.org/W4239943352', 'https://openalex.org/W6631362777', 'https://openalex.org/W2057653135', 'https://openalex.org/W1528778941', 'https://openalex.org/W2513125788', 'https://openalex.org/W2614542633', 'https://openalex.org/W2295297373', 'https://openalex.org/W2786902352', 'https://openalex.org/W2787223168', 'https://openalex.org/W2889228998', 'https://openalex.org/W6677154653', 'https://openalex.org/W1545920196', 'https://openalex.org/W6786045457', 'https://openalex.org/W6678947187', 'https://openalex.org/W6712553779', 'https://openalex.org/W2514600732', 'https://openalex.org/W2160815625', 'https://openalex.org/W2963620343', 'https://openalex.org/W6748325621', 'https://openalex.org/W6640777149', 'https://openalex.org/W2055408826', 'https://openalex.org/W2072563337', 'https://openalex.org/W6973666849', 'https://openalex.org/W2005708641', 'https://openalex.org/W6696934422', 'https://openalex.org/W2400549570', 'https://openalex.org/W2120209245', 'https://openalex.org/W162588823', 'https://openalex.org/W2345811097']",2019-08-28
https://openalex.org/W4226177016,https://doi.org/10.1109/taslp.2022.3171975,Non-Parametric Bayesian Subspace Models for Acoustic Unit Discovery,"This work investigates subspace non-parametric models for the task of learning a set of acoustic units from unlabeled speech recordings. We constrain the base-measure of a Dirichlet-Process mixture with a phonetic subspace---estimated from other source languages---to build an \emph{educated prior}, thereby forcing the learned acoustic units to resemble phones of known source languages. Two types of models are proposed: (i) the Subspace HMM (SHMM) which assumes that the phonetic subspace is the same for every language, (ii) the Hierarchical-Subspace HMM (H-SHMM) which relaxes this assumption and allows to have a language-specific subspace estimated on the unlabeled target data. These models are applied on 3 languages: English, Yoruba and Mboshi and they are compared with various competitive acoustic units discovery baselines. Experimental results show that both subspace models outperform other systems in terms of clustering quality and segmentation accuracy. Moreover, we observe that the H-SHMM provides results superior to the SHMM supporting the idea that language-specific priors are preferable to language-agnostic priors for acoustic unit discovery.","['https://openalex.org/W2928408492', 'https://openalex.org/W2962907457', 'https://openalex.org/W2193413348', 'https://openalex.org/W6687566353', 'https://openalex.org/W2545177271', 'https://openalex.org/W2483390977', 'https://openalex.org/W2786608204', 'https://openalex.org/W6973666849', 'https://openalex.org/W2963620343', 'https://openalex.org/W6697293080', 'https://openalex.org/W2940544976', 'https://openalex.org/W2100768664', 'https://openalex.org/W6675022971', 'https://openalex.org/W2399576818', 'https://openalex.org/W2586754519', 'https://openalex.org/W2347098582', 'https://openalex.org/W6704752648', 'https://openalex.org/W3100270690', 'https://openalex.org/W3125709657', 'https://openalex.org/W2979476256', 'https://openalex.org/W6769196770', 'https://openalex.org/W2991557631', 'https://openalex.org/W6770596778', 'https://openalex.org/W2750248772', 'https://openalex.org/W2888911345', 'https://openalex.org/W2927191280', 'https://openalex.org/W3100202343', 'https://openalex.org/W3161215977', 'https://openalex.org/W2109176692', 'https://openalex.org/W6676755231', 'https://openalex.org/W1503207992', 'https://openalex.org/W1990005915', 'https://openalex.org/W2612661529', 'https://openalex.org/W4241943628', 'https://openalex.org/W4212863985', 'https://openalex.org/W2077804127', 'https://openalex.org/W4394867155', 'https://openalex.org/W6865393803', 'https://openalex.org/W2107638917', 'https://openalex.org/W4237840503', 'https://openalex.org/W6645634967', 'https://openalex.org/W7048738093', 'https://openalex.org/W2408460974', 'https://openalex.org/W2762715843', 'https://openalex.org/W1516111018', 'https://openalex.org/W6784355959', 'https://openalex.org/W6744702808', 'https://openalex.org/W2195354', 'https://openalex.org/W3092791109', 'https://openalex.org/W2401271873', 'https://openalex.org/W2084534958', 'https://openalex.org/W2572097499', 'https://openalex.org/W2401396251', 'https://openalex.org/W6600087822', 'https://openalex.org/W6712757354', 'https://openalex.org/W2752796333', 'https://openalex.org/W6731521493', 'https://openalex.org/W2972374322', 'https://openalex.org/W6712648922', 'https://openalex.org/W2099111195', 'https://openalex.org/W3093096176', 'https://openalex.org/W3112613336', 'https://openalex.org/W1494198834', 'https://openalex.org/W2125838338', 'https://openalex.org/W2115979064', 'https://openalex.org/W3198134274', 'https://openalex.org/W2072169887', 'https://openalex.org/W2805607737', 'https://openalex.org/W3209059054', 'https://openalex.org/W6804030475', 'https://openalex.org/W3198429080', 'https://openalex.org/W6677308353', 'https://openalex.org/W6640963894', 'https://openalex.org/W6639732818']",2022-01-01
https://openalex.org/W2810166208,https://doi.org/10.1109/taslp.2018.2852500,Dirichlet Process Mixture of Mixtures Model for Unsupervised Subword Modeling,"We develop a parallelizable Markov chain Monte Carlo sampler for a Dirichlet process mixture of mixtures model. Our sampler jointly infers a codebook and clusters. The codebook is a global collection of components. Clusters are mixtures, defined over the codebook. We combine a nonergodic Gibbs sampler with two layers of split and merge samplers on codebook and mixture level to form a valid ergodic chain. We design an additional switch sampler for components that supports convergence in our experimental results. In the use case of unsupervised subword modeling, we show that our method infers complex classes from real speech feature vectors that consistently show higher quality on several evaluation metrics. At the same time, we infer fewer classes that represent subword units more consistently and show longer durations, compared to a standard Dirichlet process mixture model sampler.","['https://openalex.org/W2152051032', 'https://openalex.org/W2090861223', 'https://openalex.org/W6712444837', 'https://openalex.org/W1994897900', 'https://openalex.org/W2072169887', 'https://openalex.org/W6631362777', 'https://openalex.org/W2044138293', 'https://openalex.org/W2145410271', 'https://openalex.org/W6636447677', 'https://openalex.org/W6712553779', 'https://openalex.org/W2124629003', 'https://openalex.org/W6748489002', 'https://openalex.org/W6704305767', 'https://openalex.org/W2509930204', 'https://openalex.org/W6748342566', 'https://openalex.org/W1920769845', 'https://openalex.org/W1977286842', 'https://openalex.org/W2586754519', 'https://openalex.org/W2101998432', 'https://openalex.org/W6674922813', 'https://openalex.org/W2062373184', 'https://openalex.org/W6678007500', 'https://openalex.org/W1971807270', 'https://openalex.org/W6677150433', 'https://openalex.org/W6973666849', 'https://openalex.org/W1967687583', 'https://openalex.org/W6697293080', 'https://openalex.org/W2069429561', 'https://openalex.org/W6678007862', 'https://openalex.org/W6602662854', 'https://openalex.org/W6678947187', 'https://openalex.org/W1599512239', 'https://openalex.org/W6684363390', 'https://openalex.org/W2106554350', 'https://openalex.org/W1975427826', 'https://openalex.org/W2162021827', 'https://openalex.org/W2002342963', 'https://openalex.org/W1560013842', 'https://openalex.org/W2399576818', 'https://openalex.org/W2128032727', 'https://openalex.org/W1613448136', 'https://openalex.org/W2395899413', 'https://openalex.org/W3101455611', 'https://openalex.org/W2787447541', 'https://openalex.org/W2786902352', 'https://openalex.org/W1589170661', 'https://openalex.org/W2786608204', 'https://openalex.org/W2345811097', 'https://openalex.org/W1524333225', 'https://openalex.org/W2615915281', 'https://openalex.org/W2405618829', 'https://openalex.org/W2963620343', 'https://openalex.org/W2120636621', 'https://openalex.org/W2600717762', 'https://openalex.org/W2100163972', 'https://openalex.org/W2113037082', 'https://openalex.org/W2120432176', 'https://openalex.org/W64187236']",2018-07-02
https://openalex.org/W3110371022,https://doi.org/10.1109/taslp.2020.3042016,Tackling Perception Bias in Unsupervised Phoneme Discovery Using DPGMM-RNN Hybrid Model and Functional Load,"The human perception of phonemes is biased against speech sounds. The lack of correspondence between perceptual phonemes and acoustic signals forms a big challenge in designing unsupervised algorithms to distinguish phonemes from sound. We propose the DPGMM-RNN hybrid model that improves phoneme categorization by relieving the fragmentation problem. We also merge segments with low functional load, which is the work done by segment contrasts to differentiate between utterances, just like humans who convert unambiguous segments into phonemes as units for immediate perception. Our results show that the DPGMM-RNN hybrid model relieves the fragmentation problem and improves phoneme discriminability. The minimal functional load merge compresses a segment system, preserves information and keeps phoneme discriminability.","['https://openalex.org/W2579555219', 'https://openalex.org/W2051512859', 'https://openalex.org/W6711866534', 'https://openalex.org/W2252172689', 'https://openalex.org/W1994158173', 'https://openalex.org/W6824062597', 'https://openalex.org/W2586754519', 'https://openalex.org/W6674922813', 'https://openalex.org/W6677461952', 'https://openalex.org/W6602180557', 'https://openalex.org/W6713016582', 'https://openalex.org/W2038101708', 'https://openalex.org/W6678947187', 'https://openalex.org/W2940544976', 'https://openalex.org/W4254499902', 'https://openalex.org/W2117041980', 'https://openalex.org/W6675022971', 'https://openalex.org/W91681889', 'https://openalex.org/W2068247585', 'https://openalex.org/W1977531436', 'https://openalex.org/W2015394094', 'https://openalex.org/W2011334394', 'https://openalex.org/W4246695671', 'https://openalex.org/W6682569104', 'https://openalex.org/W1600008395', 'https://openalex.org/W6713593416', 'https://openalex.org/W2950414763', 'https://openalex.org/W6745117592', 'https://openalex.org/W2752796333', 'https://openalex.org/W2095089846', 'https://openalex.org/W6675354045', 'https://openalex.org/W6734901337', 'https://openalex.org/W6680970901', 'https://openalex.org/W6713256719', 'https://openalex.org/W2972374322', 'https://openalex.org/W2963830550', 'https://openalex.org/W2759889345', 'https://openalex.org/W2347098582', 'https://openalex.org/W2787447541', 'https://openalex.org/W6712553779', 'https://openalex.org/W2056133372', 'https://openalex.org/W2057007397', 'https://openalex.org/W2170659185', 'https://openalex.org/W6973666849', 'https://openalex.org/W2114347655', 'https://openalex.org/W2020607164', 'https://openalex.org/W6638159135', 'https://openalex.org/W6712444837', 'https://openalex.org/W6712202099', 'https://openalex.org/W2010548701', 'https://openalex.org/W2017339534', 'https://openalex.org/W2506406911', 'https://openalex.org/W2509930204', 'https://openalex.org/W2826003142', 'https://openalex.org/W2089381378', 'https://openalex.org/W2345811097', 'https://openalex.org/W2750248772', 'https://openalex.org/W2403015869', 'https://openalex.org/W3145269374', 'https://openalex.org/W2151967501', 'https://openalex.org/W2395899413', 'https://openalex.org/W2396043527', 'https://openalex.org/W1984314602', 'https://openalex.org/W1796128977', 'https://openalex.org/W2396733358', 'https://openalex.org/W2054496875', 'https://openalex.org/W52412328', 'https://openalex.org/W2101234009', 'https://openalex.org/W2796346319', 'https://openalex.org/W2963799213', 'https://openalex.org/W2054168569', 'https://openalex.org/W2963620343', 'https://openalex.org/W2786608204', 'https://openalex.org/W2100768664', 'https://openalex.org/W1635512741', 'https://openalex.org/W2758785877', 'https://openalex.org/W2973026522', 'https://openalex.org/W2138615112', 'https://openalex.org/W2963618559', 'https://openalex.org/W2100163972', 'https://openalex.org/W2895297209', 'https://openalex.org/W2402366697', 'https://openalex.org/W2117126688', 'https://openalex.org/W2399576818', 'https://openalex.org/W2404799143', 'https://openalex.org/W2593779438', 'https://openalex.org/W2128032727']",2020-12-02
https://openalex.org/W3045592404,https://doi.org/10.48550/arxiv.2007.15074,Exploiting Cross-Lingual Knowledge in Unsupervised Acoustic Modeling for Low-Resource Languages,"(Short version of Abstract) This thesis describes an investigation on unsupervised acoustic modeling (UAM) for automatic speech recognition (ASR) in the zero-resource scenario, where only untranscribed speech data is assumed to be available. UAM is not only important in addressing the general problem of data scarcity in ASR technology development but also essential to many non-mainstream applications, for examples, language protection, language acquisition and pathological speech assessment. The present study is focused on two research problems. The first problem concerns unsupervised discovery of basic (subword level) speech units in a given language. Under the zero-resource condition, the speech units could be inferred only from the acoustic signals, without requiring or involving any linguistic direction and/or constraints. The second problem is referred to as unsupervised subword modeling. In its essence a frame-level feature representation needs to be learned from untranscribed speech. The learned feature representation is the basis of subword unit discovery. It is desired to be linguistically discriminative and robust to non-linguistic factors. Particularly extensive use of cross-lingual knowledge in subword unit discovery and modeling is a focus of this research.","['https://openalex.org/W2962693497', 'https://openalex.org/W2963826681', 'https://openalex.org/W2345811097', 'https://openalex.org/W2120209245', 'https://openalex.org/W2963618559', 'https://openalex.org/W2026858810', 'https://openalex.org/W2963620343', 'https://openalex.org/W2402963799', 'https://openalex.org/W2396043527', 'https://openalex.org/W1975728937', 'https://openalex.org/W1599512239', 'https://openalex.org/W2102113734', 'https://openalex.org/W1967924372', 'https://openalex.org/W2024490156', 'https://openalex.org/W1833498382', 'https://openalex.org/W2124558353', 'https://openalex.org/W2115685217', 'https://openalex.org/W2962826786', 'https://openalex.org/W2513125788', 'https://openalex.org/W2112688413', 'https://openalex.org/W2786902352', 'https://openalex.org/W2399978376', 'https://openalex.org/W2888911345', 'https://openalex.org/W2972706021', 'https://openalex.org/W2293634267', 'https://openalex.org/W2964245029', 'https://openalex.org/W2025198378', 'https://openalex.org/W2072563337', 'https://openalex.org/W2404799143', 'https://openalex.org/W2895297209', 'https://openalex.org/W1957665339', 'https://openalex.org/W2641832364', 'https://openalex.org/W2963134917', 'https://openalex.org/W2787223168', 'https://openalex.org/W2962689740', 'https://openalex.org/W2162021827', 'https://openalex.org/W2586754519', 'https://openalex.org/W2399576818', 'https://openalex.org/W1545920196', 'https://openalex.org/W3125709657', 'https://openalex.org/W2401464865', 'https://openalex.org/W2110589736', 'https://openalex.org/W2187089797', 'https://openalex.org/W2048526313', 'https://openalex.org/W2509930204', 'https://openalex.org/W2964121744', 'https://openalex.org/W2962799225', 'https://openalex.org/W1778492285', 'https://openalex.org/W2936120996', 'https://openalex.org/W2295297373', 'https://openalex.org/W2128032727', 'https://openalex.org/W2057007397', 'https://openalex.org/W1959608418', 'https://openalex.org/W2889228998', 'https://openalex.org/W1524333225', 'https://openalex.org/W2398071208', 'https://openalex.org/W2787426069', 'https://openalex.org/W2796339975', 'https://openalex.org/W1631260214', 'https://openalex.org/W2963381607', 'https://openalex.org/W2972812366', 'https://openalex.org/W2787447541', 'https://openalex.org/W2122272452', 'https://openalex.org/W2080972498', 'https://openalex.org/W1526516369', 'https://openalex.org/W2125838338', 'https://openalex.org/W1965842648', 'https://openalex.org/W2514012605', 'https://openalex.org/W2152051032', 'https://openalex.org/W2614542633', 'https://openalex.org/W3095049654', 'https://openalex.org/W2748598007', 'https://openalex.org/W2056786202', 'https://openalex.org/W308497914', 'https://openalex.org/W2403015869', 'https://openalex.org/W1950396994', 'https://openalex.org/W2002342963', 'https://openalex.org/W2160815625', 'https://openalex.org/W2785860501', 'https://openalex.org/W1984076147', 'https://openalex.org/W162588823', 'https://openalex.org/W2963137467', 'https://openalex.org/W2100768664', 'https://openalex.org/W2914746235', 'https://openalex.org/W2005708641', 'https://openalex.org/W2963684067', 'https://openalex.org/W2062914951', 'https://openalex.org/W2057653135', 'https://openalex.org/W2015688877', 'https://openalex.org/W3145738572', 'https://openalex.org/W2810166208', 'https://openalex.org/W2100969003', 'https://openalex.org/W2594951208', 'https://openalex.org/W2400549570', 'https://openalex.org/W2083904075', 'https://openalex.org/W2962684181', 'https://openalex.org/W2140567543', 'https://openalex.org/W2072349636', 'https://openalex.org/W1965555277', 'https://openalex.org/W2116422968', 'https://openalex.org/W3007486152', 'https://openalex.org/W2785415724', 'https://openalex.org/W2171752983', 'https://openalex.org/W2078769636', 'https://openalex.org/W2750248772', 'https://openalex.org/W2347098582', 'https://openalex.org/W2405703722', 'https://openalex.org/W185189022', 'https://openalex.org/W1528778941', 'https://openalex.org/W2168319451', 'https://openalex.org/W2766219058', 'https://openalex.org/W1796128977', 'https://openalex.org/W3105242324', 'https://openalex.org/W2106554350', 'https://openalex.org/W2518312472', 'https://openalex.org/W2121997342', 'https://openalex.org/W2972574141', 'https://openalex.org/W2786608204', 'https://openalex.org/W1578200545', 'https://openalex.org/W2127498532', 'https://openalex.org/W2826003142']",2020-07-29
https://openalex.org/W3081171875,https://doi.org/10.1109/access.2020.3019253,Individual Commute Time Recognition Based on the Hierarchical Semantic Model,"Individual commute time recognition is essential for traffic demand management. However, this problem has yet to be studied. In this study, we propose a hierarchical semantic model (HSM) to recognize individual commute time. To the best of our knowledge, this work is the first to integrates large scale travellers commute time prediction at an individual level. HSM consists of a low and a high semantic layer. The low semantic layer models spatial, temporal and environmental information, whereas the high semantic layer recognises commute time using the hidden Markov model on the basis of the low semantic layer outputs. Experimental results demonstrate the effectiveness of our proposed model for individual commute time recognition.","['https://openalex.org/W1991133427', 'https://openalex.org/W2148048965', 'https://openalex.org/W6623247700', 'https://openalex.org/W2466559273', 'https://openalex.org/W2147917079', 'https://openalex.org/W2105594594', 'https://openalex.org/W2090766960', 'https://openalex.org/W2068006147', 'https://openalex.org/W6610471430', 'https://openalex.org/W2770711225', 'https://openalex.org/W2582293461', 'https://openalex.org/W2783414436', 'https://openalex.org/W2902882345', 'https://openalex.org/W2791827527', 'https://openalex.org/W2804739511', 'https://openalex.org/W2906075775', 'https://openalex.org/W2097316030', 'https://openalex.org/W2169189000', 'https://openalex.org/W1562033953', 'https://openalex.org/W6605479355', 'https://openalex.org/W156634103', 'https://openalex.org/W2090805767', 'https://openalex.org/W2043980986', 'https://openalex.org/W2797009285', 'https://openalex.org/W2042493333', 'https://openalex.org/W2775580576', 'https://openalex.org/W2277981673', 'https://openalex.org/W2917930670', 'https://openalex.org/W2624829428', 'https://openalex.org/W2114092915', 'https://openalex.org/W178496478', 'https://openalex.org/W6712963495', 'https://openalex.org/W1977808218', 'https://openalex.org/W2602044573', 'https://openalex.org/W1596652937', 'https://openalex.org/W1065174047', 'https://openalex.org/W2586754519', 'https://openalex.org/W2400718301', 'https://openalex.org/W134960717', 'https://openalex.org/W293916754', 'https://openalex.org/W819707303']",2020-01-01
https://openalex.org/W3200461789,https://doi.org/10.36227/techrxiv.16618135.v1,Non-Parametric Bayesian Subspace Models for Acoustic Unit Discovery,"This work investigates subspace non-parametric models for the task of learning a set of acoustic units from unlabeled speech recordings. We constrain the base-measure of a Dirichlet-Process mixture with a phonetic subspace---estimated from other source languages---to build an \emph{educated prior}, thereby forcing the learned acoustic units to resemble phones of known source languages. Two types of models are proposed: (i) the Subspace HMM (SHMM) which assumes that the phonetic subspace is the same for every language, (ii) the Hierarchical-Subspace HMM (H-SHMM) which relaxes this assumption and allows to have a language-specific subspace estimated on the unlabeled target data. These models are applied on 3 languages: English, Yoruba and Mboshi and they are compared with various competitive acoustic units discovery baselines. Experimental results show that both subspace models outperform other systems in terms of clustering quality and segmentation accuracy. Moreover, we observe that the H-SHMM provides results superior to the SHMM supporting the idea that language-specific priors are preferable to language-agnostic priors for acoustic unit discovery.","['https://openalex.org/W2928408492', 'https://openalex.org/W2193413348', 'https://openalex.org/W2545177271', 'https://openalex.org/W2483390977', 'https://openalex.org/W6973666849', 'https://openalex.org/W6697293080', 'https://openalex.org/W2940544976', 'https://openalex.org/W2100768664', 'https://openalex.org/W2399576818', 'https://openalex.org/W2586754519', 'https://openalex.org/W6704752648', 'https://openalex.org/W3125709657', 'https://openalex.org/W2979476256', 'https://openalex.org/W2991557631', 'https://openalex.org/W2750248772', 'https://openalex.org/W2888911345', 'https://openalex.org/W2927191280', 'https://openalex.org/W3100202343', 'https://openalex.org/W6676755231', 'https://openalex.org/W1503207992', 'https://openalex.org/W2077804127', 'https://openalex.org/W4394867155', 'https://openalex.org/W4237840503', 'https://openalex.org/W7048738093', 'https://openalex.org/W2762715843', 'https://openalex.org/W6784355959', 'https://openalex.org/W2195354', 'https://openalex.org/W2401271873', 'https://openalex.org/W2572097499', 'https://openalex.org/W2401396251', 'https://openalex.org/W2752796333', 'https://openalex.org/W2972374322', 'https://openalex.org/W3093096176', 'https://openalex.org/W3112613336', 'https://openalex.org/W1494198834', 'https://openalex.org/W2125838338', 'https://openalex.org/W2115979064', 'https://openalex.org/W2072169887']",2021-09-17
https://openalex.org/W4392884616,https://doi.org/10.5121/ijci.2024.130201,Direct Punjabi to English Speech Translation using Discrete Units,"Speech-to-speech translation is yet to reach the same level of coverage as text-to-text translation systems. The current speech technology is highly limited in its coverage of over 7000 languages spoken worldwide, leaving more than half of the population deprived of such technology and shared experiences. With voice-assisted technology (such as social robots and speech-to-text apps) and auditory content (such as podcasts and lectures) on the rise, ensuring that the technology is available for all is more important than ever. Speech translation can play a vital role in mitigating technological disparity and creating a more inclusive society. With a motive to contribute towards speech translation research for low-resource languages, our work presents a direct speech-to-speech translation model for one of the Indic languages called Punjabi to English. Additionally, we explore the performance of using a discrete representation of speech called discrete acoustic units as input to the Transformer-based translation model. The model, abbreviated as Unit-to-Unit Translation (U2UT), takes a sequence of discrete units of the source language (the language being translated from) and outputs a sequence of discrete units of the target language (the language being translated to). Our results show that the U2UT model performs better than the Speechto-Unit Translation (S2UT) model by a 3.69 BLEU score.","['https://openalex.org/W6761353324', 'https://openalex.org/W6796730497', 'https://openalex.org/W6852909395', 'https://openalex.org/W4309129001', 'https://openalex.org/W6798080464', 'https://openalex.org/W4221164184', 'https://openalex.org/W4378505287', 'https://openalex.org/W2924093092', 'https://openalex.org/W4378473793', 'https://openalex.org/W4283121045', 'https://openalex.org/W6842730932', 'https://openalex.org/W2936184970', 'https://openalex.org/W4387162606', 'https://openalex.org/W3036601975', 'https://openalex.org/W3169320628', 'https://openalex.org/W4387687030', 'https://openalex.org/W3025165719', 'https://openalex.org/W6739901393', 'https://openalex.org/W6810419249', 'https://openalex.org/W4226543485', 'https://openalex.org/W4384648564', 'https://openalex.org/W4311731008', 'https://openalex.org/W3142316150', 'https://openalex.org/W4381827575', 'https://openalex.org/W4386566860', 'https://openalex.org/W3215465553', 'https://openalex.org/W2980109192', 'https://openalex.org/W4310079959', 'https://openalex.org/W4311550865', 'https://openalex.org/W4285077564', 'https://openalex.org/W3095410713', 'https://openalex.org/W2995929068', 'https://openalex.org/W3054645415', 'https://openalex.org/W2292087804', 'https://openalex.org/W3106807794', 'https://openalex.org/W3119308075', 'https://openalex.org/W2966095117', 'https://openalex.org/W4308756394', 'https://openalex.org/W4281621399', 'https://openalex.org/W4293332626', 'https://openalex.org/W1494198834', 'https://openalex.org/W4311000453', 'https://openalex.org/W6898505805', 'https://openalex.org/W6750200984', 'https://openalex.org/W2972495969', 'https://openalex.org/W4382202628', 'https://openalex.org/W3097777922', 'https://openalex.org/W3180374548', 'https://openalex.org/W4306393960', 'https://openalex.org/W3209059054', 'https://openalex.org/W3174758275', 'https://openalex.org/W3139878283', 'https://openalex.org/W4385570009', 'https://openalex.org/W4389524529', 'https://openalex.org/W4296070387', 'https://openalex.org/W4226444650', 'https://openalex.org/W4385245566', 'https://openalex.org/W4385570550', 'https://openalex.org/W4287854499', 'https://openalex.org/W4386142098', 'https://openalex.org/W2963532001', 'https://openalex.org/W4378105483', 'https://openalex.org/W4385571229', 'https://openalex.org/W3030437843', 'https://openalex.org/W4389518827', 'https://openalex.org/W4301980136', 'https://openalex.org/W4385572318', 'https://openalex.org/W4287887366', 'https://openalex.org/W4319862635', 'https://openalex.org/W2937197076']",2024-03-10
https://openalex.org/W4392884833,https://doi.org/10.5121/ijci.2024.1302,,"Speech-to-speech translation is yet to reach the same level of coverage as text-to-text translation systems.The current speech technology is highly limited in its coverage of over 7000 languages spoken worldwide, leaving more than half of the population deprived of such technology and shared experiences.With voice-assisted technology (such as social robots and speech-to-text apps) and auditory content (such as podcasts and lectures) on the rise, ensuring that the technology is available for all is more important than ever.Speech translation can play a vital role in mitigating technological disparity and creating a more inclusive society.With a motive to contribute towards speech translation research for low-resource languages, our work presents a direct speech-to-speech translation model for one of the Indic languages called Punjabi to English.Additionally, we explore the performance of using a discrete representation of speech called discrete acoustic units as input to the Transformer-based translation model.The model, abbreviated as Unit-to-Unit Translation (U2UT), takes a sequence of discrete units of the source language (the language being translated from) and outputs a sequence of discrete units of the target language (the language being translated to).Our results show that the U2UT model performs better than the Speechto-Unit Translation (S2UT) model by a 3.69 BLEU score.","['https://openalex.org/W6761353324', 'https://openalex.org/W6796730497', 'https://openalex.org/W6852909395', 'https://openalex.org/W4309129001', 'https://openalex.org/W6798080464', 'https://openalex.org/W4221164184', 'https://openalex.org/W4378505287', 'https://openalex.org/W2924093092', 'https://openalex.org/W4378473793', 'https://openalex.org/W4283121045', 'https://openalex.org/W6842730932', 'https://openalex.org/W2936184970', 'https://openalex.org/W4387162606', 'https://openalex.org/W3036601975', 'https://openalex.org/W3169320628', 'https://openalex.org/W4387687030', 'https://openalex.org/W3025165719', 'https://openalex.org/W6739901393', 'https://openalex.org/W6810419249', 'https://openalex.org/W4226543485', 'https://openalex.org/W4384648564', 'https://openalex.org/W4311731008', 'https://openalex.org/W3142316150', 'https://openalex.org/W4381827575', 'https://openalex.org/W4386566860', 'https://openalex.org/W3215465553', 'https://openalex.org/W2980109192', 'https://openalex.org/W4310079959', 'https://openalex.org/W4311550865', 'https://openalex.org/W4285077564', 'https://openalex.org/W3095410713', 'https://openalex.org/W2995929068', 'https://openalex.org/W3054645415', 'https://openalex.org/W2292087804', 'https://openalex.org/W3106807794', 'https://openalex.org/W3119308075', 'https://openalex.org/W2966095117', 'https://openalex.org/W4308756394', 'https://openalex.org/W4281621399', 'https://openalex.org/W4293332626', 'https://openalex.org/W1494198834', 'https://openalex.org/W4311000453', 'https://openalex.org/W6898505805', 'https://openalex.org/W6750200984', 'https://openalex.org/W4385570009', 'https://openalex.org/W4385572318', 'https://openalex.org/W2972495969', 'https://openalex.org/W4287854499', 'https://openalex.org/W4301980136', 'https://openalex.org/W4382202628', 'https://openalex.org/W4386142098', 'https://openalex.org/W4385571229', 'https://openalex.org/W2963532001', 'https://openalex.org/W4306393960', 'https://openalex.org/W3174758275', 'https://openalex.org/W4385570550', 'https://openalex.org/W4226444650', 'https://openalex.org/W3097777922', 'https://openalex.org/W3209059054', 'https://openalex.org/W4296070387', 'https://openalex.org/W4319862635', 'https://openalex.org/W3139878283', 'https://openalex.org/W4385245566', 'https://openalex.org/W4389524529', 'https://openalex.org/W3180374548', 'https://openalex.org/W2937197076', 'https://openalex.org/W4389518827', 'https://openalex.org/W4378105483', 'https://openalex.org/W3030437843', 'https://openalex.org/W4287887366']",2024-03-10
https://openalex.org/W4295308567,https://doi.org/10.1109/jstsp.2022.3206084,Self-Supervised Language Learning From Raw Audio: Lessons From the Zero Resource Speech Challenge,International audience,"['https://openalex.org/W2483390977', 'https://openalex.org/W1529056402', 'https://openalex.org/W3197580070', 'https://openalex.org/W2098500169', 'https://openalex.org/W2117041980', 'https://openalex.org/W2170659185', 'https://openalex.org/W6675022971', 'https://openalex.org/W2025482506', 'https://openalex.org/W2020607164', 'https://openalex.org/W2346964103', 'https://openalex.org/W2963620343', 'https://openalex.org/W2940544976', 'https://openalex.org/W3093096176', 'https://openalex.org/W3197259906', 'https://openalex.org/W2396043527', 'https://openalex.org/W2402366697', 'https://openalex.org/W2399576818', 'https://openalex.org/W2745710152', 'https://openalex.org/W2345811097', 'https://openalex.org/W1796128977', 'https://openalex.org/W2522012644', 'https://openalex.org/W2404799143', 'https://openalex.org/W2400549570', 'https://openalex.org/W2787223168', 'https://openalex.org/W2786902352', 'https://openalex.org/W2787447541', 'https://openalex.org/W2787426069', 'https://openalex.org/W2785415724', 'https://openalex.org/W3100270690', 'https://openalex.org/W3144810982', 'https://openalex.org/W2057007397', 'https://openalex.org/W2964169922', 'https://openalex.org/W2407614114', 'https://openalex.org/W2398490608', 'https://openalex.org/W4313182775', 'https://openalex.org/W4296710617', 'https://openalex.org/W2468716020', 'https://openalex.org/W3097159218', 'https://openalex.org/W3097485645', 'https://openalex.org/W2748009955', 'https://openalex.org/W2347098582', 'https://openalex.org/W2972374322', 'https://openalex.org/W2972964185', 'https://openalex.org/W2972867623', 'https://openalex.org/W2950414763', 'https://openalex.org/W2947445680', 'https://openalex.org/W3003750857', 'https://openalex.org/W2973013862', 'https://openalex.org/W3097056138', 'https://openalex.org/W3161215977', 'https://openalex.org/W3024040651', 'https://openalex.org/W3096262326', 'https://openalex.org/W3097692357', 'https://openalex.org/W3096359985', 'https://openalex.org/W3095361818', 'https://openalex.org/W3096216486', 'https://openalex.org/W6786696081', 'https://openalex.org/W3198815374', 'https://openalex.org/W3197349023', 'https://openalex.org/W3197381195', 'https://openalex.org/W6809593508', 'https://openalex.org/W4292825791', 'https://openalex.org/W3204915839', 'https://openalex.org/W6839738141', 'https://openalex.org/W3209993061', 'https://openalex.org/W6731521493', 'https://openalex.org/W6728823536', 'https://openalex.org/W2395899413', 'https://openalex.org/W2406349064', 'https://openalex.org/W6844194202', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W6811170316', 'https://openalex.org/W1993755070', 'https://openalex.org/W6676025551', 'https://openalex.org/W2110627398', 'https://openalex.org/W3212943633', 'https://openalex.org/W4283332789', 'https://openalex.org/W3156899336', 'https://openalex.org/W4285250921', 'https://openalex.org/W6840200333', 'https://openalex.org/W4297841853', 'https://openalex.org/W2114347655', 'https://openalex.org/W2079460648', 'https://openalex.org/W2049142189', 'https://openalex.org/W1967924372', 'https://openalex.org/W2126377586', 'https://openalex.org/W3044967013', 'https://openalex.org/W2117126688', 'https://openalex.org/W3198782837', 'https://openalex.org/W2964115348', 'https://openalex.org/W2598638573', 'https://openalex.org/W6790356757', 'https://openalex.org/W2995181338', 'https://openalex.org/W3016181583', 'https://openalex.org/W2014307400', 'https://openalex.org/W2996728628', 'https://openalex.org/W2963425185', 'https://openalex.org/W2170682101', 'https://openalex.org/W2103318667', 'https://openalex.org/W2080100102', 'https://openalex.org/W6691746754', 'https://openalex.org/W1854884267', 'https://openalex.org/W2963366649', 'https://openalex.org/W2176085882', 'https://openalex.org/W6680094886', 'https://openalex.org/W2026487812', 'https://openalex.org/W2142625445', 'https://openalex.org/W1494198834', 'https://openalex.org/W6729977899', 'https://openalex.org/W2619697695', 'https://openalex.org/W2586148577', 'https://openalex.org/W2949387496', 'https://openalex.org/W3160251434', 'https://openalex.org/W2332013542', 'https://openalex.org/W2069857264', 'https://openalex.org/W3209984917', 'https://openalex.org/W3198217962', 'https://openalex.org/W6810531757', 'https://openalex.org/W4225726571', 'https://openalex.org/W3093121832', 'https://openalex.org/W3148101939', 'https://openalex.org/W3007068036', 'https://openalex.org/W2972495969', 'https://openalex.org/W3180374548', 'https://openalex.org/W6799081819', 'https://openalex.org/W4287854499', 'https://openalex.org/W4287591426', 'https://openalex.org/W2973026522', 'https://openalex.org/W3098643042', 'https://openalex.org/W4307680525', 'https://openalex.org/W3099142230', 'https://openalex.org/W4285483774', 'https://openalex.org/W1973746598', 'https://openalex.org/W3186843219', 'https://openalex.org/W3096196861', 'https://openalex.org/W4297808394', 'https://openalex.org/W4394671563', 'https://openalex.org/W3160704724', 'https://openalex.org/W3097682198', 'https://openalex.org/W2132631284', 'https://openalex.org/W4221161768', 'https://openalex.org/W4226199158']",2022-09-12
https://openalex.org/W4285114014,https://doi.org/10.18653/v1/2022.acl-srw.15,Scoping natural language processing in Indonesian and Malay for education applications,"Indonesian and Malay are underrepresented in the development of natural language processing (NLP) technologies and available resources are difficult to find. A clear picture of existing work can invigorate and inform how researchers conceptualise worthwhile projects. Using an education sector project to motivate the study, we conducted a wide-ranging overview of Indonesian and Malay human language technologies and corpus work. We charted 657 included studies according to Hirschberg and Manning's 2015 description of NLP, concluding that the field was dominated by exploratory corpus work, machine reading of text gathered from the Internet, and sentiment analysis. In this paper, we identify most published authors and research hubs, and make a number of recommendations to encourage future collaboration and efficiency within NLP in Indonesian and Malay.","['https://openalex.org/W2785910358', 'https://openalex.org/W2766359537', 'https://openalex.org/W2535634137', 'https://openalex.org/W3015884017', 'https://openalex.org/W2787886961', 'https://openalex.org/W3167490019', 'https://openalex.org/W2888836568', 'https://openalex.org/W3120342742', 'https://openalex.org/W4401047862', 'https://openalex.org/W4390219078', 'https://openalex.org/W3012321455', 'https://openalex.org/W2990190303', 'https://openalex.org/W2914407211', 'https://openalex.org/W2944268210', 'https://openalex.org/W2945036586', 'https://openalex.org/W3158335200', 'https://openalex.org/W2912187167', 'https://openalex.org/W2902320061', 'https://openalex.org/W2957782849', 'https://openalex.org/W3191873725', 'https://openalex.org/W2785716456', 'https://openalex.org/W2786860311', 'https://openalex.org/W2908220973', 'https://openalex.org/W2769679540', 'https://openalex.org/W2910746229', 'https://openalex.org/W3137156863', 'https://openalex.org/W2888839285', 'https://openalex.org/W3111434626', 'https://openalex.org/W3010964056', 'https://openalex.org/W2759599846', 'https://openalex.org/W2105321265', 'https://openalex.org/W3034610791', 'https://openalex.org/W2593680041', 'https://openalex.org/W2909159847', 'https://openalex.org/W3144188103', 'https://openalex.org/W2997701690', 'https://openalex.org/W3016890750', 'https://openalex.org/W2908994570', 'https://openalex.org/W2308338022', 'https://openalex.org/W3142754618', 'https://openalex.org/W3000720443', 'https://openalex.org/W2600695088', 'https://openalex.org/W2128529468', 'https://openalex.org/W2281591411', 'https://openalex.org/W2912029571', 'https://openalex.org/W3011465266', 'https://openalex.org/W2973186443', 'https://openalex.org/W3125335606', 'https://openalex.org/W2946426362', 'https://openalex.org/W3003750857', 'https://openalex.org/W2979331487', 'https://openalex.org/W3030632891', 'https://openalex.org/W2946929313', 'https://openalex.org/W2963443688', 'https://openalex.org/W2989796633', 'https://openalex.org/W2548482495', 'https://openalex.org/W2794360735', 'https://openalex.org/W3169558314', 'https://openalex.org/W3170298412', 'https://openalex.org/W2766561076', 'https://openalex.org/W2997003084', 'https://openalex.org/W2808051947', 'https://openalex.org/W3188006241', 'https://openalex.org/W2576398564', 'https://openalex.org/W2761489884', 'https://openalex.org/W2518800590', 'https://openalex.org/W3045614712', 'https://openalex.org/W2980110173', 'https://openalex.org/W3025820403', 'https://openalex.org/W2916541587', 'https://openalex.org/W2927700198', 'https://openalex.org/W2950372439', 'https://openalex.org/W3014300186', 'https://openalex.org/W2754921686', 'https://openalex.org/W3096013431', 'https://openalex.org/W2912916121', 'https://openalex.org/W3006050938', 'https://openalex.org/W3082302385', 'https://openalex.org/W4301181925', 'https://openalex.org/W3127786201', 'https://openalex.org/W2793817673', 'https://openalex.org/W2768382261', 'https://openalex.org/W3034047491', 'https://openalex.org/W2999109430', 'https://openalex.org/W3171317404', 'https://openalex.org/W2785675894', 'https://openalex.org/W2608955778', 'https://openalex.org/W3198028910', 'https://openalex.org/W4205552180', 'https://openalex.org/W4253406917', 'https://openalex.org/W4252435641', 'https://openalex.org/W2570426407', 'https://openalex.org/W2914968166', 'https://openalex.org/W3193556690', 'https://openalex.org/W3105668574', 'https://openalex.org/W2815393564', 'https://openalex.org/W2796131631', 'https://openalex.org/W2967526790', 'https://openalex.org/W2902129306', 'https://openalex.org/W2620054379', 'https://openalex.org/W3188061932', 'https://openalex.org/W2522742170', 'https://openalex.org/W3094098378', 'https://openalex.org/W2347126719', 'https://openalex.org/W2806851206', 'https://openalex.org/W2911295418', 'https://openalex.org/W2991918928', 'https://openalex.org/W2937442668', 'https://openalex.org/W2787205576', 'https://openalex.org/W3133667170', 'https://openalex.org/W2806820836', 'https://openalex.org/W3161590189', 'https://openalex.org/W2608568886', 'https://openalex.org/W3126876915', 'https://openalex.org/W2620187298', 'https://openalex.org/W2593678616', 'https://openalex.org/W2787936194', 'https://openalex.org/W3020333167', 'https://openalex.org/W3006318321', 'https://openalex.org/W3106714682', 'https://openalex.org/W3118615836', 'https://openalex.org/W2343004018', 'https://openalex.org/W2320617275', 'https://openalex.org/W3079443720', 'https://openalex.org/W3006343898', 'https://openalex.org/W2914402937', 'https://openalex.org/W3095996396', 'https://openalex.org/W4247011519', 'https://openalex.org/W3108136915', 'https://openalex.org/W3035127930', 'https://openalex.org/W2573761686', 'https://openalex.org/W2784059283', 'https://openalex.org/W2595261013', 'https://openalex.org/W2996583951', 'https://openalex.org/W2793884006', 'https://openalex.org/W2996791130', 'https://openalex.org/W3098877845', 'https://openalex.org/W3082113259', 'https://openalex.org/W3043843904', 'https://openalex.org/W3156468118', 'https://openalex.org/W3175524270', 'https://openalex.org/W2951107454', 'https://openalex.org/W3011741173', 'https://openalex.org/W2889088930', 'https://openalex.org/W2889549216', 'https://openalex.org/W3004901654', 'https://openalex.org/W3159992816', 'https://openalex.org/W2892130889', 'https://openalex.org/W2914729011', 'https://openalex.org/W2790783857', 'https://openalex.org/W3004125813', 'https://openalex.org/W3080997347', 'https://openalex.org/W2986929165', 'https://openalex.org/W2805864138', 'https://openalex.org/W2902371504', 'https://openalex.org/W2739688273', 'https://openalex.org/W2938041762', 'https://openalex.org/W3190062546', 'https://openalex.org/W3200337733', 'https://openalex.org/W3005133516', 'https://openalex.org/W2889339709', 'https://openalex.org/W3014073856', 'https://openalex.org/W2587007004', 'https://openalex.org/W3046343061', 'https://openalex.org/W3000239963', 'https://openalex.org/W3127004965', 'https://openalex.org/W3097098538', 'https://openalex.org/W3119059017', 'https://openalex.org/W3035689184', 'https://openalex.org/W2973319843', 'https://openalex.org/W2912022564', 'https://openalex.org/W3156761824', 'https://openalex.org/W2972617191', 'https://openalex.org/W3160620413', 'https://openalex.org/W2890384882', 'https://openalex.org/W2557800753', 'https://openalex.org/W3024656694', 'https://openalex.org/W3081932574', 'https://openalex.org/W3189424591', 'https://openalex.org/W2969905756', 'https://openalex.org/W2346196143', 'https://openalex.org/W3131684216', 'https://openalex.org/W2793310538', 'https://openalex.org/W3012044803', 'https://openalex.org/W3134157045', 'https://openalex.org/W2899816737', 'https://openalex.org/W2971157009', 'https://openalex.org/W2744072818', 'https://openalex.org/W2768863797', 'https://openalex.org/W3198730096', 'https://openalex.org/W2783293652', 'https://openalex.org/W2892813356', 'https://openalex.org/W2889648809', 'https://openalex.org/W3033675505', 'https://openalex.org/W2897149133', 'https://openalex.org/W2765619474', 'https://openalex.org/W2621145749', 'https://openalex.org/W3133194243', 'https://openalex.org/W3137808379', 'https://openalex.org/W2799036097', 'https://openalex.org/W2776615741', 'https://openalex.org/W2472116542', 'https://openalex.org/W3020915581', 'https://openalex.org/W2740860032', 'https://openalex.org/W2766056037', 'https://openalex.org/W2769245847', 'https://openalex.org/W2529018162', 'https://openalex.org/W3105543199', 'https://openalex.org/W2766705981', 'https://openalex.org/W2783693193', 'https://openalex.org/W2765199231', 'https://openalex.org/W3141795885', 'https://openalex.org/W2761113469', 'https://openalex.org/W2765256054', 'https://openalex.org/W2948933207', 'https://openalex.org/W2765099312', 'https://openalex.org/W3137667865', 'https://openalex.org/W3028048818', 'https://openalex.org/W3005889908', 'https://openalex.org/W2244154047', 'https://openalex.org/W2787709230', 'https://openalex.org/W2977590596', 'https://openalex.org/W3114934890', 'https://openalex.org/W2995500257', 'https://openalex.org/W2810245259', 'https://openalex.org/W2763877810', 'https://openalex.org/W3142766333', 'https://openalex.org/W3126910220', 'https://openalex.org/W2586497004', 'https://openalex.org/W2514734817', 'https://openalex.org/W2914384032', 'https://openalex.org/W2790169658', 'https://openalex.org/W3149617983', 'https://openalex.org/W2896039991', 'https://openalex.org/W2346735000', 'https://openalex.org/W3199572437', 'https://openalex.org/W2783397397', 'https://openalex.org/W3033034077', 'https://openalex.org/W3086966320', 'https://openalex.org/W3016792169', 'https://openalex.org/W4319345165', 'https://openalex.org/W3133737868', 'https://openalex.org/W2993054853', 'https://openalex.org/W2944521398', 'https://openalex.org/W2801109130', 'https://openalex.org/W2767188604', 'https://openalex.org/W2490160489', 'https://openalex.org/W3189217652', 'https://openalex.org/W2766080364', 'https://openalex.org/W3137692635', 'https://openalex.org/W2988430602', 'https://openalex.org/W2891378911', 'https://openalex.org/W2531310868', 'https://openalex.org/W3033914564', 'https://openalex.org/W2979438387', 'https://openalex.org/W2965944178', 'https://openalex.org/W2777246534', 'https://openalex.org/W2746763201', 'https://openalex.org/W2969012838', 'https://openalex.org/W2933487192', 'https://openalex.org/W2602281553', 'https://openalex.org/W3173023103', 'https://openalex.org/W3005285695', 'https://openalex.org/W3155024638', 'https://openalex.org/W2944762197', 'https://openalex.org/W3094203466', 'https://openalex.org/W3085012454', 'https://openalex.org/W3011801524', 'https://openalex.org/W3009456793', 'https://openalex.org/W3155618463', 'https://openalex.org/W2791329706', 'https://openalex.org/W2973089652', 'https://openalex.org/W3000816625', 'https://openalex.org/W2905596980', 'https://openalex.org/W2996309330', 'https://openalex.org/W2625189579', 'https://openalex.org/W3092542905', 'https://openalex.org/W2793015393', 'https://openalex.org/W2972775773', 'https://openalex.org/W4403789977', 'https://openalex.org/W3132746979', 'https://openalex.org/W2759765025', 'https://openalex.org/W3014061204', 'https://openalex.org/W2345601783', 'https://openalex.org/W2593739431', 'https://openalex.org/W2791539854', 'https://openalex.org/W3171868814', 'https://openalex.org/W3120486233', 'https://openalex.org/W3134389903', 'https://openalex.org/W2901229000', 'https://openalex.org/W3058857442', 'https://openalex.org/W2766724669', 'https://openalex.org/W2901019676', 'https://openalex.org/W3026980472', 'https://openalex.org/W3096493096', 'https://openalex.org/W2792225724', 'https://openalex.org/W2610921553', 'https://openalex.org/W2547253535', 'https://openalex.org/W2949421773', 'https://openalex.org/W2735803390', 'https://openalex.org/W2967713415', 'https://openalex.org/W3112597416', 'https://openalex.org/W2068264290', 'https://openalex.org/W2970702953', 'https://openalex.org/W567338163', 'https://openalex.org/W2757853837', 'https://openalex.org/W2055527449', 'https://openalex.org/W3183260361', 'https://openalex.org/W2785781915', 'https://openalex.org/W2981737912', 'https://openalex.org/W2571725237', 'https://openalex.org/W3047925692', 'https://openalex.org/W3172927942', 'https://openalex.org/W2768914344', 'https://openalex.org/W2785214507', 'https://openalex.org/W4387871617', 'https://openalex.org/W2889516935', 'https://openalex.org/W3210030168', 'https://openalex.org/W2766417290', 'https://openalex.org/W3159997281', 'https://openalex.org/W2964803557', 'https://openalex.org/W2560014971', 'https://openalex.org/W3111934825']",2022-01-01
https://openalex.org/W3129009457,https://doi.org/10.48550/arxiv.2102.01192,Generative Spoken Language Modeling from Raw Audio,"We introduce Generative Spoken Language Modeling, the task of learning the acoustic and linguistic characteristics of a language from raw audio (no text, no labels), and a set of metrics to automatically evaluate the learned representations at acoustic and linguistic levels for both encoding and generation. We set up baseline systems consisting of a discrete speech encoder (returning pseudo-text units), a generative language model (trained on pseudo-text), and a speech decoder (generating a waveform from pseudo-text) all trained without supervision and validate the proposed metrics with human evaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that the number of discrete units (50, 100, or 200) matters in a task-dependent and encoder-dependent way, and that some combinations approach text-based systems.","['https://openalex.org/W2787560479', 'https://openalex.org/W2996383576', 'https://openalex.org/W3097787369', 'https://openalex.org/W3096216486', 'https://openalex.org/W2100768664', 'https://openalex.org/W3095698432', 'https://openalex.org/W2963341956', 'https://openalex.org/W3035202887', 'https://openalex.org/W2890983311', 'https://openalex.org/W2949382160', 'https://openalex.org/W1494198834', 'https://openalex.org/W3098403858', 'https://openalex.org/W3148040514', 'https://openalex.org/W3039910566', 'https://openalex.org/W2937090315', 'https://openalex.org/W2842511635', 'https://openalex.org/W2972374322', 'https://openalex.org/W2973157397', 'https://openalex.org/W3095361818', 'https://openalex.org/W3163296124', 'https://openalex.org/W3095948607', 'https://openalex.org/W2963618559', 'https://openalex.org/W2973049979', 'https://openalex.org/W3095292526', 'https://openalex.org/W3049206033', 'https://openalex.org/W2933138175', 'https://openalex.org/W2750248772', 'https://openalex.org/W3096323553', 'https://openalex.org/W2127141656', 'https://openalex.org/W2971274815', 'https://openalex.org/W2950180292', 'https://openalex.org/W3003875258', 'https://openalex.org/W2888911345', 'https://openalex.org/W2963403868', 'https://openalex.org/W2160473997', 'https://openalex.org/W2963799213', 'https://openalex.org/W3096359985', 'https://openalex.org/W2947445680', 'https://openalex.org/W3099782249', 'https://openalex.org/W3003750857', 'https://openalex.org/W3033038061', 'https://openalex.org/W3112034174', 'https://openalex.org/W2347098582', 'https://openalex.org/W3015213852', 'https://openalex.org/W3015265920', 'https://openalex.org/W2346964103', 'https://openalex.org/W3125087428', 'https://openalex.org/W2982223350', 'https://openalex.org/W2963300588', 'https://openalex.org/W2483390977', 'https://openalex.org/W2962850167', 'https://openalex.org/W2963456134', 'https://openalex.org/W2577366047', 'https://openalex.org/W3024040651', 'https://openalex.org/W3093096176', 'https://openalex.org/W2982399380', 'https://openalex.org/W3148101939', 'https://openalex.org/W2965373594', 'https://openalex.org/W3114436296', 'https://openalex.org/W3015356564', 'https://openalex.org/W2972943112']",2021-02-01
https://openalex.org/W3098557217,https://doi.org/10.1109/taslp.2020.3038524,An Overview of Voice Conversion and Its Challenges: From Statistical Modeling to Deep Learning,"Speaker identity is one of the important characteristics of human speech. In voice conversion, we change the speaker identity from one to another, while keeping the linguistic content unchanged. Voice conversion involves multiple speech processing techniques, such as speech analysis, spectral conversion, prosody conversion, speaker characterization, and vocoding. With the recent advances in theory and practice, we are now able to produce human-like voice quality with high speaker similarity. In this article, we provide a comprehensive overview of the state-of-the-art of voice conversion techniques and their performance evaluation methods from the statistical approaches to deep learning, and discuss their promise and limitations. We will also report the recent Voice Conversion Challenges (VCC), the performance of the current state of technology, and provide a summary of the available resources for voice conversion research.","['https://openalex.org/W2169579015', 'https://openalex.org/W2800788625', 'https://openalex.org/W2122159244', 'https://openalex.org/W2144351953', 'https://openalex.org/W6713985435', 'https://openalex.org/W2154920538', 'https://openalex.org/W6677570743', 'https://openalex.org/W6711854987', 'https://openalex.org/W2086796102', 'https://openalex.org/W2116261113', 'https://openalex.org/W2064675550', 'https://openalex.org/W2517513811', 'https://openalex.org/W1689711448', 'https://openalex.org/W6739901393', 'https://openalex.org/W6679434410', 'https://openalex.org/W2057609679', 'https://openalex.org/W6697270646', 'https://openalex.org/W2179428711', 'https://openalex.org/W2052274528', 'https://openalex.org/W2171019095', 'https://openalex.org/W2518172956', 'https://openalex.org/W6712160772', 'https://openalex.org/W2005438552', 'https://openalex.org/W1991402032', 'https://openalex.org/W2137983211', 'https://openalex.org/W2157412983', 'https://openalex.org/W2114543868', 'https://openalex.org/W6605232188', 'https://openalex.org/W2576309025', 'https://openalex.org/W2401207139', 'https://openalex.org/W6765779288', 'https://openalex.org/W2975414524', 'https://openalex.org/W3024752295', 'https://openalex.org/W2972394484', 'https://openalex.org/W6732637432', 'https://openalex.org/W6745697700', 'https://openalex.org/W2963609956', 'https://openalex.org/W6729924827', 'https://openalex.org/W2511311723', 'https://openalex.org/W2157331557', 'https://openalex.org/W2899877258', 'https://openalex.org/W2897353073', 'https://openalex.org/W2767052532', 'https://openalex.org/W2963073614', 'https://openalex.org/W6756102363', 'https://openalex.org/W6737778391', 'https://openalex.org/W1902237438', 'https://openalex.org/W6602386084', 'https://openalex.org/W2161476805', 'https://openalex.org/W2327501763', 'https://openalex.org/W2135832479', 'https://openalex.org/W2402356521', 'https://openalex.org/W2785608393', 'https://openalex.org/W1965912016', 'https://openalex.org/W2152974894', 'https://openalex.org/W6693024853', 'https://openalex.org/W1552314771', 'https://openalex.org/W6676626839', 'https://openalex.org/W2963403924', 'https://openalex.org/W1562777581', 'https://openalex.org/W2802935216', 'https://openalex.org/W2964195110', 'https://openalex.org/W2515028311', 'https://openalex.org/W2963539064', 'https://openalex.org/W2889329491', 'https://openalex.org/W6840412704', 'https://openalex.org/W2996414377', 'https://openalex.org/W2588445447', 'https://openalex.org/W6685748340', 'https://openalex.org/W6637576829', 'https://openalex.org/W1572730534', 'https://openalex.org/W6606229746', 'https://openalex.org/W6631184489', 'https://openalex.org/W2123299109', 'https://openalex.org/W2145130307', 'https://openalex.org/W2963035245', 'https://openalex.org/W6601306257', 'https://openalex.org/W2512087624', 'https://openalex.org/W2404839462', 'https://openalex.org/W2785978752', 'https://openalex.org/W2532202862', 'https://openalex.org/W3007908230', 'https://openalex.org/W2911340057', 'https://openalex.org/W2972939746', 'https://openalex.org/W2973115941', 'https://openalex.org/W2749651610', 'https://openalex.org/W2786868129', 'https://openalex.org/W2963522141', 'https://openalex.org/W2803229097', 'https://openalex.org/W1509691205', 'https://openalex.org/W6696767757', 'https://openalex.org/W2021986246', 'https://openalex.org/W3096864844', 'https://openalex.org/W2963808252', 'https://openalex.org/W2651834199', 'https://openalex.org/W2938583109', 'https://openalex.org/W2156142001', 'https://openalex.org/W1969728648', 'https://openalex.org/W6691687657', 'https://openalex.org/W2972999331', 'https://openalex.org/W2331222812', 'https://openalex.org/W6769702557', 'https://openalex.org/W2013996527', 'https://openalex.org/W2150769028', 'https://openalex.org/W6777157395', 'https://openalex.org/W2964069186', 'https://openalex.org/W2153057929', 'https://openalex.org/W2151262064', 'https://openalex.org/W6635216677', 'https://openalex.org/W2947196194', 'https://openalex.org/W2507912506', 'https://openalex.org/W6746801104', 'https://openalex.org/W2962896155', 'https://openalex.org/W2518312472', 'https://openalex.org/W2574092538', 'https://openalex.org/W2806000759', 'https://openalex.org/W2532494225', 'https://openalex.org/W2120847449', 'https://openalex.org/W2802455234', 'https://openalex.org/W2797877891', 'https://openalex.org/W2796495654', 'https://openalex.org/W2067234399', 'https://openalex.org/W2129082420', 'https://openalex.org/W2428180336', 'https://openalex.org/W1963778986', 'https://openalex.org/W2096564043', 'https://openalex.org/W2121387787', 'https://openalex.org/W2903365642', 'https://openalex.org/W2941094131', 'https://openalex.org/W2889064624', 'https://openalex.org/W2972359262', 'https://openalex.org/W6748409065', 'https://openalex.org/W6936113694', 'https://openalex.org/W2963300588', 'https://openalex.org/W2800289214', 'https://openalex.org/W6756159577', 'https://openalex.org/W2981087920', 'https://openalex.org/W2936802426', 'https://openalex.org/W6720742747', 'https://openalex.org/W2745896134', 'https://openalex.org/W2601846999', 'https://openalex.org/W6603838645', 'https://openalex.org/W3026777299', 'https://openalex.org/W2962780374', 'https://openalex.org/W2802650881', 'https://openalex.org/W3034600949', 'https://openalex.org/W2798951647', 'https://openalex.org/W2962793481', 'https://openalex.org/W2984809863', 'https://openalex.org/W7008786231', 'https://openalex.org/W2887511658', 'https://openalex.org/W2803714261', 'https://openalex.org/W2888932932', 'https://openalex.org/W2963767194', 'https://openalex.org/W7002029315', 'https://openalex.org/W2989855043', 'https://openalex.org/W6734815144', 'https://openalex.org/W2890300789', 'https://openalex.org/W2973108288', 'https://openalex.org/W2972313557', 'https://openalex.org/W2938836228', 'https://openalex.org/W2049686551', 'https://openalex.org/W2889544410', 'https://openalex.org/W2126143605', 'https://openalex.org/W2145892079', 'https://openalex.org/W6770488218', 'https://openalex.org/W6745117592', 'https://openalex.org/W2962850167', 'https://openalex.org/W2964243274', 'https://openalex.org/W6750298367', 'https://openalex.org/W2954386831', 'https://openalex.org/W3100270690', 'https://openalex.org/W6757028937', 'https://openalex.org/W3012404734', 'https://openalex.org/W2963800363', 'https://openalex.org/W6745992979', 'https://openalex.org/W3012970712', 'https://openalex.org/W2997400081', 'https://openalex.org/W3076645148', 'https://openalex.org/W2888858245', 'https://openalex.org/W2787133659', 'https://openalex.org/W6761382815', 'https://openalex.org/W2902070858', 'https://openalex.org/W6730095352', 'https://openalex.org/W2936002583', 'https://openalex.org/W3008297462', 'https://openalex.org/W3025680351', 'https://openalex.org/W3096939667', 'https://openalex.org/W2885820941', 'https://openalex.org/W3025182306', 'https://openalex.org/W3048768961', 'https://openalex.org/W6781784828', 'https://openalex.org/W6771070734', 'https://openalex.org/W6783904828', 'https://openalex.org/W6687506355', 'https://openalex.org/W2892734764', 'https://openalex.org/W6775882176', 'https://openalex.org/W2972970915', 'https://openalex.org/W6777869919', 'https://openalex.org/W3006777338', 'https://openalex.org/W2901254300', 'https://openalex.org/W2168810263', 'https://openalex.org/W2019849101', 'https://openalex.org/W2077801020', 'https://openalex.org/W6635180523', 'https://openalex.org/W6712849908', 'https://openalex.org/W1968689546', 'https://openalex.org/W2395380967', 'https://openalex.org/W1599112982', 'https://openalex.org/W2168531172', 'https://openalex.org/W6712034360', 'https://openalex.org/W3096524539', 'https://openalex.org/W6640963894', 'https://openalex.org/W2972659941', 'https://openalex.org/W3015434413', 'https://openalex.org/W2804998325', 'https://openalex.org/W2922283382', 'https://openalex.org/W6762931180', 'https://openalex.org/W2752796333', 'https://openalex.org/W2889061305', 'https://openalex.org/W2972849140', 'https://openalex.org/W2020834817', 'https://openalex.org/W2143116775', 'https://openalex.org/W2156477760', 'https://openalex.org/W1524403222', 'https://openalex.org/W6680012447', 'https://openalex.org/W2239901959', 'https://openalex.org/W2013608223', 'https://openalex.org/W2125760250', 'https://openalex.org/W2158291955', 'https://openalex.org/W2123003832', 'https://openalex.org/W2167873188', 'https://openalex.org/W2083806749', 'https://openalex.org/W2747744257', 'https://openalex.org/W6752378368', 'https://openalex.org/W6720691552', 'https://openalex.org/W6783889628', 'https://openalex.org/W3002433751', 'https://openalex.org/W2102148524', 'https://openalex.org/W1574447377', 'https://openalex.org/W2107860279', 'https://openalex.org/W6777853906', 'https://openalex.org/W2039240409', 'https://openalex.org/W3015338123', 'https://openalex.org/W2990440871', 'https://openalex.org/W2972309641', 'https://openalex.org/W2963175743', 'https://openalex.org/W2998498479', 'https://openalex.org/W2972789651', 'https://openalex.org/W2972910332', 'https://openalex.org/W2972882294', 'https://openalex.org/W6740753539', 'https://openalex.org/W2011916518', 'https://openalex.org/W6767111847', 'https://openalex.org/W278779762', 'https://openalex.org/W1975163393', 'https://openalex.org/W1526513105', 'https://openalex.org/W2269051851', 'https://openalex.org/W2093107730', 'https://openalex.org/W2603810172', 'https://openalex.org/W3034089333', 'https://openalex.org/W6600593648', 'https://openalex.org/W6605781699', 'https://openalex.org/W2128446656', 'https://openalex.org/W2120605154', 'https://openalex.org/W6603221913', 'https://openalex.org/W2514429423', 'https://openalex.org/W2105160541', 'https://openalex.org/W2076268407', 'https://openalex.org/W1985690171', 'https://openalex.org/W2017425464', 'https://openalex.org/W2114659828', 'https://openalex.org/W1977362459', 'https://openalex.org/W2484196375', 'https://openalex.org/W2154986869', 'https://openalex.org/W2093898552', 'https://openalex.org/W6694691911', 'https://openalex.org/W2136166660', 'https://openalex.org/W6636924704', 'https://openalex.org/W2171717581', 'https://openalex.org/W2060554399', 'https://openalex.org/W1523372075', 'https://openalex.org/W2786387703', 'https://openalex.org/W6675938391', 'https://openalex.org/W2130086727', 'https://openalex.org/W2795109282', 'https://openalex.org/W2791322592', 'https://openalex.org/W6603559778', 'https://openalex.org/W6714062273', 'https://openalex.org/W2996286887', 'https://openalex.org/W4385245566', 'https://openalex.org/W2613904329', 'https://openalex.org/W2758785877', 'https://openalex.org/W1608375737', 'https://openalex.org/W3146803896', 'https://openalex.org/W2577886607', 'https://openalex.org/W2116955094', 'https://openalex.org/W3125709657', 'https://openalex.org/W2962882868', 'https://openalex.org/W3047107405', 'https://openalex.org/W4293398859', 'https://openalex.org/W3142619648', 'https://openalex.org/W4288337064', 'https://openalex.org/W2963927338', 'https://openalex.org/W2970006822', 'https://openalex.org/W2111550316', 'https://openalex.org/W2964308564', 'https://openalex.org/W2395980997', 'https://openalex.org/W3027343259', 'https://openalex.org/W87473629', 'https://openalex.org/W155193863', 'https://openalex.org/W15066456', 'https://openalex.org/W2907162034', 'https://openalex.org/W2963618559', 'https://openalex.org/W2964281804', 'https://openalex.org/W2608338293', 'https://openalex.org/W3099078140', 'https://openalex.org/W2768959015', 'https://openalex.org/W3095990227', 'https://openalex.org/W3049756574', 'https://openalex.org/W3143596294', 'https://openalex.org/W3102628737', 'https://openalex.org/W4301206121', 'https://openalex.org/W3143465836', 'https://openalex.org/W2099471712', 'https://openalex.org/W33829787', 'https://openalex.org/W2766812927', 'https://openalex.org/W2135029798', 'https://openalex.org/W2396025094', 'https://openalex.org/W3090528452', 'https://openalex.org/W3094635600', 'https://openalex.org/W2898654681', 'https://openalex.org/W2107740512', 'https://openalex.org/W2176804518', 'https://openalex.org/W2553897675', 'https://openalex.org/W3100378519', 'https://openalex.org/W2804078698', 'https://openalex.org/W3082130377', 'https://openalex.org/W2527729766', 'https://openalex.org/W2250668798', 'https://openalex.org/W3096567388', 'https://openalex.org/W1588266896', 'https://openalex.org/W2949382160', 'https://openalex.org/W4320013936', 'https://openalex.org/W3015669407', 'https://openalex.org/W3005862564', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963889406', 'https://openalex.org/W3016151052', 'https://openalex.org/W59075858', 'https://openalex.org/W2970997853', 'https://openalex.org/W4289305009', 'https://openalex.org/W1588037970', 'https://openalex.org/W2404109308', 'https://openalex.org/W2294246205', 'https://openalex.org/W3004402693', 'https://openalex.org/W2963981733', 'https://openalex.org/W2408526116', 'https://openalex.org/W3102326097', 'https://openalex.org/W3102905810', 'https://openalex.org/W2971074500', 'https://openalex.org/W2277581165', 'https://openalex.org/W142801778', 'https://openalex.org/W79241043', 'https://openalex.org/W2473388484', 'https://openalex.org/W2937579788', 'https://openalex.org/W3025044797', 'https://openalex.org/W2964307104', 'https://openalex.org/W2774848319', 'https://openalex.org/W2955616469', 'https://openalex.org/W2964265128', 'https://openalex.org/W3124274650', 'https://openalex.org/W2557915412', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963784072', 'https://openalex.org/W2605762339', 'https://openalex.org/W2133564696', 'https://openalex.org/W2964167449', 'https://openalex.org/W2970351109', 'https://openalex.org/W3090052686', 'https://openalex.org/W2294351487', 'https://openalex.org/W2963890275', 'https://openalex.org/W156398200', 'https://openalex.org/W3154451338', 'https://openalex.org/W2402019113', 'https://openalex.org/W3034420534', 'https://openalex.org/W129217914', 'https://openalex.org/W2519091744', 'https://openalex.org/W4298580827', 'https://openalex.org/W1741199358', 'https://openalex.org/W1520370180', 'https://openalex.org/W1959608418', 'https://openalex.org/W2591927543', 'https://openalex.org/W2733416080', 'https://openalex.org/W2994715919', 'https://openalex.org/W4295731579', 'https://openalex.org/W2397499194', 'https://openalex.org/W3101689408', 'https://openalex.org/W2962919088', 'https://openalex.org/W2967606780', 'https://openalex.org/W2210497074', 'https://openalex.org/W3034058691', 'https://openalex.org/W2408732577', 'https://openalex.org/W2264797740', 'https://openalex.org/W3168542456', 'https://openalex.org/W95152782', 'https://openalex.org/W2963330667', 'https://openalex.org/W1658960529', 'https://openalex.org/W2273671322', 'https://openalex.org/W3020975377']",2020-11-17
https://openalex.org/W4372260214,https://doi.org/10.1109/icassp49357.2023.10095960,Preserving Background Sound in Noise-Robust Voice Conversion Via Multi-Task Learning,"Background sound is an informative form of art that is helpful in providing a more immersive experience in real-application voice conversion (VC) scenarios. However, prior research about VC, mainly focusing on clean voices, pay rare attention to VC with background sound. The critical problem for preserving background sound in VC is inevitable speech distortion by the neural separation model and the cascade mismatch between the source separation model and the VC model. In this paper, we propose an end-to-end framework via multitask learning which sequentially cascades a source separation (SS) module, a bottleneck feature extraction module and a VC module. Specifically, the source separation task explicitly considers critical phase information and limits the distortion caused by the imperfect separation process. The source separation task, the typical VC task and the unified task share a uniform reconstruction loss constrained by joint training to reduce the mismatch between the SS and VC modules. Experimental results demonstrate that our proposed framework significantly outperforms the baseline systems while achieving comparable quality and speaker similarity to the VC models trained with clean data.","['https://openalex.org/W2907262790', 'https://openalex.org/W4283836776', 'https://openalex.org/W6802128655', 'https://openalex.org/W3022057435', 'https://openalex.org/W3016120385', 'https://openalex.org/W4205582447', 'https://openalex.org/W2576309025', 'https://openalex.org/W3098557217', 'https://openalex.org/W3081753361', 'https://openalex.org/W4226075807', 'https://openalex.org/W3206706278', 'https://openalex.org/W3096408984', 'https://openalex.org/W6783867762', 'https://openalex.org/W6727697161', 'https://openalex.org/W3197478142', 'https://openalex.org/W3096090308', 'https://openalex.org/W3163464523', 'https://openalex.org/W6633117090', 'https://openalex.org/W4224916404', 'https://openalex.org/W3197659778', 'https://openalex.org/W4226039367', 'https://openalex.org/W3163475957', 'https://openalex.org/W3168719651', 'https://openalex.org/W3092028330', 'https://openalex.org/W1552314771', 'https://openalex.org/W2527729766', 'https://openalex.org/W3104309246', 'https://openalex.org/W4298310324', 'https://openalex.org/W3200209270']",2023-05-05
https://openalex.org/W4283689139,https://doi.org/10.21437/interspeech.2022-686,Self-supervised Context-aware Style Representation for Expressive Speech Synthesis,"Expressive speech synthesis, like audiobook synthesis, is still challenging for style representation learning and prediction.Deriving from reference audio or predicting style tags from text requires a huge amount of labeled data, which is costly to acquire and difficult to define and annotate accurately.In this paper, we propose a novel framework for learning style representation from abundant plain text in a self-supervised manner.It leverages an emotion lexicon and uses contrastive learning and deep clustering.We further integrate the style representation as a conditioned embedding in a multi-style Transformer TTS.Comparing with multi-style TTS by predicting style tags trained on the same dataset but with human annotations, our method achieves improved results according to subjective evaluations on both in-domain and out-of-domain test sets in audiobook speech.Moreover, with implicit context-aware style representation, the emotion transition of synthesized audio in a long paragraph appears more natural.The audio samples are available on the demo website.","['https://openalex.org/W2964086597', 'https://openalex.org/W2187089797', 'https://openalex.org/W3183873439', 'https://openalex.org/W4214573674', 'https://openalex.org/W3005680577', 'https://openalex.org/W4295731579', 'https://openalex.org/W2962691331', 'https://openalex.org/W2951902588', 'https://openalex.org/W4287248095', 'https://openalex.org/W2741943936', 'https://openalex.org/W2906797124', 'https://openalex.org/W2970006822', 'https://openalex.org/W3144810982', 'https://openalex.org/W3015212100', 'https://openalex.org/W2964074409', 'https://openalex.org/W3025408396', 'https://openalex.org/W2770743791', 'https://openalex.org/W3160506022', 'https://openalex.org/W3035323028', 'https://openalex.org/W4287592659', 'https://openalex.org/W3034576826', 'https://openalex.org/W3171153522', 'https://openalex.org/W2896457183', 'https://openalex.org/W2794490148', 'https://openalex.org/W2903739847', 'https://openalex.org/W2885800352']",2022-09-16
https://openalex.org/W4372267192,https://doi.org/10.1109/icassp49357.2023.10095120,Distinguishable Speaker Anonymization Based on Formant and Fundamental Frequency Scaling,"Speech data on the Internet are proliferating exponentially because of the emergence of social media, and the sharing of such personal data raises obvious security and privacy concerns. One solution to mitigate these concerns involves concealing speaker identities before sharing speech data, also referred to as speaker anonymization. In our previous work, we have developed an automatic speaker verification (ASV)-model-free anonymization framework to protect speaker privacy while preserving speech intelligibility. Although the framework ranked first place in VoicePrivacy 2022 challenge, the anonymization was imperfect, since the speaker distinguishability of the anonymized speech was deteriorated. To address this issue, in this paper, we directly model the formant distribution and fundamental frequency (F0) to represent speaker identity and anonymize the source speech by the uniformly scaling formant and F0. By directly scaling the formant and F0, the speaker distinguishability degradation of the anonymized speech caused by the introduction of other speakers is prevented. The experimental results demonstrate that our proposed framework can improve the speaker distinguishability and significantly outperforms our previous framework in voice distinctiveness. Furthermore, our proposed method can trade off the privacy-utility by using different scaling factors.","['https://openalex.org/W2954217241', 'https://openalex.org/W3043999252', 'https://openalex.org/W4210970267', 'https://openalex.org/W3197008493', 'https://openalex.org/W3196623473', 'https://openalex.org/W4221114270', 'https://openalex.org/W2972848589', 'https://openalex.org/W3096424617', 'https://openalex.org/W6843630785', 'https://openalex.org/W4297841874', 'https://openalex.org/W3180600971', 'https://openalex.org/W4296069300', 'https://openalex.org/W6843253040', 'https://openalex.org/W2091425152', 'https://openalex.org/W3197478142', 'https://openalex.org/W2963609956', 'https://openalex.org/W6783867762', 'https://openalex.org/W3144035034', 'https://openalex.org/W2140567543', 'https://openalex.org/W6839738141', 'https://openalex.org/W2031529434', 'https://openalex.org/W1494198834', 'https://openalex.org/W2972359262', 'https://openalex.org/W6727697161', 'https://openalex.org/W3081871358', 'https://openalex.org/W2527729766', 'https://openalex.org/W4297437784']",2023-05-05
https://openalex.org/W4205742757,https://doi.org/10.1016/j.specom.2021.11.006,"Emotional voice conversion: Theory, databases and ESD","In this paper, we first provide a review of the state-of-the-art emotional voice conversion research, and the existing emotional speech databases. We then motivate the development of a novel emotional speech database (ESD) that addresses the increasing research need. With this paper, the ESD database1 is now made available to the research community. The ESD database consists of 350 parallel utterances spoken by 10 native English and 10 native Chinese speakers and covers 5 emotion categories (neutral, happy, angry, sad and surprise). More than 29 h of speech data were recorded in a controlled acoustic environment. The database is suitable for multi-speaker and cross-lingual emotional voice conversion studies. As case studies, we implement several state-of-the-art emotional voice conversion systems on the ESD database. This paper provides a reference study on ESD in conjunction with its release.","['https://openalex.org/W6679459330', 'https://openalex.org/W2035962301', 'https://openalex.org/W2077801020', 'https://openalex.org/W6660513934', 'https://openalex.org/W2984809863', 'https://openalex.org/W3012970712', 'https://openalex.org/W6943714206', 'https://openalex.org/W2997399314', 'https://openalex.org/W6748472120', 'https://openalex.org/W6781381461', 'https://openalex.org/W1971362524', 'https://openalex.org/W4245744384', 'https://openalex.org/W6768045828', 'https://openalex.org/W2129703931', 'https://openalex.org/W2478838513', 'https://openalex.org/W6772034331', 'https://openalex.org/W6607193717', 'https://openalex.org/W2146334809', 'https://openalex.org/W2342475039', 'https://openalex.org/W6784094172', 'https://openalex.org/W2030931454', 'https://openalex.org/W3095930733', 'https://openalex.org/W2086796102', 'https://openalex.org/W4245885054', 'https://openalex.org/W2963767194', 'https://openalex.org/W3136699727', 'https://openalex.org/W6760740948', 'https://openalex.org/W2785608393', 'https://openalex.org/W6617505242', 'https://openalex.org/W2190260761', 'https://openalex.org/W6663401105', 'https://openalex.org/W2157412983', 'https://openalex.org/W6781851318', 'https://openalex.org/W1966797434', 'https://openalex.org/W2074788634', 'https://openalex.org/W2759244429', 'https://openalex.org/W2066090536', 'https://openalex.org/W6776193403', 'https://openalex.org/W3012404734', 'https://openalex.org/W6676463824', 'https://openalex.org/W1978136968', 'https://openalex.org/W6602386084', 'https://openalex.org/W2121387787', 'https://openalex.org/W2167510900', 'https://openalex.org/W2090777335', 'https://openalex.org/W2085662862', 'https://openalex.org/W6749868174', 'https://openalex.org/W6632144934', 'https://openalex.org/W2899361462', 'https://openalex.org/W6725771000', 'https://openalex.org/W6674946611', 'https://openalex.org/W2025905516', 'https://openalex.org/W2105160541', 'https://openalex.org/W6634507583', 'https://openalex.org/W6728445508', 'https://openalex.org/W1974843131', 'https://openalex.org/W6608969598', 'https://openalex.org/W2148846882', 'https://openalex.org/W6754339523', 'https://openalex.org/W2005885879', 'https://openalex.org/W6678506862', 'https://openalex.org/W6766442783', 'https://openalex.org/W2081837280', 'https://openalex.org/W6756504009', 'https://openalex.org/W6822748163', 'https://openalex.org/W6770351630', 'https://openalex.org/W4395958010', 'https://openalex.org/W6603838645', 'https://openalex.org/W1994198923', 'https://openalex.org/W6676417963', 'https://openalex.org/W2966387353', 'https://openalex.org/W6604530167', 'https://openalex.org/W3155308523', 'https://openalex.org/W6785011255', 'https://openalex.org/W6726425448', 'https://openalex.org/W2510170536', 'https://openalex.org/W6785004522', 'https://openalex.org/W3094635600', 'https://openalex.org/W2803193013', 'https://openalex.org/W2793479148', 'https://openalex.org/W6743301281', 'https://openalex.org/W2950542544', 'https://openalex.org/W6725596946', 'https://openalex.org/W7028336066', 'https://openalex.org/W2018658329', 'https://openalex.org/W6686269052', 'https://openalex.org/W6726456654', 'https://openalex.org/W6713120313', 'https://openalex.org/W2576309025', 'https://openalex.org/W2471520273', 'https://openalex.org/W4240592325', 'https://openalex.org/W2981087920', 'https://openalex.org/W6711854987', 'https://openalex.org/W3095012670', 'https://openalex.org/W2039800941', 'https://openalex.org/W6752614923', 'https://openalex.org/W2915760784', 'https://openalex.org/W2165857685', 'https://openalex.org/W6775864090', 'https://openalex.org/W6761309493', 'https://openalex.org/W2149628368', 'https://openalex.org/W6768268957', 'https://openalex.org/W6744609156', 'https://openalex.org/W2165894799', 'https://openalex.org/W176245312', 'https://openalex.org/W2803098682', 'https://openalex.org/W2894821115', 'https://openalex.org/W6775618791', 'https://openalex.org/W2057563799', 'https://openalex.org/W6768278692', 'https://openalex.org/W3097962967', 'https://openalex.org/W2972366998', 'https://openalex.org/W6752524494', 'https://openalex.org/W6754355654', 'https://openalex.org/W2785978752', 'https://openalex.org/W6781662123', 'https://openalex.org/W6774817744', 'https://openalex.org/W6754413460', 'https://openalex.org/W2941094131', 'https://openalex.org/W6758191806', 'https://openalex.org/W2753840835', 'https://openalex.org/W6639015303', 'https://openalex.org/W6678929712', 'https://openalex.org/W6679436768', 'https://openalex.org/W6737641361', 'https://openalex.org/W6755930846', 'https://openalex.org/W6683855838', 'https://openalex.org/W6676002441', 'https://openalex.org/W6767795613', 'https://openalex.org/W6758366080', 'https://openalex.org/W2972699445', 'https://openalex.org/W2120605154', 'https://openalex.org/W6720742747', 'https://openalex.org/W6770189186', 'https://openalex.org/W6739901393', 'https://openalex.org/W6713858911', 'https://openalex.org/W6698098957', 'https://openalex.org/W6742769468', 'https://openalex.org/W6679081286', 'https://openalex.org/W6684200426', 'https://openalex.org/W6775421620', 'https://openalex.org/W4395959004', 'https://openalex.org/W2330979245', 'https://openalex.org/W2883743124', 'https://openalex.org/W2085013480', 'https://openalex.org/W6601306257', 'https://openalex.org/W2972359262', 'https://openalex.org/W2996414377', 'https://openalex.org/W2897353073', 'https://openalex.org/W2972999331', 'https://openalex.org/W3154451338', 'https://openalex.org/W4235201968', 'https://openalex.org/W3025680351', 'https://openalex.org/W6785164032', 'https://openalex.org/W6784688130', 'https://openalex.org/W3096939667', 'https://openalex.org/W2962793481', 'https://openalex.org/W6774467145', 'https://openalex.org/W6608022165', 'https://openalex.org/W2158504378', 'https://openalex.org/W4245692952', 'https://openalex.org/W2972667718', 'https://openalex.org/W4300482596', 'https://openalex.org/W3101689408', 'https://openalex.org/W4285074441', 'https://openalex.org/W388732865', 'https://openalex.org/W4236377898', 'https://openalex.org/W3206840074', 'https://openalex.org/W3168542456', 'https://openalex.org/W2962896155', 'https://openalex.org/W4319782399', 'https://openalex.org/W2972659941', 'https://openalex.org/W3047855478', 'https://openalex.org/W2512798946', 'https://openalex.org/W4302402037', 'https://openalex.org/W2610171128', 'https://openalex.org/W4235670393', 'https://openalex.org/W4248867778', 'https://openalex.org/W2787928077', 'https://openalex.org/W2914763987', 'https://openalex.org/W3045447113', 'https://openalex.org/W4385245566', 'https://openalex.org/W3014201970', 'https://openalex.org/W2936412271', 'https://openalex.org/W3098557217', 'https://openalex.org/W3194143312', 'https://openalex.org/W4255540942', 'https://openalex.org/W2161736993', 'https://openalex.org/W4250640105', 'https://openalex.org/W4253874399', 'https://openalex.org/W3198791321', 'https://openalex.org/W3197993066', 'https://openalex.org/W2774848319', 'https://openalex.org/W2162295204', 'https://openalex.org/W2802975219', 'https://openalex.org/W1959608418', 'https://openalex.org/W3082130377', 'https://openalex.org/W2559655401', 'https://openalex.org/W1588037970', 'https://openalex.org/W3008297462', 'https://openalex.org/W2614201817', 'https://openalex.org/W2963035245', 'https://openalex.org/W2767014232', 'https://openalex.org/W1490382748', 'https://openalex.org/W2483796469', 'https://openalex.org/W4252531498', 'https://openalex.org/W3096567388', 'https://openalex.org/W2505874150', 'https://openalex.org/W3095169545', 'https://openalex.org/W4237670716', 'https://openalex.org/W2810914326', 'https://openalex.org/W2995401804', 'https://openalex.org/W3080698515', 'https://openalex.org/W2998572311', 'https://openalex.org/W2606429533', 'https://openalex.org/W4376524021', 'https://openalex.org/W3015669407', 'https://openalex.org/W2998249245', 'https://openalex.org/W3099078140', 'https://openalex.org/W3109943296', 'https://openalex.org/W2187089797', 'https://openalex.org/W4210849719', 'https://openalex.org/W2593463961']",2021-12-20
https://openalex.org/W3096939667,https://doi.org/10.21437/interspeech.2020-2014,Converting Anyone’s Emotion: Towards Speaker-Independent Emotional Voice Conversion,"Emotional voice conversion aims to convert the emotion of speech from one state to another while preserving the linguistic content and speaker identity.The prior studies on emotional voice conversion are mostly carried out under the assumption that emotion is speaker-dependent.We consider that there is a common code between speakers for emotional expression in a spoken language, therefore, a speaker-independent mapping between emotional states is possible.In this paper, we propose a speaker-independent emotional voice conversion framework, that can convert anyone's emotion without the need for parallel data.We propose a VAW-GAN based encoderdecoder structure to learn the spectrum and prosody mapping.We perform prosody conversion by using continuous wavelet transform (CWT) to model the temporal dependencies.We also investigate the use of F0 as an additional input to the decoder to improve emotion conversion performance.Experiments show that the proposed speaker-independent framework achieves competitive results for both seen and unseen speakers.","['https://openalex.org/W2517513811', 'https://openalex.org/W2889064624', 'https://openalex.org/W2883743124', 'https://openalex.org/W2040587156', 'https://openalex.org/W3016151052', 'https://openalex.org/W2793479148', 'https://openalex.org/W2511640485', 'https://openalex.org/W2997399314', 'https://openalex.org/W1966797434', 'https://openalex.org/W3144792678', 'https://openalex.org/W3012970712', 'https://openalex.org/W2127589467', 'https://openalex.org/W1537481423', 'https://openalex.org/W2123864036', 'https://openalex.org/W2120605154', 'https://openalex.org/W2946809691', 'https://openalex.org/W1994198923', 'https://openalex.org/W2785608393', 'https://openalex.org/W2161736993', 'https://openalex.org/W2136922672', 'https://openalex.org/W2941094131', 'https://openalex.org/W2972366998', 'https://openalex.org/W2051217765', 'https://openalex.org/W2785978752', 'https://openalex.org/W2889544410', 'https://openalex.org/W2105160541', 'https://openalex.org/W2938833595', 'https://openalex.org/W2899361462', 'https://openalex.org/W113106864', 'https://openalex.org/W2962896155', 'https://openalex.org/W2889112744', 'https://openalex.org/W2973135352', 'https://openalex.org/W3025680351', 'https://openalex.org/W3015805741', 'https://openalex.org/W2319003035', 'https://openalex.org/W2148846882', 'https://openalex.org/W2984809863', 'https://openalex.org/W2810914326', 'https://openalex.org/W2081837280', 'https://openalex.org/W1988189165', 'https://openalex.org/W2077801020']",2020-10-25
https://openalex.org/W2947445680,https://doi.org/10.21437/interspeech.2019-2048,Unsupervised End-to-End Learning of Discrete Linguistic Units for Voice Conversion,"We present an unsupervised end-to-end training scheme where we discover\ndiscrete subword units from speech without using any labels. The discrete\nsubword units are learned under an ASR-TTS autoencoder reconstruction setting,\nwhere an ASR-Encoder is trained to discover a set of common linguistic units\ngiven a variety of speakers, and a TTS-Decoder trained to project the\ndiscovered units back to the designated speech. We propose a discrete encoding\nmethod, Multilabel-Binary Vectors (MBV), to make the ASR-TTS autoencoder\ndifferentiable. We found that the proposed encoding method offers automatic\nextraction of speech content from speaker style, and is sufficient to cover\nfull linguistic content in a given language. Therefore, the TTS-Decoder can\nsynthesize speech with the same content as the input of ASR-Encoder but with\ndifferent speaker characteristics, which achieves voice conversion (VC). We\nfurther improve the quality of VC using adversarial training, where we train a\nTTS-Patcher that augments the output of TTS-Decoder. Objective and subjective\nevaluations show that the proposed approach offers strong VC results as it\neliminates speaker identity while preserving content within speech. In the\nZeroSpeech 2019 Challenge, we achieved outstanding performance in terms of low\nbitrate.\n","['https://openalex.org/W4295521014', 'https://openalex.org/W2547039119', 'https://openalex.org/W2963691546', 'https://openalex.org/W2899518769', 'https://openalex.org/W2963609956', 'https://openalex.org/W2550241133', 'https://openalex.org/W1522301498', 'https://openalex.org/W2792995953', 'https://openalex.org/W2020607164', 'https://openalex.org/W2767754137', 'https://openalex.org/W2940544976', 'https://openalex.org/W2963073614', 'https://openalex.org/W2963618559', 'https://openalex.org/W2964135678', 'https://openalex.org/W2962974898', 'https://openalex.org/W2532494225', 'https://openalex.org/W4320013936', 'https://openalex.org/W2964243274', 'https://openalex.org/W2347098582', 'https://openalex.org/W4394670483', 'https://openalex.org/W2598638573', 'https://openalex.org/W2099471712', 'https://openalex.org/W2950776302', 'https://openalex.org/W2963830550', 'https://openalex.org/W2963684067', 'https://openalex.org/W2395899413', 'https://openalex.org/W3125709657', 'https://openalex.org/W4288107125', 'https://openalex.org/W2962736743', 'https://openalex.org/W2608207374', 'https://openalex.org/W2476548250', 'https://openalex.org/W2547875792', 'https://openalex.org/W2548275288', 'https://openalex.org/W2963799213', 'https://openalex.org/W2951216052', 'https://openalex.org/W2962879692', 'https://openalex.org/W2758785877', 'https://openalex.org/W2059652594', 'https://openalex.org/W2963571336']",2019-09-13
https://openalex.org/W2938833595,https://doi.org/10.1109/icassp.2019.8683865,Sequence-to-sequence Modelling of F0 for Speech Emotion Conversion,"Voice interfaces are becoming wildly popular and driving demand for more advanced speech synthesis and voice transformation systems. Current text-to-speech methods produce realistic sounding voices, but they lack the emotional expressivity that listeners expect, given the context of the interaction and the phrase being spoken. Emotional voice conversion is a research domain concerned with generating expressive speech from neutral synthesised speech or natural human voice. This research investigated the effectiveness of using a sequence-to-sequence (seq2seq) encoder-decoder based model to transform the intonation of a human voice from neutral to expressive speech, with some preliminary introduction of linguistic conditioning. A subjective experiment conducted on the task of speech emotion recognition by listeners successfully demonstrated the effectiveness of the proposed sequence-to-sequence models to produce convincing voice emotion transformations. In particular, conditioning the model on the position of the syllable in the phrase significantly improved recognition rates.","['https://openalex.org/W6602504493', 'https://openalex.org/W6713858911', 'https://openalex.org/W2184310502', 'https://openalex.org/W6751998403', 'https://openalex.org/W6696843773', 'https://openalex.org/W1576227399', 'https://openalex.org/W2517513811', 'https://openalex.org/W2613654124', 'https://openalex.org/W2747914378', 'https://openalex.org/W2513051195', 'https://openalex.org/W6600618385', 'https://openalex.org/W2808514527', 'https://openalex.org/W2746192915', 'https://openalex.org/W2106421426', 'https://openalex.org/W2740504963', 'https://openalex.org/W2886195082', 'https://openalex.org/W6604530167', 'https://openalex.org/W3142087749', 'https://openalex.org/W1570629387', 'https://openalex.org/W6683488333', 'https://openalex.org/W2018338387', 'https://openalex.org/W6679436768', 'https://openalex.org/W6736356763', 'https://openalex.org/W2157331557', 'https://openalex.org/W6679434410', 'https://openalex.org/W2964308564', 'https://openalex.org/W2949382160', 'https://openalex.org/W15106243', 'https://openalex.org/W2407110532', 'https://openalex.org/W2604184139', 'https://openalex.org/W2133564696', 'https://openalex.org/W2805307277', 'https://openalex.org/W2605141709', 'https://openalex.org/W113106864', 'https://openalex.org/W2294797155', 'https://openalex.org/W2130942839', 'https://openalex.org/W63438600', 'https://openalex.org/W2158504378', 'https://openalex.org/W2519091744', 'https://openalex.org/W2674088828']",2019-04-17
https://openalex.org/W4372260157,https://doi.org/10.1109/icassp49357.2023.10095842,Nonparallel Emotional Voice Conversion for Unseen Speaker-Emotion Pairs Using Dual Domain Adversarial Network &amp; Virtual Domain Pairing,"Primary goal of an emotional voice conversion (EVC) system is to convert the emotion of a given speech signal from one style to another style without modifying the linguistic content of the signal. Most of the state-of-the-art approaches convert emotions for seen speaker-emotion combinations only. In this paper, we tackle the problem of converting the emotion of speakers whose only neutral data are present during the time of training and testing (i.e., unseen speaker-emotion combinations). To this end, we extend a recently proposed StartGANv2-VC architecture by utilizing dual encoders for learning the speaker and emotion style embeddings separately along with dual domain source classifiers. For achieving the conversion to unseen speaker-emotion combinations, we propose a Virtual Domain Pairing (VDP) training strategy, which virtually incorporates the speaker-emotion pairs that are not present in the real data without compromising the min-max game of a discriminator and generator in adversarial training. We evaluate the proposed method using a Hindi emotional database.","['https://openalex.org/W4205742757', 'https://openalex.org/W2161736993', 'https://openalex.org/W2040587156', 'https://openalex.org/W2511640485', 'https://openalex.org/W2517513811', 'https://openalex.org/W2899361462', 'https://openalex.org/W2938833595', 'https://openalex.org/W3096791258', 'https://openalex.org/W6802762330', 'https://openalex.org/W6793489163', 'https://openalex.org/W3196643119', 'https://openalex.org/W3197993066', 'https://openalex.org/W3034600949', 'https://openalex.org/W6796575454', 'https://openalex.org/W3163573274', 'https://openalex.org/W3194143312', 'https://openalex.org/W3096939667', 'https://openalex.org/W2972366998', 'https://openalex.org/W4224301045', 'https://openalex.org/W4225264140', 'https://openalex.org/W3196667132', 'https://openalex.org/W2883743124', 'https://openalex.org/W6775580011', 'https://openalex.org/W3025680351', 'https://openalex.org/W3095169545', 'https://openalex.org/W6761075046', 'https://openalex.org/W3015338123', 'https://openalex.org/W2962788625', 'https://openalex.org/W3015336668', 'https://openalex.org/W3168292814', 'https://openalex.org/W3147311044', 'https://openalex.org/W3205065526']",2023-05-05
https://openalex.org/W3163573274,https://doi.org/10.1109/icassp39728.2021.9413391,Seen and Unseen Emotional Style Transfer for Voice Conversion with A New Emotional Speech Dataset,"Emotional voice conversion aims to transform emotional prosody in speech while preserving the linguistic content and speaker identity. Prior studies show that it is possible to disentangle emotional prosody using an encoder-decoder network conditioned on discrete representation, such as one-hot emotion labels. Such networks learn to remember a fixed set of emotional styles. In this paper, we propose a novel framework based on variational auto-encoding Wasserstein generative adversarial network (VAW-GAN), which makes use of a pre-trained speech emotion recognition (SER) model to transfer emotional style during training and at run-time inference. In this way, the network is able to transfer both seen and unseen emotional style to a new utterance. We show that the proposed framework achieves remarkable performance by consistently outperforming the baseline framework. This paper also marks the release of an emotional speech dataset (ESD) for voice conversion, which has multiple speakers and languages.","['https://openalex.org/W6781784828', 'https://openalex.org/W3015841875', 'https://openalex.org/W6773966927', 'https://openalex.org/W2471520273', 'https://openalex.org/W2885005742', 'https://openalex.org/W2146334809', 'https://openalex.org/W3008297462', 'https://openalex.org/W6683855838', 'https://openalex.org/W2040587156', 'https://openalex.org/W2511640485', 'https://openalex.org/W2517513811', 'https://openalex.org/W2938833595', 'https://openalex.org/W2883743124', 'https://openalex.org/W3025680351', 'https://openalex.org/W3095169545', 'https://openalex.org/W2899361462', 'https://openalex.org/W2149628368', 'https://openalex.org/W3098557217', 'https://openalex.org/W1966797434', 'https://openalex.org/W3016151052', 'https://openalex.org/W2105160541', 'https://openalex.org/W2937154351', 'https://openalex.org/W2120605154', 'https://openalex.org/W2086796102', 'https://openalex.org/W2941094131', 'https://openalex.org/W6711854987', 'https://openalex.org/W6634507583', 'https://openalex.org/W3096939667', 'https://openalex.org/W2759925408', 'https://openalex.org/W2962896155', 'https://openalex.org/W2795109282', 'https://openalex.org/W6750489868', 'https://openalex.org/W6772230580', 'https://openalex.org/W3014201970', 'https://openalex.org/W2161736993', 'https://openalex.org/W1581458799', 'https://openalex.org/W2187089797', 'https://openalex.org/W3044380931', 'https://openalex.org/W3168542456', 'https://openalex.org/W2080119116', 'https://openalex.org/W4295731579', 'https://openalex.org/W2319003035', 'https://openalex.org/W2714549561', 'https://openalex.org/W2963272440', 'https://openalex.org/W2998249245', 'https://openalex.org/W2963927338', 'https://openalex.org/W2396025094', 'https://openalex.org/W3047107405', 'https://openalex.org/W3006108364', 'https://openalex.org/W2608338293', 'https://openalex.org/W2794490148']",2021-05-13
https://openalex.org/W4225264140,https://doi.org/10.1109/icassp43922.2022.9746179,Cross-Speaker Style Transfer for Text-to-Speech Using Data Augmentation,"We address the problem of cross-speaker style transfer for text-to-speech (TTS) using data augmentation via voice conversion. We assume to have a corpus of neutral non-expressive data from a target speaker and supporting conversational expressive data from different speakers. Our goal is to build a TTS system that is expressive, while retaining the target speaker's identity. The proposed approach relies on voice conversion to first generate high-quality data from the set of supporting expressive speakers. The voice converted data is then pooled with natural data from the target speaker and used to train a single-speaker multi-style TTS system. We provide evidence that this approach is efficient, flexible, and scalable. The method is evaluated using one or more supporting speakers, as well as a variable amount of supporting data. We further provide evidence that this approach allows some controllability of speaking style, when using multiple supporting speakers. We conclude by scaling our proposed technology to a set of 14 speakers across 7 languages. Results indicate that our technology consistently improves synthetic samples in terms of style similarity, while retaining the target speaker's identity.","['https://openalex.org/W6640963894', 'https://openalex.org/W2904459034', 'https://openalex.org/W3096457008', 'https://openalex.org/W3094785744', 'https://openalex.org/W6760861152', 'https://openalex.org/W3139170550', 'https://openalex.org/W3194613004', 'https://openalex.org/W3135418837', 'https://openalex.org/W3083565479', 'https://openalex.org/W3007067948', 'https://openalex.org/W6679434410', 'https://openalex.org/W2954386831', 'https://openalex.org/W2962788625', 'https://openalex.org/W2749651610', 'https://openalex.org/W3198712562', 'https://openalex.org/W3162770051', 'https://openalex.org/W3198104520', 'https://openalex.org/W2795109282', 'https://openalex.org/W3015645837', 'https://openalex.org/W6796730497', 'https://openalex.org/W6750489868', 'https://openalex.org/W2964243274', 'https://openalex.org/W3195366750', 'https://openalex.org/W6797157791', 'https://openalex.org/W3161492781', 'https://openalex.org/W3022876224', 'https://openalex.org/W3161890269', 'https://openalex.org/W6762533536', 'https://openalex.org/W3015805741', 'https://openalex.org/W2133564696', 'https://openalex.org/W3174758275', 'https://openalex.org/W1959608418', 'https://openalex.org/W3195171908', 'https://openalex.org/W4295731579', 'https://openalex.org/W2945478979', 'https://openalex.org/W2932022923', 'https://openalex.org/W2187089797', 'https://openalex.org/W2794490148']",2022-04-27
https://openalex.org/W3216296943,https://doi.org/10.1109/icassp43922.2022.9746405,One-Shot Voice Conversion For Style Transfer Based On Speaker Adaptation,"One-shot style transfer is a challenging task, since training on one utterance makes model extremely easy to over-fit to training data and causes low speaker similarity and lack of expressiveness. In this paper, we build on the recognition-synthesis framework and propose a one-shot voice conversion approach for style transfer based on speaker adaptation. First, a speaker normalization module is adopted to remove speaker-related information in bottleneck features extracted by ASR. Second, we adopt weight regularization in the adaptation process to prevent over-fitting caused by using only one utterance from target speaker as training data. Finally, to comprehensively decouple the speech factors, i.e., content, speaker, style, and transfer source style to the target, a prosody module is used to extract prosody representation. Experiments show that our approach is superior to the state-of-the-art one-shot VC systems in terms of style and speaker similarity; additionally, our approach also maintains good speech quality.","['https://openalex.org/W3196616101', 'https://openalex.org/W2973154337', 'https://openalex.org/W3197659778', 'https://openalex.org/W6790220310', 'https://openalex.org/W6748588790', 'https://openalex.org/W3194347141', 'https://openalex.org/W6794277933', 'https://openalex.org/W3034218934', 'https://openalex.org/W6603838645', 'https://openalex.org/W2404839462', 'https://openalex.org/W2795109282', 'https://openalex.org/W6726528559', 'https://openalex.org/W3094635600', 'https://openalex.org/W3097892637', 'https://openalex.org/W6762533536', 'https://openalex.org/W3197943112', 'https://openalex.org/W2532494225', 'https://openalex.org/W3163475957', 'https://openalex.org/W2962896155', 'https://openalex.org/W2963091184', 'https://openalex.org/W3135654121', 'https://openalex.org/W3097001834', 'https://openalex.org/W3163573274', 'https://openalex.org/W2972659941', 'https://openalex.org/W4247282941', 'https://openalex.org/W1494198834', 'https://openalex.org/W2518172956', 'https://openalex.org/W95152782', 'https://openalex.org/W3128910262', 'https://openalex.org/W4295731579', 'https://openalex.org/W2788357188', 'https://openalex.org/W2945478979', 'https://openalex.org/W4287185706']",2022-04-27
https://openalex.org/W4226474318,https://doi.org/10.1109/asru51503.2021.9687906,Expressive Voice Conversion: A Joint Framework for Speaker Identity and Emotional Style Transfer,"Traditional voice conversion (VC) has been focused on speaker identity conversion for speech with a neutral expression. We note that emotional expression plays an essential role in daily communication, and the emotional style of speech can be speaker-dependent. In this paper, we study a technique to jointly convert the speaker identity and speaker-dependent emotional style, that is called expressive voice conversion. We propose a StarGAN-based framework to learn a many-to-many mapping across different speakers, that takes into account speaker-dependent emotional style without the need for parallel data. To this end, we condition the generator on emotional style encoding derived from a pre-trained speech emotion recognition (SER) model. The experiments validate the effectiveness of our proposed framework in both objective and subjective evaluations. To our best knowledge, this is the first study on expressive voice conversion.","['https://openalex.org/W2963539064', 'https://openalex.org/W6746801104', 'https://openalex.org/W3113603766', 'https://openalex.org/W6737196085', 'https://openalex.org/W2889061305', 'https://openalex.org/W3015805741', 'https://openalex.org/W3095936335', 'https://openalex.org/W2937579788', 'https://openalex.org/W2902070858', 'https://openalex.org/W2911340057', 'https://openalex.org/W3197993066', 'https://openalex.org/W2471520273', 'https://openalex.org/W6794157452', 'https://openalex.org/W2123771434', 'https://openalex.org/W2532494225', 'https://openalex.org/W2146334809', 'https://openalex.org/W6711854987', 'https://openalex.org/W2107860279', 'https://openalex.org/W3095361818', 'https://openalex.org/W2938583109', 'https://openalex.org/W6781662123', 'https://openalex.org/W2493477844', 'https://openalex.org/W2105401819', 'https://openalex.org/W2120605154', 'https://openalex.org/W2136922672', 'https://openalex.org/W2019849101', 'https://openalex.org/W6639121642', 'https://openalex.org/W2085662862', 'https://openalex.org/W2885005742', 'https://openalex.org/W2937154351', 'https://openalex.org/W3163573274', 'https://openalex.org/W6781784828', 'https://openalex.org/W3015841875', 'https://openalex.org/W6772230580', 'https://openalex.org/W3014201970', 'https://openalex.org/W1966797434', 'https://openalex.org/W6634507583', 'https://openalex.org/W2972667718', 'https://openalex.org/W2051217765', 'https://openalex.org/W1994198923', 'https://openalex.org/W6777858202', 'https://openalex.org/W2938833595', 'https://openalex.org/W6796050588', 'https://openalex.org/W3025680351', 'https://openalex.org/W2899361462', 'https://openalex.org/W3142644187', 'https://openalex.org/W6781851318', 'https://openalex.org/W3008297462', 'https://openalex.org/W2018338387', 'https://openalex.org/W2123299109', 'https://openalex.org/W2167440618', 'https://openalex.org/W2997399314', 'https://openalex.org/W2144362431', 'https://openalex.org/W2963767194', 'https://openalex.org/W6840412704', 'https://openalex.org/W3015241559', 'https://openalex.org/W3004993066', 'https://openalex.org/W6603838645', 'https://openalex.org/W2963035245', 'https://openalex.org/W6720742747', 'https://openalex.org/W4205742757', 'https://openalex.org/W95152782', 'https://openalex.org/W3098557217', 'https://openalex.org/W2998249245', 'https://openalex.org/W3102628737', 'https://openalex.org/W2187089797', 'https://openalex.org/W2483796469', 'https://openalex.org/W144831613', 'https://openalex.org/W3198791321', 'https://openalex.org/W2396025094', 'https://openalex.org/W2802975219', 'https://openalex.org/W4390912423', 'https://openalex.org/W1869734671', 'https://openalex.org/W2998572311', 'https://openalex.org/W2774848319', 'https://openalex.org/W3096939667', 'https://openalex.org/W2264797740', 'https://openalex.org/W2473388484', 'https://openalex.org/W3130305523', 'https://openalex.org/W3168542456', 'https://openalex.org/W2608338293']",2021-12-13
https://openalex.org/W4398152753,https://doi.org/10.1109/taslp.2024.3402088,InstructTTS: Modelling Expressive TTS in Discrete Latent Space With Natural Language Style Prompt,"Expressive text-to-speech (TTS) aims to synthesize speech with varying speaking styles to better reflect human speech patterns. In this study, we attempt to use natural language as a style prompt to control the styles in the synthetic speech, <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">e.g.</i> , ""Sigh tone in full of sad mood with some helpless feeling"". Considering that there is no existing TTS corpus that is suitable to benchmark this novel task, we first construct a speech corpus whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named InstructTTS, which is novel in the sense of the following aspects: (1) We fully take advantage of self-supervised learning and cross-modal metric learning and propose a novel three-stage training procedure to obtain a robust sentence embedding model that can effectively capture semantic information from the style prompts and control the speaking style in the generated speech. (2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonly-used mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt. Extensive objective and subjective evaluation has been conducted to verify the effectiveness and expressiveness of InstructTTS. Experimental results show that InstructTTS can synthesize high-fidelity and natural speech with style prompts controlling the speaking style. Synthesized samples are available online.","['https://openalex.org/W2963609956', 'https://openalex.org/W6778823374', 'https://openalex.org/W6796464841', 'https://openalex.org/W2972699445', 'https://openalex.org/W2969658393', 'https://openalex.org/W6750489868', 'https://openalex.org/W6749555683', 'https://openalex.org/W6752888775', 'https://openalex.org/W4385822534', 'https://openalex.org/W6848735303', 'https://openalex.org/W6795807602', 'https://openalex.org/W6791353385', 'https://openalex.org/W4210913346', 'https://openalex.org/W2752796333', 'https://openalex.org/W6762931180', 'https://openalex.org/W3180355996', 'https://openalex.org/W6769196770', 'https://openalex.org/W3209059054', 'https://openalex.org/W4307323391', 'https://openalex.org/W3215615641', 'https://openalex.org/W6802805937', 'https://openalex.org/W4367359628', 'https://openalex.org/W6852581948', 'https://openalex.org/W3197294703', 'https://openalex.org/W6777694618', 'https://openalex.org/W3196584150', 'https://openalex.org/W4313316128', 'https://openalex.org/W6846143095', 'https://openalex.org/W3197704090', 'https://openalex.org/W3198123658', 'https://openalex.org/W4375869257', 'https://openalex.org/W6795288823', 'https://openalex.org/W4312388283', 'https://openalex.org/W6810940779', 'https://openalex.org/W6800989748', 'https://openalex.org/W6838815585', 'https://openalex.org/W6846176957', 'https://openalex.org/W6783182287', 'https://openalex.org/W3198213150', 'https://openalex.org/W6849416043', 'https://openalex.org/W4312933868', 'https://openalex.org/W6679045638', 'https://openalex.org/W6796163713', 'https://openalex.org/W6798447524', 'https://openalex.org/W6766673545', 'https://openalex.org/W3156636935', 'https://openalex.org/W6844194202', 'https://openalex.org/W4312096566', 'https://openalex.org/W2157364932', 'https://openalex.org/W6752051073', 'https://openalex.org/W6779459370', 'https://openalex.org/W6796730497', 'https://openalex.org/W6783867762', 'https://openalex.org/W4226132755', 'https://openalex.org/W2963073614', 'https://openalex.org/W4385245566', 'https://openalex.org/W6840815571', 'https://openalex.org/W6838510122', 'https://openalex.org/W4381786045', 'https://openalex.org/W3198533616', 'https://openalex.org/W1494198834', 'https://openalex.org/W6631190155', 'https://openalex.org/W4382202703', 'https://openalex.org/W6757817989', 'https://openalex.org/W2107860279', 'https://openalex.org/W2133665775', 'https://openalex.org/W2067295501', 'https://openalex.org/W2130086727', 'https://openalex.org/W2107740512', 'https://openalex.org/W2058819080', 'https://openalex.org/W6810926057', 'https://openalex.org/W2007962718', 'https://openalex.org/W4388323056', 'https://openalex.org/W6780218876', 'https://openalex.org/W4297808394', 'https://openalex.org/W1522301498', 'https://openalex.org/W4313679638', 'https://openalex.org/W4281771798', 'https://openalex.org/W3174758275', 'https://openalex.org/W2908510526', 'https://openalex.org/W4372279529']",2024-01-01
https://openalex.org/W4385822787,https://doi.org/10.21437/interspeech.2023-1779,PromptStyle: Controllable Style Transfer for Text-to-Speech with Natural Language Descriptions,,[],2023-08-14
https://openalex.org/W4375869257,https://doi.org/10.1109/icassp49357.2023.10096285,Prompttts: Controllable Text-To-Speech With Text Descriptions,"Using a text description as prompt to guide the generation of text or images (e.g., GPT-3 or DALLE-2) has drawn wide attention recently. Beyond text and image generation, in this work, we explore the possibility of utilizing text descriptions to guide speech synthesis. Thus, we develop a text-to-speech (TTS) system (dubbed as PromptTTS) that takes a prompt with both style and content descriptions as input to synthesize the corresponding speech. Specifically, PromptTTS consists of a style encoder and a content encoder to extract the corresponding representations from the prompt, and a speech decoder to synthesize speech according to the extracted style and content representations. Compared with previous works in controllable TTS that require users to have acoustic knowledge to understand style factors such as prosody and pitch, PromptTTS is more user-friendly since text descriptions are a more natural way to express speech style (e.g., ""A lady whispers to her friend slowly""). Given that there is no TTS dataset with prompts, to benchmark the task of PromptTTS, we construct and release a dataset containing prompts with style and content information and the corresponding speech. Experiments show that PromptTTS can generate speech with precise style control and high speech quality. Audio samples and our dataset are publicly available <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .","['https://openalex.org/W6750489868', 'https://openalex.org/W6778823374', 'https://openalex.org/W2972677740', 'https://openalex.org/W6755207826', 'https://openalex.org/W160640889', 'https://openalex.org/W3197541421', 'https://openalex.org/W349236604', 'https://openalex.org/W3176617251', 'https://openalex.org/W4285247752', 'https://openalex.org/W6783867762', 'https://openalex.org/W2972359262', 'https://openalex.org/W4221167022', 'https://openalex.org/W6779879114', 'https://openalex.org/W6713645886', 'https://openalex.org/W6778883912', 'https://openalex.org/W3094650042', 'https://openalex.org/W3174758275', 'https://openalex.org/W4224035735', 'https://openalex.org/W2896457183', 'https://openalex.org/W4280561221', 'https://openalex.org/W3092028330', 'https://openalex.org/W3033411150', 'https://openalex.org/W3158631574', 'https://openalex.org/W2794490148', 'https://openalex.org/W2405756170', 'https://openalex.org/W4292779060', 'https://openalex.org/W3034445277']",2023-05-05
https://openalex.org/W2519091744,https://doi.org/10.48550/arxiv.1609.03499,WaveNet: A Generative Model for Raw Audio,"This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.",[],2016-09-12
https://openalex.org/W4312933868,https://doi.org/10.1109/cvpr52688.2022.01042,High-Resolution Image Synthesis with Latent Diffusion Models,"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.","['https://openalex.org/W2962770929', 'https://openalex.org/W6745560452', 'https://openalex.org/W2963073614', 'https://openalex.org/W6840815571', 'https://openalex.org/W6797359156', 'https://openalex.org/W6772676370', 'https://openalex.org/W6795308914', 'https://openalex.org/W6790830454', 'https://openalex.org/W6799838802', 'https://openalex.org/W6765779288', 'https://openalex.org/W6779823529', 'https://openalex.org/W6692550842', 'https://openalex.org/W6786904838', 'https://openalex.org/W6800989748', 'https://openalex.org/W2970810102', 'https://openalex.org/W6787283782', 'https://openalex.org/W3203631022', 'https://openalex.org/W6637568146', 'https://openalex.org/W2982763192', 'https://openalex.org/W6797201126', 'https://openalex.org/W6639102338', 'https://openalex.org/W6768377600', 'https://openalex.org/W2962974533', 'https://openalex.org/W6757857098', 'https://openalex.org/W6793578827', 'https://openalex.org/W6678815747', 'https://openalex.org/W6729767818', 'https://openalex.org/W6747808759', 'https://openalex.org/W6799880410', 'https://openalex.org/W2561196672', 'https://openalex.org/W6755312952', 'https://openalex.org/W6779879114', 'https://openalex.org/W6787335730', 'https://openalex.org/W6785302134', 'https://openalex.org/W2937274663', 'https://openalex.org/W6782760101', 'https://openalex.org/W6786494455', 'https://openalex.org/W6796042156', 'https://openalex.org/W6640963894', 'https://openalex.org/W6788995615', 'https://openalex.org/W6783182287', 'https://openalex.org/W6797016743', 'https://openalex.org/W6771275388', 'https://openalex.org/W6797906067', 'https://openalex.org/W3140633421', 'https://openalex.org/W6732492507', 'https://openalex.org/W6794269193', 'https://openalex.org/W6637373629', 'https://openalex.org/W6803132585', 'https://openalex.org/W3035687950', 'https://openalex.org/W2129069237', 'https://openalex.org/W6794386015', 'https://openalex.org/W3176823897', 'https://openalex.org/W6790978476', 'https://openalex.org/W6762931180', 'https://openalex.org/W6713645886', 'https://openalex.org/W1909320841', 'https://openalex.org/W6785102375', 'https://openalex.org/W6639824700', 'https://openalex.org/W2741137940', 'https://openalex.org/W6768817161', 'https://openalex.org/W6739901393', 'https://openalex.org/W2752796333', 'https://openalex.org/W6804566349', 'https://openalex.org/W2423557781', 'https://openalex.org/W2732026016', 'https://openalex.org/W6796588791', 'https://openalex.org/W6791190667', 'https://openalex.org/W2962785568', 'https://openalex.org/W6625168331', 'https://openalex.org/W6783637136', 'https://openalex.org/W6793801364', 'https://openalex.org/W6761628794', 'https://openalex.org/W6758800702', 'https://openalex.org/W6676297131', 'https://openalex.org/W6755207826', 'https://openalex.org/W6795288823', 'https://openalex.org/W6786375611', 'https://openalex.org/W6796242362', 'https://openalex.org/W6783713337', 'https://openalex.org/W6775632556', 'https://openalex.org/W6714644935', 'https://openalex.org/W6780573874', 'https://openalex.org/W6687045409', 'https://openalex.org/W6780593937', 'https://openalex.org/W6801666068', 'https://openalex.org/W6775110864', 'https://openalex.org/W3120110650', 'https://openalex.org/W2031342017', 'https://openalex.org/W4293320219', 'https://openalex.org/W4286869901', 'https://openalex.org/W3169064633', 'https://openalex.org/W3190116597', 'https://openalex.org/W3165905282', 'https://openalex.org/W3206395542', 'https://openalex.org/W3002120414', 'https://openalex.org/W1583912456', 'https://openalex.org/W2980282514', 'https://openalex.org/W3041956526', 'https://openalex.org/W4294643831', 'https://openalex.org/W3209532394', 'https://openalex.org/W3129651364', 'https://openalex.org/W3129576130', 'https://openalex.org/W3174711319', 'https://openalex.org/W3110257065', 'https://openalex.org/W2188365844', 'https://openalex.org/W3123097577', 'https://openalex.org/W2907097116', 'https://openalex.org/W4301206121', 'https://openalex.org/W3034445277', 'https://openalex.org/W4298289240', 'https://openalex.org/W3177150392', 'https://openalex.org/W2125389028', 'https://openalex.org/W4295521014', 'https://openalex.org/W4297813370', 'https://openalex.org/W3035574324', 'https://openalex.org/W3196163807', 'https://openalex.org/W3171313410', 'https://openalex.org/W2405756170', 'https://openalex.org/W2783391889', 'https://openalex.org/W3153854932', 'https://openalex.org/W3130440474', 'https://openalex.org/W3152733922', 'https://openalex.org/W2896457183', 'https://openalex.org/W4288372760', 'https://openalex.org/W967544008', 'https://openalex.org/W2963799213', 'https://openalex.org/W2108598243', 'https://openalex.org/W4287182033', 'https://openalex.org/W2953318193', 'https://openalex.org/W3111387095', 'https://openalex.org/W3165647589', 'https://openalex.org/W4323654151', 'https://openalex.org/W3216237230', 'https://openalex.org/W3180355996', 'https://openalex.org/W3036167779', 'https://openalex.org/W4297798428', 'https://openalex.org/W1861492603', 'https://openalex.org/W3199003182', 'https://openalex.org/W3163884521', 'https://openalex.org/W1901129140', 'https://openalex.org/W2962820504', 'https://openalex.org/W4287553002', 'https://openalex.org/W3103556460', 'https://openalex.org/W4301908144', 'https://openalex.org/W3190965961', 'https://openalex.org/W3162926177', 'https://openalex.org/W4287250916', 'https://openalex.org/W4385245566', 'https://openalex.org/W2971074500', 'https://openalex.org/W4288099666', 'https://openalex.org/W3120243996', 'https://openalex.org/W3155072588', 'https://openalex.org/W2964122153', 'https://openalex.org/W4287121833', 'https://openalex.org/W3037032032', 'https://openalex.org/W3175528029', 'https://openalex.org/W3121370741', 'https://openalex.org/W1686810756', 'https://openalex.org/W3118605064', 'https://openalex.org/W3121480429', 'https://openalex.org/W1959608418', 'https://openalex.org/W2952716587', 'https://openalex.org/W4297800839', 'https://openalex.org/W3174301209']",2022-06-01
https://openalex.org/W2107860279,https://doi.org/10.1109/pacrim.1993.407206,Mel-cepstral distance measure for objective speech quality assessment,"The author proposes a perceptually motivated modification to the cepstral distance measure (CD) based on the mel frequency scale and critical-band filtering. The new objective parameter is referred to as the mel cepstral distance (MCD). The author measures and compares the performance of the CD and MCD algorithms by applying them to a dataset representing low-bit-rate code-excited linear prediction (CELP)-coded speech with simulated channel conditions. The improvement in correlation with subjective DAM scores indicates that critical band filtering (and frequency warping) allows better modeling of perceived quality.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>","['https://openalex.org/W2013139519', 'https://openalex.org/W2146283888', 'https://openalex.org/W2148154194', 'https://openalex.org/W2163181067', 'https://openalex.org/W2479702386', 'https://openalex.org/W6726809291', 'https://openalex.org/W2078911760', 'https://openalex.org/W4285719527', 'https://openalex.org/W2519954739']",2002-12-30
https://openalex.org/W2067295501,https://doi.org/10.1109/icassp.2010.5495701,A short-time objective intelligibility measure for time-frequency weighted noisy speech,"Existing objective speech-intelligibility measures are suitable for several types of degradation, however, it turns out that they are less appropriate for methods where noisy speech is processed by a time-frequency (TF) weighting, e.g., noise reduction and speech separation. In this paper, we present an objective intelligibility measure, which shows high correlation (rho=0.95) with the intelligibility of both noisy, and TF-weighted noisy speech. The proposed method shows significantly better performance than three other, more sophisticated, objective measures. Furthermore, it is based on an intermediate intelligibility measure for short-time (approximately 400 ms) TF-regions, and uses a simple DFT-based TF-decomposition. In addition, a free Matlab implementation is provided.","['https://openalex.org/W6717306377', 'https://openalex.org/W2109452115', 'https://openalex.org/W6629499117', 'https://openalex.org/W6635827916', 'https://openalex.org/W2087126002', 'https://openalex.org/W2007790905', 'https://openalex.org/W2041777218', 'https://openalex.org/W1987831012', 'https://openalex.org/W2084458166', 'https://openalex.org/W1985226152', 'https://openalex.org/W3147539069', 'https://openalex.org/W2047068213', 'https://openalex.org/W2074658690', 'https://openalex.org/W1985029311', 'https://openalex.org/W2057889776', 'https://openalex.org/W1494634987', 'https://openalex.org/W2121973264', 'https://openalex.org/W1599328249', 'https://openalex.org/W2419243463']",2010-03-01
https://openalex.org/W1574447377,https://doi.org/10.1007/978-3-642-00296-0_5,Pearson Correlation Coefficient,,[],2009-01-01
https://openalex.org/W2118841860,https://doi.org/10.1109/asru.2005.1566529,Towards unsupervised pattern discovery in speech,"We present an unsupervised algorithm for discovering acoustic patterns in speech by finding matching subsequences between pairs of utterances. The approach we describe is, in theory, language and topic independent, and is particularly well suited for processing large amounts of speech from a single speaker. A variation of dynamic time warping (DTW), which we call segmental DTW, is used to performing the pairwise utterance comparison. Using academic lecture data, we describe two potentially useful applications for the segmental DTW output: augmenting speech recognition transcriptions for information retrieval and speech segment clustering for unsupervised word discovery. Some preliminary qualitative results for both experiments are shown and the implications for future work and applications are discussed","['https://openalex.org/W2009566340', 'https://openalex.org/W2028903194', 'https://openalex.org/W2159344129', 'https://openalex.org/W2114510609', 'https://openalex.org/W2122686984', 'https://openalex.org/W156258841', 'https://openalex.org/W2046134527', 'https://openalex.org/W2099402246', 'https://openalex.org/W1560013842']",2005-01-01
https://openalex.org/W2114347655,https://doi.org/10.1109/tasl.2007.909282,Unsupervised Pattern Discovery in Speech,"We present a novel approach to speech processing based on the principle of pattern discovery. Our work represents a departure from traditional models of speech recognition, where the end goal is to classify speech into categories defined by a prespecified inventory of lexical units (i.e., phones or words). Instead, we attempt to discover such an inventory in an unsupervised manner by exploiting the structure of repeating patterns within the speech signal. We show how pattern discovery can be used to automatically acquire lexical entities directly from an untranscribed audio stream. Our approach to unsupervised word acquisition utilizes a segmental variant of a widely used dynamic programming technique, which allows us to find matching acoustic patterns between spoken utterances. By aggregating information about these matching patterns across audio streams, we demonstrate how to group similar acoustic sequences together to form clusters corresponding to lexical entities such as words and short multiword phrases. On a corpus of academic lecture material, we demonstrate that clusters found using this technique exhibit high purity and that many of the corresponding lexical identities are relevant to the underlying audio stream.","['https://openalex.org/W2111732304', 'https://openalex.org/W2095293504', 'https://openalex.org/W4251670979', 'https://openalex.org/W2086891622', 'https://openalex.org/W2089458547', 'https://openalex.org/W6635336695', 'https://openalex.org/W2099402246', 'https://openalex.org/W3021051756', 'https://openalex.org/W2143235280', 'https://openalex.org/W4237938692', 'https://openalex.org/W2074546930', 'https://openalex.org/W2161952424', 'https://openalex.org/W6678164431', 'https://openalex.org/W2112006116', 'https://openalex.org/W2028903194', 'https://openalex.org/W2114510609', 'https://openalex.org/W1980862600', 'https://openalex.org/W2103447044', 'https://openalex.org/W2121947440', 'https://openalex.org/W2009566340', 'https://openalex.org/W2074231493', 'https://openalex.org/W4245668478', 'https://openalex.org/W2054849588', 'https://openalex.org/W2016243284', 'https://openalex.org/W1610605641', 'https://openalex.org/W2138370049', 'https://openalex.org/W1964917299', 'https://openalex.org/W2140277151', 'https://openalex.org/W2118841860', 'https://openalex.org/W2128160875', 'https://openalex.org/W2164463707', 'https://openalex.org/W2009570821', 'https://openalex.org/W3036063182', 'https://openalex.org/W2122228338', 'https://openalex.org/W2999905431', 'https://openalex.org/W2165874743', 'https://openalex.org/W1589182518', 'https://openalex.org/W2107917162', 'https://openalex.org/W1499245496', 'https://openalex.org/W1483126227', 'https://openalex.org/W1978394996', 'https://openalex.org/W2171009857', 'https://openalex.org/W1530250655', 'https://openalex.org/W1207633162']",2007-12-20
https://openalex.org/W2117041980,https://doi.org/10.3115/1557690.1557736,Unsupervised learning of acoustic sub-word units,"Accurate unsupervised learning of phonemes of a language directly from speech is demonstrated via an algorithm for joint unsupervised learning of the topology and parameters of a hidden Markov model (HMM); states and short state-sequences through this HMM correspond to the learnt sub-word units. The algorithm, originally proposed for unsupervised learning of allophonic variations within a given phoneme set, has been adapted to learn without any knowledge of the phonemes. An evaluation methodology is also proposed, whereby the state-sequence that aligns to a test utterance is transduced in an automatic manner to a phoneme-sequence and compared to its manual transcription. Over 85% phoneme recognition accuracy is demonstrated for speaker-dependent learning from fluent, large-vocabulary speech.","['https://openalex.org/W4298166701', 'https://openalex.org/W3034729383', 'https://openalex.org/W2101536075', 'https://openalex.org/W1971081490', 'https://openalex.org/W1949782964', 'https://openalex.org/W1918599710']",2008-01-01
https://openalex.org/W2126203737,https://doi.org/10.1109/asru.2009.5372931,Unsupervised spoken keyword spotting via segmental DTW on Gaussian posteriorgrams,"In this paper, we present an unsupervised learning framework to address the problem of detecting spoken keywords. Without any transcription information, a Gaussian Mixture Model is trained to label speech frames with a Gaussian posteriorgram. Given one or more spoken examples of a keyword, we use segmental dynamic time warping to compare the Gaussian posteriorgrams between keyword samples and test utterances. The keyword detection result is then obtained by ranking the distortion scores of all the test utterances. We examine the TIMIT corpus as a development set to tune the parameters in our system, and the MIT Lecture corpus for more substantial evaluation. The results demonstrate the viability and effectiveness of our unsupervised learning framework on the keyword spotting task.","['https://openalex.org/W201187342', 'https://openalex.org/W6649874456', 'https://openalex.org/W2122797512', 'https://openalex.org/W2099415988', 'https://openalex.org/W2153426278', 'https://openalex.org/W2155427043', 'https://openalex.org/W2171019095', 'https://openalex.org/W2116969213', 'https://openalex.org/W2114347655', 'https://openalex.org/W2111732304', 'https://openalex.org/W1975113979', 'https://openalex.org/W2103371184', 'https://openalex.org/W2103933358', 'https://openalex.org/W2107076535', 'https://openalex.org/W2405666970', 'https://openalex.org/W2115220208', 'https://openalex.org/W1578200545', 'https://openalex.org/W2086891622', 'https://openalex.org/W2128160875', 'https://openalex.org/W2101536075', 'https://openalex.org/W6684930139', 'https://openalex.org/W1999121090', 'https://openalex.org/W2169384404', 'https://openalex.org/W2142570735']",2009-12-01
https://openalex.org/W2509930204,https://doi.org/10.21437/interspeech.2016-988,Supervised Learning of Acoustic Models in a Zero Resource Setting to Improve DPGMM Clustering,,"['https://openalex.org/W2345811097', 'https://openalex.org/W2064210461', 'https://openalex.org/W2117041980', 'https://openalex.org/W2963620343', 'https://openalex.org/W2119187236', 'https://openalex.org/W1524333225', 'https://openalex.org/W2118841860', 'https://openalex.org/W66167291', 'https://openalex.org/W2395899413', 'https://openalex.org/W2126203737', 'https://openalex.org/W2001619934', 'https://openalex.org/W2002342963', 'https://openalex.org/W2399576818', 'https://openalex.org/W2160719354', 'https://openalex.org/W2402014506', 'https://openalex.org/W2128032727']",2016-08-29
https://openalex.org/W2137826140,https://doi.org/10.1109/icassp.2002.5743879,Unsupervised acoustic model training,"This paper describes some recent experiments using unsupervised techniques for acoustic model training in order to reduce the system development cost. The approach uses a speech recognizer to transcribe unannotated raw broadcast news data. The hypothesized transcription is used to create labels for the training data. Experiments providing supervision only via the language model training materials show that including texts which are contemporaneous with the audio data is not crucial for success of the approach, and that the acoustic models can be initialized with as little as 10 minutes of manually annotated data. These experiments demonstrate that unsupervised training is a viable training scheme and can dramatically reduce the cost of building acoustic models.","['https://openalex.org/W2016243284', 'https://openalex.org/W6871885761', 'https://openalex.org/W6603364914', 'https://openalex.org/W1654858490', 'https://openalex.org/W6677717300', 'https://openalex.org/W6631036137', 'https://openalex.org/W2146871184', 'https://openalex.org/W2143562645', 'https://openalex.org/W82490022', 'https://openalex.org/W2918078540', 'https://openalex.org/W4402490925', 'https://openalex.org/W4285719527', 'https://openalex.org/W2117590177', 'https://openalex.org/W1555037511']",2002-05-01
https://openalex.org/W2402014506,https://doi.org/10.1137/1.9781611973440.107,Discriminant Analysis for Unsupervised Feature Selection,"Previous chapter Next chapter Full AccessProceedings Proceedings of the 2014 SIAM International Conference on Data Mining (SDM)Discriminant Analysis for Unsupervised Feature SelectionJiliang Tang, Xia Hu, Huiji Gao, and Huan LiuJiliang Tang, Xia Hu, Huiji Gao, and Huan Liupp.938 - 946Chapter DOI:https://doi.org/10.1137/1.9781611973440.107PDFBibTexSections ToolsAdd to favoritesExport CitationTrack CitationsEmail SectionsAboutAbstract Feature selection has been proven to be efficient in preparing high dimensional data for data mining and machine learning. As most data is unlabeled, unsupervised feature selection has attracted more and more attention in recent years. Discriminant analysis has been proven to be a powerful technique to select discriminative features for supervised feature selection. To apply discriminant analysis, we usually need label information which is absent for unlabeled data. This gap makes it challenging to apply discriminant analysis for unsupervised feature selection. In this paper, we investigate how to exploit discriminant analysis in unsupervised scenarios to select discriminative features. We introduce the concept of pseudo labels, which enable discriminant analysis on unlabeled data, propose a novel unsupervised feature selection framework DisUFS which incorporates learning discriminative features with generating pseudo labels, and develop an effective algorithm for DisUFS. Experimental results on different types of real-world data demonstrate the effectiveness of the proposed framework DisUFS. Previous chapter Next chapter RelatedDetails Published:2014eISBN:978-1-61197-344-0 https://doi.org/10.1137/1.9781611973440Book Series Name:ProceedingsBook Code:PRDT14Book Pages:1-1086","['https://openalex.org/W2053090063', 'https://openalex.org/W1619226191', 'https://openalex.org/W2149620660', 'https://openalex.org/W140777655', 'https://openalex.org/W2064210461', 'https://openalex.org/W1528703258', 'https://openalex.org/W2096364142', 'https://openalex.org/W2135346934', 'https://openalex.org/W2099322651', 'https://openalex.org/W1581499779', 'https://openalex.org/W2154053567', 'https://openalex.org/W2009501510', 'https://openalex.org/W2121007818', 'https://openalex.org/W1607408214', 'https://openalex.org/W1980053379', 'https://openalex.org/W2408567027', 'https://openalex.org/W2132771435', 'https://openalex.org/W2071952657', 'https://openalex.org/W2165644552', 'https://openalex.org/W2020652789', 'https://openalex.org/W2158933803', 'https://openalex.org/W2120000263', 'https://openalex.org/W2143426320', 'https://openalex.org/W1990009229', 'https://openalex.org/W2171837816', 'https://openalex.org/W1560107318', 'https://openalex.org/W2950109329', 'https://openalex.org/W2799061466', 'https://openalex.org/W2616354903', 'https://openalex.org/W2102831150', 'https://openalex.org/W1972294657', 'https://openalex.org/W2128873747', 'https://openalex.org/W2963951026', 'https://openalex.org/W1871180460']",2014-04-28
https://openalex.org/W2056786202,https://doi.org/10.1109/tsa.2004.838537,Unsupervised training of acoustic models for large vocabulary continuous speech recognition,"For large vocabulary continuous speech recognition systems, the amount of acoustic training data is of crucial importance. In the past, large amounts of speech were thus recorded from various sources and had to be transcribed manually. It is thus desirable to train a recognizer with as little manually transcribed acoustic data as possible. Since untranscribed speech is available in various forms nowadays, the unsupervised training of a speech recognizer on recognized transcriptions is studied in this paper. A low-cost recognizer trained with between one and six h of manually transcribed speech is used to recognize 72 h of untranscribed acoustic data. These transcriptions are then used in combination with a confidence measure to train an improved recognizer. The effect of the confidence measure which is used to detect possible recognition errors is studied systematically. Finally, the unsupervised training is applied iteratively. Starting with only one h of transcribed acoustic data, a recognition system is trained fully automatically. With this iterative training procedure, the word error rates are reduced from 71.3% to 38.3% on the Broadcast News'96 evaluation test set and from 65.6% to 29.3% on the Broadcast News'98 evaluation test set. In comparison with an optimized system trained with the manually generated transcriptions of the complete 72 h training corpus, the word error rates increase by 14.3% relative and 18.6% relative, respectively.","['https://openalex.org/W1885478233', 'https://openalex.org/W2134659216', 'https://openalex.org/W2113094193', 'https://openalex.org/W2164579587', 'https://openalex.org/W6600238503', 'https://openalex.org/W2009267849', 'https://openalex.org/W6677717300', 'https://openalex.org/W4402490925', 'https://openalex.org/W1975113979', 'https://openalex.org/W6637124410', 'https://openalex.org/W2171761326', 'https://openalex.org/W6601375071', 'https://openalex.org/W1920769845', 'https://openalex.org/W2541906951', 'https://openalex.org/W5671660', 'https://openalex.org/W1654858490', 'https://openalex.org/W1555037511', 'https://openalex.org/W34303869', 'https://openalex.org/W2117590177', 'https://openalex.org/W4285719527', 'https://openalex.org/W1975381461']",2004-12-20
https://openalex.org/W1975113979,https://doi.org/10.1006/csla.2001.0186,Lightly supervised and unsupervised acoustic model training,,"['https://openalex.org/W2096571067', 'https://openalex.org/W2143562645', 'https://openalex.org/W1549285799', 'https://openalex.org/W2016243284', 'https://openalex.org/W2100969003', 'https://openalex.org/W4402490925', 'https://openalex.org/W2146871184', 'https://openalex.org/W2095833672']",2002-01-01
https://openalex.org/W2401464865,https://doi.org/10.21437/interspeech.2011-184,Towards unsupervised training of speaker independent acoustic models,"Can we automatically discover speaker independent phonemelike subword units with zero resources in a surprise language? There have been a number of recent efforts to automatically discover repeated spoken terms without a recognizer. This paper investigates the feasibility of using these results as constraints for unsupervised acoustic model training. We start with a relatively small set of word types, as well as their locations in the speech. The training process assumes that repetitions of the same (unknown) word share the same (unknown) sequence of subword units. For each word type, we train a whole-word hidden Markov model with Gaussian mixture observation densities and collapse correlated states across the word types using spectral clustering. We find that the resulting state clusters align reasonably well along phonetic lines. In evaluating cross-speaker word similarity, the proposed techniques outperform both raw acoustic features and language-mismatched acoustic models.","['https://openalex.org/W2117041980', 'https://openalex.org/W2114347655', 'https://openalex.org/W2099415988', 'https://openalex.org/W1539935047', 'https://openalex.org/W30845872', 'https://openalex.org/W2167655920', 'https://openalex.org/W1964917299', 'https://openalex.org/W1606268232', 'https://openalex.org/W308497914', 'https://openalex.org/W2079460648']",2011-08-27
https://openalex.org/W2078769636,https://doi.org/10.1016/j.csl.2013.05.002,Unsupervised training of an HMM-based self-organizing unit recognizer with applications to topic classification and keyword discovery,,"['https://openalex.org/W6608294043', 'https://openalex.org/W6683874881', 'https://openalex.org/W2087863531', 'https://openalex.org/W6674847979', 'https://openalex.org/W6684527036', 'https://openalex.org/W6681627878', 'https://openalex.org/W6652900202', 'https://openalex.org/W6679182551', 'https://openalex.org/W6601311673', 'https://openalex.org/W1975113979', 'https://openalex.org/W6633159005', 'https://openalex.org/W2106284094', 'https://openalex.org/W6603381559', 'https://openalex.org/W2099424804', 'https://openalex.org/W149529473', 'https://openalex.org/W6677452305', 'https://openalex.org/W1507177964', 'https://openalex.org/W6610841746', 'https://openalex.org/W6713082324', 'https://openalex.org/W6631419337', 'https://openalex.org/W6670629611', 'https://openalex.org/W6684865868', 'https://openalex.org/W2143203634', 'https://openalex.org/W30845872', 'https://openalex.org/W308497914', 'https://openalex.org/W2399869768', 'https://openalex.org/W203607283', 'https://openalex.org/W2105778889', 'https://openalex.org/W82886505', 'https://openalex.org/W2167845555', 'https://openalex.org/W2009388533', 'https://openalex.org/W2153635508', 'https://openalex.org/W2079460648', 'https://openalex.org/W2164922523', 'https://openalex.org/W2118841860', 'https://openalex.org/W1552962923', 'https://openalex.org/W2099415988', 'https://openalex.org/W3120421331', 'https://openalex.org/W4285719527', 'https://openalex.org/W51277926', 'https://openalex.org/W202343879', 'https://openalex.org/W2130180273', 'https://openalex.org/W2162042658', 'https://openalex.org/W2168347480', 'https://openalex.org/W1526995323']",2013-05-20
https://openalex.org/W2064210461,https://doi.org/10.1145/1273496.1273562,Adaptive dimension reduction using discriminant analysis and <i>K</i> -means clustering,"We combine linear discriminant analysis (LDA) and K-means clustering into a coherent framework to adaptively select the most discriminative subspace. We use K-means clustering to generate class labels and use LDA to do subspace selection. The clustering process is thus integrated with the subspace selection process and the data are then simultaneously clustered while the feature subspaces are selected. We show the rich structure of the general LDA-Km framework by examining its variants and their relationships to earlier approaches. Relations among PCA, LDA, K-means are clarified. Extensive experimental results on real-world datasets show the effectiveness of our approach.","['https://openalex.org/W2138072998', 'https://openalex.org/W2168466186', 'https://openalex.org/W2133576408', 'https://openalex.org/W2099242680', 'https://openalex.org/W2571268788', 'https://openalex.org/W2043545458', 'https://openalex.org/W6630096803', 'https://openalex.org/W2149148023', 'https://openalex.org/W1480376833', 'https://openalex.org/W6776535907', 'https://openalex.org/W6680012447', 'https://openalex.org/W2140942285', 'https://openalex.org/W2146820706', 'https://openalex.org/W2006533296', 'https://openalex.org/W2052730000', 'https://openalex.org/W2052819443', 'https://openalex.org/W1493217831', 'https://openalex.org/W2799061466', 'https://openalex.org/W1575173685', 'https://openalex.org/W2139850885', 'https://openalex.org/W1531259569', 'https://openalex.org/W3143596294', 'https://openalex.org/W1929593512', 'https://openalex.org/W1672197616', 'https://openalex.org/W2966207845', 'https://openalex.org/W1550273947', 'https://openalex.org/W2148694408', 'https://openalex.org/W4292023222', 'https://openalex.org/W2135029798', 'https://openalex.org/W2141087758', 'https://openalex.org/W2476894455', 'https://openalex.org/W4230946174']",2007-06-20
https://openalex.org/W4234482113,https://doi.org/10.4324/9781410611147,Detection Theory,,[],2004-09-22
https://openalex.org/W2146950091,https://doi.org/10.1109/18.61115,Divergence measures based on the Shannon entropy,"A novel class of information-theoretic divergence measures based on the Shannon entropy is introduced. Unlike the well-known Kullback divergences, the new measures do not require the condition of absolute continuity to be satisfied by the probability distributions involved. More importantly, their close relationship with the variational distance and the probability of misclassification error are established in terms of bounds. These bounds are crucial in many applications of divergence measures. The measures are also well characterized by the properties of nonnegativity, finiteness, semiboundedness, and boundedness.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>","['https://openalex.org/W1971105294', 'https://openalex.org/W4248709535', 'https://openalex.org/W2129000925', 'https://openalex.org/W2154945989', 'https://openalex.org/W2102699036', 'https://openalex.org/W2149197198', 'https://openalex.org/W2084157359', 'https://openalex.org/W1973429478', 'https://openalex.org/W1965555277', 'https://openalex.org/W2073946873', 'https://openalex.org/W1998701233', 'https://openalex.org/W2039699818', 'https://openalex.org/W2163166770', 'https://openalex.org/W1973375658', 'https://openalex.org/W2006087861', 'https://openalex.org/W6606071154', 'https://openalex.org/W2091952242', 'https://openalex.org/W2006681603', 'https://openalex.org/W2129802470', 'https://openalex.org/W2095456658', 'https://openalex.org/W6694881519', 'https://openalex.org/W152055444', 'https://openalex.org/W1593125407', 'https://openalex.org/W2278264611', 'https://openalex.org/W564239482', 'https://openalex.org/W3127518054', 'https://openalex.org/W4300906944', 'https://openalex.org/W2332768468', 'https://openalex.org/W2107289862', 'https://openalex.org/W1552416095', 'https://openalex.org/W1989008275', 'https://openalex.org/W2142901448']",1991-01-01
https://openalex.org/W2113641473,https://doi.org/10.1109/18.87000,The zero-frequency problem: estimating the probabilities of novel events in adaptive text compression,"Approaches to the zero-frequency problem in adaptive text compression are discussed. This problem relates to the estimation of the likelihood of a novel event occurring. Although several methods have been used, their suitability has been on empirical evaluation rather than a well-founded model. The authors propose the application of a Poisson process model of novelty. Its ability to predict novel tokens is evaluated, and it consistently outperforms existing methods. It is applied to a practical statistical coding scheme, where a slight modification is required to avoid divergence. The result is a well-founded zero-frequency model that explains observed differences in the performance of existing methods, and offers a small improvement in the coding efficiency of text compression over the best method previously known.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>","['https://openalex.org/W2095306947', 'https://openalex.org/W4247267886', 'https://openalex.org/W1975965284', 'https://openalex.org/W2161628678', 'https://openalex.org/W2152851525', 'https://openalex.org/W2046419776', 'https://openalex.org/W2911940095', 'https://openalex.org/W1974717499', 'https://openalex.org/W2107745473', 'https://openalex.org/W2129652681', 'https://openalex.org/W2119047110', 'https://openalex.org/W2122962290', 'https://openalex.org/W2146368895', 'https://openalex.org/W2171763016', 'https://openalex.org/W2007108779', 'https://openalex.org/W4285719527', 'https://openalex.org/W2126163471', 'https://openalex.org/W2045740840', 'https://openalex.org/W45615632', 'https://openalex.org/W2611071497', 'https://openalex.org/W17078302', 'https://openalex.org/W1519253855']",1991-07-01
https://openalex.org/W2128032727,,Parallel Sampling of DP Mixture Models using Sub-Cluster Splits,"We present an MCMC sampler for Dirichlet process mixture models that can be parallelized to achieve significant computational gains. We combine a non-ergodic, restricted Gibbs iteration with split/merge proposals in a manner that produces an ergodic Markov chain. Each cluster is augmented with two sub-clusters to construct likely split moves. Unlike some previous parallel samplers, the proposed sampler enforces the correct stationary distribution of the Markov chain without the need for finite approximations. Empirical results illustrate that the new sampler exhibits better convergence properties than current methods.","['https://openalex.org/W2112796928', 'https://openalex.org/W2138309709', 'https://openalex.org/W2053405531', 'https://openalex.org/W2127498532', 'https://openalex.org/W3120740533', 'https://openalex.org/W2158266063', 'https://openalex.org/W2155762504', 'https://openalex.org/W1971807270', 'https://openalex.org/W3145738572', 'https://openalex.org/W2467379829', 'https://openalex.org/W2062882942', 'https://openalex.org/W2100231460', 'https://openalex.org/W2062373184', 'https://openalex.org/W2163229341', 'https://openalex.org/W2170886255', 'https://openalex.org/W2118036030', 'https://openalex.org/W2125858773', 'https://openalex.org/W29489373', 'https://openalex.org/W2072169887', 'https://openalex.org/W2105767795', 'https://openalex.org/W2069429561', 'https://openalex.org/W1833498382', 'https://openalex.org/W2080972498', 'https://openalex.org/W2162021827', 'https://openalex.org/W2101998432', 'https://openalex.org/W1551893515', 'https://openalex.org/W2110717342', 'https://openalex.org/W1500803570', 'https://openalex.org/W2132827946', 'https://openalex.org/W2091797506', 'https://openalex.org/W2082630584']",2013-12-05
https://openalex.org/W2345811097,https://doi.org/10.1016/j.procs.2016.04.032,Unsupervised Linear Discriminant Analysis for Supporting DPGMM Clustering in the Zero Resource Scenario,"In this work we make use of unsupervised linear discriminant analysis (LDA) to support acoustic unit discovery in a zero resource scenario. The idea is to automatically find a mapping of feature vectors into a subspace that is more suitable for Dirichlet process Gaussian mixture model (DPGMM) based clustering, without the need of supervision. Supervised acoustic modeling typically makes use of feature transformations such as LDA to minimize intra-class discriminability, to maximize inter-class discriminability and to extract relevant informations from high-dimensional features spanning larger contexts. The need of class labels makes it difficult to use this technique in a zero resource setting where the classes and even their amount are unknown. To overcome this issue we use a first iteration of DPGMM clustering on standard features to generate labels for the data, that serve as basis for learning a proper transformation. A second clustering operates on the transformed features. The application of unsupervised LDA demonstrably leads to better clustering results given the unsupervised data. We show that the improved input features consistently outperform our baseline input features.","['https://openalex.org/W2118841860', 'https://openalex.org/W2114347655', 'https://openalex.org/W2117041980', 'https://openalex.org/W2126203737', 'https://openalex.org/W1502957213', 'https://openalex.org/W1997505733', 'https://openalex.org/W2399576818', 'https://openalex.org/W2001619934', 'https://openalex.org/W2064210461', 'https://openalex.org/W2402014506', 'https://openalex.org/W1981276685', 'https://openalex.org/W2346964103', 'https://openalex.org/W2395899413', 'https://openalex.org/W2294798173', 'https://openalex.org/W2071128523', 'https://openalex.org/W66167291', 'https://openalex.org/W2119187236', 'https://openalex.org/W2786608204', 'https://openalex.org/W2128032727']",2016-01-01
https://openalex.org/W2395899413,https://doi.org/10.21437/interspeech.2013-441,Evaluating speech features with the minimal-pair ABX task: analysis of the classical MFC/PLP pipeline,"We present a new framework for the evaluation of speech representations in zero-resource settings, that extends and complements previous work by Carlin, Jansen and Hermansky [1].In particular, we replace their Same/Different discrimination task by several Minimal-Pair ABX (MP-ABX) tasks.We explain the analytical advantages of this new framework and apply it to decompose the standard signal processing pipelines for computing PLP and MFC coefficients.This method enables us to confirm and quantify a variety of well-known and not-so-well-known results in a single framework.","['https://openalex.org/W2407151108', 'https://openalex.org/W2232131953', 'https://openalex.org/W2980286501', 'https://openalex.org/W2137075158', 'https://openalex.org/W2090861223', 'https://openalex.org/W4285719527', 'https://openalex.org/W2096765209', 'https://openalex.org/W2986535481', 'https://openalex.org/W4247807440', 'https://openalex.org/W2054139811', 'https://openalex.org/W168991039', 'https://openalex.org/W2406820985', 'https://openalex.org/W2089177488', 'https://openalex.org/W2025482506', 'https://openalex.org/W2160719354', 'https://openalex.org/W156237177', 'https://openalex.org/W48303286', 'https://openalex.org/W2400113920', 'https://openalex.org/W2117041980', 'https://openalex.org/W2100768664', 'https://openalex.org/W282666689', 'https://openalex.org/W1480485976']",2013-08-25
https://openalex.org/W2399576818,https://doi.org/10.21437/interspeech.2015-642,Parallel inference of dirichlet process Gaussian mixture models for unsupervised acoustic modeling: a feasibility study,"We adopt a Dirichlet process Gaussian mixture model (DPGMM) for unsupervised acoustic modeling and represent speech frames with Gaussian posteriorgrams. The model performs unsupervised clustering on untranscribed data, and each Gaussian component can be considered as a cluster of sounds from various speakers. The model infers its model complexity (i.e. the number of Gaussian components) from the data. For computation efficiency, we use a parallel sampler for the model inference. Our experiments are conducted on the corpus provided by the zero resource speech challenge. Experimental results show that the unsupervised DPGMM posteriorgrams obviously outperformMFCC, and perform comparably to the posteriorgrams derived from language-mismatched phoneme recognizers in terms of the error rate of ABX discrimination test. The error rates can be further reduced by the fusion of these two kinds of posteriorgrams.","['https://openalex.org/W2406349064', 'https://openalex.org/W2119187236', 'https://openalex.org/W2106284094', 'https://openalex.org/W2114347655', 'https://openalex.org/W2049142189', 'https://openalex.org/W2125534887', 'https://openalex.org/W2127498532', 'https://openalex.org/W2151967501', 'https://openalex.org/W66167291', 'https://openalex.org/W2125247927', 'https://openalex.org/W2395899413', 'https://openalex.org/W2079460648', 'https://openalex.org/W1967924372', 'https://openalex.org/W2020607164', 'https://openalex.org/W2213520355', 'https://openalex.org/W2399363370', 'https://openalex.org/W2126203737', 'https://openalex.org/W2406820985', 'https://openalex.org/W2117041980', 'https://openalex.org/W2100768664', 'https://openalex.org/W1997505733', 'https://openalex.org/W2170659185', 'https://openalex.org/W2078769636', 'https://openalex.org/W2052697931', 'https://openalex.org/W2171019095', 'https://openalex.org/W2065136108', 'https://openalex.org/W1503398984', 'https://openalex.org/W2168319451', 'https://openalex.org/W2044138293', 'https://openalex.org/W2080972498', 'https://openalex.org/W1975728937', 'https://openalex.org/W2162021827', 'https://openalex.org/W1833498382', 'https://openalex.org/W1545920196', 'https://openalex.org/W2154085905', 'https://openalex.org/W2128032727', 'https://openalex.org/W2110589736']",2015-09-06
https://openalex.org/W66167291,,NLP on Spoken Documents Without ASR,"There is considerable interest in interdisciplinary combinations of automatic speech recognition (ASR), machine learning, natural language processing, text classification and information retrieval. Many of these boxes, especially ASR, are often based on considerable linguistic resources. We would like to be able to process spoken documents with few (if any) resources. Moreover, connecting black boxes in series tends to multiply errors, especially when the key terms are out-ofvocabulary (OOV). The proposed alternative applies text processing directly to the speech without a dependency on ASR. The method finds long ( ∼ 1 sec) repetitions in speech, and clusters them into pseudo-terms (roughly phrases). Document clustering and classification work surprisingly well on pseudoterms; performance on a Switchboard task approaches a baseline using gold standard manual transcriptions. 1","['https://openalex.org/W1970381522', 'https://openalex.org/W2047940964', 'https://openalex.org/W2149684865', 'https://openalex.org/W2166637769', 'https://openalex.org/W2119187236', 'https://openalex.org/W2170580867', 'https://openalex.org/W2113227740', 'https://openalex.org/W2096175520', 'https://openalex.org/W2130180273', 'https://openalex.org/W2105778889', 'https://openalex.org/W2122537498', 'https://openalex.org/W2110441437', 'https://openalex.org/W2099415988', 'https://openalex.org/W165878654', 'https://openalex.org/W2072240081', 'https://openalex.org/W2130975838', 'https://openalex.org/W2160218441', 'https://openalex.org/W2114347655', 'https://openalex.org/W1974930421', 'https://openalex.org/W30845872', 'https://openalex.org/W2127849236']",2010-10-09
https://openalex.org/W1631260214,https://doi.org/10.21437/icslp.2002-303,SRILM - an extensible language modeling toolkit,,"['https://openalex.org/W2121227244', 'https://openalex.org/W1582730572', 'https://openalex.org/W1528470941', 'https://openalex.org/W1551413707', 'https://openalex.org/W1549285799', 'https://openalex.org/W2594610113', 'https://openalex.org/W2075401516', 'https://openalex.org/W2100506586', 'https://openalex.org/W2084084380', 'https://openalex.org/W2162132066', 'https://openalex.org/W2139884663', 'https://openalex.org/W2124246394', 'https://openalex.org/W2127836646', 'https://openalex.org/W32217939', 'https://openalex.org/W1797288984', 'https://openalex.org/W2158195707', 'https://openalex.org/W1536904578', 'https://openalex.org/W2097978681', 'https://openalex.org/W1904457459']",2002-09-16
https://openalex.org/W2786608204,https://doi.org/10.21437/interspeech.2015-638,The zero resource speech challenge 2015,"établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.","['https://openalex.org/W2052697931', 'https://openalex.org/W2114347655', 'https://openalex.org/W1524333225', 'https://openalex.org/W2787426069', 'https://openalex.org/W2079460648', 'https://openalex.org/W2572097499', 'https://openalex.org/W2101509422', 'https://openalex.org/W2787447541', 'https://openalex.org/W2466918907', 'https://openalex.org/W2055408826', 'https://openalex.org/W2170659185', 'https://openalex.org/W2049142189', 'https://openalex.org/W2251025892', 'https://openalex.org/W2150389998', 'https://openalex.org/W2284628133', 'https://openalex.org/W979905723', 'https://openalex.org/W2117126688', 'https://openalex.org/W1796128977', 'https://openalex.org/W2786608204', 'https://openalex.org/W1922655562', 'https://openalex.org/W1545920196', 'https://openalex.org/W2404799143', 'https://openalex.org/W2128032727', 'https://openalex.org/W1967924372', 'https://openalex.org/W91681889', 'https://openalex.org/W2126377586', 'https://openalex.org/W2238331496', 'https://openalex.org/W2078769636', 'https://openalex.org/W2786902352', 'https://openalex.org/W2395899413', 'https://openalex.org/W2247128061', 'https://openalex.org/W2483390977', 'https://openalex.org/W2117041980', 'https://openalex.org/W2949328740', 'https://openalex.org/W2100768664', 'https://openalex.org/W2406349064', 'https://openalex.org/W2101281673', 'https://openalex.org/W2044138293', 'https://openalex.org/W2346964103', 'https://openalex.org/W2057007397', 'https://openalex.org/W2020607164', 'https://openalex.org/W2787223168', 'https://openalex.org/W2399576818', 'https://openalex.org/W2964169922', 'https://openalex.org/W2963211739', 'https://openalex.org/W2785860501']",2015-09-06
https://openalex.org/W1524333225,,The Kaldi Speech Recognition Toolkit,"Abstract—We describe the design of Kaldi, a free, open-source toolkit for speech recognition research. Kaldi provides a speech recognition system based on finite-state transducers (using the freely available OpenFst), together with detailed documentation and scripts for building complete recognition systems. Kaldi is written is C++, and the core library supports modeling of arbitrary phonetic-context sizes, acoustic modeling with subspace Gaussian mixture models (SGMM) as well as standard Gaussian mixture models, together with all commonly used linear and affine transforms. Kaldi is released under the Apache License v2.0, which is highly nonrestrictive, making it suitable for a wide community of users. I.","['https://openalex.org/W38163879', 'https://openalex.org/W1582482241', 'https://openalex.org/W1981706894', 'https://openalex.org/W2045644151', 'https://openalex.org/W2122028591', 'https://openalex.org/W2002342963', 'https://openalex.org/W2046932483', 'https://openalex.org/W31925794', 'https://openalex.org/W2106554350', 'https://openalex.org/W2148428486', 'https://openalex.org/W2124629003', 'https://openalex.org/W2157421955', 'https://openalex.org/W1491238342', 'https://openalex.org/W2146871184', 'https://openalex.org/W173010698']",2011-01-01
https://openalex.org/W2119187236,,Making Sense of Sound: Unsupervised Topic Segmentation over Acoustic Input,"We address the task of unsupervised topic segmentation of speech data operating over raw acoustic information. In contrast to existing algorithms for topic segmentation of speech, our approach does not require input transcripts. Our method predicts topic changes by analyzing the distribution of reoccurring acoustic patterns in the speech signal corresponding to a single speaker. The algorithm robustly handles noise inherent in acoustic matching by intelligently aggregating information about the similarity profile from multiple local comparisons. Our experiments show that audio-based segmentation compares favorably with transcriptbased segmentation computed over noisy transcripts. These results demonstrate the desirability of our method for applications where a speech recognizer is not available, or its output has a high word error rate. 1","['https://openalex.org/W2140277151', 'https://openalex.org/W1530250655', 'https://openalex.org/W1557074680', 'https://openalex.org/W2028757787', 'https://openalex.org/W2949496004', 'https://openalex.org/W3127686677', 'https://openalex.org/W2150134853', 'https://openalex.org/W2619993508', 'https://openalex.org/W2159083595', 'https://openalex.org/W2952343510', 'https://openalex.org/W1554663460', 'https://openalex.org/W2106918957', 'https://openalex.org/W1824380880', 'https://openalex.org/W3036063182', 'https://openalex.org/W2114510609', 'https://openalex.org/W2118612506', 'https://openalex.org/W2148818577', 'https://openalex.org/W2121947440', 'https://openalex.org/W2100873065', 'https://openalex.org/W2074546930', 'https://openalex.org/W88864901', 'https://openalex.org/W2426479676', 'https://openalex.org/W2124449078', 'https://openalex.org/W1488461535', 'https://openalex.org/W2161952424']",2007-06-01
https://openalex.org/W3148201686,https://doi.org/10.1109/icassp.2002.1005880,Unsupervised acoustic model training,,[],2002-01-01
https://openalex.org/W2100768664,,A Nonparametric Bayesian Approach to Acoustic Model Discovery,"We investigate the problem of acoustic modeling in which prior language-specific knowledge and transcribed data are unavailable. We present an unsupervised model that simultaneously segments the speech, discovers a proper set of sub-word units (e.g., phones) and learns a Hidden Markov Model (HMM) for each induced acoustic unit. Our approach is formulated as a Dirichlet process mixture model in which each mixture is an HMM that represents a sub-word unit. We apply our model to the TIMIT corpus, and the results demonstrate that our model discovers sub-word units that are highly correlated with English phones and also produces better segmentation than the state-of-the-art unsupervised baseline. We test the quality of the learned acoustic models on a spoken term detection task. Compared to the baselines, our model improves the relative precision of top hits by at least 22.1 % and outperforms a language-mismatched acoustic model. 1","['https://openalex.org/W2099415988', 'https://openalex.org/W2080972498', 'https://openalex.org/W53570656', 'https://openalex.org/W2117041980', 'https://openalex.org/W2120636621', 'https://openalex.org/W2154093685', 'https://openalex.org/W2077804127', 'https://openalex.org/W2401464865', 'https://openalex.org/W2121997342', 'https://openalex.org/W2026858810', 'https://openalex.org/W1957665339', 'https://openalex.org/W1990005915', 'https://openalex.org/W2083904075', 'https://openalex.org/W3127686677', 'https://openalex.org/W2045656233', 'https://openalex.org/W2126377586', 'https://openalex.org/W2148154194', 'https://openalex.org/W2111732304', 'https://openalex.org/W2171752983', 'https://openalex.org/W2126203737', 'https://openalex.org/W3104490327', 'https://openalex.org/W2403642609']",2012-07-08
https://openalex.org/W2346964103,https://doi.org/10.1016/j.procs.2016.04.031,The Zero Resource Speech Challenge 2015: Proposed Approaches and Results,"This paper reports on the results of the Zero Resource Speech Challenge 2015, the first unified benchmark for zero resource speech technology, which aims at the unsupervised discovery of subword and word units from raw speech. This paper discusses the motivation for the challenge, its data sets, tasks and baseline systems. We outline the ideas behind the systems that were submitted for the two challenge tracks: unsupervised subword unit modeling and spoken term discovery, and summarize their results. The results obtained by participating teams show great promise; many systems beat the provided baselines and some even perform better than comparable supervised systems.","['https://openalex.org/W2025482506', 'https://openalex.org/W2346964103', 'https://openalex.org/W2400668844', 'https://openalex.org/W2396043527', 'https://openalex.org/W2402366697', 'https://openalex.org/W2399576818', 'https://openalex.org/W2407614114', 'https://openalex.org/W2398490608', 'https://openalex.org/W1796128977', 'https://openalex.org/W2404799143', 'https://openalex.org/W2044138293', 'https://openalex.org/W2238331496', 'https://openalex.org/W2247128061', 'https://openalex.org/W2395899413', 'https://openalex.org/W2406349064', 'https://openalex.org/W2251025892', 'https://openalex.org/W2057007397', 'https://openalex.org/W2117126688', 'https://openalex.org/W2052697931', 'https://openalex.org/W2011845089', 'https://openalex.org/W2786608204']",2016-01-01
https://openalex.org/W2347098582,https://doi.org/10.1016/j.procs.2016.04.033,Variational Inference for Acoustic Unit Discovery,"Recently, several nonparametric Bayesian models have been proposed to automatically discover acoustic units in unlabeled data. Most of them are trained using various versions of the Gibbs Sampling (GS) method. In this work, we consider Variational Bayes (VB) as alternative inference process. Even though VB yields an approximate solution of the posterior distribution it can be easily parallelized which makes it more suitable for large database. Results show that, notwithstanding VB inference is an order of magnitude faster, it outperforms GS in terms of accuracy.","['https://openalex.org/W2170353620', 'https://openalex.org/W1796128977', 'https://openalex.org/W6959456255', 'https://openalex.org/W1967687583', 'https://openalex.org/W2127498532', 'https://openalex.org/W1635512741', 'https://openalex.org/W6624852173', 'https://openalex.org/W2040891197', 'https://openalex.org/W2154099718', 'https://openalex.org/W1506806321', 'https://openalex.org/W2619993508', 'https://openalex.org/W2120636621', 'https://openalex.org/W1516111018', 'https://openalex.org/W2100768664']",2016-01-01
https://openalex.org/W2745710152,https://doi.org/10.21437/glu.2017-6,Partitioning of Posteriorgrams Using Siamese Models for Unsupervised Acoustic Modelling,"Unsupervised methods tend to discover highly speaker-specific representations of speech.We propose a method for improving the quality of posteriorgrams generated from an unsupervised model through partitioning of the latent classes.We do this by training a sparse siamese model to find a linear transformation of the input posteriorgrams to lower-dimensional posteriorgrams.The siamese model makes use of same-category and differentcategory speech fragment pairs obtained by unsupervised term discovery.After training, the model is converted into an exact partitioning of the posteriorgrams.We evaluate the model on the minimal-pair ABX task in the context of the Zero Resource Speech Challenge.We are able to demonstrate that our method significantly reduces the dimensionality of standard Gaussian mixture model posteriorgrams, while still making them more robust to speaker variations.This suggests that the model may be viable as a general post-processing step to improve probabilistic acoustic features obtained by unsupervised learning.","['https://openalex.org/W2404799143', 'https://openalex.org/W2964121744', 'https://openalex.org/W1987971958', 'https://openalex.org/W1486019888', 'https://openalex.org/W2052697931', 'https://openalex.org/W113159538', 'https://openalex.org/W2171590421', 'https://openalex.org/W2101234009', 'https://openalex.org/W1796128977', 'https://openalex.org/W2401464865', 'https://openalex.org/W2395899413', 'https://openalex.org/W2097473479', 'https://openalex.org/W1991274470', 'https://openalex.org/W2245493112', 'https://openalex.org/W2346964103', 'https://openalex.org/W2114347655', 'https://openalex.org/W2400549570', 'https://openalex.org/W2057007397', 'https://openalex.org/W2117041980', 'https://openalex.org/W2786608204', 'https://openalex.org/W2078769636', 'https://openalex.org/W1522301498', 'https://openalex.org/W2128780426', 'https://openalex.org/W2072396742', 'https://openalex.org/W1967924372', 'https://openalex.org/W1545920196', 'https://openalex.org/W2079460648', 'https://openalex.org/W148837159', 'https://openalex.org/W2399576818', 'https://openalex.org/W2384495648', 'https://openalex.org/W2345811097', 'https://openalex.org/W2100768664', 'https://openalex.org/W2400864884', 'https://openalex.org/W2170659185', 'https://openalex.org/W2104104263', 'https://openalex.org/W1957665339', 'https://openalex.org/W2345968833', 'https://openalex.org/W2036964623', 'https://openalex.org/W2132481658']",2017-08-25
https://openalex.org/W1967924372,https://doi.org/10.1109/icassp.2013.6639241,Weak top-down constraints for unsupervised acoustic model training,"Typical supervised acoustic model training relies on strong top-down constraints provided by dynamic programming alignment of the input observations to phonetic sequences derived from orthographic word transcripts and pronunciation dictionaries. This paper investigates a much weaker form of top-down supervision for use in place of transcripts and dictionaries in the zero resource setting. Our proposed constraints, which can be produced using recent spoken term discovery systems, come in the form of pairs of isolated word examples that share the same unknown type. For each pair, we perform a dynamic programming alignment of the acoustic observations of the two constituent examples, generating an inventory of cross-speaker frame pairs that each provide evidence that the same subword unit model should account for them. We find these weak top-down constraints are capable of improving model speaker independence by up to 57% relative over bottom-up training alone.","['https://openalex.org/W2128160875', 'https://openalex.org/W156237177', 'https://openalex.org/W2401464865', 'https://openalex.org/W6714100551', 'https://openalex.org/W6602705600', 'https://openalex.org/W2406820985', 'https://openalex.org/W2114478143', 'https://openalex.org/W6675022971', 'https://openalex.org/W2117041980', 'https://openalex.org/W2399869768', 'https://openalex.org/W3148201686', 'https://openalex.org/W2103933358', 'https://openalex.org/W2090861223', 'https://openalex.org/W1539935047', 'https://openalex.org/W2121947440', 'https://openalex.org/W1606268232', 'https://openalex.org/W2114347655', 'https://openalex.org/W2062914951', 'https://openalex.org/W2126203737', 'https://openalex.org/W2057007397', 'https://openalex.org/W30845872', 'https://openalex.org/W2079460648', 'https://openalex.org/W2407964052', 'https://openalex.org/W2100768664', 'https://openalex.org/W2407151108', 'https://openalex.org/W66167291']",2013-05-01
https://openalex.org/W2052697931,https://doi.org/10.1109/slt.2014.7078558,Phonetics embedding learning with side information,"We show that it is possible to learn an efficient acoustic model using only a<br>small amount of easily available word-level similarity annotations. In contrast<br>to the detailed phonetic labeling required by classical speech recognition<br>technologies, the only information our method requires are pairs of<br>speech excerpts which are known to be similar (same word) and pairs of<br>speech excerpts which are known to be different (different words). An acoustic model is obtained by training shallow and deep neural networks, using an<br>architecture and a cost function well-adapted to the nature of the provided information. The resulting model is evaluated on an ABX minimal-pair discrimination task and is shown to perform much better (11.8% ABX error<br>rate) than raw speech features (19.6%), not far from a fully supervised baseline (best neural network: 9.2%, HMM-GMM: 11%).","['https://openalex.org/W2162505970', 'https://openalex.org/W2035424729', 'https://openalex.org/W2143612262', 'https://openalex.org/W6670693114', 'https://openalex.org/W6908809', 'https://openalex.org/W2395899413', 'https://openalex.org/W2025482506', 'https://openalex.org/W6657703491', 'https://openalex.org/W6677328822', 'https://openalex.org/W30845872', 'https://openalex.org/W2252172689', 'https://openalex.org/W2054948443', 'https://openalex.org/W2136549906', 'https://openalex.org/W1993755070', 'https://openalex.org/W2114347655', 'https://openalex.org/W2153767712', 'https://openalex.org/W6602180557', 'https://openalex.org/W2078993594', 'https://openalex.org/W4285719527', 'https://openalex.org/W2171590421', 'https://openalex.org/W2117154949', 'https://openalex.org/W2407712691', 'https://openalex.org/W2029582325', 'https://openalex.org/W1576278180', 'https://openalex.org/W52412328', 'https://openalex.org/W2038056950', 'https://openalex.org/W2138621090', 'https://openalex.org/W2127589108', 'https://openalex.org/W2142152793', 'https://openalex.org/W1997460147']",2014-12-01
https://openalex.org/W1796128977,https://doi.org/10.21437/interspeech.2015-644,A comparison of neural network methods for unsupervised representation learning on the zero resource speech challenge,"The success of supervised deep neural networks (DNNs) in speech recognition cannot be transferred to zero-resource languages where the requisite transcriptions are unavailable. We investigate unsupervised neural network based methods for learning frame-level representations. Good frame representations eliminate differences in accent, gender, channel characteristics, and other factors to model subword units for withinand acrossspeaker phonetic discrimination. We enhance the correspondence autoencoder (cAE) and show that it can transform Mel Frequency Cepstral Coefficients (MFCCs) into more effective frame representations given a set of matched word pairs from an unsupervised term discovery (UTD) system. The cAE combines the feature extraction power of autoencoders with the weak supervision signal from UTD pairs to better approximate the extrinsic task’s objective during training. We use the Zero Resource Speech Challenge’s minimal triphone pair ABX discrimination task to evaluate our methods. Optimizing a cAE architecture on English and applying it to a zero-resource language, Xitsonga, we obtain a relative error rate reduction of 35% compared to the original MFCCs. We also show that Xitsonga frame representations extracted from the bottleneck layer of a supervised DNN trained on English can be further enhanced by the cAE, yielding a relative error rate reduction of 39%.","['https://openalex.org/W2044138293', 'https://openalex.org/W2107878631', 'https://openalex.org/W2112688413', 'https://openalex.org/W1606347560', 'https://openalex.org/W2002318609', 'https://openalex.org/W2786608204', 'https://openalex.org/W2057007397', 'https://openalex.org/W2026369565', 'https://openalex.org/W66167291', 'https://openalex.org/W2146502635', 'https://openalex.org/W2100495367', 'https://openalex.org/W2395899413', 'https://openalex.org/W2017257315', 'https://openalex.org/W2145094598', 'https://openalex.org/W2148154194', 'https://openalex.org/W2407151108', 'https://openalex.org/W2052697931', 'https://openalex.org/W3028642772', 'https://openalex.org/W2145410271', 'https://openalex.org/W2020607164', 'https://openalex.org/W4285719527', 'https://openalex.org/W2025768430', 'https://openalex.org/W1524333225', 'https://openalex.org/W1545920196', 'https://openalex.org/W2406349064']",2015-09-06
https://openalex.org/W2020607164,https://doi.org/10.1109/icassp.2014.6855085,An auto-encoder based approach to unsupervised learning of subword units,In this paper we propose an autoencoder-based method for the unsupervised identification of subword units. We experiment with different types and architectures of autoencoders to asses what autoencoder properties are most important for this task. We first show that the encoded representation of speech produced by standard autencoders is more effective than Gaussian posteriorgrams in a spoken query classification task. Finally we evaluate the subword inventories produced by the proposed method both in terms of classification accuracy in a word classification task (with lexicon size up to 263 words) and in terms of consistency between subword transcription of different word examples of a same word type. The evaluation is carried out on Italian and American English datasets. © 2014 IEEE.,"['https://openalex.org/W6789826613', 'https://openalex.org/W2100495367', 'https://openalex.org/W7011707065', 'https://openalex.org/W6676071220', 'https://openalex.org/W2035424729', 'https://openalex.org/W2117041980', 'https://openalex.org/W2123237149', 'https://openalex.org/W6677919164', 'https://openalex.org/W6675022971', 'https://openalex.org/W2167655920', 'https://openalex.org/W1964917299', 'https://openalex.org/W6681096077', 'https://openalex.org/W2099415988', 'https://openalex.org/W2401464865', 'https://openalex.org/W2107789863', 'https://openalex.org/W2100768664', 'https://openalex.org/W2619993508', 'https://openalex.org/W3127686677', 'https://openalex.org/W2145094598', 'https://openalex.org/W2118858186', 'https://openalex.org/W2072128103', 'https://openalex.org/W2997574889', 'https://openalex.org/W2010800472']",2014-05-01
https://openalex.org/W2826003142,https://doi.org/10.1109/icassp.2018.8461648,Speaker Invariant Feature Extraction for Zero-Resource Languages with Adversarial Learning,"We introduce a novel type of representation learning to obtain a speaker invariant feature for zero-resource languages. Speaker adaptation is an important technique to build a robust acoustic model. For a zero-resource language, however, conventional model-dependent speaker adaptation methods such as constrained maximum likelihood linear regression are insufficient because the acoustic model of the target language is not accessible. Therefore, we introduce a model-independent feature extraction based on a neural network. Specifically, we introduce a multi-task learning to a bottleneck feature-based approach to make bottleneck feature invariant to a change of speakers. The proposed network simultaneously tackles two tasks: phoneme and speaker classifications. This network trains a feature extractor in an adversarial manner to allow it to map input data into a discriminative representation to predict phonemes, whereas it is difficult to predict speakers. We conduct phone discriminant experiments in Zero Resource Speech Challenge 2017. Experimental results showed that our multi-task network yielded more discriminative features eliminating the variety in speakers.","['https://openalex.org/W6638159135', 'https://openalex.org/W6640425456', 'https://openalex.org/W6637618735', 'https://openalex.org/W2510867321', 'https://openalex.org/W6605273041', 'https://openalex.org/W6686687092', 'https://openalex.org/W2078169166', 'https://openalex.org/W6777926273', 'https://openalex.org/W2119187236', 'https://openalex.org/W6675022971', 'https://openalex.org/W6712553779', 'https://openalex.org/W6602705600', 'https://openalex.org/W6712202099', 'https://openalex.org/W2404799143', 'https://openalex.org/W2126203737', 'https://openalex.org/W2002342963', 'https://openalex.org/W6661255737', 'https://openalex.org/W6640036494', 'https://openalex.org/W6712444837', 'https://openalex.org/W6638667902', 'https://openalex.org/W6725564769', 'https://openalex.org/W2730658205', 'https://openalex.org/W2150769028', 'https://openalex.org/W2064675550', 'https://openalex.org/W66167291', 'https://openalex.org/W1836465849', 'https://openalex.org/W128628490', 'https://openalex.org/W2396043527', 'https://openalex.org/W2509930204', 'https://openalex.org/W2100768664', 'https://openalex.org/W2963207607', 'https://openalex.org/W1796128977', 'https://openalex.org/W2043878967', 'https://openalex.org/W1945616565', 'https://openalex.org/W2399576818', 'https://openalex.org/W2949117887', 'https://openalex.org/W2185814970', 'https://openalex.org/W3028642772', 'https://openalex.org/W1731081199', 'https://openalex.org/W2395899413', 'https://openalex.org/W1904365287']",2018-04-01
https://openalex.org/W1510007267,https://doi.org/10.1007/978-94-011-5730-8,An Introduction to Text-to-Speech Synthesis,,[],1997-01-01
https://openalex.org/W2134202996,https://doi.org/10.1109/icassp.2014.6854069,Automatic discovery of a phonetic inventory for unwritten languages for statistical speech synthesis,"Speech synthesis systems are typically built with speech data and transcriptions. In this paper, we try to build synthesis systems when no transcriptions or knowledge about the language are available. It is usually necessary to at least possess phonetic knowledge about the language. In this paper, we propose an automated way of obtaining phones and phonetic knowledge about the corpus at hand by making use of Articulatory Features (AFs). An Articulatory Feature predictor is trained on a bootstrap corpus in an arbitrary other language using a three-hidden layer neural network. This neural network is run on the speech corpus to extract AFs. Hierarchical clustering is used to cluster the AFs into categories i.e. phones. Phonetic information about each of these inferred phones is obtained by computing the mean of the AFs in each cluster. Results of systems built with this framework in multiple languages are reported.","['https://openalex.org/W2404169761', 'https://openalex.org/W6603838645', 'https://openalex.org/W6608197479', 'https://openalex.org/W2150612204', 'https://openalex.org/W6697285287', 'https://openalex.org/W1986174057', 'https://openalex.org/W6730233244', 'https://openalex.org/W2116648050', 'https://openalex.org/W1981457580', 'https://openalex.org/W1966264494', 'https://openalex.org/W2401299519', 'https://openalex.org/W6602682705', 'https://openalex.org/W2128446656', 'https://openalex.org/W1490506669', 'https://openalex.org/W2296704011', 'https://openalex.org/W202879582', 'https://openalex.org/W2397987315', 'https://openalex.org/W66627554', 'https://openalex.org/W1560013842', 'https://openalex.org/W4389138872', 'https://openalex.org/W95152782', 'https://openalex.org/W2551677481']",2014-05-01
https://openalex.org/W2962699523,https://doi.org/10.1109/asru.2017.8268950,Listening while speaking: Speech chain by deep learning,"Despite the close relationship between speech perception and production, research in automatic speech recognition (ASR) and text-to-speech synthesis (TTS) has progressed more or less independently without exerting much mutual influence on each other. In human communication, on the other hand, a closed-loop speech chain mechanism with auditory feedback from the speaker's mouth to her ear is crucial. In this paper, we take a step further and develop a closed-loop speech chain model based on deep learning. The sequence-to-sequence model in close-loop architecture allows us to train our model on the concatenation of both labeled and unlabeled data. While ASR transcribes the unlabeled speech features, TTS attempts to reconstruct the original speech waveform based on the text from ASR. In the opposite direction, ASR also attempts to reconstruct the original text transcription given the synthesized speech. To the best of our knowledge, this is the first deep learning model that integrates human speech perception and production behaviors. Our experimental results show that the proposed approach significantly improved the performance more than separate systems that were only trained with labeled data.","['https://openalex.org/W6696126599', 'https://openalex.org/W6608432165', 'https://openalex.org/W2042196737', 'https://openalex.org/W1608375737', 'https://openalex.org/W2150658333', 'https://openalex.org/W1600722501', 'https://openalex.org/W1935012542', 'https://openalex.org/W2143612262', 'https://openalex.org/W2168249605', 'https://openalex.org/W6712560600', 'https://openalex.org/W6675380101', 'https://openalex.org/W2128160875', 'https://openalex.org/W6670693114', 'https://openalex.org/W2101346879', 'https://openalex.org/W2120847449', 'https://openalex.org/W1990005915', 'https://openalex.org/W1861172732', 'https://openalex.org/W2133129980', 'https://openalex.org/W2063223471', 'https://openalex.org/W2115040572', 'https://openalex.org/W6679436768', 'https://openalex.org/W2327501763', 'https://openalex.org/W6630875275', 'https://openalex.org/W2546938941', 'https://openalex.org/W2102003408', 'https://openalex.org/W1514535095', 'https://openalex.org/W2130942839', 'https://openalex.org/W2949382160', 'https://openalex.org/W2398826216', 'https://openalex.org/W2963357083', 'https://openalex.org/W1921523184', 'https://openalex.org/W2157331557', 'https://openalex.org/W1810943226', 'https://openalex.org/W2078993594', 'https://openalex.org/W3210141620', 'https://openalex.org/W206967138', 'https://openalex.org/W2604184139', 'https://openalex.org/W2422843715', 'https://openalex.org/W2964308564', 'https://openalex.org/W1586532344', 'https://openalex.org/W2519091744', 'https://openalex.org/W2591927543', 'https://openalex.org/W2133564696', 'https://openalex.org/W1902237438']",2017-12-01
https://openalex.org/W2532494225,https://doi.org/10.1109/apsipa.2016.7820786,Voice conversion from non-parallel corpora using variational auto-encoder,"We propose a flexible framework for spectral conversion (SC) that facilitates training with unaligned corpora. Many SC frameworks require parallel corpora, phonetic alignments, or explicit frame-wise correspondence for learning conversion functions or for synthesizing a target spectrum with the aid of alignments. However, these requirements gravely limit the scope of practical applications of SC due to scarcity or even unavailability of parallel corpora. We propose an SC framework based on variational auto-encoder which enables us to exploit non-parallel corpora. The framework comprises an encoder that learns speaker-independent phonetic representations and a decoder that learns to reconstruct the designated speaker. It removes the requirement of parallel corpora or phonetic alignments to train a spectral conversion system. We report objective and subjective evaluations to validate our proposed method and compare it to SC methods that have access to aligned corpora.","['https://openalex.org/W2512087624', 'https://openalex.org/W2290946177', 'https://openalex.org/W2169579015', 'https://openalex.org/W2067175291', 'https://openalex.org/W2161476805', 'https://openalex.org/W2407281753', 'https://openalex.org/W6631184489', 'https://openalex.org/W6675944832', 'https://openalex.org/W6640963894', 'https://openalex.org/W2121387787', 'https://openalex.org/W2019849101', 'https://openalex.org/W2157412983', 'https://openalex.org/W2156477760', 'https://openalex.org/W2290463584', 'https://openalex.org/W6696767757', 'https://openalex.org/W1517202054', 'https://openalex.org/W2120605154', 'https://openalex.org/W1977362459', 'https://openalex.org/W2473388484', 'https://openalex.org/W2049686551', 'https://openalex.org/W6631190155', 'https://openalex.org/W6637242042', 'https://openalex.org/W1665214252', 'https://openalex.org/W2108501770', 'https://openalex.org/W2032130465', 'https://openalex.org/W2964121744', 'https://openalex.org/W2294351487', 'https://openalex.org/W1520370180', 'https://openalex.org/W2949416428', 'https://openalex.org/W1522301498', 'https://openalex.org/W1959608418']",2016-12-01
https://openalex.org/W2963830550,https://doi.org/10.21437/interspeech.2018-1830,Multi-target Voice Conversion without Parallel Data by Adversarially Learning Disentangled Audio Representations,"Recently, cycle-consistent adversarial network (Cycle-GAN) has been successfully applied to voice conversion to a different speaker without parallel data, although in those approaches an individual model is needed for each target speaker.In this paper, we propose an adversarial learning framework for voice conversion, with which a single model can be trained to convert the voice to many different speakers, all without parallel data, by separating the speaker characteristics from the linguistic content in speech signals.An autoencoder is first trained to extract speaker-independent latent representations and speaker embedding separately using another auxiliary speaker classifier to regularize the latent representation.The decoder then takes the speaker-independent latent representation and the target speaker embedding as the input to generate the voice of the target speaker with the linguistic content of the source utterance.The quality of decoder output is further improved by patching with the residual signal produced by another pair of generator and discriminator.A target speaker set size of 20 was tested in the preliminary experiments, and very good voice quality was obtained.Conventional voice conversion metrics are reported.We also show that the speaker information has been properly reduced from the latent representations.","['https://openalex.org/W2502312327', 'https://openalex.org/W2962974898', 'https://openalex.org/W2964135678', 'https://openalex.org/W2950776302', 'https://openalex.org/W2476548250', 'https://openalex.org/W2621350877', 'https://openalex.org/W2120605154', 'https://openalex.org/W2157412983', 'https://openalex.org/W2758785877', 'https://openalex.org/W1523372075', 'https://openalex.org/W2962896155', 'https://openalex.org/W4295521014', 'https://openalex.org/W2396025094', 'https://openalex.org/W2127520494', 'https://openalex.org/W2532494225', 'https://openalex.org/W4298426053', 'https://openalex.org/W2547364378', 'https://openalex.org/W1509691205', 'https://openalex.org/W1959608418', 'https://openalex.org/W4320013936', 'https://openalex.org/W2963073614', 'https://openalex.org/W2057609679', 'https://openalex.org/W2156142001', 'https://openalex.org/W2086796102', 'https://openalex.org/W2963808252', 'https://openalex.org/W2056852181', 'https://openalex.org/W2148846882', 'https://openalex.org/W2651834199', 'https://openalex.org/W2608207374', 'https://openalex.org/W2774848319', 'https://openalex.org/W1987992317', 'https://openalex.org/W2105160541', 'https://openalex.org/W2619034550']",2018-08-28
https://openalex.org/W2902070858,https://doi.org/10.23919/eusipco.2018.8553236,CycleGAN-VC: Non-parallel Voice Conversion Using Cycle-Consistent Adversarial Networks,"We propose a non-parallel voice-conversion (VC) method that can learn a mapping from source to target speech without relying on parallel data. The proposed method is particularly noteworthy in that it is general purpose and high quality and works without any extra data, modules, or alignment procedure. Our method, called CycleGAN-VC, uses a cycle-consistent adversarial network (CycleGAN) with gated convolutional neural networks (CNNs) and an identity-mapping loss. A CycleGAN learns forward and inverse mappings simultaneously using adversarial and cycle-consistency losses. This makes it possible to find an optimal pseudo pair from non-parallel data. Furthermore, the adversarial loss can bring the converted speech close to the target one on the basis of indistinguishability without explicit density estimation. This allows to avoid over-smoothing caused by statistical averaging, which occurs in many conventional statistical model-based VC methods that represent data distribution explicitly. We configure a CycleGAN with gated CNNs and train it with an identity-mapping loss. This allows the mapping function to capture sequential and hierarchical structures while preserving linguistic information. We evaluated our method on a non-parallel VC task. An objective evaluation showed that the converted feature sequence was near natural in terms of global variance and modulation spectra, which are structural indicators highly correlated with subjective evaluation. A subjective evaluation showed that the quality of the converted speech was comparable to that obtained with a Gaussian mixture model-based parallel VC method even though CycleGAN-VC is trained under disadvantageous conditions (non-parallel and half the amount of data).","['https://openalex.org/W10800834', 'https://openalex.org/W6635216677', 'https://openalex.org/W1987992317', 'https://openalex.org/W2473388484', 'https://openalex.org/W2658996865', 'https://openalex.org/W2746654391', 'https://openalex.org/W6602007935', 'https://openalex.org/W2153057929', 'https://openalex.org/W2518312472', 'https://openalex.org/W6684957941', 'https://openalex.org/W2474531669', 'https://openalex.org/W2666408839', 'https://openalex.org/W2123003832', 'https://openalex.org/W1977362459', 'https://openalex.org/W2962793481', 'https://openalex.org/W142801778', 'https://openalex.org/W2963444790', 'https://openalex.org/W6735204497', 'https://openalex.org/W6730095352', 'https://openalex.org/W6731370813', 'https://openalex.org/W2194775991', 'https://openalex.org/W2515028311', 'https://openalex.org/W6640185926', 'https://openalex.org/W6637242042', 'https://openalex.org/W6631190155', 'https://openalex.org/W2593414223', 'https://openalex.org/W2120605154', 'https://openalex.org/W2105160541', 'https://openalex.org/W2475998840', 'https://openalex.org/W2086796102', 'https://openalex.org/W2127520494', 'https://openalex.org/W2157412983', 'https://openalex.org/W2057609679', 'https://openalex.org/W2787748320', 'https://openalex.org/W6711854987', 'https://openalex.org/W1509691205', 'https://openalex.org/W1974745215', 'https://openalex.org/W2022125261', 'https://openalex.org/W2142300631', 'https://openalex.org/W2148846882', 'https://openalex.org/W2056852181', 'https://openalex.org/W2747744257', 'https://openalex.org/W2161727827', 'https://openalex.org/W6702130928', 'https://openalex.org/W2156142001', 'https://openalex.org/W2123771434', 'https://openalex.org/W2471520273', 'https://openalex.org/W2476548250', 'https://openalex.org/W6631309588', 'https://openalex.org/W2651834199', 'https://openalex.org/W2532494225', 'https://openalex.org/W1903029394', 'https://openalex.org/W2962896155', 'https://openalex.org/W1523372075', 'https://openalex.org/W2963970792', 'https://openalex.org/W2586453598', 'https://openalex.org/W2951939904', 'https://openalex.org/W2331128040', 'https://openalex.org/W4293398859', 'https://openalex.org/W2774848319', 'https://openalex.org/W2964121744', 'https://openalex.org/W2396025094', 'https://openalex.org/W2169579015', 'https://openalex.org/W2964341837', 'https://openalex.org/W1522301498', 'https://openalex.org/W1665214252', 'https://openalex.org/W1588266896', 'https://openalex.org/W2099471712', 'https://openalex.org/W2598581049', 'https://openalex.org/W4320013936', 'https://openalex.org/W2567070169', 'https://openalex.org/W1921523184', 'https://openalex.org/W49412823', 'https://openalex.org/W2502312327']",2018-09-01
https://openalex.org/W2747192917,https://doi.org/10.21437/interspeech.2017-1476,Unsupervised Speech Signal to Symbol Transformation for Zero Resource Speech Applications,"Zero resource speech processing refers to a scenario where no or minimal transcribed data is available. In this paper, we propose a three-step unsupervised approach to zero resource speech processing, which does not require any other information/dataset. In the first step, we segment the speech signal into phonemelike units, resulting in a large number of varying length segments. The second step involves clustering the varying-length segments into a finite number of clusters so that each segment can be labeled with a cluster index. The unsupervised transcriptions, thus obtained, can be thought of as a sequence of virtual phone labels. In the third step, a deep neural network classifier is trained to map the feature vectors extracted from the signal to its corresponding virtual phone label. The virtual phone posteriors extracted from the DNN are used as features in the zero resource speech processing. The effectiveness of the proposed approach is evaluated on both ABX and spoken term discovery tasks (STD) using spontaneous American English and Tsonga language datasets, provided as part of zero resource 2015 challenge. It is observed that the proposed system outperforms baselines, supplied along the datasets, in both the tasks without any task specific modifications.","['https://openalex.org/W2406349064', 'https://openalex.org/W2140991203', 'https://openalex.org/W2398490608', 'https://openalex.org/W2126203737', 'https://openalex.org/W2346964103', 'https://openalex.org/W2407614114', 'https://openalex.org/W2020607164', 'https://openalex.org/W1673310716', 'https://openalex.org/W2025543856', 'https://openalex.org/W2251025892', 'https://openalex.org/W2100308344', 'https://openalex.org/W2402366697', 'https://openalex.org/W2079460648', 'https://openalex.org/W2400549570', 'https://openalex.org/W2100768664', 'https://openalex.org/W2121997342', 'https://openalex.org/W2017257315', 'https://openalex.org/W2078769636', 'https://openalex.org/W2395899413', 'https://openalex.org/W2170659185', 'https://openalex.org/W2117041980', 'https://openalex.org/W2160815625', 'https://openalex.org/W1796128977', 'https://openalex.org/W2399576818']",2017-08-16
https://openalex.org/W2803005441,https://doi.org/10.1007/978-981-13-0020-2_13,Unsupervised Segmentation of Speech Signals Using Kernel-Gram Matrices,,"['https://openalex.org/W2125838338', 'https://openalex.org/W2428180336', 'https://openalex.org/W6604987197', 'https://openalex.org/W2165698076', 'https://openalex.org/W2045036776', 'https://openalex.org/W2054665642', 'https://openalex.org/W1555009096', 'https://openalex.org/W1589615572', 'https://openalex.org/W2096270777', 'https://openalex.org/W2104908186', 'https://openalex.org/W2171752983', 'https://openalex.org/W2098363562', 'https://openalex.org/W2115307058', 'https://openalex.org/W2083904075', 'https://openalex.org/W2114347655', 'https://openalex.org/W16500752', 'https://openalex.org/W146439846', 'https://openalex.org/W175164482', 'https://openalex.org/W2121997342', 'https://openalex.org/W1523336921', 'https://openalex.org/W2586482422', 'https://openalex.org/W2084584260', 'https://openalex.org/W4244165801', 'https://openalex.org/W1673310716', 'https://openalex.org/W1635512741', 'https://openalex.org/W2786608204', 'https://openalex.org/W2057007397', 'https://openalex.org/W2398490608', 'https://openalex.org/W2407614114', 'https://openalex.org/W2037298640', 'https://openalex.org/W2100768664', 'https://openalex.org/W1558402681', 'https://openalex.org/W4299681030', 'https://openalex.org/W2907162034', 'https://openalex.org/W2063678701', 'https://openalex.org/W2619993508', 'https://openalex.org/W2100233488', 'https://openalex.org/W4285719527', 'https://openalex.org/W3021908862', 'https://openalex.org/W3144621507']",2018-01-01
https://openalex.org/W2890718354,https://doi.org/10.1109/icassp.2018.8462264,Phoneme Based Embedded Segmental K-Means for Unsupervised Term Discovery,"Identifying and grouping the frequently occurring word-like patterns from raw acoustic waveforms is an important task in the zero resource speech processing. Embedded segmental K-means (ES-KMeans) discovers both the word boundaries and the word types from raw data. Starting from an initial set of subword boundaries, the ES-Kmeans iteratively eliminates some of the boundaries to arrive at frequently occurring longer word patterns. Notice that the initial word boundaries will not be adjusted during the process. As a result, the performance of the ES-Kmeans critically depends on the initial subword boundaries. Originally, syllable boundaries were used to initialize ES-Kmeans. In this paper, we propose to use a phoneme segmentation method that produces boundaries closer to true boundaries for ES-KMeans initialization. The use of shorter units increases the number of initial boundaries which leads to a significant increment in the computational complexity. To reduce the computational cost, we extract compact lower dimensional embeddings from an auto-encoder. The proposed algorithm is benchmarked on Zero Resource 2017 challenge, which consists of 70 hours of unlabeled data across three languages, viz. English, French, and Mandarin. The proposed algorithm outperforms the baseline system without any language-specific parameter tuning.","['https://openalex.org/W6638159135', 'https://openalex.org/W6675022971', 'https://openalex.org/W2117041980', 'https://openalex.org/W2078769636', 'https://openalex.org/W2106284094', 'https://openalex.org/W2114347655', 'https://openalex.org/W2057007397', 'https://openalex.org/W2032943813', 'https://openalex.org/W1778492285', 'https://openalex.org/W6691362072', 'https://openalex.org/W2154093685', 'https://openalex.org/W4244165801', 'https://openalex.org/W2020944885', 'https://openalex.org/W2516890051', 'https://openalex.org/W2163922914', 'https://openalex.org/W2020607164', 'https://openalex.org/W6602092770', 'https://openalex.org/W2747192917', 'https://openalex.org/W2346964103', 'https://openalex.org/W2170659185', 'https://openalex.org/W6973666849', 'https://openalex.org/W2295297373', 'https://openalex.org/W6751832535', 'https://openalex.org/W2398490608', 'https://openalex.org/W2059652594', 'https://openalex.org/W1577418252', 'https://openalex.org/W6644682428', 'https://openalex.org/W2190506272', 'https://openalex.org/W1977556410', 'https://openalex.org/W2786608204', 'https://openalex.org/W2100768664', 'https://openalex.org/W51277926', 'https://openalex.org/W2747943889', 'https://openalex.org/W2803005441', 'https://openalex.org/W2964169922', 'https://openalex.org/W1796128977', 'https://openalex.org/W2251025892']",2018-04-01
https://openalex.org/W2598638573,https://doi.org/10.21437/ssw.2016-33,Merlin: An Open Source Neural Network Speech Synthesis System,"We introduce the Merlin speech synthesis toolkit for neural network-based speech synthesis.The system takes linguistic features as input, and employs neural networks to predict acoustic features, which are then passed to a vocoder to produce the speech waveform.Various neural network architectures are implemented, including a standard feedforward neural network, mixture density neural network, recurrent neural network (RNN), long short-term memory (LSTM) recurrent neural network, amongst others.The toolkit is Open Source, written in Python, and is extensible.This paper briefly describes the system, and provides some benchmarking results on a freelyavailable corpus.","['https://openalex.org/W2402539796', 'https://openalex.org/W2100167690', 'https://openalex.org/W2129142580', 'https://openalex.org/W2399853303', 'https://openalex.org/W1924770834', 'https://openalex.org/W2406990556', 'https://openalex.org/W2049686551', 'https://openalex.org/W1990505856', 'https://openalex.org/W2402103843', 'https://openalex.org/W2045158511', 'https://openalex.org/W2111284386', 'https://openalex.org/W2134973740', 'https://openalex.org/W1888924187', 'https://openalex.org/W1571950845', 'https://openalex.org/W2397670047', 'https://openalex.org/W2102003408', 'https://openalex.org/W3123963976', 'https://openalex.org/W1613141907', 'https://openalex.org/W133102907', 'https://openalex.org/W2964060510', 'https://openalex.org/W1576227399', 'https://openalex.org/W2294797155', 'https://openalex.org/W2806733747', 'https://openalex.org/W2079735306', 'https://openalex.org/W1499332833', 'https://openalex.org/W2239904444', 'https://openalex.org/W1502723613', 'https://openalex.org/W1120805016', 'https://openalex.org/W2020024436', 'https://openalex.org/W2170980774', 'https://openalex.org/W31154030', 'https://openalex.org/W2395700867', 'https://openalex.org/W1544516254', 'https://openalex.org/W2404881427', 'https://openalex.org/W2043003570', 'https://openalex.org/W2471520273', 'https://openalex.org/W1543299179', 'https://openalex.org/W2136374105']",2016-09-13
https://openalex.org/W2059652594,https://doi.org/10.1109/asru.2013.6707765,Fixed-dimensional acoustic embeddings of variable-length segments in low-resource settings,"Measures of acoustic similarity between words or other units are critical for segmental exemplar-based acoustic models, spoken term discovery, and query-by-example search. Dynamic time warping (DTW) alignment cost has been the most commonly used measure, but it has well-known inadequacies. Some recently proposed alternatives require large amounts of training data. In the interest of finding more efficient, accurate, and low-resource alternatives, we consider the problem of embedding speech segments of arbitrary length into fixed-dimensional spaces in which simple distances (such as cosine or Euclidean) serve as a proxy for linguistically meaningful (phonetic, lexical, etc.) dissimilarities. Such embeddings would enable efficient audio indexing and permit application of standard distance learning techniques to segmental acoustic modeling. In this paper, we explore several supervised and unsupervised approaches to this problem and evaluate them on an acoustic word discrimination task. We identify several embedding algorithms that match or improve upon the DTW baseline in low-resource settings.","['https://openalex.org/W1579848672', 'https://openalex.org/W2170580867', 'https://openalex.org/W137541753', 'https://openalex.org/W2395342389', 'https://openalex.org/W1974540032', 'https://openalex.org/W2111732304', 'https://openalex.org/W2126203737', 'https://openalex.org/W2799046698', 'https://openalex.org/W2110889574', 'https://openalex.org/W2097527229', 'https://openalex.org/W2147717514', 'https://openalex.org/W1583581687', 'https://openalex.org/W6683338658', 'https://openalex.org/W2053186076', 'https://openalex.org/W6683161245', 'https://openalex.org/W2097308346', 'https://openalex.org/W6675747103', 'https://openalex.org/W1984857732', 'https://openalex.org/W2097207027', 'https://openalex.org/W2023952145', 'https://openalex.org/W2407964052', 'https://openalex.org/W1978741356', 'https://openalex.org/W6634500723', 'https://openalex.org/W2114347655', 'https://openalex.org/W2397535009', 'https://openalex.org/W6714100551', 'https://openalex.org/W3148981562', 'https://openalex.org/W2117459613', 'https://openalex.org/W2057007397', 'https://openalex.org/W30845872', 'https://openalex.org/W2997701990', 'https://openalex.org/W2569190383', 'https://openalex.org/W2407151108', 'https://openalex.org/W2158139921', 'https://openalex.org/W2104290444', 'https://openalex.org/W2025482506', 'https://openalex.org/W2157444450', 'https://openalex.org/W2151660570', 'https://openalex.org/W1576613474', 'https://openalex.org/W2164365067']",2013-12-01
https://openalex.org/W1975728937,https://doi.org/10.1109/taslp.2014.2387382,Acoustic Segment Modeling with Spectral Clustering Methods,"This paper presents a study of spectral clustering-based approaches to acoustic segment modeling (ASM). ASM aims at finding the underlying phoneme-like speech units and building the corresponding acoustic models in the unsupervised setting, where no prior linguistic knowledge and manual transcriptions are available. A typical ASM process involves three stages, namely initial segmentation, segment labeling, and iterative modeling. This work focuses on the improvement of segment labeling. Specifically, we use posterior features as the segment representations, and apply spectral clustering algorithms on the posterior representations. We propose a Gaussian component clustering (GCC) approach and a segment clustering (SC) approach. GCC applies spectral clustering on a set of Gaussian components, and SC applies spectral clustering on a large number of speech segments. Moreover, to exploit the complementary information of different posterior representations, a multiview segment clustering (MSC) approach is proposed. MSC simultaneously utilizes multiple posterior representations to cluster speech segments. To address the computational problem of spectral clustering in dealing with large numbers of speech segments, we use inner product similarity graph and make reformulations to avoid the explicit computation of the affinity matrix and Laplacian matrix. We carried out two sets of experiments for evaluation. First, we evaluated the ASM accuracy on the OGI-MTS dataset, and it was shown that our approach could yield 18.7% relative purity improvement and 15.1% relative NMI improvement compared with the baseline approach. Second, we examined the performances of our approaches in the real application of zero-resource query-by-example spoken term detection on SWS2012 dataset, and it was shown that our approaches could provide consistent improvement on four different testing scenarios with three evaluation metrics.","['https://openalex.org/W2154085905', 'https://openalex.org/W2171019095', 'https://openalex.org/W2126203737', 'https://openalex.org/W1991727948', 'https://openalex.org/W2103611452', 'https://openalex.org/W2102708851', 'https://openalex.org/W177307080', 'https://openalex.org/W1635512741', 'https://openalex.org/W1983042831', 'https://openalex.org/W2170659185', 'https://openalex.org/W107837170', 'https://openalex.org/W2405703722', 'https://openalex.org/W2113387946', 'https://openalex.org/W2402963799', 'https://openalex.org/W2167655920', 'https://openalex.org/W6682991666', 'https://openalex.org/W2401464865', 'https://openalex.org/W6675134712', 'https://openalex.org/W2405459681', 'https://openalex.org/W6685161094', 'https://openalex.org/W2056786202', 'https://openalex.org/W1975113979', 'https://openalex.org/W308497914', 'https://openalex.org/W2167845555', 'https://openalex.org/W1964917299', 'https://openalex.org/W2101536075', 'https://openalex.org/W2110589736', 'https://openalex.org/W2116422968', 'https://openalex.org/W134006180', 'https://openalex.org/W1974477236', 'https://openalex.org/W4250589301', 'https://openalex.org/W2094721296', 'https://openalex.org/W6629386585', 'https://openalex.org/W2799046698', 'https://openalex.org/W2104663520', 'https://openalex.org/W4213009331', 'https://openalex.org/W6606613475', 'https://openalex.org/W2106284094', 'https://openalex.org/W2026704977', 'https://openalex.org/W2171752983', 'https://openalex.org/W6677803786', 'https://openalex.org/W2026858810', 'https://openalex.org/W2083904075', 'https://openalex.org/W2403015869', 'https://openalex.org/W2399869768', 'https://openalex.org/W2009341345', 'https://openalex.org/W2099415988', 'https://openalex.org/W2055408826', 'https://openalex.org/W2103933358', 'https://openalex.org/W1967924372', 'https://openalex.org/W6675022971', 'https://openalex.org/W2025482506', 'https://openalex.org/W2078769636', 'https://openalex.org/W2166782149', 'https://openalex.org/W6640828828', 'https://openalex.org/W6684578312', 'https://openalex.org/W6677844713', 'https://openalex.org/W2132914434', 'https://openalex.org/W2090764203', 'https://openalex.org/W1578200545', 'https://openalex.org/W2147152072', 'https://openalex.org/W88081813', 'https://openalex.org/W185189022', 'https://openalex.org/W2119388857', 'https://openalex.org/W2101324110', 'https://openalex.org/W2154415691', 'https://openalex.org/W2121997342', 'https://openalex.org/W2170353620', 'https://openalex.org/W2165874743', 'https://openalex.org/W2187089797', 'https://openalex.org/W2296319761', 'https://openalex.org/W1957665339', 'https://openalex.org/W161183367', 'https://openalex.org/W1490960657', 'https://openalex.org/W2100768664', 'https://openalex.org/W2402937204']",2015-01-05
https://openalex.org/W2964169922,,,"Unsupervised segmentation and clustering of unlabelled speech are core problems in zero-resource speech processing. Most approaches lie at methodological extremes: some use probabilistic Bayesian models with convergence guarantees, while others opt for more efficient heuristic techniques. Despite competitive performance in previous work, the full Bayesian approach is difficult to scale to large speech corpora. We introduce an approximation to a recent Bayesian model that still has a clear objective function but improves efficiency by using hard clustering and segmentation rather than full Bayesian inference. Like its Bayesian counterpart, this embedded segmental K-means model (ES-KMeans) represents arbitrary-length word segments as fixed-dimensional acoustic word embeddings. We first compare ES-KMeans to previous approaches on common English and Xitsonga data sets (5 and 2.5 hours of speech): ES-KMeans outperforms a leading heuristic method in word segmentation, giving similar scores to the Bayesian model while being 5 times faster with fewer hyperparameters. However, its clusters are less pure than those of the other models. We then show that ES-KMeans scales to larger corpora by applying it to the 5 languages of the Zero Resource Speech Challenge 2017 (up to 45 hours), where it performs competitively compared to the challenge baseline.","['https://openalex.org/W2048648518', 'https://openalex.org/W2758697525', 'https://openalex.org/W3102667484', 'https://openalex.org/W2951216052', 'https://openalex.org/W2117041980', 'https://openalex.org/W2114347655', 'https://openalex.org/W2962736743', 'https://openalex.org/W2463237750', 'https://openalex.org/W2686360660', 'https://openalex.org/W2719865699', 'https://openalex.org/W2641832364', 'https://openalex.org/W1796128977', 'https://openalex.org/W2291770225', 'https://openalex.org/W2295297373', 'https://openalex.org/W2020607164', 'https://openalex.org/W2346964103', 'https://openalex.org/W2963311389', 'https://openalex.org/W2161562001', 'https://openalex.org/W2116330964', 'https://openalex.org/W2190506272', 'https://openalex.org/W113159538', 'https://openalex.org/W2962980711', 'https://openalex.org/W2032943813', 'https://openalex.org/W3098643042', 'https://openalex.org/W2963571336', 'https://openalex.org/W2483390977', 'https://openalex.org/W2251025892', 'https://openalex.org/W2154093685', 'https://openalex.org/W1577418252', 'https://openalex.org/W2566587499', 'https://openalex.org/W1778492285', 'https://openalex.org/W2072396742', 'https://openalex.org/W2143776582', 'https://openalex.org/W2022058071', 'https://openalex.org/W2468716020', 'https://openalex.org/W2059652594', 'https://openalex.org/W2100768664', 'https://openalex.org/W51277926', 'https://openalex.org/W2091746061', 'https://openalex.org/W2398490608', 'https://openalex.org/W2142775654', 'https://openalex.org/W1590183771', 'https://openalex.org/W2516890051', 'https://openalex.org/W2010188467', 'https://openalex.org/W2057007397', 'https://openalex.org/W2786608204']",
https://openalex.org/W2471520273,https://doi.org/10.1587/transinf.2015edp7457,WORLD: A Vocoder-Based High-Quality Speech Synthesis System for Real-Time Applications,"A vocoder-based speech synthesis system, named WORLD, was developed in an effort to improve the sound quality of real-time applications using speech. Speech analysis, manipulation, and synthesis on the basis of vocoders are used in various kinds of speech research. Although several high-quality speech synthesis systems have been developed, real-time processing has been difficult with them because of their high computational costs. This new speech synthesis system has not only sound quality but also quick processing. It consists of three analysis algorithms and one synthesis algorithm proposed in our previous research. The effectiveness of the system was evaluated by comparing its output with against natural speech including consonants. Its processing speed was also compared with those of conventional systems. The results showed that WORLD was superior to the other systems in terms of both sound quality and processing speed. In particular, it was over ten times faster than the conventional systems, and the real time factor (RTF) indicated that it was fast enough for real-time processing.","['https://openalex.org/W2142747467', 'https://openalex.org/W2082321072', 'https://openalex.org/W4245284779', 'https://openalex.org/W1572730534', 'https://openalex.org/W2049686551', 'https://openalex.org/W4235716345', 'https://openalex.org/W2428180336', 'https://openalex.org/W2164764235', 'https://openalex.org/W125998882', 'https://openalex.org/W2077446647', 'https://openalex.org/W2130081501', 'https://openalex.org/W2131062138', 'https://openalex.org/W2030351702', 'https://openalex.org/W1563089615', 'https://openalex.org/W2012086895', 'https://openalex.org/W2094130157', 'https://openalex.org/W773905565', 'https://openalex.org/W86348706', 'https://openalex.org/W2064948657', 'https://openalex.org/W2091425152', 'https://openalex.org/W1975079546', 'https://openalex.org/W2097645910', 'https://openalex.org/W2579537926', 'https://openalex.org/W2018458134', 'https://openalex.org/W2109189270', 'https://openalex.org/W1498609987', 'https://openalex.org/W2052002522', 'https://openalex.org/W2404204166', 'https://openalex.org/W1926768285', 'https://openalex.org/W198589515', 'https://openalex.org/W2032030908', 'https://openalex.org/W2150619194', 'https://openalex.org/W1563460361', 'https://openalex.org/W2122627441', 'https://openalex.org/W2129142580', 'https://openalex.org/W2556247670']",2016-01-01
https://openalex.org/W2940544976,https://doi.org/10.21437/interspeech.2019-2904,The Zero Resource Speech Challenge 2019: TTS Without T,"We present the Zero Resource Speech Challenge 2019, which proposes to build a\nspeech synthesizer without any text or phonetic labels: hence, TTS without T\n(text-to-speech without text). We provide raw audio for a target voice in an\nunknown language (the Voice dataset), but no alignment, text or labels.\nParticipants must discover subword units in an unsupervised way (using the Unit\nDiscovery dataset) and align them to the voice recordings in a way that works\nbest for the purpose of synthesizing novel utterances from novel speakers,\nsimilar to the target speaker's voice. We describe the metrics used for\nevaluation, a baseline system consisting of unsupervised subword unit discovery\nplus a standard TTS system, and a topline TTS using gold phoneme\ntranscriptions. We present an overview of the 19 submitted systems from 10\nteams and discuss the main results.\n","['https://openalex.org/W3125709657', 'https://openalex.org/W2962693497', 'https://openalex.org/W2963830550', 'https://openalex.org/W1524333225', 'https://openalex.org/W2892140764', 'https://openalex.org/W2134202996', 'https://openalex.org/W2964243274', 'https://openalex.org/W2972867623', 'https://openalex.org/W2963799213', 'https://openalex.org/W2774848319', 'https://openalex.org/W2962699523', 'https://openalex.org/W2973013862', 'https://openalex.org/W2947445680', 'https://openalex.org/W2766812927', 'https://openalex.org/W2963620343', 'https://openalex.org/W2950414763', 'https://openalex.org/W2547039119', 'https://openalex.org/W2598638573', 'https://openalex.org/W2745710152', 'https://openalex.org/W2519091744', 'https://openalex.org/W2972374322', 'https://openalex.org/W2584032004', 'https://openalex.org/W2964135678', 'https://openalex.org/W2532494225', 'https://openalex.org/W2347098582', 'https://openalex.org/W2972964185', 'https://openalex.org/W2346964103', 'https://openalex.org/W2787447541', 'https://openalex.org/W2964115348', 'https://openalex.org/W2020607164']",2019-09-13
https://openalex.org/W2057007397,https://doi.org/10.1109/asru.2011.6163965,Efficient spoken term discovery using randomized algorithms,"Spoken term discovery is the task of automatically identifying words and phrases in speech data by searching for long repeated acoustic patterns. Initial solutions relied on exhaustive dynamic time warping-based searches across the entire similarity matrix, a method whose scalability is ultimately limited by the O(n <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup> ) nature of the search space. Recent strategies have attempted to improve search efficiency by using either unsupervised or mismatched-language acoustic models to reduce the complexity of the feature representation. Taking a completely different approach, this paper investigates the use of randomized algorithms that operate directly on the raw acoustic features to produce sparse approximate similarity matrices in O(n) space and O(n log n) time. We demonstrate these techniques facilitate spoken term discovery performance capable of outperforming a model-based strategy in the zero resource setting.","['https://openalex.org/W2112038498', 'https://openalex.org/W2116343079', 'https://openalex.org/W6680730250', 'https://openalex.org/W2113932204', 'https://openalex.org/W2397535009', 'https://openalex.org/W2170580867', 'https://openalex.org/W6795644987', 'https://openalex.org/W2122056984', 'https://openalex.org/W6714100551', 'https://openalex.org/W2152565070', 'https://openalex.org/W2401464865', 'https://openalex.org/W2079460648', 'https://openalex.org/W2147717514', 'https://openalex.org/W1606268232', 'https://openalex.org/W4230940751', 'https://openalex.org/W2108120477', 'https://openalex.org/W2114347655', 'https://openalex.org/W308497914', 'https://openalex.org/W2166343746', 'https://openalex.org/W30845872', 'https://openalex.org/W2096834449', 'https://openalex.org/W1539935047', 'https://openalex.org/W6602705600', 'https://openalex.org/W2112107221', 'https://openalex.org/W2140427797', 'https://openalex.org/W3160851792', 'https://openalex.org/W66167291', 'https://openalex.org/W2012833704', 'https://openalex.org/W2407151108']",2011-12-01
https://openalex.org/W2398490608,https://doi.org/10.21437/interspeech.2015-645,Unsupervised word discovery from speech using automatic segmentation into syllable-like units,,"['https://openalex.org/W2252172689', 'https://openalex.org/W2164356097', 'https://openalex.org/W2400622388', 'https://openalex.org/W2056486423', 'https://openalex.org/W2055408826', 'https://openalex.org/W1993287287', 'https://openalex.org/W1991274470', 'https://openalex.org/W2020944885', 'https://openalex.org/W2053952913', 'https://openalex.org/W1985163701', 'https://openalex.org/W2029948425', 'https://openalex.org/W2004833594', 'https://openalex.org/W2044138293', 'https://openalex.org/W2049142189', 'https://openalex.org/W2078769636', 'https://openalex.org/W2071665560', 'https://openalex.org/W2059824090', 'https://openalex.org/W28194048', 'https://openalex.org/W2251025892', 'https://openalex.org/W2114777034', 'https://openalex.org/W2067539019', 'https://openalex.org/W2133148365', 'https://openalex.org/W1965635292', 'https://openalex.org/W2026764577', 'https://openalex.org/W1774234760', 'https://openalex.org/W2126203737']",2015-09-06
https://openalex.org/W2407614114,https://doi.org/10.21437/interspeech.2015-646,An evaluation of graph clustering methods for unsupervised term discovery,"Unsupervised term discovery (UTD) is the task of automatically identifying the repeated words and phrases in a collection of speech audio without relying on any language-specific resources. While the solution space for the task is far from fully explored, the dominant approach to date decomposes the discovery problem into two steps, where (i) segmental dynamic time warping is used to search the speech audio for repeated acoustic patterns, and (ii) these individual repetitions are partitioned into word/phrase categories using graph clustering. In this paper, we perform an unprecedented evaluation of a wide range of advanced graph clustering methods for the UTD task. We conduct our study in the evaluation framework of the Zero Resource Speech Challenge. We find that, for a range of features and languages, modularity-based clustering improves UTD performance most consistently, often by a wide margin. When paired with out-of-language deep neural net bottleneck features, we find performance near that of a high-resource UTD system.","['https://openalex.org/W66167291', 'https://openalex.org/W2164998314', 'https://openalex.org/W2047940964', 'https://openalex.org/W2059652594', 'https://openalex.org/W2060108852', 'https://openalex.org/W2025482506', 'https://openalex.org/W2151936673', 'https://openalex.org/W2168080440', 'https://openalex.org/W2786608204', 'https://openalex.org/W2052697931', 'https://openalex.org/W1539935047', 'https://openalex.org/W1991274470', 'https://openalex.org/W2132202037', 'https://openalex.org/W2089458547', 'https://openalex.org/W1967924372', 'https://openalex.org/W1580203226', 'https://openalex.org/W1545920196', 'https://openalex.org/W2036964623', 'https://openalex.org/W2072396742', 'https://openalex.org/W1983345514', 'https://openalex.org/W2018562712', 'https://openalex.org/W2401464865', 'https://openalex.org/W2124209874', 'https://openalex.org/W2062914951', 'https://openalex.org/W2294799344', 'https://openalex.org/W2131681506', 'https://openalex.org/W2057007397', 'https://openalex.org/W30845872', 'https://openalex.org/W229097380', 'https://openalex.org/W2078769636', 'https://openalex.org/W2109726592', 'https://openalex.org/W2114347655', 'https://openalex.org/W2095293504']",2015-09-06
https://openalex.org/W2774848319,https://doi.org/10.48550/arxiv.1711.11293,Parallel-Data-Free Voice Conversion Using Cycle-Consistent Adversarial Networks,"We propose a parallel-data-free voice-conversion (VC) method that can learn a mapping from source to target speech without relying on parallel data. The proposed method is general purpose, high quality, and parallel-data free and works without any extra data, modules, or alignment procedure. It also avoids over-smoothing, which occurs in many conventional statistical model-based VC methods. Our method, called CycleGAN-VC, uses a cycle-consistent adversarial network (CycleGAN) with gated convolutional neural networks (CNNs) and an identity-mapping loss. A CycleGAN learns forward and inverse mappings simultaneously using adversarial and cycle-consistency losses. This makes it possible to find an optimal pseudo pair from unpaired data. Furthermore, the adversarial loss contributes to reducing over-smoothing of the converted feature sequence. We configure a CycleGAN with gated CNNs and train it with an identity-mapping loss. This allows the mapping function to capture sequential and hierarchical structures while preserving linguistic information. We evaluated our method on a parallel-data-free VC task. An objective evaluation showed that the converted feature sequence was near natural in terms of global variance and modulation spectra. A subjective evaluation showed that the quality of the converted speech was comparable to that obtained with a Gaussian mixture model-based method under advantageous conditions with parallel and twice the amount of data.","['https://openalex.org/W2951939904', 'https://openalex.org/W2502312327', 'https://openalex.org/W2396025094', 'https://openalex.org/W2156142001', 'https://openalex.org/W2153057929', 'https://openalex.org/W2949496494', 'https://openalex.org/W142801778', 'https://openalex.org/W2747744257', 'https://openalex.org/W49412823', 'https://openalex.org/W2123003832', 'https://openalex.org/W2057609679', 'https://openalex.org/W10800834', 'https://openalex.org/W2518312472', 'https://openalex.org/W2086796102', 'https://openalex.org/W1588266896', 'https://openalex.org/W2962793481', 'https://openalex.org/W2964121744', 'https://openalex.org/W2142300631', 'https://openalex.org/W2169579015', 'https://openalex.org/W2586453598', 'https://openalex.org/W2105160541', 'https://openalex.org/W1523372075', 'https://openalex.org/W2475998840', 'https://openalex.org/W2666408839', 'https://openalex.org/W2099471712', 'https://openalex.org/W2553897675', 'https://openalex.org/W2120605154', 'https://openalex.org/W2476548250', 'https://openalex.org/W2161727827', 'https://openalex.org/W2949650786', 'https://openalex.org/W2608015370', 'https://openalex.org/W1977362459']",2017-11-30
https://openalex.org/W2165874743,,On Spectral Clustering: Analysis and an algorithm,"Despite many empirical successes of spectral clustering methods -- algorithms that cluster points using eigenvectors of matrices derived from the distances between the points -- there are several unresolved issues. First, there is a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems.","['https://openalex.org/W2120377775', 'https://openalex.org/W1981193610', 'https://openalex.org/W2171009857', 'https://openalex.org/W2123320529', 'https://openalex.org/W2160167256', 'https://openalex.org/W609249866', 'https://openalex.org/W2141376824', 'https://openalex.org/W2140095548', 'https://openalex.org/W2083761303', 'https://openalex.org/W2067976091', 'https://openalex.org/W2130891992', 'https://openalex.org/W658559791', 'https://openalex.org/W1578099820']",2001-01-03
https://openalex.org/W2141465109,,Learning Spectral Clustering,"Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters with points in the same cluster having high similarity and points in different clusters having low similarity. In this paper, we derive a new cost function for spectral clustering based on a measure of error between a given partition and a solution of the spectral relaxation of a minimum normalized cut problem. Minimizing this cost function with respect to the partition leads to a new spectral clustering algorithm. Minimizing with respect to the similarity matrix leads to an algorithm for learning the similarity matrix. We develop a tractable approximation of our cost function that is based on the power method of computing eigenvectors. 1","['https://openalex.org/W2117154949', 'https://openalex.org/W2044222806', 'https://openalex.org/W2171522835', 'https://openalex.org/W1969558191', 'https://openalex.org/W2123320529', 'https://openalex.org/W2798909945', 'https://openalex.org/W2912210943', 'https://openalex.org/W2097649450', 'https://openalex.org/W2139850885', 'https://openalex.org/W2165874743', 'https://openalex.org/W2171009857', 'https://openalex.org/W2121927366', 'https://openalex.org/W2137557016', 'https://openalex.org/W2155754954', 'https://openalex.org/W2124101779', 'https://openalex.org/W2134089414', 'https://openalex.org/W2121947440']",2003-12-09
https://openalex.org/W2996383576,,vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations,We propose vq-wav2vec to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The algorithm uses either a gumbel softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition.,"['https://openalex.org/W2346964103', 'https://openalex.org/W2518108298', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963403868', 'https://openalex.org/W2962901777', 'https://openalex.org/W10548402', 'https://openalex.org/W2842511635', 'https://openalex.org/W2547875792', 'https://openalex.org/W2520160253', 'https://openalex.org/W2962784628', 'https://openalex.org/W3011411500', 'https://openalex.org/W3127686677', 'https://openalex.org/W2947591107', 'https://openalex.org/W2124509324', 'https://openalex.org/W2941814890', 'https://openalex.org/W2963425185', 'https://openalex.org/W2965373594', 'https://openalex.org/W2936295285', 'https://openalex.org/W2926827382', 'https://openalex.org/W2973049979', 'https://openalex.org/W2153579005', 'https://openalex.org/W2889282842', 'https://openalex.org/W2936774411', 'https://openalex.org/W2940180244', 'https://openalex.org/W2987741655', 'https://openalex.org/W2963382687', 'https://openalex.org/W2933138175', 'https://openalex.org/W2786459654', 'https://openalex.org/W1494198834', 'https://openalex.org/W2794209590', 'https://openalex.org/W1885680957', 'https://openalex.org/W2963807318', 'https://openalex.org/W2963620343', 'https://openalex.org/W2141440284']",2020-04-30
https://openalex.org/W3096338464,https://doi.org/10.21437/interspeech.2020-1800,Iterative Pseudo-Labeling for Speech Recognition,"Pseudo-labeling has recently shown promise in end-to-end automatic speech recognition (ASR).We study Iterative Pseudo-Labeling (IPL), a semi-supervised algorithm which efficiently performs multiple iterations of pseudo-labeling on unlabeled data as the acoustic model evolves.In particular, IPL fine tunes an existing model at each iteration using both labeled data and a subset of unlabeled data.We study the main components of IPL: decoding with a language model and data augmentation.We then demonstrate the effectiveness of IPL by achieving state-of-the-art word-error rate on the LIBRISPEECH test sets in both standard and low-resource setting.We also study the effect of language models trained on different corpora to show IPL can effectively utilize additional text.Finally, we release a new large in-domain text corpus which does not overlap with the LIBRISPEECH training transcriptions to foster research in low-resource, semi-supervised ASR.","['https://openalex.org/W2963400424', 'https://openalex.org/W2095705004', 'https://openalex.org/W1975113979', 'https://openalex.org/W2981857663', 'https://openalex.org/W82886505', 'https://openalex.org/W3006827623', 'https://openalex.org/W3035160371', 'https://openalex.org/W3026041220', 'https://openalex.org/W3015265920', 'https://openalex.org/W4385245566', 'https://openalex.org/W2998532468', 'https://openalex.org/W2972818416', 'https://openalex.org/W2975381464', 'https://openalex.org/W2146502635', 'https://openalex.org/W2404463488', 'https://openalex.org/W3036601975', 'https://openalex.org/W2904818793', 'https://openalex.org/W2936774411', 'https://openalex.org/W2046932483', 'https://openalex.org/W2033256038', 'https://openalex.org/W2988736778', 'https://openalex.org/W2134800885', 'https://openalex.org/W4286784498', 'https://openalex.org/W2941814890', 'https://openalex.org/W2139453310', 'https://openalex.org/W1494198834', 'https://openalex.org/W3095350795', 'https://openalex.org/W2171761326', 'https://openalex.org/W2995181338', 'https://openalex.org/W2976223659', 'https://openalex.org/W3015522062', 'https://openalex.org/W2991213871', 'https://openalex.org/W2963250244']",2020-10-25
https://openalex.org/W3015737168,https://doi.org/10.1109/icassp40776.2020.9052940,Sequence-Level Consistency Training for Semi-Supervised End-to-End Automatic Speech Recognition,"This paper presents a novel semi-supervised end-to-end automatic speech recognition (ASR) method that employs consistency training with the use of unlabeled data. In consistency training, unlabeled data can be utilized for constraining a model such that it becomes invariant to small deformation. In fact, considering consistency can make the model robust to a variety of input examples. While previous studies have applied consistency training to primitive classification problems, no studies have employed consistency training to tackle sequence-to-sequence generation problems including end-to- end ASR. One problem is that existing consistency training schemes cannot take sequence-level generation consistency into consideration. In this paper, we propose a sequence-level consistency training scheme specialized to handle sequence-to-sequence generation problems. Our key idea is to consider the consistency of the generation function by utilizing beam search decoding results. For semi- supervised learning, we adopt Transformer as the end-to-end ASR model, and SpecAugment as the deformation function in consistency training. Our experiments show that our semi-supervised learning proposal with sequence-level consistency training can efficiently improve ASR performance using unlabeled speech data.","['https://openalex.org/W6601563604', 'https://openalex.org/W6754299077', 'https://openalex.org/W6760966419', 'https://openalex.org/W2972778435', 'https://openalex.org/W2972389417', 'https://openalex.org/W2962824709', 'https://openalex.org/W2936774411', 'https://openalex.org/W2962699523', 'https://openalex.org/W6758673196', 'https://openalex.org/W6753186555', 'https://openalex.org/W2889213362', 'https://openalex.org/W2963681135', 'https://openalex.org/W6747398299', 'https://openalex.org/W2888930363', 'https://openalex.org/W6742537141', 'https://openalex.org/W6640708543', 'https://openalex.org/W2972864786', 'https://openalex.org/W6676562027', 'https://openalex.org/W2608712415', 'https://openalex.org/W2327501763', 'https://openalex.org/W6735390525', 'https://openalex.org/W6739901393', 'https://openalex.org/W2521999726', 'https://openalex.org/W2963739817', 'https://openalex.org/W6764051988', 'https://openalex.org/W2972889948', 'https://openalex.org/W6736346607', 'https://openalex.org/W6739769129', 'https://openalex.org/W2963736842', 'https://openalex.org/W6765939562', 'https://openalex.org/W2912937645', 'https://openalex.org/W2963414781', 'https://openalex.org/W2892009249', 'https://openalex.org/W1942035323', 'https://openalex.org/W2746192915', 'https://openalex.org/W2962369866', 'https://openalex.org/W2963403868', 'https://openalex.org/W2962826786', 'https://openalex.org/W2645998928', 'https://openalex.org/W2964159205', 'https://openalex.org/W2883586237', 'https://openalex.org/W2936078256', 'https://openalex.org/W2597757402', 'https://openalex.org/W37526647', 'https://openalex.org/W2951970475', 'https://openalex.org/W4385245566']",2020-04-09
https://openalex.org/W1836465849,https://doi.org/10.48550/arxiv.1502.03167,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,"Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.","['https://openalex.org/W1563686443', 'https://openalex.org/W2084894614', 'https://openalex.org/W1915968771', 'https://openalex.org/W2034368206', 'https://openalex.org/W2146502635', 'https://openalex.org/W2914484425', 'https://openalex.org/W2123649031', 'https://openalex.org/W2127230474', 'https://openalex.org/W104184427', 'https://openalex.org/W1762484328', 'https://openalex.org/W1815076433', 'https://openalex.org/W1814328102', 'https://openalex.org/W2952020226', 'https://openalex.org/W2152424459', 'https://openalex.org/W2963504252', 'https://openalex.org/W1533861849', 'https://openalex.org/W2950179405', 'https://openalex.org/W2168231600', 'https://openalex.org/W1665214252', 'https://openalex.org/W2095705004', 'https://openalex.org/W1677182931', 'https://openalex.org/W2112796928', 'https://openalex.org/W2134583763']",2015-02-11
https://openalex.org/W3026041220,https://doi.org/10.21437/interspeech.2020-1470,Improved Noisy Student Training for Automatic Speech Recognition,"Recently, a semi-supervised learning method known as ""noisy student training""\nhas been shown to improve image classification performance of deep networks\nsignificantly. Noisy student training is an iterative self-training method that\nleverages augmentation to improve network performance. In this work, we adapt\nand improve noisy student training for automatic speech recognition, employing\n(adaptive) SpecAugment as the augmentation method. We find effective methods to\nfilter, balance and augment the data generated in between self-training\niterations. By doing so, we are able to obtain word error rates (WERs)\n4.2%/8.6% on the clean/noisy LibriSpeech test sets by only using the clean 100h\nsubset of LibriSpeech as the supervised set and the rest (860h) as the\nunlabeled set. Furthermore, we are able to achieve WERs 1.7%/3.4% on the\nclean/noisy LibriSpeech test sets by using the unlab-60k subset of LibriLight\nas the unlabeled set for LibriSpeech 960h. We are thus able to improve upon the\nprevious state-of-the-art clean/noisy test WERs achieved on LibriSpeech 100h\n(4.74%/12.20%) and LibriSpeech (1.9%/4.1%).\n","['https://openalex.org/W2088622183', 'https://openalex.org/W1915251500', 'https://openalex.org/W3125709657', 'https://openalex.org/W1494198834', 'https://openalex.org/W2964245029', 'https://openalex.org/W2111316763', 'https://openalex.org/W2327501763', 'https://openalex.org/W165878654', 'https://openalex.org/W3103005696', 'https://openalex.org/W3015522062', 'https://openalex.org/W1828163288', 'https://openalex.org/W1489125746', 'https://openalex.org/W2976223659', 'https://openalex.org/W2998532468', 'https://openalex.org/W2995181338', 'https://openalex.org/W2962907457', 'https://openalex.org/W2973049979', 'https://openalex.org/W2963425185', 'https://openalex.org/W2101210369', 'https://openalex.org/W2996383576', 'https://openalex.org/W2995746049', 'https://openalex.org/W3015995734', 'https://openalex.org/W2940322076', 'https://openalex.org/W1993660824', 'https://openalex.org/W2963920996', 'https://openalex.org/W2962369866', 'https://openalex.org/W2121879602', 'https://openalex.org/W2577366047', 'https://openalex.org/W2117590177', 'https://openalex.org/W3001197829', 'https://openalex.org/W4285719527', 'https://openalex.org/W2936774411', 'https://openalex.org/W2981952041', 'https://openalex.org/W2944255943', 'https://openalex.org/W3016010032', 'https://openalex.org/W2972943112', 'https://openalex.org/W2912492482', 'https://openalex.org/W3035160371', 'https://openalex.org/W3015265920', 'https://openalex.org/W2062366414', 'https://openalex.org/W2979476256']",2020-10-25
https://openalex.org/W3016011332,https://doi.org/10.1109/icassp40776.2020.9054438,Generative Pre-Training for Speech with Autoregressive Predictive Coding,"Learning meaningful and general representations from unannotated speech that are applicable to a wide range of tasks remains challenging. In this paper we propose to use autoregressive predictive coding (APC), a recently proposed self-supervised objective, as a generative pre-training approach for learning meaningful, non-specific, and transferable speech representations. We pre-train APC on large-scale unlabeled data and conduct transfer learning experiments on three speech applications that require different information about speech characteristics to perform well: speech recognition, speech translation, and speaker identification. Extensive experiments show that APC not only outperforms surface features (e.g., log Mel spectrograms) and other popular representation learning methods on all three tasks, but is also effective at reducing downstream labeled data size and model parameters. We also investigate the use of Transformers for modeling APC and find it superior to RNNs.","['https://openalex.org/W2889028433', 'https://openalex.org/W2943845043', 'https://openalex.org/W6761563299', 'https://openalex.org/W2963026768', 'https://openalex.org/W6752888775', 'https://openalex.org/W6764498146', 'https://openalex.org/W2973217961', 'https://openalex.org/W2973034126', 'https://openalex.org/W6745117592', 'https://openalex.org/W179875071', 'https://openalex.org/W6739901393', 'https://openalex.org/W2964199361', 'https://openalex.org/W6748148878', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962739339', 'https://openalex.org/W2194775991', 'https://openalex.org/W6755977528', 'https://openalex.org/W4300558631', 'https://openalex.org/W2973049979', 'https://openalex.org/W2973157397', 'https://openalex.org/W6898505805', 'https://openalex.org/W2842511635', 'https://openalex.org/W2973048981', 'https://openalex.org/W2963720603', 'https://openalex.org/W2963425185', 'https://openalex.org/W2963317665', 'https://openalex.org/W3100270690', 'https://openalex.org/W2972943112', 'https://openalex.org/W2884305338', 'https://openalex.org/W6631190155', 'https://openalex.org/W6623517193', 'https://openalex.org/W2024490156', 'https://openalex.org/W6766673545', 'https://openalex.org/W6755207826', 'https://openalex.org/W2899134946', 'https://openalex.org/W6748215858', 'https://openalex.org/W1522301498', 'https://openalex.org/W3125709657', 'https://openalex.org/W2963432880', 'https://openalex.org/W2785350307', 'https://openalex.org/W2941814890', 'https://openalex.org/W2808706139', 'https://openalex.org/W854541894', 'https://openalex.org/W2896457183', 'https://openalex.org/W2963045354', 'https://openalex.org/W2758785877', 'https://openalex.org/W2101105183', 'https://openalex.org/W2952127920', 'https://openalex.org/W2963341956', 'https://openalex.org/W4385245566', 'https://openalex.org/W2965373594', 'https://openalex.org/W2963403868', 'https://openalex.org/W2899663614', 'https://openalex.org/W4297808394', 'https://openalex.org/W2963618559', 'https://openalex.org/W2964121744']",2020-04-09
https://openalex.org/W3025165719,https://doi.org/10.48550/arxiv.2005.08100,Conformer: Convolution-augmented Transformer for Speech Recognition,"Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.","['https://openalex.org/W1964175594', 'https://openalex.org/W3021469861', 'https://openalex.org/W2767286248', 'https://openalex.org/W1828163288', 'https://openalex.org/W2952180055', 'https://openalex.org/W3016010032', 'https://openalex.org/W3015194534', 'https://openalex.org/W2937843571', 'https://openalex.org/W2979636403', 'https://openalex.org/W2932319281', 'https://openalex.org/W2892009249', 'https://openalex.org/W2908336025', 'https://openalex.org/W2963970792', 'https://openalex.org/W2964110616', 'https://openalex.org/W2095705004', 'https://openalex.org/W2928941594', 'https://openalex.org/W2936774411', 'https://openalex.org/W2994771587', 'https://openalex.org/W2963542740', 'https://openalex.org/W2963420686', 'https://openalex.org/W2626778328', 'https://openalex.org/W2981413347', 'https://openalex.org/W2962760690', 'https://openalex.org/W1522301498', 'https://openalex.org/W2112739286', 'https://openalex.org/W1995562189', 'https://openalex.org/W3019527251', 'https://openalex.org/W2798858969', 'https://openalex.org/W2981857663', 'https://openalex.org/W2972818416', 'https://openalex.org/W1494198834', 'https://openalex.org/W2963414781', 'https://openalex.org/W2981581604', 'https://openalex.org/W2948981900']",2020-05-16
https://openalex.org/W3095783102,https://doi.org/10.48550/arxiv.2011.01576,Improving RNN transducer with normalized jointer network,"Recurrent neural transducer (RNN-T) is a promising end-to-end (E2E) model in automatic speech recognition (ASR). It has shown superior performance compared to traditional hybrid ASR systems. However, training RNN-T from scratch is still challenging. We observe a huge gradient variance during RNN-T training and suspect it hurts the performance. In this work, we analyze the cause of the huge gradient variance in RNN-T training and proposed a new \textit{normalized jointer network} to overcome it. We also propose to enhance the RNN-T network with a modified conformer encoder network and transformer-XL predictor networks to achieve the best performance. Experiments are conducted on the open 170-hour AISHELL-1 and industrial-level 30000-hour mandarin speech dataset. On the AISHELL-1 dataset, our RNN-T system gets state-of-the-art results on AISHELL-1's streaming and non-streaming benchmark with CER 6.15\% and 5.37\% respectively. We further compare our RNN-T system with our well trained commercial hybrid system on 30000-hour-industry audio data and get 9\% relative improvement without pre-training or external language model.","['https://openalex.org/W2936774411', 'https://openalex.org/W2963403868', 'https://openalex.org/W2962784628', 'https://openalex.org/W2327501763', 'https://openalex.org/W2963308316', 'https://openalex.org/W2963242190', 'https://openalex.org/W1828163288', 'https://openalex.org/W3033604713', 'https://openalex.org/W3096686110', 'https://openalex.org/W2963827914', 'https://openalex.org/W2750499125', 'https://openalex.org/W3008898571', 'https://openalex.org/W3021469861', 'https://openalex.org/W2963414781', 'https://openalex.org/W2976556660', 'https://openalex.org/W2911109671', 'https://openalex.org/W3033017368', 'https://openalex.org/W3015194534', 'https://openalex.org/W1522301498', 'https://openalex.org/W2746192915', 'https://openalex.org/W2972895620', 'https://openalex.org/W2963382687', 'https://openalex.org/W2127141656', 'https://openalex.org/W2941814890', 'https://openalex.org/W3025165719', 'https://openalex.org/W3015315932']",2020-11-03
https://openalex.org/W2883725317,https://doi.org/10.1007/978-3-030-01264-9_9,Deep Clustering for Unsupervised Learning of Visual Features,,"['https://openalex.org/W1520997877', 'https://openalex.org/W2110798204', 'https://openalex.org/W1955857676', 'https://openalex.org/W2963474899', 'https://openalex.org/W6600007113', 'https://openalex.org/W180242331', 'https://openalex.org/W2108598243', 'https://openalex.org/W343636949', 'https://openalex.org/W2962824366', 'https://openalex.org/W6776948474', 'https://openalex.org/W1893585201', 'https://openalex.org/W1480376833', 'https://openalex.org/W1677182931', 'https://openalex.org/W2139427956', 'https://openalex.org/W2963446712', 'https://openalex.org/W6601052344', 'https://openalex.org/W2086052791', 'https://openalex.org/W2100031962', 'https://openalex.org/W4205743601', 'https://openalex.org/W2308529009', 'https://openalex.org/W2112796928', 'https://openalex.org/W1989684337', 'https://openalex.org/W2136655611', 'https://openalex.org/W2962749380', 'https://openalex.org/W2321533354', 'https://openalex.org/W2750549109', 'https://openalex.org/W2511428026', 'https://openalex.org/W2575671312', 'https://openalex.org/W2963420272', 'https://openalex.org/W2212363941', 'https://openalex.org/W2141362318', 'https://openalex.org/W2148809531', 'https://openalex.org/W2117539524', 'https://openalex.org/W2062118960', 'https://openalex.org/W2121947440', 'https://openalex.org/W2162762921', 'https://openalex.org/W219040644', 'https://openalex.org/W2963749571', 'https://openalex.org/W2113221323', 'https://openalex.org/W2962852342', 'https://openalex.org/W1849277567', 'https://openalex.org/W2326925005', 'https://openalex.org/W2558661413', 'https://openalex.org/W1544092585', 'https://openalex.org/W2613718673', 'https://openalex.org/W2774008708', 'https://openalex.org/W2134670479', 'https://openalex.org/W2949578333', 'https://openalex.org/W2163605009', 'https://openalex.org/W2099471712', 'https://openalex.org/W2095705004', 'https://openalex.org/W2108282816', 'https://openalex.org/W2560977758', 'https://openalex.org/W2964074409', 'https://openalex.org/W2174726731', 'https://openalex.org/W2148349024', 'https://openalex.org/W2113896236', 'https://openalex.org/W1686810756', 'https://openalex.org/W2412782625', 'https://openalex.org/W2145094598', 'https://openalex.org/W2103716973', 'https://openalex.org/W2554692997', 'https://openalex.org/W1625255723', 'https://openalex.org/W2607510315', 'https://openalex.org/W2962877362', 'https://openalex.org/W2949117887']",2018-01-01
https://openalex.org/W2842511635,,Representation Learning with Contrastive Predictive Coding,"While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.","['https://openalex.org/W2412320034', 'https://openalex.org/W2187089797', 'https://openalex.org/W2160660844', 'https://openalex.org/W2152808281', 'https://openalex.org/W2964127152', 'https://openalex.org/W2163605009', 'https://openalex.org/W2302255633', 'https://openalex.org/W2014902591', 'https://openalex.org/W343636949', 'https://openalex.org/W2157331557', 'https://openalex.org/W2950726992', 'https://openalex.org/W1494198834', 'https://openalex.org/W2106053110', 'https://openalex.org/W2160815625', 'https://openalex.org/W2606347107', 'https://openalex.org/W1522301498', 'https://openalex.org/W2321533354', 'https://openalex.org/W2963403868', 'https://openalex.org/W2127958135', 'https://openalex.org/W3099206234', 'https://openalex.org/W219040644', 'https://openalex.org/W2964043796', 'https://openalex.org/W2326925005', 'https://openalex.org/W1895577753', 'https://openalex.org/W2259472270', 'https://openalex.org/W2786036274', 'https://openalex.org/W2952186591', 'https://openalex.org/W1836465849', 'https://openalex.org/W2949382160', 'https://openalex.org/W2962824366', 'https://openalex.org/W3037932933', 'https://openalex.org/W2114524997', 'https://openalex.org/W1681397005', 'https://openalex.org/W2130942839', 'https://openalex.org/W2070246124', 'https://openalex.org/W2037034710', 'https://openalex.org/W2950577311', 'https://openalex.org/W2112129677', 'https://openalex.org/W2119885245', 'https://openalex.org/W2950797609', 'https://openalex.org/W2963762683', 'https://openalex.org/W2146444479', 'https://openalex.org/W2152790380', 'https://openalex.org/W2131744502', 'https://openalex.org/W3189092450', 'https://openalex.org/W1566289585', 'https://openalex.org/W1524333225', 'https://openalex.org/W2949536664', 'https://openalex.org/W2157364932', 'https://openalex.org/W2117539524']",2018-07-10
https://openalex.org/W1494198834,https://doi.org/10.1109/icassp.2015.7178964,Librispeech: An ASR corpus based on public domain audio books,"This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.","['https://openalex.org/W2113641473', 'https://openalex.org/W2090755665', 'https://openalex.org/W2148154194', 'https://openalex.org/W2106554350', 'https://openalex.org/W2087064593', 'https://openalex.org/W1647671624', 'https://openalex.org/W1517939602', 'https://openalex.org/W2037740282', 'https://openalex.org/W1599512239', 'https://openalex.org/W6631362777', 'https://openalex.org/W6603477829', 'https://openalex.org/W2024490156', 'https://openalex.org/W2164107060', 'https://openalex.org/W6712802073', 'https://openalex.org/W6677973343', 'https://openalex.org/W6636811518', 'https://openalex.org/W6738902873', 'https://openalex.org/W2097927681', 'https://openalex.org/W2026369565', 'https://openalex.org/W2330075180', 'https://openalex.org/W2950186769', 'https://openalex.org/W2125234026', 'https://openalex.org/W1524333225', 'https://openalex.org/W1934041838', 'https://openalex.org/W1631260214', 'https://openalex.org/W2397159106', 'https://openalex.org/W2620757702', 'https://openalex.org/W85707815', 'https://openalex.org/W2916535084']",2015-04-01
https://openalex.org/W3094800360,https://doi.org/10.48550/arxiv.2010.13956,Recent Developments on ESPnet Toolkit Boosted by Conformer,"In this study, we present recent developments on ESPnet: End-to-End Speech Processing toolkit, which mainly involves a recently proposed architecture called Conformer, Convolution-augmented Transformer. This paper shows the results for a wide range of end-to-end speech processing applications, such as automatic speech recognition (ASR), speech translations (ST), speech separation (SS) and text-to-speech (TTS). Our experiments reveal various training tips and significant performance benefits obtained with the Conformer on different tasks. These results are competitive or even outperform the current state-of-art Transformer models. We are preparing to release all-in-one recipes using open source and publicly available corpora for all the above tasks with pre-trained models. Our aim for this work is to contribute to our research community by reducing the burden of preparing state-of-the-art research environments usually requiring high resources.","['https://openalex.org/W3115011379', 'https://openalex.org/W2970730223', 'https://openalex.org/W2127851351', 'https://openalex.org/W2734774145', 'https://openalex.org/W1524333225', 'https://openalex.org/W3016160783', 'https://openalex.org/W3085139254', 'https://openalex.org/W3025165719', 'https://openalex.org/W3034949308', 'https://openalex.org/W2963403868', 'https://openalex.org/W2749581528', 'https://openalex.org/W2127141656', 'https://openalex.org/W2963587345', 'https://openalex.org/W2972389417', 'https://openalex.org/W2972818416', 'https://openalex.org/W2903739847', 'https://openalex.org/W2962780374', 'https://openalex.org/W2964110616', 'https://openalex.org/W3037217258', 'https://openalex.org/W2964308564', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963542740', 'https://openalex.org/W2886180730', 'https://openalex.org/W2963250244', 'https://openalex.org/W3015338123', 'https://openalex.org/W2407080277', 'https://openalex.org/W2936774411', 'https://openalex.org/W2963970792', 'https://openalex.org/W2526425061', 'https://openalex.org/W3008191852', 'https://openalex.org/W2892009249']",2020-10-26
https://openalex.org/W2064675550,https://doi.org/10.1162/neco.1997.9.8.1735,Long Short-Term Memory,"Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.","['https://openalex.org/W2165432985', 'https://openalex.org/W2107878631', 'https://openalex.org/W2121553911', 'https://openalex.org/W2086756055', 'https://openalex.org/W2048060899', 'https://openalex.org/W2103452139', 'https://openalex.org/W1971209221', 'https://openalex.org/W2143503258', 'https://openalex.org/W2154890045', 'https://openalex.org/W2007431958', 'https://openalex.org/W1984375561', 'https://openalex.org/W2123716044', 'https://openalex.org/W1982291194', 'https://openalex.org/W2114471683', 'https://openalex.org/W2036317923', 'https://openalex.org/W1963605035', 'https://openalex.org/W2156960699', 'https://openalex.org/W1971129545', 'https://openalex.org/W2128499899', 'https://openalex.org/W2119216348', 'https://openalex.org/W2144001972', 'https://openalex.org/W4285719527', 'https://openalex.org/W2146367896', 'https://openalex.org/W2131387845', 'https://openalex.org/W2136939460', 'https://openalex.org/W202296964', 'https://openalex.org/W1881179843', 'https://openalex.org/W2129831132', 'https://openalex.org/W2162704388', 'https://openalex.org/W2057653135', 'https://openalex.org/W2168934702', 'https://openalex.org/W2118127772', 'https://openalex.org/W2111871983', 'https://openalex.org/W1674799117']",1997-11-01
https://openalex.org/W3110458199,https://doi.org/10.48550/arxiv.2011.11588,The Zero Resource Speech Benchmark 2021: Metrics and baselines for\n unsupervised spoken language modeling,"We introduce a new unsupervised task, spoken language modeling: the learning\nof linguistic representations from raw audio signals without any labels, along\nwith the Zero Resource Speech Benchmark 2021: a suite of 4 black-box, zero-shot\nmetrics probing for the quality of the learned models at 4 linguistic levels:\nphonetics, lexicon, syntax and semantics. We present the results and analyses\nof a composite baseline made of the concatenation of three unsupervised\nsystems: self-supervised contrastive representation learning (CPC), clustering\n(k-means) and language modeling (LSTM or BERT). The language models learn on\nthe basis of the pseudo-text derived from clustering the learned\nrepresentations. This simple pipeline shows better than chance performance on\nall four metrics, demonstrating the feasibility of spoken language modeling\nfrom raw speech. It also yields worse performance compared to text-based\n'topline' systems trained on the same data, delineating the space to be\nexplored by more sophisticated end-to-end models.\n","['https://openalex.org/W2963341956', 'https://openalex.org/W2972447203', 'https://openalex.org/W3099782249', 'https://openalex.org/W2252211741', 'https://openalex.org/W3016011332', 'https://openalex.org/W2137735870', 'https://openalex.org/W2963419157', 'https://openalex.org/W3003875258', 'https://openalex.org/W2963425185', 'https://openalex.org/W2809981375', 'https://openalex.org/W2142625445', 'https://openalex.org/W3034775979', 'https://openalex.org/W2103318667', 'https://openalex.org/W3093096176', 'https://openalex.org/W2973026522', 'https://openalex.org/W2933138175', 'https://openalex.org/W3129289122', 'https://openalex.org/W2395899413', 'https://openalex.org/W2176085882', 'https://openalex.org/W2741692265', 'https://openalex.org/W3102342027', 'https://openalex.org/W2014307400', 'https://openalex.org/W2965373594', 'https://openalex.org/W2080100102', 'https://openalex.org/W2995181338', 'https://openalex.org/W2346964103', 'https://openalex.org/W2251012068', 'https://openalex.org/W2251025892', 'https://openalex.org/W3005511757', 'https://openalex.org/W2963366649', 'https://openalex.org/W2963403868', 'https://openalex.org/W2549835527', 'https://openalex.org/W2593779438', 'https://openalex.org/W2973049979', 'https://openalex.org/W2132631284', 'https://openalex.org/W2026487812', 'https://openalex.org/W2963620343', 'https://openalex.org/W1494198834', 'https://openalex.org/W1854884267', 'https://openalex.org/W2988736778', 'https://openalex.org/W2889947987', 'https://openalex.org/W2996383576', 'https://openalex.org/W2963751529', 'https://openalex.org/W2170682101', 'https://openalex.org/W2996728628', 'https://openalex.org/W2842511635', 'https://openalex.org/W2910243263']",2020-11-23
https://openalex.org/W2765741717,https://doi.org/10.1007/978-3-319-70096-0_39,Deep Clustering with Convolutional Autoencoders,,"['https://openalex.org/W6600075554', 'https://openalex.org/W2112796928', 'https://openalex.org/W2603986758', 'https://openalex.org/W2136655611', 'https://openalex.org/W2131828344', 'https://openalex.org/W2604647163', 'https://openalex.org/W2405933695', 'https://openalex.org/W174941419', 'https://openalex.org/W2741943936', 'https://openalex.org/W6600480908', 'https://openalex.org/W2962852342', 'https://openalex.org/W2919115771', 'https://openalex.org/W2187089797', 'https://openalex.org/W3179321675', 'https://openalex.org/W1836465849', 'https://openalex.org/W2095705004', 'https://openalex.org/W2533545350', 'https://openalex.org/W2964074409', 'https://openalex.org/W2571899125']",2017-01-01
https://openalex.org/W2982223350,https://doi.org/10.1109/icassp40776.2020.9054458,Mockingjay: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders,"We present Mockingjay as a new speech representation learning approach, where\nbidirectional Transformer encoders are pre-trained on a large amount of\nunlabeled speech. Previous speech representation methods learn through\nconditioning on past frames and predicting information about future frames.\nWhereas Mockingjay is designed to predict the current frame through jointly\nconditioning on both past and future contexts. The Mockingjay representation\nimproves performance for a wide range of downstream tasks, including phoneme\nclassification, speaker recognition, and sentiment classification on spoken\ncontent, while outperforming other approaches. Mockingjay is empirically\npowerful and can be fine-tuned with downstream models, with only 2 epochs we\nfurther improve performance dramatically. In a low resource setting with only\n0.1% of labeled data, we outperform the result of Mel-features that uses all\n100% labeled data.\n","['https://openalex.org/W2947445680', 'https://openalex.org/W6607333740', 'https://openalex.org/W6755207826', 'https://openalex.org/W6780226713', 'https://openalex.org/W2964089206', 'https://openalex.org/W2972451902', 'https://openalex.org/W6737778391', 'https://openalex.org/W2270070752', 'https://openalex.org/W6766673545', 'https://openalex.org/W6768021236', 'https://openalex.org/W2963571336', 'https://openalex.org/W2963425185', 'https://openalex.org/W2972943112', 'https://openalex.org/W2842511635', 'https://openalex.org/W2979476256', 'https://openalex.org/W2973049979', 'https://openalex.org/W3100270690', 'https://openalex.org/W2998649947', 'https://openalex.org/W6739901393', 'https://openalex.org/W2962739339', 'https://openalex.org/W6631190155', 'https://openalex.org/W1494198834', 'https://openalex.org/W2747874407', 'https://openalex.org/W6674330103', 'https://openalex.org/W2883409523', 'https://openalex.org/W2996428491', 'https://openalex.org/W2943493972', 'https://openalex.org/W4297808394', 'https://openalex.org/W3125709657', 'https://openalex.org/W2613904329', 'https://openalex.org/W2336585117', 'https://openalex.org/W2975059944', 'https://openalex.org/W2787560479', 'https://openalex.org/W2965373594', 'https://openalex.org/W1522301498', 'https://openalex.org/W4385245566', 'https://openalex.org/W2996383576', 'https://openalex.org/W179875071', 'https://openalex.org/W2896457183', 'https://openalex.org/W2626778328', 'https://openalex.org/W3037932933', 'https://openalex.org/W2095705004']",2020-04-09
https://openalex.org/W2995181338,https://doi.org/10.1109/icassp40776.2020.9052942,Libri-Light: A Benchmark for ASR with Limited or No Supervision,"We introduce a new collection of spoken English audio suitable for training\nspeech recognition systems under limited or no supervision. It is derived from\nopen-source audio books from the LibriVox project. It contains over 60K hours\nof audio, which is, to our knowledge, the largest freely-available corpus of\nspeech. The audio has been segmented using voice activity detection and is\ntagged with SNR, speaker ID and genre descriptions. Additionally, we provide\nbaseline systems and evaluation metrics working under three settings: (1) the\nzero resource/unsupervised setting (ABX), (2) the semi-supervised setting (PER,\nCER) and (3) the distant supervision setting (WER). Settings (2) and (3) use\nlimited textual resources (10 minutes to 10 hours) aligned with the speech.\nSetting (3) uses large amounts of unaligned text. They are evaluated on the\nstandard LibriSpeech dev and test sets for comparison with the supervised\nstate-of-the-art.\n","['https://openalex.org/W2963620343', 'https://openalex.org/W2940544976', 'https://openalex.org/W6629717138', 'https://openalex.org/W2937197076', 'https://openalex.org/W6712444837', 'https://openalex.org/W2592866267', 'https://openalex.org/W2953190524', 'https://openalex.org/W3005511757', 'https://openalex.org/W6751433836', 'https://openalex.org/W6770514103', 'https://openalex.org/W6756326128', 'https://openalex.org/W1970890968', 'https://openalex.org/W6656619859', 'https://openalex.org/W2127141656', 'https://openalex.org/W2973049979', 'https://openalex.org/W2842511635', 'https://openalex.org/W6775452034', 'https://openalex.org/W2346964103', 'https://openalex.org/W4234016251', 'https://openalex.org/W6747270024', 'https://openalex.org/W6679855610', 'https://openalex.org/W2944255943', 'https://openalex.org/W6748342566', 'https://openalex.org/W2972630480', 'https://openalex.org/W2963425185', 'https://openalex.org/W2161482971', 'https://openalex.org/W4288107125', 'https://openalex.org/W2979476256', 'https://openalex.org/W2963340922', 'https://openalex.org/W2899377381', 'https://openalex.org/W4297818305', 'https://openalex.org/W2134800885', 'https://openalex.org/W2593779438', 'https://openalex.org/W3103005696', 'https://openalex.org/W3016181583', 'https://openalex.org/W4297808394', 'https://openalex.org/W2988736778', 'https://openalex.org/W2804648901', 'https://openalex.org/W2025198378', 'https://openalex.org/W2161391345', 'https://openalex.org/W2787447541', 'https://openalex.org/W2926063217', 'https://openalex.org/W2794753807', 'https://openalex.org/W4300047444', 'https://openalex.org/W3015522062', 'https://openalex.org/W2395899413', 'https://openalex.org/W1494198834', 'https://openalex.org/W2996383576', 'https://openalex.org/W2973026522', 'https://openalex.org/W2781384251']",2020-04-09
https://openalex.org/W2996728628,https://doi.org/10.1162/tacl_a_00321,BLiMP: The Benchmark of Linguistic Minimal Pairs for English (Electronic Resources),"We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs—that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4%. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.","['https://openalex.org/W2888922637', 'https://openalex.org/W2963025830', 'https://openalex.org/W1986120881', 'https://openalex.org/W2963403868', 'https://openalex.org/W2167723982', 'https://openalex.org/W2995181640', 'https://openalex.org/W2163730805', 'https://openalex.org/W2923014074', 'https://openalex.org/W2750779823', 'https://openalex.org/W2962961857', 'https://openalex.org/W2973957133', 'https://openalex.org/W2953369973', 'https://openalex.org/W2787560479', 'https://openalex.org/W2911109671', 'https://openalex.org/W2963494889', 'https://openalex.org/W3037191812', 'https://openalex.org/W2918996109', 'https://openalex.org/W2963026768', 'https://openalex.org/W1582487387', 'https://openalex.org/W2064675550', 'https://openalex.org/W2158195707', 'https://openalex.org/W2770232188', 'https://openalex.org/W4365799947', 'https://openalex.org/W2978670439', 'https://openalex.org/W2972351548', 'https://openalex.org/W2759181158', 'https://openalex.org/W2170716495', 'https://openalex.org/W4252209867', 'https://openalex.org/W2964117978', 'https://openalex.org/W179875071', 'https://openalex.org/W2730712696', 'https://openalex.org/W1502957213', 'https://openalex.org/W2549835527', 'https://openalex.org/W2971044268', 'https://openalex.org/W2531882892', 'https://openalex.org/W2994665957', 'https://openalex.org/W4388123003', 'https://openalex.org/W2964204621', 'https://openalex.org/W2891399254', 'https://openalex.org/W1984471812', 'https://openalex.org/W1974795422', 'https://openalex.org/W2963341956', 'https://openalex.org/W2599674900', 'https://openalex.org/W2612690371', 'https://openalex.org/W4245765565', 'https://openalex.org/W2864832950', 'https://openalex.org/W2563574619', 'https://openalex.org/W2124669395', 'https://openalex.org/W2515741950', 'https://openalex.org/W2141440284', 'https://openalex.org/W2972752636', 'https://openalex.org/W2981852735', 'https://openalex.org/W2902967615', 'https://openalex.org/W1586060904', 'https://openalex.org/W2962926715', 'https://openalex.org/W2251930319', 'https://openalex.org/W2024988999', 'https://openalex.org/W2990704537']",2020-07-29
https://openalex.org/W2965373594,https://doi.org/10.4230/oasics.ldk.2021.22,Towards Learning Terminological Concept Systems from Multilingual Natural Language Text,"Terminological Concept Systems (TCS) provide a means of organizing, structuring and representing domain-specific multilingual information and are important to ensure terminological consistency in many tasks, such as translation and cross-border communication. While several approaches to (semi-)automatic term extraction exist, learning their interrelations is vastly underexplored. We propose an automated method to extract terms and relations across natural languages and specialized domains. To this end, we adapt pretrained multilingual neural language models, which we evaluate on term extraction standard datasets with best performing results and a combination of relation extraction standard datasets with competitive results. Code and dataset are publicly available.","['https://openalex.org/W2170973209', 'https://openalex.org/W2462831000', 'https://openalex.org/W2963807318', 'https://openalex.org/W1599016936', 'https://openalex.org/W2933138175', 'https://openalex.org/W2963748441', 'https://openalex.org/W2805206884', 'https://openalex.org/W2937297214', 'https://openalex.org/W2963846996', 'https://openalex.org/W2525127255', 'https://openalex.org/W2950501607', 'https://openalex.org/W2963112338', 'https://openalex.org/W2947813521', 'https://openalex.org/W2945785363', 'https://openalex.org/W2930786691', 'https://openalex.org/W2945290257', 'https://openalex.org/W2920812691', 'https://openalex.org/W2944815030', 'https://openalex.org/W2963403868', 'https://openalex.org/W2948629866', 'https://openalex.org/W2962753370', 'https://openalex.org/W2914526845', 'https://openalex.org/W2963341956', 'https://openalex.org/W2978670439', 'https://openalex.org/W2793353489', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963310665', 'https://openalex.org/W2396767181', 'https://openalex.org/W2962784628', 'https://openalex.org/W2130158090', 'https://openalex.org/W1566289585', 'https://openalex.org/W2899771611', 'https://openalex.org/W131533222', 'https://openalex.org/W2963756346', 'https://openalex.org/W2251939518', 'https://openalex.org/W2970597249', 'https://openalex.org/W2990704537', 'https://openalex.org/W2963026768', 'https://openalex.org/W2914120296', 'https://openalex.org/W2938830017', 'https://openalex.org/W2945260553', 'https://openalex.org/W2899663614', 'https://openalex.org/W2963323070', 'https://openalex.org/W2784121710', 'https://openalex.org/W1840435438']",2021-01-01
https://openalex.org/W2152790380,,Noise-contrastive estimation: A new estimation principle for unnormalized statistical models,"We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity. We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance. In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field. 1","['https://openalex.org/W2496945004', 'https://openalex.org/W2155118899', 'https://openalex.org/W2097620723', 'https://openalex.org/W2142615865', 'https://openalex.org/W2134653808', 'https://openalex.org/W3140968660', 'https://openalex.org/W2116064496', 'https://openalex.org/W1802356529', 'https://openalex.org/W1559616325', 'https://openalex.org/W2130184048', 'https://openalex.org/W1548802052', 'https://openalex.org/W1582813132', 'https://openalex.org/W2110176223', 'https://openalex.org/W2165363188']",2010-03-31
https://openalex.org/W1538131130,https://doi.org/10.5555/303568.303704,"Convolutional networks for images, speech, and time series",International audience,[],1998-10-01
https://openalex.org/W3093579165,https://doi.org/10.48550/arxiv.2010.10504,Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition,"We employ a combination of recent developments in semi-supervised learning for automatic speech recognition to obtain state-of-the-art results on LibriSpeech utilizing the unlabeled audio of the Libri-Light dataset. More precisely, we carry out noisy student training with SpecAugment using giant Conformer models pre-trained using wav2vec 2.0 pre-training. By doing so, we are able to achieve word-error-rates (WERs) 1.4%/2.6% on the LibriSpeech test/test-other sets against the current state-of-the-art WERs 1.7%/3.3%.","['https://openalex.org/W2064675550', 'https://openalex.org/W3021469861', 'https://openalex.org/W3015522062', 'https://openalex.org/W3035160371', 'https://openalex.org/W2964245029', 'https://openalex.org/W2767286248', 'https://openalex.org/W3025165719', 'https://openalex.org/W2963403868', 'https://openalex.org/W1494198834', 'https://openalex.org/W2981952041', 'https://openalex.org/W2962907457', 'https://openalex.org/W2977728428', 'https://openalex.org/W165878654', 'https://openalex.org/W2976223659', 'https://openalex.org/W3125709657', 'https://openalex.org/W2911109671', 'https://openalex.org/W3035003500', 'https://openalex.org/W2794753807', 'https://openalex.org/W3006827623', 'https://openalex.org/W2121879602', 'https://openalex.org/W1993660824', 'https://openalex.org/W2946856970', 'https://openalex.org/W1915251500', 'https://openalex.org/W2998532468', 'https://openalex.org/W2940322076', 'https://openalex.org/W3036982689', 'https://openalex.org/W2117590177', 'https://openalex.org/W2932319281', 'https://openalex.org/W2802422770', 'https://openalex.org/W1489125746', 'https://openalex.org/W3099782249', 'https://openalex.org/W2973049979', 'https://openalex.org/W2995181338', 'https://openalex.org/W3026041220', 'https://openalex.org/W2936774411', 'https://openalex.org/W2781384251', 'https://openalex.org/W3015995734', 'https://openalex.org/W2088622183', 'https://openalex.org/W1828163288', 'https://openalex.org/W3001197829', 'https://openalex.org/W2962369866', 'https://openalex.org/W2962911098', 'https://openalex.org/W2111316763', 'https://openalex.org/W2991213871', 'https://openalex.org/W3016010032', 'https://openalex.org/W2193413348', 'https://openalex.org/W2987806402', 'https://openalex.org/W3103005696', 'https://openalex.org/W2963341956', 'https://openalex.org/W3015265920', 'https://openalex.org/W2926827382', 'https://openalex.org/W3036601975', 'https://openalex.org/W2979476256']",2020-10-20
https://openalex.org/W3099782249,,wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations,,[],2020-06-20
https://openalex.org/W2095705004,,Dropout: a simple way to prevent neural networks from overfitting,"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned ” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.","['https://openalex.org/W2085040216', 'https://openalex.org/W2335728318', 'https://openalex.org/W2611675901', 'https://openalex.org/W2096873754', 'https://openalex.org/W1993882792', 'https://openalex.org/W189596042', 'https://openalex.org/W35527955', 'https://openalex.org/W2963574257', 'https://openalex.org/W2053229256', 'https://openalex.org/W2147800946', 'https://openalex.org/W2136922672', 'https://openalex.org/W2103359087', 'https://openalex.org/W2100495367', 'https://openalex.org/W2971788173', 'https://openalex.org/W1567512734', 'https://openalex.org/W2546302380', 'https://openalex.org/W1492459858', 'https://openalex.org/W2135046866', 'https://openalex.org/W2145094598', 'https://openalex.org/W2150717117', 'https://openalex.org/W1524333225', 'https://openalex.org/W2163605009', 'https://openalex.org/W137106866', 'https://openalex.org/W2183112036', 'https://openalex.org/W2294059674', 'https://openalex.org/W2156163116', 'https://openalex.org/W3118608800', 'https://openalex.org/W2156297475', 'https://openalex.org/W2962820688', 'https://openalex.org/W2152722485', 'https://openalex.org/W2025768430', 'https://openalex.org/W2114296159', 'https://openalex.org/W2158542502', 'https://openalex.org/W2114733238', 'https://openalex.org/W2949821452', 'https://openalex.org/W2131241448']",2014-01-01
https://openalex.org/W2972943112,https://doi.org/10.21437/interspeech.2019-1473,An Unsupervised Autoregressive Model for Speech Representation Learning,"This paper proposes a novel unsupervised autoregressive neural model for learning generic speech representations.In contrast to other speech representation learning methods that aim to remove noise or speaker variabilities, ours is designed to preserve information for a wide range of downstream tasks.In addition, the proposed model does not require any phonetic or word boundary labels, allowing the model to benefit from large quantities of unlabeled data.Speech representations learned by our model significantly improve performance on both phone classification and speaker verification over the surface features and other supervised and unsupervised approaches.Further analysis shows that different levels of speech information are captured by our model at different layers.In particular, the lower layers tend to be more discriminative for speakers, while the upper layers provide more phonetic content.","['https://openalex.org/W1522301498', 'https://openalex.org/W2962850167', 'https://openalex.org/W179875071', 'https://openalex.org/W2963571336', 'https://openalex.org/W2525778437', 'https://openalex.org/W2951585248', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962739339', 'https://openalex.org/W2997574889', 'https://openalex.org/W343636949', 'https://openalex.org/W2599837529', 'https://openalex.org/W2550241133', 'https://openalex.org/W2758785877', 'https://openalex.org/W2150769028', 'https://openalex.org/W2888329843', 'https://openalex.org/W2519091744', 'https://openalex.org/W219040644', 'https://openalex.org/W2963425185', 'https://openalex.org/W4297808394', 'https://openalex.org/W2963026768', 'https://openalex.org/W2963317665', 'https://openalex.org/W2963609956', 'https://openalex.org/W1686946872', 'https://openalex.org/W2190506272', 'https://openalex.org/W3125709657', 'https://openalex.org/W2024490156', 'https://openalex.org/W2194775991', 'https://openalex.org/W2896457183', 'https://openalex.org/W2827410935', 'https://openalex.org/W2395899413']",2019-09-13
https://openalex.org/W1957665339,https://doi.org/10.1109/icassp.1988.196629,A segment model based approach to speech recognition,"Proposes a global acoustic segment model for characterizing fundamental speech sound units and their interactions based upon a general framework of hidden Markov models (HMM). Each segment model represents a class of acoustically similar sounds. The intra-segment variability of each sound class is modeled by an HMM, and the sound-to-sound transition rules are characterized by a probabilistic intersegment transition matrix. An acoustically-derived lexicon is used to construct word models based upon subword segment models. The proposed segment model was tested on a speaker-trained, isolated word, speech recognition task with a vocabulary of 1109 basic English words. In the current study, only 128 segment models were used, and recognition was performed by optimally aligning the test utterance with all acoustic lexicon entries using a maximum likelihood Viterbi decoding algorithm. Based upon a database of three male speakers, the average word recognition accuracy for the top candidate was 85% and increased to 96% and 98% for the top 3 and top 5 candidates, respectively.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>","['https://openalex.org/W2134383396', 'https://openalex.org/W1950396994', 'https://openalex.org/W1987294319', 'https://openalex.org/W6779487094', 'https://openalex.org/W3035139526', 'https://openalex.org/W596388300']",2003-01-06
https://openalex.org/W2594951208,https://doi.org/10.1109/iscslp.2016.7918442,Exploiting language-mismatched phoneme recognizers for unsupervised acoustic modeling,"This paper describes an investigation on acoustic modeling in the absence of transcribed training data. We propose to use language-mismatched phoneme recognizers to assist unsupervised segmentation and segment clustering of a new language. Using a language-mismatched recognizer, an input utterance is divided into many variable-length segments. Each segment is represented by a feature vector that is derived from the phoneme posterior probabilities. A spectral clustering algorithm is developed to group the segments into a prescribed number of clusters, which represent a set of basic speech units in the target language. By exploiting multiple recognizers for different languages, a wider phonetic space can be covered, leading to improved performance of segmentation and clustering. Experimental results on a multilingual speech database confirm the effectiveness of the proposed method.","['https://openalex.org/W2106284094', 'https://openalex.org/W1975728937', 'https://openalex.org/W185189022', 'https://openalex.org/W2519200395', 'https://openalex.org/W2055408826', 'https://openalex.org/W4213009331', 'https://openalex.org/W1967924372', 'https://openalex.org/W1578200545', 'https://openalex.org/W1957665339', 'https://openalex.org/W2100768664']",2016-10-01
https://openalex.org/W2962693497,https://doi.org/10.1109/icassp.2018.8461545,Bayesian Models for Unit Discovery on a Very Low Resource Language,Accepted to ICASSP 2018,"['https://openalex.org/W6640963894', 'https://openalex.org/W1516111018', 'https://openalex.org/W6704885441', 'https://openalex.org/W6705006301', 'https://openalex.org/W2564058731', 'https://openalex.org/W2140991203', 'https://openalex.org/W2142390309', 'https://openalex.org/W6739631751', 'https://openalex.org/W6740233529', 'https://openalex.org/W2750248772', 'https://openalex.org/W6719357382', 'https://openalex.org/W6606665087', 'https://openalex.org/W2033413759', 'https://openalex.org/W2347098582', 'https://openalex.org/W6675022971', 'https://openalex.org/W6638813818', 'https://openalex.org/W2126377586', 'https://openalex.org/W2963620343', 'https://openalex.org/W6691362072', 'https://openalex.org/W2057007397', 'https://openalex.org/W2345799635', 'https://openalex.org/W2719865699', 'https://openalex.org/W2251025892', 'https://openalex.org/W1959608418', 'https://openalex.org/W4294562888', 'https://openalex.org/W4300047444', 'https://openalex.org/W166614460', 'https://openalex.org/W2347145335', 'https://openalex.org/W1833498382', 'https://openalex.org/W2762715843', 'https://openalex.org/W2464234964', 'https://openalex.org/W1506806321', 'https://openalex.org/W2100768664', 'https://openalex.org/W4237840503', 'https://openalex.org/W2641832364']",2018-04-01
https://openalex.org/W2963799213,https://doi.org/10.48550/arxiv.1711.00937,Neural Discrete Representation Learning,"Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of ""posterior collapse"" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.",[],2017-11-02
https://openalex.org/W2972794572,https://doi.org/10.21437/interspeech.2019-2981,Unsupervised Acoustic Segmentation and Clustering Using Siamese Network Embeddings,,"['https://openalex.org/W2052697931', 'https://openalex.org/W126222424', 'https://openalex.org/W2166637769', 'https://openalex.org/W2190506272', 'https://openalex.org/W2963620343', 'https://openalex.org/W2747192917', 'https://openalex.org/W2126203737', 'https://openalex.org/W2020607164', 'https://openalex.org/W2170659185', 'https://openalex.org/W2251025892', 'https://openalex.org/W2404799143', 'https://openalex.org/W1796128977', 'https://openalex.org/W1545920196', 'https://openalex.org/W2100768664', 'https://openalex.org/W2160815625', 'https://openalex.org/W2468716020', 'https://openalex.org/W2398490608', 'https://openalex.org/W2964169922']",2019-09-13
https://openalex.org/W3044483536,https://doi.org/10.21437/interspeech.2020-1170,Unsupervised Subword Modeling Using Autoregressive Pretraining and Cross-Lingual Phone-Aware Modeling,"This study addresses unsupervised subword modeling, i.e., learning feature\nrepresentations that can distinguish subword units of a language. The proposed\napproach adopts a two-stage bottleneck feature (BNF) learning framework,\nconsisting of autoregressive predictive coding (APC) as a front-end and a\nDNN-BNF model as a back-end. APC pretrained features are set as input features\nto a DNN-BNF model. A language-mismatched ASR system is used to provide\ncross-lingual phone labels for DNN-BNF model training. Finally, BNFs are\nextracted as the subword-discriminative feature representation. A second aim of\nthis work is to investigate the robustness of our approach's effectiveness to\ndifferent amounts of training data. The results on Libri-light and the\nZeroSpeech 2017 databases show that APC is effective in front-end feature\npretraining. Our whole system outperforms the state of the art on both\ndatabases. Cross-lingual phone labels for English data by a Dutch ASR\noutperform those by a Mandarin ASR, possibly linked to the larger similarity of\nDutch compared to Mandarin with English. Our system is less sensitive to\ntraining data amount when the training data is over 50 hours. APC pretraining\nleads to a reduction of needed training material from over 5,000 hours to\naround 200 hours with little performance degradation.\n","['https://openalex.org/W2012897754', 'https://openalex.org/W2064675550', 'https://openalex.org/W2995181338', 'https://openalex.org/W4297808394', 'https://openalex.org/W2963618559', 'https://openalex.org/W2842511635', 'https://openalex.org/W29952999', 'https://openalex.org/W2786902352', 'https://openalex.org/W162588823', 'https://openalex.org/W3016181583', 'https://openalex.org/W2964121744', 'https://openalex.org/W2927191280', 'https://openalex.org/W2468716020', 'https://openalex.org/W1522301498', 'https://openalex.org/W2950414763', 'https://openalex.org/W2972943112', 'https://openalex.org/W2963799213', 'https://openalex.org/W1524333225', 'https://openalex.org/W2787426069', 'https://openalex.org/W3104842308', 'https://openalex.org/W2949510815', 'https://openalex.org/W2758785877', 'https://openalex.org/W2085628288', 'https://openalex.org/W1494198834', 'https://openalex.org/W2293634267', 'https://openalex.org/W3016011332', 'https://openalex.org/W2399576818', 'https://openalex.org/W2786608204', 'https://openalex.org/W3125709657', 'https://openalex.org/W2972374322', 'https://openalex.org/W2963620343', 'https://openalex.org/W4300047444', 'https://openalex.org/W2514741789', 'https://openalex.org/W2787447541']",2020-10-25
https://openalex.org/W2147768505,https://doi.org/10.1109/tasl.2011.2134090,Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition,"We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance. Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8% and 9.2% (or relative error reduction of 16.0% and 23.2%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum-likelihood (ML) criteria, respectively.","['https://openalex.org/W2151231072', 'https://openalex.org/W6630673164', 'https://openalex.org/W2131033001', 'https://openalex.org/W2155445312', 'https://openalex.org/W2096170322', 'https://openalex.org/W2037740282', 'https://openalex.org/W2156373089', 'https://openalex.org/W2160373860', 'https://openalex.org/W2147627917', 'https://openalex.org/W6719392674', 'https://openalex.org/W2159501365', 'https://openalex.org/W6680300913', 'https://openalex.org/W6675321185', 'https://openalex.org/W2141778357', 'https://openalex.org/W2040118574', 'https://openalex.org/W2099832642', 'https://openalex.org/W2025768430', 'https://openalex.org/W2100495367', 'https://openalex.org/W4231109964', 'https://openalex.org/W1498436455', 'https://openalex.org/W6602970364', 'https://openalex.org/W2106004777', 'https://openalex.org/W2117130368', 'https://openalex.org/W6638304892', 'https://openalex.org/W2546302380', 'https://openalex.org/W2116064496', 'https://openalex.org/W6678801095', 'https://openalex.org/W6608710415', 'https://openalex.org/W2084514013', 'https://openalex.org/W2125964738', 'https://openalex.org/W2148632119', 'https://openalex.org/W2156615793', 'https://openalex.org/W2136922672', 'https://openalex.org/W178496478', 'https://openalex.org/W2913932916', 'https://openalex.org/W6683819664', 'https://openalex.org/W2012897754', 'https://openalex.org/W2102264166', 'https://openalex.org/W1993882792', 'https://openalex.org/W6608133726', 'https://openalex.org/W6677604277', 'https://openalex.org/W1533861849', 'https://openalex.org/W2159948109', 'https://openalex.org/W1517381888', 'https://openalex.org/W132821814', 'https://openalex.org/W177307080', 'https://openalex.org/W6682143068', 'https://openalex.org/W2141969549', 'https://openalex.org/W811578723', 'https://openalex.org/W6675604347', 'https://openalex.org/W6825030834', 'https://openalex.org/W2168077532', 'https://openalex.org/W2022768064', 'https://openalex.org/W6679825673', 'https://openalex.org/W2155045454', 'https://openalex.org/W2103283951', 'https://openalex.org/W2125234026', 'https://openalex.org/W1983334819', 'https://openalex.org/W2131700150', 'https://openalex.org/W42107399', 'https://openalex.org/W6606278960', 'https://openalex.org/W2027915610', 'https://openalex.org/W2024539680', 'https://openalex.org/W2021396747', 'https://openalex.org/W2158289097', 'https://openalex.org/W2115439799', 'https://openalex.org/W6682398751', 'https://openalex.org/W2158808283', 'https://openalex.org/W2165712214', 'https://openalex.org/W1972845249', 'https://openalex.org/W2070725064', 'https://openalex.org/W2147794814', 'https://openalex.org/W2071489795', 'https://openalex.org/W2093794678', 'https://openalex.org/W2169189000', 'https://openalex.org/W2118716571', 'https://openalex.org/W6638054831', 'https://openalex.org/W2103359087', 'https://openalex.org/W2131344613', 'https://openalex.org/W2124914669', 'https://openalex.org/W73112891', 'https://openalex.org/W137106866', 'https://openalex.org/W1813659000', 'https://openalex.org/W2148357004', 'https://openalex.org/W2161893161', 'https://openalex.org/W2465919943', 'https://openalex.org/W217970951', 'https://openalex.org/W4243847155', 'https://openalex.org/W2125569215', 'https://openalex.org/W2606321545', 'https://openalex.org/W2138857742', 'https://openalex.org/W1553004968', 'https://openalex.org/W2150907703', 'https://openalex.org/W2613634265', 'https://openalex.org/W2990138404', 'https://openalex.org/W2072128103', 'https://openalex.org/W2135341757', 'https://openalex.org/W2546191734', 'https://openalex.org/W2169434751', 'https://openalex.org/W1526741802', 'https://openalex.org/W2117499988', 'https://openalex.org/W196761320', 'https://openalex.org/W2397682113', 'https://openalex.org/W1792764070', 'https://openalex.org/W2148177126', 'https://openalex.org/W155708904', 'https://openalex.org/W1513862252', 'https://openalex.org/W2104997912', 'https://openalex.org/W246799964']",2011-04-06
https://openalex.org/W2347145335,https://doi.org/10.1016/j.procs.2016.04.023,Breaking the Unwritten Language Barrier: The BULB Project,International audience,"['https://openalex.org/W4388277788', 'https://openalex.org/W2397643736', 'https://openalex.org/W1712048714', 'https://openalex.org/W2132534451', 'https://openalex.org/W2101281673', 'https://openalex.org/W6652311901', 'https://openalex.org/W2156985047', 'https://openalex.org/W191292882', 'https://openalex.org/W2063655091', 'https://openalex.org/W141348503', 'https://openalex.org/W2095907708', 'https://openalex.org/W2530395991', 'https://openalex.org/W2122228338', 'https://openalex.org/W2140991203', 'https://openalex.org/W2601422447', 'https://openalex.org/W2117126688', 'https://openalex.org/W1969608442', 'https://openalex.org/W2166270474', 'https://openalex.org/W1885617648', 'https://openalex.org/W1993799394', 'https://openalex.org/W2345799635', 'https://openalex.org/W289413641', 'https://openalex.org/W2092052114', 'https://openalex.org/W6610006008', 'https://openalex.org/W2162465526', 'https://openalex.org/W192980855', 'https://openalex.org/W65599759', 'https://openalex.org/W3040954139', 'https://openalex.org/W2160131530', 'https://openalex.org/W2251146352', 'https://openalex.org/W1577146168', 'https://openalex.org/W2014840417', 'https://openalex.org/W2055535974', 'https://openalex.org/W2006969979', 'https://openalex.org/W1689157505', 'https://openalex.org/W182534140', 'https://openalex.org/W2251001376']",2016-01-01
https://openalex.org/W3093096176,https://doi.org/10.21437/interspeech.2020-2743,The Zero Resource Speech Challenge 2020: Discovering Discrete Subword and Word Units,"We present the Zero Resource Speech Challenge 2020, which aims at learning speech representations from raw audio signals without any labels. It combines the data sets and metrics from two previous benchmarks (2017 and 2019) and features two tasks which tap into two levels of speech representation. The first task is to discover low bit-rate subword representations that optimize the quality of speech synthesis; the second one is to discover word-like units from unsegmented raw speech. We present the results of the twenty submitted models and discuss the implications of the main findings for unsupervised speech learning.",[],2020-10-25
https://openalex.org/W2101234009,https://doi.org/10.48550/arxiv.1201.0490,Scikit-learn: Machine Learning in Python,"Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.org.","['https://openalex.org/W2118585731', 'https://openalex.org/W2040387238', 'https://openalex.org/W1496508106', 'https://openalex.org/W2153635508', 'https://openalex.org/W2146292423', 'https://openalex.org/W2047804403', 'https://openalex.org/W2024933578', 'https://openalex.org/W2035776949', 'https://openalex.org/W2097360283', 'https://openalex.org/W2152799677', 'https://openalex.org/W2063978378', 'https://openalex.org/W2097850441', 'https://openalex.org/W1571024744']",2012-01-02
https://openalex.org/W3045485643,,Handbook of the International Phonetic Association: A Guide to the Use of the International Phonetic Alphabet,,[],1999-06-28
https://openalex.org/W2950414763,https://doi.org/10.21437/interspeech.2019-1337,Combining Adversarial Training and Disentangled Speech Representation for Robust Zero-Resource Subword Modeling,"This study addresses the problem of unsupervised subword unit discovery from untranscribed speech. It forms the basis of the ultimate goal of ZeroSpeech 2019, building text-to-speech systems without text labels. In this work, unit discovery is formulated as a pipeline of phonetically discriminative feature learning and unit inference. One major difficulty in robust unsupervised feature learning is dealing with speaker variation. Here the robustness towards speaker variation is achieved by applying adversarial training and FHVAE based disentangled speech representation learning. A comparison of the two approaches as well as their combination is studied in a DNN-bottleneck feature (DNN-BNF) architecture. Experiments are conducted on ZeroSpeech 2019 and 2017. Experimental results on ZeroSpeech 2017 show that both approaches are effective while the latter is more prominent, and that their combination brings further marginal improvement in across-speaker condition. Results on ZeroSpeech 2019 show that in the ABX discriminability task, our approaches significantly outperform the official baseline, and are competitive to or even outperform the official topline. The proposed unit sequence smoothing algorithm improves synthesis quality, at a cost of slight decrease in ABX discriminability.","['https://openalex.org/W2516890051', 'https://openalex.org/W1882958252', 'https://openalex.org/W2826003142', 'https://openalex.org/W2128032727', 'https://openalex.org/W2786608204', 'https://openalex.org/W1545920196', 'https://openalex.org/W2399576818', 'https://openalex.org/W2963620343', 'https://openalex.org/W2758785877', 'https://openalex.org/W2598638573', 'https://openalex.org/W2964121744', 'https://openalex.org/W2889228998', 'https://openalex.org/W2940544976', 'https://openalex.org/W1522301498', 'https://openalex.org/W2786902352', 'https://openalex.org/W4288107125', 'https://openalex.org/W2949510815', 'https://openalex.org/W2796339975', 'https://openalex.org/W4289564011', 'https://openalex.org/W2587088898', 'https://openalex.org/W2906459023', 'https://openalex.org/W2402144811', 'https://openalex.org/W1796128977', 'https://openalex.org/W2963618559', 'https://openalex.org/W3125709657', 'https://openalex.org/W2547039119', 'https://openalex.org/W2787426069', 'https://openalex.org/W1524333225', 'https://openalex.org/W2785860501', 'https://openalex.org/W2936120996', 'https://openalex.org/W2787447541', 'https://openalex.org/W2404799143', 'https://openalex.org/W2953384591', 'https://openalex.org/W4300047444', 'https://openalex.org/W2963826681']",2019-09-13
https://openalex.org/W3100202343,https://doi.org/10.1109/icassp39728.2021.9414899,A Hierarchical Subspace Model for Language-Attuned Acoustic Unit Discovery,"In this work, we propose a hierarchical subspace model for acoustic unit discovery. In this approach, we frame the task as one of learning embeddings on a low-dimensional phonetic subspace, and simultaneously specify the subspace itself as an embedding on a hyper-subspace. We train the hyper-subspace on a set of transcribed languages and transfer it to the target language. In the target language, we infer both the language and unit embeddings in an unsupervised manner, and in so doing, we simultaneously learn a subspace of units specific to that language and the units that dwell on it. We conduct our experiments on TIMIT and two low-resource languages: Mboshi and Yoruba. Results show that our model outperforms major acoustic unit discovery techniques, both in terms of clustering quality and segmentation accuracy.","['https://openalex.org/W6679792166', 'https://openalex.org/W1494198834', 'https://openalex.org/W2786902352', 'https://openalex.org/W2888911345', 'https://openalex.org/W2927191280', 'https://openalex.org/W6865393803', 'https://openalex.org/W2962693497', 'https://openalex.org/W2641832364', 'https://openalex.org/W6640963894', 'https://openalex.org/W1981706894', 'https://openalex.org/W6744702808', 'https://openalex.org/W2940544976', 'https://openalex.org/W2127498532', 'https://openalex.org/W2963620343', 'https://openalex.org/W6769196770', 'https://openalex.org/W3100270690', 'https://openalex.org/W6770596778', 'https://openalex.org/W6675022971', 'https://openalex.org/W2752796333', 'https://openalex.org/W6973666849', 'https://openalex.org/W2347098582', 'https://openalex.org/W2483390977', 'https://openalex.org/W3092791109', 'https://openalex.org/W2084534958', 'https://openalex.org/W6712757354', 'https://openalex.org/W2195354', 'https://openalex.org/W6712648922', 'https://openalex.org/W6731521493', 'https://openalex.org/W2973026522', 'https://openalex.org/W1522301498', 'https://openalex.org/W2100768664', 'https://openalex.org/W2401396251', 'https://openalex.org/W2972574141', 'https://openalex.org/W2963799213', 'https://openalex.org/W2996383576', 'https://openalex.org/W2401271873', 'https://openalex.org/W2762715843', 'https://openalex.org/W2995680346', 'https://openalex.org/W2572097499', 'https://openalex.org/W2786608204', 'https://openalex.org/W1959608418', 'https://openalex.org/W2134670479']",2021-05-13
https://openalex.org/W2750248772,https://doi.org/10.21437/interspeech.2017-1160,Hidden Markov Model Variational Autoencoder for Acoustic Unit Discovery,"Variational Autoencoders (VAEs) have been shown to provide efficient neural-network-based approximate Bayesian inference for observation models for which exact inference is intractable. Its extension, the so-called Structured VAE (SVAE) allows inference in the presence of both discrete and continuous latent variables. Inspired by this extension, we developed a VAE with Hidden Markov Models (HMMs) as latent models. We applied the resulting HMM-VAE to the task of acoustic unit discovery in a zero resource scenario. Starting from an initial model based on variational inference in an HMM with Gaussian Mixture Model (GMM) emission probabilities, the accuracy of the acoustic unit discovery could be significantly improved by the HMM-VAE. In doing so we were able to demonstrate for an unsupervised learning task what is well-known in the supervised learning case: Neural networks provide superior modeling power compared to GMMs.","['https://openalex.org/W1796128977', 'https://openalex.org/W2468716020', 'https://openalex.org/W2159283619', 'https://openalex.org/W2078769636', 'https://openalex.org/W2347098582', 'https://openalex.org/W2962695963', 'https://openalex.org/W2117041980', 'https://openalex.org/W1971081490', 'https://openalex.org/W2142384583', 'https://openalex.org/W2556467266', 'https://openalex.org/W2100768664', 'https://openalex.org/W2077804127', 'https://openalex.org/W2949416428']",2017-08-16
https://openalex.org/W1557247526,https://doi.org/10.1017/cbo9780511975981,The Cambridge Handbook of Endangered Languages,"It is generally agreed that about 7,000 languages are spoken across the world today and at least half may no longer be spoken by the end of this century. This state-of-the-art Handbook examines the reasons behind this dramatic loss of linguistic diversity, why it matters, and what can be done to document and support endangered languages. The volume is relevant not only to researchers in language endangerment, language shift and language death, but to anyone interested in the languages and cultures of the world. It is accessible both to specialists and non-specialists: researchers will find cutting-edge contributions from acknowledged experts in their fields, while students, activists and other interested readers will find a wealth of readable yet thorough and up-to-date information","['https://openalex.org/W154663075', 'https://openalex.org/W2003370336', 'https://openalex.org/W1572930218', 'https://openalex.org/W4388346577', 'https://openalex.org/W2090589756', 'https://openalex.org/W4246293254', 'https://openalex.org/W2116789377', 'https://openalex.org/W4246050336', 'https://openalex.org/W1504666850', 'https://openalex.org/W3119260024', 'https://openalex.org/W2021486712', 'https://openalex.org/W2564006213', 'https://openalex.org/W2156717791', 'https://openalex.org/W2063971808', 'https://openalex.org/W2020145776', 'https://openalex.org/W4242707637', 'https://openalex.org/W2054222589', 'https://openalex.org/W2332674500', 'https://openalex.org/W6640713718', 'https://openalex.org/W1026484344', 'https://openalex.org/W4300386811', 'https://openalex.org/W2039960521', 'https://openalex.org/W7038976351', 'https://openalex.org/W4246491833', 'https://openalex.org/W1491507086', 'https://openalex.org/W2113756706', 'https://openalex.org/W2134567606', 'https://openalex.org/W6676376104', 'https://openalex.org/W2009074326', 'https://openalex.org/W2074298458', 'https://openalex.org/W4239035862', 'https://openalex.org/W4206928602', 'https://openalex.org/W1974419670', 'https://openalex.org/W6756979935', 'https://openalex.org/W1971802393', 'https://openalex.org/W2086616868', 'https://openalex.org/W2036911256', 'https://openalex.org/W4230530027', 'https://openalex.org/W2119320261', 'https://openalex.org/W2052845081', 'https://openalex.org/W2130745087', 'https://openalex.org/W2139271273', 'https://openalex.org/W2049031397', 'https://openalex.org/W615303599', 'https://openalex.org/W1590416689', 'https://openalex.org/W1560540151', 'https://openalex.org/W2065786033', 'https://openalex.org/W7019824772', 'https://openalex.org/W4253736497', 'https://openalex.org/W2145819899', 'https://openalex.org/W2039407758', 'https://openalex.org/W2040782685', 'https://openalex.org/W6681793756', 'https://openalex.org/W3159666107', 'https://openalex.org/W4250322288', 'https://openalex.org/W1996024969', 'https://openalex.org/W2154378727', 'https://openalex.org/W2127834163', 'https://openalex.org/W2123133589', 'https://openalex.org/W2013635347', 'https://openalex.org/W2100012790', 'https://openalex.org/W2014688734', 'https://openalex.org/W1995493685', 'https://openalex.org/W2577241600', 'https://openalex.org/W419325547', 'https://openalex.org/W1967444345', 'https://openalex.org/W6713421447', 'https://openalex.org/W1987754365', 'https://openalex.org/W2114520262', 'https://openalex.org/W6701094194', 'https://openalex.org/W2011957947', 'https://openalex.org/W2002902611', 'https://openalex.org/W6655235958', 'https://openalex.org/W2166784187', 'https://openalex.org/W2030008308', 'https://openalex.org/W1785678793', 'https://openalex.org/W2015354337', 'https://openalex.org/W2037283028', 'https://openalex.org/W2070460091', 'https://openalex.org/W7010918937', 'https://openalex.org/W4388117266', 'https://openalex.org/W6724430021', 'https://openalex.org/W1956884327', 'https://openalex.org/W2004352782', 'https://openalex.org/W2162830027', 'https://openalex.org/W4255832261', 'https://openalex.org/W4298849138', 'https://openalex.org/W6826770781', 'https://openalex.org/W2016625141', 'https://openalex.org/W2328007085', 'https://openalex.org/W2043821322', 'https://openalex.org/W217343055', 'https://openalex.org/W4249296716', 'https://openalex.org/W6722785364', 'https://openalex.org/W6992253608', 'https://openalex.org/W2036104176', 'https://openalex.org/W1979126885', 'https://openalex.org/W2026997813', 'https://openalex.org/W2132583302', 'https://openalex.org/W2079695567', 'https://openalex.org/W1979080086', 'https://openalex.org/W1486697269', 'https://openalex.org/W2064913606', 'https://openalex.org/W4206969047', 'https://openalex.org/W2477678498', 'https://openalex.org/W6832991761', 'https://openalex.org/W2002598399', 'https://openalex.org/W6667871521', 'https://openalex.org/W2000196122', 'https://openalex.org/W2090889119', 'https://openalex.org/W1547654455', 'https://openalex.org/W1489670474', 'https://openalex.org/W2106238400', 'https://openalex.org/W1994739812', 'https://openalex.org/W4289857609', 'https://openalex.org/W2138097116', 'https://openalex.org/W2053219914', 'https://openalex.org/W4231813407', 'https://openalex.org/W2163280776', 'https://openalex.org/W6752348048', 'https://openalex.org/W2168314013', 'https://openalex.org/W4243756587', 'https://openalex.org/W2094816852', 'https://openalex.org/W2025141507', 'https://openalex.org/W2130171177', 'https://openalex.org/W6609491109', 'https://openalex.org/W1974099575', 'https://openalex.org/W1852515163', 'https://openalex.org/W2327524449', 'https://openalex.org/W6689950238', 'https://openalex.org/W2092402023', 'https://openalex.org/W1987360898', 'https://openalex.org/W2038798701', 'https://openalex.org/W2071759089', 'https://openalex.org/W6700433298', 'https://openalex.org/W4255909688', 'https://openalex.org/W2099737558', 'https://openalex.org/W1985177116', 'https://openalex.org/W1981224570', 'https://openalex.org/W2060952241', 'https://openalex.org/W1548472825', 'https://openalex.org/W2075134643', 'https://openalex.org/W6729755211', 'https://openalex.org/W1983888999', 'https://openalex.org/W1499529646', 'https://openalex.org/W6724335368', 'https://openalex.org/W2074642687', 'https://openalex.org/W4388122583', 'https://openalex.org/W1998845929', 'https://openalex.org/W604520381', 'https://openalex.org/W4388277788', 'https://openalex.org/W1974696257', 'https://openalex.org/W625686783', 'https://openalex.org/W4240364066', 'https://openalex.org/W2948199331', 'https://openalex.org/W1974209402', 'https://openalex.org/W746375014', 'https://openalex.org/W6731083333', 'https://openalex.org/W1606241218', 'https://openalex.org/W184370164', 'https://openalex.org/W6648000555', 'https://openalex.org/W2504968574', 'https://openalex.org/W2094691449', 'https://openalex.org/W6637785039', 'https://openalex.org/W2069011018', 'https://openalex.org/W2061272101', 'https://openalex.org/W2121042406', 'https://openalex.org/W4236479194', 'https://openalex.org/W1523961089', 'https://openalex.org/W2022046414', 'https://openalex.org/W4301056702', 'https://openalex.org/W4240298715', 'https://openalex.org/W1964186809', 'https://openalex.org/W2041853932', 'https://openalex.org/W2086921673', 'https://openalex.org/W4206904392', 'https://openalex.org/W2141018189', 'https://openalex.org/W6721843415', 'https://openalex.org/W2146175865', 'https://openalex.org/W2105799497', 'https://openalex.org/W2058079578', 'https://openalex.org/W4231567758', 'https://openalex.org/W2464008132', 'https://openalex.org/W4388122691', 'https://openalex.org/W4232134068', 'https://openalex.org/W2032479376', 'https://openalex.org/W4388343465', 'https://openalex.org/W4407145287', 'https://openalex.org/W4300481244', 'https://openalex.org/W2058239805', 'https://openalex.org/W4254871114', 'https://openalex.org/W4235099738', 'https://openalex.org/W4251880587', 'https://openalex.org/W6678193663', 'https://openalex.org/W2144965263', 'https://openalex.org/W2057421777', 'https://openalex.org/W2141069019', 'https://openalex.org/W1982954510', 'https://openalex.org/W2125647233', 'https://openalex.org/W2094077495', 'https://openalex.org/W2050525805', 'https://openalex.org/W1987981437', 'https://openalex.org/W1515810707', 'https://openalex.org/W2916440619', 'https://openalex.org/W1973086277', 'https://openalex.org/W2020683187', 'https://openalex.org/W2169871554', 'https://openalex.org/W2097204143', 'https://openalex.org/W2065699009', 'https://openalex.org/W2043053864', 'https://openalex.org/W6720263907', 'https://openalex.org/W238590562', 'https://openalex.org/W2049667129', 'https://openalex.org/W2172287600', 'https://openalex.org/W1977578854', 'https://openalex.org/W2096301754', 'https://openalex.org/W1544229832', 'https://openalex.org/W4205144645', 'https://openalex.org/W2038278196', 'https://openalex.org/W2142087888', 'https://openalex.org/W6632486973', 'https://openalex.org/W4376475474', 'https://openalex.org/W4250716560', 'https://openalex.org/W2071930355', 'https://openalex.org/W1991561267', 'https://openalex.org/W2078333088', 'https://openalex.org/W2482069464', 'https://openalex.org/W2047989434', 'https://openalex.org/W2157904933', 'https://openalex.org/W1967094207', 'https://openalex.org/W2156153917', 'https://openalex.org/W4230452630', 'https://openalex.org/W2034533915', 'https://openalex.org/W2079144356', 'https://openalex.org/W4388117258', 'https://openalex.org/W2169814948', 'https://openalex.org/W569787612', 'https://openalex.org/W2170250595', 'https://openalex.org/W2111915329', 'https://openalex.org/W6603797618', 'https://openalex.org/W2342426630', 'https://openalex.org/W4248634141', 'https://openalex.org/W2009929646', 'https://openalex.org/W435038712', 'https://openalex.org/W6683844983', 'https://openalex.org/W4293582985', 'https://openalex.org/W2110939808', 'https://openalex.org/W2090386862', 'https://openalex.org/W614388687', 'https://openalex.org/W978767298', 'https://openalex.org/W2007907819', 'https://openalex.org/W2062639350', 'https://openalex.org/W2049907881', 'https://openalex.org/W2033587655', 'https://openalex.org/W4376475216', 'https://openalex.org/W2085851207', 'https://openalex.org/W2101500500', 'https://openalex.org/W4253917825', 'https://openalex.org/W2030348348', 'https://openalex.org/W1987417000', 'https://openalex.org/W2071736781', 'https://openalex.org/W2946940662', 'https://openalex.org/W4388129611', 'https://openalex.org/W4299311694', 'https://openalex.org/W3148931049', 'https://openalex.org/W1597085513', 'https://openalex.org/W135664851', 'https://openalex.org/W2024563586', 'https://openalex.org/W1963648969', 'https://openalex.org/W1981722800', 'https://openalex.org/W2497230682', 'https://openalex.org/W4388330668', 'https://openalex.org/W4212987881', 'https://openalex.org/W1560271729', 'https://openalex.org/W2067425266', 'https://openalex.org/W4205420192', 'https://openalex.org/W2021918949', 'https://openalex.org/W2948444341', 'https://openalex.org/W2045251381', 'https://openalex.org/W6706188931', 'https://openalex.org/W4388166710', 'https://openalex.org/W2245224115', 'https://openalex.org/W2072063440', 'https://openalex.org/W1984406562', 'https://openalex.org/W1985369060', 'https://openalex.org/W2620388127', 'https://openalex.org/W2004875992', 'https://openalex.org/W2108836181', 'https://openalex.org/W1975893016', 'https://openalex.org/W2294464570', 'https://openalex.org/W1512720232', 'https://openalex.org/W4210560522', 'https://openalex.org/W4234554352', 'https://openalex.org/W6837182118', 'https://openalex.org/W4300827321', 'https://openalex.org/W1992080516', 'https://openalex.org/W2077306531', 'https://openalex.org/W6617007890', 'https://openalex.org/W2040926585', 'https://openalex.org/W2025079147', 'https://openalex.org/W4210619526', 'https://openalex.org/W2039865461', 'https://openalex.org/W2024592825', 'https://openalex.org/W2040392560', 'https://openalex.org/W2077849470', 'https://openalex.org/W2494969504', 'https://openalex.org/W2051974815', 'https://openalex.org/W1972157449', 'https://openalex.org/W2034232325', 'https://openalex.org/W1974272028', 'https://openalex.org/W2067463418', 'https://openalex.org/W2130568803', 'https://openalex.org/W2130708203', 'https://openalex.org/W1504059513', 'https://openalex.org/W2315122114', 'https://openalex.org/W2158364633', 'https://openalex.org/W2145454375', 'https://openalex.org/W1026965472', 'https://openalex.org/W2098563971', 'https://openalex.org/W1993389049', 'https://openalex.org/W1967123613', 'https://openalex.org/W6654556557', 'https://openalex.org/W6724201950', 'https://openalex.org/W4206956780', 'https://openalex.org/W6739079366', 'https://openalex.org/W4206413123', 'https://openalex.org/W2045037618', 'https://openalex.org/W2041762427', 'https://openalex.org/W2002365936', 'https://openalex.org/W2506364653', 'https://openalex.org/W4255171975', 'https://openalex.org/W1588360982', 'https://openalex.org/W2064865952', 'https://openalex.org/W4211112906', 'https://openalex.org/W2036150574', 'https://openalex.org/W2318670788', 'https://openalex.org/W2090780154', 'https://openalex.org/W2075132952', 'https://openalex.org/W599351149', 'https://openalex.org/W1521255875', 'https://openalex.org/W2002065645', 'https://openalex.org/W2160804712', 'https://openalex.org/W4246190295', 'https://openalex.org/W4388119254', 'https://openalex.org/W2030721283', 'https://openalex.org/W3141925209', 'https://openalex.org/W1995359043', 'https://openalex.org/W1969707068', 'https://openalex.org/W6989290223', 'https://openalex.org/W2078416428', 'https://openalex.org/W1995777441', 'https://openalex.org/W2086212124', 'https://openalex.org/W2156972453', 'https://openalex.org/W4238722546', 'https://openalex.org/W4211105222', 'https://openalex.org/W2016816178', 'https://openalex.org/W4249889109', 'https://openalex.org/W2084957543', 'https://openalex.org/W4205630731', 'https://openalex.org/W2018164523', 'https://openalex.org/W605911916', 'https://openalex.org/W2069296719', 'https://openalex.org/W636649538', 'https://openalex.org/W2066026993', 'https://openalex.org/W2102955653', 'https://openalex.org/W4300043747', 'https://openalex.org/W1965659603', 'https://openalex.org/W641800204', 'https://openalex.org/W4302470906', 'https://openalex.org/W2057473704', 'https://openalex.org/W1966884770', 'https://openalex.org/W2129914999', 'https://openalex.org/W1550391390', 'https://openalex.org/W2064134494', 'https://openalex.org/W6811050949', 'https://openalex.org/W294369058', 'https://openalex.org/W2043024108', 'https://openalex.org/W2089115516', 'https://openalex.org/W1970154918', 'https://openalex.org/W2040047873', 'https://openalex.org/W2028547979', 'https://openalex.org/W2064723607', 'https://openalex.org/W2068392174', 'https://openalex.org/W2062495510', 'https://openalex.org/W4376475319', 'https://openalex.org/W6690597596', 'https://openalex.org/W2047310534', 'https://openalex.org/W588312862', 'https://openalex.org/W6682815581', 'https://openalex.org/W6622081735', 'https://openalex.org/W6600165393', 'https://openalex.org/W4388174267', 'https://openalex.org/W4234904311', 'https://openalex.org/W644016886', 'https://openalex.org/W2144573879', 'https://openalex.org/W4301141443', 'https://openalex.org/W6629652747', 'https://openalex.org/W6660135082', 'https://openalex.org/W4256679411', 'https://openalex.org/W1574047653', 'https://openalex.org/W2012336572', 'https://openalex.org/W6722047715', 'https://openalex.org/W1981179783', 'https://openalex.org/W2145918021', 'https://openalex.org/W1974979268', 'https://openalex.org/W2023456589', 'https://openalex.org/W6668494604', 'https://openalex.org/W2114744303', 'https://openalex.org/W2077250480', 'https://openalex.org/W6636915900', 'https://openalex.org/W2289266088', 'https://openalex.org/W2144727423', 'https://openalex.org/W4246299772', 'https://openalex.org/W2147574427', 'https://openalex.org/W2000456814', 'https://openalex.org/W2095603794', 'https://openalex.org/W6664203646', 'https://openalex.org/W2045830397', 'https://openalex.org/W266716723', 'https://openalex.org/W2015449228', 'https://openalex.org/W4234035084', 'https://openalex.org/W4255064897', 'https://openalex.org/W2052679602', 'https://openalex.org/W7009074354', 'https://openalex.org/W2137327599', 'https://openalex.org/W6995672608', 'https://openalex.org/W2023999554', 'https://openalex.org/W3199235885', 'https://openalex.org/W2075371279', 'https://openalex.org/W2161410879', 'https://openalex.org/W1991549410', 'https://openalex.org/W4246169968', 'https://openalex.org/W1984934844', 'https://openalex.org/W4241524628', 'https://openalex.org/W4206342331', 'https://openalex.org/W6721338766', 'https://openalex.org/W2014840417', 'https://openalex.org/W2106228109', 'https://openalex.org/W4213254494', 'https://openalex.org/W6672806835', 'https://openalex.org/W4300991363', 'https://openalex.org/W6622518206', 'https://openalex.org/W6644998818', 'https://openalex.org/W2045998208', 'https://openalex.org/W2314970197', 'https://openalex.org/W1532845219', 'https://openalex.org/W1594180829', 'https://openalex.org/W1589922002', 'https://openalex.org/W592148693', 'https://openalex.org/W565268756', 'https://openalex.org/W614761587', 'https://openalex.org/W4300939116', 'https://openalex.org/W4250819726', 'https://openalex.org/W4232547885', 'https://openalex.org/W588733776', 'https://openalex.org/W2612242964', 'https://openalex.org/W2382969794', 'https://openalex.org/W2479180839', 'https://openalex.org/W2560556331', 'https://openalex.org/W1525187948', 'https://openalex.org/W4299613993', 'https://openalex.org/W2321921660', 'https://openalex.org/W2147818125', 'https://openalex.org/W1587670526', 'https://openalex.org/W1824479642', 'https://openalex.org/W2034398662', 'https://openalex.org/W2039508670', 'https://openalex.org/W2023960466', 'https://openalex.org/W4401800908', 'https://openalex.org/W2054106423', 'https://openalex.org/W612177002', 'https://openalex.org/W4244759427', 'https://openalex.org/W2321368448', 'https://openalex.org/W1967199935', 'https://openalex.org/W2797446062', 'https://openalex.org/W2062783322', 'https://openalex.org/W4361868726', 'https://openalex.org/W2154239000', 'https://openalex.org/W2320868164', 'https://openalex.org/W2305441979', 'https://openalex.org/W577035194', 'https://openalex.org/W1972711991', 'https://openalex.org/W1495775177', 'https://openalex.org/W1556626764', 'https://openalex.org/W1577993600', 'https://openalex.org/W2060981625', 'https://openalex.org/W1503888610', 'https://openalex.org/W1678457318', 'https://openalex.org/W1592537130', 'https://openalex.org/W1526974435', 'https://openalex.org/W578607727', 'https://openalex.org/W620327823', 'https://openalex.org/W2034923633', 'https://openalex.org/W1578548807', 'https://openalex.org/W2135943618', 'https://openalex.org/W1562130571', 'https://openalex.org/W4214909032', 'https://openalex.org/W147871884', 'https://openalex.org/W2041197998', 'https://openalex.org/W2939053375', 'https://openalex.org/W627307370', 'https://openalex.org/W2117331106', 'https://openalex.org/W1494684063', 'https://openalex.org/W854432089', 'https://openalex.org/W1575010870', 'https://openalex.org/W1757912280', 'https://openalex.org/W635626261', 'https://openalex.org/W579988415', 'https://openalex.org/W92505981', 'https://openalex.org/W161048274', 'https://openalex.org/W2586935691', 'https://openalex.org/W636414615', 'https://openalex.org/W1520437558', 'https://openalex.org/W2502529050', 'https://openalex.org/W656185092', 'https://openalex.org/W2147366595', 'https://openalex.org/W2065923122', 'https://openalex.org/W2730501584', 'https://openalex.org/W578369245', 'https://openalex.org/W2067140204', 'https://openalex.org/W134403060', 'https://openalex.org/W2057746856', 'https://openalex.org/W205415461', 'https://openalex.org/W2006531474', 'https://openalex.org/W4300005306', 'https://openalex.org/W2059586294', 'https://openalex.org/W591272886', 'https://openalex.org/W3180071353', 'https://openalex.org/W2243553570', 'https://openalex.org/W574683210', 'https://openalex.org/W2120942285', 'https://openalex.org/W1511971904', 'https://openalex.org/W4249997211', 'https://openalex.org/W358749868', 'https://openalex.org/W588745044', 'https://openalex.org/W1806327413', 'https://openalex.org/W1600079433', 'https://openalex.org/W2590412208', 'https://openalex.org/W4241435712', 'https://openalex.org/W2477771395', 'https://openalex.org/W2129092073', 'https://openalex.org/W565006054', 'https://openalex.org/W3104752447', 'https://openalex.org/W654937016', 'https://openalex.org/W1992817680', 'https://openalex.org/W2028282192', 'https://openalex.org/W2485212631', 'https://openalex.org/W1495375933', 'https://openalex.org/W590248509', 'https://openalex.org/W658826727', 'https://openalex.org/W641901253', 'https://openalex.org/W2163896222', 'https://openalex.org/W4452575', 'https://openalex.org/W1585299157', 'https://openalex.org/W2335285403', 'https://openalex.org/W2070951624', 'https://openalex.org/W4246773291', 'https://openalex.org/W622531438', 'https://openalex.org/W4313166971', 'https://openalex.org/W2107839072', 'https://openalex.org/W567951187', 'https://openalex.org/W1520776910', 'https://openalex.org/W401519424', 'https://openalex.org/W2240824655', 'https://openalex.org/W2795474186', 'https://openalex.org/W2047329470', 'https://openalex.org/W2054584708', 'https://openalex.org/W2326463343', 'https://openalex.org/W2006397122', 'https://openalex.org/W259473723', 'https://openalex.org/W1533653761', 'https://openalex.org/W1515537137', 'https://openalex.org/W2030996988', 'https://openalex.org/W228804211', 'https://openalex.org/W2090325723', 'https://openalex.org/W2613455522', 'https://openalex.org/W2063678023', 'https://openalex.org/W1982447291', 'https://openalex.org/W2070292027', 'https://openalex.org/W4233282516', 'https://openalex.org/W2049850256', 'https://openalex.org/W1517294575', 'https://openalex.org/W1531724895', 'https://openalex.org/W1971421484', 'https://openalex.org/W1996726160', 'https://openalex.org/W2904540331', 'https://openalex.org/W1540408279', 'https://openalex.org/W2028326564', 'https://openalex.org/W604415445', 'https://openalex.org/W567084793', 'https://openalex.org/W2123465109', 'https://openalex.org/W655805702', 'https://openalex.org/W1993833464', 'https://openalex.org/W2092223621', 'https://openalex.org/W2758808490', 'https://openalex.org/W581019459', 'https://openalex.org/W2513812005', 'https://openalex.org/W586200827', 'https://openalex.org/W336819675', 'https://openalex.org/W1544928458', 'https://openalex.org/W3131414776', 'https://openalex.org/W2302634985', 'https://openalex.org/W1543483555', 'https://openalex.org/W1996649194', 'https://openalex.org/W2797082695', 'https://openalex.org/W643919017', 'https://openalex.org/W2090300030', 'https://openalex.org/W4251938957', 'https://openalex.org/W2065859395', 'https://openalex.org/W2088998700', 'https://openalex.org/W2619844712', 'https://openalex.org/W1940534701', 'https://openalex.org/W1599047181', 'https://openalex.org/W1562010071', 'https://openalex.org/W595625263', 'https://openalex.org/W1987024261', 'https://openalex.org/W2010994950', 'https://openalex.org/W1529850385', 'https://openalex.org/W2474056321', 'https://openalex.org/W614921202', 'https://openalex.org/W4233644348', 'https://openalex.org/W2068250825', 'https://openalex.org/W4301093700', 'https://openalex.org/W3012133981', 'https://openalex.org/W1982926857', 'https://openalex.org/W1555780498', 'https://openalex.org/W1566384601', 'https://openalex.org/W2116752311', 'https://openalex.org/W589130460', 'https://openalex.org/W2057980114', 'https://openalex.org/W1771106021', 'https://openalex.org/W2606726050', 'https://openalex.org/W3082087892', 'https://openalex.org/W1483468372', 'https://openalex.org/W2726381464', 'https://openalex.org/W2076725206', 'https://openalex.org/W1562505411', 'https://openalex.org/W4249654474', 'https://openalex.org/W2324557238', 'https://openalex.org/W570225155', 'https://openalex.org/W1593079583', 'https://openalex.org/W4322392356', 'https://openalex.org/W2796500957', 'https://openalex.org/W1594774086', 'https://openalex.org/W2078633788', 'https://openalex.org/W923476341', 'https://openalex.org/W2168708086', 'https://openalex.org/W2014220952', 'https://openalex.org/W4230354800', 'https://openalex.org/W2259674250', 'https://openalex.org/W582882662', 'https://openalex.org/W2016811420', 'https://openalex.org/W1531512012', 'https://openalex.org/W1990290881', 'https://openalex.org/W2097200626', 'https://openalex.org/W653836325', 'https://openalex.org/W2070162908', 'https://openalex.org/W4250274388', 'https://openalex.org/W1979731710', 'https://openalex.org/W4298944483', 'https://openalex.org/W2773068894', 'https://openalex.org/W649436300', 'https://openalex.org/W2072578645', 'https://openalex.org/W2004038836', 'https://openalex.org/W1493697846', 'https://openalex.org/W1932392742', 'https://openalex.org/W2801014266', 'https://openalex.org/W2068626038', 'https://openalex.org/W639865318', 'https://openalex.org/W4241086854', 'https://openalex.org/W590958915', 'https://openalex.org/W1990709125', 'https://openalex.org/W568186341', 'https://openalex.org/W2086461330', 'https://openalex.org/W2066846533', 'https://openalex.org/W624041690', 'https://openalex.org/W2146395459', 'https://openalex.org/W2022182797', 'https://openalex.org/W3217185451', 'https://openalex.org/W4251294179', 'https://openalex.org/W2476122338', 'https://openalex.org/W2007678310', 'https://openalex.org/W3021920403', 'https://openalex.org/W1647671624', 'https://openalex.org/W649801000', 'https://openalex.org/W1603699530', 'https://openalex.org/W2909231624', 'https://openalex.org/W1583758360', 'https://openalex.org/W2401501732', 'https://openalex.org/W2321704223', 'https://openalex.org/W1542819857', 'https://openalex.org/W572264085', 'https://openalex.org/W1550708636', 'https://openalex.org/W2332324770', 'https://openalex.org/W3164908431', 'https://openalex.org/W1496562433', 'https://openalex.org/W4232259805', 'https://openalex.org/W1963824135', 'https://openalex.org/W615088441', 'https://openalex.org/W1868544948', 'https://openalex.org/W1502187416', 'https://openalex.org/W427034204', 'https://openalex.org/W2085418212', 'https://openalex.org/W2562840230', 'https://openalex.org/W1496903218', 'https://openalex.org/W2108558814', 'https://openalex.org/W1594560709', 'https://openalex.org/W1952967124', 'https://openalex.org/W245097788', 'https://openalex.org/W258008434', 'https://openalex.org/W2046853237', 'https://openalex.org/W2252767756', 'https://openalex.org/W4233160667', 'https://openalex.org/W594098150', 'https://openalex.org/W151063029', 'https://openalex.org/W1995759672', 'https://openalex.org/W1032384138', 'https://openalex.org/W2162467318', 'https://openalex.org/W570594514', 'https://openalex.org/W2069854804', 'https://openalex.org/W4229044800', 'https://openalex.org/W355080324', 'https://openalex.org/W69917654', 'https://openalex.org/W1521568674', 'https://openalex.org/W4388152787', 'https://openalex.org/W2071552912', 'https://openalex.org/W589198331', 'https://openalex.org/W2039654910', 'https://openalex.org/W406358763', 'https://openalex.org/W2106531581', 'https://openalex.org/W3149829789', 'https://openalex.org/W16234938', 'https://openalex.org/W2186071512', 'https://openalex.org/W2608191729', 'https://openalex.org/W2055448370', 'https://openalex.org/W1601125268', 'https://openalex.org/W108345351', 'https://openalex.org/W2162510130']",1999-01-01
https://openalex.org/W2514741789,https://doi.org/10.21437/interspeech.2016-595,Purely Sequence-Trained Neural Networks for ASR Based on Lattice-Free MMI,,"['https://openalex.org/W2407080277', 'https://openalex.org/W2402146185', 'https://openalex.org/W2963920996', 'https://openalex.org/W2079623482', 'https://openalex.org/W2127141656', 'https://openalex.org/W1499864241', 'https://openalex.org/W2131342762', 'https://openalex.org/W2250357346', 'https://openalex.org/W1533416326', 'https://openalex.org/W2400997536', 'https://openalex.org/W1987238397', 'https://openalex.org/W2098318492', 'https://openalex.org/W2132991150', 'https://openalex.org/W2288502450']",2016-08-29
https://openalex.org/W3025286576,https://doi.org/10.21437/interspeech.2020-2513,That Sounds Familiar: An Analysis of Phonetic Representations Transfer Across Languages,"Only a handful of the world's languages are abundant with the resources that enable practical applications of speech processing technologies. One of the methods to overcome this problem is to use the resources existing in other languages to train a multilingual automatic speech recognition (ASR) model, which, intuitively, should learn some universal phonetic representations. In this work, we focus on gaining a deeper understanding of how general these representations might be, and how individual phones are getting improved in a multilingual setting. To that end, we select a phonetically diverse set of languages, and perform a series of monolingual, multilingual and crosslingual (zero-shot) experiments. The ASR is trained to recognize the International Phonetic Alphabet (IPA) token sequences. We observe significant improvements across all languages in the multilingual setting, and stark degradation in the crosslingual setting, where the model, among other errors, considers Javanese as a tone language. Notably, as little as 10 hours of the target language training data tremendously reduces ASR error rates. Our analysis uncovered that even the phones that are unique to a single language can benefit greatly from adding training data from other languages - an encouraging result for the low-resource speech community.","['https://openalex.org/W2972818416', 'https://openalex.org/W2164505566', 'https://openalex.org/W2963292011', 'https://openalex.org/W2088499802', 'https://openalex.org/W2094097435', 'https://openalex.org/W2295676751', 'https://openalex.org/W2962780374', 'https://openalex.org/W3045485643', 'https://openalex.org/W2526425061', 'https://openalex.org/W2152092236', 'https://openalex.org/W2085628288', 'https://openalex.org/W1992912377', 'https://openalex.org/W2786835190', 'https://openalex.org/W3015877095', 'https://openalex.org/W66627554', 'https://openalex.org/W47568227', 'https://openalex.org/W2990910192']",2020-10-25
https://openalex.org/W2762715843,https://doi.org/10.48550/arxiv.1710.03501,A Very Low Resource Language Speech Corpus for Computational Language Documentation Experiments,"Most speech and language technologies are trained with massive amounts of speech and text information. However, most of the world languages do not have such resources or stable orthography. Systems constructed under these almost zero resource conditions are not only promising for speech technology but also for computational language documentation. The goal of computational language documentation is to help field linguists to (semi-)automatically analyze and annotate audio recordings of endangered and unwritten languages. Example tasks are automatic phoneme discovery or lexicon discovery from the speech signal. This paper presents a speech corpus collected during a realistic language documentation process. It is made up of 5k speech utterances in Mboshi (Bantu C25) aligned to French text translations. Speech transcriptions are also made available: they correspond to a non-standard graphemic form close to the language phonology. We present how the data was collected, cleaned and processed and we illustrate its use through a zero-resource task: spoken term discovery. The dataset is made available to the community for reproducible computational language documentation experiments and their evaluation.","['https://openalex.org/W847516984', 'https://openalex.org/W2620638943', 'https://openalex.org/W1557247526', 'https://openalex.org/W2592914315', 'https://openalex.org/W2466918907', 'https://openalex.org/W2756778986', 'https://openalex.org/W2747287964', 'https://openalex.org/W2347145335', 'https://openalex.org/W2122228338', 'https://openalex.org/W2126377586', 'https://openalex.org/W2553206119', 'https://openalex.org/W2590585939', 'https://openalex.org/W2251408482', 'https://openalex.org/W2345799635', 'https://openalex.org/W2668277942']",2017-10-10
https://openalex.org/W3094197178,https://doi.org/10.1109/icassp39728.2021.9414478,How Phonotactics Affect Multilingual and Zero-Shot ASR Performance,"The idea of combining multiple languages' recordings to train a single\nautomatic speech recognition (ASR) model brings the promise of the emergence of\nuniversal speech representation. Recently, a Transformer encoder-decoder model\nhas been shown to leverage multilingual data well in IPA transcriptions of\nlanguages presented during training. However, the representations it learned\nwere not successful in zero-shot transfer to unseen languages. Because that\nmodel lacks an explicit factorization of the acoustic model (AM) and language\nmodel (LM), it is unclear to what degree the performance suffered from\ndifferences in pronunciation or the mismatch in phonotactics. To gain more\ninsight into the factors limiting zero-shot ASR transfer, we replace the\nencoder-decoder with a hybrid ASR system consisting of a separate AM and LM.\nThen, we perform an extensive evaluation of monolingual, multilingual, and\ncrosslingual (zero-shot) acoustic and language models on a set of 13\nphonetically diverse languages. We show that the gain from modeling\ncrosslingual phonotactics is limited, and imposing a too strong model can hurt\nthe zero-shot transfer. Furthermore, we find that a multilingual LM hurts a\nmultilingual ASR system's performance, and retaining only the target language's\nphonotactic data in LM training is preferable.\n","['https://openalex.org/W6602682705', 'https://openalex.org/W2147768505', 'https://openalex.org/W3088761213', 'https://openalex.org/W6631362777', 'https://openalex.org/W2888867175', 'https://openalex.org/W2514741789', 'https://openalex.org/W2085628288', 'https://openalex.org/W2158069733', 'https://openalex.org/W6713762819', 'https://openalex.org/W2120209245', 'https://openalex.org/W3005578234', 'https://openalex.org/W6774503009', 'https://openalex.org/W2025198378', 'https://openalex.org/W2526425061', 'https://openalex.org/W3095732712', 'https://openalex.org/W3095838132', 'https://openalex.org/W3016010032', 'https://openalex.org/W66627554', 'https://openalex.org/W3015877095', 'https://openalex.org/W3045485643', 'https://openalex.org/W3025286576', 'https://openalex.org/W2316803017', 'https://openalex.org/W2407080277', 'https://openalex.org/W1631260214', 'https://openalex.org/W1524333225', 'https://openalex.org/W86969866']",2021-05-13
https://openalex.org/W2327501763,https://doi.org/10.1109/icassp.2016.7472621,"Listen, attend and spell: A neural network for large vocabulary conversational speech recognition","We present Listen, Attend and Spell (LAS), a neural speech recognizer that transcribes speech utterances directly to characters without pronunciation models, HMMs or other components of traditional speech recognizers. In LAS, the neural network architecture subsumes the acoustic, pronunciation and language models making it not only an end-to-end trained system but an end-to-end model. In contrast to DNN-HMM, CTC and most other models, LAS makes no independence assumptions about the probability distribution of the output character sequences given the acoustic sequence. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits each character conditioned on all previous characters, and the entire acoustic sequence. On a Google voice search task, LAS achieves a WER of 14.1% without a dictionary or an external language model and 10.3% with language model rescoring over the top 32 beams. In comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0% on the same set.","['https://openalex.org/W6679434410', 'https://openalex.org/W6635078382', 'https://openalex.org/W6623517193', 'https://openalex.org/W6630875275', 'https://openalex.org/W6676562027', 'https://openalex.org/W2064675550', 'https://openalex.org/W2005708641', 'https://openalex.org/W6674758992', 'https://openalex.org/W6680587008', 'https://openalex.org/W2112796928', 'https://openalex.org/W6679429981', 'https://openalex.org/W6629052376', 'https://openalex.org/W2160815625', 'https://openalex.org/W2963211739', 'https://openalex.org/W6696982659', 'https://openalex.org/W6679436768', 'https://openalex.org/W2399086392', 'https://openalex.org/W6675365184', 'https://openalex.org/W2157331557', 'https://openalex.org/W2143612262', 'https://openalex.org/W6621543089', 'https://openalex.org/W6635768407', 'https://openalex.org/W6631362777', 'https://openalex.org/W6640090968', 'https://openalex.org/W6684859321', 'https://openalex.org/W1533416326', 'https://openalex.org/W2293858598', 'https://openalex.org/W2963920996', 'https://openalex.org/W1922655562', 'https://openalex.org/W854541894', 'https://openalex.org/W2109886035', 'https://openalex.org/W2130942839', 'https://openalex.org/W1600744878', 'https://openalex.org/W1484868577', 'https://openalex.org/W1586532344', 'https://openalex.org/W2168231600', 'https://openalex.org/W1514535095', 'https://openalex.org/W2964308564', 'https://openalex.org/W2133564696', 'https://openalex.org/W2131342762', 'https://openalex.org/W2293009711', 'https://openalex.org/W2099257174', 'https://openalex.org/W2102113734', 'https://openalex.org/W2504108613', 'https://openalex.org/W648786980', 'https://openalex.org/W2138660131', 'https://openalex.org/W1524333225', 'https://openalex.org/W2962826786']",2016-03-01
https://openalex.org/W66627554,https://doi.org/10.21437/icslp.2002-151,Globalphone: a multilingual speech and text database developed at karlsruhe university,"This paper describes the design, collection, and current status of the multilingual database GlobalPhone, an ongoing project since 1995 at Karlsruhe University.GlobalPhone is a highquality read speech and text database in a large variety of languages which is suitable for the development of large vocabulary speech recognition systems in many languages.It has already been successfully applied to language independent and language adaptive speech recognition.GlobalPhone currently covers 15 languages Arabic, Chinese (Mandarin and Shanghai), Croatian, Czech, French, German, Japanese, Korean, Portuguese, Russian, Spanish, Swedish, Tamil, and Turkish.The corpus contains more than 300 hours of transcribed speech spoken by more than 1500 native, adult speakers and will soon be available from ELRA.","['https://openalex.org/W2145489946', 'https://openalex.org/W2033436836', 'https://openalex.org/W2587282545']",2002-09-16
