[
  {
    "metadata": {
      "title": "Mapping the Audio Landscape for Innovative Music Sample Generation",
      "summary": "This paper introduces the Generative Sample Map (GESAM), a novel two-stage unsupervised learning framework capable of generating high-quality and expressive audio samples for music production. Recent generative approaches based on language models rely on text prompts as conditions. However, fine nuances in musical audio samples can hardly be described in the modality of text. For addressing this shortcoming, we propose to learn a highly descriptive latent 2D audio map by a Variational Autoencoder (VAE) which is then utilized for conditioning a Transformer model. We demonstrate the Transformer model's ability to achieve high generation quality and compare its performance against two baseline models. By selecting points on the map that compresses the manifold of the audio training set into 2D, we enable a more natural interaction with the model. We showcase this capability through an interactive demo interface, which is accessible on our website https://limchr.github.io/gesam/",
      "abstract": "This paper introduces the Generative Sample Map (GESAM), a novel two-stage unsupervised learning framework capable of generating high-quality and expressive audio samples for music production. Recent generative approaches based on language models rely on text prompts as conditions. However, fine nuances in musical audio samples can hardly be described in the modality of text. For addressing this shortcoming, we propose to learn a highly descriptive latent 2D audio map by a Variational Autoencoder (VAE) which is then utilized for conditioning a Transformer model. We demonstrate the Transformer model's ability to achieve high generation quality and compare its performance against two baseline models. By selecting points on the map that compresses the manifold of the audio training set into 2D, we enable a more natural interaction with the model. We showcase this capability through an interactive demo interface, which is accessible on our website https://limchr.github.io/gesam/",
      "doi": "https://doi.org/10.1145/3652583.3657586",
      "openalex_id": "https://openalex.org/W4399418461",
      "arxiv_id": "",
      "publication_date": "2024-05-30",
      "published": "2024-05-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "BeatNet+: Real‑Time Rhythm Analysis for Diverse Music Audio",
      "summary": "This paper presents a comprehensive study on real-time music rhythm analysis, covering joint beat and downbeat tracking for diverse kinds of music signals. We introduce BeatNet+, a two-stage approach to real-time rhythm analysis built on a previous state-of-the-art method named BeatNet. The main innovation of the proposed method is the auxiliary training strategy that helps the neural network model to learn a representation invariant to the amount of percussive components in the music. Together with other architectural improvements, this strategy significantly improves the model performance for generic music. Another innovation is on the adaptation strategies that help develop real-time rhythm analysis models for challenging music scenarios, including isolated singing voices and non-percussive music. Two adaptation strategies are proposed and experimented with using different neural architectures and training schemes. Comprehensive experiments and comparisons with multiple baselines are conducted, and results show that BeatNet+ achieves superior beat tracking and downbeat tracking F1 scores for generic music, isolated singing voices, and non-percussive audio, with competitive latency and computational complexity. Finally, we release beat and downbeat annotations for two datasets that are designed for other tasks, and revised annotations of three existing datasets. We also release the code repository and pre-trained models on GitHub.",
      "abstract": "This paper presents a comprehensive study on real-time music rhythm analysis, covering joint beat and downbeat tracking for diverse kinds of music signals. We introduce BeatNet+, a two-stage approach to real-time rhythm analysis built on a previous state-of-the-art method named BeatNet. The main innovation of the proposed method is the auxiliary training strategy that helps the neural network model to learn a representation invariant to the amount of percussive components in the music. Together with other architectural improvements, this strategy significantly improves the model performance for generic music. Another innovation is on the adaptation strategies that help develop real-time rhythm analysis models for challenging music scenarios, including isolated singing voices and non-percussive music. Two adaptation strategies are proposed and experimented with using different neural architectures and training schemes. Comprehensive experiments and comparisons with multiple baselines are conducted, and results show that BeatNet+ achieves superior beat tracking and downbeat tracking F1 scores for generic music, isolated singing voices, and non-percussive audio, with competitive latency and computational complexity. Finally, we release beat and downbeat annotations for two datasets that are designed for other tasks, and revised annotations of three existing datasets. We also release the code repository and pre-trained models on GitHub.",
      "doi": "https://doi.org/10.5334/tismir.198",
      "openalex_id": "https://openalex.org/W4405115893",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Self-Supervised Audio Teacher-Student Transformer for Both Clip-Level and Frame-Level Tasks",
      "summary": "Self-supervised learning (SSL) has emerged as a popular approach for learning audio representations. One goal of audio self-supervised pre-training is to transfer knowledge to downstream audio tasks, generally including clip-level and frame-level tasks. While frame-level tasks are important for fine-grained acoustic scene/event understanding, prior studies primarily evaluate on clip-level downstream tasks. In order to tackle both clip-level and frame-level tasks, this paper proposes Audio Teacher-Student Transformer (ATST), with a clip-level version (named ATST-Clip) and a frame-level version (named ATST-Frame), responsible for learning clip-level and frame-level representations, respectively. Both methods use a Transformer encoder and a teacher-student training scheme. We have carefully designed a view creation strategy for ATST-Clip and ATST-Frame. Specifically, ATST-Clip uses segment-wise data augmentations, and ATST-Frame integrates frame-wise data augmentations and masking. Experimental results show that our ATST-Frame model obtains state-of-the-art (SOTA) performances on most of the clip-level and frame-level downstream tasks. Especially, it outperforms other models by a large margin on the frame-level sound event detection task. In addition, the performance can be further improved by combining the two models through knowledge distillation.",
      "abstract": "Self-supervised learning (SSL) has emerged as a popular approach for learning audio representations. One goal of audio self-supervised pre-training is to transfer knowledge to downstream audio tasks, generally including clip-level and frame-level tasks. While frame-level tasks are important for fine-grained acoustic scene/event understanding, prior studies primarily evaluate on clip-level downstream tasks. In order to tackle both clip-level and frame-level tasks, this paper proposes Audio Teacher-Student Transformer (ATST), with a clip-level version (named ATST-Clip) and a frame-level version (named ATST-Frame), responsible for learning clip-level and frame-level representations, respectively. Both methods use a Transformer encoder and a teacher-student training scheme. We have carefully designed a view creation strategy for ATST-Clip and ATST-Frame. Specifically, ATST-Clip uses segment-wise data augmentations, and ATST-Frame integrates frame-wise data augmentations and masking. Experimental results show that our ATST-Frame model obtains state-of-the-art (SOTA) performances on most of the clip-level and frame-level downstream tasks. Especially, it outperforms other models by a large margin on the frame-level sound event detection task. In addition, the performance can be further improved by combining the two models through knowledge distillation.",
      "doi": "https://doi.org/10.1109/taslp.2024.3352248",
      "openalex_id": "https://openalex.org/W4390738640",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation",
      "summary": "We consider the task of generating diverse and realistic videos guided by natural audio samples from a wide variety of semantic classes. For this task, the videos are required to be aligned both globally and temporally with the input audio: globally, the input audio is semantically associated with the entire output video, and temporally, each segment of the input audio is associated with a corresponding segment of that video. We utilize an existing text-conditioned video generation model and a pre-trained audio encoder model. The proposed method is based on a lightweight adaptor network, which learns to map the audio-based representation to the input representation expected by the text-to-video generation model. As such, it also enables video generation conditioned on text, audio, and, for the first time as far as we can ascertain, on both text and audio. We validate our method extensively on three datasets demonstrating significant semantic diversity of audio-video samples and further propose a novel evaluation metric (AV-Align) to assess the alignment of generated videos with input audio samples. AV-Align is based on the detection and comparison of energy peaks in both modalities. In comparison to recent state-of-the-art approaches, our method generates videos that are better aligned with the input sound, both with respect to content and temporal axis. We also show that videos produced by our method present higher visual quality and are more diverse. Code and samples are available at: https://pages.cs.huji.ac.il/adiyoss-lab/TempoTokens/.",
      "abstract": "We consider the task of generating diverse and realistic videos guided by natural audio samples from a wide variety of semantic classes. For this task, the videos are required to be aligned both globally and temporally with the input audio: globally, the input audio is semantically associated with the entire output video, and temporally, each segment of the input audio is associated with a corresponding segment of that video. We utilize an existing text-conditioned video generation model and a pre-trained audio encoder model. The proposed method is based on a lightweight adaptor network, which learns to map the audio-based representation to the input representation expected by the text-to-video generation model. As such, it also enables video generation conditioned on text, audio, and, for the first time as far as we can ascertain, on both text and audio. We validate our method extensively on three datasets demonstrating significant semantic diversity of audio-video samples and further propose a novel evaluation metric (AV-Align) to assess the alignment of generated videos with input audio samples. AV-Align is based on the detection and comparison of energy peaks in both modalities. In comparison to recent state-of-the-art approaches, our method generates videos that are better aligned with the input sound, both with respect to content and temporal axis. We also show that videos produced by our method present higher visual quality and are more diverse. Code and samples are available at: https://pages.cs.huji.ac.il/adiyoss-lab/TempoTokens/.",
      "doi": "https://doi.org/10.1609/aaai.v38i7.28486",
      "openalex_id": "https://openalex.org/W4393147998",
      "arxiv_id": "",
      "publication_date": "2024-03-24",
      "published": "2024-03-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "CED: Consistent Ensemble Distillation for Audio Tagging",
      "summary": "Augmentation and knowledge distillation (KD) are well-established techniques employed in audio classification tasks, aimed at enhancing performance and reducing model sizes on the widely recognized Audioset (AS) benchmark. Although both techniques are effective individually, their combined use, called consistent teaching, hasn't been explored before. This paper proposes CED, a simple training framework that distils student models from large teacher ensembles with consistent teaching. To achieve this, CED efficiently stores logits as well as the augmentation methods on disk, making it scalable to large-scale datasets. Central to CED's efficacy is its label-free nature, meaning that only the stored logits are used for the optimization of a student model only requiring 0.3% additional disk space for AS. The study trains various transformer-based models, including a 10M parameter model achieving a 49.0 mean average precision (mAP) on AS. Pretrained models and code are available online.",
      "abstract": "Augmentation and knowledge distillation (KD) are well-established techniques employed in audio classification tasks, aimed at enhancing performance and reducing model sizes on the widely recognized Audioset (AS) benchmark. Although both techniques are effective individually, their combined use, called consistent teaching, hasn't been explored before. This paper proposes CED, a simple training framework that distils student models from large teacher ensembles with consistent teaching. To achieve this, CED efficiently stores logits as well as the augmentation methods on disk, making it scalable to large-scale datasets. Central to CED's efficacy is its label-free nature, meaning that only the stored logits are used for the optimization of a student model only requiring 0.3% additional disk space for AS. The study trains various transformer-based models, including a 10M parameter model achieving a 49.0 mean average precision (mAP) on AS. Pretrained models and code are available online.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446348",
      "openalex_id": "https://openalex.org/W4392904001",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "BAVS: Bootstrapping Audio-Visual Segmentation by Integrating Foundation Knowledge",
      "summary": "Given an audio-visual pair, audio-visual segmentation (AVS) aims to locate sounding sources by predicting pixel-wise maps. Previous methods assume that each sound component in an audio signal always has a visual counterpart in the image. However, this assumption overlooks that off-screen sounds and background noise often contaminate the audio recordings in real-world scenarios. They impose significant challenges on building a consistent semantic mapping between audio and visual signals for AVS models and thus impede precise sound localization. In this work, we propose a two-stage bootstrapping audio-visual segmentation framework by incorporating multi-modal foundation knowledge <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$^{1}$</tex-math></inline-formula> In a nutshell, our BAVS is designed to eliminate the interference of background noise or off-screen sounds in segmentation by establishing the audio-visual correspondences in an explicit manner. In the first stage, we employ a segmentation model to localize potential sounding objects from visual data without being affected by contaminated audio signals. Meanwhile, we also utilize a foundation audio classification model to discern audio semantics. Considering the audio tags provided by the audio foundation model are noisy, associating object masks with audio tags is not trivial. Thus, in the second stage, we develop an audio-visual semantic integration strategy (AVIS) to localize the authentic-sounding objects. Here, we construct an audio-visual tree based on the hierarchical correspondence between sounds and object categories. We then examine the label concurrency between the localized objects and classified audio tags by tracing the audio-visual tree. With AVIS, we can effectively segment real-sounding objects. Extensive experiments demonstrate the superiority of our method on AVS datasets, particularly in scenarios involving background noise. Our project website is <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://yenanliu.github.io/AVSS.github.io/</uri> .",
      "abstract": "Given an audio-visual pair, audio-visual segmentation (AVS) aims to locate sounding sources by predicting pixel-wise maps. Previous methods assume that each sound component in an audio signal always has a visual counterpart in the image. However, this assumption overlooks that off-screen sounds and background noise often contaminate the audio recordings in real-world scenarios. They impose significant challenges on building a consistent semantic mapping between audio and visual signals for AVS models and thus impede precise sound localization. In this work, we propose a two-stage bootstrapping audio-visual segmentation framework by incorporating multi-modal foundation knowledge <inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$^{1}$</tex-math></inline-formula> In a nutshell, our BAVS is designed to eliminate the interference of background noise or off-screen sounds in segmentation by establishing the audio-visual correspondences in an explicit manner. In the first stage, we employ a segmentation model to localize potential sounding objects from visual data without being affected by contaminated audio signals. Meanwhile, we also utilize a foundation audio classification model to discern audio semantics. Considering the audio tags provided by the audio foundation model are noisy, associating object masks with audio tags is not trivial. Thus, in the second stage, we develop an audio-visual semantic integration strategy (AVIS) to localize the authentic-sounding objects. Here, we construct an audio-visual tree based on the hierarchical correspondence between sounds and object categories. We then examine the label concurrency between the localized objects and classified audio tags by tracing the audio-visual tree. With AVIS, we can effectively segment real-sounding objects. Extensive experiments demonstrate the superiority of our method on AVS datasets, particularly in scenarios involving background noise. Our project website is <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://yenanliu.github.io/AVSS.github.io/</uri> .",
      "doi": "https://doi.org/10.1109/tmm.2024.3405622",
      "openalex_id": "https://openalex.org/W4399146361",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SFC-Sup: Robust Two-Stage Underwater Acoustic Target Recognition Method Based on Supervised Contrastive Learning",
      "summary": "This paper presents an underwater acoustic target recognition method to reduce recognition errors in continuous recordings caused by variations in ship operating conditions. The proposed method comprises two-stages: the spectral feature classification and the supervised contrastive learning, and it is called as SFC-Sup as a result in this paper. In the first stage, a new spectral feature classification strategy is designed to choose appropriate feature sets for contrastive learning, based on which an instance discrimination pretext task is created by utilizing different spectral features to capture invariant features across segments under different operating conditions. In the second stage, a dynamic weighted loss function is introduced to guide the joint optimization process in the framework of contrastive learning. Different to existing methods which focus on improving the recognition accuracy by designing features for individual segments, the proposed two-stage method SFC-Sup considers consistent features across diverse segments, which is expected to improve recognition accuracy in a continuous recording. Experimental results demonstrate that in the presence of complex operating conditions, SFC-Sup exhibits superior stability and enhances recognition accuracy by 2.06% compared to state-of-the-art methods.",
      "abstract": "This paper presents an underwater acoustic target recognition method to reduce recognition errors in continuous recordings caused by variations in ship operating conditions. The proposed method comprises two-stages: the spectral feature classification and the supervised contrastive learning, and it is called as SFC-Sup as a result in this paper. In the first stage, a new spectral feature classification strategy is designed to choose appropriate feature sets for contrastive learning, based on which an instance discrimination pretext task is created by utilizing different spectral features to capture invariant features across segments under different operating conditions. In the second stage, a dynamic weighted loss function is introduced to guide the joint optimization process in the framework of contrastive learning. Different to existing methods which focus on improving the recognition accuracy by designing features for individual segments, the proposed two-stage method SFC-Sup considers consistent features across diverse segments, which is expected to improve recognition accuracy in a continuous recording. Experimental results demonstrate that in the presence of complex operating conditions, SFC-Sup exhibits superior stability and enhances recognition accuracy by 2.06% compared to state-of-the-art methods.",
      "doi": "https://doi.org/10.1109/tgrs.2023.3329653",
      "openalex_id": "https://openalex.org/W4388240219",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Improving Audio Captioning Models with Fine-Grained Audio Features, Text Embedding Supervision, and LLM Mix-Up Augmentation",
      "summary": "Automated audio captioning (AAC) aims to generate informative descriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state-of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Following the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large language models (LLMs). Specifically, we utilize BEATS to extract fine-grained audio features. Then, we employ Instructor LLM to fetch text embeddings of captions, and infuse their language-modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmentation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state-of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge.",
      "abstract": "Automated audio captioning (AAC) aims to generate informative descriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state-of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Following the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large language models (LLMs). Specifically, we utilize BEATS to extract fine-grained audio features. Then, we employ Instructor LLM to fetch text embeddings of captions, and infuse their language-modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmentation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state-of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447215",
      "openalex_id": "https://openalex.org/W4392903033",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Fast ship radiated noise recognition using three-dimensional mel-spectrograms with an additive attention based transformer",
      "summary": "Passive recognition of ship-radiated noise plays a crucial role in military and economic domains. However, underwater environments pose significant challenges due to inherent noise, reverberation, and time-varying acoustic channels. This paper introduces a novel approach for ship target recognition and classification by leveraging the power of three-dimensional (3D) Mel-spectrograms and an additive attention based Transformer (ADDTr). The proposed method utilizes 3D Mel-spectrograms to capture the temporal variations in both target signal and ambient noise, thereby enhancing both categories’ distinguishable characteristics. By incorporating an additional spatial dimension, the modeling of reverberation effects becomes possible. Through analysis of spatial patterns and changes within the spectrograms, distortions caused by reverberation can be estimated and compensated, so that the clarity of the target signals can be improved. The proposed ADDTr leverages an additive attention mechanism to focus on informative acoustic features while suppressing the influence of noisy or distorted components. This attention-based approach not only enhances the discriminative power of the model but also accelerates the recognition process. It efficiently captures both temporal and spatial dependencies, enabling accurate analysis of complex acoustic signals and precise predictions. Comprehensive comparisons with state-of-the-art acoustic target recognition models on the ShipsEar dataset demonstrate the superiority of the proposed ADDTr approach. Achieving an accuracy of 96.82% with the lowest computation costs, ADDTr outperforms other models.",
      "abstract": "Passive recognition of ship-radiated noise plays a crucial role in military and economic domains. However, underwater environments pose significant challenges due to inherent noise, reverberation, and time-varying acoustic channels. This paper introduces a novel approach for ship target recognition and classification by leveraging the power of three-dimensional (3D) Mel-spectrograms and an additive attention based Transformer (ADDTr). The proposed method utilizes 3D Mel-spectrograms to capture the temporal variations in both target signal and ambient noise, thereby enhancing both categories’ distinguishable characteristics. By incorporating an additional spatial dimension, the modeling of reverberation effects becomes possible. Through analysis of spatial patterns and changes within the spectrograms, distortions caused by reverberation can be estimated and compensated, so that the clarity of the target signals can be improved. The proposed ADDTr leverages an additive attention mechanism to focus on informative acoustic features while suppressing the influence of noisy or distorted components. This attention-based approach not only enhances the discriminative power of the model but also accelerates the recognition process. It efficiently captures both temporal and spatial dependencies, enabling accurate analysis of complex acoustic signals and precise predictions. Comprehensive comparisons with state-of-the-art acoustic target recognition models on the ShipsEar dataset demonstrate the superiority of the proposed ADDTr approach. Achieving an accuracy of 96.82% with the lowest computation costs, ADDTr outperforms other models.",
      "doi": "https://doi.org/10.3389/fmars.2023.1280708",
      "openalex_id": "https://openalex.org/W4388973434",
      "arxiv_id": "",
      "publication_date": "2023-11-24",
      "published": "2023-11-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ASiT: Local-Global Audio Spectrogram vIsion Transformer for Event Classification",
      "summary": "Transformers, which were originally developed for natural language processing, have recently generated significant interest in the computer vision and audio communities due to their flexibility in learning long-range relationships. Constrained by the data hungry nature of transformers and the limited amount of labelled data, most transformer-based models for audio tasks are finetuned from ImageNet pretrained models, despite the huge gap between the domain of natural images and audio. This has motivated the research in self-supervised pretraining of audio transformers, which reduces the dependency on large amounts of labeled data and focuses on extracting concise representations of audio spectrograms. In this paper, we propose \\textbf{L}ocal-\\textbf{G}lobal \\textbf{A}udio \\textbf{S}pectrogram v\\textbf{I}sion \\textbf{T}ransformer, namely ASiT, a novel self-supervised learning framework that captures local and global contextual information by employing group masked model learning and self-distillation. We evaluate our pretrained models on both audio and speech classification tasks, including audio event classification, keyword spotting, and speaker identification. We further conduct comprehensive ablation studies, including evaluations of different pretraining strategies. The proposed ASiT framework significantly boosts the performance on all tasks and sets a new state-of-the-art performance in five audio and speech classification tasks, outperforming recent methods, including the approaches that use additional datasets for pretraining.",
      "abstract": "Transformers, which were originally developed for natural language processing, have recently generated significant interest in the computer vision and audio communities due to their flexibility in learning long-range relationships. Constrained by the data hungry nature of transformers and the limited amount of labelled data, most transformer-based models for audio tasks are finetuned from ImageNet pretrained models, despite the huge gap between the domain of natural images and audio. This has motivated the research in self-supervised pretraining of audio transformers, which reduces the dependency on large amounts of labeled data and focuses on extracting concise representations of audio spectrograms. In this paper, we propose \\textbf{L}ocal-\\textbf{G}lobal \\textbf{A}udio \\textbf{S}pectrogram v\\textbf{I}sion \\textbf{T}ransformer, namely ASiT, a novel self-supervised learning framework that captures local and global contextual information by employing group masked model learning and self-distillation. We evaluate our pretrained models on both audio and speech classification tasks, including audio event classification, keyword spotting, and speaker identification. We further conduct comprehensive ablation studies, including evaluations of different pretraining strategies. The proposed ASiT framework significantly boosts the performance on all tasks and sets a new state-of-the-art performance in five audio and speech classification tasks, outperforming recent methods, including the approaches that use additional datasets for pretraining.",
      "doi": "https://doi.org/10.48550/arxiv.2211.13189",
      "openalex_id": "https://openalex.org/W4309957268",
      "arxiv_id": "",
      "publication_date": "2022-11-23",
      "published": "2022-11-23",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research",
      "summary": "The advancement of audio-language (AL) multimodal learning tasks has been significant in recent years. However, researchers face challenges due to the costly and time-consuming collection process of existing audio-language datasets, which are limited in size. To address this data scarcity issue, we introduce WavCaps, the first large-scale weakly-labelled audio captioning dataset, comprising approximately 400k audio clips with paired captions. We sourced audio clips and their raw descriptions from web sources and a sound event detection dataset. However, the online-harvested raw descriptions are highly noisy and unsuitable for direct use in tasks such as automated audio captioning. To overcome this issue, we propose a three-stage processing pipeline for filtering noisy data and generating high-quality captions, where ChatGPT, a large language model, is leveraged to filter and transform raw descriptions automatically. We conduct a comprehensive analysis of the characteristics of WavCaps dataset and evaluate it on multiple downstream audio-language multimodal learning tasks. The systems trained on WavCaps outperform previous state-of-the-art (SOTA) models by a significant margin. Our aspiration is for the WavCaps dataset we have proposed to facilitate research in audio-language multimodal learning and demonstrate the potential of utilizing ChatGPT to enhance academic research. Our dataset and codes are available at https://github.com/XinhaoMei/WavCaps.",
      "abstract": "The advancement of audio-language (AL) multimodal learning tasks has been significant in recent years. However, researchers face challenges due to the costly and time-consuming collection process of existing audio-language datasets, which are limited in size. To address this data scarcity issue, we introduce WavCaps, the first large-scale weakly-labelled audio captioning dataset, comprising approximately 400k audio clips with paired captions. We sourced audio clips and their raw descriptions from web sources and a sound event detection dataset. However, the online-harvested raw descriptions are highly noisy and unsuitable for direct use in tasks such as automated audio captioning. To overcome this issue, we propose a three-stage processing pipeline for filtering noisy data and generating high-quality captions, where ChatGPT, a large language model, is leveraged to filter and transform raw descriptions automatically. We conduct a comprehensive analysis of the characteristics of WavCaps dataset and evaluate it on multiple downstream audio-language multimodal learning tasks. The systems trained on WavCaps outperform previous state-of-the-art (SOTA) models by a significant margin. Our aspiration is for the WavCaps dataset we have proposed to facilitate research in audio-language multimodal learning and demonstrate the potential of utilizing ChatGPT to enhance academic research. Our dataset and codes are available at https://github.com/XinhaoMei/WavCaps.",
      "doi": "https://doi.org/10.48550/arxiv.2303.17395",
      "openalex_id": "https://openalex.org/W4361864998",
      "arxiv_id": "",
      "publication_date": "2023-03-30",
      "published": "2023-03-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Exploring Self-supervised Contrastive Learning of Spatial Sound Event Representation",
      "summary": "In this study, we present a simple multi-channel framework for contrastive learning (MC-SimCLR) to encode 'what' and 'where' of spatial audios. MC-SimCLR learns joint spectral and spatial representations from unlabeled spatial audios, thereby enhancing both event classification and sound localization in downstream tasks. At its core, we propose a multi-level data augmentation pipeline that augments different levels of audio features, including waveforms, Mel spectrograms, and generalized cross-correlation (GCC) features. In addition, we introduce simple yet effective channel-wise augmentation methods to randomly swap the order of the microphones and mask Mel and GCC channels. By using these augmentations, we find that linear layers on top of the learned representation significantly outperform supervised models in terms of both event classification accuracy and localization error. We also perform a comprehensive analysis of the effect of each augmentation method and a comparison of the fine-tuning performance using different amounts of labeled data.",
      "abstract": "In this study, we present a simple multi-channel framework for contrastive learning (MC-SimCLR) to encode 'what' and 'where' of spatial audios. MC-SimCLR learns joint spectral and spatial representations from unlabeled spatial audios, thereby enhancing both event classification and sound localization in downstream tasks. At its core, we propose a multi-level data augmentation pipeline that augments different levels of audio features, including waveforms, Mel spectrograms, and generalized cross-correlation (GCC) features. In addition, we introduce simple yet effective channel-wise augmentation methods to randomly swap the order of the microphones and mask Mel and GCC channels. By using these augmentations, we find that linear layers on top of the learned representation significantly outperform supervised models in terms of both event classification accuracy and localization error. We also perform a comprehensive analysis of the effect of each augmentation method and a comparison of the fine-tuning performance using different amounts of labeled data.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447391",
      "openalex_id": "https://openalex.org/W4392903751",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SSL-Net: A Synergistic Spectral and Learning-Based Network for Efficient Bird Sound Classification",
      "summary": "Efficient and accurate bird sound classification is of important for ecology, habitat protection and scientific research, as it plays a central role in monitoring the distribution and abundance of species. However, prevailing methods typically demand extensively labeled audio datasets and have highly customized frameworks, imposing substantial computational and annotation loads. In this study, we present an efficient and general framework called SSL-Net, which combines spectral and learned features to identify different bird sounds. Encouraging empirical results gleaned from a standard field-collected bird audio dataset validate the efficacy of our method in extracting features efficiently and achieving heightened performance in bird sound classification, even when working with limited sample sizes. Furthermore, we present three feature fusion strategies, aiding engineers and researchers in their selection through quantitative analysis.",
      "abstract": "Efficient and accurate bird sound classification is of important for ecology, habitat protection and scientific research, as it plays a central role in monitoring the distribution and abundance of species. However, prevailing methods typically demand extensively labeled audio datasets and have highly customized frameworks, imposing substantial computational and annotation loads. In this study, we present an efficient and general framework called SSL-Net, which combines spectral and learned features to identify different bird sounds. Encouraging empirical results gleaned from a standard field-collected bird audio dataset validate the efficacy of our method in extracting features efficiently and achieving heightened performance in bird sound classification, even when working with limited sample sizes. Furthermore, we present three feature fusion strategies, aiding engineers and researchers in their selection through quantitative analysis.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10445889",
      "openalex_id": "https://openalex.org/W4392904623",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Foundation Models for Video Understanding: A Survey",
      "summary": "Video Foundation Models (ViFMs) aim to develop general-purpose representations for various video understanding tasks by leveraging large-scale datasets and powerful models to capture robust and generic features from video data.This survey analyzes over 200 methods, offering a comprehensive overview of benchmarks and evaluation metrics across 15 distinct video tasks, categorized into three main groups.Additionally, we provide an in-depth performance analysis of these models for the six most common video tasks.We identify three main approaches to constructing ViFMs: 1) Image-based ViFMs, which adapt image foundation models for video tasks; 2) Video-based ViFMs, which utilize video-specific encoding methods; and 3) Universal Foundation Models (UFMs), which integrate multiple modalities (image, video, audio, text, etc.) within a single framework.Each approach is further subdivided based on either practical implementation perspectives or pretraining objective types.By comparing the performance of various ViFMs on common video tasks, we offer valuable insights into their strengths and weaknesses, guiding future advancements in video understanding.Our analysis reveals that image-based ViFMs consistently outperform video-based ViFMs on most video understanding tasks.Additionally, UFMs, which leverage diverse modalities, demonstrate superior performance across all video tasks.We provide the comprehensive list of ViFMs studied in this work at: https://github.com/NeeluMadan/ViFMSurvey.git.",
      "abstract": "Video Foundation Models (ViFMs) aim to develop general-purpose representations for various video understanding tasks by leveraging large-scale datasets and powerful models to capture robust and generic features from video data.This survey analyzes over 200 methods, offering a comprehensive overview of benchmarks and evaluation metrics across 15 distinct video tasks, categorized into three main groups.Additionally, we provide an in-depth performance analysis of these models for the six most common video tasks.We identify three main approaches to constructing ViFMs: 1) Image-based ViFMs, which adapt image foundation models for video tasks; 2) Video-based ViFMs, which utilize video-specific encoding methods; and 3) Universal Foundation Models (UFMs), which integrate multiple modalities (image, video, audio, text, etc.) within a single framework.Each approach is further subdivided based on either practical implementation perspectives or pretraining objective types.By comparing the performance of various ViFMs on common video tasks, we offer valuable insights into their strengths and weaknesses, guiding future advancements in video understanding.Our analysis reveals that image-based ViFMs consistently outperform video-based ViFMs on most video understanding tasks.Additionally, UFMs, which leverage diverse modalities, demonstrate superior performance across all video tasks.We provide the comprehensive list of ViFMs studied in this work at: https://github.com/NeeluMadan/ViFMSurvey.git.",
      "doi": "https://doi.org/10.36227/techrxiv.171769139.99464428/v1",
      "openalex_id": "https://openalex.org/W4399393261",
      "arxiv_id": "",
      "publication_date": "2024-06-06",
      "published": "2024-06-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "DTF-AT: Decoupled Time-Frequency Audio Transformer for Event Classification",
      "summary": "Convolutional neural networks (CNNs) and Transformer-based networks have recently enjoyed significant attention for various audio classification and tagging tasks following their wide adoption in the computer vision domain. Despite the difference in information distribution between audio spectrograms and natural images, there has been limited exploration of effective information retrieval from spectrograms using domain-specific layers tailored for the audio domain. In this paper, we leverage the power of the Multi-Axis Vision Transformer (MaxViT) to create DTF-AT (Decoupled Time-Frequency Audio Transformer) that facilitates interactions across time, frequency, spatial, and channel dimensions. The proposed DTF-AT architecture is rigorously evaluated across diverse audio and speech classification tasks, consistently establishing new benchmarks for state-of-the-art (SOTA) performance. Notably, on the challenging AudioSet 2M classification task, our approach demonstrates a substantial improvement of 4.4% when the model is trained from scratch and 3.2% when the model is initialised from ImageNet-1K pretrained weights. In addition, we present comprehensive ablation studies to investigate the impact and efficacy of our proposed approach. The codebase and pretrained weights are available on https://github.com/ta012/DTFAT.git",
      "abstract": "Convolutional neural networks (CNNs) and Transformer-based networks have recently enjoyed significant attention for various audio classification and tagging tasks following their wide adoption in the computer vision domain. Despite the difference in information distribution between audio spectrograms and natural images, there has been limited exploration of effective information retrieval from spectrograms using domain-specific layers tailored for the audio domain. In this paper, we leverage the power of the Multi-Axis Vision Transformer (MaxViT) to create DTF-AT (Decoupled Time-Frequency Audio Transformer) that facilitates interactions across time, frequency, spatial, and channel dimensions. The proposed DTF-AT architecture is rigorously evaluated across diverse audio and speech classification tasks, consistently establishing new benchmarks for state-of-the-art (SOTA) performance. Notably, on the challenging AudioSet 2M classification task, our approach demonstrates a substantial improvement of 4.4% when the model is trained from scratch and 3.2% when the model is initialised from ImageNet-1K pretrained weights. In addition, we present comprehensive ablation studies to investigate the impact and efficacy of our proposed approach. The codebase and pretrained weights are available on https://github.com/ta012/DTFAT.git",
      "doi": "https://doi.org/10.1609/aaai.v38i16.29716",
      "openalex_id": "https://openalex.org/W4393160941",
      "arxiv_id": "",
      "publication_date": "2024-03-24",
      "published": "2024-03-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Enabling Dataspaces Using Foundation Models: Technical, Legal and Ethical Considerations and Future Trends",
      "summary": "Foundation Models are pivotal in advancing arti ficial intelligence, driving notable progress across diverse areas. When merged with dataspace, these models enhance our capabil ity to develop algorithms that are powerful, predictive, and honor data sovereignty and quality. This paper highlights the potential benefits of a comprehensive repository of Foundation Models, contextualized within dataspace. Such an archive can streamline research, development, and education by offering a comparative analysis of various models and their applications. While serving as a consistent reference point for model assessment and fostering collaborative learning, the repository does face challenges like un biased evaluations, data privacy, and comprehensive information delivery. The paper also notes the importance of the repository being globally applicable, ethically constructed, and user-friendly. We delve into the nuances of integrating Foundation Models within dataspace, balancing the repository’s strengths against its limitations.",
      "abstract": "Foundation Models are pivotal in advancing arti ficial intelligence, driving notable progress across diverse areas. When merged with dataspace, these models enhance our capabil ity to develop algorithms that are powerful, predictive, and honor data sovereignty and quality. This paper highlights the potential benefits of a comprehensive repository of Foundation Models, contextualized within dataspace. Such an archive can streamline research, development, and education by offering a comparative analysis of various models and their applications. While serving as a consistent reference point for model assessment and fostering collaborative learning, the repository does face challenges like un biased evaluations, data privacy, and comprehensive information delivery. The paper also notes the importance of the repository being globally applicable, ethically constructed, and user-friendly. We delve into the nuances of integrating Foundation Models within dataspace, balancing the repository’s strengths against its limitations.",
      "doi": "https://doi.org/10.1109/bigdata59044.2023.10386933",
      "openalex_id": "https://openalex.org/W4391092942",
      "arxiv_id": "",
      "publication_date": "2023-12-15",
      "published": "2023-12-15",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "On the Use of an Acoustic Sensor in the Tasks of Determining Defects in the Roadway",
      "summary": "In this paper, an approach to determining pavement defects using an acoustic sensor is considered. The aim of the study is to propose the idea of developing an inexpensive method for detecting various types of damage to the roadway, such as cracks, potholes and irregularities, using acoustic analysis. In the course of the work, the characteristics of sound waves that occur when exposed to defective sections of the road surface were studied. The acoustic sensor allows you to analyze these sound signals and determine the presence and nature of defects based on their changes. The technique includes data collection using an acoustic sensor, signal processing using machine learning algorithms to classify defects and visualization of the results.",
      "abstract": "In this paper, an approach to determining pavement defects using an acoustic sensor is considered. The aim of the study is to propose the idea of developing an inexpensive method for detecting various types of damage to the roadway, such as cracks, potholes and irregularities, using acoustic analysis. In the course of the work, the characteristics of sound waves that occur when exposed to defective sections of the road surface were studied. The acoustic sensor allows you to analyze these sound signals and determine the presence and nature of defects based on their changes. The technique includes data collection using an acoustic sensor, signal processing using machine learning algorithms to classify defects and visualization of the results.",
      "doi": "https://doi.org/10.1109/ieeeconf60226.2024.10496721",
      "openalex_id": "https://openalex.org/W4394860713",
      "arxiv_id": "",
      "publication_date": "2024-03-12",
      "published": "2024-03-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Paᗧ-HuBERT: Self-Supervised Music Source Separation Via Primitive Auditory Clustering And Hidden-Unit Bert",
      "summary": "In spite of the progress in music source separation research, the small amount of publicly-available clean source data remains a constant limiting factor for performance. Thus, recent advances in self-supervised learning present a largely-unexplored opportunity for improving separation models by leveraging unlabelled music data. In this paper, we propose a self-supervised learning framework for music source separation inspired by the HuBERT speech representation model. We first investigate the potential impact of the original HuBERT model by inserting an adapted version of it into the well-known Demucs V2 time-domain separation architecture. We then propose Paᗧ-HuBERT, a time-frequency-domain self-supervised model, that we later use in combination with a ResU-Net decoder for source separation. Paᗧ-HuBERT uses primitive auditory features of music as unsupervised clustering labels to initialize the self-supervised pretraining process using the Free Music Archive (FMA) dataset. The resulting framework achieves better source-to-distortion ratio (SDR) performance on the MusDB18 test set than the original Demucs V2 and Res-U-Net models. We further demonstrate that it can boost performance with small amounts of supervised data. Ultimately, our proposed framework is an effective solution to the challenge of limited clean source data for music source separation.",
      "abstract": "In spite of the progress in music source separation research, the small amount of publicly-available clean source data remains a constant limiting factor for performance. Thus, recent advances in self-supervised learning present a largely-unexplored opportunity for improving separation models by leveraging unlabelled music data. In this paper, we propose a self-supervised learning framework for music source separation inspired by the HuBERT speech representation model. We first investigate the potential impact of the original HuBERT model by inserting an adapted version of it into the well-known Demucs V2 time-domain separation architecture. We then propose Paᗧ-HuBERT, a time-frequency-domain self-supervised model, that we later use in combination with a ResU-Net decoder for source separation. Paᗧ-HuBERT uses primitive auditory features of music as unsupervised clustering labels to initialize the self-supervised pretraining process using the Free Music Archive (FMA) dataset. The resulting framework achieves better source-to-distortion ratio (SDR) performance on the MusDB18 test set than the original Demucs V2 and Res-U-Net models. We further demonstrate that it can boost performance with small amounts of supervised data. Ultimately, our proposed framework is an effective solution to the challenge of limited clean source data for music source separation.",
      "doi": "https://doi.org/10.1109/icasspw59220.2023.10193575",
      "openalex_id": "https://openalex.org/W4385478423",
      "arxiv_id": "",
      "publication_date": "2023-06-04",
      "published": "2023-06-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Weak Supervised Sound Event Detection Based on Puzzle CAM",
      "summary": "The sound event detection method based on deep learning has achieved excellent performance. However, the training of high-performance sound event detection models depends on high-quality strong-label datasets, which brings great pressure and cost to the labeling work of datasets. In this paper, we propose a weakly supervised sound event detection method based on Puzzle Class Activation Map(CAM), which aims to obtain the timestamp information of sound events from a sound event classification network trained by weak labels. Specifically, the method discovers more complete regions of sound events by minimizing the difference between the CAM of the original features and the merged CAM of the separated feature blocks. CAM highlights the feature capture in the time dimension during the model decision-making process. We determine the occurrence time of the sound event based on the position of the frame where the feature is located, and use the model classification prediction to guide the CAM output to achieve weakly supervised sound event detection. Experiments on the Domestic Environment Sound Event Detection Dataset demonstrate that our proposed method exhibits superior performance compared to the baseline system in the task of sound event detection. Specifically, The CAM of a single system achieves the best PSDS2 value of 0.732. Furthermore, when utilizing the CAM ensemble of multiple systems, the PSDS2 value improves to 0.751. Both of these values are higher than those achieved by the baseline system.",
      "abstract": "The sound event detection method based on deep learning has achieved excellent performance. However, the training of high-performance sound event detection models depends on high-quality strong-label datasets, which brings great pressure and cost to the labeling work of datasets. In this paper, we propose a weakly supervised sound event detection method based on Puzzle Class Activation Map(CAM), which aims to obtain the timestamp information of sound events from a sound event classification network trained by weak labels. Specifically, the method discovers more complete regions of sound events by minimizing the difference between the CAM of the original features and the merged CAM of the separated feature blocks. CAM highlights the feature capture in the time dimension during the model decision-making process. We determine the occurrence time of the sound event based on the position of the frame where the feature is located, and use the model classification prediction to guide the CAM output to achieve weakly supervised sound event detection. Experiments on the Domestic Environment Sound Event Detection Dataset demonstrate that our proposed method exhibits superior performance compared to the baseline system in the task of sound event detection. Specifically, The CAM of a single system achieves the best PSDS2 value of 0.732. Furthermore, when utilizing the CAM ensemble of multiple systems, the PSDS2 value improves to 0.751. Both of these values are higher than those achieved by the baseline system.",
      "doi": "https://doi.org/10.1109/access.2023.3305633",
      "openalex_id": "https://openalex.org/W4385834290",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "On Frequency-Wise Normalizations for Better Recording Device Generalization in Audio Spectrogram Transformers",
      "summary": "Varying conditions between the data seen at training and at application time remain a major challenge for machine learning. We study this problem in the context of Acoustic Scene Classification (ASC) with mismatching recording devices. Previous works successfully employed frequency-wise normalization of inputs and hidden layer activations in convolutional neural networks to reduce the recording device discrepancy. The main objective of this work was to adopt frequency-wise normalization for Audio Spectrogram Transformers (ASTs), which have recently become the dominant model architecture in ASC. To this end, we first investigate how recording device characteristics are encoded in the hidden layer activations of ASTs. We find that recording device information is initially encoded in the frequency dimension; however, after the first self-attention block, it is largely transformed into the token dimension. Based on this observation, we conjecture that suppressing recording device characteristics in the input spectrogram is the most effective. We propose a frequency-centering operation for spectrograms that improves the ASC performance on unseen recording devices on average by up to 18.2 percentage points.",
      "abstract": "Varying conditions between the data seen at training and at application time remain a major challenge for machine learning. We study this problem in the context of Acoustic Scene Classification (ASC) with mismatching recording devices. Previous works successfully employed frequency-wise normalization of inputs and hidden layer activations in convolutional neural networks to reduce the recording device discrepancy. The main objective of this work was to adopt frequency-wise normalization for Audio Spectrogram Transformers (ASTs), which have recently become the dominant model architecture in ASC. To this end, we first investigate how recording device characteristics are encoded in the hidden layer activations of ASTs. We find that recording device information is initially encoded in the frequency dimension; however, after the first self-attention block, it is largely transformed into the token dimension. Based on this observation, we conjecture that suppressing recording device characteristics in the input spectrogram is the most effective. We propose a frequency-centering operation for spectrograms that improves the ASC performance on unseen recording devices on average by up to 18.2 percentage points.",
      "doi": "https://doi.org/10.23919/eusipco58844.2023.10289774",
      "openalex_id": "https://openalex.org/W4388183682",
      "arxiv_id": "",
      "publication_date": "2023-09-04",
      "published": "2023-09-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Masked Autoencoders with Multi-Window Local-Global Attention Are Better Audio Learners",
      "summary": "In this work, we propose a Multi-Window Masked Autoencoder (MW-MAE) fitted with a novel Multi-Window Multi-Head Attention (MW-MHA) module that facilitates the modelling of local-global interactions in every decoder transformer block through attention heads of several distinct local and global windows. Empirical results on ten downstream audio tasks show that MW-MAEs consistently outperform standard MAEs in overall performance and learn better general-purpose audio representations, along with demonstrating considerably better scaling characteristics. Investigating attention distances and entropies reveals that MW-MAE encoders learn heads with broader local and global attention. Analyzing attention head feature representations through Projection Weighted Canonical Correlation Analysis (PWCCA) shows that attention heads with the same window sizes across the decoder layers of the MW-MAE learn correlated feature representations which enables each block to independently capture local and global information, leading to a decoupled decoder feature hierarchy. Code for feature extraction and downstream experiments along with pre-trained models will be released publically.",
      "abstract": "In this work, we propose a Multi-Window Masked Autoencoder (MW-MAE) fitted with a novel Multi-Window Multi-Head Attention (MW-MHA) module that facilitates the modelling of local-global interactions in every decoder transformer block through attention heads of several distinct local and global windows. Empirical results on ten downstream audio tasks show that MW-MAEs consistently outperform standard MAEs in overall performance and learn better general-purpose audio representations, along with demonstrating considerably better scaling characteristics. Investigating attention distances and entropies reveals that MW-MAE encoders learn heads with broader local and global attention. Analyzing attention head feature representations through Projection Weighted Canonical Correlation Analysis (PWCCA) shows that attention heads with the same window sizes across the decoder layers of the MW-MAE learn correlated feature representations which enables each block to independently capture local and global information, leading to a decoupled decoder feature hierarchy. Code for feature extraction and downstream experiments along with pre-trained models will be released publically.",
      "doi": "https://doi.org/10.48550/arxiv.2306.00561",
      "openalex_id": "https://openalex.org/W4379256313",
      "arxiv_id": "",
      "publication_date": "2023-06-01",
      "published": "2023-06-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "uaMix-MAE: Efficient Tuning of Pretrained Audio Transformers with Unsupervised Audio Mixtures",
      "summary": "Masked Autoencoders (MAEs) learn rich low-level representations from unlabeled data but require substantial labeled data to effectively adapt to downstream tasks. Conversely, Instance Discrimination (ID) emphasizes high-level semantics, offering a potential solution to alleviate annotation requirements in MAEs. Although combining these two approaches can address downstream tasks with limited labeled data, naively integrating ID into MAEs leads to extended training times and high computational costs. To address this challenge, we introduce uaMix-MAE, an efficient ID tuning strategy that leverages unsupervised audio mixtures. Utilizing contrastive tuning, uaMix-MAE aligns the representations of pretrained MAEs, thereby facilitating effective adaptation to task-specific semantics. To optimize the model with small amounts of unlabeled data, we propose an audio mixing technique that manipulates audio samples in both input and virtual label spaces. Experiments in low/few-shot settings demonstrate that uaMix-MAE achieves 4 − 6% accuracy improvements over various benchmarks when tuned with limited unlabeled data, such as AudioSet-20K.",
      "abstract": "Masked Autoencoders (MAEs) learn rich low-level representations from unlabeled data but require substantial labeled data to effectively adapt to downstream tasks. Conversely, Instance Discrimination (ID) emphasizes high-level semantics, offering a potential solution to alleviate annotation requirements in MAEs. Although combining these two approaches can address downstream tasks with limited labeled data, naively integrating ID into MAEs leads to extended training times and high computational costs. To address this challenge, we introduce uaMix-MAE, an efficient ID tuning strategy that leverages unsupervised audio mixtures. Utilizing contrastive tuning, uaMix-MAE aligns the representations of pretrained MAEs, thereby facilitating effective adaptation to task-specific semantics. To optimize the model with small amounts of unlabeled data, we propose an audio mixing technique that manipulates audio samples in both input and virtual label spaces. Experiments in low/few-shot settings demonstrate that uaMix-MAE achieves 4 − 6% accuracy improvements over various benchmarks when tuned with limited unlabeled data, such as AudioSet-20K.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446342",
      "openalex_id": "https://openalex.org/W4392902846",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Enhancing Audio Generation Diversity with Visual Information",
      "summary": "Audio and sound generation has garnered significant attention in recent years, with a primary focus on improving the quality of generated audios. However, there has been limited research on enhancing the diversity of generated audio, particularly when it comes to audio generation within specific categories. Current models tend to produce homogeneous audio samples within a category. This work aims to address this limitation by improving the diversity of generated audio with visual information. We propose a clustering-based method, leveraging visual information to guide the model in generating distinct audio content within each category. Results on seven categories indicate that extra visual input can largely enhance audio generation diversity. Audio samples are available at DemoWeb.",
      "abstract": "Audio and sound generation has garnered significant attention in recent years, with a primary focus on improving the quality of generated audios. However, there has been limited research on enhancing the diversity of generated audio, particularly when it comes to audio generation within specific categories. Current models tend to produce homogeneous audio samples within a category. This work aims to address this limitation by improving the diversity of generated audio with visual information. We propose a clustering-based method, leveraging visual information to guide the model in generating distinct audio content within each category. Results on seven categories indicate that extra visual input can largely enhance audio generation diversity. Audio samples are available at DemoWeb.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447384",
      "openalex_id": "https://openalex.org/W4392909638",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Impact of Parroting Mode on Cross-Lingual Speaker Recognition",
      "summary": "People use multiple languages in their daily lives across regions worldwide, which motivated us to investigate cross-lingual speaker recognition. In this work, we propose to collect recordings of Mandarin and Spanish, namely the Mandarin-Spanish-Speech Dataset (MSSD-40), to analyze the performance of various audio embeddings for cross-lingual speaker recognition tasks. All participants are fluent in Mandarin, but none of the participants have prior knowledge of the Spanish language. As such, they have been advised to adopt a parroting mode of Spanish speech production, wherein they simply repeat the sounds emanating from the loudspeaker. Using this approach, variations resulting from individual differences in language fluency can be reduced, enabling us to focus on the anatomical aspects of the speech production mechanism.Embeddings extracted from models pre-trained with a large number of audio segments have become effective solutions for coping with audio analysis tasks using small datasets. Preliminary experimental results using two collected multi-lingual datasets indicate that both embedding methods and the language employed will affect the robustness of the speaker recognition task. Precisely, stable performance is observed when familiar languages are used. BEATs embedding generates the best outcome in all languages when no fine-tuning is exercised.",
      "abstract": "People use multiple languages in their daily lives across regions worldwide, which motivated us to investigate cross-lingual speaker recognition. In this work, we propose to collect recordings of Mandarin and Spanish, namely the Mandarin-Spanish-Speech Dataset (MSSD-40), to analyze the performance of various audio embeddings for cross-lingual speaker recognition tasks. All participants are fluent in Mandarin, but none of the participants have prior knowledge of the Spanish language. As such, they have been advised to adopt a parroting mode of Spanish speech production, wherein they simply repeat the sounds emanating from the loudspeaker. Using this approach, variations resulting from individual differences in language fluency can be reduced, enabling us to focus on the anatomical aspects of the speech production mechanism.Embeddings extracted from models pre-trained with a large number of audio segments have become effective solutions for coping with audio analysis tasks using small datasets. Preliminary experimental results using two collected multi-lingual datasets indicate that both embedding methods and the language employed will affect the robustness of the speaker recognition task. Precisely, stable performance is observed when familiar languages are used. BEATs embedding generates the best outcome in all languages when no fine-tuning is exercised.",
      "doi": "https://doi.org/10.1109/ism59092.2023.00035",
      "openalex_id": "https://openalex.org/W4392980346",
      "arxiv_id": "",
      "publication_date": "2023-12-11",
      "published": "2023-12-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Weakly Labeled Sound Event Detection using Attention Mechanism with Teacher-Student Model",
      "summary": "Sound Event Detection (SED) enables identifying and categorizing sound events within audio signals. In this study, we investigate the role of the self and multi-head attention mechanism in enhancing SED performance with the teacher-student learning model in the knowledge distillation context. The study contributes to developing SED methodologies by focusing on detecting events and their temporal boundaries (onset and offset) in weakly labeled and unlabeled sounds. The attention mechanism allows the model to focus on different parts of the audio sequence based on the context, making it robust for tasks where temporal relationships and context matter. Specifically, we present extensive experiments using low- and high-level audio features, including Mel Frequency Cepstral Coefficients (MFCC), Log Mel-Spectrogram (log-Mel), Bidirectional Encoder representation from Audio Transformers (BEATs), Audio Spectrogram Transformer (AST), and Pretrained Audio Neural Networks (PANNs) to assess the performance of individual features with different attention mechanisms. We evaluate the attention mechanism and low-and high-level feature performances with the baseline teacher-student model of the Sound Event Detection with Weak Labels and Synthetic Soundscapes Challenge. Our experiments on the performance dataset show that the proposed attention-based model improves the F1 scores in all features.",
      "abstract": "Sound Event Detection (SED) enables identifying and categorizing sound events within audio signals. In this study, we investigate the role of the self and multi-head attention mechanism in enhancing SED performance with the teacher-student learning model in the knowledge distillation context. The study contributes to developing SED methodologies by focusing on detecting events and their temporal boundaries (onset and offset) in weakly labeled and unlabeled sounds. The attention mechanism allows the model to focus on different parts of the audio sequence based on the context, making it robust for tasks where temporal relationships and context matter. Specifically, we present extensive experiments using low- and high-level audio features, including Mel Frequency Cepstral Coefficients (MFCC), Log Mel-Spectrogram (log-Mel), Bidirectional Encoder representation from Audio Transformers (BEATs), Audio Spectrogram Transformer (AST), and Pretrained Audio Neural Networks (PANNs) to assess the performance of individual features with different attention mechanisms. We evaluate the attention mechanism and low-and high-level feature performances with the baseline teacher-student model of the Sound Event Detection with Weak Labels and Synthetic Soundscapes Challenge. Our experiments on the performance dataset show that the proposed attention-based model improves the F1 scores in all features.",
      "doi": "https://doi.org/10.1109/ism59092.2023.00034",
      "openalex_id": "https://openalex.org/W4392980966",
      "arxiv_id": "",
      "publication_date": "2023-12-11",
      "published": "2023-12-11",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speech2Face3D: A Two‐Stage Transfer‐Learning Framework for Speech‐Driven 3D Facial Animation",
      "summary": "ABSTRACT High‐fidelity, speech‐driven 3D facial animation is crucial for immersive applications and virtual avatars. Nevertheless, advancement is impeded by two principal challenges: (1) a lack of high‐quality 3D data, and (2) inadequate modelling of the multi‐scale characteristics of speech signals. In this paper, we present Speech2Face3D, a novel two‐stage transfer‐learning framework that pretrains on large‐scale pseudo‐3D facial data derived from 2D videos and subsequently finetunes on smaller yet high‐fidelity 3D datasets. This design leverages the richness of easily accessible 2D resources while mitigating reconstruction noise through a simple temporal smoothing step. Our approach further introduces a Multi‐Scale Hierarchical Audio Encoder to capture subtle phoneme transitions, mid‐range prosody, and longer‐range emotional cues. Extensive experiments on public 3D benchmarks demonstrate that our method achieves state‐of‐the‐art performance on lip synchronization, expression fidelity, and temporal coherence metrics. Qualitative user evaluations validate these quantitative improvements. Speech2Face3D is a robust and scalable framework for utilizing extensive 2D data to generate precise and realistic 3D facial animations only based on speech.",
      "abstract": "ABSTRACT High‐fidelity, speech‐driven 3D facial animation is crucial for immersive applications and virtual avatars. Nevertheless, advancement is impeded by two principal challenges: (1) a lack of high‐quality 3D data, and (2) inadequate modelling of the multi‐scale characteristics of speech signals. In this paper, we present Speech2Face3D, a novel two‐stage transfer‐learning framework that pretrains on large‐scale pseudo‐3D facial data derived from 2D videos and subsequently finetunes on smaller yet high‐fidelity 3D datasets. This design leverages the richness of easily accessible 2D resources while mitigating reconstruction noise through a simple temporal smoothing step. Our approach further introduces a Multi‐Scale Hierarchical Audio Encoder to capture subtle phoneme transitions, mid‐range prosody, and longer‐range emotional cues. Extensive experiments on public 3D benchmarks demonstrate that our method achieves state‐of‐the‐art performance on lip synchronization, expression fidelity, and temporal coherence metrics. Qualitative user evaluations validate these quantitative improvements. Speech2Face3D is a robust and scalable framework for utilizing extensive 2D data to generate precise and realistic 3D facial animations only based on speech.",
      "doi": "https://doi.org/10.1049/ipr2.70155",
      "openalex_id": "https://openalex.org/W4412613010",
      "arxiv_id": "",
      "publication_date": "2025-01-01",
      "published": "2025-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models",
      "summary": "In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker adaptation. This work achieves the first human-level TTS on both single and multispeaker datasets, showcasing the potential of style diffusion and adversarial training with large SLMs. The audio demos and source code are available at https://styletts2.github.io/.",
      "abstract": "In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker adaptation. This work achieves the first human-level TTS on both single and multispeaker datasets, showcasing the potential of style diffusion and adversarial training with large SLMs. The audio demos and source code are available at https://styletts2.github.io/.",
      "doi": "https://doi.org/10.48550/arxiv.2306.07691",
      "openalex_id": "https://openalex.org/W4380714711",
      "arxiv_id": "",
      "publication_date": "2023-06-13",
      "published": "2023-06-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "VoiceFlow: Efficient Text-To-Speech with Rectified Flow Matching",
      "summary": "Although diffusion models in text-to-speech have become a popular choice due to their strong generative ability, the intrinsic complexity of sampling from diffusion models harms their efficiency. Alternatively, we propose VoiceFlow, an acoustic model that utilizes a rectified flow matching algorithm to achieve high synthesis quality with a limited number of sampling steps. VoiceFlow formulates the process of generating mel-spectrograms into an ordinary differential equation conditional on text inputs, whose vector field is then estimated. The rectified flow technique then effectively straightens its sampling trajectory for efficient synthesis. Subjective and objective evaluations on both single and multi-speaker corpora showed the superior synthesis quality of VoiceFlow compared to the diffusion counterpart. Ablation studies further verified the validity of the rectified flow technique in VoiceFlow.",
      "abstract": "Although diffusion models in text-to-speech have become a popular choice due to their strong generative ability, the intrinsic complexity of sampling from diffusion models harms their efficiency. Alternatively, we propose VoiceFlow, an acoustic model that utilizes a rectified flow matching algorithm to achieve high synthesis quality with a limited number of sampling steps. VoiceFlow formulates the process of generating mel-spectrograms into an ordinary differential equation conditional on text inputs, whose vector field is then estimated. The rectified flow technique then effectively straightens its sampling trajectory for efficient synthesis. Subjective and objective evaluations on both single and multi-speaker corpora showed the superior synthesis quality of VoiceFlow compared to the diffusion counterpart. Ablation studies further verified the validity of the rectified flow technique in VoiceFlow.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10445948",
      "openalex_id": "https://openalex.org/W4392904491",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Generation-Based Target Speech Extraction with Speech Discretization and Vocoder",
      "summary": "Target speech extraction (TSE) is a task aiming at isolating the speech of a specific target speaker from an audio mixture, with the help of an auxiliary recording of that target speaker. Most existing TSE methods employ discrimination-based models to estimate the target speaker's proportion in the mixture, but they often fail to compensate for the missing or highly corrupted frequency components in the speech signal. In contrast, the generation-based methods can naturally handle such scenarios via speech resynthesis. In this paper, we propose a novel discrete token based TSE approach by combining state-of-the-art speech discretization and vocoder techniques. By predicting a sequence of discrete tokens with the auxiliary audio and employing a vocoder that takes discrete tokens as input, the target speech can be effectively re-synthesized while eliminating interference. Our experiments conducted on the WSJ0-2mix and Libri2mix datasets demonstrate that our proposed method yields high-quality target speech without interference.",
      "abstract": "Target speech extraction (TSE) is a task aiming at isolating the speech of a specific target speaker from an audio mixture, with the help of an auxiliary recording of that target speaker. Most existing TSE methods employ discrimination-based models to estimate the target speaker's proportion in the mixture, but they often fail to compensate for the missing or highly corrupted frequency components in the speech signal. In contrast, the generation-based methods can naturally handle such scenarios via speech resynthesis. In this paper, we propose a novel discrete token based TSE approach by combining state-of-the-art speech discretization and vocoder techniques. By predicting a sequence of discrete tokens with the auxiliary audio and employing a vocoder that takes discrete tokens as input, the target speech can be effectively re-synthesized while eliminating interference. Our experiments conducted on the WSJ0-2mix and Libri2mix datasets demonstrate that our proposed method yields high-quality target speech without interference.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446418",
      "openalex_id": "https://openalex.org/W4392903977",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "STEMGEN: A Music Generation Model That Listens",
      "summary": "End-to-end generation of musical audio using deep learning techniques has seen an explosion of activity recently. However, most models concentrate on generating fully mixed music in response to abstract conditioning information. In this work, we present an alternative paradigm for producing music generation models that can listen and respond to musical context. We describe how such a model can be constructed using a non-autoregressive, transformer-based model architecture and present a number of novel architectural and sampling improvements. We train the described architecture on both an open-source and a proprietary dataset. We evaluate the produced models using standard quality metrics and a new approach based on music information retrieval descriptors. The resulting model reaches the audio quality of state-of-the-art text-conditioned models, as well as exhibiting strong musical coherence with its context.",
      "abstract": "End-to-end generation of musical audio using deep learning techniques has seen an explosion of activity recently. However, most models concentrate on generating fully mixed music in response to abstract conditioning information. In this work, we present an alternative paradigm for producing music generation models that can listen and respond to musical context. We describe how such a model can be constructed using a non-autoregressive, transformer-based model architecture and present a number of novel architectural and sampling improvements. We train the described architecture on both an open-source and a proprietary dataset. We evaluate the produced models using standard quality metrics and a new approach based on music information retrieval descriptors. The resulting model reaches the audio quality of state-of-the-art text-conditioned models, as well as exhibiting strong musical coherence with its context.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446088",
      "openalex_id": "https://openalex.org/W4392904237",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "V2Meow: Meowing to the Visual Beat via Video-to-Music Generation",
      "summary": "Video-to-music generation demands both a temporally localized high-quality listening experience and globally aligned video-acoustic signatures. While recent music generation models excel at the former through advanced audio codecs, the exploration of video-acoustic signatures has been confined to specific visual scenarios. In contrast, our research confronts the challenge of learning globally aligned signatures between video and music directly from paired music and videos, without explicitly modeling domain-specific rhythmic or semantic relationships. We propose V2Meow, a video-to-music generation system capable of producing high-quality music audio for a diverse range of video input types using a multi-stage autoregressive model. Trained on 5k hours of music audio clips paired with video frames mined from in-the-wild music videos, V2Meow is competitive with previous domain-specific models when evaluated in a zero-shot manner. It synthesizes high-fidelity music audio waveforms solely by conditioning on pre-trained general-purpose visual features extracted from video frames, with optional style control via text prompts. Through both qualitative and quantitative evaluations, we demonstrate that our model outperforms various existing music generation systems in terms of visual-audio correspondence and audio quality. Music samples are available at tinyurl.com/v2meow.",
      "abstract": "Video-to-music generation demands both a temporally localized high-quality listening experience and globally aligned video-acoustic signatures. While recent music generation models excel at the former through advanced audio codecs, the exploration of video-acoustic signatures has been confined to specific visual scenarios. In contrast, our research confronts the challenge of learning globally aligned signatures between video and music directly from paired music and videos, without explicitly modeling domain-specific rhythmic or semantic relationships. We propose V2Meow, a video-to-music generation system capable of producing high-quality music audio for a diverse range of video input types using a multi-stage autoregressive model. Trained on 5k hours of music audio clips paired with video frames mined from in-the-wild music videos, V2Meow is competitive with previous domain-specific models when evaluated in a zero-shot manner. It synthesizes high-fidelity music audio waveforms solely by conditioning on pre-trained general-purpose visual features extracted from video frames, with optional style control via text prompts. Through both qualitative and quantitative evaluations, we demonstrate that our model outperforms various existing music generation systems in terms of visual-audio correspondence and audio quality. Music samples are available at tinyurl.com/v2meow.",
      "doi": "https://doi.org/10.1609/aaai.v38i5.28299",
      "openalex_id": "https://openalex.org/W4393148499",
      "arxiv_id": "",
      "publication_date": "2024-03-24",
      "published": "2024-03-24",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Generalized Multi-Source Inference for Text Conditioned Music Diffusion Models",
      "summary": "Multi-Source Diffusion Models (MSDM) allow for compositional musical generation tasks: generating a set of coherent sources, creating accompaniments, and performing source separation. Despite their versatility, they require estimating the joint distribution over the sources, necessitating pre-separated musical data, which is rarely available, and fixing the number and type of sources at training time. This paper generalizes MSDM to arbitrary time-domain diffusion models conditioned on text embeddings. These models do not require separated data as they are trained on mixtures, can parameterize an arbitrary number of sources, and allow for rich semantic control. We propose an inference procedure enabling the coherent generation of sources and accompaniments. Additionally, we adapt the Dirac separator of MSDM to perform source separation. We experiment with diffusion models trained on Slakh2100 and MTG-Jamendo, showcasing competitive generation and separation results in a relaxed data setting.",
      "abstract": "Multi-Source Diffusion Models (MSDM) allow for compositional musical generation tasks: generating a set of coherent sources, creating accompaniments, and performing source separation. Despite their versatility, they require estimating the joint distribution over the sources, necessitating pre-separated musical data, which is rarely available, and fixing the number and type of sources at training time. This paper generalizes MSDM to arbitrary time-domain diffusion models conditioned on text embeddings. These models do not require separated data as they are trained on mixtures, can parameterize an arbitrary number of sources, and allow for rich semantic control. We propose an inference procedure enabling the coherent generation of sources and accompaniments. Additionally, we adapt the Dirac separator of MSDM to perform source separation. We experiment with diffusion models trained on Slakh2100 and MTG-Jamendo, showcasing competitive generation and separation results in a relaxed data setting.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447122",
      "openalex_id": "https://openalex.org/W4392904047",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "CPTGZ: Generating Chinese Guzheng Music From Chinese Paintings Based on Diffusion Model",
      "summary": "In the context of rapid advancements in artificial intelligence technology, AI-powered music composition has demonstrated remarkable creative capabilities. However, no existing music generation model has been able to produce authentic waveform-level traditional Chinese music. To explore the potential of this field and address the limitations of current technologies in generating traditional Chinese music, this study introduces CPTGZ (Chinese Painting to Guzheng Music), a music generation model based on latent diffusion and Transformer architectures. CPTGZ aims to achieve automatic generation of waveform-level Guzheng music from Chinese paintings, thereby addressing the inability of existing music generation models to produce traditional Chinese music.To support the development and training of the model, we constructed a large-scale dataset of paired Chinese paintings and Guzheng music, consisting of 22,103 sample pairs. Through experimental evaluation, we found that CPTGZ exhibits excellent performance in terms of music quality and Guzheng-specific characteristics. The results demonstrate that our model can generate Chinese Guzheng music pieces highly correlated in style and semantics with the input Chinese paintings. Furthermore, the musical qualities of the generated Guzheng compositions demonstrate the characteristics of traditional Chinese music, thus validating the feasibility and effectiveness of our model.This research contributes to the field of AI-driven music generation by addressing the specific challenges of creating authentic traditional Chinese music, particularly Guzheng compositions, based on visual art inputs. The successful implementation of CPTGZ not only opens new avenues for cross-modal generation in the domain of culturally specific art forms, but also demonstrates the potential for AI to preserve and innovate within traditional art forms.",
      "abstract": "In the context of rapid advancements in artificial intelligence technology, AI-powered music composition has demonstrated remarkable creative capabilities. However, no existing music generation model has been able to produce authentic waveform-level traditional Chinese music. To explore the potential of this field and address the limitations of current technologies in generating traditional Chinese music, this study introduces CPTGZ (Chinese Painting to Guzheng Music), a music generation model based on latent diffusion and Transformer architectures. CPTGZ aims to achieve automatic generation of waveform-level Guzheng music from Chinese paintings, thereby addressing the inability of existing music generation models to produce traditional Chinese music.To support the development and training of the model, we constructed a large-scale dataset of paired Chinese paintings and Guzheng music, consisting of 22,103 sample pairs. Through experimental evaluation, we found that CPTGZ exhibits excellent performance in terms of music quality and Guzheng-specific characteristics. The results demonstrate that our model can generate Chinese Guzheng music pieces highly correlated in style and semantics with the input Chinese paintings. Furthermore, the musical qualities of the generated Guzheng compositions demonstrate the characteristics of traditional Chinese music, thus validating the feasibility and effectiveness of our model.This research contributes to the field of AI-driven music generation by addressing the specific challenges of creating authentic traditional Chinese music, particularly Guzheng compositions, based on visual art inputs. The successful implementation of CPTGZ not only opens new avenues for cross-modal generation in the domain of culturally specific art forms, but also demonstrates the potential for AI to preserve and innovate within traditional art forms.",
      "doi": "https://doi.org/10.1109/access.2024.3476998",
      "openalex_id": "https://openalex.org/W4403277663",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Stack-and-Delay: A New Codebook Pattern for Music Generation",
      "summary": "Language modeling based music generation relies on discrete representations of audio frames. An audio frame (e.g. 20ms) is typically represented by a set of discrete codes (e.g. 4) computed by a neural codec. Autoregressive decoding typically generates a few thousands of codes per song, which is prohibitively slow and implies introducing some parallel decoding. In this paper we compare different decoding strategies that aim to understand what codes can be decoded in parallel without penalizing the quality too much. We propose a novel stack-and-delay style of decoding to improve upon the vanilla (flattened codes) decoding, with a 4 fold inference speedup. This brings inference speed close to that of the previous state of the art (delay strategy). For the same inference efficiency budget the proposed approach outperforms in objective evaluations, almost closing the gap with vanilla quality-wise. The results are supported by spectral analysis and listening tests, which demonstrate that the samples produced by the new model exhibit improved high-frequency rendering and better maintenance of harmonics and rhythm patterns.",
      "abstract": "Language modeling based music generation relies on discrete representations of audio frames. An audio frame (e.g. 20ms) is typically represented by a set of discrete codes (e.g. 4) computed by a neural codec. Autoregressive decoding typically generates a few thousands of codes per song, which is prohibitively slow and implies introducing some parallel decoding. In this paper we compare different decoding strategies that aim to understand what codes can be decoded in parallel without penalizing the quality too much. We propose a novel stack-and-delay style of decoding to improve upon the vanilla (flattened codes) decoding, with a 4 fold inference speedup. This brings inference speed close to that of the previous state of the art (delay strategy). For the same inference efficiency budget the proposed approach outperforms in objective evaluations, almost closing the gap with vanilla quality-wise. The results are supported by spectral analysis and listening tests, which demonstrate that the samples produced by the new model exhibit improved high-frequency rendering and better maintenance of harmonics and rhythm patterns.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10447392",
      "openalex_id": "https://openalex.org/W4392909680",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
      "summary": "Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-modal content. With discrete speech representations, we construct SpeechInstruct, the first large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow cross-modal human instructions and highlight the potential of handling multiple modalities with one model. Code and models are available in https://github.com/0nutation/SpeechGPT. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.",
      "abstract": "Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-modal content. With discrete speech representations, we construct SpeechInstruct, the first large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow cross-modal human instructions and highlight the potential of handling multiple modalities with one model. Code and models are available in https://github.com/0nutation/SpeechGPT. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.",
      "doi": "https://doi.org/10.18653/v1/2023.findings-emnlp.1055",
      "openalex_id": "https://openalex.org/W4389524500",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MRRL: Modifying the Reference via Reinforcement Learning for Non-Autoregressive Joint Multiple Intent Detection and Slot Filling",
      "summary": "With the rise of non-autoregressive approach, some non-autoregressive models for joint multiple intent detection and slot filling have obtained the promising inference speed. However, most existing SLU models (1) suffer from the multi-modality problem that leads to reference intents and slots may not be suitable for training; (2) lack of alignment between the correct predictions of the two tasks, which extremely limits the overall accuracy. Therefore, in this paper, we propose Modifying the Reference via Reinforcement Learning (MRRL), a novel method for multiple intent detection and slot filling, which introduces a modifier and employs reinforcement learning. Specifically, we try to provide the better training target for the non-autoregressive SLU model via modifying the reference based on the output of the non-autoregressive SLU model, and propose a suitability reward to ensure that the output of the modifier module could fit well with the output of the non-autoregressive SLU model and does not deviate too far from the reference. In addition, we also propose a compromise reward to realize a flexible trade-off between the two subtasks. Experiments on two multi-intent datasets and non-autoregressive baselines demonstrate that our MRRL could consistently improve the performance of baselines. More encouragingly, our best variant achieves new state-of-the-art results, outperforming the previous best approach by 3.6 overall accuracy on MixATIS dataset.",
      "abstract": "With the rise of non-autoregressive approach, some non-autoregressive models for joint multiple intent detection and slot filling have obtained the promising inference speed. However, most existing SLU models (1) suffer from the multi-modality problem that leads to reference intents and slots may not be suitable for training; (2) lack of alignment between the correct predictions of the two tasks, which extremely limits the overall accuracy. Therefore, in this paper, we propose Modifying the Reference via Reinforcement Learning (MRRL), a novel method for multiple intent detection and slot filling, which introduces a modifier and employs reinforcement learning. Specifically, we try to provide the better training target for the non-autoregressive SLU model via modifying the reference based on the output of the non-autoregressive SLU model, and propose a suitability reward to ensure that the output of the modifier module could fit well with the output of the non-autoregressive SLU model and does not deviate too far from the reference. In addition, we also propose a compromise reward to realize a flexible trade-off between the two subtasks. Experiments on two multi-intent datasets and non-autoregressive baselines demonstrate that our MRRL could consistently improve the performance of baselines. More encouragingly, our best variant achieves new state-of-the-art results, outperforming the previous best approach by 3.6 overall accuracy on MixATIS dataset.",
      "doi": "https://doi.org/10.18653/v1/2023.findings-emnlp.704",
      "openalex_id": "https://openalex.org/W4389519009",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Optical Character Recognition Systems for Accurate Interpretation of Handwritten Telugu Scripts",
      "summary": "Visual data dominates the digital landscape these days. However, there remains a significant gap in the digitization of textual content in regional languages such as Telugu. This research addresses this gap and utilizes the capabilities of the programming language and set of specialized libraries pytesseract for optical character recognition (OCR), PIL for image processing and Levenshtein for accuracy evaluation to appropriately extract text from images in the Telugu script. There are several important steps in this project. Initially, this work sets up a dependable system for uploading pictures. Secondly, carefully extracting text from these submitted photos using py-tesseract. Using the Levenshtein distance measure, and then compared with the ground truth to determine correctness. This metric measures the accuracy of the text output through looking at the slight variations between the word error rate (WER) and character error rate (CER). This is capable of finding misspelled words and unknown characters providing a precise evaluation of the OCR's effectiveness. This study also examines the current state of OCR in Telugu text processing and indicates the benefits of current technologies for locating and converting Telugu text discovered in image documents. It also demonstrates improvements in OCR technology.",
      "abstract": "Visual data dominates the digital landscape these days. However, there remains a significant gap in the digitization of textual content in regional languages such as Telugu. This research addresses this gap and utilizes the capabilities of the programming language and set of specialized libraries pytesseract for optical character recognition (OCR), PIL for image processing and Levenshtein for accuracy evaluation to appropriately extract text from images in the Telugu script. There are several important steps in this project. Initially, this work sets up a dependable system for uploading pictures. Secondly, carefully extracting text from these submitted photos using py-tesseract. Using the Levenshtein distance measure, and then compared with the ground truth to determine correctness. This metric measures the accuracy of the text output through looking at the slight variations between the word error rate (WER) and character error rate (CER). This is capable of finding misspelled words and unknown characters providing a precise evaluation of the OCR's effectiveness. This study also examines the current state of OCR in Telugu text processing and indicates the benefits of current technologies for locating and converting Telugu text discovered in image documents. It also demonstrates improvements in OCR technology.",
      "doi": "https://doi.org/10.1109/ic2pct60090.2024.10486323",
      "openalex_id": "https://openalex.org/W4394583388",
      "arxiv_id": "",
      "publication_date": "2024-02-09",
      "published": "2024-02-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone",
      "summary": "YourTTS brings the power of a multilingual approach to the task of zero-shot multi-speaker TTS. Our method builds upon the VITS model and adds several novel modifications for zero-shot multi-speaker and multilingual training. We achieved state-of-the-art (SOTA) results in zero-shot multi-speaker TTS and results comparable to SOTA in zero-shot voice conversion on the VCTK dataset. Additionally, our approach achieves promising results in a target language with a single-speaker dataset, opening possibilities for zero-shot multi-speaker TTS and zero-shot voice conversion systems in low-resource languages. Finally, it is possible to fine-tune the YourTTS model with less than 1 minute of speech and achieve state-of-the-art results in voice similarity and with reasonable quality. This is important to allow synthesis for speakers with a very different voice or recording characteristics from those seen during training.",
      "abstract": "YourTTS brings the power of a multilingual approach to the task of zero-shot multi-speaker TTS. Our method builds upon the VITS model and adds several novel modifications for zero-shot multi-speaker and multilingual training. We achieved state-of-the-art (SOTA) results in zero-shot multi-speaker TTS and results comparable to SOTA in zero-shot voice conversion on the VCTK dataset. Additionally, our approach achieves promising results in a target language with a single-speaker dataset, opening possibilities for zero-shot multi-speaker TTS and zero-shot voice conversion systems in low-resource languages. Finally, it is possible to fine-tune the YourTTS model with less than 1 minute of speech and achieve state-of-the-art results in voice similarity and with reasonable quality. This is important to allow synthesis for speakers with a very different voice or recording characteristics from those seen during training.",
      "doi": "https://doi.org/10.48550/arxiv.2112.02418",
      "openalex_id": "https://openalex.org/W4200631896",
      "arxiv_id": "",
      "publication_date": "2021-12-04",
      "published": "2021-12-04",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "VQTTS: High-Fidelity Text-to-Speech Synthesis with Self-Supervised VQ Acoustic Feature",
      "summary": "The mainstream neural text-to-speech(TTS) pipeline is a cascade system,\\nincluding an acoustic model(AM) that predicts acoustic feature from the input\\ntranscript and a vocoder that generates waveform according to the given\\nacoustic feature. However, the acoustic feature in current TTS systems is\\ntypically mel-spectrogram, which is highly correlated along both time and\\nfrequency axes in a complicated way, leading to a great difficulty for the AM\\nto predict. Although high-fidelity audio can be generated by recent neural\\nvocoders from ground-truth(GT) mel-spectrogram, the gap between the GT and the\\npredicted mel-spectrogram from AM degrades the performance of the entire TTS\\nsystem. In this work, we propose VQTTS, consisting of an AM txt2vec and a\\nvocoder vec2wav, which uses self-supervised vector-quantized(VQ) acoustic\\nfeature rather than mel-spectrogram. We redesign both the AM and the vocoder\\naccordingly. In particular, txt2vec basically becomes a classification model\\ninstead of a traditional regression model while vec2wav uses an additional\\nfeature encoder before HifiGAN generator for smoothing the discontinuous\\nquantized feature. Our experiments show that vec2wav achieves better\\nreconstruction performance than HifiGAN when using self-supervised VQ acoustic\\nfeature. Moreover, our entire TTS system VQTTS achieves state-of-the-art\\nperformance in terms of naturalness among all current publicly available TTS\\nsystems.\\n",
      "abstract": "The mainstream neural text-to-speech(TTS) pipeline is a cascade system,\\nincluding an acoustic model(AM) that predicts acoustic feature from the input\\ntranscript and a vocoder that generates waveform according to the given\\nacoustic feature. However, the acoustic feature in current TTS systems is\\ntypically mel-spectrogram, which is highly correlated along both time and\\nfrequency axes in a complicated way, leading to a great difficulty for the AM\\nto predict. Although high-fidelity audio can be generated by recent neural\\nvocoders from ground-truth(GT) mel-spectrogram, the gap between the GT and the\\npredicted mel-spectrogram from AM degrades the performance of the entire TTS\\nsystem. In this work, we propose VQTTS, consisting of an AM txt2vec and a\\nvocoder vec2wav, which uses self-supervised vector-quantized(VQ) acoustic\\nfeature rather than mel-spectrogram. We redesign both the AM and the vocoder\\naccordingly. In particular, txt2vec basically becomes a classification model\\ninstead of a traditional regression model while vec2wav uses an additional\\nfeature encoder before HifiGAN generator for smoothing the discontinuous\\nquantized feature. Our experiments show that vec2wav achieves better\\nreconstruction performance than HifiGAN when using self-supervised VQ acoustic\\nfeature. Moreover, our entire TTS system VQTTS achieves state-of-the-art\\nperformance in terms of naturalness among all current publicly available TTS\\nsystems.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2022-489",
      "openalex_id": "https://openalex.org/W4226132755",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "A pitch extraction algorithm tuned for automatic speech recognition",
      "summary": "In this paper we present an algorithm that produces pitch and probability-of-voicing estimates for use as features in automatic speech recognition systems. These features give large performance improvements on tonal languages for ASR systems, and even substantial improvements for non-tonal languages. Our method, which we are calling the Kaldi pitch tracker (because we are adding it to the Kaldi ASR toolkit), is a highly modified version of the getf0 (RAPT) algorithm. Unlike the original getf0 we do not make a hard decision whether any given frame is voiced or unvoiced; instead, we assign a pitch even to unvoiced frames while constraining the pitch trajectory to be continuous. Our algorithm also produces a quantity that can be used as a probability of voicing measure; it is based on the normalized autocorrelation measure that our pitch extractor uses. We present results on data from various languages in the BABEL project, and show a large improvement over systems without tonal features and systems where pitch and POV information was obtained from SAcC or getf0.",
      "abstract": "In this paper we present an algorithm that produces pitch and probability-of-voicing estimates for use as features in automatic speech recognition systems. These features give large performance improvements on tonal languages for ASR systems, and even substantial improvements for non-tonal languages. Our method, which we are calling the Kaldi pitch tracker (because we are adding it to the Kaldi ASR toolkit), is a highly modified version of the getf0 (RAPT) algorithm. Unlike the original getf0 we do not make a hard decision whether any given frame is voiced or unvoiced; instead, we assign a pitch even to unvoiced frames while constraining the pitch trajectory to be continuous. Our algorithm also produces a quantity that can be used as a probability of voicing measure; it is based on the normalized autocorrelation measure that our pitch extractor uses. We present results on data from various languages in the BABEL project, and show a large improvement over systems without tonal features and systems where pitch and POV information was obtained from SAcC or getf0.",
      "doi": "https://doi.org/10.1109/icassp.2014.6854049",
      "openalex_id": "https://openalex.org/W2085628288",
      "arxiv_id": "",
      "publication_date": "2014-05-01",
      "published": "2014-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Vector Quantized Diffusion Model for Text-to-Image Synthesis",
      "summary": "We present the vector quantized diffusion (VQ-Diffusion) model for text-to-image generation. This method is based on a vector quantized variational autoencoder (VQ-VAE) whose latent space is modeled by a conditional variant of the recently developed Denoising Diffusion Probabilistic Model (DDPM). We find that this latent-space method is well-suited for text-to-image generation tasks because it not only eliminates the unidirectional bias with existing methods but also allows us to incorporate a mask-and-replace diffusion strategy to avoid the accumulation of errors, which is a serious problem with existing methods. Our experiments show that the VQ-Diffusion produces significantly better text-to-image generation results when compared with conventional autoregressive (AR) models with similar numbers of parameters. Compared with previous GAN-based text-to-image methods, our VQ-Diffusion can handle more complex scenes and improve the synthesized image quality by a large margin. Finally, we show that the image generation computation in our method can be made highly efficient by reparameterization. With traditional AR methods, the text-to-image generation time increases linearly with the output image resolution and hence is quite time consuming even for normal size images. The VQ-Diffusion allows us to achieve a better trade-off between quality and speed. Our experiments indicate that the VQ-Diffusion model with the reparameterization is fifteen times faster than traditional AR methods while achieving a better image quality.",
      "abstract": "We present the vector quantized diffusion (VQ-Diffusion) model for text-to-image generation. This method is based on a vector quantized variational autoencoder (VQ-VAE) whose latent space is modeled by a conditional variant of the recently developed Denoising Diffusion Probabilistic Model (DDPM). We find that this latent-space method is well-suited for text-to-image generation tasks because it not only eliminates the unidirectional bias with existing methods but also allows us to incorporate a mask-and-replace diffusion strategy to avoid the accumulation of errors, which is a serious problem with existing methods. Our experiments show that the VQ-Diffusion produces significantly better text-to-image generation results when compared with conventional autoregressive (AR) models with similar numbers of parameters. Compared with previous GAN-based text-to-image methods, our VQ-Diffusion can handle more complex scenes and improve the synthesized image quality by a large margin. Finally, we show that the image generation computation in our method can be made highly efficient by reparameterization. With traditional AR methods, the text-to-image generation time increases linearly with the output image resolution and hence is quite time consuming even for normal size images. The VQ-Diffusion allows us to achieve a better trade-off between quality and speed. Our experiments indicate that the VQ-Diffusion model with the reparameterization is fifteen times faster than traditional AR methods while achieving a better image quality.",
      "doi": "https://doi.org/10.48550/arxiv.2111.14822",
      "openalex_id": "https://openalex.org/W3217345456",
      "arxiv_id": "",
      "publication_date": "2021-11-29",
      "published": "2021-11-29",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
      "summary": "Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.",
      "abstract": "Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.",
      "doi": "https://doi.org/10.48550/arxiv.2005.08100",
      "openalex_id": "https://openalex.org/W3025165719",
      "arxiv_id": "",
      "publication_date": "2020-05-16",
      "published": "2020-05-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "summary": "Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.",
      "abstract": "Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.",
      "doi": "https://doi.org/10.1109/taslp.2021.3122291",
      "openalex_id": "https://openalex.org/W3169320628",
      "arxiv_id": "",
      "publication_date": "2021-01-01",
      "published": "2021-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search",
      "summary": "Recently, text-to-speech (TTS) models such as FastSpeech and ParaNet have been proposed to generate mel-spectrograms from text in parallel. Despite the advantage, the parallel TTS models cannot be trained without guidance from autoregressive TTS models as their external aligners. In this work, we propose Glow-TTS, a flow-based generative model for parallel TTS that does not require any external aligner. By combining the properties of flows and dynamic programming, the proposed model searches for the most probable monotonic alignment between text and the latent representation of speech on its own. We demonstrate that enforcing hard monotonic alignments enables robust TTS, which generalizes to long utterances, and employing generative flows enables fast, diverse, and controllable speech synthesis. Glow-TTS obtains an order-of-magnitude speed-up over the autoregressive model, Tacotron 2, at synthesis with comparable speech quality. We further show that our model can be easily extended to a multi-speaker setting.",
      "abstract": "Recently, text-to-speech (TTS) models such as FastSpeech and ParaNet have been proposed to generate mel-spectrograms from text in parallel. Despite the advantage, the parallel TTS models cannot be trained without guidance from autoregressive TTS models as their external aligners. In this work, we propose Glow-TTS, a flow-based generative model for parallel TTS that does not require any external aligner. By combining the properties of flows and dynamic programming, the proposed model searches for the most probable monotonic alignment between text and the latent representation of speech on its own. We demonstrate that enforcing hard monotonic alignments enables robust TTS, which generalizes to long utterances, and employing generative flows enables fast, diverse, and controllable speech synthesis. Glow-TTS obtains an order-of-magnitude speed-up over the autoregressive model, Tacotron 2, at synthesis with comparable speech quality. We further show that our model can be easily extended to a multi-speaker setting.",
      "doi": "https://doi.org/10.48550/arxiv.2005.11129",
      "openalex_id": "https://openalex.org/W3026874504",
      "arxiv_id": "",
      "publication_date": "2020-05-22",
      "published": "2020-05-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis",
      "summary": "Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.",
      "abstract": "Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.",
      "doi": "https://doi.org/10.48550/arxiv.2010.05646",
      "openalex_id": "https://openalex.org/W3092028330",
      "arxiv_id": "",
      "publication_date": "2020-10-12",
      "published": "2020-10-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speech Resynthesis from Discrete Disentangled Self-Supervised Representations",
      "summary": "We propose using self-supervised discrete representations for the task of\\nspeech resynthesis. To generate disentangled representation, we separately\\nextract low-bitrate representations for speech content, prosodic information,\\nand speaker identity. This allows to synthesize speech in a controllable\\nmanner. We analyze various state-of-the-art, self-supervised representation\\nlearning methods and shed light on the advantages of each method while\\nconsidering reconstruction quality and disentanglement properties.\\nSpecifically, we evaluate the F0 reconstruction, speaker identification\\nperformance (for both resynthesis and voice conversion), recordings'\\nintelligibility, and overall quality using subjective human evaluation. Lastly,\\nwe demonstrate how these representations can be used for an ultra-lightweight\\nspeech codec. Using the obtained representations, we can get to a rate of 365\\nbits per second while providing better speech quality than the baseline\\nmethods. Audio samples can be found under the following link:\\nspeechbot.github.io/resynthesis.\\n",
      "abstract": "We propose using self-supervised discrete representations for the task of\\nspeech resynthesis. To generate disentangled representation, we separately\\nextract low-bitrate representations for speech content, prosodic information,\\nand speaker identity. This allows to synthesize speech in a controllable\\nmanner. We analyze various state-of-the-art, self-supervised representation\\nlearning methods and shed light on the advantages of each method while\\nconsidering reconstruction quality and disentanglement properties.\\nSpecifically, we evaluate the F0 reconstruction, speaker identification\\nperformance (for both resynthesis and voice conversion), recordings'\\nintelligibility, and overall quality using subjective human evaluation. Lastly,\\nwe demonstrate how these representations can be used for an ultra-lightweight\\nspeech codec. Using the obtained representations, we can get to a rate of 365\\nbits per second while providing better speech quality than the baseline\\nmethods. Audio samples can be found under the following link:\\nspeechbot.github.io/resynthesis.\\n",
      "doi": "https://doi.org/10.21437/interspeech.2021-475",
      "openalex_id": "https://openalex.org/W3140429000",
      "arxiv_id": "",
      "publication_date": "2021-08-27",
      "published": "2021-08-27",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech",
      "summary": "Recently, denoising diffusion probabilistic models and generative score matching have shown high potential in modelling complex data distributions while stochastic calculus has provided a unified point of view on these techniques allowing for flexible inference schemes. In this paper we introduce Grad-TTS, a novel text-to-speech model with score-based decoder producing mel-spectrograms by gradually transforming noise predicted by encoder and aligned with text input by means of Monotonic Alignment Search. The framework of stochastic differential equations helps us to generalize conventional diffusion probabilistic models to the case of reconstructing data from noise with different parameters and allows to make this reconstruction flexible by explicitly controlling trade-off between sound quality and inference speed. Subjective human evaluation shows that Grad-TTS is competitive with state-of-the-art text-to-speech approaches in terms of Mean Opinion Score. We will make the code publicly available shortly.",
      "abstract": "Recently, denoising diffusion probabilistic models and generative score matching have shown high potential in modelling complex data distributions while stochastic calculus has provided a unified point of view on these techniques allowing for flexible inference schemes. In this paper we introduce Grad-TTS, a novel text-to-speech model with score-based decoder producing mel-spectrograms by gradually transforming noise predicted by encoder and aligned with text input by means of Monotonic Alignment Search. The framework of stochastic differential equations helps us to generalize conventional diffusion probabilistic models to the case of reconstructing data from noise with different parameters and allows to make this reconstruction flexible by explicitly controlling trade-off between sound quality and inference speed. Subjective human evaluation shows that Grad-TTS is competitive with state-of-the-art text-to-speech approaches in terms of Mean Opinion Score. We will make the code publicly available shortly.",
      "doi": "https://doi.org/10.48550/arxiv.2105.06337",
      "openalex_id": "https://openalex.org/W3162673269",
      "arxiv_id": "",
      "publication_date": "2021-05-13",
      "published": "2021-05-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "X-Vectors: Robust DNN Embeddings for Speaker Recognition",
      "summary": "In this paper, we use data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition. The DNN, which is trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings that we call x-vectors. Prior studies have found that embeddings leverage large-scale training datasets better than i-vectors. However, it can be challenging to collect substantial quantities of labeled data for training. We use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese. We find that while augmentation is beneficial in the PLDA classifier, it is not helpful in the i-vector extractor. However, the x-vector DNN effectively exploits data augmentation, due to its supervised training. As a result, the x-vectors achieve superior performance on the evaluation datasets.",
      "abstract": "In this paper, we use data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition. The DNN, which is trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings that we call x-vectors. Prior studies have found that embeddings leverage large-scale training datasets better than i-vectors. However, it can be challenging to collect substantial quantities of labeled data for training. We use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese. We find that while augmentation is beneficial in the PLDA classifier, it is not helpful in the i-vector extractor. However, the x-vector DNN effectively exploits data augmentation, due to its supervised training. As a result, the x-vectors achieve superior performance on the evaluation datasets.",
      "doi": "https://doi.org/10.1109/icassp.2018.8461375",
      "openalex_id": "https://openalex.org/W2890964092",
      "arxiv_id": "",
      "publication_date": "2018-04-01",
      "published": "2018-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "EdiTTS: Score-based Editing for Controllable Text-to-Speech",
      "summary": "We present EdiTTS, an off-the-shelf speech editing methodology based on score-based generative modeling for text-to-speech synthesis. EdiTTS allows for targeted, granular editing of audio, both in terms of content and pitch, without the need for any additional training, task-specific optimization, or architectural modifications to the score-based model backbone. Specifically, we apply coarse yet deliberate perturbations in the Gaussian prior space to induce desired behavior from the diffusion model while applying masks and softening kernels to ensure that iterative edits are applied only to the target region. Through listening tests and speech-to-text back transcription, we show that EdiTTS outperforms existing baselines and produces robust samples that satisfy user-imposed requirements.",
      "abstract": "We present EdiTTS, an off-the-shelf speech editing methodology based on score-based generative modeling for text-to-speech synthesis. EdiTTS allows for targeted, granular editing of audio, both in terms of content and pitch, without the need for any additional training, task-specific optimization, or architectural modifications to the score-based model backbone. Specifically, we apply coarse yet deliberate perturbations in the Gaussian prior space to induce desired behavior from the diffusion model while applying masks and softening kernels to ensure that iterative edits are applied only to the target region. Through listening tests and speech-to-text back transcription, we show that EdiTTS outperforms existing baselines and produces robust samples that satisfy user-imposed requirements.",
      "doi": "https://doi.org/10.48550/arxiv.2110.02584",
      "openalex_id": "https://openalex.org/W3204550533",
      "arxiv_id": "",
      "publication_date": "2021-10-06",
      "published": "2021-10-06",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "RetrieverTTS: Modeling Decomposed Factors for Text-Based Speech Insertion",
      "summary": "This paper proposes a new \"decompose-and-edit\" paradigm for the text-based speech insertion task that facilitates arbitrarylength speech insertion and even full sentence generation.In the proposed paradigm, global and local factors in speech are explicitly decomposed and separately manipulated to achieve high speaker similarity and continuous prosody.Specifically, we proposed to represent the global factors by multiple tokens, which are extracted by cross-attention operation and then injected back by link-attention operation.Due to the rich representation of global factors, we manage to achieve high speaker similarity in a zero-shot manner.In addition, we introduce a prosody smoothing task to make the local prosody factor context-aware and therefore achieve satisfactory prosody continuity.We further achieve high voice quality with an adversarial training stage.In the subjective test, our method achieves state-of-the-art performance in both naturalness and similarity.Audio samples can be found at https://ydcustc.github.io/retrieverTTS-demo/.",
      "abstract": "This paper proposes a new \"decompose-and-edit\" paradigm for the text-based speech insertion task that facilitates arbitrarylength speech insertion and even full sentence generation.In the proposed paradigm, global and local factors in speech are explicitly decomposed and separately manipulated to achieve high speaker similarity and continuous prosody.Specifically, we proposed to represent the global factors by multiple tokens, which are extracted by cross-attention operation and then injected back by link-attention operation.Due to the rich representation of global factors, we manage to achieve high speaker similarity in a zero-shot manner.In addition, we introduce a prosody smoothing task to make the local prosody factor context-aware and therefore achieve satisfactory prosody continuity.We further achieve high voice quality with an adversarial training stage.In the subjective test, our method achieves state-of-the-art performance in both naturalness and similarity.Audio samples can be found at https://ydcustc.github.io/retrieverTTS-demo/.",
      "doi": "https://doi.org/10.21437/interspeech.2022-245",
      "openalex_id": "https://openalex.org/W4283722828",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations",
      "summary": "We propose vq-wav2vec to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The algorithm uses either a gumbel softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition.",
      "abstract": "We propose vq-wav2vec to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The algorithm uses either a gumbel softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition.",
      "doi": "https://doi.org/10.48550/arxiv.1910.05453",
      "openalex_id": "https://openalex.org/W2979476256",
      "arxiv_id": "",
      "publication_date": "2019-10-12",
      "published": "2019-10-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ESPnet: End-to-End Speech Processing Toolkit",
      "summary": "This paper introduces a new open source platform for end-to-end speech processing named ESPnet. ESPnet mainly focuses on end-to-end automatic speech recognition (ASR), and adopts widely-used dynamic neural network toolkits, Chainer and PyTorch, as a main deep learning engine. ESPnet also follows the Kaldi ASR toolkit style for data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments. This paper explains a major architecture of this software platform, several important functionalities, which differentiate ESPnet from other open source ASR toolkits, and experimental results with major ASR benchmarks.",
      "abstract": "This paper introduces a new open source platform for end-to-end speech processing named ESPnet. ESPnet mainly focuses on end-to-end automatic speech recognition (ASR), and adopts widely-used dynamic neural network toolkits, Chainer and PyTorch, as a main deep learning engine. ESPnet also follows the Kaldi ASR toolkit style for data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments. This paper explains a major architecture of this software platform, several important functionalities, which differentiate ESPnet from other open source ASR toolkits, and experimental results with major ASR benchmarks.",
      "doi": "https://doi.org/10.21437/interspeech.2018-1456",
      "openalex_id": "https://openalex.org/W2962780374",
      "arxiv_id": "",
      "publication_date": "2018-08-28",
      "published": "2018-08-28",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision",
      "summary": "Abstract We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to “reading”) and from semantic tokens to low-level acoustic tokens (“speaking”). Decoupling these two tasks enables training of the “speaking” module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the “reading” component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in naturalness and acoustic quality.",
      "abstract": "Abstract We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to “reading”) and from semantic tokens to low-level acoustic tokens (“speaking”). Decoupling these two tasks enables training of the “speaking” module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the “reading” component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in naturalness and acoustic quality.",
      "doi": "https://doi.org/10.1162/tacl_a_00618",
      "openalex_id": "https://openalex.org/W4390075359",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "DiffVoice: Text-to-Speech with Latent Diffusion",
      "summary": "In this work, we present DiffVoice, a novel text-to-speech model based on latent diffusion. We propose to first encode speech signals into a phoneme-rate latent representation with a variational autoencoder enhanced by adversarial training, and then jointly model the duration and the latent representation with a diffusion model. Subjective evaluations on LJSpeech and LibriTTS datasets demonstrate that our method beats the best publicly available systems in naturalness. By adopting recent generative inverse problem solving algorithms for diffusion models, DiffVoice achieves the state-of-the-art performance in text-based speech editing, and zero-shot adaptation.",
      "abstract": "In this work, we present DiffVoice, a novel text-to-speech model based on latent diffusion. We propose to first encode speech signals into a phoneme-rate latent representation with a variational autoencoder enhanced by adversarial training, and then jointly model the duration and the latent representation with a diffusion model. Subjective evaluations on LJSpeech and LibriTTS datasets demonstrate that our method beats the best publicly available systems in naturalness. By adopting recent generative inverse problem solving algorithms for diffusion models, DiffVoice achieves the state-of-the-art performance in text-based speech editing, and zero-shot adaptation.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10095100",
      "openalex_id": "https://openalex.org/W4375869198",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers",
      "summary": "Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild datasets is important to capture the diversity in human speech such as speaker identities, prosodies, and styles (e.g., singing). Current large TTS systems usually quantize speech into discrete tokens and use language models to generate these tokens one by one, which suffer from unstable prosody, word skipping/repeating issue, and poor voice quality. In this paper, we develop NaturalSpeech 2, a TTS system that leverages a neural audio codec with residual vector quantizers to get the quantized latent vectors and uses a diffusion model to generate these latent vectors conditioned on text input. To enhance the zero-shot capability that is important to achieve diverse speech synthesis, we design a speech prompting mechanism to facilitate in-context learning in the diffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to large-scale datasets with 44K hours of speech and singing data and evaluate its voice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS systems by a large margin in terms of prosody/timbre similarity, robustness, and voice quality in a zero-shot setting, and performs novel zero-shot singing synthesis with only a speech prompt. Audio samples are available at https://speechresearch.github.io/naturalspeech2.",
      "abstract": "Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild datasets is important to capture the diversity in human speech such as speaker identities, prosodies, and styles (e.g., singing). Current large TTS systems usually quantize speech into discrete tokens and use language models to generate these tokens one by one, which suffer from unstable prosody, word skipping/repeating issue, and poor voice quality. In this paper, we develop NaturalSpeech 2, a TTS system that leverages a neural audio codec with residual vector quantizers to get the quantized latent vectors and uses a diffusion model to generate these latent vectors conditioned on text input. To enhance the zero-shot capability that is important to achieve diverse speech synthesis, we design a speech prompting mechanism to facilitate in-context learning in the diffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to large-scale datasets with 44K hours of speech and singing data and evaluate its voice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS systems by a large margin in terms of prosody/timbre similarity, robustness, and voice quality in a zero-shot setting, and performs novel zero-shot singing synthesis with only a speech prompt. Audio samples are available at https://speechresearch.github.io/naturalspeech2.",
      "doi": "https://doi.org/10.48550/arxiv.2304.09116",
      "openalex_id": "https://openalex.org/W4366460484",
      "arxiv_id": "",
      "publication_date": "2023-04-18",
      "published": "2023-04-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt",
      "summary": "Expressive text-to-speech (TTS) aims to synthesize different speaking style speech according to human's demands. Nowadays, there are two common ways to control speaking styles: (1) Pre-defining a group of speaking style and using categorical index to denote different speaking style. However, there are limitations in the diversity of expressiveness, as these models can only generate the pre-defined styles. (2) Using reference speech as style input, which results in a problem that the extracted style information is not intuitive or interpretable. In this study, we attempt to use natural language as style prompt to control the styles in the synthetic speech, e.g., \"Sigh tone in full of sad mood with some helpless feeling\". Considering that there is no existing TTS corpus which is proper to benchmark this novel task, we first construct a speech corpus, whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named as InstructTTS, which is novel in the sense of following aspects: (1) We fully take the advantage of self-supervised learning and cross-modal metric learning, and propose a novel three-stage training procedure to obtain a robust sentence embedding model, which can effectively capture semantic information from the style prompts and control the speaking style in the generated speech. (2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonly-used mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt.",
      "abstract": "Expressive text-to-speech (TTS) aims to synthesize different speaking style speech according to human's demands. Nowadays, there are two common ways to control speaking styles: (1) Pre-defining a group of speaking style and using categorical index to denote different speaking style. However, there are limitations in the diversity of expressiveness, as these models can only generate the pre-defined styles. (2) Using reference speech as style input, which results in a problem that the extracted style information is not intuitive or interpretable. In this study, we attempt to use natural language as style prompt to control the styles in the synthetic speech, e.g., \"Sigh tone in full of sad mood with some helpless feeling\". Considering that there is no existing TTS corpus which is proper to benchmark this novel task, we first construct a speech corpus, whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named as InstructTTS, which is novel in the sense of following aspects: (1) We fully take the advantage of self-supervised learning and cross-modal metric learning, and propose a novel three-stage training procedure to obtain a robust sentence embedding model, which can effectively capture semantic information from the style prompts and control the speaking style in the generated speech. (2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonly-used mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt.",
      "doi": "https://doi.org/10.48550/arxiv.2301.13662",
      "openalex_id": "https://openalex.org/W4318907788",
      "arxiv_id": "",
      "publication_date": "2023-01-31",
      "published": "2023-01-31",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Adam: A Method for Stochastic Optimization",
      "summary": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
      "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
      "doi": "https://doi.org/10.48550/arxiv.1412.6980",
      "openalex_id": "https://openalex.org/W1522301498",
      "arxiv_id": "",
      "publication_date": "2014-12-22",
      "published": "2014-12-22",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis",
      "summary": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with control over speech variation and style transfer. Flowtron borrows insights from IAF and revamps Tacotron in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be manipulated to control many aspects of speech synthesis (pitch, tone, speech rate, cadence, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. In addition, we provide results on control of speech variation, interpolation between samples and style transfer between speakers seen and unseen during training. Code and pre-trained models will be made publicly available at https://github.com/NVIDIA/flowtron",
      "abstract": "In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with control over speech variation and style transfer. Flowtron borrows insights from IAF and revamps Tacotron in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be manipulated to control many aspects of speech synthesis (pitch, tone, speech rate, cadence, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. In addition, we provide results on control of speech variation, interpolation between samples and style transfer between speakers seen and unseen during training. Code and pre-trained models will be made publicly available at https://github.com/NVIDIA/flowtron",
      "doi": "https://doi.org/10.48550/arxiv.2005.05957",
      "openalex_id": "https://openalex.org/W3025528898",
      "arxiv_id": "",
      "publication_date": "2020-05-12",
      "published": "2020-05-12",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "summary": "We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.",
      "abstract": "We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.",
      "doi": "https://doi.org/10.48550/arxiv.2006.11477",
      "openalex_id": "https://openalex.org/W3036601975",
      "arxiv_id": "",
      "publication_date": "2020-06-20",
      "published": "2020-06-20",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training",
      "summary": "Motivated by the success of masked language modeling (MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks (the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light 60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec 2.0 and HuBERT, our model shows 5% to 10% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec 2.0 by more than 30% relatively.",
      "abstract": "Motivated by the success of masked language modeling (MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks (the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light 60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec 2.0 and HuBERT, our model shows 5% to 10% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec 2.0 by more than 30% relatively.",
      "doi": "https://doi.org/10.1109/asru51503.2021.9688253",
      "openalex_id": "https://openalex.org/W4226033575",
      "arxiv_id": "",
      "publication_date": "2021-12-13",
      "published": "2021-12-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "FastSpeech 2: Fast and High-Quality End-to-End Text-to-Speech",
      "summary": "Non-autoregressive text to speech (TTS) models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even surpass autoregressive models. Audio samples are available at https://speechresearch.github.io/fastspeech2/.",
      "abstract": "Non-autoregressive text to speech (TTS) models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even surpass autoregressive models. Audio samples are available at https://speechresearch.github.io/fastspeech2/.",
      "doi": "https://doi.org/10.57702/v7a23pbp",
      "openalex_id": "https://openalex.org/W3033411150",
      "arxiv_id": "",
      "publication_date": "2024-01-01",
      "published": "2024-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "AudioLM: A Language Modeling Approach to Audio Generation",
      "summary": "We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.",
      "abstract": "We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.",
      "doi": "https://doi.org/10.1109/taslp.2023.3288409",
      "openalex_id": "https://openalex.org/W4381786045",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions",
      "summary": "This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.",
      "abstract": "This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.",
      "doi": "https://doi.org/10.1109/icassp.2018.8461368",
      "openalex_id": "https://openalex.org/W2964243274",
      "arxiv_id": "",
      "publication_date": "2018-04-01",
      "published": "2018-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech",
      "summary": "This paper introduces a new speech corpus called \"LibriTTS\" designed for text-to-speech use.It is derived from the original audio and text materials of the LibriSpeech corpus, which has been used for training and evaluating automatic speech recognition systems.The new corpus inherits desired properties of the LibriSpeech corpus while addressing a number of issues which make LibriSpeech less than ideal for text-to-speech work.The released corpus consists of 585 hours of speech data at 24kHz sampling rate from 2,456 speakers and the corresponding texts.Experimental results show that neural end-to-end TTS models trained from the LibriTTS corpus achieved above 4.0 in mean opinion scores in naturalness in five out of six evaluation speakers.",
      "abstract": "This paper introduces a new speech corpus called \"LibriTTS\" designed for text-to-speech use.It is derived from the original audio and text materials of the LibriSpeech corpus, which has been used for training and evaluating automatic speech recognition systems.The new corpus inherits desired properties of the LibriSpeech corpus while addressing a number of issues which make LibriSpeech less than ideal for text-to-speech work.The released corpus consists of 585 hours of speech data at 24kHz sampling rate from 2,456 speakers and the corresponding texts.Experimental results show that neural end-to-end TTS models trained from the LibriTTS corpus achieved above 4.0 in mean opinion scores in naturalness in five out of six evaluation speakers.",
      "doi": "https://doi.org/10.21437/interspeech.2019-2441",
      "openalex_id": "https://openalex.org/W2972359262",
      "arxiv_id": "",
      "publication_date": "2019-09-13",
      "published": "2019-09-13",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Paralinguistics-Enhanced Large Language Modeling of Spoken Dialogue",
      "summary": "Large Language Models (LLMs) have demonstrated superior abilities in tasks such as chatting, reasoning, and question-answering. However, standard LLMs may ignore crucial paralinguistic information, such as sentiment, emotion, and speaking style, which are essential for achieving natural, human-like spoken conversation, especially when such information is conveyed by acoustic cues. We therefore propose Paralinguistics-enhanced Generative Pretrained Transformer (ParalinGPT), an LLM that utilizes text and speech modalities to better model the linguistic content and paralinguistic attributes of spoken dialogue. The model takes the conversational context of text, speech embeddings, and paralinguistic attributes as input prompts within a serialized multitasking multimodal framework. Specifically, our framework serializes tasks in the order of current paralinguistic attribute prediction, response paralinguistic attribute prediction, and response text generation with autoregressive conditioning. We utilize the Switchboard-1 corpus, including its sentiment labels as the paralinguistic attribute, as our spoken dialogue dataset. Experimental results indicate the proposed serialized multitasking method outperforms typical sequence classification techniques on current and response sentiment classification. Furthermore, leveraging conversational context and speech embeddings significantly improves both response text generation and sentiment prediction. Our proposed framework achieves relative improvements of 6.7%, 12.0%, and 3.5% in current sentiment accuracy, response sentiment accuracy, and response text BLEU score, respectively.",
      "abstract": "Large Language Models (LLMs) have demonstrated superior abilities in tasks such as chatting, reasoning, and question-answering. However, standard LLMs may ignore crucial paralinguistic information, such as sentiment, emotion, and speaking style, which are essential for achieving natural, human-like spoken conversation, especially when such information is conveyed by acoustic cues. We therefore propose Paralinguistics-enhanced Generative Pretrained Transformer (ParalinGPT), an LLM that utilizes text and speech modalities to better model the linguistic content and paralinguistic attributes of spoken dialogue. The model takes the conversational context of text, speech embeddings, and paralinguistic attributes as input prompts within a serialized multitasking multimodal framework. Specifically, our framework serializes tasks in the order of current paralinguistic attribute prediction, response paralinguistic attribute prediction, and response text generation with autoregressive conditioning. We utilize the Switchboard-1 corpus, including its sentiment labels as the paralinguistic attribute, as our spoken dialogue dataset. Experimental results indicate the proposed serialized multitasking method outperforms typical sequence classification techniques on current and response sentiment classification. Furthermore, leveraging conversational context and speech embeddings significantly improves both response text generation and sentiment prediction. Our proposed framework achieves relative improvements of 6.7%, 12.0%, and 3.5% in current sentiment accuracy, response sentiment accuracy, and response text BLEU score, respectively.",
      "doi": "https://doi.org/10.1109/icassp48485.2024.10446933",
      "openalex_id": "https://openalex.org/W4392931281",
      "arxiv_id": "",
      "publication_date": "2024-03-18",
      "published": "2024-03-18",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Disentangled Feature Learning for Real-Time Neural Speech Coding",
      "summary": "Recently end-to-end neural audio/speech coding has shown its great potential to outperform traditional signal analysis based audio codecs. This is mostly achieved by following the VQ-VAE paradigm where blind features are learned, vector-quantized and coded. In this paper, instead of blind end-to-end learning, we propose to learn disentangled features for real-time neural speech coding. Specifically, more global-like speaker identity and local content features are learned with disentanglement to represent speech. Such a compact feature decomposition not only achieves better coding efficiency by exploiting bit allocation among different features but also provides the flexibility to do audio editing in embedding space, such as voice conversion in real-time communications. Both subjective and objective results demonstrate its coding efficiency and we find that the learned disentangled features show comparable performance on any-to-any voice conversion with modern self-supervised speech representation learning models with far less parameters and low latency, showing the potential of our neural coding framework.",
      "abstract": "Recently end-to-end neural audio/speech coding has shown its great potential to outperform traditional signal analysis based audio codecs. This is mostly achieved by following the VQ-VAE paradigm where blind features are learned, vector-quantized and coded. In this paper, instead of blind end-to-end learning, we propose to learn disentangled features for real-time neural speech coding. Specifically, more global-like speaker identity and local content features are learned with disentanglement to represent speech. Such a compact feature decomposition not only achieves better coding efficiency by exploiting bit allocation among different features but also provides the flexibility to do audio editing in embedding space, such as voice conversion in real-time communications. Both subjective and objective results demonstrate its coding efficiency and we find that the learned disentangled features show comparable performance on any-to-any voice conversion with modern self-supervised speech representation learning models with far less parameters and low latency, showing the potential of our neural coding framework.",
      "doi": "https://doi.org/10.1109/icassp49357.2023.10094723",
      "openalex_id": "https://openalex.org/W4372259964",
      "arxiv_id": "",
      "publication_date": "2023-05-05",
      "published": "2023-05-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Textless Speech-to-Speech Translation on Real Data",
      "summary": "Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan Wang, Sravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu, Wei-Ning Hsu. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
      "abstract": "Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan Wang, Sravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu, Wei-Ning Hsu. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
      "doi": "https://doi.org/10.18653/v1/2022.naacl-main.63",
      "openalex_id": "https://openalex.org/W4287854499",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MizAR 60 for Mizar 50",
      "summary": "As a present to Mizar on its 50th anniversary, we develop an AI/TP system that automatically proves about 60% of the Mizar theorems in the hammer setting. We also automatically prove 75% of the Mizar theorems when the automated provers are helped by using only the premises used in the human-written Mizar proofs. We describe the methods and large-scale experiments leading to these results. This includes in particular the E and Vampire provers, their ENIGMA and Deepire learning modifications, a number of learning-based premise selection methods, and the incremental loop that interleaves growing a corpus of millions of ATP proofs with training increasingly strong AI/TP systems on them. We also present a selection of Mizar problems that were proved automatically.",
      "abstract": "As a present to Mizar on its 50th anniversary, we develop an AI/TP system that automatically proves about 60% of the Mizar theorems in the hammer setting. We also automatically prove 75% of the Mizar theorems when the automated provers are helped by using only the premises used in the human-written Mizar proofs. We describe the methods and large-scale experiments leading to these results. This includes in particular the E and Vampire provers, their ENIGMA and Deepire learning modifications, a number of learning-based premise selection methods, and the incremental loop that interleaves growing a corpus of millions of ATP proofs with training increasingly strong AI/TP systems on them. We also present a selection of Mizar problems that were proved automatically.",
      "doi": "https://doi.org/10.4230/lipics.itp.2023.19",
      "openalex_id": "https://openalex.org/W4385245566",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Direct Speech-to-Speech Translation With Discrete Units",
      "summary": "Ann Lee, Peng-Jen Chen, Changhan Wang, Jiatao Gu, Sravya Popuri, Xutai Ma, Adam Polyak, Yossi Adi, Qing He, Yun Tang, Juan Pino, Wei-Ning Hsu. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
      "abstract": "Ann Lee, Peng-Jen Chen, Changhan Wang, Jiatao Gu, Sravya Popuri, Xutai Ma, Adam Polyak, Yossi Adi, Qing He, Yun Tang, Juan Pino, Wei-Ning Hsu. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
      "doi": "https://doi.org/10.18653/v1/2022.acl-long.235",
      "openalex_id": "https://openalex.org/W3180374548",
      "arxiv_id": "",
      "publication_date": "2022-01-01",
      "published": "2022-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units",
      "summary": "Hirofumi Inaguma, Sravya Popuri, Ilia Kulikov, Peng-Jen Chen, Changhan Wang, Yu-An Chung, Yun Tang, Ann Lee, Shinji Watanabe, Juan Pino. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
      "abstract": "Hirofumi Inaguma, Sravya Popuri, Ilia Kulikov, Peng-Jen Chen, Changhan Wang, Yu-An Chung, Yun Tang, Ann Lee, Shinji Watanabe, Juan Pino. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
      "doi": "https://doi.org/10.18653/v1/2023.acl-long.872",
      "openalex_id": "https://openalex.org/W4385570550",
      "arxiv_id": "",
      "publication_date": "2023-01-01",
      "published": "2023-01-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Libri-Light: A Benchmark for ASR with Limited or No Supervision",
      "summary": "We introduce a new collection of spoken English audio suitable for training\\nspeech recognition systems under limited or no supervision. It is derived from\\nopen-source audio books from the LibriVox project. It contains over 60K hours\\nof audio, which is, to our knowledge, the largest freely-available corpus of\\nspeech. The audio has been segmented using voice activity detection and is\\ntagged with SNR, speaker ID and genre descriptions. Additionally, we provide\\nbaseline systems and evaluation metrics working under three settings: (1) the\\nzero resource/unsupervised setting (ABX), (2) the semi-supervised setting (PER,\\nCER) and (3) the distant supervision setting (WER). Settings (2) and (3) use\\nlimited textual resources (10 minutes to 10 hours) aligned with the speech.\\nSetting (3) uses large amounts of unaligned text. They are evaluated on the\\nstandard LibriSpeech dev and test sets for comparison with the supervised\\nstate-of-the-art.\\n",
      "abstract": "We introduce a new collection of spoken English audio suitable for training\\nspeech recognition systems under limited or no supervision. It is derived from\\nopen-source audio books from the LibriVox project. It contains over 60K hours\\nof audio, which is, to our knowledge, the largest freely-available corpus of\\nspeech. The audio has been segmented using voice activity detection and is\\ntagged with SNR, speaker ID and genre descriptions. Additionally, we provide\\nbaseline systems and evaluation metrics working under three settings: (1) the\\nzero resource/unsupervised setting (ABX), (2) the semi-supervised setting (PER,\\nCER) and (3) the distant supervision setting (WER). Settings (2) and (3) use\\nlimited textual resources (10 minutes to 10 hours) aligned with the speech.\\nSetting (3) uses large amounts of unaligned text. They are evaluated on the\\nstandard LibriSpeech dev and test sets for comparison with the supervised\\nstate-of-the-art.\\n",
      "doi": "https://doi.org/10.1109/icassp40776.2020.9052942",
      "openalex_id": "https://openalex.org/W2995181338",
      "arxiv_id": "",
      "publication_date": "2020-04-09",
      "published": "2020-04-09",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Looking to listen at the cocktail party",
      "summary": "We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to \"focus\" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVS peech , a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest).",
      "abstract": "We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to \"focus\" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVS peech , a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest).",
      "doi": "https://doi.org/10.1145/3197517.3201357",
      "openalex_id": "https://openalex.org/W4289665794",
      "arxiv_id": "",
      "publication_date": "2018-07-30",
      "published": "2018-07-30",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "MLS: A Large-Scale Multilingual Dataset for Speech Research",
      "summary": "This paper introduces Multilingual LibriSpeech (MLS) dataset, a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages, including about 44.5K hours of English and a total of about 6K hours for other languages. Additionally, we provide Language Models (LM) and baseline Automatic Speech Recognition (ASR) models and for all the languages in our dataset. We believe such a large transcribed dataset will open new avenues in ASR and Text-To-Speech (TTS) research. The dataset will be made freely available for anyone at http://www.openslr.org.",
      "abstract": "This paper introduces Multilingual LibriSpeech (MLS) dataset, a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages, including about 44.5K hours of English and a total of about 6K hours for other languages. Additionally, we provide Language Models (LM) and baseline Automatic Speech Recognition (ASR) models and for all the languages in our dataset. We believe such a large transcribed dataset will open new avenues in ASR and Text-To-Speech (TTS) research. The dataset will be made freely available for anyone at http://www.openslr.org.",
      "doi": "https://doi.org/10.21437/interspeech.2020-2826",
      "openalex_id": "https://openalex.org/W3095410713",
      "arxiv_id": "",
      "publication_date": "2020-10-25",
      "published": "2020-10-25",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)",
      "summary": "Description The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) contains 7356 files (total size: 24.8 GB). The dataset contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. All conditions are available in three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound). Note, there are no song files for Actor_18. The RAVDESS was developed by Dr Steven R. Livingstone, who now leads the Affective Data Science Lab, and Dr Frank A. Russo who leads the SMART Lab. Citing the RAVDESS The RAVDESS is released under a Creative Commons Attribution license, so please cite the RAVDESS if it is used in your work in any form. Published academic papers should use the academic paper citation for our PLoS1 paper. Personal works, such as machine learning projects/blog posts, should provide a URL to this Zenodo page, though a reference to our PLoS1 paper would also be appreciated. Academic paper citation Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391. Personal use citation Include a link to this Zenodo page - https://zenodo.org/record/1188976 Commercial Licenses Commercial licenses for the RAVDESS can be purchased. For more information, please visit our license page of fees, or contact us at ravdess@gmail.com. Contact Information If you would like further information about the RAVDESS, to purchase a commercial license, or if you experience any issues downloading files, please contact us at ravdess@gmail.com. Example Videos Watch a sample of the RAVDESS speech and song videos. Emotion Classification Users If you're interested in using machine learning to classify emotional expressions with the RAVDESS, please see our new RAVDESS Facial Landmark Tracking data set [Zenodo project page]. Construction and Validation Full details on the construction and perceptual validation of the RAVDESS are described in our PLoS ONE paper - https://doi.org/10.1371/journal.pone.0196391. The RAVDESS contains 7356 files. Each file was rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained adult research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity, interrater reliability, and test-retest intrarater reliability were reported. Validation data is open-access, and can be downloaded along with our paper from PLoS ONE. Contents Audio-only files Audio-only files of all actors (01-24) are available as two separate zip files (~200 MB each): Speech file (Audio_Speech_Actors_01-24.zip, 215 MB) contains 1440 files: 60 trials per actor x 24 actors = 1440. Song file (Audio_Song_Actors_01-24.zip, 198 MB) contains 1012 files: 44 trials per actor x 23 actors = 1012. Audio-Visual and Video-only files Video files are provided as separate zip downloads for each actor (01-24, ~500 MB each), and are split into separate speech and song downloads: Speech files (Video_Speech_Actor_01.zip to Video_Speech_Actor_24.zip) collectively contains 2880 files: 60 trials per actor x 2 modalities (AV, VO) x 24 actors = 2880. Song files (Video_Song_Actor_01.zip to Video_Song_Actor_24.zip) collectively contains 2024 files: 44 trials per actor x 2 modalities (AV, VO) x 23 actors = 2024. File Summary In total, the RAVDESS collection includes 7356 files (2880+2024+1440+1012 files). File naming convention Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics: Filename identifiers Modality (01 = full-AV, 02 = video-only, 03 = audio-only). Vocal channel (01 = speech, 02 = song). Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised). Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion. Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\"). Repetition (01 = 1st repetition, 02 = 2nd repetition). Actor (01 to 24. Odd numbered actors are male, even numbered actors are female). Filename example: 02-01-06-01-02-01-12.mp4 Video-only (02) Speech (01) Fearful (06) Normal intensity (01) Statement \"dogs\" (02) 1st Repetition (01) 12th Actor (12) Female, as the actor ID number is even. License information The RAVDESS is released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License, CC BY-NC-SA 4.0 Commercial licenses for the RAVDESS can also be purchased. For more information, please visit our license fee page, or contact us at ravdess@gmail.com. Related Data sets RAVDESS Facial Landmark Tracking data set [Zenodo project page].",
      "abstract": "Description The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) contains 7356 files (total size: 24.8 GB). The dataset contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. All conditions are available in three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound). Note, there are no song files for Actor_18. The RAVDESS was developed by Dr Steven R. Livingstone, who now leads the Affective Data Science Lab, and Dr Frank A. Russo who leads the SMART Lab. Citing the RAVDESS The RAVDESS is released under a Creative Commons Attribution license, so please cite the RAVDESS if it is used in your work in any form. Published academic papers should use the academic paper citation for our PLoS1 paper. Personal works, such as machine learning projects/blog posts, should provide a URL to this Zenodo page, though a reference to our PLoS1 paper would also be appreciated. Academic paper citation Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391. Personal use citation Include a link to this Zenodo page - https://zenodo.org/record/1188976 Commercial Licenses Commercial licenses for the RAVDESS can be purchased. For more information, please visit our license page of fees, or contact us at ravdess@gmail.com. Contact Information If you would like further information about the RAVDESS, to purchase a commercial license, or if you experience any issues downloading files, please contact us at ravdess@gmail.com. Example Videos Watch a sample of the RAVDESS speech and song videos. Emotion Classification Users If you're interested in using machine learning to classify emotional expressions with the RAVDESS, please see our new RAVDESS Facial Landmark Tracking data set [Zenodo project page]. Construction and Validation Full details on the construction and perceptual validation of the RAVDESS are described in our PLoS ONE paper - https://doi.org/10.1371/journal.pone.0196391. The RAVDESS contains 7356 files. Each file was rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained adult research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity, interrater reliability, and test-retest intrarater reliability were reported. Validation data is open-access, and can be downloaded along with our paper from PLoS ONE. Contents Audio-only files Audio-only files of all actors (01-24) are available as two separate zip files (~200 MB each): Speech file (Audio_Speech_Actors_01-24.zip, 215 MB) contains 1440 files: 60 trials per actor x 24 actors = 1440. Song file (Audio_Song_Actors_01-24.zip, 198 MB) contains 1012 files: 44 trials per actor x 23 actors = 1012. Audio-Visual and Video-only files Video files are provided as separate zip downloads for each actor (01-24, ~500 MB each), and are split into separate speech and song downloads: Speech files (Video_Speech_Actor_01.zip to Video_Speech_Actor_24.zip) collectively contains 2880 files: 60 trials per actor x 2 modalities (AV, VO) x 24 actors = 2880. Song files (Video_Song_Actor_01.zip to Video_Song_Actor_24.zip) collectively contains 2024 files: 44 trials per actor x 2 modalities (AV, VO) x 23 actors = 2024. File Summary In total, the RAVDESS collection includes 7356 files (2880+2024+1440+1012 files). File naming convention Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics: Filename identifiers Modality (01 = full-AV, 02 = video-only, 03 = audio-only). Vocal channel (01 = speech, 02 = song). Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised). Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion. Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\"). Repetition (01 = 1st repetition, 02 = 2nd repetition). Actor (01 to 24. Odd numbered actors are male, even numbered actors are female). Filename example: 02-01-06-01-02-01-12.mp4 Video-only (02) Speech (01) Fearful (06) Normal intensity (01) Statement \"dogs\" (02) 1st Repetition (01) 12th Actor (12) Female, as the actor ID number is even. License information The RAVDESS is released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License, CC BY-NC-SA 4.0 Commercial licenses for the RAVDESS can also be purchased. For more information, please visit our license fee page, or contact us at ravdess@gmail.com. Related Data sets RAVDESS Facial Landmark Tracking data set [Zenodo project page].",
      "doi": "https://doi.org/10.5281/zenodo.1188976",
      "openalex_id": "https://openalex.org/W3081192838",
      "arxiv_id": "",
      "publication_date": "2018-04-05",
      "published": "2018-04-05",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Low Bit-rate Speech Coding with VQ-VAE and a WaveNet Decoder",
      "summary": "In order to efficiently transmit and store speech signals, speech codecs create a minimally redundant representation of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on VQ-VAE with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosody-transparent and speaker-independent model trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits perceptual quality which is around halfway between the MELP codec at 2.4 kbps and AMR-WB codec at 23.05 kbps. In addition, when training on high-quality recorded speech with the test speaker included in the training set, a model coding speech at 1.6 kbps produces output of similar perceptual quality to that generated by AMR-WB at 23.05 kbps.",
      "abstract": "In order to efficiently transmit and store speech signals, speech codecs create a minimally redundant representation of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on VQ-VAE with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosody-transparent and speaker-independent model trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits perceptual quality which is around halfway between the MELP codec at 2.4 kbps and AMR-WB codec at 23.05 kbps. In addition, when training on high-quality recorded speech with the test speaker included in the training set, a model coding speech at 1.6 kbps produces output of similar perceptual quality to that generated by AMR-WB at 23.05 kbps.",
      "doi": "https://doi.org/10.1109/icassp.2019.8683277",
      "openalex_id": "https://openalex.org/W2935711438",
      "arxiv_id": "",
      "publication_date": "2019-04-16",
      "published": "2019-04-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Reducing F0 Frame Error of F0 tracking algorithms under noisy conditions with an unvoiced/voiced classification frontend",
      "summary": "In this paper, we propose an F0 Frame Error (FFE) metric which combines Gross Pitch Error (GPE) and Voicing Decision Error (VDE) to objectively evaluate the performance of fundamental frequency (F0) tracking methods. A GPE-VDE curve is then developed to show the trade-off between GPE and VDE. In addition, we introduce a model-based Unvoiced/Voiced (U/V) classification frontend which can be used by any F0 tracking algorithm. In the U/V classification, we train speaker independent U/V models, and then adapt them to speaker dependent models in an unsupervised fashion. The U/V classification result is taken as a mask for F0 tracking. Experiments using the KEELE corpus with additive noise show that our statistically-based U/V classifier can reduce VDE and FFE for the pitch tracker TEMPO in both white and babble noise conditions, and that minimizing FFE instead of VDE results in a reduction in error rates for a number of F0 tracking algorithms, especially in babble noise.",
      "abstract": "In this paper, we propose an F0 Frame Error (FFE) metric which combines Gross Pitch Error (GPE) and Voicing Decision Error (VDE) to objectively evaluate the performance of fundamental frequency (F0) tracking methods. A GPE-VDE curve is then developed to show the trade-off between GPE and VDE. In addition, we introduce a model-based Unvoiced/Voiced (U/V) classification frontend which can be used by any F0 tracking algorithm. In the U/V classification, we train speaker independent U/V models, and then adapt them to speaker dependent models in an unsupervised fashion. The U/V classification result is taken as a mask for F0 tracking. Experiments using the KEELE corpus with additive noise show that our statistically-based U/V classifier can reduce VDE and FFE for the pitch tracker TEMPO in both white and babble noise conditions, and that minimizing FFE instead of VDE results in a reduction in error rates for a number of F0 tracking algorithms, especially in babble noise.",
      "doi": "https://doi.org/10.1109/icassp.2009.4960497",
      "openalex_id": "https://openalex.org/W2107740512",
      "arxiv_id": "",
      "publication_date": "2009-04-01",
      "published": "2009-04-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022",
      "summary": "We present the UTokyo-SaruLab mean opinion score (MOS) prediction system submitted to VoiceMOS Challenge 2022.The challenge is to predict the MOS values of speech samples collected from previous Blizzard Challenges and Voice Conversion Challenges for two tracks: a main track for in-domain prediction and an out-of-domain (OOD) track for which there is less labeled data from different listening tests.Our system is based on ensemble learning of strong and weak learners.Strong learners incorporate several improvements to the previous finetuning models of self-supervised learning (SSL) models, while weak learners use basic machine-learning methods to predict scores from SSL features.In the Challenge, our system had the highest score on several metrics for both the main and OOD tracks.In addition, we conducted ablation studies to investigate the effectiveness of our proposed methods.",
      "abstract": "We present the UTokyo-SaruLab mean opinion score (MOS) prediction system submitted to VoiceMOS Challenge 2022.The challenge is to predict the MOS values of speech samples collected from previous Blizzard Challenges and Voice Conversion Challenges for two tracks: a main track for in-domain prediction and an out-of-domain (OOD) track for which there is less labeled data from different listening tests.Our system is based on ensemble learning of strong and weak learners.Strong learners incorporate several improvements to the previous finetuning models of self-supervised learning (SSL) models, while weak learners use basic machine-learning methods to predict scores from SSL features.In the Challenge, our system had the highest score on several metrics for both the main and OOD tracks.In addition, we conducted ablation studies to investigate the effectiveness of our proposed methods.",
      "doi": "https://doi.org/10.21437/interspeech.2022-439",
      "openalex_id": "https://openalex.org/W4225956675",
      "arxiv_id": "",
      "publication_date": "2022-09-16",
      "published": "2022-09-16",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "ViSQOL v3: An Open Source Production Ready Objective Speech and Audio Metric",
      "summary": "The 12th International Conference on Quality of Multimedia Experience (QoMEX), Athlone, Ireland (held online due to coronavirus outbreak), 26-28 May 2020",
      "abstract": "The 12th International Conference on Quality of Multimedia Experience (QoMEX), Athlone, Ireland (held online due to coronavirus outbreak), 26-28 May 2020",
      "doi": "https://doi.org/10.1109/qomex48832.2020.9123150",
      "openalex_id": "https://openalex.org/W3037038648",
      "arxiv_id": "",
      "publication_date": "2020-05-01",
      "published": "2020-05-01",
      "source": "openalex_snowball"
    }
  },
  {
    "metadata": {
      "title": "Librispeech: An ASR corpus based on public domain audio books",
      "summary": "This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.",
      "abstract": "This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.",
      "doi": "https://doi.org/10.1109/icassp.2015.7178964",
      "openalex_id": "https://openalex.org/W1494198834",
      "arxiv_id": "",
      "publication_date": "2015-04-01",
      "published": "2015-04-01",
      "source": "openalex_snowball"
    }
  }
]