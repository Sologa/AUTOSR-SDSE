doi,title,abstract,included,decision_source,final_verdict,landing_url,pdf_url,year,source_anchor,source_term,metadata_json,arxiv_id,openalex_id
10.48550/arxiv.2206.11706,A Temporal Extension of Latent Dirichlet Allocation for Unsupervised Acoustic Unit Discovery,"Latent Dirichlet allocation (LDA) is widely used for unsupervised topic modelling on sets of documents. No temporal information is used in the model. However, there is often a relationship between the corresponding topics of consecutive tokens. In this paper, we present an extension to LDA that uses a Markov chain to model temporal information. We use this new model for acoustic unit discovery from speech. As input tokens, the model takes a discretised encoding of speech from a vector quantised (VQ) neural network with 512 codes. The goal is then to map these 512 VQ codes to 50 phone-like units (topics) in order to more closely resemble true phones. In contrast to the base LDA, which only considers how VQ codes co-occur within utterances (documents), the Markov chain LDA additionally captures how consecutive codes follow one another. This extension leads to an increase in cluster quality and phone segmentation results compared to the base LDA. Compared to a recent vector quantised neural network approach that also learns 50 units, the extended LDA model performs better in phone segmentation but worse in mutual information.",1,,include (junior:5),https://arxiv.org/abs/2206.11706v2,https://arxiv.org/pdf/2206.11706v2,2022,discrete speech tokens,unit discovery,"{""arxiv_id"": ""2206.11706"", ""title"": ""A Temporal Extension of Latent Dirichlet Allocation for Unsupervised Acoustic Unit Discovery"", ""summary"": ""Latent Dirichlet allocation (LDA) is widely used for unsupervised topic modelling on sets of documents. No temporal information is used in the model. However, there is often a relationship between the corresponding topics of consecutive tokens. In this paper, we present an extension to LDA that uses a Markov chain to model temporal information. We use this new model for acoustic unit discovery from speech. As input tokens, the model takes a discretised encoding of speech from a vector quantised (VQ) neural network with 512 codes. The goal is then to map these 512 VQ codes to 50 phone-like units (topics) in order to more closely resemble true phones. In contrast to the base LDA, which only considers how VQ codes co-occur within utterances (documents), the Markov chain LDA additionally captures how consecutive codes follow one another. This extension leads to an increase in cluster quality and phone segmentation results compared to the base LDA. Compared to a recent vector quantised neural network approach that also learns 50 units, the extended LDA model performs better in phone segmentation but worse in mutual information."", ""authors"": [""Werner van der Merwe"", ""Herman Kamper"", ""Johan du Preez""], ""published"": ""2022-06-23T13:53:59Z"", ""updated"": ""2022-06-29T07:47:53Z"", ""categories"": [""eess.AS"", ""cs.CL"", ""cs.LG"", ""stat.ML""], ""pdf_url"": ""https://arxiv.org/pdf/2206.11706v2"", ""landing_url"": ""https://arxiv.org/abs/2206.11706v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2206.11706""}",2206.11706,https://openalex.org/W4283459031
10.48550/arxiv.2210.06007,JukeDrummer: Conditional Beat-aware Audio-domain Drum Accompaniment Generation via Transformer VQ-VAE,"This paper proposes a model that generates a drum track in the audio domain to play along to a user-provided drum-free recording. Specifically, using paired data of drumless tracks and the corresponding human-made drum tracks, we train a Transformer model to improvise the drum part of an unseen drumless recording. We combine two approaches to encode the input audio. First, we train a vector-quantized variational autoencoder (VQ-VAE) to represent the input audio with discrete codes, which can then be readily used in a Transformer. Second, using an audio-domain beat tracking model, we compute beat-related features of the input audio and use them as embeddings in the Transformer. Instead of generating the drum track directly as waveforms, we use a separate VQ-VAE to encode the mel-spectrogram of a drum track into another set of discrete codes, and train the Transformer to predict the sequence of drum-related discrete codes. The output codes are then converted to a mel-spectrogram with a decoder, and then to the waveform with a vocoder. We report both objective and subjective evaluations of variants of the proposed model, demonstrating that the model with beat information generates drum accompaniment that is rhythmically and stylistically consistent with the input audio.",1,,include (junior:4),https://arxiv.org/abs/2210.06007v2,https://arxiv.org/pdf/2210.06007v2,2022,speech representation,vq-vae,"{""arxiv_id"": ""2210.06007"", ""title"": ""JukeDrummer: Conditional Beat-aware Audio-domain Drum Accompaniment Generation via Transformer VQ-VAE"", ""summary"": ""This paper proposes a model that generates a drum track in the audio domain to play along to a user-provided drum-free recording. Specifically, using paired data of drumless tracks and the corresponding human-made drum tracks, we train a Transformer model to improvise the drum part of an unseen drumless recording. We combine two approaches to encode the input audio. First, we train a vector-quantized variational autoencoder (VQ-VAE) to represent the input audio with discrete codes, which can then be readily used in a Transformer. Second, using an audio-domain beat tracking model, we compute beat-related features of the input audio and use them as embeddings in the Transformer. Instead of generating the drum track directly as waveforms, we use a separate VQ-VAE to encode the mel-spectrogram of a drum track into another set of discrete codes, and train the Transformer to predict the sequence of drum-related discrete codes. The output codes are then converted to a mel-spectrogram with a decoder, and then to the waveform with a vocoder. We report both objective and subjective evaluations of variants of the proposed model, demonstrating that the model with beat information generates drum accompaniment that is rhythmically and stylistically consistent with the input audio."", ""authors"": [""Yueh-Kao Wu"", ""Ching-Yu Chiu"", ""Yi-Hsuan Yang""], ""published"": ""2022-10-12T08:23:20Z"", ""updated"": ""2022-10-31T08:54:08Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2210.06007v2"", ""landing_url"": ""https://arxiv.org/abs/2210.06007v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2210.06007""}",2210.06007,https://openalex.org/W4306177919
10.48550/arxiv.2212.09058,BEATs: Audio Pre-Training with Acoustic Tokenizers,"The massive growth of self-supervised learning (SSL) has been witnessed in language, vision, speech, and audio domains over the past few years. While discrete label prediction is widely adopted for other modalities, the state-of-the-art audio SSL models still employ reconstruction loss for pre-training. Compared with reconstruction loss, semantic-rich discrete label prediction encourages the SSL model to abstract the high-level audio semantics and discard the redundant details as in human perception. However, a semantic-rich acoustic tokenizer for general audio pre-training is usually not straightforward to obtain, due to the continuous property of audio and unavailable phoneme sequences like speech. To tackle this challenge, we propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. In the first iteration, we use random projection as the acoustic tokenizer to train an audio SSL model in a mask and label prediction manner. Then, we train an acoustic tokenizer for the next iteration by distilling the semantic knowledge from the pre-trained or fine-tuned audio SSL model. The iteration is repeated with the hope of mutual promotion of the acoustic tokenizer and audio SSL model. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for audio-only models without using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats.",1,,include (junior:5),https://arxiv.org/abs/2212.09058v1,https://arxiv.org/pdf/2212.09058v1,2022,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2212.09058"", ""title"": ""BEATs: Audio Pre-Training with Acoustic Tokenizers"", ""summary"": ""The massive growth of self-supervised learning (SSL) has been witnessed in language, vision, speech, and audio domains over the past few years. While discrete label prediction is widely adopted for other modalities, the state-of-the-art audio SSL models still employ reconstruction loss for pre-training. Compared with reconstruction loss, semantic-rich discrete label prediction encourages the SSL model to abstract the high-level audio semantics and discard the redundant details as in human perception. However, a semantic-rich acoustic tokenizer for general audio pre-training is usually not straightforward to obtain, due to the continuous property of audio and unavailable phoneme sequences like speech. To tackle this challenge, we propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. In the first iteration, we use random projection as the acoustic tokenizer to train an audio SSL model in a mask and label prediction manner. Then, we train an acoustic tokenizer for the next iteration by distilling the semantic knowledge from the pre-trained or fine-tuned audio SSL model. The iteration is repeated with the hope of mutual promotion of the acoustic tokenizer and audio SSL model. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for audio-only models without using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats."", ""authors"": [""Sanyuan Chen"", ""Yu Wu"", ""Chengyi Wang"", ""Shujie Liu"", ""Daniel Tompkins"", ""Zhuo Chen"", ""Furu Wei""], ""published"": ""2022-12-18T10:41:55Z"", ""updated"": ""2022-12-18T10:41:55Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.CL"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2212.09058v1"", ""landing_url"": ""https://arxiv.org/abs/2212.09058v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2212.09058""}",2212.09058,https://openalex.org/W4312048190
10.48550/arxiv.2303.02939,FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model,"Neural text-to-speech (TTS) generally consists of cascaded architecture with separately optimized acoustic model and vocoder, or end-to-end architecture with continuous mel-spectrograms or self-extracted speech frames as the intermediate representations to bridge acoustic model and vocoder, which suffers from two limitations: 1) the continuous acoustic frames are hard to predict with phoneme only, and acoustic information like duration or pitch is also needed to solve the one-to-many problem, which is not easy to scale on large scale and noise datasets; 2) to achieve diverse speech output based on continuous speech features, complex VAE or flow-based models are usually required. In this paper, we propose FoundationTTS, a new speech synthesis system with a neural audio codec for discrete speech token extraction and waveform reconstruction and a large language model for discrete token generation from linguistic (phoneme) tokens. Specifically, 1) we propose a hierarchical codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN), which first extracts continuous frame-level speech representations with fine-grained codec, and extracts a discrete token from each continuous speech frame with coarse-grained codec; 2) we jointly optimize speech token, linguistic tokens, speaker token together with a large language model and predict the discrete speech tokens autoregressively. Experiments show that FoundationTTS achieves a MOS gain of +0.14 compared to the baseline system. In ASR customization tasks, our method achieves 7.09\% and 10.35\% WERR respectively over two strong customized ASR baselines.",1,,include (junior:5),https://arxiv.org/abs/2303.02939v3,https://arxiv.org/pdf/2303.02939v3,2023,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2303.02939"", ""title"": ""FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model"", ""summary"": ""Neural text-to-speech (TTS) generally consists of cascaded architecture with separately optimized acoustic model and vocoder, or end-to-end architecture with continuous mel-spectrograms or self-extracted speech frames as the intermediate representations to bridge acoustic model and vocoder, which suffers from two limitations: 1) the continuous acoustic frames are hard to predict with phoneme only, and acoustic information like duration or pitch is also needed to solve the one-to-many problem, which is not easy to scale on large scale and noise datasets; 2) to achieve diverse speech output based on continuous speech features, complex VAE or flow-based models are usually required. In this paper, we propose FoundationTTS, a new speech synthesis system with a neural audio codec for discrete speech token extraction and waveform reconstruction and a large language model for discrete token generation from linguistic (phoneme) tokens. Specifically, 1) we propose a hierarchical codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN), which first extracts continuous frame-level speech representations with fine-grained codec, and extracts a discrete token from each continuous speech frame with coarse-grained codec; 2) we jointly optimize speech token, linguistic tokens, speaker token together with a large language model and predict the discrete speech tokens autoregressively. Experiments show that FoundationTTS achieves a MOS gain of +0.14 compared to the baseline system. In ASR customization tasks, our method achieves 7.09\\% and 10.35\\% WERR respectively over two strong customized ASR baselines."", ""authors"": [""Ruiqing Xue"", ""Yanqing Liu"", ""Lei He"", ""Xu Tan"", ""Linquan Liu"", ""Edward Lin"", ""Sheng Zhao""], ""published"": ""2023-03-06T07:17:15Z"", ""updated"": ""2023-03-08T03:06:47Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2303.02939v3"", ""landing_url"": ""https://arxiv.org/abs/2303.02939v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2303.02939""}",2303.02939,https://openalex.org/W4323697273
10.48550/arxiv.2304.02160,Pac-HuBERT: Self-Supervised Music Source Separation via Primitive Auditory Clustering and Hidden-Unit BERT,"In spite of the progress in music source separation research, the small amount of publicly-available clean source data remains a constant limiting factor for performance. Thus, recent advances in self-supervised learning present a largely-unexplored opportunity for improving separation models by leveraging unlabelled music data. In this paper, we propose a self-supervised learning framework for music source separation inspired by the HuBERT speech representation model. We first investigate the potential impact of the original HuBERT model by inserting an adapted version of it into the well-known Demucs V2 time-domain separation model architecture. We then propose a time-frequency-domain self-supervised model, Pac-HuBERT (for primitive auditory clustering HuBERT), that we later use in combination with a Res-U-Net decoder for source separation. Pac-HuBERT uses primitive auditory features of music as unsupervised clustering labels to initialize the self-supervised pretraining process using the Free Music Archive (FMA) dataset. The resulting framework achieves better source-to-distortion ratio (SDR) performance on the MusDB18 test set than the original Demucs V2 and Res-U-Net models. We further demonstrate that it can boost performance with small amounts of supervised data. Ultimately, our proposed framework is an effective solution to the challenge of limited clean source data for music source separation.",1,,include (senior:4),https://arxiv.org/abs/2304.02160v1,https://arxiv.org/pdf/2304.02160v1,2023,discrete speech tokens,hubert,"{""arxiv_id"": ""2304.02160"", ""title"": ""Pac-HuBERT: Self-Supervised Music Source Separation via Primitive Auditory Clustering and Hidden-Unit BERT"", ""summary"": ""In spite of the progress in music source separation research, the small amount of publicly-available clean source data remains a constant limiting factor for performance. Thus, recent advances in self-supervised learning present a largely-unexplored opportunity for improving separation models by leveraging unlabelled music data. In this paper, we propose a self-supervised learning framework for music source separation inspired by the HuBERT speech representation model. We first investigate the potential impact of the original HuBERT model by inserting an adapted version of it into the well-known Demucs V2 time-domain separation model architecture. We then propose a time-frequency-domain self-supervised model, Pac-HuBERT (for primitive auditory clustering HuBERT), that we later use in combination with a Res-U-Net decoder for source separation. Pac-HuBERT uses primitive auditory features of music as unsupervised clustering labels to initialize the self-supervised pretraining process using the Free Music Archive (FMA) dataset. The resulting framework achieves better source-to-distortion ratio (SDR) performance on the MusDB18 test set than the original Demucs V2 and Res-U-Net models. We further demonstrate that it can boost performance with small amounts of supervised data. Ultimately, our proposed framework is an effective solution to the challenge of limited clean source data for music source separation."", ""authors"": [""Ke Chen"", ""Gordon Wichern"", ""Fran√ßois G. Germain"", ""Jonathan Le Roux""], ""published"": ""2023-04-04T23:19:53Z"", ""updated"": ""2023-04-04T23:19:53Z"", ""categories"": [""cs.SD"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2304.02160v1"", ""landing_url"": ""https://arxiv.org/abs/2304.02160v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2304.02160""}",2304.0216,https://openalex.org/W4362679244
10.48550/arxiv.2304.04974,Wav2code: Restore Clean Speech Representations via Codebook Lookup for Noise-Robust ASR,"Automatic speech recognition (ASR) has gained remarkable successes thanks to recent advances of deep learning, but it usually degrades significantly under real-world noisy conditions. Recent works introduce speech enhancement (SE) as front-end to improve speech quality, which is proved effective but may not be optimal for downstream ASR due to speech distortion problem. Based on that, latest works combine SE and currently popular self-supervised learning (SSL) to alleviate distortion and improve noise robustness. Despite the effectiveness, the speech distortion caused by conventional SE still cannot be cleared out. In this paper, we propose a self-supervised framework named Wav2code to implement a feature-level SE with reduced distortions for noise-robust ASR. First, in pre-training stage the clean speech representations from SSL model are sent to lookup a discrete codebook via nearest-neighbor feature matching, the resulted code sequence are then exploited to reconstruct the original clean representations, in order to store them in codebook as prior. Second, during finetuning we propose a Transformer-based code predictor to accurately predict clean codes by modeling the global dependency of input noisy representations, which enables discovery and restoration of high-quality clean representations with reduced distortions. Furthermore, we propose an interactive feature fusion network to combine original noisy and the restored clean representations to consider both fidelity and quality, resulting in more informative features for downstream ASR. Finally, experiments on both synthetic and real noisy datasets demonstrate that Wav2code can solve the speech distortion and improve ASR performance under various noisy conditions, resulting in stronger robustness.",1,,include (junior:5),https://arxiv.org/abs/2304.04974v3,https://arxiv.org/pdf/2304.04974v3,2023,speech representation,codebook,"{""arxiv_id"": ""2304.04974"", ""title"": ""Wav2code: Restore Clean Speech Representations via Codebook Lookup for Noise-Robust ASR"", ""summary"": ""Automatic speech recognition (ASR) has gained remarkable successes thanks to recent advances of deep learning, but it usually degrades significantly under real-world noisy conditions. Recent works introduce speech enhancement (SE) as front-end to improve speech quality, which is proved effective but may not be optimal for downstream ASR due to speech distortion problem. Based on that, latest works combine SE and currently popular self-supervised learning (SSL) to alleviate distortion and improve noise robustness. Despite the effectiveness, the speech distortion caused by conventional SE still cannot be cleared out. In this paper, we propose a self-supervised framework named Wav2code to implement a feature-level SE with reduced distortions for noise-robust ASR. First, in pre-training stage the clean speech representations from SSL model are sent to lookup a discrete codebook via nearest-neighbor feature matching, the resulted code sequence are then exploited to reconstruct the original clean representations, in order to store them in codebook as prior. Second, during finetuning we propose a Transformer-based code predictor to accurately predict clean codes by modeling the global dependency of input noisy representations, which enables discovery and restoration of high-quality clean representations with reduced distortions. Furthermore, we propose an interactive feature fusion network to combine original noisy and the restored clean representations to consider both fidelity and quality, resulting in more informative features for downstream ASR. Finally, experiments on both synthetic and real noisy datasets demonstrate that Wav2code can solve the speech distortion and improve ASR performance under various noisy conditions, resulting in stronger robustness."", ""authors"": [""Yuchen Hu"", ""Chen Chen"", ""Qiushi Zhu"", ""Eng Siong Chng""], ""published"": ""2023-04-11T04:46:12Z"", ""updated"": ""2024-04-18T06:39:57Z"", ""categories"": [""eess.AS"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2304.04974v3"", ""landing_url"": ""https://arxiv.org/abs/2304.04974v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2304.04974""}",2304.04974,https://openalex.org/W4365205190
10.1609/aaai.v38i16.29747,UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding,"The utilization of discrete speech tokens, divided into semantic tokens and acoustic tokens, has been proven superior to traditional acoustic feature mel-spectrograms in terms of naturalness and robustness for text-to-speech (TTS) synthesis. Recent popular models, such as VALL-E and SPEAR-TTS, allow zero-shot speaker adaptation through auto-regressive (AR) continuation of acoustic tokens extracted from a short speech prompt. However, these AR models are restricted to generate speech only in a left-to-right direction, making them unsuitable for speech editing where both preceding and following contexts are provided. Furthermore, these models rely on acoustic tokens, which have audio quality limitations imposed by the performance of audio codec models. In this study, we propose a unified context-aware TTS framework called UniCATS, which is capable of both speech continuation and editing. UniCATS comprises two components, an acoustic model CTX-txt2vec and a vocoder CTX-vec2wav. CTX-txt2vec employs contextual VQ-diffusion to predict semantic tokens from the input text, enabling it to incorporate the semantic context and maintain seamless concatenation with the surrounding context. Following that, CTX-vec2wav utilizes contextual vocoding to convert these semantic tokens into waveforms, taking into consideration the acoustic context. Our experimental results demonstrate that CTX-vec2wav outperforms HifiGAN and AudioLM in terms of speech resynthesis from semantic tokens. Moreover, we show that UniCATS achieves state-of-the-art performance in both speech continuation and editing.",1,,include (junior:4),https://arxiv.org/abs/2306.07547v6,https://arxiv.org/pdf/2306.07547v6,2023,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2306.07547"", ""title"": ""UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding"", ""summary"": ""The utilization of discrete speech tokens, divided into semantic tokens and acoustic tokens, has been proven superior to traditional acoustic feature mel-spectrograms in terms of naturalness and robustness for text-to-speech (TTS) synthesis. Recent popular models, such as VALL-E and SPEAR-TTS, allow zero-shot speaker adaptation through auto-regressive (AR) continuation of acoustic tokens extracted from a short speech prompt. However, these AR models are restricted to generate speech only in a left-to-right direction, making them unsuitable for speech editing where both preceding and following contexts are provided. Furthermore, these models rely on acoustic tokens, which have audio quality limitations imposed by the performance of audio codec models. In this study, we propose a unified context-aware TTS framework called UniCATS, which is capable of both speech continuation and editing. UniCATS comprises two components, an acoustic model CTX-txt2vec and a vocoder CTX-vec2wav. CTX-txt2vec employs contextual VQ-diffusion to predict semantic tokens from the input text, enabling it to incorporate the semantic context and maintain seamless concatenation with the surrounding context. Following that, CTX-vec2wav utilizes contextual vocoding to convert these semantic tokens into waveforms, taking into consideration the acoustic context. Our experimental results demonstrate that CTX-vec2wav outperforms HifiGAN and AudioLM in terms of speech resynthesis from semantic tokens. Moreover, we show that UniCATS achieves state-of-the-art performance in both speech continuation and editing."", ""authors"": [""Chenpeng Du"", ""Yiwei Guo"", ""Feiyu Shen"", ""Zhijun Liu"", ""Zheng Liang"", ""Xie Chen"", ""Shuai Wang"", ""Hui Zhang"", ""Kai Yu""], ""published"": ""2023-06-13T05:38:34Z"", ""updated"": ""2024-03-28T13:56:33Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2306.07547v6"", ""landing_url"": ""https://arxiv.org/abs/2306.07547v6"", ""doi"": ""https://doi.org/10.1609/aaai.v38i16.29747""}",2306.07547,https://openalex.org/W4393147067
10.48550/arxiv.2306.08920,Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation,"The excellent generalization ability of self-supervised learning (SSL) for speech foundation models has garnered significant attention. HuBERT is a successful example that utilizes offline clustering to convert speech features into discrete units for a masked language modeling pretext task. However, simply clustering features as targets by k-means does not fully inspire the model's performance. In this work, we present an unsupervised method to improve SSL targets. Two models are proposed, MonoBERT and PolyBERT, which leverage context-independent and context-dependent phoneme-based units for pre-training. Our models outperform other SSL models significantly on the LibriSpeech benchmark without the need for iterative re-clustering and re-training. Furthermore, our models equipped with context-dependent units even outperform target-improvement models that use labeled data during pre-training. How we progressively improve the unit discovery process is demonstrated through experiments.",1,,include (junior:4),https://arxiv.org/abs/2306.08920v1,https://arxiv.org/pdf/2306.08920v1,2023,discrete speech tokens,unit discovery,"{""arxiv_id"": ""2306.08920"", ""title"": ""Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation"", ""summary"": ""The excellent generalization ability of self-supervised learning (SSL) for speech foundation models has garnered significant attention. HuBERT is a successful example that utilizes offline clustering to convert speech features into discrete units for a masked language modeling pretext task. However, simply clustering features as targets by k-means does not fully inspire the model's performance. In this work, we present an unsupervised method to improve SSL targets. Two models are proposed, MonoBERT and PolyBERT, which leverage context-independent and context-dependent phoneme-based units for pre-training. Our models outperform other SSL models significantly on the LibriSpeech benchmark without the need for iterative re-clustering and re-training. Furthermore, our models equipped with context-dependent units even outperform target-improvement models that use labeled data during pre-training. How we progressively improve the unit discovery process is demonstrated through experiments."", ""authors"": [""Ziyang Ma"", ""Zhisheng Zheng"", ""Guanrou Yang"", ""Yu Wang"", ""Chao Zhang"", ""Xie Chen""], ""published"": ""2023-06-15T07:45:12Z"", ""updated"": ""2023-06-15T07:45:12Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2306.08920v1"", ""landing_url"": ""https://arxiv.org/abs/2306.08920v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2306.08920""}",2306.0892,https://openalex.org/W4380994056
10.48550/arxiv.2306.10521,LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models,"Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling.",1,,include (junior:4),https://arxiv.org/abs/2306.10521v2,https://arxiv.org/pdf/2306.10521v2,2023,acoustic tokens,offline clustering,"{""arxiv_id"": ""2306.10521"", ""title"": ""LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models"", ""summary"": ""Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling."", ""authors"": [""Zhichao Wang"", ""Yuanzhe Chen"", ""Lei Xie"", ""Qiao Tian"", ""Yuping Wang""], ""published"": ""2023-06-18T10:59:06Z"", ""updated"": ""2023-08-21T02:21:06Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2306.10521v2"", ""landing_url"": ""https://arxiv.org/abs/2306.10521v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2306.10521""}",2306.10521,https://openalex.org/W4381558479
10.48550/arxiv.2307.04686,VampNet: Music Generation via Masked Acoustic Token Modeling,"We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.",1,,include (senior:4),https://arxiv.org/abs/2307.04686v2,https://arxiv.org/pdf/2307.04686v2,2023,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2307.04686"", ""title"": ""VampNet: Music Generation via Masked Acoustic Token Modeling"", ""summary"": ""We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online."", ""authors"": [""Hugo Flores Garcia"", ""Prem Seetharaman"", ""Rithesh Kumar"", ""Bryan Pardo""], ""published"": ""2023-07-10T16:42:03Z"", ""updated"": ""2023-07-12T17:06:41Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2307.04686v2"", ""landing_url"": ""https://arxiv.org/abs/2307.04686v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2307.04686""}",2307.04686,https://openalex.org/W4383993968
10.48550/arxiv.2308.16692,SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models,"Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.",1,,include (junior:4),https://arxiv.org/abs/2308.16692v2,https://arxiv.org/pdf/2308.16692v2,2023,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2308.16692"", ""title"": ""SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models"", ""summary"": ""Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/."", ""authors"": [""Xin Zhang"", ""Dong Zhang"", ""Shimin Li"", ""Yaqian Zhou"", ""Xipeng Qiu""], ""published"": ""2023-08-31T12:53:09Z"", ""updated"": ""2024-01-23T01:56:57Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2308.16692v2"", ""landing_url"": ""https://arxiv.org/abs/2308.16692v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2308.16692""}",2308.16692,https://openalex.org/W4386384714
10.48550/arxiv.2309.00126,QS-TTS: Towards Semi-Supervised Text-to-Speech Synthesis via Vector-Quantized Self-Supervised Speech Representation Learning,"This paper proposes a novel semi-supervised TTS framework, QS-TTS, to improve TTS quality with lower supervised data requirements via Vector-Quantized Self-Supervised Speech Representation Learning (VQ-S3RL) utilizing more unlabeled speech audio. This framework comprises two VQ-S3R learners: first, the principal learner aims to provide a generative Multi-Stage Multi-Codebook (MSMC) VQ-S3R via the MSMC-VQ-GAN combined with the contrastive S3RL, while decoding it back to the high-quality audio; then, the associate learner further abstracts the MSMC representation into a highly-compact VQ representation through a VQ-VAE. These two generative VQ-S3R learners provide profitable speech representations and pre-trained models for TTS, significantly improving synthesis quality with the lower requirement for supervised data. QS-TTS is evaluated comprehensively under various scenarios via subjective and objective tests in experiments. The results powerfully demonstrate the superior performance of QS-TTS, winning the highest MOS over supervised or semi-supervised baseline TTS approaches, especially in low-resource scenarios. Moreover, comparing various speech representations and transfer learning methods in TTS further validates the notable improvement of the proposed VQ-S3RL to TTS, showing the best audio quality and intelligibility metrics. The trend of slower decay in the synthesis quality of QS-TTS with decreasing supervised data further highlights its lower requirements for supervised data, indicating its great potential in low-resource scenarios.",1,,include (junior:5),https://arxiv.org/abs/2309.00126v1,https://arxiv.org/pdf/2309.00126v1,2023,speech representation,vector quantization,"{""arxiv_id"": ""2309.00126"", ""title"": ""QS-TTS: Towards Semi-Supervised Text-to-Speech Synthesis via Vector-Quantized Self-Supervised Speech Representation Learning"", ""summary"": ""This paper proposes a novel semi-supervised TTS framework, QS-TTS, to improve TTS quality with lower supervised data requirements via Vector-Quantized Self-Supervised Speech Representation Learning (VQ-S3RL) utilizing more unlabeled speech audio. This framework comprises two VQ-S3R learners: first, the principal learner aims to provide a generative Multi-Stage Multi-Codebook (MSMC) VQ-S3R via the MSMC-VQ-GAN combined with the contrastive S3RL, while decoding it back to the high-quality audio; then, the associate learner further abstracts the MSMC representation into a highly-compact VQ representation through a VQ-VAE. These two generative VQ-S3R learners provide profitable speech representations and pre-trained models for TTS, significantly improving synthesis quality with the lower requirement for supervised data. QS-TTS is evaluated comprehensively under various scenarios via subjective and objective tests in experiments. The results powerfully demonstrate the superior performance of QS-TTS, winning the highest MOS over supervised or semi-supervised baseline TTS approaches, especially in low-resource scenarios. Moreover, comparing various speech representations and transfer learning methods in TTS further validates the notable improvement of the proposed VQ-S3RL to TTS, showing the best audio quality and intelligibility metrics. The trend of slower decay in the synthesis quality of QS-TTS with decreasing supervised data further highlights its lower requirements for supervised data, indicating its great potential in low-resource scenarios."", ""authors"": [""Haohan Guo"", ""Fenglong Xie"", ""Jiawen Kang"", ""Yujia Xiao"", ""Xixin Wu"", ""Helen Meng""], ""published"": ""2023-08-31T20:25:44Z"", ""updated"": ""2023-08-31T20:25:44Z"", ""categories"": [""cs.SD"", ""cs.CL"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2309.00126v1"", ""landing_url"": ""https://arxiv.org/abs/2309.00126v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2309.00126""}",2309.00126,https://openalex.org/W4386435838
10.48550/arxiv.2309.00169,RepCodec: A Speech Representation Codec for Speech Tokenization,"With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing.",1,,include (junior:4),https://arxiv.org/abs/2309.00169v3,https://arxiv.org/pdf/2309.00169v3,2023,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2309.00169"", ""title"": ""RepCodec: A Speech Representation Codec for Speech Tokenization"", ""summary"": ""With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing."", ""authors"": [""Zhichao Huang"", ""Chutong Meng"", ""Tom Ko""], ""published"": ""2023-08-31T23:26:10Z"", ""updated"": ""2024-07-22T09:53:44Z"", ""categories"": [""eess.AS"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2309.00169v3"", ""landing_url"": ""https://arxiv.org/abs/2309.00169v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2309.00169""}",2309.00169,https://openalex.org/W4386437507
10.48550/arxiv.2309.07937,Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks,"We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",1,,include (junior:4),https://arxiv.org/abs/2309.07937v3,https://arxiv.org/pdf/2309.07937v3,2023,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2309.07937"", ""title"": ""Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks"", ""summary"": ""We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work."", ""authors"": [""Soumi Maiti"", ""Yifan Peng"", ""Shukjae Choi"", ""Jee-weon Jung"", ""Xuankai Chang"", ""Shinji Watanabe""], ""published"": ""2023-09-14T03:13:18Z"", ""updated"": ""2024-01-24T15:36:31Z"", ""categories"": [""eess.AS"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2309.07937v3"", ""landing_url"": ""https://arxiv.org/abs/2309.07937v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2309.07937""}",2309.07937,https://openalex.org/W4386874750
10.48550/arxiv.2309.11977,Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts,"Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.",1,,include (junior:5),https://arxiv.org/abs/2309.11977v3,https://arxiv.org/pdf/2309.11977v3,2023,acoustic tokens,speaker similarity,"{""arxiv_id"": ""2309.11977"", ""title"": ""Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts"", ""summary"": ""Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt."", ""authors"": [""Shun Lei"", ""Yixuan Zhou"", ""Liyang Chen"", ""Dan Luo"", ""Zhiyong Wu"", ""Xixin Wu"", ""Shiyin Kang"", ""Tao Jiang"", ""Yahui Zhou"", ""Yuxing Han"", ""Helen Meng""], ""published"": ""2023-09-21T11:22:22Z"", ""updated"": ""2024-04-09T08:39:52Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2309.11977v3"", ""landing_url"": ""https://arxiv.org/abs/2309.11977v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2309.11977""}",2309.11977,https://openalex.org/W4386977793
10.48550/arxiv.2310.14580,Acoustic BPE for Speech Generation with Discrete Tokens,"Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.",1,,include (junior:4),https://arxiv.org/abs/2310.14580v4,https://arxiv.org/pdf/2310.14580v4,2023,discrete speech tokens,acoustic bpe,"{""arxiv_id"": ""2310.14580"", ""title"": ""Acoustic BPE for Speech Generation with Discrete Tokens"", ""summary"": ""Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks."", ""authors"": [""Feiyu Shen"", ""Yiwei Guo"", ""Chenpeng Du"", ""Xie Chen"", ""Kai Yu""], ""published"": ""2023-10-23T05:38:41Z"", ""updated"": ""2024-01-15T05:53:31Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2310.14580v4"", ""landing_url"": ""https://arxiv.org/abs/2310.14580v4"", ""doi"": ""https://doi.org/10.48550/arXiv.2310.14580""}",2310.1458,https://openalex.org/W4387929314
10.48550/arxiv.2311.02898,Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction,"We introduce a text-to-speech(TTS) framework based on a neural transducer. We use discretized semantic tokens acquired from wav2vec2.0 embeddings, which makes it easy to adopt a neural transducer for the TTS framework enjoying its monotonic alignment constraints. The proposed model first generates aligned semantic tokens using the neural transducer, then synthesizes a speech sample from the semantic tokens using a non-autoregressive(NAR) speech generator. This decoupled framework alleviates the training complexity of TTS and allows each stage to focus on 1) linguistic and alignment modeling and 2) fine-grained acoustic modeling, respectively. Experimental results on the zero-shot adaptive TTS show that the proposed model exceeds the baselines in speech quality and speaker similarity via objective and subjective measures. We also investigate the inference speed and prosody controllability of our proposed model, showing the potential of the neural transducer for TTS frameworks.",1,,include (senior:5),https://arxiv.org/abs/2311.02898v2,https://arxiv.org/pdf/2311.02898v2,2023,discrete speech tokens,semantic tokens,"{""arxiv_id"": ""2311.02898"", ""title"": ""Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction"", ""summary"": ""We introduce a text-to-speech(TTS) framework based on a neural transducer. We use discretized semantic tokens acquired from wav2vec2.0 embeddings, which makes it easy to adopt a neural transducer for the TTS framework enjoying its monotonic alignment constraints. The proposed model first generates aligned semantic tokens using the neural transducer, then synthesizes a speech sample from the semantic tokens using a non-autoregressive(NAR) speech generator. This decoupled framework alleviates the training complexity of TTS and allows each stage to focus on 1) linguistic and alignment modeling and 2) fine-grained acoustic modeling, respectively. Experimental results on the zero-shot adaptive TTS show that the proposed model exceeds the baselines in speech quality and speaker similarity via objective and subjective measures. We also investigate the inference speed and prosody controllability of our proposed model, showing the potential of the neural transducer for TTS frameworks."", ""authors"": [""Minchan Kim"", ""Myeonghun Jeong"", ""Byoung Jin Choi"", ""Dongjune Lee"", ""Nam Soo Kim""], ""published"": ""2023-11-06T06:13:39Z"", ""updated"": ""2023-11-08T05:52:39Z"", ""categories"": [""eess.AS"", ""cs.LG""], ""pdf_url"": ""https://arxiv.org/pdf/2311.02898v2"", ""landing_url"": ""https://arxiv.org/abs/2311.02898v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2311.02898""}",2311.02898,https://openalex.org/W4388512545
10.48550/arxiv.2401.01498,Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction,"We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similarity, both objectively and subjectively. We also delve into the inference speed and prosody control capabilities of our approach, highlighting the potential of neural transducers in TTS frameworks.",1,,include (senior:4),https://arxiv.org/abs/2401.01498v1,https://arxiv.org/pdf/2401.01498v1,2024,discrete speech tokens,semantic tokens,"{""arxiv_id"": ""2401.01498"", ""title"": ""Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction"", ""summary"": ""We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similarity, both objectively and subjectively. We also delve into the inference speed and prosody control capabilities of our approach, highlighting the potential of neural transducers in TTS frameworks."", ""authors"": [""Minchan Kim"", ""Myeonghun Jeong"", ""Byoung Jin Choi"", ""Semin Kim"", ""Joun Yeop Lee"", ""Nam Soo Kim""], ""published"": ""2024-01-03T02:03:36Z"", ""updated"": ""2024-01-03T02:03:36Z"", ""categories"": [""eess.AS"", ""cs.CL"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2401.01498v1"", ""landing_url"": ""https://arxiv.org/abs/2401.01498v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2401.01498""}",2401.01498,https://openalex.org/W4390601257
10.48550/arxiv.2405.09768,Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model,"Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.",1,,include (junior:4),https://arxiv.org/abs/2405.09768v1,https://arxiv.org/pdf/2405.09768v1,2024,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2405.09768"", ""title"": ""Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model"", ""summary"": ""Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis."", ""authors"": [""Siyang Wang"", ""√âva Sz√©kely""], ""published"": ""2024-05-16T02:18:41Z"", ""updated"": ""2024-05-16T02:18:41Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2405.09768v1"", ""landing_url"": ""https://arxiv.org/abs/2405.09768v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2405.09768""}",2405.09768,https://openalex.org/W4397028227
10.48550/arxiv.2406.10735,How Should We Extract Discrete Audio Tokens from Self-Supervised Models?,"Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.",1,,include (junior:5),https://arxiv.org/abs/2406.10735v1,https://arxiv.org/pdf/2406.10735v1,2024,semantic tokens,offline clustering,"{""arxiv_id"": ""2406.10735"", ""title"": ""How Should We Extract Discrete Audio Tokens from Self-Supervised Models?"", ""summary"": ""Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications."", ""authors"": [""Pooneh Mousavi"", ""Jarod Duret"", ""Salah Zaiem"", ""Luca Della Libera"", ""Artem Ploujnikov"", ""Cem Subakan"", ""Mirco Ravanelli""], ""published"": ""2024-06-15T20:43:07Z"", ""updated"": ""2024-06-15T20:43:07Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""cs.CL"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.10735v1"", ""landing_url"": ""https://arxiv.org/abs/2406.10735v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.10735""}",2406.10735,https://openalex.org/W4399794549
10.48550/arxiv.2406.11037,NAST: Noise Aware Speech Tokenization for Speech Language Models,"Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.",1,,include (junior:4),https://arxiv.org/abs/2406.11037v1,https://arxiv.org/pdf/2406.11037v1,2024,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2406.11037"", ""title"": ""NAST: Noise Aware Speech Tokenization for Speech Language Models"", ""summary"": ""Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST."", ""authors"": [""Shoval Messica"", ""Yossi Adi""], ""published"": ""2024-06-16T18:20:45Z"", ""updated"": ""2024-06-16T18:20:45Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.11037v1"", ""landing_url"": ""https://arxiv.org/abs/2406.11037v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.11037""}",2406.11037,https://openalex.org/W4399794786
10.48550/arxiv.2406.13431,Children's Speech Recognition through Discrete Token Enhancement,"Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters.",1,,include (junior:4),https://arxiv.org/abs/2406.13431v2,https://arxiv.org/pdf/2406.13431v2,2024,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2406.13431"", ""title"": ""Children's Speech Recognition through Discrete Token Enhancement"", ""summary"": ""Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters."", ""authors"": [""Vrunda N. Sukhadia"", ""Shammur Absar Chowdhury""], ""published"": ""2024-06-19T10:45:12Z"", ""updated"": ""2024-06-24T15:31:59Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.13431v2"", ""landing_url"": ""https://arxiv.org/abs/2406.13431v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.13431""}",2406.13431,https://openalex.org/W4399912799
10.48550/arxiv.2406.17310,High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model,"We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity.",1,,include (senior:4),https://arxiv.org/abs/2406.17310v1,https://arxiv.org/pdf/2406.17310v1,2024,acoustic tokens,offline clustering,"{""arxiv_id"": ""2406.17310"", ""title"": ""High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model"", ""summary"": ""We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity."", ""authors"": [""Joun Yeop Lee"", ""Myeonghun Jeong"", ""Minchan Kim"", ""Ji-Hyun Lee"", ""Hoon-Young Cho"", ""Nam Soo Kim""], ""published"": ""2024-06-25T06:46:47Z"", ""updated"": ""2024-06-25T06:46:47Z"", ""categories"": [""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2406.17310v1"", ""landing_url"": ""https://arxiv.org/abs/2406.17310v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2406.17310""}",2406.1731,https://openalex.org/W4400064835
10.48550/arxiv.2407.03892,On the Effectiveness of Acoustic BPE in Decoder-Only TTS,"Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS.",1,,include (senior:4),https://arxiv.org/abs/2407.03892v1,https://arxiv.org/pdf/2407.03892v1,2024,discrete speech tokens,acoustic bpe,"{""arxiv_id"": ""2407.03892"", ""title"": ""On the Effectiveness of Acoustic BPE in Decoder-Only TTS"", ""summary"": ""Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS."", ""authors"": [""Bohan Li"", ""Feiyu Shen"", ""Yiwei Guo"", ""Shuai Wang"", ""Xie Chen"", ""Kai Yu""], ""published"": ""2024-07-04T12:35:32Z"", ""updated"": ""2024-07-04T12:35:32Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2407.03892v1"", ""landing_url"": ""https://arxiv.org/abs/2407.03892v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2407.03892""}",2407.03892,https://openalex.org/W4400434132
10.48550/arxiv.2407.05407,CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens,"Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.",1,,include (senior:5),https://arxiv.org/abs/2407.05407v2,https://arxiv.org/pdf/2407.05407v2,2024,discrete speech tokens,semantic tokens,"{""arxiv_id"": ""2407.05407"", ""title"": ""CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens"", ""summary"": ""Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models."", ""authors"": [""Zhihao Du"", ""Qian Chen"", ""Shiliang Zhang"", ""Kai Hu"", ""Heng Lu"", ""Yexin Yang"", ""Hangrui Hu"", ""Siqi Zheng"", ""Yue Gu"", ""Ziyang Ma"", ""Zhifu Gao"", ""Zhijie Yan""], ""published"": ""2024-07-07T15:16:19Z"", ""updated"": ""2024-07-09T07:42:51Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2407.05407v2"", ""landing_url"": ""https://arxiv.org/abs/2407.05407v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2407.05407""}",2407.05407,https://openalex.org/W4400518537
10.48550/arxiv.2407.15835,dMel: Speech Tokenization made Simple,"Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text.",1,,include (junior:5),https://arxiv.org/abs/2407.15835v3,https://arxiv.org/pdf/2407.15835v3,2024,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2407.15835"", ""title"": ""dMel: Speech Tokenization made Simple"", ""summary"": ""Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text."", ""authors"": [""Richard He Bai"", ""Tatiana Likhomanenko"", ""Ruixiang Zhang"", ""Zijin Gu"", ""Zakaria Aldeneh"", ""Navdeep Jaitly""], ""published"": ""2024-07-22T17:51:53Z"", ""updated"": ""2025-05-21T16:55:34Z"", ""categories"": [""cs.CL"", ""cs.AI"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2407.15835v3"", ""landing_url"": ""https://arxiv.org/abs/2407.15835v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2407.15835""}",2407.15835,https://openalex.org/W4402856752
10.48550/arxiv.2409.00750,MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer,"The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g. phone), which may compromise their naturalness. In this paper, we introduce Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the mask-and-predict learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. Experiments with 100K hours of in-the-wild speech demonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility. Audio samples are available at https://maskgct.github.io/. We release our code and model checkpoints at https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct.",1,,include (junior:4),https://arxiv.org/abs/2409.00750v3,https://arxiv.org/pdf/2409.00750v3,2024,acoustic tokens,gumbel vq,"{""arxiv_id"": ""2409.00750"", ""title"": ""MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer"", ""summary"": ""The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g. phone), which may compromise their naturalness. In this paper, we introduce Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the mask-and-predict learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. Experiments with 100K hours of in-the-wild speech demonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility. Audio samples are available at https://maskgct.github.io/. We release our code and model checkpoints at https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct."", ""authors"": [""Yuancheng Wang"", ""Haoyue Zhan"", ""Liwei Liu"", ""Ruihong Zeng"", ""Haotian Guo"", ""Jiachen Zheng"", ""Qiang Zhang"", ""Xueyao Zhang"", ""Shunsi Zhang"", ""Zhizheng Wu""], ""published"": ""2024-09-01T15:26:30Z"", ""updated"": ""2024-10-20T14:25:49Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2409.00750v3"", ""landing_url"": ""https://arxiv.org/abs/2409.00750v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.00750""}",2409.0075,https://openalex.org/W4402954014
10.48550/arxiv.2409.03701,LAST: Language Model Aware Speech Tokenization,"Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.",1,,include (junior:4),https://arxiv.org/abs/2409.03701v2,https://arxiv.org/pdf/2409.03701v2,2024,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2409.03701"", ""title"": ""LAST: Language Model Aware Speech Tokenization"", ""summary"": ""Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches."", ""authors"": [""Arnon Turetzky"", ""Yossi Adi""], ""published"": ""2024-09-05T16:57:39Z"", ""updated"": ""2024-09-10T14:45:15Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2409.03701v2"", ""landing_url"": ""https://arxiv.org/abs/2409.03701v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.03701""}",2409.03701,https://openalex.org/W4403590045
10.48550/arxiv.2409.05004,Disentangling the Prosody and Semantic Information with Pre-trained Model for In-Context Learning based Zero-Shot Voice Conversion,"Voice conversion (VC) aims to modify the speaker's timbre while retaining speech content. Previous approaches have tokenized the outputs from self-supervised into semantic tokens, facilitating disentanglement of speech content information. Recently, in-context learning (ICL) has emerged in text-to-speech (TTS) systems for effectively modeling specific characteristics such as timbre through context conditioning. This paper proposes an ICL capability enhanced VC system (ICL-VC) employing a mask and reconstruction training strategy based on flow-matching generative models. Augmented with semantic tokens, our experiments on the LibriTTS dataset demonstrate that ICL-VC improves speaker similarity. Additionally, we find that k-means is a versatile tokenization method applicable to various pre-trained models. However, the ICL-VC system faces challenges in preserving the prosody of the source speech. To mitigate this issue, we propose incorporating prosody embeddings extracted from a pre-trained emotion recognition model into our system. Integration of prosody embeddings notably enhances the system's capability to preserve source speech prosody, as validated on the Emotional Speech Database.",1,,include (senior:4),https://arxiv.org/abs/2409.05004v2,https://arxiv.org/pdf/2409.05004v2,2024,semantic tokens,speaker similarity,"{""arxiv_id"": ""2409.05004"", ""title"": ""Disentangling the Prosody and Semantic Information with Pre-trained Model for In-Context Learning based Zero-Shot Voice Conversion"", ""summary"": ""Voice conversion (VC) aims to modify the speaker's timbre while retaining speech content. Previous approaches have tokenized the outputs from self-supervised into semantic tokens, facilitating disentanglement of speech content information. Recently, in-context learning (ICL) has emerged in text-to-speech (TTS) systems for effectively modeling specific characteristics such as timbre through context conditioning. This paper proposes an ICL capability enhanced VC system (ICL-VC) employing a mask and reconstruction training strategy based on flow-matching generative models. Augmented with semantic tokens, our experiments on the LibriTTS dataset demonstrate that ICL-VC improves speaker similarity. Additionally, we find that k-means is a versatile tokenization method applicable to various pre-trained models. However, the ICL-VC system faces challenges in preserving the prosody of the source speech. To mitigate this issue, we propose incorporating prosody embeddings extracted from a pre-trained emotion recognition model into our system. Integration of prosody embeddings notably enhances the system's capability to preserve source speech prosody, as validated on the Emotional Speech Database."", ""authors"": [""Zhengyang Chen"", ""Shuai Wang"", ""Mingyang Zhang"", ""Xuechen Liu"", ""Junichi Yamagishi"", ""Yanmin Qian""], ""published"": ""2024-09-08T07:24:03Z"", ""updated"": ""2024-09-10T07:36:03Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2409.05004v2"", ""landing_url"": ""https://arxiv.org/abs/2409.05004v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.05004""}",2409.05004,https://openalex.org/W4403590122
10.48550/arxiv.2409.11228,Learning Source Disentanglement in Neural Audio Codec,"Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.",1,,include (junior:4),https://arxiv.org/abs/2409.11228v2,https://arxiv.org/pdf/2409.11228v2,2024,discrete speech tokens,neural audio codec,"{""arxiv_id"": ""2409.11228"", ""title"": ""Learning Source Disentanglement in Neural Audio Codec"", ""summary"": ""Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process."", ""authors"": [""Xiaoyu Bie"", ""Xubo Liu"", ""Ga√´l Richard""], ""published"": ""2024-09-17T14:21:02Z"", ""updated"": ""2025-02-11T10:35:04Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2409.11228v2"", ""landing_url"": ""https://arxiv.org/abs/2409.11228v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2409.11228""}",2409.11228,https://openalex.org/W4403707096
10.21437/interspeech.2024-2366,Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer,"Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0",1,,include (junior:5),https://arxiv.org/abs/2410.08325v1,https://arxiv.org/pdf/2410.08325v1,2024,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2410.08325"", ""title"": ""Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer"", ""summary"": ""Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0"", ""authors"": [""Slava Shechtman"", ""Avihu Dekel""], ""published"": ""2024-10-10T19:29:05Z"", ""updated"": ""2024-10-10T19:29:05Z"", ""categories"": [""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2410.08325v1"", ""landing_url"": ""https://arxiv.org/abs/2410.08325v1"", ""doi"": ""https://doi.org/10.21437/Interspeech.2024-2366""}",2410.08325,https://openalex.org/W4402112548
10.48550/arxiv.2410.14411,SNAC: Multi-Scale Neural Audio Codec,"Neural audio codecs have recently gained popularity because they can represent audio signals with high fidelity at very low bitrates, making it feasible to use language modeling approaches for audio generation and understanding. Residual Vector Quantization (RVQ) has become the standard technique for neural audio compression using a cascade of VQ codebooks. This paper proposes the Multi-Scale Neural Audio Codec, a simple extension of RVQ where the quantizers can operate at different temporal resolutions. By applying a hierarchy of quantizers at variable frame rates, the codec adapts to the audio structure across multiple timescales. This leads to more efficient compression, as demonstrated by extensive objective and subjective evaluations. The code and model weights are open-sourced at https://github.com/hubertsiuzdak/snac.",1,,include (junior:4),https://arxiv.org/abs/2410.14411v1,https://arxiv.org/pdf/2410.14411v1,2024,discrete speech tokens,neural audio codec,"{""arxiv_id"": ""2410.14411"", ""title"": ""SNAC: Multi-Scale Neural Audio Codec"", ""summary"": ""Neural audio codecs have recently gained popularity because they can represent audio signals with high fidelity at very low bitrates, making it feasible to use language modeling approaches for audio generation and understanding. Residual Vector Quantization (RVQ) has become the standard technique for neural audio compression using a cascade of VQ codebooks. This paper proposes the Multi-Scale Neural Audio Codec, a simple extension of RVQ where the quantizers can operate at different temporal resolutions. By applying a hierarchy of quantizers at variable frame rates, the codec adapts to the audio structure across multiple timescales. This leads to more efficient compression, as demonstrated by extensive objective and subjective evaluations. The code and model weights are open-sourced at https://github.com/hubertsiuzdak/snac."", ""authors"": [""Hubert Siuzdak"", ""Florian Gr√∂tschla"", ""Luca A. Lanzend√∂rfer""], ""published"": ""2024-10-18T12:24:05Z"", ""updated"": ""2024-10-18T12:24:05Z"", ""categories"": [""cs.SD"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2410.14411v1"", ""landing_url"": ""https://arxiv.org/abs/2410.14411v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.14411""}",2410.14411,https://openalex.org/W4403995790
10.48550/arxiv.2410.15017,DM-Codec: Distilling Multimodal Representations for Speech Tokenization,"Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. Code, samples, and checkpoints are available at https://github.com/mubtasimahasan/DM-Codec.",1,,include (junior:5),https://arxiv.org/abs/2410.15017v2,https://arxiv.org/pdf/2410.15017v2,2024,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2410.15017"", ""title"": ""DM-Codec: Distilling Multimodal Representations for Speech Tokenization"", ""summary"": ""Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. Code, samples, and checkpoints are available at https://github.com/mubtasimahasan/DM-Codec."", ""authors"": [""Md Mubtasim Ahasan"", ""Md Fahim"", ""Tasnim Mohiuddin"", ""A K M Mahbubur Rahman"", ""Aman Chadha"", ""Tariq Iqbal"", ""M Ashraful Amin"", ""Md Mofijul Islam"", ""Amin Ahsan Ali""], ""published"": ""2024-10-19T07:14:14Z"", ""updated"": ""2025-09-29T08:08:40Z"", ""categories"": [""cs.CL"", ""cs.AI"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2410.15017v2"", ""landing_url"": ""https://arxiv.org/abs/2410.15017v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.15017""}",2410.15017,https://openalex.org/W4404088256
10.48550/arxiv.2410.15764,LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec,"Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework.",1,,include (junior:5),https://arxiv.org/abs/2410.15764v3,https://arxiv.org/pdf/2410.15764v3,2024,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2410.15764"", ""title"": ""LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec"", ""summary"": ""Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework."", ""authors"": [""Yiwei Guo"", ""Zhihan Li"", ""Chenpeng Du"", ""Hankun Wang"", ""Xie Chen"", ""Kai Yu""], ""published"": ""2024-10-21T08:23:31Z"", ""updated"": ""2025-05-21T16:46:32Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2410.15764v3"", ""landing_url"": ""https://arxiv.org/abs/2410.15764v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.15764""}",2410.15764,https://openalex.org/W4404088948
10.48550/arxiv.2410.24177,DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models,"Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",1,,include (junior:5),https://arxiv.org/abs/2410.24177v1,https://arxiv.org/pdf/2410.24177v1,2024,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2410.24177"", ""title"": ""DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models"", ""summary"": ""Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs."", ""authors"": [""Heng-Jui Chang"", ""Hongyu Gong"", ""Changhan Wang"", ""James Glass"", ""Yu-An Chung""], ""published"": ""2024-10-31T17:43:13Z"", ""updated"": ""2024-10-31T17:43:13Z"", ""categories"": [""eess.AS"", ""cs.CL"", ""cs.LG"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2410.24177v1"", ""landing_url"": ""https://arxiv.org/abs/2410.24177v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2410.24177""}",2410.24177,https://openalex.org/W4404348734
10.48550/arxiv.2411.14100,BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken Term Detection,"Spoken term detection (STD) is often hindered by reliance on frame-level features and the computationally intensive DTW-based template matching, limiting its practicality. To address these challenges, we propose a novel approach that encodes speech into discrete, speaker-agnostic semantic tokens. This facilitates fast retrieval using text-based search algorithms and effectively handles out-of-vocabulary terms. Our approach focuses on generating consistent token sequences across varying utterances of the same term. We also propose a bidirectional state space modeling within the Mamba encoder, trained in a self-supervised learning framework, to learn contextual frame-level features that are further encoded into discrete tokens. Our analysis shows that our speech tokens exhibit greater speaker invariance than those from existing tokenizers, making them more suitable for STD tasks. Empirical evaluation on LibriSpeech and TIMIT databases indicates that our method outperforms existing STD baselines while being more efficient.",1,,include (junior:4),https://arxiv.org/abs/2411.14100v2,https://arxiv.org/pdf/2411.14100v2,2024,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2411.14100"", ""title"": ""BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken Term Detection"", ""summary"": ""Spoken term detection (STD) is often hindered by reliance on frame-level features and the computationally intensive DTW-based template matching, limiting its practicality. To address these challenges, we propose a novel approach that encodes speech into discrete, speaker-agnostic semantic tokens. This facilitates fast retrieval using text-based search algorithms and effectively handles out-of-vocabulary terms. Our approach focuses on generating consistent token sequences across varying utterances of the same term. We also propose a bidirectional state space modeling within the Mamba encoder, trained in a self-supervised learning framework, to learn contextual frame-level features that are further encoded into discrete tokens. Our analysis shows that our speech tokens exhibit greater speaker invariance than those from existing tokenizers, making them more suitable for STD tasks. Empirical evaluation on LibriSpeech and TIMIT databases indicates that our method outperforms existing STD baselines while being more efficient."", ""authors"": [""Anup Singh"", ""Kris Demuynck"", ""Vipul Arora""], ""published"": ""2024-11-21T13:05:18Z"", ""updated"": ""2024-12-21T19:15:27Z"", ""categories"": [""eess.AS"", ""cs.CL"", ""cs.IR""], ""pdf_url"": ""https://arxiv.org/pdf/2411.14100v2"", ""landing_url"": ""https://arxiv.org/abs/2411.14100v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2411.14100""}",2411.141,https://openalex.org/W4404652703
10.48550/arxiv.2411.14642,VQalAttent: a Transparent Speech Generation Pipeline based on Transformer-learned VQ-VAE Latent Space,"Generating high-quality speech efficiently remains a key challenge for generative models in speech synthesis. This paper introduces VQalAttent, a lightweight model designed to generate fake speech with tunable performance and interpretability. Leveraging the AudioMNIST dataset, consisting of human utterances of decimal digits (0-9), our method employs a two-step architecture: first, a scalable vector quantized autoencoder (VQ-VAE) that compresses audio spectrograms into discrete latent representations, and second, a decoder-only transformer that learns the probability model of these latents. Trained transformer generates similar latent sequences, convertible to audio spectrograms by the VQ-VAE decoder, from which we generate fake utterances. Interpreting statistical and perceptual quality of the fakes, depending on the dimension and the extrinsic information of the latent space, enables guided improvements in larger, commercial generative models. As a valuable tool for understanding and refining audio synthesis, our results demonstrate VQalAttent's capacity to generate intelligible speech samples with limited computational resources, while the modularity and transparency of the training pipeline helps easily correlate the analytics with modular modifications, hence providing insights for the more complex models.",1,,include (junior:5),https://arxiv.org/abs/2411.14642v1,https://arxiv.org/pdf/2411.14642v1,2024,discrete speech tokens,vq-vae,"{""arxiv_id"": ""2411.14642"", ""title"": ""VQalAttent: a Transparent Speech Generation Pipeline based on Transformer-learned VQ-VAE Latent Space"", ""summary"": ""Generating high-quality speech efficiently remains a key challenge for generative models in speech synthesis. This paper introduces VQalAttent, a lightweight model designed to generate fake speech with tunable performance and interpretability. Leveraging the AudioMNIST dataset, consisting of human utterances of decimal digits (0-9), our method employs a two-step architecture: first, a scalable vector quantized autoencoder (VQ-VAE) that compresses audio spectrograms into discrete latent representations, and second, a decoder-only transformer that learns the probability model of these latents. Trained transformer generates similar latent sequences, convertible to audio spectrograms by the VQ-VAE decoder, from which we generate fake utterances. Interpreting statistical and perceptual quality of the fakes, depending on the dimension and the extrinsic information of the latent space, enables guided improvements in larger, commercial generative models. As a valuable tool for understanding and refining audio synthesis, our results demonstrate VQalAttent's capacity to generate intelligible speech samples with limited computational resources, while the modularity and transparency of the training pipeline helps easily correlate the analytics with modular modifications, hence providing insights for the more complex models."", ""authors"": [""Armani Rodriguez"", ""Silvija Kokalj-Filipovic""], ""published"": ""2024-11-22T00:21:39Z"", ""updated"": ""2024-11-22T00:21:39Z"", ""categories"": [""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2411.14642v1"", ""landing_url"": ""https://arxiv.org/abs/2411.14642v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2411.14642""}",2411.14642,https://openalex.org/W4404985564
10.48550/arxiv.2412.10117,CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models,"In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.",1,,include (junior:5),https://arxiv.org/abs/2412.10117v3,https://arxiv.org/pdf/2412.10117v3,2024,discrete speech tokens,gumbel vq,"{""arxiv_id"": ""2412.10117"", ""title"": ""CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models"", ""summary"": ""In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2."", ""authors"": [""Zhihao Du"", ""Yuxuan Wang"", ""Qian Chen"", ""Xian Shi"", ""Xiang Lv"", ""Tianyu Zhao"", ""Zhifu Gao"", ""Yexin Yang"", ""Changfeng Gao"", ""Hui Wang"", ""Fan Yu"", ""Huadai Liu"", ""Zhengyan Sheng"", ""Yue Gu"", ""Chong Deng"", ""Wen Wang"", ""Shiliang Zhang"", ""Zhijie Yan"", ""Jingren Zhou""], ""published"": ""2024-12-13T12:59:39Z"", ""updated"": ""2024-12-25T11:54:03Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""cs.LG"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2412.10117v3"", ""landing_url"": ""https://arxiv.org/abs/2412.10117v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2412.10117""}",2412.10117,https://openalex.org/W4405433404
10.48550/arxiv.2412.19248,Causal Speech Enhancement with Predicting Semantics based on Quantized Self-supervised Learning Features,"Real-time speech enhancement (SE) is essential to online speech communication. Causal SE models use only the previous context while predicting future information, such as phoneme continuation, may help performing causal SE. The phonetic information is often represented by quantizing latent features of self-supervised learning (SSL) models. This work is the first to incorporate SSL features with causality into an SE model. The causal SSL features are encoded and combined with spectrogram features using feature-wise linear modulation to estimate a mask for enhancing the noisy input speech. Simultaneously, we quantize the causal SSL features using vector quantization to represent phonetic characteristics as semantic tokens. The model not only encodes SSL features but also predicts the future semantic tokens in multi-task learning (MTL). The experimental results using VoiceBank + DEMAND dataset show that our proposed method achieves 2.88 in PESQ, especially with semantic prediction MTL, in which we confirm that the semantic prediction played an important role in causal SE.",1,,include (junior:5),https://arxiv.org/abs/2412.19248v1,https://arxiv.org/pdf/2412.19248v1,2024,semantic tokens,pesq,"{""arxiv_id"": ""2412.19248"", ""title"": ""Causal Speech Enhancement with Predicting Semantics based on Quantized Self-supervised Learning Features"", ""summary"": ""Real-time speech enhancement (SE) is essential to online speech communication. Causal SE models use only the previous context while predicting future information, such as phoneme continuation, may help performing causal SE. The phonetic information is often represented by quantizing latent features of self-supervised learning (SSL) models. This work is the first to incorporate SSL features with causality into an SE model. The causal SSL features are encoded and combined with spectrogram features using feature-wise linear modulation to estimate a mask for enhancing the noisy input speech. Simultaneously, we quantize the causal SSL features using vector quantization to represent phonetic characteristics as semantic tokens. The model not only encodes SSL features but also predicts the future semantic tokens in multi-task learning (MTL). The experimental results using VoiceBank + DEMAND dataset show that our proposed method achieves 2.88 in PESQ, especially with semantic prediction MTL, in which we confirm that the semantic prediction played an important role in causal SE."", ""authors"": [""Emiru Tsunoo"", ""Yuki Saito"", ""Wataru Nakata"", ""Hiroshi Saruwatari""], ""published"": ""2024-12-26T15:08:36Z"", ""updated"": ""2024-12-26T15:08:36Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2412.19248v1"", ""landing_url"": ""https://arxiv.org/abs/2412.19248v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2412.19248""}",2412.19248,https://openalex.org/W4405902171
10.48550/arxiv.2501.00018,SECodec: Structural Entropy-based Compressive Speech Representation Codec for Speech Language Models,"With the rapid advancement of large language models (LLMs), discrete speech representations have become crucial for integrating speech into LLMs. Existing methods for speech representation discretization rely on a predefined codebook size and Euclidean distance-based quantization. However, 1) the size of codebook is a critical parameter that affects both codec performance and downstream task training efficiency. 2) The Euclidean distance-based quantization may lead to audio distortion when the size of the codebook is controlled within a reasonable range. In fact, in the field of information compression, structural information and entropy guidance are crucial, but previous methods have largely overlooked these factors. Therefore, we address the above issues from an information-theoretic perspective, we present SECodec, a novel speech representation codec based on structural entropy (SE) for building speech language models. Specifically, we first model speech as a graph, clustering the speech features nodes within the graph and extracting the corresponding codebook by hierarchically and disentangledly minimizing 2D SE. Then, to address the issue of audio distortion, we propose a new quantization method. This method still adheres to the 2D SE minimization principle, adaptively selecting the most suitable token corresponding to the cluster for each incoming original speech node. Furthermore, we develop a Structural Entropy-based Speech Language Model (SESLM) that leverages SECodec. Experimental results demonstrate that SECodec performs comparably to EnCodec in speech reconstruction, and SESLM surpasses VALL-E in zero-shot text-to-speech tasks. Code, demo speeches, speech feature graph, SE codebook, and models are available at https://github.com/wlq2019/SECodec.",1,,include (junior:5),https://arxiv.org/abs/2501.00018v1,https://arxiv.org/pdf/2501.00018v1,2024,speech representation,codebook,"{""arxiv_id"": ""2501.00018"", ""title"": ""SECodec: Structural Entropy-based Compressive Speech Representation Codec for Speech Language Models"", ""summary"": ""With the rapid advancement of large language models (LLMs), discrete speech representations have become crucial for integrating speech into LLMs. Existing methods for speech representation discretization rely on a predefined codebook size and Euclidean distance-based quantization. However, 1) the size of codebook is a critical parameter that affects both codec performance and downstream task training efficiency. 2) The Euclidean distance-based quantization may lead to audio distortion when the size of the codebook is controlled within a reasonable range. In fact, in the field of information compression, structural information and entropy guidance are crucial, but previous methods have largely overlooked these factors. Therefore, we address the above issues from an information-theoretic perspective, we present SECodec, a novel speech representation codec based on structural entropy (SE) for building speech language models. Specifically, we first model speech as a graph, clustering the speech features nodes within the graph and extracting the corresponding codebook by hierarchically and disentangledly minimizing 2D SE. Then, to address the issue of audio distortion, we propose a new quantization method. This method still adheres to the 2D SE minimization principle, adaptively selecting the most suitable token corresponding to the cluster for each incoming original speech node. Furthermore, we develop a Structural Entropy-based Speech Language Model (SESLM) that leverages SECodec. Experimental results demonstrate that SECodec performs comparably to EnCodec in speech reconstruction, and SESLM surpasses VALL-E in zero-shot text-to-speech tasks. Code, demo speeches, speech feature graph, SE codebook, and models are available at https://github.com/wlq2019/SECodec."", ""authors"": [""Linqin Wang"", ""Yaping Liu"", ""Zhengtao Yu"", ""Shengxiang Gao"", ""Cunli Mao"", ""Yuxin Huang"", ""Wenjun Wang"", ""Ling Dong""], ""published"": ""2024-12-16T03:33:05Z"", ""updated"": ""2024-12-16T03:33:05Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2501.00018v1"", ""landing_url"": ""https://arxiv.org/abs/2501.00018v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2501.00018""}",2501.00018,https://openalex.org/W4406051734
10.48550/arxiv.2502.02942,GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling,"Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability.",1,,include (junior:5),https://arxiv.org/abs/2502.02942v1,https://arxiv.org/pdf/2502.02942v1,2025,acoustic tokens,offline clustering,"{""arxiv_id"": ""2502.02942"", ""title"": ""GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling"", ""summary"": ""Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \\textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability."", ""authors"": [""Jixun Yao"", ""Hexin Liu"", ""Chen Chen"", ""Yuchen Hu"", ""EngSiong Chng"", ""Lei Xie""], ""published"": ""2025-02-05T07:14:39Z"", ""updated"": ""2025-02-05T07:14:39Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2502.02942v1"", ""landing_url"": ""https://arxiv.org/abs/2502.02942v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2502.02942""}",2502.02942,https://openalex.org/W4407212698
10.48550/arxiv.2502.06490,Recent Advances in Discrete Speech Tokens: A Review,"The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.",1,,include (junior:4),https://arxiv.org/abs/2502.06490v4,https://arxiv.org/pdf/2502.06490v4,2025,discrete speech tokens,vector quantization,"{""arxiv_id"": ""2502.06490"", ""title"": ""Recent Advances in Discrete Speech Tokens: A Review"", ""summary"": ""The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens."", ""authors"": [""Yiwei Guo"", ""Zhihan Li"", ""Hankun Wang"", ""Bohan Li"", ""Chongtian Shao"", ""Hanglei Zhang"", ""Chenpeng Du"", ""Xie Chen"", ""Shujie Liu"", ""Kai Yu""], ""published"": ""2025-02-10T14:08:25Z"", ""updated"": ""2025-12-12T05:18:11Z"", ""categories"": [""eess.AS"", ""cs.AI"", ""cs.MM"", ""cs.SD"", ""eess.SP""], ""pdf_url"": ""https://arxiv.org/pdf/2502.06490v4"", ""landing_url"": ""https://arxiv.org/abs/2502.06490v4"", ""doi"": ""https://doi.org/10.48550/arXiv.2502.06490""}",2502.0649,https://openalex.org/W4407359094
10.1109/jstsp.2024.3488557,Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations,"Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.",1,,include (junior:4),https://arxiv.org/abs/2503.12115v2,https://arxiv.org/pdf/2503.12115v2,2025,semantic tokens,speech tokenization,"{""arxiv_id"": ""2503.12115"", ""title"": ""Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations"", ""summary"": ""Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks."", ""authors"": [""Xue Jiang"", ""Xiulian Peng"", ""Yuan Zhang"", ""Yan Lu""], ""published"": ""2025-03-15T12:50:43Z"", ""updated"": ""2025-10-15T06:52:30Z"", ""categories"": [""cs.SD"", ""cs.AI"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2503.12115v2"", ""landing_url"": ""https://arxiv.org/abs/2503.12115v2"", ""doi"": ""https://doi.org/10.1109/JSTSP.2024.3488557""}",2503.12115,https://openalex.org/W4403918744
10.48550/arxiv.2503.20499,FireRedTTS-1S: An Upgraded Streamable Foundation Text-to-Speech System,"In this work, we upgrade FireRedTTS to a new version, FireRedTTS-1S, a high-quality streaming foundation text-to-speech system. FireRedTTS-1S achieves streaming speech generation via two steps: text-to-semantic decoding and semantic-to-acoustic decoding. In text-to-semantic decoding, a semantic-aware speech tokenizer converts the speech signal into semantic tokens, which can be synthesized from the text via a language model in an auto-regressive manner. Meanwhile, the semantic-to-acoustic decoding module simultaneously translates generated semantic tokens into the speech signal in a streaming way. We implement two approaches to achieve this module: 1) a chunk-wise streamable flow-matching approach, and 2) a multi-stream language model-based approach. They both present high-quality and streamable speech generation but differ in real-time factor (RTF) and latency. Specifically, flow-matching decoding can generate speech by chunks, presenting a lower RTF of 0.1 but a higher latency of 300ms. Instead, the multi-stream language model generates speech by frames in an autoregressive manner, presenting a higher RTF of 0.3 but a low latency of 150ms. In experiments on zero-shot voice cloning, the objective results validate FireRedTTS-1S as a high-quality foundation model with comparable intelligibility and speaker similarity over industrial baseline systems. Furthermore, the subjective score of FireRedTTS-1S highlights its impressive synthesis performance, achieving comparable quality to the ground-truth recordings. These results validate FireRedTTS-1S as a high-quality streaming foundation TTS system.",1,,include (senior:4),https://arxiv.org/abs/2503.20499v3,https://arxiv.org/pdf/2503.20499v3,2025,semantic tokens,speaker similarity,"{""arxiv_id"": ""2503.20499"", ""title"": ""FireRedTTS-1S: An Upgraded Streamable Foundation Text-to-Speech System"", ""summary"": ""In this work, we upgrade FireRedTTS to a new version, FireRedTTS-1S, a high-quality streaming foundation text-to-speech system. FireRedTTS-1S achieves streaming speech generation via two steps: text-to-semantic decoding and semantic-to-acoustic decoding. In text-to-semantic decoding, a semantic-aware speech tokenizer converts the speech signal into semantic tokens, which can be synthesized from the text via a language model in an auto-regressive manner. Meanwhile, the semantic-to-acoustic decoding module simultaneously translates generated semantic tokens into the speech signal in a streaming way. We implement two approaches to achieve this module: 1) a chunk-wise streamable flow-matching approach, and 2) a multi-stream language model-based approach. They both present high-quality and streamable speech generation but differ in real-time factor (RTF) and latency. Specifically, flow-matching decoding can generate speech by chunks, presenting a lower RTF of 0.1 but a higher latency of 300ms. Instead, the multi-stream language model generates speech by frames in an autoregressive manner, presenting a higher RTF of 0.3 but a low latency of 150ms. In experiments on zero-shot voice cloning, the objective results validate FireRedTTS-1S as a high-quality foundation model with comparable intelligibility and speaker similarity over industrial baseline systems. Furthermore, the subjective score of FireRedTTS-1S highlights its impressive synthesis performance, achieving comparable quality to the ground-truth recordings. These results validate FireRedTTS-1S as a high-quality streaming foundation TTS system."", ""authors"": [""Hao-Han Guo"", ""Yao Hu"", ""Fei-Yu Shen"", ""Xu Tang"", ""Yi-Chen Wu"", ""Feng-Long Xie"", ""Kun Xie""], ""published"": ""2025-03-26T12:39:06Z"", ""updated"": ""2025-05-26T11:34:20Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2503.20499v3"", ""landing_url"": ""https://arxiv.org/abs/2503.20499v3"", ""doi"": ""https://doi.org/10.48550/arXiv.2503.20499""}",2503.20499,https://openalex.org/W4416417394
10.48550/arxiv.2504.07053,TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling,"Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io.",1,,include (junior:4),https://arxiv.org/abs/2504.07053v2,https://arxiv.org/pdf/2504.07053v2,2025,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2504.07053"", ""title"": ""TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling"", ""summary"": ""Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io."", ""authors"": [""Liang-Hsuan Tseng"", ""Yi-Chang Chen"", ""Kuan-Yi Lee"", ""Da-Shan Shiu"", ""Hung-yi Lee""], ""published"": ""2025-04-09T17:14:33Z"", ""updated"": ""2025-05-22T14:49:03Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2504.07053v2"", ""landing_url"": ""https://arxiv.org/abs/2504.07053v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2504.07053""}",2504.07053,https://openalex.org/W4417248906
10.48550/arxiv.2505.13000,"DualCodec: A Low-Frame-Rate, Semantically-Enhanced Neural Audio Codec for Speech Generation","Neural audio codecs form the foundational building blocks for language model (LM)-based speech generation. Typically, there is a trade-off between frame rate and audio quality. This study introduces a low-frame-rate, semantically enhanced codec model. Existing approaches distill semantically rich self-supervised (SSL) representations into the first-layer codec tokens. This work proposes DualCodec, a dual-stream encoding approach that integrates SSL and waveform representations within an end-to-end codec framework. In this setting, DualCodec enhances the semantic information in the first-layer codec and enables the codec system to maintain high audio quality while operating at a low frame rate. Note that a low-frame-rate codec improves the efficiency of speech generation. Experimental results on audio codec and speech generation tasks confirm the effectiveness of the proposed DualCodec compared to state-of-the-art codec systems, such as Mimi Codec, SpeechTokenizer, DAC, and Encodec. Demos are available at: https://dualcodec.github.io, code is available at: https://github.com/jiaqili3/DualCodec",1,,include (senior:4),https://arxiv.org/abs/2505.13000v2,https://arxiv.org/pdf/2505.13000v2,2025,discrete speech tokens,frame rate,"{""arxiv_id"": ""2505.13000"", ""title"": ""DualCodec: A Low-Frame-Rate, Semantically-Enhanced Neural Audio Codec for Speech Generation"", ""summary"": ""Neural audio codecs form the foundational building blocks for language model (LM)-based speech generation. Typically, there is a trade-off between frame rate and audio quality. This study introduces a low-frame-rate, semantically enhanced codec model. Existing approaches distill semantically rich self-supervised (SSL) representations into the first-layer codec tokens. This work proposes DualCodec, a dual-stream encoding approach that integrates SSL and waveform representations within an end-to-end codec framework. In this setting, DualCodec enhances the semantic information in the first-layer codec and enables the codec system to maintain high audio quality while operating at a low frame rate. Note that a low-frame-rate codec improves the efficiency of speech generation. Experimental results on audio codec and speech generation tasks confirm the effectiveness of the proposed DualCodec compared to state-of-the-art codec systems, such as Mimi Codec, SpeechTokenizer, DAC, and Encodec. Demos are available at: https://dualcodec.github.io, code is available at: https://github.com/jiaqili3/DualCodec"", ""authors"": [""Jiaqi Li"", ""Xiaolong Lin"", ""Zhekai Li"", ""Shixi Huang"", ""Yuancheng Wang"", ""Chaoren Wang"", ""Zhenpeng Zhan"", ""Zhizheng Wu""], ""published"": ""2025-05-19T11:41:08Z"", ""updated"": ""2025-10-01T15:01:57Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2505.13000v2"", ""landing_url"": ""https://arxiv.org/abs/2505.13000v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2505.13000""}",2505.13,https://openalex.org/W4415433988
10.48550/arxiv.2505.13830,Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising,"Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models.",1,,include (junior:4),https://arxiv.org/abs/2505.13830v2,https://arxiv.org/pdf/2505.13830v2,2025,discrete speech tokens,acoustic tokens,"{""arxiv_id"": ""2505.13830"", ""title"": ""Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising"", ""summary"": ""Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models."", ""authors"": [""Ye-Xin Lu"", ""Hui-Peng Du"", ""Fei Liu"", ""Yang Ai"", ""Zhen-Hua Ling""], ""published"": ""2025-05-20T02:18:45Z"", ""updated"": ""2025-05-22T04:41:35Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2505.13830v2"", ""landing_url"": ""https://arxiv.org/abs/2505.13830v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2505.13830""}",2505.1383,https://openalex.org/W4416445296
10.48550/arxiv.2505.14989,Discrete Audio Representations for Automated Audio Captioning,"Discrete audio representations, termed audio tokens, are broadly categorized into semantic and acoustic tokens, typically generated through unsupervised tokenization of continuous audio representations. However, their applicability to automated audio captioning (AAC) remains underexplored. This paper systematically investigates the viability of audio token-driven models for AAC through comparative analyses of various tokenization methods. Our findings reveal that audio tokenization leads to performance degradation in AAC models compared to those that directly utilize continuous audio representations. To address this issue, we introduce a supervised audio tokenizer trained with an audio tagging objective. Unlike unsupervised tokenizers, which lack explicit semantic understanding, the proposed tokenizer effectively captures audio event information. Experiments conducted on the Clotho dataset demonstrate that the proposed audio tokens outperform conventional audio tokens in the AAC task.",1,,include (junior:4),https://arxiv.org/abs/2505.14989v1,https://arxiv.org/pdf/2505.14989v1,2025,acoustic tokens,gumbel vq,"{""arxiv_id"": ""2505.14989"", ""title"": ""Discrete Audio Representations for Automated Audio Captioning"", ""summary"": ""Discrete audio representations, termed audio tokens, are broadly categorized into semantic and acoustic tokens, typically generated through unsupervised tokenization of continuous audio representations. However, their applicability to automated audio captioning (AAC) remains underexplored. This paper systematically investigates the viability of audio token-driven models for AAC through comparative analyses of various tokenization methods. Our findings reveal that audio tokenization leads to performance degradation in AAC models compared to those that directly utilize continuous audio representations. To address this issue, we introduce a supervised audio tokenizer trained with an audio tagging objective. Unlike unsupervised tokenizers, which lack explicit semantic understanding, the proposed tokenizer effectively captures audio event information. Experiments conducted on the Clotho dataset demonstrate that the proposed audio tokens outperform conventional audio tokens in the AAC task."", ""authors"": [""Jingguang Tian"", ""Haoqin Sun"", ""Xinhui Hu"", ""Xinkang Xu""], ""published"": ""2025-05-21T00:27:38Z"", ""updated"": ""2025-05-21T00:27:38Z"", ""categories"": [""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2505.14989v1"", ""landing_url"": ""https://arxiv.org/abs/2505.14989v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2505.14989""}",2505.14989,https://openalex.org/W4415327346
10.48550/arxiv.2505.17446,Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models,"The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed/variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding.",1,,include (senior:4),https://arxiv.org/abs/2505.17446v2,https://arxiv.org/pdf/2505.17446v2,2025,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2505.17446"", ""title"": ""Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models"", ""summary"": ""The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed/variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding."", ""authors"": [""Shunsuke Kando"", ""Yusuke Miyao"", ""Shinnosuke Takamichi""], ""published"": ""2025-05-23T04:03:27Z"", ""updated"": ""2025-05-31T13:32:13Z"", ""categories"": [""cs.CL"", ""cs.SD"", ""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2505.17446v2"", ""landing_url"": ""https://arxiv.org/abs/2505.17446v2"", ""doi"": ""https://doi.org/10.48550/arXiv.2505.17446""}",2505.17446,https://openalex.org/W4415433821
10.48550/arxiv.2505.24496,Speech Token Prediction via Compressed-to-fine Language Modeling for Speech Generation,"Neural audio codecs, used as speech tokenizers, have demonstrated remarkable potential in the field of speech generation. However, to ensure high-fidelity audio reconstruction, neural audio codecs typically encode audio into long sequences of speech tokens, posing a significant challenge for downstream language models in long-context modeling. We observe that speech token sequences exhibit short-range dependency: due to the monotonic alignment between text and speech in text-to-speech (TTS) tasks, the prediction of the current token primarily relies on its local context, while long-range tokens contribute less to the current token prediction and often contain redundant information. Inspired by this observation, we propose a \textbf{compressed-to-fine language modeling} approach to address the challenge of long sequence speech tokens within neural codec language models: (1) \textbf{Fine-grained Initial and Short-range Information}: Our approach retains the prompt and local tokens during prediction to ensure text alignment and the integrity of paralinguistic information; (2) \textbf{Compressed Long-range Context}: Our approach compresses long-range token spans into compact representations to reduce redundant information while preserving essential semantics. Extensive experiments on various neural audio codecs and downstream language models validate the effectiveness and generalizability of the proposed approach, highlighting the importance of token compression in improving speech generation within neural codec language models. The demo of audio samples will be available at https://anonymous.4open.science/r/SpeechTokenPredictionViaCompressedToFinedLM.",1,,include (junior:4),https://arxiv.org/abs/2505.24496v1,https://arxiv.org/pdf/2505.24496v1,2025,discrete speech tokens,speech tokenization,"{""arxiv_id"": ""2505.24496"", ""title"": ""Speech Token Prediction via Compressed-to-fine Language Modeling for Speech Generation"", ""summary"": ""Neural audio codecs, used as speech tokenizers, have demonstrated remarkable potential in the field of speech generation. However, to ensure high-fidelity audio reconstruction, neural audio codecs typically encode audio into long sequences of speech tokens, posing a significant challenge for downstream language models in long-context modeling. We observe that speech token sequences exhibit short-range dependency: due to the monotonic alignment between text and speech in text-to-speech (TTS) tasks, the prediction of the current token primarily relies on its local context, while long-range tokens contribute less to the current token prediction and often contain redundant information. Inspired by this observation, we propose a \\textbf{compressed-to-fine language modeling} approach to address the challenge of long sequence speech tokens within neural codec language models: (1) \\textbf{Fine-grained Initial and Short-range Information}: Our approach retains the prompt and local tokens during prediction to ensure text alignment and the integrity of paralinguistic information; (2) \\textbf{Compressed Long-range Context}: Our approach compresses long-range token spans into compact representations to reduce redundant information while preserving essential semantics. Extensive experiments on various neural audio codecs and downstream language models validate the effectiveness and generalizability of the proposed approach, highlighting the importance of token compression in improving speech generation within neural codec language models. The demo of audio samples will be available at https://anonymous.4open.science/r/SpeechTokenPredictionViaCompressedToFinedLM."", ""authors"": [""Wenrui Liu"", ""Qian Chen"", ""Wen Wang"", ""Yafeng Chen"", ""Jin Xu"", ""Zhifang Guo"", ""Guanrou Yang"", ""Weiqin Li"", ""Xiaoda Yang"", ""Tao Jin"", ""Minghui Fang"", ""Jialong Zuo"", ""Bai Jionghao"", ""Zemin Liu""], ""published"": ""2025-05-30T11:47:29Z"", ""updated"": ""2025-05-30T11:47:29Z"", ""categories"": [""eess.AS""], ""pdf_url"": ""https://arxiv.org/pdf/2505.24496v1"", ""landing_url"": ""https://arxiv.org/abs/2505.24496v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2505.24496""}",2505.24496,https://openalex.org/W4414857663
10.48550/arxiv.2506.00843,HASRD: Hierarchical Acoustic and Semantic Representation Disentanglement,"Effective speech representations for spoken language models must balance semantic relevance with acoustic fidelity for high-quality reconstruction. However, existing approaches struggle to achieve both simultaneously. To address this, we introduce Hierarchical Acoustic and Semantic Representation Disentanglement (HASRD, pronounced `hazard'), a framework that factorizes self-supervised learning representations into discrete semantic and acoustic tokens. HASRD assigns the semantic representation to the first codebook, while encoding acoustic residuals in subsequent codebooks. This preserves ASR performance while achieving high-quality reconstruction. Additionally, we enhance HASRD's encoder efficiency, improving ASR performance without compromising reconstruction quality. Compared to SpeechTokenizer, HASRD achieves a 44% relative WER improvement, superior reconstruction quality, and 2x lower bitrate, demonstrating its effectiveness in disentangling acoustic and semantic information.",1,,include (junior:5),https://arxiv.org/abs/2506.00843v1,https://arxiv.org/pdf/2506.00843v1,2025,acoustic tokens,gumbel vq,"{""arxiv_id"": ""2506.00843"", ""title"": ""HASRD: Hierarchical Acoustic and Semantic Representation Disentanglement"", ""summary"": ""Effective speech representations for spoken language models must balance semantic relevance with acoustic fidelity for high-quality reconstruction. However, existing approaches struggle to achieve both simultaneously. To address this, we introduce Hierarchical Acoustic and Semantic Representation Disentanglement (HASRD, pronounced `hazard'), a framework that factorizes self-supervised learning representations into discrete semantic and acoustic tokens. HASRD assigns the semantic representation to the first codebook, while encoding acoustic residuals in subsequent codebooks. This preserves ASR performance while achieving high-quality reconstruction. Additionally, we enhance HASRD's encoder efficiency, improving ASR performance without compromising reconstruction quality. Compared to SpeechTokenizer, HASRD achieves a 44% relative WER improvement, superior reconstruction quality, and 2x lower bitrate, demonstrating its effectiveness in disentangling acoustic and semantic information."", ""authors"": [""Amir Hussein"", ""Sameer Khurana"", ""Gordon Wichern"", ""Francois G. Germain"", ""Jonathan Le Roux""], ""published"": ""2025-06-01T05:38:39Z"", ""updated"": ""2025-06-01T05:38:39Z"", ""categories"": [""eess.AS"", ""cs.SD""], ""pdf_url"": ""https://arxiv.org/pdf/2506.00843v1"", ""landing_url"": ""https://arxiv.org/abs/2506.00843v1"", ""doi"": ""https://doi.org/10.48550/arXiv.2506.00843""}",2506.00843,https://openalex.org/W4414893720
10.48550/arxiv.2506.09349,DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations,"Recent studies on end-to-end (E2E) speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing E2E approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents DrVoice, a parallel speech-text voice conversation model based on joint autoregressive modeling, featuring dual-resolution speech representations. Notably, while current methods utilize mainly 12.5Hz input audio representation, our proposed dual-resolution mechanism reduces the input frequency for the LLM to 5Hz, significantly reducing computational cost and alleviating the frequency discrepancy between speech and text tokens and in turn better exploiting LLMs' capabilities. Experimental results demonstrate that DrVoice-7B establishes new state-of-the-art (SOTA) on prominent speech benchmarks including OpenAudioBench, VoiceBench, UltraEval-Audio and Big Bench Audio, making it a leading open-source speech foundation model in ~7B models.",1,,include (senior:4),https://arxiv.org/abs/2506.09349v4,https://arxiv.org/pdf/2506.09349v4,2025,discrete speech tokens,offline clustering,"{""arxiv_id"": ""2506.09349"", ""title"": ""DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations"", ""summary"": ""Recent studies on end-to-end (E2E) speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing E2E approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents DrVoice, a parallel speech-text voice conversation model based on joint autoregressive modeling, featuring dual-resolution speech representations. Notably, while current methods utilize mainly 12.5Hz input audio representation, our proposed dual-resolution mechanism reduces the input frequency for the LLM to 5Hz, significantly reducing computational cost and alleviating the frequency discrepancy between speech and text tokens and in turn better exploiting LLMs' capabilities. Experimental results demonstrate that DrVoice-7B establishes new state-of-the-art (SOTA) on prominent speech benchmarks including OpenAudioBench, VoiceBench, UltraEval-Audio and Big Bench Audio, making it a leading open-source speech foundation model in ~7B models."", ""authors"": [""Chao-Hong Tan"", ""Qian Chen"", ""Wen Wang"", ""Chong Deng"", ""Qinglin Zhang"", ""Luyao Cheng"", ""Hai Yu"", ""Xin Zhang"", ""Xiang Lv"", ""Tianyu Zhao"", ""Chong Zhang"", ""Yukun Ma"", ""Yafeng Chen"", ""Hui Wang"", ""Jiaqing Liu"", ""Xiangang Li"", ""Jieping Ye""], ""published"": ""2025-06-11T02:57:22Z"", ""updated"": ""2025-12-23T08:50:59Z"", ""categories"": [""cs.CL""], ""pdf_url"": ""https://arxiv.org/pdf/2506.09349v4"", ""landing_url"": ""https://arxiv.org/abs/2506.09349v4"", ""doi"": ""https://doi.org/10.48550/arXiv.2506.09349""}",2506.09349,https://openalex.org/W4415190729
