openalex_id,doi,title,abstract,referenced_works,publication_date
https://openalex.org/W4398152753,https://doi.org/10.1109/taslp.2024.3402088,InstructTTS: Modelling Expressive TTS in Discrete Latent Space With Natural Language Style Prompt,"Expressive text-to-speech (TTS) aims to synthesize speech with varying speaking styles to better reflect human speech patterns. In this study, we attempt to use natural language as a style prompt to control the styles in the synthetic speech, <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">e.g.</i> , ""Sigh tone in full of sad mood with some helpless feeling"". Considering that there is no existing TTS corpus that is suitable to benchmark this novel task, we first construct a speech corpus whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named InstructTTS, which is novel in the sense of the following aspects: (1) We fully take advantage of self-supervised learning and cross-modal metric learning and propose a novel three-stage training procedure to obtain a robust sentence embedding model that can effectively capture semantic information from the style prompts and control the speaking style in the generated speech. (2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonly-used mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt. Extensive objective and subjective evaluation has been conducted to verify the effectiveness and expressiveness of InstructTTS. Experimental results show that InstructTTS can synthesize high-fidelity and natural speech with style prompts controlling the speaking style. Synthesized samples are available online.","['https://openalex.org/W2963609956', 'https://openalex.org/W6778823374', 'https://openalex.org/W6796464841', 'https://openalex.org/W2972699445', 'https://openalex.org/W2969658393', 'https://openalex.org/W6750489868', 'https://openalex.org/W6749555683', 'https://openalex.org/W6752888775', 'https://openalex.org/W4385822534', 'https://openalex.org/W6848735303', 'https://openalex.org/W6795807602', 'https://openalex.org/W6791353385', 'https://openalex.org/W4210913346', 'https://openalex.org/W2752796333', 'https://openalex.org/W6762931180', 'https://openalex.org/W3180355996', 'https://openalex.org/W6769196770', 'https://openalex.org/W3209059054', 'https://openalex.org/W4307323391', 'https://openalex.org/W3215615641', 'https://openalex.org/W6802805937', 'https://openalex.org/W4367359628', 'https://openalex.org/W6852581948', 'https://openalex.org/W3197294703', 'https://openalex.org/W6777694618', 'https://openalex.org/W3196584150', 'https://openalex.org/W4313316128', 'https://openalex.org/W6846143095', 'https://openalex.org/W3197704090', 'https://openalex.org/W3198123658', 'https://openalex.org/W4375869257', 'https://openalex.org/W6795288823', 'https://openalex.org/W4312388283', 'https://openalex.org/W6810940779', 'https://openalex.org/W6800989748', 'https://openalex.org/W6838815585', 'https://openalex.org/W6846176957', 'https://openalex.org/W6783182287', 'https://openalex.org/W3198213150', 'https://openalex.org/W6849416043', 'https://openalex.org/W4312933868', 'https://openalex.org/W6679045638', 'https://openalex.org/W6796163713', 'https://openalex.org/W6798447524', 'https://openalex.org/W6766673545', 'https://openalex.org/W3156636935', 'https://openalex.org/W6844194202', 'https://openalex.org/W4312096566', 'https://openalex.org/W2157364932', 'https://openalex.org/W6752051073', 'https://openalex.org/W6779459370', 'https://openalex.org/W6796730497', 'https://openalex.org/W6783867762', 'https://openalex.org/W4226132755', 'https://openalex.org/W2963073614', 'https://openalex.org/W4385245566', 'https://openalex.org/W6840815571', 'https://openalex.org/W6838510122', 'https://openalex.org/W4381786045', 'https://openalex.org/W3198533616', 'https://openalex.org/W1494198834', 'https://openalex.org/W6631190155', 'https://openalex.org/W4382202703', 'https://openalex.org/W6757817989', 'https://openalex.org/W2107860279', 'https://openalex.org/W2133665775', 'https://openalex.org/W2067295501', 'https://openalex.org/W2130086727', 'https://openalex.org/W2107740512', 'https://openalex.org/W2058819080', 'https://openalex.org/W6810926057', 'https://openalex.org/W2007962718', 'https://openalex.org/W4388323056', 'https://openalex.org/W6780218876', 'https://openalex.org/W4297808394', 'https://openalex.org/W1522301498', 'https://openalex.org/W4313679638', 'https://openalex.org/W4281771798', 'https://openalex.org/W3174758275', 'https://openalex.org/W2908510526', 'https://openalex.org/W4372279529']",2024-01-01
https://openalex.org/W4402301063,https://doi.org/10.1109/taslp.2024.3451951,ZMM-TTS: Zero-Shot Multilingual and Multispeaker Speech Synthesis Conditioned on Self-Supervised Discrete Speech Representations,"Neural text-to-speech (TTS) has achieved human-like synthetic speech for single-speaker, single-language synthesis. Multilingual TTS systems are limited to resource-rich languages due to the lack of large paired text and studio-quality audio data. TTS systems are typically built using a single speaker’s voice, but there is growing interest in developing systems that can synthesize voices for new speakers using only a few seconds of their speech. This paper presents ZMM-TTS, a multilingual and multispeaker framework utilizing quantized latent speech representations from a large-scale, pre-trained, self-supervised model. Our paper combines text-based and speech-based self-supervised learning models for multilingual speech synthesis. Our proposed model has zero-shot generalization ability not only for unseen speakers but also for unseen languages. We have conducted comprehensive subjective and objective evaluations through a series of experiments. Our model has proven effective in terms of speech naturalness and similarity for both seen and unseen speakers in six high-resource languages. We also tested the efficiency of our method on two hypothetically low-resource languages. The results are promising, indicating that our proposed approach can synthesize audio that is intelligible and has a high degree of similarity to the target speaker’s voice, even without any training data for the new, unseen language.","['https://openalex.org/W2963609956', 'https://openalex.org/W6778823374', 'https://openalex.org/W2964243274', 'https://openalex.org/W4391020683', 'https://openalex.org/W4385764360', 'https://openalex.org/W2972473628', 'https://openalex.org/W3095012670', 'https://openalex.org/W6805710207', 'https://openalex.org/W3197324626', 'https://openalex.org/W4296068816', 'https://openalex.org/W4372267432', 'https://openalex.org/W4385823466', 'https://openalex.org/W6752888775', 'https://openalex.org/W3015826515', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W4225274946', 'https://openalex.org/W3206189675', 'https://openalex.org/W6803547063', 'https://openalex.org/W3207300132', 'https://openalex.org/W6848735303', 'https://openalex.org/W4385822745', 'https://openalex.org/W4226380987', 'https://openalex.org/W4381786045', 'https://openalex.org/W4390075359', 'https://openalex.org/W4252812408', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W6851724922', 'https://openalex.org/W6862144568', 'https://openalex.org/W6858915148', 'https://openalex.org/W6748588790', 'https://openalex.org/W6750489868', 'https://openalex.org/W3196584150', 'https://openalex.org/W3161436426', 'https://openalex.org/W6811227718', 'https://openalex.org/W2903739847', 'https://openalex.org/W6763832098', 'https://openalex.org/W3150572638', 'https://openalex.org/W3097297926', 'https://openalex.org/W6800393981', 'https://openalex.org/W4283640572', 'https://openalex.org/W6796464841', 'https://openalex.org/W2973084242', 'https://openalex.org/W4392114301', 'https://openalex.org/W2964002616', 'https://openalex.org/W3048217770', 'https://openalex.org/W3194464626', 'https://openalex.org/W6752124048', 'https://openalex.org/W3205533980', 'https://openalex.org/W4297841714', 'https://openalex.org/W3197763626', 'https://openalex.org/W4225096077', 'https://openalex.org/W4385329631', 'https://openalex.org/W3198429080', 'https://openalex.org/W4226132755', 'https://openalex.org/W4382202703', 'https://openalex.org/W4281760581', 'https://openalex.org/W4385822479', 'https://openalex.org/W4392903365', 'https://openalex.org/W6853937136', 'https://openalex.org/W6850334629', 'https://openalex.org/W4389600306', 'https://openalex.org/W6783867762', 'https://openalex.org/W3196001064', 'https://openalex.org/W4225534571', 'https://openalex.org/W4296068817', 'https://openalex.org/W4367721746', 'https://openalex.org/W3197273793', 'https://openalex.org/W3095410713', 'https://openalex.org/W2084534958', 'https://openalex.org/W2972802841', 'https://openalex.org/W6917585676', 'https://openalex.org/W6783527727', 'https://openalex.org/W7062081054', 'https://openalex.org/W4225956675', 'https://openalex.org/W2972359262', 'https://openalex.org/W1494198834', 'https://openalex.org/W2187089797', 'https://openalex.org/W4392538788', 'https://openalex.org/W4226424742', 'https://openalex.org/W4366460484', 'https://openalex.org/W4313679638', 'https://openalex.org/W3090254849', 'https://openalex.org/W4388927799', 'https://openalex.org/W4323651091']",2024-01-01
https://openalex.org/W4406345321,https://doi.org/10.1109/ojsp.2025.3529377,SpoofCeleb: Speech Deepfake Detection and SASV in the Wild,"This paper introduces SpoofCeleb, a dataset designed for Speech Deepfake Detection (SDD) and Spoofing-robust Automatic Speaker Verification (SASV), utilizing source data from real-world conditions and spoofing attacks generated by Text-To-Speech (TTS) systems also trained on the same real-world data. Robust recognition systems require speech data recorded in varied acoustic environments with different levels of noise to be trained. However, current datasets typically include clean, high-quality recordings (bona fide data) due to the requirements for TTS training; studio-quality or well-recorded read speech is typically necessary to train TTS models. Current SDD datasets also have limited usefulness for training SASV models due to insufficient speaker diversity. SpoofCeleb leverages a fully automated pipeline we developed that processes the VoxCeleb1 dataset, transforming it into a suitable form for TTS training. We subsequently train 23 contemporary TTS systems. SpoofCeleb comprises over 2.5 million utterances from 1,251 unique speakers, collected under natural, real-world conditions. The dataset includes carefully partitioned training, validation, and evaluation sets with well-controlled experimental protocols. We present the baseline results for both SDD and SASV tasks. All data, protocols, and baselines are publicly available at <uri>https://jungjee.github.io/spoofceleb</uri>.","['https://openalex.org/W6846539466', 'https://openalex.org/W6848735303', 'https://openalex.org/W4382202703', 'https://openalex.org/W4402111459', 'https://openalex.org/W3026777299', 'https://openalex.org/W2141074280', 'https://openalex.org/W2176804518', 'https://openalex.org/W4221138880', 'https://openalex.org/W4297841768', 'https://openalex.org/W2726515241', 'https://openalex.org/W6873073503', 'https://openalex.org/W6799942944', 'https://openalex.org/W4297841787', 'https://openalex.org/W6693868787', 'https://openalex.org/W3197358873', 'https://openalex.org/W4372266958', 'https://openalex.org/W4313306150', 'https://openalex.org/W6799089041', 'https://openalex.org/W6852755086', 'https://openalex.org/W3196774886', 'https://openalex.org/W4401437283', 'https://openalex.org/W4403955716', 'https://openalex.org/W4402351686', 'https://openalex.org/W4226148876', 'https://openalex.org/W2989571531', 'https://openalex.org/W4402112137', 'https://openalex.org/W3211424380', 'https://openalex.org/W6603838645', 'https://openalex.org/W3095410713', 'https://openalex.org/W2807550049', 'https://openalex.org/W3035505965', 'https://openalex.org/W4385822293', 'https://openalex.org/W6847363464', 'https://openalex.org/W4402112192', 'https://openalex.org/W2903739847', 'https://openalex.org/W2962780374', 'https://openalex.org/W6795261426', 'https://openalex.org/W4392931276', 'https://openalex.org/W6789577077', 'https://openalex.org/W6783182287', 'https://openalex.org/W6783867762', 'https://openalex.org/W3015338123', 'https://openalex.org/W2990440871', 'https://openalex.org/W6838843145', 'https://openalex.org/W2963300588', 'https://openalex.org/W6853096648', 'https://openalex.org/W6796464841', 'https://openalex.org/W2972569067', 'https://openalex.org/W3198694222', 'https://openalex.org/W3163596559', 'https://openalex.org/W3201773091', 'https://openalex.org/W4319862221', 'https://openalex.org/W3024869864', 'https://openalex.org/W4402111799', 'https://openalex.org/W4385823275', 'https://openalex.org/W4225956675', 'https://openalex.org/W3161480375', 'https://openalex.org/W4392487686', 'https://openalex.org/W2808631503', 'https://openalex.org/W4297841773']",2025-01-01
https://openalex.org/W4392903365,https://doi.org/10.1109/icassp48485.2024.10448074,Extending Multilingual Speech Synthesis to 100+ Languages without Transcribed Data,"Collecting high-quality studio recordings of audio is challenging, which limits the language coverage of text-to-speech (TTS) systems. This paper proposes a framework for scaling a multilingual TTS model to 100+ languages using found data without supervision. The proposed framework combines speech-text encoder pretraining with unsupervised training using untranscribed speech and unspoken text data sources, thereby leveraging massively multilingual joint speech and text representation learning. Without any transcribed speech in a new language, this TTS model can generate intelligible speech in >30 unseen languages (CER difference of <10% to ground truth). With just 15 minutes of transcribed, found data, we can reduce the intelligibility difference to 1% or less from the ground-truth, and achieve naturalness scores that match the ground-truth in several languages.","['https://openalex.org/W6762242920', 'https://openalex.org/W2889028433', 'https://openalex.org/W3096303254', 'https://openalex.org/W4390075359', 'https://openalex.org/W4297841354', 'https://openalex.org/W4385764360', 'https://openalex.org/W6780218876', 'https://openalex.org/W4226033575', 'https://openalex.org/W6810673746', 'https://openalex.org/W6810259195', 'https://openalex.org/W4226120743', 'https://openalex.org/W4319862218', 'https://openalex.org/W4297841605', 'https://openalex.org/W4382202703', 'https://openalex.org/W4297841714', 'https://openalex.org/W6848735303', 'https://openalex.org/W6784642771', 'https://openalex.org/W6696449567', 'https://openalex.org/W3119308075', 'https://openalex.org/W6838929754', 'https://openalex.org/W2025638820', 'https://openalex.org/W2494654097', 'https://openalex.org/W6798575157', 'https://openalex.org/W6846618694', 'https://openalex.org/W4285189120', 'https://openalex.org/W6852909395', 'https://openalex.org/W3097777922', 'https://openalex.org/W6850218400', 'https://openalex.org/W4319862245', 'https://openalex.org/W2962699523', 'https://openalex.org/W3161296985', 'https://openalex.org/W3197294703', 'https://openalex.org/W6840815571', 'https://openalex.org/W3035390927', 'https://openalex.org/W2970854433', 'https://openalex.org/W6755207826', 'https://openalex.org/W6762521896', 'https://openalex.org/W3169483174', 'https://openalex.org/W4372259881', 'https://openalex.org/W4378105483', 'https://openalex.org/W4288099666', 'https://openalex.org/W4319862635', 'https://openalex.org/W3095410713', 'https://openalex.org/W4313679638', 'https://openalex.org/W4323066695', 'https://openalex.org/W3036601975', 'https://openalex.org/W4375869005', 'https://openalex.org/W4221155340', 'https://openalex.org/W3181257032', 'https://openalex.org/W2952468927']",2024-03-18
https://openalex.org/W4387941731,https://doi.org/10.21437/blizzard.2023-4,The IMS Toucan System for the Blizzard Challenge 2023,"For our contribution to the Blizzard Challenge 2023, we improved on the system we submitted to the Blizzard Challenge 2021.Our approach entails a rule-based text-to-phoneme processing system that includes rule-based disambiguation of homographs in the French language.It then transforms the phonemes to spectrograms as intermediate representations using a fast and efficient non-autoregressive synthesis architecture based on Conformer and Glow.A GAN based neural vocoder that combines recent state-of-the-art approaches converts the spectrogram to the final wave.We carefully designed the data processing, training, and inference procedures for the challenge data.Our system identifier is G. Open source code and demo are available.","['https://openalex.org/W3150572638', 'https://openalex.org/W4387941731', 'https://openalex.org/W4319862723', 'https://openalex.org/W4372260248', 'https://openalex.org/W2970641574', 'https://openalex.org/W1522301498', 'https://openalex.org/W4286950013', 'https://openalex.org/W4382202681', 'https://openalex.org/W2739748921', 'https://openalex.org/W2794490148', 'https://openalex.org/W2970006822', 'https://openalex.org/W4287761884', 'https://openalex.org/W3092028330', 'https://openalex.org/W4283390072', 'https://openalex.org/W4386764866', 'https://openalex.org/W4385822778', 'https://openalex.org/W2805064398', 'https://openalex.org/W4287121924', 'https://openalex.org/W2191779130', 'https://openalex.org/W4302759964', 'https://openalex.org/W4225746985', 'https://openalex.org/W4320013936', 'https://openalex.org/W4404752329', 'https://openalex.org/W3033411150', 'https://openalex.org/W4366460484', 'https://openalex.org/W3095410713', 'https://openalex.org/W3097777922', 'https://openalex.org/W2884225676', 'https://openalex.org/W2880875857', 'https://openalex.org/W4297841590', 'https://openalex.org/W4382202703', 'https://openalex.org/W3182954816', 'https://openalex.org/W2607084763', 'https://openalex.org/W4285189120', 'https://openalex.org/W3119308075', 'https://openalex.org/W4395957972', 'https://openalex.org/W4372260612', 'https://openalex.org/W2986154550', 'https://openalex.org/W4281736089']",2023-08-29
https://openalex.org/W4385488985,https://doi.org/10.1109/icasspw59220.2023.10193157,A Comparative Study of Self-Supervised Speech Representations in Read and Spontaneous TTS,"Recent work has explored using self-supervised learning (SSL) speech representations such as wav2vec2.0 as the representation medium in standard two-stage TTS, in place of conventionally used mel-spectrograms. It is however unclear which speech SSL is the better fit for TTS, and whether or not the performance differs between read and spontaneous TTS, the later of which is arguably more challenging. This study aims at addressing these questions by testing several speech SSLs, including different layers of the same SSL, in two-stage TTS on both read and spontaneous corpora, while maintaining constant TTS model architecture and training settings. Results from listening tests show that the 9th layer of 12-layer wav2vec2.0 (ASR finetuned) outperforms other tested SSLs and mel-spectrogram, in both read and spontaneous TTS. Our work sheds light on both how speech SSL can readily improve current TTS systems, and how SSLs compare in the challenging generative task of TTS. Audio examples can be found at https://www.speech.kth.se/tts-demos/ssr_tts","['https://openalex.org/W4297841714', 'https://openalex.org/W4226132755', 'https://openalex.org/W3197580070', 'https://openalex.org/W2973049979', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3140429000', 'https://openalex.org/W6790356757', 'https://openalex.org/W3163596720', 'https://openalex.org/W4226380987', 'https://openalex.org/W4319862479', 'https://openalex.org/W2964243274', 'https://openalex.org/W3150572638', 'https://openalex.org/W6778823374', 'https://openalex.org/W6777694618', 'https://openalex.org/W6783867762', 'https://openalex.org/W6796730497', 'https://openalex.org/W2972606665', 'https://openalex.org/W2973177710', 'https://openalex.org/W4382202703', 'https://openalex.org/W2901285216', 'https://openalex.org/W3016151952', 'https://openalex.org/W3196001064', 'https://openalex.org/W3011535310', 'https://openalex.org/W3202278141', 'https://openalex.org/W4394671563']",2023-06-04
https://openalex.org/W4392904607,https://doi.org/10.1109/icassp48485.2024.10448356,Considering Temporal Connection between Turns for Conversational Speech Synthesis,"Conversational speech synthesis aims to synthesize speech of an individual speaker based on history conversation. However, most studies in conversational speech synthesis only focus on the synthesis performance of the current speaker's turn and neglect the temporal relationship between turns of interlocutors. Therefore, we consider the temporal connection between turns for conversational speech synthesis, which is crucial for the naturalness and coherence of conversations. Specifically, this paper formulates a task in which there is no overlap between turns and only one history turn is considered. To complete this task, an acoustic model is proposed which leverages multi-modal (including text and speech) information from previous turn to predict the acoustic features of not only current turn but also the inter-turn gap. The model is designed based on MQTTS and incorporates the global acoustic representation and BERT-based local semantic representation of previous turn when predicting the acoustic features of each frame. Experimental results demonstrate that with the introduction of global acoustic information and local semantic information, our model achieves better performance on the temporal connection between turns and the quality of synthetic speech. Audio samples can be found in https://mkd-mkd.github.io/icassp2024.","['https://openalex.org/W6749555683', 'https://openalex.org/W3015282541', 'https://openalex.org/W6796464841', 'https://openalex.org/W2395956841', 'https://openalex.org/W2973177710', 'https://openalex.org/W3115920274', 'https://openalex.org/W4296068977', 'https://openalex.org/W4307680525', 'https://openalex.org/W3151309757', 'https://openalex.org/W3198152857', 'https://openalex.org/W4225300652', 'https://openalex.org/W6852750031', 'https://openalex.org/W6603931906', 'https://openalex.org/W4382202703', 'https://openalex.org/W2896457183', 'https://openalex.org/W3198217962', 'https://openalex.org/W6783867762', 'https://openalex.org/W4385245566', 'https://openalex.org/W3196027980', 'https://openalex.org/W4372341043', 'https://openalex.org/W3216976702', 'https://openalex.org/W4377000449', 'https://openalex.org/W2979476256', 'https://openalex.org/W3033411150', 'https://openalex.org/W3174758275']",2024-03-18
https://openalex.org/W4393159110,https://doi.org/10.1609/aaai.v38i3.28056,Expand-and-Quantize: Unsupervised Semantic Segmentation Using High-Dimensional Space and Product Quantization,"Unsupervised semantic segmentation (USS) aims to discover and recognize meaningful categories without any labels. For a successful USS, two key abilities are required: 1) information compression and 2) clustering capability. Previous methods have relied on feature dimension reduction for information compression, however, this approach may hinder the process of clustering. In this paper, we propose a novel USS framework called Expand-and-Quantize Unsupervised Semantic Segmentation (EQUSS), which combines the benefits of high-dimensional spaces for better clustering and product quantization for effective information compression. Our extensive experiments demonstrate that EQUSS achieves state-of-the-art results on three standard benchmarks. In addition, we analyze the entropy of USS features, which is the first step towards understanding USS from the perspective of information theory.","['https://openalex.org/W2981227223', 'https://openalex.org/W6730587030', 'https://openalex.org/W2883725317', 'https://openalex.org/W3159481202', 'https://openalex.org/W3148215308', 'https://openalex.org/W2340897893', 'https://openalex.org/W2161278885', 'https://openalex.org/W2066792529', 'https://openalex.org/W3008115128', 'https://openalex.org/W3021450295', 'https://openalex.org/W4221167013', 'https://openalex.org/W3137561054', 'https://openalex.org/W6687483927', 'https://openalex.org/W6785340122', 'https://openalex.org/W6796581206', 'https://openalex.org/W4283737104', 'https://openalex.org/W3198680251', 'https://openalex.org/W6678775411', 'https://openalex.org/W2012592962', 'https://openalex.org/W2924027593', 'https://openalex.org/W4226221010', 'https://openalex.org/W4382240639', 'https://openalex.org/W2982512683', 'https://openalex.org/W2161236525', 'https://openalex.org/W2948678175', 'https://openalex.org/W4306177415', 'https://openalex.org/W2124386111', 'https://openalex.org/W3043515726', 'https://openalex.org/W2962858109', 'https://openalex.org/W4361194087', 'https://openalex.org/W6648982606', 'https://openalex.org/W2892220819', 'https://openalex.org/W3119189390', 'https://openalex.org/W3217619776', 'https://openalex.org/W3048889251', 'https://openalex.org/W2754084392', 'https://openalex.org/W2950180292', 'https://openalex.org/W4226352467', 'https://openalex.org/W4307535422', 'https://openalex.org/W4225718869', 'https://openalex.org/W2561196672', 'https://openalex.org/W3172288085', 'https://openalex.org/W3022414928', 'https://openalex.org/W2242818861', 'https://openalex.org/W2952793010', 'https://openalex.org/W3017022649', 'https://openalex.org/W1959608418', 'https://openalex.org/W3101749733', 'https://openalex.org/W4382202703', 'https://openalex.org/W3168867926', 'https://openalex.org/W3107725261', 'https://openalex.org/W3009561768', 'https://openalex.org/W2993383518', 'https://openalex.org/W2124509324', 'https://openalex.org/W4386065436', 'https://openalex.org/W4386076264', 'https://openalex.org/W4212875960', 'https://openalex.org/W3035665735', 'https://openalex.org/W2194775991', 'https://openalex.org/W3206138617', 'https://openalex.org/W1686946872']",2024-03-24
https://openalex.org/W4411643490,https://doi.org/10.4018/979-8-3693-5477-3.ch007,The Role of Artificial Intelligence in Linguistic Corpus Analysis,"This chapter explores the transformative role of artificial intelligence (AI) in linguistic corpus analysis within applied linguistics. It begins with an overview of applied linguistics and the significant methodological development of corpus analysis, emphasizing its applications in teaching, language use analysis, and lexicography. The integration of AI in linguistic research is examined, focusing on advancements in natural language processing (NLP) and machine learning, which enhance the depth and accuracy of language analysis. The chapter highlights how AI-driven tools, such as deep learning models and neural networks, facilitate complex tasks like speech recognition, sentiment analysis, and text classification. It also discusses the evolution of corpus analysis tools and the impact of AI innovations on language teaching and translation research. The chapter concludes by addressing the challenges and future trends in AI and corpus analysis, underscoring the potential for interdisciplinary collaboration and the ethical considerations involved in AI applications in linguistics.","['https://openalex.org/W2003898308', 'https://openalex.org/W3056136627', 'https://openalex.org/W3012348148', 'https://openalex.org/W2161411600', 'https://openalex.org/W2123747430', 'https://openalex.org/W2903938540', 'https://openalex.org/W4382202703', 'https://openalex.org/W2968719776', 'https://openalex.org/W4384206993', 'https://openalex.org/W2759200442', 'https://openalex.org/W3087309461', 'https://openalex.org/W3119532183', 'https://openalex.org/W3136971013', 'https://openalex.org/W2982166382', 'https://openalex.org/W3003814985', 'https://openalex.org/W2075457132', 'https://openalex.org/W1976772830', 'https://openalex.org/W4386544253', 'https://openalex.org/W4386644478', 'https://openalex.org/W2998932151', 'https://openalex.org/W4287847662', 'https://openalex.org/W3018827121', 'https://openalex.org/W1524429405', 'https://openalex.org/W2029594649', 'https://openalex.org/W3156333129', 'https://openalex.org/W1527199003', 'https://openalex.org/W2888846440', 'https://openalex.org/W4323059975', 'https://openalex.org/W4226394354', 'https://openalex.org/W2437096199', 'https://openalex.org/W360593475', 'https://openalex.org/W4389429017', 'https://openalex.org/W4281476767', 'https://openalex.org/W1658357284', 'https://openalex.org/W4323237704', 'https://openalex.org/W1882911078', 'https://openalex.org/W4319923055', 'https://openalex.org/W3103252638', 'https://openalex.org/W2924677654', 'https://openalex.org/W4221151672', 'https://openalex.org/W4229018250', 'https://openalex.org/W4293061779', 'https://openalex.org/W4313181389', 'https://openalex.org/W3103536442']",2025-02-21
https://openalex.org/W4392903524,https://doi.org/10.1109/icassp48485.2024.10446203,Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding,"Recently, there has been a growing interest in text-to-speech (TTS) methods that can be trained with minimal supervision by combining two types of discrete speech representations and using two sequence-to-sequence tasks to decouple TTS. However, existing methods suffer from three problems: the high-frequency waveform distortion of discrete speech representations, the prosodic averaging problem caused by the duration prediction model in non-autoregressive frameworks, and difficulty in prediction due to the information redundancy and dimension explosion of existing semantic coding methods. To address these problems, three progressive methods are proposed. First, we propose Diff-LM-Speech, an autoregressive structure consisting of a language model and diffusion models, which models the semantic embedding into the mel-spectrogram based on a diffusion model to achieve higher audio quality. We also introduce a prompt encoder structure based on a variational autoencoder and a prosody bottleneck to improve prompt representation ability. Second, we propose Tetra-Diff-Speech, a non-autoregressive structure consisting of four diffusion model-based modules that design a duration diffusion model to achieve diverse prosodic expressions. Finally, we propose Tri-Diff-Speech, a non-autoregressive structure consisting of three diffusion model-based modules that verify the non-necessity of existing semantic coding models and achieve the best results. Experimental results show that our proposed methods outperform baseline methods. We provide a website with audio samples. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>","['https://openalex.org/W2963609956', 'https://openalex.org/W2591927543', 'https://openalex.org/W2903739847', 'https://openalex.org/W6763832098', 'https://openalex.org/W6777694618', 'https://openalex.org/W3161296985', 'https://openalex.org/W6778883912', 'https://openalex.org/W4381786045', 'https://openalex.org/W4390075359', 'https://openalex.org/W4296068981', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3215615641', 'https://openalex.org/W6779823529', 'https://openalex.org/W6847363464', 'https://openalex.org/W4372260402', 'https://openalex.org/W2752782242', 'https://openalex.org/W4312069022', 'https://openalex.org/W4292779060', 'https://openalex.org/W3036601975', 'https://openalex.org/W3094002217', 'https://openalex.org/W2946200149', 'https://openalex.org/W4382603054', 'https://openalex.org/W3036167779', 'https://openalex.org/W2519091744', 'https://openalex.org/W4307323391', 'https://openalex.org/W1522301498', 'https://openalex.org/W4402301063', 'https://openalex.org/W4366460484', 'https://openalex.org/W3026874504', 'https://openalex.org/W4313679638', 'https://openalex.org/W4323651091']",2024-03-18
https://openalex.org/W4410815513,https://doi.org/10.1016/j.csl.2025.101825,"ASVspoof 5: Design, collection and validation of resources for spoofing, deepfake, and adversarial attack detection using crowdsourced speech","ASVspoof 5 is the fifth edition in a series of challenges which promote the study of speech spoofing and deepfake attacks as well as the design of detection solutions. We introduce the ASVspoof 5 database which is generated in a crowdsourced fashion from data collected in diverse acoustic conditions (cf. studio-quality data for earlier ASVspoof databases) and from ∼2000 speakers (cf. ∼100 earlier). The database contains attacks generated with 32 different algorithms, also crowdsourced, and optimised to varying degrees using new surrogate detection models. Among them are attacks generated with a mix of legacy and contemporary text-tospeech synthesis and voice conversion models, in addition to adversarial attacks which are incorporated for the first time. ASVspoof 5 protocols comprise seven speaker-disjoint partitions. They include two distinct partitions for the training of different sets of attack models, two more for the development and evaluation of surrogate detection models, and then three additional partitions which comprise the ASVspoof 5 training, development and evaluation sets. An auxiliary set of data collected from an additional 30k speakers can also be used to train speaker encoders for the implementation of attack algorithms. Also described herein is an experimental validation of the new ASVspoof 5 database using a set of automatic speaker verification and spoof/deepfake baseline detectors. With the exception of protocols and tools for the generation of spoofed/deepfake speech, the resources described in this paper, already used by participants of the ASVspoof 5 challenge in 2024, are now all freely available to the community.","['https://openalex.org/W3096037340', 'https://openalex.org/W2995929068', 'https://openalex.org/W6748588790', 'https://openalex.org/W6787054596', 'https://openalex.org/W6780218876', 'https://openalex.org/W6869067041', 'https://openalex.org/W4200631896', 'https://openalex.org/W6790220310', 'https://openalex.org/W3082563516', 'https://openalex.org/W3110588169', 'https://openalex.org/W2157331557', 'https://openalex.org/W6623517193', 'https://openalex.org/W2808631503', 'https://openalex.org/W3202278141', 'https://openalex.org/W3015826515', 'https://openalex.org/W4307323391', 'https://openalex.org/W3097945073', 'https://openalex.org/W3024869864', 'https://openalex.org/W2112606109', 'https://openalex.org/W3016970897', 'https://openalex.org/W4402301063', 'https://openalex.org/W1945616565', 'https://openalex.org/W3097777922', 'https://openalex.org/W4367721746', 'https://openalex.org/W3016160783', 'https://openalex.org/W2194775991', 'https://openalex.org/W6682221467', 'https://openalex.org/W2884225676', 'https://openalex.org/W6602488901', 'https://openalex.org/W3201773091', 'https://openalex.org/W4297841773', 'https://openalex.org/W3096084197', 'https://openalex.org/W4297841768', 'https://openalex.org/W4402111459', 'https://openalex.org/W2972667718', 'https://openalex.org/W6635894473', 'https://openalex.org/W3026874504', 'https://openalex.org/W3169905056', 'https://openalex.org/W6752910514', 'https://openalex.org/W6640963894', 'https://openalex.org/W2154278880', 'https://openalex.org/W2745896134', 'https://openalex.org/W2401207139', 'https://openalex.org/W2696967604', 'https://openalex.org/W3092028330', 'https://openalex.org/W3150572638', 'https://openalex.org/W2921802966', 'https://openalex.org/W6769743888', 'https://openalex.org/W4381198892', 'https://openalex.org/W3006808893', 'https://openalex.org/W2963035245', 'https://openalex.org/W6857434696', 'https://openalex.org/W4319862723', 'https://openalex.org/W6635268374', 'https://openalex.org/W4281492411', 'https://openalex.org/W3200527256', 'https://openalex.org/W4385823275', 'https://openalex.org/W2726515241', 'https://openalex.org/W4385823466', 'https://openalex.org/W2255859285', 'https://openalex.org/W6853630802', 'https://openalex.org/W1494198834', 'https://openalex.org/W2936774411', 'https://openalex.org/W3162673269', 'https://openalex.org/W3204602440', 'https://openalex.org/W3095410713', 'https://openalex.org/W2963300588', 'https://openalex.org/W4311000453', 'https://openalex.org/W2964052309', 'https://openalex.org/W6870485457', 'https://openalex.org/W3204661432', 'https://openalex.org/W6639824700', 'https://openalex.org/W2115040572', 'https://openalex.org/W2399816321', 'https://openalex.org/W2964243274', 'https://openalex.org/W4385822463', 'https://openalex.org/W4392487686', 'https://openalex.org/W4280492375', 'https://openalex.org/W2890964092', 'https://openalex.org/W3110257065', 'https://openalex.org/W2774128158', 'https://openalex.org/W2507912506', 'https://openalex.org/W6637162671', 'https://openalex.org/W3163596559', 'https://openalex.org/W4403955374', 'https://openalex.org/W2936802426', 'https://openalex.org/W1481604723', 'https://openalex.org/W6739901393', 'https://openalex.org/W2962788625', 'https://openalex.org/W4403955716', 'https://openalex.org/W4402112305', 'https://openalex.org/W6857471407', 'https://openalex.org/W3119308075', 'https://openalex.org/W2794490148', 'https://openalex.org/W4372266958', 'https://openalex.org/W3026777299', 'https://openalex.org/W4385823321', 'https://openalex.org/W4405181098', 'https://openalex.org/W3150635270', 'https://openalex.org/W2176804518', 'https://openalex.org/W4235154690', 'https://openalex.org/W4225746985', 'https://openalex.org/W2588445447', 'https://openalex.org/W3197358873', 'https://openalex.org/W3194208059', 'https://openalex.org/W4385975857', 'https://openalex.org/W4221154745', 'https://openalex.org/W3082130377', 'https://openalex.org/W3197940657', 'https://openalex.org/W2962793481', 'https://openalex.org/W3034302232', 'https://openalex.org/W2171033594', 'https://openalex.org/W2788357188', 'https://openalex.org/W2519091744', 'https://openalex.org/W3090254849', 'https://openalex.org/W4400373483', 'https://openalex.org/W2219249508', 'https://openalex.org/W3174758275', 'https://openalex.org/W3113738713', 'https://openalex.org/W2408141691', 'https://openalex.org/W1597121597', 'https://openalex.org/W3103934428', 'https://openalex.org/W2998572311', 'https://openalex.org/W1586405805', 'https://openalex.org/W2144499799', 'https://openalex.org/W2967606780', 'https://openalex.org/W2592497314', 'https://openalex.org/W4402112533', 'https://openalex.org/W2504979835', 'https://openalex.org/W3036601975', 'https://openalex.org/W2150658333', 'https://openalex.org/W2187089797']",2025-05-28
https://openalex.org/W4392931282,https://doi.org/10.1109/icassp48485.2024.10448495,High-Fidelity Speech Synthesis with Minimal Supervision: All Using Diffusion Models,"Text-to-speech (TTS) methods have shown promising results in voice cloning, but they require a large number of labeled text-speech pairs. Minimally-supervised speech synthesis decouples TTS by combining two types of discrete speech representations(semantic & acoustic) and using two sequence-to-sequence tasks to enable training with minimal supervision. However, existing methods suffer from information redundancy and dimension explosion in semantic representation, and high-frequency waveform distortion in discrete acoustic representation. Autoregressive frameworks exhibit typical instability and uncontrollability issues. And non-autoregressive frameworks suffer from prosodic averaging caused by duration prediction models. To address these issues, we propose a minimally-supervised high-fidelity speech synthesis method, where all modules are constructed based on the diffusion models. The non-autoregressive framework enhances controllability, and the duration diffusion model enables diversified prosodic expression. Contrastive Token-Acoustic Pretraining (CTAP) is used as an intermediate semantic representation to solve the problems of information redundancy and dimension explosion in existing semantic coding methods. Mel-spectrogram is used as the acoustic representation. Both semantic and acoustic representations are predicted by continuous variable regression tasks to solve the problem of high-frequency fine-grained waveform distortion. Experimental results show that our proposed method outperforms the baseline method. We provide audio samples on our website. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>","['https://openalex.org/W2963609956', 'https://openalex.org/W6734815144', 'https://openalex.org/W2903739847', 'https://openalex.org/W6763832098', 'https://openalex.org/W6777694618', 'https://openalex.org/W3161296985', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W4307323391', 'https://openalex.org/W3215615641', 'https://openalex.org/W6860417749', 'https://openalex.org/W4381786045', 'https://openalex.org/W6848735303', 'https://openalex.org/W6850334629', 'https://openalex.org/W4390075359', 'https://openalex.org/W4296068981', 'https://openalex.org/W6851724922', 'https://openalex.org/W6853888607', 'https://openalex.org/W6779823529', 'https://openalex.org/W4226033575', 'https://openalex.org/W4392903524', 'https://openalex.org/W4254489317', 'https://openalex.org/W4392909624', 'https://openalex.org/W4319779739', 'https://openalex.org/W2752782242', 'https://openalex.org/W6631190155', 'https://openalex.org/W4372260402', 'https://openalex.org/W4312069022', 'https://openalex.org/W4402301063', 'https://openalex.org/W4366460484', 'https://openalex.org/W4323651091', 'https://openalex.org/W4382603054', 'https://openalex.org/W3036167779', 'https://openalex.org/W3036601975', 'https://openalex.org/W4313679638', 'https://openalex.org/W1522301498', 'https://openalex.org/W2519091744', 'https://openalex.org/W2946200149', 'https://openalex.org/W3026874504']",2024-03-18
https://openalex.org/W4392904491,https://doi.org/10.1109/icassp48485.2024.10445948,VoiceFlow: Efficient Text-To-Speech with Rectified Flow Matching,"Although diffusion models in text-to-speech have become a popular choice due to their strong generative ability, the intrinsic complexity of sampling from diffusion models harms their efficiency. Alternatively, we propose VoiceFlow, an acoustic model that utilizes a rectified flow matching algorithm to achieve high synthesis quality with a limited number of sampling steps. VoiceFlow formulates the process of generating mel-spectrograms into an ordinary differential equation conditional on text inputs, whose vector field is then estimated. The rectified flow technique then effectively straightens its sampling trajectory for efficient synthesis. Subjective and objective evaluations on both single and multi-speaker corpora showed the superior synthesis quality of VoiceFlow compared to the diffusion counterpart. Ablation studies further verified the validity of the rectified flow technique in VoiceFlow.","['https://openalex.org/W2975414524', 'https://openalex.org/W6796464841', 'https://openalex.org/W6777694618', 'https://openalex.org/W6778083308', 'https://openalex.org/W4226132755', 'https://openalex.org/W4386133927', 'https://openalex.org/W6795261426', 'https://openalex.org/W3158762648', 'https://openalex.org/W4375869198', 'https://openalex.org/W4393147067', 'https://openalex.org/W6849953009', 'https://openalex.org/W4375869364', 'https://openalex.org/W4297841480', 'https://openalex.org/W6795288823', 'https://openalex.org/W4296069326', 'https://openalex.org/W6802527329', 'https://openalex.org/W6809884996', 'https://openalex.org/W6811291704', 'https://openalex.org/W4285605725', 'https://openalex.org/W6811008979', 'https://openalex.org/W4304099317', 'https://openalex.org/W6783713337', 'https://openalex.org/W4372260261', 'https://openalex.org/W6838327568', 'https://openalex.org/W4387968164', 'https://openalex.org/W6850614898', 'https://openalex.org/W6846539466', 'https://openalex.org/W6843731886', 'https://openalex.org/W6849790239', 'https://openalex.org/W6853888607', 'https://openalex.org/W6853068122', 'https://openalex.org/W6917585676', 'https://openalex.org/W2972359262', 'https://openalex.org/W6631362777', 'https://openalex.org/W6783867762', 'https://openalex.org/W2972394484', 'https://openalex.org/W4380550897', 'https://openalex.org/W4382603054']",2024-03-18
https://openalex.org/W4392902857,https://doi.org/10.1109/icassp48485.2024.10446160,SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention,"Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary unseen target speaker timbre, while keeping the linguistic content unchanged. Although the voice of generated speech can be controlled by providing the speaker embedding of the target speaker, the speaker similarity still lags behind the ground truth recordings. In this paper, we propose SEF-VC, a speaker embedding free voice conversion model, which is designed to learn and incorporate speaker timbre from reference speech via a powerful position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic tokens in a non-autoregressive manner. The concise design of SEF-VC enhances its training stability and voice conversion performance. Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality speech with better similarity to target reference than strong zero-shot VC baselines, even for very short reference speeches.","['https://openalex.org/W2532494225', 'https://openalex.org/W6762533536', 'https://openalex.org/W6776390925', 'https://openalex.org/W6810585344', 'https://openalex.org/W4221146610', 'https://openalex.org/W6752910514', 'https://openalex.org/W3196584150', 'https://openalex.org/W4386133927', 'https://openalex.org/W6805710207', 'https://openalex.org/W6769196770', 'https://openalex.org/W3209059054', 'https://openalex.org/W3140429000', 'https://openalex.org/W3197763626', 'https://openalex.org/W4224916404', 'https://openalex.org/W3161695192', 'https://openalex.org/W2890964092', 'https://openalex.org/W2804998325', 'https://openalex.org/W4280552695', 'https://openalex.org/W4225329057', 'https://openalex.org/W4224916568', 'https://openalex.org/W3160950295', 'https://openalex.org/W4381786045', 'https://openalex.org/W4386132131', 'https://openalex.org/W4393147067', 'https://openalex.org/W6783867762', 'https://openalex.org/W3097777922', 'https://openalex.org/W2972359262', 'https://openalex.org/W2972659941', 'https://openalex.org/W4375869015', 'https://openalex.org/W6631362777', 'https://openalex.org/W4378945745', 'https://openalex.org/W3154134465']",2024-03-18
https://openalex.org/W4396696854,https://doi.org/10.36244/icj.2024.1.5,Advancements in Expressive Speech Synthesis: a Review,"In recent years, we have witnessed a fast and wide spread acceptance of speech synthesis technology in, leading to the transition toward a society characterized by a strong desire to incorporate these applications in their daily lives. We provide a comprehensive survey on the recent advancements in the field of expressive Text-To- Speech systems. Among different methods to represent expressivity, this paper focuses the development of ex pressive TTS systems, emphasizing the methodologies employed to enhance the quality and expressiveness of synthetic speech, such as style transfer and improving speaker variability. After that, we point out some of the subjective and objective metrics that are used to evaluate the quality of synthesized speech. Fi nally, we point out the realm of child speech synthesis, a domain that has been neglected for some time. This underscores that the f ield of research in children's speech synthesis is still wide open for exploration and development. Overall, this paper presents a comprehensive overview of historical and contemporary trends and future directions in speech synthesis research.","['https://openalex.org/W4286515285', 'https://openalex.org/W2969521066', 'https://openalex.org/W3206725777', 'https://openalex.org/W2801291345', 'https://openalex.org/W3130016944', 'https://openalex.org/W4383220533', 'https://openalex.org/W2571786242', 'https://openalex.org/W3097538987', 'https://openalex.org/W2120847449', 'https://openalex.org/W3161492781', 'https://openalex.org/W4213302250', 'https://openalex.org/W4386133927', 'https://openalex.org/W3162770051', 'https://openalex.org/W4385478010', 'https://openalex.org/W4224301045', 'https://openalex.org/W349236604', 'https://openalex.org/W4296068767', 'https://openalex.org/W3198048667', 'https://openalex.org/W4294619240', 'https://openalex.org/W1643818298', 'https://openalex.org/W4300553212', 'https://openalex.org/W4294640375', 'https://openalex.org/W158957827', 'https://openalex.org/W3150572638', 'https://openalex.org/W2407523469', 'https://openalex.org/W1988743634', 'https://openalex.org/W1999885698', 'https://openalex.org/W2129082420', 'https://openalex.org/W2035589087', 'https://openalex.org/W2102146461', 'https://openalex.org/W4247621788', 'https://openalex.org/W2808706139', 'https://openalex.org/W2972384058', 'https://openalex.org/W4382371192', 'https://openalex.org/W2168531172', 'https://openalex.org/W2885800352', 'https://openalex.org/W2964058423', 'https://openalex.org/W4312564489', 'https://openalex.org/W3092028330', 'https://openalex.org/W3015853838', 'https://openalex.org/W4372318157', 'https://openalex.org/W2963830550', 'https://openalex.org/W2964138190', 'https://openalex.org/W2963609956', 'https://openalex.org/W4225276642', 'https://openalex.org/W4297841267', 'https://openalex.org/W2964243274', 'https://openalex.org/W2760103357', 'https://openalex.org/W3116819420', 'https://openalex.org/W4296069154', 'https://openalex.org/W3094785744', 'https://openalex.org/W2519091744', 'https://openalex.org/W3195366750', 'https://openalex.org/W4281736089', 'https://openalex.org/W2794490148', 'https://openalex.org/W183794703', 'https://openalex.org/W2394902181', 'https://openalex.org/W4385822455', 'https://openalex.org/W4295731579', 'https://openalex.org/W3126283728', 'https://openalex.org/W2165698076', 'https://openalex.org/W3197668461', 'https://openalex.org/W4281909621', 'https://openalex.org/W2161736993', 'https://openalex.org/W3016159759', 'https://openalex.org/W2932022923', 'https://openalex.org/W2946200149', 'https://openalex.org/W3174758275', 'https://openalex.org/W1570629387', 'https://openalex.org/W4385329631', 'https://openalex.org/W4308840623', 'https://openalex.org/W2125047278', 'https://openalex.org/W4221167708', 'https://openalex.org/W4312096453', 'https://openalex.org/W4319779739', 'https://openalex.org/W3135418837', 'https://openalex.org/W3198082505', 'https://openalex.org/W2102003408', 'https://openalex.org/W4297841867', 'https://openalex.org/W2976159681', 'https://openalex.org/W2003547693', 'https://openalex.org/W3163031268', 'https://openalex.org/W4312339389', 'https://openalex.org/W2921059071', 'https://openalex.org/W2121177394', 'https://openalex.org/W3132489765', 'https://openalex.org/W3015338123', 'https://openalex.org/W2791477301', 'https://openalex.org/W4297841496', 'https://openalex.org/W2066452495', 'https://openalex.org/W1523817494', 'https://openalex.org/W3038088231', 'https://openalex.org/W4296965655', 'https://openalex.org/W2034995162', 'https://openalex.org/W4386159913']",2024-01-01
https://openalex.org/W4396494551,https://doi.org/10.18280/ts.410247,Synthesis and Restoration of Traditional Ethnic Musical Instrument Timbres Based on Time-Frequency Analysis,"With the advent of the digital age, the preservation and restoration of the timbres of traditional ethnic musical instruments have emerged as significant areas of study in musicology and signal processing.Music serves not only as a bridge between history and culture but also plays an irreplaceable role in expressing ethnic characteristics and emotions.The timbres of traditional ethnic musical instruments, owing to their unique musical expressiveness and cultural value, have attracted widespread attention from both the academic and industrial sectors.However, many valuable timbre recordings are facing threats of damage and disappearance due to limitations in old recording technologies and preservation conditions.Moreover, existing timbre processing technologies still require improvements in separation accuracy, synthesis authenticity, and restoration naturalness.This study aims to achieve efficient separation, authentic synthesis, and natural restoration of the sounds of traditional ethnic musical instruments through advanced signal processing methods.Initially, this paper discusses a sound separation technique for traditional ethnic musical instruments based on time-frequency analysis, addressing the issue of insufficient resolution in complex audio signals.Subsequently, it proposes a timbre synthesis method based on the Transformer deep learning model, which can understand and reproduce the delicate timbral characteristics of musical instruments.Finally, addressing the continuity issue in timbre restoration, this paper introduces an innovative restoration technique to enhance the quality of damaged audio restoration and auditory consistency.Through the application of these methods, this study not only contributes to the protection and restoration of traditional timbres but also advances related audio processing technologies.","['https://openalex.org/W4310697922', 'https://openalex.org/W3132606237', 'https://openalex.org/W4385823161', 'https://openalex.org/W4366493008', 'https://openalex.org/W4365800698', 'https://openalex.org/W4385823117', 'https://openalex.org/W4323339370', 'https://openalex.org/W2973507319', 'https://openalex.org/W4375868847', 'https://openalex.org/W4362711784', 'https://openalex.org/W4386133927', 'https://openalex.org/W4296425809', 'https://openalex.org/W4376457142', 'https://openalex.org/W3122300013', 'https://openalex.org/W2883077235']",2024-04-30
https://openalex.org/W4392903159,https://doi.org/10.1109/icassp48485.2024.10446063,Acoustic BPE for Speech Generation with Discrete Tokens,"Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.","['https://openalex.org/W2973049979', 'https://openalex.org/W3209059054', 'https://openalex.org/W2150593711', 'https://openalex.org/W4226033575', 'https://openalex.org/W3209984917', 'https://openalex.org/W6769196770', 'https://openalex.org/W6780218876', 'https://openalex.org/W6790356757', 'https://openalex.org/W2964243274', 'https://openalex.org/W4226132755', 'https://openalex.org/W4372260125', 'https://openalex.org/W4386133927', 'https://openalex.org/W4381786045', 'https://openalex.org/W4390075359', 'https://openalex.org/W6755207826', 'https://openalex.org/W4226507725', 'https://openalex.org/W4385822683', 'https://openalex.org/W6777028661', 'https://openalex.org/W2972374322', 'https://openalex.org/W4372260534', 'https://openalex.org/W2972359262', 'https://openalex.org/W2995181338', 'https://openalex.org/W6769627184', 'https://openalex.org/W6853244311', 'https://openalex.org/W3024605872', 'https://openalex.org/W4288089799', 'https://openalex.org/W3036601975', 'https://openalex.org/W4394671563', 'https://openalex.org/W2896457183', 'https://openalex.org/W4393147067']",2024-03-18
https://openalex.org/W4409523447,https://doi.org/10.1038/s40494-025-01668-0,Discrete diffusion model with contrastive learning for music to natural and long dance generation,"Abstract With the deep integration of culture and technology, the digital research on cultural content like music and dance is constantly evolving. This paper focuses on the research of music and dance generation, aiming to boost cultural dissemination. The core challenge of this task is to generate natural dance sequences that align with the duration of the provided music. Therefore, we propose a discrete diffusion model with contrastive learning. First, the dance VQ-VAE model is introduced and pre-trained to learn the mapping relationship between dance data and discrete token sequences. Second, the Music-conditional Contrast Learning loss is designed to enhance the training of the discrete diffusion model, enabling it to predict discrete token sequences conditioned on musical features. Subsequently, the discrete token sequences are decoded into dance sequences with the dance VQ-VAE decoder. Finally, the temporal consistency between multiple sequences is enhanced by implementing time constraints to generate long dance sequences.","['https://openalex.org/W2070460706', 'https://openalex.org/W4394667479', 'https://openalex.org/W2995928604', 'https://openalex.org/W4390954692', 'https://openalex.org/W3035486405', 'https://openalex.org/W4386065807', 'https://openalex.org/W3036167779', 'https://openalex.org/W2129069237', 'https://openalex.org/W3096831136', 'https://openalex.org/W3103543904', 'https://openalex.org/W4392909102', 'https://openalex.org/W4391305822', 'https://openalex.org/W569478347', 'https://openalex.org/W3180355996', 'https://openalex.org/W3008715487', 'https://openalex.org/W3129576130', 'https://openalex.org/W4303448003', 'https://openalex.org/W4287802874', 'https://openalex.org/W4390871712', 'https://openalex.org/W606806978', 'https://openalex.org/W2982625143', 'https://openalex.org/W3144253442', 'https://openalex.org/W4312635677', 'https://openalex.org/W4313145975', 'https://openalex.org/W3135367836', 'https://openalex.org/W4286611314', 'https://openalex.org/W2897492344', 'https://openalex.org/W3204221554', 'https://openalex.org/W4386071567', 'https://openalex.org/W4206861281', 'https://openalex.org/W4312671789', 'https://openalex.org/W6798623513', 'https://openalex.org/W4304084213', 'https://openalex.org/W4283074862', 'https://openalex.org/W3160694286', 'https://openalex.org/W4238826863', 'https://openalex.org/W2191779130', 'https://openalex.org/W3010876998', 'https://openalex.org/W4313167431']",2025-04-17
https://openalex.org/W4385805114,https://doi.org/10.1109/cvprw59228.2023.00615,Multi-modal Facial Affective Analysis based on Masked Autoencoder,"Human affective behavior analysis focuses on analyzing human expressions or other behaviors to enhance the understanding of human psychology. The CVPR 2023 Competition on Affective Behavior Analysis in-the-wild (ABAW) is dedicated to providing high-quality and large-scale Affwild2 for the recognition of commonly used emotion representations, such as Action Units (AU), basic expression categories (EXPR), and Valence-Arousal (VA). The competition is committed to making significant strides in improving the accuracy and practicality of affective analysis research in real-world scenarios. In this paper, we introduce our submission to the CVPR 2023: ABAW5. Our approach involves several key components. First, we utilize the visual information from a Masked Autoencoder (MAE) model that has been pre-trained on a large-scale face image dataset in a self-supervised manner. Next, we finetune the MAE encoder on the image frames from the Aff-wild2 for AU, EXPR and VA tasks, which can be regarded as a static and uni-modal training. Additionally, we leverage the multi-modal and temporal information from the videos and implement a transformer-based framework to fuse the multimodal features. Our approach achieves impressive results in the ABAW5 competition, with an average F1 score of 55.49% and 41.21% in the AU and EXPR tracks, respectively, and an average CCC of 0.6372 in the VA track. Our approach ranks first in the EXPR and AU tracks, and second in the VA track. Extensive quantitative experiments and ablation studies demonstrate the effectiveness of our proposed method.","['https://openalex.org/W4313156423', 'https://openalex.org/W3209454485', 'https://openalex.org/W6635087628', 'https://openalex.org/W2980299316', 'https://openalex.org/W2526050071', 'https://openalex.org/W3210530853', 'https://openalex.org/W3179028823', 'https://openalex.org/W3162474807', 'https://openalex.org/W6784333009', 'https://openalex.org/W6739901393', 'https://openalex.org/W6633802082', 'https://openalex.org/W6798391549', 'https://openalex.org/W6762718338', 'https://openalex.org/W6845706862', 'https://openalex.org/W4307045186', 'https://openalex.org/W4292829120', 'https://openalex.org/W3197642003', 'https://openalex.org/W2510725918', 'https://openalex.org/W3034429256', 'https://openalex.org/W3011227460', 'https://openalex.org/W2969985801', 'https://openalex.org/W3024869864', 'https://openalex.org/W2807126412', 'https://openalex.org/W6780218876', 'https://openalex.org/W6774314701', 'https://openalex.org/W2481681431', 'https://openalex.org/W2745497104', 'https://openalex.org/W6757817989', 'https://openalex.org/W1834627138', 'https://openalex.org/W6846300701', 'https://openalex.org/W6741276872', 'https://openalex.org/W3209397829', 'https://openalex.org/W3138516171', 'https://openalex.org/W4214612132', 'https://openalex.org/W4312769845', 'https://openalex.org/W2045472600', 'https://openalex.org/W4221166187', 'https://openalex.org/W2051297709', 'https://openalex.org/W6770092901', 'https://openalex.org/W3210812086', 'https://openalex.org/W3126750668', 'https://openalex.org/W4385815442', 'https://openalex.org/W6794746887', 'https://openalex.org/W3185372235', 'https://openalex.org/W6630649318', 'https://openalex.org/W4292794012', 'https://openalex.org/W2713788831', 'https://openalex.org/W6769650007', 'https://openalex.org/W2798536775', 'https://openalex.org/W6793244244', 'https://openalex.org/W6800945776', 'https://openalex.org/W3005680577', 'https://openalex.org/W2955425717', 'https://openalex.org/W3200032182', 'https://openalex.org/W3208945181', 'https://openalex.org/W2981958954', 'https://openalex.org/W3122081138', 'https://openalex.org/W4221149405', 'https://openalex.org/W3209738570', 'https://openalex.org/W4221155463', 'https://openalex.org/W1563795667', 'https://openalex.org/W2908510526', 'https://openalex.org/W3161318013', 'https://openalex.org/W4385245566', 'https://openalex.org/W4327810650', 'https://openalex.org/W4385805174', 'https://openalex.org/W4327992746', 'https://openalex.org/W4221144400', 'https://openalex.org/W4287082647', 'https://openalex.org/W3180874665', 'https://openalex.org/W4307783939', 'https://openalex.org/W1588539311', 'https://openalex.org/W4221148654', 'https://openalex.org/W4288102735', 'https://openalex.org/W4221143482', 'https://openalex.org/W4220659256', 'https://openalex.org/W4327810652', 'https://openalex.org/W3036601975', 'https://openalex.org/W4221148657', 'https://openalex.org/W4221148554', 'https://openalex.org/W4385801686', 'https://openalex.org/W3148234074', 'https://openalex.org/W4297775537', 'https://openalex.org/W3094502228', 'https://openalex.org/W3115865297', 'https://openalex.org/W4330338944', 'https://openalex.org/W4304699880', 'https://openalex.org/W2738672149', 'https://openalex.org/W1509966554', 'https://openalex.org/W4330336233', 'https://openalex.org/W3100513545']",2023-06-01
https://openalex.org/W4361199948,https://doi.org/10.23919/icact56868.2023.10079686,Chinese ASR and NER Improvement Based on Whisper Fine-Tuning,"Based on 680k hours of weakly supervised multilingual and multi-task speech transcription/translation data, Whisper [1] has developed a robust system for both Automated Speech Recognition (ASR) and Speech Translation (ST). Whisper provides a simple model architecture based on Mel spectrum + two-layer convolution + Seq2seq Transformer architecture, which is easy to fine-tune on conditional generation tasks. This paper analyzes how to fine-tune Chinese ASR [2] and NER tasks based on Whisper, including (1) how to design different prompts for different generative tasks; (2) how to train ASR and NER tasks at the same time; (3) whether the performance can be further improved by using weak supervision for data enhancement. Experiments based on AISHELL [3] and AISHELL-NER [4] data, and multi-task fine-tuning based on Whisper can effectively improve the performance of ASR and NER.","['https://openalex.org/W6847363464', 'https://openalex.org/W2946558277', 'https://openalex.org/W6752788575', 'https://openalex.org/W4221139374', 'https://openalex.org/W3097777922', 'https://openalex.org/W4285107548', 'https://openalex.org/W6811100430', 'https://openalex.org/W4285163683', 'https://openalex.org/W6780218876', 'https://openalex.org/W3210530853', 'https://openalex.org/W3095918555', 'https://openalex.org/W4285819432', 'https://openalex.org/W2943357157', 'https://openalex.org/W3034379414', 'https://openalex.org/W4385245566', 'https://openalex.org/W2627092829', 'https://openalex.org/W4246768052', 'https://openalex.org/W2962904552', 'https://openalex.org/W2914417638', 'https://openalex.org/W3204917342', 'https://openalex.org/W2608377846', 'https://openalex.org/W4226251829', 'https://openalex.org/W4311000453', 'https://openalex.org/W3036601975']",2023-02-19
https://openalex.org/W4323322934,https://doi.org/10.1145/3544548.3580706,WESPER: Zero-shot and Realtime Whisper to Normal Voice Conversion for Whisper-based Speech Interactions,"Recognizing whispered speech and converting it to normal speech creates many\npossibilities for speech interaction. Because the sound pressure of whispered\nspeech is significantly lower than that of normal speech, it can be used as a\nsemi-silent speech interaction in public places without being audible to\nothers. Converting whispers to normal speech also improves the speech quality\nfor people with speech or hearing impairments. However, conventional speech\nconversion techniques do not provide sufficient conversion quality or require\nspeaker-dependent datasets consisting of pairs of whispered and normal speech\nutterances. To address these problems, we propose WESPER, a zero-shot,\nreal-time whisper-to-normal speech conversion mechanism based on\nself-supervised learning. WESPER consists of a speech-to-unit (STU) encoder,\nwhich generates hidden speech units common to both whispered and normal speech,\nand a unit-to-speech (UTS) decoder, which reconstructs speech from the encoded\nspeech units. Unlike the existing methods, this conversion is user-independent\nand does not require a paired dataset for whispered and normal speech. The UTS\ndecoder can reconstruct speech in any target speaker's voice from speech units,\nand it requires only an unlabeled target speaker's speech data. We confirmed\nthat the quality of the speech converted from a whisper was improved while\npreserving its natural prosody. Additionally, we confirmed the effectiveness of\nthe proposed approach to perform speech reconstruction for people with speech\nor hearing disabilities. (project page: http://lab.rekimoto.org/projects/wesper\n)\n","['https://openalex.org/W2302259492', 'https://openalex.org/W1904948456', 'https://openalex.org/W2928664166', 'https://openalex.org/W4297677272', 'https://openalex.org/W4283031696', 'https://openalex.org/W3161704465', 'https://openalex.org/W4287752738', 'https://openalex.org/W2008120082', 'https://openalex.org/W2896457183', 'https://openalex.org/W2896622783', 'https://openalex.org/W3208900277', 'https://openalex.org/W2127141656', 'https://openalex.org/W2768731046', 'https://openalex.org/W3154296827', 'https://openalex.org/W2083932711', 'https://openalex.org/W3209059054', 'https://openalex.org/W4288079605', 'https://openalex.org/W2805669069', 'https://openalex.org/W2774848319', 'https://openalex.org/W2793257307', 'https://openalex.org/W2941048526', 'https://openalex.org/W3092028330', 'https://openalex.org/W4290607240', 'https://openalex.org/W4394671563', 'https://openalex.org/W3015465870', 'https://openalex.org/W2747874407', 'https://openalex.org/W2786672974', 'https://openalex.org/W2737426676', 'https://openalex.org/W4293239651', 'https://openalex.org/W1494198834', 'https://openalex.org/W4289666046', 'https://openalex.org/W6967178875', 'https://openalex.org/W4307475373', 'https://openalex.org/W3130016944', 'https://openalex.org/W2516855573', 'https://openalex.org/W1969139822', 'https://openalex.org/W2897318954', 'https://openalex.org/W2518172956', 'https://openalex.org/W2121895216', 'https://openalex.org/W3210530853', 'https://openalex.org/W4307536511', 'https://openalex.org/W110017167', 'https://openalex.org/W3036601975', 'https://openalex.org/W2945478979', 'https://openalex.org/W3105124182', 'https://openalex.org/W2604184139', 'https://openalex.org/W3102538494', 'https://openalex.org/W3197642003', 'https://openalex.org/W2499854530', 'https://openalex.org/W96541173', 'https://openalex.org/W3113594615']",2023-04-19
https://openalex.org/W4372266927,https://doi.org/10.1109/icassp49357.2023.10094705,Self-Supervised Learning for Speech Enhancement Through Synthesis,"Modern speech enhancement (SE) networks typically implement noise suppression through time-frequency masking, latent representation masking, or discriminative signal prediction. In contrast, some recent works explore SE via generative speech synthesis, where the system's output is synthesized by a neural vocoder after an inherently lossy feature-denoising step. In this paper, we propose a denoising vocoder (DeVo) approach, where a vocoder accepts noisy representations and learns to directly synthesize clean speech. We leverage rich representations from self-supervised learning (SSL) speech models to discover relevant features. We conduct a candidate search across 15 potential SSL front-ends and subsequently train our vocoder adversarially with the best SSL configuration. Additionally, we demonstrate a causal version capable of running on streaming audio with 10ms latency and minimal performance degradation. Finally, we conduct both objective evaluations and subjective listening studies to show our system improves objective metrics and outperforms an existing state-of-the-art SE model subjectively.","['https://openalex.org/W2972443522', 'https://openalex.org/W3202667537', 'https://openalex.org/W2141998673', 'https://openalex.org/W4226403810', 'https://openalex.org/W1596515083', 'https://openalex.org/W4224933800', 'https://openalex.org/W6838724896', 'https://openalex.org/W3196475561', 'https://openalex.org/W4296069122', 'https://openalex.org/W2144404214', 'https://openalex.org/W4310873011', 'https://openalex.org/W4225302959', 'https://openalex.org/W3028019732', 'https://openalex.org/W3097945073', 'https://openalex.org/W6803547063', 'https://openalex.org/W3091570562', 'https://openalex.org/W6783867762', 'https://openalex.org/W6724944384', 'https://openalex.org/W3197580070', 'https://openalex.org/W3097906045', 'https://openalex.org/W1482149378', 'https://openalex.org/W4200483526', 'https://openalex.org/W3163296124', 'https://openalex.org/W3134695619', 'https://openalex.org/W2757519008', 'https://openalex.org/W3210530853', 'https://openalex.org/W3209059054', 'https://openalex.org/W3206252155', 'https://openalex.org/W3016181583', 'https://openalex.org/W3004597053', 'https://openalex.org/W3209984917', 'https://openalex.org/W6780218876', 'https://openalex.org/W3092028330', 'https://openalex.org/W2519091744', 'https://openalex.org/W4301371414', 'https://openalex.org/W4283652729', 'https://openalex.org/W2507009361', 'https://openalex.org/W3036601975']",2023-05-05
https://openalex.org/W4372264243,https://doi.org/10.1109/icassp49357.2023.10097120,Multi-Speaker Speech Synthesis from Electromyographic Signals by Soft Speech Unit Prediction,"Electromyographic (EMG) signals of articulatory muscles reflect the speech production process even if the user is speaking silently i.e. moving the articulators without producing audible sound. We propose Speech-Unit-based EMG-to-Speech (SU-E2S), a system which relies on EMG to synthesize speech which contains the articulated content but is vocalized in another voice, determined by an acoustic reference utterance. It is based on a Voice Conversion (VC) system which decomposes acoustic speech into continuous soft speech units and a speaker embedding and then reconstructs acoustic features. SU-E2S performs speech synthesis by predicting soft speech units from EMG and using them as input to the VC system. Experiments show that the SU-E2S output is on par in terms of intelligibility of predicting acoustic features directly from EMG, but adds the functionality of synthesizing speech in other voices.","['https://openalex.org/W3161695192', 'https://openalex.org/W3140429000', 'https://openalex.org/W4291824895', 'https://openalex.org/W169623250', 'https://openalex.org/W3210530853', 'https://openalex.org/W3197659778', 'https://openalex.org/W2008120082', 'https://openalex.org/W2768153200', 'https://openalex.org/W6772349387', 'https://openalex.org/W4224926225', 'https://openalex.org/W2395817516', 'https://openalex.org/W6917585676', 'https://openalex.org/W1846051119', 'https://openalex.org/W6847363464', 'https://openalex.org/W3197763626', 'https://openalex.org/W1932968309', 'https://openalex.org/W6783867762', 'https://openalex.org/W3197912330', 'https://openalex.org/W3024869864', 'https://openalex.org/W4286747238', 'https://openalex.org/W2745644908', 'https://openalex.org/W2160594205', 'https://openalex.org/W4283383330', 'https://openalex.org/W6762533536', 'https://openalex.org/W2419247625', 'https://openalex.org/W2537327199', 'https://openalex.org/W3175752069', 'https://openalex.org/W3105229878', 'https://openalex.org/W3092028330', 'https://openalex.org/W2998572311', 'https://openalex.org/W4311000453', 'https://openalex.org/W2945478979']",2023-05-05
https://openalex.org/W4391021367,https://doi.org/10.1109/asru57964.2023.10389651,Using Joint Training Speaker Encoder With Consistency Loss to Achieve Cross-Lingual Voice Conversion and Expressive Voice Conversion,"Voice conversion systems have made significant advancements in terms of naturalness and similarity in common voice conversion tasks. However, their performance in more complex tasks such as cross-lingual voice conversion and expressive voice conversion remains imperfect. In this study, we propose a novel approach that combines a joint training speaker encoder and content features extracted from the cross-lingual speech recognition model Whisper to achieve high-quality cross-lingual voice conversion. Additionally, we introduce a speaker consistency loss to the joint encoder, which improves the similarity between the converted speech and the reference speech. To further explore the capabilities of the joint speaker encoder, we use the Phonetic posteriorgram as the content feature, which enables the model to effectively reproduce both the speaker characteristics and the emotional aspects of the reference speech.","['https://openalex.org/W2902070858', 'https://openalex.org/W4205742757', 'https://openalex.org/W3135654121', 'https://openalex.org/W2938583109', 'https://openalex.org/W4226474318', 'https://openalex.org/W3210530853', 'https://openalex.org/W4226320669', 'https://openalex.org/W4225892118', 'https://openalex.org/W3083423753', 'https://openalex.org/W3098557217', 'https://openalex.org/W6796464841', 'https://openalex.org/W6778823374', 'https://openalex.org/W3158762648', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W4372260053', 'https://openalex.org/W4375869015', 'https://openalex.org/W3168719651', 'https://openalex.org/W6805710207', 'https://openalex.org/W3197991492', 'https://openalex.org/W6847363464', 'https://openalex.org/W6848735303', 'https://openalex.org/W6763832098', 'https://openalex.org/W3198533616', 'https://openalex.org/W2972359262', 'https://openalex.org/W6767245602', 'https://openalex.org/W6772349387', 'https://openalex.org/W2963242190', 'https://openalex.org/W2936842814', 'https://openalex.org/W2535388113', 'https://openalex.org/W6753049143', 'https://openalex.org/W4286747238', 'https://openalex.org/W4385823191', 'https://openalex.org/W3165478005', 'https://openalex.org/W2810914326', 'https://openalex.org/W2998572311', 'https://openalex.org/W2969049672', 'https://openalex.org/W3033411150', 'https://openalex.org/W4313679638', 'https://openalex.org/W2946200149', 'https://openalex.org/W4311000453']",2023-12-16
https://openalex.org/W4392909814,https://doi.org/10.1109/icassp48485.2024.10446271,FSD: An Initial Chinese Dataset for Fake Song Detection,"Singing voice synthesis and singing voice conversion have significantly advanced, revolutionizing musical experiences. However, the rise of ""Deepfake Songs"" generated by these technologies raises concerns about authenticity. Unlike Audio DeepFake Detection (ADD), the field of song deepfake detection lacks specialized datasets or methods for song authenticity verification. In this paper, we initially construct a Chinese Fake Song Detection (FSD) dataset to investigate the field of song deepfake detection. The fake songs in the FSD dataset are generated by five state-of-the-art singing voice synthesis and singing voice conversion methods. Our initial experiments on FSD revealed the ineffectiveness of existing speech-trained ADD models for the task of song deepfake detection. Thus, we employ the FSD dataset for the training of ADD models. We subsequently evaluate these models under two scenarios: one with the original songs and another with separated vocal tracks. Experiment results show that song-trained ADD models exhibit a 38.58% reduction in average equal error rate compared to speech-trained ADD models on the FSD test set.","['https://openalex.org/W3158762648', 'https://openalex.org/W6783382068', 'https://openalex.org/W3081279708', 'https://openalex.org/W4372338328', 'https://openalex.org/W3095948607', 'https://openalex.org/W3016007107', 'https://openalex.org/W4381198892', 'https://openalex.org/W4221138880', 'https://openalex.org/W3201773091', 'https://openalex.org/W6780218876', 'https://openalex.org/W4225854381', 'https://openalex.org/W4226264925', 'https://openalex.org/W4297841787', 'https://openalex.org/W6796464841', 'https://openalex.org/W3210530853', 'https://openalex.org/W6783867762', 'https://openalex.org/W2963175743', 'https://openalex.org/W6779577414', 'https://openalex.org/W6779823529', 'https://openalex.org/W6839738141', 'https://openalex.org/W4385822304', 'https://openalex.org/W4296068763', 'https://openalex.org/W2972811785', 'https://openalex.org/W2936802426', 'https://openalex.org/W3036167779', 'https://openalex.org/W2967606780', 'https://openalex.org/W3092028330', 'https://openalex.org/W3034302232', 'https://openalex.org/W3082910224', 'https://openalex.org/W3036601975']",2024-03-18
https://openalex.org/W4391305764,https://doi.org/10.1109/taslp.2024.3359352,Disentanglement in a GAN for Unconditional Speech Synthesis,"Can we develop a model that can synthesize realistic speech directly from a latent space, without explicit conditioning? Despite several efforts over the last decade, previous adversarial and diffusion-based approaches still struggle to achieve this, even on small-vocabulary datasets. To address this, we propose AudioStyleGAN (ASGAN) – a generative adversarial network for unconditional speech synthesis tailored to learn a disentangled latent space. Building upon the StyleGAN family of image synthesis models, ASGAN maps sampled noise to a disentangled latent vector which is then mapped to a sequence of audio features so that signal aliasing is suppressed at every layer. To successfully train ASGAN, we introduce a number of new techniques, including a modification to adaptive discriminator augmentation which probabilistically skips discriminator updates. We apply it on the small-vocabulary Google Speech Commands digits dataset, where it achieves state-of-the-art results in unconditional speech synthesis. It is also substantially faster than existing top-performing diffusion models. We confirm that ASGAN's latent space is disentangled: we demonstrate how simple linear operations in the space can be used to perform several tasks unseen during training. Specifically, we perform evaluations in voice conversion, speech enhancement, speaker verification, and keyword classification. Our work indicates that GANs are still highly competitive in the unconditional speech synthesis landscape, and that disentangled latent spaces can be used to aid generalization to unseen tasks. Code, models, samples: <uri xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">https://github.com/RF5/simple-asgan/</uri> .","['https://openalex.org/W6755257315', 'https://openalex.org/W3031894486', 'https://openalex.org/W3174807077', 'https://openalex.org/W4385535562', 'https://openalex.org/W6783182287', 'https://openalex.org/W6810325043', 'https://openalex.org/W6679045638', 'https://openalex.org/W6790978476', 'https://openalex.org/W6809885388', 'https://openalex.org/W6838639034', 'https://openalex.org/W2962770929', 'https://openalex.org/W6779093361', 'https://openalex.org/W6750665317', 'https://openalex.org/W4319862675', 'https://openalex.org/W6790356757', 'https://openalex.org/W3209059054', 'https://openalex.org/W3140429000', 'https://openalex.org/W3198217962', 'https://openalex.org/W6849109464', 'https://openalex.org/W4386071957', 'https://openalex.org/W3095488131', 'https://openalex.org/W3162850270', 'https://openalex.org/W4368304415', 'https://openalex.org/W4385571190', 'https://openalex.org/W6839243001', 'https://openalex.org/W6783867762', 'https://openalex.org/W6780179280', 'https://openalex.org/W6745560452', 'https://openalex.org/W2103869314', 'https://openalex.org/W1494198834', 'https://openalex.org/W2933138175', 'https://openalex.org/W6631190155', 'https://openalex.org/W6749927861', 'https://openalex.org/W3035187956', 'https://openalex.org/W2549139847', 'https://openalex.org/W6718379498', 'https://openalex.org/W2623550831', 'https://openalex.org/W6765779288', 'https://openalex.org/W6743994364', 'https://openalex.org/W4296068974', 'https://openalex.org/W3197580070', 'https://openalex.org/W3210530853', 'https://openalex.org/W1552314771', 'https://openalex.org/W2067295501', 'https://openalex.org/W6762533536', 'https://openalex.org/W4225309689', 'https://openalex.org/W3147966746', 'https://openalex.org/W2962788625', 'https://openalex.org/W3173102319', 'https://openalex.org/W3106255532', 'https://openalex.org/W4224035735', 'https://openalex.org/W2797583228', 'https://openalex.org/W2014555931', 'https://openalex.org/W2519091744']",2024-01-01
https://openalex.org/W4401652690,https://doi.org/10.3389/frsip.2024.1339159,Reimagining speech: a scoping review of deep learning-based methods for non-parallel voice conversion,"Research on deep learning-powered voice conversion (VC) in speech-to-speech scenarios are gaining increasing popularity. Although many of the works in the field of voice conversion share a common global pipeline, there is considerable diversity in the underlying structures, methods, and neural sub-blocks used across research efforts. Thus, obtaining a comprehensive understanding of the reasons behind the choice of the different methods included when training voice conversion models can be challenging, and the actual hurdles in the proposed solutions are often unclear. To shed light on these aspects, this paper presents a scoping review that explores the use of deep learning in speech analysis, synthesis, and disentangled speech representation learning within modern voice conversion systems. We screened 628 publications from more than 38 venues between 2017 and 2023, followed by an in-depth review of a final database of 130 eligible studies. Based on the review, we summarise the most frequently used approaches to voice conversion based on deep learning and highlight common pitfalls. We condense the knowledge gathered to identify main challenges, supply solutions grounded in the analysis and provide recommendations for future research directions.","['https://openalex.org/W2118850452', 'https://openalex.org/W3195390317', 'https://openalex.org/W2075950485', 'https://openalex.org/W3113738713', 'https://openalex.org/W6846042280', 'https://openalex.org/W6780218876', 'https://openalex.org/W6800638409', 'https://openalex.org/W4379522932', 'https://openalex.org/W3095930733', 'https://openalex.org/W3097692357', 'https://openalex.org/W6810709333', 'https://openalex.org/W2091425152', 'https://openalex.org/W3136699727', 'https://openalex.org/W6803547063', 'https://openalex.org/W4309395027', 'https://openalex.org/W4385823264', 'https://openalex.org/W6850489759', 'https://openalex.org/W2091538045', 'https://openalex.org/W6810298122', 'https://openalex.org/W3158484908', 'https://openalex.org/W6847224110', 'https://openalex.org/W6851260135', 'https://openalex.org/W4286747238', 'https://openalex.org/W6771763809', 'https://openalex.org/W3113603766', 'https://openalex.org/W6807756524', 'https://openalex.org/W2079283960', 'https://openalex.org/W6849542685', 'https://openalex.org/W3097777922', 'https://openalex.org/W3196643119', 'https://openalex.org/W4312994048', 'https://openalex.org/W3135780427', 'https://openalex.org/W3209059054', 'https://openalex.org/W4297841676', 'https://openalex.org/W6803398443', 'https://openalex.org/W3002433751', 'https://openalex.org/W6784059072', 'https://openalex.org/W2973135352', 'https://openalex.org/W6848319342', 'https://openalex.org/W6748409065', 'https://openalex.org/W2946555236', 'https://openalex.org/W6756504009', 'https://openalex.org/W6761382815', 'https://openalex.org/W2972667718', 'https://openalex.org/W3010732772', 'https://openalex.org/W2049686551', 'https://openalex.org/W6748253163', 'https://openalex.org/W6792630005', 'https://openalex.org/W3132220150', 'https://openalex.org/W6850317796', 'https://openalex.org/W6846967393', 'https://openalex.org/W4280552695', 'https://openalex.org/W6850217783', 'https://openalex.org/W6769593479', 'https://openalex.org/W3215590351', 'https://openalex.org/W2793667044', 'https://openalex.org/W6852418399', 'https://openalex.org/W3204457821', 'https://openalex.org/W2972399707', 'https://openalex.org/W2471520273', 'https://openalex.org/W2901669506', 'https://openalex.org/W3096697361', 'https://openalex.org/W6702590400', 'https://openalex.org/W6796756982', 'https://openalex.org/W4296069266', 'https://openalex.org/W4226251144', 'https://openalex.org/W6857604550', 'https://openalex.org/W4385823126', 'https://openalex.org/W4367291458', 'https://openalex.org/W6631362777', 'https://openalex.org/W4384500817', 'https://openalex.org/W6776440307', 'https://openalex.org/W6776390925', 'https://openalex.org/W6762533536', 'https://openalex.org/W6839738141', 'https://openalex.org/W2988491238', 'https://openalex.org/W3163498938', 'https://openalex.org/W2973049979', 'https://openalex.org/W6810375697', 'https://openalex.org/W3098557217', 'https://openalex.org/W2972853549', 'https://openalex.org/W2083806749', 'https://openalex.org/W2129082420', 'https://openalex.org/W6675215834', 'https://openalex.org/W2156142001', 'https://openalex.org/W4313455347', 'https://openalex.org/W2518172956', 'https://openalex.org/W2130942839', 'https://openalex.org/W4376622218', 'https://openalex.org/W6794884141', 'https://openalex.org/W6849388766', 'https://openalex.org/W4221141917', 'https://openalex.org/W2891378911', 'https://openalex.org/W6640989505', 'https://openalex.org/W2519091744', 'https://openalex.org/W3210530853', 'https://openalex.org/W4322629454', 'https://openalex.org/W3197659778', 'https://openalex.org/W3198082505', 'https://openalex.org/W6809846428', 'https://openalex.org/W4312732823', 'https://openalex.org/W6810428317', 'https://openalex.org/W4386132131', 'https://openalex.org/W6785711547', 'https://openalex.org/W3096524539', 'https://openalex.org/W6775421620', 'https://openalex.org/W3041738652', 'https://openalex.org/W6732251480', 'https://openalex.org/W6839145123', 'https://openalex.org/W4296068989', 'https://openalex.org/W4376619349', 'https://openalex.org/W4296068587', 'https://openalex.org/W6860313990', 'https://openalex.org/W4390912423', 'https://openalex.org/W6849266739', 'https://openalex.org/W6803539737', 'https://openalex.org/W2996414377', 'https://openalex.org/W4388348985', 'https://openalex.org/W3097952294', 'https://openalex.org/W2973142754', 'https://openalex.org/W6774197554', 'https://openalex.org/W6785164032', 'https://openalex.org/W6784688130', 'https://openalex.org/W3167106704', 'https://openalex.org/W3100378519', 'https://openalex.org/W4221146610', 'https://openalex.org/W4327767739', 'https://openalex.org/W3196700074', 'https://openalex.org/W4224928197', 'https://openalex.org/W2596397435', 'https://openalex.org/W4224920040', 'https://openalex.org/W3204602440', 'https://openalex.org/W3105549447', 'https://openalex.org/W3144417991', 'https://openalex.org/W4319862675', 'https://openalex.org/W2937579788', 'https://openalex.org/W3135654121', 'https://openalex.org/W4312613101', 'https://openalex.org/W3015805741', 'https://openalex.org/W4312096673', 'https://openalex.org/W3168719651', 'https://openalex.org/W4392910523', 'https://openalex.org/W3005784640', 'https://openalex.org/W4310895557', 'https://openalex.org/W2105698384', 'https://openalex.org/W3160950295', 'https://openalex.org/W4319309700', 'https://openalex.org/W4206934353', 'https://openalex.org/W4224085515', 'https://openalex.org/W4376624754', 'https://openalex.org/W4224916404', 'https://openalex.org/W4392909950', 'https://openalex.org/W3101689408', 'https://openalex.org/W3130168980', 'https://openalex.org/W4328027527', 'https://openalex.org/W3213785244', 'https://openalex.org/W2990138404', 'https://openalex.org/W2982223350', 'https://openalex.org/W3015434413', 'https://openalex.org/W3163573274', 'https://openalex.org/W4319586549', 'https://openalex.org/W1524333225', 'https://openalex.org/W3161695192', 'https://openalex.org/W2902070858', 'https://openalex.org/W4407601776', 'https://openalex.org/W4319862691', 'https://openalex.org/W2100649345', 'https://openalex.org/W4407774794', 'https://openalex.org/W4312806563', 'https://openalex.org/W4224916425', 'https://openalex.org/W1583837637', 'https://openalex.org/W4200027410', 'https://openalex.org/W4312097414', 'https://openalex.org/W3142644187']",2024-08-16
https://openalex.org/W4407388754,https://doi.org/10.3390/jcp5010006,Partial Fake Speech Attacks in the Real World Using Deepfake Audio,"Advances in deep learning have led to dramatic improvements in generative synthetic speech, eliminating robotic speech patterns to create speech that is indistinguishable from a human voice. Although these advances are extremely useful in various applications, they also facilitate powerful attacks against both humans and machines. Recently, a new type of speech attack called partial fake (PF) speech has emerged. This paper studies how well humans and machines, including speaker recognition systems and existing fake-speech detection tools, can distinguish between human voice and computer-generated speech. Our study shows that both humans and machines can be easily deceived by PF speech, and the current defences against PF speech are insufficient. These findings emphasise the urgency of increasing awareness for humans and creating new automated defences against PF speech for machines.","['https://openalex.org/W2125245899', 'https://openalex.org/W4200631896', 'https://openalex.org/W4385815354', 'https://openalex.org/W4385764360', 'https://openalex.org/W6779871621', 'https://openalex.org/W4372270423', 'https://openalex.org/W4377865285', 'https://openalex.org/W4372337821', 'https://openalex.org/W4382999039', 'https://openalex.org/W4389166059', 'https://openalex.org/W3201506562', 'https://openalex.org/W4388980081', 'https://openalex.org/W3146945401', 'https://openalex.org/W4379193822', 'https://openalex.org/W3187722398', 'https://openalex.org/W4388903318', 'https://openalex.org/W2168959749', 'https://openalex.org/W4390517533', 'https://openalex.org/W4385993905', 'https://openalex.org/W4391021739', 'https://openalex.org/W4385993937', 'https://openalex.org/W2165848216', 'https://openalex.org/W3154451338', 'https://openalex.org/W4377079685', 'https://openalex.org/W4225329057', 'https://openalex.org/W4386132131', 'https://openalex.org/W4225746985', 'https://openalex.org/W3210530853', 'https://openalex.org/W4372260053', 'https://openalex.org/W3083423753', 'https://openalex.org/W4221138880', 'https://openalex.org/W4226264925', 'https://openalex.org/W3196584150', 'https://openalex.org/W4319862431', 'https://openalex.org/W4385823472', 'https://openalex.org/W2808706139', 'https://openalex.org/W4402600203', 'https://openalex.org/W3032558098', 'https://openalex.org/W2998572311', 'https://openalex.org/W3208526032', 'https://openalex.org/W4386730427', 'https://openalex.org/W3183869905', 'https://openalex.org/W4313306150', 'https://openalex.org/W4225527248', 'https://openalex.org/W4385823361', 'https://openalex.org/W4388856757', 'https://openalex.org/W4400678935', 'https://openalex.org/W3165478005']",2025-02-08
https://openalex.org/W4391021772,https://doi.org/10.1109/asru57964.2023.10389649,VITS-Based Singing Voice Conversion System with DSPGAN Post-Processing for SVCC2023,"This paper presents the T02 team's system for the Singing Voice Conversion Challenge 2023 (SVCC2023). Our system entails a VITS-based SVC model, incorporating three modules: a feature extractor, a voice converter, and a postprocessor. Specifically, the feature extractor provides F0 contours and extracts speaker-independent linguistic content from the input singing voice by leveraging a HuBERT model. The voice converter is employed to recompose the speaker timbre, F0, and linguistic content to generate the waveform of the target speaker. Besides, to further improve the audio quality, a fine-tuned DSPGAN vocoder is introduced to resynthesise the waveform. Given the limited target speaker data, we utilize a two-stage training strategy to adapt the base model to the target speaker. During model adaptation, several tricks, such as data augmentation and joint training with auxiliary singer data, are involved. Official challenge results show that our system achieves superior performance, especially in the cross-domain task, ranking 1st and 2nd in naturalness and similarity, respectively. Further ablation justifies the effectiveness of our system design.","['https://openalex.org/W6854646373', 'https://openalex.org/W2963830550', 'https://openalex.org/W3097952294', 'https://openalex.org/W6810889627', 'https://openalex.org/W6781547035', 'https://openalex.org/W3012498027', 'https://openalex.org/W3025192522', 'https://openalex.org/W4247282941', 'https://openalex.org/W3197943112', 'https://openalex.org/W6786824797', 'https://openalex.org/W3209059054', 'https://openalex.org/W6769196770', 'https://openalex.org/W4372338328', 'https://openalex.org/W3210530853', 'https://openalex.org/W3170751106', 'https://openalex.org/W6839679458', 'https://openalex.org/W6796464841', 'https://openalex.org/W4372346850', 'https://openalex.org/W6839738141', 'https://openalex.org/W2118774185', 'https://openalex.org/W2990440871', 'https://openalex.org/W6783867762', 'https://openalex.org/W6772349387', 'https://openalex.org/W6917585676', 'https://openalex.org/W3198020407', 'https://openalex.org/W2067709094', 'https://openalex.org/W4296068763', 'https://openalex.org/W4285345683', 'https://openalex.org/W6803547063', 'https://openalex.org/W2963300588', 'https://openalex.org/W6739901393', 'https://openalex.org/W4226320669', 'https://openalex.org/W4391021724', 'https://openalex.org/W4224309976', 'https://openalex.org/W4385245566', 'https://openalex.org/W4301371414', 'https://openalex.org/W4245692952', 'https://openalex.org/W2913668833', 'https://openalex.org/W4283731195', 'https://openalex.org/W2998572311', 'https://openalex.org/W3092028330', 'https://openalex.org/W2979476256', 'https://openalex.org/W3110041771']",2023-12-16
https://openalex.org/W4386158910,https://doi.org/10.1109/icme55011.2023.00291,Adversarial Speaker Disentanglement Using Unannotated External Data for Self-supervised Representation-based Voice Conversion,"Nowadays, recognition-synthesis-based methods have been quite popular with voice conversion (VC). By introducing linguistics features with good disentangling characters extracted from an automatic speech recognition (ASR) model, the VC performance achieved considerable breakthroughs. Recently, self-supervised learning (SSL) methods trained with a large-scale unannotated speech corpus have been applied to downstream tasks focusing on the content information, which is suitable for VC tasks. However, a huge amount of speaker information in SSL representations degrades timbre similarity and the quality of converted speech significantly. To address this problem, we proposed a high-similarity any-to-one voice conversion method with the input of SSL representations. We incorporated adversarial training mechanisms in the synthesis module using external unannotated corpora. Two auxiliary discriminators were trained to distinguish whether a sequence of mel-spectrograms has been converted by the acoustic model and whether a sequence of content embeddings contains speaker information from external corpora. Experimental results show that our proposed method achieves comparable similarity and higher naturalness than the supervised method, which needs a huge amount of annotated corpora for training and is applicable to improve similarity for VC methods with other SSL representations as input.","['https://openalex.org/W6785090365', 'https://openalex.org/W6763832098', 'https://openalex.org/W3210530853', 'https://openalex.org/W2518172956', 'https://openalex.org/W3216296943', 'https://openalex.org/W6839738141', 'https://openalex.org/W6762533536', 'https://openalex.org/W4225322394', 'https://openalex.org/W6803547063', 'https://openalex.org/W6780218876', 'https://openalex.org/W6802329053', 'https://openalex.org/W3198815374', 'https://openalex.org/W3198082505', 'https://openalex.org/W6847037835', 'https://openalex.org/W3170751106', 'https://openalex.org/W6783867762', 'https://openalex.org/W1494198834', 'https://openalex.org/W2972659941', 'https://openalex.org/W3209059054', 'https://openalex.org/W4221162793', 'https://openalex.org/W3036601975', 'https://openalex.org/W2946200149', 'https://openalex.org/W3205154814', 'https://openalex.org/W4372340947', 'https://openalex.org/W2945478979', 'https://openalex.org/W4301371414', 'https://openalex.org/W3092028330', 'https://openalex.org/W3094002217', 'https://openalex.org/W4283659485']",2023-07-01
https://openalex.org/W4391021757,https://doi.org/10.1109/asru57964.2023.10389621,QUICKVC: A Lightweight VITS-Based Any-to-Many Voice Conversion Model using ISTFT for Faster Conversion,"With the development of automatic speech recognition and text-to-speech technology, high-quality voice conversion can be achieved by extracting source content information and target speaker information to reconstruct waveforms. However, current methods still require improvement in terms of inference speed. In this study, we propose a lightweight VITS-based voice conversion model that uses the HuBERT-Soft model to extract content information features. Unlike the original VITS model, we use the inverse short-time Fourier transform to replace the most computationally expensive part. Through subjective and objective experiments on synthesized speech, the proposed model is capable of natural speech generation and it is very efficient at inference time. Experimental results show that our model can generate samples at over 5000 KHz on the 3090 GPU and over 250 KHz on the i9-10900K CPU, achieving faster speed in comparison to baseline methods using the same hardware configuration.","['https://openalex.org/W2902070858', 'https://openalex.org/W6762533536', 'https://openalex.org/W3083423753', 'https://openalex.org/W3163568691', 'https://openalex.org/W2171019095', 'https://openalex.org/W3154451338', 'https://openalex.org/W2972667718', 'https://openalex.org/W3161695192', 'https://openalex.org/W3210530853', 'https://openalex.org/W2964243274', 'https://openalex.org/W6763832098', 'https://openalex.org/W3162427973', 'https://openalex.org/W6796464841', 'https://openalex.org/W4372260053', 'https://openalex.org/W6784299611', 'https://openalex.org/W4372270423', 'https://openalex.org/W6783867762', 'https://openalex.org/W4221152438', 'https://openalex.org/W3209059054', 'https://openalex.org/W2194775991', 'https://openalex.org/W2593414223', 'https://openalex.org/W6772349387', 'https://openalex.org/W1494198834', 'https://openalex.org/W6757817989', 'https://openalex.org/W6802527329', 'https://openalex.org/W3197659778', 'https://openalex.org/W2515028311', 'https://openalex.org/W4200225196', 'https://openalex.org/W1875842236', 'https://openalex.org/W2946200149', 'https://openalex.org/W3165478005', 'https://openalex.org/W3092028330', 'https://openalex.org/W2998572311']",2023-12-16
https://openalex.org/W4392931065,https://doi.org/10.1109/icassp48485.2024.10447283,Learning Disentangled Speech Representations with Contrastive Learning and Time-Invariant Retrieval,"Voice conversion refers to transferring speaker identity with well-preserved content. Better disentanglement of speech representations leads to better voice conversion. Recent studies have found that phonetic information from input audio has the potential ability to well represent content. Besides, the speaker-style modeling with pre-trained models making the process more complex. To tackle these issues, we introduce an new method named ""CTVC"" which utilizes disen-tangled speech representations with contrastive learning and time-invariant retrieval.Specifically, a similarity-based compression module is used to facilitate a more intimate connection between the frame-level hidden features and linguistic information at phoneme-level. Additionally, a time-invariant retrieval is proposed for timbre extraction based on multiple segmentation and mutual information. Experimental results demonstrate that ""CTVC"" outperforms previous studies and improves the sound quality and similarity of converted results.","['https://openalex.org/W4287889585', 'https://openalex.org/W4225939199', 'https://openalex.org/W4386113956', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W3210530853', 'https://openalex.org/W4372260053', 'https://openalex.org/W2962788625', 'https://openalex.org/W2890964092', 'https://openalex.org/W6762533536', 'https://openalex.org/W4221141917', 'https://openalex.org/W4210282084', 'https://openalex.org/W2973154337', 'https://openalex.org/W4225322394', 'https://openalex.org/W6777179611', 'https://openalex.org/W2747874407', 'https://openalex.org/W6790622591', 'https://openalex.org/W2842511635', 'https://openalex.org/W3096524539', 'https://openalex.org/W3015805741', 'https://openalex.org/W4375869089', 'https://openalex.org/W3198533616', 'https://openalex.org/W6783867762', 'https://openalex.org/W2945478979', 'https://openalex.org/W3026092005', 'https://openalex.org/W3126283728', 'https://openalex.org/W4297808394', 'https://openalex.org/W3092028330']",2024-03-18
https://openalex.org/W4298397149,https://doi.org/10.1145/3552466.3556524,Acoustic or Pattern? Speech Spoofing Countermeasure based on Image Pre-training Models,"Traditional speech spoofing countermeasures (CM) typically contain a frontend which extract a two dimensional feature from the waveform, and a Convolutional Neural Network (CNN) based backend classifier. This pipeline is similar to an image classification task, in some degree. Pre-training is a widely used paradigm in many fields. Self-supervised pre-trained frontend such as Wav2Vec 2.0 has shown superior improvement in the speech spoofing detection task. However, these pre-trained models are only trained by bonafide utterances. Moreover, acoustic pre-trained frontends can also be used in the text-to-speech (TTS) and voice conversion (VC) task, which reveals that commonalities of speech are learnt by them, rather than discriminative information between real and fake data. Speech spoofing detection task and image classification task share the same pipeline. Based on the hypothesis that CNNs follow the same pattern in capturing artefacts in these two tasks, we apply image pre-trained CNN model to detect spoofed utterances, counterintuitively. To supplement the model with potentially missing acoustic features, we concatenate Jitter and Shimmer features to the output embedding. Our proposed CM achieve top-level performance on the ASVspoof 2019 dataset.","['https://openalex.org/W3213029956', 'https://openalex.org/W3036601975', 'https://openalex.org/W3205631867', 'https://openalex.org/W3010845645', 'https://openalex.org/W3199956586', 'https://openalex.org/W3015420010', 'https://openalex.org/W2108598243', 'https://openalex.org/W2745744274', 'https://openalex.org/W3146945401', 'https://openalex.org/W3203575608', 'https://openalex.org/W3209059054', 'https://openalex.org/W2752782242', 'https://openalex.org/W3170179936', 'https://openalex.org/W2976059681', 'https://openalex.org/W3041816239', 'https://openalex.org/W2128466129', 'https://openalex.org/W3006808893', 'https://openalex.org/W2963466847', 'https://openalex.org/W4283077666', 'https://openalex.org/W4226264925', 'https://openalex.org/W15681643', 'https://openalex.org/W3211424380', 'https://openalex.org/W3163596559', 'https://openalex.org/W4225527248', 'https://openalex.org/W2528333531', 'https://openalex.org/W1985198345', 'https://openalex.org/W2590129515', 'https://openalex.org/W3199161700', 'https://openalex.org/W3210530853', 'https://openalex.org/W3200493434', 'https://openalex.org/W3206995232', 'https://openalex.org/W3212117663', 'https://openalex.org/W4224920249', 'https://openalex.org/W3016138785', 'https://openalex.org/W2123299109', 'https://openalex.org/W4221138880', 'https://openalex.org/W3127781933', 'https://openalex.org/W3198486673', 'https://openalex.org/W2295107390', 'https://openalex.org/W3158663310', 'https://openalex.org/W3199131409', 'https://openalex.org/W3100448777']",2022-10-01
https://openalex.org/W4319862700,https://doi.org/10.1109/slt54892.2023.10023345,Extracting Speaker and Emotion Information from Self-Supervised Speech Models via Channel-Wise Correlations,"Self-supervised learning of speech representations from large amounts of unlabeled data has enabled state-of-the-art results in several speech processing tasks. Aggregating these speech representations across time is typically approached by using descriptive statistics, and in particular, using the first - and second-order statistics of representation coefficients. In this paper, we examine an alternative way of extracting speaker and emotion information from self-supervised trained models, based on the correlations between the coefficients of the representations - correlation pooling. We show improvements over mean pooling and further gains when the pooling methods are combined via fusion. The code is available at github.com/Lamomal/s3prl_correlation.","['https://openalex.org/W3209059054', 'https://openalex.org/W3198275944', 'https://openalex.org/W3209984917', 'https://openalex.org/W3197580070', 'https://openalex.org/W2890964092', 'https://openalex.org/W3135006803', 'https://openalex.org/W3197392277', 'https://openalex.org/W2475287302', 'https://openalex.org/W2963804033', 'https://openalex.org/W4385245566', 'https://openalex.org/W2962739339', 'https://openalex.org/W6810126160', 'https://openalex.org/W3210530853', 'https://openalex.org/W2726515241', 'https://openalex.org/W6800175519', 'https://openalex.org/W2982474723', 'https://openalex.org/W4283078310', 'https://openalex.org/W3202370288', 'https://openalex.org/W2808631503', 'https://openalex.org/W2784163702', 'https://openalex.org/W6771667380', 'https://openalex.org/W3197642003', 'https://openalex.org/W2146334809', 'https://openalex.org/W2602034649', 'https://openalex.org/W3178795096', 'https://openalex.org/W3103152812', 'https://openalex.org/W4221161148', 'https://openalex.org/W2810637193', 'https://openalex.org/W3194763370', 'https://openalex.org/W4236965008']",2023-01-09
https://openalex.org/W4383878503,https://doi.org/10.3390/app13148049,Enhancing Voice Cloning Quality through Data Selection and Alignment-Based Metrics,"Voice cloning, an emerging field in the speech-processing area, aims to generate synthetic utterances that closely resemble the voices of specific individuals. In this study, we investigated the impact of various techniques on improving the quality of voice cloning, specifically focusing on a low-quality dataset. To contrast our findings, we also used two high-quality corpora for comparative analysis. We conducted exhaustive evaluations of the quality of the gathered corpora in order to select the most-suitable data for the training of a voice-cloning system. Following these measurements, we conducted a series of ablations by removing audio files with a lower signal-to-noise ratio and higher variability in utterance speed from the corpora in order to decrease their heterogeneity. Furthermore, we introduced a novel algorithm that calculates the fraction of aligned input characters by exploiting the attention matrix of the Tacotron 2 text-to-speech system. This algorithm provides a valuable metric for evaluating the alignment quality during the voice-cloning process. We present the results of our experiments, demonstrating that the performed ablations significantly increased the quality of synthesised audio for the challenging low-quality corpus. Notably, our findings indicated that models trained on a 3 h corpus from a pre-trained model exhibit comparable audio quality to models trained from scratch using significantly larger amounts of data.","['https://openalex.org/W4223889713', 'https://openalex.org/W4308599798', 'https://openalex.org/W2972394484', 'https://openalex.org/W3202278141', 'https://openalex.org/W3196225973', 'https://openalex.org/W2964243274', 'https://openalex.org/W6783867762', 'https://openalex.org/W3196475561', 'https://openalex.org/W4214874905', 'https://openalex.org/W4200295417', 'https://openalex.org/W3161018648', 'https://openalex.org/W2972659941', 'https://openalex.org/W2945478979', 'https://openalex.org/W3169739675', 'https://openalex.org/W2902070858', 'https://openalex.org/W3025680351', 'https://openalex.org/W3142644187', 'https://openalex.org/W3210530853', 'https://openalex.org/W3209059054', 'https://openalex.org/W3026874504', 'https://openalex.org/W3196584150', 'https://openalex.org/W2970006822', 'https://openalex.org/W4382202681', 'https://openalex.org/W3198020407', 'https://openalex.org/W2972359262', 'https://openalex.org/W2913714131', 'https://openalex.org/W2955448105', 'https://openalex.org/W3031995935', 'https://openalex.org/W3208642953', 'https://openalex.org/W3173715137', 'https://openalex.org/W1494198834', 'https://openalex.org/W3197824033', 'https://openalex.org/W3145029257', 'https://openalex.org/W3097206152', 'https://openalex.org/W4296068974', 'https://openalex.org/W6631362777', 'https://openalex.org/W6917585676', 'https://openalex.org/W6631190155', 'https://openalex.org/W2963103134', 'https://openalex.org/W3092028330']",2023-07-10
https://openalex.org/W4385801150,https://doi.org/10.1109/cvprw59228.2023.00613,Multi-modal Emotion Reaction Intensity Estimation with Temporal Augmentation,"Emotion reaction intensity (ERI) estimation aims to estimate the emotion intensities of subjects reacting to various video-based stimuli. It plays an important role in human affective behavior analysis. In this paper, we proposed a effective solution for addressing the task of ERI estimation in the fifth Affective Behavior Analysis in the wild (ABAW) competition. Based on multi-modal information, We first extract uni-modal features from images, speeches and texts, respectively and then regress the intensities of 7 emotions. To enhance the model generalization and capture context information, we employ the Temporal Augmentation module to adapt to various video samples and the Temporal SE Block to reweight temporal features adaptively. The extensive experiments conducted on large-scale dataset, Hume-Reaction, demonstrate the effectiveness of our approach. Our method achieves average pearson's correlations coefficient of 0.4160 on the validation set and obtain third place in the ERI Estimation Challenge of ABAW 2023.","['https://openalex.org/W6635087628', 'https://openalex.org/W3162474807', 'https://openalex.org/W2809229100', 'https://openalex.org/W2239141610', 'https://openalex.org/W6784333009', 'https://openalex.org/W6755207826', 'https://openalex.org/W6779068807', 'https://openalex.org/W4313156423', 'https://openalex.org/W2752782242', 'https://openalex.org/W2713788831', 'https://openalex.org/W4385804798', 'https://openalex.org/W6784050962', 'https://openalex.org/W3210530853', 'https://openalex.org/W6869477534', 'https://openalex.org/W2963315828', 'https://openalex.org/W2133669904', 'https://openalex.org/W2510725918', 'https://openalex.org/W2143570397', 'https://openalex.org/W2964350391', 'https://openalex.org/W6630649318', 'https://openalex.org/W4297510847', 'https://openalex.org/W2963839617', 'https://openalex.org/W6640212811', 'https://openalex.org/W6780218876', 'https://openalex.org/W6780226713', 'https://openalex.org/W2807126412', 'https://openalex.org/W6749825310', 'https://openalex.org/W6727853262', 'https://openalex.org/W6757817989', 'https://openalex.org/W6726497184', 'https://openalex.org/W2745497104', 'https://openalex.org/W6846300701', 'https://openalex.org/W4385805042', 'https://openalex.org/W3209397829', 'https://openalex.org/W1834627138', 'https://openalex.org/W4297510548', 'https://openalex.org/W2746419079', 'https://openalex.org/W4304092044', 'https://openalex.org/W2250539671', 'https://openalex.org/W6794746887', 'https://openalex.org/W6770092901', 'https://openalex.org/W4385815442', 'https://openalex.org/W4292794012', 'https://openalex.org/W3126750668', 'https://openalex.org/W6850358395', 'https://openalex.org/W6769650007', 'https://openalex.org/W2798536775', 'https://openalex.org/W6793244244', 'https://openalex.org/W2908510526', 'https://openalex.org/W3100513545', 'https://openalex.org/W3036601975', 'https://openalex.org/W3092424727', 'https://openalex.org/W3122081138', 'https://openalex.org/W4288102735', 'https://openalex.org/W3161318013', 'https://openalex.org/W4323076384', 'https://openalex.org/W3148234074', 'https://openalex.org/W1924770834', 'https://openalex.org/W3122890974', 'https://openalex.org/W1509966554', 'https://openalex.org/W4307783939', 'https://openalex.org/W2896457183', 'https://openalex.org/W1588539311', 'https://openalex.org/W4399568832', 'https://openalex.org/W2524797241', 'https://openalex.org/W2792764867', 'https://openalex.org/W2963263347', 'https://openalex.org/W3094502228', 'https://openalex.org/W2981958954', 'https://openalex.org/W2519091744']",2023-06-01
https://openalex.org/W4389722535,https://doi.org/10.1109/ojsp.2023.3343342,Streaming ASR Encoder for Whisper-to-Speech Online Voice Conversion,"Whispered speech is a quiet voice without vocalization. One of the common cases of using whispered speech is a technique that can help overcome stuttering. But whispered speech can be uncomfortable and difficult to understand in everyday communication. To address these problems, we propose a method of low-delayed whisper-to-speech voice conversion, which can be useful in real life communication of people with disordered speech. As part of our research, we study the impact of streaming Automatic Speech Recognition models on the quality of voice conversion, comparing different streaming models and methods for model adaptation to streaming settings, and showing the importance of using such models in cases of low-delayed voice conversion.","['https://openalex.org/W3176579494', 'https://openalex.org/W6845912186', 'https://openalex.org/W1997168595', 'https://openalex.org/W2768731046', 'https://openalex.org/W3146125295', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W1540787848', 'https://openalex.org/W2972471621', 'https://openalex.org/W2889417860', 'https://openalex.org/W4318752101', 'https://openalex.org/W6767111847', 'https://openalex.org/W2752796333', 'https://openalex.org/W6735204497', 'https://openalex.org/W2987496713', 'https://openalex.org/W6781924407', 'https://openalex.org/W4323322934', 'https://openalex.org/W6778823374', 'https://openalex.org/W6783867762', 'https://openalex.org/W3197642003', 'https://openalex.org/W3202370288', 'https://openalex.org/W3210530853', 'https://openalex.org/W3015194534', 'https://openalex.org/W3016010032', 'https://openalex.org/W6787040858', 'https://openalex.org/W3096888553', 'https://openalex.org/W3094543484', 'https://openalex.org/W1494198834', 'https://openalex.org/W6771467084', 'https://openalex.org/W3163159230', 'https://openalex.org/W6799174933', 'https://openalex.org/W6796464841', 'https://openalex.org/W2972359262', 'https://openalex.org/W6772349387', 'https://openalex.org/W3198020407', 'https://openalex.org/W6917585676', 'https://openalex.org/W3083423753', 'https://openalex.org/W6849755210', 'https://openalex.org/W2964243274', 'https://openalex.org/W3024869864', 'https://openalex.org/W6847363464', 'https://openalex.org/W3025165719', 'https://openalex.org/W4385822293', 'https://openalex.org/W3111562797', 'https://openalex.org/W4321277041', 'https://openalex.org/W3162665866', 'https://openalex.org/W4287121924', 'https://openalex.org/W3097777922', 'https://openalex.org/W2970006822', 'https://openalex.org/W3030437843', 'https://openalex.org/W2998572311', 'https://openalex.org/W4309397917', 'https://openalex.org/W4311000453', 'https://openalex.org/W2951939904', 'https://openalex.org/W3169320628', 'https://openalex.org/W3092028330', 'https://openalex.org/W2963799213', 'https://openalex.org/W4287687956', 'https://openalex.org/W3036601975', 'https://openalex.org/W3033411150', 'https://openalex.org/W4307106469']",2023-12-14
https://openalex.org/W4391021548,https://doi.org/10.1109/asru57964.2023.10389779,A Comparative Study of Voice Conversion Models With Large-Scale Speech and Singing Data: The T13 Systems for the Singing Voice Conversion Challenge 2023,"This paper presents our systems (denoted as T13) for the singing voice conversion challenge (SVCC) 2023. For both in-domain and cross-domain English singing voice conversion (SVC) tasks (Task 1 and Task 2), we adopt a recognition-synthesis approach with self-supervised learning-based representation. To achieve data-efficient SVC with a limited amount of target singer/speaker's data (150 to 160 utterances for SVCC 2023), we first train a diffusion-based any-to-any voice conversion model using publicly available large-scale 750 hours of speech and singing data. Then, we fine-tune the model for each target singer/speaker of Task 1 and Task 2. Large-scale listening tests conducted by SVCC 2023 show that our T13 system achieves competitive naturalness and speaker similarity for the harder cross-domain SVC (Task 2), which implies the generalization ability of our proposed method. Our objective evaluation results show that using large datasets is particularly beneficial for cross-domain SVC.","['https://openalex.org/W2921857201', 'https://openalex.org/W3095948607', 'https://openalex.org/W3170751106', 'https://openalex.org/W3205154814', 'https://openalex.org/W4372338328', 'https://openalex.org/W2963035245', 'https://openalex.org/W6840412704', 'https://openalex.org/W4296068763', 'https://openalex.org/W3159302906', 'https://openalex.org/W6772925044', 'https://openalex.org/W2067709094', 'https://openalex.org/W6779090866', 'https://openalex.org/W3181854487', 'https://openalex.org/W4285345683', 'https://openalex.org/W1494198834', 'https://openalex.org/W6854646373', 'https://openalex.org/W6839738141', 'https://openalex.org/W3210530853', 'https://openalex.org/W4287889585', 'https://openalex.org/W6779823529', 'https://openalex.org/W4226320669', 'https://openalex.org/W6803547063', 'https://openalex.org/W4372262501', 'https://openalex.org/W3209059054', 'https://openalex.org/W2747070883', 'https://openalex.org/W2519648275', 'https://openalex.org/W6749555683', 'https://openalex.org/W6750489868', 'https://openalex.org/W6790220310', 'https://openalex.org/W6840815571', 'https://openalex.org/W6849956205', 'https://openalex.org/W4297841714', 'https://openalex.org/W6846936884', 'https://openalex.org/W6783867762', 'https://openalex.org/W6936113694', 'https://openalex.org/W2972359262', 'https://openalex.org/W3158762648', 'https://openalex.org/W6745289305', 'https://openalex.org/W6757817989', 'https://openalex.org/W6838843145', 'https://openalex.org/W6631190155', 'https://openalex.org/W4225956675', 'https://openalex.org/W3209984917', 'https://openalex.org/W6847363464', 'https://openalex.org/W2765486990', 'https://openalex.org/W3041870963', 'https://openalex.org/W4382322852', 'https://openalex.org/W4320459320']",2023-12-16
https://openalex.org/W4400621997,https://doi.org/10.21437/odyssey.2024-25,Converting Anyone's Voice: End-to-End Expressive Voice Conversion with A Conditional Diffusion Model,"Expressive voice conversion (VC) conducts speaker identity conversion for emotional speakers by jointly converting speaker identity and emotional style.Emotional style modeling for arbitrary speakers in expressive VC has not been extensively explored.Previous approaches have relied on vocoders for speech reconstruction, which makes speech quality heavily dependent on the performance of vocoders.A major challenge of expressive VC lies in emotion prosody modeling.To address these challenges, this paper proposes a fully end-to-end expressive VC framework based on a conditional denoising diffusion probabilistic model (DDPM).We utilize speech units derived from self-supervised speech models as content conditioning, along with deep features extracted from speech emotion recognition and speaker verification systems to model emotional style and speaker identity.Objective and subjective evaluations show the effectiveness of our framework.Codes and samples are publicly available.","['https://openalex.org/W2972659941', 'https://openalex.org/W4307741680', 'https://openalex.org/W2120605154', 'https://openalex.org/W2107860279', 'https://openalex.org/W4205742757', 'https://openalex.org/W2187089797', 'https://openalex.org/W4297841465', 'https://openalex.org/W2973049979', 'https://openalex.org/W3036167779', 'https://openalex.org/W4281492411', 'https://openalex.org/W3210530853', 'https://openalex.org/W2107740512', 'https://openalex.org/W3098557217', 'https://openalex.org/W3140429000', 'https://openalex.org/W3197771950', 'https://openalex.org/W4385822362', 'https://openalex.org/W2885005742', 'https://openalex.org/W3168719651', 'https://openalex.org/W4372260434', 'https://openalex.org/W4385823368', 'https://openalex.org/W4283780157', 'https://openalex.org/W3096939667', 'https://openalex.org/W4286747238', 'https://openalex.org/W2659927845', 'https://openalex.org/W3036601975', 'https://openalex.org/W4303519914', 'https://openalex.org/W3014201970', 'https://openalex.org/W3163475957', 'https://openalex.org/W3129651364', 'https://openalex.org/W4226225516', 'https://openalex.org/W4287116649', 'https://openalex.org/W3163573274', 'https://openalex.org/W1494198834', 'https://openalex.org/W4226474318', 'https://openalex.org/W3162879338', 'https://openalex.org/W4221147462', 'https://openalex.org/W4226320669', 'https://openalex.org/W2972374322', 'https://openalex.org/W4287239560', 'https://openalex.org/W2945478979', 'https://openalex.org/W2979476256', 'https://openalex.org/W2146334809', 'https://openalex.org/W3169320628', 'https://openalex.org/W4313316128', 'https://openalex.org/W4285111045', 'https://openalex.org/W4287690582', 'https://openalex.org/W3204602440', 'https://openalex.org/W2962788625', 'https://openalex.org/W4213469706', 'https://openalex.org/W2130086727', 'https://openalex.org/W2077801020', 'https://openalex.org/W2888922217', 'https://openalex.org/W2919171658']",2024-06-18
https://openalex.org/W4393028308,https://doi.org/10.1109/isriti60336.2023.10467285,SRAG: Speech Retrieval Augmented Generation for Spoken Language Understanding,"Retrieval augmented generation (RAG) has shown promise for enhancing natural language understanding (NLU) capabilities of large language models (LLMs) by retrieving relevant knowledge as prompts. Extending RAG to spoken language understanding (SLU) represents an important research direction. This paper proposes a RAG approach for improving SLU. First, the encoder of a pretrained automatic speech recognition model is utilized for speech retrieval over the training set. The corresponding texts and intent labels are then formulated as prompts to guide the SLU decoder. Furthermore, a prompt attention mechanism is introduced to strengthen the attention between generation and prompts. Experiments demonstrate that the proposed speech RAG approach substantially outperforms conventional end-to-end and cascaded SLU models in intent prediction from speech. This highlights the efficacy of leveraging retrieval-based prompting to incorporate external knowledge for advancing SLU.","['https://openalex.org/W4322743583', 'https://openalex.org/W3204917342', 'https://openalex.org/W6777615688', 'https://openalex.org/W4285247752', 'https://openalex.org/W4312651322', 'https://openalex.org/W3100460087', 'https://openalex.org/W2971167298', 'https://openalex.org/W4361199948', 'https://openalex.org/W4385571567', 'https://openalex.org/W4385807419', 'https://openalex.org/W3161223924', 'https://openalex.org/W4372347505', 'https://openalex.org/W4385571440', 'https://openalex.org/W4385245566', 'https://openalex.org/W6839483240', 'https://openalex.org/W3210530853', 'https://openalex.org/W4385822890', 'https://openalex.org/W4285819432', 'https://openalex.org/W4288391450', 'https://openalex.org/W4322825254', 'https://openalex.org/W4311000453', 'https://openalex.org/W3027879771', 'https://openalex.org/W4393141148']",2023-12-11
https://openalex.org/W4405490053,https://doi.org/10.1109/embc53108.2024.10781707,Cross-Speaker Training and Adaptation for Electromyography-to-Speech Conversion,"Surface Electromyography (EMG) signals of articulatory muscles can be used to synthesize acoustic speech with Electromyography-to-Speech (ETS) models. Recent models have improved the synthesis quality by combining training data from multiple recordings of single speakers. In this work, we evaluated whether using recordings of multiple speakers also increases performance and if cross-speaker models can be adapted to unseen speakers with limited data. We recorded the EMG-Vox corpus, which consists of EMG and audio signals of four speakers with five sessions each. We compared cross-speaker models with single-speaker counterparts and conducted adaptation experiments. Cross-speaker models achieved on average significantly better performance than single-speaker models. Experiments with balanced data indicated that this improvement stemmed from a larger training set. Performing speaker adaptation from cross-speaker models showed higher synthesis quality than training from scratch and was at least on par with session adaptation for most speakers. To the best of our knowledge, this is the first work to report that cross-speaker ETS models yielded better results than single-speaker models.","['https://openalex.org/W2768153200', 'https://openalex.org/W2008120082', 'https://openalex.org/W169623250', 'https://openalex.org/W2537327199', 'https://openalex.org/W2419247625', 'https://openalex.org/W3105229878', 'https://openalex.org/W3005193431', 'https://openalex.org/W1996685892', 'https://openalex.org/W2395817516', 'https://openalex.org/W3097783286', 'https://openalex.org/W6775593909', 'https://openalex.org/W4283383330', 'https://openalex.org/W4372341338', 'https://openalex.org/W4291824895', 'https://openalex.org/W6857879404', 'https://openalex.org/W6894213946', 'https://openalex.org/W6803516594', 'https://openalex.org/W2747874407', 'https://openalex.org/W3175752069', 'https://openalex.org/W6783867762', 'https://openalex.org/W4372264243', 'https://openalex.org/W3210530853', 'https://openalex.org/W3024869864', 'https://openalex.org/W6772349387', 'https://openalex.org/W4388275972', 'https://openalex.org/W3211329195', 'https://openalex.org/W2990138404', 'https://openalex.org/W2998572311']",2024-07-15
https://openalex.org/W4386536200,https://doi.org/10.1109/taslp.2023.3313414,MSM-VC: High-Fidelity Source Style Transfer for Non-Parallel Voice Conversion by Multi-Scale Style Modeling,"In addition to conveying the linguistic content from source speech to converted speech, maintaining the speaking style of source speech also plays an important role in the voice conversion (VC) task, which is essential in many scenarios with highly expressive source speech, such as dubbing and data augmentation. Previous work generally took explicit prosodic features or fixed-length style embedding extracted from source speech to model the speaking style of source speech, which is insufficient to achieve comprehensive style modeling and target speaker timbre preservation. Inspired by the style's multi-scale nature of human speech, a multi-scale style modeling method for the VC task, referred to as MSM-VC, is proposed in this paper. MSM-VC models the speaking style of source speech from different levels, i.e., global, local, and frame levels. To effectively convey the speaking style and meanwhile prevent timbre leakage from source speech to converted speech, each level's style is modeled by specific representation. Specifically, prosodic features, pre-trained ASR model's bottleneck features, and features extracted by a model trained with a self-supervised strategy are adopted to model the frame, local, and global-level styles, respectively. Besides, to balance the performance of source style modeling and target speaker timbre preservation, an explicit constraint module consisting of a pre-trained speech emotion recognition model and a speaker classifier is introduced to MSM-VC. This explicit constraint module also makes it possible to simulate the style transfer inference process during the training to improve the disentanglement ability and alleviate the mismatch between training and inference. Experiments performed on the highly expressive speech corpus demonstrate that MSM-VC is superior to the state-of-the-art VC methods for modeling source speech style while maintaining good speech quality and speaker similarity. Furthermore, ablation analysis indicates the indispensable of every style level's modeling and the effectiveness of each module.","['https://openalex.org/W2120605154', 'https://openalex.org/W2156477760', 'https://openalex.org/W1977362459', 'https://openalex.org/W6762533536', 'https://openalex.org/W3198082505', 'https://openalex.org/W2532494225', 'https://openalex.org/W2972659941', 'https://openalex.org/W2964135678', 'https://openalex.org/W2963539064', 'https://openalex.org/W2518172956', 'https://openalex.org/W3216296943', 'https://openalex.org/W4390912371', 'https://openalex.org/W3094635600', 'https://openalex.org/W3197943112', 'https://openalex.org/W4210774711', 'https://openalex.org/W4286747238', 'https://openalex.org/W4226474318', 'https://openalex.org/W6749555683', 'https://openalex.org/W6750489868', 'https://openalex.org/W2904459034', 'https://openalex.org/W4244543785', 'https://openalex.org/W6769299913', 'https://openalex.org/W6810681921', 'https://openalex.org/W6776390925', 'https://openalex.org/W2141340689', 'https://openalex.org/W3146550708', 'https://openalex.org/W3197704090', 'https://openalex.org/W4319586674', 'https://openalex.org/W6790220310', 'https://openalex.org/W3205154814', 'https://openalex.org/W3007067948', 'https://openalex.org/W6769196770', 'https://openalex.org/W3168542456', 'https://openalex.org/W2902070858', 'https://openalex.org/W6746238782', 'https://openalex.org/W2759925408', 'https://openalex.org/W4226421465', 'https://openalex.org/W3097892637', 'https://openalex.org/W2885800352', 'https://openalex.org/W3160329778', 'https://openalex.org/W3010916717', 'https://openalex.org/W6755300632', 'https://openalex.org/W3207354624', 'https://openalex.org/W3194143312', 'https://openalex.org/W6778823374', 'https://openalex.org/W2973158936', 'https://openalex.org/W6785629321', 'https://openalex.org/W3152136404', 'https://openalex.org/W3158374895', 'https://openalex.org/W2896457183', 'https://openalex.org/W6795952400', 'https://openalex.org/W3198104520', 'https://openalex.org/W3163568691', 'https://openalex.org/W3197659778', 'https://openalex.org/W4294311176', 'https://openalex.org/W3097777922', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963091184', 'https://openalex.org/W3161695192', 'https://openalex.org/W3210530853', 'https://openalex.org/W2091318760', 'https://openalex.org/W3163573274', 'https://openalex.org/W3016159759', 'https://openalex.org/W3194347141', 'https://openalex.org/W3203407300', 'https://openalex.org/W1494198834', 'https://openalex.org/W6631362777', 'https://openalex.org/W3024869864', 'https://openalex.org/W3015598461', 'https://openalex.org/W6936113694', 'https://openalex.org/W4226225516']",2023-01-01
https://openalex.org/W4386712580,https://doi.org/10.21437/spsc.2023-3,Vocoder drift compensation by x-vector alignment in speaker anonymisation,"For the most popular x-vector-based approaches to speaker anonymisation, the bulk of the anonymisation can stem from vocoding rather than from the core anonymisation function which is used to substitute an original speaker x-vector with that of a fictitious pseudo-speaker.This phenomenon can impede the design of better anonymisation systems since there is a lack of fine-grained control over the x-vector space.The work reported in this paper explores the origin of so-called vocoder drift and shows that it is due to the mismatch between the substituted x-vector and the original representations of the linguistic content, intonation and prosody.Also reported is an original approach to vocoder drift compensation.While anonymisation performance degrades as expected, compensation reduces vocoder drift substantially, offers improved control over the x-vector space and lays a foundation for the design of better anonymisation functions in the future.","['https://openalex.org/W6776763299', 'https://openalex.org/W6810404315', 'https://openalex.org/W2947731696', 'https://openalex.org/W4297904260', 'https://openalex.org/W4307934091', 'https://openalex.org/W4221152846', 'https://openalex.org/W3216758611', 'https://openalex.org/W4306311719', 'https://openalex.org/W4379540158', 'https://openalex.org/W2115098197', 'https://openalex.org/W3210530853', 'https://openalex.org/W3024869864', 'https://openalex.org/W3092028330', 'https://openalex.org/W6761768988', 'https://openalex.org/W1494198834', 'https://openalex.org/W2998572311', 'https://openalex.org/W4292597003', 'https://openalex.org/W4226120298', 'https://openalex.org/W1522301498', 'https://openalex.org/W4210970267', 'https://openalex.org/W4281250109', 'https://openalex.org/W4319862433', 'https://openalex.org/W2187089797', 'https://openalex.org/W4385822420', 'https://openalex.org/W2972359262', 'https://openalex.org/W4287632938', 'https://openalex.org/W4284706634', 'https://openalex.org/W4293112954', 'https://openalex.org/W2972848589']",2023-08-19
https://openalex.org/W4386609311,https://doi.org/10.1109/lsp.2023.3313515,Rhythm Modeling for Voice Conversion,"Voice conversion aims to transform source speech into a different target voice. However, typical voice conversion systems do not account for rhythm, which is an important factor in the perception of speaker identity. To bridge this gap, we introduce Urhythmic—an unsupervised method for rhythm conversion that does not require parallel data or text transcriptions. Using self-supervised representations, we first divide source audio into segments approximating sonorants, obstruents, and silences. Then we model rhythm by estimating speaking rate or the duration distribution of each segment type. Finally, we match the target speaking rate or rhythm by time-stretching the speech segments. Experiments show that Urhythmic outperforms existing unsupervised methods in terms of quality and prosody.","['https://openalex.org/W2149569147', 'https://openalex.org/W2083679487', 'https://openalex.org/W1975216662', 'https://openalex.org/W2172080330', 'https://openalex.org/W2148933165', 'https://openalex.org/W4385823240', 'https://openalex.org/W3134921434', 'https://openalex.org/W2897353073', 'https://openalex.org/W2899877258', 'https://openalex.org/W2785978752', 'https://openalex.org/W4225892118', 'https://openalex.org/W4385822966', 'https://openalex.org/W3083423753', 'https://openalex.org/W6796575454', 'https://openalex.org/W4297841435', 'https://openalex.org/W4281492411', 'https://openalex.org/W3210530853', 'https://openalex.org/W2752796333', 'https://openalex.org/W3095361818', 'https://openalex.org/W3140429000', 'https://openalex.org/W3198134274', 'https://openalex.org/W4245896628', 'https://openalex.org/W2096774922', 'https://openalex.org/W2049510512', 'https://openalex.org/W2064218608', 'https://openalex.org/W2155295207', 'https://openalex.org/W1494198834', 'https://openalex.org/W6936113694', 'https://openalex.org/W6783867762', 'https://openalex.org/W6917585676', 'https://openalex.org/W2747874407', 'https://openalex.org/W6847363464', 'https://openalex.org/W3083776549', 'https://openalex.org/W2779206865', 'https://openalex.org/W2515028311', 'https://openalex.org/W4210699844', 'https://openalex.org/W3165478005', 'https://openalex.org/W3099078140', 'https://openalex.org/W3017502571']",2023-01-01
https://openalex.org/W4406273203,https://doi.org/10.1109/icassp49660.2025.10887856,"AnCoGen: Analysis, Control and Generation of Speech with a Masked Autoencoder","This article introduces AnCoGen, a novel method that leverages a masked autoencoder to unify the analysis, control, and generation of speech signals within a single model. AnCoGen can analyze speech by estimating key attributes, such as speaker identity, pitch, content, loudness, signal-to-noise ratio, and clarity index. In addition, it can generate speech from these attributes and allow precise control of the synthesized speech by modifying them. Extensive experiments demonstrated the effectiveness of AnCoGen across speech analysis-resynthesis, pitch estimation, pitch modification, and speech enhancement.","['https://openalex.org/W2022554507', 'https://openalex.org/W2164764235', 'https://openalex.org/W2150980750', 'https://openalex.org/W2088432713', 'https://openalex.org/W2109114466', 'https://openalex.org/W2032026410', 'https://openalex.org/W2428180336', 'https://openalex.org/W2116428736', 'https://openalex.org/W2152205330', 'https://openalex.org/W2471520273', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W3209059054', 'https://openalex.org/W3140429000', 'https://openalex.org/W4313156423', 'https://openalex.org/W4385484923', 'https://openalex.org/W6783867762', 'https://openalex.org/W2962866891', 'https://openalex.org/W3024869864', 'https://openalex.org/W4387865047', 'https://openalex.org/W2752796333', 'https://openalex.org/W3210530853', 'https://openalex.org/W3094502228', 'https://openalex.org/W6739901393', 'https://openalex.org/W3096408984', 'https://openalex.org/W6757632829', 'https://openalex.org/W3096893582', 'https://openalex.org/W2952218014', 'https://openalex.org/W1494198834', 'https://openalex.org/W6802983977', 'https://openalex.org/W6777776875', 'https://openalex.org/W2972541922', 'https://openalex.org/W6757817989', 'https://openalex.org/W2964058413', 'https://openalex.org/W1552314771', 'https://openalex.org/W2141998673', 'https://openalex.org/W2962788625', 'https://openalex.org/W4297841766', 'https://openalex.org/W4372260337', 'https://openalex.org/W4225302959', 'https://openalex.org/W2403891086', 'https://openalex.org/W2118774185', 'https://openalex.org/W1975079546', 'https://openalex.org/W4386076005', 'https://openalex.org/W6762931180']",2025-03-12
https://openalex.org/W4297841546,https://doi.org/10.21437/interspeech.2022-11369,A Temporal Extension of Latent Dirichlet Allocation for Unsupervised Acoustic Unit Discovery,"Latent Dirichlet allocation (LDA) is widely used for unsupervised topic modelling on sets of documents.No temporal information is used in the model.However, there is often a relationship between the corresponding topics of consecutive tokens.In this paper, we present an extension to LDA that uses a Markov chain to model temporal information.We use this new model for acoustic unit discovery from speech.As input tokens, the model takes a discretised encoding of speech from a vector quantised (VQ) neural network with 512 codes.The goal is then to map these 512 VQ codes to 50 phone-like units (topics) in order to more closely resemble true phones.In contrast to the base LDA, which only considers how VQ codes co-occur within utterances (documents), the Markov chain LDA additionally captures how consecutive codes follow one another.This extension leads to an increase in cluster quality and phone segmentation results compared to the base LDA.Compared to a recent vector quantised neural network approach that also learns 50 units, the extended LDA model performs better in phone segmentation but worse in mutual information.","['https://openalex.org/W3204081232', 'https://openalex.org/W1590183771', 'https://openalex.org/W4231510805', 'https://openalex.org/W4297808394', 'https://openalex.org/W1532325895', 'https://openalex.org/W2972867623', 'https://openalex.org/W2145410271', 'https://openalex.org/W2780786457', 'https://openalex.org/W2347098582', 'https://openalex.org/W4288107125', 'https://openalex.org/W1593793857', 'https://openalex.org/W2963799213', 'https://openalex.org/W1965555277', 'https://openalex.org/W3093427098', 'https://openalex.org/W4254018622', 'https://openalex.org/W3210530853', 'https://openalex.org/W2787447541', 'https://openalex.org/W2091746061', 'https://openalex.org/W3198134274', 'https://openalex.org/W4313182775', 'https://openalex.org/W4287591426', 'https://openalex.org/W2927191280', 'https://openalex.org/W1511986666', 'https://openalex.org/W130754613', 'https://openalex.org/W2750248772', 'https://openalex.org/W1506806321', 'https://openalex.org/W2962693497', 'https://openalex.org/W2347145335', 'https://openalex.org/W4221151701', 'https://openalex.org/W3095361818', 'https://openalex.org/W2641832364', 'https://openalex.org/W2100768664', 'https://openalex.org/W3198395459', 'https://openalex.org/W3205900445']",2022-09-16
https://openalex.org/W4390481276,https://doi.org/10.1109/icet59753.2023.10374900,UrduSpeakXLSR: Multilingual Model for Urdu Speech Recognition,"Speech recognition, a disruptive technology, has revolutionized human-machine interaction. While numerous Automatic Speech Recognition (ASR) models are publicly available via HuggingFace, the majority cater to English language. For Urdu, however, models are scarce or closed-source, with open-sourced ones often lack the robustness. Our research addresses this scarcity, focusing on the challenges posed by dialects, slangs, and accents. In this work, we introduce an innovative ASR model leveraging the pretrained XLS-R model based on Wav2Vec2.0 architecture, trained on CommonVoice corpus V11. Our approach outperforms other deep learning-based techniques both qualitatively and quantitatively, offering promising results for an under-resourced language.","['https://openalex.org/W6755207826', 'https://openalex.org/W3097777922', 'https://openalex.org/W3012624518', 'https://openalex.org/W6739901393', 'https://openalex.org/W3210530853', 'https://openalex.org/W6780218876', 'https://openalex.org/W4224979832', 'https://openalex.org/W3016728631', 'https://openalex.org/W6810321870', 'https://openalex.org/W4317382084', 'https://openalex.org/W124924985', 'https://openalex.org/W2523298034', 'https://openalex.org/W4224069448', 'https://openalex.org/W4297536219', 'https://openalex.org/W108866686', 'https://openalex.org/W3094667432', 'https://openalex.org/W2327501763', 'https://openalex.org/W3154887765', 'https://openalex.org/W4285247530', 'https://openalex.org/W6779919476', 'https://openalex.org/W6841063267', 'https://openalex.org/W3213029956', 'https://openalex.org/W4312473364', 'https://openalex.org/W3198429080', 'https://openalex.org/W6771467084', 'https://openalex.org/W6847363464', 'https://openalex.org/W2896457183', 'https://openalex.org/W4385245566', 'https://openalex.org/W4225615476', 'https://openalex.org/W3036601975', 'https://openalex.org/W3030437843', 'https://openalex.org/W4290802904']",2023-11-06
https://openalex.org/W4392931776,https://doi.org/10.1109/icassp48485.2024.10446284,ESVC: Combining Adaptive Style Fusion and Multi-Level Feature Disentanglement for Expressive Singing Voice Conversion,"Nowadays, singing voice conversion (SVC) has made great strides in both naturalness and similarity for common SVC with a neutral expression. However, besides singer identity, emotional expression is also essential to convey the singer's emotions and attitudes, but current SVC systems can not effectively support it. In this paper, we propose an expressive SVC framework called ESVC, which can convert singer identity and emotional style simultaneously. ESVC combines the ideas of style fusion and feature disentanglement, seeking to maximize fidelity in terms of emotional style and singer identity. Firstly, for style information penetration, we employ adaptive instance normalization (AdaIN) to fuse the content feature and style feature. Secondly, given the possibility of information leakage, two disentanglement-oriented methods are introduced to decouple different kinds of singing features. Mutual information (MI) is used to reduce the correlation between linguistic content, fundamental frequency (F0) and expressive feature, while adversarial triplet loss is exerted for decoupling identity and emotional elements. To the best of our knowledge, ESVC is the first SVC system to jointly convert singer identity and emotional style. Objective and subjective experiments demonstrate that our system significantly outperforms the state-of-the-art SVC model in terms of style expressiveness.","['https://openalex.org/W4391021724', 'https://openalex.org/W4372260053', 'https://openalex.org/W2066598518', 'https://openalex.org/W2294038178', 'https://openalex.org/W3205154814', 'https://openalex.org/W3095948607', 'https://openalex.org/W3207300132', 'https://openalex.org/W4372338328', 'https://openalex.org/W3209984917', 'https://openalex.org/W3209059054', 'https://openalex.org/W6839738141', 'https://openalex.org/W3016007107', 'https://openalex.org/W6786824797', 'https://openalex.org/W3163568691', 'https://openalex.org/W4224927653', 'https://openalex.org/W4226320669', 'https://openalex.org/W4286747238', 'https://openalex.org/W4384028538', 'https://openalex.org/W6849600165', 'https://openalex.org/W2603777577', 'https://openalex.org/W3197659778', 'https://openalex.org/W2096733369', 'https://openalex.org/W6796464841', 'https://openalex.org/W3210530853', 'https://openalex.org/W4391021367', 'https://openalex.org/W6779459370', 'https://openalex.org/W4221153051', 'https://openalex.org/W3096939667', 'https://openalex.org/W4296068763', 'https://openalex.org/W4285345683', 'https://openalex.org/W4205742757', 'https://openalex.org/W3110041771', 'https://openalex.org/W4320451749']",2024-03-18
https://openalex.org/W4398777658,https://doi.org/10.47191/etj/v9i05.10,Speech Cloning: Text-To-Speech Using VITS,"Voice is one of the most common and natural communication methods for humans. Voice is becoming the primary interface for AI voice assistants like Amazon Alexa, as well as in autos and smart home devices. Homes and so on. As human-machine communication becomes more common, researchers are exploring technology that mimics genuine speech. Speech cloning is the practice of copying or mimicking another person's speech, usually utilizing modern technology and artificial intelligence (AI). This entails producing a synthetic or cloned version of someone's voice that sounds very similar to the actual speaker. The objective is to produce speech that is indistinguishable from the genuine person, both in tone and intonation. Instant Voice Cloning (IVC) in text-to-speech (TTS) synthesis refers to the TTS model's capacity to copy the voice of any reference speaker based on a short audio sample, without requiring extra speaker-specific training. This method is usually referred to as zero-shot TTS. IVC provides users with the flexibility to tailor the generated voice, offering significant value across diverse real-world applications. Examples include media content creation, personalized chatbots, and multi-modal interactions between humans and computers or extensive language models.","['https://openalex.org/W4389363808', 'https://openalex.org/W4372260053', 'https://openalex.org/W3210530853', 'https://openalex.org/W3026874504', 'https://openalex.org/W2996286887', 'https://openalex.org/W4287121924', 'https://openalex.org/W2963090522', 'https://openalex.org/W3007345776', 'https://openalex.org/W4382603054', 'https://openalex.org/W3140429000', 'https://openalex.org/W3092028330']",2024-05-24
https://openalex.org/W4387505238,https://doi.org/10.13064/ksss.2023.15.3.069,Zero-shot voice conversion with HuBERT,"This study introduces an innovative model for zero-shot voice conversion that utilizes the capabilities of HuBERT. Zero-shot voice conversion models can transform the speech of one speaker to mimic that of another, even when the model has not been exposed to the target speaker","['https://openalex.org/W3198020407', 'https://openalex.org/W6749126569', 'https://openalex.org/W2896457183', 'https://openalex.org/W1810943226', 'https://openalex.org/W2064675550', 'https://openalex.org/W3209059054', 'https://openalex.org/W1836465849', 'https://openalex.org/W2523060838', 'https://openalex.org/W3026874504', 'https://openalex.org/W3169905056', 'https://openalex.org/W6631190155', 'https://openalex.org/W6752910514', 'https://openalex.org/W3092028330', 'https://openalex.org/W3173617765', 'https://openalex.org/W2191779130', 'https://openalex.org/W6682222085', 'https://openalex.org/W2945478979', 'https://openalex.org/W6610566761', 'https://openalex.org/W2973049979', 'https://openalex.org/W2131774270', 'https://openalex.org/W4366460484', 'https://openalex.org/W3098557217', 'https://openalex.org/W2129069237', 'https://openalex.org/W3210530853', 'https://openalex.org/W2515028311']",2023-09-01
https://openalex.org/W4394713256,https://doi.org/10.1109/bigcomp60711.2024.00041,An Optimized EMG Encoder to Minimize Soft Speech Loss for Speech to EMG Conversions,"Electromyography (EMG) to speech conversions is a standard problem to facilitate speech impaired individuals for communication via EMG (EMG to Speech). However, dataset acquisition is a cumbersome process and highly dependent on acquisition configuration. The availability of EMG signals can be made by tackling the inverse problem (Speech to EMG). In this paper, we propose an optimized EMG encoder which enhanced EMG feature extraction and in turn leads to improvements in soft speech units' representations. To validate the efficacy of our proposed enhanced EMG encoder, we utilized state-of-the-art speech to EMG generative adversarial network (STE-GANs). We witnessed a significant improvements in synthesized EMG signals after utilizing proposed EMG encoder which improves soft speech losses by producing enhanced speech units during training of STE-GANs. The extensive results are presented on public dataset.","['https://openalex.org/W3105229878', 'https://openalex.org/W3097407310', 'https://openalex.org/W3198898142', 'https://openalex.org/W4385823094', 'https://openalex.org/W3175752069', 'https://openalex.org/W3092028330', 'https://openalex.org/W3210530853', 'https://openalex.org/W4372264243', 'https://openalex.org/W6687483927', 'https://openalex.org/W2752782242', 'https://openalex.org/W4234552385', 'https://openalex.org/W2519091744']",2024-02-18
https://openalex.org/W4405272327,https://doi.org/10.1109/biosig61931.2024.10786731,Quantifying Source Speaker Leakage in One-to-One Voice Conversion,"Using a multi-accented corpus of parallel utterances for use with commercial speech devices, we present a case study to show that it is possible to quantify a degree of confidence about a source speaker's identity in the case of one-to-one voice conversion. Following voice conversion using a HiFi-GAN vocoder, we compare information leakage for a range speaker characteristics; assuming a ""worst-case"" white-box scenario, we quantify our confidence to perform inference and narrow the pool of likely source speakers, reinforcing the regulatory obligation and moral duty that providers of synthetic voices have to ensure the privacy of their speakers' data.","['https://openalex.org/W4210970267', 'https://openalex.org/W4372260476', 'https://openalex.org/W6850325175', 'https://openalex.org/W3209059054', 'https://openalex.org/W3197763626', 'https://openalex.org/W2069221573', 'https://openalex.org/W6691687657', 'https://openalex.org/W3024869864', 'https://openalex.org/W3210530853', 'https://openalex.org/W4221152846', 'https://openalex.org/W2972359262', 'https://openalex.org/W2808631503', 'https://openalex.org/W2115098197', 'https://openalex.org/W6783867762', 'https://openalex.org/W6712926106', 'https://openalex.org/W2525788160', 'https://openalex.org/W3201497420', 'https://openalex.org/W4375869485', 'https://openalex.org/W2109394932', 'https://openalex.org/W3162390194', 'https://openalex.org/W3005862564', 'https://openalex.org/W2951082691', 'https://openalex.org/W3184252821', 'https://openalex.org/W2592076957', 'https://openalex.org/W3092028330']",2024-09-25
https://openalex.org/W4409045741,https://doi.org/10.1145/3727341,The Interpretable and Transferable Adversarial Attack against Synthetic Speech Detectors,"Existing work finds it challenging for adversarial examples to transfer among different synthetic speech detectors because of cross-feature and cross-model. To enhance the transferability of adversarial examples, we propose a spectral saliency analysis method and gain insight into the underlying detection mechanisms of existing detectors for the first time. These insights offer an interpretable basis for why adversarial examples are challenging to transfer between synthetic speech detection models. Then we further propose a two-stage adversarial attack framework. Specifically, the first stage leverages insights into the model detection mechanism to design a random time-frequency masking module, the random offset module, and 1D convolution to generate transferable and robust adversarial examples. In the second stage, to mitigate the problem of obvious noise in the low-energy frames of the carrier in existing adversarial attacks, we perform secondary optimization on frames below the Signal-Noise-Rate threshold to enhance its auditory quality. Extensive experimental results demonstrate that the proposed method significantly enhances the transferability and robustness of adversarial examples, while simultaneously preserving the acoustic quality compared to typical approaches.","['https://openalex.org/W3153453329', 'https://openalex.org/W4390045124', 'https://openalex.org/W4386090485', 'https://openalex.org/W4308642956', 'https://openalex.org/W4285148065', 'https://openalex.org/W2774644650', 'https://openalex.org/W2969542116', 'https://openalex.org/W2294797155', 'https://openalex.org/W4387969869', 'https://openalex.org/W4312743281', 'https://openalex.org/W3170179936', 'https://openalex.org/W3201773091', 'https://openalex.org/W4385823506', 'https://openalex.org/W4313443012', 'https://openalex.org/W3091508658', 'https://openalex.org/W2962747881', 'https://openalex.org/W2107860279', 'https://openalex.org/W2963542245', 'https://openalex.org/W4381198892', 'https://openalex.org/W2576309025', 'https://openalex.org/W4385807439', 'https://openalex.org/W2602814339', 'https://openalex.org/W4372260558', 'https://openalex.org/W2295634712', 'https://openalex.org/W4385800847', 'https://openalex.org/W3163596559', 'https://openalex.org/W4309577009', 'https://openalex.org/W2473388484', 'https://openalex.org/W3210530853', 'https://openalex.org/W4402754002', 'https://openalex.org/W4313387422', 'https://openalex.org/W3026777299', 'https://openalex.org/W2962847335', 'https://openalex.org/W3015958938', 'https://openalex.org/W4366148664', 'https://openalex.org/W4383503656', 'https://openalex.org/W4221138880', 'https://openalex.org/W4392908976', 'https://openalex.org/W3047561893', 'https://openalex.org/W4387968133', 'https://openalex.org/W3096023981', 'https://openalex.org/W4365800072', 'https://openalex.org/W3163547718', 'https://openalex.org/W1530404542', 'https://openalex.org/W1581253957', 'https://openalex.org/W4388867283']",2025-04-01
https://openalex.org/W4410362024,https://doi.org/10.3390/app15105503,MPFM-VC: A Voice Conversion Algorithm Based on Multi-Dimensional Perception Flow Matching,"Voice conversion (VC) is an advanced technology that enables the transformation of raw speech into high-quality audio resembling the target speaker’s voice while preserving the original linguistic content and prosodic patterns. In this study, we propose a voice conversion algorithm, Multi-Dimensional Perception Flow Matching (MPFM-VC). Unlike traditional approaches that directly generate waveform outputs, MPFM-VC models the evolutionary trajectory of mel spectrograms with a flow-matching framework and incorporates a multi-dimensional feature perception network to enhance the stability and quality of speech synthesis. Additionally, we introduce a content perturbation method during training to improve the model’s generalization ability and reduce inference-time artifacts. To further increase speaker similarity, an adversarial training mechanism on speaker embeddings is employed to achieve effective disentanglement between content and speaker identity representations, thereby enhancing the timbre consistency of the converted speech. Experimental results for both speech and singing voice conversion tasks show that MPFM-VC achieves competitive performance compared to existing state-of-the-art VC models in both subjective and objective evaluation metrics. The synthesized speech shows improved naturalness, clarity, and timbre fidelity in both objective and subjective evaluations, suggesting the potential effectiveness of the proposed approach.","['https://openalex.org/W4308840623', 'https://openalex.org/W2946200149', 'https://openalex.org/W2963609956', 'https://openalex.org/W2964243274', 'https://openalex.org/W6757079273', 'https://openalex.org/W2804998325', 'https://openalex.org/W2945478979', 'https://openalex.org/W2972659941', 'https://openalex.org/W2808706139', 'https://openalex.org/W4311000453', 'https://openalex.org/W4283659485', 'https://openalex.org/W2112796928', 'https://openalex.org/W1909320841', 'https://openalex.org/W6739901393', 'https://openalex.org/W2129069237', 'https://openalex.org/W6783867762', 'https://openalex.org/W4221152438', 'https://openalex.org/W3169905056', 'https://openalex.org/W4385823163', 'https://openalex.org/W4372260053', 'https://openalex.org/W4284690259', 'https://openalex.org/W4402115949', 'https://openalex.org/W4391021724', 'https://openalex.org/W4391021772', 'https://openalex.org/W4391021798', 'https://openalex.org/W3162673269', 'https://openalex.org/W3204602440', 'https://openalex.org/W4393156644', 'https://openalex.org/W4376312477', 'https://openalex.org/W4392538976', 'https://openalex.org/W4382603054', 'https://openalex.org/W4392904491', 'https://openalex.org/W4409348103', 'https://openalex.org/W4402112245', 'https://openalex.org/W4405173504', 'https://openalex.org/W1901129140', 'https://openalex.org/W4385822304', 'https://openalex.org/W3210530853', 'https://openalex.org/W4385823321', 'https://openalex.org/W2963539064', 'https://openalex.org/W2902070858', 'https://openalex.org/W3198533616', 'https://openalex.org/W2936774411', 'https://openalex.org/W3162512456', 'https://openalex.org/W4403345601', 'https://openalex.org/W3092028330']",2025-05-14
https://openalex.org/W4385571229,https://doi.org/10.18653/v1/2023.findings-acl.307,Speech-to-Speech Translation for a Real-world Unwritten Language,"Peng-Jen Chen, Kevin Tran, Yilin Yang, Jingfei Du, Justine Kao, Yu-An Chung, Paden Tomasello, Paul-Ambroise Duquenne, Holger Schwenk, Hongyu Gong, Hirofumi Inaguma, Sravya Popuri, Changhan Wang, Juan Pino, Wei-Ning Hsu, Ann Lee. Findings of the Association for Computational Linguistics: ACL 2023. 2023.","['https://openalex.org/W3193521535', 'https://openalex.org/W2933138175', 'https://openalex.org/W4308756394', 'https://openalex.org/W2972495969', 'https://openalex.org/W2949328740', 'https://openalex.org/W3030437843', 'https://openalex.org/W4287854499', 'https://openalex.org/W4221153524', 'https://openalex.org/W2963250244', 'https://openalex.org/W3196509775', 'https://openalex.org/W3210177631', 'https://openalex.org/W1494198834', 'https://openalex.org/W2963532001', 'https://openalex.org/W4288025890', 'https://openalex.org/W3101648800', 'https://openalex.org/W630532510', 'https://openalex.org/W3213018012', 'https://openalex.org/W4307770080', 'https://openalex.org/W2995181338', 'https://openalex.org/W3117789470', 'https://openalex.org/W2250600805', 'https://openalex.org/W2121016490', 'https://openalex.org/W2945700568', 'https://openalex.org/W4281789500', 'https://openalex.org/W3120929527', 'https://openalex.org/W4287072252', 'https://openalex.org/W3175871055', 'https://openalex.org/W2975381464', 'https://openalex.org/W4226033575', 'https://openalex.org/W4226444650', 'https://openalex.org/W2136545725', 'https://openalex.org/W4291566970', 'https://openalex.org/W3119308075', 'https://openalex.org/W3142316150', 'https://openalex.org/W2914120296', 'https://openalex.org/W3092085609', 'https://openalex.org/W3140429000', 'https://openalex.org/W4385570550', 'https://openalex.org/W2973088264', 'https://openalex.org/W4296070387', 'https://openalex.org/W4367841185', 'https://openalex.org/W3035390927', 'https://openalex.org/W3097777922', 'https://openalex.org/W3015698636', 'https://openalex.org/W3007068036', 'https://openalex.org/W3107826490', 'https://openalex.org/W3169320628', 'https://openalex.org/W3174864715', 'https://openalex.org/W3036601975']",2023-01-01
https://openalex.org/W4406385636,https://doi.org/10.1038/s41586-024-08359-z,Joint speech and text machine translation for up to 100 languages,"Creating the Babel Fish, a tool that helps individuals translate speech between any two languages, requires advanced technological innovation and linguistic expertise. Although conventional speech-to-speech translation systems composed of multiple subsystems performing translation in a cascaded fashion exist<sup>1-3</sup>, scalable and high-performing unified systems<sup>4,5</sup> remain underexplored. To address this gap, here we introduce SEAMLESSM4T-Massively Multilingual and Multimodal Machine Translation-a single model that supports speech-to-speech translation (101 to 36 languages), speech-to-text translation (from 101 to 96 languages), text-to-speech translation (from 96 to 36 languages), text-to-text translation (96 languages) and automatic speech recognition (96 languages). Built using a new multimodal corpus of automatically aligned speech translations and other publicly available data, SEAMLESSM4T is one of the first multilingual systems that can translate from and into English for both speech and text. Moreover, it outperforms the existing state-of-the-art cascaded systems, achieving up to 8% and 23% higher BLEU (Bilingual Evaluation Understudy) scores in speech-to-text and speech-to-speech tasks, respectively. Beyond quality, when tested for robustness, our system is, on average, approximately 50% more resilient against background noise and speaker variations in speech-to-text tasks than the previous state-of-the-art systems. We evaluated SEAMLESSM4T on added toxicity and gender bias to assess translation safety. For the former, we included two strategies for added toxicity mitigation working at either training or inference time. Finally, all contributions in this work are publicly available for non-commercial use to propel further research on inclusive speech translation technologies.","['https://openalex.org/W2097203679', 'https://openalex.org/W1538023239', 'https://openalex.org/W2136545725', 'https://openalex.org/W2936184970', 'https://openalex.org/W2972495969', 'https://openalex.org/W3180374548', 'https://openalex.org/W6798080464', 'https://openalex.org/W4399365598', 'https://openalex.org/W6855524384', 'https://openalex.org/W2250342921', 'https://openalex.org/W2101105183', 'https://openalex.org/W3024869864', 'https://openalex.org/W3139878283', 'https://openalex.org/W4385569960', 'https://openalex.org/W4385574194', 'https://openalex.org/W3039695075', 'https://openalex.org/W4281621399', 'https://openalex.org/W4319862635', 'https://openalex.org/W4323651091', 'https://openalex.org/W4311000453', 'https://openalex.org/W4381827575', 'https://openalex.org/W4200631896', 'https://openalex.org/W6852909395', 'https://openalex.org/W4404781204', 'https://openalex.org/W4309129001', 'https://openalex.org/W4385571229', 'https://openalex.org/W4280617721', 'https://openalex.org/W4303648927', 'https://openalex.org/W4389518873', 'https://openalex.org/W4297841625', 'https://openalex.org/W4390810311', 'https://openalex.org/W4402670286', 'https://openalex.org/W2952328691', 'https://openalex.org/W3197577761', 'https://openalex.org/W4283809028', 'https://openalex.org/W3035070478', 'https://openalex.org/W4377865270', 'https://openalex.org/W4389524081', 'https://openalex.org/W4403431383', 'https://openalex.org/W3012624518', 'https://openalex.org/W4312391725', 'https://openalex.org/W4394778654', 'https://openalex.org/W2032374598', 'https://openalex.org/W4285131748', 'https://openalex.org/W4307416501', 'https://openalex.org/W3196509775', 'https://openalex.org/W3127012371', 'https://openalex.org/W3197771105', 'https://openalex.org/W4323066695', 'https://openalex.org/W2798389157', 'https://openalex.org/W2962735107', 'https://openalex.org/W2973088264', 'https://openalex.org/W4308756394', 'https://openalex.org/W4385572318', 'https://openalex.org/W4286359908', 'https://openalex.org/W6861295083', 'https://openalex.org/W4311731008', 'https://openalex.org/W4385570550', 'https://openalex.org/W3007068036', 'https://openalex.org/W3213029956', 'https://openalex.org/W4384648564', 'https://openalex.org/W4226033575', 'https://openalex.org/W3025165719', 'https://openalex.org/W3097777922', 'https://openalex.org/W2963250244', 'https://openalex.org/W2962784628', 'https://openalex.org/W4385573923', 'https://openalex.org/W2899716505', 'https://openalex.org/W2964161387', 'https://openalex.org/W3096490862', 'https://openalex.org/W6739901393', 'https://openalex.org/W4283834483', 'https://openalex.org/W6778823374', 'https://openalex.org/W3194000401', 'https://openalex.org/W4311994673', 'https://openalex.org/W4385569956', 'https://openalex.org/W4385574250']",2025-01-15
https://openalex.org/W4392903062,https://doi.org/10.1109/icassp48485.2024.10448426,Translatotron 3: Speech to Speech Translation with Monolingual Data,"This paper presents Translatotron 3, a novel approach to unsupervised direct speech-to-speech translation from monolingual speech-text datasets by combining masked autoencoder, unsupervised embedding mapping, and back-translation. Experimental results in speech-to-speech translation tasks between Spanish and English show that Translatotron 3 outperforms a baseline cascade system, reporting 18.14 BLEU points improvement on the synthesized Unpaired-Conversational dataset. In contrast to supervised approaches that necessitate real paired data, or specialized modeling to replicate para-/non-linguistic information such as pauses, speaking rates, and speaker identity, Translatotron 3 showcases its capability to retain it.","['https://openalex.org/W6841035593', 'https://openalex.org/W2097203679', 'https://openalex.org/W2136545725', 'https://openalex.org/W2972495969', 'https://openalex.org/W3180374548', 'https://openalex.org/W2963216553', 'https://openalex.org/W6745740328', 'https://openalex.org/W6745388339', 'https://openalex.org/W4313156423', 'https://openalex.org/W2936774411', 'https://openalex.org/W6744957266', 'https://openalex.org/W3175871055', 'https://openalex.org/W4287854499', 'https://openalex.org/W6838789019', 'https://openalex.org/W4223622550', 'https://openalex.org/W4280601369', 'https://openalex.org/W3142316150', 'https://openalex.org/W4385893869', 'https://openalex.org/W6847247304', 'https://openalex.org/W6845958605', 'https://openalex.org/W3215615641', 'https://openalex.org/W4226033575', 'https://openalex.org/W4307323391', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3007068036', 'https://openalex.org/W6853998256', 'https://openalex.org/W4372349107', 'https://openalex.org/W6855650468', 'https://openalex.org/W4372191700', 'https://openalex.org/W4385570101', 'https://openalex.org/W3205644108', 'https://openalex.org/W4285112515', 'https://openalex.org/W4392979802', 'https://openalex.org/W2741602058', 'https://openalex.org/W2964266061', 'https://openalex.org/W2899134946', 'https://openalex.org/W2962793481', 'https://openalex.org/W2964161387', 'https://openalex.org/W6771467084', 'https://openalex.org/W4319862245', 'https://openalex.org/W6765510844', 'https://openalex.org/W6846600677', 'https://openalex.org/W3197324626', 'https://openalex.org/W6784545093', 'https://openalex.org/W6748409065', 'https://openalex.org/W6810701745', 'https://openalex.org/W3196509775', 'https://openalex.org/W4385970143', 'https://openalex.org/W4226444650', 'https://openalex.org/W4381827575', 'https://openalex.org/W4307783813', 'https://openalex.org/W3091928890', 'https://openalex.org/W4299579390', 'https://openalex.org/W3036601975', 'https://openalex.org/W4298393544', 'https://openalex.org/W4281789500', 'https://openalex.org/W4385570550']",2024-03-18
https://openalex.org/W4385893869,https://doi.org/10.18653/v1/2023.acl-long.602,Simple and Effective Unsupervised Speech Translation,"Changhan Wang, Hirofumi Inaguma, Peng-Jen Chen, Ilia Kulikov, Yun Tang, Wei-Ning Hsu, Michael Auli, Juan Pino. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.","['https://openalex.org/W6813608150', 'https://openalex.org/W2939710050', 'https://openalex.org/W3198771897', 'https://openalex.org/W3196659080', 'https://openalex.org/W3118578889', 'https://openalex.org/W2963834942', 'https://openalex.org/W3107826490', 'https://openalex.org/W2973048981', 'https://openalex.org/W2934852845', 'https://openalex.org/W3213029956', 'https://openalex.org/W2962799225', 'https://openalex.org/W2949328740', 'https://openalex.org/W2997574889', 'https://openalex.org/W2125529971', 'https://openalex.org/W2903739847', 'https://openalex.org/W2962824887', 'https://openalex.org/W4303941982', 'https://openalex.org/W4298393544', 'https://openalex.org/W4226444650', 'https://openalex.org/W3174446152', 'https://openalex.org/W4385245566', 'https://openalex.org/W3035390927', 'https://openalex.org/W2605131327', 'https://openalex.org/W2785350307', 'https://openalex.org/W3142316150', 'https://openalex.org/W2901607128', 'https://openalex.org/W2466918907', 'https://openalex.org/W2964079874', 'https://openalex.org/W3196509775', 'https://openalex.org/W4385570550', 'https://openalex.org/W2593011301', 'https://openalex.org/W3119308075', 'https://openalex.org/W2973049979', 'https://openalex.org/W3036601975', 'https://openalex.org/W4297841354', 'https://openalex.org/W4319862670', 'https://openalex.org/W4287072252', 'https://openalex.org/W3173767661', 'https://openalex.org/W3034586846', 'https://openalex.org/W4287173589', 'https://openalex.org/W4296070453', 'https://openalex.org/W1526974435', 'https://openalex.org/W2964172053', 'https://openalex.org/W3174864715', 'https://openalex.org/W4299579390']",2023-01-01
https://openalex.org/W4385569956,https://doi.org/10.18653/v1/2023.acl-long.504,BLASER: A Text-Free Speech-to-Speech Translation Evaluation Metric,"Mingda Chen, Paul-Ambroise Duquenne, Pierre Andrews, Justine Kao, Alexandre Mourachko, Holger Schwenk, Marta R. Costa-jussà. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.","['https://openalex.org/W2933138175', 'https://openalex.org/W3201257124', 'https://openalex.org/W3036601975', 'https://openalex.org/W2996286887', 'https://openalex.org/W3213018012', 'https://openalex.org/W2973088264', 'https://openalex.org/W2945700568', 'https://openalex.org/W3213618310', 'https://openalex.org/W2508316494', 'https://openalex.org/W3197771105', 'https://openalex.org/W2903376039', 'https://openalex.org/W2101105183', 'https://openalex.org/W2973049979', 'https://openalex.org/W3117789470', 'https://openalex.org/W2406330699', 'https://openalex.org/W2097203679', 'https://openalex.org/W3015698636', 'https://openalex.org/W2963532001', 'https://openalex.org/W3001434439', 'https://openalex.org/W2758950307', 'https://openalex.org/W2970791445', 'https://openalex.org/W4385893869', 'https://openalex.org/W2972802841', 'https://openalex.org/W3180374548', 'https://openalex.org/W2250342921', 'https://openalex.org/W4221155340', 'https://openalex.org/W3119308075', 'https://openalex.org/W3169320628', 'https://openalex.org/W3213029956', 'https://openalex.org/W4307205778', 'https://openalex.org/W4280617721', 'https://openalex.org/W3186081172', 'https://openalex.org/W3198214208', 'https://openalex.org/W4285077564', 'https://openalex.org/W2963672008', 'https://openalex.org/W3030437843', 'https://openalex.org/W2916548775', 'https://openalex.org/W4286359908', 'https://openalex.org/W222053410', 'https://openalex.org/W1494198834', 'https://openalex.org/W2133459682', 'https://openalex.org/W4285140158', 'https://openalex.org/W4385572318', 'https://openalex.org/W2936695845', 'https://openalex.org/W3196509775', 'https://openalex.org/W3035390927', 'https://openalex.org/W2915756181', 'https://openalex.org/W2127141656', 'https://openalex.org/W4385570550', 'https://openalex.org/W4287854499', 'https://openalex.org/W2396366106', 'https://openalex.org/W3140429000', 'https://openalex.org/W3198429080', 'https://openalex.org/W3035252911', 'https://openalex.org/W4385574194', 'https://openalex.org/W4205646617', 'https://openalex.org/W3105214104', 'https://openalex.org/W2130086727', 'https://openalex.org/W4285158119', 'https://openalex.org/W2903188467', 'https://openalex.org/W3039695075', 'https://openalex.org/W4226120743', 'https://openalex.org/W2970641574', 'https://openalex.org/W4206711328', 'https://openalex.org/W2149327368', 'https://openalex.org/W3038033387', 'https://openalex.org/W4385571229']",2023-01-01
https://openalex.org/W4392902778,https://doi.org/10.1109/icassp48485.2024.10447926,Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing,"Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length – this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>","['https://openalex.org/W6769269733', 'https://openalex.org/W3168212167', 'https://openalex.org/W4221153524', 'https://openalex.org/W6739365718', 'https://openalex.org/W3205644108', 'https://openalex.org/W6810259195', 'https://openalex.org/W4223622550', 'https://openalex.org/W4287890956', 'https://openalex.org/W4221163209', 'https://openalex.org/W4226120743', 'https://openalex.org/W6846505686', 'https://openalex.org/W4385570757', 'https://openalex.org/W4391021666', 'https://openalex.org/W4385822683', 'https://openalex.org/W6853998256', 'https://openalex.org/W6777028661', 'https://openalex.org/W6790356757', 'https://openalex.org/W6848735303', 'https://openalex.org/W6852781825', 'https://openalex.org/W3180374548', 'https://openalex.org/W4372349107', 'https://openalex.org/W3209984917', 'https://openalex.org/W6898634591', 'https://openalex.org/W2512924740', 'https://openalex.org/W3001434439', 'https://openalex.org/W6781811055', 'https://openalex.org/W2752796333', 'https://openalex.org/W6798098866', 'https://openalex.org/W4307323391', 'https://openalex.org/W3209059054', 'https://openalex.org/W4375869259', 'https://openalex.org/W2963250244', 'https://openalex.org/W3035207248', 'https://openalex.org/W4388017359', 'https://openalex.org/W3089472875', 'https://openalex.org/W2936774411', 'https://openalex.org/W2964012862', 'https://openalex.org/W4225308107', 'https://openalex.org/W6679434410', 'https://openalex.org/W2327501763', 'https://openalex.org/W2127141656', 'https://openalex.org/W6638749077', 'https://openalex.org/W2766219058', 'https://openalex.org/W4386566728', 'https://openalex.org/W3037217258', 'https://openalex.org/W3153583341', 'https://openalex.org/W6839510803', 'https://openalex.org/W4385565440', 'https://openalex.org/W4319862255', 'https://openalex.org/W3173767661', 'https://openalex.org/W4385571303', 'https://openalex.org/W2963532001', 'https://openalex.org/W4285223043', 'https://openalex.org/W6838475196', 'https://openalex.org/W2758375579', 'https://openalex.org/W4385570550', 'https://openalex.org/W4377865046', 'https://openalex.org/W4221155340', 'https://openalex.org/W4313679638', 'https://openalex.org/W4394671563', 'https://openalex.org/W2624871570', 'https://openalex.org/W3046368065', 'https://openalex.org/W3215615641', 'https://openalex.org/W4381827575', 'https://openalex.org/W3024605872']",2024-03-18
https://openalex.org/W4385569716,https://doi.org/10.18653/v1/2023.acl-long.251,Back Translation for Speech-to-text Translation Without Transcripts,"The success of end-to-end speech-to-text translation (ST) is often achieved by utilizing source transcripts, e.g., by pre-training with automatic speech recognition (ASR) and machine translation (MT) tasks, or by introducing additional ASR and MT data. Unfortunately, transcripts are only sometimes available since numerous unwritten languages exist worldwide. In this paper, we aim to utilize large amounts of target-side monolingual data to enhance ST without transcripts. Motivated by the remarkable success of back translation in MT, we develop a back translation algorithm for ST (BT4ST) to synthesize pseudo ST data from monolingual target data. To ease the challenges posed by short-to-long generation and one-to-many mapping, we introduce self-supervised discrete units and achieve back translation by cascading a target-to-unit model and a unit-to-speech model. With our synthetic ST data, we achieve an average boost of 2.3 BLEU on MuST-C En-De, En-Fr, and En-Es datasets. More experiments show that our method is especially effective in low-resource scenarios.","['https://openalex.org/W4221155340', 'https://openalex.org/W4309129001', 'https://openalex.org/W2933138175', 'https://openalex.org/W3213873715', 'https://openalex.org/W3102811925', 'https://openalex.org/W4385570154', 'https://openalex.org/W3173666333', 'https://openalex.org/W4312056676', 'https://openalex.org/W3196292088', 'https://openalex.org/W222053410', 'https://openalex.org/W2949328740', 'https://openalex.org/W2970279348', 'https://openalex.org/W3128910262', 'https://openalex.org/W3113908264', 'https://openalex.org/W2101105183', 'https://openalex.org/W2963216553', 'https://openalex.org/W3120929527', 'https://openalex.org/W2508809683', 'https://openalex.org/W4385245566', 'https://openalex.org/W3202201199', 'https://openalex.org/W3034571331', 'https://openalex.org/W4287890956', 'https://openalex.org/W4297677272', 'https://openalex.org/W4320085355', 'https://openalex.org/W4312052802', 'https://openalex.org/W2964172053', 'https://openalex.org/W3176711365', 'https://openalex.org/W3176382501', 'https://openalex.org/W3176455679', 'https://openalex.org/W4385570550', 'https://openalex.org/W3180374548', 'https://openalex.org/W3173767661', 'https://openalex.org/W4226120743', 'https://openalex.org/W3103169714', 'https://openalex.org/W4226092197', 'https://openalex.org/W4285158119', 'https://openalex.org/W3205644108', 'https://openalex.org/W4287854499', 'https://openalex.org/W3140429000', 'https://openalex.org/W2997436923', 'https://openalex.org/W4221163209', 'https://openalex.org/W4385571004', 'https://openalex.org/W3209059054', 'https://openalex.org/W3033411150', 'https://openalex.org/W2886095922', 'https://openalex.org/W3113676066', 'https://openalex.org/W1522301498', 'https://openalex.org/W2962788625', 'https://openalex.org/W4285215858', 'https://openalex.org/W2970015022', 'https://openalex.org/W4206028909', 'https://openalex.org/W4372260139', 'https://openalex.org/W3034474651', 'https://openalex.org/W2466918907', 'https://openalex.org/W4223622550', 'https://openalex.org/W3162471442', 'https://openalex.org/W3207222250', 'https://openalex.org/W2964161387', 'https://openalex.org/W3210177631', 'https://openalex.org/W2963532001', 'https://openalex.org/W4385573012', 'https://openalex.org/W3105825505', 'https://openalex.org/W4287854398', 'https://openalex.org/W4281982771', 'https://openalex.org/W3036601975', 'https://openalex.org/W3172698324', 'https://openalex.org/W2889326796', 'https://openalex.org/W2945700568', 'https://openalex.org/W4385571229', 'https://openalex.org/W3006988520', 'https://openalex.org/W3186200218', 'https://openalex.org/W3092028330', 'https://openalex.org/W4311000453', 'https://openalex.org/W3198217962', 'https://openalex.org/W3162037819']",2023-01-01
https://openalex.org/W4392904292,https://doi.org/10.1109/icassp48485.2024.10446888,Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-Training and Multi-Modal Tokens,"In this paper, we propose methods to build a powerful and efficient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehension and language modeling from a large-scale pre-trained vision-language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech features of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteristics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: bit.ly/3Z9T6LJ.","['https://openalex.org/W6807079506', 'https://openalex.org/W6630875275', 'https://openalex.org/W2752796333', 'https://openalex.org/W4375868850', 'https://openalex.org/W3206387123', 'https://openalex.org/W6790356757', 'https://openalex.org/W4385570550', 'https://openalex.org/W4296070387', 'https://openalex.org/W6855650468', 'https://openalex.org/W4375868953', 'https://openalex.org/W4307680525', 'https://openalex.org/W6777028661', 'https://openalex.org/W4385823403', 'https://openalex.org/W3209059054', 'https://openalex.org/W4385822683', 'https://openalex.org/W4390874021', 'https://openalex.org/W2586148577', 'https://openalex.org/W4319862477', 'https://openalex.org/W4386071467', 'https://openalex.org/W6791353385', 'https://openalex.org/W6850204008', 'https://openalex.org/W3180355996', 'https://openalex.org/W6802517614', 'https://openalex.org/W1861492603', 'https://openalex.org/W68733909', 'https://openalex.org/W2972394484', 'https://openalex.org/W4372260534', 'https://openalex.org/W3174311593', 'https://openalex.org/W3155217823', 'https://openalex.org/W3140429000', 'https://openalex.org/W6783867762', 'https://openalex.org/W3094502228', 'https://openalex.org/W4213019189', 'https://openalex.org/W6677929280', 'https://openalex.org/W4390871839', 'https://openalex.org/W2962862718', 'https://openalex.org/W1905882502', 'https://openalex.org/W6780218876', 'https://openalex.org/W6898505805', 'https://openalex.org/W2133459682', 'https://openalex.org/W6682631176', 'https://openalex.org/W1956340063', 'https://openalex.org/W2506483933', 'https://openalex.org/W6796464841', 'https://openalex.org/W4385245566', 'https://openalex.org/W2886641317', 'https://openalex.org/W6676497082', 'https://openalex.org/W4287854499', 'https://openalex.org/W3037465386', 'https://openalex.org/W4206865574', 'https://openalex.org/W3024605872', 'https://openalex.org/W3036601975', 'https://openalex.org/W4385970143', 'https://openalex.org/W2154652894', 'https://openalex.org/W2109586012', 'https://openalex.org/W4394671563', 'https://openalex.org/W4320458302', 'https://openalex.org/W3099142230']",2024-03-18
https://openalex.org/W4392903174,https://doi.org/10.1109/icassp48485.2024.10447331,TranSentence: speech-to-speech Translation via Language-Agnostic Sentence-Level Speech Encoding without Language-Parallel Data,"Although there has been significant advancement in the field of speech-to-speech translation, conventional models still require language-parallel speech data between the source and target languages for training. In this paper, we introduce TranSentence, a novel speech-to-speech translation without language-parallel speech data. To achieve this, we first adopt a language-agnostic sentence-level speech encoding that captures the semantic information of speech, irrespective of language. We then train our model to generate speech based on the encoded embedding obtained from a language-agnostic sentence-level speech encoder that is pre-trained with various languages. With this method, despite training exclusively on the target language's monolingual data, we can generate target language speech in the inference stage using language-agnostic speech embedding from the source language speech. Furthermore, we extend TranSentence to multilingual speech-to-speech translation. The experimental results demonstrate that TranSentence is superior to other models.","['https://openalex.org/W2097203679', 'https://openalex.org/W2136545725', 'https://openalex.org/W4372260088', 'https://openalex.org/W2972495969', 'https://openalex.org/W6841035593', 'https://openalex.org/W3209059054', 'https://openalex.org/W3180374548', 'https://openalex.org/W3140429000', 'https://openalex.org/W4287854499', 'https://openalex.org/W4372260139', 'https://openalex.org/W4372349107', 'https://openalex.org/W4385570550', 'https://openalex.org/W4372191700', 'https://openalex.org/W6852849868', 'https://openalex.org/W4392903062', 'https://openalex.org/W6783867762', 'https://openalex.org/W4385572318', 'https://openalex.org/W3213029956', 'https://openalex.org/W6771467084', 'https://openalex.org/W6810701745', 'https://openalex.org/W6780218876', 'https://openalex.org/W3198429080', 'https://openalex.org/W6838789019', 'https://openalex.org/W2997520586', 'https://openalex.org/W4378498579', 'https://openalex.org/W3092028330', 'https://openalex.org/W3036601975']",2024-03-18
https://openalex.org/W4403918744,https://doi.org/10.1109/jstsp.2024.3488557,Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations,"Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.","['https://openalex.org/W6853998256', 'https://openalex.org/W4389524500', 'https://openalex.org/W6790356757', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W2151083697', 'https://openalex.org/W4392931281', 'https://openalex.org/W3198217962', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W4381786045', 'https://openalex.org/W6848735303', 'https://openalex.org/W4390075359', 'https://openalex.org/W4386132131', 'https://openalex.org/W4392902611', 'https://openalex.org/W2752796333', 'https://openalex.org/W6855885476', 'https://openalex.org/W4372259964', 'https://openalex.org/W4287854499', 'https://openalex.org/W4377231659', 'https://openalex.org/W4385245566', 'https://openalex.org/W6864145738', 'https://openalex.org/W3180374548', 'https://openalex.org/W4385570550', 'https://openalex.org/W2995181338', 'https://openalex.org/W3163464523', 'https://openalex.org/W4289665794', 'https://openalex.org/W3095410713', 'https://openalex.org/W3081192838', 'https://openalex.org/W2935711438', 'https://openalex.org/W6853515095', 'https://openalex.org/W2130086727', 'https://openalex.org/W2107740512', 'https://openalex.org/W3196475561', 'https://openalex.org/W4225956675', 'https://openalex.org/W3037038648', 'https://openalex.org/W6805710207', 'https://openalex.org/W1494198834', 'https://openalex.org/W2030931454', 'https://openalex.org/W2972359262', 'https://openalex.org/W6810701745', 'https://openalex.org/W6783867762']",2024-10-30
https://openalex.org/W4385570468,https://doi.org/10.18653/v1/2023.findings-acl.509,Duplex Diffusion Models Improve Speech-to-Speech Translation,"Speech-to-speech translation is a typical sequence-to-sequence learning task that naturally has two directions. How to effectively leverage bidirectional supervision signals to produce high-fidelity audio for both directions? Existing approaches either train two separate models or a multitask-learned model with low efficiency and inferior performance. In this paper, we propose a duplex diffusion model that applies diffusion probabilistic models to both sides of a reversible duplex Conformer, so that either end can simultaneously input and output a distinct language’s speech. Our model enables reversible speech translation by simply flipping the input and output ends. Experiments show that our model achieves the first success of reversible speech translation with significant improvements of ASR-BLEU scores compared with a list of state-of-the-art baselines.","['https://openalex.org/W2933138175', 'https://openalex.org/W2939710050', 'https://openalex.org/W3036601975', 'https://openalex.org/W4320930577', 'https://openalex.org/W2144995019', 'https://openalex.org/W3043665049', 'https://openalex.org/W2963925437', 'https://openalex.org/W4296070387', 'https://openalex.org/W3092028330', 'https://openalex.org/W2136545725', 'https://openalex.org/W3034772996', 'https://openalex.org/W2127141656', 'https://openalex.org/W3012492057', 'https://openalex.org/W2101105183', 'https://openalex.org/W4226444650', 'https://openalex.org/W2995181338', 'https://openalex.org/W2605131327', 'https://openalex.org/W3168212167', 'https://openalex.org/W3001434439', 'https://openalex.org/W3169320628', 'https://openalex.org/W2036291627', 'https://openalex.org/W2737740651', 'https://openalex.org/W3107826490', 'https://openalex.org/W4312933868', 'https://openalex.org/W2962834855', 'https://openalex.org/W4385570550', 'https://openalex.org/W1538023239', 'https://openalex.org/W3097777922', 'https://openalex.org/W4287812455', 'https://openalex.org/W4385245566', 'https://openalex.org/W3129651364', 'https://openalex.org/W2807352297', 'https://openalex.org/W3096490862', 'https://openalex.org/W3180374548', 'https://openalex.org/W3036167779', 'https://openalex.org/W4394666973', 'https://openalex.org/W4221153524', 'https://openalex.org/W2973049979', 'https://openalex.org/W4287072252', 'https://openalex.org/W2567070169', 'https://openalex.org/W3140429000', 'https://openalex.org/W4226060115', 'https://openalex.org/W3162236869', 'https://openalex.org/W2097203679']",2023-01-01
https://openalex.org/W4392884616,https://doi.org/10.5121/ijci.2024.130201,Direct Punjabi to English Speech Translation using Discrete Units,"Speech-to-speech translation is yet to reach the same level of coverage as text-to-text translation systems. The current speech technology is highly limited in its coverage of over 7000 languages spoken worldwide, leaving more than half of the population deprived of such technology and shared experiences. With voice-assisted technology (such as social robots and speech-to-text apps) and auditory content (such as podcasts and lectures) on the rise, ensuring that the technology is available for all is more important than ever. Speech translation can play a vital role in mitigating technological disparity and creating a more inclusive society. With a motive to contribute towards speech translation research for low-resource languages, our work presents a direct speech-to-speech translation model for one of the Indic languages called Punjabi to English. Additionally, we explore the performance of using a discrete representation of speech called discrete acoustic units as input to the Transformer-based translation model. The model, abbreviated as Unit-to-Unit Translation (U2UT), takes a sequence of discrete units of the source language (the language being translated from) and outputs a sequence of discrete units of the target language (the language being translated to). Our results show that the U2UT model performs better than the Speechto-Unit Translation (S2UT) model by a 3.69 BLEU score.","['https://openalex.org/W6761353324', 'https://openalex.org/W6796730497', 'https://openalex.org/W6852909395', 'https://openalex.org/W4309129001', 'https://openalex.org/W6798080464', 'https://openalex.org/W4221164184', 'https://openalex.org/W4378505287', 'https://openalex.org/W2924093092', 'https://openalex.org/W4378473793', 'https://openalex.org/W4283121045', 'https://openalex.org/W6842730932', 'https://openalex.org/W2936184970', 'https://openalex.org/W4387162606', 'https://openalex.org/W3036601975', 'https://openalex.org/W3169320628', 'https://openalex.org/W4387687030', 'https://openalex.org/W3025165719', 'https://openalex.org/W6739901393', 'https://openalex.org/W6810419249', 'https://openalex.org/W4226543485', 'https://openalex.org/W4384648564', 'https://openalex.org/W4311731008', 'https://openalex.org/W3142316150', 'https://openalex.org/W4381827575', 'https://openalex.org/W4386566860', 'https://openalex.org/W3215465553', 'https://openalex.org/W2980109192', 'https://openalex.org/W4310079959', 'https://openalex.org/W4311550865', 'https://openalex.org/W4285077564', 'https://openalex.org/W3095410713', 'https://openalex.org/W2995929068', 'https://openalex.org/W3054645415', 'https://openalex.org/W2292087804', 'https://openalex.org/W3106807794', 'https://openalex.org/W3119308075', 'https://openalex.org/W2966095117', 'https://openalex.org/W4308756394', 'https://openalex.org/W4281621399', 'https://openalex.org/W4293332626', 'https://openalex.org/W1494198834', 'https://openalex.org/W4311000453', 'https://openalex.org/W6898505805', 'https://openalex.org/W6750200984', 'https://openalex.org/W2972495969', 'https://openalex.org/W4382202628', 'https://openalex.org/W3097777922', 'https://openalex.org/W3180374548', 'https://openalex.org/W4306393960', 'https://openalex.org/W3209059054', 'https://openalex.org/W3174758275', 'https://openalex.org/W3139878283', 'https://openalex.org/W4385570009', 'https://openalex.org/W4389524529', 'https://openalex.org/W4296070387', 'https://openalex.org/W4226444650', 'https://openalex.org/W4385245566', 'https://openalex.org/W4385570550', 'https://openalex.org/W4287854499', 'https://openalex.org/W4386142098', 'https://openalex.org/W2963532001', 'https://openalex.org/W4378105483', 'https://openalex.org/W4385571229', 'https://openalex.org/W3030437843', 'https://openalex.org/W4389518827', 'https://openalex.org/W4301980136', 'https://openalex.org/W4385572318', 'https://openalex.org/W4287887366', 'https://openalex.org/W4319862635', 'https://openalex.org/W2937197076']",2024-03-10
https://openalex.org/W4392909850,https://doi.org/10.1109/icassp48485.2024.10446620,M2BART: Multilingual and Multimodal Encoder-Decoder Pre-Training for Any-to-Any Machine Translation,"Speech and language models are advancing towards universality.A single model can now handle translations across 200 languages and transcriptions for over 100 languages.Universal models simplify development, deployment, and importantly, transfer knowledge to less-resourced languages or modes.This paper introduces M2BART, a streamlined multilingual and multimodal framework for encoderdecoder models.It employs a self-supervised speech tokenizer, bridging speech and text, and is pre-trained with a unified objective for both unimodal and multimodal, unsupervised and supervised data.When tested on Spanish-to-English and English-to-Hokkien translations, M2BART consistently surpassed competitors.We also showcase an innovative translation model enabling zero-shot transfers even without labeled data.","['https://openalex.org/W4285077564', 'https://openalex.org/W3173767661', 'https://openalex.org/W3180374548', 'https://openalex.org/W6841035593', 'https://openalex.org/W4287854499', 'https://openalex.org/W4392979802', 'https://openalex.org/W6788335241', 'https://openalex.org/W6803092890', 'https://openalex.org/W3034999214', 'https://openalex.org/W3209059054', 'https://openalex.org/W4223622550', 'https://openalex.org/W4385573012', 'https://openalex.org/W6810259195', 'https://openalex.org/W4226120743', 'https://openalex.org/W4372191700', 'https://openalex.org/W3205644108', 'https://openalex.org/W6769196770', 'https://openalex.org/W6846857581', 'https://openalex.org/W6790356757', 'https://openalex.org/W2962784628', 'https://openalex.org/W4226507725', 'https://openalex.org/W3140429000', 'https://openalex.org/W4296070387', 'https://openalex.org/W6780218876', 'https://openalex.org/W3119308075', 'https://openalex.org/W2995181338', 'https://openalex.org/W6769263558', 'https://openalex.org/W1494198834', 'https://openalex.org/W2799473636', 'https://openalex.org/W3095410713', 'https://openalex.org/W6771467084', 'https://openalex.org/W4385571229', 'https://openalex.org/W3203407300', 'https://openalex.org/W3196509775', 'https://openalex.org/W3015698636', 'https://openalex.org/W3198429080', 'https://openalex.org/W4385570550', 'https://openalex.org/W3207222250', 'https://openalex.org/W4221155340', 'https://openalex.org/W4307323391', 'https://openalex.org/W3101648800', 'https://openalex.org/W3030437843', 'https://openalex.org/W2979476256']",2024-03-18
https://openalex.org/W4386274141,https://doi.org/10.1109/iri58017.2023.00023,Speech-to-speech Low-resource Translation,"Speech-to-speech translation (S2ST), particularly in the context of low-resource languages, plays a vital role in facilitating global communication. However, comprehensive research in this emerging field is lacking, especially concerning translation without the use of text. The objective of this study is to bridge the gap by conducting a systematic review of existing literature on S2ST for low-resource languages. We discovered 455 articles by searching the Scopus, IEEE Xplore, and ACM Digital Library databases, focusing on identifying research trends. The results highlight significant topics covered in the literature, marking a transition from traditional neural network methodologies to advanced transformer-based models. Our findings provide a robust overview of the S2ST landscape, identifying challenges and potential solutions for future research, particularly regarding the application of this technology in low-resource settings. The research contribution of this study is the insights gleaned will benefit academics and professionals seeking a comprehensive understanding of S2ST for low-resource languages.","['https://openalex.org/W3125505924', 'https://openalex.org/W4372349107', 'https://openalex.org/W4285732289', 'https://openalex.org/W6804076346', 'https://openalex.org/W3146142859', 'https://openalex.org/W3165048362', 'https://openalex.org/W4372260139', 'https://openalex.org/W3175871055', 'https://openalex.org/W2973088264', 'https://openalex.org/W6802673433', 'https://openalex.org/W6739901393', 'https://openalex.org/W3015522062', 'https://openalex.org/W3008549139', 'https://openalex.org/W2910606374', 'https://openalex.org/W3014118193', 'https://openalex.org/W3040757906', 'https://openalex.org/W6810419249', 'https://openalex.org/W6798080464', 'https://openalex.org/W6847166416', 'https://openalex.org/W6796464841', 'https://openalex.org/W6847247304', 'https://openalex.org/W2963506925', 'https://openalex.org/W3169905056', 'https://openalex.org/W4287854499', 'https://openalex.org/W4385245566', 'https://openalex.org/W3208277898', 'https://openalex.org/W3214292156', 'https://openalex.org/W4385572318', 'https://openalex.org/W4385570550', 'https://openalex.org/W3180374548']",2023-08-01
https://openalex.org/W4394785933,https://doi.org/10.55041/ijsrem30665,Enhancing Study Experience using Handwritten Character and Digit Recognition and Text Summarization,"The integration of Handwritten characters and, Digit Recognition and Deep Learning in education heralds a transformative era in learning methodologies. This abstract delves into the multifaceted benefits derived from the amalgamation of these technologies, redefining the educational landscape. Handwritten characters and Digit Recognition technology facilitates the seamless digitization of handwritten content, transcending the limitations of manual note-taking. Its introduction into educational frameworks enhances accessibility, promotes organization, and augments the searchability of diverse educational materials. Deep Learning, acting in tandem with Handwriting Recognition, amplifies these advantages manifold. Deep learning powered study assistants offer personalized learning experiences tailored to individual needs, adapting to varied learning styles. Additionally, these assistants facilitate collaborative opportunities, providing real time feedback and evaluation tools that revolutionize the learning process. Key Words: Image recognition, CNN, RNN, LSTM, Neural Networks, SVM, Deep Learning, Convolutional layer, PreLU, ReLU, Text Summarization, Extractive text summarization, Tokenization.","['https://openalex.org/W3168760030', 'https://openalex.org/W3203275851', 'https://openalex.org/W4327773476', 'https://openalex.org/W4316658917', 'https://openalex.org/W4323338580', 'https://openalex.org/W4320559437', 'https://openalex.org/W4226153103', 'https://openalex.org/W4285072972', 'https://openalex.org/W4386590854']",2024-04-13
https://openalex.org/W4372260363,https://doi.org/10.1109/icassp49357.2023.10095704,Improving Noisy Student Training on Non-Target Domain Data for Automatic Speech Recognition,"Noisy Student Training (NST) has recently demonstrated extremely strong performance in Automatic Speech Recognition (ASR). In this paper, we propose a data selection strategy named LM Filter to improve the performance of NST on non-target domain data in ASR tasks. Hypotheses with and without a Language Model are generated and the CER differences between them are utilized as a filter threshold. Results reveal that significant improvements of 10.4% compared with no data filtering baselines. We can achieve 3.31% CER in AISHELL-1 test set, which is best result from our knowledge without any other supervised data. We also perform evaluations on the supervised 1000 hour AISHELL-2 dataset and competitive results of 4.73% CER can be achieved.","['https://openalex.org/W3160628828', 'https://openalex.org/W3026041220', 'https://openalex.org/W6780218876', 'https://openalex.org/W4296068755', 'https://openalex.org/W2088622183', 'https://openalex.org/W2111316763', 'https://openalex.org/W1993660824', 'https://openalex.org/W1975113979', 'https://openalex.org/W2062164080', 'https://openalex.org/W6784614252', 'https://openalex.org/W4297841894', 'https://openalex.org/W3203407300', 'https://openalex.org/W2963242190', 'https://openalex.org/W2936774411', 'https://openalex.org/W6754473786', 'https://openalex.org/W1588359339', 'https://openalex.org/W3213673948', 'https://openalex.org/W2143577772', 'https://openalex.org/W3197478142', 'https://openalex.org/W3097777922', 'https://openalex.org/W2101210369', 'https://openalex.org/W6768222176', 'https://openalex.org/W6681579320', 'https://openalex.org/W6761825139', 'https://openalex.org/W6770506093', 'https://openalex.org/W3035160371', 'https://openalex.org/W2982640424', 'https://openalex.org/W3093579165', 'https://openalex.org/W2976223659', 'https://openalex.org/W2889048668', 'https://openalex.org/W2145837098', 'https://openalex.org/W2991213871', 'https://openalex.org/W2943152387', 'https://openalex.org/W3036601975']",2023-05-05
https://openalex.org/W4389315131,https://doi.org/10.21437/chime.2023-9,NTT Multi-Speaker ASR System for the DASR Task of CHiME-7 Challenge,"We introduce our submission to the Distant automatic speech recognition (DSAR) task of the CHiME 7 challenge.Our system uses end-to-end diarization with vector clustering (EEND-VC), guided source separation (GSS), and attention-based encoder-decoder and transducer-based ASR systems.Our submission exploits pre-trained self-supervised learning (SSL) models to build strong diarization and ASR modules.We also explore data augmentation using contrastive data selection based on representations from SSL models.Besides, we use self-supervised adaptation (SSA) to adapt these modules to the recording conditions of each session.Our DASR system achieves a 36 % diarization error rate (DER) reduction and 47 % word error rate reduction (WER) over the baseline on the main track of the evaluation set and ranked third in the challenge.","['https://openalex.org/W2070707809', 'https://openalex.org/W3016400019', 'https://openalex.org/W4382319414', 'https://openalex.org/W3162648834', 'https://openalex.org/W3094122712', 'https://openalex.org/W4226069388', 'https://openalex.org/W2563666542', 'https://openalex.org/W3094706770', 'https://openalex.org/W3024869864', 'https://openalex.org/W3209984917', 'https://openalex.org/W2164502538', 'https://openalex.org/W1494198834', 'https://openalex.org/W6688816777', 'https://openalex.org/W2981087920', 'https://openalex.org/W2974231335', 'https://openalex.org/W3025165719', 'https://openalex.org/W4307933114', 'https://openalex.org/W4323207610', 'https://openalex.org/W4302010383', 'https://openalex.org/W4380993313', 'https://openalex.org/W1828163288', 'https://openalex.org/W4372266855', 'https://openalex.org/W7027429494', 'https://openalex.org/W3097758424', 'https://openalex.org/W4225311996', 'https://openalex.org/W2985913104', 'https://openalex.org/W3157070662', 'https://openalex.org/W3002397307', 'https://openalex.org/W6859224607', 'https://openalex.org/W6842938114', 'https://openalex.org/W4378501282', 'https://openalex.org/W4319862255', 'https://openalex.org/W3196857193', 'https://openalex.org/W4297841877', 'https://openalex.org/W2219249508', 'https://openalex.org/W3163903701', 'https://openalex.org/W3097777922', 'https://openalex.org/W3020336359', 'https://openalex.org/W3015783745', 'https://openalex.org/W4385823399', 'https://openalex.org/W3163560333', 'https://openalex.org/W3140898556', 'https://openalex.org/W4385823338', 'https://openalex.org/W4389315128', 'https://openalex.org/W4372260519', 'https://openalex.org/W4372183461', 'https://openalex.org/W2884797218', 'https://openalex.org/W3015636705', 'https://openalex.org/W4297841894', 'https://openalex.org/W4389315118', 'https://openalex.org/W2289394825']",2023-08-25
https://openalex.org/W4372341252,https://doi.org/10.1109/icassp49357.2023.10095264,Unsupervised Fine-Tuning Data Selection for ASR Using Self-Supervised Speech Models,"Self-supervised learning (SSL) has been able to leverage unlabeled data to boost the performance of automatic speech recognition (ASR) models when we have access to only a small amount of transcribed speech data. However, this raises the question of which subset of the available unlabeled data should be selected for transcription. Our work investigates different unsupervised data selection techniques for fine-tuning the HuBERT model under a limited transcription budget. We investigate the impact of speaker diversity, gender bias, and topic diversity on the downstream ASR performance. We also devise two novel techniques for unsupervised data selection: pre-training loss based data selection and the perplexity of byte pair encoded clustered units (PBPE) and we show how these techniques compare to pure random data selection. Finally, we analyze the correlations between the inherent characteristics of the selected fine-tuning subsets as well as how these characteristics correlate with the resultant word error rate. We demonstrate the importance of token diversity, speaker diversity, and topic diversity in achieving the best performance in terms of WER.","['https://openalex.org/W2995181338', 'https://openalex.org/W2799473636', 'https://openalex.org/W2933138175', 'https://openalex.org/W3209059054', 'https://openalex.org/W6780218876', 'https://openalex.org/W1494198834', 'https://openalex.org/W3194438854', 'https://openalex.org/W4224939570', 'https://openalex.org/W4297841894', 'https://openalex.org/W3102342027', 'https://openalex.org/W3036601975', 'https://openalex.org/W3101648800', 'https://openalex.org/W2963250244', 'https://openalex.org/W3030437843', 'https://openalex.org/W3198771897']",2023-05-05
https://openalex.org/W4411550778,https://doi.org/10.1109/access.2025.3582395,A Survey on Data Selection for Efficient Speech Processing,"While recent advances in speech processing have led to substantial performance improvements across diverse tasks, they often demand significantly higher computational costs and resources. To address this efficiency challenge, data selection has emerged as a crucial strategy. This survey provides a comprehensive overview and introduces a unifying taxonomy for data selection methods in speech processing, structured along three key dimensions: selection granularity (sample-level vs. segment-level), selection process (static, dynamic, or active learning), and selection criteria (uncertainty, diversity, or hybrid approaches). Through systematic analysis across major speech tasks, including automatic speech recognition, text-to-speech synthesis, audio anti-spoofing, speaker recognition, and emotion recognition, we evaluate the effectiveness and applicability of diverse data selection strategies. Our analysis reveals that targeted data selection not only alleviates computational burdens but often enhances model robustness and performance by strategically filtering redundant, noisy, or detrimental training examples. By synthesizing insights scattered across disparate speech domains, we identify common principles, highlight task-specific challenges, and reveal emerging research trends. Finally, we outline promising future research directions in data selection for efficient speech processing.","['https://openalex.org/W3200768929', 'https://openalex.org/W4205954114', 'https://openalex.org/W3014690389', 'https://openalex.org/W139267174', 'https://openalex.org/W4372272479', 'https://openalex.org/W2398014749', 'https://openalex.org/W2963683295', 'https://openalex.org/W2394984893', 'https://openalex.org/W2071355383', 'https://openalex.org/W4391021776', 'https://openalex.org/W2892309119', 'https://openalex.org/W4297841840', 'https://openalex.org/W4283661468', 'https://openalex.org/W350664472', 'https://openalex.org/W4391828777', 'https://openalex.org/W1979374416', 'https://openalex.org/W4317940367', 'https://openalex.org/W1534970351', 'https://openalex.org/W2056503573', 'https://openalex.org/W2022147808', 'https://openalex.org/W2146621297', 'https://openalex.org/W2888546277', 'https://openalex.org/W2110006374', 'https://openalex.org/W176617277', 'https://openalex.org/W2008955157', 'https://openalex.org/W4399800606', 'https://openalex.org/W4224939570', 'https://openalex.org/W4402112540', 'https://openalex.org/W4388692895', 'https://openalex.org/W4245118430', 'https://openalex.org/W3006921636', 'https://openalex.org/W4319862460', 'https://openalex.org/W2108687396', 'https://openalex.org/W2312116756', 'https://openalex.org/W3096077898', 'https://openalex.org/W2401326655', 'https://openalex.org/W4381198892', 'https://openalex.org/W7712974', 'https://openalex.org/W4406462011', 'https://openalex.org/W2770111097', 'https://openalex.org/W4402081939', 'https://openalex.org/W3160567372', 'https://openalex.org/W2161482971', 'https://openalex.org/W1537003061', 'https://openalex.org/W4407768711', 'https://openalex.org/W2113577323', 'https://openalex.org/W2935875038', 'https://openalex.org/W4405561555', 'https://openalex.org/W3162334527', 'https://openalex.org/W2753324760', 'https://openalex.org/W4306295041', 'https://openalex.org/W2049416212', 'https://openalex.org/W4285192675', 'https://openalex.org/W2797259383', 'https://openalex.org/W2047042030', 'https://openalex.org/W2294722231', 'https://openalex.org/W2394717182', 'https://openalex.org/W4391892620', 'https://openalex.org/W1967452917', 'https://openalex.org/W3010912466', 'https://openalex.org/W3194438854', 'https://openalex.org/W4405709596', 'https://openalex.org/W3003302592', 'https://openalex.org/W2972745048', 'https://openalex.org/W2181464176', 'https://openalex.org/W2963931415', 'https://openalex.org/W2032020816', 'https://openalex.org/W2142328794', 'https://openalex.org/W2513095863', 'https://openalex.org/W4287690992', 'https://openalex.org/W3213673948', 'https://openalex.org/W2292208301', 'https://openalex.org/W4385567291', 'https://openalex.org/W4408354270', 'https://openalex.org/W29612862', 'https://openalex.org/W2183343480', 'https://openalex.org/W2632855849', 'https://openalex.org/W4383878503', 'https://openalex.org/W2095629250', 'https://openalex.org/W2042700175', 'https://openalex.org/W4388821109', 'https://openalex.org/W1984985727', 'https://openalex.org/W56593604', 'https://openalex.org/W2090151917', 'https://openalex.org/W2661967629', 'https://openalex.org/W2166936558', 'https://openalex.org/W2154151056', 'https://openalex.org/W2149847971', 'https://openalex.org/W1509837333', 'https://openalex.org/W2888945202', 'https://openalex.org/W2246178008', 'https://openalex.org/W4403662188', 'https://openalex.org/W2922455739', 'https://openalex.org/W4402538002', 'https://openalex.org/W2398284433', 'https://openalex.org/W2395021687', 'https://openalex.org/W4402669778', 'https://openalex.org/W1984164833', 'https://openalex.org/W2024529640', 'https://openalex.org/W1491460777', 'https://openalex.org/W2996906606', 'https://openalex.org/W2407298348', 'https://openalex.org/W4322716750', 'https://openalex.org/W3191159189', 'https://openalex.org/W4391021756', 'https://openalex.org/W4392181767', 'https://openalex.org/W3001279689', 'https://openalex.org/W4287071136', 'https://openalex.org/W4399554695', 'https://openalex.org/W2500376910', 'https://openalex.org/W1576434095', 'https://openalex.org/W2401835717', 'https://openalex.org/W1889806910', 'https://openalex.org/W2027709829', 'https://openalex.org/W2296111744', 'https://openalex.org/W4372341252', 'https://openalex.org/W4297841894', 'https://openalex.org/W2430252546', 'https://openalex.org/W4396700080', 'https://openalex.org/W2148712804', 'https://openalex.org/W1996490534', 'https://openalex.org/W200293622', 'https://openalex.org/W284864749', 'https://openalex.org/W2889414197', 'https://openalex.org/W4296068833', 'https://openalex.org/W4386977657', 'https://openalex.org/W4206567542', 'https://openalex.org/W4290713748', 'https://openalex.org/W333690958', 'https://openalex.org/W2111897348', 'https://openalex.org/W4409883409', 'https://openalex.org/W1535656002', 'https://openalex.org/W2128986198', 'https://openalex.org/W4406858842', 'https://openalex.org/W4402706451', 'https://openalex.org/W4385822247', 'https://openalex.org/W2606538758', 'https://openalex.org/W4392931291', 'https://openalex.org/W3197762302', 'https://openalex.org/W3196630372', 'https://openalex.org/W1247580014', 'https://openalex.org/W4392904733', 'https://openalex.org/W2590420008', 'https://openalex.org/W2765864768', 'https://openalex.org/W4385569885', 'https://openalex.org/W2396822996', 'https://openalex.org/W3162425752', 'https://openalex.org/W4408356223', 'https://openalex.org/W4385807528', 'https://openalex.org/W2127499922', 'https://openalex.org/W2972561578', 'https://openalex.org/W1954397307', 'https://openalex.org/W2160815625', 'https://openalex.org/W2408234571', 'https://openalex.org/W2884733175', 'https://openalex.org/W2913512413', 'https://openalex.org/W3092459676', 'https://openalex.org/W2073536519', 'https://openalex.org/W2160056366', 'https://openalex.org/W4403662050', 'https://openalex.org/W2972787773', 'https://openalex.org/W2408513201', 'https://openalex.org/W2210331589', 'https://openalex.org/W2724838446', 'https://openalex.org/W2081074144']",2025-01-01
https://openalex.org/W4372349107,https://doi.org/10.1109/icassp49357.2023.10096797,Textless Direct Speech-to-Speech Translation with Discrete Speech Representation,"Research on speech-to-speech translation (S2ST) has progressed rapidly in recent years. Many end-to-end systems have been proposed and show advantages over conventional cascade systems, which are often composed of recognition, translation and synthesis sub-systems. However, most of end-to-end systems still rely on intermediate textual supervision during training, which makes it infeasible to work for languages without written forms. In this work, we propose a novel model, Textless Translatotron, which is based on Translatotron 2 [1], for training an end-to-end direct S2ST model without any textual supervision. Instead of jointly training with an auxiliary task predicting target phonemes as in Translatotron 2, the proposed model uses an auxiliary task predicting discrete speech representations which are obtained from learned or random speech quantizers. When a speech encoder pre-trained with unsupervised speech data is used for both models, the proposed model obtains translation quality nearly on-par with Translatotron 2 on the multilingual CVSS-C corpus [2] as well as the bilingual Fisher Spanish-English corpus [3]. On the latter, it outperforms the prior state-of-the-art textless model by +18.5 BLEU.","['https://openalex.org/W4287848452', 'https://openalex.org/W4280601369', 'https://openalex.org/W3175871055', 'https://openalex.org/W3007068036', 'https://openalex.org/W3026041220', 'https://openalex.org/W6760633627', 'https://openalex.org/W4221153524', 'https://openalex.org/W3142316150', 'https://openalex.org/W3213029956', 'https://openalex.org/W6810701745', 'https://openalex.org/W6841035593', 'https://openalex.org/W6802857679', 'https://openalex.org/W3180374548', 'https://openalex.org/W6838789019', 'https://openalex.org/W4287854499', 'https://openalex.org/W3209059054', 'https://openalex.org/W6780218876', 'https://openalex.org/W6810673746', 'https://openalex.org/W4226033575', 'https://openalex.org/W4296070387', 'https://openalex.org/W2842511635', 'https://openalex.org/W2752796333', 'https://openalex.org/W6784333009', 'https://openalex.org/W6810259195', 'https://openalex.org/W6631943919', 'https://openalex.org/W4285181910', 'https://openalex.org/W4296069143', 'https://openalex.org/W2972495969', 'https://openalex.org/W3037465386', 'https://openalex.org/W4388277788', 'https://openalex.org/W3207738474', 'https://openalex.org/W60702959', 'https://openalex.org/W3094502228', 'https://openalex.org/W1533861849', 'https://openalex.org/W4221161761', 'https://openalex.org/W4281789500', 'https://openalex.org/W2928941594', 'https://openalex.org/W4226444650', 'https://openalex.org/W4297808394', 'https://openalex.org/W4221155340', 'https://openalex.org/W2963799213', 'https://openalex.org/W4287072252', 'https://openalex.org/W3036601975', 'https://openalex.org/W3186200218']",2023-05-05
https://openalex.org/W4221153524,https://doi.org/10.21437/interspeech.2022-10938,Leveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation,"End-to-end speech-to-speech translation (S2ST) without relying on intermediate text representations is a rapidly emerging frontier of research.Recent works have demonstrated that the performance of such direct S2ST systems is approaching that of conventional cascade S2ST when trained on comparable datasets.However, in practice, the performance of direct S2ST is bounded by the availability of paired S2ST training data.In this work, we explore multiple approaches for leveraging much more widely available unsupervised and weakly-supervised speech and text data to improve the performance of direct S2ST based on Translatotron 2. With our most effective approaches, the average translation quality of direct S2ST on 21 language pairs on the CVSS-C corpus is improved by +13.6 BLEU (or +113% relatively), as compared to the previous state-of-the-art trained without additional data.The improvements on low-resource language are even more significant (+398% relatively on average).Our comparative studies suggest future research directions for S2ST and speech representation learning.","['https://openalex.org/W3091928890', 'https://openalex.org/W2257408573', 'https://openalex.org/W3030437843', 'https://openalex.org/W2970279348', 'https://openalex.org/W4287213456', 'https://openalex.org/W4300558631', 'https://openalex.org/W3207738474', 'https://openalex.org/W4221155340', 'https://openalex.org/W3213029956', 'https://openalex.org/W2972448360', 'https://openalex.org/W3011411500', 'https://openalex.org/W2914120296', 'https://openalex.org/W3097787369', 'https://openalex.org/W2292087804', 'https://openalex.org/W2958953787', 'https://openalex.org/W4287072252', 'https://openalex.org/W3207222250', 'https://openalex.org/W2928941594', 'https://openalex.org/W3036601975', 'https://openalex.org/W4297808394', 'https://openalex.org/W4285158119', 'https://openalex.org/W2903193068', 'https://openalex.org/W4287329822', 'https://openalex.org/W3026041220', 'https://openalex.org/W3197324626', 'https://openalex.org/W3196509775', 'https://openalex.org/W3175871055', 'https://openalex.org/W3173767661', 'https://openalex.org/W3169483174', 'https://openalex.org/W2605131327', 'https://openalex.org/W4385245566', 'https://openalex.org/W3119308075', 'https://openalex.org/W3169320628', 'https://openalex.org/W4226033575', 'https://openalex.org/W3095410713', 'https://openalex.org/W3007068036', 'https://openalex.org/W2964172053', 'https://openalex.org/W2560647685', 'https://openalex.org/W2964161387', 'https://openalex.org/W2964085268', 'https://openalex.org/W3213018012', 'https://openalex.org/W4287854499', 'https://openalex.org/W4226444650', 'https://openalex.org/W2963779652', 'https://openalex.org/W3097777922', 'https://openalex.org/W3142316150']",2022-09-16
https://openalex.org/W4372260052,https://doi.org/10.1109/icassp49357.2023.10095973,Enhancing Speech-To-Speech Translation with Multiple TTS Targets,"It has been known that direct speech-to-speech translation (S2ST) models usually suffer from the data scarcity issue because of the limited existing parallel materials for both source and target speech. Therefore to train a direct S2ST system, previous works usually utilize text-to-speech (TTS) systems to generate samples in the target language by augmenting the data from speech-to-text translation (S2TT). However, there is a limited investigation into how the synthesized target speech would affect the S2ST models. In this work, we analyze the effect of changing synthesized target speech for direct S2ST models. We find that simply combining the target speech from different TTS systems can potentially improve the S2ST performances. Following that, we also propose a multi-task framework that jointly optimizes the S2ST system with multiple targets from different TTS systems. Extensive experiments demonstrate that our proposed framework achieves consistent improvements (2.8 BLEU) over the baselines on the Fisher Spanish-English dataset.","['https://openalex.org/W206967138', 'https://openalex.org/W6802659129', 'https://openalex.org/W4287854499', 'https://openalex.org/W3016160783', 'https://openalex.org/W6810778535', 'https://openalex.org/W2963532001', 'https://openalex.org/W6810701745', 'https://openalex.org/W6757817989', 'https://openalex.org/W3161236344', 'https://openalex.org/W6783867762', 'https://openalex.org/W4296070387', 'https://openalex.org/W3001434439', 'https://openalex.org/W6802857679', 'https://openalex.org/W6917585676', 'https://openalex.org/W2154304575', 'https://openalex.org/W2139647714', 'https://openalex.org/W6778823374', 'https://openalex.org/W6784545093', 'https://openalex.org/W2058094241', 'https://openalex.org/W3197580070', 'https://openalex.org/W6790356757', 'https://openalex.org/W6777028661', 'https://openalex.org/W3209059054', 'https://openalex.org/W2152834109', 'https://openalex.org/W4285250921', 'https://openalex.org/W3207300132', 'https://openalex.org/W3140429000', 'https://openalex.org/W6796464841', 'https://openalex.org/W2964243274', 'https://openalex.org/W3015338123', 'https://openalex.org/W3142316150', 'https://openalex.org/W3180374548', 'https://openalex.org/W3175871055', 'https://openalex.org/W2972495969', 'https://openalex.org/W7061659707', 'https://openalex.org/W4221153524', 'https://openalex.org/W6841035593', 'https://openalex.org/W3024605872', 'https://openalex.org/W4226444650', 'https://openalex.org/W4224821740', 'https://openalex.org/W3092028330', 'https://openalex.org/W2242221029', 'https://openalex.org/W4394671563', 'https://openalex.org/W3207738474', 'https://openalex.org/W3091928890', 'https://openalex.org/W3169905056', 'https://openalex.org/W3033411150', 'https://openalex.org/W4287072252', 'https://openalex.org/W2908510526', 'https://openalex.org/W3206375275']",2023-05-05
https://openalex.org/W3213029956,https://doi.org/10.21437/interspeech.2022-143,XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale,"This paper presents XLS-R, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0.We train models with up to 2B parameters on nearly half a million hours of publicly available speech audio in 128 languages, an order of magnitude more public data than the largest known prior work.Our evaluation covers a wide range of tasks, domains, data regimes and languages, both high and low-resource.On the CoVoST-2 speech translation benchmark, we improve the previous state of the art by an average of 7.4 BLEU over 21 translation directions into English.For speech recognition, XLS-R improves over the best known prior work on BABEL, MLS, CommonVoice as well as VoxPopuli, lowering error rates by 14-34% relative on average.XLS-R also sets a new state of the art on VoxLin-gua107 language identification.Moreover, we show that with sufficient model size, cross-lingual pretraining can perform as well as English-only pretraining when translating English speech into other languages, a setting which favors monolingual pretraining.We hope XLS-R can help to improve speech processing tasks for many more languages of the world.Models and code are available at www.github.","['https://openalex.org/W3185293939', 'https://openalex.org/W2842511635', 'https://openalex.org/W2726515241', 'https://openalex.org/W3030437843', 'https://openalex.org/W2898630520', 'https://openalex.org/W2973049979', 'https://openalex.org/W3198429080', 'https://openalex.org/W2979476256', 'https://openalex.org/W3156643189', 'https://openalex.org/W2989539713', 'https://openalex.org/W3036601975', 'https://openalex.org/W1494198834', 'https://openalex.org/W3034978746', 'https://openalex.org/W3159481202', 'https://openalex.org/W2127141656', 'https://openalex.org/W2914120296', 'https://openalex.org/W2896457183', 'https://openalex.org/W3193521535', 'https://openalex.org/W2963403868', 'https://openalex.org/W3082274269', 'https://openalex.org/W3174724858', 'https://openalex.org/W3204696009', 'https://openalex.org/W2996159613', 'https://openalex.org/W4288089799', 'https://openalex.org/W3016181583', 'https://openalex.org/W4287391717', 'https://openalex.org/W3093517588', 'https://openalex.org/W2964121744', 'https://openalex.org/W3107826490', 'https://openalex.org/W2124509324', 'https://openalex.org/W3098903812', 'https://openalex.org/W3159134453', 'https://openalex.org/W2963027641', 'https://openalex.org/W2671812860', 'https://openalex.org/W3197580070', 'https://openalex.org/W3035390927', 'https://openalex.org/W2970119519', 'https://openalex.org/W2958953787', 'https://openalex.org/W1522301498', 'https://openalex.org/W3198771897', 'https://openalex.org/W3002741552', 'https://openalex.org/W2338908902', 'https://openalex.org/W3102342027', 'https://openalex.org/W2811079561', 'https://openalex.org/W2975381464', 'https://openalex.org/W3119866685', 'https://openalex.org/W4287213456', 'https://openalex.org/W3169483174', 'https://openalex.org/W4210463634', 'https://openalex.org/W2962942158', 'https://openalex.org/W2331143823', 'https://openalex.org/W2963341956', 'https://openalex.org/W4226033575', 'https://openalex.org/W3005680577', 'https://openalex.org/W2962784628', 'https://openalex.org/W3139878283', 'https://openalex.org/W2950170869', 'https://openalex.org/W2970049541', 'https://openalex.org/W4385245566', 'https://openalex.org/W2101105183', 'https://openalex.org/W3160525311', 'https://openalex.org/W3160799772', 'https://openalex.org/W3144173820', 'https://openalex.org/W3093579165', 'https://openalex.org/W2996383576', 'https://openalex.org/W4297808394', 'https://openalex.org/W2292087804', 'https://openalex.org/W3214173179', 'https://openalex.org/W3054645415', 'https://openalex.org/W3095410713', 'https://openalex.org/W2995181338', 'https://openalex.org/W3167533889', 'https://openalex.org/W3035579820', 'https://openalex.org/W3032816972', 'https://openalex.org/W2933138175', 'https://openalex.org/W2891555348', 'https://openalex.org/W3119308075', 'https://openalex.org/W3099782249', 'https://openalex.org/W3015213852', 'https://openalex.org/W3035524453', 'https://openalex.org/W2547875792', 'https://openalex.org/W4292779060', 'https://openalex.org/W3173767661', 'https://openalex.org/W2730658205']",2022-09-16
https://openalex.org/W3211278025,https://doi.org/10.1561/116.00000050,Recent Advances in End-to-End Automatic Speech Recognition,"Recently, the speech community is seeing a significant trend of moving from deep neural network based hybrid modeling to end-to-end (E2E) modeling for automatic speech recognition (ASR). While E2E models achieve the state-of-the-art results in most benchmarks in terms of ASR accuracy, hybrid models are still used in a large proportion of commercial ASR systems at the current time. There are lots of practical factors that affect the production model deployment decision. Traditional hybrid models, being optimized for production for decades, are usually good at these factors. Without providing excellent solutions to all these factors, it is hard for E2E models to be widely commercialized. In this paper, we will overview the recent advances in E2E models, focusing on technologies addressing those challenges from the industry’s perspective.","['https://openalex.org/W2526425061', 'https://openalex.org/W2973049979', 'https://openalex.org/W2964107261', 'https://openalex.org/W3094780479', 'https://openalex.org/W2962699523', 'https://openalex.org/W2972621414', 'https://openalex.org/W2521999726', 'https://openalex.org/W3193959417', 'https://openalex.org/W2147800946', 'https://openalex.org/W3161324588', 'https://openalex.org/W3152118754', 'https://openalex.org/W3197956343', 'https://openalex.org/W3095311338', 'https://openalex.org/W2102113734', 'https://openalex.org/W2973138437', 'https://openalex.org/W2973091309', 'https://openalex.org/W2972943112', 'https://openalex.org/W3097125541', 'https://openalex.org/W2746192915', 'https://openalex.org/W2911544293', 'https://openalex.org/W3097625183', 'https://openalex.org/W2608712415', 'https://openalex.org/W3097384956', 'https://openalex.org/W2327501763', 'https://openalex.org/W3008898571', 'https://openalex.org/W3016234571', 'https://openalex.org/W2949975180', 'https://openalex.org/W2962826786', 'https://openalex.org/W3097904285', 'https://openalex.org/W1978660892', 'https://openalex.org/W2963071736', 'https://openalex.org/W3197681820', 'https://openalex.org/W1989549063', 'https://openalex.org/W3167533889', 'https://openalex.org/W3008191852', 'https://openalex.org/W3196843354', 'https://openalex.org/W2963362078', 'https://openalex.org/W2996383576', 'https://openalex.org/W3156323585', 'https://openalex.org/W2972889948', 'https://openalex.org/W3112702554', 'https://openalex.org/W2939069254', 'https://openalex.org/W2398972335', 'https://openalex.org/W2221409856', 'https://openalex.org/W2972818416', 'https://openalex.org/W3196364802', 'https://openalex.org/W2962760690', 'https://openalex.org/W3134568285', 'https://openalex.org/W3196869722', 'https://openalex.org/W3097882114', 'https://openalex.org/W3194334755', 'https://openalex.org/W2938974662', 'https://openalex.org/W3096798607', 'https://openalex.org/W2940322076', 'https://openalex.org/W3196835955', 'https://openalex.org/W3162899666', 'https://openalex.org/W3095697114', 'https://openalex.org/W3162431424', 'https://openalex.org/W2962728618', 'https://openalex.org/W2520160253', 'https://openalex.org/W2460742184', 'https://openalex.org/W2095705004', 'https://openalex.org/W2894835365', 'https://openalex.org/W3093579165', 'https://openalex.org/W2605141709', 'https://openalex.org/W2972650231', 'https://openalex.org/W3195179105', 'https://openalex.org/W1992475611', 'https://openalex.org/W3096122506', 'https://openalex.org/W3094821064', 'https://openalex.org/W2127141656', 'https://openalex.org/W3042007685', 'https://openalex.org/W2963747784', 'https://openalex.org/W3170360470', 'https://openalex.org/W3137976833', 'https://openalex.org/W2560647685', 'https://openalex.org/W3145511196', 'https://openalex.org/W3099782249', 'https://openalex.org/W3163865502', 'https://openalex.org/W2294370754', 'https://openalex.org/W2889624961', 'https://openalex.org/W2963827914', 'https://openalex.org/W3198368663', 'https://openalex.org/W3204696009', 'https://openalex.org/W3147984406', 'https://openalex.org/W1994606281', 'https://openalex.org/W3008283340', 'https://openalex.org/W2627092829', 'https://openalex.org/W2963574857', 'https://openalex.org/W2963773971', 'https://openalex.org/W3097239815', 'https://openalex.org/W3185108477', 'https://openalex.org/W3146505093', 'https://openalex.org/W2776984718', 'https://openalex.org/W2972880214', 'https://openalex.org/W3136989387', 'https://openalex.org/W2963194026', 'https://openalex.org/W2962893195', 'https://openalex.org/W3015190365', 'https://openalex.org/W3016010032', 'https://openalex.org/W2962940707', 'https://openalex.org/W3015556645', 'https://openalex.org/W3197976714', 'https://openalex.org/W2402040300', 'https://openalex.org/W3097961301', 'https://openalex.org/W3096524176', 'https://openalex.org/W3011339933', 'https://openalex.org/W3007528493', 'https://openalex.org/W3160799772', 'https://openalex.org/W3196783077', 'https://openalex.org/W2886319145', 'https://openalex.org/W2786835190', 'https://openalex.org/W3006752097', 'https://openalex.org/W2972991710', 'https://openalex.org/W3015579786', 'https://openalex.org/W3198614518', 'https://openalex.org/W2293634267', 'https://openalex.org/W3094667432', 'https://openalex.org/W2890197052', 'https://openalex.org/W3160207687', 'https://openalex.org/W3096815019', 'https://openalex.org/W2697044473', 'https://openalex.org/W3162315798', 'https://openalex.org/W2515801922', 'https://openalex.org/W3015585292', 'https://openalex.org/W3025165719', 'https://openalex.org/W2951327905', 'https://openalex.org/W2904818793', 'https://openalex.org/W3095184753', 'https://openalex.org/W3167207712', 'https://openalex.org/W2964182776', 'https://openalex.org/W2963403868', 'https://openalex.org/W3162309234', 'https://openalex.org/W2963654155', 'https://openalex.org/W3092122846', 'https://openalex.org/W2589857635', 'https://openalex.org/W3007227084', 'https://openalex.org/W2964272710', 'https://openalex.org/W3150122400', 'https://openalex.org/W2064675550', 'https://openalex.org/W2973048981', 'https://openalex.org/W3198094329', 'https://openalex.org/W2952992734', 'https://openalex.org/W2890952074', 'https://openalex.org/W3198134544', 'https://openalex.org/W3141464856', 'https://openalex.org/W2963523217', 'https://openalex.org/W2973040747', 'https://openalex.org/W2913041008', 'https://openalex.org/W3008549139', 'https://openalex.org/W3205788551', 'https://openalex.org/W2160815625', 'https://openalex.org/W2889374926', 'https://openalex.org/W3008912312', 'https://openalex.org/W2972799770', 'https://openalex.org/W1982728343', 'https://openalex.org/W3008762051', 'https://openalex.org/W3184976814', 'https://openalex.org/W2157331557', 'https://openalex.org/W2888779557', 'https://openalex.org/W2963785710', 'https://openalex.org/W3158343646', 'https://openalex.org/W2963070863', 'https://openalex.org/W2750499125', 'https://openalex.org/W3198455051', 'https://openalex.org/W3008181812', 'https://openalex.org/W2889129739', 'https://openalex.org/W2973172693', 'https://openalex.org/W2605131327', 'https://openalex.org/W2121879602', 'https://openalex.org/W2963226322', 'https://openalex.org/W2741480356', 'https://openalex.org/W3197201410', 'https://openalex.org/W3080248383', 'https://openalex.org/W2911291251', 'https://openalex.org/W1586532344', 'https://openalex.org/W3112157188', 'https://openalex.org/W3015654466', 'https://openalex.org/W1821462560', 'https://openalex.org/W2962780374', 'https://openalex.org/W2962765220', 'https://openalex.org/W3096518646', 'https://openalex.org/W2963400424', 'https://openalex.org/W2892009249', 'https://openalex.org/W3148654612', 'https://openalex.org/W3163169798', 'https://openalex.org/W3016167541', 'https://openalex.org/W3180902893', 'https://openalex.org/W1915251500', 'https://openalex.org/W2913851961', 'https://openalex.org/W2963211739', 'https://openalex.org/W2964038834', 'https://openalex.org/W2936123380', 'https://openalex.org/W3149509723', 'https://openalex.org/W2962704885', 'https://openalex.org/W3160766462', 'https://openalex.org/W3198769601', 'https://openalex.org/W2963027641', 'https://openalex.org/W2963292011', 'https://openalex.org/W2739883972', 'https://openalex.org/W2972625221', 'https://openalex.org/W2748816379', 'https://openalex.org/W3197478142', 'https://openalex.org/W3007589762', 'https://openalex.org/W3198715852', 'https://openalex.org/W3128478537', 'https://openalex.org/W2963483399', 'https://openalex.org/W3163560333', 'https://openalex.org/W3015974384', 'https://openalex.org/W3205495812', 'https://openalex.org/W3015501067', 'https://openalex.org/W3015834770', 'https://openalex.org/W2949767489', 'https://openalex.org/W3163203022', 'https://openalex.org/W3189164341', 'https://openalex.org/W3096032230', 'https://openalex.org/W3178647810', 'https://openalex.org/W3097522836', 'https://openalex.org/W2937402758', 'https://openalex.org/W3161031484', 'https://openalex.org/W2982413405', 'https://openalex.org/W3148906368', 'https://openalex.org/W3016120074', 'https://openalex.org/W1922655562', 'https://openalex.org/W2964309797', 'https://openalex.org/W3096686110', 'https://openalex.org/W3008870153', 'https://openalex.org/W2799800213', 'https://openalex.org/W3007433671', 'https://openalex.org/W2971840980', 'https://openalex.org/W2899879954', 'https://openalex.org/W3015746570', 'https://openalex.org/W3198858282', 'https://openalex.org/W2962962542', 'https://openalex.org/W3157697407', 'https://openalex.org/W3021515889', 'https://openalex.org/W3018441253', 'https://openalex.org/W2921496354', 'https://openalex.org/W2963382687', 'https://openalex.org/W2592356860', 'https://openalex.org/W2025198378', 'https://openalex.org/W2901907199', 'https://openalex.org/W3197604215', 'https://openalex.org/W3161375121', 'https://openalex.org/W2973127116', 'https://openalex.org/W3162244132', 'https://openalex.org/W2885724687', 'https://openalex.org/W3008434450', 'https://openalex.org/W3016011332', 'https://openalex.org/W2912492482', 'https://openalex.org/W3198413388', 'https://openalex.org/W2972389417', 'https://openalex.org/W3015411292', 'https://openalex.org/W3047893908', 'https://openalex.org/W2600628583', 'https://openalex.org/W3097794466', 'https://openalex.org/W3186652349', 'https://openalex.org/W2766219058', 'https://openalex.org/W3094841848', 'https://openalex.org/W2936774411', 'https://openalex.org/W3202419788', 'https://openalex.org/W3152221657', 'https://openalex.org/W3162665866', 'https://openalex.org/W3140783160', 'https://openalex.org/W3140235797', 'https://openalex.org/W3096073522', 'https://openalex.org/W2916997151', 'https://openalex.org/W3097643313', 'https://openalex.org/W81726370', 'https://openalex.org/W3160551958', 'https://openalex.org/W3015194534', 'https://openalex.org/W3094979069', 'https://openalex.org/W3178203035', 'https://openalex.org/W3095822285', 'https://openalex.org/W3197898596', 'https://openalex.org/W2964084166', 'https://openalex.org/W3198442913', 'https://openalex.org/W1828163288', 'https://openalex.org/W3163300396', 'https://openalex.org/W2194775991', 'https://openalex.org/W2398042854', 'https://openalex.org/W2407080277', 'https://openalex.org/W3141756469', 'https://openalex.org/W3196435865', 'https://openalex.org/W3162293946', 'https://openalex.org/W3015671919', 'https://openalex.org/W3097338456', 'https://openalex.org/W2117671523', 'https://openalex.org/W3096702180', 'https://openalex.org/W3008008574', 'https://openalex.org/W3200955206', 'https://openalex.org/W2963303028', 'https://openalex.org/W3163793923', 'https://openalex.org/W3198004110', 'https://openalex.org/W3008037978', 'https://openalex.org/W3162847598', 'https://openalex.org/W3151526698', 'https://openalex.org/W2928941594', 'https://openalex.org/W3008174054', 'https://openalex.org/W3096123040', 'https://openalex.org/W3196784225', 'https://openalex.org/W2964308564']",2022-04-20
https://openalex.org/W4381786045,https://doi.org/10.1109/taslp.2023.3288409,AudioLM: A Language Modeling Approach to Audio Generation,"We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.","['https://openalex.org/W3197259906', 'https://openalex.org/W6840487619', 'https://openalex.org/W6839643428', 'https://openalex.org/W3160799772', 'https://openalex.org/W6739901393', 'https://openalex.org/W6809593508', 'https://openalex.org/W6790356757', 'https://openalex.org/W4292825791', 'https://openalex.org/W2995181338', 'https://openalex.org/W6734901337', 'https://openalex.org/W6802517614', 'https://openalex.org/W3148101939', 'https://openalex.org/W6810081322', 'https://openalex.org/W1494198834', 'https://openalex.org/W4226033575', 'https://openalex.org/W3215615641', 'https://openalex.org/W6755207826', 'https://openalex.org/W2395899413', 'https://openalex.org/W3037038648', 'https://openalex.org/W4287887366', 'https://openalex.org/W3198217962', 'https://openalex.org/W4296068815', 'https://openalex.org/W3097777922', 'https://openalex.org/W3180355996', 'https://openalex.org/W6810311916', 'https://openalex.org/W6776218486', 'https://openalex.org/W4312633146', 'https://openalex.org/W1728888090', 'https://openalex.org/W6778883912', 'https://openalex.org/W6782760101', 'https://openalex.org/W6783867762', 'https://openalex.org/W6767111847', 'https://openalex.org/W6783182287', 'https://openalex.org/W3095095816', 'https://openalex.org/W6783944145', 'https://openalex.org/W4226380987', 'https://openalex.org/W6771324808', 'https://openalex.org/W6798182279', 'https://openalex.org/W3144810982', 'https://openalex.org/W3162391496', 'https://openalex.org/W3209059054', 'https://openalex.org/W6769196770', 'https://openalex.org/W6748409065', 'https://openalex.org/W3131922516', 'https://openalex.org/W6769627184', 'https://openalex.org/W6735849998', 'https://openalex.org/W6762931180', 'https://openalex.org/W2972354707', 'https://openalex.org/W2963182577', 'https://openalex.org/W6843673214', 'https://openalex.org/W6805710207', 'https://openalex.org/W6800767084', 'https://openalex.org/W2752796333', 'https://openalex.org/W6768435317', 'https://openalex.org/W6844194202', 'https://openalex.org/W3186609711', 'https://openalex.org/W6780218876', 'https://openalex.org/W6755182157', 'https://openalex.org/W3198815374', 'https://openalex.org/W4292779060', 'https://openalex.org/W4283388932', 'https://openalex.org/W2604231067', 'https://openalex.org/W4221161768', 'https://openalex.org/W2896457183', 'https://openalex.org/W2996286887', 'https://openalex.org/W2519091744', 'https://openalex.org/W4224308101', 'https://openalex.org/W2995359496', 'https://openalex.org/W4288348042', 'https://openalex.org/W3123097577', 'https://openalex.org/W2963799213', 'https://openalex.org/W4287802874', 'https://openalex.org/W2950547518', 'https://openalex.org/W3129651364', 'https://openalex.org/W4385245566', 'https://openalex.org/W3092028330', 'https://openalex.org/W3177813494', 'https://openalex.org/W2979476256', 'https://openalex.org/W3206395542', 'https://openalex.org/W2593779438', 'https://openalex.org/W4226275767', 'https://openalex.org/W2970006822', 'https://openalex.org/W4309793872', 'https://openalex.org/W3036601975', 'https://openalex.org/W4225680573', 'https://openalex.org/W4394671563', 'https://openalex.org/W4288089799', 'https://openalex.org/W3198123200', 'https://openalex.org/W4294619240', 'https://openalex.org/W4298580827', 'https://openalex.org/W2971074500', 'https://openalex.org/W4297808394']",2023-01-01
https://openalex.org/W3169320628,https://doi.org/10.1109/taslp.2021.3122291,HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units,"Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.","['https://openalex.org/W2937090315', 'https://openalex.org/W6790168579', 'https://openalex.org/W6745117592', 'https://openalex.org/W2962850167', 'https://openalex.org/W2155273149', 'https://openalex.org/W2032210463', 'https://openalex.org/W3011411500', 'https://openalex.org/W3033038061', 'https://openalex.org/W6947929050', 'https://openalex.org/W3026041220', 'https://openalex.org/W2933138175', 'https://openalex.org/W3097777922', 'https://openalex.org/W2347098582', 'https://openalex.org/W6784637704', 'https://openalex.org/W6675022971', 'https://openalex.org/W3008525923', 'https://openalex.org/W2750248772', 'https://openalex.org/W6844194202', 'https://openalex.org/W3160799772', 'https://openalex.org/W6755207826', 'https://openalex.org/W2962739339', 'https://openalex.org/W6771917389', 'https://openalex.org/W2883725317', 'https://openalex.org/W3035524453', 'https://openalex.org/W2995181338', 'https://openalex.org/W1494198834', 'https://openalex.org/W6777232839', 'https://openalex.org/W6786669483', 'https://openalex.org/W6631190155', 'https://openalex.org/W2150593711', 'https://openalex.org/W6675354045', 'https://openalex.org/W6784436999', 'https://openalex.org/W2973157397', 'https://openalex.org/W6783797576', 'https://openalex.org/W6770514103', 'https://openalex.org/W3015522062', 'https://openalex.org/W6772883055', 'https://openalex.org/W4254197176', 'https://openalex.org/W3161101519', 'https://openalex.org/W3096338464', 'https://openalex.org/W6779997284', 'https://openalex.org/W6786614245', 'https://openalex.org/W6779326418', 'https://openalex.org/W6778883912', 'https://openalex.org/W6766673545', 'https://openalex.org/W6769311223', 'https://openalex.org/W6780483730', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W2972943112', 'https://openalex.org/W6780218876', 'https://openalex.org/W2982223350', 'https://openalex.org/W2110073835', 'https://openalex.org/W3035202887', 'https://openalex.org/W3016011332', 'https://openalex.org/W3003875258', 'https://openalex.org/W3015265920', 'https://openalex.org/W2127141656', 'https://openalex.org/W6784614252', 'https://openalex.org/W2888911345', 'https://openalex.org/W2752796333', 'https://openalex.org/W3093788532', 'https://openalex.org/W2164579587', 'https://openalex.org/W2996383576', 'https://openalex.org/W3025035610', 'https://openalex.org/W2965373594', 'https://openalex.org/W2101234009', 'https://openalex.org/W3135676170', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963618559', 'https://openalex.org/W3036601975', 'https://openalex.org/W3036224891', 'https://openalex.org/W3025165719', 'https://openalex.org/W3126816608', 'https://openalex.org/W2988736778', 'https://openalex.org/W3027083471', 'https://openalex.org/W3099782249', 'https://openalex.org/W2998532468', 'https://openalex.org/W3144810982', 'https://openalex.org/W3093579165', 'https://openalex.org/W2953190524', 'https://openalex.org/W2926827382', 'https://openalex.org/W2073459066', 'https://openalex.org/W1522301498', 'https://openalex.org/W2982399380', 'https://openalex.org/W3030163527', 'https://openalex.org/W3125709657', 'https://openalex.org/W3035060554', 'https://openalex.org/W3013571468', 'https://openalex.org/W3107668149', 'https://openalex.org/W3093533780', 'https://openalex.org/W3112034174', 'https://openalex.org/W2842511635', 'https://openalex.org/W2963799213', 'https://openalex.org/W2100768664', 'https://openalex.org/W1553004968']",2021-01-01
https://openalex.org/W4319302797,https://doi.org/10.1109/twc.2023.3240969,Deep Learning Enabled Semantic Communications With Speech Recognition and Synthesis,"In this paper, we develop a deep learning based semantic communication system for speech transmission, named DeepSC-ST. We take the speech recognition and speech synthesis as the transmission tasks of the communication system, respectively. First, the speech recognition-related semantic features are extracted for transmission by a joint semantic-channel encoder and the text is recovered at the receiver based on the received semantic features, which significantly reduces the required amount of data transmission without performance degradation. Then, we perform speech synthesis at the receiver, which dedicates to re-generate the speech signals by feeding the recognized text and the speaker information into a neural network module. To enable the DeepSC-ST adaptive to dynamic channel environments, we identify a robust model to cope with different channel conditions. According to the simulation results, the proposed DeepSC-ST significantly outperforms conventional communication systems and existing DL-enabled communication systems, especially in the low signal-to-noise ratio (SNR) regime. A software demonstration is further developed as a proof-of-concept of the DeepSC-ST.","['https://openalex.org/W3166791908', 'https://openalex.org/W2749651610', 'https://openalex.org/W4221153574', 'https://openalex.org/W2131774270', 'https://openalex.org/W6790617156', 'https://openalex.org/W2165291881', 'https://openalex.org/W3200514756', 'https://openalex.org/W6778823374', 'https://openalex.org/W6779337556', 'https://openalex.org/W3206039297', 'https://openalex.org/W2975414524', 'https://openalex.org/W6796587295', 'https://openalex.org/W2127141656', 'https://openalex.org/W4312051474', 'https://openalex.org/W4313350220', 'https://openalex.org/W3102125291', 'https://openalex.org/W4226115138', 'https://openalex.org/W3015282541', 'https://openalex.org/W2964243274', 'https://openalex.org/W1499332833', 'https://openalex.org/W6749489859', 'https://openalex.org/W2150658333', 'https://openalex.org/W2428180336', 'https://openalex.org/W1576227399', 'https://openalex.org/W2102003408', 'https://openalex.org/W2963609956', 'https://openalex.org/W2963174256', 'https://openalex.org/W2615762555', 'https://openalex.org/W3036851434', 'https://openalex.org/W2886374543', 'https://openalex.org/W3199651421', 'https://openalex.org/W6810966997', 'https://openalex.org/W3097777922', 'https://openalex.org/W3160799772', 'https://openalex.org/W3016011332', 'https://openalex.org/W2962760690', 'https://openalex.org/W2963827914', 'https://openalex.org/W2293634267', 'https://openalex.org/W6675365184', 'https://openalex.org/W6780218876', 'https://openalex.org/W2964084166', 'https://openalex.org/W4210330825', 'https://openalex.org/W3016010032', 'https://openalex.org/W3007227084', 'https://openalex.org/W2125838338', 'https://openalex.org/W4317350116', 'https://openalex.org/W2022011789', 'https://openalex.org/W1993882792', 'https://openalex.org/W3214462705', 'https://openalex.org/W4226047696', 'https://openalex.org/W6637108112', 'https://openalex.org/W2143612262', 'https://openalex.org/W2160815625', 'https://openalex.org/W6687566353', 'https://openalex.org/W2284321898', 'https://openalex.org/W4233996382', 'https://openalex.org/W2060108852', 'https://openalex.org/W3033411150', 'https://openalex.org/W1686946872', 'https://openalex.org/W2193413348', 'https://openalex.org/W2102113734', 'https://openalex.org/W4287761884', 'https://openalex.org/W3036601975', 'https://openalex.org/W2901997113', 'https://openalex.org/W2996286887', 'https://openalex.org/W2963691546', 'https://openalex.org/W2519091744', 'https://openalex.org/W2993383518', 'https://openalex.org/W3168068147', 'https://openalex.org/W4226150551', 'https://openalex.org/W3128413826']",2023-02-06
https://openalex.org/W3140429000,https://doi.org/10.21437/interspeech.2021-475,Speech Resynthesis from Discrete Disentangled Self-Supervised Representations,"We propose using self-supervised discrete representations for the task of\nspeech resynthesis. To generate disentangled representation, we separately\nextract low-bitrate representations for speech content, prosodic information,\nand speaker identity. This allows to synthesize speech in a controllable\nmanner. We analyze various state-of-the-art, self-supervised representation\nlearning methods and shed light on the advantages of each method while\nconsidering reconstruction quality and disentanglement properties.\nSpecifically, we evaluate the F0 reconstruction, speaker identification\nperformance (for both resynthesis and voice conversion), recordings'\nintelligibility, and overall quality using subjective human evaluation. Lastly,\nwe demonstrate how these representations can be used for an ultra-lightweight\nspeech codec. Using the obtained representations, we can get to a rate of 365\nbits per second while providing better speech quality than the baseline\nmethods. Audio samples can be found under the following link:\nspeechbot.github.io/resynthesis.\n","['https://openalex.org/W2963618559', 'https://openalex.org/W2890983311', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963799213', 'https://openalex.org/W2973049979', 'https://openalex.org/W2963300588', 'https://openalex.org/W2097645910', 'https://openalex.org/W3095948607', 'https://openalex.org/W2944079609', 'https://openalex.org/W2995181338', 'https://openalex.org/W2292235217', 'https://openalex.org/W3096216486', 'https://openalex.org/W2972867623', 'https://openalex.org/W2964167449', 'https://openalex.org/W2935711438', 'https://openalex.org/W2940544976', 'https://openalex.org/W1494198834', 'https://openalex.org/W3210177631', 'https://openalex.org/W2808631503', 'https://openalex.org/W2120847449', 'https://openalex.org/W3021164770', 'https://openalex.org/W3096656254', 'https://openalex.org/W3148101939', 'https://openalex.org/W1885680957', 'https://openalex.org/W2115098197', 'https://openalex.org/W1498609987', 'https://openalex.org/W3093427098', 'https://openalex.org/W3016098186', 'https://openalex.org/W3099782249', 'https://openalex.org/W3095361818', 'https://openalex.org/W2130086727', 'https://openalex.org/W2527729766', 'https://openalex.org/W2842511635', 'https://openalex.org/W3025878903', 'https://openalex.org/W2924551963', 'https://openalex.org/W3096323553', 'https://openalex.org/W2949382160', 'https://openalex.org/W2750248772', 'https://openalex.org/W3163296124', 'https://openalex.org/W3098403858', 'https://openalex.org/W2963341956', 'https://openalex.org/W2775336875', 'https://openalex.org/W1959608418', 'https://openalex.org/W3160799772', 'https://openalex.org/W2114925438', 'https://openalex.org/W2963091184', 'https://openalex.org/W2107740512']",2021-08-27
https://openalex.org/W3203407300,https://doi.org/10.1109/icassp43922.2022.9746682,WENETSPEECH: A 10000+ Hours Multi-Domain Mandarin Corpus for Speech Recognition,"In this paper, we present WenetSpeech, a multi-domain Mandarin corpus consisting of 10000+ hours high-quality labeled speech, 2400+ hours weakly labeled speech, and about 10000 hours unlabeled speech, with 22400+ hours in total. We collect the data from YouTube and Podcast, which covers a variety of speaking styles, scenarios, domains, topics and noisy conditions. An optical character recognition (OCR) method is introduced to generate the audio/text segmentation candidates for the YouTube data on the corresponding video subtitles, while a high-quality ASR transcription system is used to generate audio/text pair candidates for the Podcast data. Then we propose a novel end-to-end label error detection approach to further validate and filter the candidates. We also provide three manually labelled high-quality test sets along with WenetSpeech for evaluation – Dev for cross-validation purpose in training, Test_Net, collected from Internet for matched test, and Test_Meeting, recorded from real meetings for more challenging mismatched test. Baseline systems trained with WenetSpeech are provided for three popular speech recognition toolkits, namely Kaldi, ESPnet, and WeNet, and recognition results on the three test sets are also provided as benchmarks. To the best of our knowledge, WenetSpeech is the current largest open-source Mandarin speech corpus with transcriptions, which benefits research on production-level speech recognition.","['https://openalex.org/W2739883972', 'https://openalex.org/W6739901393', 'https://openalex.org/W2514741789', 'https://openalex.org/W2963211739', 'https://openalex.org/W2194187530', 'https://openalex.org/W3163793923', 'https://openalex.org/W2936774411', 'https://openalex.org/W2802023636', 'https://openalex.org/W2888867175', 'https://openalex.org/W2327501763', 'https://openalex.org/W6780815891', 'https://openalex.org/W6728030952', 'https://openalex.org/W2892009249', 'https://openalex.org/W3097777922', 'https://openalex.org/W6631362777', 'https://openalex.org/W2158791054', 'https://openalex.org/W2933138175', 'https://openalex.org/W2962780374', 'https://openalex.org/W3197478142', 'https://openalex.org/W2973049979', 'https://openalex.org/W3198429080', 'https://openalex.org/W6687566353', 'https://openalex.org/W6754473786', 'https://openalex.org/W2127141656', 'https://openalex.org/W2143612262', 'https://openalex.org/W2519818067', 'https://openalex.org/W6638749077', 'https://openalex.org/W6796463219', 'https://openalex.org/W3151526698', 'https://openalex.org/W2147768505', 'https://openalex.org/W6623517193', 'https://openalex.org/W2160815625', 'https://openalex.org/W6780218876', 'https://openalex.org/W6795952400', 'https://openalex.org/W3160799772', 'https://openalex.org/W2963242190', 'https://openalex.org/W1494198834', 'https://openalex.org/W3198694222', 'https://openalex.org/W3095410713', 'https://openalex.org/W1828163288', 'https://openalex.org/W2193413348', 'https://openalex.org/W2963403868', 'https://openalex.org/W3099782249', 'https://openalex.org/W2526425061', 'https://openalex.org/W2184045248', 'https://openalex.org/W3128442956', 'https://openalex.org/W3042170933', 'https://openalex.org/W3036601975', 'https://openalex.org/W3168612151', 'https://openalex.org/W3165666670', 'https://openalex.org/W3037057938', 'https://openalex.org/W2889048668', 'https://openalex.org/W3169688220', 'https://openalex.org/W1524333225', 'https://openalex.org/W4385245566', 'https://openalex.org/W3197917733', 'https://openalex.org/W854541894', 'https://openalex.org/W4287173589']",2022-04-27
https://openalex.org/W4223622550,https://doi.org/10.18653/v1/2022.acl-long.105,Unified Speech-Text Pre-training for Speech Translation and Recognition,"Yun Tang, Hongyu Gong, Ning Dong, Changhan Wang, Wei-Ning Hsu, Jiatao Gu, Alexei Baevski, Xian Li, Abdelrahman Mohamed, Michael Auli, Juan Pino. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.","['https://openalex.org/W2979476256', 'https://openalex.org/W3096490862', 'https://openalex.org/W3093579165', 'https://openalex.org/W3034772996', 'https://openalex.org/W2963250244', 'https://openalex.org/W3015522062', 'https://openalex.org/W3162037819', 'https://openalex.org/W3035003500', 'https://openalex.org/W3161101519', 'https://openalex.org/W3173767661', 'https://openalex.org/W2962739339', 'https://openalex.org/W2995181338', 'https://openalex.org/W3096109555', 'https://openalex.org/W3178203035', 'https://openalex.org/W3176711365', 'https://openalex.org/W3148001440', 'https://openalex.org/W2964161387', 'https://openalex.org/W3035202887', 'https://openalex.org/W2963532001', 'https://openalex.org/W1524333225', 'https://openalex.org/W2963779652', 'https://openalex.org/W3207222250', 'https://openalex.org/W4394666973', 'https://openalex.org/W3036601975', 'https://openalex.org/W4210690962', 'https://openalex.org/W3034999214', 'https://openalex.org/W4287213456', 'https://openalex.org/W3096338464', 'https://openalex.org/W1522301498', 'https://openalex.org/W2901389167', 'https://openalex.org/W4287329822', 'https://openalex.org/W3054645415', 'https://openalex.org/W1494198834', 'https://openalex.org/W2896457183', 'https://openalex.org/W3160799772', 'https://openalex.org/W4308349017', 'https://openalex.org/W2936774411', 'https://openalex.org/W3107826490', 'https://openalex.org/W2804648901', 'https://openalex.org/W4226033575', 'https://openalex.org/W4297808394']",2022-01-01
https://openalex.org/W4375869259,https://doi.org/10.1109/icassp49357.2023.10096149,Comparative Layer-Wise Analysis of Self-Supervised Speech Models,"Many self-supervised speech models, varying in their pre-training objective, input modality, and pre-training data, have been proposed in the last few years. Despite impressive successes on downstream tasks, we still have a limited understanding of the properties encoded by the models and the differences across models. In this work, we examine the intermediate representations for a variety of recent models. Specifically, we measure acoustic, phonetic, and word-level properties encoded in individual layers, using a lightweight analysis tool based on canonical correlation analysis (CCA). We find that these properties evolve across layers differently depending on the model, and the variations relate to the choice of pre-training objective. We further investigate the utility of our analyses for downstream tasks by comparing the property trends with performance on speech recognition and spoken language understanding tasks. We discover that CCA trends provide reliable guidance to choose layers of interest for downstream tasks and that single-layer performance often matches or improves upon using all layers, suggesting implications for more efficient use of pre-trained models. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>","['https://openalex.org/W6780218876', 'https://openalex.org/W3200287550', 'https://openalex.org/W6809593508', 'https://openalex.org/W3198429080', 'https://openalex.org/W4226380987', 'https://openalex.org/W6839364956', 'https://openalex.org/W6811170316', 'https://openalex.org/W4281492411', 'https://openalex.org/W3209984917', 'https://openalex.org/W3160799772', 'https://openalex.org/W4237723258', 'https://openalex.org/W6810168380', 'https://openalex.org/W3200129129', 'https://openalex.org/W6845712036', 'https://openalex.org/W6845628702', 'https://openalex.org/W3162133897', 'https://openalex.org/W6752726010', 'https://openalex.org/W3100460087', 'https://openalex.org/W6629717138', 'https://openalex.org/W6764072591', 'https://openalex.org/W4385574560', 'https://openalex.org/W2970726176', 'https://openalex.org/W6788328058', 'https://openalex.org/W6839512648', 'https://openalex.org/W6845853456', 'https://openalex.org/W3217031383', 'https://openalex.org/W3197580070', 'https://openalex.org/W3198275944', 'https://openalex.org/W4285250921', 'https://openalex.org/W2951025380', 'https://openalex.org/W4283694096', 'https://openalex.org/W1494198834', 'https://openalex.org/W4221153068', 'https://openalex.org/W4294103325', 'https://openalex.org/W3036601975', 'https://openalex.org/W4375869060', 'https://openalex.org/W4226103796', 'https://openalex.org/W4307536852', 'https://openalex.org/W4280638376', 'https://openalex.org/W4226199158', 'https://openalex.org/W4221161768', 'https://openalex.org/W3121914243', 'https://openalex.org/W4319862479']",2023-05-05
https://openalex.org/W3205533980,https://doi.org/10.1109/icassp43922.2022.9746929,Wav2vec-Switch: Contrastive Learning from Original-Noisy Speech Pairs for Robust Speech Recognition,"The goal of self-supervised learning (SSL) for automatic speech recognition (ASR) is to learn good speech representations from a large amount of unlabeled speech for the downstream ASR task. However, most SSL frameworks do not consider noise robustness which is crucial for real-world applications. In this paper we propose wav2vec-Switch, a method to encode noise robustness into contextualized representations of speech via contrastive learning. Specifically, we feed original-noisy speech pairs simultaneously into the wav2vec 2.0 network. In addition to the existing contrastive learning task, we switch the quantized representations of the original and noisy speech as additional prediction targets of each other. By doing this, it enforces the network to have consistent predictions for the original and noisy speech, thus allows to learn contextualized representation with noise robustness. Our experiments on synthe-sized and real noisy data show the effectiveness of our method: it achieves 2.9–4.9% relative word error rate (WER) reduction on the synthesized noisy LibriSpeech data without deterioration on the original data, and 5.7% on CHiME-4 real 1-channel noisy data compared to a data augmentation baseline even with a strong language model for decoding. Our results on CHiME-4 can match or even surpass those with well-designed speech enhancement components.","['https://openalex.org/W2933138175', 'https://openalex.org/W2024490156', 'https://openalex.org/W6688816777', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962892438', 'https://openalex.org/W3008912312', 'https://openalex.org/W2982223350', 'https://openalex.org/W3015265920', 'https://openalex.org/W6788335241', 'https://openalex.org/W3160799772', 'https://openalex.org/W6769196770', 'https://openalex.org/W6780218876', 'https://openalex.org/W4226033575', 'https://openalex.org/W6784614252', 'https://openalex.org/W2559260703', 'https://openalex.org/W2124509324', 'https://openalex.org/W2962894366', 'https://openalex.org/W6729448088', 'https://openalex.org/W2680270903', 'https://openalex.org/W2802248956', 'https://openalex.org/W2127141656', 'https://openalex.org/W3096710170', 'https://openalex.org/W2973049979', 'https://openalex.org/W2842511635', 'https://openalex.org/W2399016933', 'https://openalex.org/W3016011332', 'https://openalex.org/W2963522845', 'https://openalex.org/W3015312544', 'https://openalex.org/W2998616931', 'https://openalex.org/W3032514799', 'https://openalex.org/W6773243159', 'https://openalex.org/W3008762051', 'https://openalex.org/W6739901393', 'https://openalex.org/W3197411683', 'https://openalex.org/W3036601975', 'https://openalex.org/W2219249508', 'https://openalex.org/W4297808394', 'https://openalex.org/W2979476256', 'https://openalex.org/W2547875792', 'https://openalex.org/W3015213852', 'https://openalex.org/W3099782249', 'https://openalex.org/W2963403868', 'https://openalex.org/W2996383576', 'https://openalex.org/W4385245566', 'https://openalex.org/W3167207712', 'https://openalex.org/W3093579165']",2022-04-27
https://openalex.org/W4319862670,https://doi.org/10.1109/slt54892.2023.10023187,Towards End-to-End Unsupervised Speech Recognition,"Unsupervised speech recognition has shown great potential to make Automatic Speech Recognition (ASR) systems accessible to every language. However, existing methods still heavily rely on hand-crafted pre-processing. Similar to the trend of making supervised speech recognition end-to-end, we introduce wav2vec-U 2.0 which does away with all audio-side pre-processing and improves accuracy through better architecture. In addition, we introduce an auxiliary self-supervised objective that ties model predictions back to the input. Experiments show that wav2vec-U 2.0 improves unsupervised recognition results across different languages while being conceptually simpler.","['https://openalex.org/W6917638038', 'https://openalex.org/W6779919476', 'https://openalex.org/W2933138175', 'https://openalex.org/W2954930777', 'https://openalex.org/W6780218876', 'https://openalex.org/W2730658205', 'https://openalex.org/W3096831136', 'https://openalex.org/W6754671720', 'https://openalex.org/W6773553514', 'https://openalex.org/W6760519848', 'https://openalex.org/W3015419784', 'https://openalex.org/W3096656254', 'https://openalex.org/W6762242920', 'https://openalex.org/W3095173472', 'https://openalex.org/W3097777922', 'https://openalex.org/W6638667902', 'https://openalex.org/W6678720029', 'https://openalex.org/W6735913928', 'https://openalex.org/W1778492285', 'https://openalex.org/W6757424787', 'https://openalex.org/W6687566353', 'https://openalex.org/W3026041220', 'https://openalex.org/W3096338464', 'https://openalex.org/W6784614252', 'https://openalex.org/W2802422770', 'https://openalex.org/W3160799772', 'https://openalex.org/W6770506093', 'https://openalex.org/W2936774411', 'https://openalex.org/W6771467084', 'https://openalex.org/W1494198834', 'https://openalex.org/W2951974815', 'https://openalex.org/W2802557066', 'https://openalex.org/W6751433836', 'https://openalex.org/W6750012685', 'https://openalex.org/W6795952400', 'https://openalex.org/W6757699909', 'https://openalex.org/W2046932483', 'https://openalex.org/W2962799225', 'https://openalex.org/W3016181583', 'https://openalex.org/W3036601975', 'https://openalex.org/W2952711665', 'https://openalex.org/W2904818793', 'https://openalex.org/W2934852845', 'https://openalex.org/W2964079874', 'https://openalex.org/W3198429080', 'https://openalex.org/W2193413348', 'https://openalex.org/W1836465849', 'https://openalex.org/W3093579165', 'https://openalex.org/W2125529971', 'https://openalex.org/W2991213871', 'https://openalex.org/W4295521014', 'https://openalex.org/W4287173589', 'https://openalex.org/W2890536590', 'https://openalex.org/W3030437843', 'https://openalex.org/W2804648901']",2023-01-09
https://openalex.org/W4226390724,https://doi.org/10.21437/interspeech.2022-10839,"End-to-End Integration of Speech Recognition, Speech Enhancement, and Self-Supervised Learning Representation","This work presents our end-to-end (E2E) automatic speech recognition (ASR) model targetting at robust speech recognition, called Integraded speech Recognition with enhanced speech Input for Self-supervised learning representation (IRIS).Compared with conventional E2E ASR models, the proposed E2E model integrates two important modules including a speech enhancement (SE) module and a self-supervised learning representation (SSLR) module.The SE module enhances the noisy speech.Then the SSLR module extracts features from enhanced speech to be used for speech recognition (ASR).To train the proposed model, we establish an efficient learning scheme.Evaluation results on the monaural CHiME-4 task show that the IRIS model achieves the best performance reported in the literature for the single-channel CHiME-4 benchmark (2.0% for the real development and 3.9% for the real test) thanks to the powerful pre-trained SSLR module and the finetuned SE module.","['https://openalex.org/W2160815625', 'https://openalex.org/W2936774411', 'https://openalex.org/W2143612262', 'https://openalex.org/W4221138681', 'https://openalex.org/W3015199127', 'https://openalex.org/W4287120025', 'https://openalex.org/W2042141988', 'https://openalex.org/W4286893829', 'https://openalex.org/W3160799772', 'https://openalex.org/W2937484199', 'https://openalex.org/W2954695182', 'https://openalex.org/W2526425061', 'https://openalex.org/W2995181338', 'https://openalex.org/W2952218014', 'https://openalex.org/W3032514799', 'https://openalex.org/W3119308075', 'https://openalex.org/W2962892438', 'https://openalex.org/W3036601975', 'https://openalex.org/W2998616931', 'https://openalex.org/W2559260703', 'https://openalex.org/W4226380987', 'https://openalex.org/W1495679096', 'https://openalex.org/W2510867321', 'https://openalex.org/W4287989344', 'https://openalex.org/W1855892484', 'https://openalex.org/W2962935966', 'https://openalex.org/W3093579165', 'https://openalex.org/W1922655562', 'https://openalex.org/W3209984917', 'https://openalex.org/W4285761032', 'https://openalex.org/W2962780374', 'https://openalex.org/W2963980003', 'https://openalex.org/W4297841603']",2022-09-16
https://openalex.org/W4297841875,https://doi.org/10.21437/interspeech.2022-10674,Cross-lingual Self-Supervised Speech Representations for Improved Dysarthric Speech Recognition,"State-of-the-art automatic speech recognition (ASR) systems perform well on healthy speech.However, the performance on impaired speech still remains an issue.The current study explores the usefulness of using Wav2Vec self-supervised speech representations as features for training an ASR system for dysarthric speech.Dysarthric speech recognition is particularly difficult as several aspects of speech such as articulation, prosody and phonation can be impaired.Specifically, we train an acoustic model with features extracted from Wav2Vec, Hubert, and the cross-lingual XLSR model.Results suggest that speech representations pretrained on large unlabelled data can improve word error rate (WER) performance.In particular, features from the multilingual model led to lower WERs than filterbanks (Fbank) or models trained on a single language.Improvements were observed in English speakers with cerebral palsy caused dysarthria (UASpeech corpus), Spanish speakers with Parkinsonian dysarthria (PC-GITA corpus) and Italian speakers with paralysis-based dysarthria (EasyCall corpus).Compared to using Fbank features, XLSR-based features reduced WERs by 6.8%, 22.0%, and 7.0% for the UASpeech, PC-GITA, and EasyCall corpus, respectively.","['https://openalex.org/W3198529186', 'https://openalex.org/W2549411651', 'https://openalex.org/W2962780374', 'https://openalex.org/W2251281161', 'https://openalex.org/W2187089797', 'https://openalex.org/W3196495667', 'https://openalex.org/W3198786495', 'https://openalex.org/W2140978740', 'https://openalex.org/W2890681361', 'https://openalex.org/W2065506310', 'https://openalex.org/W2888348876', 'https://openalex.org/W3197646400', 'https://openalex.org/W4287553982', 'https://openalex.org/W180052447', 'https://openalex.org/W3041561163', 'https://openalex.org/W2798571323', 'https://openalex.org/W3036601975', 'https://openalex.org/W2936861580', 'https://openalex.org/W2064610785', 'https://openalex.org/W3198429080', 'https://openalex.org/W2999996462', 'https://openalex.org/W3097909406', 'https://openalex.org/W2031072412', 'https://openalex.org/W3196525293', 'https://openalex.org/W2888807255', 'https://openalex.org/W3197642003', 'https://openalex.org/W3097777922', 'https://openalex.org/W3160799772']",2022-09-16
https://openalex.org/W3207558756,https://doi.org/10.1109/asru51503.2021.9688137,An Exploration of Self-Supervised Pretrained Representations for End-to-End Speech Recognition,"Self-supervised pretraining on speech data has achieved a lot of progress. High-fidelity representation of the speech signal is learned from a lot of untranscribed data and shows promising performance. Recently, there are several works focusing on evaluating the quality of self-supervised pretrained representations on various tasks with-out domain restriction, e.g. SUPERB. However, such evaluations do not provide a comprehensive comparison among many ASR benchmark corpora. In this paper, we focus on the general applications of pretrained speech representations, on advanced end-to-end automatic speech recognition (E2E-ASR) models. We select sev-eral pretrained speech representations and present the experimental results on various open-source and publicly available corpora for E2E-ASR. Without any modification of the back-end model archi-tectures or training strategy, some of the experiments with pretrained representations, e.g., WSJ, WSJ0-2mix with HuBERT, reach or out-perform current state-of-the-art (SOTA) recognition performance. Moreover, we further explore more scenarios for whether the pre-training representations are effective, such as the cross-language or overlapped speech. The scripts, configuratons and the trained mod-els have been released in ESPnet to let the community reproduce our experiments and improve them.","['https://openalex.org/W1989314204', 'https://openalex.org/W3016181583', 'https://openalex.org/W2127141656', 'https://openalex.org/W2995181338', 'https://openalex.org/W3151851237', 'https://openalex.org/W3037217258', 'https://openalex.org/W6682948231', 'https://openalex.org/W3163793923', 'https://openalex.org/W6768009688', 'https://openalex.org/W2526425061', 'https://openalex.org/W3197580070', 'https://openalex.org/W6685380521', 'https://openalex.org/W2962780374', 'https://openalex.org/W2143612262', 'https://openalex.org/W2155273149', 'https://openalex.org/W6784776607', 'https://openalex.org/W2973049979', 'https://openalex.org/W2842511635', 'https://openalex.org/W6780218876', 'https://openalex.org/W6769196770', 'https://openalex.org/W2883725317', 'https://openalex.org/W3160799772', 'https://openalex.org/W2058094241', 'https://openalex.org/W6770506093', 'https://openalex.org/W6633847657', 'https://openalex.org/W3015522062', 'https://openalex.org/W2463955103', 'https://openalex.org/W6755207826', 'https://openalex.org/W2972943112', 'https://openalex.org/W3097286738', 'https://openalex.org/W2982223350', 'https://openalex.org/W6780361010', 'https://openalex.org/W6739901393', 'https://openalex.org/W3097777922', 'https://openalex.org/W6754299077', 'https://openalex.org/W3198694222', 'https://openalex.org/W1494198834', 'https://openalex.org/W2963807318', 'https://openalex.org/W6791904447', 'https://openalex.org/W2148613904', 'https://openalex.org/W2559260703', 'https://openalex.org/W2221409856', 'https://openalex.org/W2963242190', 'https://openalex.org/W6785749098', 'https://openalex.org/W1526236009', 'https://openalex.org/W6691770337', 'https://openalex.org/W6779752190', 'https://openalex.org/W2251321385', 'https://openalex.org/W3099782249', 'https://openalex.org/W2996383576', 'https://openalex.org/W3139918052', 'https://openalex.org/W1569447338', 'https://openalex.org/W2152790380', 'https://openalex.org/W1855892484', 'https://openalex.org/W2963341956', 'https://openalex.org/W2979476256', 'https://openalex.org/W2896457183', 'https://openalex.org/W2892009249', 'https://openalex.org/W2991213871', 'https://openalex.org/W3036601975', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964074409', 'https://openalex.org/W3198858531', 'https://openalex.org/W4385245566', 'https://openalex.org/W3041561163', 'https://openalex.org/W3101648800', 'https://openalex.org/W3105169251', 'https://openalex.org/W2972818416', 'https://openalex.org/W3095292526', 'https://openalex.org/W4297808394']",2021-12-13
https://openalex.org/W4292969786,https://doi.org/10.1109/jstsp.2022.3200911,RemixIT: Continual self-training of speech enhancement models via bootstrapped remixing,"We present RemixIT, a simple yet effective self-supervised method for training speech enhancement without the need of a single isolated in-domain speech nor a noise waveform. Our approach overcomes limitations of previous methods which make them dependent on clean in-domain target signals and thus, sensitive to any domain mismatch between train and test samples. RemixIT is based on a continuous self-training scheme in which a pre-trained teacher model on out-of-domain data infers estimated pseudo-target signals for in-domain mixtures. Then, by permuting the estimated clean and noise signals and remixing them together, we generate a new set of bootstrapped mixtures and corresponding pseudo-targets which are used to train the student network. Vice-versa, the teacher periodically refines its estimates using the updated parameters of the latest student models. Experimental results on multiple speech enhancement datasets and tasks not only show the superiority of our method over prior approaches but also showcase that RemixIT can be combined with any separation model as well as be applied towards any semi-supervised and unsupervised domain adaptation task. Our analysis, paired with empirical evidence, sheds light on the inside functioning of our self-training scheme wherein the student model keeps obtaining better performance while observing severely degraded pseudo-targets.","['https://openalex.org/W3164223074', 'https://openalex.org/W3163816345', 'https://openalex.org/W2998572311', 'https://openalex.org/W3160733670', 'https://openalex.org/W3160799772', 'https://openalex.org/W2890820256', 'https://openalex.org/W2963341071', 'https://openalex.org/W3163114796', 'https://openalex.org/W1522301498', 'https://openalex.org/W2981436548', 'https://openalex.org/W2766672686', 'https://openalex.org/W2998678989', 'https://openalex.org/W3122264812', 'https://openalex.org/W2953070460', 'https://openalex.org/W3097945073', 'https://openalex.org/W3162538144', 'https://openalex.org/W4301350573', 'https://openalex.org/W3001197829', 'https://openalex.org/W2949558265', 'https://openalex.org/W3096338464', 'https://openalex.org/W3134695619', 'https://openalex.org/W4295122182', 'https://openalex.org/W2593116425', 'https://openalex.org/W3197103473', 'https://openalex.org/W4226029295', 'https://openalex.org/W3197227964', 'https://openalex.org/W3191469971', 'https://openalex.org/W2460742184', 'https://openalex.org/W3094607766', 'https://openalex.org/W4226219411', 'https://openalex.org/W2949756029', 'https://openalex.org/W3198098585', 'https://openalex.org/W3097034112', 'https://openalex.org/W2972864514', 'https://openalex.org/W2975955714', 'https://openalex.org/W3015219411', 'https://openalex.org/W3016912202', 'https://openalex.org/W2963189033', 'https://openalex.org/W2589857635', 'https://openalex.org/W1983108229', 'https://openalex.org/W4324109905', 'https://openalex.org/W2964058413', 'https://openalex.org/W1494198834', 'https://openalex.org/W4287119748', 'https://openalex.org/W3015337486', 'https://openalex.org/W3160085755', 'https://openalex.org/W3163652268', 'https://openalex.org/W3097906045', 'https://openalex.org/W2998616931', 'https://openalex.org/W2923728956', 'https://openalex.org/W3003875258', 'https://openalex.org/W3036601975', 'https://openalex.org/W4214784181', 'https://openalex.org/W3185109982', 'https://openalex.org/W2141998673', 'https://openalex.org/W3093839391', 'https://openalex.org/W1897240248', 'https://openalex.org/W2765407302', 'https://openalex.org/W2972541922', 'https://openalex.org/W3160320508', 'https://openalex.org/W4205689591', 'https://openalex.org/W2973220283', 'https://openalex.org/W3035160371', 'https://openalex.org/W2996501936', 'https://openalex.org/W4226236567', 'https://openalex.org/W3041561163', 'https://openalex.org/W2962843322', 'https://openalex.org/W2962369866', 'https://openalex.org/W1552314771', 'https://openalex.org/W2939518062', 'https://openalex.org/W3121132363']",2022-02-17
https://openalex.org/W4391021724,https://doi.org/10.1109/asru57964.2023.10389671,The Singing Voice Conversion Challenge 2023,"We present the latest iteration of the voice conversion challenge (VCC) series, a bi-annual scientific event aiming to compare and understand different voice conversion (VC) systems based on a common dataset. This year we shifted our focus to singing voice conversion (SVC), thus named the challenge the Singing Voice Conversion Challenge (SVCC). A new database was constructed for two tasks, namely in-domain and cross-domain SVC. The challenge was run for two months, and in total we received 26 submissions, including 2 baselines. Through a large-scale crowd-sourced listening test, we observed that for both tasks, although human-level naturalness was achieved by the top system, no team was able to obtain a similarity score as high as the target speakers. Also, as expected, cross-domain SVC is harder than in-domain SVC, especially in the similarity aspect. We also investigated whether existing objective measurements were able to predict perceptual performance, and found that only few of them could reach a significant correlation.","['https://openalex.org/W2576309025', 'https://openalex.org/W3098557217', 'https://openalex.org/W2022125261', 'https://openalex.org/W2052871313', 'https://openalex.org/W2022241205', 'https://openalex.org/W2403098732', 'https://openalex.org/W2056852181', 'https://openalex.org/W2473388484', 'https://openalex.org/W2963035245', 'https://openalex.org/W6840412704', 'https://openalex.org/W4235201968', 'https://openalex.org/W2294038178', 'https://openalex.org/W2401296648', 'https://openalex.org/W3012498027', 'https://openalex.org/W3025192522', 'https://openalex.org/W3196667132', 'https://openalex.org/W2972812066', 'https://openalex.org/W3016007107', 'https://openalex.org/W3163568691', 'https://openalex.org/W3095948607', 'https://openalex.org/W3170751106', 'https://openalex.org/W4224927653', 'https://openalex.org/W4226320669', 'https://openalex.org/W3181854487', 'https://openalex.org/W4287889585', 'https://openalex.org/W3160799772', 'https://openalex.org/W3209984917', 'https://openalex.org/W6839738141', 'https://openalex.org/W6802142237', 'https://openalex.org/W6796464841', 'https://openalex.org/W6783867762', 'https://openalex.org/W2990440871', 'https://openalex.org/W6838843145', 'https://openalex.org/W4372262501', 'https://openalex.org/W4372346850', 'https://openalex.org/W1590808459', 'https://openalex.org/W6628615481', 'https://openalex.org/W6781547035', 'https://openalex.org/W4280572880', 'https://openalex.org/W3096567388', 'https://openalex.org/W3118753411', 'https://openalex.org/W3081753361', 'https://openalex.org/W2973071600', 'https://openalex.org/W2962780374', 'https://openalex.org/W3163793923', 'https://openalex.org/W3197580070', 'https://openalex.org/W6847363464', 'https://openalex.org/W3016243847', 'https://openalex.org/W4297841773', 'https://openalex.org/W3202278141', 'https://openalex.org/W4225956675', 'https://openalex.org/W4311000453']",2023-12-16
https://openalex.org/W4386254667,https://doi.org/10.1145/3617833,"Multimodality Representation Learning: A Survey on Evolution, Pretraining and Its Applications","Multimodality Representation Learning, as a technique of learning to embed information from different modalities and their correlations, has achieved remarkable success on a variety of applications, such as Visual Question Answering (VQA), Natural Language for Visual Reasoning (NLVR), and Vision Language Retrieval (VLR). Among these applications, cross-modal interaction and complementary information from different modalities are crucial for advanced models to perform any multimodal task, e.g., understand, recognize, retrieve, or generate optimally. Researchers have proposed diverse methods to address these tasks. The different variants of transformer-based architectures performed extraordinarily on multiple modalities. This survey presents the comprehensive literature on the evolution and enhancement of deep learning multimodal architectures to deal with textual, visual and audio features for diverse cross-modal and modern multimodal tasks. This study summarizes the ( i ) recent task-specific deep learning methodologies, ( ii ) the pretraining types and multimodal pretraining objectives, ( iii ) from state-of-the-art pretrained multimodal approaches to unifying architectures, and ( iv ) multimodal task categories and possible future improvements that can be devised for better multimodal learning. Moreover, we prepare a dataset section for new researchers that covers most of the benchmarks for pretraining and finetuning. Finally, major challenges, gaps, and potential research topics are explored. A constantly-updated paperlist related to our survey is maintained at https://github.com/marslanm/multimodality-representation-learning .","['https://openalex.org/W3164901500', 'https://openalex.org/W2619383789', 'https://openalex.org/W3177048205', 'https://openalex.org/W1933349210', 'https://openalex.org/W2946165673', 'https://openalex.org/W3168463823', 'https://openalex.org/W2968124245', 'https://openalex.org/W2966715458', 'https://openalex.org/W3034727271', 'https://openalex.org/W2015394094', 'https://openalex.org/W2053101950', 'https://openalex.org/W2026012689', 'https://openalex.org/W2061781940', 'https://openalex.org/W2125336414', 'https://openalex.org/W2135776491', 'https://openalex.org/W2130162821', 'https://openalex.org/W2056403322', 'https://openalex.org/W4385573610', 'https://openalex.org/W2106277773', 'https://openalex.org/W2142540378', 'https://openalex.org/W2919115771', 'https://openalex.org/W1686810756', 'https://openalex.org/W2194775991', 'https://openalex.org/W2064675550', 'https://openalex.org/W6739901393', 'https://openalex.org/W4312810944', 'https://openalex.org/W2471094925', 'https://openalex.org/W2963176022', 'https://openalex.org/W2998631105', 'https://openalex.org/W2964067226', 'https://openalex.org/W3208314443', 'https://openalex.org/W2618530766', 'https://openalex.org/W3036601975', 'https://openalex.org/W3184679245', 'https://openalex.org/W3011727199', 'https://openalex.org/W4225832925', 'https://openalex.org/W4225683910', 'https://openalex.org/W3213351348', 'https://openalex.org/W2767290858', 'https://openalex.org/W3198196812', 'https://openalex.org/W2964138343', 'https://openalex.org/W4220967417', 'https://openalex.org/W4213325639', 'https://openalex.org/W3122006038', 'https://openalex.org/W3166287017', 'https://openalex.org/W3112077297', 'https://openalex.org/W2766046458', 'https://openalex.org/W3175824375', 'https://openalex.org/W1861492603', 'https://openalex.org/W2025768430', 'https://openalex.org/W4312044727', 'https://openalex.org/W3034871396', 'https://openalex.org/W3034943799', 'https://openalex.org/W3137069976', 'https://openalex.org/W2606594151', 'https://openalex.org/W2963177403', 'https://openalex.org/W3167471487', 'https://openalex.org/W4283798830', 'https://openalex.org/W3173220247', 'https://openalex.org/W3170767867', 'https://openalex.org/W2965373594', 'https://openalex.org/W2911489562', 'https://openalex.org/W3185909895', 'https://openalex.org/W3171975879', 'https://openalex.org/W3093814160', 'https://openalex.org/W3034238904', 'https://openalex.org/W3164896303', 'https://openalex.org/W3172021172', 'https://openalex.org/W3202384916', 'https://openalex.org/W3035652667', 'https://openalex.org/W3090449556', 'https://openalex.org/W3105232955', 'https://openalex.org/W3174010726', 'https://openalex.org/W3096609285', 'https://openalex.org/W2886641317', 'https://openalex.org/W2997591391', 'https://openalex.org/W1773149199', 'https://openalex.org/W3091588028', 'https://openalex.org/W3182683290', 'https://openalex.org/W3173909648', 'https://openalex.org/W3101065397', 'https://openalex.org/W4318718936', 'https://openalex.org/W3160799772', 'https://openalex.org/W2153579005', 'https://openalex.org/W2963691697', 'https://openalex.org/W3034723486', 'https://openalex.org/W3100307207', 'https://openalex.org/W2277195237', 'https://openalex.org/W2250384498', 'https://openalex.org/W2904565150', 'https://openalex.org/W4200634001', 'https://openalex.org/W3143320354', 'https://openalex.org/W3154596443', 'https://openalex.org/W3034999214', 'https://openalex.org/W4376312115', 'https://openalex.org/W3176675602', 'https://openalex.org/W3205981739', 'https://openalex.org/W3003484198', 'https://openalex.org/W2986619406', 'https://openalex.org/W4221167941', 'https://openalex.org/W3130071011', 'https://openalex.org/W3104210310', 'https://openalex.org/W2953104586', 'https://openalex.org/W3211495814', 'https://openalex.org/W3112034174', 'https://openalex.org/W2891205112', 'https://openalex.org/W3006974783', 'https://openalex.org/W3035626590', 'https://openalex.org/W2768175742', 'https://openalex.org/W2152826865', 'https://openalex.org/W2962865004', 'https://openalex.org/W2343307093', 'https://openalex.org/W2963023579', 'https://openalex.org/W3122367007', 'https://openalex.org/W3116238789', 'https://openalex.org/W3010206893', 'https://openalex.org/W3216542025', 'https://openalex.org/W2798683079', 'https://openalex.org/W3148187679', 'https://openalex.org/W2946595845', 'https://openalex.org/W2970487286', 'https://openalex.org/W2766462585', 'https://openalex.org/W3022924198', 'https://openalex.org/W2809476703', 'https://openalex.org/W2912305564', 'https://openalex.org/W4206390933', 'https://openalex.org/W2963115613', 'https://openalex.org/W3194633557', 'https://openalex.org/W2970231061', 'https://openalex.org/W3014611590', 'https://openalex.org/W3035232877', 'https://openalex.org/W1895577753', 'https://openalex.org/W2550553598', 'https://openalex.org/W2963084599', 'https://openalex.org/W3186187670', 'https://openalex.org/W3013503438', 'https://openalex.org/W2947312908', 'https://openalex.org/W3180718533', 'https://openalex.org/W3210213497', 'https://openalex.org/W2583695460', 'https://openalex.org/W2963644680', 'https://openalex.org/W6638319203', 'https://openalex.org/W2963966654', 'https://openalex.org/W2890952074', 'https://openalex.org/W2029199293', 'https://openalex.org/W2964243274', 'https://openalex.org/W2585824449', 'https://openalex.org/W2964352155', 'https://openalex.org/W2972563022', 'https://openalex.org/W2973767881', 'https://openalex.org/W3196563995', 'https://openalex.org/W4302275554', 'https://openalex.org/W3165439041', 'https://openalex.org/W2963360627', 'https://openalex.org/W3109387650', 'https://openalex.org/W3000226596', 'https://openalex.org/W2883409523', 'https://openalex.org/W2396881363', 'https://openalex.org/W2085411191', 'https://openalex.org/W125693051', 'https://openalex.org/W2982573303', 'https://openalex.org/W2425121537', 'https://openalex.org/W2765716052', 'https://openalex.org/W2606982687', 'https://openalex.org/W2964487155', 'https://openalex.org/W2597425697', 'https://openalex.org/W3035574168', 'https://openalex.org/W2533598788', 'https://openalex.org/W2465534249', 'https://openalex.org/W3128564814', 'https://openalex.org/W2140030375', 'https://openalex.org/W4312910992', 'https://openalex.org/W6844146455', 'https://openalex.org/W3177654849', 'https://openalex.org/W3199725751', 'https://openalex.org/W4281653378', 'https://openalex.org/W3184735396', 'https://openalex.org/W3177174258', 'https://openalex.org/W3165938948', 'https://openalex.org/W3139732141', 'https://openalex.org/W4281643269', 'https://openalex.org/W3136499730', 'https://openalex.org/W4287176715', 'https://openalex.org/W4299828299', 'https://openalex.org/W4287778518', 'https://openalex.org/W4294170691', 'https://openalex.org/W4287887264', 'https://openalex.org/W2991044292', 'https://openalex.org/W4287113019', 'https://openalex.org/W4287389467', 'https://openalex.org/W4385245566', 'https://openalex.org/W4297677855', 'https://openalex.org/W4297689875', 'https://openalex.org/W4385569837', 'https://openalex.org/W1797268635', 'https://openalex.org/W3104279398', 'https://openalex.org/W4285550688']",2023-08-29
https://openalex.org/W4285159263,https://doi.org/10.18653/v1/2022.nlp4convai-1.12,Multimodal Conversational AI: A Survey of Datasets and Approaches,"As humans, we experience the world with all our senses or modalities (sound, sight, touch, smell, and taste). We use these modalities, particularly sight and touch, to convey and interpret specific meanings. Multimodal expressions are central to conversations; a rich set of modalities amplify and often compensate for each other. A multimodal conversational AI system answers questions, fulfills tasks, and emulates human conversations by understanding and expressing itself via multiple modalities. This paper motivates, defines, and mathematically formulates the multimodal conversational research objective. We provide a taxonomy of research required to solve the objective: multimodal representation, fusion, alignment, translation, and co-learning. We survey state-of-the-art datasets and approaches for each research area and highlight their limiting assumptions. Finally, we identify multimodal co-learning as a promising direction for multimodal conversational AI research.","['https://openalex.org/W2068547472', 'https://openalex.org/W2964352131', 'https://openalex.org/W2962749469', 'https://openalex.org/W2965313405', 'https://openalex.org/W2963567240', 'https://openalex.org/W1686810756', 'https://openalex.org/W2152790380', 'https://openalex.org/W3035060554', 'https://openalex.org/W1970244761', 'https://openalex.org/W3205786327', 'https://openalex.org/W2964067226', 'https://openalex.org/W3015817524', 'https://openalex.org/W4200633014', 'https://openalex.org/W3145385912', 'https://openalex.org/W3203663566', 'https://openalex.org/W3037309139', 'https://openalex.org/W4288089799', 'https://openalex.org/W2401425944', 'https://openalex.org/W2835434549', 'https://openalex.org/W2619383789', 'https://openalex.org/W2063971957', 'https://openalex.org/W2558809543', 'https://openalex.org/W3116323972', 'https://openalex.org/W1996941442', 'https://openalex.org/W4287757777', 'https://openalex.org/W4287795696', 'https://openalex.org/W2984008963', 'https://openalex.org/W3090449556', 'https://openalex.org/W3043840704', 'https://openalex.org/W3203711169', 'https://openalex.org/W2964218959', 'https://openalex.org/W1488163396', 'https://openalex.org/W2153606381', 'https://openalex.org/W4306716473', 'https://openalex.org/W1990576211', 'https://openalex.org/W3005680577', 'https://openalex.org/W2892245540', 'https://openalex.org/W2951583236', 'https://openalex.org/W3156555225', 'https://openalex.org/W2963206148', 'https://openalex.org/W4288093753', 'https://openalex.org/W3036846138', 'https://openalex.org/W2041076459', 'https://openalex.org/W2791169651', 'https://openalex.org/W4287065279', 'https://openalex.org/W3154596443', 'https://openalex.org/W3091588028', 'https://openalex.org/W3205715971', 'https://openalex.org/W2998536339', 'https://openalex.org/W2157190406', 'https://openalex.org/W3189817881', 'https://openalex.org/W3104078590', 'https://openalex.org/W1933349210', 'https://openalex.org/W2045372110', 'https://openalex.org/W2890394457', 'https://openalex.org/W2252122141', 'https://openalex.org/W2102765684', 'https://openalex.org/W3034758614', 'https://openalex.org/W3167070827', 'https://openalex.org/W3108528212', 'https://openalex.org/W4236521339', 'https://openalex.org/W154472438', 'https://openalex.org/W1591706642', 'https://openalex.org/W1540429825', 'https://openalex.org/W2337252826', 'https://openalex.org/W4312910992', 'https://openalex.org/W2952132648', 'https://openalex.org/W2896457183', 'https://openalex.org/W3160799772', 'https://openalex.org/W10957333', 'https://openalex.org/W3170560353', 'https://openalex.org/W2959918500', 'https://openalex.org/W2963578915', 'https://openalex.org/W3010094231', 'https://openalex.org/W2963541336', 'https://openalex.org/W1614298861', 'https://openalex.org/W3035448310', 'https://openalex.org/W4288624561', 'https://openalex.org/W2164450870', 'https://openalex.org/W2394741298', 'https://openalex.org/W4394667336', 'https://openalex.org/W2137871902', 'https://openalex.org/W2062955551', 'https://openalex.org/W2998563994', 'https://openalex.org/W2126579184', 'https://openalex.org/W2048343491', 'https://openalex.org/W3100110884', 'https://openalex.org/W2184188583', 'https://openalex.org/W2511428026', 'https://openalex.org/W2139659117', 'https://openalex.org/W3177301891', 'https://openalex.org/W2142192571', 'https://openalex.org/W24089286', 'https://openalex.org/W2963224792', 'https://openalex.org/W2061496281', 'https://openalex.org/W2963963856', 'https://openalex.org/W2961193895', 'https://openalex.org/W2164186291', 'https://openalex.org/W4385245566', 'https://openalex.org/W2988937804', 'https://openalex.org/W3117238912', 'https://openalex.org/W2990152177', 'https://openalex.org/W2963656855', 'https://openalex.org/W2981851019', 'https://openalex.org/W3035451444', 'https://openalex.org/W2048679005', 'https://openalex.org/W2990408345', 'https://openalex.org/W2583816737', 'https://openalex.org/W1522515958', 'https://openalex.org/W1975179095', 'https://openalex.org/W2915552958', 'https://openalex.org/W3200689778', 'https://openalex.org/W2425121537', 'https://openalex.org/W2962883855', 'https://openalex.org/W2975357369', 'https://openalex.org/W2890052321', 'https://openalex.org/W3166396011', 'https://openalex.org/W3175300676', 'https://openalex.org/W2126891226', 'https://openalex.org/W3036601975', 'https://openalex.org/W2772001136', 'https://openalex.org/W2966715458', 'https://openalex.org/W2583186419', 'https://openalex.org/W2983256121', 'https://openalex.org/W3035524453', 'https://openalex.org/W2963890755', 'https://openalex.org/W1518951372', 'https://openalex.org/W4287900772', 'https://openalex.org/W2963491014', 'https://openalex.org/W3134294468', 'https://openalex.org/W124086577', 'https://openalex.org/W3034636873', 'https://openalex.org/W3035118106', 'https://openalex.org/W3082884418', 'https://openalex.org/W38568571', 'https://openalex.org/W2584992898', 'https://openalex.org/W3198407242', 'https://openalex.org/W4287704453', 'https://openalex.org/W2549139847', 'https://openalex.org/W2098562545', 'https://openalex.org/W2096391593', 'https://openalex.org/W1527575280', 'https://openalex.org/W2593116425', 'https://openalex.org/W2043511823', 'https://openalex.org/W2972018816', 'https://openalex.org/W2891021031', 'https://openalex.org/W2964213933', 'https://openalex.org/W2970231061', 'https://openalex.org/W3091546937', 'https://openalex.org/W3009045332', 'https://openalex.org/W2140881187', 'https://openalex.org/W2151096985', 'https://openalex.org/W639708223']",2022-01-01
https://openalex.org/W3198098585,https://doi.org/10.21437/interspeech.2021-571,Momentum Pseudo-Labeling for Semi-Supervised Speech Recognition,"Pseudo-labeling (PL) has been shown to be effective in semisupervised automatic speech recognition (ASR), where a base model is self-trained with pseudo-labels generated from unlabeled data.While PL can be further improved by iteratively updating pseudo-labels as the model evolves, most of the previous approaches involve inefficient retraining of the model or intricate control of the label update.We present momentum pseudo-labeling (MPL), a simple yet effective strategy for semisupervised ASR.MPL consists of a pair of online and offline models that interact and learn from each other, inspired by the mean teacher method.The online model is trained to predict pseudo-labels generated on the fly by the offline model.The offline model maintains a momentum-based moving average of the online model.MPL is performed in a single training process and the interaction between the two models effectively helps them reinforce each other to improve the ASR performance.We apply MPL to an end-to-end ASR model based on the connectionist temporal classification.The experimental results demonstrate that MPL effectively improves over the base model and is scalable to different semi-supervised scenarios with varying amounts of data or domain mismatch.","['https://openalex.org/W3097882114', 'https://openalex.org/W3163464943', 'https://openalex.org/W4385245566', 'https://openalex.org/W2889213362', 'https://openalex.org/W3026041220', 'https://openalex.org/W2973049979', 'https://openalex.org/W2962824709', 'https://openalex.org/W3096338464', 'https://openalex.org/W3101648800', 'https://openalex.org/W2972389417', 'https://openalex.org/W2972889948', 'https://openalex.org/W3015522062', 'https://openalex.org/W2892008152', 'https://openalex.org/W2913851961', 'https://openalex.org/W1522301498', 'https://openalex.org/W2953070460', 'https://openalex.org/W2160815625', 'https://openalex.org/W82886505', 'https://openalex.org/W2962699523', 'https://openalex.org/W2972818416', 'https://openalex.org/W2327501763', 'https://openalex.org/W2111316763', 'https://openalex.org/W2951970475', 'https://openalex.org/W3016011332', 'https://openalex.org/W2998532468', 'https://openalex.org/W2988736778', 'https://openalex.org/W2936774411', 'https://openalex.org/W2962780374', 'https://openalex.org/W3015265920', 'https://openalex.org/W3160799772', 'https://openalex.org/W2963739817', 'https://openalex.org/W3197223534', 'https://openalex.org/W2952711665', 'https://openalex.org/W2979476256', 'https://openalex.org/W2962369866', 'https://openalex.org/W3162833755', 'https://openalex.org/W2143612262', 'https://openalex.org/W3103005696', 'https://openalex.org/W2102113734', 'https://openalex.org/W1494198834', 'https://openalex.org/W3096273170', 'https://openalex.org/W1524333225', 'https://openalex.org/W2963979492', 'https://openalex.org/W3015737168', 'https://openalex.org/W854541894', 'https://openalex.org/W3015537910']",2021-08-27
https://openalex.org/W4372270126,https://doi.org/10.1109/icassp49357.2023.10096988,Wav2Seq: Pre-Training Speech-to-Text Encoder-Decoder Models Using Pseudo Languages,"We introduce Wav2Seq, the first self-supervised approach to pre-train both parts of encoder-decoder models for speech data. We induce a pseudo language as a compact discrete representation, and formulate a self-supervised pseudo speech recognition task — transcribing audio inputs into pseudo subword sequences. This process stands on its own, or can be applied as low-cost second-stage pre-training. We experiment with automatic speech recognition (ASR), spoken named entity recognition, and speech-to-text translation. We set new state-of-the-art results for end-to-end spoken named entity recognition, and show consistent improvements on 8 language pairs for speech-to-text translation, even when competing methods use additional text data for training. On ASR, our approach enables encoder-decoder methods to benefit from pre-training for all parts of the network, and shows comparable performance to highly optimized recent methods.","['https://openalex.org/W3097777922', 'https://openalex.org/W3200129129', 'https://openalex.org/W3095173472', 'https://openalex.org/W6675354045', 'https://openalex.org/W6638749077', 'https://openalex.org/W2963347649', 'https://openalex.org/W6810007534', 'https://openalex.org/W6729239390', 'https://openalex.org/W2933138175', 'https://openalex.org/W2995181338', 'https://openalex.org/W2953190524', 'https://openalex.org/W6668990524', 'https://openalex.org/W2962780374', 'https://openalex.org/W2142838865', 'https://openalex.org/W2973049979', 'https://openalex.org/W2842511635', 'https://openalex.org/W6623517193', 'https://openalex.org/W3095918555', 'https://openalex.org/W6679436768', 'https://openalex.org/W2914417638', 'https://openalex.org/W3198299542', 'https://openalex.org/W2327501763', 'https://openalex.org/W6769627184', 'https://openalex.org/W3034999214', 'https://openalex.org/W6755207826', 'https://openalex.org/W6769196770', 'https://openalex.org/W3213029956', 'https://openalex.org/W3119308075', 'https://openalex.org/W3198429080', 'https://openalex.org/W4226278833', 'https://openalex.org/W3001434439', 'https://openalex.org/W6850036870', 'https://openalex.org/W3173767661', 'https://openalex.org/W6790356757', 'https://openalex.org/W3205644108', 'https://openalex.org/W6601894380', 'https://openalex.org/W4226103796', 'https://openalex.org/W3197580070', 'https://openalex.org/W6727336983', 'https://openalex.org/W6795346631', 'https://openalex.org/W6780218876', 'https://openalex.org/W6781927165', 'https://openalex.org/W1494198834', 'https://openalex.org/W4287887773', 'https://openalex.org/W1828163288', 'https://openalex.org/W2101234009', 'https://openalex.org/W3160799772', 'https://openalex.org/W2130942839', 'https://openalex.org/W3036601975', 'https://openalex.org/W46679369', 'https://openalex.org/W4394671563', 'https://openalex.org/W4221145109', 'https://openalex.org/W854541894', 'https://openalex.org/W2520160253', 'https://openalex.org/W4297808394', 'https://openalex.org/W3054645415', 'https://openalex.org/W4288089799', 'https://openalex.org/W4372270126', 'https://openalex.org/W2073459066', 'https://openalex.org/W2896457183', 'https://openalex.org/W2979476256', 'https://openalex.org/W2549416390']",2023-05-05
https://openalex.org/W4319601891,https://doi.org/10.3390/s23041843,Novel Speech Recognition Systems Applied to Forensics within Child Exploitation: Wav2vec2.0 vs. Whisper,"The growth in online child exploitation material is a significant challenge for European Law Enforcement Agencies (LEAs). One of the most important sources of such online information corresponds to audio material that needs to be analyzed to find evidence in a timely and practical manner. That is why LEAs require a next-generation AI-powered platform to process audio data from online sources. We propose the use of speech recognition and keyword spotting to transcribe audiovisual data and to detect the presence of keywords related to child abuse. The considered models are based on two of the most accurate neural-based architectures to date: Wav2vec2.0 and Whisper. The systems were tested under an extensive set of scenarios in different languages. Additionally, keeping in mind that obtaining data from LEAs are very sensitive, we explore the use of federated learning to provide more robust systems for the addressed application, while maintaining the privacy of the data from LEAs. The considered models achieved a word error rate between 11% and 25%, depending on the language. In addition, the systems are able to recognize a set of spotted words with true-positive rates between 82% and 98%, depending on the language. Finally, federated learning strategies show that they can maintain and even improve the performance of the systems when compared to centralized trained models. The proposed systems set the basis for an AI-powered platform for automatic analysis of audio in the context of forensic applications of child abuse. The use of federated learning is also promising for the addressed scenario, where data privacy is an important issue to be managed.","['https://openalex.org/W3181867036', 'https://openalex.org/W2900026217', 'https://openalex.org/W2989223447', 'https://openalex.org/W2193413348', 'https://openalex.org/W2102113734', 'https://openalex.org/W2327501763', 'https://openalex.org/W6623517193', 'https://openalex.org/W2608712415', 'https://openalex.org/W3197478142', 'https://openalex.org/W3015537910', 'https://openalex.org/W2752782242', 'https://openalex.org/W2144499799', 'https://openalex.org/W3205201903', 'https://openalex.org/W6780218876', 'https://openalex.org/W4297841598', 'https://openalex.org/W3160799772', 'https://openalex.org/W3209059054', 'https://openalex.org/W3097777922', 'https://openalex.org/W3163793923', 'https://openalex.org/W6743073161', 'https://openalex.org/W6807767519', 'https://openalex.org/W3086809868', 'https://openalex.org/W6786061324', 'https://openalex.org/W3025567392', 'https://openalex.org/W3162072286', 'https://openalex.org/W3097714942', 'https://openalex.org/W3094793624', 'https://openalex.org/W3198429080', 'https://openalex.org/W3119308075', 'https://openalex.org/W3095410713', 'https://openalex.org/W2995929068', 'https://openalex.org/W3139878283', 'https://openalex.org/W3213029956', 'https://openalex.org/W3210615406', 'https://openalex.org/W2782717521', 'https://openalex.org/W3127012371', 'https://openalex.org/W2251321385', 'https://openalex.org/W6743784355', 'https://openalex.org/W6784697952', 'https://openalex.org/W2963040451', 'https://openalex.org/W6633582421', 'https://openalex.org/W3212799896', 'https://openalex.org/W3186550062', 'https://openalex.org/W2151244813', 'https://openalex.org/W3213348952', 'https://openalex.org/W3103802018', 'https://openalex.org/W1561748129', 'https://openalex.org/W2744999500', 'https://openalex.org/W3036601975']",2023-02-07
https://openalex.org/W3200129129,https://doi.org/10.1109/icassp43922.2022.9747432,Performance-Efficiency Trade-Offs in Unsupervised Pre-Training for Speech Recognition,"This paper is a study of performance-efficiency trade-offs in pre-trained models for automatic speech recognition (ASR). We focus on wav2vec 2.0, and formalize several architecture designs that influence both the model performance and its efficiency. Putting together all our observations, we introduce SEW-D (Squeezed and Efficient Wav2vec with Disentangled Attention), a pre-trained model architecture with significant improvements along both performance and efficiency dimensions across a variety of training setups. For example, under the 100h-960h semi-supervised setup on LibriSpeech, SEW-D achieves a 1.9x inference speedup compared to wav2vec 2.0, with a 13.5% relative reduction in word error rate. With a similar inference time, SEW reduces word error rate by 25–50% across different model sizes.","['https://openalex.org/W6755862295', 'https://openalex.org/W6727336983', 'https://openalex.org/W6779326418', 'https://openalex.org/W6780218876', 'https://openalex.org/W6779919476', 'https://openalex.org/W3198771897', 'https://openalex.org/W6795346631', 'https://openalex.org/W6629717138', 'https://openalex.org/W2799473636', 'https://openalex.org/W3119308075', 'https://openalex.org/W2842511635', 'https://openalex.org/W2127141656', 'https://openalex.org/W3197642003', 'https://openalex.org/W2933138175', 'https://openalex.org/W6784637704', 'https://openalex.org/W6766673545', 'https://openalex.org/W1991133427', 'https://openalex.org/W6755207826', 'https://openalex.org/W6770717842', 'https://openalex.org/W6779068807', 'https://openalex.org/W3198299542', 'https://openalex.org/W6774314701', 'https://openalex.org/W6784614252', 'https://openalex.org/W6739901393', 'https://openalex.org/W3097777922', 'https://openalex.org/W6697501834', 'https://openalex.org/W6729448088', 'https://openalex.org/W2963925437', 'https://openalex.org/W6638667902', 'https://openalex.org/W10548402', 'https://openalex.org/W2964121744', 'https://openalex.org/W2896457183', 'https://openalex.org/W3135828102', 'https://openalex.org/W3027083471', 'https://openalex.org/W1836465849', 'https://openalex.org/W2988736778', 'https://openalex.org/W3021469861', 'https://openalex.org/W2987283559', 'https://openalex.org/W2626778328', 'https://openalex.org/W3005680577', 'https://openalex.org/W2963341956', 'https://openalex.org/W4385245566', 'https://openalex.org/W3126565544', 'https://openalex.org/W2804648901', 'https://openalex.org/W3122890974', 'https://openalex.org/W2520160253', 'https://openalex.org/W2981363336', 'https://openalex.org/W3026408381', 'https://openalex.org/W2795935804', 'https://openalex.org/W2975381464', 'https://openalex.org/W1828163288', 'https://openalex.org/W2622263826', 'https://openalex.org/W3153287399', 'https://openalex.org/W3093533780', 'https://openalex.org/W4297808394', 'https://openalex.org/W1494198834', 'https://openalex.org/W3036601975', 'https://openalex.org/W3099782249', 'https://openalex.org/W2898700502', 'https://openalex.org/W3035524453', 'https://openalex.org/W3004728855', 'https://openalex.org/W3025165719', 'https://openalex.org/W1524333225', 'https://openalex.org/W3162391496', 'https://openalex.org/W2973049979', 'https://openalex.org/W3033187248', 'https://openalex.org/W2155273149', 'https://openalex.org/W2331143823', 'https://openalex.org/W2952509486', 'https://openalex.org/W2345968833', 'https://openalex.org/W4322614701', 'https://openalex.org/W2963970792', 'https://openalex.org/W3144173820', 'https://openalex.org/W2143612262', 'https://openalex.org/W2965373594', 'https://openalex.org/W2547875792', 'https://openalex.org/W3157916917', 'https://openalex.org/W3093579165', 'https://openalex.org/W3046727238', 'https://openalex.org/W3144073776', 'https://openalex.org/W179875071', 'https://openalex.org/W3127866760', 'https://openalex.org/W3160799772', 'https://openalex.org/W3198429080', 'https://openalex.org/W3160525311', 'https://openalex.org/W2327501763', 'https://openalex.org/W3164279099', 'https://openalex.org/W2982413405', 'https://openalex.org/W2892009249', 'https://openalex.org/W3001279689', 'https://openalex.org/W2949117887', 'https://openalex.org/W3035060554', 'https://openalex.org/W3037057938', 'https://openalex.org/W2160815625', 'https://openalex.org/W2979476256', 'https://openalex.org/W2193413348', 'https://openalex.org/W3101648800', 'https://openalex.org/W2946948417', 'https://openalex.org/W2970971581', 'https://openalex.org/W2296701362', 'https://openalex.org/W2995181338', 'https://openalex.org/W3134206242']",2022-04-27
https://openalex.org/W4293793697,https://doi.org/10.1109/jstsp.2022.3200910,Towards Better Domain Adaptation for Self-Supervised Models: A Case Study of Child ASR,"Recently, self-supervised learning (SSL) from unlabelled speech data has\ngained increased attention in the automatic speech recognition (ASR) community.\nTypical SSL methods include autoregressive predictive coding (APC), Wav2vec2.0,\nand hidden unit BERT (HuBERT). However, SSL models are biased to the\npretraining data. When SSL models are finetuned with data from another domain,\ndomain shifting occurs and might cause limited knowledge transfer for\ndownstream tasks. In this paper, we propose a novel framework, domain\nresponsible adaptation and finetuning (DRAFT), to reduce domain shifting in\npretrained speech models, and evaluate it for a causal and non-causal\ntransformer. For the causal transformer, an extension of APC (E-APC) is\nproposed to learn richer information from unlabelled data by using multiple\ntemporally-shifted sequences to perform prediction. For the non-causal\ntransformer, various solutions for using the bidirectional APC (Bi-APC) are\ninvestigated. In addition, the DRAFT framework is examined for Wav2vec2.0 and\nHuBERT methods, which use non-causal transformers as the backbone. The\nexperiments are conducted on child ASR (using the OGI and MyST databases) using\nSSL models trained with unlabelled adult speech data from Librispeech. The\nrelative WER improvements of up to 19.7% on the two child tasks are observed\nwhen compared to the pretrained models without adaptation. With the proposed\nmethods (E-APC and DRAFT), the relative WER improvements are even larger (30%\nand 19% on the OGI and MyST data, respectively) when compared to the models\nwithout using pretraining methods.\n","['https://openalex.org/W1994888226', 'https://openalex.org/W1992152912', 'https://openalex.org/W3096713376', 'https://openalex.org/W3198261723', 'https://openalex.org/W3209984917', 'https://openalex.org/W3204696009', 'https://openalex.org/W6788335241', 'https://openalex.org/W4226033575', 'https://openalex.org/W3205644108', 'https://openalex.org/W3160345865', 'https://openalex.org/W3003875258', 'https://openalex.org/W2982223350', 'https://openalex.org/W3041561163', 'https://openalex.org/W3197580070', 'https://openalex.org/W3189296823', 'https://openalex.org/W3207558756', 'https://openalex.org/W3198484663', 'https://openalex.org/W3198447122', 'https://openalex.org/W2972943112', 'https://openalex.org/W3016011332', 'https://openalex.org/W3097053302', 'https://openalex.org/W3161005563', 'https://openalex.org/W6844194202', 'https://openalex.org/W2973049979', 'https://openalex.org/W6780218876', 'https://openalex.org/W6769196770', 'https://openalex.org/W3160799772', 'https://openalex.org/W3209059054', 'https://openalex.org/W3016181583', 'https://openalex.org/W6787141514', 'https://openalex.org/W3205533980', 'https://openalex.org/W6802503112', 'https://openalex.org/W3206559778', 'https://openalex.org/W3198771897', 'https://openalex.org/W3202037040', 'https://openalex.org/W3203098807', 'https://openalex.org/W4297841489', 'https://openalex.org/W3201225328', 'https://openalex.org/W6759579507', 'https://openalex.org/W3165404421', 'https://openalex.org/W2971840980', 'https://openalex.org/W6738045163', 'https://openalex.org/W4283073456', 'https://openalex.org/W2111460811', 'https://openalex.org/W6632100814', 'https://openalex.org/W3015356564', 'https://openalex.org/W2896457183', 'https://openalex.org/W2933138175', 'https://openalex.org/W3035202887', 'https://openalex.org/W2471933213', 'https://openalex.org/W2005708641', 'https://openalex.org/W3097345069', 'https://openalex.org/W3015265920', 'https://openalex.org/W2060277733', 'https://openalex.org/W6799245484', 'https://openalex.org/W3198094329', 'https://openalex.org/W3186998975', 'https://openalex.org/W1494198834', 'https://openalex.org/W2995181338', 'https://openalex.org/W17704661', 'https://openalex.org/W2107162140', 'https://openalex.org/W6631943919', 'https://openalex.org/W2407080277', 'https://openalex.org/W2936774411', 'https://openalex.org/W4225274946', 'https://openalex.org/W2988736778', 'https://openalex.org/W3169320628', 'https://openalex.org/W3113594615', 'https://openalex.org/W3036601975', 'https://openalex.org/W2963211188', 'https://openalex.org/W4214784181', 'https://openalex.org/W4287989344', 'https://openalex.org/W2964303773', 'https://openalex.org/W1533861849', 'https://openalex.org/W3203561692', 'https://openalex.org/W4297808394', 'https://openalex.org/W4302764113', 'https://openalex.org/W4320169525', 'https://openalex.org/W1538131130', 'https://openalex.org/W2973513151', 'https://openalex.org/W3202079424', 'https://openalex.org/W3186596101', 'https://openalex.org/W2979476256', 'https://openalex.org/W4287241171', 'https://openalex.org/W3174997050', 'https://openalex.org/W4290711009', 'https://openalex.org/W4285761032', 'https://openalex.org/W4308349017', 'https://openalex.org/W4287374065']",2022-08-22
https://openalex.org/W4361990931,https://doi.org/10.1109/taslp.2023.3263789,A CTC Alignment-Based Non-Autoregressive Transformer for End-to-End Automatic Speech Recognition,"Recently, end-to-end models have been widely used in automatic speech\nrecognition (ASR) systems. Two of the most representative approaches are\nconnectionist temporal classification (CTC) and attention-based encoder-decoder\n(AED) models. Autoregressive transformers, variants of AED, adopt an\nautoregressive mechanism for token generation and thus are relatively slow\nduring inference. In this paper, we present a comprehensive study of a CTC\nAlignment-based Single-Step Non-Autoregressive Transformer (CASS-NAT) for\nend-to-end ASR. In CASS-NAT, word embeddings in the autoregressive transformer\n(AT) are substituted with token-level acoustic embeddings (TAE) that are\nextracted from encoder outputs with the acoustical boundary information offered\nby the CTC alignment. TAE can be obtained in parallel, resulting in a parallel\ngeneration of output tokens. During training, Viterbi-alignment is used for TAE\ngeneration, and multiple training strategies are further explored to improve\nthe word error rate (WER) performance. During inference, an error-based\nalignment sampling method is investigated in depth to reduce the alignment\nmismatch in the training and testing processes. Experimental results show that\nthe CASS-NAT has a WER that is close to AT on various ASR tasks, while\nproviding a ~24x inference speedup. With and without self-supervised learning,\nwe achieve new state-of-the-art results for non-autoregressive models on\nseveral datasets. We also analyze the behavior of the CASS-NAT decoder to\nexplain why it can perform similarly to AT. We find that TAEs have similar\nfunctionality to word embeddings for grammatical structures, which might\nindicate the possibility of learning some semantic information from TAEs\nwithout a language model.\n","['https://openalex.org/W3095311338', 'https://openalex.org/W3211278025', 'https://openalex.org/W2127141656', 'https://openalex.org/W2327501763', 'https://openalex.org/W6638749077', 'https://openalex.org/W3016010032', 'https://openalex.org/W4385245566', 'https://openalex.org/W2892009249', 'https://openalex.org/W6746208923', 'https://openalex.org/W2963434219', 'https://openalex.org/W3100753857', 'https://openalex.org/W6789821389', 'https://openalex.org/W3112157188', 'https://openalex.org/W3097882114', 'https://openalex.org/W3164692279', 'https://openalex.org/W3097874139', 'https://openalex.org/W4225644313', 'https://openalex.org/W3196500669', 'https://openalex.org/W3169714379', 'https://openalex.org/W6774835902', 'https://openalex.org/W3197140813', 'https://openalex.org/W3147414526', 'https://openalex.org/W3162431424', 'https://openalex.org/W3198259287', 'https://openalex.org/W3160799772', 'https://openalex.org/W2972389417', 'https://openalex.org/W6769935647', 'https://openalex.org/W2981413347', 'https://openalex.org/W6751097180', 'https://openalex.org/W3097777922', 'https://openalex.org/W3095173472', 'https://openalex.org/W4246193833', 'https://openalex.org/W3015960524', 'https://openalex.org/W3162249256', 'https://openalex.org/W3197148831', 'https://openalex.org/W2981857663', 'https://openalex.org/W6780218876', 'https://openalex.org/W2766219058', 'https://openalex.org/W3015974384', 'https://openalex.org/W3028545098', 'https://openalex.org/W2899423466', 'https://openalex.org/W2964110616', 'https://openalex.org/W1494198834', 'https://openalex.org/W2963242190', 'https://openalex.org/W6691770337', 'https://openalex.org/W2107162140', 'https://openalex.org/W2963250244', 'https://openalex.org/W2407080277', 'https://openalex.org/W2936774411', 'https://openalex.org/W2972439411', 'https://openalex.org/W6766978945', 'https://openalex.org/W3203407300', 'https://openalex.org/W6769921281', 'https://openalex.org/W6636510571', 'https://openalex.org/W3206573929', 'https://openalex.org/W4293793697', 'https://openalex.org/W4221151577', 'https://openalex.org/W4283073456', 'https://openalex.org/W2896457183', 'https://openalex.org/W3035445001', 'https://openalex.org/W1828163288', 'https://openalex.org/W4287117559', 'https://openalex.org/W2251321385', 'https://openalex.org/W4295312788', 'https://openalex.org/W3126267552', 'https://openalex.org/W4291566970', 'https://openalex.org/W4287989347', 'https://openalex.org/W2973513151', 'https://openalex.org/W2985287635', 'https://openalex.org/W2097117768', 'https://openalex.org/W3036601975', 'https://openalex.org/W1614298861', 'https://openalex.org/W4288072840', 'https://openalex.org/W2767206889', 'https://openalex.org/W2995999067', 'https://openalex.org/W4295253143']",2023-01-01
https://openalex.org/W4285158365,https://doi.org/10.18653/v1/2022.findings-acl.113,End-to-End Speech Translation for Code Switched Speech,"Code switching (CS) refers to the phenomenon of interchangeably using words and phrases from different languages. CS can pose significant accuracy challenges to NLP, due to the often monolingual nature of the underlying systems. In this work, we focus on CS in the context of English/Spanish conversations for the task of speech translation (ST), generating and evaluating both transcript and translation. To evaluate model performance on this task, we create a novel ST corpus derived from existing public data sets. We explore various ST architectures across two dimensions: cascaded (transcribe then translate) vs end-to-end (jointly transcribe and translate) and unidirectional (source -> target) vs bidirectional (source <-> target). We show that our ST architectures, and especially our bidirectional end-to-end architecture, perform well on CS speech, even when no CS training data is used.","['https://openalex.org/W3166943438', 'https://openalex.org/W3096122506', 'https://openalex.org/W1980379483', 'https://openalex.org/W2936627440', 'https://openalex.org/W3169483174', 'https://openalex.org/W3184351855', 'https://openalex.org/W2031868076', 'https://openalex.org/W3107826490', 'https://openalex.org/W3096215352', 'https://openalex.org/W2153433699', 'https://openalex.org/W2572438855', 'https://openalex.org/W3168030157', 'https://openalex.org/W2891616026', 'https://openalex.org/W3036601975', 'https://openalex.org/W3032433061', 'https://openalex.org/W2899073901', 'https://openalex.org/W4287694131', 'https://openalex.org/W3162157691', 'https://openalex.org/W3138920323', 'https://openalex.org/W2884515606', 'https://openalex.org/W3105681039', 'https://openalex.org/W2064360999', 'https://openalex.org/W2798038877', 'https://openalex.org/W97072897', 'https://openalex.org/W1531318651', 'https://openalex.org/W2130466720', 'https://openalex.org/W2972417954', 'https://openalex.org/W2963676641', 'https://openalex.org/W3034625919', 'https://openalex.org/W3007376164', 'https://openalex.org/W3134568285', 'https://openalex.org/W4300437062', 'https://openalex.org/W3165821503', 'https://openalex.org/W2134817403', 'https://openalex.org/W3042170933', 'https://openalex.org/W2963779652', 'https://openalex.org/W3054645415', 'https://openalex.org/W3092085609', 'https://openalex.org/W2786835190', 'https://openalex.org/W2963532001', 'https://openalex.org/W3152534858', 'https://openalex.org/W2550821151', 'https://openalex.org/W222053410', 'https://openalex.org/W2973049979', 'https://openalex.org/W2743691218', 'https://openalex.org/W2911876655', 'https://openalex.org/W4247535068', 'https://openalex.org/W4241136891', 'https://openalex.org/W2039387825', 'https://openalex.org/W3173767661', 'https://openalex.org/W3018406493', 'https://openalex.org/W3171500670', 'https://openalex.org/W3105912780', 'https://openalex.org/W3160799772', 'https://openalex.org/W4244293341', 'https://openalex.org/W2078834097', 'https://openalex.org/W3180180466', 'https://openalex.org/W87424008']",2022-01-01
https://openalex.org/W4289824098,https://doi.org/10.1109/jstsp.2022.3195367,Momentum Pseudo-Labeling: Semi-Supervised ASR With Continuously Improving Pseudo-Labels,"End-to-end automatic speech recognition (ASR) has become a popular alternative to traditional module-based systems, simplifying the model-building process with a single deep neural network architecture. However, the training of end-to-end ASR systems is generally data-hungry: a large amount of labeled data (speech-text pairs) is necessary to learn direct speech-to-text conversion effectively. To make the training less dependent on labeled data, pseudo-labeling, a semi-supervised learning approach, has been successfully introduced to end-to-end ASR, where a seed model is self-trained with pseudo-labels generated from unlabeled (speech-only) data. Here, we propose <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">momentum pseudo-labeling</i> (MPL), a simple yet effective strategy for semi-supervised ASR. MPL consists of a pair of <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">online</i> and <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">offline</i> models that interact and learn from each other, inspired by the mean teacher method. The online model is trained to predict pseudo-labels generated on the fly by the offline model. The offline model maintains an exponential moving average of the online model parameters. The interaction between the two models allows better ASR training on unlabeled data by continuously improving the quality of pseudo-labels. We apply MPL to a connectionist temporal classification-based model and evaluate it on various semi-supervised scenarios with varying amounts of data or domain mismatch. The results demonstrate that MPL significantly improves the seed model by stabilizing the training on unlabeled data. Moreover, we present additional techniques, e.g., the use of Conformer and an external language model, to further enhance MPL, which leads to better performance than other semi-supervised methods based on pseudo-labeling.","['https://openalex.org/W2160815625', 'https://openalex.org/W2143612262', 'https://openalex.org/W6675365184', 'https://openalex.org/W6623517193', 'https://openalex.org/W2327501763', 'https://openalex.org/W2127141656', 'https://openalex.org/W2144499799', 'https://openalex.org/W6679436768', 'https://openalex.org/W6679434410', 'https://openalex.org/W2892009249', 'https://openalex.org/W3015537910', 'https://openalex.org/W3097777922', 'https://openalex.org/W2962824709', 'https://openalex.org/W3103005696', 'https://openalex.org/W2972818416', 'https://openalex.org/W2995181338', 'https://openalex.org/W4210606444', 'https://openalex.org/W2962699523', 'https://openalex.org/W2963739817', 'https://openalex.org/W3161143478', 'https://openalex.org/W2889213362', 'https://openalex.org/W2913851961', 'https://openalex.org/W6762242920', 'https://openalex.org/W2896457183', 'https://openalex.org/W3015356564', 'https://openalex.org/W3015265920', 'https://openalex.org/W6774314701', 'https://openalex.org/W2973049979', 'https://openalex.org/W3016011332', 'https://openalex.org/W6769196770', 'https://openalex.org/W3160799772', 'https://openalex.org/W2111316763', 'https://openalex.org/W2940322076', 'https://openalex.org/W3015522062', 'https://openalex.org/W3015737168', 'https://openalex.org/W3096273170', 'https://openalex.org/W3141100132', 'https://openalex.org/W3096338464', 'https://openalex.org/W3095350795', 'https://openalex.org/W3026041220', 'https://openalex.org/W3197223534', 'https://openalex.org/W3163464943', 'https://openalex.org/W3162833755', 'https://openalex.org/W6764051988', 'https://openalex.org/W6733814495', 'https://openalex.org/W3198098585', 'https://openalex.org/W3205405669', 'https://openalex.org/W3035524453', 'https://openalex.org/W6779326418', 'https://openalex.org/W6638523607', 'https://openalex.org/W2892008152', 'https://openalex.org/W2973040747', 'https://openalex.org/W3046667470', 'https://openalex.org/W2936774411', 'https://openalex.org/W6765939562', 'https://openalex.org/W6768222176', 'https://openalex.org/W3035160371', 'https://openalex.org/W4225755266', 'https://openalex.org/W6739901393', 'https://openalex.org/W2627092829', 'https://openalex.org/W3163793923', 'https://openalex.org/W2531409750', 'https://openalex.org/W2964110616', 'https://openalex.org/W6763608318', 'https://openalex.org/W2577366047', 'https://openalex.org/W3206573929', 'https://openalex.org/W3160766462', 'https://openalex.org/W3197916665', 'https://openalex.org/W3193590960', 'https://openalex.org/W6638667902', 'https://openalex.org/W6733590821', 'https://openalex.org/W4250482878', 'https://openalex.org/W1494198834', 'https://openalex.org/W2799473636', 'https://openalex.org/W6631362777', 'https://openalex.org/W2963979492', 'https://openalex.org/W2962780374', 'https://openalex.org/W6631190155', 'https://openalex.org/W1710082047', 'https://openalex.org/W3007073761', 'https://openalex.org/W82886505', 'https://openalex.org/W6784614252', 'https://openalex.org/W6724804524', 'https://openalex.org/W6780226713', 'https://openalex.org/W6840182953', 'https://openalex.org/W2407080277', 'https://openalex.org/W3097747488', 'https://openalex.org/W6780218876', 'https://openalex.org/W4286912221', 'https://openalex.org/W2502312327', 'https://openalex.org/W3101648800', 'https://openalex.org/W3156828761', 'https://openalex.org/W2948981900', 'https://openalex.org/W3093579165', 'https://openalex.org/W1821462560', 'https://openalex.org/W3007328579']",2022-08-05
https://openalex.org/W4221102486,https://doi.org/10.1101/2022.03.14.484195,Dissecting neural computations of the human auditory pathway using deep neural networks for speech,"Abstract The human auditory system extracts rich linguistic abstractions from the speech signal. Traditional approaches to understand this complex process have used classical linear feature encoding models, with limited success. Artificial neural networks have recently achieved remarkable speech recognition performance and offer potential alternative computational models of speech processing. We used the speech representations learned by state-of-the-art deep neural network (DNN) models to investigate neural coding across the ascending auditory pathway from the peripheral auditory nerve to auditory speech cortex. We found that representations in hierarchical layers of the DNN correlated well to neural activity throughout the ascending auditory system. Unsupervised speech models achieve the optimal neural correlations among all models evaluated. Deeper DNN layers with context-dependent computations were essential for populations of high order auditory cortex encoding, and the computations were aligned to phonemic and syllabic context structures in speech. Accordingly, DNN models trained on a specific language (English or Mandarin) predicted cortical responses in native speakers of each language. These results reveal convergence between representations learned in DNN models and the biological auditory pathway and provide new approaches to modeling neural coding in the auditory cortex.","['https://openalex.org/W2005854605', 'https://openalex.org/W2048520715', 'https://openalex.org/W2068247585', 'https://openalex.org/W2564416932', 'https://openalex.org/W2058511777', 'https://openalex.org/W2056133372', 'https://openalex.org/W2043013057', 'https://openalex.org/W2135704565', 'https://openalex.org/W2171525519', 'https://openalex.org/W2079207700', 'https://openalex.org/W2138164020', 'https://openalex.org/W3205578964', 'https://openalex.org/W3160799772', 'https://openalex.org/W2166206801', 'https://openalex.org/W2800311957', 'https://openalex.org/W3134212080', 'https://openalex.org/W2058616551', 'https://openalex.org/W2274405424', 'https://openalex.org/W3191346660', 'https://openalex.org/W2063303346', 'https://openalex.org/W2104752510', 'https://openalex.org/W2135563147', 'https://openalex.org/W3119948327', 'https://openalex.org/W3210923133', 'https://openalex.org/W1981276685', 'https://openalex.org/W3035725276', 'https://openalex.org/W2344975321', 'https://openalex.org/W2084598301', 'https://openalex.org/W2121754469', 'https://openalex.org/W160442870', 'https://openalex.org/W3193706553', 'https://openalex.org/W2021039798', 'https://openalex.org/W1494198834', 'https://openalex.org/W2807663970', 'https://openalex.org/W2169988537', 'https://openalex.org/W2478440082', 'https://openalex.org/W3131166446', 'https://openalex.org/W2168217710', 'https://openalex.org/W2010865263', 'https://openalex.org/W2120263555', 'https://openalex.org/W1968707255', 'https://openalex.org/W1611006584', 'https://openalex.org/W2990790638', 'https://openalex.org/W2749756027', 'https://openalex.org/W2925340039', 'https://openalex.org/W2075236435', 'https://openalex.org/W2978365189', 'https://openalex.org/W2951670472', 'https://openalex.org/W1965248225', 'https://openalex.org/W2904580710', 'https://openalex.org/W2919115771', 'https://openalex.org/W1504499192', 'https://openalex.org/W1982278227', 'https://openalex.org/W1988865477', 'https://openalex.org/W2725520289', 'https://openalex.org/W2027905379', 'https://openalex.org/W2051353051', 'https://openalex.org/W2162711368', 'https://openalex.org/W2012803517', 'https://openalex.org/W3094040012', 'https://openalex.org/W2922023527', 'https://openalex.org/W3039802694', 'https://openalex.org/W3038121656', 'https://openalex.org/W3187035014', 'https://openalex.org/W154652619', 'https://openalex.org/W2765117211', 'https://openalex.org/W6702248584', 'https://openalex.org/W2127141656', 'https://openalex.org/W2168103112']",2022-03-15
https://openalex.org/W4394773771,https://doi.org/10.1162/tacl_a_00656,What Do Self-Supervised Speech Models Know About Words?,"Abstract Many self-supervised speech models (S3Ms) have been introduced over the last few years, improving performance and data efficiency on various speech tasks. However, these empirical successes alone do not give a complete picture of what is learned during pre-training. Recent work has begun analyzing how S3Ms encode certain properties, such as phonetic and speaker information, but we still lack a proper understanding of knowledge encoded at the word level and beyond. In this work, we use lightweight analysis methods to study segment-level linguistic properties—word identity, boundaries, pronunciation, syntactic features, and semantic features—encoded in S3Ms. We present a comparative study of layer-wise representations from ten S3Ms and find that (i) the frame-level representations within each word segment are not all equally informative, and (ii) the pre-training objective and model size heavily influence the accessibility and distribution of linguistic information across layers. We also find that on several tasks—word discrimination, word segmentation, and semantic sentence similarity—S3Ms trained with visual grounding outperform their speech-only counterparts. Finally, our task-based analyses demonstrate improved performance on word segmentation and acoustic word discrimination while using simpler methods than prior work.1","['https://openalex.org/W4385823003', 'https://openalex.org/W6754420807', 'https://openalex.org/W4296710617', 'https://openalex.org/W3044967013', 'https://openalex.org/W4385823338', 'https://openalex.org/W6795952400', 'https://openalex.org/W6810007534', 'https://openalex.org/W6780218876', 'https://openalex.org/W4319862401', 'https://openalex.org/W3202070718', 'https://openalex.org/W2906152891', 'https://openalex.org/W3198782837', 'https://openalex.org/W2407151108', 'https://openalex.org/W3203140070', 'https://openalex.org/W3198694222', 'https://openalex.org/W3209984917', 'https://openalex.org/W6803547063', 'https://openalex.org/W6748452836', 'https://openalex.org/W3209993061', 'https://openalex.org/W6755207826', 'https://openalex.org/W3093096176', 'https://openalex.org/W6787335539', 'https://openalex.org/W2251253014', 'https://openalex.org/W2963419157', 'https://openalex.org/W6839512648', 'https://openalex.org/W4372346125', 'https://openalex.org/W2166637769', 'https://openalex.org/W3097777922', 'https://openalex.org/W2962753610', 'https://openalex.org/W6731763572', 'https://openalex.org/W2970862333', 'https://openalex.org/W2025341678', 'https://openalex.org/W3174311593', 'https://openalex.org/W6792927658', 'https://openalex.org/W3160799772', 'https://openalex.org/W3095706145', 'https://openalex.org/W4225529283', 'https://openalex.org/W1967924372', 'https://openalex.org/W6839364956', 'https://openalex.org/W2995181338', 'https://openalex.org/W4313182775', 'https://openalex.org/W2190506272', 'https://openalex.org/W4223651314', 'https://openalex.org/W6761472960', 'https://openalex.org/W3096656254', 'https://openalex.org/W6790356757', 'https://openalex.org/W2059652594', 'https://openalex.org/W4319862479', 'https://openalex.org/W4391021793', 'https://openalex.org/W4385823426', 'https://openalex.org/W2972584841', 'https://openalex.org/W3163596720', 'https://openalex.org/W1632114991', 'https://openalex.org/W2747874407', 'https://openalex.org/W3198266945', 'https://openalex.org/W4306317873', 'https://openalex.org/W2065157922', 'https://openalex.org/W4281492411', 'https://openalex.org/W6752726010', 'https://openalex.org/W4303649106', 'https://openalex.org/W3110458199', 'https://openalex.org/W4307680525', 'https://openalex.org/W2963259843', 'https://openalex.org/W1494198834', 'https://openalex.org/W4226380987', 'https://openalex.org/W4375869259', 'https://openalex.org/W4287887773', 'https://openalex.org/W4224875474', 'https://openalex.org/W6786885278', 'https://openalex.org/W2250539671', 'https://openalex.org/W2145410271', 'https://openalex.org/W3034273309', 'https://openalex.org/W6745682157', 'https://openalex.org/W1558402681', 'https://openalex.org/W3155744586', 'https://openalex.org/W4206075291', 'https://openalex.org/W4375869060', 'https://openalex.org/W6948152991', 'https://openalex.org/W2932675979', 'https://openalex.org/W2962736743', 'https://openalex.org/W6788328058', 'https://openalex.org/W4385823328', 'https://openalex.org/W6810168380', 'https://openalex.org/W3150750326', 'https://openalex.org/W4385571440', 'https://openalex.org/W4226103796', 'https://openalex.org/W1606268232', 'https://openalex.org/W2946417913', 'https://openalex.org/W4285250921', 'https://openalex.org/W2963482440', 'https://openalex.org/W2251066368', 'https://openalex.org/W3198815374', 'https://openalex.org/W3150635893', 'https://openalex.org/W6739901393', 'https://openalex.org/W3008003211', 'https://openalex.org/W2970820321', 'https://openalex.org/W3119308075', 'https://openalex.org/W4372270126', 'https://openalex.org/W4385574560', 'https://openalex.org/W4385822254', 'https://openalex.org/W6853627120', 'https://openalex.org/W3197580070', 'https://openalex.org/W4385484924', 'https://openalex.org/W4386273179', 'https://openalex.org/W4385573456', 'https://openalex.org/W398859631', 'https://openalex.org/W569478347', 'https://openalex.org/W3096196861', 'https://openalex.org/W4283694096', 'https://openalex.org/W2593390416', 'https://openalex.org/W4394671563', 'https://openalex.org/W4385245566', 'https://openalex.org/W4319779871', 'https://openalex.org/W2891205112', 'https://openalex.org/W4280638376', 'https://openalex.org/W2602024037', 'https://openalex.org/W4310895557']",2024-01-01
https://openalex.org/W4285203343,https://doi.org/10.1109/taslp.2022.3171967,Self-Supervised Pre-Training for Attention-Based Encoder-Decoder ASR Model,"End-to-end (E2E) models, including the attention-based encoder-decoder (AED) models, have achieved promising performance on the automatic speech recognition (ASR) task. However, the supervised training process of the E2E model needs a large amount of speech-text paired data. In contrast, self-supervised pre-training can pre-train the model on the unlabeled data and then fine-tune it on the limited labeled data to realize better performance. Most of the previous self-supervised pre-training methods focus on learning hidden representations from speech but ignore how to utilize the unpaired text. As a result, previous works often pre-train an acoustic encoder and then fine-tune it as a classification based ASR model, such as Connectionist Temporal Classification (CTC) based model, rather than an AED model. In this paper, we propose a self-supervised pre-training method for the AED model (SP-AED). The SP-AED method contains acoustic pre-training for the encoder, linguistic pre-training for the decoder, and an adaptive combination fine-tuning for the whole system. We first design a linguistic pre-training method for decoder by utilizing the text-only data. The decoder will be pre-trained as a noise-condition language model to learn the prior distribution of the text. Then, we pre-train the AED encoder with the wav2vec2.0 method with some modifications. Finally, we combine the pre-trained encoder and decoder and fine-tune them on the limited labeled data. We design an adaptive combination method during fine-tuning by modifying the decoder’s input and output to prevent catastrophic forgetting. Experiments prove that compared with the random initialized models, the SP-AED pre-trained models can realize up to 17% relative improvement. And with similar model size or computational cost, we can get comparable results to other classification-based models on both English and Chinese corpus.","['https://openalex.org/W6769365793', 'https://openalex.org/W3015280134', 'https://openalex.org/W6770514103', 'https://openalex.org/W6769196770', 'https://openalex.org/W2973049979', 'https://openalex.org/W6844194202', 'https://openalex.org/W2972889948', 'https://openalex.org/W3209984917', 'https://openalex.org/W6784614252', 'https://openalex.org/W6780218876', 'https://openalex.org/W1494198834', 'https://openalex.org/W6754473786', 'https://openalex.org/W2963242190', 'https://openalex.org/W2944073381', 'https://openalex.org/W3160799772', 'https://openalex.org/W2962784628', 'https://openalex.org/W6786669483', 'https://openalex.org/W6778265221', 'https://openalex.org/W6780361010', 'https://openalex.org/W3209059054', 'https://openalex.org/W3154932257', 'https://openalex.org/W6779919476', 'https://openalex.org/W6804030475', 'https://openalex.org/W6687566353', 'https://openalex.org/W6746892368', 'https://openalex.org/W3097286738', 'https://openalex.org/W3035202887', 'https://openalex.org/W3016011332', 'https://openalex.org/W3160345865', 'https://openalex.org/W6769238691', 'https://openalex.org/W2982223350', 'https://openalex.org/W6784776607', 'https://openalex.org/W6739901393', 'https://openalex.org/W3097777922', 'https://openalex.org/W6729448088', 'https://openalex.org/W2964110616', 'https://openalex.org/W2964012862', 'https://openalex.org/W2973132140', 'https://openalex.org/W6754278344', 'https://openalex.org/W2752796333', 'https://openalex.org/W6640963894', 'https://openalex.org/W3163605596', 'https://openalex.org/W6755207826', 'https://openalex.org/W2883586237', 'https://openalex.org/W2962739339', 'https://openalex.org/W6779326418', 'https://openalex.org/W6774314701', 'https://openalex.org/W3035524453', 'https://openalex.org/W6745117592', 'https://openalex.org/W3100270690', 'https://openalex.org/W2951974815', 'https://openalex.org/W2972943112', 'https://openalex.org/W3018242354', 'https://openalex.org/W2972977747', 'https://openalex.org/W6679436768', 'https://openalex.org/W3015457435', 'https://openalex.org/W2962824709', 'https://openalex.org/W2327501763', 'https://openalex.org/W6728030952', 'https://openalex.org/W6761563299', 'https://openalex.org/W3163793923', 'https://openalex.org/W2972818416', 'https://openalex.org/W2972389417', 'https://openalex.org/W2766219058', 'https://openalex.org/W3122931219', 'https://openalex.org/W2963739817', 'https://openalex.org/W2990906560', 'https://openalex.org/W3096626135', 'https://openalex.org/W3198858531', 'https://openalex.org/W3041561163', 'https://openalex.org/W2981991061', 'https://openalex.org/W2758785877', 'https://openalex.org/W3015208154', 'https://openalex.org/W2889048668', 'https://openalex.org/W3155427814', 'https://openalex.org/W3026842484', 'https://openalex.org/W3007328579', 'https://openalex.org/W2896457183', 'https://openalex.org/W3093579165', 'https://openalex.org/W4385245566', 'https://openalex.org/W3198429080', 'https://openalex.org/W3112034174', 'https://openalex.org/W2941814890', 'https://openalex.org/W2193413348', 'https://openalex.org/W3036601975', 'https://openalex.org/W2979476256', 'https://openalex.org/W2988736778', 'https://openalex.org/W2526425061', 'https://openalex.org/W1959608418', 'https://openalex.org/W2982095018', 'https://openalex.org/W3213029956', 'https://openalex.org/W3005680577', 'https://openalex.org/W3035060554', 'https://openalex.org/W2547875792', 'https://openalex.org/W2963799213', 'https://openalex.org/W4297808394', 'https://openalex.org/W2887997457', 'https://openalex.org/W2130942839']",2022-01-01
https://openalex.org/W4287889585,https://doi.org/10.1109/jstsp.2022.3193761,A Comparative Study of Self-Supervised Speech Representation Based Voice Conversion,"We present a large-scale comparative study of self-supervised speech representation (S3R)-based voice conversion (VC). In the context of recognition-synthesis VC, S3Rs are attractive owing to their potential to replace expensive supervised representations such as phonetic posteriorgrams (PPGs), which are commonly adopted by state-of-the-art VC systems. Using S3PRL-VC, an open-source VC software we previously developed, we provide a series of in-depth objective and subjective analyses under three VC settings: intra-/cross-lingual any-to-one (A2O) and any-to-any (A2A) VC, using the voice conversion challenge 2020 (VCC2020) dataset. We investigated S3R-based VC in various aspects, including model type, multilinguality, and supervision. We also studied the effect of a post-discretization process with k-means clustering and showed how it improves in the A2A setting. Finally, the comparison with state-of-the-art VC systems demonstrates the competitiveness of S3R-based VC and also sheds light on the possible improving directions.","['https://openalex.org/W2576309025', 'https://openalex.org/W3098557217', 'https://openalex.org/W2124331435', 'https://openalex.org/W2123003832', 'https://openalex.org/W2403098732', 'https://openalex.org/W2022125261', 'https://openalex.org/W2048646122', 'https://openalex.org/W2187234408', 'https://openalex.org/W2100819376', 'https://openalex.org/W2156142001', 'https://openalex.org/W2120605154', 'https://openalex.org/W2118850452', 'https://openalex.org/W2902070858', 'https://openalex.org/W6640963894', 'https://openalex.org/W2532494225', 'https://openalex.org/W2962896155', 'https://openalex.org/W2752796333', 'https://openalex.org/W2972374322', 'https://openalex.org/W2972659941', 'https://openalex.org/W2996414377', 'https://openalex.org/W6840412704', 'https://openalex.org/W3092368332', 'https://openalex.org/W3081565196', 'https://openalex.org/W4249468441', 'https://openalex.org/W4247587863', 'https://openalex.org/W4244543785', 'https://openalex.org/W6780218876', 'https://openalex.org/W3198275944', 'https://openalex.org/W3207300132', 'https://openalex.org/W3197580070', 'https://openalex.org/W2889329491', 'https://openalex.org/W3140429000', 'https://openalex.org/W3161695192', 'https://openalex.org/W3161627112', 'https://openalex.org/W3197763626', 'https://openalex.org/W4210774711', 'https://openalex.org/W3094635600', 'https://openalex.org/W2518172956', 'https://openalex.org/W2897353073', 'https://openalex.org/W3015434413', 'https://openalex.org/W6769196770', 'https://openalex.org/W3091890228', 'https://openalex.org/W3016011332', 'https://openalex.org/W3097286738', 'https://openalex.org/W2982223350', 'https://openalex.org/W3041561163', 'https://openalex.org/W3198858531', 'https://openalex.org/W6844194202', 'https://openalex.org/W3016181583', 'https://openalex.org/W2973049979', 'https://openalex.org/W3160799772', 'https://openalex.org/W3015213852', 'https://openalex.org/W1494198834', 'https://openalex.org/W2995181338', 'https://openalex.org/W1635512741', 'https://openalex.org/W6739901393', 'https://openalex.org/W2595110011', 'https://openalex.org/W2801493797', 'https://openalex.org/W2963609956', 'https://openalex.org/W2964243274', 'https://openalex.org/W6936113694', 'https://openalex.org/W2046056978', 'https://openalex.org/W2981087920', 'https://openalex.org/W2808631503', 'https://openalex.org/W6795952400', 'https://openalex.org/W6783867762', 'https://openalex.org/W3015265920', 'https://openalex.org/W2471520273', 'https://openalex.org/W3083776549', 'https://openalex.org/W3025844872', 'https://openalex.org/W3198429080', 'https://openalex.org/W2786868129', 'https://openalex.org/W2973108288', 'https://openalex.org/W2903365642', 'https://openalex.org/W4285250921', 'https://openalex.org/W3099078140', 'https://openalex.org/W1959608418', 'https://openalex.org/W4297808394', 'https://openalex.org/W3101689408']",2022-07-25
https://openalex.org/W4386025763,https://doi.org/10.1109/taslp.2023.3306709,Alternative Pseudo-Labeling for Semi-Supervised Automatic Speech Recognition,"When labeled data is insufficient, semi-supervised learning with the pseudo-labeling technique can significantly improve the performance of automatic speech recognition. However, pseudo-labels are often noisy, containing numerous incorrect tokens. Taking noisy labels as ground-truth in the loss function results in suboptimal performance. Previous works attempted to mitigate this issue by either filtering out the nosiest pseudo-labels or improving the overall quality of pseudo-labels. While these methods are effective to some extent, it is unrealistic to entirely eliminate incorrect tokens in pseudo-labels. In this work, we propose a novel framework named alternative pseudo-labeling to tackle the issue of noisy pseudo-labels from the perspective of the training objective. The framework comprises several components. Firstly, a generalized CTC loss function is introduced to handle noisy pseudo-labels by accepting alternative tokens in the positions of incorrect tokens. Applying this loss function in pseudo-labeling requires detecting incorrect tokens in the predicted pseudo-labels. In this work, we adopt a confidence-based error detection method that identifies the incorrect tokens by comparing their confidence scores with a given threshold, thus necessitating the confidence score to be discriminative. Hence, the second proposed technique is the contrastive CTC loss function that widens the confidence gap between the correctly and incorrectly predicted tokens, thereby improving the error detection ability. Additionally, obtaining satisfactory performance with confidence-based error detection typically requires extensive threshold tuning. Instead, we propose an automatic thresholding method that uses labeled data as a proxy for determining the threshold, thus saving the pain of manual tuning. Experiments demonstrate that alternative pseudo-labeling outperforms existing pseudo-labeling approaches on datasets in various domains and languages.","['https://openalex.org/W3163464943', 'https://openalex.org/W3202184514', 'https://openalex.org/W3015522062', 'https://openalex.org/W2963211739', 'https://openalex.org/W4225755266', 'https://openalex.org/W1494198834', 'https://openalex.org/W3096338464', 'https://openalex.org/W6839041728', 'https://openalex.org/W2802248956', 'https://openalex.org/W3162833755', 'https://openalex.org/W3197223534', 'https://openalex.org/W6783314596', 'https://openalex.org/W3023953056', 'https://openalex.org/W2046932483', 'https://openalex.org/W4221040649', 'https://openalex.org/W3211278025', 'https://openalex.org/W6784614252', 'https://openalex.org/W3095350795', 'https://openalex.org/W2799473636', 'https://openalex.org/W6762913911', 'https://openalex.org/W4385656656', 'https://openalex.org/W3119635706', 'https://openalex.org/W6804742662', 'https://openalex.org/W3163169798', 'https://openalex.org/W2936774411', 'https://openalex.org/W6638749077', 'https://openalex.org/W6617145748', 'https://openalex.org/W2327501763', 'https://openalex.org/W3198840231', 'https://openalex.org/W2085598899', 'https://openalex.org/W2933138175', 'https://openalex.org/W2127141656', 'https://openalex.org/W3004534439', 'https://openalex.org/W3162105464', 'https://openalex.org/W3198098585', 'https://openalex.org/W3160799772', 'https://openalex.org/W3026041220', 'https://openalex.org/W2888867175', 'https://openalex.org/W2402146185', 'https://openalex.org/W6780218876', 'https://openalex.org/W3097777922', 'https://openalex.org/W2963242190', 'https://openalex.org/W2978426779', 'https://openalex.org/W3093579165', 'https://openalex.org/W582134693', 'https://openalex.org/W4280634279', 'https://openalex.org/W3036601975', 'https://openalex.org/W1828163288', 'https://openalex.org/W3216565018', 'https://openalex.org/W3101648800', 'https://openalex.org/W4287647128']",2023-01-01
https://openalex.org/W3205715971,https://doi.org/10.1109/icassp43922.2022.9746449,Multi-Modal Pre-Training for Automated Speech Recognition,"Traditionally, research in automated speech recognition has focused on local-first encoding of audio representations to predict the spoken phonemes in an utterance. Unfortunately, approaches relying on such hyper-local information tend to be vulnerable to both local-level corruption (such as audio-frame drops, or loud noises) and global-level noise (such as environmental noise, or background noise) that has not been seen during training. In this work, we introduce a novel approach that leverages a self-supervised learning technique based on masked language modeling to compute a global, multi-modal encoding of the environment in which the utterance occurs. We then use a new deep-fusion framework to integrate this global context into a traditional ASR method, and demonstrate that the resulting method can outperform baseline methods by up to 7% on Librispeech; gains on internal datasets range from 6% (on larger models) to 45% (on smaller models).","['https://openalex.org/W6793736971', 'https://openalex.org/W6755207826', 'https://openalex.org/W2981851019', 'https://openalex.org/W3160799772', 'https://openalex.org/W6793728465', 'https://openalex.org/W6810168380', 'https://openalex.org/W3143320354', 'https://openalex.org/W4214612132', 'https://openalex.org/W6790307280', 'https://openalex.org/W6754337694', 'https://openalex.org/W3149629662', 'https://openalex.org/W3097777922', 'https://openalex.org/W6780294235', 'https://openalex.org/W1494198834', 'https://openalex.org/W3097647528', 'https://openalex.org/W2711861986', 'https://openalex.org/W3016010032', 'https://openalex.org/W2911629330', 'https://openalex.org/W2143612262', 'https://openalex.org/W6631190155', 'https://openalex.org/W6770506093', 'https://openalex.org/W2936774411', 'https://openalex.org/W3095173472', 'https://openalex.org/W2972818416', 'https://openalex.org/W3021469861', 'https://openalex.org/W3126721948', 'https://openalex.org/W2963376890', 'https://openalex.org/W2963341956', 'https://openalex.org/W2402144811', 'https://openalex.org/W3147387781', 'https://openalex.org/W3162391496', 'https://openalex.org/W3037309139', 'https://openalex.org/W3025165719', 'https://openalex.org/W2991213871', 'https://openalex.org/W3007328579', 'https://openalex.org/W2964051877', 'https://openalex.org/W2887051120', 'https://openalex.org/W4221153068', 'https://openalex.org/W2964121744', 'https://openalex.org/W3154596443', 'https://openalex.org/W2896457183', 'https://openalex.org/W1522301498', 'https://openalex.org/W3157916917']",2022-04-27
https://openalex.org/W4385656656,https://doi.org/10.1109/taslp.2023.3301230,Boosting Cross-Domain Speech Recognition With Self-Supervision,"The cross-domain performance of automatic speech recognition (ASR) could be severely hampered due to the mismatch between training and testing distributions. Since the target domain usually lacks labeled data, and domain shifts exist at acoustic and linguistic levels, it is challenging to perform unsupervised domain adaptation (UDA) for ASR. Previous work has shown that self-supervised learning (SSL) or pseudo-labeling (PL) is effective in UDA by exploiting the self-supervisions of unlabeled data. However, these self-supervisions also face performance degradation in mismatched domain distributions, which previous work fails to address. This work presents a systematic UDA framework to fully utilize the unlabeled data with self-supervision in the pre-training and fine-tuning paradigm. On the one hand, we apply continued pre-training and data replay techniques to mitigate the domain mismatch of the SSL pre-trained model. On the other hand, we propose a domain-adaptive fine-tuning approach based on the PL technique with three unique modifications: Firstly, we design a dual-branch PL method to decrease the sensitivity to the erroneous pseudo-labels; Secondly, we devise an uncertainty-aware confidence filtering strategy to improve pseudo-label correctness; Thirdly, we introduce a two-step PL approach to incorporate target domain linguistic knowledge, thus generating more accurate target domain pseudo-labels. Experimental results on various cross-domain scenarios demonstrate that the proposed approach effectively boosts the cross-domain performance and significantly outperforms previous approaches.","['https://openalex.org/W3211278025', 'https://openalex.org/W4221040649', 'https://openalex.org/W3008870153', 'https://openalex.org/W3018242354', 'https://openalex.org/W3015457435', 'https://openalex.org/W3112702554', 'https://openalex.org/W2889423969', 'https://openalex.org/W3165404421', 'https://openalex.org/W2962894366', 'https://openalex.org/W2963364041', 'https://openalex.org/W2963057973', 'https://openalex.org/W3094667432', 'https://openalex.org/W3161143478', 'https://openalex.org/W4225262679', 'https://openalex.org/W3194701944', 'https://openalex.org/W2962684181', 'https://openalex.org/W6779345877', 'https://openalex.org/W6758833332', 'https://openalex.org/W6762753787', 'https://openalex.org/W2896457183', 'https://openalex.org/W3202037040', 'https://openalex.org/W3198447122', 'https://openalex.org/W3198771897', 'https://openalex.org/W3163464943', 'https://openalex.org/W3198098585', 'https://openalex.org/W6779157540', 'https://openalex.org/W3091002423', 'https://openalex.org/W3015522062', 'https://openalex.org/W3026041220', 'https://openalex.org/W3163169798', 'https://openalex.org/W3034238904', 'https://openalex.org/W6840848472', 'https://openalex.org/W4226246059', 'https://openalex.org/W6639480849', 'https://openalex.org/W2998115938', 'https://openalex.org/W6780218876', 'https://openalex.org/W2973157397', 'https://openalex.org/W2972943112', 'https://openalex.org/W3041561163', 'https://openalex.org/W3160799772', 'https://openalex.org/W3197580070', 'https://openalex.org/W4226033575', 'https://openalex.org/W3163605596', 'https://openalex.org/W4285203343', 'https://openalex.org/W6811201773', 'https://openalex.org/W3212799896', 'https://openalex.org/W3160235762', 'https://openalex.org/W4225529283', 'https://openalex.org/W4226380987', 'https://openalex.org/W6773005947', 'https://openalex.org/W6802864417', 'https://openalex.org/W6838923433', 'https://openalex.org/W3096273170', 'https://openalex.org/W3015737168', 'https://openalex.org/W2940322076', 'https://openalex.org/W6770506093', 'https://openalex.org/W3096338464', 'https://openalex.org/W3095350795', 'https://openalex.org/W4225755266', 'https://openalex.org/W2033256038', 'https://openalex.org/W2289874485', 'https://openalex.org/W2510349098', 'https://openalex.org/W3119635706', 'https://openalex.org/W6762913911', 'https://openalex.org/W3198840231', 'https://openalex.org/W2127141656', 'https://openalex.org/W6784614252', 'https://openalex.org/W3197223534', 'https://openalex.org/W3209976096', 'https://openalex.org/W2591949110', 'https://openalex.org/W1494198834', 'https://openalex.org/W2799473636', 'https://openalex.org/W6771467084', 'https://openalex.org/W6691770337', 'https://openalex.org/W2933138175', 'https://openalex.org/W2936774411', 'https://openalex.org/W6797001314', 'https://openalex.org/W3163793923', 'https://openalex.org/W6788335241', 'https://openalex.org/W6791904447', 'https://openalex.org/W3198270883', 'https://openalex.org/W2327501763', 'https://openalex.org/W2144499799', 'https://openalex.org/W4225699246', 'https://openalex.org/W3101648800', 'https://openalex.org/W3139918052', 'https://openalex.org/W3037190319']",2023-08-08
https://openalex.org/W3161101519,https://doi.org/10.1109/icassp39728.2021.9414079,Contrastive Semi-Supervised Learning for ASR,"Pseudo-labeling is the most adopted method for pre-training automatic speech recognition (ASR) models. However, its performance suffers with degrading quality of the supervised teacher model.Inspired by the successes of contrastive representation learning for both computer vision and speech applications, and more recently for supervised learning of visual objects [1], we propose Contrastive Semi-supervised Learning (CSL). CSL eschews directly predicting teacher generated pseudo-labels in favor of utilizing them to select positive and negative examples.In the challenging task of transcribing public social media videos, using CSL reduces the WER by 8%, compared to the standard Cross-Entropy pseudo-labeling (CE-PL), when 10hr of supervised data is used to annotate 75,000hr of videos. The WER reduction jumps to 19% under the ultra low-resource condition of using 1hr labels for teacher supervision. In out-of-domain conditions, CSL generalizes much better showing up to 17% WER reduction compared to the strongest CE-PL pre-trained model.","['https://openalex.org/W6777687200', 'https://openalex.org/W2936774411', 'https://openalex.org/W6745245109', 'https://openalex.org/W6775452034', 'https://openalex.org/W3026041220', 'https://openalex.org/W2940322076', 'https://openalex.org/W6679909955', 'https://openalex.org/W2033256038', 'https://openalex.org/W3035524453', 'https://openalex.org/W2842511635', 'https://openalex.org/W2973049979', 'https://openalex.org/W6780218876', 'https://openalex.org/W6770514103', 'https://openalex.org/W6713762819', 'https://openalex.org/W2110073835', 'https://openalex.org/W2981857663', 'https://openalex.org/W6600633252', 'https://openalex.org/W6631190155', 'https://openalex.org/W2056786202', 'https://openalex.org/W3094647783', 'https://openalex.org/W2139698650', 'https://openalex.org/W6871885761', 'https://openalex.org/W6770506093', 'https://openalex.org/W6776700526', 'https://openalex.org/W3016011332', 'https://openalex.org/W3015265920', 'https://openalex.org/W2982223350', 'https://openalex.org/W3008525923', 'https://openalex.org/W6795346631', 'https://openalex.org/W6739622702', 'https://openalex.org/W2127141656', 'https://openalex.org/W2407080277', 'https://openalex.org/W2622263826', 'https://openalex.org/W3160799772', 'https://openalex.org/W3096338464', 'https://openalex.org/W2963112338', 'https://openalex.org/W2763421725', 'https://openalex.org/W4297808394', 'https://openalex.org/W2134797427', 'https://openalex.org/W4402490925', 'https://openalex.org/W1555037511', 'https://openalex.org/W2988736778', 'https://openalex.org/W3100345210', 'https://openalex.org/W2979286696', 'https://openalex.org/W3036601975', 'https://openalex.org/W3015522062', 'https://openalex.org/W2964121744', 'https://openalex.org/W15497043', 'https://openalex.org/W2991213871', 'https://openalex.org/W4287812705', 'https://openalex.org/W1522301498', 'https://openalex.org/W3099782249']",2021-05-13
https://openalex.org/W4283324001,https://doi.org/10.21437/interspeech.2022-936,Supervision-Guided Codebooks for Masked Prediction in Speech Pre-training,"Recently, masked prediction pre-training has seen remarkable progress in self-supervised learning (SSL) for speech recognition.It usually requires a codebook obtained in an unsupervised way, making it less accurate and difficult to interpret.We propose two supervision-guided codebook generation approaches to improve automatic speech recognition (ASR) performance and also the pre-training efficiency, either through decoding with a hybrid ASR system to generate phoneme-level alignments (named PBERT), or performing clustering on the supervised speech features extracted from an end-to-end CTC model (named CTC clustering).Both the hybrid and CTC models are trained on the same small amount of labeled speech as used in fine-tuning.Experiments demonstrate significant superiority of our methods to various SSL and self-training baselines, with up to 17.0% relative WER reduction.Our pre-trained models also show good transferability in a non-ASR speech task.","['https://openalex.org/W3096338464', 'https://openalex.org/W2940322076', 'https://openalex.org/W4297808394', 'https://openalex.org/W2896457183', 'https://openalex.org/W2888867175', 'https://openalex.org/W1993660824', 'https://openalex.org/W2514741789', 'https://openalex.org/W2079623482', 'https://openalex.org/W2150769028', 'https://openalex.org/W3197580070', 'https://openalex.org/W2953190524', 'https://openalex.org/W3016011332', 'https://openalex.org/W4288089799', 'https://openalex.org/W2150907703', 'https://openalex.org/W2908510526', 'https://openalex.org/W4288072840', 'https://openalex.org/W1877570817', 'https://openalex.org/W1524333225', 'https://openalex.org/W2808631503', 'https://openalex.org/W2160815625', 'https://openalex.org/W4385245566', 'https://openalex.org/W1494198834', 'https://openalex.org/W2973049979', 'https://openalex.org/W3093579165', 'https://openalex.org/W4221145109', 'https://openalex.org/W3209984917', 'https://openalex.org/W3026041220', 'https://openalex.org/W4285666836', 'https://openalex.org/W3008525923', 'https://openalex.org/W4225741214', 'https://openalex.org/W3015522062', 'https://openalex.org/W4221161761', 'https://openalex.org/W3036601975', 'https://openalex.org/W3160799772', 'https://openalex.org/W3160525311', 'https://openalex.org/W2979476256']",2022-09-16
https://openalex.org/W4392904442,https://doi.org/10.1109/icassp48485.2024.10446812,Frame-Level Emotional State Alignment Method for Speech Emotion Recognition,"Speech emotion recognition (SER) systems aim to recognize human emotional state during human-computer interaction. Most existing SER systems are trained based on utterance-level labels. However, not all frames in an audio have affective states consistent with utterance-level label, which makes it difficult for the model to distinguish the true emotion of the audio and perform poorly. To address this problem, we propose a frame-level emotional state alignment method for SER. First, we fine-tune HuBERT model to obtain an SER system with task-adaptive pretraining (TAPT) method, and extract embeddings from its transformer layers to form frame-level pseudo-emotion labels with clustering. Then, the pseudo labels are used to pretrain HuBERT. Hence, each frame from the output of HuBERT has corresponding emotional information. Finally, we fine-tune the above pretrained HuBERT for SER by adding an attention layer on the top of it, which can focus only on those frames that are emotionally more consistent with utterance-level label. The experimental results performed on IEMOCAP indicate that our proposed method performs better than state-of-the-art (SOTA) methods. The codes are available at github repository <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .","['https://openalex.org/W2625297138', 'https://openalex.org/W2940259008', 'https://openalex.org/W3162993161', 'https://openalex.org/W4225320271', 'https://openalex.org/W2896457183', 'https://openalex.org/W2973049979', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W3160799772', 'https://openalex.org/W4372341110', 'https://openalex.org/W4221162793', 'https://openalex.org/W4372259942', 'https://openalex.org/W3197642003', 'https://openalex.org/W4375869379', 'https://openalex.org/W3197156295', 'https://openalex.org/W6839738141', 'https://openalex.org/W2146334809', 'https://openalex.org/W3034238904', 'https://openalex.org/W4372266023', 'https://openalex.org/W4372260431', 'https://openalex.org/W4372267501', 'https://openalex.org/W3036601975']",2024-03-18
https://openalex.org/W4290712827,https://doi.org/10.1109/jstsp.2022.3197315,Non-Contrastive Self-Supervised Learning for Utterance-Level Information Extraction From Speech,"In recent studies, self-supervised pre-trained models tend to outperform\nsupervised pre-trained models in transfer learning. In particular,\nself-supervised learning (SSL) of utterance-level speech representation can be\nused in speech applications that require discriminative representation of\nconsistent attributes within an utterance: speaker, language, emotion, and age.\nExisting frame-level self-supervised speech representation, e.g., wav2vec, can\nbe used as utterance-level representation with pooling, but the models are\nusually large. There are also SSL techniques to learn utterance-level\nrepresentation. One of the most successful is a contrastive method, which\nrequires negative sampling: selecting alternative samples to contrast with the\ncurrent sample (anchor). However, this does not ensure that all the negative\nsamples belong to classes different from the anchor class without labels. This\npaper applies a non-contrastive self-supervised method to learn utterance-level\nembeddings. We adapted DIstillation with NO labels (DINO) from computer vision\nto speech. Unlike contrastive methods, DINO does not require negative sampling.\nWe compared DINO to x-vector trained in a supervised manner. When transferred\nto down-stream tasks (speaker verification, speech emotion recognition (SER),\nand Alzheimer's disease detection), DINO outperformed x-vector. We studied the\ninfluence of several aspects during transfer learning such as dividing the\nfine-tuning process into steps, chunk lengths, or augmentation. During\nfine-tuning, tuning the last affine layers first and then the whole network\nsurpassed fine-tuning all at once. Using shorter chunk lengths, although they\ngenerate more diverse inputs, did not necessarily improve performance, implying\nspeech segments at least with a specific length are required for better\nperformance per application. Augmentation was helpful in SER.\n","['https://openalex.org/W2896457183', 'https://openalex.org/W2911489562', 'https://openalex.org/W6780218876', 'https://openalex.org/W6779977557', 'https://openalex.org/W6779326418', 'https://openalex.org/W3159481202', 'https://openalex.org/W3015213852', 'https://openalex.org/W2972943112', 'https://openalex.org/W2982223350', 'https://openalex.org/W3198858531', 'https://openalex.org/W3041561163', 'https://openalex.org/W3160799772', 'https://openalex.org/W3197580070', 'https://openalex.org/W6745117592', 'https://openalex.org/W2972705840', 'https://openalex.org/W3097448764', 'https://openalex.org/W3016175755', 'https://openalex.org/W6781368565', 'https://openalex.org/W3161606033', 'https://openalex.org/W3160397447', 'https://openalex.org/W2963182768', 'https://openalex.org/W3015707499', 'https://openalex.org/W3207346153', 'https://openalex.org/W3162890625', 'https://openalex.org/W3097288651', 'https://openalex.org/W3197368257', 'https://openalex.org/W2969985801', 'https://openalex.org/W2748488820', 'https://openalex.org/W2890964092', 'https://openalex.org/W2194775991', 'https://openalex.org/W6739901393', 'https://openalex.org/W2979593053', 'https://openalex.org/W4205234379', 'https://openalex.org/W6733814495', 'https://openalex.org/W2086161653', 'https://openalex.org/W1589137271', 'https://openalex.org/W6784400926', 'https://openalex.org/W2808631503', 'https://openalex.org/W2146334809', 'https://openalex.org/W3154143698', 'https://openalex.org/W2726515241', 'https://openalex.org/W6631190155', 'https://openalex.org/W2696967604', 'https://openalex.org/W2928165649', 'https://openalex.org/W3100859887', 'https://openalex.org/W3036601975', 'https://openalex.org/W4286981691', 'https://openalex.org/W3002741552', 'https://openalex.org/W4385245566', 'https://openalex.org/W2953070460', 'https://openalex.org/W1522301498', 'https://openalex.org/W2758785877', 'https://openalex.org/W3044308976']",2022-08-08
https://openalex.org/W3179803166,https://doi.org/10.1109/asru51503.2021.9688093,Layer-Wise Analysis of a Self-Supervised Speech Representation Model,"Recently proposed self-supervised learning approaches have been successful for pre-training speech representation models. The utility of these learned representations has been observed empirically, but not much has been studied about the type or extent of information encoded in the pre-trained representations themselves. Developing such insights can help understand the capabilities and limits of these models and enable the research community to more efficiently develop their usage for downstream applications. In this work, we begin to fill this gap by examining one recent and successful pre-trained model (wav2vec 2.0), via its intermediate representation vectors, using a suite of analysis tools. We use the metrics of canonical correlation, mutual information, and performance on simple downstream tasks with non-parametric probes, in order to (i) query for acoustic and linguistic information content, (ii) characterize the evolution of information across model layers, and (iii) understand how fine-tuning the model for automatic speech recognition (ASR) affects these observations. Our findings motivate modifying the fine-tuning protocol for ASR, which produces improved word error rates in a low-resource setting.","['https://openalex.org/W2251253014', 'https://openalex.org/W3146777637', 'https://openalex.org/W2250539671', 'https://openalex.org/W2932675979', 'https://openalex.org/W6752726010', 'https://openalex.org/W6761472960', 'https://openalex.org/W3044967013', 'https://openalex.org/W3095706145', 'https://openalex.org/W1545920196', 'https://openalex.org/W2407151108', 'https://openalex.org/W4237723258', 'https://openalex.org/W2970820321', 'https://openalex.org/W6745682157', 'https://openalex.org/W6755207826', 'https://openalex.org/W343636949', 'https://openalex.org/W3034273309', 'https://openalex.org/W6788328058', 'https://openalex.org/W3163596720', 'https://openalex.org/W6786696081', 'https://openalex.org/W3162133897', 'https://openalex.org/W2946417913', 'https://openalex.org/W2586148577', 'https://openalex.org/W3160799772', 'https://openalex.org/W1494198834', 'https://openalex.org/W6780218876', 'https://openalex.org/W3015265920', 'https://openalex.org/W3197580070', 'https://openalex.org/W3198771897', 'https://openalex.org/W6779919476', 'https://openalex.org/W6795952400', 'https://openalex.org/W3198299542', 'https://openalex.org/W2906152891', 'https://openalex.org/W2963259843', 'https://openalex.org/W2842511635', 'https://openalex.org/W2973049979', 'https://openalex.org/W2973157397', 'https://openalex.org/W3003875258', 'https://openalex.org/W3167533889', 'https://openalex.org/W3016011332', 'https://openalex.org/W2982223350', 'https://openalex.org/W2153579005', 'https://openalex.org/W2963425185', 'https://openalex.org/W2988217457', 'https://openalex.org/W3034709122', 'https://openalex.org/W2127141656', 'https://openalex.org/W2995181338', 'https://openalex.org/W2972584841', 'https://openalex.org/W6743149223', 'https://openalex.org/W3165666670', 'https://openalex.org/W2767204723', 'https://openalex.org/W3099782249', 'https://openalex.org/W2942810103', 'https://openalex.org/W3005680577', 'https://openalex.org/W2025341678', 'https://openalex.org/W2747874407', 'https://openalex.org/W3121914243', 'https://openalex.org/W3110458199', 'https://openalex.org/W3126074026', 'https://openalex.org/W3144173820', 'https://openalex.org/W3096196861', 'https://openalex.org/W3153287399', 'https://openalex.org/W2963759780', 'https://openalex.org/W2963341956', 'https://openalex.org/W3037057938']",2021-12-13
https://openalex.org/W3206559778,https://doi.org/10.1109/icassp43922.2022.9747897,Don't Speak Too Fast: The Impact of Data Bias on Self-Supervised Speech Models,"Self-supervised Speech Models (S3Ms) have been proven successful in many speech downstream tasks, like ASR. However, how pretraining data affects S3Ms' downstream behavior remains an unexplored issue. In this paper, we study how pre-training data affects S3Ms by pre-training models on biased datasets targeting different factors of speech, including gender, content, and prosody, and evaluate these pre-trained S3Ms on selected downstream tasks in SUPERB Benchmark. Our experiments show that S3Ms have tolerance toward gender bias. Moreover, we find that the content of speech has little impact on the performance of S3Ms across downstream tasks, but S3Ms do show a preference toward a slower speech rate.","['https://openalex.org/W6752726010', 'https://openalex.org/W2146334809', 'https://openalex.org/W2972584841', 'https://openalex.org/W2981087920', 'https://openalex.org/W3015265920', 'https://openalex.org/W2962739339', 'https://openalex.org/W3016011332', 'https://openalex.org/W3097286738', 'https://openalex.org/W3035202887', 'https://openalex.org/W6780361010', 'https://openalex.org/W2982223350', 'https://openalex.org/W6755207826', 'https://openalex.org/W3096626135', 'https://openalex.org/W3160345865', 'https://openalex.org/W3198771897', 'https://openalex.org/W3174086521', 'https://openalex.org/W3177397495', 'https://openalex.org/W6785000729', 'https://openalex.org/W3162133897', 'https://openalex.org/W6750665317', 'https://openalex.org/W3096017728', 'https://openalex.org/W3197580070', 'https://openalex.org/W1494198834', 'https://openalex.org/W3160799772', 'https://openalex.org/W6782066904', 'https://openalex.org/W6778265221', 'https://openalex.org/W3135008008', 'https://openalex.org/W6778691839', 'https://openalex.org/W6787898146', 'https://openalex.org/W3197340960', 'https://openalex.org/W3115108114', 'https://openalex.org/W3035070478', 'https://openalex.org/W3112204873', 'https://openalex.org/W3164428426', 'https://openalex.org/W3049256661', 'https://openalex.org/W3097568057', 'https://openalex.org/W3114056688', 'https://openalex.org/W3035357389', 'https://openalex.org/W4294103325', 'https://openalex.org/W2963759780', 'https://openalex.org/W3026842484', 'https://openalex.org/W3041561163', 'https://openalex.org/W2928075308', 'https://openalex.org/W2797583228', 'https://openalex.org/W3144173820', 'https://openalex.org/W3159783145', 'https://openalex.org/W2896457183', 'https://openalex.org/W2963341956', 'https://openalex.org/W3031202003']",2022-04-27
https://openalex.org/W4296070390,https://doi.org/10.21437/interspeech.2022-11093,Online Continual Learning of End-to-End Speech Recognition Models,"Continual Learning, also known as Lifelong Learning, aims to continually learn from new data as it becomes available.While prior research on continual learning in automatic speech recognition has focused on the adaptation of models across multiple different speech recognition tasks, in this paper we propose an experimental setting for online continual learning for automatic speech recognition of a single task.Specifically focusing on the case where additional training data for the same task becomes available incrementally over time, we demonstrate the effectiveness of performing incremental model updates to end-to-end speech recognition models with an online Gradient Episodic Memory (GEM) method.Moreover, we show that with online continual learning and a selective sampling strategy, we can maintain an accuracy that is similar to retraining a model from scratch while requiring significantly lower computation costs.We have also verified our method with self-supervised learning (SSL) features.","['https://openalex.org/W1682403713', 'https://openalex.org/W4312744085', 'https://openalex.org/W3097968133', 'https://openalex.org/W3041214984', 'https://openalex.org/W3101648800', 'https://openalex.org/W2963559848', 'https://openalex.org/W3021931813', 'https://openalex.org/W2963588172', 'https://openalex.org/W4319988532', 'https://openalex.org/W4249647390', 'https://openalex.org/W2533523411', 'https://openalex.org/W4287241171', 'https://openalex.org/W2902456977', 'https://openalex.org/W3106286430', 'https://openalex.org/W3160799772', 'https://openalex.org/W4289366620', 'https://openalex.org/W2902625698', 'https://openalex.org/W4288336773', 'https://openalex.org/W2043701535', 'https://openalex.org/W2554616628', 'https://openalex.org/W2997188627', 'https://openalex.org/W3125116114', 'https://openalex.org/W4295883599', 'https://openalex.org/W4301163820', 'https://openalex.org/W2972818416', 'https://openalex.org/W2926477959', 'https://openalex.org/W3091787298', 'https://openalex.org/W3186596101', 'https://openalex.org/W3023953056', 'https://openalex.org/W2251321385', 'https://openalex.org/W2964048876', 'https://openalex.org/W2939911019', 'https://openalex.org/W2560647685', 'https://openalex.org/W2964189064', 'https://openalex.org/W4298116016', 'https://openalex.org/W2250357346', 'https://openalex.org/W2902417425']",2022-09-16
https://openalex.org/W4375869046,https://doi.org/10.1109/icassp49357.2023.10095825,Early Detection of Cognitive Decline Using Voice Assistant Commands,"Early detection of Alzheimer's Disease and Related Dementias (ADRD) is critical in treating the progression of the disease. Previous studies have shown that ADRD can be detected and classified using machine learning models trained on samples of spontaneous speech. We propose using Voice-Assistant Systems (VAS), e.g., Amazon Alexa, to monitor and collect data from at-risk adults, and we show that this data can be used to achieve functional accuracy in classifying their cognitive status. In this paper, we develop multiple unique feature sets from VAS data that can be used in the training of machine learning models. We then perform multi-class classification, binary classification, and regression using these features on our dataset of older adults with three varying stages of cognitive decline interacting with VAS. Our results show that the VAS data can be used to classify Dementia (DM), Mild Cognitive Impairment (MCI), and Healthy Control (HC) participants with an accuracy up to 74.7%, and classify between HC and MCI with accuracy up to 62.8%.","['https://openalex.org/W3201279130', 'https://openalex.org/W2239141610', 'https://openalex.org/W6755207826', 'https://openalex.org/W3196495667', 'https://openalex.org/W4296069274', 'https://openalex.org/W2965717703', 'https://openalex.org/W2081686552', 'https://openalex.org/W1494198834', 'https://openalex.org/W3160799772', 'https://openalex.org/W2955219525', 'https://openalex.org/W2995181338', 'https://openalex.org/W3097779478', 'https://openalex.org/W3200827588', 'https://openalex.org/W3205171606', 'https://openalex.org/W2896457183']",2023-05-05
https://openalex.org/W4386590854,https://doi.org/10.1109/lsp.2023.3313513,Direct Text to Speech Translation System Using Acoustic Units,"This paper proposes a direct text to speech translation system using discrete\nacoustic units. This framework employs text in different source languages as\ninput to generate speech in the target language without the need for text\ntranscriptions in this language. Motivated by the success of acoustic units in\nprevious works for direct speech to speech translation systems, we use the same\npipeline to extract the acoustic units using a speech encoder combined with a\nclustering algorithm. Once units are obtained, an encoder-decoder architecture\nis trained to predict them. Then a vocoder generates speech from units. Our\napproach for direct text to speech translation was tested on the new CVSS\ncorpus with two different text mBART models employed as initialisation. The\nsystems presented report competitive performance for most of the language pairs\nevaluated. Besides, results show a remarkable improvement when initialising our\nproposed architecture with a model pre-trained with more languages.\n","['https://openalex.org/W6810701745', 'https://openalex.org/W4367841185', 'https://openalex.org/W2964243274', 'https://openalex.org/W3140429000', 'https://openalex.org/W3209059054', 'https://openalex.org/W3160799772', 'https://openalex.org/W6803675045', 'https://openalex.org/W3160525311', 'https://openalex.org/W4287854499', 'https://openalex.org/W2933138175', 'https://openalex.org/W3001434439', 'https://openalex.org/W3213029956', 'https://openalex.org/W3119308075', 'https://openalex.org/W6783867762', 'https://openalex.org/W4375869259', 'https://openalex.org/W6790356757', 'https://openalex.org/W2972495969', 'https://openalex.org/W6841035593', 'https://openalex.org/W3180374548', 'https://openalex.org/W3169483174', 'https://openalex.org/W3173767661', 'https://openalex.org/W4286359908', 'https://openalex.org/W6810259195', 'https://openalex.org/W4226444650', 'https://openalex.org/W4394671563', 'https://openalex.org/W4287072252', 'https://openalex.org/W4221155340', 'https://openalex.org/W3092028330', 'https://openalex.org/W3213018012', 'https://openalex.org/W3169320628', 'https://openalex.org/W3107826490']",2023-01-01
https://openalex.org/W4379806368,https://doi.org/10.1145/3591106.3592296,CLAP: Contrastive Language-Audio Pre-training Model for Multi-modal Sentiment Analysis,"Multi-modal Sentiment Analysis (MSA) is a hotspot of multi-modal fusion. To make full use of the correlation and complementarity between modalities in the process of fusing multi-modal data, we propose a two-stage framework of Contrastive Language-Audio Pre-training (CLAP) for the MSA task: 1) Making contrastive pre-training on an unlabeled large-scaled external data to yield better single-modal representations; 2) Adopting a Transformer-based multi-modal fusion module, to achieve further single-modal feature optimization and sentiment prediction via the task-driven training process. Our work fully demonstrates the importance and necessity of core elements such as pre-training, contrastive learning, and representation learning for the MSA task and significantly outperforms existing methods on two well-recognized MSA benchmarks.","['https://openalex.org/W2767249564', 'https://openalex.org/W2964266095', 'https://openalex.org/W3206529771', 'https://openalex.org/W3214432797', 'https://openalex.org/W3093051361', 'https://openalex.org/W3160799772', 'https://openalex.org/W3193520233', 'https://openalex.org/W2885719698', 'https://openalex.org/W2964346351', 'https://openalex.org/W1494198834', 'https://openalex.org/W2250539671', 'https://openalex.org/W2251394420', 'https://openalex.org/W3015427680', 'https://openalex.org/W3135367836', 'https://openalex.org/W3034266838', 'https://openalex.org/W3037572520', 'https://openalex.org/W2964051877', 'https://openalex.org/W2079725295', 'https://openalex.org/W2168465881', 'https://openalex.org/W3167098825', 'https://openalex.org/W3173909648', 'https://openalex.org/W3128412859', 'https://openalex.org/W2964010806', 'https://openalex.org/W2883409523', 'https://openalex.org/W2556418146', 'https://openalex.org/W2964236337', 'https://openalex.org/W4301104990']",2023-06-08
https://openalex.org/W4406385914,https://doi.org/10.1044/2024_jslhr-23-00740,Speech Technology for Automatic Recognition and Assessment of Dysarthric Speech: An Overview,"Purpose: In this review article, we present an extensive overview of recent developments in the area of dysarthric speech research. One of the key objectives of speech technology research is to improve the quality of life of its users, as evidenced by the focus of current research trends on creating inclusive conversational interfaces that cater to pathological speech, out of which dysarthric speech is an important example. Applications of speech technology research for dysarthric speech demand a clear understanding of the acoustics of dysarthric speech as well as of speech technologies, including machine learning and deep neural networks for speech processing. Method: We review studies pertaining to speech technology and dysarthric speech. Specifically, we discuss dysarthric speech corpora, acoustic analysis, intelligibility assessment, and automatic speech recognition. We also delve into deep learning approaches for automatic assessment and recognition of dysarthric speech. Ethics committee or institutional review board did not apply to this study. Conclusions: Overcoming the challenge of limited data and exploring new avenues in data collection, artificial intelligence–powered analysis and teletherapy hold immense potential for significant advancements in dysarthria research. To make longer and faster strides, researchers typically rely on existing research and data on a global scale. Therefore, it is imperative to consolidate the existing research and present it in a form that can serve as a basis for future work. In this review article, we have reviewed the contributions of speech technologists to the area of dysarthric speech with a focus on acoustic analysis, speech features, and techniques used. By focusing on the existing research and future directions, researchers can develop more effective tools and interventions to improve communication, quality of life, and overall well-being for people with dysarthria.","['https://openalex.org/W3123500774', 'https://openalex.org/W2889227169', 'https://openalex.org/W4297841331', 'https://openalex.org/W2023126832', 'https://openalex.org/W2889162002', 'https://openalex.org/W4297841798', 'https://openalex.org/W2999959629', 'https://openalex.org/W2508408760', 'https://openalex.org/W2512988127', 'https://openalex.org/W2657285084', 'https://openalex.org/W2107514422', 'https://openalex.org/W2586068280', 'https://openalex.org/W3004503465', 'https://openalex.org/W4293354476', 'https://openalex.org/W2982534039', 'https://openalex.org/W3096156096', 'https://openalex.org/W2047309943', 'https://openalex.org/W2087891313', 'https://openalex.org/W2401277329', 'https://openalex.org/W3198429080', 'https://openalex.org/W2061487423', 'https://openalex.org/W2065840343', 'https://openalex.org/W2037453437', 'https://openalex.org/W2042402023', 'https://openalex.org/W6654544852', 'https://openalex.org/W2164843002', 'https://openalex.org/W2107570013', 'https://openalex.org/W244960350', 'https://openalex.org/W3095123370', 'https://openalex.org/W4221162789', 'https://openalex.org/W2793670347', 'https://openalex.org/W4284706223', 'https://openalex.org/W3091860345', 'https://openalex.org/W4297841875', 'https://openalex.org/W3097639020', 'https://openalex.org/W2064996887', 'https://openalex.org/W2170951778', 'https://openalex.org/W3160799772', 'https://openalex.org/W2768204486', 'https://openalex.org/W3110715635', 'https://openalex.org/W4308243510', 'https://openalex.org/W4224315877', 'https://openalex.org/W2050966398', 'https://openalex.org/W4250658925', 'https://openalex.org/W2027370878', 'https://openalex.org/W253551670', 'https://openalex.org/W180052447', 'https://openalex.org/W2001702556', 'https://openalex.org/W2889469831', 'https://openalex.org/W2894784310', 'https://openalex.org/W2394686366', 'https://openalex.org/W2595767284', 'https://openalex.org/W2403170434', 'https://openalex.org/W2159739798', 'https://openalex.org/W2062366328', 'https://openalex.org/W2056500762', 'https://openalex.org/W2081318766', 'https://openalex.org/W2064965815', 'https://openalex.org/W2031072412', 'https://openalex.org/W3140132582', 'https://openalex.org/W2405992935', 'https://openalex.org/W2990287485', 'https://openalex.org/W2140360678', 'https://openalex.org/W97005208', 'https://openalex.org/W2295276961', 'https://openalex.org/W2000428399', 'https://openalex.org/W2095660652', 'https://openalex.org/W2024904926', 'https://openalex.org/W2889443538', 'https://openalex.org/W3198206382', 'https://openalex.org/W2572561003', 'https://openalex.org/W2251281161', 'https://openalex.org/W4250746596', 'https://openalex.org/W2165698076', 'https://openalex.org/W2011410855', 'https://openalex.org/W3049525147', 'https://openalex.org/W4296068414', 'https://openalex.org/W4388853939', 'https://openalex.org/W178045984', 'https://openalex.org/W1995193848', 'https://openalex.org/W2018363392', 'https://openalex.org/W2016125416', 'https://openalex.org/W2250686550', 'https://openalex.org/W3157063407', 'https://openalex.org/W2026303233', 'https://openalex.org/W2115692477', 'https://openalex.org/W2170511794', 'https://openalex.org/W1979721417', 'https://openalex.org/W2118059413', 'https://openalex.org/W3015693260', 'https://openalex.org/W1987743150', 'https://openalex.org/W4297841823', 'https://openalex.org/W2750570963', 'https://openalex.org/W3197646400', 'https://openalex.org/W2749128330', 'https://openalex.org/W2888807255', 'https://openalex.org/W2514170666', 'https://openalex.org/W2900035714', 'https://openalex.org/W2398169716', 'https://openalex.org/W2936861580', 'https://openalex.org/W3014690389', 'https://openalex.org/W4313985071', 'https://openalex.org/W4321368667', 'https://openalex.org/W2524979052', 'https://openalex.org/W2747302936', 'https://openalex.org/W4296068432', 'https://openalex.org/W4304109163', 'https://openalex.org/W4372260311', 'https://openalex.org/W2625561335', 'https://openalex.org/W4372341972', 'https://openalex.org/W2912309762', 'https://openalex.org/W2015441235', 'https://openalex.org/W2398936787']",2025-01-15
https://openalex.org/W4297841405,https://doi.org/10.21437/interspeech.2022-10884,Phonetic Analysis of Self-supervised Representations of English Speech,"We present an analysis of discrete units discovered via selfsupervised representation learning on English speech.We focus on units produced by a pre-trained HuBERT model due to its wide adoption in ASR, speech synthesis, and many other tasks.Whereas previous work has evaluated the quality of such quantization models in aggregate over all phones for a given language, we break our analysis down into broad phonetic classes, taking into account specific aspects of their articulation when considering their alignment to discrete units.We find that these units correspond to sub-phonetic events, and that fine dynamics such as the distinct closure and release portions of plosives tend to be represented by sequences of discrete units.Our work provides a reference for the phonetic properties of discrete units discovered by HuBERT, facilitating analyses of many speech applications based on this model.","['https://openalex.org/W1494198834', 'https://openalex.org/W4288107125', 'https://openalex.org/W1524333225', 'https://openalex.org/W3160200310', 'https://openalex.org/W3140429000', 'https://openalex.org/W3036601975', 'https://openalex.org/W3197580070', 'https://openalex.org/W2896457183', 'https://openalex.org/W3095361818', 'https://openalex.org/W3160799772', 'https://openalex.org/W2998572311', 'https://openalex.org/W4206711328', 'https://openalex.org/W2748598007', 'https://openalex.org/W3169320628', 'https://openalex.org/W4394671563', 'https://openalex.org/W2395899413']",2022-09-16
https://openalex.org/W3210580323,https://doi.org/10.1145/3461615.3485408,Crossmodal Clustered Contrastive Learning: Grounding of Spoken Language to Gesture,"Crossmodal grounding is a key technical challenge when generating relevant and well-timed gestures from spoken language. Often, the same gesture can accompany semantically different spoken language phrases which makes crossmodal grounding especially challenging. For example, a gesture (semi-circular with both hands) could co-occur with semantically different phrases ""entire bottom row"" (referring to a physical point) and ""molecules expand and decay"" (referring to a scientific phenomena). In this paper, we introduce a self-supervised approach to learn representations better suited to such many-to-one grounding relationships between spoken language and gestures. As part of this approach, we propose a new contrastive loss function, Crossmodal Cluster NCE, that guides the model to learn spoken language representations which are consistent with the similarities in the gesture space. This gesture-aware space can help us generate more relevant gestures given language as input. We demonstrate the effectiveness of our approach on a publicly available dataset through quantitative and qualitative evaluations. Our proposed methodology significantly outperforms prior approaches for gestures-language grounding. Link to code: https://github.com/dondongwon/CC_NCE_GENEA.","['https://openalex.org/W3098994456', 'https://openalex.org/W3115266783', 'https://openalex.org/W2981133004', 'https://openalex.org/W2142824220', 'https://openalex.org/W2235920218', 'https://openalex.org/W2131734538', 'https://openalex.org/W1208039178', 'https://openalex.org/W2992492498', 'https://openalex.org/W2981802563', 'https://openalex.org/W2962795401', 'https://openalex.org/W2901872500', 'https://openalex.org/W3035524453', 'https://openalex.org/W3160799772', 'https://openalex.org/W164929510', 'https://openalex.org/W2922298118', 'https://openalex.org/W2141855524', 'https://openalex.org/W2296371640', 'https://openalex.org/W2035046981', 'https://openalex.org/W815877816', 'https://openalex.org/W2024536104', 'https://openalex.org/W3108316907', 'https://openalex.org/W2072617587', 'https://openalex.org/W283036004', 'https://openalex.org/W1901129140', 'https://openalex.org/W2802076562', 'https://openalex.org/W2151539117', 'https://openalex.org/W2973049979', 'https://openalex.org/W2780124704', 'https://openalex.org/W2007337857', 'https://openalex.org/W2161676006', 'https://openalex.org/W3083173864', 'https://openalex.org/W2967443589', 'https://openalex.org/W3099073275', 'https://openalex.org/W3125775899', 'https://openalex.org/W3102619627', 'https://openalex.org/W2727071501']",2021-10-18
https://openalex.org/W4313182760,https://doi.org/10.1109/access.2022.3230688,Self-Supervised Learning of Neural Speech Representations From Unlabeled Intracranial Signals,"Neuroprosthetics have demonstrated the potential to decode speech from intracranial brain signals, and hold promise for one day returning the ability to speak to those who have lost it. However, data in this domain is scarce, highly variable, and costly to label for supervised modeling. In order to address these constraints, we present brain2vec, a transformer-based approach for learning feature representations from intracranial electroencephalogram data. Brain2vec combines a self-supervised learning methodology, neuroanatomical positional embeddings, and the contextual representations of transformers to achieve three novelties: (1) learning from unlabeled intracranial brain signals, (2) learning from multiple participants simultaneously, all while (3) utilizing only raw unprocessed data. To assess our approach, we use a leave-one-participant-out validation procedure to separate brain2vec&#x2019;s feature learning from the holdout participant&#x2019;s speech-related supervised classification tasks. With only two linear layers, we achieve 90&#x0025; accuracy on a canonical speech detection task, 42&#x0025; accuracy on a more challenging 4-class speech-related behavior recognition, and 53&#x0025; accuracy when applied to a 10-class, few-shot word classification task. Combined with the visualizations of unsupervised class separation in the learned features, our results evidence brain2vec&#x2019;s ability to learn highly generalized representations of neural activity without the need for labels or consistent sensor location.","['https://openalex.org/W2138672527', 'https://openalex.org/W2151544568', 'https://openalex.org/W2032275524', 'https://openalex.org/W1488659701', 'https://openalex.org/W2010958885', 'https://openalex.org/W2804300206', 'https://openalex.org/W2940585064', 'https://openalex.org/W2991284028', 'https://openalex.org/W3198923947', 'https://openalex.org/W2964982538', 'https://openalex.org/W3180220247', 'https://openalex.org/W2388216700', 'https://openalex.org/W2806473864', 'https://openalex.org/W6739901393', 'https://openalex.org/W6755207826', 'https://openalex.org/W6778883912', 'https://openalex.org/W3168718178', 'https://openalex.org/W6793736971', 'https://openalex.org/W6790307280', 'https://openalex.org/W3096609285', 'https://openalex.org/W4214612132', 'https://openalex.org/W4214755140', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W4226033575', 'https://openalex.org/W4231807801', 'https://openalex.org/W6796431958', 'https://openalex.org/W2544894372', 'https://openalex.org/W4241074797', 'https://openalex.org/W2169918686', 'https://openalex.org/W2139886607', 'https://openalex.org/W3160799772', 'https://openalex.org/W4311759904', 'https://openalex.org/W6780226713', 'https://openalex.org/W2124509324', 'https://openalex.org/W6729448088', 'https://openalex.org/W6769196770', 'https://openalex.org/W6631190155', 'https://openalex.org/W4206386677', 'https://openalex.org/W1984535857', 'https://openalex.org/W2889288776', 'https://openalex.org/W3083361803', 'https://openalex.org/W2019862519', 'https://openalex.org/W4205549622', 'https://openalex.org/W6636195581', 'https://openalex.org/W4224926216', 'https://openalex.org/W4286462232', 'https://openalex.org/W1592448314', 'https://openalex.org/W4206978609', 'https://openalex.org/W2950151997', 'https://openalex.org/W2896457183', 'https://openalex.org/W4292779060', 'https://openalex.org/W4385245566', 'https://openalex.org/W3126721948', 'https://openalex.org/W3036601975', 'https://openalex.org/W1522301498', 'https://openalex.org/W2979476256', 'https://openalex.org/W3171462997', 'https://openalex.org/W2187089797']",2022-01-01
https://openalex.org/W3205710300,https://doi.org/10.1109/icassp43922.2022.9746852,Injecting Text and Cross-Lingual Supervision in Few-Shot Learning from Self-Supervised Models,"Self-supervised model pretraining has recently garnered significant interest. However, using additional resources in fine-tuning these models has received less attention. We demonstrate how universal phoneset acoustic models can leverage cross-lingual supervision to improve transfer of pretrained self-supervised representations to new languages. We also show how target-language text can be used to enable and improve fine-tuning with the lattice-free maximum mutual information (LF-MMI) objective. In three low-resource languages these techniques greatly improved few-shot learning performance.","['https://openalex.org/W6801528457', 'https://openalex.org/W3119308075', 'https://openalex.org/W3016181583', 'https://openalex.org/W3102342027', 'https://openalex.org/W2963292011', 'https://openalex.org/W3095903821', 'https://openalex.org/W6785749098', 'https://openalex.org/W1494198834', 'https://openalex.org/W6762796984', 'https://openalex.org/W3198771897', 'https://openalex.org/W3049256661', 'https://openalex.org/W2938309556', 'https://openalex.org/W3198275944', 'https://openalex.org/W4210690962', 'https://openalex.org/W3160799772', 'https://openalex.org/W6792432315', 'https://openalex.org/W2127141656', 'https://openalex.org/W2963431393', 'https://openalex.org/W3096032230', 'https://openalex.org/W6787141514', 'https://openalex.org/W2046932483', 'https://openalex.org/W2025268506', 'https://openalex.org/W6844194202', 'https://openalex.org/W2033436836', 'https://openalex.org/W2752796333', 'https://openalex.org/W6640963894', 'https://openalex.org/W2936547119', 'https://openalex.org/W2972943112', 'https://openalex.org/W6780218876', 'https://openalex.org/W6769196770', 'https://openalex.org/W3015522062', 'https://openalex.org/W3198429080', 'https://openalex.org/W2995181338', 'https://openalex.org/W3097798370', 'https://openalex.org/W2514741789', 'https://openalex.org/W6631362777', 'https://openalex.org/W2911629330', 'https://openalex.org/W6636811518', 'https://openalex.org/W2786459654', 'https://openalex.org/W3160641957', 'https://openalex.org/W2786234940', 'https://openalex.org/W6766978945', 'https://openalex.org/W6601939441', 'https://openalex.org/W2964137095', 'https://openalex.org/W73572011', 'https://openalex.org/W3036601975', 'https://openalex.org/W2401231614', 'https://openalex.org/W2970971581', 'https://openalex.org/W1631260214', 'https://openalex.org/W3204224625', 'https://openalex.org/W4322714819', 'https://openalex.org/W3141100132', 'https://openalex.org/W47568227', 'https://openalex.org/W1524333225', 'https://openalex.org/W2949759968', 'https://openalex.org/W3113594615', 'https://openalex.org/W2946006146', 'https://openalex.org/W4297808394', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963400424', 'https://openalex.org/W2979476256', 'https://openalex.org/W3101648800', 'https://openalex.org/W3137720654', 'https://openalex.org/W2996383576', 'https://openalex.org/W2842511635', 'https://openalex.org/W3099782249', 'https://openalex.org/W4295312788', 'https://openalex.org/W3197074955', 'https://openalex.org/W1959608418', 'https://openalex.org/W3015265920']",2022-04-27
https://openalex.org/W4297841871,https://doi.org/10.21437/interspeech.2022-390,Content-Context Factorized Representations for Automated Speech Recognition,"Deep neural networks have largely demonstrated their ability to perform automated speech recognition (ASR) by extracting meaningful features from input audio frames.Such features, however, may consist not only of information about the spoken language content, but also may contain information about unnecessary contexts such as background noise and sounds or speaker identity, accent, or protected attributes.Such information can directly harm generalization performance, by introducing spurious correlations between the spoken words and the context in which such words were spoken.In this work, we introduce an unsupervised, encoder-agnostic method for factoring speech-encoder representations into explicit content-encoding representations and spurious context-encoding representations.By doing so, we demonstrate improved performance on standard ASR benchmarks, as well as improved performance in both real-world and artificially noisy ASR scenarios.","['https://openalex.org/W2143612262', 'https://openalex.org/W2963226019', 'https://openalex.org/W4225456955', 'https://openalex.org/W1494198834', 'https://openalex.org/W4221153068', 'https://openalex.org/W3205715971', 'https://openalex.org/W3036601975', 'https://openalex.org/W3200607244', 'https://openalex.org/W2753738274', 'https://openalex.org/W4287116649', 'https://openalex.org/W2504507112', 'https://openalex.org/W3161436426', 'https://openalex.org/W171902450', 'https://openalex.org/W3097777922', 'https://openalex.org/W3189092450', 'https://openalex.org/W2936774411', 'https://openalex.org/W3005680577', 'https://openalex.org/W2993842823', 'https://openalex.org/W3205065526', 'https://openalex.org/W3097256596', 'https://openalex.org/W2963250244', 'https://openalex.org/W2170653751', 'https://openalex.org/W3160799772', 'https://openalex.org/W3205316472', 'https://openalex.org/W1731081199', 'https://openalex.org/W2896457183']",2022-09-16
https://openalex.org/W4312120634,https://doi.org/10.23919/apsipaasc55919.2022.9979824,ESPnet-ONNX: Bridging a Gap Between Research and Production,"In the field of deep learning, researchers often focus on inventing novel neural network models and improving benchmarks. In contrast, application developers are interested in making models suitable for actual products, which involves optimizing a model for faster inference and adapting a model to various platforms (e.g., C++ and Python). In this work, to fill the gap between the two, we establish an effective procedure for optimizing a PyTorch-based research-oriented model for deployment, taking ESPnet, a widely used toolkit for speech processing, as an instance. We introduce different techniques to ESPnet, including converting a model into an ONNX format, fusing nodes in a graph, and quantizing parameters, which lead to approximately 1.3-2x speedup in various tasks (i.e., ASR, TTS, speech translation, and spoken language understanding) while keeping its performance without any additional training. Our ESPnet-ONNX will be publicly available at https://github.com/espnet/espnet_onnx.","['https://openalex.org/W6739901393', 'https://openalex.org/W3097777922', 'https://openalex.org/W2892009249', 'https://openalex.org/W2972818416', 'https://openalex.org/W3095552229', 'https://openalex.org/W3163793923', 'https://openalex.org/W6769196770', 'https://openalex.org/W6780218876', 'https://openalex.org/W3160799772', 'https://openalex.org/W3197580070', 'https://openalex.org/W6767671539', 'https://openalex.org/W3197478142', 'https://openalex.org/W6678000929', 'https://openalex.org/W6677103964', 'https://openalex.org/W6678583879', 'https://openalex.org/W6736780897', 'https://openalex.org/W3197148831', 'https://openalex.org/W2946794439', 'https://openalex.org/W6762945437', 'https://openalex.org/W2972778435', 'https://openalex.org/W2889129739', 'https://openalex.org/W2892008152', 'https://openalex.org/W6675365184', 'https://openalex.org/W6746208923', 'https://openalex.org/W3112157188', 'https://openalex.org/W6774835902', 'https://openalex.org/W3097882114', 'https://openalex.org/W3206573929', 'https://openalex.org/W6767111847', 'https://openalex.org/W6778823374', 'https://openalex.org/W6796464841', 'https://openalex.org/W3162919436', 'https://openalex.org/W4224917162', 'https://openalex.org/W2962780374', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963925437', 'https://openalex.org/W2964110616', 'https://openalex.org/W6768205276', 'https://openalex.org/W6776320331', 'https://openalex.org/W6774500937', 'https://openalex.org/W6779648490', 'https://openalex.org/W4224918069', 'https://openalex.org/W3217016993', 'https://openalex.org/W4221138270', 'https://openalex.org/W4297841482', 'https://openalex.org/W4312825166', 'https://openalex.org/W1494198834', 'https://openalex.org/W4224612669', 'https://openalex.org/W6783867762', 'https://openalex.org/W4285158119', 'https://openalex.org/W4285188582', 'https://openalex.org/W3037217258', 'https://openalex.org/W3100460087', 'https://openalex.org/W3217767527', 'https://openalex.org/W2043701535', 'https://openalex.org/W2058094241', 'https://openalex.org/W3017746288', 'https://openalex.org/W2120972216', 'https://openalex.org/W2970006822', 'https://openalex.org/W4287756219', 'https://openalex.org/W4297689207', 'https://openalex.org/W4385245566', 'https://openalex.org/W3007625080', 'https://openalex.org/W4288347855', 'https://openalex.org/W2125389748', 'https://openalex.org/W3007328579', 'https://openalex.org/W2974231335', 'https://openalex.org/W3033411150', 'https://openalex.org/W3092028330', 'https://openalex.org/W2975044525', 'https://openalex.org/W2979476256', 'https://openalex.org/W3036601975']",2022-11-07
https://openalex.org/W4224928150,https://doi.org/10.1109/icassp43922.2022.9747414,Masked Acoustic Unit for Mispronunciation Detection and Correction,"Computer-Assisted Pronunciation Training (CAPT) plays an important role in language learning. Conventional ASR-based CAPT methods require expensive annotation of the ground truth pronunciation for the supervised training. Mean-while, certain undefined non-native phonemes cannot be correctly classified into standard phonemes, making the annotation process challenging and subjective. On the other hand, ASR-based CAPT methods only give the learner text-based feedback about the mispronunciation, but cannot teach the learner how to pronounce the sentence correctly. To solve these limitations, we propose to use the acoustic unit (AU) as the intermediary feature for both mispronunciation detection and correction. The proposed method uses the masked AU sequence and the target phonemes to detect the error AU and then corrects it. This method can give the learner speech-based self-imitating feedback, making our CAPT powerful for education.","['https://openalex.org/W3095361818', 'https://openalex.org/W2067517679', 'https://openalex.org/W2973028772', 'https://openalex.org/W3097777922', 'https://openalex.org/W6739901393', 'https://openalex.org/W6629717138', 'https://openalex.org/W2888954148', 'https://openalex.org/W6729448088', 'https://openalex.org/W6778823374', 'https://openalex.org/W6769767169', 'https://openalex.org/W3081817774', 'https://openalex.org/W6777380884', 'https://openalex.org/W6681666576', 'https://openalex.org/W6761144201', 'https://openalex.org/W6780218876', 'https://openalex.org/W2963448630', 'https://openalex.org/W6775056728', 'https://openalex.org/W2139008940', 'https://openalex.org/W6795346631', 'https://openalex.org/W3015338123', 'https://openalex.org/W3036601975', 'https://openalex.org/W2938359332', 'https://openalex.org/W3013336802', 'https://openalex.org/W3033411150', 'https://openalex.org/W3097515180', 'https://openalex.org/W4385245566', 'https://openalex.org/W2547875792', 'https://openalex.org/W1494198834', 'https://openalex.org/W2144374888', 'https://openalex.org/W3160799772']",2022-04-27
https://openalex.org/W4224927737,https://doi.org/10.1109/icassp43922.2022.9746097,An Exploration of Hubert with Large Number of Cluster Units and Model Assessment Using Bayesian Information Criterion,"Self-supervised learning (SSL) has become one of the most important technologies to realize spoken dialogue systems for languages that do not have much audio data and its transcription available. Speech representation models are one of the keys to achieving this, and have been actively studied in recent years. Among them, Hidden-Unit BERT (HuBERT) has shown promising results in automatic speech recognition (ASR) tasks. However, previous studies have investigated with limited iterations and cluster units. We explore HuBERT with larger numbers of clusters and iterations in order to obtain better speech representation. Furthermore, we introduce the Bayesian Information Criterion (BIC) as the performance measure of the model. Experimental results show that our model achieves the best performance in 5 out of 8 scores in the 4 metrics for the Zero Resource Speech 2021 task. It also outperforms the HuBERT BASE model trained with 960-hour LibriSpeech (LS) even though our model is only trained with 100-hour LS. In addition, we report that BIC is useful as a clue for determining the appropriate number of clusters to improve performance on phonetic, lexical, and syntactic metrics. Finally, we show that these findings are also effective for the ASR task.","['https://openalex.org/W6784531339', 'https://openalex.org/W6794548583', 'https://openalex.org/W3207558756', 'https://openalex.org/W2896457183', 'https://openalex.org/W2168175751', 'https://openalex.org/W3197259906', 'https://openalex.org/W3198815374', 'https://openalex.org/W3197349023', 'https://openalex.org/W3197381195', 'https://openalex.org/W6682948231', 'https://openalex.org/W6739901393', 'https://openalex.org/W6641076745', 'https://openalex.org/W6795952400', 'https://openalex.org/W2123799894', 'https://openalex.org/W6790356757', 'https://openalex.org/W3016011332', 'https://openalex.org/W2405331948', 'https://openalex.org/W6844194202', 'https://openalex.org/W6780218876', 'https://openalex.org/W2982223350', 'https://openalex.org/W6786696081', 'https://openalex.org/W6795346631', 'https://openalex.org/W3093096176', 'https://openalex.org/W6766673545', 'https://openalex.org/W2995181338', 'https://openalex.org/W1494198834', 'https://openalex.org/W2741692265', 'https://openalex.org/W2395899413', 'https://openalex.org/W6768080748', 'https://openalex.org/W6770941677', 'https://openalex.org/W2975381464', 'https://openalex.org/W4385245566', 'https://openalex.org/W3197580070', 'https://openalex.org/W3036601975', 'https://openalex.org/W3160799772', 'https://openalex.org/W4287591426', 'https://openalex.org/W4394671563', 'https://openalex.org/W2996728628', 'https://openalex.org/W3163793923', 'https://openalex.org/W4297808394', 'https://openalex.org/W4287173589', 'https://openalex.org/W1963627370', 'https://openalex.org/W2965373594', 'https://openalex.org/W2152790380']",2022-04-27
https://openalex.org/W4375869286,https://doi.org/10.1109/icassp49357.2023.10096978,Cross-Training: A Semi-Supervised Training Scheme for Speech Recognition,"Semi-supervised training can be performed by jointly optimizing supervised and unsupervised losses. In many settings, supervised and unsupervised losses are inconsistent, and this inconsistency creates instability in training. As a solution, we propose cross-training: instead of training one network with two losses, we train two separate networks, each with a different loss; we then tie the parameters of the networks by minimizing an additional L2 loss between the parameters. This L2 loss acts as a knowledge bridge between the networks. It forces the networks to be similar; therefore both can learn from each other. This paper introduces the cross-training scheme to develop a stable contrastive siamese (c-siam) network. Our experiments on LibriSpeech and Google's Voice-Search/YouTube datasets show that (1) cross-training provides 20% relative WER improvement over the SOTA systems on the LibriSpeech dataset; (2) cross-training stabilizes c-siam training and significantly outperforms SOTA systems on small supervised datasets; (3) cross-training is effective for cascaded encoders, unlike the original c-siam which shows weak convergence characteristics.","['https://openalex.org/W3016970897', 'https://openalex.org/W3212799896', 'https://openalex.org/W2151742940', 'https://openalex.org/W1494198834', 'https://openalex.org/W3163203022', 'https://openalex.org/W3016010032', 'https://openalex.org/W3204696009', 'https://openalex.org/W3008181812', 'https://openalex.org/W3160799772', 'https://openalex.org/W4225307083', 'https://openalex.org/W3209984917', 'https://openalex.org/W4225755266', 'https://openalex.org/W4224919704', 'https://openalex.org/W3016234571', 'https://openalex.org/W3041561163', 'https://openalex.org/W3036601975', 'https://openalex.org/W2936774411', 'https://openalex.org/W3097777922', 'https://openalex.org/W4226033575', 'https://openalex.org/W4297808394', 'https://openalex.org/W2973049979', 'https://openalex.org/W1828163288', 'https://openalex.org/W4221161761', 'https://openalex.org/W2972943112', 'https://openalex.org/W3103934428', 'https://openalex.org/W4221145109', 'https://openalex.org/W2964110616']",2023-05-05
https://openalex.org/W3214576767,https://doi.org/10.1109/icassp43922.2022.9747242,Characterizing the Adversarial Vulnerability of Speech self-Supervised Learning,"A leaderboard named Speech processing Universal PERformance Benchmark (SUPERB), which aims at benchmarking the performance of a shared self-supervised learning (SSL) speech model across various downstream speech tasks with minimal modification of architectures and a small amount of data, has fueled the research for speech representation learning. The SUPERB demonstrates speech SSL upstream models improve the performance of various downstream tasks through just minimal adaptation. As the paradigm of the self-supervised learning upstream model followed by downstream tasks arouses more attention in the speech community, characterizing the adversarial robustness of such paradigm is of high priority. In this paper, we make the first attempt to investigate the adversarial vulnerability of such paradigm under the attacks from both zero-knowledge adversaries and limited-knowledge adversaries. The experimental results illustrate that the paradigm proposed by SUPERB is seriously vulnerable to limited-knowledge adversaries, and the attacks generated by zero-knowledge adversaries are with transferability. The XAB test verifies the imperceptibility of crafted adversarial attacks.","['https://openalex.org/W6719080892', 'https://openalex.org/W2977838803', 'https://openalex.org/W6751425476', 'https://openalex.org/W6784808798', 'https://openalex.org/W2972584841', 'https://openalex.org/W2972949456', 'https://openalex.org/W2962898354', 'https://openalex.org/W2890964092', 'https://openalex.org/W2146334809', 'https://openalex.org/W2962747881', 'https://openalex.org/W2972532204', 'https://openalex.org/W6797155818', 'https://openalex.org/W3015811740', 'https://openalex.org/W3198570670', 'https://openalex.org/W3162096084', 'https://openalex.org/W6797766004', 'https://openalex.org/W6799852253', 'https://openalex.org/W3006808893', 'https://openalex.org/W3016138785', 'https://openalex.org/W6777776875', 'https://openalex.org/W6637162671', 'https://openalex.org/W2726515241', 'https://openalex.org/W3197580070', 'https://openalex.org/W6755779437', 'https://openalex.org/W6631362777', 'https://openalex.org/W2964301649', 'https://openalex.org/W6760326341', 'https://openalex.org/W2973252307', 'https://openalex.org/W6775459693', 'https://openalex.org/W3096614974', 'https://openalex.org/W6755207826', 'https://openalex.org/W3096171739', 'https://openalex.org/W3143465836', 'https://openalex.org/W3096023981', 'https://openalex.org/W3160799772', 'https://openalex.org/W6780218876', 'https://openalex.org/W6750665317', 'https://openalex.org/W6629717138', 'https://openalex.org/W3027008958', 'https://openalex.org/W3131975723', 'https://openalex.org/W3035164673', 'https://openalex.org/W3191044189', 'https://openalex.org/W2952730822', 'https://openalex.org/W4300511536', 'https://openalex.org/W3166581665', 'https://openalex.org/W3176667198', 'https://openalex.org/W3009053050', 'https://openalex.org/W2898435086', 'https://openalex.org/W2797583228', 'https://openalex.org/W2963341956', 'https://openalex.org/W2980486495', 'https://openalex.org/W3033893956', 'https://openalex.org/W3036601975', 'https://openalex.org/W3099782249', 'https://openalex.org/W3173081491', 'https://openalex.org/W1524333225', 'https://openalex.org/W2964153729', 'https://openalex.org/W1494198834', 'https://openalex.org/W2460937040', 'https://openalex.org/W4297683418', 'https://openalex.org/W1673923490', 'https://openalex.org/W4287063346', 'https://openalex.org/W2803609229', 'https://openalex.org/W3095698432', 'https://openalex.org/W3157923770', 'https://openalex.org/W2896457183', 'https://openalex.org/W3166633154', 'https://openalex.org/W3099944122', 'https://openalex.org/W3161223924', 'https://openalex.org/W2923292931']",2022-04-27
https://openalex.org/W4297841509,https://doi.org/10.21437/interspeech.2022-388,Transplantation of Conversational Speaking Style with Interjections in Sequence-to-Sequence Speech Synthesis,"Sequence-to-Sequence Text-to-Speech architectures that directly generate low level acoustic features from phonetic sequences are known to produce natural and expressive speech when provided with adequate amounts of training data.Such systems can learn and transfer desired speaking styles from one seen speaker to another (in multi-style multi-speaker settings), which is highly desirable for creating scalable and customizable Human-Computer Interaction systems.In this work we explore one-to-many style transfer from a dedicated single-speaker conversational corpus with style nuances and interjections.We elaborate on the corpus design and explore the feasibility of such style transfer when assisted with Voice-Conversion-based data augmentation.In a set of subjective listening experiments, this approach resulted in high-fidelity style transfer with no quality degradation.However, a certain voice persona shift was observed, requiring further improvements in voice conversion.","['https://openalex.org/W3095012670', 'https://openalex.org/W2963091184', 'https://openalex.org/W3140429000', 'https://openalex.org/W2985067290', 'https://openalex.org/W3161492781', 'https://openalex.org/W3198712562', 'https://openalex.org/W2604184139', 'https://openalex.org/W2547875792', 'https://openalex.org/W3163143169', 'https://openalex.org/W2964243274', 'https://openalex.org/W2515028311', 'https://openalex.org/W2888797456', 'https://openalex.org/W3022876224', 'https://openalex.org/W3160799772', 'https://openalex.org/W3101882441', 'https://openalex.org/W3144667183', 'https://openalex.org/W2691405956', 'https://openalex.org/W1731081199', 'https://openalex.org/W2039143993', 'https://openalex.org/W3193700177', 'https://openalex.org/W4298326265', 'https://openalex.org/W3151309757', 'https://openalex.org/W4295731579', 'https://openalex.org/W3091928890', 'https://openalex.org/W2548228487', 'https://openalex.org/W2972606665']",2022-09-16
https://openalex.org/W4385993881,https://doi.org/10.21437/ssw.2023-29,Learning Multilingual Expressive Speech Representation for Prosody Prediction without Parallel Data,"We propose a method for speech-to-speech emotionpreserving translation that operates at the level of discrete speech units.Our approach relies on the use of multilingual emotion embedding that can capture affective information in a language-independent manner.We show that this embedding can be used to predict the pitch and duration of speech units in a target language, allowing us to resynthesize the source speech signal with the same emotional content.We evaluate our approach to English and French speech signals and show that it outperforms a baseline method that does not use emotional information, including when the emotion embedding is extracted from a different language.Even if this preliminary study does not address directly the machine translation issue, our results demonstrate the effectiveness of our approach for cross-lingual emotion preservation in the context of speech resynthesis.","['https://openalex.org/W3198429080', 'https://openalex.org/W3122349645', 'https://openalex.org/W2972602947', 'https://openalex.org/W4224918181', 'https://openalex.org/W4394671563', 'https://openalex.org/W3208019461', 'https://openalex.org/W2805064398', 'https://openalex.org/W3197642003', 'https://openalex.org/W3211224152', 'https://openalex.org/W4205742757', 'https://openalex.org/W3160799772', 'https://openalex.org/W4307741680', 'https://openalex.org/W2144005487', 'https://openalex.org/W589698015', 'https://openalex.org/W2963087748', 'https://openalex.org/W4393713332', 'https://openalex.org/W2115098197', 'https://openalex.org/W2313339984', 'https://openalex.org/W2030931454', 'https://openalex.org/W3092028330', 'https://openalex.org/W3033411150', 'https://openalex.org/W2138615112', 'https://openalex.org/W3180374548', 'https://openalex.org/W3175871055', 'https://openalex.org/W2890964092', 'https://openalex.org/W2936184970', 'https://openalex.org/W2146334809', 'https://openalex.org/W2951442257', 'https://openalex.org/W3140429000', 'https://openalex.org/W1501669607', 'https://openalex.org/W175750906', 'https://openalex.org/W3036601975']",2023-08-18
https://openalex.org/W4385570462,https://doi.org/10.18653/v1/2023.findings-acl.503,Fine-grained Artificial Neurons in Audio-transformers for Disentangling Neural Auditory Encoding,"Mengyue Zhou, Xu Liu, David Liu, Zihao Wu, Zhengliang Liu, Lin Zhao, Dajiang Zhu, Lei Guo, Junwei Han, Tianming Liu, Xintao Hu. Findings of the Association for Computational Linguistics: ACL 2023. 2023.","['https://openalex.org/W2210407171', 'https://openalex.org/W1985466488', 'https://openalex.org/W3036601975', 'https://openalex.org/W4382237478', 'https://openalex.org/W3134212080', 'https://openalex.org/W2945038412', 'https://openalex.org/W2015578534', 'https://openalex.org/W3211949750', 'https://openalex.org/W4385245566', 'https://openalex.org/W1979129202', 'https://openalex.org/W2112891119', 'https://openalex.org/W2193413348', 'https://openalex.org/W2981381476', 'https://openalex.org/W4205106608', 'https://openalex.org/W3160799772', 'https://openalex.org/W2164898340', 'https://openalex.org/W2952834402', 'https://openalex.org/W4226064907', 'https://openalex.org/W2887326070', 'https://openalex.org/W3125209268', 'https://openalex.org/W3205578964', 'https://openalex.org/W4288375898', 'https://openalex.org/W2013366824', 'https://openalex.org/W3102027770', 'https://openalex.org/W2149044933', 'https://openalex.org/W2972324944', 'https://openalex.org/W2973049979', 'https://openalex.org/W2736149814', 'https://openalex.org/W4304465621', 'https://openalex.org/W2047752187', 'https://openalex.org/W4283332789', 'https://openalex.org/W2266497176', 'https://openalex.org/W1967542346', 'https://openalex.org/W3096017728', 'https://openalex.org/W1994341528', 'https://openalex.org/W4281765823', 'https://openalex.org/W2079207700', 'https://openalex.org/W3021934057', 'https://openalex.org/W2063951486', 'https://openalex.org/W4221102486', 'https://openalex.org/W1992219297', 'https://openalex.org/W2344975321', 'https://openalex.org/W3036927415', 'https://openalex.org/W2076923487', 'https://openalex.org/W3182301746', 'https://openalex.org/W4221115875', 'https://openalex.org/W2005066437', 'https://openalex.org/W2800311957', 'https://openalex.org/W4308343432', 'https://openalex.org/W3203937348', 'https://openalex.org/W1966721490', 'https://openalex.org/W2016015135', 'https://openalex.org/W1965248225', 'https://openalex.org/W2274405424']",2023-01-01
https://openalex.org/W4296069135,https://doi.org/10.21437/interspeech.2022-10558,Exploring representation learning for small-footprint keyword spotting,"In this paper, we investigate representation learning for low-resource\nkeyword spotting (KWS). The main challenges of KWS are limited labeled data and\nlimited available device resources. To address those challenges, we explore\nrepresentation learning for KWS by self-supervised contrastive learning and\nself-training with pretrained model. First, local-global contrastive siamese\nnetworks (LGCSiam) are designed to learn similar utterance-level\nrepresentations for similar audio samplers by proposed local-global contrastive\nloss without requiring ground-truth. Second, a self-supervised pretrained\nWav2Vec 2.0 model is applied as a constraint module (WVC) to force the KWS\nmodel to learn frame-level acoustic representations. By the LGCSiam and WVC\nmodules, the proposed small-footprint KWS model can be pretrained with\nunlabeled data. Experiments on speech commands dataset show that the\nself-training WVC module and the self-supervised LGCSiam module significantly\nimprove accuracy, especially in the case of training on a small labeled\ndataset.\n","['https://openalex.org/W4385245566', 'https://openalex.org/W2748659049', 'https://openalex.org/W3036601975', 'https://openalex.org/W3144247233', 'https://openalex.org/W2510945575', 'https://openalex.org/W3160799772', 'https://openalex.org/W2797583228', 'https://openalex.org/W2034940213', 'https://openalex.org/W4297808394', 'https://openalex.org/W4287592659', 'https://openalex.org/W2896457183', 'https://openalex.org/W2888930363', 'https://openalex.org/W2769912137', 'https://openalex.org/W2963242190', 'https://openalex.org/W2889511491', 'https://openalex.org/W2973226577', 'https://openalex.org/W2936774411', 'https://openalex.org/W3005680577', 'https://openalex.org/W2963628261', 'https://openalex.org/W2507319753', 'https://openalex.org/W2973049979', 'https://openalex.org/W4295312788', 'https://openalex.org/W2507580616', 'https://openalex.org/W2972792496', 'https://openalex.org/W3095949666']",2022-09-16
https://openalex.org/W4375869237,https://doi.org/10.1109/icassp49357.2023.10095280,Fully Unsupervised Topic Clustering of Unlabelled Spoken Audio Using Self-Supervised Representation Learning and Topic Model,"Unsupervised topic clustering of spoken audio is an important research topic for zero-resourced unwritten languages. A classical approach is to find a set of spoken terms from only the audio based on dynamic time warping or generative modeling (e.g., hidden Markov model), and apply a topic model to classify topics. The spoken term discovery is the most important and difficult part. In this paper, we propose to combine self-supervised representation learning (SSRL) methods as a component of spoken term discovery and probabilistic topic models. Most SSRL methods pre-train a model which predicts high-quality pseudo labels generated from an audio-only corpus. These pseudo labels can be used to produce a sequence of pseudo subwords by applying deduplication and a subword model. Then, we apply a topic model based on latent Dirichlet allocation for these pseudo-subword sequences in an unsupervised manner. The clustering performance is evaluated on the Fisher corpus using normalized mutual information. We confirm the improvement of the proposed method and its effectiveness compared to an existing approach using dynamic time warping and topic models although the experimental setups are not directly comparable.","['https://openalex.org/W2399869768', 'https://openalex.org/W3198694222', 'https://openalex.org/W2009388533', 'https://openalex.org/W2995181338', 'https://openalex.org/W6795346631', 'https://openalex.org/W6603931906', 'https://openalex.org/W2114347655', 'https://openalex.org/W3119308075', 'https://openalex.org/W6850036870', 'https://openalex.org/W6601894380', 'https://openalex.org/W2750248772', 'https://openalex.org/W2933138175', 'https://openalex.org/W2963230678', 'https://openalex.org/W3209984917', 'https://openalex.org/W6636440780', 'https://openalex.org/W2888039742', 'https://openalex.org/W6780218876', 'https://openalex.org/W2147152072', 'https://openalex.org/W6844194202', 'https://openalex.org/W4226390724', 'https://openalex.org/W3197580070', 'https://openalex.org/W4281492411', 'https://openalex.org/W6786696081', 'https://openalex.org/W3150635893', 'https://openalex.org/W6790356757', 'https://openalex.org/W3197259906', 'https://openalex.org/W3207558756', 'https://openalex.org/W2896457183', 'https://openalex.org/W6795952400', 'https://openalex.org/W3198815374', 'https://openalex.org/W3197349023', 'https://openalex.org/W3197381195', 'https://openalex.org/W2347098582', 'https://openalex.org/W2078769636', 'https://openalex.org/W2719865699', 'https://openalex.org/W2118714763', 'https://openalex.org/W6639619044', 'https://openalex.org/W51277926', 'https://openalex.org/W4296069143', 'https://openalex.org/W3160799772', 'https://openalex.org/W4372270126', 'https://openalex.org/W46679369', 'https://openalex.org/W1612003148', 'https://openalex.org/W1880262756', 'https://openalex.org/W4394671563', 'https://openalex.org/W4287591426', 'https://openalex.org/W97072897', 'https://openalex.org/W3036601975', 'https://openalex.org/W4297808394', 'https://openalex.org/W4287173589']",2023-05-05
https://openalex.org/W4385484929,https://doi.org/10.1109/icasspw59220.2023.10193570,Efficient Utilization of Large Pre-Trained Models for Low Resource ASR,"Unsupervised representation learning has recently helped automatic speech recognition (ASR) to tackle tasks with limited labeled data. Following this, hardware limitations and applications give rise to the question how to take advantage of large pre-trained models efficiently and reduce their complexity. In this work, we study a challenging low resource conversational telephony speech corpus from the medical domain in Vietnamese and German. We show the benefits of using unsupervised techniques beyond simple fine-tuning of large pre-trained models, discuss how to adapt them to a practical telephony task including bandwidth transfer and investigate different data conditions for pre-training and fine-tuning. We outperform the project baselines by 22% relative using pre-training techniques. Further gains of 29% can be achieved by refinements of architecture and training and 6% by adding 0.8 h of in-domain adaptation data.","['https://openalex.org/W3212799896', 'https://openalex.org/W6792432315', 'https://openalex.org/W4206375145', 'https://openalex.org/W2187428966', 'https://openalex.org/W2933138175', 'https://openalex.org/W6788335241', 'https://openalex.org/W6779919476', 'https://openalex.org/W6757717501', 'https://openalex.org/W4281492411', 'https://openalex.org/W2291975472', 'https://openalex.org/W4307539315', 'https://openalex.org/W3203140070', 'https://openalex.org/W2962728618', 'https://openalex.org/W4225501245', 'https://openalex.org/W4226380987', 'https://openalex.org/W4297841844', 'https://openalex.org/W3198771897', 'https://openalex.org/W2973049979', 'https://openalex.org/W6844194202', 'https://openalex.org/W3160799772', 'https://openalex.org/W6780218876', 'https://openalex.org/W3198429080', 'https://openalex.org/W2905489173', 'https://openalex.org/W3167207712', 'https://openalex.org/W3137720654', 'https://openalex.org/W4297808394', 'https://openalex.org/W3036601975']",2023-06-04
https://openalex.org/W4386158868,https://doi.org/10.1109/icme55011.2023.00296,Speech Topic Classification Based on Pre-trained and Graph Networks,"Speech Topic Classification (STC) automatically classifies audio clips into predefined categories, which is widely used in short video, personalized recommendation and other fields. At present, the common system is composed of two parts: first, the speech is converted into text by automatic speech recognition (ASR), and then the text topic is classified by natural language processing (NLP). Most of them have problems such as error propagation and lack of global structure. So in this paper, we propose a new end-to-end framework based on a pre-trained model and graph network. The pre-trained model is used to extract the semantic features with sequential structure instead of acoustic features, and the combination with the global features of conversational context constructed by graph network has achieved good results on the Fisher dataset.","['https://openalex.org/W4224919082', 'https://openalex.org/W3008083857', 'https://openalex.org/W3160799772', 'https://openalex.org/W2962946486', 'https://openalex.org/W4226074103', 'https://openalex.org/W2963288440', 'https://openalex.org/W2130180273', 'https://openalex.org/W2142749559', 'https://openalex.org/W3209984917', 'https://openalex.org/W6780218876', 'https://openalex.org/W6739901393', 'https://openalex.org/W2962810718', 'https://openalex.org/W2124509324', 'https://openalex.org/W6679436768', 'https://openalex.org/W2347098582', 'https://openalex.org/W2105778889', 'https://openalex.org/W2062914951', 'https://openalex.org/W2889173507', 'https://openalex.org/W51277926', 'https://openalex.org/W4297808394', 'https://openalex.org/W3128442956', 'https://openalex.org/W2130942839', 'https://openalex.org/W3175898847', 'https://openalex.org/W4385245566', 'https://openalex.org/W2896457183', 'https://openalex.org/W3036601975', 'https://openalex.org/W2963230678']",2023-07-01
https://openalex.org/W4372260564,https://doi.org/10.1109/icassp49357.2023.10094725,Trinet: Stabilizing Self-Supervised Learning From Complete or Slow Collapse,"Self-supervised learning (SSL) models confront challenges of abrupt informational collapse or slow dimensional collapse. We propose TriNet, which introduces a novel triple-branch architecture for preventing collapse and stabilizing the pretraining. TriNet learns the SSL latent embedding space and incorporates it to a higher level space for predicting pseudo target vectors generated by a frozen teacher. Our experimental results show that the proposed method notably stabilizes and accelerates pre-training and achieves a relative word error rate reduction (WERR) of 6.06% compared to the state-of- the-art (SOTA) Data2vec for a downstream benchmark ASR task. We will release our code at https://github.com/tencent-ailab/.","['https://openalex.org/W6779977557', 'https://openalex.org/W6810007534', 'https://openalex.org/W6780218876', 'https://openalex.org/W3160799772', 'https://openalex.org/W6774314701', 'https://openalex.org/W3198608154', 'https://openalex.org/W6779326418', 'https://openalex.org/W3171007011', 'https://openalex.org/W6802387851', 'https://openalex.org/W6795754764', 'https://openalex.org/W3197411683', 'https://openalex.org/W6810363402', 'https://openalex.org/W6843487242', 'https://openalex.org/W6780730929', 'https://openalex.org/W6791742336', 'https://openalex.org/W4296068987', 'https://openalex.org/W6810673746', 'https://openalex.org/W2940322076', 'https://openalex.org/W3015522062', 'https://openalex.org/W3096338464', 'https://openalex.org/W3026041220', 'https://openalex.org/W6784614252', 'https://openalex.org/W3160525311', 'https://openalex.org/W4283324001', 'https://openalex.org/W6790850890', 'https://openalex.org/W6810786073', 'https://openalex.org/W2933138175', 'https://openalex.org/W3159481202', 'https://openalex.org/W1494198834', 'https://openalex.org/W2995181338', 'https://openalex.org/W2127141656']",2023-05-05
https://openalex.org/W4213076711,https://doi.org/10.3390/app12041889,Evaluating Novel Speech Transcription Architectures on the Spanish RTVE2020 Database,"This work presents three novel speech recognition architectures evaluated on the Spanish RTVE2020 dataset, employed as the main evaluation set in the Albayzín S2T Transcription Challenge 2020. The main objective was to improve the performance of the systems previously submitted by the authors to the challenge, in which the primary system scored the second position. The novel systems are based on both DNN-HMM and E2E acoustic models, for which fully- and self-supervised learning methods were included. As a result, the new speech recognition engines clearly outperformed the performance of the initial systems from the previous best WER of 19.27 to the new best of 17.60 achieved by the DNN-HMM based system. This work therefore describes an interesting benchmark of the latest acoustic models over a highly challenging dataset, and identifies the most optimal ones depending on the expected quality, the available resources and the required latency.","['https://openalex.org/W3185921853', 'https://openalex.org/W3160799772', 'https://openalex.org/W6631362777', 'https://openalex.org/W3015537910', 'https://openalex.org/W3135567093', 'https://openalex.org/W2160815625', 'https://openalex.org/W2193413348', 'https://openalex.org/W2102113734', 'https://openalex.org/W2327501763', 'https://openalex.org/W6623517193', 'https://openalex.org/W2608712415', 'https://openalex.org/W3197478142', 'https://openalex.org/W3207558756', 'https://openalex.org/W1494198834', 'https://openalex.org/W3163211337', 'https://openalex.org/W3006827623', 'https://openalex.org/W3095838132', 'https://openalex.org/W2973215447', 'https://openalex.org/W3209059054', 'https://openalex.org/W2981857663', 'https://openalex.org/W3210615406', 'https://openalex.org/W2250389772', 'https://openalex.org/W6607111733', 'https://openalex.org/W2936774411', 'https://openalex.org/W2407080277', 'https://openalex.org/W2402146185', 'https://openalex.org/W2802201485', 'https://openalex.org/W2964054038', 'https://openalex.org/W3198429080', 'https://openalex.org/W3119308075', 'https://openalex.org/W3095410713', 'https://openalex.org/W3139878283', 'https://openalex.org/W3213029956', 'https://openalex.org/W2888867175', 'https://openalex.org/W3197580070', 'https://openalex.org/W854541894']",2022-02-11
https://openalex.org/W4375869012,https://doi.org/10.1109/icassp49357.2023.10096078,More Speaking or More Speakers?,"Self-training (ST) and self-supervised learning (SSL) methods have demonstrated strong improvements in automatic speech recognition (ASR). In spite of these advances, to the best of our knowledge, there is no analysis of how the composition of the labelled and unlabelled datasets used in these methods affects the results. In this work we aim to analyse the effect of number of speakers in the training data on a recent SSL algorithm (wav2vec 2.0), and a recent ST algorithm (slimIPL). We perform a systematic analysis on both labeled and unlabeled data by varying the number of speakers while keeping the number of hours fixed and vice versa. Our findings suggest that SSL requires a large amount of unlabeled data to produce high accuracy results, while ST requires a sufficient number of speakers in the labelled data, especially in the low-regime setting. In this manner these two approaches improve supervised learning in different regimes of data composition.","['https://openalex.org/W6802818367', 'https://openalex.org/W6768080748', 'https://openalex.org/W3204696009', 'https://openalex.org/W2963925437', 'https://openalex.org/W2108776455', 'https://openalex.org/W6839738141', 'https://openalex.org/W1494198834', 'https://openalex.org/W3160525311', 'https://openalex.org/W6797264745', 'https://openalex.org/W4289824098', 'https://openalex.org/W6681435938', 'https://openalex.org/W3160799772', 'https://openalex.org/W6780218876', 'https://openalex.org/W2936774411', 'https://openalex.org/W2127141656', 'https://openalex.org/W6810185941', 'https://openalex.org/W6749396741', 'https://openalex.org/W4225755266', 'https://openalex.org/W3197223534', 'https://openalex.org/W3198098585', 'https://openalex.org/W6810007534', 'https://openalex.org/W4226033575', 'https://openalex.org/W3096338464', 'https://openalex.org/W3015522062', 'https://openalex.org/W4221154554', 'https://openalex.org/W2963956526', 'https://openalex.org/W2975381464', 'https://openalex.org/W4221145109', 'https://openalex.org/W4283659485', 'https://openalex.org/W4287124724', 'https://openalex.org/W3036601975', 'https://openalex.org/W2146502635', 'https://openalex.org/W4286902103']",2023-05-05
https://openalex.org/W4385570401,https://doi.org/10.18653/v1/2023.iwslt-1.16,I2R’s End-to-End Speech Translation System for IWSLT 2023 Offline Shared Task,"This paper describes I2R’s submission to the offline speech translation track for IWSLT 2023. We focus on an end-to-end approach for translation from English audio to German text, one of the three available language directions in this year’s edition. The I2R system leverages on pretrained models that have been exposed to large-scale audio and text data for our base model. We introduce several stages of additional pretraining followed by fine-tuning to adapt the system for the downstream speech translation task. The strategy is supplemented by other techniques such as data augmentation, domain tagging, knowledge distillation, and model ensemble, among others. We evaluate the system on several publicly available test sets for comparison.","['https://openalex.org/W2949328740', 'https://openalex.org/W4385570170', 'https://openalex.org/W4319862474', 'https://openalex.org/W630532510', 'https://openalex.org/W4287887977', 'https://openalex.org/W2605131327', 'https://openalex.org/W3102816807', 'https://openalex.org/W4221163209', 'https://openalex.org/W4288089799', 'https://openalex.org/W4285107548', 'https://openalex.org/W3209984917', 'https://openalex.org/W2250357346', 'https://openalex.org/W3100806282', 'https://openalex.org/W3092085609', 'https://openalex.org/W3039695075', 'https://openalex.org/W1494198834', 'https://openalex.org/W3036601975', 'https://openalex.org/W3015703505', 'https://openalex.org/W3030437843', 'https://openalex.org/W3196509775', 'https://openalex.org/W3097301532', 'https://openalex.org/W4285158119', 'https://openalex.org/W3119308075', 'https://openalex.org/W3198429080', 'https://openalex.org/W3174446152', 'https://openalex.org/W4300980246', 'https://openalex.org/W3160799772', 'https://openalex.org/W3175746962', 'https://openalex.org/W3176711365', 'https://openalex.org/W4285201770', 'https://openalex.org/W22168010', 'https://openalex.org/W4288817190', 'https://openalex.org/W3034999214', 'https://openalex.org/W4226120743', 'https://openalex.org/W3015698636', 'https://openalex.org/W2419539795', 'https://openalex.org/W2760452458', 'https://openalex.org/W4285167065']",2023-01-01
https://openalex.org/W4390188605,https://doi.org/10.1109/dasc/picom/cbdcom/cy59711.2023.10361493,Singing Voice Conversion Between Popular Music and Chinese Opera Based on VITS,"Singing Voice Conversion is an audio processing technique designed to convert one singer's voice into another singer's voice, while preserving the singing characteristics and emotional expression in the original audio. Although this technol-ogy has been widely used in the field of music, attempts in the field of Chinese opera are very scarce. This paper attempts to transfer the timbre of popular singers to Chinese opera through the neural network-based vocoder model VITS to replace the original opera timbre, and finally achieve the effect of popular singers singing Chinese opera. The experimental results show that the generated audio can better maintain the timbre of popular singers, and can clearly feel the auditory characteristics of Chinese opera.","['https://openalex.org/W4213344512', 'https://openalex.org/W2294038178', 'https://openalex.org/W2972812066', 'https://openalex.org/W3095948607', 'https://openalex.org/W3012498027', 'https://openalex.org/W4221151538', 'https://openalex.org/W3016243847', 'https://openalex.org/W6796464841', 'https://openalex.org/W3207340675', 'https://openalex.org/W6805710207', 'https://openalex.org/W3160799772', 'https://openalex.org/W2963300588', 'https://openalex.org/W6739901393', 'https://openalex.org/W6783867762', 'https://openalex.org/W3092028330', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394034078']",2023-11-14
https://openalex.org/W4392903509,https://doi.org/10.1109/icassp48485.2024.10448052,Hubertopic: Enhancing Semantic Representation of Hubert Through Self-Supervision Utilizing Topic Model,"Recently, the usefulness of self-supervised representation learning (SSRL) methods has been confirmed in various downstream tasks. Many of these models, as exemplified by HuBERT and WavLM, use pseudo-labels generated from spectral features or the model's own representation features. From previous studies, it is known that the pseudo-labels contain semantic information. However, the masked prediction task, the learning criterion of HuBERT, focuses on local contextual information and may not make effective use of global semantic information such as speaker, theme of speech, and so on. In this paper, we propose a new approach to enrich the semantic representation of HuBERT. We apply topic model to pseudo-labels to generate a topic label for each utterance. An auxiliary topic classification task is added to HuBERT by using topic labels as teachers. This allows additional global semantic information to be incorporated in an unsupervised manner. Experimental results demonstrate that our method achieves comparable or better performance than the baseline in most tasks, including automatic speech recognition and five out of the eight SUPERB tasks. Moreover, we find that topic labels include various information about utterance, such as gender, speaker, and its theme. This highlights the effectiveness of our approach in capturing multifaceted semantic nuances.","['https://openalex.org/W3197580070', 'https://openalex.org/W4285250921', 'https://openalex.org/W3207558756', 'https://openalex.org/W4319862410', 'https://openalex.org/W3189296823', 'https://openalex.org/W4385822439', 'https://openalex.org/W4296068815', 'https://openalex.org/W4382202628', 'https://openalex.org/W4281492411', 'https://openalex.org/W3160799772', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209984917', 'https://openalex.org/W4226033575', 'https://openalex.org/W6810673746', 'https://openalex.org/W4372260526', 'https://openalex.org/W4375869237', 'https://openalex.org/W3140429000', 'https://openalex.org/W4385823059', 'https://openalex.org/W6839738141', 'https://openalex.org/W4385822740', 'https://openalex.org/W4372347505', 'https://openalex.org/W6639619044', 'https://openalex.org/W2896457183', 'https://openalex.org/W4220861476', 'https://openalex.org/W6810311456', 'https://openalex.org/W1494198834', 'https://openalex.org/W108866686', 'https://openalex.org/W2962780374', 'https://openalex.org/W4221142221', 'https://openalex.org/W3036601975', 'https://openalex.org/W4231510805']",2024-03-18
https://openalex.org/W3213873715,,Textless Speech Emotion Conversion using Decomposed and Discrete Representations,"Speech emotion conversion is the task of modifying the perceived emotion of a speech utterance while preserving the lexical content and speaker identity. In this study, we cast the problem of emotion conversion as a spoken language translation task. We decompose speech into discrete and disentangled learned representations, consisting of content units, F0, speaker, and emotion. First, we modify the speech content by translating the content units to a target emotion, and then predict the prosodic features based on these units. Finally, the speech waveform is generated by feeding the predicted representations into a neural vocoder. Such a paradigm allows us to go beyond spectral and parametric changes of the signal, and model non-verbal vocalizations, such as laughter insertion, yawning removal, etc. We demonstrate objectively and subjectively that the proposed method is superior to the baselines in terms of perceived emotion and audio quality. We rigorously evaluate all components of such a complex system and conclude with an extensive model analysis and ablation study to better emphasize the architectural choices, strengths and weaknesses of the proposed method. Samples and code will be publicly available under the following link: https://speechbot.github.io/emotion.","['https://openalex.org/W2963341956', 'https://openalex.org/W3033411150', 'https://openalex.org/W3157923770', 'https://openalex.org/W2963799213', 'https://openalex.org/W3174758275', 'https://openalex.org/W3185206490', 'https://openalex.org/W2888911345', 'https://openalex.org/W2962866891', 'https://openalex.org/W3095948607', 'https://openalex.org/W2745376520', 'https://openalex.org/W2040587156', 'https://openalex.org/W1966797434', 'https://openalex.org/W3048217718', 'https://openalex.org/W3198217962', 'https://openalex.org/W2018658329', 'https://openalex.org/W3168292814', 'https://openalex.org/W3045354608', 'https://openalex.org/W3160799772', 'https://openalex.org/W3144639365', 'https://openalex.org/W3004402693', 'https://openalex.org/W3163573274', 'https://openalex.org/W2979790850', 'https://openalex.org/W2511640485', 'https://openalex.org/W2964243274', 'https://openalex.org/W2517513811', 'https://openalex.org/W3025035610', 'https://openalex.org/W3097787369', 'https://openalex.org/W2793479148', 'https://openalex.org/W2160473997', 'https://openalex.org/W3015669407', 'https://openalex.org/W2077801020', 'https://openalex.org/W3161223924', 'https://openalex.org/W3015719316', 'https://openalex.org/W2347098582', 'https://openalex.org/W2963403868', 'https://openalex.org/W3015213852', 'https://openalex.org/W2973138167', 'https://openalex.org/W2810914326', 'https://openalex.org/W2972366998', 'https://openalex.org/W2933138175', 'https://openalex.org/W2148846882', 'https://openalex.org/W2114925438', 'https://openalex.org/W3096457008', 'https://openalex.org/W3098403858', 'https://openalex.org/W2115098197', 'https://openalex.org/W3020570669', 'https://openalex.org/W2899361462', 'https://openalex.org/W3140429000', 'https://openalex.org/W3096323553', 'https://openalex.org/W2963618559', 'https://openalex.org/W2973049979', 'https://openalex.org/W3095930733', 'https://openalex.org/W2938833595', 'https://openalex.org/W3167167480', 'https://openalex.org/W3015241559', 'https://openalex.org/W1494198834', 'https://openalex.org/W3098557217', 'https://openalex.org/W2527729766', 'https://openalex.org/W3034794073', 'https://openalex.org/W3210177631', 'https://openalex.org/W2083905766', 'https://openalex.org/W3087287714', 'https://openalex.org/W3099782249', 'https://openalex.org/W3003875258', 'https://openalex.org/W2750248772', 'https://openalex.org/W2982399380', 'https://openalex.org/W3039910566', 'https://openalex.org/W3035202887', 'https://openalex.org/W2101105183', 'https://openalex.org/W3142644187', 'https://openalex.org/W3025044797', 'https://openalex.org/W95152782', 'https://openalex.org/W2963300588', 'https://openalex.org/W2471520273', 'https://openalex.org/W3144988954']",2021-11-14
https://openalex.org/W3205032693,https://doi.org/10.48550/arxiv.2110.09930,Speech Representation Learning Through Self-supervised Pretraining And Multi-task Finetuning,"Speech representation learning plays a vital role in speech processing. Among them, self-supervised learning (SSL) has become an important research direction. It has been shown that an SSL pretraining model can achieve excellent performance in various downstream tasks of speech processing. On the other hand, supervised multi-task learning (MTL) is another representation learning paradigm, which has been proven effective in computer vision (CV) and natural language processing (NLP). However, there is no systematic research on the general representation learning model trained by supervised MTL in speech processing. In this paper, we show that MTL finetuning can further improve SSL pretraining. We analyze the generalizability of supervised MTL finetuning to examine if the speech representation learned by MTL finetuning can generalize to unseen new tasks.","['https://openalex.org/W3160799772', 'https://openalex.org/W125693051', 'https://openalex.org/W2890538051', 'https://openalex.org/W2973157397', 'https://openalex.org/W3197580070', 'https://openalex.org/W2842511635', 'https://openalex.org/W2963677766', 'https://openalex.org/W2963310665', 'https://openalex.org/W3112034174', 'https://openalex.org/W2962743139', 'https://openalex.org/W3027008958', 'https://openalex.org/W2972584841', 'https://openalex.org/W3099782249', 'https://openalex.org/W2809324505', 'https://openalex.org/W3041561163', 'https://openalex.org/W2973049979', 'https://openalex.org/W2996383576', 'https://openalex.org/W2972943112', 'https://openalex.org/W2146334809', 'https://openalex.org/W1494198834', 'https://openalex.org/W3161223924', 'https://openalex.org/W2031489346', 'https://openalex.org/W3128413221', 'https://openalex.org/W3161940574', 'https://openalex.org/W3106539628', 'https://openalex.org/W2981087920', 'https://openalex.org/W2982223350', 'https://openalex.org/W3015213852', 'https://openalex.org/W2963341956']",2021-10-18
https://openalex.org/W3169948435,https://doi.org/10.48550/arxiv.2106.02869,Integrating Auxiliary Information in Self-supervised Learning,"This paper presents to integrate the auxiliary information (e.g., additional attributes for data such as the hashtags for Instagram images) in the self-supervised learning process. We first observe that the auxiliary information may bring us useful information about data structures: for instance, the Instagram images with the same hashtags can be semantically similar. Hence, to leverage the structural information from the auxiliary information, we present to construct data clusters according to the auxiliary information. Then, we introduce the Clustering InfoNCE (Cl-InfoNCE) objective that learns similar representations for augmented variants of data from the same cluster and dissimilar representations for data from different clusters. Our approach contributes as follows: 1) Comparing to conventional self-supervised representations, the auxiliary-information-infused self-supervised representations bring the performance closer to the supervised representations; 2) The presented Cl-InfoNCE can also work with unsupervised constructed clusters (e.g., k-means clusters) and outperform strong clustering-based self-supervised learning approaches, such as the Prototypical Contrastive Learning (PCL) method; 3) We show that Cl-InfoNCE may be a better approach to leverage the data clustering information, by comparing it to the baseline approach - learning to predict the clustering assignments with cross-entropy loss. For analysis, we connect the goodness of the learned representations with the statistical relationships: i) the mutual information between the labels and the clusters and ii) the conditional entropy of the clusters given the labels.","['https://openalex.org/W1797268635', 'https://openalex.org/W3126816608', 'https://openalex.org/W3099782249', 'https://openalex.org/W2925140953', 'https://openalex.org/W2951237705', 'https://openalex.org/W3022061250', 'https://openalex.org/W2518918709', 'https://openalex.org/W3138513046', 'https://openalex.org/W3035524453', 'https://openalex.org/W2194775991', 'https://openalex.org/W2093848332', 'https://openalex.org/W2962739339', 'https://openalex.org/W3160799772', 'https://openalex.org/W3034978746', 'https://openalex.org/W2117539524', 'https://openalex.org/W2963341956', 'https://openalex.org/W2842511635', 'https://openalex.org/W3100345210', 'https://openalex.org/W2883725317', 'https://openalex.org/W3135367836', 'https://openalex.org/W2081580037', 'https://openalex.org/W2158131535', 'https://openalex.org/W2973049979', 'https://openalex.org/W2963703197', 'https://openalex.org/W3042786565', 'https://openalex.org/W3036224891', 'https://openalex.org/W2962843773']",2021-06-05
https://openalex.org/W4390096798,https://doi.org/10.1109/taslp.2023.3345150,Universal Cross-Lingual Data Generation for Low Resource ASR,"Significant advances in end-to-end (E2E) automatic speech recognition (ASR) have primarily been concentrated on languages rich in annotated data. Nevertheless, a large proportion of languages worldwide, which are typically low-resource, continue to pose significant challenges. To address this issue, this study presents a novel speech synthesis framework based on data splicing that leverages self-supervised learning (SSL) units from Hidden Unit BERT (HuBERT) as universal phonetic units. In our framework, the SSL phonetic units serve as crucial bridges between speech and text across different languages. By leveraging these units, we successfully splice speech fragments from high-resource languages into synthesized speech that maintains acoustic coherence with text from low-resource languages. To further enhance the practicality of the framework, we introduce a sampling strategy based on confidence scores assigned to the speech segments used in data splicing. The application of this confidence sampling strategy in data splicing significantly accelerates ASR model convergence and enhances overall ASR performance. Experimental results on the <sc xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">CommonVoice</small> dataset show 25-35% relative improvement for four Indo-European languages and about 20% for Turkish using a 4-gram language model for rescoring, under a 10-hour low-resource setup. Furthermore, we showcase the scalability of our framework by incorporating a larger unsupervised speech corpus for generating speech fragments in data splicing, resulting in an additional 10% relative improvement.","['https://openalex.org/W2799800213', 'https://openalex.org/W6791904447', 'https://openalex.org/W6847363464', 'https://openalex.org/W3211278025', 'https://openalex.org/W2963303951', 'https://openalex.org/W2964309797', 'https://openalex.org/W4210463634', 'https://openalex.org/W2963292011', 'https://openalex.org/W2633221078', 'https://openalex.org/W2106440210', 'https://openalex.org/W4206662530', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W4224919704', 'https://openalex.org/W6787141514', 'https://openalex.org/W6770514103', 'https://openalex.org/W4285144981', 'https://openalex.org/W2962704885', 'https://openalex.org/W2894835365', 'https://openalex.org/W3096710170', 'https://openalex.org/W3015585292', 'https://openalex.org/W3095184753', 'https://openalex.org/W2395387595', 'https://openalex.org/W3096338464', 'https://openalex.org/W3026041220', 'https://openalex.org/W3160799772', 'https://openalex.org/W3209984917', 'https://openalex.org/W6810007534', 'https://openalex.org/W2896457183', 'https://openalex.org/W2982223350', 'https://openalex.org/W3096485810', 'https://openalex.org/W3041561163', 'https://openalex.org/W4297841831', 'https://openalex.org/W3198429080', 'https://openalex.org/W3213029956', 'https://openalex.org/W3094667432', 'https://openalex.org/W3193461931', 'https://openalex.org/W2962699523', 'https://openalex.org/W6792680588', 'https://openalex.org/W3162244132', 'https://openalex.org/W2889385306', 'https://openalex.org/W4225272718', 'https://openalex.org/W3162425752', 'https://openalex.org/W2144499799', 'https://openalex.org/W3081416955', 'https://openalex.org/W6762242920', 'https://openalex.org/W3015419784', 'https://openalex.org/W2889028433', 'https://openalex.org/W4210811812', 'https://openalex.org/W6606618604', 'https://openalex.org/W4385245566', 'https://openalex.org/W4297841830', 'https://openalex.org/W6771467084', 'https://openalex.org/W2933138175', 'https://openalex.org/W6796464841', 'https://openalex.org/W2962780374', 'https://openalex.org/W3113594615', 'https://openalex.org/W165119805', 'https://openalex.org/W2988736778', 'https://openalex.org/W3147900189']",2023-12-22
https://openalex.org/W4392903942,https://doi.org/10.1109/icassp48485.2024.10447164,Task Oriented Dialogue as a Catalyst for Self-Supervised Automatic Speech Recognition,"While word error rates of automatic speech recognition (ASR) systems have consistently fallen, natural language understanding (NLU) applications built on top of ASR systems still attribute significant numbers of failures to low-quality speech recognition results. Existing assistant systems collect large numbers of these unsuccessful interactions, but these systems usually fail to learn from these interactions, even in an offline fashion. In this work, we introduce CLC: Contrastive Learning for Conversations, a family of methods for contrastive fine-tuning of models in a self-supervised fashion, making use of easily detectable artifacts in unsuccessful conversations with assistants. We demonstrate that our CLC family of approaches can improve the performance of ASR models on OD3, a new public large-scale semi-synthetic meta-dataset of audio task-oriented dialogues, by up to 19.2%. These gains transfer to real-world systems as well, where we show that CLC can help to improve performance by up to 6.7% over baselines. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>","['https://openalex.org/W4313703382', 'https://openalex.org/W6847363464', 'https://openalex.org/W6780218876', 'https://openalex.org/W3160799772', 'https://openalex.org/W3205715971', 'https://openalex.org/W4372347360', 'https://openalex.org/W4386058713', 'https://openalex.org/W6803544266', 'https://openalex.org/W4372349774', 'https://openalex.org/W6852997429', 'https://openalex.org/W2886025712', 'https://openalex.org/W4225985539', 'https://openalex.org/W2973172693', 'https://openalex.org/W4224918838', 'https://openalex.org/W4226420874', 'https://openalex.org/W6849128116', 'https://openalex.org/W2962767983', 'https://openalex.org/W4297841871', 'https://openalex.org/W6776700526', 'https://openalex.org/W2963491014', 'https://openalex.org/W6754637164', 'https://openalex.org/W4389010490', 'https://openalex.org/W3156555225', 'https://openalex.org/W4221146627', 'https://openalex.org/W6767671539', 'https://openalex.org/W6805710207', 'https://openalex.org/W6771467084', 'https://openalex.org/W2251058040', 'https://openalex.org/W4210673375', 'https://openalex.org/W3097777922', 'https://openalex.org/W3214803981', 'https://openalex.org/W3148654612', 'https://openalex.org/W6761205521', 'https://openalex.org/W2974231335', 'https://openalex.org/W2964006684', 'https://openalex.org/W4377865080', 'https://openalex.org/W3030437843', 'https://openalex.org/W3209721572', 'https://openalex.org/W3036601975', 'https://openalex.org/W4287812705']",2024-03-18
https://openalex.org/W4392908913,https://doi.org/10.1109/icassp48485.2024.10445745,Learning Semantic Information from Raw Audio Signal Using Both Contextual and Phonetic Representations,"We propose a framework to learn semantics from raw audio signals using two types of representations, encoding contextual and phonetic information respectively. Specifically, we introduce a speech-to-unit processing pipeline that captures two types of representations with different time resolutions. For the language model, we adopt a dual-channel architecture to incorporate both types of representation. We also present new training objectives, masked context reconstruction and masked context prediction, that push models to learn semantics effectively. Experiments on the sSIMI metric of Zero Resource Speech Benchmark 2021 and Fluent Speech Command dataset show our framework learns semantics better than models trained with only one type of representation.","['https://openalex.org/W6786696081', 'https://openalex.org/W6790356757', 'https://openalex.org/W4287887366', 'https://openalex.org/W6844194202', 'https://openalex.org/W6755207826', 'https://openalex.org/W3198815374', 'https://openalex.org/W3197349023', 'https://openalex.org/W3197381195', 'https://openalex.org/W4224918488', 'https://openalex.org/W4224927737', 'https://openalex.org/W6809593508', 'https://openalex.org/W6780218876', 'https://openalex.org/W3160799772', 'https://openalex.org/W3198217962', 'https://openalex.org/W4307680525', 'https://openalex.org/W3197259906', 'https://openalex.org/W3197324626', 'https://openalex.org/W3197744084', 'https://openalex.org/W3161302809', 'https://openalex.org/W3096109555', 'https://openalex.org/W3095771422', 'https://openalex.org/W4292825791', 'https://openalex.org/W6770596778', 'https://openalex.org/W4226380987', 'https://openalex.org/W2142625445', 'https://openalex.org/W1494198834', 'https://openalex.org/W2972584841', 'https://openalex.org/W3198594919', 'https://openalex.org/W2995680346', 'https://openalex.org/W4394671563', 'https://openalex.org/W4297808394', 'https://openalex.org/W3036601975', 'https://openalex.org/W2896457183', 'https://openalex.org/W4221161768', 'https://openalex.org/W4287591426']",2024-03-18
https://openalex.org/W4394782978,https://doi.org/10.1145/3639592.3639623,"Multi-stage Multi-modalities Fusion of Lip, Tongue and Acoustics Information for Speech Recognition","The ultrasound tongue imaging (UTI) and lip video are commonly used to capture the acoustic clue to obtain the visual articulatory information of speakers. However, single signal of UTI or lip video cannot completely represent the pronunciation process of speakers. In this paper, we proposed to use the convolutional neural network (CNN)-based framework to fuse the lip and tongue movement information to represent the pronunciation process of speakers. In addition, we designed the multi-stage fusion framework (MF-SR) to fuse the lip-tongue visual information and the acoustic features extracted from the speech. To evaluate our proposed method, we designed the data stream comparative experiments, the speech pattern comparative experiments, and the data increment experiments based on TAL1 dataset. The results show that the best word error rate (WER) of our proposed method on audio-visual speech recognition task is 20.03%. The best WER of our proposed model on visual-only speech recognition task is 23.34%, which is reduced by 1.75% compared with the baseline method. The results illustrate that our proposed method can effectively further improve the performance of the lip-tongue-audio fusion speech recognition.","['https://openalex.org/W2009357672', 'https://openalex.org/W2109272898', 'https://openalex.org/W2401960014', 'https://openalex.org/W1967225830', 'https://openalex.org/W3016011581', 'https://openalex.org/W2996970093', 'https://openalex.org/W1987337335', 'https://openalex.org/W2768153200', 'https://openalex.org/W2941048526', 'https://openalex.org/W2905699130', 'https://openalex.org/W2397670449', 'https://openalex.org/W2427345690', 'https://openalex.org/W2008120082', 'https://openalex.org/W3143787022', 'https://openalex.org/W3196749493', 'https://openalex.org/W1995735739', 'https://openalex.org/W2963272864', 'https://openalex.org/W2963522845', 'https://openalex.org/W4310348293', 'https://openalex.org/W2964172053', 'https://openalex.org/W3160799772', 'https://openalex.org/W3122100772', 'https://openalex.org/W4375869379', 'https://openalex.org/W3112616666', 'https://openalex.org/W4360993686', 'https://openalex.org/W2087681821', 'https://openalex.org/W2963787388', 'https://openalex.org/W2963979492', 'https://openalex.org/W2295119550', 'https://openalex.org/W2769531941', 'https://openalex.org/W3198275944', 'https://openalex.org/W4299508085', 'https://openalex.org/W4206963459']",2023-12-16
https://openalex.org/W4408154964,https://doi.org/10.3390/bdcc9030059,Comparative Analysis of Audio Feature Extraction for Real-Time Talking Portrait Synthesis,"This paper explores advancements in real-time talking-head generation, focusing on overcoming challenges in Audio Feature Extraction (AFE), which often introduces latency and limits responsiveness in real-time applications. To address these issues, we propose and implement a fully integrated system that replaces conventional AFE models with OpenAI’s Whisper, leveraging its encoder to optimize processing and improve overall system efficiency. Our evaluation of two open-source real-time models across three different datasets shows that Whisper not only accelerates processing but also improves specific aspects of rendering quality, resulting in more realistic and responsive talking-head interactions. Although interviewer training systems are considered a potential application, the primary contribution of this work is the improvement of the technical foundations necessary for creating responsive AI avatars. These advancements enable more immersive interactions and expand the scope of AI-driven applications, including educational tools and simulated training environments.","['https://openalex.org/W3039587732', 'https://openalex.org/W3029186289', 'https://openalex.org/W2536863175', 'https://openalex.org/W1851686334', 'https://openalex.org/W2103249953', 'https://openalex.org/W139318871', 'https://openalex.org/W4235562789', 'https://openalex.org/W4281650626', 'https://openalex.org/W4401507490', 'https://openalex.org/W4283520499', 'https://openalex.org/W4200150166', 'https://openalex.org/W6780218876', 'https://openalex.org/W3160799772', 'https://openalex.org/W4392974949', 'https://openalex.org/W4400524955', 'https://openalex.org/W4311000453', 'https://openalex.org/W2193413348', 'https://openalex.org/W7034400414', 'https://openalex.org/W4382777461', 'https://openalex.org/W2175420983', 'https://openalex.org/W2793048903', 'https://openalex.org/W4391708057', 'https://openalex.org/W4200174933', 'https://openalex.org/W3016011332', 'https://openalex.org/W4385318467', 'https://openalex.org/W4403792004', 'https://openalex.org/W3211147706', 'https://openalex.org/W4390872116', 'https://openalex.org/W2131774270', 'https://openalex.org/W3097792222', 'https://openalex.org/W2124509324', 'https://openalex.org/W1494198834', 'https://openalex.org/W2995181338', 'https://openalex.org/W3011411500', 'https://openalex.org/W2962785568', 'https://openalex.org/W2963081548', 'https://openalex.org/W6765779288', 'https://openalex.org/W2395639500', 'https://openalex.org/W4250903264', 'https://openalex.org/W2616137712', 'https://openalex.org/W2614986935', 'https://openalex.org/W6772922406', 'https://openalex.org/W6792452179', 'https://openalex.org/W4301206121', 'https://openalex.org/W3000442796', 'https://openalex.org/W3139130199', 'https://openalex.org/W3036601975']",2025-03-04
https://openalex.org/W3209984917,https://doi.org/10.1109/jstsp.2022.3188113,WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing,"Self-supervised learning (SSL) achieves great success in speech recognition,\nwhile limited exploration has been attempted for other speech processing tasks.\nAs speech signal contains multi-faceted information including speaker identity,\nparalinguistics, spoken content, etc., learning universal representations for\nall speech tasks is challenging. To tackle the problem, we propose a new\npre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM\njointly learns masked speech prediction and denoising in pre-training. By this\nmeans, WavLM does not only keep the speech content modeling capability by the\nmasked speech prediction, but also improves the potential to non-ASR tasks by\nthe speech denoising. In addition, WavLM employs gated relative position bias\nfor the Transformer structure to better capture the sequence ordering of input\nspeech. We also scale up the training dataset from 60k hours to 94k hours.\nWavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and\nbrings significant improvements for various speech processing tasks on their\nrepresentative benchmarks. The code and pre-trained models are available at\nhttps://aka.ms/wavlm.\n","['https://openalex.org/W2896457183', 'https://openalex.org/W6763701032', 'https://openalex.org/W6769627184', 'https://openalex.org/W6844194202', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W6788335241', 'https://openalex.org/W3197580070', 'https://openalex.org/W2995181338', 'https://openalex.org/W6791904447', 'https://openalex.org/W3198270883', 'https://openalex.org/W2963122170', 'https://openalex.org/W3008181812', 'https://openalex.org/W3198771897', 'https://openalex.org/W3175898847', 'https://openalex.org/W3198694222', 'https://openalex.org/W3119308075', 'https://openalex.org/W3024869864', 'https://openalex.org/W2981087920', 'https://openalex.org/W3016232124', 'https://openalex.org/W3163842642', 'https://openalex.org/W3212886388', 'https://openalex.org/W4225661121', 'https://openalex.org/W2951974815', 'https://openalex.org/W2962850167', 'https://openalex.org/W6745117592', 'https://openalex.org/W3100270690', 'https://openalex.org/W3097286738', 'https://openalex.org/W2972943112', 'https://openalex.org/W3016011332', 'https://openalex.org/W3035202887', 'https://openalex.org/W3198858531', 'https://openalex.org/W3041561163', 'https://openalex.org/W2982223350', 'https://openalex.org/W3015265920', 'https://openalex.org/W3206996142', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W3015356564', 'https://openalex.org/W4226033575', 'https://openalex.org/W2973157397', 'https://openalex.org/W3015213852', 'https://openalex.org/W3212799896', 'https://openalex.org/W3206252155', 'https://openalex.org/W3204696009', 'https://openalex.org/W6739901393', 'https://openalex.org/W3197042120', 'https://openalex.org/W3194687854', 'https://openalex.org/W1494198834', 'https://openalex.org/W6796242362', 'https://openalex.org/W3016181583', 'https://openalex.org/W2808631503', 'https://openalex.org/W6688816777', 'https://openalex.org/W2969985801', 'https://openalex.org/W6801723603', 'https://openalex.org/W3094374485', 'https://openalex.org/W2157161740', 'https://openalex.org/W125553504', 'https://openalex.org/W3095212884', 'https://openalex.org/W6779069803', 'https://openalex.org/W3178462146', 'https://openalex.org/W4220731890', 'https://openalex.org/W3196857193', 'https://openalex.org/W2696967604', 'https://openalex.org/W2972949456', 'https://openalex.org/W123007118', 'https://openalex.org/W3095189764', 'https://openalex.org/W2069681747', 'https://openalex.org/W2765425905', 'https://openalex.org/W2803322398', 'https://openalex.org/W2117678320', 'https://openalex.org/W2030486566', 'https://openalex.org/W6757817989', 'https://openalex.org/W3160936850', 'https://openalex.org/W3193846000', 'https://openalex.org/W3205495812', 'https://openalex.org/W6770506093', 'https://openalex.org/W3016010032', 'https://openalex.org/W3095173472', 'https://openalex.org/W3097777922', 'https://openalex.org/W2127141656', 'https://openalex.org/W2936774411', 'https://openalex.org/W6768080748', 'https://openalex.org/W2331143823', 'https://openalex.org/W2953190524', 'https://openalex.org/W3205644108', 'https://openalex.org/W2963618559', 'https://openalex.org/W3026868282', 'https://openalex.org/W2963341956', 'https://openalex.org/W2996383576', 'https://openalex.org/W2950813464', 'https://openalex.org/W3004728855', 'https://openalex.org/W3125709657', 'https://openalex.org/W3099782249', 'https://openalex.org/W4287120025', 'https://openalex.org/W3178296206', 'https://openalex.org/W2219249508', 'https://openalex.org/W2758785877', 'https://openalex.org/W4385245566', 'https://openalex.org/W4297808394', 'https://openalex.org/W3169688220', 'https://openalex.org/W4300427991', 'https://openalex.org/W3125596972', 'https://openalex.org/W2975381464', 'https://openalex.org/W4308349017', 'https://openalex.org/W4287374065', 'https://openalex.org/W2991213871', 'https://openalex.org/W3095292526', 'https://openalex.org/W3112034174', 'https://openalex.org/W3082274269', 'https://openalex.org/W2972712416', 'https://openalex.org/W3157923770', 'https://openalex.org/W3144173820', 'https://openalex.org/W3169320628', 'https://openalex.org/W4288089799', 'https://openalex.org/W3198698812', 'https://openalex.org/W4307023467', 'https://openalex.org/W2970597249', 'https://openalex.org/W2963403868', 'https://openalex.org/W3174648740', 'https://openalex.org/W3139918052', 'https://openalex.org/W4285250921', 'https://openalex.org/W2726515241', 'https://openalex.org/W4294796045', 'https://openalex.org/W4306169301', 'https://openalex.org/W3165647589', 'https://openalex.org/W2979476256', 'https://openalex.org/W3033627755', 'https://openalex.org/W2842511635', 'https://openalex.org/W2908510526', 'https://openalex.org/W3002741552', 'https://openalex.org/W3036601975', 'https://openalex.org/W3162648834']",2022-07-04
https://openalex.org/W3041561163,https://doi.org/10.1109/taslp.2021.3095662,TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech,"We introduce a self-supervised speech pre-training method called TERA, which\nstands for Transformer Encoder Representations from Alteration. Recent\napproaches often learn by using a single auxiliary task like contrastive\nprediction, autoregressive prediction, or masked reconstruction. Unlike\nprevious methods, we use alteration along three orthogonal axes to pre-train\nTransformer Encoders on a large amount of unlabeled speech. The model learns\nthrough the reconstruction of acoustic frames from their altered counterpart,\nwhere we use a stochastic policy to alter along various dimensions: time,\nfrequency, and magnitude. TERA can be used for speech representations\nextraction or fine-tuning with downstream models. We evaluate TERA on several\ndownstream tasks, including phoneme classification, keyword spotting, speaker\nrecognition, and speech recognition. We present a large-scale comparison of\nvarious self-supervised models. TERA achieves strong performance in the\ncomparison by improving upon surface features and outperforming previous\nmodels. In our experiments, we study the effect of applying different\nalteration techniques, pre-training on more data, and pre-training on various\nfeatures. We analyze different model sizes and find that smaller models are\nstrong representation learners than larger models, while larger models are more\neffective for downstream fine-tuning than smaller models. Furthermore, we show\nthe proposed method is transferable to downstream datasets not used in\npre-training.\n","['https://openalex.org/W3096171739', 'https://openalex.org/W6763701032', 'https://openalex.org/W2927746189', 'https://openalex.org/W6768021236', 'https://openalex.org/W2962739339', 'https://openalex.org/W6607333740', 'https://openalex.org/W2127141656', 'https://openalex.org/W6739901393', 'https://openalex.org/W6784776607', 'https://openalex.org/W6763238093', 'https://openalex.org/W2973049979', 'https://openalex.org/W6844194202', 'https://openalex.org/W2982223350', 'https://openalex.org/W3003875258', 'https://openalex.org/W6777232839', 'https://openalex.org/W3096626135', 'https://openalex.org/W3096485810', 'https://openalex.org/W6778265221', 'https://openalex.org/W6777859476', 'https://openalex.org/W2794209590', 'https://openalex.org/W2125496931', 'https://openalex.org/W2962901777', 'https://openalex.org/W2995181338', 'https://openalex.org/W2002342963', 'https://openalex.org/W2964227577', 'https://openalex.org/W2077804127', 'https://openalex.org/W3035202887', 'https://openalex.org/W3097286738', 'https://openalex.org/W3096017728', 'https://openalex.org/W3015265920', 'https://openalex.org/W6840487619', 'https://openalex.org/W3015949486', 'https://openalex.org/W3100270690', 'https://openalex.org/W2947445680', 'https://openalex.org/W2982039329', 'https://openalex.org/W2973157397', 'https://openalex.org/W3033038061', 'https://openalex.org/W3015356564', 'https://openalex.org/W6769196770', 'https://openalex.org/W6773205534', 'https://openalex.org/W2972943112', 'https://openalex.org/W3016181583', 'https://openalex.org/W6631362777', 'https://openalex.org/W3016011332', 'https://openalex.org/W2972451902', 'https://openalex.org/W1494198834', 'https://openalex.org/W6757817989', 'https://openalex.org/W3015412890', 'https://openalex.org/W2936774411', 'https://openalex.org/W3024182269', 'https://openalex.org/W1635512741', 'https://openalex.org/W2943493972', 'https://openalex.org/W2981991061', 'https://openalex.org/W3125709657', 'https://openalex.org/W2980708516', 'https://openalex.org/W4385245566', 'https://openalex.org/W1524333225', 'https://openalex.org/W3160345865', 'https://openalex.org/W1522301498', 'https://openalex.org/W2964303116', 'https://openalex.org/W2950577311', 'https://openalex.org/W3030987249', 'https://openalex.org/W1614298861', 'https://openalex.org/W179875071', 'https://openalex.org/W3099782249', 'https://openalex.org/W3102342027', 'https://openalex.org/W3036601975', 'https://openalex.org/W2947454875', 'https://openalex.org/W2979476256', 'https://openalex.org/W3095292526', 'https://openalex.org/W2950813464', 'https://openalex.org/W3148040514', 'https://openalex.org/W2842511635', 'https://openalex.org/W2996428491', 'https://openalex.org/W4297808394', 'https://openalex.org/W3104896896', 'https://openalex.org/W2908510526', 'https://openalex.org/W1608367484', 'https://openalex.org/W2949667497', 'https://openalex.org/W2970597249', 'https://openalex.org/W3015412285', 'https://openalex.org/W3198858531', 'https://openalex.org/W4288348042', 'https://openalex.org/W2988736778', 'https://openalex.org/W2996383576', 'https://openalex.org/W2965373594', 'https://openalex.org/W2963341956', 'https://openalex.org/W3026957705', 'https://openalex.org/W3026842484', 'https://openalex.org/W2963403868']",2021-01-01
https://openalex.org/W4281492411,https://doi.org/10.1109/jstsp.2022.3207050,Self-Supervised Speech Representation Learning: A Review,"Although supervised deep learning has revolutionized speech and audio\nprocessing, it has necessitated the building of specialist models for\nindividual tasks and application scenarios. It is likewise difficult to apply\nthis to dialects and languages for which only limited labeled data is\navailable. Self-supervised representation learning methods promise a single\nuniversal model that would benefit a wide variety of tasks and domains. Such\nmethods have shown success in natural language processing and computer vision\ndomains, achieving new levels of performance while reducing the number of\nlabels required for many downstream scenarios. Speech representation learning\nis experiencing similar progress in three main categories: generative,\ncontrastive, and predictive methods. Other approaches rely on multi-modal data\nfor pre-training, mixing text or visual data streams with speech. Although\nself-supervised speech representation is still a nascent research area, it is\nclosely related to acoustic word embedding and learning with zero lexical\nresources, both of which have seen active research for many years. This review\npresents approaches for self-supervised speech representation learning and\ntheir connection to other research areas. Since many current methods focus\nsolely on automatic speech recognition as a downstream task, we review recent\nefforts on benchmarking learned representations to extend the application\nbeyond speech recognition.\n","['https://openalex.org/W2919115771', 'https://openalex.org/W2160815625', 'https://openalex.org/W811578723', 'https://openalex.org/W1555037511', 'https://openalex.org/W1975113979', 'https://openalex.org/W2110073835', 'https://openalex.org/W2124537004', 'https://openalex.org/W6683825394', 'https://openalex.org/W2163922914', 'https://openalex.org/W1901616594', 'https://openalex.org/W4244017338', 'https://openalex.org/W4239390603', 'https://openalex.org/W6675401909', 'https://openalex.org/W1902027874', 'https://openalex.org/W2100495367', 'https://openalex.org/W6800751262', 'https://openalex.org/W3207924272', 'https://openalex.org/W3035725276', 'https://openalex.org/W6773996589', 'https://openalex.org/W3185341429', 'https://openalex.org/W6784023748', 'https://openalex.org/W3011574394', 'https://openalex.org/W3023371261', 'https://openalex.org/W6811170316', 'https://openalex.org/W6772230580', 'https://openalex.org/W1703050006', 'https://openalex.org/W2048648518', 'https://openalex.org/W2100969003', 'https://openalex.org/W1979447841', 'https://openalex.org/W1877570817', 'https://openalex.org/W6680522077', 'https://openalex.org/W1487784522', 'https://openalex.org/W2155230809', 'https://openalex.org/W2150769028', 'https://openalex.org/W2408021097', 'https://openalex.org/W6681096077', 'https://openalex.org/W6680106237', 'https://openalex.org/W2145889472', 'https://openalex.org/W2113606819', 'https://openalex.org/W2067474491', 'https://openalex.org/W6736430770', 'https://openalex.org/W2116064496', 'https://openalex.org/W2923014074', 'https://openalex.org/W3197580070', 'https://openalex.org/W2326925005', 'https://openalex.org/W2883725317', 'https://openalex.org/W343636949', 'https://openalex.org/W6640963894', 'https://openalex.org/W6639732818', 'https://openalex.org/W3217536461', 'https://openalex.org/W2962739339', 'https://openalex.org/W6767997687', 'https://openalex.org/W2896457183', 'https://openalex.org/W6766673545', 'https://openalex.org/W6844194202', 'https://openalex.org/W3035524453', 'https://openalex.org/W6774670964', 'https://openalex.org/W6779997284', 'https://openalex.org/W2962907457', 'https://openalex.org/W3026041220', 'https://openalex.org/W3096338464', 'https://openalex.org/W6811088048', 'https://openalex.org/W2913340405', 'https://openalex.org/W2291975472', 'https://openalex.org/W3112702554', 'https://openalex.org/W4226380987', 'https://openalex.org/W2973157397', 'https://openalex.org/W6617744952', 'https://openalex.org/W6712395597', 'https://openalex.org/W2962850167', 'https://openalex.org/W6745117592', 'https://openalex.org/W6757193177', 'https://openalex.org/W2752796333', 'https://openalex.org/W3140429000', 'https://openalex.org/W3198217962', 'https://openalex.org/W6790356757', 'https://openalex.org/W6690026940', 'https://openalex.org/W2097012520', 'https://openalex.org/W1945356021', 'https://openalex.org/W3100270690', 'https://openalex.org/W6729448088', 'https://openalex.org/W2972867623', 'https://openalex.org/W2035424729', 'https://openalex.org/W2020607164', 'https://openalex.org/W2396043527', 'https://openalex.org/W1545920196', 'https://openalex.org/W1796128977', 'https://openalex.org/W2932675979', 'https://openalex.org/W2972943112', 'https://openalex.org/W3016011332', 'https://openalex.org/W2020883660', 'https://openalex.org/W2146444479', 'https://openalex.org/W3035202887', 'https://openalex.org/W3097286738', 'https://openalex.org/W3015265920', 'https://openalex.org/W2982223350', 'https://openalex.org/W6769238691', 'https://openalex.org/W6778265221', 'https://openalex.org/W3003875258', 'https://openalex.org/W3160345865', 'https://openalex.org/W3196919915', 'https://openalex.org/W3041561163', 'https://openalex.org/W3198858531', 'https://openalex.org/W3096485810', 'https://openalex.org/W3196798358', 'https://openalex.org/W6674330103', 'https://openalex.org/W3015213852', 'https://openalex.org/W2963425185', 'https://openalex.org/W6840487619', 'https://openalex.org/W3015949486', 'https://openalex.org/W2982039329', 'https://openalex.org/W2963571336', 'https://openalex.org/W3148040514', 'https://openalex.org/W2963317665', 'https://openalex.org/W2973049979', 'https://openalex.org/W3016181583', 'https://openalex.org/W3102342027', 'https://openalex.org/W6769196770', 'https://openalex.org/W6780218876', 'https://openalex.org/W3197411683', 'https://openalex.org/W4226033575', 'https://openalex.org/W3198608154', 'https://openalex.org/W3015356564', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W6810007534', 'https://openalex.org/W6810673746', 'https://openalex.org/W6677884823', 'https://openalex.org/W6682948231', 'https://openalex.org/W2124509324', 'https://openalex.org/W6762931180', 'https://openalex.org/W3159481202', 'https://openalex.org/W1536680647', 'https://openalex.org/W6766978945', 'https://openalex.org/W6600971220', 'https://openalex.org/W2096391593', 'https://openalex.org/W1974783905', 'https://openalex.org/W2091863306', 'https://openalex.org/W4237938692', 'https://openalex.org/W142945732', 'https://openalex.org/W2594690981', 'https://openalex.org/W6810168380', 'https://openalex.org/W2016538560', 'https://openalex.org/W4396724964', 'https://openalex.org/W2296654356', 'https://openalex.org/W6686207219', 'https://openalex.org/W900447646', 'https://openalex.org/W6606244218', 'https://openalex.org/W4237723258', 'https://openalex.org/W6631216910', 'https://openalex.org/W6639103823', 'https://openalex.org/W6728094556', 'https://openalex.org/W6693697572', 'https://openalex.org/W1484147505', 'https://openalex.org/W2130055251', 'https://openalex.org/W2049252044', 'https://openalex.org/W6678885645', 'https://openalex.org/W1531883353', 'https://openalex.org/W6633682082', 'https://openalex.org/W2136189984', 'https://openalex.org/W4297841641', 'https://openalex.org/W3157861865', 'https://openalex.org/W6864391120', 'https://openalex.org/W6729977899', 'https://openalex.org/W2962862718', 'https://openalex.org/W2971709506', 'https://openalex.org/W3197828817', 'https://openalex.org/W3200287550', 'https://openalex.org/W2988907666', 'https://openalex.org/W3196698946', 'https://openalex.org/W3205715971', 'https://openalex.org/W6809593508', 'https://openalex.org/W6770596778', 'https://openalex.org/W2586148577', 'https://openalex.org/W2964115348', 'https://openalex.org/W2963902314', 'https://openalex.org/W4224875474', 'https://openalex.org/W3095293218', 'https://openalex.org/W2963330681', 'https://openalex.org/W2920166246', 'https://openalex.org/W2895651543', 'https://openalex.org/W2973135958', 'https://openalex.org/W6803092890', 'https://openalex.org/W1577418252', 'https://openalex.org/W1496120315', 'https://openalex.org/W2962980711', 'https://openalex.org/W2964169922', 'https://openalex.org/W6720204814', 'https://openalex.org/W2296681920', 'https://openalex.org/W2059652594', 'https://openalex.org/W2889313720', 'https://openalex.org/W2963720603', 'https://openalex.org/W6786885278', 'https://openalex.org/W2407151108', 'https://openalex.org/W2190506272', 'https://openalex.org/W3037530970', 'https://openalex.org/W3201254286', 'https://openalex.org/W3150635893', 'https://openalex.org/W2995181338', 'https://openalex.org/W2593116425', 'https://openalex.org/W4289665794', 'https://openalex.org/W6603931906', 'https://openalex.org/W1494198834', 'https://openalex.org/W2024490156', 'https://openalex.org/W6771467084', 'https://openalex.org/W3095410713', 'https://openalex.org/W3119308075', 'https://openalex.org/W6696449567', 'https://openalex.org/W3213029956', 'https://openalex.org/W3206252155', 'https://openalex.org/W3198694222', 'https://openalex.org/W2799473636', 'https://openalex.org/W6691509046', 'https://openalex.org/W2166637769', 'https://openalex.org/W3139878283', 'https://openalex.org/W1526236009', 'https://openalex.org/W2963242190', 'https://openalex.org/W2963127222', 'https://openalex.org/W2884797218', 'https://openalex.org/W6712941328', 'https://openalex.org/W2883409523', 'https://openalex.org/W6936113694', 'https://openalex.org/W2726515241', 'https://openalex.org/W2972584841', 'https://openalex.org/W1567520911', 'https://openalex.org/W6748215858', 'https://openalex.org/W3196509775', 'https://openalex.org/W2094544353', 'https://openalex.org/W6727418883', 'https://openalex.org/W6712757354', 'https://openalex.org/W6731521493', 'https://openalex.org/W6688816777', 'https://openalex.org/W2883595988', 'https://openalex.org/W6750665317', 'https://openalex.org/W2775794021', 'https://openalex.org/W6736723571', 'https://openalex.org/W2972894903', 'https://openalex.org/W3033038061', 'https://openalex.org/W942963634', 'https://openalex.org/W3197223534', 'https://openalex.org/W6784614252', 'https://openalex.org/W6753575415', 'https://openalex.org/W3160554450', 'https://openalex.org/W6803378298', 'https://openalex.org/W3189296823', 'https://openalex.org/W3093096176', 'https://openalex.org/W6809947431', 'https://openalex.org/W3006926732', 'https://openalex.org/W4225713393', 'https://openalex.org/W3096216486', 'https://openalex.org/W3095361818', 'https://openalex.org/W2251253014', 'https://openalex.org/W6795952400', 'https://openalex.org/W2970820321', 'https://openalex.org/W6801828775', 'https://openalex.org/W3198815374', 'https://openalex.org/W3024182269', 'https://openalex.org/W4224934179', 'https://openalex.org/W3096017728', 'https://openalex.org/W3162133897', 'https://openalex.org/W6784614126', 'https://openalex.org/W2786608204', 'https://openalex.org/W3207558756', 'https://openalex.org/W3198771897', 'https://openalex.org/W3198429080', 'https://openalex.org/W2962799225', 'https://openalex.org/W2802557066', 'https://openalex.org/W6735913928', 'https://openalex.org/W6751433836', 'https://openalex.org/W6744957266', 'https://openalex.org/W2899134946', 'https://openalex.org/W6750365303', 'https://openalex.org/W6757699909', 'https://openalex.org/W2962799131', 'https://openalex.org/W6738077056', 'https://openalex.org/W6760519848', 'https://openalex.org/W3214697273', 'https://openalex.org/W6678282225', 'https://openalex.org/W4319862670', 'https://openalex.org/W6745740328', 'https://openalex.org/W6745388339', 'https://openalex.org/W3204917342', 'https://openalex.org/W2962699523', 'https://openalex.org/W2963739817', 'https://openalex.org/W3015280134', 'https://openalex.org/W2963796886', 'https://openalex.org/W2963581463', 'https://openalex.org/W2972889948', 'https://openalex.org/W2119717200', 'https://openalex.org/W2046932483', 'https://openalex.org/W6640059789', 'https://openalex.org/W2577366047', 'https://openalex.org/W2964243274', 'https://openalex.org/W6756385397', 'https://openalex.org/W2940200615', 'https://openalex.org/W3008480565', 'https://openalex.org/W3024464021', 'https://openalex.org/W3016008406', 'https://openalex.org/W2883586237', 'https://openalex.org/W2964012862', 'https://openalex.org/W3097632072', 'https://openalex.org/W2963216553', 'https://openalex.org/W2025482506', 'https://openalex.org/W2963620343', 'https://openalex.org/W2940544976', 'https://openalex.org/W6786696081', 'https://openalex.org/W2347098582', 'https://openalex.org/W2787447541', 'https://openalex.org/W2972374322', 'https://openalex.org/W3197381195', 'https://openalex.org/W6803066952', 'https://openalex.org/W4307680525', 'https://openalex.org/W4221146627', 'https://openalex.org/W6805530253', 'https://openalex.org/W6777028661', 'https://openalex.org/W4287854499', 'https://openalex.org/W6759579507', 'https://openalex.org/W3176828726', 'https://openalex.org/W6787411158', 'https://openalex.org/W4225274946', 'https://openalex.org/W4226162428', 'https://openalex.org/W6796551075', 'https://openalex.org/W3203140070', 'https://openalex.org/W6685943813', 'https://openalex.org/W3137147200', 'https://openalex.org/W3085139254', 'https://openalex.org/W3198039885', 'https://openalex.org/W6839738141', 'https://openalex.org/W6803547063', 'https://openalex.org/W4297841871', 'https://openalex.org/W3214576767', 'https://openalex.org/W4296068785', 'https://openalex.org/W3209376089', 'https://openalex.org/W4221140371', 'https://openalex.org/W4292825791', 'https://openalex.org/W3101648800', 'https://openalex.org/W4200635400', 'https://openalex.org/W3088409176', 'https://openalex.org/W4286918540', 'https://openalex.org/W2102409316', 'https://openalex.org/W3211224152', 'https://openalex.org/W4288348042', 'https://openalex.org/W2981991061', 'https://openalex.org/W2998249245', 'https://openalex.org/W3009561768', 'https://openalex.org/W2242818861', 'https://openalex.org/W2997574889', 'https://openalex.org/W1515020792', 'https://openalex.org/W2965373594', 'https://openalex.org/W2530846021', 'https://openalex.org/W4297808394', 'https://openalex.org/W1915251500', 'https://openalex.org/W4295116917', 'https://openalex.org/W2964303773', 'https://openalex.org/W3026842484', 'https://openalex.org/W2125290066', 'https://openalex.org/W2219249508', 'https://openalex.org/W2095705004', 'https://openalex.org/W22517275', 'https://openalex.org/W4221145109', 'https://openalex.org/W4287591426', 'https://openalex.org/W2898727538', 'https://openalex.org/W2797583228', 'https://openalex.org/W3195577433', 'https://openalex.org/W3099142230', 'https://openalex.org/W3107298252', 'https://openalex.org/W3213873715', 'https://openalex.org/W2973026522', 'https://openalex.org/W4289750118', 'https://openalex.org/W4394671563', 'https://openalex.org/W3207222250', 'https://openalex.org/W3024605872', 'https://openalex.org/W3161411634', 'https://openalex.org/W2973727699', 'https://openalex.org/W4320013820', 'https://openalex.org/W1508165687', 'https://openalex.org/W2138204974']",2022-09-15
https://openalex.org/W4224933800,https://doi.org/10.1109/icassp43922.2022.9746303,Investigating Self-Supervised Learning for Speech Enhancement and Separation,"Speech enhancement and separation are two fundamental tasks for robust speech processing. Speech enhancement suppresses background noise while speech separation extracts target speech from interfering speakers. Despite a great number of supervised learning-based enhancement and separation methods having been proposed and achieving good performance, studies on applying self-supervised learning (SSL) to enhancement and separation are limited. In this paper, we evaluate 13 SSL upstream methods on speech enhancement and separation downstream tasks. Our experimental results on Voicebank-DEMAND and Libri2Mix show that some SSL representations consistently outperform baseline features including the short-time Fourier transform (STFT) magnitude and log Mel filterbank (FBANK). Furthermore, we analyze the factors that make existing SSL frameworks difficult to apply to speech enhancement and separation and discuss the representation properties desired for both tasks. Our study is included as the official speech enhancement and separation downstreams for SUPERB.","['https://openalex.org/W2842511635', 'https://openalex.org/W3041561163', 'https://openalex.org/W3035725276', 'https://openalex.org/W6785000729', 'https://openalex.org/W3197580070', 'https://openalex.org/W3161223924', 'https://openalex.org/W2982223350', 'https://openalex.org/W6784776607', 'https://openalex.org/W3097286738', 'https://openalex.org/W2972943112', 'https://openalex.org/W3095717210', 'https://openalex.org/W6787335539', 'https://openalex.org/W3015356564', 'https://openalex.org/W3197642003', 'https://openalex.org/W3136499730', 'https://openalex.org/W2962866211', 'https://openalex.org/W2962739339', 'https://openalex.org/W3034781633', 'https://openalex.org/W6755207826', 'https://openalex.org/W2973049979', 'https://openalex.org/W3035524453', 'https://openalex.org/W3209059054', 'https://openalex.org/W6780218876', 'https://openalex.org/W6777776875', 'https://openalex.org/W6629717138', 'https://openalex.org/W3198771897', 'https://openalex.org/W6771812881', 'https://openalex.org/W6802609832', 'https://openalex.org/W2067295501', 'https://openalex.org/W1552314771', 'https://openalex.org/W2972541922', 'https://openalex.org/W2221409856', 'https://openalex.org/W3005511757', 'https://openalex.org/W2734774145', 'https://openalex.org/W2558649592', 'https://openalex.org/W2963341071', 'https://openalex.org/W3097945073', 'https://openalex.org/W2952218014', 'https://openalex.org/W6768815455', 'https://openalex.org/W3096893582', 'https://openalex.org/W3163652268', 'https://openalex.org/W3185109982', 'https://openalex.org/W4237168004', 'https://openalex.org/W3020336359', 'https://openalex.org/W1897240248', 'https://openalex.org/W6791480293', 'https://openalex.org/W6762114000', 'https://openalex.org/W4232282348', 'https://openalex.org/W2802304149', 'https://openalex.org/W3197912330', 'https://openalex.org/W2460742184', 'https://openalex.org/W3015213852', 'https://openalex.org/W2094721231', 'https://openalex.org/W2757519008', 'https://openalex.org/W6753000030', 'https://openalex.org/W6769196770', 'https://openalex.org/W6803164887', 'https://openalex.org/W6803394801', 'https://openalex.org/W2896457183', 'https://openalex.org/W3036601975', 'https://openalex.org/W3015199127', 'https://openalex.org/W3099330747', 'https://openalex.org/W3097568057', 'https://openalex.org/W3198275944', 'https://openalex.org/W3208743843', 'https://openalex.org/W1494198834', 'https://openalex.org/W2995181338', 'https://openalex.org/W2949558265', 'https://openalex.org/W3198858531', 'https://openalex.org/W2883725317', 'https://openalex.org/W3027008958', 'https://openalex.org/W3135990229', 'https://openalex.org/W4297808394', 'https://openalex.org/W3206252155', 'https://openalex.org/W3209984917', 'https://openalex.org/W2979476256']",2022-04-27
https://openalex.org/W4309651822,https://doi.org/10.1145/3557915.3561026,Leveraging language foundation models for human mobility forecasting,"In this paper, we propose a novel pipeline that leverages language foundation models for temporal sequential pattern mining, such as for human mobility forecasting tasks. For example, in the task of predicting Place-of-Interest (POI) customer flows, typically the number of visits is extracted from historical logs, and only the numerical data are used to predict visitor flows. In this research, we perform the forecasting task directly on the natural language input that includes all kinds of information such as numerical values and contextual semantic information. Specific prompts are introduced to transform numerical temporal sequences into sentences so that existing language models can be directly applied. We design an AuxMobLCast pipeline for predicting the number of visitors in each POI, integrating an auxiliary POI category classification task with the encoder-decoder architecture. This research provides empirical evidence of the effectiveness of the proposed AuxMobLCast pipeline to discover sequential patterns in mobility forecasting tasks. The results, evaluated on three real-world datasets, demonstrate that pre-trained language foundation models also have good performance in forecasting temporal sequences. This study could provide visionary insights and lead to new research directions for predicting human mobility.","['https://openalex.org/W3015356564', 'https://openalex.org/W3105643199', 'https://openalex.org/W2788114581', 'https://openalex.org/W2954731415', 'https://openalex.org/W2539781657', 'https://openalex.org/W2804713279', 'https://openalex.org/W3012735076', 'https://openalex.org/W3177318507', 'https://openalex.org/W2962817854']",2022-11-01
https://openalex.org/W4312951904,https://doi.org/10.1109/access.2022.3225198,Evaluating Self-Supervised Speech Representations for Speech Emotion Recognition,"Self-supervised learning has recently been implemented widely in speech processing areas, replacing conventional acoustic feature extraction to extract meaningful information from speech. One of the challenging applications of speech processing is to extract affective information from speech, commonly called speech emotion recognition. Until now, it is not clear the position of these speech representations compared to the classical acoustic feature. This paper evaluates nineteen self-supervised speech representations and one classical acoustic feature for five distinct speech emotion recognition datasets on the same classifier. We calculate the effect size among twenty speech representations to show the magnitude of relative differences from the top to the lowest performance. The top three are WavLM Large, UniSpeech-SAT Large, and HuBERT Large, with negligible effect sizes among them. The significance test supports the difference among self-supervised speech representations. The best prediction for each dataset is shown in the form of a confusion matrix to gain insights into the best performance of speech representations for each emotion category based on the training data from balanced vs. unbalanced datasets, English vs. Japanese corpus, and five vs. six emotion categories. Despite showing their competitiveness, this exploration of self-supervised learning for speech emotion recognition also shows their limitations on models pre-trained on small data and trained on unbalanced datasets.","['https://openalex.org/W2648194195', 'https://openalex.org/W3198528147', 'https://openalex.org/W4221089191', 'https://openalex.org/W3203140070', 'https://openalex.org/W3097488938', 'https://openalex.org/W3015707499', 'https://openalex.org/W6785591002', 'https://openalex.org/W3166353905', 'https://openalex.org/W2526050071', 'https://openalex.org/W2146334809', 'https://openalex.org/W2342475039', 'https://openalex.org/W2742542661', 'https://openalex.org/W2883409523', 'https://openalex.org/W2611652970', 'https://openalex.org/W2971275122', 'https://openalex.org/W4212956106', 'https://openalex.org/W2037557484', 'https://openalex.org/W3197580070', 'https://openalex.org/W3041561163', 'https://openalex.org/W6799344719', 'https://openalex.org/W1565746575', 'https://openalex.org/W3006926732', 'https://openalex.org/W3197994565', 'https://openalex.org/W3196876847', 'https://openalex.org/W3204696009', 'https://openalex.org/W3122349645', 'https://openalex.org/W3015356564', 'https://openalex.org/W3209984917', 'https://openalex.org/W3206252155', 'https://openalex.org/W3097286738', 'https://openalex.org/W2972943112', 'https://openalex.org/W2982223350', 'https://openalex.org/W3198858531', 'https://openalex.org/W3101133634', 'https://openalex.org/W4307482387']",2022-01-01
https://openalex.org/W3203147359,https://doi.org/10.3390/app11198872,Improving Aphasic Speech Recognition by Using Novel Semi-Supervised Learning Methods on AphasiaBank for English and Spanish,"Automatic speech recognition in patients with aphasia is a challenging task for which studies have been published in a few languages. Reasonably, the systems reported in the literature within this field show significantly lower performance than those focused on transcribing non-pathological clean speech. It is mainly due to the difficulty of recognizing a more unintelligible voice, as well as due to the scarcity of annotated aphasic data. This work is mainly focused on applying novel semi-supervised learning methods to the AphasiaBank dataset in order to deal with these two major issues, reporting improvements for the English language and providing the first benchmark for the Spanish language for which less than one hour of transcribed aphasic speech was used for training. In addition, the influence of reinforcing the training and decoding processes with out-of-domain acoustic and text data is described by using different strategies and configurations to fine-tune the hyperparameters and the final recognition systems. The interesting results obtained encourage extending this technological approach to other languages and scenarios where the scarcity of annotated data to train recognition models is a challenging reality.","['https://openalex.org/W2142499410', 'https://openalex.org/W2276983060', 'https://openalex.org/W2119783969', 'https://openalex.org/W6673351960', 'https://openalex.org/W3011512709', 'https://openalex.org/W2057579359', 'https://openalex.org/W1988176774', 'https://openalex.org/W4239848939', 'https://openalex.org/W2790441751', 'https://openalex.org/W2132344070', 'https://openalex.org/W1965248225', 'https://openalex.org/W2154489474', 'https://openalex.org/W1894429194', 'https://openalex.org/W2323425210', 'https://openalex.org/W2916935581', 'https://openalex.org/W2996492109', 'https://openalex.org/W2593898616', 'https://openalex.org/W2796439139', 'https://openalex.org/W1985579076', 'https://openalex.org/W2502847729', 'https://openalex.org/W2756314874', 'https://openalex.org/W2961302496', 'https://openalex.org/W2964539095', 'https://openalex.org/W2936774411', 'https://openalex.org/W3026041220', 'https://openalex.org/W3042475105', 'https://openalex.org/W6780458824', 'https://openalex.org/W2912822460', 'https://openalex.org/W2165859881', 'https://openalex.org/W3127557305', 'https://openalex.org/W2508777706', 'https://openalex.org/W3095387535', 'https://openalex.org/W2990699120', 'https://openalex.org/W1999318234', 'https://openalex.org/W6601908271', 'https://openalex.org/W3198429080', 'https://openalex.org/W3015356564', 'https://openalex.org/W2127141656', 'https://openalex.org/W2292087804', 'https://openalex.org/W3095410713', 'https://openalex.org/W1494198834', 'https://openalex.org/W2134800885', 'https://openalex.org/W3040479486', 'https://openalex.org/W46948191', 'https://openalex.org/W3138613653', 'https://openalex.org/W2092876913']",2021-09-24
https://openalex.org/W4319862480,https://doi.org/10.1109/slt54892.2023.10022724,How Does Pre-Trained Wav2Vec 2.0 Perform on Domain-Shifted Asr? an Extensive Benchmark on Air Traffic Control Communications,"Recent work on self-supervised pre-training focus on leveraging large-scale unlabeled speech data to build robust end-to-end (E2E) acoustic models (AM) that can be later fine-tuned on downstream tasks e.g., automatic speech recognition (ASR). Yet, few works investigated the impact on performance when the data properties substantially differ between the pre-training and fine-tuning phases, termed domain shift. We target this scenario by analyzing the robustness of Wav2Vec 2.0 and XLS-R models on downstream ASR for a completely unseen domain, air traffic control (ATC) communications. We benchmark these two models on several open-source and challenging ATC databases with signal-to-noise ratio between 5 to 20 dB. Relative word error rate (WER) reductions between 20% to 40% are obtained in comparison to hybrid-based ASR baselines by only fine-tuning E2E acoustic models with a smaller fraction of labeled data. We analyze WERs on the low-resource scenario and gender bias carried by one ATC dataset.","['https://openalex.org/W2973049979', 'https://openalex.org/W4247726808', 'https://openalex.org/W6769196770', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209984917', 'https://openalex.org/W6810007534', 'https://openalex.org/W6795952400', 'https://openalex.org/W1494198834', 'https://openalex.org/W4286685063', 'https://openalex.org/W3196463186', 'https://openalex.org/W3198744226', 'https://openalex.org/W3198771897', 'https://openalex.org/W3206559778', 'https://openalex.org/W4297841287', 'https://openalex.org/W6802818367', 'https://openalex.org/W2563053575', 'https://openalex.org/W6806547490', 'https://openalex.org/W3096806943', 'https://openalex.org/W3109258989', 'https://openalex.org/W6800440395', 'https://openalex.org/W4319862470', 'https://openalex.org/W2147590749', 'https://openalex.org/W2726599793', 'https://openalex.org/W2903601559', 'https://openalex.org/W3133953372', 'https://openalex.org/W1540624770', 'https://openalex.org/W3196895794', 'https://openalex.org/W6800888048', 'https://openalex.org/W4225294518', 'https://openalex.org/W2514741789', 'https://openalex.org/W2127141656', 'https://openalex.org/W3160641957', 'https://openalex.org/W3015356564', 'https://openalex.org/W4221140371', 'https://openalex.org/W3102342027', 'https://openalex.org/W3198484663', 'https://openalex.org/W2916984808', 'https://openalex.org/W6601969362', 'https://openalex.org/W6631362777', 'https://openalex.org/W2995181338', 'https://openalex.org/W2979826702', 'https://openalex.org/W3197876970', 'https://openalex.org/W6674330103', 'https://openalex.org/W6755977528', 'https://openalex.org/W6757817989', 'https://openalex.org/W2936774411', 'https://openalex.org/W3007073761', 'https://openalex.org/W3163203022', 'https://openalex.org/W4221147513', 'https://openalex.org/W4221145109', 'https://openalex.org/W2979476256', 'https://openalex.org/W3197608144', 'https://openalex.org/W4286902103', 'https://openalex.org/W3197201292', 'https://openalex.org/W49079112', 'https://openalex.org/W2095705004', 'https://openalex.org/W2899663614', 'https://openalex.org/W4287173589', 'https://openalex.org/W3036601975']",2023-01-09
https://openalex.org/W3162133897,https://doi.org/10.1109/icassp39728.2021.9414321,Similarity Analysis of Self-Supervised Speech Representations,"Self-supervised speech representation learning has recently been a prosperous research topic. Many algorithms have been proposed for learning useful representations from large-scale unlabeled data, and their applications to a wide range of speech tasks have also been investigated. However, there has been little research focusing on understanding the properties of existing approaches. In this work, we aim to provide a comparative study of some of the most representative self-supervised algorithms. Specifically, we quantify the similarities between different self-supervised representations using existing similarity measures. We also design probing tasks to study the correlation between the models' pre-training loss and the amount of specific speech information contained in their learned representations. In addition to showing how various self-supervised models behave differently given the same input, our study also finds that the training objective has a higher impact on representation similarity than architectural choices such as building blocks (RNN/Transformer/CNN) and directionality (uni/bidirectional). Our results also suggest that there exists a strong correlation between pre-training loss and downstream performance for some self-supervised algorithms.","['https://openalex.org/W6771812881', 'https://openalex.org/W1494198834', 'https://openalex.org/W3035202887', 'https://openalex.org/W6772641181', 'https://openalex.org/W3016181583', 'https://openalex.org/W2973049979', 'https://openalex.org/W2962739339', 'https://openalex.org/W6682948231', 'https://openalex.org/W6780361010', 'https://openalex.org/W2936774411', 'https://openalex.org/W3015356564', 'https://openalex.org/W3016011332', 'https://openalex.org/W3015265920', 'https://openalex.org/W6777859476', 'https://openalex.org/W3096485810', 'https://openalex.org/W3049256661', 'https://openalex.org/W3097787369', 'https://openalex.org/W3097053302', 'https://openalex.org/W3044483536', 'https://openalex.org/W3096656254', 'https://openalex.org/W6743489131', 'https://openalex.org/W2842511635', 'https://openalex.org/W6777372923', 'https://openalex.org/W6780218876', 'https://openalex.org/W2982223350', 'https://openalex.org/W2748318213', 'https://openalex.org/W2972943112', 'https://openalex.org/W6769238691', 'https://openalex.org/W3003875258', 'https://openalex.org/W2973157397', 'https://openalex.org/W6774314701', 'https://openalex.org/W3097286738', 'https://openalex.org/W6755641643', 'https://openalex.org/W3040997499', 'https://openalex.org/W6761472960', 'https://openalex.org/W2160654481', 'https://openalex.org/W6631216910', 'https://openalex.org/W6745682157', 'https://openalex.org/W3036601975', 'https://openalex.org/W3160345865', 'https://openalex.org/W2998649947', 'https://openalex.org/W1523385540', 'https://openalex.org/W2981991061', 'https://openalex.org/W2767204723', 'https://openalex.org/W2963341956', 'https://openalex.org/W4297808394', 'https://openalex.org/W2995181338', 'https://openalex.org/W3041561163', 'https://openalex.org/W2752168051', 'https://openalex.org/W3005680577', 'https://openalex.org/W3034487470', 'https://openalex.org/W3099782249', 'https://openalex.org/W2963503967', 'https://openalex.org/W2942810103', 'https://openalex.org/W3026957705', 'https://openalex.org/W2962813140', 'https://openalex.org/W4297730150', 'https://openalex.org/W2152790380']",2021-05-13
https://openalex.org/W3173081491,https://doi.org/10.1109/taslp.2021.3133189,Improving the Adversarial Robustness for Speaker Verification by Self-Supervised Learning,"Previous works have shown that automatic speaker verification (ASV) is seriously vulnerable to malicious spoofing attacks, such as replay, synthetic speech, and recently emerged adversarial attacks. Great efforts have been dedicated to defending ASV against replay and synthetic speech; however, only a few approaches have been explored to deal with adversarial attacks. All the existing approaches to tackle adversarial attacks for ASV require the knowledge for adversarial samples generation, but it is impractical for defenders to know the exact attack algorithms that are applied by the in-the-wild attackers. This work is among the first to perform adversarial defense for ASV without knowing the specific attack algorithms. Inspired by self-supervised learning models (SSLMs) that possess the merits of alleviating the superficial noise in the inputs and reconstructing clean samples from the interrupted ones, this work regards adversarial perturbations as one kind of noise and conducts adversarial defense for ASV by SSLMs. Specifically, we propose to perform adversarial defense from two perspectives: 1) adversarial perturbation purification and 2) adversarial perturbation detection. The purification module aims at alleviating the adversarial perturbations in the samples and pulling the contaminated adversarial inputs back towards the decision boundary. Experimental results show that our proposed purification module effectively counters adversarial attacks and outperforms traditional filters from both alleviating the adversarial noise and maintaining the performance of genuine samples. The detection module aims at detecting adversarial samples from genuine ones based on the statistical properties of ASV scores derived by a unique ASV integrating with different number of SSLMs. Experimental results show that our detection module helps shield the ASV by detecting adversarial samples. Both purification and detection methods are helpful for defending against different kinds of attack algorithms. Moreover, since there is no common metric for evaluating the ASV performance under adversarial attacks, this work also formalizes evaluation metrics for adversarial defense considering both purification and detection based approaches into account. We sincerely encourage future works to benchmark their approaches based on the proposed evaluation framework.","['https://openalex.org/W3039832962', 'https://openalex.org/W2150769028', 'https://openalex.org/W6713414053', 'https://openalex.org/W2121812409', 'https://openalex.org/W2395750323', 'https://openalex.org/W2039057510', 'https://openalex.org/W2046056978', 'https://openalex.org/W2748488820', 'https://openalex.org/W2890964092', 'https://openalex.org/W3015899421', 'https://openalex.org/W2584329820', 'https://openalex.org/W2114925438', 'https://openalex.org/W2587150483', 'https://openalex.org/W6639499869', 'https://openalex.org/W2123299109', 'https://openalex.org/W1980581462', 'https://openalex.org/W15681643', 'https://openalex.org/W748732769', 'https://openalex.org/W2128466129', 'https://openalex.org/W3095259706', 'https://openalex.org/W2962747881', 'https://openalex.org/W6765397690', 'https://openalex.org/W2745896134', 'https://openalex.org/W2176804518', 'https://openalex.org/W3034598492', 'https://openalex.org/W2972811785', 'https://openalex.org/W3161011913', 'https://openalex.org/W3197134965', 'https://openalex.org/W6637162671', 'https://openalex.org/W2964301649', 'https://openalex.org/W6640090968', 'https://openalex.org/W6748288002', 'https://openalex.org/W2963058500', 'https://openalex.org/W2898435086', 'https://openalex.org/W2973252307', 'https://openalex.org/W6760326341', 'https://openalex.org/W6746295503', 'https://openalex.org/W6746882984', 'https://openalex.org/W3006808893', 'https://openalex.org/W3096171739', 'https://openalex.org/W3096023981', 'https://openalex.org/W3016138785', 'https://openalex.org/W6799852253', 'https://openalex.org/W2962904371', 'https://openalex.org/W3143465836', 'https://openalex.org/W2972678295', 'https://openalex.org/W3160548127', 'https://openalex.org/W3015811740', 'https://openalex.org/W3015958938', 'https://openalex.org/W2972532204', 'https://openalex.org/W3007679772', 'https://openalex.org/W3153453329', 'https://openalex.org/W6745795967', 'https://openalex.org/W3095570773', 'https://openalex.org/W3096614974', 'https://openalex.org/W2190237714', 'https://openalex.org/W2963242190', 'https://openalex.org/W2972908178', 'https://openalex.org/W3097794099', 'https://openalex.org/W3095498208', 'https://openalex.org/W2726515241', 'https://openalex.org/W2752782242', 'https://openalex.org/W6640425456', 'https://openalex.org/W3162096084', 'https://openalex.org/W2963422555', 'https://openalex.org/W3024869864', 'https://openalex.org/W6769178842', 'https://openalex.org/W3142516134', 'https://openalex.org/W2747238065', 'https://openalex.org/W2972885011', 'https://openalex.org/W6714259624', 'https://openalex.org/W2064364374', 'https://openalex.org/W2963542245', 'https://openalex.org/W2180612164', 'https://openalex.org/W6844194202', 'https://openalex.org/W3016181583', 'https://openalex.org/W3102342027', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W3015356564', 'https://openalex.org/W2972943112', 'https://openalex.org/W3016011332', 'https://openalex.org/W179875071', 'https://openalex.org/W3041561163', 'https://openalex.org/W2982223350', 'https://openalex.org/W3148040514', 'https://openalex.org/W6755207826', 'https://openalex.org/W6766673545', 'https://openalex.org/W6768021236', 'https://openalex.org/W6739901393', 'https://openalex.org/W3034190247', 'https://openalex.org/W3035106315', 'https://openalex.org/W3107328291', 'https://openalex.org/W2936774411', 'https://openalex.org/W3010925296', 'https://openalex.org/W2808631503', 'https://openalex.org/W6631190155', 'https://openalex.org/W2963420686', 'https://openalex.org/W1873522754', 'https://openalex.org/W3024195404', 'https://openalex.org/W2407374891', 'https://openalex.org/W2964153729', 'https://openalex.org/W2962717526', 'https://openalex.org/W1524333225', 'https://openalex.org/W3005511757', 'https://openalex.org/W2955054437', 'https://openalex.org/W2981461916', 'https://openalex.org/W2965373594', 'https://openalex.org/W2988736778', 'https://openalex.org/W3033893956', 'https://openalex.org/W2963389226', 'https://openalex.org/W2770312844', 'https://openalex.org/W2964121744', 'https://openalex.org/W2996383576', 'https://openalex.org/W2963207607', 'https://openalex.org/W2923292931', 'https://openalex.org/W2985489290', 'https://openalex.org/W2842511635', 'https://openalex.org/W2782403400', 'https://openalex.org/W3035501253', 'https://openalex.org/W2980486495', 'https://openalex.org/W3025035610', 'https://openalex.org/W2404874347', 'https://openalex.org/W3091838242', 'https://openalex.org/W2975059944', 'https://openalex.org/W4288091954', 'https://openalex.org/W2896457183', 'https://openalex.org/W4287063346', 'https://openalex.org/W2767951891', 'https://openalex.org/W3016518327', 'https://openalex.org/W1922655562', 'https://openalex.org/W3131975723', 'https://openalex.org/W3026494492', 'https://openalex.org/W2626778328']",2021-12-09
https://openalex.org/W4387620292,https://doi.org/10.1002/brx2.29,Understanding the brain with attention: A survey of transformers in brain sciences,"Abstract Owing to their superior capabilities and advanced achievements, Transformers have gradually attracted attention with regard to understanding complex brain processing mechanisms. This study aims to comprehensively review and discuss the applications of Transformers in brain sciences. First, we present a brief introduction of the critical architecture of Transformers. Then, we overview and analyze their most relevant applications in brain sciences, including brain disease diagnosis, brain age prediction, brain anomaly detection, semantic segmentation, multi‐modal registration, functional Magnetic Resonance Imaging (fMRI) modeling, Electroencephalogram (EEG) processing, and multi‐task collaboration. We organize the model details and open sources for reference and replication. In addition, we discuss the quantitative assessments, model complexity, and optimization of Transformers, which are topics of great concern in the field. Finally, we explore possible future challenges and opportunities, exploiting some concrete and recent cases to provoke discussion and innovation. We hope that this review will stimulate interest in further research on Transformers in the context of brain sciences.","['https://openalex.org/W2163302580', 'https://openalex.org/W1975389666', 'https://openalex.org/W1979843512', 'https://openalex.org/W4313622139', 'https://openalex.org/W2997569720', 'https://openalex.org/W2794480229', 'https://openalex.org/W3136216669', 'https://openalex.org/W4306955484', 'https://openalex.org/W4206706211', 'https://openalex.org/W2896457183', 'https://openalex.org/W3096609285', 'https://openalex.org/W4212875960', 'https://openalex.org/W4323357155', 'https://openalex.org/W4362660511', 'https://openalex.org/W4213203281', 'https://openalex.org/W4384155732', 'https://openalex.org/W4323923206', 'https://openalex.org/W2194775991', 'https://openalex.org/W1903029394', 'https://openalex.org/W3168997536', 'https://openalex.org/W2601369343', 'https://openalex.org/W2795455408', 'https://openalex.org/W3133458480', 'https://openalex.org/W4312560592', 'https://openalex.org/W4327550249', 'https://openalex.org/W3119866685', 'https://openalex.org/W4312795296', 'https://openalex.org/W4362603432', 'https://openalex.org/W3203841574', 'https://openalex.org/W2963684088', 'https://openalex.org/W3128465392', 'https://openalex.org/W2799900537', 'https://openalex.org/W947140380', 'https://openalex.org/W4214520160', 'https://openalex.org/W3137120824', 'https://openalex.org/W4327972770', 'https://openalex.org/W4283382342', 'https://openalex.org/W3138516171', 'https://openalex.org/W2131774270', 'https://openalex.org/W3037983807', 'https://openalex.org/W3097030750', 'https://openalex.org/W3209632425', 'https://openalex.org/W3139049060', 'https://openalex.org/W2157331557', 'https://openalex.org/W1924770834', 'https://openalex.org/W2064675550', 'https://openalex.org/W3120434070', 'https://openalex.org/W2989473642', 'https://openalex.org/W3126466764', 'https://openalex.org/W2093004466', 'https://openalex.org/W4224304145', 'https://openalex.org/W2589870402', 'https://openalex.org/W2952378269', 'https://openalex.org/W4323315257', 'https://openalex.org/W4377102779', 'https://openalex.org/W4323663706', 'https://openalex.org/W4303628775', 'https://openalex.org/W6908809', 'https://openalex.org/W4312204014', 'https://openalex.org/W3165742354', 'https://openalex.org/W4324141359', 'https://openalex.org/W4324285151', 'https://openalex.org/W4320712817', 'https://openalex.org/W3203926658', 'https://openalex.org/W4386352534', 'https://openalex.org/W2990581109', 'https://openalex.org/W4311442013', 'https://openalex.org/W3028223987', 'https://openalex.org/W3110391382', 'https://openalex.org/W2892120842', 'https://openalex.org/W2978647781', 'https://openalex.org/W2530977706', 'https://openalex.org/W2961560364', 'https://openalex.org/W3195419882', 'https://openalex.org/W3198401109', 'https://openalex.org/W4309094718', 'https://openalex.org/W4313413262', 'https://openalex.org/W2752782242', 'https://openalex.org/W4324297125', 'https://openalex.org/W3088782011', 'https://openalex.org/W2751741310', 'https://openalex.org/W4283364477', 'https://openalex.org/W3046154154', 'https://openalex.org/W4309793872', 'https://openalex.org/W3034314048', 'https://openalex.org/W3034648032', 'https://openalex.org/W3169077988', 'https://openalex.org/W3147184966', 'https://openalex.org/W3160366495', 'https://openalex.org/W4200597473', 'https://openalex.org/W4311252280', 'https://openalex.org/W4225370003', 'https://openalex.org/W3132455321', 'https://openalex.org/W2028158228', 'https://openalex.org/W2063552084', 'https://openalex.org/W2963840672', 'https://openalex.org/W1901129140', 'https://openalex.org/W2464708700', 'https://openalex.org/W4221163766', 'https://openalex.org/W4306410378', 'https://openalex.org/W2097117768', 'https://openalex.org/W4224941077', 'https://openalex.org/W3202941780', 'https://openalex.org/W3202553706', 'https://openalex.org/W4283080861', 'https://openalex.org/W4295940432', 'https://openalex.org/W4367665788', 'https://openalex.org/W2884436604', 'https://openalex.org/W4296123084', 'https://openalex.org/W4295938041', 'https://openalex.org/W4322707229', 'https://openalex.org/W4293659888', 'https://openalex.org/W4307726656', 'https://openalex.org/W4367623618', 'https://openalex.org/W2133665775', 'https://openalex.org/W4368356857', 'https://openalex.org/W2300744495', 'https://openalex.org/W2915935453', 'https://openalex.org/W4283271083', 'https://openalex.org/W4283399238', 'https://openalex.org/W4364359870', 'https://openalex.org/W2467946152', 'https://openalex.org/W4309544075', 'https://openalex.org/W4287225552', 'https://openalex.org/W3202642596', 'https://openalex.org/W4386160755', 'https://openalex.org/W4310467538', 'https://openalex.org/W3164617602', 'https://openalex.org/W2978005227', 'https://openalex.org/W2136145485', 'https://openalex.org/W2752785527', 'https://openalex.org/W2604920239', 'https://openalex.org/W2891631795', 'https://openalex.org/W2922479016', 'https://openalex.org/W2964209782', 'https://openalex.org/W2630837129', 'https://openalex.org/W4226497331', 'https://openalex.org/W3156621598', 'https://openalex.org/W2980223643', 'https://openalex.org/W4283727542', 'https://openalex.org/W2964015378', 'https://openalex.org/W4225157420', 'https://openalex.org/W3199376581', 'https://openalex.org/W4296056803', 'https://openalex.org/W2950023118', 'https://openalex.org/W2618995589', 'https://openalex.org/W4298005750', 'https://openalex.org/W4309631262', 'https://openalex.org/W4290833709', 'https://openalex.org/W4376866854', 'https://openalex.org/W4291801260', 'https://openalex.org/W4224285717', 'https://openalex.org/W4295795206', 'https://openalex.org/W4288368497', 'https://openalex.org/W4372270328', 'https://openalex.org/W4319317749', 'https://openalex.org/W2783641782', 'https://openalex.org/W2509382533', 'https://openalex.org/W4306756967', 'https://openalex.org/W4360871668', 'https://openalex.org/W4289538860', 'https://openalex.org/W3173912422', 'https://openalex.org/W3107800993', 'https://openalex.org/W4316171088', 'https://openalex.org/W4205558134', 'https://openalex.org/W3177342940', 'https://openalex.org/W4205466227', 'https://openalex.org/W4221147059', 'https://openalex.org/W4312892255', 'https://openalex.org/W2345112827', 'https://openalex.org/W2095705004', 'https://openalex.org/W3154396742', 'https://openalex.org/W2611482981', 'https://openalex.org/W2010371409', 'https://openalex.org/W3015356564', 'https://openalex.org/W3157952885', 'https://openalex.org/W4319662928', 'https://openalex.org/W4321018175', 'https://openalex.org/W4362679702', 'https://openalex.org/W4367000547', 'https://openalex.org/W3190908553', 'https://openalex.org/W4362581540', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963351448', 'https://openalex.org/W3136424010', 'https://openalex.org/W3185341429', 'https://openalex.org/W3173777717', 'https://openalex.org/W4292434331', 'https://openalex.org/W4385246848', 'https://openalex.org/W4313156423', 'https://openalex.org/W3015468748', 'https://openalex.org/W4380551362', 'https://openalex.org/W4376864677', 'https://openalex.org/W3129490616', 'https://openalex.org/W2972785004', 'https://openalex.org/W4212774754', 'https://openalex.org/W4224094774', 'https://openalex.org/W2736213625', 'https://openalex.org/W4205570975', 'https://openalex.org/W2136704614', 'https://openalex.org/W2612690371', 'https://openalex.org/W4284963222', 'https://openalex.org/W4292731283', 'https://openalex.org/W3137989279', 'https://openalex.org/W4312446817', 'https://openalex.org/W3134405878', 'https://openalex.org/W4210786716', 'https://openalex.org/W4385486090', 'https://openalex.org/W2982007782', 'https://openalex.org/W2953683903', 'https://openalex.org/W3209859545', 'https://openalex.org/W4283022110', 'https://openalex.org/W2019251278', 'https://openalex.org/W4287391717', 'https://openalex.org/W3094771832', 'https://openalex.org/W2162809807', 'https://openalex.org/W3016391357', 'https://openalex.org/W4206131136', 'https://openalex.org/W4362457803', 'https://openalex.org/W2913668833', 'https://openalex.org/W2559655401']",2023-09-01
https://openalex.org/W4377289660,https://doi.org/10.3390/aerospace10050490,A Virtual Simulation-Pilot Agent for Training of Air Traffic Controllers,"In this paper we propose a novel virtual simulation-pilot engine for speeding up air traffic controller (ATCo) training by integrating different state-of-the-art artificial intelligence (AI)-based tools. The virtual simulation-pilot engine receives spoken communications from ATCo trainees, and it performs automatic speech recognition and understanding. Thus, it goes beyond only transcribing the communication and can also understand its meaning. The output is subsequently sent to a response generator system, which resembles the spoken read-back that pilots give to the ATCo trainees. The overall pipeline is composed of the following submodules: (i) an automatic speech recognition (ASR) system that transforms audio into a sequence of words; (ii) a high-level air traffic control (ATC)-related entity parser that understands the transcribed voice communication; and (iii) a text-to-speech submodule that generates a spoken utterance that resembles a pilot based on the situation of the dialogue. Our system employs state-of-the-art AI-based tools such as Wav2Vec 2.0, Conformer, BERT and Tacotron models. To the best of our knowledge, this is the first work fully based on open-source ATC resources and AI tools. In addition, we develop a robust and modular system with optional submodules that can enhance the system’s performance by incorporating real-time surveillance data, metadata related to exercises (such as sectors or runways), or even a deliberate read-back error to train ATCo trainees to identify them. Our ASR system can reach as low as 5.5% and 15.9% absolute word error rates (WER) on high- and low-quality ATC audio. We also demonstrate that adding surveillance data into the ASR can yield a callsign detection accuracy of more than 96%.","['https://openalex.org/W2912581782', 'https://openalex.org/W3019166713', 'https://openalex.org/W2964236337', 'https://openalex.org/W2972584841', 'https://openalex.org/W2163164853', 'https://openalex.org/W1897674656', 'https://openalex.org/W2563053575', 'https://openalex.org/W4225294518', 'https://openalex.org/W3196895794', 'https://openalex.org/W4319862470', 'https://openalex.org/W3129824274', 'https://openalex.org/W3109258989', 'https://openalex.org/W2904620939', 'https://openalex.org/W3212070308', 'https://openalex.org/W4308724730', 'https://openalex.org/W3133953372', 'https://openalex.org/W3198853881', 'https://openalex.org/W4320478778', 'https://openalex.org/W3198429080', 'https://openalex.org/W6778823374', 'https://openalex.org/W2916984808', 'https://openalex.org/W2734721103', 'https://openalex.org/W2779312438', 'https://openalex.org/W2782465018', 'https://openalex.org/W2765441328', 'https://openalex.org/W4383612876', 'https://openalex.org/W3010760586', 'https://openalex.org/W4207024570', 'https://openalex.org/W6739901393', 'https://openalex.org/W2046932483', 'https://openalex.org/W2131342762', 'https://openalex.org/W811578723', 'https://openalex.org/W2888867175', 'https://openalex.org/W3096806943', 'https://openalex.org/W2726599793', 'https://openalex.org/W3198744226', 'https://openalex.org/W6806547490', 'https://openalex.org/W4308092402', 'https://openalex.org/W6631362777', 'https://openalex.org/W2102113734', 'https://openalex.org/W6623517193', 'https://openalex.org/W2973049979', 'https://openalex.org/W3015356564', 'https://openalex.org/W4286685063', 'https://openalex.org/W3209984917', 'https://openalex.org/W4319862480', 'https://openalex.org/W2857028992', 'https://openalex.org/W2068882115', 'https://openalex.org/W6683738474', 'https://openalex.org/W3033187248', 'https://openalex.org/W2066452495', 'https://openalex.org/W2082728976', 'https://openalex.org/W2111284386', 'https://openalex.org/W2963609956', 'https://openalex.org/W2964243274', 'https://openalex.org/W4308840623', 'https://openalex.org/W3198213150', 'https://openalex.org/W3197055704', 'https://openalex.org/W2917987043', 'https://openalex.org/W3024085360', 'https://openalex.org/W3025260599', 'https://openalex.org/W2403186097', 'https://openalex.org/W7033595006', 'https://openalex.org/W6686338792', 'https://openalex.org/W3097777922', 'https://openalex.org/W6632100814', 'https://openalex.org/W3160641957', 'https://openalex.org/W1494198834', 'https://openalex.org/W6674330103', 'https://openalex.org/W6631190155', 'https://openalex.org/W2972818416', 'https://openalex.org/W2144499799', 'https://openalex.org/W2903250132', 'https://openalex.org/W2979826702', 'https://openalex.org/W3197876970', 'https://openalex.org/W1815076433', 'https://openalex.org/W6757817989', 'https://openalex.org/W1538131130', 'https://openalex.org/W2095705004', 'https://openalex.org/W2952230511', 'https://openalex.org/W3007328579', 'https://openalex.org/W4206254204']",2023-05-22
https://openalex.org/W4388766108,https://doi.org/10.2478/msr-2023-0033,Evaluating the Performance of wav2vec Embedding for Parkinson's Disease Detection,"Abstract Speech is one of the most serious manifestations of Parkinson's disease (PD). Sophisticated language/speech models have already demonstrated impressive performance on a variety of tasks, including classification. By analysing large amounts of data from a given setting, these models can identify patterns that would be difficult for clinicians to detect. We focus on evaluating the performance of a large self-supervised speech representation model, wav2vec, for PD classification. Based on the computed wav2vec embedding for each available speech signal, we calculated two sets of 512 derived features, wav2vec-sum and wav2vec-mean. Unlike traditional signal processing methods, this approach can learn a suitable representation of the signal directly from the data without requiring manual or hand-crafted feature extraction. Using an ensemble random forest classifier, we evaluated the embedding-based features on three different healthy vs. PD datasets (participants rhythmically repeat syllables /pa/, Italian dataset and English dataset). The obtained results showed that the wav2vec signal representation was accurate, with a minimum area under the receiver operating characteristic curve (AUROC) of 0.77 for the /pa/ task and the best AUROC of 0.98 for the Italian speech classification. The findings highlight the potential of the generalisability of the wav2vec features and the performance of these features in the cross-database scenarios.","['https://openalex.org/W2131884118', 'https://openalex.org/W1965453486', 'https://openalex.org/W1974898145', 'https://openalex.org/W2142430469', 'https://openalex.org/W4365143687', 'https://openalex.org/W1431429584', 'https://openalex.org/W2073122936', 'https://openalex.org/W1966484475', 'https://openalex.org/W2764216414', 'https://openalex.org/W2094217267', 'https://openalex.org/W2767111132', 'https://openalex.org/W3181310845', 'https://openalex.org/W4206713267', 'https://openalex.org/W4205242090', 'https://openalex.org/W3196337184', 'https://openalex.org/W3155712419', 'https://openalex.org/W3204237524', 'https://openalex.org/W3008039831', 'https://openalex.org/W2946315310', 'https://openalex.org/W2946595724', 'https://openalex.org/W3004815729', 'https://openalex.org/W2978284105', 'https://openalex.org/W2973049979', 'https://openalex.org/W3016181583', 'https://openalex.org/W2967688728', 'https://openalex.org/W3015356564', 'https://openalex.org/W4319750153', 'https://openalex.org/W4295938936', 'https://openalex.org/W4297841875', 'https://openalex.org/W4382137576', 'https://openalex.org/W3026400514', 'https://openalex.org/W2933138175', 'https://openalex.org/W4297841455', 'https://openalex.org/W4388766108', 'https://openalex.org/W2980122484', 'https://openalex.org/W3213556736']",2023-11-17
https://openalex.org/W4392909068,https://doi.org/10.1109/icassp48485.2024.10447929,"Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study","Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. Our configurations and trained models are released in ESPnet to foster future research efforts.","['https://openalex.org/W2160815625', 'https://openalex.org/W2515753980', 'https://openalex.org/W2127141656', 'https://openalex.org/W2144499799', 'https://openalex.org/W6623517193', 'https://openalex.org/W4385245566', 'https://openalex.org/W3097777922', 'https://openalex.org/W3163793923', 'https://openalex.org/W4319862255', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W6847363464', 'https://openalex.org/W2398826216', 'https://openalex.org/W4385822683', 'https://openalex.org/W3015356564', 'https://openalex.org/W4385570101', 'https://openalex.org/W4381786045', 'https://openalex.org/W4375869259', 'https://openalex.org/W2962780374', 'https://openalex.org/W2752796333', 'https://openalex.org/W3215615641', 'https://openalex.org/W1494198834', 'https://openalex.org/W2166637769', 'https://openalex.org/W3198694222', 'https://openalex.org/W2799473636', 'https://openalex.org/W6755559483', 'https://openalex.org/W3198587774', 'https://openalex.org/W6898634591', 'https://openalex.org/W2963242190', 'https://openalex.org/W6771467084', 'https://openalex.org/W4385822439', 'https://openalex.org/W4391021675', 'https://openalex.org/W4386566728', 'https://openalex.org/W4385565440', 'https://openalex.org/W3100460087', 'https://openalex.org/W2899274165', 'https://openalex.org/W3024605872', 'https://openalex.org/W4313679638', 'https://openalex.org/W854541894', 'https://openalex.org/W2963799213', 'https://openalex.org/W3036601975', 'https://openalex.org/W4307323391', 'https://openalex.org/W4381827575', 'https://openalex.org/W4200635400', 'https://openalex.org/W4385970143', 'https://openalex.org/W3101648800']",2024-03-18
https://openalex.org/W3161663055,https://doi.org/10.1109/icassp39728.2021.9414896,The Role of Task and Acoustic Similarity in Audio Transfer Learning: Insights from the Speech Emotion Recognition Case,"With the rise of deep learning, deep knowledge transfer has emerged as one of the most effective techniques for getting state-of-the-art performance using deep neural networks. A lot of recent research has focused on understanding the mechanisms of transfer learning in the image and language domains. We perform a similar investigation for the case of speech emotion recognition (SER), and conclude that transfer learning for SER is influenced both by the choice of pre-training task and by the differences in acoustic conditions between the upstream and downstream data sets, with the former having a bigger impact. The effect of each factor is isolated by first transferring knowledge between different tasks on the same data, and then from the original data to corrupted versions of it but for the same task. We also demonstrate that layers closer to the input see more adaptation than ones closer to the output in both cases, a finding which explains why previous works often found it necessary to fine-tune all layers during transfer learning.","['https://openalex.org/W2163922914', 'https://openalex.org/W6755207826', 'https://openalex.org/W3023371261', 'https://openalex.org/W4255421341', 'https://openalex.org/W2980113592', 'https://openalex.org/W6782436372', 'https://openalex.org/W116068320', 'https://openalex.org/W2775019026', 'https://openalex.org/W3094550259', 'https://openalex.org/W3082319384', 'https://openalex.org/W6746941363', 'https://openalex.org/W3100270690', 'https://openalex.org/W2972943112', 'https://openalex.org/W2973157397', 'https://openalex.org/W3015356564', 'https://openalex.org/W2982223350', 'https://openalex.org/W6780218876', 'https://openalex.org/W3025664727', 'https://openalex.org/W2803098682', 'https://openalex.org/W2399733683', 'https://openalex.org/W2889467522', 'https://openalex.org/W2407346284', 'https://openalex.org/W2050752817', 'https://openalex.org/W2936451900', 'https://openalex.org/W3015707499', 'https://openalex.org/W2593116425', 'https://openalex.org/W6637373629', 'https://openalex.org/W2742542661', 'https://openalex.org/W2936173226', 'https://openalex.org/W2937977583', 'https://openalex.org/W2003653478', 'https://openalex.org/W4245744384', 'https://openalex.org/W2726515241', 'https://openalex.org/W2146334809', 'https://openalex.org/W6604254268', 'https://openalex.org/W2557283755', 'https://openalex.org/W1686810756', 'https://openalex.org/W3036601975', 'https://openalex.org/W2118789253', 'https://openalex.org/W2763323349', 'https://openalex.org/W2963194800', 'https://openalex.org/W2896457183', 'https://openalex.org/W3081410576', 'https://openalex.org/W2584401907']",2021-05-13
https://openalex.org/W4372346804,https://doi.org/10.1109/icassp49357.2023.10095656,Evaluating Parameter-Efficient Transfer Learning Approaches on SURE Benchmark for Speech Understanding,"Fine-tuning is widely used as the default algorithm for transfer learning from pre-trained models. Parameter inefficiency can however arise when, during transfer learning, all the parameters of a large pre-trained model need to be updated for individual downstream tasks. As the number of parameters grows, fine-tuning is prone to overfitting and catastrophic forgetting. In addition, full fine-tuning can become prohibitively expensive when the model is used for many tasks. To mitigate this issue, parameter-efficient transfer learning algorithms, such as adapters and prefix tuning, have been proposed as a way to introduce a few trainable parameters that can be plugged into large pre-trained language models such as BERT, HuBERT. In this paper, we introduce the Speech UndeRstanding Evaluation (SURE) benchmark for parameter-efficient learning for various speech processing tasks. Additionally, we introduce a new adapter, ConvAdapter, based on 1D convolution. We show that ConvAdapter outperforms the standard adapters while showing comparable performance against prefix tuning and Low-Rank Adaptation with only 0.94% of trainable parameters.","['https://openalex.org/W3209059054', 'https://openalex.org/W3161223924', 'https://openalex.org/W6759579507', 'https://openalex.org/W3197580070', 'https://openalex.org/W3015356564', 'https://openalex.org/W6780218876', 'https://openalex.org/W6738045163', 'https://openalex.org/W2887280559', 'https://openalex.org/W4226513777', 'https://openalex.org/W3097338456', 'https://openalex.org/W4226162428', 'https://openalex.org/W6810218461', 'https://openalex.org/W2963686995', 'https://openalex.org/W6743731764', 'https://openalex.org/W6838929754', 'https://openalex.org/W6790356757', 'https://openalex.org/W6728184133', 'https://openalex.org/W6777155081', 'https://openalex.org/W6750665317', 'https://openalex.org/W2971840980', 'https://openalex.org/W4225274946', 'https://openalex.org/W3173767661', 'https://openalex.org/W3168867926', 'https://openalex.org/W3174770825', 'https://openalex.org/W6810452837', 'https://openalex.org/W4205991051', 'https://openalex.org/W4375868763', 'https://openalex.org/W3153675281', 'https://openalex.org/W4224930323', 'https://openalex.org/W2963211188', 'https://openalex.org/W3036601975', 'https://openalex.org/W2964303773', 'https://openalex.org/W2797583228', 'https://openalex.org/W2752782242', 'https://openalex.org/W4394671563', 'https://openalex.org/W2531409750', 'https://openalex.org/W4319862635']",2023-05-05
https://openalex.org/W4396735698,https://doi.org/10.1007/s11063-024-11614-z,Multi-view Self-supervised Learning and Multi-scale Feature Fusion for Automatic Speech Recognition,"Abstract To address the challenges of the poor representation capability and low data utilization rate of end-to-end speech recognition models in deep learning, this study proposes an end-to-end speech recognition model based on multi-scale feature fusion and multi-view self-supervised learning (MM-ASR). It adopts a multi-task learning paradigm for training. The proposed method emphasizes the importance of inter-layer information within shared encoders, aiming to enhance the model’s characterization capability via the multi-scale feature fusion module. Moreover, we apply multi-view self-supervised learning to effectively exploit data information. Our approach is rigorously evaluated on the Aishell-1 dataset and further validated its effectiveness on the English corpus WSJ. The experimental results demonstrate a noteworthy 4.6 $$\%$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mo>%</mml:mo> </mml:math> reduction in character error rate, indicating significantly improved speech recognition performance . These findings showcase the effectiveness and potential of our proposed MM-ASR model for end-to-end speech recognition tasks.","['https://openalex.org/W2067132423', 'https://openalex.org/W2143612262', 'https://openalex.org/W2160815625', 'https://openalex.org/W2964539095', 'https://openalex.org/W3211278025', 'https://openalex.org/W2127141656', 'https://openalex.org/W4225299129', 'https://openalex.org/W4226212269', 'https://openalex.org/W2144499799', 'https://openalex.org/W2526425061', 'https://openalex.org/W2963414781', 'https://openalex.org/W6600106792', 'https://openalex.org/W3097777922', 'https://openalex.org/W3197478142', 'https://openalex.org/W4221167707', 'https://openalex.org/W2963242190', 'https://openalex.org/W2951974815', 'https://openalex.org/W2962850167', 'https://openalex.org/W3100270690', 'https://openalex.org/W3097286738', 'https://openalex.org/W2972943112', 'https://openalex.org/W3016011332', 'https://openalex.org/W3035202887', 'https://openalex.org/W3198858531', 'https://openalex.org/W3041561163', 'https://openalex.org/W2982223350', 'https://openalex.org/W3015265920', 'https://openalex.org/W2973049979', 'https://openalex.org/W3015356564', 'https://openalex.org/W3209059054', 'https://openalex.org/W2113325037', 'https://openalex.org/W1903029394', 'https://openalex.org/W3024869864', 'https://openalex.org/W3141035251', 'https://openalex.org/W4221154746', 'https://openalex.org/W2752782242', 'https://openalex.org/W2928165649', 'https://openalex.org/W4221154745', 'https://openalex.org/W2194775991', 'https://openalex.org/W2964110616', 'https://openalex.org/W6702248584', 'https://openalex.org/W4210690962', 'https://openalex.org/W2972818416', 'https://openalex.org/W3096396467', 'https://openalex.org/W2962780374', 'https://openalex.org/W4385823456', 'https://openalex.org/W4385822407', 'https://openalex.org/W4392910797', 'https://openalex.org/W2808939837', 'https://openalex.org/W3150425637', 'https://openalex.org/W3007328579']",2024-05-08
https://openalex.org/W4401886013,https://doi.org/10.3390/s24175520,Analyzing Wav2Vec 1.0 Embeddings for Cross-Database Parkinson’s Disease Detection and Speech Features Extraction,"Advancements in deep learning speech representations have facilitated the effective use of extensive unlabeled speech datasets for Parkinson’s disease (PD) modeling with minimal annotated data. This study employs the non-fine-tuned wav2vec 1.0 architecture to develop machine learning models for PD speech diagnosis tasks, such as cross-database classification and regression to predict demographic and articulation characteristics. The primary aim is to analyze overlapping components within the embeddings on both classification and regression tasks, investigating whether latent speech representations in PD are shared across models, particularly for related tasks. Firstly, evaluation using three multi-language PD datasets showed that wav2vec accurately detected PD based on speech, outperforming feature extraction using mel-frequency cepstral coefficients in the proposed cross-database classification scenarios. In cross-database scenarios using Italian and English-read texts, wav2vec demonstrated performance comparable to intra-dataset evaluations. We also compared our cross-database findings against those of other related studies. Secondly, wav2vec proved effective in regression, modeling various quantitative speech characteristics related to articulation and aging. Ultimately, subsequent analysis of important features examined the presence of significant overlaps between classification and regression models. The feature importance experiments discovered shared features across trained models, with increased sharing for related tasks, further suggesting that wav2vec contributes to improved generalizability. The study proposes wav2vec embeddings as a next promising step toward a speech-based universal model to assist in the evaluation of PD.","['https://openalex.org/W2908201961', 'https://openalex.org/W2951934944', 'https://openalex.org/W4220814249', 'https://openalex.org/W4378576117', 'https://openalex.org/W4323568500', 'https://openalex.org/W4319298024', 'https://openalex.org/W3026400514', 'https://openalex.org/W2973049979', 'https://openalex.org/W3015356564', 'https://openalex.org/W3197642003', 'https://openalex.org/W4372338196', 'https://openalex.org/W4387378260', 'https://openalex.org/W3198429080', 'https://openalex.org/W2153761521', 'https://openalex.org/W3203161143', 'https://openalex.org/W4211044101', 'https://openalex.org/W6682801700', 'https://openalex.org/W2066674357', 'https://openalex.org/W3196337184', 'https://openalex.org/W4396844657', 'https://openalex.org/W4388766108', 'https://openalex.org/W3207031444', 'https://openalex.org/W4366084182', 'https://openalex.org/W3016088911', 'https://openalex.org/W2951042025', 'https://openalex.org/W3194973925', 'https://openalex.org/W3094211490', 'https://openalex.org/W4206713267', 'https://openalex.org/W4304688131', 'https://openalex.org/W4372334073', 'https://openalex.org/W3211805594', 'https://openalex.org/W2746624190', 'https://openalex.org/W3153178673', 'https://openalex.org/W4212840609', 'https://openalex.org/W3037887574', 'https://openalex.org/W4385571422', 'https://openalex.org/W4382918145', 'https://openalex.org/W3044967013', 'https://openalex.org/W4385822952', 'https://openalex.org/W2890964092', 'https://openalex.org/W4296068413', 'https://openalex.org/W3209059054', 'https://openalex.org/W4387580450', 'https://openalex.org/W3015705854', 'https://openalex.org/W3039358396', 'https://openalex.org/W4382137576', 'https://openalex.org/W4387165835', 'https://openalex.org/W4395661434', 'https://openalex.org/W4391825007', 'https://openalex.org/W3170940710', 'https://openalex.org/W4385822692', 'https://openalex.org/W4390030622', 'https://openalex.org/W4295951577', 'https://openalex.org/W4387306788', 'https://openalex.org/W4210676598', 'https://openalex.org/W2767111132', 'https://openalex.org/W2191779130', 'https://openalex.org/W4388699437', 'https://openalex.org/W4372338343', 'https://openalex.org/W4393039024', 'https://openalex.org/W4392163942', 'https://openalex.org/W2806504660', 'https://openalex.org/W4225116051', 'https://openalex.org/W4386493078', 'https://openalex.org/W3016181583', 'https://openalex.org/W4386083664', 'https://openalex.org/W4385822779', 'https://openalex.org/W4366714310', 'https://openalex.org/W4385728310', 'https://openalex.org/W4297841844', 'https://openalex.org/W4225862430', 'https://openalex.org/W2153797245', 'https://openalex.org/W4293045515', 'https://openalex.org/W3096196861', 'https://openalex.org/W3130533174', 'https://openalex.org/W4207022729']",2024-08-26
https://openalex.org/W4297841455,https://doi.org/10.21437/interspeech.2022-10439,Exploring Capabilities of Monolingual Audio Transformers using Large\n Datasets in Automatic Speech Recognition of Czech,"In this paper, we present our progress in pretraining Czech monolingual audio\ntransformers from a large dataset containing more than 80 thousand hours of\nunlabeled speech, and subsequently fine-tuning the model on automatic speech\nrecognition tasks using a combination of in-domain data and almost 6 thousand\nhours of out-of-domain transcribed speech. We are presenting a large palette of\nexperiments with various fine-tuning setups evaluated on two public datasets\n(CommonVoice and VoxPopuli) and one extremely challenging dataset from the\nMALACH project. Our results show that monolingual Wav2Vec 2.0 models are robust\nASR systems, which can take advantage of large labeled and unlabeled datasets\nand successfully compete with state-of-the-art LVCSR systems. Moreover, Wav2Vec\nmodels proved to be good zero-shot learners when no training data are available\nfor the target ASR task.\n","['https://openalex.org/W4385245566', 'https://openalex.org/W2127923419', 'https://openalex.org/W3030437843', 'https://openalex.org/W3119308075', 'https://openalex.org/W2979826702', 'https://openalex.org/W3036601975', 'https://openalex.org/W2134800885', 'https://openalex.org/W3197652246', 'https://openalex.org/W2981910996', 'https://openalex.org/W3198429080', 'https://openalex.org/W3207583491', 'https://openalex.org/W3200506395', 'https://openalex.org/W3207988762', 'https://openalex.org/W3213029956', 'https://openalex.org/W3015356564']",2022-06-15
https://openalex.org/W4388068584,https://doi.org/10.21437/sigul.2023-9,The Applicability of Wav2Vec2 and Whisper for Low-Resource Maltese ASR,"Maltese is a low-resource language with limited digital tools, including automatic speech recognition. With very limited datasets of Maltese speech available, a recent project, MASRI, developed further speech datasets and produced an initial prototype trained using the Jasper architecture. The best system achieved 55.05% WER on the MASRI test set. Our work builds upon this, producing a further two-and-a half-hour annotated speech corpus from a domain in which no data was previously available (Parliament of Malta). Moreover, we experiment with existing pre-trained self-supervised models (Wav2Vec2.0 and Whisper) and further fine-tune these models on Maltese annotated data. A total of 30 Maltese ASR models are trained and evaluated using the WER and the CER. The results indicate that the performance of the models scales with the quantity of data, although not linearly. The best model achieves state-of-the-art results of 8.53% WER and 1.93% CER on a test set extracted from the CommonVoice project and 24.98% WER and 8.37% CER on the MASRI test set.","['https://openalex.org/W2120209245', 'https://openalex.org/W1993660824', 'https://openalex.org/W2939710050', 'https://openalex.org/W3037057938', 'https://openalex.org/W3213029956', 'https://openalex.org/W4311000453', 'https://openalex.org/W3031570736', 'https://openalex.org/W6761449031', 'https://openalex.org/W3213247366', 'https://openalex.org/W3036601975', 'https://openalex.org/W3015356564', 'https://openalex.org/W1494198834', 'https://openalex.org/W3119308075', 'https://openalex.org/W2995929068', 'https://openalex.org/W2991262856', 'https://openalex.org/W3198429080', 'https://openalex.org/W2973049979', 'https://openalex.org/W3030437843', 'https://openalex.org/W4287689669', 'https://openalex.org/W2973215447']",2023-08-18
https://openalex.org/W4372260307,https://doi.org/10.1109/icassp49357.2023.10096923,Token2vec: A Joint Self-Supervised Pre-Training Framework Using Unpaired Speech and Text,"Self-supervised pre-training has been successful in both text and speech processing. Speech and text offer different but complementary information. The question is whether we are able to perform a speech-text joint pre-training on unpaired speech and text. In this paper, we take the idea of self-supervised pre-training one step further and propose token2vec, a novel joint pre-training framework for unpaired speech and text based on discrete representations of speech. Specifically, we introduce two modality-specific tokenizers for speech and text. Based on these tokenizers, we convert speech/text sequences into discrete speech/text token sequences consisting of similar language units, thus mitigating the domain mismatch problem and length mismatch problem, which are caused by the distinct characteristics between speech and text. Finally, we feed the discrete speech and text tokens into a modality-agnostic Transformer encoder and pre-train with token-level masking language modeling (tMLM). Experiments show that token2vec is significantly superior to various speech-only pre-training baselines, with up to 17.7% relative WER reduction. Token2vec model is also validated on a non-ASR task, i.e., spoken intent classification, and shows good transferability.","['https://openalex.org/W6779271971', 'https://openalex.org/W6762392948', 'https://openalex.org/W6803092890', 'https://openalex.org/W6755207826', 'https://openalex.org/W6771812881', 'https://openalex.org/W6739901393', 'https://openalex.org/W6750615492', 'https://openalex.org/W6631190155', 'https://openalex.org/W3011411500', 'https://openalex.org/W2933138175', 'https://openalex.org/W3209059054', 'https://openalex.org/W6780218876', 'https://openalex.org/W3148001440', 'https://openalex.org/W3162534564', 'https://openalex.org/W6810259195', 'https://openalex.org/W4286359908', 'https://openalex.org/W6790356757', 'https://openalex.org/W6769196770', 'https://openalex.org/W3180374548', 'https://openalex.org/W6843330092', 'https://openalex.org/W4226120743', 'https://openalex.org/W3015356564', 'https://openalex.org/W1494198834', 'https://openalex.org/W6810062440', 'https://openalex.org/W6840472489', 'https://openalex.org/W6845338303', 'https://openalex.org/W6766673545', 'https://openalex.org/W6763701032', 'https://openalex.org/W3209984917', 'https://openalex.org/W6810007534', 'https://openalex.org/W3197580070', 'https://openalex.org/W4283324001', 'https://openalex.org/W2943552823', 'https://openalex.org/W2923014074', 'https://openalex.org/W3036601975', 'https://openalex.org/W2995181338', 'https://openalex.org/W4221145109', 'https://openalex.org/W4300980246', 'https://openalex.org/W3035579820', 'https://openalex.org/W4221155340', 'https://openalex.org/W4394671563', 'https://openalex.org/W4223646224', 'https://openalex.org/W2896457183', 'https://openalex.org/W2979476256', 'https://openalex.org/W1522301498', 'https://openalex.org/W3207222250', 'https://openalex.org/W4385245566', 'https://openalex.org/W4381786045', 'https://openalex.org/W2970597249', 'https://openalex.org/W4285819380', 'https://openalex.org/W2965373594']",2023-05-05
https://openalex.org/W4391771405,https://doi.org/10.1007/s00521-024-09435-1,Exploring emergent syllables in end-to-end automatic speech recognizers through model explainability technique,"Abstract Automatic speech recognition systems based on end-to-end models (E2E-ASRs) can achieve comparable performance to conventional ASR systems while reproducing all their essential parts automatically, from speech units to the language model. However, they hide the underlying perceptual processes modelled, if any, and they have lower adaptability to multiple application contexts, and, furthermore, they require powerful hardware and an extensive amount of training data. Model-explainability techniques can explore the internal dynamics of these ASR systems and possibly understand and explain the processes conducting to their decisions and outputs. Understanding these processes can help enhance ASR performance and reduce the required training data and hardware significantly. In this paper, we probe the internal dynamics of three E2E-ASRs pre-trained for English by building an acoustic-syllable boundary detector for Italian and Spanish based on the E2E-ASRs’ internal encoding layer outputs. We demonstrate that the shallower E2E-ASR layers spontaneously form a rhythmic component correlated with prominent syllables, central in human speech processing. This finding highlights a parallel between the analysed E2E-ASRs and human speech recognition. Our results contribute to the body of knowledge by providing a human-explainable insight into behaviours encoded in popular E2E-ASR systems.","['https://openalex.org/W2624147939', 'https://openalex.org/W2953343412', 'https://openalex.org/W4210705875', 'https://openalex.org/W3172678107', 'https://openalex.org/W2048068389', 'https://openalex.org/W2063292270', 'https://openalex.org/W2094016999', 'https://openalex.org/W4361994483', 'https://openalex.org/W2167200294', 'https://openalex.org/W2017881378', 'https://openalex.org/W2075335860', 'https://openalex.org/W23079662', 'https://openalex.org/W2067620280', 'https://openalex.org/W2494888996', 'https://openalex.org/W2981857663', 'https://openalex.org/W2104981903', 'https://openalex.org/W1529663760', 'https://openalex.org/W1992272902', 'https://openalex.org/W2403023380', 'https://openalex.org/W1569206236', 'https://openalex.org/W2144273389', 'https://openalex.org/W2253740514', 'https://openalex.org/W3211278025', 'https://openalex.org/W6605019098', 'https://openalex.org/W2127141656', 'https://openalex.org/W2792511626', 'https://openalex.org/W6603308362', 'https://openalex.org/W3142589538', 'https://openalex.org/W1573926077', 'https://openalex.org/W2972324944', 'https://openalex.org/W3163652268', 'https://openalex.org/W6601517772', 'https://openalex.org/W3097777922', 'https://openalex.org/W2144499799', 'https://openalex.org/W2995523160', 'https://openalex.org/W2975495759', 'https://openalex.org/W2981731882', 'https://openalex.org/W2947411064', 'https://openalex.org/W3131917094', 'https://openalex.org/W4210725851', 'https://openalex.org/W4220662349', 'https://openalex.org/W4294559022', 'https://openalex.org/W4386179745', 'https://openalex.org/W3087507349', 'https://openalex.org/W2927351257', 'https://openalex.org/W2986734036', 'https://openalex.org/W3041378136', 'https://openalex.org/W4377014103', 'https://openalex.org/W4386566596', 'https://openalex.org/W3202070718', 'https://openalex.org/W2971597241', 'https://openalex.org/W2972808286', 'https://openalex.org/W2973094925', 'https://openalex.org/W3034273309', 'https://openalex.org/W3163596720', 'https://openalex.org/W4226380987', 'https://openalex.org/W3015412285', 'https://openalex.org/W2110123218', 'https://openalex.org/W2032020816', 'https://openalex.org/W2949687910', 'https://openalex.org/W2106404689', 'https://openalex.org/W1977532506', 'https://openalex.org/W71670877', 'https://openalex.org/W2141820854', 'https://openalex.org/W2094544353', 'https://openalex.org/W3195642667', 'https://openalex.org/W2064675550', 'https://openalex.org/W2609874355', 'https://openalex.org/W3187996717', 'https://openalex.org/W4307265270', 'https://openalex.org/W4312161569', 'https://openalex.org/W2131703294', 'https://openalex.org/W2313364181', 'https://openalex.org/W4292581907', 'https://openalex.org/W1975879668', 'https://openalex.org/W2161368496', 'https://openalex.org/W4221102486', 'https://openalex.org/W3016185014', 'https://openalex.org/W4281492411', 'https://openalex.org/W2163680580', 'https://openalex.org/W2167188741', 'https://openalex.org/W4226390724', 'https://openalex.org/W4319862477', 'https://openalex.org/W2973049979', 'https://openalex.org/W4381786045', 'https://openalex.org/W3203140070', 'https://openalex.org/W3015383801', 'https://openalex.org/W4237670716', 'https://openalex.org/W3034942609', 'https://openalex.org/W3015356564', 'https://openalex.org/W4312227358', 'https://openalex.org/W2408211607', 'https://openalex.org/W2043482182', 'https://openalex.org/W333784849', 'https://openalex.org/W2404317780', 'https://openalex.org/W3151526698', 'https://openalex.org/W2025290119', 'https://openalex.org/W2981822782', 'https://openalex.org/W1979173241', 'https://openalex.org/W2801371855', 'https://openalex.org/W3128663598', 'https://openalex.org/W4296068616', 'https://openalex.org/W4385573307', 'https://openalex.org/W4385991041', 'https://openalex.org/W3168346546', 'https://openalex.org/W4200444305', 'https://openalex.org/W2947092141', 'https://openalex.org/W2735362131', 'https://openalex.org/W2117527004', 'https://openalex.org/W1502283197', 'https://openalex.org/W1578856370', 'https://openalex.org/W3144192465']",2024-02-13
https://openalex.org/W3198039885,https://doi.org/10.21437/interspeech.2021-1454,Improving Streaming Transformer Based ASR Under a Framework of Self-Supervised Learning,"Recently self-supervised learning has emerged as an effective approach to improve the performance of automatic speech recognition (ASR).Under such a framework, the neural network is usually pre-trained with massive unlabeled data and then fine-tuned with limited labeled data.However, the nonstreaming architecture like bidirectional transformer is usually adopted by the neural network to achieve competitive results, which can not be used in streaming scenarios.In this paper, we mainly focus on improving the performance of streaming transformer under the self-supervised learning framework.Specifically, we propose a novel two-stage training method during finetuning, which combines knowledge distilling and self-training.The proposed training method achieves 16.3% relative word error rate (WER) reduction on Librispeech noisy test set.Finally, by only using the 100h clean subset of Librispeech as the labeled data and the rest (860h) as the unlabeled data, our streaming transformer based model obtains competitive WERs 3.5/8.7 on Librispeech clean/noisy test sets.","['https://openalex.org/W2973040747', 'https://openalex.org/W3162665866', 'https://openalex.org/W3015356564', 'https://openalex.org/W4385245566', 'https://openalex.org/W2991213871', 'https://openalex.org/W3096518646', 'https://openalex.org/W3161873870', 'https://openalex.org/W1836465849', 'https://openalex.org/W2913718171', 'https://openalex.org/W3005680577', 'https://openalex.org/W2979476256', 'https://openalex.org/W1690739335', 'https://openalex.org/W3096888553', 'https://openalex.org/W2941814890', 'https://openalex.org/W3036601975', 'https://openalex.org/W2802023636', 'https://openalex.org/W2973049979', 'https://openalex.org/W1494198834', 'https://openalex.org/W3160525311', 'https://openalex.org/W2962739339', 'https://openalex.org/W3105966348', 'https://openalex.org/W3093579165', 'https://openalex.org/W3160628828', 'https://openalex.org/W2896457183', 'https://openalex.org/W3034781633', 'https://openalex.org/W2944828972']",2021-08-27
https://openalex.org/W3123796542,https://doi.org/10.3389/fnhum.2021.653659,BENDR: Using Transformers and a Contrastive Self-Supervised Learning Task to Learn From Massive Amounts of EEG Data,"Deep neural networks (DNNs) used for brain–computer interface (BCI) classification are commonly expected to learn general features when trained across a variety of contexts, such that these features could be fine-tuned to specific contexts. While some success is found in such an approach, we suggest that this interpretation is limited and an alternative would better leverage the newly (publicly) available massive electroencephalography (EEG) datasets. We consider how to adapt techniques and architectures used for language modeling (LM) that appear capable of ingesting awesome amounts of data toward the development of encephalography modeling with DNNs in the same vein. We specifically adapt an approach effectively used for automatic speech recognition, which similarly (to LMs) uses a self-supervised training objective to learn compressed representations of raw data signals. After adaptation to EEG, we find that a single pre-trained model is capable of modeling completely novel raw EEG sequences recorded with differing hardware, and different subjects performing different tasks. Furthermore, both the internal representations of this model and the entire architecture can be fine-tuned to a variety of downstream BCI and EEG classification tasks, outperforming prior work in more task-specific (sleep stage classification) self-supervision.","['https://openalex.org/W1968417898', 'https://openalex.org/W6760212410', 'https://openalex.org/W3015356564', 'https://openalex.org/W2994411342', 'https://openalex.org/W3101658985', 'https://openalex.org/W2963919481', 'https://openalex.org/W6685520387', 'https://openalex.org/W6779977557', 'https://openalex.org/W3097286738', 'https://openalex.org/W3014215018', 'https://openalex.org/W2027101175', 'https://openalex.org/W6941460233', 'https://openalex.org/W2108598243', 'https://openalex.org/W6755207826', 'https://openalex.org/W2950976972', 'https://openalex.org/W2888355470', 'https://openalex.org/W6784333009', 'https://openalex.org/W2902034646', 'https://openalex.org/W3006385477', 'https://openalex.org/W2162800060', 'https://openalex.org/W2144499799', 'https://openalex.org/W6756427901', 'https://openalex.org/W2194775991', 'https://openalex.org/W6718683173', 'https://openalex.org/W6725739302', 'https://openalex.org/W6780086851', 'https://openalex.org/W6725762072', 'https://openalex.org/W6777859476', 'https://openalex.org/W3011411500', 'https://openalex.org/W2060472216', 'https://openalex.org/W6960066922', 'https://openalex.org/W2144691514', 'https://openalex.org/W6631190155', 'https://openalex.org/W6752381671', 'https://openalex.org/W2914209400', 'https://openalex.org/W3112130169', 'https://openalex.org/W3085768000', 'https://openalex.org/W6684191040', 'https://openalex.org/W2559463885', 'https://openalex.org/W2919115771', 'https://openalex.org/W2726279040', 'https://openalex.org/W2794345050', 'https://openalex.org/W2130847623', 'https://openalex.org/W6761563299', 'https://openalex.org/W2920016582', 'https://openalex.org/W6756131066', 'https://openalex.org/W2345279893', 'https://openalex.org/W6769627184', 'https://openalex.org/W6763301180', 'https://openalex.org/W6756859573', 'https://openalex.org/W2923329249', 'https://openalex.org/W2963355311', 'https://openalex.org/W2912270846', 'https://openalex.org/W2151669316', 'https://openalex.org/W2741907166', 'https://openalex.org/W2890122878', 'https://openalex.org/W3004368638', 'https://openalex.org/W6783399553', 'https://openalex.org/W2119163516', 'https://openalex.org/W2842511635', 'https://openalex.org/W2003621157', 'https://openalex.org/W4238100585', 'https://openalex.org/W2964252002', 'https://openalex.org/W6638389677', 'https://openalex.org/W2746829572', 'https://openalex.org/W3000231660', 'https://openalex.org/W3102003537', 'https://openalex.org/W3030163527', 'https://openalex.org/W1825675169', 'https://openalex.org/W3036982689', 'https://openalex.org/W2964121744', 'https://openalex.org/W2901616798', 'https://openalex.org/W2947706148', 'https://openalex.org/W2941814890', 'https://openalex.org/W2626778328', 'https://openalex.org/W3035060554', 'https://openalex.org/W2944828972', 'https://openalex.org/W3035618017', 'https://openalex.org/W3092435621', 'https://openalex.org/W3099782249', 'https://openalex.org/W3082274269', 'https://openalex.org/W2330462633', 'https://openalex.org/W2901026139', 'https://openalex.org/W2163605009', 'https://openalex.org/W2510153535', 'https://openalex.org/W3026957705', 'https://openalex.org/W2804935296', 'https://openalex.org/W2980679849', 'https://openalex.org/W2174492417', 'https://openalex.org/W3098033339', 'https://openalex.org/W3094502228', 'https://openalex.org/W2899663614', 'https://openalex.org/W2963341956', 'https://openalex.org/W3102455230', 'https://openalex.org/W2970941190', 'https://openalex.org/W2917551568', 'https://openalex.org/W2991391304']",2021-06-23
https://openalex.org/W4367624800,https://doi.org/10.3390/bdcc7020085,An Ensemble-Learning-Based Technique for Bimodal Sentiment Analysis,"Human communication is predominantly expressed through speech and writing, which are powerful mediums for conveying thoughts and opinions. Researchers have been studying the analysis of human sentiments for a long time, including the emerging area of bimodal sentiment analysis in natural language processing (NLP). Bimodal sentiment analysis has gained attention in various areas such as social opinion mining, healthcare, banking, and more. However, there is a limited amount of research on bimodal conversational sentiment analysis, which is challenging due to the complex nature of how humans express sentiment cues across different modalities. To address this gap in research, a comparison of multiple data modality models has been conducted on the widely used MELD dataset, which serves as a benchmark for sentiment analysis in the research community. The results show the effectiveness of combining acoustic and linguistic representations using a proposed neural-network-based ensemble learning technique over six transformer and deep-learning-based models, achieving state-of-the-art accuracy.","['https://openalex.org/W4239817869', 'https://openalex.org/W1687157824', 'https://openalex.org/W4230277160', 'https://openalex.org/W3209072429', 'https://openalex.org/W2963873807', 'https://openalex.org/W3088631780', 'https://openalex.org/W2095176743', 'https://openalex.org/W2250539671', 'https://openalex.org/W4235305945', 'https://openalex.org/W2170938075', 'https://openalex.org/W2985882473', 'https://openalex.org/W2980927909', 'https://openalex.org/W2625297138', 'https://openalex.org/W2962770129', 'https://openalex.org/W2747664154', 'https://openalex.org/W2889462515', 'https://openalex.org/W2972463723', 'https://openalex.org/W2021137987', 'https://openalex.org/W3198531509', 'https://openalex.org/W3110845139', 'https://openalex.org/W3008863810', 'https://openalex.org/W2964287450', 'https://openalex.org/W3048053677', 'https://openalex.org/W2945819472', 'https://openalex.org/W2789041267', 'https://openalex.org/W3211367374', 'https://openalex.org/W2931364255', 'https://openalex.org/W3122100772', 'https://openalex.org/W158626325', 'https://openalex.org/W2914312680', 'https://openalex.org/W2141599568', 'https://openalex.org/W2962739339', 'https://openalex.org/W4206571794', 'https://openalex.org/W3015489952', 'https://openalex.org/W2963182768', 'https://openalex.org/W2911489562', 'https://openalex.org/W2963103134', 'https://openalex.org/W4285262944', 'https://openalex.org/W2136922672', 'https://openalex.org/W2919115771', 'https://openalex.org/W3112595245', 'https://openalex.org/W2250966211', 'https://openalex.org/W6676984168', 'https://openalex.org/W2923014074', 'https://openalex.org/W2973049979', 'https://openalex.org/W6780218876', 'https://openalex.org/W3015356564', 'https://openalex.org/W1494198834', 'https://openalex.org/W2577905684', 'https://openalex.org/W2055911634', 'https://openalex.org/W2889717020', 'https://openalex.org/W2087618018', 'https://openalex.org/W4205184193', 'https://openalex.org/W4385883944', 'https://openalex.org/W6731034454', 'https://openalex.org/W2295001676', 'https://openalex.org/W2889374687', 'https://openalex.org/W2399733683', 'https://openalex.org/W2954345346', 'https://openalex.org/W2143350951', 'https://openalex.org/W2149940198', 'https://openalex.org/W2584561145', 'https://openalex.org/W2964010806', 'https://openalex.org/W2740550900', 'https://openalex.org/W2964051877', 'https://openalex.org/W3014475539', 'https://openalex.org/W2965453734', 'https://openalex.org/W3173396651', 'https://openalex.org/W2787581402', 'https://openalex.org/W2891359673', 'https://openalex.org/W2805662932', 'https://openalex.org/W2899099710', 'https://openalex.org/W2963686995', 'https://openalex.org/W6719202312', 'https://openalex.org/W2801842676', 'https://openalex.org/W2979826702', 'https://openalex.org/W2946063542', 'https://openalex.org/W2584429674', 'https://openalex.org/W4281401666', 'https://openalex.org/W3036601975', 'https://openalex.org/W2461769152', 'https://openalex.org/W2566785176']",2023-04-30
https://openalex.org/W4382567930,https://doi.org/10.1109/lra.2023.3290521,A Deep Learning-Based Approach for Foot Placement Prediction,"Foot placement prediction can be important for exoskeleton and prosthesis controllers, human-robot interaction, or body-worn systems to prevent slips or trips. Previous studies investigating foot placement prediction have been limited to predicting foot placement during the swing phase, and do not fully consider contextual information such as the preceding step or the stance phase before push-off. In this study, we propose a deep learning-based foot placement prediction approach, sequentially processing data from three IMU sensors mounted on the pelvis and feet. The raw sensor data are pre-processed to generate multi-variable time-series data for training two deep learning models, where the first model estimates the gait progression and the second model subsequently predicts the next foot placement. The ground truth gait phase data and foot placement data are acquired from a motion capture system. Ten healthy subjects were invited to walk naturally at different speeds on a treadmill. In cross-subject learning, the trained models had a mean distance error of 5.93 cm for foot placement prediction. In single-subject learning, the prediction accuracy improved with additional training data, and a mean distance error of 2.60 cm was achieved by fine-tuning the cross-subject validated models with the target subject data. Even from 25–81% in the gait cycle, mean distance errors were only 6.99 cm and 3.22 cm for cross-subject learning and single-subject learning, respectively.","['https://openalex.org/W2055698923', 'https://openalex.org/W2280152108', 'https://openalex.org/W3206939027', 'https://openalex.org/W2228102949', 'https://openalex.org/W2908715333', 'https://openalex.org/W4313563153', 'https://openalex.org/W4313244055', 'https://openalex.org/W2883744822', 'https://openalex.org/W1977699608', 'https://openalex.org/W2132992113', 'https://openalex.org/W3134792577', 'https://openalex.org/W4205475452', 'https://openalex.org/W4206579646', 'https://openalex.org/W6941505506', 'https://openalex.org/W4308194475', 'https://openalex.org/W4246449400', 'https://openalex.org/W3193894556', 'https://openalex.org/W2896938208', 'https://openalex.org/W3207392858', 'https://openalex.org/W3184541706', 'https://openalex.org/W1896503986', 'https://openalex.org/W2979302956', 'https://openalex.org/W3145377020', 'https://openalex.org/W6796761347', 'https://openalex.org/W6755207826', 'https://openalex.org/W3015356564', 'https://openalex.org/W6631190155', 'https://openalex.org/W1981780459', 'https://openalex.org/W6676179485', 'https://openalex.org/W2162229274', 'https://openalex.org/W2884968643', 'https://openalex.org/W2606317729', 'https://openalex.org/W2088920718', 'https://openalex.org/W3170863103', 'https://openalex.org/W4206405834', 'https://openalex.org/W2896457183', 'https://openalex.org/W1522301498']",2023-06-29
https://openalex.org/W4319862287,https://doi.org/10.1109/slt54892.2023.10022697,On the Efficiency of Integrating Self-Supervised Learning and Meta-Learning for User-Defined Few-Shot Keyword Spotting,"User-defined keyword spotting is a task to detect new spoken terms defined by users. This can be viewed as a few-shot learning problem since it is unreasonable for users to define their desired keywords by providing many examples. To solve this problem, previous works try to incorporate self-supervised learning models or apply meta-learning algorithms. But it is unclear whether self-supervised learning and meta-learning are complementary and which combination of the two types of approaches is most effective for few-shot keyword discovery. In this work, we systematically study these questions by utilizing various self-supervised learning models and combining them with a wide variety of meta-learning algorithms. Our result shows that HuBERT combined with Matching network achieves the best result and is robust to the changes of few-shot examples.","['https://openalex.org/W3096197423', 'https://openalex.org/W6736057607', 'https://openalex.org/W6781011824', 'https://openalex.org/W6735236233', 'https://openalex.org/W3015399080', 'https://openalex.org/W3162273446', 'https://openalex.org/W1494198834', 'https://openalex.org/W2991234496', 'https://openalex.org/W3197564965', 'https://openalex.org/W3144247233', 'https://openalex.org/W6762718338', 'https://openalex.org/W6771467084', 'https://openalex.org/W3163571828', 'https://openalex.org/W6780218876', 'https://openalex.org/W3015213852', 'https://openalex.org/W2963070905', 'https://openalex.org/W3096277532', 'https://openalex.org/W2842511635', 'https://openalex.org/W3041561163', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W3015356564', 'https://openalex.org/W6768532476', 'https://openalex.org/W6790291204', 'https://openalex.org/W6750254146', 'https://openalex.org/W2964105864', 'https://openalex.org/W6717697761', 'https://openalex.org/W6750665317', 'https://openalex.org/W2972541922', 'https://openalex.org/W3197580070', 'https://openalex.org/W6786543096', 'https://openalex.org/W3036601975', 'https://openalex.org/W3046052470', 'https://openalex.org/W2963341924', 'https://openalex.org/W4294646197', 'https://openalex.org/W4297808394', 'https://openalex.org/W2187089797', 'https://openalex.org/W3111307227', 'https://openalex.org/W2797583228', 'https://openalex.org/W2601450892']",2023-01-09
https://openalex.org/W3201071655,https://doi.org/10.3390/electronics10182259,A Self-Supervised Model for Language Identification Integrating Phonological Knowledge,"In this paper, a self-supervised learning pre-trained model is proposed and successfully applied in language identification task (LID). A Transformer encoder is employed and multi-task strategy is used to train the self-supervised model: the first task is to reconstruct the masking spans of input frames and the second task is a supervision task where the phoneme and phonological labels are used with Connectionist Temporal Classification (CTC) loss. By using this multi-task learning loss, the model is expected to capture high-level speech representation in phonological space. Meanwhile, an adaptive loss is also applied for multi-task learning to balance the weight between different tasks. After the pretraining stage, the self-supervised model is used for xvector systems. Our LID experiments are carried out on the oriental language recognition (OLR) challenge data corpus and 1 s, 3 s, Full-length test sets are selected. Experimental results show that on 1 s test set, feature extraction model approach can get best performance and in 3 s, Full-length test, the fine-tuning approach can reach the best performance. Furthermore, our results prove that the multi-task training strategy is effective and the proposed model can get the best performance.","['https://openalex.org/W3162390194', 'https://openalex.org/W2973049979', 'https://openalex.org/W3015356564', 'https://openalex.org/W6776061696', 'https://openalex.org/W2172287020', 'https://openalex.org/W2056119007', 'https://openalex.org/W29056978', 'https://openalex.org/W155708904', 'https://openalex.org/W2982223350', 'https://openalex.org/W3148040514', 'https://openalex.org/W2973157397', 'https://openalex.org/W3015499614', 'https://openalex.org/W3011380426', 'https://openalex.org/W3011926625', 'https://openalex.org/W6753930865', 'https://openalex.org/W2746036396', 'https://openalex.org/W2408021097', 'https://openalex.org/W2153181479', 'https://openalex.org/W2963063081', 'https://openalex.org/W2067093722', 'https://openalex.org/W2892009249', 'https://openalex.org/W2972818416', 'https://openalex.org/W2936774411', 'https://openalex.org/W3041561163', 'https://openalex.org/W2127141656', 'https://openalex.org/W2807627734', 'https://openalex.org/W2963307329', 'https://openalex.org/W6631362777', 'https://openalex.org/W2911291251', 'https://openalex.org/W3163421828', 'https://openalex.org/W2918804987', 'https://openalex.org/W2894819042', 'https://openalex.org/W2887682992', 'https://openalex.org/W3007328579']",2021-09-14
https://openalex.org/W4312120626,https://doi.org/10.23919/apsipaasc55919.2022.9980000,Speech Intelligibility Prediction for Hearing Aids Using an Auditory Model and Acoustic Parameters,"Objective speech intelligibility (SI) metrics for hearing-impaired people play an important role in hearing aid development. The work on improving SI prediction also became the basis of the first Clarity Prediction Challenge (CPC1). This study investigates a physiological auditory model called EarModel and acoustic parameters for SI prediction. EarModel is utilized because it provides advantages in estimating human hearing, both normal and impaired. The hearing-impaired condition is simulated in EarModel based on audiograms; thus, the SI perceived by hearing-impaired people is more accurately predicted. Moreover, the extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS) and WavLM, as additional acoustic parameters for estimating the difficulty levels of given utterances, are included to achieve improved prediction accuracy. The proposed method is evaluated on the CPC1 database. The results show that the proposed method improves the SI prediction effects of the baseline and hearing aid speech prediction index (HASPI). Additionally, an ablation test shows that incorporating the eGeMAPS and WavLM can significantly contribute to the prediction model by increasing the Pearson correlation coefficient by more than 15% and decreasing the root-mean-square error (RMSE) by more than 10.00 in both closed-set and open-set tracks.","['https://openalex.org/W2911182057', 'https://openalex.org/W1858668325', 'https://openalex.org/W2277906811', 'https://openalex.org/W2736307756', 'https://openalex.org/W2102063915', 'https://openalex.org/W2138759370', 'https://openalex.org/W2153038597', 'https://openalex.org/W349236604', 'https://openalex.org/W2793348675', 'https://openalex.org/W2092644348', 'https://openalex.org/W2060514933', 'https://openalex.org/W4297841451', 'https://openalex.org/W4224071249', 'https://openalex.org/W2903293443', 'https://openalex.org/W2317830494', 'https://openalex.org/W2117945314', 'https://openalex.org/W3204684168', 'https://openalex.org/W3032727804', 'https://openalex.org/W2046849846', 'https://openalex.org/W3197832224', 'https://openalex.org/W2141998673', 'https://openalex.org/W2528926462', 'https://openalex.org/W2008806652', 'https://openalex.org/W2136376066', 'https://openalex.org/W2097198422', 'https://openalex.org/W3160157930', 'https://openalex.org/W2167317466', 'https://openalex.org/W3015356564', 'https://openalex.org/W3209984917', 'https://openalex.org/W3209059054', 'https://openalex.org/W2239141610', 'https://openalex.org/W2090777335', 'https://openalex.org/W2076125144', 'https://openalex.org/W3155736297', 'https://openalex.org/W2140294870', 'https://openalex.org/W3117892752', 'https://openalex.org/W4297841328', 'https://openalex.org/W3036601975']",2022-11-07
https://openalex.org/W4396747569,https://doi.org/10.1007/s11063-024-11613-0,WaveVC: Speech and Fundamental Frequency Consistent Raw Audio Voice Conversion,"Abstract Voice conversion (VC) is a task for changing the speech of a source speaker to the target voice while preserving linguistic information of the source speech. The existing VC methods typically use mel-spectrogram as both input and output, so a separate vocoder is required to transform mel-spectrogram into waveform. Therefore, the VC performance varies depending on the vocoder performance, and noisy speech can be generated due to problems such as train-test mismatch. In this paper, we propose a speech and fundamental frequency consistent raw audio voice conversion method called WaveVC. Unlike other methods, WaveVC does not require a separate vocoder and can perform VC directly on raw audio waveform using 1D convolution. This eliminates the issue of performance degradation caused by the train-test mismatch of the vocoder. In the training phase, WaveVC employs speech loss and F0 loss to preserve the content of the source speech and generate F0 consistent speech using the pre-trained networks. WaveVC is capable of converting voices while maintaining consistency in speech and fundamental frequency. In the test phase, the F0 feature of the source speech is concatenated with a content embedding vector to ensure the converted speech follows the fundamental frequency flow of the source speech. WaveVC achieves higher performances than baseline methods in both many-to-many VC and any-to-any VC. The converted samples are available online.","['https://openalex.org/W2104604339', 'https://openalex.org/W4242185151', 'https://openalex.org/W2972812066', 'https://openalex.org/W2022125261', 'https://openalex.org/W2972659941', 'https://openalex.org/W3015805741', 'https://openalex.org/W3015434413', 'https://openalex.org/W3096524539', 'https://openalex.org/W3197659778', 'https://openalex.org/W3096831136', 'https://openalex.org/W2963767194', 'https://openalex.org/W2963539064', 'https://openalex.org/W2972667718', 'https://openalex.org/W3196667132', 'https://openalex.org/W3015338123', 'https://openalex.org/W2963411216', 'https://openalex.org/W2963609956', 'https://openalex.org/W2904459034', 'https://openalex.org/W6632668414', 'https://openalex.org/W6776948474', 'https://openalex.org/W2603777577', 'https://openalex.org/W3163475957', 'https://openalex.org/W3161627112', 'https://openalex.org/W4221146610', 'https://openalex.org/W3168719651', 'https://openalex.org/W2932319787', 'https://openalex.org/W2526425061', 'https://openalex.org/W4294982498', 'https://openalex.org/W2998572311', 'https://openalex.org/W2471520273', 'https://openalex.org/W4375868976', 'https://openalex.org/W3161558238', 'https://openalex.org/W3015356564', 'https://openalex.org/W3205878676', 'https://openalex.org/W2962788625', 'https://openalex.org/W3095173472', 'https://openalex.org/W2969985801', 'https://openalex.org/W3095361818']",2024-05-07
https://openalex.org/W4392902995,https://doi.org/10.1109/icassp48485.2024.10446344,Unsupervised Accent Adaptation Through Masked Language Model Correction of Discrete Self-Supervised Speech Units,"Self-supervised pre-trained speech models have strongly improved speech recognition, yet they are still sensitive to domain shifts and accented or atypical speech. Many of these models rely on quantisation or clustering to learn discrete acoustic units. We propose to correct the discovered discrete units for accented speech back to a standard pronunciation in an unsupervised manner. A masked language model is trained on discrete units from a standard accent and iteratively corrects an accented token sequence by masking unexpected cluster sequences and predicting their common variant. Small accent adapter blocks are inserted in the pre-trained model and fine-tuned by predicting the corrected clusters, which leads to an increased robustness of the pre-trained model towards a target accent, and this without supervision. We are able to improve a state-of-the-art HuBERT Large model on a downstream accented speech recognition task by altering the training regime with the proposed method.","['https://openalex.org/W3209059054', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209984917', 'https://openalex.org/W6810673746', 'https://openalex.org/W4226033575', 'https://openalex.org/W3163596720', 'https://openalex.org/W3198771897', 'https://openalex.org/W6839738141', 'https://openalex.org/W4385822676', 'https://openalex.org/W4385823170', 'https://openalex.org/W659550629', 'https://openalex.org/W6755207826', 'https://openalex.org/W1494198834', 'https://openalex.org/W6771467084', 'https://openalex.org/W3011411500', 'https://openalex.org/W6768851824', 'https://openalex.org/W6759579507', 'https://openalex.org/W4375868763', 'https://openalex.org/W6790356757', 'https://openalex.org/W6795952400', 'https://openalex.org/W2988975212', 'https://openalex.org/W3097882114', 'https://openalex.org/W4385567350', 'https://openalex.org/W3197580070', 'https://openalex.org/W2127141656', 'https://openalex.org/W6769196770', 'https://openalex.org/W3015356564', 'https://openalex.org/W4385822683', 'https://openalex.org/W3160475509', 'https://openalex.org/W4292387508', 'https://openalex.org/W3162812479', 'https://openalex.org/W4319862412', 'https://openalex.org/W3015723617', 'https://openalex.org/W4394671563']",2024-03-18
https://openalex.org/W4402256224,https://doi.org/10.55662/jst.2024.5406,Securing Wireless Networks Against Emerging Threats: An Overview of Protocols and Solutions,"As wireless networks have become an integral part of modern communication infrastructure, ensuring their security against a rapidly evolving threat landscape is a critical concern. This research article provides a comprehensive overview of the emerging threats targeting wireless networks, including advanced persistent threats, man-in-the-middle (MitM) attacks, and AI-driven adaptive malware. With the advent of new technologies such as 5G, the Internet of Things (IoT), and artificial intelligence (AI), the attack surface for wireless networks has significantly expanded, demanding more robust and adaptive security protocols. The paper analyzes the efficacy of current wireless security protocols, such as WPA3 and the 802.11i standard, in addressing these emerging vulnerabilities. While these protocols have introduced significant improvements, they are not without limitations. The article further explores innovative solutions such as blockchain-based security frameworks, AI-powered threat detection systems, and the future potential of quantum cryptography in safeguarding wireless communications. Through a critical review of recent case studies and empirical data, the article highlights the key challenges that organizations face in securing wireless networks, particularly in IoT environments where security standards lag behind technological advancements. The research concludes that while existing protocols provide foundational security, they must be continuously updated and augmented with cutting-edge technologies to counter the growing sophistication of cyberattacks. This article aims to provide insights into the state of wireless network security and offer practical recommendations for enhancing security protocols. Future research directions are also discussed, focusing on the integration of AI-driven threat intelligence and the standardization of security protocols across various wireless technologies. The findings underscore the importance of proactive security measures to safeguard wireless networks in an increasingly interconnected world.","['https://openalex.org/W3114632476', 'https://openalex.org/W4291023040', 'https://openalex.org/W4283697304', 'https://openalex.org/W6743229011', 'https://openalex.org/W2973049979', 'https://openalex.org/W3005680577', 'https://openalex.org/W3171007011', 'https://openalex.org/W3035524453', 'https://openalex.org/W3035060554', 'https://openalex.org/W2963341956', 'https://openalex.org/W3041561163', 'https://openalex.org/W3015356564', 'https://openalex.org/W4281492411', 'https://openalex.org/W3190152617', 'https://openalex.org/W2920777619', 'https://openalex.org/W4221163898', 'https://openalex.org/W2963465221', 'https://openalex.org/W4289946024', 'https://openalex.org/W4221145109', 'https://openalex.org/W6926573507', 'https://openalex.org/W4324093415', 'https://openalex.org/W2986058825', 'https://openalex.org/W2061667795', 'https://openalex.org/W2903220614', 'https://openalex.org/W2251599967', 'https://openalex.org/W4283219445', 'https://openalex.org/W3105108969', 'https://openalex.org/W2017633143', 'https://openalex.org/W4256346655', 'https://openalex.org/W2922404196', 'https://openalex.org/W2113928343', 'https://openalex.org/W2738819672', 'https://openalex.org/W2514396107', 'https://openalex.org/W4206171084', 'https://openalex.org/W3100362635', 'https://openalex.org/W3092728502', 'https://openalex.org/W3008629209', 'https://openalex.org/W2753002651', 'https://openalex.org/W3011937595']",2024-09-05
https://openalex.org/W4402349570,https://doi.org/10.1145/3678515,StethoSpeech: Speech Generation Through a Clinical Stethoscope Attached to the Skin,"We introduce StethoSpeech, a silent speech interface that transforms flesh-conducted vibrations behind the ear into speech. This innovation is designed to improve social interactions for those with voice disorders, and furthermore enable discreet public communication. Unlike prior efforts, StethoSpeech does not require (a) paired-speech data for recorded vibrations and (b) a specialized device for recording vibrations, as it can work with an off-the-shelf clinical stethoscope. The novelty of our framework lies in the overall design, simulation of the ground-truth speech, and a sequence-to-sequence translation network, which works in the latent space. We present comprehensive experiments on the existing CSTR NAM TIMIT Plus corpus and our proposed StethoText: a large-scale synchronized database of non-audible murmur and text for speech research. Our results show that StethoSpeech provides natural-sounding and intelligible speech, significantly outperforming existing methods on several quantitative and qualitative metrics. Additionally, we showcase its capacity to extend its application to speakers not encountered during training and its effectiveness in challenging, noisy environments. Speech samples are available at https://stethospeech.github.io/StethoSpeech/.","['https://openalex.org/W3015356564', 'https://openalex.org/W4381786045', 'https://openalex.org/W3005302394', 'https://openalex.org/W3209984917', 'https://openalex.org/W830267248', 'https://openalex.org/W2963341956', 'https://openalex.org/W4390872483', 'https://openalex.org/W1986910277', 'https://openalex.org/W1635512741', 'https://openalex.org/W2770785043', 'https://openalex.org/W2127141656', 'https://openalex.org/W1986593580', 'https://openalex.org/W1999209169', 'https://openalex.org/W3209059054', 'https://openalex.org/W4386076005', 'https://openalex.org/W2071431631', 'https://openalex.org/W2793257307', 'https://openalex.org/W3160728073', 'https://openalex.org/W4385574033', 'https://openalex.org/W3015465870', 'https://openalex.org/W3090484329', 'https://openalex.org/W2160800342', 'https://openalex.org/W1991169524', 'https://openalex.org/W4385823362', 'https://openalex.org/W2933138175', 'https://openalex.org/W2987496713', 'https://openalex.org/W3140429000', 'https://openalex.org/W3035626590', 'https://openalex.org/W4323322934', 'https://openalex.org/W4388821341', 'https://openalex.org/W2973049979', 'https://openalex.org/W2888796252', 'https://openalex.org/W4393171356', 'https://openalex.org/W4321009928', 'https://openalex.org/W2120605154', 'https://openalex.org/W2156060621', 'https://openalex.org/W1540787848', 'https://openalex.org/W2095026407', 'https://openalex.org/W2889413431', 'https://openalex.org/W3004196347', 'https://openalex.org/W2303272654', 'https://openalex.org/W4361763861', 'https://openalex.org/W2946200149', 'https://openalex.org/W68041361', 'https://openalex.org/W2914304175', 'https://openalex.org/W4386566590']",2024-08-22
https://openalex.org/W4392904360,https://doi.org/10.1109/icassp48485.2024.10445917,Class: Continual Learning Approach for Speech Super-Resolution,"Supervised deep learning has significantly improved bandwidth extension (BWE), whereas the emergence of self-supervised learning (SSL) has prompted the combined exploration of SSL and BWE. Although SSL-based deep learning models have shown to produce better representations than their supervised counterparts when trained naively, their effectiveness diminishes in when the model learns different tasks sequentially. To address this problem, we propose a continual learning framework called CLASS, which incorporates continual learning (CL) and self-supervised pretraining (SSP) to improve BWE performance. The framework integrates SSP and BWE fine-tuning tasks with CL approaches, enabling the model to retain its representation knowledge while adapting to BWE as a target task. We employ the CL fine-tuning loss or exponential moving average algorithm to gradually update model parameters and learn to resemble wideband from narrowband signals without losing information from a previous task. In addition, we present the new continual loss with extended version of elastic weight consolidation by updating fisher information matrix for better BWE performance. Our experimental results demonstrate that the proposed method outperforms the baseline approach on the TIMIT dataset. Furthermore, we explore the impact of different hyperparameter settings, contributing to a more comprehensive understanding of the performance of the proposed framework.","['https://openalex.org/W2100285470', 'https://openalex.org/W2171815504', 'https://openalex.org/W1517841224', 'https://openalex.org/W2802253983', 'https://openalex.org/W6741681139', 'https://openalex.org/W6767367760', 'https://openalex.org/W3213952100', 'https://openalex.org/W3211264909', 'https://openalex.org/W3015356564', 'https://openalex.org/W3209059054', 'https://openalex.org/W2060277733', 'https://openalex.org/W1584431645', 'https://openalex.org/W4225777901', 'https://openalex.org/W3168615033', 'https://openalex.org/W6685726866', 'https://openalex.org/W2560647685', 'https://openalex.org/W6733814495', 'https://openalex.org/W6639024717', 'https://openalex.org/W2963788399', 'https://openalex.org/W4293363567', 'https://openalex.org/W2141998673', 'https://openalex.org/W2516001803', 'https://openalex.org/W1552314771', 'https://openalex.org/W4298174377', 'https://openalex.org/W1844261860']",2024-03-18
https://openalex.org/W3200104365,https://doi.org/10.1109/ijcnn52387.2021.9533328,Audio DistilBERT: A Distilled Audio BERT for Speech Representation Learning,"Self-supervised speech representation learning has been considered as an outstanding manner to improve the performance of downstream tasks. However, those models are often too cumbersome, which sets a barrier to deploy them on the edge and improves the threshold of the pre-training process. In this paper, we propose Audio DistilBERT, a distilled BERT-style speech representation learning method. It learns dark knowledge from a larger teacher model through one new designed loss which combines soft and hard targets. By doing this, it can achieve competitive performance with fewer parameters and faster inference time. The experimental results among two downstream tasks show that the proposed method can retain above 98% performance of the large model with about 1.8× smaller model size and over 1.6× faster inference speed. In a low-resource environment with very few labeled data and pretraining steps, our model also exhibits similar or even better performance compared to the large model. Furthermore, we explore the knowledge transfer competence between the teacher and student model.","['https://openalex.org/W6631362777', 'https://openalex.org/W6739901393', 'https://openalex.org/W2936774411', 'https://openalex.org/W6766673545', 'https://openalex.org/W6773206665', 'https://openalex.org/W6840487619', 'https://openalex.org/W3015213852', 'https://openalex.org/W6755207826', 'https://openalex.org/W3034560159', 'https://openalex.org/W3035030897', 'https://openalex.org/W6768851824', 'https://openalex.org/W2970454332', 'https://openalex.org/W6768086466', 'https://openalex.org/W3034457371', 'https://openalex.org/W6730179637', 'https://openalex.org/W6769238691', 'https://openalex.org/W6637551013', 'https://openalex.org/W6776076330', 'https://openalex.org/W2997666887', 'https://openalex.org/W2982223350', 'https://openalex.org/W6769196770', 'https://openalex.org/W2972943112', 'https://openalex.org/W2973049979', 'https://openalex.org/W6780218876', 'https://openalex.org/W6636510571', 'https://openalex.org/W1494198834', 'https://openalex.org/W2997006708', 'https://openalex.org/W6638523607', 'https://openalex.org/W3035365026', 'https://openalex.org/W2963140444', 'https://openalex.org/W2947454875', 'https://openalex.org/W4297808394', 'https://openalex.org/W2963341956', 'https://openalex.org/W3015356564', 'https://openalex.org/W3036601975', 'https://openalex.org/W2996383576', 'https://openalex.org/W2964118293', 'https://openalex.org/W1821462560', 'https://openalex.org/W1690739335', 'https://openalex.org/W2896457183', 'https://openalex.org/W2963403868', 'https://openalex.org/W4288348042', 'https://openalex.org/W3099782249', 'https://openalex.org/W3041561163', 'https://openalex.org/W1614298861', 'https://openalex.org/W2842511635', 'https://openalex.org/W3148040514', 'https://openalex.org/W2965373594', 'https://openalex.org/W2979476256', 'https://openalex.org/W2981991061', 'https://openalex.org/W2978017171', 'https://openalex.org/W3003875258', 'https://openalex.org/W1524333225', 'https://openalex.org/W3025035610', 'https://openalex.org/W4385245566', 'https://openalex.org/W2561238782', 'https://openalex.org/W3105966348']",2021-07-18
https://openalex.org/W4243326049,https://doi.org/10.31234/osf.io/jngzq,SCALa: A blueprint for computational models of language acquisition in social context,"Theories and data on language acquisition suggest a range of cues are used, ranging from information on structure found in the linguistic signal itself, to information gleaned from the environmental context or through social interaction. We propose a blueprint for computational models of the early language learner (SCALa, for Socio-Computational Architecture of Language Acquisition) that makes explicit the connection between the kinds of information available to the social learner and the computational mechanisms required to extract language-relevant information and learn from it. SCALa integrates a range of views on language acquisition, further allowing us to make precise recommendations for future large-scale empirical research.","['https://openalex.org/W1963493092', 'https://openalex.org/W2980577029', 'https://openalex.org/W6634761788', 'https://openalex.org/W3015356564', 'https://openalex.org/W2885156775', 'https://openalex.org/W2895356663', 'https://openalex.org/W2785533964', 'https://openalex.org/W6778883912', 'https://openalex.org/W2997253105', 'https://openalex.org/W6781919819', 'https://openalex.org/W2488227055', 'https://openalex.org/W2747789846', 'https://openalex.org/W6791910184', 'https://openalex.org/W6845709182', 'https://openalex.org/W6765529972', 'https://openalex.org/W2766298282', 'https://openalex.org/W6636124663', 'https://openalex.org/W4234313254', 'https://openalex.org/W2896457183', 'https://openalex.org/W2483390977', 'https://openalex.org/W2011238950', 'https://openalex.org/W2159190230', 'https://openalex.org/W3005081886', 'https://openalex.org/W2613446889', 'https://openalex.org/W1964174279', 'https://openalex.org/W1613027418', 'https://openalex.org/W2132730112', 'https://openalex.org/W4253896841', 'https://openalex.org/W2004393017', 'https://openalex.org/W2580178245', 'https://openalex.org/W2165545766', 'https://openalex.org/W6739847781', 'https://openalex.org/W2132951686', 'https://openalex.org/W2165345255', 'https://openalex.org/W3023371261', 'https://openalex.org/W2110958718', 'https://openalex.org/W6780328113', 'https://openalex.org/W2132852532', 'https://openalex.org/W1989044351', 'https://openalex.org/W4247178956', 'https://openalex.org/W2975059944', 'https://openalex.org/W2888800758', 'https://openalex.org/W2026764577', 'https://openalex.org/W6608280181', 'https://openalex.org/W6630560557', 'https://openalex.org/W2905623858', 'https://openalex.org/W1500567887', 'https://openalex.org/W1964485490', 'https://openalex.org/W6630410620', 'https://openalex.org/W1562911371', 'https://openalex.org/W2160487945', 'https://openalex.org/W3082004699', 'https://openalex.org/W6683384601', 'https://openalex.org/W6713645886', 'https://openalex.org/W3005274572', 'https://openalex.org/W6678778180', 'https://openalex.org/W1965192788', 'https://openalex.org/W1980862600', 'https://openalex.org/W7028590591', 'https://openalex.org/W2030512997', 'https://openalex.org/W3118210634', 'https://openalex.org/W6679269181', 'https://openalex.org/W2059168261', 'https://openalex.org/W2154600605', 'https://openalex.org/W2811139869', 'https://openalex.org/W2902328740', 'https://openalex.org/W2931316642', 'https://openalex.org/W2160464066', 'https://openalex.org/W6682685583', 'https://openalex.org/W1897242287', 'https://openalex.org/W7064937209', 'https://openalex.org/W3125955384', 'https://openalex.org/W2419654175', 'https://openalex.org/W2135943618', 'https://openalex.org/W2990241049', 'https://openalex.org/W2102040782', 'https://openalex.org/W4246559809', 'https://openalex.org/W2145810838', 'https://openalex.org/W6658726316', 'https://openalex.org/W1903073458', 'https://openalex.org/W2960404386', 'https://openalex.org/W2140025977', 'https://openalex.org/W1507558854', 'https://openalex.org/W1515851193', 'https://openalex.org/W4212995409', 'https://openalex.org/W2154521990', 'https://openalex.org/W4239576168', 'https://openalex.org/W2045031658', 'https://openalex.org/W2168488947', 'https://openalex.org/W3138659559', 'https://openalex.org/W2089883580', 'https://openalex.org/W4292779060', 'https://openalex.org/W4250098443', 'https://openalex.org/W3080620940', 'https://openalex.org/W4252588748', 'https://openalex.org/W2003867956', 'https://openalex.org/W2405756170', 'https://openalex.org/W1971771135', 'https://openalex.org/W4287765185', 'https://openalex.org/W4297798492', 'https://openalex.org/W2962753610', 'https://openalex.org/W2979476256', 'https://openalex.org/W2981851019', 'https://openalex.org/W2996728628', 'https://openalex.org/W2158286954', 'https://openalex.org/W4235553294', 'https://openalex.org/W4287721377', 'https://openalex.org/W2073475009', 'https://openalex.org/W4251435902', 'https://openalex.org/W2996428491', 'https://openalex.org/W4249366912', 'https://openalex.org/W4236132222']",2021-09-01
https://openalex.org/W4372260408,https://doi.org/10.1109/icassp49357.2023.10096854,Self-Supervised Accent Learning for Under-Resourced Accents Using Native Language Data,"In this paper, we propose a novel method to improve the accuracy of an English speech recognizer for a target accent using the corresponding native language data. Collecting labeled data for all accents of English to train an end-to-end neural speech recognizer for English is a difficult and expensive task. Also, finding a pool of representative English speakers for any arbitrary accent to collect unlabeled data can be a difficult task. However, collecting unlabeled speech data for any native language is a much simpler task. It is important to note that the accents of most non-native English speakers are heavily biased by the co-articulation of sounds in their own native language. In view of this, we propose to use unlabeled native language data to learn self-supervised representations during the pre-training stage. The pre-trained model is then fine-tuned using limited labeled English data for the target accent. Experiments using native language data to pre-train an English recognizer followed by fine-tuning using target accented English show significant improvements in word error rates on four different accents (Great Britain, Korean, Chinese, Spanish).","['https://openalex.org/W6679436768', 'https://openalex.org/W4385245566', 'https://openalex.org/W3008587939', 'https://openalex.org/W6780218876', 'https://openalex.org/W3095173472', 'https://openalex.org/W2946417913', 'https://openalex.org/W6770506093', 'https://openalex.org/W3209059054', 'https://openalex.org/W3015356564', 'https://openalex.org/W6792224355', 'https://openalex.org/W3095410713', 'https://openalex.org/W3162812479', 'https://openalex.org/W3112702554', 'https://openalex.org/W2972798094', 'https://openalex.org/W3162061711', 'https://openalex.org/W2964099675', 'https://openalex.org/W3026041220', 'https://openalex.org/W3044481399', 'https://openalex.org/W6770514103', 'https://openalex.org/W2933138175', 'https://openalex.org/W6779192484', 'https://openalex.org/W6777781272', 'https://openalex.org/W6771467084', 'https://openalex.org/W2895676041', 'https://openalex.org/W3137832819', 'https://openalex.org/W3030437843', 'https://openalex.org/W2991213871', 'https://openalex.org/W2130942839', 'https://openalex.org/W2988736778', 'https://openalex.org/W3036601975']",2023-05-05
https://openalex.org/W4221140961,https://doi.org/10.1109/jstsp.2022.3200909,Are Discrete Units Necessary for Spoken Language Modeling?,"Recent work in spoken language modeling shows the possibility of learning a language unsupervisedly from raw audio without any text labels. The approach relies first on transforming the audio into a sequence of discrete units (or pseudo-text) and then training a language model directly on such pseudo-text. Is such a discrete bottleneck necessary, potentially introducing irreversible errors in the encoding of the speech signal, or could we learn a language model without discrete units at all? In this work, we study the role of discrete versus continuous representations in spoken language modeling. We show that discretization is indeed essential for good results in spoken language modeling. We show that discretization removes linguistically irrelevant information from the continuous features, helping to improve language modeling performances. On the basis of this study, we train a language model on the discrete units of the HuBERT features, reaching new state-of-the-art results in the lexical, syntactic and semantic metrics of the Zero Resource Speech Challenge 2021 (Track 1 - Speech Only).","['https://openalex.org/W6778883912', 'https://openalex.org/W2896457183', 'https://openalex.org/W6766673545', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W4226033575', 'https://openalex.org/W6786696081', 'https://openalex.org/W3197259906', 'https://openalex.org/W3198815374', 'https://openalex.org/W3015356564', 'https://openalex.org/W6769196770', 'https://openalex.org/W2973049979', 'https://openalex.org/W6790356757', 'https://openalex.org/W2964243274', 'https://openalex.org/W1494198834', 'https://openalex.org/W6844194202', 'https://openalex.org/W3016181583', 'https://openalex.org/W2933138175', 'https://openalex.org/W3088059392', 'https://openalex.org/W3024308166', 'https://openalex.org/W4287887366']",2022-08-23
https://openalex.org/W4225311910,https://doi.org/10.1109/icassp43922.2022.9746012,End-to-End Low Resource Keyword Spotting Through Character Recognition and Beam-Search Re-Scoring,"This paper describes an end-to-end approach to perform keyword spotting with a pre-trained acoustic model that uses recurrent neural networks and connectionist temporal classification loss. Our approach is specifically designed for low-resource keyword spotting tasks where extremely small amounts of in-domain data are available to train the system. The pre-trained model, largely used in ASR tasks, is fine-tuned on in-domain audio recordings. In inference the model output is matched against the set of predefined keywords using a beam-search re-scoring based on the edit distance.We demonstrate that this approach significantly outperforms the best state-of-the art systems on a well known keyword spotting benchmark, namely ""google speech commands"". Moreover, com-pared against state-of-the-art methods, our proposed approach is extremely robust in case of limited in domain training material. We show that a very small performance reduction is observed when fine tuning with a very small fraction (around 5%) of the training set.We report an extensive set of experiments on two keyword spotting tasks, varying training sizes and correlating keyword classification accuracy with character error rates provided by the system. We also report an ablation study to assess on the contribution of the out-of-domain pre-training and of the beam-search re-scoring.","['https://openalex.org/W2973049979', 'https://openalex.org/W6631190155', 'https://openalex.org/W3196974791', 'https://openalex.org/W2148116692', 'https://openalex.org/W6798439660', 'https://openalex.org/W3015356564', 'https://openalex.org/W6769196770', 'https://openalex.org/W1494604982', 'https://openalex.org/W3019546258', 'https://openalex.org/W6638749077', 'https://openalex.org/W6774097463', 'https://openalex.org/W3196509839', 'https://openalex.org/W1494198834', 'https://openalex.org/W1553469512', 'https://openalex.org/W2143612262', 'https://openalex.org/W6675365184', 'https://openalex.org/W2612800309', 'https://openalex.org/W6753916174', 'https://openalex.org/W6688089860', 'https://openalex.org/W3160200750', 'https://openalex.org/W6729959887', 'https://openalex.org/W6655657813', 'https://openalex.org/W6750665317', 'https://openalex.org/W3008790380', 'https://openalex.org/W6678540272', 'https://openalex.org/W6718561954', 'https://openalex.org/W6739901393', 'https://openalex.org/W6775319061', 'https://openalex.org/W6770506093', 'https://openalex.org/W3198035615', 'https://openalex.org/W3196496149', 'https://openalex.org/W6788809509', 'https://openalex.org/W2888641632', 'https://openalex.org/W1828163288', 'https://openalex.org/W4294621971', 'https://openalex.org/W4287864706', 'https://openalex.org/W2979476256', 'https://openalex.org/W2991213871', 'https://openalex.org/W1522301498', 'https://openalex.org/W2887979278', 'https://openalex.org/W2102113734', 'https://openalex.org/W3015210390', 'https://openalex.org/W2021289069', 'https://openalex.org/W2797583228', 'https://openalex.org/W4385245566', 'https://openalex.org/W2442329935', 'https://openalex.org/W2212465773', 'https://openalex.org/W4287071136', 'https://openalex.org/W3119913666', 'https://openalex.org/W2122797512']",2022-04-27
https://openalex.org/W4382935115,https://doi.org/10.1109/issc59246.2023.10162078,Well Said: An Analysis of the Speech Characteristics in the LibriSpeech Corpus,"Recent trends in speech quality research have shown the effectiveness models trained with large datasets of unlabelled natural speech. The LibriSpeech corpus contains approximately 1000 hours of English language read audiobooks and has been a popular dataset for many data driven speech technology models from automatic speech recognition to accent recognition and speech quality prediction. While the curators of LibriSpeech balanced the dataset to ensure there was a balance of speakers and content without overlaps, speech characteristics were not a design consideration. In this paper we use six algorithms to analyse speech for pitch, intensity and rate across the seven subsets of the LibriSpeech corpus. We find a good distribution between speakers and within some speakers for the characteristics tested. We show that speech characteristics are well balanced across subsets. We conclude that this validation makes LibriSpeech corpus to assist in the development of an objective model for synthetic speech quality prediction.","['https://openalex.org/W3015338123', 'https://openalex.org/W6779337556', 'https://openalex.org/W4280596009', 'https://openalex.org/W6843673214', 'https://openalex.org/W2191779130', 'https://openalex.org/W3016160783', 'https://openalex.org/W6798326965', 'https://openalex.org/W6840412704', 'https://openalex.org/W3202278141', 'https://openalex.org/W6810643248', 'https://openalex.org/W3015499614', 'https://openalex.org/W3015356564', 'https://openalex.org/W3014344307', 'https://openalex.org/W2091425152', 'https://openalex.org/W2118774185', 'https://openalex.org/W2962866891', 'https://openalex.org/W2984591755', 'https://openalex.org/W2085628288', 'https://openalex.org/W4296068974', 'https://openalex.org/W6780218876', 'https://openalex.org/W4235201968', 'https://openalex.org/W4296069264', 'https://openalex.org/W3198275944', 'https://openalex.org/W6845760781', 'https://openalex.org/W1494198834', 'https://openalex.org/W3082130377', 'https://openalex.org/W4294619240', 'https://openalex.org/W1495955203', 'https://openalex.org/W4392981073', 'https://openalex.org/W3036601975', 'https://openalex.org/W4382935141', 'https://openalex.org/W3182954816', 'https://openalex.org/W7336839', 'https://openalex.org/W4287761884']",2023-06-13
https://openalex.org/W4390482765,https://doi.org/10.3390/electronics13010190,Adapting Pre-Trained Self-Supervised Learning Model for Speech Recognition with Light-Weight Adapters,"Self-supervised learning (SSL) is an effective way of learning rich and transferable speech representations from unlabeled data to benefit downstream tasks. However, effectively incorporating a pre-trained SSL model into an automatic speech recognition (ASR) system remains challenging. In this paper, we propose a network architecture with light-weight adapters to adapt a pre-trained SSL model for an end-to-end (E2E) ASR. An adapter is introduced in each SSL network layer and trained on the downstream ASR task, while the parameters of the pre-trained SSL network layers remain unchanged. By carrying over all pre-trained parameters, we avoid the catastrophic forgetting problem. At the same time, we allow the network to quickly adapt to ASR task with light-weight adapters. The experiments using LibriSpeech and Wall Street Journal (WSJ) datasets show that (1) the proposed adapter-based fine-tuning consistently outperforms full-fledged training in low-resource scenarios, with up to 17.5%/12.2% relative word error rate (WER) reduction on the 10 min LibriSpeech split; (2) the adapter-based adaptation also shows competitive performance in high-resource scenarios, which further validates the effectiveness of the adapters.","['https://openalex.org/W2766219058', 'https://openalex.org/W2327501763', 'https://openalex.org/W2102113734', 'https://openalex.org/W6623517193', 'https://openalex.org/W2962824709', 'https://openalex.org/W4221161761', 'https://openalex.org/W2982223350', 'https://openalex.org/W3016011332', 'https://openalex.org/W3160345865', 'https://openalex.org/W3100270690', 'https://openalex.org/W3035202887', 'https://openalex.org/W2973049979', 'https://openalex.org/W2973157397', 'https://openalex.org/W3015213852', 'https://openalex.org/W3198608154', 'https://openalex.org/W3096485810', 'https://openalex.org/W6780361010', 'https://openalex.org/W3209984917', 'https://openalex.org/W3015356564', 'https://openalex.org/W3003875258', 'https://openalex.org/W3122931219', 'https://openalex.org/W1682403713', 'https://openalex.org/W3175604467', 'https://openalex.org/W4221148459', 'https://openalex.org/W4319586905', 'https://openalex.org/W2995181338', 'https://openalex.org/W1494198834', 'https://openalex.org/W2024490156', 'https://openalex.org/W3206252155', 'https://openalex.org/W3202370288', 'https://openalex.org/W4224933800', 'https://openalex.org/W3049256661', 'https://openalex.org/W4389799531', 'https://openalex.org/W3171007011', 'https://openalex.org/W3173783447', 'https://openalex.org/W179875071', 'https://openalex.org/W2950813464', 'https://openalex.org/W3016181583', 'https://openalex.org/W2616957565', 'https://openalex.org/W4312884055', 'https://openalex.org/W4390190100', 'https://openalex.org/W4385571322', 'https://openalex.org/W6739901393', 'https://openalex.org/W2194775991', 'https://openalex.org/W2908336025', 'https://openalex.org/W4226278833', 'https://openalex.org/W3015265920', 'https://openalex.org/W3026041220', 'https://openalex.org/W3160235762', 'https://openalex.org/W2962780374', 'https://openalex.org/W3041561163', 'https://openalex.org/W3155427814']",2024-01-01
https://openalex.org/W4402255177,https://doi.org/10.55662/jst.2023.4403,Beyond Labels: A Comprehensive Review of Self-Supervised Learning and Intrinsic Data Properties,"Self-supervised learning (SSL) has become a transformative approach in the field of machine learning, offering a powerful means to harness the vast amounts of unlabeled data available across various domains. By creating auxiliary tasks that generate supervisory signals directly from the data, SSL mitigates the dependency on large, labeled datasets, thereby expanding the applicability of machine learning models. This paper provides a comprehensive exploration of SSL techniques applied to diverse data types, including images, text, audio, and time-series data. We delve into the underlying principles that drive SSL, examine common methodologies, and highlight specific algorithms tailored to each data type. Additionally, we address the unique challenges encountered in applying SSL across different domains and propose future research directions that could further enhance the capabilities and effectiveness of SSL. Through this analysis, we underscore SSL's potential to significantly advance the development of robust, generalizable models capable of tackling complex real-world problems.","['https://openalex.org/W3114632476', 'https://openalex.org/W4291023040', 'https://openalex.org/W4283697304', 'https://openalex.org/W6743229011', 'https://openalex.org/W2973049979', 'https://openalex.org/W3005680577', 'https://openalex.org/W3171007011', 'https://openalex.org/W3035524453', 'https://openalex.org/W3035060554', 'https://openalex.org/W2963341956', 'https://openalex.org/W3041561163', 'https://openalex.org/W3015356564', 'https://openalex.org/W4281492411', 'https://openalex.org/W3190152617', 'https://openalex.org/W2920777619', 'https://openalex.org/W4221163898', 'https://openalex.org/W2963465221', 'https://openalex.org/W4289946024', 'https://openalex.org/W4221145109', 'https://openalex.org/W6926573507']",2023-08-20
https://openalex.org/W4394785928,https://doi.org/10.1101/2024.04.10.24305599,Analyzing wav2vec embedding in Parkinson’s disease speech: A study on cross-database classification and regression tasks,"Abstract Advancements in deep learning speech representations have facilitated the effective use of extensive datasets comprised of unlabeled speech signals, and have achieved success in modeling tasks associated with Parkinson’s disease (PD) with minimal annotated data. This study focuses on PD non-fine-tuned wav2vec 1.0 architecture. Utilizing features derived from wav2vec embedding, we develop machine learning models tailored for clinically relevant PD speech diagnosis tasks, such as cross-database classification and regression to predict demographic and articulation characteristics, for instance, modeling the subjects’ age and number of characters per second. The primary aim is to conduct feature importance analysis on both classification and regression tasks, investigating whether latent discrete speech representations in PD are shared across models, particularly for related tasks. The proposed wav2vec-based models were evaluated on PD versus healthy controls using three multi-language-task PD datasets. Results indicated that wav2vec accurately detected PD based on speech, outperforming feature extraction using mel-frequency cepstral coefficients in the proposed cross-database scenarios. Furthermore, wav2vec proved effective in regression, modeling various quantitative speech characteristics related to intelligibility and aging. Subsequent analysis of important features, obtained using scikit-learn feature importance built-in tools and the Shapley additive explanations method, examined the presence of significant overlaps between classification and regression models. The feature importance experiments discovered shared features across trained models, with increased sharing for related tasks, further suggesting that wav2vec contributes to improved generalizability. In conclusion, the study proposes wav2vec embedding as a promising step toward a speech-based universal model to assist in the evaluation of PD.","['https://openalex.org/W2908201961', 'https://openalex.org/W2951934944', 'https://openalex.org/W4220814249', 'https://openalex.org/W4378576117', 'https://openalex.org/W4323568500', 'https://openalex.org/W4319298024', 'https://openalex.org/W3026400514', 'https://openalex.org/W4286797312', 'https://openalex.org/W3036601975', 'https://openalex.org/W3015356564', 'https://openalex.org/W4287235826', 'https://openalex.org/W4372338196', 'https://openalex.org/W4387378260', 'https://openalex.org/W4287752738', 'https://openalex.org/W2153761521', 'https://openalex.org/W4211044101', 'https://openalex.org/W2153797245', 'https://openalex.org/W2066674357', 'https://openalex.org/W3196337184', 'https://openalex.org/W4388766108', 'https://openalex.org/W3207031444', 'https://openalex.org/W3203161143', 'https://openalex.org/W4366084182', 'https://openalex.org/W3016088911', 'https://openalex.org/W2951042025', 'https://openalex.org/W3194973925', 'https://openalex.org/W3094211490', 'https://openalex.org/W4206713267', 'https://openalex.org/W4304688131', 'https://openalex.org/W4372334073', 'https://openalex.org/W3211805594', 'https://openalex.org/W2746624190', 'https://openalex.org/W3153178673', 'https://openalex.org/W4212840609', 'https://openalex.org/W3037887574', 'https://openalex.org/W4385571422', 'https://openalex.org/W4382918145', 'https://openalex.org/W3044967013', 'https://openalex.org/W4385822952', 'https://openalex.org/W2890964092', 'https://openalex.org/W4296068413', 'https://openalex.org/W4287119707', 'https://openalex.org/W4387580450', 'https://openalex.org/W3015705854', 'https://openalex.org/W3039358396', 'https://openalex.org/W4382766228', 'https://openalex.org/W4382137576', 'https://openalex.org/W4387165835', 'https://openalex.org/W3170940710', 'https://openalex.org/W4385822692', 'https://openalex.org/W4390030622', 'https://openalex.org/W4295951577', 'https://openalex.org/W4387306788', 'https://openalex.org/W4210676598', 'https://openalex.org/W2767111132', 'https://openalex.org/W2191779130', 'https://openalex.org/W3016181583', 'https://openalex.org/W4386083664', 'https://openalex.org/W4385822779', 'https://openalex.org/W4366714310', 'https://openalex.org/W4225091506', 'https://openalex.org/W4385728310', 'https://openalex.org/W4283716267', 'https://openalex.org/W4225862430', 'https://openalex.org/W3096196861']",2024-04-12
https://openalex.org/W4407545490,https://doi.org/10.1145/3716892,End-to-end Trajectory Generation - Contrasting Deep Generative Models and Language Models,"Due to the limited availability of actual large-scale datasets, realistic synthetic trajectory data play a crucial role in various research domains, including spatiotemporal data mining and data management, and domain-driven research related to transportation planning and urban analytics. Existing generation methods rely on predefined heuristics and cannot learn the unknown underlying generative mechanisms. This work introduces two end-to-end approaches for trajectory generation. The first approach comprises deep generative VAE-like models that factorize global and local semantics (habits vs. random routing change). We further enhance this approach by developing novel inference strategies based on variational inference and constrained optimization to ensure the validity of spatiotemporal aspects. This novel deep neural network architecture implements generative and inference models with dynamic latent priors. The second approach introduces a language model (LM) inspired generation as another benchmarking and foundational approach. The LM-inspired approach conceptualizes trajectories as sentences with the aim of predicting the likelihood of subsequent locations on a trajectory, given the locations as context. As a result, the LM-inspired approach implicitly learns the inherent spatiotemporal structure and other embedded semantics within the trajectories. These proposed methods demonstrate substantial quantitative and qualitative improvements over existing approaches, as evidenced by extensive experimental evaluations.","['https://openalex.org/W3015356564', 'https://openalex.org/W3048236912', 'https://openalex.org/W2331545288', 'https://openalex.org/W3034784236', 'https://openalex.org/W2922439754', 'https://openalex.org/W2079735306', 'https://openalex.org/W2753738274', 'https://openalex.org/W4309651445', 'https://openalex.org/W4394729861', 'https://openalex.org/W4206566734', 'https://openalex.org/W2772068906', 'https://openalex.org/W2795016801', 'https://openalex.org/W3029191757', 'https://openalex.org/W4309651995', 'https://openalex.org/W2774941429', 'https://openalex.org/W1993233092', 'https://openalex.org/W3095980543', 'https://openalex.org/W4385880144', 'https://openalex.org/W2126101691', 'https://openalex.org/W2173289914', 'https://openalex.org/W3033117456', 'https://openalex.org/W2026730210', 'https://openalex.org/W4309651822', 'https://openalex.org/W4309652856', 'https://openalex.org/W4385270475', 'https://openalex.org/W2967803341', 'https://openalex.org/W3126748244', 'https://openalex.org/W3198377975', 'https://openalex.org/W2608239929', 'https://openalex.org/W4205171160', 'https://openalex.org/W2766642606', 'https://openalex.org/W2750779823']",2025-02-13
https://openalex.org/W4295938830,https://doi.org/10.1007/978-3-031-16270-1_25,Transformer-based Automatic Speech Recognition of Formal and Colloquial\n Czech in MALACH Project,"Czech is a very specific language due to its large differences between the\nformal and the colloquial form of speech. While the formal (written) form is\nused mainly in official documents, literature, and public speeches, the\ncolloquial (spoken) form is used widely among people in casual speeches. This\ngap introduces serious problems for ASR systems, especially when training or\nevaluating ASR models on datasets containing a lot of colloquial speech, such\nas the MALACH project. In this paper, we are addressing this problem in the\nlight of a new paradigm in end-to-end ASR systems -- recently introduced\nself-supervised audio Transformers. Specifically, we are investigating the\ninfluence of colloquial speech on the performance of Wav2Vec 2.0 models and\ntheir ability to transcribe colloquial speech directly into formal transcripts.\nWe are presenting results with both formal and colloquial forms in the training\ntranscripts, language models, and evaluation transcripts.\n","['https://openalex.org/W2979826702', 'https://openalex.org/W3198429080', 'https://openalex.org/W1553630706', 'https://openalex.org/W3209984917', 'https://openalex.org/W2134800885', 'https://openalex.org/W1595275880', 'https://openalex.org/W3015356564', 'https://openalex.org/W3213029956', 'https://openalex.org/W3119308075', 'https://openalex.org/W3036601975', 'https://openalex.org/W3209059054', 'https://openalex.org/W4385245566', 'https://openalex.org/W3200506395', 'https://openalex.org/W2896457183', 'https://openalex.org/W296228825', 'https://openalex.org/W2127141656', 'https://openalex.org/W3041561163', 'https://openalex.org/W3169320628', 'https://openalex.org/W2127923419']",2022-06-15
https://openalex.org/W3142867067,https://doi.org/10.21437/interspeech.2021-2013,Utilizing Self-Supervised Representations for MOS Prediction,"Speech quality assessment has been a critical issue in speech processing for decades. Existing automatic evaluations usually require clean references or parallel ground truth data, which is infeasible when the amount of data soars. Subjective tests, on the other hand, do not need any additional clean or parallel data and correlates better to human perception. However, such a test is expensive and time-consuming because crowd work is necessary. It thus becomes highly desired to develop an automatic evaluation approach that correlates well with human perception while not requiring ground truth data. In this paper, we use self-supervised pre-trained models for MOS prediction. We show their representations can distinguish between clean and noisy audios. Then, we fine-tune these pre-trained models followed by simple linear layers in an end-to-end manner. The experiment results showed that our framework outperforms the two previous state-of-the-art models by a significant improvement on Voice Conversion Challenge 2018 and achieves comparable or superior performance on Voice Conversion Challenge 2016. We also conducted an ablation study to further investigate how each module benefits the task. The experiment results are implemented and reproducible with publicly available toolkits.","['https://openalex.org/W2557915412', 'https://openalex.org/W2140651276', 'https://openalex.org/W3095953768', 'https://openalex.org/W2187089797', 'https://openalex.org/W2078483536', 'https://openalex.org/W2098507061', 'https://openalex.org/W2096270777', 'https://openalex.org/W2796495654', 'https://openalex.org/W1494198834', 'https://openalex.org/W2473388484', 'https://openalex.org/W2842511635', 'https://openalex.org/W2336585117', 'https://openalex.org/W2107860279', 'https://openalex.org/W2949844463', 'https://openalex.org/W3015356564', 'https://openalex.org/W2797563284', 'https://openalex.org/W2075843680', 'https://openalex.org/W2995181338', 'https://openalex.org/W3096918678', 'https://openalex.org/W3135703935', 'https://openalex.org/W2972394484', 'https://openalex.org/W2887924216', 'https://openalex.org/W2922332774', 'https://openalex.org/W3041561163', 'https://openalex.org/W2219249508', 'https://openalex.org/W1778065289', 'https://openalex.org/W3112495938', 'https://openalex.org/W1552314771', 'https://openalex.org/W2972541922', 'https://openalex.org/W2511311723', 'https://openalex.org/W3099782249', 'https://openalex.org/W2067295501', 'https://openalex.org/W2972943112', 'https://openalex.org/W2963403868', 'https://openalex.org/W1995945562']",2021-08-27
https://openalex.org/W3217463528,https://doi.org/10.1155/2021/9915130,Leveraging Multimodal Out-of-Domain Information to Improve Low-Resource Speech Translation,"Speech translation (ST) is a bimodal conversion task from source speech to the target text. Generally, deep learning-based ST systems require sufficient training data to obtain a competitive result, even with a state-of-the-art model. However, the training data is usually unable to meet the completeness condition due to the small sample problems. Most low-resource ST tasks improve data integrity with a single model, but this optimization has a single dimension and limited effectiveness. In contrast, multimodality is introduced to leverage different dimensions of data features for multiperspective modeling. This approach mutually addresses the gaps in the different modalities to enhance the representation of the data and improve the utilization of the training samples. Therefore, it is a new challenge to leverage the enormous multimodal out-of-domain information to improve the low-resource tasks. This paper describes how to use multimodal out-of-domain information to improve low-resource models. First, we propose a low-resource ST framework to reconstruct large-scale label-free audio by combining self-supervised learning. At the same time, we introduce a machine translation (MT) pretraining model to complement text embedding and fine-tune decoding. In addition, we analyze the similarity at the decoder side. We reduce multimodal invalid pseudolabels by performing random depth pruning in the similarity layer to minimize error propagation and use additional CTC loss in the nonsimilarity layer to optimize the ensemble loss. Finally, we study the weighting ratio of the fusion technique in the multimodal decoder. Our experiment results show that the proposed method is promising for low-resource ST, with improvements of up to +3.6 BLEU points compared to baseline low-resource ST models.","['https://openalex.org/W2327501763', 'https://openalex.org/W2133564696', 'https://openalex.org/W2788575190', 'https://openalex.org/W2995732811', 'https://openalex.org/W2972448360', 'https://openalex.org/W3027207958', 'https://openalex.org/W3018997018', 'https://openalex.org/W2973620348', 'https://openalex.org/W2582956876', 'https://openalex.org/W2948845926', 'https://openalex.org/W3035490255', 'https://openalex.org/W3184351855', 'https://openalex.org/W2899716505', 'https://openalex.org/W2982129078', 'https://openalex.org/W3008837824', 'https://openalex.org/W2605131327', 'https://openalex.org/W2786891429', 'https://openalex.org/W3162037819', 'https://openalex.org/W2891066653', 'https://openalex.org/W3096490862', 'https://openalex.org/W3008125272', 'https://openalex.org/W3008549139', 'https://openalex.org/W3196509775', 'https://openalex.org/W3160525311', 'https://openalex.org/W3017454464', 'https://openalex.org/W3121299949', 'https://openalex.org/W3006988520', 'https://openalex.org/W3036982689', 'https://openalex.org/W2975059944', 'https://openalex.org/W3036601975', 'https://openalex.org/W2963736842', 'https://openalex.org/W2963779652', 'https://openalex.org/W3007142233', 'https://openalex.org/W3015633994', 'https://openalex.org/W2997436923', 'https://openalex.org/W2946233749', 'https://openalex.org/W3156298815', 'https://openalex.org/W2194775991', 'https://openalex.org/W3015356564', 'https://openalex.org/W2908336025', 'https://openalex.org/W3156553470', 'https://openalex.org/W3146703290', 'https://openalex.org/W2972451902', 'https://openalex.org/W3162249256', 'https://openalex.org/W2975381464', 'https://openalex.org/W2962780374', 'https://openalex.org/W6631362777', 'https://openalex.org/W2407080277', 'https://openalex.org/W2936774411', 'https://openalex.org/W6631190155', 'https://openalex.org/W4300558631', 'https://openalex.org/W2936969148', 'https://openalex.org/W3094686723', 'https://openalex.org/W2962784628', 'https://openalex.org/W2982666195', 'https://openalex.org/W3034672970', 'https://openalex.org/W2964161387', 'https://openalex.org/W2981991061', 'https://openalex.org/W3015440307', 'https://openalex.org/W1524333225', 'https://openalex.org/W3035464238', 'https://openalex.org/W2941814890', 'https://openalex.org/W3173767661', 'https://openalex.org/W3054645415', 'https://openalex.org/W3100859887', 'https://openalex.org/W2952167535', 'https://openalex.org/W2950151997', 'https://openalex.org/W2998386507', 'https://openalex.org/W2949328740', 'https://openalex.org/W4297808394', 'https://openalex.org/W4385245566', 'https://openalex.org/W3102811925', 'https://openalex.org/W4394649814', 'https://openalex.org/W2952809536', 'https://openalex.org/W4287374065', 'https://openalex.org/W1522301498', 'https://openalex.org/W3034571331', 'https://openalex.org/W3034332156', 'https://openalex.org/W3168212167', 'https://openalex.org/W3035207248', 'https://openalex.org/W2964172053', 'https://openalex.org/W2996428491']",2021-11-26
https://openalex.org/W4206221133,https://doi.org/10.1109/aciiw52867.2021.9666396,Using Multimodal Transformers in Affective Computing,"Having devices capable of understanding human emotions will significantly improve the way people interact with them. Moreover, if those devices are capable of influencing the emotions of users in a positive way, this will improve their quality of life, especially for frail or dependent users. A first step towards this goal is improving the performance of emotion recognition systems. Specifically, using a multimodal approach is appealing, as the availability of different signals is growing. We believe that it is important to incorporate new architectures and techniques like the Transformer and BERT, and to investigate how to use them in a multimodal setting. Also, it is essential to develop self-supervised learning techniques to take advantage of the considerable quantity of unlabeled data available nowadays. In this extended abstract, we present our research in those directions.","['https://openalex.org/W2002055708', 'https://openalex.org/W2122098299', 'https://openalex.org/W3117020206', 'https://openalex.org/W3183235320', 'https://openalex.org/W3005387090', 'https://openalex.org/W3082142255', 'https://openalex.org/W2470957930', 'https://openalex.org/W2045528981', 'https://openalex.org/W3003257820', 'https://openalex.org/W6675354045', 'https://openalex.org/W2980927909', 'https://openalex.org/W2778978785', 'https://openalex.org/W3046832736', 'https://openalex.org/W6739901393', 'https://openalex.org/W2977259558', 'https://openalex.org/W3089557188', 'https://openalex.org/W3047607035', 'https://openalex.org/W2964051877', 'https://openalex.org/W3084283759', 'https://openalex.org/W3088631780', 'https://openalex.org/W3034266838', 'https://openalex.org/W3127166971', 'https://openalex.org/W2899427170', 'https://openalex.org/W3034520808', 'https://openalex.org/W3160016036', 'https://openalex.org/W2973049979', 'https://openalex.org/W2783433009', 'https://openalex.org/W2919854899', 'https://openalex.org/W2991276178', 'https://openalex.org/W6774822837', 'https://openalex.org/W3034633006', 'https://openalex.org/W6680300913', 'https://openalex.org/W2981851019', 'https://openalex.org/W6755207826', 'https://openalex.org/W2146334809', 'https://openalex.org/W2731964405', 'https://openalex.org/W2883409523', 'https://openalex.org/W3015356564', 'https://openalex.org/W2943183446', 'https://openalex.org/W2945616044', 'https://openalex.org/W2465534249', 'https://openalex.org/W4297468970', 'https://openalex.org/W3109709077', 'https://openalex.org/W2101234009', 'https://openalex.org/W4385245566', 'https://openalex.org/W3011425517', 'https://openalex.org/W2887997593', 'https://openalex.org/W2896457183', 'https://openalex.org/W2138857742']",2021-09-28
https://openalex.org/W4286908208,https://doi.org/10.21437/interspeech.2022-412,SCaLa: Supervised Contrastive Learning for End-to-End Speech Recognition,"End-to-end Automatic Speech Recognition (ASR) models are usually trained to optimize the loss of the whole token sequence, while neglecting explicit phonemic-granularity supervision.This could result in recognition errors due to similarphoneme confusion or phoneme reduction.To alleviate this problem, we propose a novel framework based on Supervised Contrastive Learning (SCaLa) to enhance phonemic representation learning for end-to-end ASR systems.Specifically, we extend the self-supervised Masked Contrastive Predictive Coding (MCPC) to a fully-supervised setting, where the supervision is applied in the following way.First, SCaLa masks variablelength encoder features according to phoneme boundaries given phoneme forced-alignment extracted from a pre-trained acoustic model; it then predicts the masked features via contrastive learning.The forced-alignment can provide phoneme labels to mitigate the noise introduced by positive-negative pairs in selfsupervised MCPC.Experiments on reading and spontaneous speech datasets show that our proposed approach achieves 2.8 and 1.4 points Character Error Rate (CER) absolute reductions compared to the baseline, respectively.","['https://openalex.org/W3008870153', 'https://openalex.org/W3160235762', 'https://openalex.org/W3015213852', 'https://openalex.org/W4311137818', 'https://openalex.org/W4297808394', 'https://openalex.org/W2962826786', 'https://openalex.org/W3093579165', 'https://openalex.org/W2973049979', 'https://openalex.org/W1524333225', 'https://openalex.org/W2963242190', 'https://openalex.org/W3036601975', 'https://openalex.org/W1556470778', 'https://openalex.org/W3197478142', 'https://openalex.org/W2972943112', 'https://openalex.org/W4285192675', 'https://openalex.org/W2327501763', 'https://openalex.org/W3005680577', 'https://openalex.org/W2982223350', 'https://openalex.org/W2979476256', 'https://openalex.org/W3034931686', 'https://openalex.org/W3169320628', 'https://openalex.org/W4288072840', 'https://openalex.org/W2193413348', 'https://openalex.org/W4287812705', 'https://openalex.org/W3015356564', 'https://openalex.org/W4210423654', 'https://openalex.org/W4307023467', 'https://openalex.org/W4287374065', 'https://openalex.org/W2750499125', 'https://openalex.org/W2981991061', 'https://openalex.org/W4226380987', 'https://openalex.org/W3181410678', 'https://openalex.org/W3023953056', 'https://openalex.org/W2752168051', 'https://openalex.org/W3095189764']",2022-09-16
https://openalex.org/W4390493729,https://doi.org/10.1109/cisp-bmei60920.2023.10373303,A Few-Shot Speech Keyword Spotting Method Based on Self-Supervise Learning,"Keyword spotting (KWS) plays a crucial role in enabling voice-based user interactions on smart devices. However, conventional KWS methods require a large number of predefined keywords to achieve acceptable detection accuracy, which users may find challenging to provide. In recent years, self-supervised training and large models have excelled in various audio tasks. Their general audio feature extraction capabilities align well with the low-resource nature and scalability requirements of KWS tasks. In this paper, we integrate self-supervised models with keyword transformers to tailor them for KWS tasks. Experiments show that our approach significantly outperforms previous supervised methods. Moreover, our method's advantages become even more pronounced under extremely limited resource conditions, which is of great importance for the rapid deployment of KWS systems.","['https://openalex.org/W2407023693', 'https://openalex.org/W2034940213', 'https://openalex.org/W3196509839', 'https://openalex.org/W3161425572', 'https://openalex.org/W3015639015', 'https://openalex.org/W4389520688', 'https://openalex.org/W6736057607', 'https://openalex.org/W3015399080', 'https://openalex.org/W3197564965', 'https://openalex.org/W6755207826', 'https://openalex.org/W6763701032', 'https://openalex.org/W6769627184', 'https://openalex.org/W6788335241', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W3198035615', 'https://openalex.org/W4281492411', 'https://openalex.org/W6760017818', 'https://openalex.org/W6739901393', 'https://openalex.org/W4375869243', 'https://openalex.org/W6844194202', 'https://openalex.org/W3015356564', 'https://openalex.org/W4226033575', 'https://openalex.org/W2973157397', 'https://openalex.org/W6729448088', 'https://openalex.org/W6784333009', 'https://openalex.org/W6750665317', 'https://openalex.org/W6757817989', 'https://openalex.org/W2888641632', 'https://openalex.org/W3097018422', 'https://openalex.org/W3119913666', 'https://openalex.org/W2908510526', 'https://openalex.org/W2797583228', 'https://openalex.org/W4385245566', 'https://openalex.org/W4288089799', 'https://openalex.org/W3036601975', 'https://openalex.org/W2896457183', 'https://openalex.org/W2547875792', 'https://openalex.org/W2970597249', 'https://openalex.org/W4297808394', 'https://openalex.org/W3094502228', 'https://openalex.org/W2979476256']",2023-10-28
https://openalex.org/W3031277321,https://doi.org/10.21437/interspeech.2020-2629,Improving Unsupervised Sparsespeech Acoustic Models with Categorical Reparameterization,"The Sparsespeech model is an unsupervised acoustic model that can generate discrete pseudo-labels for untranscribed speech. We extend the Sparsespeech model to allow for sampling over a random discrete variable, yielding pseudo-posteriorgrams. The degree of sparsity in this posteriorgram can be fully controlled after the model has been trained. We use the Gumbel-Softmax trick to approximately sample from a discrete distribution in the neural network and this allows us to train the network efficiently with standard backpropagation. The new and improved model is trained and evaluated on the Libri-Light corpus, a benchmark for ASR with limited or no supervision. The model is trained on 600h and 6000h of English read speech. We evaluate the improved model using the ABX error measure and a semi-supervised setting with 10h of transcribed speech. We observe a relative improvement of up to 31.4% on ABX error rates across speakers on the test set with the improved Sparsespeech model on 600h of speech data and further improvements when we scale the model to 6000h.","['https://openalex.org/W1494198834', 'https://openalex.org/W2842511635', 'https://openalex.org/W2995181338', 'https://openalex.org/W2547875792', 'https://openalex.org/W2972982164', 'https://openalex.org/W2593779438', 'https://openalex.org/W1524333225', 'https://openalex.org/W2127141656', 'https://openalex.org/W2520160253', 'https://openalex.org/W2963341956', 'https://openalex.org/W2395899413', 'https://openalex.org/W2979476256', 'https://openalex.org/W3015356564', 'https://openalex.org/W2947591107', 'https://openalex.org/W2940544976', 'https://openalex.org/W10548402']",2020-10-25
https://openalex.org/W3089824566,https://doi.org/10.48550/arxiv.2010.00578,Understanding Self-supervised Learning with Dual Deep Networks,"We propose a novel theoretical framework to understand contrastive self-supervised learning (SSL) methods that employ dual pairs of deep ReLU networks (e.g., SimCLR). First, we prove that in each SGD update of SimCLR with various loss functions, including simple contrastive loss, soft Triplet loss and InfoNCE loss, the weights at each layer are updated by a \emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. To further study what role the covariance operator plays and which features are learned in such a process, we model data generation and augmentation processes through a \emph{hierarchical latent tree model} (HLTM) and prove that the hidden neurons of deep ReLU networks can learn the latent variables in HLTM, despite the fact that the network receives \emph{no direct supervision} from these unobserved latent variables. This leads to a provable emergence of hierarchical features through the amplification of initially random selectivities through contrastive SSL. Extensive numerical studies justify our theoretical findings. Code is released in https://github.com/facebookresearch/luckmatters/tree/master/ssl.","['https://openalex.org/W2194775991', 'https://openalex.org/W3005680577', 'https://openalex.org/W2795783309', 'https://openalex.org/W3048030262', 'https://openalex.org/W3046208551', 'https://openalex.org/W2809090039', 'https://openalex.org/W3093929102', 'https://openalex.org/W2963341956', 'https://openalex.org/W3009561768', 'https://openalex.org/W3118608800', 'https://openalex.org/W2951881727', 'https://openalex.org/W3035060554', 'https://openalex.org/W1836465849', 'https://openalex.org/W2078626246', 'https://openalex.org/W3080342685', 'https://openalex.org/W3107668149', 'https://openalex.org/W2108598243', 'https://openalex.org/W2168263526', 'https://openalex.org/W2962973336', 'https://openalex.org/W3091905774', 'https://openalex.org/W3046882683', 'https://openalex.org/W2963417959', 'https://openalex.org/W2964121744', 'https://openalex.org/W2951585248', 'https://openalex.org/W2990873191', 'https://openalex.org/W3099206234', 'https://openalex.org/W2517360214', 'https://openalex.org/W2898365215', 'https://openalex.org/W2963504252', 'https://openalex.org/W3082701951', 'https://openalex.org/W2842511635', 'https://openalex.org/W3035524453', 'https://openalex.org/W2988618093', 'https://openalex.org/W3015356564', 'https://openalex.org/W2971043187', 'https://openalex.org/W3000127803', 'https://openalex.org/W2893242978', 'https://openalex.org/W2990408345', 'https://openalex.org/W3022061250', 'https://openalex.org/W2996383576', 'https://openalex.org/W2962930448', 'https://openalex.org/W2883725317', 'https://openalex.org/W2913939497', 'https://openalex.org/W2996141621', 'https://openalex.org/W2949517790', 'https://openalex.org/W3027758526', 'https://openalex.org/W2034276860', 'https://openalex.org/W3034781633', 'https://openalex.org/W3095121901', 'https://openalex.org/W2118858186', 'https://openalex.org/W2739485194', 'https://openalex.org/W3036867795']",2020-10-01
https://openalex.org/W3129009457,https://doi.org/10.48550/arxiv.2102.01192,Generative Spoken Language Modeling from Raw Audio,"We introduce Generative Spoken Language Modeling, the task of learning the acoustic and linguistic characteristics of a language from raw audio (no text, no labels), and a set of metrics to automatically evaluate the learned representations at acoustic and linguistic levels for both encoding and generation. We set up baseline systems consisting of a discrete speech encoder (returning pseudo-text units), a generative language model (trained on pseudo-text), and a speech decoder (generating a waveform from pseudo-text) all trained without supervision and validate the proposed metrics with human evaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that the number of discrete units (50, 100, or 200) matters in a task-dependent and encoder-dependent way, and that some combinations approach text-based systems.","['https://openalex.org/W2787560479', 'https://openalex.org/W2996383576', 'https://openalex.org/W3097787369', 'https://openalex.org/W3096216486', 'https://openalex.org/W2100768664', 'https://openalex.org/W3095698432', 'https://openalex.org/W2963341956', 'https://openalex.org/W3035202887', 'https://openalex.org/W2890983311', 'https://openalex.org/W2949382160', 'https://openalex.org/W1494198834', 'https://openalex.org/W3098403858', 'https://openalex.org/W3148040514', 'https://openalex.org/W3039910566', 'https://openalex.org/W2937090315', 'https://openalex.org/W2842511635', 'https://openalex.org/W2972374322', 'https://openalex.org/W2973157397', 'https://openalex.org/W3095361818', 'https://openalex.org/W3163296124', 'https://openalex.org/W3095948607', 'https://openalex.org/W2963618559', 'https://openalex.org/W2973049979', 'https://openalex.org/W3095292526', 'https://openalex.org/W3049206033', 'https://openalex.org/W2933138175', 'https://openalex.org/W2750248772', 'https://openalex.org/W3096323553', 'https://openalex.org/W2127141656', 'https://openalex.org/W2971274815', 'https://openalex.org/W2950180292', 'https://openalex.org/W3003875258', 'https://openalex.org/W2888911345', 'https://openalex.org/W2963403868', 'https://openalex.org/W2160473997', 'https://openalex.org/W2963799213', 'https://openalex.org/W3096359985', 'https://openalex.org/W2947445680', 'https://openalex.org/W3099782249', 'https://openalex.org/W3003750857', 'https://openalex.org/W3033038061', 'https://openalex.org/W3112034174', 'https://openalex.org/W2347098582', 'https://openalex.org/W3015213852', 'https://openalex.org/W3015265920', 'https://openalex.org/W2346964103', 'https://openalex.org/W3125087428', 'https://openalex.org/W2982223350', 'https://openalex.org/W2963300588', 'https://openalex.org/W2483390977', 'https://openalex.org/W2962850167', 'https://openalex.org/W2963456134', 'https://openalex.org/W2577366047', 'https://openalex.org/W3024040651', 'https://openalex.org/W3093096176', 'https://openalex.org/W2982399380', 'https://openalex.org/W3148101939', 'https://openalex.org/W2965373594', 'https://openalex.org/W3114436296', 'https://openalex.org/W3015356564', 'https://openalex.org/W2972943112']",2021-02-01
https://openalex.org/W3186596101,,Continual-wav2vec2: an Application of Continual Learning for Self-Supervised Automatic Speech Recognition.,"We present a method for continual learning of speech representations for multiple languages using self-supervised learning (SSL) and applying these for automatic speech recognition. There is an abundance of unannotated speech, so creating self-supervised representations from raw audio and finetuning on a small annotated datasets is a promising direction to build speech recognition systems. Wav2vec models perform SSL on raw audio in a pretraining phase and then finetune on a small fraction of annotated data. SSL models have produced state of the art results for ASR. However, these models are very expensive to pretrain with self-supervision. We tackle the problem of learning new language representations continually from audio without forgetting a previous language representation. We use ideas from continual learning to transfer knowledge from a previous task to speed up pretraining a new language task. Our continual-wav2vec2 model can decrease pretraining times by 32% when learning a new language task, and learn this new audio-language representation without forgetting previous language representation.","['https://openalex.org/W2941814890', 'https://openalex.org/W3170823761', 'https://openalex.org/W2996383576', 'https://openalex.org/W3037057938', 'https://openalex.org/W3025165719', 'https://openalex.org/W2583761661', 'https://openalex.org/W2963559848', 'https://openalex.org/W3198094329', 'https://openalex.org/W3099782249', 'https://openalex.org/W2113839990', 'https://openalex.org/W3015356564', 'https://openalex.org/W3016181583', 'https://openalex.org/W3093579165', 'https://openalex.org/W2426267443', 'https://openalex.org/W2060277733', 'https://openalex.org/W3101498587', 'https://openalex.org/W3102342027', 'https://openalex.org/W2952165242', 'https://openalex.org/W2970586779', 'https://openalex.org/W2462831000', 'https://openalex.org/W3134307371', 'https://openalex.org/W2963540014', 'https://openalex.org/W3116551962', 'https://openalex.org/W3097968133', 'https://openalex.org/W3030437843', 'https://openalex.org/W2765101016', 'https://openalex.org/W3110524561', 'https://openalex.org/W1494198834', 'https://openalex.org/W2737492962', 'https://openalex.org/W2973049979', 'https://openalex.org/W2547875792', 'https://openalex.org/W3153675281', 'https://openalex.org/W2473930607', 'https://openalex.org/W3002177189', 'https://openalex.org/W3008658065', 'https://openalex.org/W2963850662', 'https://openalex.org/W2933138175', 'https://openalex.org/W2964088867', 'https://openalex.org/W2187089797', 'https://openalex.org/W3160525311', 'https://openalex.org/W2902625698', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963211188', 'https://openalex.org/W2842511635', 'https://openalex.org/W2962724315']",2021-07-26
https://openalex.org/W3199443835,https://doi.org/10.48550/arxiv.2109.07349,Improving Accent Identification and Accented Speech Recognition Under a Framework of Self-supervised Learning,"Recently, self-supervised pre-training has gained success in automatic speech recognition (ASR). However, considering the difference between speech accents in real scenarios, how to identify accents and use accent features to improve ASR is still challenging. In this paper, we employ the self-supervised pre-training method for both accent identification and accented speech recognition tasks. For the former task, a standard deviation constraint loss (SDC-loss) based end-to-end (E2E) architecture is proposed to identify accents under the same language. As for accented speech recognition task, we design an accent-dependent ASR system, which can utilize additional accent input features. Furthermore, we propose a frame-level accent feature, which is extracted based on the proposed accent identification model and can be dynamically adjusted. We pre-train our models using 960 hours unlabeled LibriSpeech dataset and fine-tune them on AESRC2020 speech dataset. The experimental results show that our proposed accent-dependent ASR system is significantly ahead of the AESRC2020 baseline and achieves $6.5\%$ relative word error rate (WER) reduction compared with our accent-independent ASR system.","['https://openalex.org/W1595289649', 'https://openalex.org/W3099782249', 'https://openalex.org/W1494198834', 'https://openalex.org/W2150769028', 'https://openalex.org/W2405866807', 'https://openalex.org/W2889494795', 'https://openalex.org/W2963403868', 'https://openalex.org/W2973094925', 'https://openalex.org/W2962739339', 'https://openalex.org/W2095705004', 'https://openalex.org/W2938374794', 'https://openalex.org/W2748488820', 'https://openalex.org/W3034978746', 'https://openalex.org/W2996383576', 'https://openalex.org/W3015915933', 'https://openalex.org/W2933138175', 'https://openalex.org/W2294108103', 'https://openalex.org/W2963341956', 'https://openalex.org/W2111305191', 'https://openalex.org/W3015356564', 'https://openalex.org/W2973049979', 'https://openalex.org/W1975377661', 'https://openalex.org/W2962893195', 'https://openalex.org/W2295754094', 'https://openalex.org/W3139939832', 'https://openalex.org/W3016011332', 'https://openalex.org/W2964121744', 'https://openalex.org/W3130041558', 'https://openalex.org/W2890964092', 'https://openalex.org/W2603679025']",2021-09-15
https://openalex.org/W3197012690,https://doi.org/10.48550/arxiv.2109.01164,Scalable Data Annotation Pipeline for High-Quality Large Speech Datasets Development,"This paper introduces a human-in-the-loop (HITL) data annotation pipeline to generate high-quality, large-scale speech datasets. The pipeline combines human and machine advantages to more quickly, accurately, and cost-effectively annotate datasets with machine pre-labeling and fully manual auditing. Quality control mechanisms such as blind testing, behavior monitoring, and data validation have been adopted in the annotation pipeline to mitigate potential bias introduced by machine-generated labels. Our A/B testing and pilot results demonstrated the HITL pipeline can improve annotation speed and capacity by at least 80% and quality is comparable to or higher than manual double pass annotation. We are leveraging this scalable pipeline to create and continuously grow ultra-high volume off-the-shelf (UHV-OTS) speech corpora for multiple languages, with the capability to expand to 10,000+ hours per language annually. Customized datasets can be produced from the UHV-OTS corpora using dynamic packaging. UHV-OTS is a long-term Appen project to support commercial and academic research data needs in speech processing. Appen will donate a number of free speech datasets from the UHV-OTS each year to support academic and open source community research under the CC-BY-SA license. We are also releasing the code of the data pre-processing and pre-tagging pipeline under the Apache 2.0 license to allow reproduction of the results reported in the paper.","['https://openalex.org/W2973049979', 'https://openalex.org/W2730162096', 'https://openalex.org/W2800448574', 'https://openalex.org/W97072897', 'https://openalex.org/W2967606780', 'https://openalex.org/W2890964092', 'https://openalex.org/W3038871978', 'https://openalex.org/W3143186397', 'https://openalex.org/W2024490156', 'https://openalex.org/W3169320628', 'https://openalex.org/W2166637769', 'https://openalex.org/W2250357346', 'https://openalex.org/W2133824856', 'https://openalex.org/W3015356564', 'https://openalex.org/W3139918052', 'https://openalex.org/W2726515241', 'https://openalex.org/W3171442995', 'https://openalex.org/W3162899647', 'https://openalex.org/W2514741789', 'https://openalex.org/W1524333225', 'https://openalex.org/W3093502935', 'https://openalex.org/W1494198834', 'https://openalex.org/W3139878283', 'https://openalex.org/W3184414142', 'https://openalex.org/W3024869864', 'https://openalex.org/W3037149862', 'https://openalex.org/W3030437843', 'https://openalex.org/W2505877856']",2021-09-01
https://openalex.org/W3133501470,https://doi.org/10.1109/iscslp49672.2021.9362065,Improves Neural Acoustic Word Embeddings Query by Example Spoken Term Detection with Wav2vec Pretraining and Circle Loss,"Query by example spoken term detection (QbE-STD) is a popular keyword detection method in the absence of speech resources. It can build a keyword query system with decent performance when there are few labeled speeches and a lack of pronunciation dictionaries. In recent years, neural acoustic word embeddings (NAWEs) has become a commonly used QbE-STD method. To make the embedded features extracted by the neural network contain more accurate context information, we use wav2vec pre-training to improve the performance of the network. Compared with the Mel-frequency cepstral coefficients(MFCC) system, the average precision (AP) is relatively improved by 11.1%. We also find that the AP of the wav2vec and MFCC splicing system is better, demonstrating that wav2vec cannot contain all spectrum information. To accelerate the convergence speed of the splicing system, we use circle loss to replace the triplet loss, making the convergence about 40% epochs earlier on average. The circle loss also relatively increases AP by more than 4.9%. The AP of our best-performing system is 7.7% better than the wav2vec baseline system and 19.7% better than the MFCC baseline system.","['https://openalex.org/W4240592325', 'https://openalex.org/W2296362153', 'https://openalex.org/W2190506272', 'https://openalex.org/W2962736743', 'https://openalex.org/W6731763572', 'https://openalex.org/W3008444411', 'https://openalex.org/W2578392894', 'https://openalex.org/W2964169922', 'https://openalex.org/W2973049979', 'https://openalex.org/W3015356564', 'https://openalex.org/W2126203737', 'https://openalex.org/W1494198834', 'https://openalex.org/W2171019095', 'https://openalex.org/W2614542633', 'https://openalex.org/W2513125788', 'https://openalex.org/W1577418252', 'https://openalex.org/W3007486152', 'https://openalex.org/W2962980711', 'https://openalex.org/W2963571336', 'https://openalex.org/W2137089646', 'https://openalex.org/W3016181583', 'https://openalex.org/W2963775347', 'https://openalex.org/W2064675550', 'https://openalex.org/W2166637769', 'https://openalex.org/W3034303554', 'https://openalex.org/W6694517276', 'https://openalex.org/W6631362777', 'https://openalex.org/W2271840356', 'https://openalex.org/W2494980014', 'https://openalex.org/W2951216052', 'https://openalex.org/W1524333225', 'https://openalex.org/W2566587499']",2021-01-24
https://openalex.org/W3139534224,https://doi.org/10.1109/slt48900.2021.9383594,Acoustic Word Embeddings for Zero-Resource Languages Using Self-Supervised Contrastive Learning and Multilingual Adaptation,"Acoustic word embeddings (AWEs) are fixed-dimensional representations of variable-length speech segments. For zero-resource languages where labelled data is not available, one AWE approach is to use unsupervised autoencoder-based recurrent models. Another recent approach is to use multilingual transfer: a supervised AWE model is trained on several well-resourced languages and then applied to an unseen zero-resource language. We consider how a recent contrastive learning loss can be used in both the purely unsupervised and multilingual transfer settings. Firstly, we show that terms from an unsupervised term discovery system can be used for contrastive self-supervision, resulting in improvements over previous unsupervised monolingual AWE models. Secondly, we consider how multilingual AWE models can be adapted to a specific zero-resource language using discovered terms. We find that self-supervised contrastive adaptation outperforms adapted multilingual correspondence autoencoder and Siamese AWE models, giving the best overall results in a word discrimination task on six zero-resource languages.","['https://openalex.org/W6761050228', 'https://openalex.org/W6747899497', 'https://openalex.org/W2962736743', 'https://openalex.org/W3095706145', 'https://openalex.org/W6779278423', 'https://openalex.org/W6774180583', 'https://openalex.org/W6700872662', 'https://openalex.org/W343636949', 'https://openalex.org/W6769908858', 'https://openalex.org/W2962824366', 'https://openalex.org/W6775659032', 'https://openalex.org/W2963720603', 'https://openalex.org/W2963571336', 'https://openalex.org/W6777330019', 'https://openalex.org/W2940544976', 'https://openalex.org/W2025482506', 'https://openalex.org/W2889313720', 'https://openalex.org/W2916113431', 'https://openalex.org/W2963425185', 'https://openalex.org/W2963259843', 'https://openalex.org/W6769247734', 'https://openalex.org/W3008444411', 'https://openalex.org/W2932675979', 'https://openalex.org/W6675751002', 'https://openalex.org/W6631637103', 'https://openalex.org/W2407151108', 'https://openalex.org/W6631190155', 'https://openalex.org/W6786045457', 'https://openalex.org/W2084534958', 'https://openalex.org/W6768896439', 'https://openalex.org/W2165698076', 'https://openalex.org/W1967924372', 'https://openalex.org/W6735531217', 'https://openalex.org/W2964169922', 'https://openalex.org/W2963962561', 'https://openalex.org/W6770506093', 'https://openalex.org/W3096656254', 'https://openalex.org/W6727351083', 'https://openalex.org/W2059652594', 'https://openalex.org/W6697456849', 'https://openalex.org/W6731763572', 'https://openalex.org/W2578392894', 'https://openalex.org/W2802557066', 'https://openalex.org/W2884305338', 'https://openalex.org/W6756497944', 'https://openalex.org/W1577418252', 'https://openalex.org/W2114347655', 'https://openalex.org/W2963879199', 'https://openalex.org/W2927191280', 'https://openalex.org/W2057007397', 'https://openalex.org/W3097705090', 'https://openalex.org/W3097485645', 'https://openalex.org/W6774314701', 'https://openalex.org/W3044967013', 'https://openalex.org/W2190506272', 'https://openalex.org/W6730323794', 'https://openalex.org/W3015356564', 'https://openalex.org/W6769196770', 'https://openalex.org/W3015213852', 'https://openalex.org/W6773206665', 'https://openalex.org/W3096196861', 'https://openalex.org/W2962742544', 'https://openalex.org/W3162468843', 'https://openalex.org/W3047836878', 'https://openalex.org/W2996383576', 'https://openalex.org/W2598634450', 'https://openalex.org/W2979736514', 'https://openalex.org/W3046603443', 'https://openalex.org/W2566587499', 'https://openalex.org/W2995489995', 'https://openalex.org/W2550241133', 'https://openalex.org/W3015325583', 'https://openalex.org/W2980988419', 'https://openalex.org/W2899518769', 'https://openalex.org/W2991213871', 'https://openalex.org/W3033831118', 'https://openalex.org/W2964102148', 'https://openalex.org/W1532499126', 'https://openalex.org/W2973026522', 'https://openalex.org/W3026574827', 'https://openalex.org/W2296681920', 'https://openalex.org/W2291770225', 'https://openalex.org/W3037580942', 'https://openalex.org/W3005680577', 'https://openalex.org/W2935542736', 'https://openalex.org/W2321533354', 'https://openalex.org/W3105242324', 'https://openalex.org/W3003875258', 'https://openalex.org/W2964121744', 'https://openalex.org/W2106053110']",2021-01-19
https://openalex.org/W3204996224,,SCaLa: Supervised Contrastive Learning for End-to-End Automatic Speech Recognition.,"End-to-end Automatic Speech Recognition (ASR) models are usually trained to reduce the losses of the whole token sequences, while neglecting explicit phonemic-granularity supervision. This could lead to recognition errors due to similar-phoneme confusion or phoneme reduction. To alleviate this problem, this paper proposes a novel framework of Supervised Contrastive Learning (SCaLa) to enhance phonemic information learning for end-to-end ASR systems. Specifically, we introduce the self-supervised Masked Contrastive Predictive Coding (MCPC) into the fully-supervised setting. To supervise phoneme learning explicitly, SCaLa first masks the variable-length encoder features corresponding to phonemes given phoneme forced-alignment extracted from a pre-trained acoustic model, and then predicts the masked phonemes via contrastive learning. The phoneme forced-alignment can mitigate the noise of positive-negative pairs in self-supervised MCPC. Experimental results conducted on reading and spontaneous speech datasets show that the proposed approach achieves 2.84% and 1.38% Character Error Rate (CER) reductions compared to the baseline, respectively.","['https://openalex.org/W3034931686', 'https://openalex.org/W3023953056', 'https://openalex.org/W2973049979', 'https://openalex.org/W3156902660', 'https://openalex.org/W3181410678', 'https://openalex.org/W2134800885', 'https://openalex.org/W3015356564', 'https://openalex.org/W2750499125', 'https://openalex.org/W3100345210', 'https://openalex.org/W2972943112', 'https://openalex.org/W2963242190', 'https://openalex.org/W3160235762', 'https://openalex.org/W3095189764', 'https://openalex.org/W2996383576', 'https://openalex.org/W3191063709', 'https://openalex.org/W1524333225', 'https://openalex.org/W3167207712', 'https://openalex.org/W2981991061', 'https://openalex.org/W2842511635', 'https://openalex.org/W3197478142', 'https://openalex.org/W3034978746', 'https://openalex.org/W3015213852', 'https://openalex.org/W2982223350', 'https://openalex.org/W1556470778', 'https://openalex.org/W2127141656', 'https://openalex.org/W3099782249', 'https://openalex.org/W2962813140']",2021-10-08
https://openalex.org/W4319862708,https://doi.org/10.1109/slt54892.2023.10023407,Dual Learning for Large Vocabulary On-Device ASR,"Dual learning is a paradigm for semi-supervised machine learning that seeks to leverage unsupervised data by solving two opposite tasks at once. In this scheme, each model is used to generate pseudo-labels for unlabeled examples that are used to train the other model. Dual learning has seen some use in speech processing by pairing ASR and TTS as dual tasks. However, these results mostly address only the case of using unpaired examples to compensate for very small supervised datasets, and mostly on large, non-streaming models. Dual learning has not yet been proven effective for using unsupervised data to improve realistic on-device streaming models that are already trained on large supervised corpora. We provide this missing piece though an analysis of an on-device-sized streaming conformer trained on the entirety of Librispeech, showing relative WER improvements of 10.7%/5.2% without an LM and 11.7%/16.4% with an LM.","['https://openalex.org/W2973049979', 'https://openalex.org/W6739901393', 'https://openalex.org/W2842511635', 'https://openalex.org/W2936774411', 'https://openalex.org/W2888779557', 'https://openalex.org/W6640059789', 'https://openalex.org/W3197976839', 'https://openalex.org/W3016234571', 'https://openalex.org/W6770514103', 'https://openalex.org/W2995181338', 'https://openalex.org/W6769593479', 'https://openalex.org/W6621543089', 'https://openalex.org/W1975113979', 'https://openalex.org/W6680972403', 'https://openalex.org/W6729383884', 'https://openalex.org/W3209984917', 'https://openalex.org/W3096815019', 'https://openalex.org/W6741328424', 'https://openalex.org/W2886095922', 'https://openalex.org/W3015280134', 'https://openalex.org/W3163203022', 'https://openalex.org/W4225272718', 'https://openalex.org/W6762242920', 'https://openalex.org/W2963739817', 'https://openalex.org/W3081416955', 'https://openalex.org/W2964243274', 'https://openalex.org/W3097777922', 'https://openalex.org/W6755207826', 'https://openalex.org/W3198858531', 'https://openalex.org/W6769238691', 'https://openalex.org/W6747636377', 'https://openalex.org/W82886505', 'https://openalex.org/W3097286738', 'https://openalex.org/W3100270690', 'https://openalex.org/W2962699523', 'https://openalex.org/W2140914930', 'https://openalex.org/W569478347', 'https://openalex.org/W1915251500', 'https://openalex.org/W4385245566', 'https://openalex.org/W4210690962', 'https://openalex.org/W3015356564', 'https://openalex.org/W3036601975', 'https://openalex.org/W648786980', 'https://openalex.org/W2981991061', 'https://openalex.org/W4297808394', 'https://openalex.org/W2982223350', 'https://openalex.org/W2979476256', 'https://openalex.org/W2546938941', 'https://openalex.org/W398859631']",2023-01-09
https://openalex.org/W4385388413,https://doi.org/10.59350/tnrhy-3dq84,"These new ""artificial intelligence"" programs don't know what they're talking about",I'm sure you've seen things like ChatGPT in the news: programs that can carry out pretty convincing conversations. They are known as Large Language Models (LLMs) and are frequently referred to as being Artificial Intelligence (AI) — but I really don't like that designation as it implies some understanding.,"['https://openalex.org/W1964157254', 'https://openalex.org/W3037831233', 'https://openalex.org/W2970395295', 'https://openalex.org/W2294370754', 'https://openalex.org/W2963341956', 'https://openalex.org/W2889078912', 'https://openalex.org/W1979403545', 'https://openalex.org/W3100355250', 'https://openalex.org/W3035296331', 'https://openalex.org/W3035032094', 'https://openalex.org/W2507974895', 'https://openalex.org/W3100718630', 'https://openalex.org/W3098998028', 'https://openalex.org/W2952984539', 'https://openalex.org/W3147310958', 'https://openalex.org/W2962739339', 'https://openalex.org/W2989344603', 'https://openalex.org/W2963748441', 'https://openalex.org/W3034937117', 'https://openalex.org/W3214897310', 'https://openalex.org/W2971307358', 'https://openalex.org/W4213226808', 'https://openalex.org/W4229933927', 'https://openalex.org/W2946417913', 'https://openalex.org/W2923014074', 'https://openalex.org/W2963943967', 'https://openalex.org/W2979826702', 'https://openalex.org/W3038047279', 'https://openalex.org/W3101248447', 'https://openalex.org/W2970634364', 'https://openalex.org/W2912817604', 'https://openalex.org/W2970774467', 'https://openalex.org/W2089758871', 'https://openalex.org/W2963159690', 'https://openalex.org/W2926555354', 'https://openalex.org/W2998563994', 'https://openalex.org/W2761999826', 'https://openalex.org/W2972940946', 'https://openalex.org/W2805572053', 'https://openalex.org/W1919975936', 'https://openalex.org/W2938830017', 'https://openalex.org/W2785615365', 'https://openalex.org/W2097615438', 'https://openalex.org/W3217457012', 'https://openalex.org/W2802416800', 'https://openalex.org/W3003646990', 'https://openalex.org/W4307768528', 'https://openalex.org/W4310492983', 'https://openalex.org/W2970597249', 'https://openalex.org/W2973727699', 'https://openalex.org/W2106128590', 'https://openalex.org/W2261728763', 'https://openalex.org/W4288089799', 'https://openalex.org/W4297792382', 'https://openalex.org/W2986074609', 'https://openalex.org/W4294170691', 'https://openalex.org/W2911227954', 'https://openalex.org/W633133198', 'https://openalex.org/W2089406488', 'https://openalex.org/W3013770059', 'https://openalex.org/W4254114958', 'https://openalex.org/W2109664771', 'https://openalex.org/W3015356564', 'https://openalex.org/W2969958763', 'https://openalex.org/W3100279624', 'https://openalex.org/W2119519269', 'https://openalex.org/W1469191350', 'https://openalex.org/W2972413484', 'https://openalex.org/W4308264370', 'https://openalex.org/W2998183051', 'https://openalex.org/W3148183269', 'https://openalex.org/W2963809228', 'https://openalex.org/W4233919019', 'https://openalex.org/W4287553002', 'https://openalex.org/W3185212449', 'https://openalex.org/W1990921190', 'https://openalex.org/W4249803144', 'https://openalex.org/W2948917818', 'https://openalex.org/W2991870143', 'https://openalex.org/W2991588540', 'https://openalex.org/W3169483174', 'https://openalex.org/W2951645620', 'https://openalex.org/W2117278770', 'https://openalex.org/W3034723486', 'https://openalex.org/W4388064047', 'https://openalex.org/W2461231790', 'https://openalex.org/W2138232041', 'https://openalex.org/W2240633916', 'https://openalex.org/W2100506586', 'https://openalex.org/W2972668795', 'https://openalex.org/W3119746452', 'https://openalex.org/W4248491679', 'https://openalex.org/W4214916459', 'https://openalex.org/W2790136966', 'https://openalex.org/W2993383518', 'https://openalex.org/W2981710752', 'https://openalex.org/W2236770054', 'https://openalex.org/W2922258799', 'https://openalex.org/W4385245566', 'https://openalex.org/W3035359363', 'https://openalex.org/W2927117015', 'https://openalex.org/W2978017171', 'https://openalex.org/W3177265267', 'https://openalex.org/W2300258214', 'https://openalex.org/W3118781290', 'https://openalex.org/W2996428491', 'https://openalex.org/W1821462560', 'https://openalex.org/W3040573126', 'https://openalex.org/W4243349317', 'https://openalex.org/W2990391581', 'https://openalex.org/W4255446654', 'https://openalex.org/W4288029087', 'https://openalex.org/W2039207728', 'https://openalex.org/W3118485687', 'https://openalex.org/W2783995097', 'https://openalex.org/W2798262146', 'https://openalex.org/W2757510278', 'https://openalex.org/W2110930288', 'https://openalex.org/W4245297930', 'https://openalex.org/W4292779060', 'https://openalex.org/W3121694563', 'https://openalex.org/W3011574394', 'https://openalex.org/W2965373594', 'https://openalex.org/W2484711362', 'https://openalex.org/W4242076805', 'https://openalex.org/W2250539671', 'https://openalex.org/W3009095382', 'https://openalex.org/W3035032873', 'https://openalex.org/W3080248051', 'https://openalex.org/W3013451997', 'https://openalex.org/W2805206884', 'https://openalex.org/W2997200074', 'https://openalex.org/W3006901707', 'https://openalex.org/W3086249591', 'https://openalex.org/W2970771982', 'https://openalex.org/W2048569019', 'https://openalex.org/W4288623406', 'https://openalex.org/W2253983195', 'https://openalex.org/W4287391717', 'https://openalex.org/W2318994447', 'https://openalex.org/W305856652']",2023-01-15
https://openalex.org/W4388820319,https://doi.org/10.1109/apsipaasc58517.2023.10317133,Progressive Multi-scale Self-supervised Learning for Speech Recognition,"Self-supervised learning has shown great potential in improving automatic speech recognition (ASR) systems. However, further improvements in recognition performance could be achieved if models focus on audio content information learning. In this paper, we propose a progressive multi-scale self-supervised learning method that reinforces the learning process from easy to difficult. Our progressive strategy utilizes fine-grained target sets to compute self-supervised learning loss at the top layer while using coarse-grained target sets at intermediate layers. Additionally, to match the difficulty of the learning process, we introduce a multi-scale structure into the multi-head self-attention module. We evaluate our method on the Librispeech dataset and demonstrate its effectiveness. Our proposed method achieves a relative word error rate (WER) reduction of 13.7% and 12.7% on the test_other evaluation subsets, respectively, when fine-tuned on 10-hour and 100-hour subsets, outperforming HuBERT.","['https://openalex.org/W6784436999', 'https://openalex.org/W3096338464', 'https://openalex.org/W2120615054', 'https://openalex.org/W6739901393', 'https://openalex.org/W6755207826', 'https://openalex.org/W6810944559', 'https://openalex.org/W6791904447', 'https://openalex.org/W6687566353', 'https://openalex.org/W2127141656', 'https://openalex.org/W6725939724', 'https://openalex.org/W6947929050', 'https://openalex.org/W1494198834', 'https://openalex.org/W2973157397', 'https://openalex.org/W3041561163', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W3015356564', 'https://openalex.org/W3209059054', 'https://openalex.org/W6780218876', 'https://openalex.org/W2963735467', 'https://openalex.org/W4224934179', 'https://openalex.org/W3036601975', 'https://openalex.org/W2979476256', 'https://openalex.org/W3139918052', 'https://openalex.org/W3197223534', 'https://openalex.org/W4385245566', 'https://openalex.org/W2896457183']",2023-10-31
https://openalex.org/W4393406202,https://doi.org/10.1109/o-cocosda60357.2023.10482938,A Novel Approach for Bootstrapping and Automatic Transcription of Low Resourced Language Speech Corpus,"Automatic Speech Recognition (ASR) systems have made significant advancements in the context of high-resource languages, primarily attributable to the abundant availability of extensive and diverse speech datasets. Nevertheless, the dearth of annotated data remains a substantial hurdle when it comes to low-resource languages. This study delves into the feasibility of development of an ASR system for low-resource languages by leveraging pre-trained models from other languages. The fine-tuned model is then deployed to transcribe speech segments from news bulletins and audio content found on the web. Subsequently, the generated transcript is heuristically aligned with existing news script. This newly aligned speech corpus is used incrementally to augment the existing corpus, and thus progressively bootstrapping the ASR models. The proposed work has been effectively carried out for Dogri, a low resource language of India. The proposed approach of incremental learning and data augmentation can be applied to other low resource languages as well, and thus would help in bridging the resource gap.","['https://openalex.org/W2091746061', 'https://openalex.org/W4298382487', 'https://openalex.org/W3010812523', 'https://openalex.org/W3153199526', 'https://openalex.org/W2055487029', 'https://openalex.org/W2049155070', 'https://openalex.org/W1988099331', 'https://openalex.org/W2938732626', 'https://openalex.org/W1983190981', 'https://openalex.org/W6779089044', 'https://openalex.org/W3015356564', 'https://openalex.org/W3213618310', 'https://openalex.org/W2075201173', 'https://openalex.org/W3036601975', 'https://openalex.org/W4224322103', 'https://openalex.org/W4287075881']",2023-12-04
https://openalex.org/W4403908485,https://doi.org/10.55662/jst.2023.4104,Systematic Review of Advancing Machine Learning Through Cross-Domain Analysis of Unlabeled Data,"Self-supervised learning (SSL) has become a transformative approach in the field of machine learning, offering a powerful means to harness the vast amounts of unlabeled data available across various domains. By creating auxiliary tasks that generate supervisory signals directly from the data, SSL mitigates the dependency on large, labeled datasets, thereby expanding the applicability of machine learning models. This paper provides a comprehensive exploration of SSL techniques applied to diverse data types, including images, text, audio, and time-series data. We delve into the underlying principles that drive SSL, examine common methodologies, and highlight specific algorithms tailored to each data type. Additionally, we address the unique challenges encountered in applying SSL across different domains and propose future research directions that could further enhance the capabilities and effectiveness of SSL. Through this analysis, we underscore SSL's potential to significantly advance the development of robust, generalizable models capable of tackling complex real-world problems.","['https://openalex.org/W3114632476', 'https://openalex.org/W4291023040', 'https://openalex.org/W4283697304', 'https://openalex.org/W6743229011', 'https://openalex.org/W2973049979', 'https://openalex.org/W3005680577', 'https://openalex.org/W3171007011', 'https://openalex.org/W3035524453', 'https://openalex.org/W3035060554', 'https://openalex.org/W2963341956', 'https://openalex.org/W3041561163', 'https://openalex.org/W3015356564', 'https://openalex.org/W4281492411', 'https://openalex.org/W3190152617', 'https://openalex.org/W2920777619', 'https://openalex.org/W4221163898', 'https://openalex.org/W2963465221', 'https://openalex.org/W4289946024', 'https://openalex.org/W4221145109', 'https://openalex.org/W6926573507']",2023-01-20
https://openalex.org/W4405968073,https://doi.org/10.3390/math13010066,Multi-View Collaborative Training and Self-Supervised Learning for Group Recommendation,"Recommendation systems offer an effective solution to information overload, finding widespread application across e-commerce, news platforms, and beyond. By analyzing interaction histories, these systems automatically filter and recommend items that are most likely to resonate with users. Recently, with the swift advancement of social networking, group recommendation has emerged as a compelling research area, enabling personalized recommendations for groups of users. Unlike individual recommendation, group recommendation must consider both individual preferences and group dynamics, thereby enhancing decision-making efficiency for groups. One of the key challenges facing recommendation algorithms is data sparsity, a limitation that is even more severe in group recommendation than in traditional recommendation tasks. While various group recommendation methods attempt to address this issue, many of them still rely on single-view modeling or fail to sufficiently account for individual user preferences within a group, limiting their effectiveness. This paper addresses the data sparsity issue to improve group recommendation performance, overcoming the limitations of overlooking individual user recommendation tasks and depending on single-view modeling. We propose MCSS (multi-view collaborative training and self-supervised learning), a novel framework that harnesses both multi-view collaborative training and self-supervised learning specifically for group recommendations. By incorporating both group and individual recommendation tasks, MCSS leverages graph convolution and attention mechanisms to generate three sets of embeddings, enhancing the model’s representational power. Additionally, we design self-supervised auxiliary tasks to maximize the data utility, further enhancing performance. Through multi-task joint training, the model generates refined recommendation lists tailored to each group and individual user. Extensive validation and comparison demonstrate the method’s robustness and effectiveness, underscoring the potential of MCSS to advance state-of-the-art group recommendation.","['https://openalex.org/W6856008197', 'https://openalex.org/W4400480168', 'https://openalex.org/W4316114998', 'https://openalex.org/W2990003990', 'https://openalex.org/W2980575105', 'https://openalex.org/W4386969884', 'https://openalex.org/W4313396194', 'https://openalex.org/W6787778430', 'https://openalex.org/W3093231418', 'https://openalex.org/W3193171431', 'https://openalex.org/W3088257568', 'https://openalex.org/W3135180795', 'https://openalex.org/W4206920920', 'https://openalex.org/W3172683188', 'https://openalex.org/W3038229091', 'https://openalex.org/W2966501701', 'https://openalex.org/W3204898214', 'https://openalex.org/W3015356564', 'https://openalex.org/W2021674147', 'https://openalex.org/W2798538558', 'https://openalex.org/W3035289325', 'https://openalex.org/W2955081857', 'https://openalex.org/W2951217911', 'https://openalex.org/W4207040932', 'https://openalex.org/W4399565317', 'https://openalex.org/W4221159053', 'https://openalex.org/W3033661031', 'https://openalex.org/W3198760092', 'https://openalex.org/W4319653986', 'https://openalex.org/W3198098536', 'https://openalex.org/W2999127310', 'https://openalex.org/W4402698633', 'https://openalex.org/W6772878054', 'https://openalex.org/W3114921773', 'https://openalex.org/W2999649805', 'https://openalex.org/W3211072770', 'https://openalex.org/W3099602291', 'https://openalex.org/W4386154563']",2024-12-27
https://openalex.org/W4408887274,https://doi.org/10.1121/10.0036222,Formant-based vowel categorization for cross-lingual phone recognition,"Multilingual phone recognition models can learn language-independent pronunciation patterns from large volumes of spoken data and recognize them across languages. This potential can be harnessed to improve speech technologies for underresourced languages. However, these models are typically trained on phonological representations of speech sounds, which do not necessarily reflect the phonetic realization of speech. A mismatch between a phonological symbol and its phonetic realizations can lead to phone confusions and reduce performance. This work introduces formant-based vowel categorization aimed at improving cross-lingual vowel recognition by uncovering a vowel's phonetic quality from its formant frequencies, and reorganizing the vowel categories in a multilingual speech corpus to increase their consistency across languages. The work investigates vowel categories obtained from a trilingual multi-dialect speech corpus of Danish, Norwegian, and Swedish using three categorization techniques. Cross-lingual phone recognition experiments reveal that uniting vowel categories of different languages into a set of shared formant-based categories improves cross-lingual recognition of the shared vowels, but also interferes with recognition of vowels not present in one or more training languages. Cross-lingual evaluation on regional dialects provides inconclusive results. Nevertheless, improved recognition of individual vowels can translate to improvements in overall phone recognition on languages unseen during training.","['https://openalex.org/W2003483021', 'https://openalex.org/W3015356564', 'https://openalex.org/W3036601975', 'https://openalex.org/W141076785', 'https://openalex.org/W2058878924', 'https://openalex.org/W3198429080', 'https://openalex.org/W1986758142', 'https://openalex.org/W4232500651', 'https://openalex.org/W3094197178', 'https://openalex.org/W2061147644', 'https://openalex.org/W2157621600', 'https://openalex.org/W2002342963', 'https://openalex.org/W3197278374', 'https://openalex.org/W2050758723', 'https://openalex.org/W4234818363', 'https://openalex.org/W1986094003', 'https://openalex.org/W2143722762', 'https://openalex.org/W4240723213', 'https://openalex.org/W4306770332', 'https://openalex.org/W2100379591', 'https://openalex.org/W4388400684', 'https://openalex.org/W2884225676', 'https://openalex.org/W4244333547', 'https://openalex.org/W2053508090', 'https://openalex.org/W4206472724', 'https://openalex.org/W1973900406', 'https://openalex.org/W2990582804', 'https://openalex.org/W4245735626', 'https://openalex.org/W178306953', 'https://openalex.org/W2998284473', 'https://openalex.org/W4253522216', 'https://openalex.org/W2000529208', 'https://openalex.org/W2090365329', 'https://openalex.org/W2572670101', 'https://openalex.org/W2061296254', 'https://openalex.org/W4381546573', 'https://openalex.org/W2768833706', 'https://openalex.org/W3005578234', 'https://openalex.org/W2317919972', 'https://openalex.org/W4287854696', 'https://openalex.org/W2172213580', 'https://openalex.org/W2979826702', 'https://openalex.org/W3204224625', 'https://openalex.org/W4211116738', 'https://openalex.org/W3025286576', 'https://openalex.org/W2059450962', 'https://openalex.org/W1992468098']",2025-03-01
https://openalex.org/W4409203934,https://doi.org/10.20944/preprints202504.0397.v1,FusionX: A Symbolic-fused Multimodal Emotion Interaction Framework,"Understanding human emotion through multimodal signals—such as linguistic content, vocal acoustics, and facial expressions—remains a complex and nuanced challenge for artificial systems. Unlike humans, who intuitively infer emotions through intricate cross-modal cues, machines must systematically decode heterogeneous information. To address this gap, we propose a novel multimodal emotion recognition framework, \textbf{FusionX}, that systematically models inter-modal dynamics from multiple perspectives. FusionX decomposes multimodal input signals into three complementary types of interaction representations: modality-complete (preserving full unimodal information), modality-synergistic (capturing shared inter-modal contributions), and modality-unique (highlighting distinctive aspects of each modality). To further refine the integration of these representations, we introduce a text-prioritized fusion mechanism named \textbf{Text-Centric Hierarchical Tensor Fusion} (TCHF). This module constructs a deep hierarchical tensor network that accentuates the semantic richness of textual modality while harmonizing its contribution with the audio and visual streams. To validate FusionX, we conduct extensive evaluations across three widely-used benchmarks: MOSEI, MOSI, and IEMOCAP. Results reveal that our method significantly surpasses previous state-of-the-art baselines in both classification accuracy and regression metrics, demonstrating the superiority of hierarchical and perspective-aware interaction modeling in emotion understanding.","['https://openalex.org/W3015356564', 'https://openalex.org/W2619383789', 'https://openalex.org/W2146334809', 'https://openalex.org/W3211727363', 'https://openalex.org/W1924770834', 'https://openalex.org/W2896457183', 'https://openalex.org/W6667760467', 'https://openalex.org/W3198512197', 'https://openalex.org/W2805662932', 'https://openalex.org/W3022398031', 'https://openalex.org/W2970972665', 'https://openalex.org/W3104739527', 'https://openalex.org/W2886193235', 'https://openalex.org/W2104657103', 'https://openalex.org/W2990276885', 'https://openalex.org/W6691431627', 'https://openalex.org/W2904518532', 'https://openalex.org/W2952367383', 'https://openalex.org/W2740550900', 'https://openalex.org/W3034266838', 'https://openalex.org/W2519656895', 'https://openalex.org/W6789647394', 'https://openalex.org/W3049723069', 'https://openalex.org/W2947476638', 'https://openalex.org/W2901272442', 'https://openalex.org/W3128412859', 'https://openalex.org/W3034425145', 'https://openalex.org/W2787581402', 'https://openalex.org/W2787145221', 'https://openalex.org/W2556418146', 'https://openalex.org/W2053101950', 'https://openalex.org/W2919115771', 'https://openalex.org/W2344790094', 'https://openalex.org/W2014103885', 'https://openalex.org/W2124033848', 'https://openalex.org/W4395470960', 'https://openalex.org/W1905882502', 'https://openalex.org/W3084892943', 'https://openalex.org/W4200631085', 'https://openalex.org/W4221146921', 'https://openalex.org/W2997087088', 'https://openalex.org/W4307928432', 'https://openalex.org/W4321796477', 'https://openalex.org/W4365600875', 'https://openalex.org/W2159457224', 'https://openalex.org/W3037979089', 'https://openalex.org/W4377987956', 'https://openalex.org/W3173211653', 'https://openalex.org/W1522301498', 'https://openalex.org/W3173293897', 'https://openalex.org/W6898505805', 'https://openalex.org/W2963341956', 'https://openalex.org/W2784814091', 'https://openalex.org/W2908889761', 'https://openalex.org/W4402753940', 'https://openalex.org/W4285601004', 'https://openalex.org/W3190684574', 'https://openalex.org/W4385749771', 'https://openalex.org/W4377866581', 'https://openalex.org/W2123301721', 'https://openalex.org/W4405955038', 'https://openalex.org/W3131149726', 'https://openalex.org/W3174111043', 'https://openalex.org/W4297650285', 'https://openalex.org/W3214879577', 'https://openalex.org/W3035537500', 'https://openalex.org/W4226470037', 'https://openalex.org/W2506483933', 'https://openalex.org/W4296366985', 'https://openalex.org/W4385750420', 'https://openalex.org/W4377371950', 'https://openalex.org/W3217381797', 'https://openalex.org/W2752172973']",2025-04-06
https://openalex.org/W4409223142,https://doi.org/10.1109/access.2025.3558172,Self-Supervised Learning for Vehicle Noise Prediction With Limited Labeled Data,"With advancements in artificial intelligence, its application for developing quieter vehicles is increasingly researched in the automotive industry. Vehicle noise is highly impacted by the acceleration from electric power steering (EPS); therefore, determining the relationship between EPS-induced acceleration and vehicle noise is important. However, collecting labeled data for noise prediction is challenging because it requires expensive acceleration sensors attached to the EPS, and experts must measure the noise directly by ear. In contrast, obtaining unlabeled acceleration data from environments similar to those of the EPS is manageable. This study proposes an autoencoder-based self-supervised learning with information maximization (ASSIM) method to predict vehicle noise. ASSIM allows robust feature learning via pretraining to reconstruct unlabeled data with necessary augmentations. The experimental results demonstrate that ASSIM performs better than the other comparative methods in predicting vehicle noise in environments with less labeled data. The proposed method may aid in the design of quieter vehicles while reducing the data collection time.","['https://openalex.org/W1493952566', 'https://openalex.org/W3212113934', 'https://openalex.org/W4389897587', 'https://openalex.org/W2964744899', 'https://openalex.org/W2961638199', 'https://openalex.org/W3185542499', 'https://openalex.org/W4309947635', 'https://openalex.org/W4402978442', 'https://openalex.org/W1519726874', 'https://openalex.org/W3037911826', 'https://openalex.org/W3199387306', 'https://openalex.org/W3004643093', 'https://openalex.org/W4387546514', 'https://openalex.org/W6810510949', 'https://openalex.org/W3198860978', 'https://openalex.org/W3015356564', 'https://openalex.org/W4210690962', 'https://openalex.org/W3041561163', 'https://openalex.org/W4310633482', 'https://openalex.org/W6791742336', 'https://openalex.org/W6802757510', 'https://openalex.org/W6795754764', 'https://openalex.org/W2614506317', 'https://openalex.org/W2257273337', 'https://openalex.org/W6788103393', 'https://openalex.org/W3135012968', 'https://openalex.org/W4375869340', 'https://openalex.org/W3206996142', 'https://openalex.org/W4372260338', 'https://openalex.org/W6602375370', 'https://openalex.org/W6751823098', 'https://openalex.org/W3170889573', 'https://openalex.org/W4382239155', 'https://openalex.org/W2954996726']",2025-01-01
https://openalex.org/W4410474598,https://doi.org/10.20944/preprints202505.1219.v1,Multi-Interaction Modeling with Intelligent Coordination for Multimodal Emotion Recognition,"Emotion recognition through multimodal signals—such as speech, text, and facial cues—has garnered increasing attention due to its pivotal role in enhancing human-computer interaction and intelligent communication systems. However, existing approaches often struggle to thoroughly capture the intricacies of multimodal interactions, primarily due to the challenges in effectively fusing heterogeneous modalities while mitigating redundancy and preserving complementary information. In this study, we introduce \textbf{MIMIC}, a novel framework designed to comprehensively model complex multimodal interactions from diverse perspectives. Specifically, MIMIC introduces three parallel latent representations: a modality-preserving full interaction representation, a cross-modal shared interaction representation, and individualized modality-specific representations. Furthermore, a hierarchical semantic-driven fusion strategy is proposed to seamlessly integrate these representations into a cohesive multimodal interaction space. Extensive experiments demonstrate that our MIMIC framework not only surpasses prior state-of-the-art methods but also achieves this with remarkable efficiency, involving lower computational complexity and significantly fewer trainable parameters. Our contributions are twofold: (1) advancing a multi-perspective interaction modeling approach that enhances the depth of multimodal emotion analysis, and (2) offering a streamlined, resource-efficient framework suitable for practical deployments in emotion-aware systems.","['https://openalex.org/W2738581557', 'https://openalex.org/W2787581402', 'https://openalex.org/W2798965674', 'https://openalex.org/W2807899908', 'https://openalex.org/W2556418146', 'https://openalex.org/W2883409523', 'https://openalex.org/W2250539671', 'https://openalex.org/W2896457183', 'https://openalex.org/W3015356564', 'https://openalex.org/W2519656895', 'https://openalex.org/W2901272442', 'https://openalex.org/W2947476638', 'https://openalex.org/W2808359495', 'https://openalex.org/W2985862548', 'https://openalex.org/W2053101950', 'https://openalex.org/W2919115771', 'https://openalex.org/W2344790094', 'https://openalex.org/W2014103885', 'https://openalex.org/W2124033848', 'https://openalex.org/W4395470960', 'https://openalex.org/W1905882502', 'https://openalex.org/W3084892943', 'https://openalex.org/W4200631085', 'https://openalex.org/W4221146921', 'https://openalex.org/W2997087088', 'https://openalex.org/W4307928432', 'https://openalex.org/W4321796477', 'https://openalex.org/W4365600875', 'https://openalex.org/W2159457224', 'https://openalex.org/W3037979089', 'https://openalex.org/W4377987956', 'https://openalex.org/W3173211653', 'https://openalex.org/W1522301498', 'https://openalex.org/W3173293897', 'https://openalex.org/W6898505805', 'https://openalex.org/W2963341956', 'https://openalex.org/W2784814091', 'https://openalex.org/W2908889761', 'https://openalex.org/W4402753940', 'https://openalex.org/W4285601004', 'https://openalex.org/W3190684574', 'https://openalex.org/W4385749771', 'https://openalex.org/W4377866581', 'https://openalex.org/W2123301721', 'https://openalex.org/W4405955038', 'https://openalex.org/W3131149726', 'https://openalex.org/W3174111043', 'https://openalex.org/W4297650285', 'https://openalex.org/W3214879577', 'https://openalex.org/W3035537500', 'https://openalex.org/W4226470037', 'https://openalex.org/W2506483933', 'https://openalex.org/W4296366985', 'https://openalex.org/W4385750420', 'https://openalex.org/W4377371950', 'https://openalex.org/W3217381797', 'https://openalex.org/W2752172973']",2025-05-16
https://openalex.org/W3180374548,https://doi.org/10.18653/v1/2022.acl-long.235,Direct Speech-to-Speech Translation With Discrete Units,"Ann Lee, Peng-Jen Chen, Changhan Wang, Jiatao Gu, Sravya Popuri, Xutai Ma, Adam Polyak, Yossi Adi, Qing He, Yun Tang, Juan Pino, Wei-Ning Hsu. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.","['https://openalex.org/W2933138175', 'https://openalex.org/W3007068036', 'https://openalex.org/W3197580070', 'https://openalex.org/W3157923770', 'https://openalex.org/W3210177631', 'https://openalex.org/W2605131327', 'https://openalex.org/W2995181338', 'https://openalex.org/W3112092703', 'https://openalex.org/W3007142233', 'https://openalex.org/W3173767661', 'https://openalex.org/W3098403858', 'https://openalex.org/W3169320628', 'https://openalex.org/W2152834109', 'https://openalex.org/W2035108931', 'https://openalex.org/W3033411150', 'https://openalex.org/W4394671563', 'https://openalex.org/W2963609956', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963532001', 'https://openalex.org/W3140429000', 'https://openalex.org/W2963979492', 'https://openalex.org/W4287553982', 'https://openalex.org/W2949328740', 'https://openalex.org/W3160525311', 'https://openalex.org/W3112616666', 'https://openalex.org/W2936774411', 'https://openalex.org/W2991213871', 'https://openalex.org/W2963403868', 'https://openalex.org/W2998353611', 'https://openalex.org/W3118578889', 'https://openalex.org/W1494198834', 'https://openalex.org/W1537859740', 'https://openalex.org/W2136545725', 'https://openalex.org/W3142316150', 'https://openalex.org/W2127141656', 'https://openalex.org/W3175871055', 'https://openalex.org/W2903739847', 'https://openalex.org/W2972495969', 'https://openalex.org/W2973157397', 'https://openalex.org/W3186843219', 'https://openalex.org/W3119308075', 'https://openalex.org/W2097203679', 'https://openalex.org/W2747920239', 'https://openalex.org/W3130016944', 'https://openalex.org/W3099782249', 'https://openalex.org/W3092424727', 'https://openalex.org/W3036601975', 'https://openalex.org/W3092028330']",2022-01-01
https://openalex.org/W4287854499,https://doi.org/10.18653/v1/2022.naacl-main.63,Textless Speech-to-Speech Translation on Real Data,"Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan Wang, Sravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu, Wei-Ning Hsu. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.","['https://openalex.org/W2933138175', 'https://openalex.org/W3092424727', 'https://openalex.org/W2514828952', 'https://openalex.org/W1494198834', 'https://openalex.org/W3142316150', 'https://openalex.org/W3175301726', 'https://openalex.org/W2972495969', 'https://openalex.org/W2963979492', 'https://openalex.org/W3015698636', 'https://openalex.org/W4301980136', 'https://openalex.org/W4287079508', 'https://openalex.org/W3007068036', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963609956', 'https://openalex.org/W620750443', 'https://openalex.org/W2946200149', 'https://openalex.org/W3213873715', 'https://openalex.org/W2903739847', 'https://openalex.org/W3140429000', 'https://openalex.org/W3134644026', 'https://openalex.org/W2808631503', 'https://openalex.org/W3016160783', 'https://openalex.org/W3186843219', 'https://openalex.org/W2991213871', 'https://openalex.org/W2963799213', 'https://openalex.org/W3213018012', 'https://openalex.org/W3118578889', 'https://openalex.org/W2972466499', 'https://openalex.org/W2964243274', 'https://openalex.org/W3033411150', 'https://openalex.org/W2972802841', 'https://openalex.org/W4286984129', 'https://openalex.org/W3043665049', 'https://openalex.org/W2949328740', 'https://openalex.org/W2046056978', 'https://openalex.org/W3092028330', 'https://openalex.org/W3119308075', 'https://openalex.org/W3175871055', 'https://openalex.org/W3169320628', 'https://openalex.org/W2988736778', 'https://openalex.org/W2963532001', 'https://openalex.org/W3201257124', 'https://openalex.org/W2136545725', 'https://openalex.org/W2619368999', 'https://openalex.org/W4385245566', 'https://openalex.org/W3030437843', 'https://openalex.org/W4394671563']",2022-01-01
https://openalex.org/W3193077216,https://doi.org/10.24963/ijcai.2021/629,A Survey on Low-Resource Neural Machine Translation,"Neural approaches have achieved state-of-the-art accuracy on machine translation but suffer from the high cost of collecting large scale parallel data. Thus, a lot of research has been conducted for neural machine translation (NMT) with very limited parallel data, i.e., the low-resource setting. In this paper, we provide a survey for low-resource NMT and classify related works into three categories according to the auxiliary data they used: (1) exploiting monolingual data of source and/or target languages, (2) exploiting data from auxiliary languages, and (3) exploiting multi-modal data. We hope that our survey can help researchers to better understand this field and inspire them to design better algorithms, and help industry practitioners to choose appropriate algorithms for their applications.","['https://openalex.org/W2919290281', 'https://openalex.org/W6745740328', 'https://openalex.org/W2912815991', 'https://openalex.org/W2133564696', 'https://openalex.org/W2970925270', 'https://openalex.org/W2807535859', 'https://openalex.org/W6863994431', 'https://openalex.org/W2610245951', 'https://openalex.org/W2946928227', 'https://openalex.org/W2739978843', 'https://openalex.org/W2805394970', 'https://openalex.org/W2983040767', 'https://openalex.org/W6762792713', 'https://openalex.org/W2251743902', 'https://openalex.org/W6863318804', 'https://openalex.org/W3040377727', 'https://openalex.org/W6754111937', 'https://openalex.org/W2985509622', 'https://openalex.org/W3098341425', 'https://openalex.org/W2443536229', 'https://openalex.org/W2552839021', 'https://openalex.org/W2888541716', 'https://openalex.org/W6729383884', 'https://openalex.org/W2886095922', 'https://openalex.org/W2887516053', 'https://openalex.org/W2993286857', 'https://openalex.org/W2550821151', 'https://openalex.org/W2767982226', 'https://openalex.org/W6790825729', 'https://openalex.org/W2948884343', 'https://openalex.org/W2982399380', 'https://openalex.org/W2947992883', 'https://openalex.org/W3001434439', 'https://openalex.org/W6864487941', 'https://openalex.org/W2798926775', 'https://openalex.org/W2971254483', 'https://openalex.org/W2153579005', 'https://openalex.org/W2899336027', 'https://openalex.org/W2573834658', 'https://openalex.org/W2887920589', 'https://openalex.org/W2752630748', 'https://openalex.org/W2888456631', 'https://openalex.org/W2950780414', 'https://openalex.org/W2981852735', 'https://openalex.org/W2904770594', 'https://openalex.org/W2964910501', 'https://openalex.org/W6764027429', 'https://openalex.org/W2949911645', 'https://openalex.org/W2284660317', 'https://openalex.org/W2944815030', 'https://openalex.org/W2902169904', 'https://openalex.org/W6759363029', 'https://openalex.org/W2950428495', 'https://openalex.org/W630532510', 'https://openalex.org/W6739901393', 'https://openalex.org/W2025768430', 'https://openalex.org/W6762749209', 'https://openalex.org/W6864014924', 'https://openalex.org/W3016545553', 'https://openalex.org/W2989539713', 'https://openalex.org/W6863631769', 'https://openalex.org/W2964707311', 'https://openalex.org/W2798587284', 'https://openalex.org/W6730532957', 'https://openalex.org/W6791858558', 'https://openalex.org/W2740132093', 'https://openalex.org/W6868564194', 'https://openalex.org/W2741917668', 'https://openalex.org/W6771940952', 'https://openalex.org/W2970223278', 'https://openalex.org/W2337363174', 'https://openalex.org/W3152788712', 'https://openalex.org/W2952650870', 'https://openalex.org/W2965373594', 'https://openalex.org/W2980216782', 'https://openalex.org/W2962778428', 'https://openalex.org/W2964013027', 'https://openalex.org/W2964034111', 'https://openalex.org/W2963331137', 'https://openalex.org/W2995304149', 'https://openalex.org/W2963088995', 'https://openalex.org/W2546938941', 'https://openalex.org/W2963216553', 'https://openalex.org/W2963569817', 'https://openalex.org/W2958953787', 'https://openalex.org/W4294170691', 'https://openalex.org/W3036120435', 'https://openalex.org/W4385245566', 'https://openalex.org/W4288089799', 'https://openalex.org/W2963443683', 'https://openalex.org/W2914120296', 'https://openalex.org/W4298393544', 'https://openalex.org/W3090350559', 'https://openalex.org/W2921280978', 'https://openalex.org/W2970015022', 'https://openalex.org/W2997518171', 'https://openalex.org/W2964091381', 'https://openalex.org/W2970109976', 'https://openalex.org/W2952614664', 'https://openalex.org/W2963413917', 'https://openalex.org/W2906987001', 'https://openalex.org/W2950485982', 'https://openalex.org/W2951065878', 'https://openalex.org/W3175301726', 'https://openalex.org/W2896457183', 'https://openalex.org/W2962824887', 'https://openalex.org/W3107826490', 'https://openalex.org/W2970310632', 'https://openalex.org/W2953190730', 'https://openalex.org/W2889326796', 'https://openalex.org/W2932618389', 'https://openalex.org/W2973088264', 'https://openalex.org/W2970925677', 'https://openalex.org/W3175871055', 'https://openalex.org/W2998458489', 'https://openalex.org/W3006381853', 'https://openalex.org/W2798931235', 'https://openalex.org/W3035390927', 'https://openalex.org/W2963842982', 'https://openalex.org/W3091540052', 'https://openalex.org/W2542860122', 'https://openalex.org/W2966618235', 'https://openalex.org/W2964007535', 'https://openalex.org/W2952468927', 'https://openalex.org/W3017454464', 'https://openalex.org/W3035531963', 'https://openalex.org/W4244167344', 'https://openalex.org/W2950733326', 'https://openalex.org/W2963532104', 'https://openalex.org/W2561274697', 'https://openalex.org/W2968447912', 'https://openalex.org/W3035019713']",2021-08-01
https://openalex.org/W3163906773,https://doi.org/10.1109/icassp39728.2021.9413934,Denoispeech: Denoising Text to Speech with Frame-Level Noise Modeling,"While neural-based text to speech (TTS) models can synthesize natural and intelligible voice, they usually require high-quality speech data, which is costly to collect. In many scenarios, only noisy speech of a target speaker is available, which presents challenges for TTS model training for this speaker. Previous works usually address the challenge using two methods: 1) training the TTS model using the speech denoised with an enhancement model; 2) taking a single noise embedding as input when training with noisy speech. However, they usually cannot handle speech with real-world complicated noise such as those with high variations along time. In this paper, we develop DenoiSpeech, a TTS system that can synthesize clean speech for a speaker with noisy speech data. In DenoiSpeech, we handle real-world noisy speech by modeling the fine-grained frame-level noise with a noise condition module, which is jointly trained with the TTS model. Experimental results on real-world data show that DenoiSpeech outperforms the previous two methods by 0.31 and 0.66 MOS respectively. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>","['https://openalex.org/W2963341071', 'https://openalex.org/W6727697161', 'https://openalex.org/W2159202424', 'https://openalex.org/W2747874407', 'https://openalex.org/W6780274444', 'https://openalex.org/W6631190155', 'https://openalex.org/W2133665775', 'https://openalex.org/W3095883095', 'https://openalex.org/W2127141656', 'https://openalex.org/W3015338123', 'https://openalex.org/W6778215197', 'https://openalex.org/W2801554275', 'https://openalex.org/W6755300632', 'https://openalex.org/W2998161426', 'https://openalex.org/W6778823374', 'https://openalex.org/W2907262790', 'https://openalex.org/W2603567530', 'https://openalex.org/W2514828952', 'https://openalex.org/W6639824700', 'https://openalex.org/W1901129140', 'https://openalex.org/W3029282897', 'https://openalex.org/W3175871055', 'https://openalex.org/W2963568578', 'https://openalex.org/W3033411150', 'https://openalex.org/W3130016944', 'https://openalex.org/W1522301498', 'https://openalex.org/W2964121744', 'https://openalex.org/W2527729766', 'https://openalex.org/W4289383906']",2021-05-13
https://openalex.org/W4385570550,https://doi.org/10.18653/v1/2023.acl-long.872,UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units,"Hirofumi Inaguma, Sravya Popuri, Ilia Kulikov, Peng-Jen Chen, Changhan Wang, Yu-An Chung, Yun Tang, Ann Lee, Shinji Watanabe, Juan Pino. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.","['https://openalex.org/W3193521535', 'https://openalex.org/W2972495969', 'https://openalex.org/W4223622550', 'https://openalex.org/W2963799213', 'https://openalex.org/W2752047430', 'https://openalex.org/W3186200218', 'https://openalex.org/W3006381853', 'https://openalex.org/W2949328740', 'https://openalex.org/W4307770080', 'https://openalex.org/W2292087804', 'https://openalex.org/W2250357346', 'https://openalex.org/W2161192091', 'https://openalex.org/W3142316150', 'https://openalex.org/W3035390927', 'https://openalex.org/W4226033575', 'https://openalex.org/W2133564696', 'https://openalex.org/W3140429000', 'https://openalex.org/W2972625221', 'https://openalex.org/W2933138175', 'https://openalex.org/W4372349107', 'https://openalex.org/W2963779652', 'https://openalex.org/W2972802841', 'https://openalex.org/W3168212167', 'https://openalex.org/W3107826490', 'https://openalex.org/W3095410713', 'https://openalex.org/W2763421725', 'https://openalex.org/W4280617721', 'https://openalex.org/W3036601975', 'https://openalex.org/W4287333042', 'https://openalex.org/W4285188582', 'https://openalex.org/W2958953787', 'https://openalex.org/W2605131327', 'https://openalex.org/W3175301726', 'https://openalex.org/W3175871055', 'https://openalex.org/W2972448360', 'https://openalex.org/W4287072252', 'https://openalex.org/W2136545725', 'https://openalex.org/W3011339933', 'https://openalex.org/W2938973646', 'https://openalex.org/W4285158119', 'https://openalex.org/W2572474373', 'https://openalex.org/W4221155340', 'https://openalex.org/W4226444650', 'https://openalex.org/W3103182178', 'https://openalex.org/W3172862365', 'https://openalex.org/W3097777922', 'https://openalex.org/W1494198834', 'https://openalex.org/W4385893869', 'https://openalex.org/W3135335819', 'https://openalex.org/W3169320628', 'https://openalex.org/W3091928890', 'https://openalex.org/W3176711365', 'https://openalex.org/W3196509775', 'https://openalex.org/W3007068036', 'https://openalex.org/W2995181338', 'https://openalex.org/W3015698636', 'https://openalex.org/W4385245566', 'https://openalex.org/W3173767661', 'https://openalex.org/W2130942839', 'https://openalex.org/W3119308075', 'https://openalex.org/W2964161387', 'https://openalex.org/W2183341477', 'https://openalex.org/W2046932483', 'https://openalex.org/W4300558631', 'https://openalex.org/W22168010', 'https://openalex.org/W2963250244', 'https://openalex.org/W1538023239', 'https://openalex.org/W4287213456', 'https://openalex.org/W3139878283', 'https://openalex.org/W2936969148', 'https://openalex.org/W3030437843', 'https://openalex.org/W2963979492', 'https://openalex.org/W2806412155', 'https://openalex.org/W3092028330', 'https://openalex.org/W2157331557', 'https://openalex.org/W3012492057', 'https://openalex.org/W2973122799', 'https://openalex.org/W4287694131', 'https://openalex.org/W2963887123', 'https://openalex.org/W3139918052', 'https://openalex.org/W2903739847', 'https://openalex.org/W2095705004', 'https://openalex.org/W4226054021', 'https://openalex.org/W3100806282', 'https://openalex.org/W4281789500', 'https://openalex.org/W3174864715', 'https://openalex.org/W4287854499', 'https://openalex.org/W4221153524', 'https://openalex.org/W4296070387', 'https://openalex.org/W2963532001', 'https://openalex.org/W338621447', 'https://openalex.org/W3118578889']",2023-01-01
https://openalex.org/W4372191700,https://doi.org/10.1109/icassp49357.2023.10095616,Joint Pre-Training with Speech and Bilingual Text for Direct Speech to Speech Translation,"Direct speech-to-speech translation (S2ST) is an attractive research topic with many advantages compared to cascaded S2ST. However, direct S2ST suffers from the data scarcity problem because the corpora from the speech of the source language to the speech of the target language are very rare. To address this issue, we propose in this paper a Speech2S model, which is jointly pre-trained with unpaired speech and bilingual text data for direct speech-to-speech translation tasks. By effectively leveraging the paired text data, Speech2S is capable of modeling the cross-lingual speech conversion from source to target language. We verify the performance of the proposed Speech2S on Europarl-ST and VoxPopuli datasets. Experimental results demonstrate that Speech2S gets an improvement of about 5 BLEU scores compared to encoder-only pre-training models, and achieves a competitive or even better performance than existing state-of-the-art models <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .","['https://openalex.org/W6787407267', 'https://openalex.org/W6780218876', 'https://openalex.org/W3015698636', 'https://openalex.org/W3209059054', 'https://openalex.org/W6799081819', 'https://openalex.org/W6810259195', 'https://openalex.org/W6798080464', 'https://openalex.org/W2972495969', 'https://openalex.org/W2097203679', 'https://openalex.org/W2136545725', 'https://openalex.org/W2582956876', 'https://openalex.org/W1537859740', 'https://openalex.org/W3001434439', 'https://openalex.org/W6810419249', 'https://openalex.org/W4385573012', 'https://openalex.org/W6739901393', 'https://openalex.org/W3007068036', 'https://openalex.org/W3175871055', 'https://openalex.org/W3142316150', 'https://openalex.org/W2970550739', 'https://openalex.org/W6600880057', 'https://openalex.org/W2963532001', 'https://openalex.org/W4280601369', 'https://openalex.org/W4385572318', 'https://openalex.org/W4221153524', 'https://openalex.org/W6846738872', 'https://openalex.org/W4296070387', 'https://openalex.org/W6810701745', 'https://openalex.org/W3119308075', 'https://openalex.org/W22168010', 'https://openalex.org/W3180374548', 'https://openalex.org/W3173767661', 'https://openalex.org/W4226444650', 'https://openalex.org/W4221155340', 'https://openalex.org/W4385245566', 'https://openalex.org/W4385893869', 'https://openalex.org/W4287854499', 'https://openalex.org/W3186843219', 'https://openalex.org/W3036601975', 'https://openalex.org/W2949328740']",2023-05-05
https://openalex.org/W4390725317,https://doi.org/10.1145/3639930,Multi-granularity Knowledge Sharing in Low-resource Neural Machine Translation,"As the rapid development of deep learning methods, neural machine translation (NMT) has attracted more and more attention in recent years. However, lack of bilingual resources decreases the performance of the low-resource NMT model seriously. To overcome this problem, several studies put their efforts on knowledge transfer from high-resource language pairs to low-resource language pairs. However, these methods usually focus on one single granularity of language and the parameter sharing among different granularities in NMT is not well studied. In this article, we propose to improve the parameter sharing in low-resource NMT by introducing multi-granularity knowledge such as word, phrase and sentence. This knowledge can be monolingual and bilingual. We build the knowledge sharing model for low-resource NMT based on a multi-task learning framework, three auxiliary tasks such as syntax parsing, cross-lingual named entity recognition, and natural language generation are selected for the low-resource NMT. Experimental results show that the proposed method consistently outperforms six strong baseline systems on several low-resource language pairs.","['https://openalex.org/W2963499882', 'https://openalex.org/W2997244573', 'https://openalex.org/W3038876298', 'https://openalex.org/W2966618235', 'https://openalex.org/W3090350559', 'https://openalex.org/W3173367141', 'https://openalex.org/W3132607382', 'https://openalex.org/W3198189804', 'https://openalex.org/W2970682957', 'https://openalex.org/W3173644772', 'https://openalex.org/W2951338945', 'https://openalex.org/W2032175749', 'https://openalex.org/W2945381748', 'https://openalex.org/W2119168550', 'https://openalex.org/W2933138175', 'https://openalex.org/W2101105183', 'https://openalex.org/W2970641574', 'https://openalex.org/W2611838487', 'https://openalex.org/W3001816066', 'https://openalex.org/W2963216553', 'https://openalex.org/W3193077216', 'https://openalex.org/W3105038888', 'https://openalex.org/W2964296923', 'https://openalex.org/W2903810591', 'https://openalex.org/W3035548285', 'https://openalex.org/W2997732209', 'https://openalex.org/W2798465082', 'https://openalex.org/W3175871055', 'https://openalex.org/W3141797743', 'https://openalex.org/W2970798168', 'https://openalex.org/W2616647696', 'https://openalex.org/W4365799947']",2024-01-09
https://openalex.org/W3208643357,https://doi.org/10.1109/asru51503.2021.9688073,Assessing Evaluation Metrics for Speech-to-Speech Translation,"Speech-to-speech translation combines machine translation with speech synthesis, introducing evaluation challenges not present in either task alone. How to automatically evaluate speech-to-speech translation is an open question which has not previously been explored. Translating to speech rather than to text is often motivated by unwritten languages or languages without standardized orthographies. However, we show that the previously used automatic metric for this task is best equipped for standardized high-resource languages only. In this work, we first evaluate current metrics for speech-to-speech translation, and second assess how translation to dialectal variants rather than to standardized languages impacts various evaluation methods.","['https://openalex.org/W2262393948', 'https://openalex.org/W6788656616', 'https://openalex.org/W3008480565', 'https://openalex.org/W1995945562', 'https://openalex.org/W2963506925', 'https://openalex.org/W6732385369', 'https://openalex.org/W6752051364', 'https://openalex.org/W6775579118', 'https://openalex.org/W6786041167', 'https://openalex.org/W6780274444', 'https://openalex.org/W3142316150', 'https://openalex.org/W1989168222', 'https://openalex.org/W2101105183', 'https://openalex.org/W2250342921', 'https://openalex.org/W2963532001', 'https://openalex.org/W4240592325', 'https://openalex.org/W2972802841', 'https://openalex.org/W6630326474', 'https://openalex.org/W2139647714', 'https://openalex.org/W2136545725', 'https://openalex.org/W3163816166', 'https://openalex.org/W6761157444', 'https://openalex.org/W2536650987', 'https://openalex.org/W6712235671', 'https://openalex.org/W6768063509', 'https://openalex.org/W2933138175', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963250244', 'https://openalex.org/W3103702065', 'https://openalex.org/W2519091744', 'https://openalex.org/W2949382160', 'https://openalex.org/W3161782335', 'https://openalex.org/W2555745756', 'https://openalex.org/W2972495969', 'https://openalex.org/W1526763309', 'https://openalex.org/W2396366106', 'https://openalex.org/W2972401849', 'https://openalex.org/W1502131496', 'https://openalex.org/W3032120593', 'https://openalex.org/W2494980014', 'https://openalex.org/W1538023239', 'https://openalex.org/W4287262574', 'https://openalex.org/W3120527562', 'https://openalex.org/W3175871055', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963403868', 'https://openalex.org/W2797563284', 'https://openalex.org/W2972830686', 'https://openalex.org/W2575880437', 'https://openalex.org/W3136452804', 'https://openalex.org/W2100398002', 'https://openalex.org/W2805278996', 'https://openalex.org/W2936184970', 'https://openalex.org/W4287822193']",2021-12-13
https://openalex.org/W4389611346,https://doi.org/10.1109/ialp61005.2023.10337040,Multi-Task Self-Supervised Learning Based Tibetan-Chinese Speech-to-Speech Translation,"Speech-to-speech translation tasks are commonly tackled by using a three-level cascade system which comprises of speech recognition, machine translation, and speech synthesis. However, this approach suffers from the drawback of error accumulation at each stage. In contrast, the direct speech-to-speech translation model directly converts speech from the source language to the target language without relying on intermediate text generation, thereby avoiding the issue of incorrect transmission in cascading systems. Currently, there exist two categories for direct speech-to-speech translation methods. The first involves mapping the Mel-spectrogram of the source language speech to the Mel-spectrogram of the target language speech. However, this method often encounters challenges in convergence and producing the audible speech for the target language. The second type of methods is to learn a self-supervised discrete representation of the target language using an unlabeled speech corpus. This method entails training a sequence-to-sequence model on a real-world dataset, which then maps the source language speech to the discrete representation of the target language. Finally, a separately trained vocoder is utilized to convert the discrete unit sequence into a speech waveform. Given the limited availability of large-scale Tibetan-Chinese parallel speech corpora, this work adopts the second method to model Tibetan-Chinese speech-to-speech translation tasks. Additionally, a multi-task learning framework is designed in this work to enhance the performance of the speech translation model. Experimental results demonstrate that the Tibetan-Chinese speech-to-speech translation model based on multi-task self-supervised learning outperforms both the model based on spectrogram mapping and the single-task self-supervised learning model in terms of achieving a higher BLUE value.","['https://openalex.org/W2136545725', 'https://openalex.org/W2097203679', 'https://openalex.org/W2582956876', 'https://openalex.org/W3175871055', 'https://openalex.org/W2972495969', 'https://openalex.org/W3180374548', 'https://openalex.org/W6761447021', 'https://openalex.org/W1494198834', 'https://openalex.org/W6739901393', 'https://openalex.org/W3209059054', 'https://openalex.org/W6810701745', 'https://openalex.org/W3196509775', 'https://openalex.org/W4367841185', 'https://openalex.org/W4280601369', 'https://openalex.org/W2752796333', 'https://openalex.org/W6771467084', 'https://openalex.org/W3142316150', 'https://openalex.org/W3007068036', 'https://openalex.org/W6783867762', 'https://openalex.org/W6778823374', 'https://openalex.org/W3197580070', 'https://openalex.org/W6790356757', 'https://openalex.org/W3140429000', 'https://openalex.org/W2963532001', 'https://openalex.org/W4226444650', 'https://openalex.org/W4385245566', 'https://openalex.org/W3092028330', 'https://openalex.org/W3033411150', 'https://openalex.org/W4394671563', 'https://openalex.org/W2912083425']",2023-11-18
https://openalex.org/W4389519423,https://doi.org/10.18653/v1/2023.emnlp-main.709,DiffS2UT: A Semantic Preserving Diffusion Model for Textless Direct Speech-to-Speech Translation,"While Diffusion Generative Models have achieved great success on image generation tasks, how to efficiently and effectively incorporate them into speech generation especially translation tasks remains a non-trivial problem. Specifically, due to the low information density of speech data, the transformed discrete speech unit sequence is much longer than the corresponding text transcription, posing significant challenges to existing auto-regressive models. Furthermore, it is not optimal to brutally apply discrete diffusion on the speech unit sequence while disregarding the continuous space structure, which will degrade the generation performance significantly. In this paper, we propose a novel diffusion model by applying the diffusion forward process in the continuous speech representation space, while employing the diffusion backward process in the discrete speech unit space. In this way, we preserve the semantic structure of the continuous speech representation space in the diffusion process and integrate the continuous and discrete diffusion models. We conduct extensive experiments on the textless direct speech-to-speech translation task, where the proposed method achieves comparable results to the computationally intensive auto-regressive baselines (500 steps on average) with significantly fewer decoding steps (50 steps).","['https://openalex.org/W2933138175', 'https://openalex.org/W4287083626', 'https://openalex.org/W2963736842', 'https://openalex.org/W3092028330', 'https://openalex.org/W2963532001', 'https://openalex.org/W3140429000', 'https://openalex.org/W3175871055', 'https://openalex.org/W4287854499', 'https://openalex.org/W2998702515', 'https://openalex.org/W3110257065', 'https://openalex.org/W2136545725', 'https://openalex.org/W4287079508', 'https://openalex.org/W3169320628', 'https://openalex.org/W3015698636', 'https://openalex.org/W3024786184', 'https://openalex.org/W3210177631', 'https://openalex.org/W4312933868', 'https://openalex.org/W3036601975', 'https://openalex.org/W4385571229', 'https://openalex.org/W3172148458', 'https://openalex.org/W4306802991', 'https://openalex.org/W4385572318', 'https://openalex.org/W4300425011', 'https://openalex.org/W4287329820', 'https://openalex.org/W3142316150', 'https://openalex.org/W3107826490', 'https://openalex.org/W2962784628', 'https://openalex.org/W2129069237', 'https://openalex.org/W3213018012', 'https://openalex.org/W4281789500', 'https://openalex.org/W2767206889', 'https://openalex.org/W3007068036', 'https://openalex.org/W3036167779', 'https://openalex.org/W4287072252', 'https://openalex.org/W3119308075', 'https://openalex.org/W4385245566', 'https://openalex.org/W4312051726', 'https://openalex.org/W2972495969', 'https://openalex.org/W3168053944', 'https://openalex.org/W2097203679', 'https://openalex.org/W4372191700', 'https://openalex.org/W4366460484']",2023-01-01
https://openalex.org/W3174758275,https://doi.org/10.48550/arxiv.2106.15561,A Survey on Neural Speech Synthesis,"Text to speech (TTS), or speech synthesis, which aims to synthesize intelligible and natural speech given text, is a hot research topic in speech, language, and machine learning communities and has broad applications in the industry. As the development of deep learning and artificial intelligence, neural network-based TTS has significantly improved the quality of synthesized speech in recent years. In this paper, we conduct a comprehensive survey on neural TTS, aiming to provide a good understanding of current research and future trends. We focus on the key components in neural TTS, including text analysis, acoustic models and vocoders, and several advanced topics, including fast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc. We further summarize resources related to TTS (e.g., datasets, opensource implementations) and discuss future research directions. This survey can serve both academic researchers and industry practitioners working on TTS.","['https://openalex.org/W2104012431', 'https://openalex.org/W2069859485', 'https://openalex.org/W3015796413', 'https://openalex.org/W2105594594', 'https://openalex.org/W3097587372', 'https://openalex.org/W2963330667', 'https://openalex.org/W2997540646', 'https://openalex.org/W3031409582', 'https://openalex.org/W2963192573', 'https://openalex.org/W2940405045', 'https://openalex.org/W2962793481', 'https://openalex.org/W3082910224', 'https://openalex.org/W3144667183', 'https://openalex.org/W854541894', 'https://openalex.org/W3091928890', 'https://openalex.org/W1821462560', 'https://openalex.org/W2888456631', 'https://openalex.org/W3095790275', 'https://openalex.org/W3146431306', 'https://openalex.org/W3194208059', 'https://openalex.org/W3113850747', 'https://openalex.org/W2963927338', 'https://openalex.org/W2000513720', 'https://openalex.org/W2964281804', 'https://openalex.org/W3128930043', 'https://openalex.org/W2963302875', 'https://openalex.org/W3098869438', 'https://openalex.org/W2428180336', 'https://openalex.org/W3096656663', 'https://openalex.org/W3096086473', 'https://openalex.org/W1861172732', 'https://openalex.org/W3015645837', 'https://openalex.org/W423487555', 'https://openalex.org/W3094402293', 'https://openalex.org/W3133185996', 'https://openalex.org/W3139170550', 'https://openalex.org/W2943543019', 'https://openalex.org/W2963712897', 'https://openalex.org/W3048023795', 'https://openalex.org/W3020570669', 'https://openalex.org/W2972359262', 'https://openalex.org/W648786980', 'https://openalex.org/W3094785744', 'https://openalex.org/W28194048', 'https://openalex.org/W2962824709', 'https://openalex.org/W2991417167', 'https://openalex.org/W2990440871', 'https://openalex.org/W2972882294', 'https://openalex.org/W3099992561', 'https://openalex.org/W2401698713', 'https://openalex.org/W3095389792', 'https://openalex.org/W2976159681', 'https://openalex.org/W3095762785', 'https://openalex.org/W3037695135', 'https://openalex.org/W2919115771', 'https://openalex.org/W2546744831', 'https://openalex.org/W3023706973', 'https://openalex.org/W3097895838', 'https://openalex.org/W3028954259', 'https://openalex.org/W2009674825', 'https://openalex.org/W2904351664', 'https://openalex.org/W3174847158', 'https://openalex.org/W2962736171', 'https://openalex.org/W2973034126', 'https://openalex.org/W2963736842', 'https://openalex.org/W3146550708', 'https://openalex.org/W2765486990', 'https://openalex.org/W2924677654', 'https://openalex.org/W2972519044', 'https://openalex.org/W3098304089', 'https://openalex.org/W2972956685', 'https://openalex.org/W2945656493', 'https://openalex.org/W1608375737', 'https://openalex.org/W3156871171', 'https://openalex.org/W3094650042', 'https://openalex.org/W2964167449', 'https://openalex.org/W2584032004', 'https://openalex.org/W2964307104', 'https://openalex.org/W2152205330', 'https://openalex.org/W3163906773', 'https://openalex.org/W2963912924', 'https://openalex.org/W3169905056', 'https://openalex.org/W3103104054', 'https://openalex.org/W2994373303', 'https://openalex.org/W9334768', 'https://openalex.org/W2147880316', 'https://openalex.org/W3133963484', 'https://openalex.org/W3095410713', 'https://openalex.org/W3097795905', 'https://openalex.org/W3093185524', 'https://openalex.org/W2120847449', 'https://openalex.org/W3033194228', 'https://openalex.org/W2250357346', 'https://openalex.org/W2917688842', 'https://openalex.org/W2605141709', 'https://openalex.org/W2947723875', 'https://openalex.org/W3097264669', 'https://openalex.org/W3096082720', 'https://openalex.org/W2749651610', 'https://openalex.org/W3165905282', 'https://openalex.org/W2970997853', 'https://openalex.org/W3036167779', 'https://openalex.org/W3092316169', 'https://openalex.org/W1492383498', 'https://openalex.org/W3107262928', 'https://openalex.org/W3104081910', 'https://openalex.org/W2963827314', 'https://openalex.org/W2964138190', 'https://openalex.org/W2801554275', 'https://openalex.org/W3171962689', 'https://openalex.org/W3098403858', 'https://openalex.org/W3015853838', 'https://openalex.org/W3030437843', 'https://openalex.org/W2907262790', 'https://openalex.org/W2093450784', 'https://openalex.org/W3125708547', 'https://openalex.org/W3104557543', 'https://openalex.org/W3021902032', 'https://openalex.org/W3097032879', 'https://openalex.org/W2963432880', 'https://openalex.org/W2516559027', 'https://openalex.org/W67332896', 'https://openalex.org/W2163377725', 'https://openalex.org/W3015651285', 'https://openalex.org/W3015347659', 'https://openalex.org/W2964301388', 'https://openalex.org/W2103636088', 'https://openalex.org/W2395578248', 'https://openalex.org/W2922185715', 'https://openalex.org/W2964343746', 'https://openalex.org/W2587284713', 'https://openalex.org/W3162673269', 'https://openalex.org/W3048217770', 'https://openalex.org/W3140294857', 'https://openalex.org/W3206275820', 'https://openalex.org/W3015841875', 'https://openalex.org/W3181257032', 'https://openalex.org/W3134361330', 'https://openalex.org/W2889048668', 'https://openalex.org/W2977311057', 'https://openalex.org/W2963300588', 'https://openalex.org/W2173520492', 'https://openalex.org/W3095851005', 'https://openalex.org/W2949382160', 'https://openalex.org/W2127141656', 'https://openalex.org/W2493353997', 'https://openalex.org/W2885800352', 'https://openalex.org/W2962882868', 'https://openalex.org/W2940544976', 'https://openalex.org/W2963090522', 'https://openalex.org/W3016159759', 'https://openalex.org/W2972440097', 'https://openalex.org/W3155768880', 'https://openalex.org/W3015922793', 'https://openalex.org/W3025013833', 'https://openalex.org/W3098557217', 'https://openalex.org/W3092442149', 'https://openalex.org/W3090474612', 'https://openalex.org/W2526838772', 'https://openalex.org/W3169874582', 'https://openalex.org/W2972951102', 'https://openalex.org/W2066452495', 'https://openalex.org/W2969407538', 'https://openalex.org/W2964268978', 'https://openalex.org/W3016137096', 'https://openalex.org/W2972375191', 'https://openalex.org/W2903739847', 'https://openalex.org/W2111284386', 'https://openalex.org/W3088782775', 'https://openalex.org/W2991571120', 'https://openalex.org/W2145892079', 'https://openalex.org/W2296704011', 'https://openalex.org/W2937297115', 'https://openalex.org/W1861945760', 'https://openalex.org/W2096980176', 'https://openalex.org/W2912237252', 'https://openalex.org/W2970730223', 'https://openalex.org/W3048768961', 'https://openalex.org/W3144035034', 'https://openalex.org/W2072026461', 'https://openalex.org/W2962699523', 'https://openalex.org/W1964668808', 'https://openalex.org/W3162948689', 'https://openalex.org/W3095419383', 'https://openalex.org/W3168470634', 'https://openalex.org/W2806253224', 'https://openalex.org/W2471520273', 'https://openalex.org/W2963620343', 'https://openalex.org/W3129651364', 'https://openalex.org/W2102737569', 'https://openalex.org/W3175871055', 'https://openalex.org/W3034794073', 'https://openalex.org/W1583912456', 'https://openalex.org/W3126404598', 'https://openalex.org/W2951004968', 'https://openalex.org/W2996286887', 'https://openalex.org/W2970079483', 'https://openalex.org/W2542835211', 'https://openalex.org/W2963078821', 'https://openalex.org/W1999885698', 'https://openalex.org/W3141224548', 'https://openalex.org/W2154920538', 'https://openalex.org/W2293049663', 'https://openalex.org/W2254292464', 'https://openalex.org/W2963929654', 'https://openalex.org/W1576227399', 'https://openalex.org/W3103034946', 'https://openalex.org/W2963047245', 'https://openalex.org/W3102451458', 'https://openalex.org/W2972457126', 'https://openalex.org/W2327501763', 'https://openalex.org/W2973203693', 'https://openalex.org/W174066062', 'https://openalex.org/W2071764087', 'https://openalex.org/W3149617007', 'https://openalex.org/W3134835907', 'https://openalex.org/W1600722501', 'https://openalex.org/W3114532049', 'https://openalex.org/W3015680182', 'https://openalex.org/W2999160446', 'https://openalex.org/W2954386831', 'https://openalex.org/W3015614880', 'https://openalex.org/W2403419258', 'https://openalex.org/W2964272710', 'https://openalex.org/W3125481789', 'https://openalex.org/W3158102353', 'https://openalex.org/W2026679866', 'https://openalex.org/W3141402801', 'https://openalex.org/W3016136182', 'https://openalex.org/W2894295011', 'https://openalex.org/W3131702728', 'https://openalex.org/W2998458489', 'https://openalex.org/W2963522141', 'https://openalex.org/W3172617364', 'https://openalex.org/W3015419784', 'https://openalex.org/W1916501714', 'https://openalex.org/W2164107060', 'https://openalex.org/W2972895078', 'https://openalex.org/W2049950192', 'https://openalex.org/W3034949308', 'https://openalex.org/W2884873108', 'https://openalex.org/W3141350488', 'https://openalex.org/W3016160783', 'https://openalex.org/W3097003111', 'https://openalex.org/W3097728852', 'https://openalex.org/W2963534259', 'https://openalex.org/W2972333964', 'https://openalex.org/W2972473628', 'https://openalex.org/W3161109662', 'https://openalex.org/W2112800201', 'https://openalex.org/W2612199078', 'https://openalex.org/W1494198834', 'https://openalex.org/W2972374322', 'https://openalex.org/W3006108364', 'https://openalex.org/W2970006822', 'https://openalex.org/W2904459034', 'https://openalex.org/W3024747869', 'https://openalex.org/W3177989406', 'https://openalex.org/W162654330', 'https://openalex.org/W3096442195', 'https://openalex.org/W3035083561', 'https://openalex.org/W2182919134', 'https://openalex.org/W3035068567', 'https://openalex.org/W2964243274', 'https://openalex.org/W3097538987', 'https://openalex.org/W3147900189', 'https://openalex.org/W2962936105', 'https://openalex.org/W3097290232', 'https://openalex.org/W2603120115', 'https://openalex.org/W3161492781', 'https://openalex.org/W3015440759', 'https://openalex.org/W3111791812', 'https://openalex.org/W2166469361', 'https://openalex.org/W3081416955', 'https://openalex.org/W3153370059', 'https://openalex.org/W3041574813', 'https://openalex.org/W3148333649', 'https://openalex.org/W3163339651', 'https://openalex.org/W3135619128', 'https://openalex.org/W3015826515', 'https://openalex.org/W3047107405', 'https://openalex.org/W2122410182', 'https://openalex.org/W3130016944', 'https://openalex.org/W1526236009', 'https://openalex.org/W3007419862', 'https://openalex.org/W1810943226', 'https://openalex.org/W3150713669', 'https://openalex.org/W3025528898', 'https://openalex.org/W2963091184', 'https://openalex.org/W2963609956', 'https://openalex.org/W3178839419', 'https://openalex.org/W3167266074', 'https://openalex.org/W2808342999', 'https://openalex.org/W3169744561', 'https://openalex.org/W2294797155', 'https://openalex.org/W2990124956', 'https://openalex.org/W2252225757', 'https://openalex.org/W2747874407', 'https://openalex.org/W3097144763', 'https://openalex.org/W2162433174', 'https://openalex.org/W2972885185', 'https://openalex.org/W2972802841', 'https://openalex.org/W2888169323', 'https://openalex.org/W3095883095', 'https://openalex.org/W3150807214', 'https://openalex.org/W3130774171', 'https://openalex.org/W3029282897', 'https://openalex.org/W2963035245', 'https://openalex.org/W2929299742', 'https://openalex.org/W2550497374', 'https://openalex.org/W3123097577', 'https://openalex.org/W2973217961', 'https://openalex.org/W3015484365', 'https://openalex.org/W3015338123', 'https://openalex.org/W3094124570', 'https://openalex.org/W3160289600', 'https://openalex.org/W3022876224', 'https://openalex.org/W3096429957', 'https://openalex.org/W2972595148', 'https://openalex.org/W7606746', 'https://openalex.org/W2801291345', 'https://openalex.org/W2964308564', 'https://openalex.org/W2973084242', 'https://openalex.org/W2806733747', 'https://openalex.org/W1663973292', 'https://openalex.org/W3096702751', 'https://openalex.org/W2917245127', 'https://openalex.org/W3146104336', 'https://openalex.org/W2064076387', 'https://openalex.org/W3112470437', 'https://openalex.org/W3095491807', 'https://openalex.org/W2963782041', 'https://openalex.org/W3095545636', 'https://openalex.org/W2337335398', 'https://openalex.org/W2945078028', 'https://openalex.org/W2914049472', 'https://openalex.org/W3097376233', 'https://openalex.org/W2963242190', 'https://openalex.org/W3135197092', 'https://openalex.org/W2515943672', 'https://openalex.org/W2963568578', 'https://openalex.org/W2251811146', 'https://openalex.org/W3096456328', 'https://openalex.org/W2766812927', 'https://openalex.org/W2970925677', 'https://openalex.org/W2952127920', 'https://openalex.org/W3142087749', 'https://openalex.org/W2945544731', 'https://openalex.org/W3164133896', 'https://openalex.org/W2593414223', 'https://openalex.org/W3147652402', 'https://openalex.org/W2553303224', 'https://openalex.org/W3095787156', 'https://openalex.org/W2901997113', 'https://openalex.org/W2973190645', 'https://openalex.org/W2559246505', 'https://openalex.org/W1549877291', 'https://openalex.org/W2889028433', 'https://openalex.org/W1593247906', 'https://openalex.org/W2948211236', 'https://openalex.org/W3160919572', 'https://openalex.org/W2972574864', 'https://openalex.org/W2102003408', 'https://openalex.org/W3095199334', 'https://openalex.org/W2963796886', 'https://openalex.org/W3140429000', 'https://openalex.org/W2963975282', 'https://openalex.org/W1761875812', 'https://openalex.org/W2971905065', 'https://openalex.org/W3174572320', 'https://openalex.org/W3141515915', 'https://openalex.org/W1514101016', 'https://openalex.org/W2616705520', 'https://openalex.org/W2962739369', 'https://openalex.org/W3172387178', 'https://openalex.org/W3153534135', 'https://openalex.org/W3161782335', 'https://openalex.org/W3095012670', 'https://openalex.org/W3154443551', 'https://openalex.org/W3097297926', 'https://openalex.org/W2972597685', 'https://openalex.org/W2963403868', 'https://openalex.org/W2906797124', 'https://openalex.org/W2963175743', 'https://openalex.org/W2134973740', 'https://openalex.org/W3095990227', 'https://openalex.org/W2090755665', 'https://openalex.org/W3144988954', 'https://openalex.org/W3212683077', 'https://openalex.org/W3155727213', 'https://openalex.org/W2064675550', 'https://openalex.org/W2527729766', 'https://openalex.org/W2996573371', 'https://openalex.org/W2150658333', 'https://openalex.org/W1973424788', 'https://openalex.org/W3134354180', 'https://openalex.org/W3151450932', 'https://openalex.org/W2962691331', 'https://openalex.org/W2964169091', 'https://openalex.org/W2409550820', 'https://openalex.org/W3007859642', 'https://openalex.org/W2889606145', 'https://openalex.org/W3097514409', 'https://openalex.org/W2049686551', 'https://openalex.org/W3149705978', 'https://openalex.org/W3016090709', 'https://openalex.org/W2968203043', 'https://openalex.org/W1901616594', 'https://openalex.org/W3095459301', 'https://openalex.org/W1570629387', 'https://openalex.org/W3128910262', 'https://openalex.org/W2905933322', 'https://openalex.org/W1574901103', 'https://openalex.org/W2969521066', 'https://openalex.org/W3139510151', 'https://openalex.org/W2926840633', 'https://openalex.org/W3093733783', 'https://openalex.org/W2605320104', 'https://openalex.org/W3095999419', 'https://openalex.org/W2995233853', 'https://openalex.org/W2962879692', 'https://openalex.org/W1578102511', 'https://openalex.org/W2963308316', 'https://openalex.org/W3096303254', 'https://openalex.org/W2099471712', 'https://openalex.org/W2972999331', 'https://openalex.org/W3094002217']",2021-06-29
https://openalex.org/W3186843219,,Translatotron 2: Robust direct speech-to-speech translation.,"We present Translatotron 2, a neural direct speech-to-speech translation model that can be trained end-to-end. Translatotron 2 consists of a speech encoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention module that connects all the previous three components. Experimental results suggest that Translatotron 2 outperforms the original Translatotron by a large margin in terms of translation quality and predicted speech naturalness, and drastically improves the robustness of the predicted speech by mitigating over-generation, such as babbling or long pause. We also propose a new method for retaining the source speaker's voice in the translated speech. The trained model is restricted to retain the source speaker's voice, and unlike the original Translatotron, it is not able to generate speech in a different speaker's voice, making the model more robust for production deployment, by mitigating potential misuse for creating spoofing audio artifacts. When the new method is used together with a simple concatenation-based data augmentation, the trained Translatotron 2 model is able to retain each speaker's voice for input with speaker turns.","['https://openalex.org/W2972702018', 'https://openalex.org/W3012492057', 'https://openalex.org/W3130016944', 'https://openalex.org/W3097777922', 'https://openalex.org/W3007068036', 'https://openalex.org/W2097203679', 'https://openalex.org/W2963432880', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964243274', 'https://openalex.org/W1494198834', 'https://openalex.org/W3142316150', 'https://openalex.org/W2972495969', 'https://openalex.org/W3008181812', 'https://openalex.org/W2936774411', 'https://openalex.org/W2605131327', 'https://openalex.org/W3015440307', 'https://openalex.org/W2928941594', 'https://openalex.org/W3026041220', 'https://openalex.org/W2963590452', 'https://openalex.org/W2136545725', 'https://openalex.org/W2973212992', 'https://openalex.org/W2970730223', 'https://openalex.org/W3082130377', 'https://openalex.org/W3197324626', 'https://openalex.org/W3043665049', 'https://openalex.org/W3180374548', 'https://openalex.org/W3054645415', 'https://openalex.org/W2973084242', 'https://openalex.org/W2972359262', 'https://openalex.org/W2963799213', 'https://openalex.org/W2962788625', 'https://openalex.org/W3175871055', 'https://openalex.org/W2964161387', 'https://openalex.org/W3163793923', 'https://openalex.org/W2120847449', 'https://openalex.org/W2964307104', 'https://openalex.org/W3015922793', 'https://openalex.org/W2972473628', 'https://openalex.org/W3011535310', 'https://openalex.org/W3026777299', 'https://openalex.org/W3091928890', 'https://openalex.org/W3161296985', 'https://openalex.org/W3008125272', 'https://openalex.org/W3099782249', 'https://openalex.org/W3153287399', 'https://openalex.org/W3161436426', 'https://openalex.org/W3197294703']",2021-07-19
https://openalex.org/W4385570538,https://doi.org/10.18653/v1/2023.acl-long.479,AV-TranSpeech: Audio-Visual Robust Speech-to-Speech Translation,"Rongjie Huang, Huadai Liu, Xize Cheng, Yi Ren, Linjun Li, Zhenhui Ye, Jinzheng He, Lichao Zhang, Jinglin Liu, Xiang Yin, Zhou Zhao. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.","['https://openalex.org/W4306175879', 'https://openalex.org/W3193411928', 'https://openalex.org/W4287079508', 'https://openalex.org/W4285345683', 'https://openalex.org/W3154451338', 'https://openalex.org/W2890952074', 'https://openalex.org/W4285483538', 'https://openalex.org/W2972495969', 'https://openalex.org/W3119308075', 'https://openalex.org/W4287854499', 'https://openalex.org/W3036601975', 'https://openalex.org/W3092028330', 'https://openalex.org/W4297841864', 'https://openalex.org/W3097777922', 'https://openalex.org/W4303519914', 'https://openalex.org/W2933138175', 'https://openalex.org/W2891205112', 'https://openalex.org/W3175871055', 'https://openalex.org/W4281789500', 'https://openalex.org/W3016011581', 'https://openalex.org/W4296070387', 'https://openalex.org/W3206191467', 'https://openalex.org/W3140429000', 'https://openalex.org/W3015830103', 'https://openalex.org/W3054645415', 'https://openalex.org/W4385571229', 'https://openalex.org/W3169320628', 'https://openalex.org/W2963528589', 'https://openalex.org/W3007068036', 'https://openalex.org/W3096086473', 'https://openalex.org/W4221153524', 'https://openalex.org/W2219249508', 'https://openalex.org/W4221153068', 'https://openalex.org/W3107826490', 'https://openalex.org/W3186843219', 'https://openalex.org/W4226444650', 'https://openalex.org/W4307934980', 'https://openalex.org/W2995181338', 'https://openalex.org/W4385245566', 'https://openalex.org/W4312638101', 'https://openalex.org/W4385572318', 'https://openalex.org/W4367061106']",2023-01-01
https://openalex.org/W3200345197,https://doi.org/10.48550/arxiv.2109.07940,PDAugment: Data Augmentation by Pitch and Duration Adjustments for Automatic Lyrics Transcription,"Automatic lyrics transcription (ALT), which can be regarded as automatic speech recognition (ASR) on singing voice, is an interesting and practical topic in academia and industry. ALT has not been well developed mainly due to the dearth of paired singing voice and lyrics datasets for model training. Considering that there is a large amount of ASR training data, a straightforward method is to leverage ASR data to enhance ALT training. However, the improvement is marginal when training the ALT system directly with ASR data, because of the gap between the singing voice and standard speech data which is rooted in music-specific acoustic characteristics in singing voice. In this paper, we propose PDAugment, a data augmentation method that adjusts pitch and duration of speech at syllable level under the guidance of music scores to help ALT training. Specifically, we adjust the pitch and duration of each syllable in natural speech to those of the corresponding note extracted from music scores, so as to narrow the gap between natural speech and singing voice. Experiments on DSing30 and Dali corpus show that the ALT system equipped with our PDAugment outperforms previous state-of-the-art systems by 5.9% and 18.1% WERs respectively, demonstrating the effectiveness of PDAugment for ALT.","['https://openalex.org/W2747874407', 'https://openalex.org/W2327501763', 'https://openalex.org/W2889429804', 'https://openalex.org/W2395129396', 'https://openalex.org/W1494198834', 'https://openalex.org/W2964150074', 'https://openalex.org/W3175871055', 'https://openalex.org/W2963403868', 'https://openalex.org/W3111801244', 'https://openalex.org/W112239495', 'https://openalex.org/W3095189764', 'https://openalex.org/W3090751054', 'https://openalex.org/W2127141656', 'https://openalex.org/W2471520273', 'https://openalex.org/W2164282073', 'https://openalex.org/W2144731719', 'https://openalex.org/W1585181552', 'https://openalex.org/W3087621439', 'https://openalex.org/W3116834994', 'https://openalex.org/W2577008904', 'https://openalex.org/W2101927329', 'https://openalex.org/W2936774411', 'https://openalex.org/W2293949002', 'https://openalex.org/W2795935804', 'https://openalex.org/W2193413348', 'https://openalex.org/W2973071600', 'https://openalex.org/W182406043', 'https://openalex.org/W1522301498', 'https://openalex.org/W3025165719', 'https://openalex.org/W1828163288', 'https://openalex.org/W2903006902', 'https://openalex.org/W3132121394', 'https://openalex.org/W3081416955', 'https://openalex.org/W3015927303', 'https://openalex.org/W2059239154', 'https://openalex.org/W3016010032', 'https://openalex.org/W3015315843']",2021-09-16
https://openalex.org/W3210530853,https://doi.org/10.1109/icassp43922.2022.9746484,A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion,"The goal of voice conversion is to transform source speech into a target\nvoice, keeping the content unchanged. In this paper, we focus on\nself-supervised representation learning for voice conversion. Specifically, we\ncompare discrete and soft speech units as input features. We find that discrete\nrepresentations effectively remove speaker information but discard some\nlinguistic content - leading to mispronunciations. As a solution, we propose\nsoft speech units. To learn soft units, we predict a distribution over discrete\nspeech units. By modeling uncertainty, soft units capture more content\ninformation, improving the intelligibility and naturalness of converted speech.\nSamples available at https://ubisoft-laforge.github.io/speech/soft-vc/. Code\navailable at https://github.com/bshall/soft-vc/.\n","['https://openalex.org/W2395718496', 'https://openalex.org/W2890964092', 'https://openalex.org/W3140429000', 'https://openalex.org/W3095361818', 'https://openalex.org/W3161695192', 'https://openalex.org/W3209059054', 'https://openalex.org/W2155490028', 'https://openalex.org/W2842511635', 'https://openalex.org/W6786696081', 'https://openalex.org/W3198815374', 'https://openalex.org/W3197580070', 'https://openalex.org/W2964243274', 'https://openalex.org/W3015877095', 'https://openalex.org/W2899877258', 'https://openalex.org/W2995181338', 'https://openalex.org/W2120605154', 'https://openalex.org/W3092368332', 'https://openalex.org/W6752124048', 'https://openalex.org/W2518172956', 'https://openalex.org/W6762533536', 'https://openalex.org/W2963539064', 'https://openalex.org/W6840412704', 'https://openalex.org/W3188160682', 'https://openalex.org/W6783867762', 'https://openalex.org/W1494198834', 'https://openalex.org/W6917585676', 'https://openalex.org/W2750167318', 'https://openalex.org/W2972802841', 'https://openalex.org/W4288079962', 'https://openalex.org/W4287591426', 'https://openalex.org/W2949382160', 'https://openalex.org/W2949281321', 'https://openalex.org/W3082130377', 'https://openalex.org/W4288079605', 'https://openalex.org/W4297808394', 'https://openalex.org/W3110458199', 'https://openalex.org/W3169320628', 'https://openalex.org/W2945478979', 'https://openalex.org/W2519091744', 'https://openalex.org/W3098403858', 'https://openalex.org/W4289299319', 'https://openalex.org/W2805993470', 'https://openalex.org/W3092028330']",2022-04-27
https://openalex.org/W3197763626,https://doi.org/10.21437/interspeech.2021-1356,S2VC: A Framework for Any-to-Any Voice Conversion with Self-Supervised Pretrained Representations,"Any-to-any voice conversion (VC) aims to convert the timbre of utterances from and to any speakers seen or unseen during training.Various any-to-any VC approaches have been proposed like AUTOVC, AdaINVC, and FragmentVC.AUTOVC, and AdaINVC utilize source and target encoders to disentangle the content and speaker information of the features.Frag-mentVC utilizes two encoders to encode source and target information and adopts cross attention to align the source and target features with similar phonetic content.Moreover, pretrained features are adopted.AUTOVC used d-vector to extract speaker information, and self-supervised learning (SSL) features like wav2vec 2.0 is used in FragmentVC to extract the phonetic content information.Different from previous works, we proposed S2VC that utilizes Self-Supervised features as both source and target features for the VC model.Supervised phoneme posteriorgram (PPG), which is believed to be speaker-independent and widely used in VC to extract content information, is chosen as a strong baseline for SSL features.The objective evaluation and subjective evaluation both show models taking SSL feature CPC as both source and target features outperforms that taking PPG as source feature, suggesting that SSL features have great potential in improving VC.","['https://openalex.org/W3096864844', 'https://openalex.org/W2972659941', 'https://openalex.org/W4297808394', 'https://openalex.org/W3016181583', 'https://openalex.org/W3096918678', 'https://openalex.org/W3097777922', 'https://openalex.org/W2972394484', 'https://openalex.org/W3127686677', 'https://openalex.org/W3036601975', 'https://openalex.org/W2896457183', 'https://openalex.org/W3161627112', 'https://openalex.org/W2888922217', 'https://openalex.org/W2890964092', 'https://openalex.org/W2518172956', 'https://openalex.org/W3096524539', 'https://openalex.org/W2785325870', 'https://openalex.org/W3143523927', 'https://openalex.org/W2808706139', 'https://openalex.org/W2945478979', 'https://openalex.org/W3161695192', 'https://openalex.org/W3016011332', 'https://openalex.org/W2982223350', 'https://openalex.org/W2972943112', 'https://openalex.org/W95152782', 'https://openalex.org/W2954386831', 'https://openalex.org/W2908510526', 'https://openalex.org/W3082130377']",2021-08-27
https://openalex.org/W3207300132,https://doi.org/10.1109/icassp43922.2022.9746430,S3PRL-VC: Open-Source Voice Conversion Framework with Self-Supervised Speech Representations,"This paper introduces S3PRL-VC, an open-source voice conversion (VC) framework based on the S3PRL toolkit. In the context of recognition-synthesis VC, self-supervised speech representation (S3R) is valuable in its potential to replace the expensive supervised representation adopted by state-of-the-art VC systems. Moreover, we claim that VC is a good probing task for S3R analysis. In this work, we provide a series of in-depth analyses by benchmarking on the two tasks in VCC2020, namely intra-/cross-lingual any-to-one (A2O) VC, as well as an any-to-any (A2A) setting. We also provide comparisons between not only different S3Rs but also top systems in VCC2020 with supervised representations. Systematic objective and subjective evaluation were conducted, and we show that S3R is comparable with VCC2020 top systems in the A2O setting in terms of similarity, and achieves state-of-the-art in S3R-based A2A VC. We believe the extensive analysis, as well as the toolkit itself, contribute to not only the S3R community but also the VC community. The codebase is now open-sourced <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .","['https://openalex.org/W4244543785', 'https://openalex.org/W3197763626', 'https://openalex.org/W4210774711', 'https://openalex.org/W3197580070', 'https://openalex.org/W6780218876', 'https://openalex.org/W3198275944', 'https://openalex.org/W2046056978', 'https://openalex.org/W2889329491', 'https://openalex.org/W4249468441', 'https://openalex.org/W6752888775', 'https://openalex.org/W6739901393', 'https://openalex.org/W3083776549', 'https://openalex.org/W3092368332', 'https://openalex.org/W6840412704', 'https://openalex.org/W2518172956', 'https://openalex.org/W4247587863', 'https://openalex.org/W3081565196', 'https://openalex.org/W3140429000', 'https://openalex.org/W3161695192', 'https://openalex.org/W2120605154', 'https://openalex.org/W3161627112', 'https://openalex.org/W2156142001', 'https://openalex.org/W2595110011', 'https://openalex.org/W2963609956', 'https://openalex.org/W2801493797', 'https://openalex.org/W6936113694', 'https://openalex.org/W2964243274', 'https://openalex.org/W3025844872', 'https://openalex.org/W6783867762', 'https://openalex.org/W3098403858', 'https://openalex.org/W4385245566', 'https://openalex.org/W3092028330', 'https://openalex.org/W3099782249', 'https://openalex.org/W3082130377', 'https://openalex.org/W3036601975', 'https://openalex.org/W3185312023', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963432880', 'https://openalex.org/W2808706139', 'https://openalex.org/W2527729766']",2022-04-27
https://openalex.org/W4210774711,https://doi.org/10.1109/asru51503.2021.9688010,On Prosody Modeling for ASR+TTS Based Voice Conversion,"In voice conversion (VC), an approach showing promising results in the latest voice conversion challenge (VCC) 2020 is to first use an automatic speech recognition (ASR) model to transcribe the source speech into the underlying linguistic contents; these are then used as input by a text-to-speech (TTS) system to generate the converted speech. Such a paradigm, referred to as ASR+TTS, overlooks the modeling of prosody, which plays an important role in speech naturalness and conversion similarity. Although some researchers have considered transferring prosodic clues from the source speech, there arises a speaker mismatch during training and conversion. To address this issue, in this work, we propose to directly predict prosody from the linguistic representation in a target-speaker-dependent manner, referred to as target text prediction (TTP). We evaluate both methods on the VCC2020 benchmark and consider different linguistic representations. The results demonstrate the effectiveness of TTP in both objective and subjective evaluations.","['https://openalex.org/W3096567388', 'https://openalex.org/W2903739847', 'https://openalex.org/W2972389417', 'https://openalex.org/W2892009249', 'https://openalex.org/W6739901393', 'https://openalex.org/W3016160783', 'https://openalex.org/W2890964092', 'https://openalex.org/W2933138175', 'https://openalex.org/W2888867175', 'https://openalex.org/W2766219058', 'https://openalex.org/W2964138190', 'https://openalex.org/W2973158936', 'https://openalex.org/W3118753411', 'https://openalex.org/W3094635600', 'https://openalex.org/W4247587863', 'https://openalex.org/W2885800352', 'https://openalex.org/W2518172956', 'https://openalex.org/W2897353073', 'https://openalex.org/W3161695192', 'https://openalex.org/W3091890228', 'https://openalex.org/W3161627112', 'https://openalex.org/W3092368332', 'https://openalex.org/W6840412704', 'https://openalex.org/W6750489868', 'https://openalex.org/W3081565196', 'https://openalex.org/W2962780374', 'https://openalex.org/W2904459034', 'https://openalex.org/W2795109282', 'https://openalex.org/W2120605154', 'https://openalex.org/W6755300632', 'https://openalex.org/W2156142001', 'https://openalex.org/W6769196770', 'https://openalex.org/W4244543785', 'https://openalex.org/W6763316926', 'https://openalex.org/W3015853838', 'https://openalex.org/W3083776549', 'https://openalex.org/W1494198834', 'https://openalex.org/W3015338123', 'https://openalex.org/W3134921434', 'https://openalex.org/W3025844872', 'https://openalex.org/W2972802841', 'https://openalex.org/W2471520273', 'https://openalex.org/W2972359262', 'https://openalex.org/W2979476256', 'https://openalex.org/W4390912423', 'https://openalex.org/W2948238043', 'https://openalex.org/W4289383906', 'https://openalex.org/W4295731579', 'https://openalex.org/W3099078140', 'https://openalex.org/W4385245566', 'https://openalex.org/W2187089797', 'https://openalex.org/W2794490148']",2021-12-13
https://openalex.org/W4327661848,https://doi.org/10.1109/tpami.2023.3257839,Random Cycle Loss and Its Application to Voice Conversion,"Speech disentanglement aims to decompose independent causal factors of speech signals into separate codes. Perfect disentanglement benefits to a broad range of speech processing tasks. This paper presents a simple but effective disentanglement approach based on cycle consistency loss and random factor substitution. This leads to a novel random cycle (RC) loss that enforces analysis-and-resynthesis consistency, a main principle of reductionism. We theoretically demonstrate that the proposed RC loss can achieve independent codes if well optimized, which in turn leads to superior disentanglement when combined with information bottleneck (IB). Extensive simulation experiments were conducted to understand the properties of the RC loss, and experimental results on voice conversion further demonstrate the practical merit of the proposal. Source code and audio samples can be found on the webpage http://rc.cslt.org.","['https://openalex.org/W2043427249', 'https://openalex.org/W4236023255', 'https://openalex.org/W2089883580', 'https://openalex.org/W3156786336', 'https://openalex.org/W6745117592', 'https://openalex.org/W2974194285', 'https://openalex.org/W6750489868', 'https://openalex.org/W3046796449', 'https://openalex.org/W2964058423', 'https://openalex.org/W2156142001', 'https://openalex.org/W6756663807', 'https://openalex.org/W2150769028', 'https://openalex.org/W6755300632', 'https://openalex.org/W3197659778', 'https://openalex.org/W6637108112', 'https://openalex.org/W6790622591', 'https://openalex.org/W2532494225', 'https://openalex.org/W6762533536', 'https://openalex.org/W2798434327', 'https://openalex.org/W4241010212', 'https://openalex.org/W2018259456', 'https://openalex.org/W6776390925', 'https://openalex.org/W3207951629', 'https://openalex.org/W2128728535', 'https://openalex.org/W2123649031', 'https://openalex.org/W2163922914', 'https://openalex.org/W6756824971', 'https://openalex.org/W6718140377', 'https://openalex.org/W6744627333', 'https://openalex.org/W6748223763', 'https://openalex.org/W6748391871', 'https://openalex.org/W3166171970', 'https://openalex.org/W6751658261', 'https://openalex.org/W3036205238', 'https://openalex.org/W6781121179', 'https://openalex.org/W3126058723', 'https://openalex.org/W2120605154', 'https://openalex.org/W2105160541', 'https://openalex.org/W2086796102', 'https://openalex.org/W2127520494', 'https://openalex.org/W2157412983', 'https://openalex.org/W1509691205', 'https://openalex.org/W2396025094', 'https://openalex.org/W1974745215', 'https://openalex.org/W1977362459', 'https://openalex.org/W2902070858', 'https://openalex.org/W2937579788', 'https://openalex.org/W2963539064', 'https://openalex.org/W3198034710', 'https://openalex.org/W2752796333', 'https://openalex.org/W3096524539', 'https://openalex.org/W2972659941', 'https://openalex.org/W2518312472', 'https://openalex.org/W2518172956', 'https://openalex.org/W3197763626', 'https://openalex.org/W3161695192', 'https://openalex.org/W4385574033', 'https://openalex.org/W3024962219', 'https://openalex.org/W2946555236', 'https://openalex.org/W2962896155', 'https://openalex.org/W2963830550', 'https://openalex.org/W2937020545', 'https://openalex.org/W3198082505', 'https://openalex.org/W4221141917', 'https://openalex.org/W4225939199', 'https://openalex.org/W2962793481', 'https://openalex.org/W3015959238', 'https://openalex.org/W2972544500', 'https://openalex.org/W3160618567', 'https://openalex.org/W4285527881', 'https://openalex.org/W6796575454', 'https://openalex.org/W2118373646', 'https://openalex.org/W6631190155', 'https://openalex.org/W6779459370', 'https://openalex.org/W6936113694', 'https://openalex.org/W2046056978', 'https://openalex.org/W6783867762', 'https://openalex.org/W2972394484', 'https://openalex.org/W3122023385', 'https://openalex.org/W2902476877', 'https://openalex.org/W3036928441', 'https://openalex.org/W2569505260', 'https://openalex.org/W3101882441', 'https://openalex.org/W1686946872']",2023-03-16
https://openalex.org/W4389524060,https://doi.org/10.18653/v1/2023.findings-emnlp.541,Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units,"We introduce DISSC, a novel, lightweight method that converts the rhythm, pitch contour and timbre of a recording to a target speaker in a textless manner. Unlike DISSC, most voice conversion (VC) methods focus primarily on timbre, and ignore people’s unique speaking style (prosody). The proposed approach uses a pretrained, self-supervised model for encoding speech to discrete units, which makes it simple, effective, and fast to train. All conversion modules are only trained on reconstruction like tasks, thus suitable for any-to-many VC with no paired data. We introduce a suite of quantitative and qualitative evaluation metrics for this setup, and empirically demonstrate that DISSC significantly outperforms the evaluated baselines. Code and samples are available at https://pages.cs.huji.ac.il/adiyoss-lab/dissc/.","['https://openalex.org/W2963539064', 'https://openalex.org/W4311000453', 'https://openalex.org/W4287854499', 'https://openalex.org/W2774848319', 'https://openalex.org/W4381786045', 'https://openalex.org/W4394671563', 'https://openalex.org/W4307680525', 'https://openalex.org/W3092028330', 'https://openalex.org/W2589664214', 'https://openalex.org/W4206711328', 'https://openalex.org/W3214606349', 'https://openalex.org/W3141523618', 'https://openalex.org/W4300980117', 'https://openalex.org/W4287887366', 'https://openalex.org/W2294351487', 'https://openalex.org/W2747874407', 'https://openalex.org/W3169739675', 'https://openalex.org/W4225892118', 'https://openalex.org/W4283659485', 'https://openalex.org/W4372266960', 'https://openalex.org/W4303494982', 'https://openalex.org/W4300980462', 'https://openalex.org/W4297412183', 'https://openalex.org/W3126283728', 'https://openalex.org/W2972659941', 'https://openalex.org/W4296070387', 'https://openalex.org/W3169320628', 'https://openalex.org/W2156142001', 'https://openalex.org/W3161695192', 'https://openalex.org/W3163475957', 'https://openalex.org/W2123003832', 'https://openalex.org/W4297841435', 'https://openalex.org/W3034794073', 'https://openalex.org/W3180374548', 'https://openalex.org/W4387247604', 'https://openalex.org/W1494198834', 'https://openalex.org/W3213873715', 'https://openalex.org/W2527729766', 'https://openalex.org/W4378105483', 'https://openalex.org/W3198217962', 'https://openalex.org/W2125101937', 'https://openalex.org/W3140429000', 'https://openalex.org/W3167533889', 'https://openalex.org/W2945478979', 'https://openalex.org/W4308161937', 'https://openalex.org/W4306169273', 'https://openalex.org/W3150271208', 'https://openalex.org/W3024869864']",2023-01-01
https://openalex.org/W4312097445,https://doi.org/10.23919/apsipaasc55919.2022.9980335,Semi-supervised ASR based on Iterative Joint Training with Discrete Speech Synthesis,"This paper proposes Iterative Joint Training (IJT) with discrete speech synthesis for semi-supervised ASR. Thanks to the recent advances in Text-to-Speech synthesis, synthesized speech has become near human-level. However, a large amount of paired speech and text is indispensable to training a high quality TTS model. That prevents low-resource ASR from using TTS for data augmentation. To overcome that problem, we propose to use discrete speech representations, which may be easier to synthesize than continuous representations. The proposed method trains two models from the initial paired data; a speech recognition model whose input is discrete speech representation and a speech synthesis model that generates discrete speech from texts. Both models are repeatedly updated by pseudo-pair data generated by unpaired data using the previous models. Experimental results showed the effectiveness of the proposed method in low-resource settings. It successfully improved recognition performance by using discrete speech representations instead of conventional acoustic features in IJT experiments with a single-speaker speech corpus. Furthermore, the method improved the performance of multi-speaker speech recognition that used only single speaker pair data, unpaired multi-speaker speech, and unpaired text data from the conventional IJT approach using the conventional acoustic features.","['https://openalex.org/W2105482032', 'https://openalex.org/W1902237438', 'https://openalex.org/W2606974598', 'https://openalex.org/W6756197946', 'https://openalex.org/W6734815144', 'https://openalex.org/W2963609956', 'https://openalex.org/W2940200615', 'https://openalex.org/W2964012862', 'https://openalex.org/W2883586237', 'https://openalex.org/W3009565979', 'https://openalex.org/W3008480565', 'https://openalex.org/W6769196770', 'https://openalex.org/W2752796333', 'https://openalex.org/W2124509324', 'https://openalex.org/W2903739847', 'https://openalex.org/W3161695192', 'https://openalex.org/W2933138175', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962780374', 'https://openalex.org/W2972359262', 'https://openalex.org/W2973049979', 'https://openalex.org/W2612690371']",2022-11-07
https://openalex.org/W4400645798,https://doi.org/10.1109/access.2024.3428364,Unveiling the Linguistic Capabilities of a Self-Supervised Speech Model Through Cross-Lingual Benchmark and Layer- Wise Similarity Analysis,"Self-supervised learning (SSL), an unsupervised representation learning technique, has received widespread attention across various modalities. Speech, with its inherent complexity encompassing acoustic (e.g., speaker, phoneme, and paralinguistic cues) and linguistic (e.g., words, semantics, and syntax) aspects, prompts a fundamental question: how well can speech SSL models capture linguistic knowledge solely from speech data? This study comprehensively analyzes off-the-shelf SSL models utilizing three methods: probing tasks, layer contribution examinations, and layer-wise similarity analysis. For the probing task, to elucidate cross-lingual conditions, we introduce SpeechGLUE/SpeechJGLUE, the speech version of General Language Understanding Evaluation (GLUE) and its Japanese variant (JGLUE), both of which comprise diverse natural language understanding tasks. The probing system incorporates a weighted sum with trainable weights of all SSL layers&#x2019; outputs into downstream models, offering insight into which layer predominantly contributes to addressing tasks. The results reveal that speech SSL models can encode linguistic information, albeit less sophisticated information than with text SSL models. Moreover, later layers are mainly utilized to tackle the benchmark tasks. To highlight their primary linguistic encoding role, we call them linguistic encoding layers (LELs). However, in cross-lingual scenarios, e.g., assessing English SSL models on SpeechJGLUE, the layer contributions equalize, suggesting challenges in determining suitable layers or relying on diverse cues. Nevertheless, some English SSL models can outperform Japanese models on SpeechJGLUE, implying their robustness against language variation. Similarity analysis reveals a block structure within LELs, particularly evident in English WavLM, where the structure becomes unclear with non-English/noise input, reaffirming the presence of LELs.","['https://openalex.org/W4231109964', 'https://openalex.org/W2919115771', 'https://openalex.org/W2160815625', 'https://openalex.org/W2783089003', 'https://openalex.org/W6847363464', 'https://openalex.org/W2163922914', 'https://openalex.org/W6800751262', 'https://openalex.org/W3207924272', 'https://openalex.org/W6851949647', 'https://openalex.org/W3118485687', 'https://openalex.org/W3011574394', 'https://openalex.org/W3104667152', 'https://openalex.org/W4382246105', 'https://openalex.org/W3023371261', 'https://openalex.org/W2913939497', 'https://openalex.org/W4281492411', 'https://openalex.org/W6811170316', 'https://openalex.org/W6772230580', 'https://openalex.org/W4311137818', 'https://openalex.org/W3129850062', 'https://openalex.org/W4291023040', 'https://openalex.org/W3035725276', 'https://openalex.org/W2896457183', 'https://openalex.org/W2752796333', 'https://openalex.org/W6769196770', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W3207558756', 'https://openalex.org/W3198275944', 'https://openalex.org/W3206189675', 'https://openalex.org/W4225096077', 'https://openalex.org/W4361994820', 'https://openalex.org/W4224933800', 'https://openalex.org/W4385823298', 'https://openalex.org/W3161695192', 'https://openalex.org/W4385478010', 'https://openalex.org/W3217767527', 'https://openalex.org/W4226103796', 'https://openalex.org/W4385571440', 'https://openalex.org/W4319862410', 'https://openalex.org/W2923014074', 'https://openalex.org/W3197580070', 'https://openalex.org/W6803092890', 'https://openalex.org/W6810259195', 'https://openalex.org/W4372260307', 'https://openalex.org/W4391021557', 'https://openalex.org/W3198429080', 'https://openalex.org/W3213029956', 'https://openalex.org/W6850218400', 'https://openalex.org/W6852909395', 'https://openalex.org/W4385823338', 'https://openalex.org/W4285178177', 'https://openalex.org/W4226380987', 'https://openalex.org/W4375869259', 'https://openalex.org/W6854228366', 'https://openalex.org/W2842511635', 'https://openalex.org/W6810007534', 'https://openalex.org/W3175898847', 'https://openalex.org/W4285250921', 'https://openalex.org/W4391021682', 'https://openalex.org/W6861440175', 'https://openalex.org/W4385822439', 'https://openalex.org/W4392910434', 'https://openalex.org/W3006926732', 'https://openalex.org/W6802472062', 'https://openalex.org/W4391515560', 'https://openalex.org/W6809947431', 'https://openalex.org/W4296068815', 'https://openalex.org/W6800664022', 'https://openalex.org/W6786696081', 'https://openalex.org/W3197259906', 'https://openalex.org/W4292825791', 'https://openalex.org/W6852781825', 'https://openalex.org/W6790356757', 'https://openalex.org/W6762392948', 'https://openalex.org/W4281690148', 'https://openalex.org/W2978670439', 'https://openalex.org/W6691459498', 'https://openalex.org/W6605323724', 'https://openalex.org/W2739351760', 'https://openalex.org/W2963846996', 'https://openalex.org/W2963748441', 'https://openalex.org/W2130158090', 'https://openalex.org/W4253067820', 'https://openalex.org/W6635469476', 'https://openalex.org/W3114651185', 'https://openalex.org/W6771509786', 'https://openalex.org/W6795383682', 'https://openalex.org/W6796730497', 'https://openalex.org/W2965373594', 'https://openalex.org/W6763701032', 'https://openalex.org/W6761472960', 'https://openalex.org/W6785121581', 'https://openalex.org/W3034487470', 'https://openalex.org/W3162133897', 'https://openalex.org/W3197853473', 'https://openalex.org/W6862584478', 'https://openalex.org/W4392904752', 'https://openalex.org/W6678992168', 'https://openalex.org/W6768817161', 'https://openalex.org/W6796464841', 'https://openalex.org/W6745289305', 'https://openalex.org/W2962780374', 'https://openalex.org/W6779469704', 'https://openalex.org/W4412284989', 'https://openalex.org/W6791055906', 'https://openalex.org/W4375869301', 'https://openalex.org/W1635512741', 'https://openalex.org/W2005522781', 'https://openalex.org/W2963242190', 'https://openalex.org/W2972541922', 'https://openalex.org/W4226033575', 'https://openalex.org/W6772383348', 'https://openalex.org/W3183764363', 'https://openalex.org/W3198771897', 'https://openalex.org/W2981087920', 'https://openalex.org/W3174758275', 'https://openalex.org/W3088409176', 'https://openalex.org/W4221155340', 'https://openalex.org/W4378105483', 'https://openalex.org/W3207222250', 'https://openalex.org/W3104033643', 'https://openalex.org/W4394773771', 'https://openalex.org/W4401609044', 'https://openalex.org/W4297808394', 'https://openalex.org/W2980282514', 'https://openalex.org/W2998249245', 'https://openalex.org/W3197956630', 'https://openalex.org/W2765486990', 'https://openalex.org/W4287591426', 'https://openalex.org/W4394671563', 'https://openalex.org/W4392019371', 'https://openalex.org/W4367000428', 'https://openalex.org/W4323066695', 'https://openalex.org/W3001279689', 'https://openalex.org/W2127658298', 'https://openalex.org/W3195577433']",2024-01-01
https://openalex.org/W4388820665,https://doi.org/10.1109/apsipaasc58517.2023.10317447,DisC-VC: Disentangled and F<sub>0</sub>-Controllable Neural Voice Conversion,"Voice conversion is a task to convert a non-linguistic feature of a given utterance. Since nuance of speech strongly depends on its pitch pattern, in some applications, it would be desirable to keep the original rise/fall pitch pattern while changing the speaker identity. Some of the existing methods address this problem by either using a source-filter model or developing a neural network that takes an F <inf xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">0</inf> pattern as input to the model. Although the latter approach can achieve relatively high sound quality compared to the former one, there is no consideration for discrepancy between the target and generated F <inf xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">0</inf> patterns in its training process. In this paper, we propose a new variational-autoencoder-based voice conversion model accompanied by an auxiliary network, which ensures that the conversion result correctly reflects the specified F <inf xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">0</inf> /timbre information. We show the effectiveness of the proposed method by objective and subjective evaluations.","['https://openalex.org/W2576309025', 'https://openalex.org/W2114190875', 'https://openalex.org/W2148846882', 'https://openalex.org/W2938833595', 'https://openalex.org/W2123003832', 'https://openalex.org/W2156142001', 'https://openalex.org/W2120605154', 'https://openalex.org/W2126143605', 'https://openalex.org/W2157412983', 'https://openalex.org/W3094635600', 'https://openalex.org/W3161695192', 'https://openalex.org/W3161627112', 'https://openalex.org/W2518172956', 'https://openalex.org/W6640963894', 'https://openalex.org/W2946555236', 'https://openalex.org/W6762533536', 'https://openalex.org/W6776390925', 'https://openalex.org/W2962896155', 'https://openalex.org/W2902070858', 'https://openalex.org/W2972667718', 'https://openalex.org/W2947196194', 'https://openalex.org/W4225316945', 'https://openalex.org/W6783739589', 'https://openalex.org/W6802527329', 'https://openalex.org/W2471520273', 'https://openalex.org/W2972970915', 'https://openalex.org/W2786868129', 'https://openalex.org/W2749651610', 'https://openalex.org/W6783867762', 'https://openalex.org/W3015338123', 'https://openalex.org/W3015805741', 'https://openalex.org/W3083423753', 'https://openalex.org/W3144417991', 'https://openalex.org/W6729448088', 'https://openalex.org/W6695676441', 'https://openalex.org/W6780226713', 'https://openalex.org/W2752796333', 'https://openalex.org/W6718140377', 'https://openalex.org/W6603838645', 'https://openalex.org/W1999405202', 'https://openalex.org/W3092359270', 'https://openalex.org/W3165478005']",2023-10-31
https://openalex.org/W4385346108,https://doi.org/10.1038/s41368-023-00239-y,ChatGPT for shaping the future of dentistry: the potential of multi-modal large language model,"Abstract The ChatGPT, a lite and conversational variant of Generative Pretrained Transformer 4 (GPT-4) developed by OpenAI, is one of the milestone Large Language Models (LLMs) with billions of parameters. LLMs have stirred up much interest among researchers and practitioners in their impressive skills in natural language processing tasks, which profoundly impact various fields. This paper mainly discusses the future applications of LLMs in dentistry. We introduce two primary LLM deployment methods in dentistry, including automated dental diagnosis and cross-modal dental diagnosis, and examine their potential applications. Especially, equipped with a cross-modal encoder, a single LLM can manage multi-source data and conduct advanced natural language reasoning to perform complex clinical operations. We also present cases to demonstrate the potential of a fully automatic Multi-Modal LLM AI system for dentistry clinical application. While LLMs offer significant potential benefits, the challenges, such as data privacy, data quality, and model bias, need further study. Overall, LLMs have the potential to revolutionize dental diagnosis and treatment, which indicates a promising avenue for clinical application and research in dentistry.","['https://openalex.org/W4318219754', 'https://openalex.org/W4327681325', 'https://openalex.org/W4220726078', 'https://openalex.org/W4362557643', 'https://openalex.org/W4220717038', 'https://openalex.org/W4285988360', 'https://openalex.org/W4229448187', 'https://openalex.org/W2755631603', 'https://openalex.org/W3129300919', 'https://openalex.org/W4318612697', 'https://openalex.org/W4323980924', 'https://openalex.org/W4376616419', 'https://openalex.org/W4367556998', 'https://openalex.org/W4362601804', 'https://openalex.org/W2078396547', 'https://openalex.org/W2897583329', 'https://openalex.org/W2106107776', 'https://openalex.org/W2888120268', 'https://openalex.org/W3184324824', 'https://openalex.org/W2971258845', 'https://openalex.org/W3172427031', 'https://openalex.org/W2970771982', 'https://openalex.org/W3160137267', 'https://openalex.org/W2998385486', 'https://openalex.org/W3046375318', 'https://openalex.org/W3166664235', 'https://openalex.org/W3213921583', 'https://openalex.org/W2915623326', 'https://openalex.org/W2981852735', 'https://openalex.org/W4226278401', 'https://openalex.org/W4319662928', 'https://openalex.org/W4319460874', 'https://openalex.org/W3212516020', 'https://openalex.org/W4312933868', 'https://openalex.org/W3105892552', 'https://openalex.org/W3164670515', 'https://openalex.org/W4293262146', 'https://openalex.org/W3198549210', 'https://openalex.org/W4372270126', 'https://openalex.org/W4311000453', 'https://openalex.org/W4322618218', 'https://openalex.org/W3176743688', 'https://openalex.org/W4386083082', 'https://openalex.org/W4322739071', 'https://openalex.org/W4323313957', 'https://openalex.org/W4224246525', 'https://openalex.org/W2911489562', 'https://openalex.org/W2963716420', 'https://openalex.org/W3177828909', 'https://openalex.org/W4309643848', 'https://openalex.org/W2086229366', 'https://openalex.org/W4317891156', 'https://openalex.org/W2004910511', 'https://openalex.org/W2531468880', 'https://openalex.org/W2795959543', 'https://openalex.org/W2784499877', 'https://openalex.org/W2767767650', 'https://openalex.org/W2651948199', 'https://openalex.org/W1663984431', 'https://openalex.org/W2594522336', 'https://openalex.org/W3104761038', 'https://openalex.org/W2805183640', 'https://openalex.org/W3018776648', 'https://openalex.org/W3120395777', 'https://openalex.org/W3151286594', 'https://openalex.org/W4293067524', 'https://openalex.org/W3182099217', 'https://openalex.org/W3155203876', 'https://openalex.org/W3016417837', 'https://openalex.org/W1841482152', 'https://openalex.org/W3044701241', 'https://openalex.org/W2960622829', 'https://openalex.org/W2897596009', 'https://openalex.org/W2810136663', 'https://openalex.org/W2897355938', 'https://openalex.org/W4307431143', 'https://openalex.org/W4318293221', 'https://openalex.org/W2911313818', 'https://openalex.org/W2939839354', 'https://openalex.org/W2977271377', 'https://openalex.org/W3042883732', 'https://openalex.org/W4225380028', 'https://openalex.org/W2999399991', 'https://openalex.org/W4224217689', 'https://openalex.org/W1498293842', 'https://openalex.org/W3007177728', 'https://openalex.org/W65109133', 'https://openalex.org/W4377864459', 'https://openalex.org/W4221157572', 'https://openalex.org/W4360836968', 'https://openalex.org/W4392619039', 'https://openalex.org/W4318932456', 'https://openalex.org/W4381956285', 'https://openalex.org/W3037471945', 'https://openalex.org/W2910972514', 'https://openalex.org/W4362722944', 'https://openalex.org/W3102123817', 'https://openalex.org/W3098949126', 'https://openalex.org/W4396895148', 'https://openalex.org/W3106224367', 'https://openalex.org/W4390120219']",2023-07-28
https://openalex.org/W4375869065,https://doi.org/10.1109/icassp49357.2023.10095780,Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding,"Self-supervised speech representation learning (SSL) has shown to be effective in various downstream tasks, but SSL models are usually large and slow. Model compression techniques such as pruning aim to reduce the model size and computation without degradation in accuracy. Prior studies focus on the pruning of Transformers; however, speech models not only utilize a stack of Transformer blocks, but also combine a frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.","['https://openalex.org/W3105966348', 'https://openalex.org/W6766978945', 'https://openalex.org/W6768851824', 'https://openalex.org/W6745499552', 'https://openalex.org/W3203140070', 'https://openalex.org/W6691509046', 'https://openalex.org/W4206375145', 'https://openalex.org/W1494198834', 'https://openalex.org/W3200129129', 'https://openalex.org/W6638523607', 'https://openalex.org/W6755977528', 'https://openalex.org/W3209984917', 'https://openalex.org/W6739901393', 'https://openalex.org/W4281492411', 'https://openalex.org/W3197580070', 'https://openalex.org/W4297841557', 'https://openalex.org/W3100460087', 'https://openalex.org/W4224821750', 'https://openalex.org/W2127141656', 'https://openalex.org/W6726275242', 'https://openalex.org/W3176647794', 'https://openalex.org/W4226126941', 'https://openalex.org/W2945886900', 'https://openalex.org/W3091900426', 'https://openalex.org/W2962851801', 'https://openalex.org/W4297841903', 'https://openalex.org/W6839026989', 'https://openalex.org/W2979691890', 'https://openalex.org/W2972584841', 'https://openalex.org/W6839423864', 'https://openalex.org/W6850036870', 'https://openalex.org/W2460144244', 'https://openalex.org/W3163105696', 'https://openalex.org/W6796551075', 'https://openalex.org/W6780218876', 'https://openalex.org/W6846714717', 'https://openalex.org/W3209059054', 'https://openalex.org/W4224933800', 'https://openalex.org/W3207558756', 'https://openalex.org/W3217767527', 'https://openalex.org/W4226103796', 'https://openalex.org/W3142589538', 'https://openalex.org/W3036601975', 'https://openalex.org/W2978017171', 'https://openalex.org/W4287121455', 'https://openalex.org/W1821462560', 'https://openalex.org/W2962965870', 'https://openalex.org/W2899663614', 'https://openalex.org/W2963828549', 'https://openalex.org/W4372270126', 'https://openalex.org/W4385245566', 'https://openalex.org/W4319862410', 'https://openalex.org/W4295312788', 'https://openalex.org/W4283700140', 'https://openalex.org/W4284973286', 'https://openalex.org/W2250357346']",2023-05-05
https://openalex.org/W4385764125,https://doi.org/10.24963/ijcai.2023/761,Recent Advances in Direct Speech-to-text Translation,"Recently, speech-to-text translation has attracted more and more attention and many studies have emerged rapidly. In this paper, we present a comprehensive survey on direct speech translation aiming to summarize the current state-of-the-art techniques. First, we categorize the existing research work into three directions based on the main challenges --- modeling burden, data scarcity, and application issues. To tackle the problem of modeling burden, two main structures have been proposed, encoder-decoder framework (Transformer and the variants) and multitask frameworks. For the challenge of data scarcity, recent work resorts to many sophisticated techniques, such as data augmentation, pre-training, knowledge distillation, and multilingual modeling. We analyze and summarize the application issues, which include real-time, segmentation, named entity, gender bias, and code-switching. Finally, we discuss some promising directions for future work.","['https://openalex.org/W2788575190', 'https://openalex.org/W3205644108', 'https://openalex.org/W2989819126', 'https://openalex.org/W3035070478', 'https://openalex.org/W2582956876', 'https://openalex.org/W6809989930', 'https://openalex.org/W6863994431', 'https://openalex.org/W3093933225', 'https://openalex.org/W4312052802', 'https://openalex.org/W3161031484', 'https://openalex.org/W3113908264', 'https://openalex.org/W4221163209', 'https://openalex.org/W3164428426', 'https://openalex.org/W2973048981', 'https://openalex.org/W3025165719', 'https://openalex.org/W3163676755', 'https://openalex.org/W3169320628', 'https://openalex.org/W6925395861', 'https://openalex.org/W4302306287', 'https://openalex.org/W3162471442', 'https://openalex.org/W2899716505', 'https://openalex.org/W3172698324', 'https://openalex.org/W6803771590', 'https://openalex.org/W3112092703', 'https://openalex.org/W2936848022', 'https://openalex.org/W3213993043', 'https://openalex.org/W2936774411', 'https://openalex.org/W3033722096', 'https://openalex.org/W4221166780', 'https://openalex.org/W6776312876', 'https://openalex.org/W2936969148', 'https://openalex.org/W4223622550', 'https://openalex.org/W4221162032', 'https://openalex.org/W6864014924', 'https://openalex.org/W3153287399', 'https://openalex.org/W4223555773', 'https://openalex.org/W6863631769', 'https://openalex.org/W3036867795', 'https://openalex.org/W4320194748', 'https://openalex.org/W3163239510', 'https://openalex.org/W3152984045', 'https://openalex.org/W4287890956', 'https://openalex.org/W4297841689', 'https://openalex.org/W2998386507', 'https://openalex.org/W4308349017', 'https://openalex.org/W4372270126', 'https://openalex.org/W4221155340', 'https://openalex.org/W2949328740', 'https://openalex.org/W3209059054', 'https://openalex.org/W3096490862', 'https://openalex.org/W3034625919', 'https://openalex.org/W3097301532', 'https://openalex.org/W4285273653', 'https://openalex.org/W3162037819', 'https://openalex.org/W4226054021', 'https://openalex.org/W1556750318', 'https://openalex.org/W2972448360', 'https://openalex.org/W3097787369', 'https://openalex.org/W4385822287', 'https://openalex.org/W3177397495', 'https://openalex.org/W4287213456', 'https://openalex.org/W4285242522', 'https://openalex.org/W4224325263', 'https://openalex.org/W3177132412', 'https://openalex.org/W3196833881', 'https://openalex.org/W3034571331', 'https://openalex.org/W3176455679', 'https://openalex.org/W4285158365', 'https://openalex.org/W2973049979', 'https://openalex.org/W3097777922', 'https://openalex.org/W3168212167', 'https://openalex.org/W3176382501', 'https://openalex.org/W3173767661', 'https://openalex.org/W4288021071', 'https://openalex.org/W2964161387', 'https://openalex.org/W4385245566', 'https://openalex.org/W3176711365']",2023-08-01
https://openalex.org/W4392902876,https://doi.org/10.1109/icassp48485.2024.10447448,Retrieval Augmented End-to-End Spoken Dialog Models,"We recently developed a joint speech and language model (SLM [1]) which fuses a pretrained foundational speech model and a large language model (LLM), while preserving the in-context learning capability intrinsic to the pretrained LLM. In this paper, we apply SLM to dialog applications where the dialog states are inferred directly from the audio signal.Task-oriented dialogs often contain domain-specific entities, i.e., restaurants, hotels, train stations, and city names, which are difficult to recognize, however, critical for the downstream applications. Inspired by the RAG (retrieval-augmented generation) models, we propose a retrieval augmented SLM (ReSLM) that overcomes this weakness. We first train a retriever to retrieve text entities given audio inputs. The retrieved entities are then added as text inputs to the underlying LLM to bias model predictions. We evaluated ReSLM on speech MultiWoz task (DSTC-11 Challenge), and found that the retrieval augmentation boosts model performance, achieving joint goal accuracy (38.6% vs 32.7%), slot error rate (20.6% vs 24.8%) and ASR word error rate (5.5% vs 6.7%). While demonstrated on dialog state tracking, our approach is broadly applicable to speech tasks requiring custom contextual information or domain-specific entities.","['https://openalex.org/W4391021623', 'https://openalex.org/W6777615688', 'https://openalex.org/W6779857854', 'https://openalex.org/W3099700870', 'https://openalex.org/W6810722945', 'https://openalex.org/W4385822641', 'https://openalex.org/W6764961350', 'https://openalex.org/W4385573970', 'https://openalex.org/W6803092890', 'https://openalex.org/W4226120743', 'https://openalex.org/W4225272718', 'https://openalex.org/W4225308107', 'https://openalex.org/W6799838802', 'https://openalex.org/W6810334672', 'https://openalex.org/W4381786045', 'https://openalex.org/W4372270126', 'https://openalex.org/W6783813245', 'https://openalex.org/W6810730852', 'https://openalex.org/W6845404407', 'https://openalex.org/W2945475330', 'https://openalex.org/W6770169672', 'https://openalex.org/W2997771882', 'https://openalex.org/W3206547074', 'https://openalex.org/W4223510772', 'https://openalex.org/W6850218400', 'https://openalex.org/W6769627184', 'https://openalex.org/W2127141656', 'https://openalex.org/W4372267461', 'https://openalex.org/W6777399232', 'https://openalex.org/W6810673746', 'https://openalex.org/W3097777922', 'https://openalex.org/W4225323055', 'https://openalex.org/W4288027128', 'https://openalex.org/W3207222250', 'https://openalex.org/W4226082499', 'https://openalex.org/W4301243929', 'https://openalex.org/W4225553189', 'https://openalex.org/W2954492830', 'https://openalex.org/W4323066695', 'https://openalex.org/W3190965961']",2024-03-18
https://openalex.org/W4391021530,https://doi.org/10.1109/asru57964.2023.10389731,Prompting and Adapter Tuning For Self-Supervised Encoder-Decoder Speech Model,"Prompting and adapter tuning have emerged as efficient alternatives to fine-tuning (FT) methods. However, existing studies on speech prompting focused on classification tasks and failed on more complex sequence generation tasks. Besides, adapter tuning is primarily applied with a focus on encoder-only self-supervised models. Our experiments show that prompting on Wav2Seq, a self-supervised encoder-decoder model, surpasses previous works in sequence generation tasks. It achieves a remarkable 53% relative improvement in word error rate for ASR and a 27% in F1 score for slot filling. Additionally, prompting competes with the FT method in the low-resource scenario. Moreover, we show the transferability of prompting and adapter tuning on Wav2Seq in cross-lingual ASR. When limited trainable parameters are involved, prompting and adapter tuning consistently outperform conventional FT across 7 languages. Notably, in the low-resource scenario, prompting consistently outperforms adapter tuning.","['https://openalex.org/W2842511635', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3197580070', 'https://openalex.org/W4281492411', 'https://openalex.org/W3129009457', 'https://openalex.org/W3198217962', 'https://openalex.org/W4377865046', 'https://openalex.org/W4372270126', 'https://openalex.org/W3185341429', 'https://openalex.org/W6759579507', 'https://openalex.org/W4226162428', 'https://openalex.org/W6850477478', 'https://openalex.org/W4372270069', 'https://openalex.org/W4319862642', 'https://openalex.org/W3209984917', 'https://openalex.org/W4226278833', 'https://openalex.org/W4381786045', 'https://openalex.org/W4281672148', 'https://openalex.org/W2970476646', 'https://openalex.org/W3098267758', 'https://openalex.org/W3174770825', 'https://openalex.org/W4285247752', 'https://openalex.org/W6852859116', 'https://openalex.org/W6752946794', 'https://openalex.org/W6809645183', 'https://openalex.org/W4372346241', 'https://openalex.org/W4372260195', 'https://openalex.org/W6738045163', 'https://openalex.org/W6796581206', 'https://openalex.org/W4225410153', 'https://openalex.org/W3172698324', 'https://openalex.org/W6846600677', 'https://openalex.org/W6750665317', 'https://openalex.org/W2972584841', 'https://openalex.org/W1494198834', 'https://openalex.org/W2995181338', 'https://openalex.org/W3161223924', 'https://openalex.org/W3095410713', 'https://openalex.org/W6802744804', 'https://openalex.org/W2963211188', 'https://openalex.org/W4297808394', 'https://openalex.org/W4298312696', 'https://openalex.org/W3036601975', 'https://openalex.org/W4394671563', 'https://openalex.org/W4379539302', 'https://openalex.org/W4385245566', 'https://openalex.org/W3168867926', 'https://openalex.org/W4307783813', 'https://openalex.org/W4322825254', 'https://openalex.org/W2797583228', 'https://openalex.org/W4221155122']",2023-12-16
https://openalex.org/W4399486076,https://doi.org/10.1109/i2ct61223.2024.10544015,Text-to-Speech and Speech-to-Text Models: A Systematic Examination of Diverse Approaches,"The convergence of Text-to-Speech (TTS) and Speech-to-Text (STT) models has spearheaded an era of transformative advancements in natural language processing (NLP). This article discusses transformative advances in natural language processing (NLP). It presents a comprehensive survey that systematically examines specific STT and TTS models, such as rule-based systems and deep learning architectures, and explores their structures, methodologies, and applications. The paper classifies these models based on their underlying technologies, architectural approaches, training methods, and application domains, and provides a detailed overview of their development. It highlights industry successes and challenges, covering a range of application areas, from assistive technologies to virtual assistants and language translation. This research provides new perspectives to understand the current world of STT and TTS technologies, which will especially benefit researchers, developers, and industry players. By highlighting the empowering bridge between written and spoken language, the paper uniquely contributes to the ongoing dialogue in the field. It not only sheds light on the current state of STT and TTS technologies but also lays the groundwork for future innovations and advances that could change the way we interact with language.","['https://openalex.org/W4385569716', 'https://openalex.org/W4372270126', 'https://openalex.org/W4390075359', 'https://openalex.org/W4285603330', 'https://openalex.org/W6763832098', 'https://openalex.org/W2591927543', 'https://openalex.org/W4297536219', 'https://openalex.org/W6796464841', 'https://openalex.org/W3115402873', 'https://openalex.org/W2593011301', 'https://openalex.org/W4379931078', 'https://openalex.org/W2971348767', 'https://openalex.org/W2971786069', 'https://openalex.org/W3082351598', 'https://openalex.org/W4313043981', 'https://openalex.org/W4312909524', 'https://openalex.org/W2946200149']",2024-04-05
https://openalex.org/W4401246677,https://doi.org/10.1109/taslp.2024.3436618,SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks,"Prompting has become a practical method for utilizing pre-trained language\nmodels (LMs). This approach offers several advantages. It allows an LM to adapt\nto new tasks with minimal training and parameter updates, thus achieving\nefficiency in both storage and computation. Additionally, prompting modifies\nonly the LM's inputs and harnesses the generative capabilities of language\nmodels to address various downstream tasks in a unified manner. This\nsignificantly reduces the need for human labor in designing task-specific\nmodels. These advantages become even more evident as the number of tasks served\nby the LM scales up. Motivated by the strengths of prompting, we are the first\nto explore the potential of prompting speech LMs in the domain of speech\nprocessing. Recently, there has been a growing interest in converting speech\ninto discrete units for language modeling. Our pioneer research demonstrates\nthat these quantized speech units are highly versatile within our unified\nprompting framework. Not only can they serve as class labels, but they also\ncontain rich phonetic information that can be re-synthesized back into speech\nsignals for speech generation tasks. Specifically, we reformulate speech\nprocessing tasks into speech-to-unit generation tasks. As a result, we can\nseamlessly integrate tasks such as speech classification, sequence generation,\nand speech generation within a single, unified prompting framework. The\nexperiment results show that the prompting method can achieve competitive\nperformance compared to the strong fine-tuning method based on self-supervised\nlearning models with a similar number of trainable parameters. The prompting\nmethod also shows promising results in the few-shot setting. Moreover, with the\nadvanced speech LMs coming into the stage, the proposed prompting framework\nattains great potential.\n","['https://openalex.org/W4281492411', 'https://openalex.org/W3197580070', 'https://openalex.org/W3189296823', 'https://openalex.org/W4285250921', 'https://openalex.org/W3185341429', 'https://openalex.org/W3098267758', 'https://openalex.org/W6769627184', 'https://openalex.org/W3172642864', 'https://openalex.org/W3153427360', 'https://openalex.org/W3188542058', 'https://openalex.org/W4322766882', 'https://openalex.org/W4386187806', 'https://openalex.org/W4205991051', 'https://openalex.org/W4312651322', 'https://openalex.org/W4226162428', 'https://openalex.org/W6810313920', 'https://openalex.org/W6790356757', 'https://openalex.org/W3198217962', 'https://openalex.org/W4296070387', 'https://openalex.org/W3140429000', 'https://openalex.org/W4385822683', 'https://openalex.org/W2951082691', 'https://openalex.org/W3034999214', 'https://openalex.org/W4295308567', 'https://openalex.org/W4292825791', 'https://openalex.org/W4287887366', 'https://openalex.org/W3209059054', 'https://openalex.org/W6810673746', 'https://openalex.org/W3016181583', 'https://openalex.org/W6780218876', 'https://openalex.org/W3198608154', 'https://openalex.org/W3016011332', 'https://openalex.org/W3015265920', 'https://openalex.org/W3041561163', 'https://openalex.org/W4392909068', 'https://openalex.org/W4392909760', 'https://openalex.org/W2964243274', 'https://openalex.org/W6783867762', 'https://openalex.org/W4381786045', 'https://openalex.org/W6852781825', 'https://openalex.org/W6853530687', 'https://openalex.org/W4297841687', 'https://openalex.org/W4385822890', 'https://openalex.org/W6847363464', 'https://openalex.org/W6752946794', 'https://openalex.org/W4393157525', 'https://openalex.org/W6796456916', 'https://openalex.org/W4385823335', 'https://openalex.org/W4385245566', 'https://openalex.org/W4385567149', 'https://openalex.org/W4285286749', 'https://openalex.org/W6750665317', 'https://openalex.org/W3096251052', 'https://openalex.org/W3108231750', 'https://openalex.org/W3134187040', 'https://openalex.org/W6788231366', 'https://openalex.org/W2972584841', 'https://openalex.org/W6838356808', 'https://openalex.org/W6777335856', 'https://openalex.org/W3160747466', 'https://openalex.org/W6746278845', 'https://openalex.org/W1494198834', 'https://openalex.org/W3161223924', 'https://openalex.org/W3196509775', 'https://openalex.org/W4372260337', 'https://openalex.org/W6917585676', 'https://openalex.org/W2995181338', 'https://openalex.org/W2972495969', 'https://openalex.org/W3180374548', 'https://openalex.org/W4297841405', 'https://openalex.org/W4391021530', 'https://openalex.org/W4372270126', 'https://openalex.org/W4392902623', 'https://openalex.org/W4391021627', 'https://openalex.org/W6857054612', 'https://openalex.org/W4394671563', 'https://openalex.org/W4288089799', 'https://openalex.org/W2797583228']",2024-01-01
https://openalex.org/W4385489023,https://doi.org/10.1109/icasspw59220.2023.10193218,Channel-Aware Pretraining Of Joint Encoder-Decoder Self-Supervised Model For Telephonic-Speech ASR,"This paper proposes a novel technique to obtain better downstream ASR performance from a joint encoder-decoder self-supervised model when trained with speech pooled from two different channels (narrow and wide band). The joint encoder-decoder self-supervised model extends the HuBERT model with a Transformer decoder. HuBERT performs clustering of features and predicts the class of every input frame. In simple pooling, which is our baseline, there is no way to identify the channel information. To incorporate channel information, we have proposed non-overlapping cluster IDs for speech from different channels. Our method gives a relative improvement of $\sim 4$% over the joint encoder-decoder self-supervised model built with simple pooling of data, which serves as our baseline.","['https://openalex.org/W2912668608', 'https://openalex.org/W3209059054', 'https://openalex.org/W2962780374', 'https://openalex.org/W6850036870', 'https://openalex.org/W4226278833', 'https://openalex.org/W2013598660', 'https://openalex.org/W2133856945', 'https://openalex.org/W3198771897', 'https://openalex.org/W2973141395', 'https://openalex.org/W4281672148', 'https://openalex.org/W2400830530', 'https://openalex.org/W1517841224', 'https://openalex.org/W2749459361', 'https://openalex.org/W2399742709', 'https://openalex.org/W4372270126']",2023-06-04
https://openalex.org/W4391759660,https://doi.org/10.1109/lsp.2024.3365036,UniEnc-CASSNAT: An Encoder-Only Non-Autoregressive ASR for Speech SSL Models,"Non-autoregressive automatic speech recognition (NASR) models have gained\nattention due to their parallelism and fast inference. The encoder-based NASR,\ne.g. connectionist temporal classification (CTC), can be initialized from the\nspeech foundation models (SFM) but does not account for any dependencies among\nintermediate tokens. The encoder-decoder-based NASR, like CTC alignment-based\nsingle-step non-autoregressive transformer (CASS-NAT), can mitigate the\ndependency problem but is not able to efficiently integrate SFM. Inspired by\nthe success of recent work of speech-text joint pre-training with a shared\ntransformer encoder, we propose a new encoder-based NASR, UniEnc-CASSNAT, to\ncombine the advantages of CTC and CASS-NAT. UniEnc-CASSNAT consists of only an\nencoder as the major module, which can be the SFM. The encoder plays the role\nof both the CASS-NAT encoder and decoder by two forward passes. The first pass\nof the encoder accepts the speech signal as input, while the concatenation of\nthe speech signal and the token-level acoustic embedding is used as the input\nfor the second pass. Examined on the Librispeech 100h, MyST, and Aishell1\ndatasets, the proposed UniEnc-CASSNAT achieves state-of-the-art NASR results\nand is better or comparable to CASS-NAT with only an encoder and hence, fewer\nmodel parameters. Our codes are publicly available.\n","['https://openalex.org/W3204696009', 'https://openalex.org/W4281492411', 'https://openalex.org/W3207558756', 'https://openalex.org/W6769627184', 'https://openalex.org/W2896457183', 'https://openalex.org/W3016011332', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W4293793697', 'https://openalex.org/W3112157188', 'https://openalex.org/W3197140813', 'https://openalex.org/W3206573929', 'https://openalex.org/W2127141656', 'https://openalex.org/W3147414526', 'https://openalex.org/W3097882114', 'https://openalex.org/W3169714379', 'https://openalex.org/W3164692279', 'https://openalex.org/W3162431424', 'https://openalex.org/W4283067311', 'https://openalex.org/W4385573012', 'https://openalex.org/W3205644108', 'https://openalex.org/W4361990931', 'https://openalex.org/W4221151577', 'https://openalex.org/W4385567350', 'https://openalex.org/W3198259287', 'https://openalex.org/W1494198834', 'https://openalex.org/W2107162140', 'https://openalex.org/W2963242190', 'https://openalex.org/W4223622550', 'https://openalex.org/W4372341362', 'https://openalex.org/W2963250244', 'https://openalex.org/W4385245566', 'https://openalex.org/W4375869113', 'https://openalex.org/W3200601846', 'https://openalex.org/W4372270126', 'https://openalex.org/W4288089799']",2024-01-01
https://openalex.org/W4391260976,https://doi.org/10.11591/ijece.v14i2.pp2014-2023,Generating images using generative adversarial networks based on text descriptions,"Modern developments in the fields of natural language processing (NLP) and computer vision (CV) emphasize the increasing importance of generating images from text descriptions. The presented article analyzes and compares two key methods in this area: generative adversarial network with conditional latent semantic analysis (GAN-CLS) and ultra-long transformer network (XLNet). The main components of GAN-CLS, including the generator, discriminator, and text encoder, are discussed in the context of their functional tasks—generating images from text inputs, assessing the realism of generated images, and converting text descriptions into latent spaces, respectively. A detailed comparative analysis of the performance of GAN-CLS and XLNet, the latter of which is widely used in the organic light-emitting diode (OEL) field, is carried out. The purpose of the study is to determine the effectiveness of each method in different scenarios and then provide valuable recommendations for selecting the best method for generating images from text descriptions, taking into account specific tasks and resources. Ultimately, our paper aims to be a valuable research resource by providing scientific guidance for NLP and CV experts.","['https://openalex.org/W4281253290', 'https://openalex.org/W4293201989', 'https://openalex.org/W4285792151', 'https://openalex.org/W3212386989', 'https://openalex.org/W4226334913', 'https://openalex.org/W6847050642', 'https://openalex.org/W4295137337', 'https://openalex.org/W4308217457', 'https://openalex.org/W4296873064', 'https://openalex.org/W4362589572', 'https://openalex.org/W2966963399', 'https://openalex.org/W4361185708', 'https://openalex.org/W4384301427', 'https://openalex.org/W4372270126', 'https://openalex.org/W4210599520', 'https://openalex.org/W4294186555', 'https://openalex.org/W4366598160', 'https://openalex.org/W4312531268', 'https://openalex.org/W4285219276', 'https://openalex.org/W3216352822', 'https://openalex.org/W4225673071', 'https://openalex.org/W4312438583', 'https://openalex.org/W4214874138', 'https://openalex.org/W4210922522', 'https://openalex.org/W4313595566', 'https://openalex.org/W4310883141', 'https://openalex.org/W4312911498']",2024-01-26
https://openalex.org/W4394995419,https://doi.org/10.1109/nnice61279.2024.10498758,Sequential2.0: Self-supervised Speech Translation Based on FAT Model and Replaced Token Detection,"This paper proposes a self-supervised speech translation model based on the FAT model for knowledge distillation, which is mainly applied to bilingual business scenarios such as cross-border goods and services marketing between China and the United States. Compared with the original model, our model reduces the inference time by 16.44% and the parameter size by 12.17%, while improving the BLEU score by 12.48%. The reduced parameter size is more conducive to the deployment of the model on edge devices such as embedded systems and mobile phones. Experimental results show that using the replaced token detection algorithm is better than traditional masked language modeling, and combining the BPE algorithm and PCA algorithm for teacher model hidden state encoding helps improve translation performance. Although our model achieves better results in the Chinese-English bilingual scenario, larger models such as Whisper perform better in multilingual scenarios. Future research will focus on the knowledge distillation and optimization of large language models in multilingual scenarios, and we look forward to more participation and discussion from researchers in this field.","['https://openalex.org/W3209059054', 'https://openalex.org/W6780218876', 'https://openalex.org/W4226033575', 'https://openalex.org/W6847363464', 'https://openalex.org/W6791055906', 'https://openalex.org/W3175898847', 'https://openalex.org/W3098605233', 'https://openalex.org/W6771917389', 'https://openalex.org/W4372270126', 'https://openalex.org/W6789693907', 'https://openalex.org/W2144499799', 'https://openalex.org/W1494198834', 'https://openalex.org/W2889048668', 'https://openalex.org/W2128728535', 'https://openalex.org/W2962784628', 'https://openalex.org/W4281569374', 'https://openalex.org/W1977556410', 'https://openalex.org/W2101105183', 'https://openalex.org/W3170072081', 'https://openalex.org/W569478347', 'https://openalex.org/W4311000453', 'https://openalex.org/W3131870090']",2024-01-19
https://openalex.org/W4322730856,https://doi.org/10.1109/jas.2023.123375,Parallel Learning: Overview and Perspective for Computational Learning Across Syn2Real and Sim2Real,"The virtual-to-real paradigm, i.e., training models on virtual data and then applying them to solve real-world problems, has attracted more and more attention from various domains by successfully alleviating the data shortage problem in machine learning. To summarize the advances in recent years, this survey comprehensively reviews the literature, from the viewport of parallel intelligence. First, an extended parallel learning framework is proposed to cover main domains including computer vision, natural language processing, robotics, and autonomous driving. Second, a multi-dimensional taxonomy is designed to organize the literature in a hierarchical structure. Third, the related virtual-to-real works are analyzed and compared according to the three principles of parallel learning known as description, prediction, and prescription, which cover the methods for constructing virtual worlds, generating labeled data, domain transferring, model training and testing, as well as optimizing the strategies to guide the task-oriented data generator for better learning performance. Key issues remained in virtual-to-real are discussed. Furthermore, the future research directions from the viewpoint of parallel learning are suggested.","['https://openalex.org/W2975317124', 'https://openalex.org/W2954996726', 'https://openalex.org/W3083475656', 'https://openalex.org/W3088310808', 'https://openalex.org/W3212380917', 'https://openalex.org/W1571156902', 'https://openalex.org/W2361439446', 'https://openalex.org/W2528059250', 'https://openalex.org/W4232929002', 'https://openalex.org/W2783478760', 'https://openalex.org/W7004865476', 'https://openalex.org/W2923088732', 'https://openalex.org/W2941466173', 'https://openalex.org/W2734479159', 'https://openalex.org/W3163166826', 'https://openalex.org/W3049282540', 'https://openalex.org/W6809885388', 'https://openalex.org/W6743428213', 'https://openalex.org/W2998508940', 'https://openalex.org/W6809990877', 'https://openalex.org/W2949736877', 'https://openalex.org/W6762619590', 'https://openalex.org/W3097975205', 'https://openalex.org/W2460583509', 'https://openalex.org/W2938671123', 'https://openalex.org/W6744093492', 'https://openalex.org/W2903718012', 'https://openalex.org/W3189792414', 'https://openalex.org/W2963423876', 'https://openalex.org/W2954351360', 'https://openalex.org/W2789309368', 'https://openalex.org/W2963271314', 'https://openalex.org/W2883820570', 'https://openalex.org/W3010462080', 'https://openalex.org/W2982278021', 'https://openalex.org/W3180758453', 'https://openalex.org/W4312394134', 'https://openalex.org/W2962759496', 'https://openalex.org/W1945811542', 'https://openalex.org/W1998042868', 'https://openalex.org/W2343052201', 'https://openalex.org/W2873558679', 'https://openalex.org/W6775038058', 'https://openalex.org/W2610366607', 'https://openalex.org/W2311523351', 'https://openalex.org/W3034302825', 'https://openalex.org/W4214588288', 'https://openalex.org/W3176321665', 'https://openalex.org/W6719727587', 'https://openalex.org/W2962729993', 'https://openalex.org/W2888934629', 'https://openalex.org/W2982335979', 'https://openalex.org/W3034217102', 'https://openalex.org/W3169891778', 'https://openalex.org/W4327656064', 'https://openalex.org/W3174828871', 'https://openalex.org/W3201915713', 'https://openalex.org/W2962699523', 'https://openalex.org/W2963581463', 'https://openalex.org/W3096216486', 'https://openalex.org/W2883586237', 'https://openalex.org/W3015449694', 'https://openalex.org/W3162244132', 'https://openalex.org/W3208049241', 'https://openalex.org/W2963739817', 'https://openalex.org/W3015280134', 'https://openalex.org/W3097890746', 'https://openalex.org/W3015654635', 'https://openalex.org/W3198836239', 'https://openalex.org/W2963823140', 'https://openalex.org/W3035577668', 'https://openalex.org/W6729383884', 'https://openalex.org/W6740590407', 'https://openalex.org/W2785093437', 'https://openalex.org/W2963109507', 'https://openalex.org/W6745388339', 'https://openalex.org/W2963569817', 'https://openalex.org/W6757736129', 'https://openalex.org/W2980852478', 'https://openalex.org/W6771940952', 'https://openalex.org/W2605102758', 'https://openalex.org/W2964198579', 'https://openalex.org/W6756581457', 'https://openalex.org/W2963276406', 'https://openalex.org/W6746428009', 'https://openalex.org/W2962899390', 'https://openalex.org/W2963390419', 'https://openalex.org/W4285102154', 'https://openalex.org/W2968116426', 'https://openalex.org/W3207591725', 'https://openalex.org/W6752818329', 'https://openalex.org/W3091471537', 'https://openalex.org/W3114446492', 'https://openalex.org/W6766161879', 'https://openalex.org/W2990747716', 'https://openalex.org/W3128627450', 'https://openalex.org/W2967466692', 'https://openalex.org/W3037262181', 'https://openalex.org/W3197308999', 'https://openalex.org/W6749146968', 'https://openalex.org/W6675708744', 'https://openalex.org/W2595845486', 'https://openalex.org/W2963184939', 'https://openalex.org/W3008998212', 'https://openalex.org/W2911087563', 'https://openalex.org/W3093922502', 'https://openalex.org/W2963872524', 'https://openalex.org/W2963184124', 'https://openalex.org/W3208589965', 'https://openalex.org/W3049043369', 'https://openalex.org/W3109978116', 'https://openalex.org/W2962957005', 'https://openalex.org/W3140906301', 'https://openalex.org/W6801524367', 'https://openalex.org/W2033547469', 'https://openalex.org/W2487365028', 'https://openalex.org/W2970243568', 'https://openalex.org/W2431874326', 'https://openalex.org/W2883090707', 'https://openalex.org/W2963794516', 'https://openalex.org/W2889054948', 'https://openalex.org/W2743627947', 'https://openalex.org/W3110387477', 'https://openalex.org/W3175727500', 'https://openalex.org/W4312606716', 'https://openalex.org/W2985936292', 'https://openalex.org/W3110015970', 'https://openalex.org/W3213148824', 'https://openalex.org/W2964115968', 'https://openalex.org/W2997207016', 'https://openalex.org/W6750163814', 'https://openalex.org/W3108910236', 'https://openalex.org/W3111740481', 'https://openalex.org/W2990099924', 'https://openalex.org/W2936459520', 'https://openalex.org/W4312238089', 'https://openalex.org/W4313162344', 'https://openalex.org/W2153062878', 'https://openalex.org/W2397830550', 'https://openalex.org/W3091580890', 'https://openalex.org/W6798772658', 'https://openalex.org/W3211296466', 'https://openalex.org/W3034716600', 'https://openalex.org/W2962977206', 'https://openalex.org/W2891315523', 'https://openalex.org/W6800922437', 'https://openalex.org/W3135314352', 'https://openalex.org/W2905173465', 'https://openalex.org/W6801415009', 'https://openalex.org/W2999862950', 'https://openalex.org/W3215048956', 'https://openalex.org/W3089594962', 'https://openalex.org/W3214824629', 'https://openalex.org/W3040578939', 'https://openalex.org/W2946647719', 'https://openalex.org/W2967708755', 'https://openalex.org/W6770857864', 'https://openalex.org/W6967116505', 'https://openalex.org/W3003225285', 'https://openalex.org/W4288019406', 'https://openalex.org/W3200565286', 'https://openalex.org/W4224035735', 'https://openalex.org/W3035416506', 'https://openalex.org/W4388963919', 'https://openalex.org/W3013262970', 'https://openalex.org/W3104876774', 'https://openalex.org/W3198134448', 'https://openalex.org/W3120778962', 'https://openalex.org/W3102870931', 'https://openalex.org/W2981030070', 'https://openalex.org/W3171399186', 'https://openalex.org/W2771114757', 'https://openalex.org/W3102078777', 'https://openalex.org/W1571920584', 'https://openalex.org/W2796322794', 'https://openalex.org/W3135759888', 'https://openalex.org/W2792514232', 'https://openalex.org/W2746314669']",2023-03-01
https://openalex.org/W4321328070,https://doi.org/10.1002/prep.202200276,"Artificial Intelligence Approaches for Energetic Materials by Design: State of the Art, Challenges, and Future Directions","Abstract Artificial intelligence (AI) is rapidly emerging as a enabling tool for solving complex materials design problems. This paper aims to review recent advances in AI‐driven materials‐by‐design and their applications to energetic materials (EM). Trained with data from numerical simulations and/or physical experiments, AI models can assimilate trends and patterns within the design parameter space, identify optimal material designs (micro‐morphologies, combinations of materials in composites, etc.), and point to designs with superior/targeted property and performance metrics. We review approaches focusing on such capabilities with respect to the three main stages of materials‐by‐design, namely representation learning of microstructure morphology (i. e., shape descriptors), structure‐property‐performance (S−P−P) linkage estimation, and optimization/design exploration. We leave out “process” as much work remains to be done to establish the connectivity between process and structure. We provide a perspective view of these methods in terms of their potential, practicality, and efficacy towards the realization of materials‐by‐design. Specifically, methods in the literature are evaluated in terms of their capacity to learn from a small/limited number of data, computational complexity, generalizability/scalability to other material species and operating conditions, interpretability of the model predictions, and the burden of supervision/data annotation. Finally, we suggest a few promising future research directions for EM materials‐by‐design, such as meta‐learning, active learning, Bayesian learning, and semi‐/weakly‐supervised learning, to bridge the gap between machine learning research and EM research.","['https://openalex.org/W2794659070', 'https://openalex.org/W3048628682', 'https://openalex.org/W4210706589', 'https://openalex.org/W2947628309', 'https://openalex.org/W2049430757', 'https://openalex.org/W2036475885', 'https://openalex.org/W2780506180', 'https://openalex.org/W3197490851', 'https://openalex.org/W2141070669', 'https://openalex.org/W2050853133', 'https://openalex.org/W1988638684', 'https://openalex.org/W3081079275', 'https://openalex.org/W2608146760', 'https://openalex.org/W2888894954', 'https://openalex.org/W2753982425', 'https://openalex.org/W2767267126', 'https://openalex.org/W3092007974', 'https://openalex.org/W2790304272', 'https://openalex.org/W2809047919', 'https://openalex.org/W2789526630', 'https://openalex.org/W2078236179', 'https://openalex.org/W2112796928', 'https://openalex.org/W3216130706', 'https://openalex.org/W3195997474', 'https://openalex.org/W3115462576', 'https://openalex.org/W3015879326', 'https://openalex.org/W3013529009', 'https://openalex.org/W3086140914', 'https://openalex.org/W2338402873', 'https://openalex.org/W4292411804', 'https://openalex.org/W3033055701', 'https://openalex.org/W3193638670', 'https://openalex.org/W2784006847', 'https://openalex.org/W3212955801', 'https://openalex.org/W2108598243', 'https://openalex.org/W4367308972', 'https://openalex.org/W2079253405', 'https://openalex.org/W3208687975', 'https://openalex.org/W4210667857', 'https://openalex.org/W2925044042', 'https://openalex.org/W3149956190', 'https://openalex.org/W3159582288', 'https://openalex.org/W2896594012', 'https://openalex.org/W4210253147', 'https://openalex.org/W2151135415', 'https://openalex.org/W4280496032', 'https://openalex.org/W3156673435', 'https://openalex.org/W3033000587', 'https://openalex.org/W2102120659', 'https://openalex.org/W4281716556', 'https://openalex.org/W3016113557', 'https://openalex.org/W2963580633', 'https://openalex.org/W4206145073', 'https://openalex.org/W2972352850', 'https://openalex.org/W2606759614', 'https://openalex.org/W2562939451', 'https://openalex.org/W2963920537', 'https://openalex.org/W3203887481', 'https://openalex.org/W2995181850', 'https://openalex.org/W2194775991', 'https://openalex.org/W2163922914', 'https://openalex.org/W4221039687', 'https://openalex.org/W2962940229', 'https://openalex.org/W2782313882', 'https://openalex.org/W3187506011', 'https://openalex.org/W3024756171', 'https://openalex.org/W3203245760', 'https://openalex.org/W4298111683', 'https://openalex.org/W4282830224', 'https://openalex.org/W2942561397', 'https://openalex.org/W4206072558', 'https://openalex.org/W2963626105', 'https://openalex.org/W2752796333', 'https://openalex.org/W3180355996', 'https://openalex.org/W6739901393', 'https://openalex.org/W3096216486', 'https://openalex.org/W3211839402', 'https://openalex.org/W2008056655', 'https://openalex.org/W2118483385', 'https://openalex.org/W2437591545', 'https://openalex.org/W2782634521', 'https://openalex.org/W2916338083', 'https://openalex.org/W2337110853', 'https://openalex.org/W2911964244', 'https://openalex.org/W3152453431', 'https://openalex.org/W2888395196', 'https://openalex.org/W4221145147', 'https://openalex.org/W2116341502', 'https://openalex.org/W3110901318', 'https://openalex.org/W3095782455', 'https://openalex.org/W4310273071', 'https://openalex.org/W4306252592', 'https://openalex.org/W4283078489', 'https://openalex.org/W3093008122', 'https://openalex.org/W2086774128', 'https://openalex.org/W3004907498', 'https://openalex.org/W3135137978', 'https://openalex.org/W2963807552', 'https://openalex.org/W3198848420', 'https://openalex.org/W3032752766', 'https://openalex.org/W2810011556', 'https://openalex.org/W4250503569', 'https://openalex.org/W2902559752', 'https://openalex.org/W2973385654', 'https://openalex.org/W2094646111', 'https://openalex.org/W2383894479', 'https://openalex.org/W3164803610', 'https://openalex.org/W1498436455', 'https://openalex.org/W2991294993', 'https://openalex.org/W2964137095', 'https://openalex.org/W4292083457', 'https://openalex.org/W2143560894', 'https://openalex.org/W4299541734', 'https://openalex.org/W1595159159', 'https://openalex.org/W3007964032', 'https://openalex.org/W2091342063', 'https://openalex.org/W3170303198', 'https://openalex.org/W1510052597', 'https://openalex.org/W2802951432', 'https://openalex.org/W3203397300', 'https://openalex.org/W2790808809', 'https://openalex.org/W2257979135', 'https://openalex.org/W3127561923', 'https://openalex.org/W3170112077', 'https://openalex.org/W3098407580', 'https://openalex.org/W4285026368', 'https://openalex.org/W2919958648', 'https://openalex.org/W3206216308', 'https://openalex.org/W2395579298', 'https://openalex.org/W4255421341', 'https://openalex.org/W6776331362', 'https://openalex.org/W2204088464', 'https://openalex.org/W2791462858', 'https://openalex.org/W3157167083', 'https://openalex.org/W2964059111', 'https://openalex.org/W2560321925', 'https://openalex.org/W2111051539', 'https://openalex.org/W1567512734', 'https://openalex.org/W3018268080', 'https://openalex.org/W3102100346', 'https://openalex.org/W6678815747', 'https://openalex.org/W6797179183', 'https://openalex.org/W3034431451', 'https://openalex.org/W4225739247', 'https://openalex.org/W3047371674', 'https://openalex.org/W2963799213', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963238274', 'https://openalex.org/W3164731060', 'https://openalex.org/W4367058838', 'https://openalex.org/W2125389028', 'https://openalex.org/W582134693', 'https://openalex.org/W3104255923', 'https://openalex.org/W3174807077', 'https://openalex.org/W3190900634', 'https://openalex.org/W3101782842', 'https://openalex.org/W4320013936', 'https://openalex.org/W3101736545', 'https://openalex.org/W3163842339']",2023-02-18
https://openalex.org/W4200599724,https://doi.org/10.3389/fnbeh.2021.811737,"Toward a Computational Neuroethology of Vocal Communication: From Bioacoustics to Neurophysiology, Emerging Tools and Future Directions","Recently developed methods in computational neuroethology have enabled increasingly detailed and comprehensive quantification of animal movements and behavioral kinematics. Vocal communication behavior is well poised for application of similar large-scale quantification methods in the service of physiological and ethological studies. This review describes emerging techniques that can be applied to acoustic and vocal communication signals with the goal of enabling study beyond a small number of model species. We review a range of modern computational methods for bioacoustics, signal processing, and brain-behavior mapping. Along with a discussion of recent advances and techniques, we include challenges and broader goals in establishing a framework for the computational neuroethology of vocal communication.","['https://openalex.org/W1971567117', 'https://openalex.org/W2012685917', 'https://openalex.org/W2561443310', 'https://openalex.org/W2087148916', 'https://openalex.org/W2142509733', 'https://openalex.org/W2034920727', 'https://openalex.org/W1972934497', 'https://openalex.org/W2883879995', 'https://openalex.org/W3168306506', 'https://openalex.org/W2077561414', 'https://openalex.org/W2018435467', 'https://openalex.org/W2081208888', 'https://openalex.org/W6780218876', 'https://openalex.org/W2093291573', 'https://openalex.org/W3125244613', 'https://openalex.org/W2963959907', 'https://openalex.org/W2362300820', 'https://openalex.org/W6762913911', 'https://openalex.org/W2128653836', 'https://openalex.org/W6726371282', 'https://openalex.org/W1971200773', 'https://openalex.org/W2271175897', 'https://openalex.org/W2774728799', 'https://openalex.org/W2797173239', 'https://openalex.org/W2044222806', 'https://openalex.org/W6778883912', 'https://openalex.org/W2767760301', 'https://openalex.org/W2991289264', 'https://openalex.org/W2899428416', 'https://openalex.org/W2013017240', 'https://openalex.org/W2954932437', 'https://openalex.org/W2069266834', 'https://openalex.org/W2172536019', 'https://openalex.org/W1990405950', 'https://openalex.org/W3082273536', 'https://openalex.org/W3036384837', 'https://openalex.org/W2980030724', 'https://openalex.org/W2156287497', 'https://openalex.org/W1906221854', 'https://openalex.org/W2103379218', 'https://openalex.org/W2157482132', 'https://openalex.org/W1618147919', 'https://openalex.org/W6755257315', 'https://openalex.org/W2017487009', 'https://openalex.org/W2551837017', 'https://openalex.org/W2102224657', 'https://openalex.org/W4245284779', 'https://openalex.org/W3156592713', 'https://openalex.org/W2056393660', 'https://openalex.org/W2178703291', 'https://openalex.org/W2894276679', 'https://openalex.org/W6758675244', 'https://openalex.org/W6736723571', 'https://openalex.org/W3147539069', 'https://openalex.org/W6782889966', 'https://openalex.org/W6766955419', 'https://openalex.org/W1561793853', 'https://openalex.org/W2986026538', 'https://openalex.org/W2146305921', 'https://openalex.org/W2773125179', 'https://openalex.org/W2137233223', 'https://openalex.org/W2216090902', 'https://openalex.org/W2165306728', 'https://openalex.org/W2745263305', 'https://openalex.org/W2036899314', 'https://openalex.org/W2120221444', 'https://openalex.org/W2082238466', 'https://openalex.org/W3162317862', 'https://openalex.org/W2159840028', 'https://openalex.org/W2120847449', 'https://openalex.org/W3097777922', 'https://openalex.org/W2113429637', 'https://openalex.org/W2165545766', 'https://openalex.org/W659060387', 'https://openalex.org/W2077130029', 'https://openalex.org/W2113083357', 'https://openalex.org/W2159092826', 'https://openalex.org/W2169944034', 'https://openalex.org/W2802101759', 'https://openalex.org/W2519391998', 'https://openalex.org/W2805791143', 'https://openalex.org/W2008708986', 'https://openalex.org/W6748409065', 'https://openalex.org/W1992116147', 'https://openalex.org/W6772383348', 'https://openalex.org/W2972818416', 'https://openalex.org/W1606205649', 'https://openalex.org/W2152205330', 'https://openalex.org/W2049686551', 'https://openalex.org/W3137265087', 'https://openalex.org/W1514601846', 'https://openalex.org/W2029247433', 'https://openalex.org/W2038096727', 'https://openalex.org/W2809183397', 'https://openalex.org/W2015545476', 'https://openalex.org/W2942893872', 'https://openalex.org/W2089900668', 'https://openalex.org/W3089746580', 'https://openalex.org/W2999938474', 'https://openalex.org/W2731031236', 'https://openalex.org/W2104250137', 'https://openalex.org/W2026219038', 'https://openalex.org/W2002681612', 'https://openalex.org/W6767111847', 'https://openalex.org/W2004735066', 'https://openalex.org/W2072570988', 'https://openalex.org/W6687506355', 'https://openalex.org/W6712379442', 'https://openalex.org/W3022404557', 'https://openalex.org/W2025210087', 'https://openalex.org/W2111376597', 'https://openalex.org/W3102543255', 'https://openalex.org/W2949914308', 'https://openalex.org/W3208858482', 'https://openalex.org/W2982514584', 'https://openalex.org/W2899201823', 'https://openalex.org/W2981733351', 'https://openalex.org/W2406709878', 'https://openalex.org/W2057430594', 'https://openalex.org/W3111687703', 'https://openalex.org/W2962911378', 'https://openalex.org/W2887114371', 'https://openalex.org/W2139788486', 'https://openalex.org/W2062663442', 'https://openalex.org/W6732429163', 'https://openalex.org/W2953160592', 'https://openalex.org/W2071893783', 'https://openalex.org/W1964466469', 'https://openalex.org/W2084617765', 'https://openalex.org/W3007559067', 'https://openalex.org/W3119740068', 'https://openalex.org/W1916056948', 'https://openalex.org/W2550950725', 'https://openalex.org/W3176892407', 'https://openalex.org/W2899641618', 'https://openalex.org/W6785629122', 'https://openalex.org/W3180220247', 'https://openalex.org/W6636885848', 'https://openalex.org/W2912581782', 'https://openalex.org/W2073526836', 'https://openalex.org/W2917898389', 'https://openalex.org/W2951777958', 'https://openalex.org/W2936774411', 'https://openalex.org/W2529829744', 'https://openalex.org/W2952956973', 'https://openalex.org/W2149820199', 'https://openalex.org/W2963300588', 'https://openalex.org/W2784076614', 'https://openalex.org/W2414102402', 'https://openalex.org/W3002895431', 'https://openalex.org/W2610073446', 'https://openalex.org/W6685352114', 'https://openalex.org/W3090629579', 'https://openalex.org/W2122057150', 'https://openalex.org/W3136963322', 'https://openalex.org/W2966658381', 'https://openalex.org/W6749351710', 'https://openalex.org/W2045745009', 'https://openalex.org/W2027961462', 'https://openalex.org/W2031358653', 'https://openalex.org/W3211329195', 'https://openalex.org/W3063400758', 'https://openalex.org/W3197209004', 'https://openalex.org/W2967697509', 'https://openalex.org/W3093377669', 'https://openalex.org/W2995891495', 'https://openalex.org/W6753439088', 'https://openalex.org/W2085633807', 'https://openalex.org/W2973049979', 'https://openalex.org/W2079145130', 'https://openalex.org/W6762517625', 'https://openalex.org/W3207890212', 'https://openalex.org/W2046334150', 'https://openalex.org/W2085216964', 'https://openalex.org/W2088499899', 'https://openalex.org/W2789362687', 'https://openalex.org/W2103869314', 'https://openalex.org/W2936654294', 'https://openalex.org/W2035396734', 'https://openalex.org/W1987774416', 'https://openalex.org/W2071876445', 'https://openalex.org/W1979843934', 'https://openalex.org/W2166506296', 'https://openalex.org/W3096216486', 'https://openalex.org/W2962911926', 'https://openalex.org/W2088699857', 'https://openalex.org/W6973666849', 'https://openalex.org/W3005429488', 'https://openalex.org/W2794255441', 'https://openalex.org/W2962866211', 'https://openalex.org/W2963881567', 'https://openalex.org/W2158053377', 'https://openalex.org/W2139900704', 'https://openalex.org/W2569264471', 'https://openalex.org/W2219858099', 'https://openalex.org/W3087056597', 'https://openalex.org/W2100435836', 'https://openalex.org/W2102353396', 'https://openalex.org/W6785822339', 'https://openalex.org/W2961637526', 'https://openalex.org/W2980055757', 'https://openalex.org/W2519091744', 'https://openalex.org/W2584032004', 'https://openalex.org/W4226257716', 'https://openalex.org/W3084524316', 'https://openalex.org/W2786608204', 'https://openalex.org/W3036601975', 'https://openalex.org/W4220801857', 'https://openalex.org/W3007328579', 'https://openalex.org/W2963602772', 'https://openalex.org/W2943842929', 'https://openalex.org/W2978426779', 'https://openalex.org/W2963684088', 'https://openalex.org/W1637570796', 'https://openalex.org/W4297817572', 'https://openalex.org/W4205665640', 'https://openalex.org/W3001279689', 'https://openalex.org/W2970006822', 'https://openalex.org/W4212774754', 'https://openalex.org/W3100615582', 'https://openalex.org/W2910577860', 'https://openalex.org/W4292779060', 'https://openalex.org/W3192689482', 'https://openalex.org/W3184743205', 'https://openalex.org/W2883961755']",2021-12-20
https://openalex.org/W4319862442,https://doi.org/10.1109/slt54892.2023.10022770,Superb @ SLT 2022: Challenge on Generalization and Efficiency of Self-Supervised Speech Representation Learning,"We present the SUPERB challenge at SLT 2022, which aims at learning self-supervised speech representation for better performance, generalization, and efficiency. The challenge builds upon the SUPERB benchmark and implements metrics to measure the computation requirements of self-supervised learning (SSL) representation and to evaluate its generalizability and performance across the diverse SUPERB tasks. The SUPERB benchmark provides comprehensive coverage of popular speech processing tasks, from speech and speaker recognition to audio generation and semantic understanding. As SSL has gained interest in the speech community and showed promising outcomes, we envision the challenge to uplevel the impact of SSL techniques by motivating more practical designs of techniques beyond task performance. We summarize the results of 14 submitted models in this paper. We also discuss the main findings from those submissions and the future directions of SSL research.","['https://openalex.org/W4281492411', 'https://openalex.org/W6755207826', 'https://openalex.org/W6778883912', 'https://openalex.org/W6774314701', 'https://openalex.org/W6784614252', 'https://openalex.org/W6803378298', 'https://openalex.org/W3206252155', 'https://openalex.org/W3209059054', 'https://openalex.org/W6780218876', 'https://openalex.org/W3197580070', 'https://openalex.org/W6810046013', 'https://openalex.org/W4287889722', 'https://openalex.org/W2972943112', 'https://openalex.org/W3016011332', 'https://openalex.org/W3035202887', 'https://openalex.org/W3097286738', 'https://openalex.org/W3015265920', 'https://openalex.org/W6786669483', 'https://openalex.org/W2982223350', 'https://openalex.org/W6777232839', 'https://openalex.org/W3041561163', 'https://openalex.org/W6769238691', 'https://openalex.org/W3160345865', 'https://openalex.org/W3196919915', 'https://openalex.org/W3096485810', 'https://openalex.org/W3198858531', 'https://openalex.org/W2973157397', 'https://openalex.org/W3015213852', 'https://openalex.org/W4247726808', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W3209984917', 'https://openalex.org/W6750615492', 'https://openalex.org/W6774908330', 'https://openalex.org/W4293819776', 'https://openalex.org/W3189296823', 'https://openalex.org/W3093096176', 'https://openalex.org/W3096216486', 'https://openalex.org/W3095361818', 'https://openalex.org/W1494198834', 'https://openalex.org/W6810007534', 'https://openalex.org/W6809593508', 'https://openalex.org/W6802045652', 'https://openalex.org/W4224821750', 'https://openalex.org/W4319862477', 'https://openalex.org/W6791353385', 'https://openalex.org/W4296068785', 'https://openalex.org/W4319862641', 'https://openalex.org/W3198771897', 'https://openalex.org/W4319862416', 'https://openalex.org/W6839512648', 'https://openalex.org/W4319862404', 'https://openalex.org/W3016167541', 'https://openalex.org/W3204917342', 'https://openalex.org/W6849880362', 'https://openalex.org/W6795952400', 'https://openalex.org/W4319862642', 'https://openalex.org/W6759579507', 'https://openalex.org/W4319862700', 'https://openalex.org/W4221161768', 'https://openalex.org/W3166396011', 'https://openalex.org/W2936361916', 'https://openalex.org/W4319862670', 'https://openalex.org/W3008526508', 'https://openalex.org/W3093579165', 'https://openalex.org/W2896457183', 'https://openalex.org/W3035060554', 'https://openalex.org/W3148040514', 'https://openalex.org/W3112034174', 'https://openalex.org/W2979476256', 'https://openalex.org/W4287173589', 'https://openalex.org/W3036601975', 'https://openalex.org/W4285250921', 'https://openalex.org/W2964303773', 'https://openalex.org/W4221145109', 'https://openalex.org/W4292779060', 'https://openalex.org/W3203140070', 'https://openalex.org/W3005680577', 'https://openalex.org/W4297808394', 'https://openalex.org/W3211224152', 'https://openalex.org/W4280638376', 'https://openalex.org/W2981991061', 'https://openalex.org/W2923014074']",2023-01-09
https://openalex.org/W3093427098,https://doi.org/10.48550/arxiv.2010.05967,The Zero Resource Speech Challenge 2020: Discovering discrete subword and word units,"We present the Zero Resource Speech Challenge 2020, which aims at learning speech representations from raw audio signals without any labels. It combines the data sets and metrics from two previous benchmarks (2017 and 2019) and features two tasks which tap into two levels of speech representation. The first task is to discover low bit-rate subword representations that optimize the quality of speech synthesis; the second one is to discover word-like units from unsegmented raw speech. We present the results of the twenty submitted models and discuss the implications of the main findings for unsupervised speech learning.","['https://openalex.org/W3097485645', 'https://openalex.org/W2347098582', 'https://openalex.org/W3097692357', 'https://openalex.org/W2940544976', 'https://openalex.org/W2572097499', 'https://openalex.org/W2296607128', 'https://openalex.org/W3095361818', 'https://openalex.org/W3024040651', 'https://openalex.org/W3044386551', 'https://openalex.org/W3096216486', 'https://openalex.org/W2126377586', 'https://openalex.org/W4288107125', 'https://openalex.org/W2996383576', 'https://openalex.org/W3096262326', 'https://openalex.org/W1524333225', 'https://openalex.org/W4300047444', 'https://openalex.org/W2284628133', 'https://openalex.org/W2346964103', 'https://openalex.org/W2598638573', 'https://openalex.org/W2962693497', 'https://openalex.org/W2057007397', 'https://openalex.org/W2962699523', 'https://openalex.org/W2547039119', 'https://openalex.org/W3096359985', 'https://openalex.org/W3097056138', 'https://openalex.org/W2979476256', 'https://openalex.org/W3097159218', 'https://openalex.org/W3084014658']",2020-10-12
https://openalex.org/W4386076005,https://doi.org/10.1109/cvpr52729.2023.01802,ReVISE: Self-Supervised Speech Resynthesis with Visual Input for Universal and Generalized Speech Regeneration,"Prior works on improving speech quality with visual input typically study each type of auditory distortion separately (e.g., separation, inpainting, video-to-speech) and present tailored algorithms. This paper proposes to unify these subjects and study Generalized Speech Regeneration, where the goal is not to reconstruct the exact reference clean signal, but to focus on improving certain aspects of speech while not necessarily preserving the rest such as voice. In particular, this paper concerns intelligibility, quality, and video synchronization. We cast the problem as audio-visual speech resynthesis, which is composed of two steps: pseudo audio-visual speech recognition (P-AVSR) and pseudo text-to-speech synthesis (P-TTS). P-AVSR and P-TTS are connected by discrete units derived from a self-supervised speech model. Moreover, we utilize self-supervised audio-visual speech model to initialize P-AVSR. The proposed model is coined ReVISE. ReVISE is the first high-quality model for in-the-wild video-to-speech synthesis and achieves superior performance on all LRS3 audio-visual regeneration tasks with a single model. To demonstrates its applicability in the real world, ReVISE is also evaluated on EasyCom, an audio-visual benchmark collected under challenging acoustic conditions with only 1.6 hours of training data. Similarly, ReVISE greatly suppresses noise and improves quality. Project page: https://wnhsu.github.io/ReVISE/.","['https://openalex.org/W4312337341', 'https://openalex.org/W3160525311', 'https://openalex.org/W2585824449', 'https://openalex.org/W2981905048', 'https://openalex.org/W2015143272', 'https://openalex.org/W2609317876', 'https://openalex.org/W2075076415', 'https://openalex.org/W1482149378', 'https://openalex.org/W3182657421', 'https://openalex.org/W2568308529', 'https://openalex.org/W2141998673', 'https://openalex.org/W4200483526', 'https://openalex.org/W3163296124', 'https://openalex.org/W2160473997', 'https://openalex.org/W2141411743', 'https://openalex.org/W6734491695', 'https://openalex.org/W3160408143', 'https://openalex.org/W6735927292', 'https://openalex.org/W6780218876', 'https://openalex.org/W2912056372', 'https://openalex.org/W4224934174', 'https://openalex.org/W3162420637', 'https://openalex.org/W3162293946', 'https://openalex.org/W2952218014', 'https://openalex.org/W6794378236', 'https://openalex.org/W2587105599', 'https://openalex.org/W2962935966', 'https://openalex.org/W2296153915', 'https://openalex.org/W6774995033', 'https://openalex.org/W3161904430', 'https://openalex.org/W6839936984', 'https://openalex.org/W3209059054', 'https://openalex.org/W2995181338', 'https://openalex.org/W2593116425', 'https://openalex.org/W3216976702', 'https://openalex.org/W2107860279', 'https://openalex.org/W6783867762', 'https://openalex.org/W6790356757', 'https://openalex.org/W3099330747', 'https://openalex.org/W3140429000', 'https://openalex.org/W4394671563', 'https://openalex.org/W2891205112', 'https://openalex.org/W3179026300', 'https://openalex.org/W2936774411', 'https://openalex.org/W3035268204', 'https://openalex.org/W4221153068', 'https://openalex.org/W3036601975', 'https://openalex.org/W3180374548', 'https://openalex.org/W4285595742', 'https://openalex.org/W3203491020', 'https://openalex.org/W3119308075', 'https://openalex.org/W4296069328', 'https://openalex.org/W3030437843', 'https://openalex.org/W3123318516', 'https://openalex.org/W3092028330', 'https://openalex.org/W4281820413', 'https://openalex.org/W2899663614', 'https://openalex.org/W3097945073', 'https://openalex.org/W2604379605', 'https://openalex.org/W3020336359', 'https://openalex.org/W3157840621', 'https://openalex.org/W2963082324', 'https://openalex.org/W3096216486']",2023-06-01
https://openalex.org/W4391640582,https://doi.org/10.1109/tkde.2024.3363711,Modeling Spatio-Temporal Dynamical Systems With Neural Discrete Learning and Levels-of-Experts,"In this paper, we address the issue of modeling and estimating changes in the state of the spatio-temporal dynamical systems based on a sequence of observations like video frames. Traditional numerical simulation systems depend largely on the initial settings and correctness of the constructed partial differential equations&amp;#x00A0;(PDEs). Despite recent efforts yielding significant success in discovering data-driven PDEs with neural networks, the limitations posed by singular scenarios and the absence of local insights prevent them from performing effectively in a broader real-world context. To this end, this paper propose the universal expert module &amp;#x2013; that is, optical flow estimation component, to capture the evolution laws of general physical processes in a data-driven fashion. To enhance local insight, we painstakingly design a finer-grained physical pipeline, since local characteristics may be influenced by various internal contextual information, which may contradict the macroscopic properties of the whole system. Further, we harness currently popular neural discrete learning to unveil the underlying important features in its latent space, this process better injects interpretability, which can help us obtain a powerful prior over these discrete random variables. We conduct extensive experiments and ablations to demonstrate that the proposed framework achieves large performance margins, compared with the existing SOTA baselines. IEEE","['https://openalex.org/W4236810859', 'https://openalex.org/W2062183687', 'https://openalex.org/W4385678435', 'https://openalex.org/W3176723190', 'https://openalex.org/W3157753091', 'https://openalex.org/W6780322082', 'https://openalex.org/W6796386619', 'https://openalex.org/W2018259456', 'https://openalex.org/W4302602374', 'https://openalex.org/W2133974650', 'https://openalex.org/W2082163389', 'https://openalex.org/W2126642381', 'https://openalex.org/W4289677812', 'https://openalex.org/W4245586707', 'https://openalex.org/W2147584847', 'https://openalex.org/W607590176', 'https://openalex.org/W4292874936', 'https://openalex.org/W4256148958', 'https://openalex.org/W2995375411', 'https://openalex.org/W2203837178', 'https://openalex.org/W2131908956', 'https://openalex.org/W2147360693', 'https://openalex.org/W2067696760', 'https://openalex.org/W2040928359', 'https://openalex.org/W2104612958', 'https://openalex.org/W1993074287', 'https://openalex.org/W1983906712', 'https://openalex.org/W4386295584', 'https://openalex.org/W3184636400', 'https://openalex.org/W1517096670', 'https://openalex.org/W4287512175', 'https://openalex.org/W6747085055', 'https://openalex.org/W1987475960', 'https://openalex.org/W2586258431', 'https://openalex.org/W2104724476', 'https://openalex.org/W2255466643', 'https://openalex.org/W6637967152', 'https://openalex.org/W2618530766', 'https://openalex.org/W2765811365', 'https://openalex.org/W2194775991', 'https://openalex.org/W6628877408', 'https://openalex.org/W2995511918', 'https://openalex.org/W3034426027', 'https://openalex.org/W6771924513', 'https://openalex.org/W2899283552', 'https://openalex.org/W3163993681', 'https://openalex.org/W6839401937', 'https://openalex.org/W3138340468', 'https://openalex.org/W6783741718', 'https://openalex.org/W2752796333', 'https://openalex.org/W6757925885', 'https://openalex.org/W3159481202', 'https://openalex.org/W2963019788', 'https://openalex.org/W6746445604', 'https://openalex.org/W3133294546', 'https://openalex.org/W3080930191', 'https://openalex.org/W6779914669', 'https://openalex.org/W6751350349', 'https://openalex.org/W6746530577', 'https://openalex.org/W6929334658', 'https://openalex.org/W2749028154', 'https://openalex.org/W2803629276', 'https://openalex.org/W3041682155', 'https://openalex.org/W6838444500', 'https://openalex.org/W6795000047', 'https://openalex.org/W6810641295', 'https://openalex.org/W6803548522', 'https://openalex.org/W1578285471', 'https://openalex.org/W1578985305', 'https://openalex.org/W2131747574', 'https://openalex.org/W2269094482', 'https://openalex.org/W2963305757', 'https://openalex.org/W3109908659', 'https://openalex.org/W3147597595', 'https://openalex.org/W6762931180', 'https://openalex.org/W6790720088', 'https://openalex.org/W6850822533', 'https://openalex.org/W3214281017', 'https://openalex.org/W2963411289', 'https://openalex.org/W6677477928', 'https://openalex.org/W6691096134', 'https://openalex.org/W2963092440', 'https://openalex.org/W6634221342', 'https://openalex.org/W6677326919', 'https://openalex.org/W6735992252', 'https://openalex.org/W2808493349', 'https://openalex.org/W6766861245', 'https://openalex.org/W6763239785', 'https://openalex.org/W6759977823', 'https://openalex.org/W6784333009', 'https://openalex.org/W4293192773', 'https://openalex.org/W2921532413', 'https://openalex.org/W3088611441', 'https://openalex.org/W3193281533', 'https://openalex.org/W2964915865', 'https://openalex.org/W4285111042', 'https://openalex.org/W6745829810', 'https://openalex.org/W6757613341', 'https://openalex.org/W4313009682', 'https://openalex.org/W6771935200', 'https://openalex.org/W6771200186', 'https://openalex.org/W3171345413', 'https://openalex.org/W3166762869', 'https://openalex.org/W6793801364', 'https://openalex.org/W2996680032', 'https://openalex.org/W2935711438', 'https://openalex.org/W3096216486', 'https://openalex.org/W1867429401', 'https://openalex.org/W4249022109', 'https://openalex.org/W764651262', 'https://openalex.org/W6764350350', 'https://openalex.org/W2560474170', 'https://openalex.org/W3184447318', 'https://openalex.org/W6785723781', 'https://openalex.org/W6784227514', 'https://openalex.org/W6705521900', 'https://openalex.org/W2967033144', 'https://openalex.org/W6852542248', 'https://openalex.org/W4297941216', 'https://openalex.org/W3165462110', 'https://openalex.org/W2971074500', 'https://openalex.org/W4285483867', 'https://openalex.org/W3102272367', 'https://openalex.org/W4387491626', 'https://openalex.org/W4280513387', 'https://openalex.org/W2963799213', 'https://openalex.org/W4310301864', 'https://openalex.org/W3133405188', 'https://openalex.org/W4295289379', 'https://openalex.org/W4226053834', 'https://openalex.org/W4403791494', 'https://openalex.org/W2979786244', 'https://openalex.org/W1558736232', 'https://openalex.org/W3035664806', 'https://openalex.org/W2768975186', 'https://openalex.org/W3104861085', 'https://openalex.org/W4297772798', 'https://openalex.org/W2118688707', 'https://openalex.org/W3208915345', 'https://openalex.org/W4298857966', 'https://openalex.org/W3092923133', 'https://openalex.org/W4288347171', 'https://openalex.org/W4225619856', 'https://openalex.org/W2778033595', 'https://openalex.org/W3094502228', 'https://openalex.org/W1485009520', 'https://openalex.org/W2044520935', 'https://openalex.org/W2770604561', 'https://openalex.org/W2970427641', 'https://openalex.org/W2951736998', 'https://openalex.org/W3159218436', 'https://openalex.org/W3212171847', 'https://openalex.org/W2918222882', 'https://openalex.org/W3152733922', 'https://openalex.org/W4294306266', 'https://openalex.org/W2769257693']",2024-02-08
https://openalex.org/W3155217823,https://doi.org/10.1109/access.2021.3071541,End-to-End Image-to-Speech Generation for Untranscribed Unknown Languages,"Describing orally what we are seeing is a simple task we do in our daily life. However, in the natural language processing field, this simple task needs to be bridged by a textual modality that helps the system to generalize various objects in the image and various pronunciations in speech utterances. In this study, we propose an end-to-end Image2Speech system that does not need any textual information in its training. We use a vector-quantized variational autoencoder (VQ-VAE) model to learn the discrete representation of a speech caption in an unsupervised manner, where discrete labels are used by an image-captioning model. This self-supervised speech representation enables the Image2Speech model to be trained with the minimum amount of paired image-speech data while still maintaining the quality of the speech caption. Our experimental results with a multi-speaker natural speech dataset demonstrate our proposed text-free Image2Speech system&#x2019;s performance close to the one with textual information. Furthermore, our approach also successfully outperforms the most recent existing frameworks with phoneme-based and grounding-based Image2Speech systems.","['https://openalex.org/W6729977899', 'https://openalex.org/W2101105183', 'https://openalex.org/W2963909453', 'https://openalex.org/W6631190155', 'https://openalex.org/W2191779130', 'https://openalex.org/W2327501763', 'https://openalex.org/W6682631176', 'https://openalex.org/W2123301721', 'https://openalex.org/W1956340063', 'https://openalex.org/W2117041980', 'https://openalex.org/W2951444698', 'https://openalex.org/W2170659185', 'https://openalex.org/W2079460648', 'https://openalex.org/W2057007397', 'https://openalex.org/W2398490608', 'https://openalex.org/W3007068036', 'https://openalex.org/W4206865574', 'https://openalex.org/W3096249149', 'https://openalex.org/W6787100281', 'https://openalex.org/W3093096176', 'https://openalex.org/W2120847449', 'https://openalex.org/W2940544976', 'https://openalex.org/W2345811097', 'https://openalex.org/W2962862718', 'https://openalex.org/W2347098582', 'https://openalex.org/W2972374322', 'https://openalex.org/W2964115348', 'https://openalex.org/W2963620343', 'https://openalex.org/W3096216486', 'https://openalex.org/W2347145335', 'https://openalex.org/W3009205145', 'https://openalex.org/W6640963894', 'https://openalex.org/W2122538988', 'https://openalex.org/W2194775991', 'https://openalex.org/W2752796333', 'https://openalex.org/W2963609956', 'https://openalex.org/W6739901393', 'https://openalex.org/W2964121744', 'https://openalex.org/W1522301498', 'https://openalex.org/W1959608418', 'https://openalex.org/W2154652894', 'https://openalex.org/W2963799213', 'https://openalex.org/W3114436296', 'https://openalex.org/W2556930864', 'https://openalex.org/W2973026522', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963403868', 'https://openalex.org/W3174311593', 'https://openalex.org/W2119775030', 'https://openalex.org/W3093427098']",2021-01-01
https://openalex.org/W3161348170,https://doi.org/10.18653/v1/2021.blackboxnlp-1.11,Discrete representations in neural models of spoken language,"The distributed and continuous representations used by neural networks are at odds with representations employed in linguistics, which are typically symbolic. Vector quantization has been proposed as a way to induce discrete neural representations that are closer in nature to their linguistic counterparts. However, it is not clear which metrics are the best-suited to analyze such discrete representations. We compare the merits of four commonly used metrics in the context of weakly supervised models of spoken language. We compare the results they show when applied to two different models, while systematically studying the effect of the placement and size of the discretization layer. We find that different evaluation regimes can give inconsistent results. While we can attribute them to the properties of the different metrics in most cases, one point of concern remains: the use of minimal pairs of phoneme triples as stimuli disadvantages larger discrete unit inventories, unlike metrics applied to complete utterances. Furthermore, while in general vector quantization induces representations that correlate with units posited in linguistics, the strength of this correlation is only moderate.","['https://openalex.org/W2024081693', 'https://openalex.org/W2888912391', 'https://openalex.org/W3096216486', 'https://openalex.org/W3095361818', 'https://openalex.org/W2972867623', 'https://openalex.org/W3100813302', 'https://openalex.org/W2962862718', 'https://openalex.org/W1524333225', 'https://openalex.org/W2936295285', 'https://openalex.org/W3025429027', 'https://openalex.org/W2947591107', 'https://openalex.org/W3105148948', 'https://openalex.org/W2964204621', 'https://openalex.org/W3044967013', 'https://openalex.org/W2963430224', 'https://openalex.org/W3027324582', 'https://openalex.org/W2593779438', 'https://openalex.org/W2396043527', 'https://openalex.org/W2119775030', 'https://openalex.org/W3157861865', 'https://openalex.org/W2796315435', 'https://openalex.org/W2963620343', 'https://openalex.org/W2515741950', 'https://openalex.org/W2972374322', 'https://openalex.org/W2138615112', 'https://openalex.org/W2984673553', 'https://openalex.org/W2964054038', 'https://openalex.org/W2973047874', 'https://openalex.org/W2963799213', 'https://openalex.org/W2786608204', 'https://openalex.org/W2242818861', 'https://openalex.org/W2940544976', 'https://openalex.org/W2962776659', 'https://openalex.org/W2894164357', 'https://openalex.org/W2137010615', 'https://openalex.org/W2160654481', 'https://openalex.org/W4288107125', 'https://openalex.org/W3035750922', 'https://openalex.org/W3035305735', 'https://openalex.org/W2964121744', 'https://openalex.org/W3098952151', 'https://openalex.org/W1522301498', 'https://openalex.org/W2949079242', 'https://openalex.org/W3093096176', 'https://openalex.org/W2946296745', 'https://openalex.org/W3097286738', 'https://openalex.org/W3171345413', 'https://openalex.org/W2971709506', 'https://openalex.org/W2194775991', 'https://openalex.org/W3100270690', 'https://openalex.org/W3087357110', 'https://openalex.org/W2995680346', 'https://openalex.org/W2963902314', 'https://openalex.org/W3125709657']",2021-01-01
https://openalex.org/W4224918488,https://doi.org/10.1109/icassp43922.2022.9747259,Self Supervised Representation Learning with Deep Clustering for Acoustic Unit Discovery from Raw Speech,"The automatic discovery of acoustic sub-word units from raw speech, without any text or labels, is a growing field of research. The key challenge is to derive representations of speech that can be categorized into a small number of phoneme-like units which are speaker invariant and can broadly capture the content variability of speech. In this work, we propose a novel neural network paradigm that uses the deep clustering loss along with the autoregressive contrastive predictive coding (CPC) loss. Both the loss functions, the CPC and the clustering loss, are self-supervised. The clustering cost involves the loss function using the phoneme-like labels generated with an iterative k-means algorithm. The inclusion of this loss ensures that the model representations can be categorized into a small number of automatic speech units. We experiment with several sub-tasks described as part of the Zerospeech 2021 challenge to illustrate the effectiveness of the framework. In these experiments, we show that proposed representation learning approach improves significantly over the previous self-supervision based models as well as the wav2vec family of models on a range of word-level similarity tasks and language modeling tasks.","['https://openalex.org/W2741692265', 'https://openalex.org/W6771812881', 'https://openalex.org/W6629717138', 'https://openalex.org/W6770941677', 'https://openalex.org/W2787447541', 'https://openalex.org/W2785860501', 'https://openalex.org/W3095361818', 'https://openalex.org/W2787223168', 'https://openalex.org/W2935542736', 'https://openalex.org/W3100270690', 'https://openalex.org/W3015213852', 'https://openalex.org/W3041561163', 'https://openalex.org/W3197381195', 'https://openalex.org/W3035725276', 'https://openalex.org/W6753000030', 'https://openalex.org/W6640828828', 'https://openalex.org/W6755207826', 'https://openalex.org/W1949782964', 'https://openalex.org/W2786608204', 'https://openalex.org/W2124509324', 'https://openalex.org/W2117041980', 'https://openalex.org/W3093096176', 'https://openalex.org/W2963620343', 'https://openalex.org/W2114347655', 'https://openalex.org/W3197259906', 'https://openalex.org/W6790356757', 'https://openalex.org/W3096216486', 'https://openalex.org/W2973049979', 'https://openalex.org/W2972943112', 'https://openalex.org/W6780218876', 'https://openalex.org/W6769196770', 'https://openalex.org/W2752796333', 'https://openalex.org/W2842511635', 'https://openalex.org/W2883725317', 'https://openalex.org/W1957665339', 'https://openalex.org/W3036601975', 'https://openalex.org/W2995181338', 'https://openalex.org/W2896457183', 'https://openalex.org/W4394671563', 'https://openalex.org/W2187089797', 'https://openalex.org/W2979476256', 'https://openalex.org/W1494198834', 'https://openalex.org/W2996728628', 'https://openalex.org/W2963799213', 'https://openalex.org/W4297808394']",2022-04-27
https://openalex.org/W3094247854,https://doi.org/10.21437/interspeech.2021-1936,Unsupervised Learning of Disentangled Speech Content and Style Representation,"We present an approach for unsupervised learning of speech representation disentangling contents and styles. Our model consists of: (1) a local encoder that captures per-frame information; (2) a global encoder that captures per-utterance information; and (3) a conditional decoder that reconstructs speech given local and global latent variables. Our experiments show that (1) the local latent variables encode speech contents, as reconstructed speech can be recognized by ASR with low word error rates (WER), even with a different global encoding; (2) the global latent variables encode speaker style, as reconstructed speech shares speaker identity with the source utterance of the global encoding. Additionally, we demonstrate an useful application from our pre-trained model, where we can train a speaker recognition model from the global latent variables and achieve high accuracy by fine-tuning with as few data as one label per speaker.","['https://openalex.org/W3140968660', 'https://openalex.org/W2973049979', 'https://openalex.org/W2902070858', 'https://openalex.org/W2963272440', 'https://openalex.org/W3015212100', 'https://openalex.org/W1494198834', 'https://openalex.org/W3125709657', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963618559', 'https://openalex.org/W2963568578', 'https://openalex.org/W2099471712', 'https://openalex.org/W2963873275', 'https://openalex.org/W2962793481', 'https://openalex.org/W2964307104', 'https://openalex.org/W3043747271', 'https://openalex.org/W2963341956', 'https://openalex.org/W3097777922', 'https://openalex.org/W1959608418', 'https://openalex.org/W2842511635', 'https://openalex.org/W2946006146', 'https://openalex.org/W2803832867', 'https://openalex.org/W1522301498', 'https://openalex.org/W2792263949', 'https://openalex.org/W3099782249', 'https://openalex.org/W3096216486']",2021-08-27
https://openalex.org/W3112085130,https://doi.org/10.1109/ojsp.2021.3076914,The Effectiveness of Unsupervised Subword Modeling With Autoregressive and Cross-Lingual Phone-Aware Networks,"This study addresses unsupervised subword modeling, i.e., learning acoustic feature representations that can distinguish between subword units of a language. We propose a two-stage learning framework that combines self-supervised learning and cross-lingual knowledge transfer. The framework consists of autoregressive predictive coding (APC) as the front-end and a cross-lingual deep neural network (DNN) as the back-end. Experiments on the ABX subword discriminability task conducted with the Libri-light and ZeroSpeech 2017 databases showed that our approach is competitive or superior to state-of-the-art studies. Comprehensive and systematic analyses at the phoneme- and articulatory feature (AF)-level showed that our approach was better at capturing diphthong than monophthong vowel information, while also differences in the amount of information captured for different types of consonants were observed. Moreover, a positive correlation was found between the effectiveness of the back-end in capturing a phoneme's information and the quality of the cross-lingual phone labels assigned to the phoneme. The AF-level analysis together with t-SNE visualization results showed that the proposed approach is better than MFCC and APC features in capturing manner and place of articulation information, vowel height, and backness information. Taken together, the analyses showed that the two stages in our approach are both effective in capturing phoneme and AF information. Nevertheless, monophthong vowel information is less well captured than consonant information, which suggests that future research should focus on improving capturing monophthong vowel information.&lt;br/&gt;","['https://openalex.org/W2085628288', 'https://openalex.org/W6713762819', 'https://openalex.org/W2905332681', 'https://openalex.org/W2020607164', 'https://openalex.org/W2973157397', 'https://openalex.org/W2786902352', 'https://openalex.org/W2972841524', 'https://openalex.org/W6678947187', 'https://openalex.org/W3044483536', 'https://openalex.org/W2759889345', 'https://openalex.org/W2785860501', 'https://openalex.org/W2895297209', 'https://openalex.org/W2785415724', 'https://openalex.org/W2940544976', 'https://openalex.org/W6780483730', 'https://openalex.org/W2826003142', 'https://openalex.org/W2025722797', 'https://openalex.org/W2889228998', 'https://openalex.org/W6748325621', 'https://openalex.org/W6601192135', 'https://openalex.org/W2972943112', 'https://openalex.org/W6631190155', 'https://openalex.org/W6631362777', 'https://openalex.org/W6636811518', 'https://openalex.org/W3005578234', 'https://openalex.org/W1984076147', 'https://openalex.org/W2787223168', 'https://openalex.org/W2995181338', 'https://openalex.org/W6745117592', 'https://openalex.org/W2842511635', 'https://openalex.org/W2516890051', 'https://openalex.org/W3006358483', 'https://openalex.org/W1494198834', 'https://openalex.org/W2514741789', 'https://openalex.org/W2327501763', 'https://openalex.org/W6675365184', 'https://openalex.org/W6696934422', 'https://openalex.org/W6781819162', 'https://openalex.org/W2927673779', 'https://openalex.org/W2748598007', 'https://openalex.org/W3016368932', 'https://openalex.org/W2950414763', 'https://openalex.org/W6973666849', 'https://openalex.org/W2962824366', 'https://openalex.org/W1778492285', 'https://openalex.org/W2962693497', 'https://openalex.org/W2787447541', 'https://openalex.org/W2752796333', 'https://openalex.org/W2971041032', 'https://openalex.org/W2963620343', 'https://openalex.org/W2513125788', 'https://openalex.org/W2594951208', 'https://openalex.org/W2147768505', 'https://openalex.org/W2160815625', 'https://openalex.org/W1975728937', 'https://openalex.org/W6675022971', 'https://openalex.org/W2641832364', 'https://openalex.org/W6712553779', 'https://openalex.org/W3111682954', 'https://openalex.org/W2972374322', 'https://openalex.org/W2964245029', 'https://openalex.org/W3093096176', 'https://openalex.org/W3005511757', 'https://openalex.org/W2949510815', 'https://openalex.org/W3095361818', 'https://openalex.org/W3100270690', 'https://openalex.org/W3096216486', 'https://openalex.org/W3096359985']",2021-01-01
https://openalex.org/W3144810982,https://doi.org/10.48550/arxiv.2007.00991,Data Augmenting Contrastive Learning of Speech Representations in the\n Time Domain,"Contrastive Predictive Coding (CPC), based on predicting future segments of\nspeech based on past segments is emerging as a powerful algorithm for\nrepresentation learning of speech signal. However, it still under-performs\nother methods on unsupervised evaluation benchmarks. Here, we introduce\nWavAugment, a time-domain data augmentation library and find that applying\naugmentation in the past is generally more efficient and yields better\nperformances than other methods. We find that a combination of pitch\nmodification, additive noise and reverberation substantially increase the\nperformance of CPC (relative improvement of 18-22%), beating the reference\nLibri-light results with 600 times less data. Using an out-of-domain dataset,\ntime-domain data augmentation can push CPC to be on par with the state of the\nart on the Zero Speech Benchmark 2017. We also show that time-domain data\naugmentation consistently improves downstream limited-supervision phoneme\nclassification tasks by a factor of 12-15% relative.\n","['https://openalex.org/W3015213852', 'https://openalex.org/W2148349024', 'https://openalex.org/W2346964103', 'https://openalex.org/W2593779438', 'https://openalex.org/W2407080277', 'https://openalex.org/W3005680577', 'https://openalex.org/W2972943112', 'https://openalex.org/W2696967604', 'https://openalex.org/W2184343439', 'https://openalex.org/W2990583358', 'https://openalex.org/W2963620343', 'https://openalex.org/W2055408826', 'https://openalex.org/W2883725317', 'https://openalex.org/W3100270690', 'https://openalex.org/W2973049979', 'https://openalex.org/W4288107125', 'https://openalex.org/W2786608204', 'https://openalex.org/W3016011332', 'https://openalex.org/W2995181338', 'https://openalex.org/W4297808394', 'https://openalex.org/W3016181583', 'https://openalex.org/W3002741552', 'https://openalex.org/W2973026522', 'https://openalex.org/W2930682606', 'https://openalex.org/W2100768664', 'https://openalex.org/W2787447541', 'https://openalex.org/W3093427098', 'https://openalex.org/W3015783745', 'https://openalex.org/W3102342027', 'https://openalex.org/W4300047444', 'https://openalex.org/W3093096176', 'https://openalex.org/W2940544976', 'https://openalex.org/W3125709657', 'https://openalex.org/W4214784181', 'https://openalex.org/W2983785920', 'https://openalex.org/W2952217990', 'https://openalex.org/W2963074118', 'https://openalex.org/W2509930204', 'https://openalex.org/W2117041980', 'https://openalex.org/W2946822591', 'https://openalex.org/W1494198834', 'https://openalex.org/W2219249508', 'https://openalex.org/W2970971581', 'https://openalex.org/W2842511635', 'https://openalex.org/W2347098582', 'https://openalex.org/W3003875258', 'https://openalex.org/W4295312788', 'https://openalex.org/W1989674786', 'https://openalex.org/W2020607164', 'https://openalex.org/W2936774411']",2020-07-02
https://openalex.org/W3170201991,https://doi.org/10.1016/j.specom.2022.02.005,Unsupervised Automatic Speech Recognition: A review,"Automatic Speech Recognition (ASR) systems can be trained to achieve\nremarkable performance given large amounts of manually transcribed speech, but\nlarge labeled data sets can be difficult or expensive to acquire for all\nlanguages of interest. In this paper, we review the research literature to\nidentify models and ideas that could lead to fully unsupervised ASR, including\nunsupervised segmentation of the speech signal, unsupervised mapping from\nspeech segments to text, and semi-supervised models with nominal amounts of\nlabeled examples. The objective of the study is to identify the limitations of\nwhat can be learned from speech data alone and to understand the minimum\nrequirements for speech recognition. Identifying these limitations would help\noptimize the resources and efforts in ASR development for low-resource\nlanguages.\n","['https://openalex.org/W2395342389', 'https://openalex.org/W2962777061', 'https://openalex.org/W6744797106', 'https://openalex.org/W2523298034', 'https://openalex.org/W2578392894', 'https://openalex.org/W6655470396', 'https://openalex.org/W2396043527', 'https://openalex.org/W6780218876', 'https://openalex.org/W2074546930', 'https://openalex.org/W2064699871', 'https://openalex.org/W2407151108', 'https://openalex.org/W2399576818', 'https://openalex.org/W6748489002', 'https://openalex.org/W2972706021', 'https://openalex.org/W2962824709', 'https://openalex.org/W2586148577', 'https://openalex.org/W2510842514', 'https://openalex.org/W2963425185', 'https://openalex.org/W2972943112', 'https://openalex.org/W2804648901', 'https://openalex.org/W2963571336', 'https://openalex.org/W6607396543', 'https://openalex.org/W6758654326', 'https://openalex.org/W2144800021', 'https://openalex.org/W2097207027', 'https://openalex.org/W2940544976', 'https://openalex.org/W6697293080', 'https://openalex.org/W3093096176', 'https://openalex.org/W2110485445', 'https://openalex.org/W2107959623', 'https://openalex.org/W2107038463', 'https://openalex.org/W2758697525', 'https://openalex.org/W6677207036', 'https://openalex.org/W2056486423', 'https://openalex.org/W2111732304', 'https://openalex.org/W2160407676', 'https://openalex.org/W2122228338', 'https://openalex.org/W2126377586', 'https://openalex.org/W6622607008', 'https://openalex.org/W2127141656', 'https://openalex.org/W3097777922', 'https://openalex.org/W3097056138', 'https://openalex.org/W2889282842', 'https://openalex.org/W2972630480', 'https://openalex.org/W6680628865', 'https://openalex.org/W6761083334', 'https://openalex.org/W2556930864', 'https://openalex.org/W6685484048', 'https://openalex.org/W2194775991', 'https://openalex.org/W2345811097', 'https://openalex.org/W6748342566', 'https://openalex.org/W2090861223', 'https://openalex.org/W2160815625', 'https://openalex.org/W6754770876', 'https://openalex.org/W6804246771', 'https://openalex.org/W3114632476', 'https://openalex.org/W6656737381', 'https://openalex.org/W2407964052', 'https://openalex.org/W6664486393', 'https://openalex.org/W2126953647', 'https://openalex.org/W2126449874', 'https://openalex.org/W4252331534', 'https://openalex.org/W2059824090', 'https://openalex.org/W6771812881', 'https://openalex.org/W6756098772', 'https://openalex.org/W6632653590', 'https://openalex.org/W2468716020', 'https://openalex.org/W6735305794', 'https://openalex.org/W2407080277', 'https://openalex.org/W3096656254', 'https://openalex.org/W6773511912', 'https://openalex.org/W6765510844', 'https://openalex.org/W6634688400', 'https://openalex.org/W2100768664', 'https://openalex.org/W1778492285', 'https://openalex.org/W6665204316', 'https://openalex.org/W2114510609', 'https://openalex.org/W2618238855', 'https://openalex.org/W2962799225', 'https://openalex.org/W2078834097', 'https://openalex.org/W1965635292', 'https://openalex.org/W6684442412', 'https://openalex.org/W2963137467', 'https://openalex.org/W6766240100', 'https://openalex.org/W2089458547', 'https://openalex.org/W2096330373', 'https://openalex.org/W2083393647', 'https://openalex.org/W2114347655', 'https://openalex.org/W2145410271', 'https://openalex.org/W2013588070', 'https://openalex.org/W2750499125', 'https://openalex.org/W2072054026', 'https://openalex.org/W2010188467', 'https://openalex.org/W2398490608', 'https://openalex.org/W2768381684', 'https://openalex.org/W130754613', 'https://openalex.org/W6773553514', 'https://openalex.org/W2115867364', 'https://openalex.org/W6670225552', 'https://openalex.org/W2395899413', 'https://openalex.org/W6743102125', 'https://openalex.org/W3102519966', 'https://openalex.org/W6754496211', 'https://openalex.org/W2158266063', 'https://openalex.org/W3096359985', 'https://openalex.org/W3111682954', 'https://openalex.org/W2786608204', 'https://openalex.org/W6677007964', 'https://openalex.org/W2962799131', 'https://openalex.org/W6751975539', 'https://openalex.org/W6684399791', 'https://openalex.org/W2906122999', 'https://openalex.org/W6738476557', 'https://openalex.org/W2511733680', 'https://openalex.org/W2973013862', 'https://openalex.org/W6684944071', 'https://openalex.org/W6678470480', 'https://openalex.org/W6670629611', 'https://openalex.org/W6643712209', 'https://openalex.org/W2890964092', 'https://openalex.org/W3097485645', 'https://openalex.org/W2193413348', 'https://openalex.org/W4365806309', 'https://openalex.org/W2786902352', 'https://openalex.org/W3095361818', 'https://openalex.org/W3148186152', 'https://openalex.org/W2171019095', 'https://openalex.org/W2057007397', 'https://openalex.org/W2972937794', 'https://openalex.org/W4248805241', 'https://openalex.org/W792183615', 'https://openalex.org/W4297808394', 'https://openalex.org/W2520160253', 'https://openalex.org/W1974540032', 'https://openalex.org/W3036601975', 'https://openalex.org/W180242331', 'https://openalex.org/W2988736778', 'https://openalex.org/W3098643042', 'https://openalex.org/W3099142230', 'https://openalex.org/W2079623482', 'https://openalex.org/W2122364000', 'https://openalex.org/W3110761489', 'https://openalex.org/W4287173589', 'https://openalex.org/W3006094508', 'https://openalex.org/W2614103613', 'https://openalex.org/W4254816979', 'https://openalex.org/W3016181583', 'https://openalex.org/W4299579390', 'https://openalex.org/W2991213871', 'https://openalex.org/W3093427098', 'https://openalex.org/W2915722758', 'https://openalex.org/W2799046698', 'https://openalex.org/W1576278180', 'https://openalex.org/W2103810867', 'https://openalex.org/W2950577311', 'https://openalex.org/W2748009955', 'https://openalex.org/W1578200545', 'https://openalex.org/W1614298861', 'https://openalex.org/W2614542633', 'https://openalex.org/W2995181338', 'https://openalex.org/W30845872', 'https://openalex.org/W3216034039', 'https://openalex.org/W2963403868', 'https://openalex.org/W4303941982', 'https://openalex.org/W2913668833', 'https://openalex.org/W2963311389', 'https://openalex.org/W3152218910', 'https://openalex.org/W2798908575', 'https://openalex.org/W2949640717', 'https://openalex.org/W4288107125', 'https://openalex.org/W2952343510', 'https://openalex.org/W3158802984', 'https://openalex.org/W2899377381', 'https://openalex.org/W4238614906', 'https://openalex.org/W2963720603', 'https://openalex.org/W2020607164', 'https://openalex.org/W2926827382', 'https://openalex.org/W2151660570', 'https://openalex.org/W2912526802', 'https://openalex.org/W2802557066', 'https://openalex.org/W4285719527', 'https://openalex.org/W2126203737', 'https://openalex.org/W2037959956', 'https://openalex.org/W2601836666', 'https://openalex.org/W3150635893', 'https://openalex.org/W4385245566', 'https://openalex.org/W3099782249', 'https://openalex.org/W2950561535', 'https://openalex.org/W2964169922', 'https://openalex.org/W2963499843', 'https://openalex.org/W2114777034', 'https://openalex.org/W3204917342', 'https://openalex.org/W2079460648', 'https://openalex.org/W2167715705', 'https://openalex.org/W2889313720', 'https://openalex.org/W2963340922', 'https://openalex.org/W2003333103', 'https://openalex.org/W2963118869', 'https://openalex.org/W2137010615', 'https://openalex.org/W2973026522', 'https://openalex.org/W1545920196', 'https://openalex.org/W2169992508', 'https://openalex.org/W4300047444', 'https://openalex.org/W2964266061', 'https://openalex.org/W2059652594', 'https://openalex.org/W2117786207', 'https://openalex.org/W2963735467', 'https://openalex.org/W2004833594', 'https://openalex.org/W1579848672', 'https://openalex.org/W2964347276', 'https://openalex.org/W2025482506', 'https://openalex.org/W3198134274', 'https://openalex.org/W2787447541', 'https://openalex.org/W2964079874', 'https://openalex.org/W3165666670', 'https://openalex.org/W2927673779']",2022-03-09
https://openalex.org/W3033010920,https://doi.org/10.1016/j.neunet.2021.03.017,CiwGAN and fiwGAN: Encoding information in acoustic data to model lexical learning with Generative Adversarial Networks,"How can deep neural networks encode information that corresponds to words in human speech into raw acoustic data? This paper proposes two neural network architectures for modeling unsupervised lexical learning from raw acoustic inputs: ciwGAN (Categorical InfoWaveGAN) and fiwGAN (Featural InfoWaveGAN). These combine Deep Convolutional GAN architecture for audio data (WaveGAN; Donahue et al., 2019) with the information theoretic extension of GAN - InfoGAN (Chen et al., 2016) - and propose a new latent space structure that can model featural learning simultaneously with a higher level classification and allows for a very low-dimension vector representation of lexical items. In addition to the Generator and Discriminator networks, the architectures introduce a network that learns to retrieve latent codes from generated audio outputs. Lexical learning is thus modeled as emergent from an architecture that forces a deep neural network to output data such that unique information is retrievable from its acoustic outputs. The networks trained on lexical items from the TIMIT corpus learn to encode unique information corresponding to lexical items in the form of categorical variables in their latent space. By manipulating these variables, the network outputs specific lexical items. The network occasionally outputs innovative lexical items that violate training data, but are linguistically interpretable and highly informative for cognitive modeling and neural network interpretability. Innovative outputs suggest that phonetic and phonological representations learned by the network can be productively recombined and directly paralleled to productivity in human speech: a fiwGAN network trained on suit and dark outputs innovative start, even though it never saw start or even a [st] sequence in the training data. We also argue that setting latent featural codes to values well beyond training range results in almost categorical generation of prototypical lexical items and reveals underlying values of each latent code. Probing deep neural networks trained on well understood dependencies in speech bears implications for latent space interpretability and understanding how deep neural networks learn meaningful representations, as well as potential for unsupervised text-to-speech generation in the GAN framework.","['https://openalex.org/W6785782189', 'https://openalex.org/W6741832134', 'https://openalex.org/W2606425707', 'https://openalex.org/W2907162900', 'https://openalex.org/W6769196770', 'https://openalex.org/W6785455557', 'https://openalex.org/W3031894486', 'https://openalex.org/W2007251877', 'https://openalex.org/W6718140377', 'https://openalex.org/W6782496585', 'https://openalex.org/W3100270690', 'https://openalex.org/W6776959109', 'https://openalex.org/W6777659283', 'https://openalex.org/W6703366769', 'https://openalex.org/W2152134037', 'https://openalex.org/W6755257315', 'https://openalex.org/W6761568071', 'https://openalex.org/W6697293080', 'https://openalex.org/W6784039545', 'https://openalex.org/W6761557580', 'https://openalex.org/W6676028815', 'https://openalex.org/W6648638054', 'https://openalex.org/W2151794869', 'https://openalex.org/W2126377586', 'https://openalex.org/W6735913928', 'https://openalex.org/W2033413759', 'https://openalex.org/W6713421067', 'https://openalex.org/W6779984486', 'https://openalex.org/W6756098772', 'https://openalex.org/W2468716020', 'https://openalex.org/W6649703416', 'https://openalex.org/W2155721440', 'https://openalex.org/W6675022971', 'https://openalex.org/W1778492285', 'https://openalex.org/W6665204316', 'https://openalex.org/W2079207700', 'https://openalex.org/W3027324582', 'https://openalex.org/W2604683892', 'https://openalex.org/W2010188467', 'https://openalex.org/W3046603443', 'https://openalex.org/W2398490608', 'https://openalex.org/W2621649611', 'https://openalex.org/W1980862600', 'https://openalex.org/W6640902190', 'https://openalex.org/W6762378949', 'https://openalex.org/W2962879692', 'https://openalex.org/W2996556191', 'https://openalex.org/W2612690371', 'https://openalex.org/W52412328', 'https://openalex.org/W2100768664', 'https://openalex.org/W3100385063', 'https://openalex.org/W3209383001', 'https://openalex.org/W1516184288', 'https://openalex.org/W2173520492', 'https://openalex.org/W3095706145', 'https://openalex.org/W2789720136', 'https://openalex.org/W2122364000', 'https://openalex.org/W3093427098', 'https://openalex.org/W2963226019', 'https://openalex.org/W3095361818', 'https://openalex.org/W2623769798', 'https://openalex.org/W2107038463', 'https://openalex.org/W3097485645', 'https://openalex.org/W2401235434', 'https://openalex.org/W2945769669', 'https://openalex.org/W2888991776', 'https://openalex.org/W3105148948', 'https://openalex.org/W1513618424', 'https://openalex.org/W2963620343', 'https://openalex.org/W3097692357', 'https://openalex.org/W2559655401', 'https://openalex.org/W1583837637', 'https://openalex.org/W4320013936', 'https://openalex.org/W3098643042', 'https://openalex.org/W1700952868', 'https://openalex.org/W3106255532', 'https://openalex.org/W3022428196', 'https://openalex.org/W3093096176', 'https://openalex.org/W2151518054', 'https://openalex.org/W2468383091', 'https://openalex.org/W2963720603', 'https://openalex.org/W4300047444', 'https://openalex.org/W2099471712', 'https://openalex.org/W2059652594', 'https://openalex.org/W1997505733', 'https://openalex.org/W2963684088', 'https://openalex.org/W3125709657', 'https://openalex.org/W2979476256', 'https://openalex.org/W2739748921', 'https://openalex.org/W4255113413', 'https://openalex.org/W2894295011', 'https://openalex.org/W2973026522', 'https://openalex.org/W4295521014', 'https://openalex.org/W2972867623', 'https://openalex.org/W1993755070', 'https://openalex.org/W2963571336', 'https://openalex.org/W3085331098', 'https://openalex.org/W4297817572', 'https://openalex.org/W2582743722', 'https://openalex.org/W3097286738', 'https://openalex.org/W2940544976', 'https://openalex.org/W2996383576', 'https://openalex.org/W1949377791', 'https://openalex.org/W4376537811', 'https://openalex.org/W3212451506', 'https://openalex.org/W3127686677']",2021-03-19
https://openalex.org/W3148101939,https://doi.org/10.1109/slt48900.2021.9383461,Towards Unsupervised Learning of Speech Features in the Wild,"Recent work on unsupervised contrastive learning of speech representation has shown promising results, but so far has mostly been applied to clean, curated speech datasets. Can it also be used with unprepared audio data ""in the wild""? Here, we explore three potential problems in this setting: (i) presence of non-speech data, (ii) noisy or low quality speech data, and (iii) imbalance in speaker distribution. We show that on the Libri-light train set, which is itself a relatively clean speech-only dataset, these problems combined can already have a performance cost of up to 30% relative for the ABX score. We show that the first two problems can be alleviated by data filtering, with voice activity detection selecting speech segments, while perplexity of a model trained with clean data helping to discard entire files. We show that the third problem can be alleviated by learning a speaker embedding in the predictive branch of the model. We show that these techniques build more robust speech features that can be transferred to an ASR task in the low resource setting.","['https://openalex.org/W6743986254', 'https://openalex.org/W2127141656', 'https://openalex.org/W6769455919', 'https://openalex.org/W3015213852', 'https://openalex.org/W6770514103', 'https://openalex.org/W6714100551', 'https://openalex.org/W3015783745', 'https://openalex.org/W1494198834', 'https://openalex.org/W6739901393', 'https://openalex.org/W2842511635', 'https://openalex.org/W2983785920', 'https://openalex.org/W6780483730', 'https://openalex.org/W6761553608', 'https://openalex.org/W2111702745', 'https://openalex.org/W2124558353', 'https://openalex.org/W2147590749', 'https://openalex.org/W2430252546', 'https://openalex.org/W2079623482', 'https://openalex.org/W2889374926', 'https://openalex.org/W2963381607', 'https://openalex.org/W3015501067', 'https://openalex.org/W6712444837', 'https://openalex.org/W2973049979', 'https://openalex.org/W3093096176', 'https://openalex.org/W6697293080', 'https://openalex.org/W3016181583', 'https://openalex.org/W6771812881', 'https://openalex.org/W3003875258', 'https://openalex.org/W3102342027', 'https://openalex.org/W2346964103', 'https://openalex.org/W6844194202', 'https://openalex.org/W2940544976', 'https://openalex.org/W2796339975', 'https://openalex.org/W2133223948', 'https://openalex.org/W6750523955', 'https://openalex.org/W2787447541', 'https://openalex.org/W6780218876', 'https://openalex.org/W2950414763', 'https://openalex.org/W2055408826', 'https://openalex.org/W2962850179', 'https://openalex.org/W2973026522', 'https://openalex.org/W4289564011', 'https://openalex.org/W3036601975', 'https://openalex.org/W2995181338', 'https://openalex.org/W2988736778', 'https://openalex.org/W2795282075', 'https://openalex.org/W3093427098', 'https://openalex.org/W2407151108', 'https://openalex.org/W2963403868', 'https://openalex.org/W3144810982', 'https://openalex.org/W3099782249', 'https://openalex.org/W3016011332', 'https://openalex.org/W2963620343', 'https://openalex.org/W4385245566', 'https://openalex.org/W2990583358', 'https://openalex.org/W2593779438', 'https://openalex.org/W4297808394', 'https://openalex.org/W2963371670', 'https://openalex.org/W2753008876', 'https://openalex.org/W2786608204', 'https://openalex.org/W2395899413', 'https://openalex.org/W2930682606', 'https://openalex.org/W2953190524']",2021-01-19
https://openalex.org/W3212451506,https://doi.org/10.1162/tacl_a_00421,Identity-Based Patterns in Deep Convolutional Networks: Generative Adversarial Phonology and Reduplication,"This paper models unsupervised learning of an identity-based pattern (or copying) in speech called reduplication from raw continuous data with deep convolutional neural networks. We use the ciwGAN architecture Begu\v{s} (2021a; arXiv:2006.02951) in which learning of meaningful representations in speech emerges from a requirement that the CNNs generate informative data. We propose a technique to wug-test CNNs trained on speech and, based on four generative tests, argue that the network learns to represent an identity-based pattern in its latent space. By manipulating only two categorical variables in the latent space, we can actively turn an unreduplicated form into a reduplicated form with no other substantial changes to the output in the majority of cases. We also argue that the network extends the identity-based pattern to unobserved data. Exploration of how meaningful representations of identity-based patterns emerge in CNNs and how the latent space variables outside of the training range correlate with identity-based patterns in the output has general implications for neural network interpretability.","['https://openalex.org/W3125709657', 'https://openalex.org/W1778492285', 'https://openalex.org/W2011183451', 'https://openalex.org/W2982339091', 'https://openalex.org/W2918871811', 'https://openalex.org/W2963684088', 'https://openalex.org/W2941488222', 'https://openalex.org/W1531940724', 'https://openalex.org/W3207342693', 'https://openalex.org/W2979476256', 'https://openalex.org/W4230884310', 'https://openalex.org/W4302909688', 'https://openalex.org/W2902392284', 'https://openalex.org/W2990118161', 'https://openalex.org/W3093177323', 'https://openalex.org/W1994044313', 'https://openalex.org/W4288087796', 'https://openalex.org/W2902243809', 'https://openalex.org/W2111972908', 'https://openalex.org/W2996383576', 'https://openalex.org/W1756873532', 'https://openalex.org/W2118373646', 'https://openalex.org/W3095361818', 'https://openalex.org/W2972867623', 'https://openalex.org/W1979539191', 'https://openalex.org/W2006956639', 'https://openalex.org/W3093096176', 'https://openalex.org/W2559655401', 'https://openalex.org/W2995064471', 'https://openalex.org/W2963620343', 'https://openalex.org/W2913668833', 'https://openalex.org/W3033010920', 'https://openalex.org/W2973026522', 'https://openalex.org/W3106255532', 'https://openalex.org/W1997505733', 'https://openalex.org/W2731893058', 'https://openalex.org/W2739748921', 'https://openalex.org/W2940544976', 'https://openalex.org/W2140275137', 'https://openalex.org/W3162850270', 'https://openalex.org/W3093427098', 'https://openalex.org/W3021208382', 'https://openalex.org/W2802739856', 'https://openalex.org/W2229267541', 'https://openalex.org/W2963226019', 'https://openalex.org/W4320013936', 'https://openalex.org/W4300402905', 'https://openalex.org/W2621649611', 'https://openalex.org/W2963571336', 'https://openalex.org/W3209383001', 'https://openalex.org/W2157076315', 'https://openalex.org/W4297817572', 'https://openalex.org/W2945769669', 'https://openalex.org/W2279632331', 'https://openalex.org/W972901562', 'https://openalex.org/W1579605689', 'https://openalex.org/W2956021784', 'https://openalex.org/W3123655773', 'https://openalex.org/W3100270690', 'https://openalex.org/W1699946128', 'https://openalex.org/W2894295011']",2020-09-13
https://openalex.org/W3156299562,https://doi.org/10.18653/v1/2022.sigmorphon-1.5,A Masked Segmental Language Model for Unsupervised Natural Language Segmentation,"We introduce a Masked Segmental Language Model (MSLM) for joint language modeling and unsupervised segmentation. While near-perfect supervised methods have been developed for segmenting human-like linguistic units in resource-rich languages such as Chinese, many of the world's languages are both morphologically complex, and have no large dataset of ""gold"" segmentations for supervised training. Segmental Language Models offer a unique approach by conducting unsupervised segmentation as the byproduct of a neural language modeling objective. However, current SLMs are limited in their scalability due to their recurrent architecture. We propose a new type of SLM for use in both unsupervised and lightly supervised segmentation tasks. The MSLM is built on a span-masking transformer architecture, harnessing a masked bidirectional modeling context and attention, as well as adding the potential for model scalability. In a series of experiments, our model outperforms the segmentation quality of recurrent SLMs on Chinese, and performs similarly to the recurrent model on English.","['https://openalex.org/W3035229572', 'https://openalex.org/W179875071', 'https://openalex.org/W2096204319', 'https://openalex.org/W2964121744', 'https://openalex.org/W2126377586', 'https://openalex.org/W1522301498', 'https://openalex.org/W2962875366', 'https://openalex.org/W2117621558', 'https://openalex.org/W2963077280', 'https://openalex.org/W2127141656', 'https://openalex.org/W2295297373', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963341956', 'https://openalex.org/W2609370997', 'https://openalex.org/W1533169541', 'https://openalex.org/W3158607076', 'https://openalex.org/W2008225289', 'https://openalex.org/W2101711363', 'https://openalex.org/W2140991203', 'https://openalex.org/W2287914047', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963357986', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963403868', 'https://openalex.org/W3144818841', 'https://openalex.org/W2039133703', 'https://openalex.org/W2074546930', 'https://openalex.org/W2891546148', 'https://openalex.org/W2952343510', 'https://openalex.org/W2270070752', 'https://openalex.org/W2131774270', 'https://openalex.org/W4304111904', 'https://openalex.org/W2950577311', 'https://openalex.org/W3133631758', 'https://openalex.org/W2010906431', 'https://openalex.org/W2982399380', 'https://openalex.org/W2592647456', 'https://openalex.org/W3034999214', 'https://openalex.org/W1614298861', 'https://openalex.org/W3035032094', 'https://openalex.org/W2962739339', 'https://openalex.org/W3093427098', 'https://openalex.org/W2105738468', 'https://openalex.org/W2963899393', 'https://openalex.org/W2952468927', 'https://openalex.org/W2964045208', 'https://openalex.org/W2158266063', 'https://openalex.org/W2963735467', 'https://openalex.org/W3035193825', 'https://openalex.org/W4226246098', 'https://openalex.org/W2952125979', 'https://openalex.org/W25062297', 'https://openalex.org/W2130042265', 'https://openalex.org/W3097485645', 'https://openalex.org/W2079735306', 'https://openalex.org/W2110485445', 'https://openalex.org/W2944815030', 'https://openalex.org/W2251563156', 'https://openalex.org/W4254816979']",2022-01-01
https://openalex.org/W4287889722,https://doi.org/10.18653/v1/2022.naacl-tutorials.2,Self-supervised Representation Learning for Speech Processing,"Hung-yi Lee, Abdelrahman Mohamed, Shinji Watanabe, Tara Sainath, Karen Livescu, Shang-Wen Li, Shu-wen Yang, Katrin Kirchhoff. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorial Abstracts. 2022.","['https://openalex.org/W2979476256', 'https://openalex.org/W2899134946', 'https://openalex.org/W6813608150', 'https://openalex.org/W3139395121', 'https://openalex.org/W4226380987', 'https://openalex.org/W2116422968', 'https://openalex.org/W3002741552', 'https://openalex.org/W3093427098', 'https://openalex.org/W2171019095', 'https://openalex.org/W2064082346', 'https://openalex.org/W3088409176', 'https://openalex.org/W2401464865', 'https://openalex.org/W4297808394', 'https://openalex.org/W2934852845', 'https://openalex.org/W2100768664', 'https://openalex.org/W2079460648', 'https://openalex.org/W3096017728', 'https://openalex.org/W3041561163', 'https://openalex.org/W2067474491', 'https://openalex.org/W4297801719', 'https://openalex.org/W2153519094', 'https://openalex.org/W3118485687', 'https://openalex.org/W2962799225', 'https://openalex.org/W3198858531', 'https://openalex.org/W3160345865', 'https://openalex.org/W2982223350', 'https://openalex.org/W2804648901', 'https://openalex.org/W2973157397', 'https://openalex.org/W3140429000', 'https://openalex.org/W3197580070', 'https://openalex.org/W2972943112', 'https://openalex.org/W3097286738', 'https://openalex.org/W3197411683', 'https://openalex.org/W3189296823', 'https://openalex.org/W2973049979', 'https://openalex.org/W3015265920', 'https://openalex.org/W3196919915', 'https://openalex.org/W3036601975', 'https://openalex.org/W3035202887', 'https://openalex.org/W3016011332', 'https://openalex.org/W3169320628', 'https://openalex.org/W3148040514', 'https://openalex.org/W2981991061', 'https://openalex.org/W3207924272', 'https://openalex.org/W3096485810', 'https://openalex.org/W3112034174', 'https://openalex.org/W2964079874']",2022-01-01
https://openalex.org/W4385959364,https://doi.org/10.1145/3581783.3611810,ZRIGF: An Innovative Multimodal Framework for Zero-Resource Image-Grounded Dialogue Generation,"Image-grounded dialogue systems benefit greatly from integrating visual information, resulting in high-quality response generation. However, current models struggle to effectively utilize such information in zero-resource scenarios, mainly due to the disparity between image and text modalities. To overcome this challenge, we propose an innovative multimodal framework, called ZRIGF, which assimilates image-grounded information for dialogue generation in zero-resource situations. ZRIGF implements a two-stage learning strategy, comprising contrastive pre-training and generative pre-training. Contrastive pre-training includes a text-image matching module that maps images and texts into a unified encoded vector space, along with a text-assisted masked image modeling module that preserves pre-training visual features and fosters further multimodal feature alignment. Generative pre-training employs a multimodal fusion module and an information transfer module to produce insightful responses based on harmonized multimodal representations. Comprehensive experiments conducted on both text-based and image-grounded dialogue datasets demonstrate ZRIGF's efficacy in generating contextually pertinent and informative responses. Furthermore, we adopt a fully zero-resource scenario in the image-grounded dialogue dataset to demonstrate our framework's robust generalization capabilities in novel domains. The code is available at https://github.com/zhangbo-nlp/ZRIGF.","['https://openalex.org/W3093096176', 'https://openalex.org/W2972916088', 'https://openalex.org/W2964007535', 'https://openalex.org/W2795571593', 'https://openalex.org/W4288083516', 'https://openalex.org/W3034999214', 'https://openalex.org/W2963206148', 'https://openalex.org/W3007008027', 'https://openalex.org/W3173864402', 'https://openalex.org/W3152929911', 'https://openalex.org/W2963903950', 'https://openalex.org/W3152943448', 'https://openalex.org/W3206527789', 'https://openalex.org/W2476548250', 'https://openalex.org/W3035448310', 'https://openalex.org/W2979826702', 'https://openalex.org/W3015063797', 'https://openalex.org/W4220775189', 'https://openalex.org/W4223938549', 'https://openalex.org/W2998563994', 'https://openalex.org/W4214540501', 'https://openalex.org/W2945260553', 'https://openalex.org/W4385245566', 'https://openalex.org/W1583837637', 'https://openalex.org/W2154652894', 'https://openalex.org/W2583186419', 'https://openalex.org/W3199193983', 'https://openalex.org/W2963331137', 'https://openalex.org/W4250928698', 'https://openalex.org/W2988937804', 'https://openalex.org/W2999905431', 'https://openalex.org/W3197259906', 'https://openalex.org/W4304080179', 'https://openalex.org/W3093427098', 'https://openalex.org/W2908510526', 'https://openalex.org/W3094502228', 'https://openalex.org/W3166396011', 'https://openalex.org/W1593271688', 'https://openalex.org/W4303492595', 'https://openalex.org/W2783549597', 'https://openalex.org/W2133012565', 'https://openalex.org/W1861492603', 'https://openalex.org/W3121480429', 'https://openalex.org/W2775774141', 'https://openalex.org/W2612690371', 'https://openalex.org/W2130942839']",2023-10-26
https://openalex.org/W4287888298,https://doi.org/10.18653/v1/2022.naacl-tutorials,Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorial Abstracts,"Text-editing models have recently become a prominent alternative to seq2seq models for monolingual text-generation tasks such as grammatical error correction, simplification, and style transfer.These tasks share a common trait -they exhibit a large amount of textual overlap between the source and target texts.Text-editing models take advantage of this observation and learn to generate the output by predicting edit operations applied to the source sequence.In contrast, seq2seq models generate outputs word-by-word from scratch thus making them slow at inference time.Text-editing models provide several benefits over seq2seq models including faster inference speed, higher sample efficiency, and better control and interpretability of the outputs.This tutorial 1 provides a comprehensive overview of text-editing models and current state-of-the-art approaches, and analyzes their pros and cons.We discuss challenges related to productionization and how these models can be used to mitigate hallucination and bias, both pressing challenges in the field of text generation.","['https://openalex.org/W2979476256', 'https://openalex.org/W2899134946', 'https://openalex.org/W6813608150', 'https://openalex.org/W3139395121', 'https://openalex.org/W2891012317', 'https://openalex.org/W6863994431', 'https://openalex.org/W2747318853', 'https://openalex.org/W2802736684', 'https://openalex.org/W2423576022', 'https://openalex.org/W3034910302', 'https://openalex.org/W3173449346', 'https://openalex.org/W3173566921', 'https://openalex.org/W4286963014', 'https://openalex.org/W3099206234', 'https://openalex.org/W3101066076', 'https://openalex.org/W2982756474', 'https://openalex.org/W3200655919', 'https://openalex.org/W2952289666', 'https://openalex.org/W3212888739', 'https://openalex.org/W3166396011', 'https://openalex.org/W2742113707', 'https://openalex.org/W2994821360', 'https://openalex.org/W2949922292', 'https://openalex.org/W4287812705', 'https://openalex.org/W3016099278', 'https://openalex.org/W3166523559', 'https://openalex.org/W3102035862', 'https://openalex.org/W3168204696', 'https://openalex.org/W2963349562', 'https://openalex.org/W3154813414', 'https://openalex.org/W3173813266', 'https://openalex.org/W3122838366', 'https://openalex.org/W4312407537', 'https://openalex.org/W3090732401', 'https://openalex.org/W4288333712', 'https://openalex.org/W2934852845', 'https://openalex.org/W3129576130', 'https://openalex.org/W3174244822', 'https://openalex.org/W3203903618', 'https://openalex.org/W3152723424', 'https://openalex.org/W3099742594', 'https://openalex.org/W4288244092', 'https://openalex.org/W2804648901', 'https://openalex.org/W3169993339', 'https://openalex.org/W3098096010', 'https://openalex.org/W4206648492', 'https://openalex.org/W4205157616', 'https://openalex.org/W4231827019', 'https://openalex.org/W3036601975', 'https://openalex.org/W3176967746', 'https://openalex.org/W3199113016', 'https://openalex.org/W2890515900', 'https://openalex.org/W154472438', 'https://openalex.org/W3035202887', 'https://openalex.org/W4225716497', 'https://openalex.org/W2107961136', 'https://openalex.org/W3116921291', 'https://openalex.org/W3100955355', 'https://openalex.org/W4223956331', 'https://openalex.org/W3034773362', 'https://openalex.org/W4287235406', 'https://openalex.org/W3048911294', 'https://openalex.org/W2948445958', 'https://openalex.org/W4280518705', 'https://openalex.org/W4206294441', 'https://openalex.org/W2742426565', 'https://openalex.org/W3175362188', 'https://openalex.org/W3197766089', 'https://openalex.org/W4221158634', 'https://openalex.org/W2953388933', 'https://openalex.org/W4205899467', 'https://openalex.org/W3130196849', 'https://openalex.org/W4249573750', 'https://openalex.org/W3177281527', 'https://openalex.org/W3173644772', 'https://openalex.org/W3173658182', 'https://openalex.org/W3197907569', 'https://openalex.org/W3100124407', 'https://openalex.org/W3160345865', 'https://openalex.org/W2163922914', 'https://openalex.org/W2973157397', 'https://openalex.org/W3172399575', 'https://openalex.org/W4280616698', 'https://openalex.org/W3101792976', 'https://openalex.org/W2786464815', 'https://openalex.org/W3041561163', 'https://openalex.org/W2808359495', 'https://openalex.org/W4255340738', 'https://openalex.org/W3034408878', 'https://openalex.org/W3089659770', 'https://openalex.org/W2964079874', 'https://openalex.org/W3167665045', 'https://openalex.org/W3100949878', 'https://openalex.org/W4287891052', 'https://openalex.org/W3176910517', 'https://openalex.org/W2742097923', 'https://openalex.org/W2945236904', 'https://openalex.org/W3207924272', 'https://openalex.org/W3094613840', 'https://openalex.org/W3096485810', 'https://openalex.org/W3106484161', 'https://openalex.org/W2562979205', 'https://openalex.org/W3177048142', 'https://openalex.org/W4294871265', 'https://openalex.org/W3034383590', 'https://openalex.org/W2110078189', 'https://openalex.org/W2970170773', 'https://openalex.org/W3110388292', 'https://openalex.org/W2516809705', 'https://openalex.org/W4297808394', 'https://openalex.org/W4287614078', 'https://openalex.org/W3003420231', 'https://openalex.org/W3104597568', 'https://openalex.org/W3034184697', 'https://openalex.org/W3200626945', 'https://openalex.org/W2981991061', 'https://openalex.org/W3035140194', 'https://openalex.org/W3106061119', 'https://openalex.org/W2100768664', 'https://openalex.org/W3212809714', 'https://openalex.org/W2584561145', 'https://openalex.org/W3200253633', 'https://openalex.org/W3177308635', 'https://openalex.org/W2153332911', 'https://openalex.org/W4301372783', 'https://openalex.org/W2973049979', 'https://openalex.org/W3199079601', 'https://openalex.org/W3112034174', 'https://openalex.org/W3035532688', 'https://openalex.org/W3098495697', 'https://openalex.org/W3159630167', 'https://openalex.org/W2962799225', 'https://openalex.org/W3026092005', 'https://openalex.org/W3200787399', 'https://openalex.org/W3096017728', 'https://openalex.org/W3103542727', 'https://openalex.org/W2920707356', 'https://openalex.org/W3103751997', 'https://openalex.org/W3046208551', 'https://openalex.org/W3102363610', 'https://openalex.org/W4385572440', 'https://openalex.org/W2952140516', 'https://openalex.org/W4226139013', 'https://openalex.org/W3148040514', 'https://openalex.org/W3035487250', 'https://openalex.org/W2963080533', 'https://openalex.org/W3200768867', 'https://openalex.org/W3166972201', 'https://openalex.org/W3034318565', 'https://openalex.org/W4205371973', 'https://openalex.org/W2079460648', 'https://openalex.org/W1512387364', 'https://openalex.org/W2016196732', 'https://openalex.org/W4294875919', 'https://openalex.org/W2963247627', 'https://openalex.org/W2952826391', 'https://openalex.org/W3173783447', 'https://openalex.org/W3104847483', 'https://openalex.org/W2966789588', 'https://openalex.org/W4297801719', 'https://openalex.org/W3115295967', 'https://openalex.org/W3126337491', 'https://openalex.org/W4247180064', 'https://openalex.org/W2950892871', 'https://openalex.org/W2970161131', 'https://openalex.org/W2601450892', 'https://openalex.org/W3103819114', 'https://openalex.org/W2970641574', 'https://openalex.org/W2734693922', 'https://openalex.org/W2774217864', 'https://openalex.org/W3197580070', 'https://openalex.org/W3176032431', 'https://openalex.org/W2116422968', 'https://openalex.org/W2951936329', 'https://openalex.org/W2513866468', 'https://openalex.org/W3016011332', 'https://openalex.org/W3171153522', 'https://openalex.org/W3015265920', 'https://openalex.org/W3170759063', 'https://openalex.org/W3213720774', 'https://openalex.org/W3176108833', 'https://openalex.org/W4285114721', 'https://openalex.org/W4230418757', 'https://openalex.org/W4226380987', 'https://openalex.org/W3102925419', 'https://openalex.org/W2946531303', 'https://openalex.org/W1552847225', 'https://openalex.org/W2067474491', 'https://openalex.org/W205159212', 'https://openalex.org/W3131870090', 'https://openalex.org/W3185250692', 'https://openalex.org/W3173586048', 'https://openalex.org/W2594475271', 'https://openalex.org/W3106178264', 'https://openalex.org/W3140429000', 'https://openalex.org/W2950589543', 'https://openalex.org/W3160037564', 'https://openalex.org/W3202125623', 'https://openalex.org/W2970854433', 'https://openalex.org/W3118668786', 'https://openalex.org/W2971039193', 'https://openalex.org/W2159426623', 'https://openalex.org/W3174864715', 'https://openalex.org/W3035690155', 'https://openalex.org/W2951221758', 'https://openalex.org/W3181951703', 'https://openalex.org/W3106234277', 'https://openalex.org/W3197032408', 'https://openalex.org/W2627159663', 'https://openalex.org/W3106177076', 'https://openalex.org/W2157364932', 'https://openalex.org/W3036595416', 'https://openalex.org/W3098415518', 'https://openalex.org/W2962756504', 'https://openalex.org/W2619383789', 'https://openalex.org/W3199962080', 'https://openalex.org/W2964333805', 'https://openalex.org/W3173190788', 'https://openalex.org/W3035547806', 'https://openalex.org/W3175621180', 'https://openalex.org/W3197411683', 'https://openalex.org/W2970429618', 'https://openalex.org/W2926827382', 'https://openalex.org/W2891117443', 'https://openalex.org/W3034797437', 'https://openalex.org/W4212865381', 'https://openalex.org/W3035458998', 'https://openalex.org/W3205305305', 'https://openalex.org/W3209573585', 'https://openalex.org/W2758815496', 'https://openalex.org/W4242314266', 'https://openalex.org/W1614298861', 'https://openalex.org/W4287547182', 'https://openalex.org/W2949861626', 'https://openalex.org/W3023293091', 'https://openalex.org/W2171019095', 'https://openalex.org/W2463955103', 'https://openalex.org/W3156106752', 'https://openalex.org/W2618799552', 'https://openalex.org/W3199926081', 'https://openalex.org/W4286899123', 'https://openalex.org/W2799007037', 'https://openalex.org/W2951883849', 'https://openalex.org/W2555897561', 'https://openalex.org/W3198858531', 'https://openalex.org/W3105928338', 'https://openalex.org/W2963420481', 'https://openalex.org/W3174113198', 'https://openalex.org/W3176001432', 'https://openalex.org/W3102173443', 'https://openalex.org/W3118485687', 'https://openalex.org/W4223959328', 'https://openalex.org/W4205567737', 'https://openalex.org/W3184369217', 'https://openalex.org/W3171908007', 'https://openalex.org/W2799263800', 'https://openalex.org/W3128699722', 'https://openalex.org/W3034985160', 'https://openalex.org/W2626967530', 'https://openalex.org/W2962816513', 'https://openalex.org/W2963360413', 'https://openalex.org/W3200751675', 'https://openalex.org/W3156636935', 'https://openalex.org/W4287077228', 'https://openalex.org/W3189296823', 'https://openalex.org/W2982223350', 'https://openalex.org/W2986407524', 'https://openalex.org/W3131157458', 'https://openalex.org/W3102950138', 'https://openalex.org/W3046251064', 'https://openalex.org/W2604610161', 'https://openalex.org/W3037763555', 'https://openalex.org/W2971246534', 'https://openalex.org/W4287639392', 'https://openalex.org/W2514249165', 'https://openalex.org/W3099303748', 'https://openalex.org/W2963961878', 'https://openalex.org/W3173210704', 'https://openalex.org/W2896457183', 'https://openalex.org/W3174540647', 'https://openalex.org/W3089244638', 'https://openalex.org/W3011574394', 'https://openalex.org/W3196919915', 'https://openalex.org/W3035229828', 'https://openalex.org/W3184094520', 'https://openalex.org/W3199661826', 'https://openalex.org/W3200681609', 'https://openalex.org/W3005680577', 'https://openalex.org/W3035027743', 'https://openalex.org/W3093427098', 'https://openalex.org/W3165523615', 'https://openalex.org/W3170180819', 'https://openalex.org/W4206334925', 'https://openalex.org/W3099700870', 'https://openalex.org/W2306686872', 'https://openalex.org/W3168363436', 'https://openalex.org/W2401464865', 'https://openalex.org/W3203711169', 'https://openalex.org/W2982519139', 'https://openalex.org/W3173169192', 'https://openalex.org/W3174870841', 'https://openalex.org/W2804897457', 'https://openalex.org/W2963097937', 'https://openalex.org/W4244019523', 'https://openalex.org/W2963026686', 'https://openalex.org/W2966864197', 'https://openalex.org/W2798865369', 'https://openalex.org/W4229031767', 'https://openalex.org/W2064082346', 'https://openalex.org/W2971145411', 'https://openalex.org/W3098054859', 'https://openalex.org/W1523385540', 'https://openalex.org/W3169320628', 'https://openalex.org/W4287828713', 'https://openalex.org/W2962881743', 'https://openalex.org/W3199893015', 'https://openalex.org/W3177494585', 'https://openalex.org/W2962862931', 'https://openalex.org/W4246635576', 'https://openalex.org/W2563399268', 'https://openalex.org/W3097286738', 'https://openalex.org/W2953102540', 'https://openalex.org/W3002741552', 'https://openalex.org/W4206640861', 'https://openalex.org/W3103035585', 'https://openalex.org/W3035503910', 'https://openalex.org/W3176047188', 'https://openalex.org/W3096682293', 'https://openalex.org/W2153519094']",2022-01-01
https://openalex.org/W4226199158,https://doi.org/10.48550/arxiv.2203.01829,A Brief Overview of Unsupervised Neural Speech Representation Learning,"Unsupervised representation learning for speech processing has matured greatly in the last few years. Work in computer vision and natural language processing has paved the way, but speech data offers unique challenges. As a result, methods from other domains rarely translate directly. We review the development of unsupervised representation learning for speech over the last decade. We identify two primary model categories: self-supervised methods and probabilistic latent variable models. We describe the models and develop a comprehensive taxonomy. Finally, we discuss and compare models from the two categories.","['https://openalex.org/W3016181583', 'https://openalex.org/W2107789863', 'https://openalex.org/W2963137467', 'https://openalex.org/W2963925452', 'https://openalex.org/W1686810756', 'https://openalex.org/W3197259906', 'https://openalex.org/W3146777637', 'https://openalex.org/W4237840503', 'https://openalex.org/W2988736778', 'https://openalex.org/W3198782837', 'https://openalex.org/W3093427098', 'https://openalex.org/W4297808394', 'https://openalex.org/W3041561163', 'https://openalex.org/W2786608204', 'https://openalex.org/W2122538988', 'https://openalex.org/W1945356021', 'https://openalex.org/W2587284713', 'https://openalex.org/W2750248772', 'https://openalex.org/W2981991061', 'https://openalex.org/W3134881075', 'https://openalex.org/W3097777922', 'https://openalex.org/W3035202887', 'https://openalex.org/W2937090315', 'https://openalex.org/W44815768', 'https://openalex.org/W343636949', 'https://openalex.org/W2746710273', 'https://openalex.org/W2020607164', 'https://openalex.org/W3033038061', 'https://openalex.org/W2888911345', 'https://openalex.org/W3160345865', 'https://openalex.org/W2395899413', 'https://openalex.org/W3204915839', 'https://openalex.org/W4287374065', 'https://openalex.org/W2767754137', 'https://openalex.org/W1959608418', 'https://openalex.org/W2979476256', 'https://openalex.org/W2963420272', 'https://openalex.org/W3209993061', 'https://openalex.org/W2896457183', 'https://openalex.org/W2973157397', 'https://openalex.org/W2396566817', 'https://openalex.org/W3209984917', 'https://openalex.org/W179875071', 'https://openalex.org/W2972943112', 'https://openalex.org/W3015265920', 'https://openalex.org/W2788991015', 'https://openalex.org/W2963317665', 'https://openalex.org/W3016011332', 'https://openalex.org/W2519091744', 'https://openalex.org/W2242818861', 'https://openalex.org/W3036622477', 'https://openalex.org/W3096656254', 'https://openalex.org/W1967924372', 'https://openalex.org/W138345131', 'https://openalex.org/W3112034174', 'https://openalex.org/W2889313720', 'https://openalex.org/W2108598243', 'https://openalex.org/W3198858531', 'https://openalex.org/W4286849919', 'https://openalex.org/W3162875390', 'https://openalex.org/W2059652594', 'https://openalex.org/W4226103796', 'https://openalex.org/W4214784181', 'https://openalex.org/W1545920196', 'https://openalex.org/W1909320841', 'https://openalex.org/W2018168021', 'https://openalex.org/W1796128977', 'https://openalex.org/W2146444479', 'https://openalex.org/W3102342027', 'https://openalex.org/W2548228487', 'https://openalex.org/W3160303932', 'https://openalex.org/W3160235762', 'https://openalex.org/W4287173589', 'https://openalex.org/W4288310870', 'https://openalex.org/W4295177495', 'https://openalex.org/W2347098582', 'https://openalex.org/W3034749675', 'https://openalex.org/W2787447541', 'https://openalex.org/W2327501763', 'https://openalex.org/W3125709657', 'https://openalex.org/W3161223924', 'https://openalex.org/W2758785877', 'https://openalex.org/W2973049979', 'https://openalex.org/W2962799131', 'https://openalex.org/W4294170691', 'https://openalex.org/W2767224889', 'https://openalex.org/W3015949486', 'https://openalex.org/W3008499099', 'https://openalex.org/W2136922672', 'https://openalex.org/W2963720603', 'https://openalex.org/W3148040514', 'https://openalex.org/W2097012520', 'https://openalex.org/W2950151997', 'https://openalex.org/W3198134274', 'https://openalex.org/W3202710890', 'https://openalex.org/W3097286738', 'https://openalex.org/W4287887773', 'https://openalex.org/W3148001440', 'https://openalex.org/W4300047444', 'https://openalex.org/W2593011301', 'https://openalex.org/W4385245566', 'https://openalex.org/W592244745', 'https://openalex.org/W2963571336', 'https://openalex.org/W2076878942', 'https://openalex.org/W2936774411', 'https://openalex.org/W2097117768', 'https://openalex.org/W2963799213', 'https://openalex.org/W4297786395', 'https://openalex.org/W4287824654', 'https://openalex.org/W3198429080', 'https://openalex.org/W4288574863', 'https://openalex.org/W2100768664', 'https://openalex.org/W2163922914', 'https://openalex.org/W3197580070', 'https://openalex.org/W2152790380', 'https://openalex.org/W2168013545', 'https://openalex.org/W2406349064', 'https://openalex.org/W3015213852', 'https://openalex.org/W2194775991', 'https://openalex.org/W3209059054', 'https://openalex.org/W3036601975', 'https://openalex.org/W2962850167', 'https://openalex.org/W3095361818', 'https://openalex.org/W2963223306', 'https://openalex.org/W2982223350', 'https://openalex.org/W3203098807']",2022-03-01
https://openalex.org/W4287888896,https://doi.org/10.18653/v1/2022.sigmorphon-1,"Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology","Speech consists of a continuously-varying acoustic signal.Yet human listeners experience it as sequences of discrete speech sounds, which are used to recognise words.To examine how the human brain appropriately sequences the speech signal, we recorded two-hour magnetoencephalograms from 21 subjects listening to short narratives.Our analyses show that the brain continuously encodes the three most recently heard speech sounds in parallel, and maintains this information long past the sensory input.Each speech sound has a representation that evolves over time, jointly encoding both its phonetic features and time elapsed since onset.This allows the brain to represent the relative order and phonetic content of the phonetic sequence.These dynamic representations are active earlier when phonemes are more predictable, and are sustained longer when lexical identity is uncertain.The flexibility in the dynamics of these representations paves the way for further understanding of how such sequences may be used to interface with higher order structure such as morphemes and words.","['https://openalex.org/W4399611165', 'https://openalex.org/W4399438375', 'https://openalex.org/W2933138175', 'https://openalex.org/W2550821151', 'https://openalex.org/W3037789307', 'https://openalex.org/W2963979492', 'https://openalex.org/W3031412205', 'https://openalex.org/W2794998650', 'https://openalex.org/W2500003452', 'https://openalex.org/W190938664', 'https://openalex.org/W2114169118', 'https://openalex.org/W279210063', 'https://openalex.org/W3029286378', 'https://openalex.org/W2964121744', 'https://openalex.org/W3038117562', 'https://openalex.org/W2757376562', 'https://openalex.org/W3153847883', 'https://openalex.org/W3030075662', 'https://openalex.org/W2577255746', 'https://openalex.org/W1659651740', 'https://openalex.org/W4287854390', 'https://openalex.org/W4211010100', 'https://openalex.org/W2114003811', 'https://openalex.org/W54741546', 'https://openalex.org/W2008225289', 'https://openalex.org/W3088689279', 'https://openalex.org/W414732136', 'https://openalex.org/W3030437843', 'https://openalex.org/W3120863725', 'https://openalex.org/W2333916262', 'https://openalex.org/W2250263931', 'https://openalex.org/W4251482102', 'https://openalex.org/W2053520409', 'https://openalex.org/W199295433', 'https://openalex.org/W1552182683', 'https://openalex.org/W2800978325', 'https://openalex.org/W2034449408', 'https://openalex.org/W4287854785', 'https://openalex.org/W153553676', 'https://openalex.org/W4287887352', 'https://openalex.org/W2957991775', 'https://openalex.org/W4287887629', 'https://openalex.org/W3101509145', 'https://openalex.org/W3102657423', 'https://openalex.org/W2131774270', 'https://openalex.org/W2147272182', 'https://openalex.org/W3134073259', 'https://openalex.org/W2096204319', 'https://openalex.org/W2251652123', 'https://openalex.org/W4200055607', 'https://openalex.org/W2144862731', 'https://openalex.org/W2142795932', 'https://openalex.org/W3153224075', 'https://openalex.org/W4304111904', 'https://openalex.org/W2031750697', 'https://openalex.org/W4235239597', 'https://openalex.org/W2963216553', 'https://openalex.org/W3121334915', 'https://openalex.org/W2739853973', 'https://openalex.org/W2905332681', 'https://openalex.org/W4379540345', 'https://openalex.org/W2973029704', 'https://openalex.org/W191153079', 'https://openalex.org/W3034789084', 'https://openalex.org/W4213367174', 'https://openalex.org/W3177376490', 'https://openalex.org/W2186780112', 'https://openalex.org/W2073610029', 'https://openalex.org/W3204828734', 'https://openalex.org/W4226242454', 'https://openalex.org/W2084960574', 'https://openalex.org/W2559274376', 'https://openalex.org/W2936297157', 'https://openalex.org/W3197118322', 'https://openalex.org/W2593764120', 'https://openalex.org/W3185008966', 'https://openalex.org/W2781461271', 'https://openalex.org/W2158910967', 'https://openalex.org/W3202070718', 'https://openalex.org/W3037095707', 'https://openalex.org/W2266843417', 'https://openalex.org/W1522301498', 'https://openalex.org/W2557023021', 'https://openalex.org/W4287889967', 'https://openalex.org/W3158607076', 'https://openalex.org/W1521007682', 'https://openalex.org/W2140991203', 'https://openalex.org/W4205498194', 'https://openalex.org/W2767127552', 'https://openalex.org/W2937197076', 'https://openalex.org/W3097485645', 'https://openalex.org/W2615499310', 'https://openalex.org/W4285115758', 'https://openalex.org/W1951579782', 'https://openalex.org/W4288351520', 'https://openalex.org/W2292506390', 'https://openalex.org/W3172048174', 'https://openalex.org/W2952343510', 'https://openalex.org/W2970692082', 'https://openalex.org/W4286905107', 'https://openalex.org/W3028132812', 'https://openalex.org/W633453145', 'https://openalex.org/W1599922719', 'https://openalex.org/W4254816979', 'https://openalex.org/W2989457779', 'https://openalex.org/W4289754003', 'https://openalex.org/W2086426947', 'https://openalex.org/W2964343359', 'https://openalex.org/W2510684415', 'https://openalex.org/W2111126098', 'https://openalex.org/W2983032163', 'https://openalex.org/W3035193825', 'https://openalex.org/W179875071', 'https://openalex.org/W201532657', 'https://openalex.org/W4255743265', 'https://openalex.org/W2571532437', 'https://openalex.org/W2886452452', 'https://openalex.org/W2252054421', 'https://openalex.org/W3117518355', 'https://openalex.org/W2970597249', 'https://openalex.org/W3188334090', 'https://openalex.org/W4285208773', 'https://openalex.org/W2949992400', 'https://openalex.org/W4249773576', 'https://openalex.org/W2582743722', 'https://openalex.org/W1522263329', 'https://openalex.org/W2053306448', 'https://openalex.org/W4287887845', 'https://openalex.org/W4287855085', 'https://openalex.org/W3037474895', 'https://openalex.org/W4238596226', 'https://openalex.org/W3154420890', 'https://openalex.org/W630532510', 'https://openalex.org/W3034731894', 'https://openalex.org/W1593799327', 'https://openalex.org/W2963357986', 'https://openalex.org/W4206119265', 'https://openalex.org/W2516069911', 'https://openalex.org/W2934641558', 'https://openalex.org/W2913184320', 'https://openalex.org/W4239818589', 'https://openalex.org/W2586516948', 'https://openalex.org/W4231900804', 'https://openalex.org/W2777505348', 'https://openalex.org/W3130275310', 'https://openalex.org/W2074231493', 'https://openalex.org/W2899463613', 'https://openalex.org/W4287263097', 'https://openalex.org/W2186458025', 'https://openalex.org/W3171066367', 'https://openalex.org/W2116211107', 'https://openalex.org/W2592647456', 'https://openalex.org/W2946864768', 'https://openalex.org/W2002197930', 'https://openalex.org/W2994617333', 'https://openalex.org/W2202833113', 'https://openalex.org/W3032698460', 'https://openalex.org/W2990286807', 'https://openalex.org/W2141340689', 'https://openalex.org/W4292779060', 'https://openalex.org/W2133564696', 'https://openalex.org/W2937113075', 'https://openalex.org/W3176899693', 'https://openalex.org/W2805490244', 'https://openalex.org/W2307479170', 'https://openalex.org/W2915977242', 'https://openalex.org/W2586602577', 'https://openalex.org/W2896509746', 'https://openalex.org/W2996710805', 'https://openalex.org/W1541381880', 'https://openalex.org/W3098466758', 'https://openalex.org/W3186213131', 'https://openalex.org/W1993698725', 'https://openalex.org/W1533169541', 'https://openalex.org/W2572136487', 'https://openalex.org/W3015210281', 'https://openalex.org/W2963014409', 'https://openalex.org/W3110317399', 'https://openalex.org/W2807931239', 'https://openalex.org/W2010906431', 'https://openalex.org/W2877825950', 'https://openalex.org/W2346451634', 'https://openalex.org/W2972875282', 'https://openalex.org/W2939880993', 'https://openalex.org/W2021237916', 'https://openalex.org/W2167224731', 'https://openalex.org/W2251282831', 'https://openalex.org/W2000529208', 'https://openalex.org/W4200518306', 'https://openalex.org/W2100397666', 'https://openalex.org/W4256620352', 'https://openalex.org/W4226246098', 'https://openalex.org/W4287888444', 'https://openalex.org/W201141796', 'https://openalex.org/W2134790616', 'https://openalex.org/W2962739339', 'https://openalex.org/W3167873515', 'https://openalex.org/W1614298861', 'https://openalex.org/W2996152720', 'https://openalex.org/W2063598038', 'https://openalex.org/W2134141008', 'https://openalex.org/W4226293679', 'https://openalex.org/W2740115495', 'https://openalex.org/W2121879602', 'https://openalex.org/W2286195342', 'https://openalex.org/W2890761057', 'https://openalex.org/W2165018253', 'https://openalex.org/W2111797102', 'https://openalex.org/W3003700655', 'https://openalex.org/W1871004397', 'https://openalex.org/W1967986525', 'https://openalex.org/W4280596325', 'https://openalex.org/W4385573447', 'https://openalex.org/W114582372', 'https://openalex.org/W4302412247', 'https://openalex.org/W2889769646', 'https://openalex.org/W1561222996', 'https://openalex.org/W2051286981', 'https://openalex.org/W2043456277', 'https://openalex.org/W2737711067', 'https://openalex.org/W1984749204', 'https://openalex.org/W2117126688', 'https://openalex.org/W4297828929', 'https://openalex.org/W2771319872', 'https://openalex.org/W2413317570', 'https://openalex.org/W2963248104', 'https://openalex.org/W3184677240', 'https://openalex.org/W350796419', 'https://openalex.org/W2619269479', 'https://openalex.org/W2153533342', 'https://openalex.org/W3186068210', 'https://openalex.org/W2046242735', 'https://openalex.org/W2326557633', 'https://openalex.org/W2952468927', 'https://openalex.org/W2949973181', 'https://openalex.org/W2972720388', 'https://openalex.org/W2250759348', 'https://openalex.org/W3127686677', 'https://openalex.org/W2000982178', 'https://openalex.org/W3187024518', 'https://openalex.org/W2804639187', 'https://openalex.org/W2972788883', 'https://openalex.org/W1966371777', 'https://openalex.org/W2845772244', 'https://openalex.org/W602842521', 'https://openalex.org/W3192238591', 'https://openalex.org/W3035207248', 'https://openalex.org/W3092366774', 'https://openalex.org/W2963341956', 'https://openalex.org/W1905100302', 'https://openalex.org/W2105738468', 'https://openalex.org/W4200490596', 'https://openalex.org/W3185515067', 'https://openalex.org/W1970887833', 'https://openalex.org/W2893090564', 'https://openalex.org/W2251670681', 'https://openalex.org/W4299382473', 'https://openalex.org/W634571558', 'https://openalex.org/W2952332047', 'https://openalex.org/W3106416961', 'https://openalex.org/W2597143577', 'https://openalex.org/W2118678627', 'https://openalex.org/W131663347', 'https://openalex.org/W4287854713', 'https://openalex.org/W2461808544', 'https://openalex.org/W3203561692', 'https://openalex.org/W1995264984', 'https://openalex.org/W2794653583', 'https://openalex.org/W1582082124', 'https://openalex.org/W2055373523', 'https://openalex.org/W4233898503', 'https://openalex.org/W2962784628', 'https://openalex.org/W2964053711', 'https://openalex.org/W3034993603', 'https://openalex.org/W2962680795', 'https://openalex.org/W3119325278', 'https://openalex.org/W4322100081', 'https://openalex.org/W2963250244', 'https://openalex.org/W2235025706', 'https://openalex.org/W2140819719', 'https://openalex.org/W3100160869', 'https://openalex.org/W2963735467', 'https://openalex.org/W3082458101', 'https://openalex.org/W4200489976', 'https://openalex.org/W2739957108', 'https://openalex.org/W1987665455', 'https://openalex.org/W2060617459', 'https://openalex.org/W2505626185', 'https://openalex.org/W90483246', 'https://openalex.org/W2152561112', 'https://openalex.org/W2587019100', 'https://openalex.org/W3121914243', 'https://openalex.org/W2140745403', 'https://openalex.org/W648786980', 'https://openalex.org/W46679369', 'https://openalex.org/W2101711363', 'https://openalex.org/W146914567', 'https://openalex.org/W3032746405', 'https://openalex.org/W2017504415', 'https://openalex.org/W2140721185', 'https://openalex.org/W2130610046', 'https://openalex.org/W2937921076', 'https://openalex.org/W3037720825', 'https://openalex.org/W2955241472', 'https://openalex.org/W2045322128', 'https://openalex.org/W2476832019', 'https://openalex.org/W4287891002', 'https://openalex.org/W2908510526', 'https://openalex.org/W6908809', 'https://openalex.org/W1513168562', 'https://openalex.org/W4226271314', 'https://openalex.org/W2028125485', 'https://openalex.org/W4388156388', 'https://openalex.org/W3118872778', 'https://openalex.org/W3009327898', 'https://openalex.org/W2618836062', 'https://openalex.org/W3117751609', 'https://openalex.org/W3214133644', 'https://openalex.org/W2321375533', 'https://openalex.org/W3176285434', 'https://openalex.org/W2036742977', 'https://openalex.org/W2095510859', 'https://openalex.org/W3035229572', 'https://openalex.org/W1583314545', 'https://openalex.org/W2891546148', 'https://openalex.org/W3036601975', 'https://openalex.org/W1866333965', 'https://openalex.org/W2017163068', 'https://openalex.org/W2808940912', 'https://openalex.org/W2954942046', 'https://openalex.org/W3104723404', 'https://openalex.org/W2130903752', 'https://openalex.org/W2402767472', 'https://openalex.org/W2970963828', 'https://openalex.org/W2469697986', 'https://openalex.org/W2003573652', 'https://openalex.org/W2300419000', 'https://openalex.org/W2777397604', 'https://openalex.org/W3010725746', 'https://openalex.org/W2799124508', 'https://openalex.org/W2996087418', 'https://openalex.org/W2085799371', 'https://openalex.org/W2748879498', 'https://openalex.org/W1501957683', 'https://openalex.org/W2083553072', 'https://openalex.org/W2130042265', 'https://openalex.org/W2735552604', 'https://openalex.org/W4385245566', 'https://openalex.org/W3197642003', 'https://openalex.org/W2965373594', 'https://openalex.org/W4280645456', 'https://openalex.org/W2980466200', 'https://openalex.org/W2164105151', 'https://openalex.org/W3213934211', 'https://openalex.org/W4226225516', 'https://openalex.org/W2946322623', 'https://openalex.org/W2090266881', 'https://openalex.org/W4244165226', 'https://openalex.org/W2343674250', 'https://openalex.org/W4287854648', 'https://openalex.org/W3037142432', 'https://openalex.org/W4327656064', 'https://openalex.org/W2007780422', 'https://openalex.org/W2251737533', 'https://openalex.org/W3037582736', 'https://openalex.org/W2077636794', 'https://openalex.org/W2470595162', 'https://openalex.org/W4248053963', 'https://openalex.org/W3133632515', 'https://openalex.org/W3214933191', 'https://openalex.org/W584561297', 'https://openalex.org/W157310100', 'https://openalex.org/W2042143122', 'https://openalex.org/W3183161213', 'https://openalex.org/W2561296568', 'https://openalex.org/W2251563156', 'https://openalex.org/W2950993472', 'https://openalex.org/W2619190945', 'https://openalex.org/W2744098564', 'https://openalex.org/W2989204674', 'https://openalex.org/W309335912', 'https://openalex.org/W2065516380', 'https://openalex.org/W2948947170', 'https://openalex.org/W2508772004', 'https://openalex.org/W4250530039', 'https://openalex.org/W3166790124', 'https://openalex.org/W2752168051', 'https://openalex.org/W1972955035', 'https://openalex.org/W4288410857', 'https://openalex.org/W2101234009', 'https://openalex.org/W2012896854', 'https://openalex.org/W2295551984', 'https://openalex.org/W3183582945', 'https://openalex.org/W3093427098', 'https://openalex.org/W2950858167', 'https://openalex.org/W4255446654', 'https://openalex.org/W2491002369', 'https://openalex.org/W2071402670', 'https://openalex.org/W2807107081', 'https://openalex.org/W4394651511', 'https://openalex.org/W3026538704', 'https://openalex.org/W191454395', 'https://openalex.org/W2158266063', 'https://openalex.org/W1745498025', 'https://openalex.org/W2000195737', 'https://openalex.org/W649464194', 'https://openalex.org/W2121456165', 'https://openalex.org/W3037995636', 'https://openalex.org/W2508815538', 'https://openalex.org/W2589052591', 'https://openalex.org/W3174418826', 'https://openalex.org/W3024787440', 'https://openalex.org/W2137186284', 'https://openalex.org/W2050866747', 'https://openalex.org/W2740149041', 'https://openalex.org/W2105495317', 'https://openalex.org/W2889894161', 'https://openalex.org/W2767512843', 'https://openalex.org/W4255558108', 'https://openalex.org/W2888784389', 'https://openalex.org/W3034999214', 'https://openalex.org/W2223415196', 'https://openalex.org/W3093177323', 'https://openalex.org/W2019645080', 'https://openalex.org/W2806538304', 'https://openalex.org/W1970482722', 'https://openalex.org/W2964045208', 'https://openalex.org/W2087372583', 'https://openalex.org/W2963472176', 'https://openalex.org/W2071175130', 'https://openalex.org/W1608101656', 'https://openalex.org/W2962705709', 'https://openalex.org/W1975363639', 'https://openalex.org/W3212217168', 'https://openalex.org/W3183153947', 'https://openalex.org/W2913613274', 'https://openalex.org/W2251944986', 'https://openalex.org/W2807188009', 'https://openalex.org/W3163596720', 'https://openalex.org/W2251667755', 'https://openalex.org/W2193413348', 'https://openalex.org/W3028805921', 'https://openalex.org/W3113488190', 'https://openalex.org/W2250816155', 'https://openalex.org/W1993283377', 'https://openalex.org/W2117621558', 'https://openalex.org/W1534778275', 'https://openalex.org/W4287887703', 'https://openalex.org/W2166637769', 'https://openalex.org/W2740231755', 'https://openalex.org/W2250618788', 'https://openalex.org/W2114116616', 'https://openalex.org/W3159533391', 'https://openalex.org/W2895146711']",2022-01-01
https://openalex.org/W3157861865,https://doi.org/10.1613/jair.1.12967,"Visually Grounded Models of Spoken Language: A Survey of Datasets, Architectures and Evaluation Techniques","This survey provides an overview of the evolution of visually grounded models of spoken language over the last 20 years. Such models are inspired by the observation that when children pick up a language, they rely on a wide range of indirect and noisy clues, crucially including signals from the visual modality co-occurring with spoken utterances. Several fields have made important contributions to this approach to modeling or mimicking the process of learning language: Machine Learning, Natural Language and Speech Processing, Computer Vision and Cognitive Science. The current paper brings together these contributions in order to provide a useful introduction and overview for practitioners in all these areas. We discuss the central research questions addressed, the timeline of developments, and the datasets which enabled much of this work. We then summarize the main modeling architectures and offer an exhaustive overview of the evaluation metrics and analysis techniques.","['https://openalex.org/W2524365899', 'https://openalex.org/W3105148948', 'https://openalex.org/W3177829661', 'https://openalex.org/W6747045456', 'https://openalex.org/W2230076941', 'https://openalex.org/W2957089051', 'https://openalex.org/W2752168051', 'https://openalex.org/W2282219577', 'https://openalex.org/W6766153511', 'https://openalex.org/W2006969979', 'https://openalex.org/W2906407728', 'https://openalex.org/W2586148577', 'https://openalex.org/W4393717817', 'https://openalex.org/W3017025049', 'https://openalex.org/W1924770834', 'https://openalex.org/W2553608650', 'https://openalex.org/W6676297131', 'https://openalex.org/W2531381952', 'https://openalex.org/W2102605133', 'https://openalex.org/W2796156786', 'https://openalex.org/W2137010615', 'https://openalex.org/W2580178245', 'https://openalex.org/W2927673779', 'https://openalex.org/W2991557631', 'https://openalex.org/W2796315435', 'https://openalex.org/W2556930864', 'https://openalex.org/W2736876693', 'https://openalex.org/W4288076474', 'https://openalex.org/W4393840460', 'https://openalex.org/W2920166246', 'https://openalex.org/W2974048280', 'https://openalex.org/W6687483927', 'https://openalex.org/W3092512595', 'https://openalex.org/W3161348170', 'https://openalex.org/W2960271609', 'https://openalex.org/W3114436296', 'https://openalex.org/W2974393448', 'https://openalex.org/W2938991416', 'https://openalex.org/W2808286951', 'https://openalex.org/W2601713192', 'https://openalex.org/W2903320905', 'https://openalex.org/W1905882502', 'https://openalex.org/W2112912048', 'https://openalex.org/W3164946614', 'https://openalex.org/W2160654481', 'https://openalex.org/W3093241733', 'https://openalex.org/W6756021500', 'https://openalex.org/W6639102338', 'https://openalex.org/W2250790822', 'https://openalex.org/W6767668184', 'https://openalex.org/W2948859046', 'https://openalex.org/W1614298861', 'https://openalex.org/W3160014322', 'https://openalex.org/W1574972348', 'https://openalex.org/W2533598788', 'https://openalex.org/W3095881291', 'https://openalex.org/W3015300171', 'https://openalex.org/W3111013239', 'https://openalex.org/W2941492599', 'https://openalex.org/W6656414902', 'https://openalex.org/W6801926475', 'https://openalex.org/W2992526251', 'https://openalex.org/W3034875620', 'https://openalex.org/W2132921748', 'https://openalex.org/W2080320702', 'https://openalex.org/W2107917162', 'https://openalex.org/W3143035657', 'https://openalex.org/W6773790731', 'https://openalex.org/W2593779438', 'https://openalex.org/W3029315861', 'https://openalex.org/W1686810756', 'https://openalex.org/W6864391120', 'https://openalex.org/W2752796333', 'https://openalex.org/W3027324582', 'https://openalex.org/W6739901393', 'https://openalex.org/W3158565912', 'https://openalex.org/W2611064105', 'https://openalex.org/W2029096624', 'https://openalex.org/W6678493852', 'https://openalex.org/W2766091292', 'https://openalex.org/W2134670479', 'https://openalex.org/W2507296351', 'https://openalex.org/W2784025607', 'https://openalex.org/W6720905350', 'https://openalex.org/W2989358187', 'https://openalex.org/W2952132648', 'https://openalex.org/W2971709506', 'https://openalex.org/W385555557', 'https://openalex.org/W4288595436', 'https://openalex.org/W2965147078', 'https://openalex.org/W2984008963', 'https://openalex.org/W4287240590', 'https://openalex.org/W4393617183', 'https://openalex.org/W2973135958', 'https://openalex.org/W1861492603', 'https://openalex.org/W3159476814', 'https://openalex.org/W3200287550', 'https://openalex.org/W2972892814', 'https://openalex.org/W3121480429', 'https://openalex.org/W3174311593', 'https://openalex.org/W2995680346', 'https://openalex.org/W2988907666', 'https://openalex.org/W2963983719', 'https://openalex.org/W2962862718', 'https://openalex.org/W1797268635', 'https://openalex.org/W2964001192', 'https://openalex.org/W2963799213', 'https://openalex.org/W2108598243', 'https://openalex.org/W3196698946', 'https://openalex.org/W2963778889', 'https://openalex.org/W2972808286', 'https://openalex.org/W2123815913', 'https://openalex.org/W2963403868', 'https://openalex.org/W3100813302', 'https://openalex.org/W2119775030', 'https://openalex.org/W2964099072', 'https://openalex.org/W3213502289', 'https://openalex.org/W2194775991', 'https://openalex.org/W2095897464', 'https://openalex.org/W2125566341', 'https://openalex.org/W4297606427', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963902314', 'https://openalex.org/W2963525826', 'https://openalex.org/W4237938692', 'https://openalex.org/W2963163163', 'https://openalex.org/W2963330681', 'https://openalex.org/W2950133079', 'https://openalex.org/W3100923070', 'https://openalex.org/W2024490156', 'https://openalex.org/W2963115079', 'https://openalex.org/W3095670406', 'https://openalex.org/W2962753610', 'https://openalex.org/W4394453761', 'https://openalex.org/W4294555862', 'https://openalex.org/W4230640548', 'https://openalex.org/W3095361818', 'https://openalex.org/W2962813140', 'https://openalex.org/W3217290931', 'https://openalex.org/W4286973758', 'https://openalex.org/W4297826211', 'https://openalex.org/W3197828817', 'https://openalex.org/W3005578234', 'https://openalex.org/W3035750922', 'https://openalex.org/W4300047444', 'https://openalex.org/W3170972077', 'https://openalex.org/W2940544976']",2022-02-18
https://openalex.org/W3200287550,https://doi.org/10.1109/icassp43922.2022.9747103,Fast-Slow Transformer for Visually Grounding Speech,"We present Fast-Slow Transformer for Visually Grounding Speech, or FaST-VGS. FaST-VGS is a Transformer-based model for learning the associations between raw speech waveforms and visual images. The model unifies dual-encoder and cross-attention architectures into a single model, reaping the superior retrieval speed of the former along with the accuracy of the latter. FaST-VGS achieves state-of-the-art speech-image retrieval accuracy on benchmark datasets, and its learned representations exhibit strong performance on the ZeroSpeech 2021 phonetic and semantic tasks.","['https://openalex.org/W3155230099', 'https://openalex.org/W6755207826', 'https://openalex.org/W68733909', 'https://openalex.org/W6639102338', 'https://openalex.org/W6679792166', 'https://openalex.org/W3171668871', 'https://openalex.org/W6755977528', 'https://openalex.org/W6620707391', 'https://openalex.org/W6780218876', 'https://openalex.org/W2481240925', 'https://openalex.org/W6770596778', 'https://openalex.org/W3100813302', 'https://openalex.org/W6750651883', 'https://openalex.org/W3161204797', 'https://openalex.org/W3197467690', 'https://openalex.org/W2927673779', 'https://openalex.org/W3174311593', 'https://openalex.org/W6750194919', 'https://openalex.org/W2964099072', 'https://openalex.org/W6779753737', 'https://openalex.org/W3015300171', 'https://openalex.org/W6797832685', 'https://openalex.org/W2972892814', 'https://openalex.org/W6786696081', 'https://openalex.org/W6750570362', 'https://openalex.org/W2971709506', 'https://openalex.org/W2970231061', 'https://openalex.org/W2964001192', 'https://openalex.org/W3198411039', 'https://openalex.org/W3095293218', 'https://openalex.org/W2586850765', 'https://openalex.org/W2586148577', 'https://openalex.org/W3157861865', 'https://openalex.org/W2906407728', 'https://openalex.org/W2988907666', 'https://openalex.org/W3096372900', 'https://openalex.org/W3198815374', 'https://openalex.org/W6739901393', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962862718', 'https://openalex.org/W3197349023', 'https://openalex.org/W6729977899', 'https://openalex.org/W3036601975', 'https://openalex.org/W2963341956', 'https://openalex.org/W2842511635', 'https://openalex.org/W2995680346', 'https://openalex.org/W3114436296', 'https://openalex.org/W2962866381', 'https://openalex.org/W3175947832', 'https://openalex.org/W3110458199', 'https://openalex.org/W1861492603', 'https://openalex.org/W2896457183', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963902314', 'https://openalex.org/W3196698946', 'https://openalex.org/W2899663614', 'https://openalex.org/W3099782249', 'https://openalex.org/W3177829661', 'https://openalex.org/W2963525826', 'https://openalex.org/W639708223', 'https://openalex.org/W2134670479', 'https://openalex.org/W2796315435', 'https://openalex.org/W2556930864', 'https://openalex.org/W2295158492', 'https://openalex.org/W4287591426', 'https://openalex.org/W4385245566', 'https://openalex.org/W2277195237', 'https://openalex.org/W4287757663', 'https://openalex.org/W2963330681']",2022-04-27
https://openalex.org/W3204420730,https://doi.org/10.1109/icassp43922.2022.9746421,Visualtts: TTS with Accurate Lip-Speech Synchronization for Automatic Voice Over,"In this paper, we formulate a novel task to synthesize speech in sync with a silent pre-recorded video, denoted as automatic voice over (AVO). Unlike traditional speech synthesis, AVO seeks to generate not only human-sounding speech, but also perfect lip-speech synchronization. A natural solution to AVO is to condition the speech rendering on the temporal progression of lip sequence in the video. We propose a novel text-to-speech model that is conditioned on visual input, named VisualTTS, for accurate lip-speech synchronization. The proposed VisualTTS adopts two novel mechanisms that are 1) textual-visual attention, and 2) visual fusion strategy during acoustic decoding, which both contribute to forming accurate alignment between the input text content and lip motion in input lip sequence. Experimental results show that VisualTTS achieves accurate lip-speech synchronization and outperforms all baseline systems.","['https://openalex.org/W3098557217', 'https://openalex.org/W6735927292', 'https://openalex.org/W3081492798', 'https://openalex.org/W6748409065', 'https://openalex.org/W6754392867', 'https://openalex.org/W4289665794', 'https://openalex.org/W6739901393', 'https://openalex.org/W3163287738', 'https://openalex.org/W2015143272', 'https://openalex.org/W6787100281', 'https://openalex.org/W3155217823', 'https://openalex.org/W3035626590', 'https://openalex.org/W6794378236', 'https://openalex.org/W6754420807', 'https://openalex.org/W2903739847', 'https://openalex.org/W2046056978', 'https://openalex.org/W2964243274', 'https://openalex.org/W6778823374', 'https://openalex.org/W6754473786', 'https://openalex.org/W6763832098', 'https://openalex.org/W3015826515', 'https://openalex.org/W2963945466', 'https://openalex.org/W2963609956', 'https://openalex.org/W3168542456', 'https://openalex.org/W3084396630', 'https://openalex.org/W3168662520', 'https://openalex.org/W3095379519', 'https://openalex.org/W2963522141', 'https://openalex.org/W6732872814', 'https://openalex.org/W2964171275', 'https://openalex.org/W3008400075', 'https://openalex.org/W3130016944', 'https://openalex.org/W3157840621', 'https://openalex.org/W2578229578', 'https://openalex.org/W3114436296', 'https://openalex.org/W2889048668', 'https://openalex.org/W2949382160', 'https://openalex.org/W2970730223', 'https://openalex.org/W3101631197', 'https://openalex.org/W2527729766', 'https://openalex.org/W2890952074', 'https://openalex.org/W2891205112', 'https://openalex.org/W4298580827', 'https://openalex.org/W2963403868', 'https://openalex.org/W3123318516', 'https://openalex.org/W2964307104', 'https://openalex.org/W2519091744', 'https://openalex.org/W2946200149', 'https://openalex.org/W3033411150', 'https://openalex.org/W3174311593', 'https://openalex.org/W2604379605', 'https://openalex.org/W4385245566', 'https://openalex.org/W4298112588']",2022-04-27
https://openalex.org/W3206387123,https://doi.org/10.1109/taslp.2021.3120644,Synthesizing Spoken Descriptions of Images,"Image captioning technology has great potential in many scenarios. However, current text-based image captioning methods cannot be applied to approximately half of the world's languages due to these languages’ lack of a written form. To solve this problem, recently the image-to-speech task was proposed, which generates spoken descriptions of images bypassing any text via an intermediate representation consisting of phonemes (image-to-phoneme). Here, we present a comprehensive study on the image-to-speech task in which, 1) several representative image-to-text generation methods are implemented for the image-to-phoneme task, 2) objective metrics are sought to evaluate the image-to-phoneme task, and 3) an end-to-end image-to-speech model that is able to synthesize spoken descriptions of images bypassing both text and phonemes is proposed. Extensive experiments are conducted on the public benchmark database Flickr8k. Results of our experiments demonstrate that 1) State-of-the-art image-to-text models can perform well on the image-to-phoneme task, and 2) several evaluation metrics, including BLEU3, BLEU4, BLEU5, and ROUGE-L can be used to evaluate image-to-phoneme performance. Finally, 3) end-to-end image-to-speech bypassing text and phonemes is feasible.","['https://openalex.org/W3123798147', 'https://openalex.org/W3095030004', 'https://openalex.org/W3005578234', 'https://openalex.org/W2950133079', 'https://openalex.org/W2988907666', 'https://openalex.org/W2971709506', 'https://openalex.org/W6795412237', 'https://openalex.org/W3027851323', 'https://openalex.org/W2796315435', 'https://openalex.org/W6794343169', 'https://openalex.org/W6917585676', 'https://openalex.org/W6791006397', 'https://openalex.org/W6775036401', 'https://openalex.org/W6763832098', 'https://openalex.org/W2963902314', 'https://openalex.org/W6778823374', 'https://openalex.org/W6763643401', 'https://openalex.org/W3097538987', 'https://openalex.org/W2185175083', 'https://openalex.org/W6729977899', 'https://openalex.org/W2603567530', 'https://openalex.org/W2907262790', 'https://openalex.org/W2895420168', 'https://openalex.org/W6630875275', 'https://openalex.org/W1895577753', 'https://openalex.org/W2575842049', 'https://openalex.org/W1905882502', 'https://openalex.org/W2965846473', 'https://openalex.org/W2803259101', 'https://openalex.org/W6739901393', 'https://openalex.org/W2745461083', 'https://openalex.org/W2108598243', 'https://openalex.org/W2277195237', 'https://openalex.org/W6621543089', 'https://openalex.org/W1494198834', 'https://openalex.org/W2888867175', 'https://openalex.org/W6631362777', 'https://openalex.org/W6623517193', 'https://openalex.org/W2964243274', 'https://openalex.org/W2997591391', 'https://openalex.org/W4206865574', 'https://openalex.org/W6749352598', 'https://openalex.org/W6787100281', 'https://openalex.org/W6770596778', 'https://openalex.org/W3155217823', 'https://openalex.org/W3096249149', 'https://openalex.org/W68733909', 'https://openalex.org/W2131179926', 'https://openalex.org/W2062955551', 'https://openalex.org/W1987835821', 'https://openalex.org/W2986670728', 'https://openalex.org/W2990818246', 'https://openalex.org/W2963084599', 'https://openalex.org/W3034655362', 'https://openalex.org/W6751792830', 'https://openalex.org/W2963992143', 'https://openalex.org/W6637373629', 'https://openalex.org/W3035284526', 'https://openalex.org/W1956340063', 'https://openalex.org/W6682631176', 'https://openalex.org/W6802821423', 'https://openalex.org/W2962862718', 'https://openalex.org/W2795151422', 'https://openalex.org/W6620707391', 'https://openalex.org/W6678262379', 'https://openalex.org/W6898505805', 'https://openalex.org/W1686810756', 'https://openalex.org/W2613718673', 'https://openalex.org/W2123301721', 'https://openalex.org/W3204007566', 'https://openalex.org/W3158565912', 'https://openalex.org/W648786980', 'https://openalex.org/W2971310675', 'https://openalex.org/W3102219307', 'https://openalex.org/W4288329833', 'https://openalex.org/W3132706255', 'https://openalex.org/W2963403868', 'https://openalex.org/W2970730223', 'https://openalex.org/W2101105183', 'https://openalex.org/W3174311593', 'https://openalex.org/W3035160838', 'https://openalex.org/W1514535095', 'https://openalex.org/W2556930864', 'https://openalex.org/W854541894', 'https://openalex.org/W2963260927', 'https://openalex.org/W2949382160', 'https://openalex.org/W3033411150', 'https://openalex.org/W2995680346', 'https://openalex.org/W4385245566', 'https://openalex.org/W2519091744', 'https://openalex.org/W3130016944', 'https://openalex.org/W2154652894', 'https://openalex.org/W3114436296', 'https://openalex.org/W2968831808', 'https://openalex.org/W639708223', 'https://openalex.org/W3161204797', 'https://openalex.org/W2946200149', 'https://openalex.org/W4287324741', 'https://openalex.org/W1524333225']",2021-01-01
https://openalex.org/W4382918397,https://doi.org/10.1111/cogs.13307,Introducing Meta‐analysis in the Evaluation of Computational Models of Infant Language Development,"Abstract Computational models of child language development can help us understand the cognitive underpinnings of the language learning process, which occurs along several linguistic levels at once (e.g., prosodic and phonological). However, in light of the replication crisis, modelers face the challenge of selecting representative and consolidated infant data. Thus, it is desirable to have evaluation methodologies that could account for robust empirical reference data, across multiple infant capabilities. Moreover, there is a need for practices that can compare developmental trajectories of infants to those of models as a function of language experience and development. The present study aims to take concrete steps to address these needs by introducing the concept of comparing models with large‐scale cumulative empirical data from infants, as quantified by meta‐analyses conducted across a large number of individual behavioral studies. We formalize the connection between measurable model and human behavior, and then present a conceptual framework for meta‐analytic evaluation of computational models. We exemplify the meta‐analytic model evaluation approach with two modeling experiments on infant‐directed speech preference and native/non‐native vowel discrimination.","['https://openalex.org/W2137295896', 'https://openalex.org/W3177829661', 'https://openalex.org/W4213058750', 'https://openalex.org/W4283258029', 'https://openalex.org/W2772732614', 'https://openalex.org/W4212863985', 'https://openalex.org/W6629510986', 'https://openalex.org/W2054289822', 'https://openalex.org/W3130490927', 'https://openalex.org/W3165112351', 'https://openalex.org/W2067191956', 'https://openalex.org/W6667443654', 'https://openalex.org/W3017025049', 'https://openalex.org/W3035750922', 'https://openalex.org/W2926827382', 'https://openalex.org/W2972943112', 'https://openalex.org/W4244103449', 'https://openalex.org/W6652776558', 'https://openalex.org/W2253063626', 'https://openalex.org/W3082376609', 'https://openalex.org/W3023889965', 'https://openalex.org/W2940544976', 'https://openalex.org/W2963620343', 'https://openalex.org/W6697293080', 'https://openalex.org/W2483390977', 'https://openalex.org/W2130890537', 'https://openalex.org/W4299555621', 'https://openalex.org/W2157823046', 'https://openalex.org/W3145811386', 'https://openalex.org/W3196459653', 'https://openalex.org/W2079408923', 'https://openalex.org/W6670098071', 'https://openalex.org/W2123292690', 'https://openalex.org/W2782905495', 'https://openalex.org/W2595479191', 'https://openalex.org/W2141038596', 'https://openalex.org/W3119754616', 'https://openalex.org/W2148764920', 'https://openalex.org/W3084297320', 'https://openalex.org/W4253971549', 'https://openalex.org/W2191828469', 'https://openalex.org/W3119802905', 'https://openalex.org/W3123354371', 'https://openalex.org/W7071764571', 'https://openalex.org/W2070696251', 'https://openalex.org/W4285726357', 'https://openalex.org/W4295309037', 'https://openalex.org/W3114436296', 'https://openalex.org/W3174311593', 'https://openalex.org/W2466796873', 'https://openalex.org/W2144981148', 'https://openalex.org/W135984148', 'https://openalex.org/W4206039057', 'https://openalex.org/W3164946614', 'https://openalex.org/W4230640548', 'https://openalex.org/W2057256606', 'https://openalex.org/W4225079082', 'https://openalex.org/W1990351858', 'https://openalex.org/W2103091632', 'https://openalex.org/W2031445901', 'https://openalex.org/W4253566955', 'https://openalex.org/W3161374022', 'https://openalex.org/W2785183465', 'https://openalex.org/W6618980683', 'https://openalex.org/W3081675162', 'https://openalex.org/W2031880432', 'https://openalex.org/W6658466740', 'https://openalex.org/W1576931943', 'https://openalex.org/W2002572909', 'https://openalex.org/W3023172065', 'https://openalex.org/W1965305729', 'https://openalex.org/W6641383474', 'https://openalex.org/W2024579455', 'https://openalex.org/W2091143423', 'https://openalex.org/W2615578810', 'https://openalex.org/W3110458199', 'https://openalex.org/W3163184902', 'https://openalex.org/W1897139626', 'https://openalex.org/W2610253745', 'https://openalex.org/W1494198834', 'https://openalex.org/W2268773773', 'https://openalex.org/W2140386792', 'https://openalex.org/W6680931182', 'https://openalex.org/W3033724742', 'https://openalex.org/W2119885245', 'https://openalex.org/W2010188467', 'https://openalex.org/W4376140200', 'https://openalex.org/W2415378728', 'https://openalex.org/W3023533951', 'https://openalex.org/W3035507081', 'https://openalex.org/W3129957462', 'https://openalex.org/W2768381684', 'https://openalex.org/W3125087428', 'https://openalex.org/W2395899413', 'https://openalex.org/W2406349064', 'https://openalex.org/W3102519966', 'https://openalex.org/W4303629135', 'https://openalex.org/W2154600605', 'https://openalex.org/W3084747096', 'https://openalex.org/W2092929651', 'https://openalex.org/W2041173449', 'https://openalex.org/W2077382402', 'https://openalex.org/W6669981445', 'https://openalex.org/W2032476212', 'https://openalex.org/W2114831903', 'https://openalex.org/W7073679870', 'https://openalex.org/W1925965306', 'https://openalex.org/W6640227979', 'https://openalex.org/W2786608204', 'https://openalex.org/W6973666849', 'https://openalex.org/W2089883580', 'https://openalex.org/W3157923770', 'https://openalex.org/W3197580070', 'https://openalex.org/W3117111924', 'https://openalex.org/W4211209158', 'https://openalex.org/W2284729062', 'https://openalex.org/W2129168188', 'https://openalex.org/W2973026522', 'https://openalex.org/W612372977', 'https://openalex.org/W2482374272']",2023-07-01
https://openalex.org/W3164946614,https://doi.org/10.48550/arxiv.2109.14200,"Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? -- A computational investigation","Decades of research has studied how language learning infants learn to discriminate speech sounds, segment words, and associate words with their meanings. While gradual development of such capabilities is unquestionable, the exact nature of these skills and the underlying mental representations yet remains unclear. In parallel, computational studies have shown that basic comprehension of speech can be achieved by statistical learning between speech and concurrent referentially ambiguous visual input. These models can operate without prior linguistic knowledge such as representations of linguistic units, and without learning mechanisms specifically targeted at such units. This has raised the question of to what extent knowledge of linguistic units, such as phone(me)s, syllables, and words, could actually emerge as latent representations supporting the translation between speech and representations in other modalities, and without the units being proximal learning targets for the learner. In this study, we formulate this idea as the so-called latent language hypothesis (LLH), connecting linguistic representation learning to general predictive processing within and across sensory modalities. We review the extent that the audiovisual aspect of LLH is supported by the existing computational studies. We then explore LLH further in extensive learning simulations with different neural network models for audiovisual cross-situational learning, and comparing learning from both synthetic and real speech data. We investigate whether the latent representations learned by the networks reflect phonetic, syllabic, or lexical structure of input speech by utilizing an array of complementary evaluation metrics related to linguistic selectivity and temporal characteristics of the representations. As a result, we find that representations associated...","['https://openalex.org/W2964001192', 'https://openalex.org/W1486114846', 'https://openalex.org/W2553608650', 'https://openalex.org/W2153791616', 'https://openalex.org/W2927673779', 'https://openalex.org/W124411215', 'https://openalex.org/W2095279510', 'https://openalex.org/W2148876114', 'https://openalex.org/W2768701474', 'https://openalex.org/W3016185014', 'https://openalex.org/W2964308564', 'https://openalex.org/W2962862718', 'https://openalex.org/W2415378728', 'https://openalex.org/W1993768374', 'https://openalex.org/W3100806282', 'https://openalex.org/W3114436296', 'https://openalex.org/W3094626712', 'https://openalex.org/W2041945486', 'https://openalex.org/W2972345028', 'https://openalex.org/W2125566341', 'https://openalex.org/W2170240579', 'https://openalex.org/W1786473797', 'https://openalex.org/W1980491396', 'https://openalex.org/W2123815913', 'https://openalex.org/W1575001262', 'https://openalex.org/W385555557', 'https://openalex.org/W2556930864', 'https://openalex.org/W2107917162', 'https://openalex.org/W2149557440', 'https://openalex.org/W2021916592', 'https://openalex.org/W2160654481', 'https://openalex.org/W3029315861', 'https://openalex.org/W2101509422', 'https://openalex.org/W2973180715', 'https://openalex.org/W2586148577', 'https://openalex.org/W2133148365', 'https://openalex.org/W583680991', 'https://openalex.org/W2073791162', 'https://openalex.org/W2906152891', 'https://openalex.org/W2089883580', 'https://openalex.org/W2079207700', 'https://openalex.org/W2134670479', 'https://openalex.org/W68733909', 'https://openalex.org/W2036572838', 'https://openalex.org/W2007920248', 'https://openalex.org/W1993755070', 'https://openalex.org/W2041399291', 'https://openalex.org/W2950133079', 'https://openalex.org/W2121464381', 'https://openalex.org/W2059168261', 'https://openalex.org/W2274405424', 'https://openalex.org/W3120008787', 'https://openalex.org/W2346794446', 'https://openalex.org/W2114739781', 'https://openalex.org/W2758849341', 'https://openalex.org/W2772756919', 'https://openalex.org/W2396361046', 'https://openalex.org/W2104752510', 'https://openalex.org/W1889081078', 'https://openalex.org/W2972680241', 'https://openalex.org/W2989358187', 'https://openalex.org/W2295676751', 'https://openalex.org/W2017814685', 'https://openalex.org/W2071402670', 'https://openalex.org/W2167205507', 'https://openalex.org/W3105148948', 'https://openalex.org/W2995680346', 'https://openalex.org/W2113610578', 'https://openalex.org/W1977531436', 'https://openalex.org/W2962835968', 'https://openalex.org/W2920166246', 'https://openalex.org/W1983578042', 'https://openalex.org/W3082351043', 'https://openalex.org/W2842511635', 'https://openalex.org/W1905882502', 'https://openalex.org/W2147682057', 'https://openalex.org/W3035750922', 'https://openalex.org/W2117539524', 'https://openalex.org/W1980862600', 'https://openalex.org/W2123024445', 'https://openalex.org/W2964265128', 'https://openalex.org/W130754613', 'https://openalex.org/W2064877840', 'https://openalex.org/W2148764920', 'https://openalex.org/W2395899413', 'https://openalex.org/W3015300171', 'https://openalex.org/W2971709506', 'https://openalex.org/W1614298861', 'https://openalex.org/W2103091632', 'https://openalex.org/W2963902314', 'https://openalex.org/W1757417856', 'https://openalex.org/W2057256606', 'https://openalex.org/W2962753610']",2021-09-29
https://openalex.org/W4376140088,https://doi.org/10.31234/osf.io/nyqbm,"Computational Insights to Acquisition of Phonemes, Words, and Word Meanings in Early Language: Sequential or Parallel Acquisition?","Previous computational models of early language acquisition have shown how linguistic structure of speech can be acquired using auditory or audiovisual learning mechanisms. However, real infants have sustained access to both uni- and multimodal sensory experiences. Therefore, it is of interest how the uni- and multimodal learning mechanisms could operate in concert, and how their interplay might affect the acquisition dynamics of different linguistic representations. This paper explores these questions with a computational model capable of simultaneous auditory and audiovisual learning from speech and images. We study how the model’s latent representations reflect phonemic, lexical, and semantic knowledge as a function of language experience. We also test how the findings vary with differential emphasis on the two learning mechanisms. As a result, we find phonemic learning always starting to emerge before lexical learning, followed by semantics. However, there is also notable overlap in their development. The same pattern emerges irrespectively of the emphasis on auditory or audiovisual learning. The result illustrates how the acquisition dynamics of linguistic representations are decoupled from the primary learning objectives (mechanisms) of the learner, and how the emergence of phonemes and words can be facilitated by both auditory and audiovisual learning in a synergetic manner.","['https://openalex.org/W3157861865', 'https://openalex.org/W2586148577', 'https://openalex.org/W2553608650', 'https://openalex.org/W2146163948', 'https://openalex.org/W2130518563', 'https://openalex.org/W2396361046', 'https://openalex.org/W3114436296', 'https://openalex.org/W6684904482', 'https://openalex.org/W1905882502', 'https://openalex.org/W3164946614', 'https://openalex.org/W2103091632', 'https://openalex.org/W4390854704', 'https://openalex.org/W6639102338', 'https://openalex.org/W7011608486', 'https://openalex.org/W3163184902', 'https://openalex.org/W6801926475', 'https://openalex.org/W4221161768', 'https://openalex.org/W4224875474', 'https://openalex.org/W2415378728', 'https://openalex.org/W6645640999', 'https://openalex.org/W2395899413', 'https://openalex.org/W2059168261', 'https://openalex.org/W2039762981', 'https://openalex.org/W2089883580', 'https://openalex.org/W4200248493', 'https://openalex.org/W4250354678', 'https://openalex.org/W3036601975', 'https://openalex.org/W4297808394', 'https://openalex.org/W2481240925', 'https://openalex.org/W4286973758', 'https://openalex.org/W2170240579', 'https://openalex.org/W4230640548', 'https://openalex.org/W1980862600', 'https://openalex.org/W3174311593', 'https://openalex.org/W1861492603', 'https://openalex.org/W4246559809']",2023-05-10
https://openalex.org/W3211696375,,fairseq S^2: A Scalable and Integrable Speech Synthesis Toolkit,"This paper presents fairseq Sˆ2, a fairseq extension for speech synthesis. We implement a number of autoregressive (AR) and non-AR text-to-speech models, and their multi-speaker variants. To enable training speech synthesis models with less curated data, a number of preprocessing tools are built and their importance is shown empirically. To facilitate faster iteration of development and analysis, a suite of automatic metrics is included. Apart from the features added specifically for this extension, fairseq Sˆ2 also benefits from the scalability offered by fairseq and can be easily integrated with other state-of-the-art systems provided in this framework. The code, documentation, and pre-trained models will be made available at https://github.com/pytorch/fairseq/tree/master/examples/speech_synthesis.","['https://openalex.org/W3163906773', 'https://openalex.org/W2963712897', 'https://openalex.org/W3015826515', 'https://openalex.org/W2995680346', 'https://openalex.org/W2160473997', 'https://openalex.org/W3118578889', 'https://openalex.org/W3210177631', 'https://openalex.org/W2114925438', 'https://openalex.org/W3016160783', 'https://openalex.org/W2794490148', 'https://openalex.org/W3103029570', 'https://openalex.org/W2962691331', 'https://openalex.org/W3169320628', 'https://openalex.org/W116902681', 'https://openalex.org/W3008480565', 'https://openalex.org/W3033194228', 'https://openalex.org/W2963799213', 'https://openalex.org/W2883586237', 'https://openalex.org/W2970295111', 'https://openalex.org/W2933138175', 'https://openalex.org/W2130086727', 'https://openalex.org/W2970971581', 'https://openalex.org/W2598638573', 'https://openalex.org/W2527729766', 'https://openalex.org/W1524333225', 'https://openalex.org/W2963300588', 'https://openalex.org/W3024605872', 'https://openalex.org/W3099782249', 'https://openalex.org/W3140429000', 'https://openalex.org/W2996383576', 'https://openalex.org/W2107740512', 'https://openalex.org/W2903739847', 'https://openalex.org/W3130016944', 'https://openalex.org/W2963739817', 'https://openalex.org/W2400063444', 'https://openalex.org/W2747874407', 'https://openalex.org/W2120847449', 'https://openalex.org/W2970730223', 'https://openalex.org/W3174758275', 'https://openalex.org/W3112092703', 'https://openalex.org/W2962699523', 'https://openalex.org/W3097945073', 'https://openalex.org/W2129142580', 'https://openalex.org/W2964243274', 'https://openalex.org/W3095948607', 'https://openalex.org/W3114436296', 'https://openalex.org/W2963691546', 'https://openalex.org/W2471520273']",2021-09-14
https://openalex.org/W4388072808,https://doi.org/10.21437/sigul.2023-12,VGSAlign: Bilingual Speech Alignment of Unpaired and Untranscribed Languages using Self-Supervised Visually Grounded Speech Models,"Direct neural speech-to-speech translation (S2ST) systems enable translating speech from source to target languages without the need for text transcription.However, these systems are mostly trained using supervised learning that relies on a massive amount of parallel source-target speech data, which is often unavailable.This paper proposes a bilingual speech alignment approach called VGSAlign, as the initial solution for obtaining paired data from unknown, untranscribed, and unpaired speech data.Here, we assume the speech has auxiliary input from the visual modality that describes the semantic information.The approach then leverages the ability (1) to discover spoken words in multiple languages from the correspondences between speech segments and part of images based on self-supervised visually grounded speech models and (2) to find the visually grounded semantically equivalent between the spoken discovery of speech segments of source and target languages.By learning the representations of speech and images, VGSAlign shows the potential to achieve bilingual speech alignment based on visual representation.Furthermore, experimental results show that the proposed approach could work effectively with unknown, untranscribed, and unpaired speech without being trained on any supervised tasks.","['https://openalex.org/W6798937733', 'https://openalex.org/W2136545725', 'https://openalex.org/W3036601975', 'https://openalex.org/W3017535695', 'https://openalex.org/W6810419249', 'https://openalex.org/W4307006458', 'https://openalex.org/W6795952400', 'https://openalex.org/W2765961751', 'https://openalex.org/W2914120296', 'https://openalex.org/W4221167019', 'https://openalex.org/W4226061604', 'https://openalex.org/W4376632801', 'https://openalex.org/W2556930864', 'https://openalex.org/W2586148577', 'https://openalex.org/W2580178245', 'https://openalex.org/W2601713192', 'https://openalex.org/W3169320628', 'https://openalex.org/W4224875474', 'https://openalex.org/W2796156786', 'https://openalex.org/W4361866019', 'https://openalex.org/W4287555375', 'https://openalex.org/W2107695330', 'https://openalex.org/W2402515122', 'https://openalex.org/W2766730959', 'https://openalex.org/W3159481202', 'https://openalex.org/W2842511635', 'https://openalex.org/W2974393448', 'https://openalex.org/W3114436296', 'https://openalex.org/W2920166246', 'https://openalex.org/W2611064105', 'https://openalex.org/W1905882502', 'https://openalex.org/W1686810756', 'https://openalex.org/W3017604292', 'https://openalex.org/W4287854499', 'https://openalex.org/W2964001192', 'https://openalex.org/W4312528757', 'https://openalex.org/W4287173589', 'https://openalex.org/W4372266917', 'https://openalex.org/W2963778889', 'https://openalex.org/W2936184970', 'https://openalex.org/W3174311593', 'https://openalex.org/W4385893869', 'https://openalex.org/W3100806282', 'https://openalex.org/W2963330681', 'https://openalex.org/W1861492603', 'https://openalex.org/W4296070453', 'https://openalex.org/W4297808394', 'https://openalex.org/W4288595436', 'https://openalex.org/W2962753610', 'https://openalex.org/W2988907666', 'https://openalex.org/W3180521376', 'https://openalex.org/W4298393544', 'https://openalex.org/W4297841354']",2023-08-18
https://openalex.org/W4385573012,https://doi.org/10.18653/v1/2022.emnlp-main.108,SpeechUT: Bridging Speech and Text with Hidden-Unit for Encoder-Decoder Based Speech-Text Pre-training,"The rapid development of single-modal pre-training has prompted researchers to pay more attention to cross-modal pre-training methods. In this paper, we propose a unified-modal speech-unit-text pre-training model, SpeechUT, to connect the representations of a speech encoder and a text decoder with a shared unit encoder. Leveraging hidden-unit as an interface to align speech and text, we can decompose the speech-to-text model into a speech-to-unit model and a unit-to-text model, which can be jointly pre-trained with unpaired speech and text data respectively. Our proposed SpeechUT is fine-tuned and evaluated on automatic speech recognition (ASR) and speech translation (ST) tasks. Experimental results show that SpeechUT gets substantial improvements over strong baselines, and achieves state-of-the-art performance on both the LibriSpeech ASR and MuST-C ST tasks. To better understand the proposed SpeechUT, detailed analyses are conducted. The code and pre-trained models are available at https://aka.ms/SpeechUT.","['https://openalex.org/W3193521535', 'https://openalex.org/W2933138175', 'https://openalex.org/W2766219058', 'https://openalex.org/W2963925437', 'https://openalex.org/W2945260553', 'https://openalex.org/W4226278833', 'https://openalex.org/W3162313915', 'https://openalex.org/W4221145109', 'https://openalex.org/W3209059054', 'https://openalex.org/W2970597249', 'https://openalex.org/W3036601975', 'https://openalex.org/W2945700568', 'https://openalex.org/W3160525311', 'https://openalex.org/W4224934179', 'https://openalex.org/W2739883972', 'https://openalex.org/W2187089797', 'https://openalex.org/W4320194748', 'https://openalex.org/W4221163209', 'https://openalex.org/W3205644108', 'https://openalex.org/W3207222250', 'https://openalex.org/W2991213871', 'https://openalex.org/W1915251500', 'https://openalex.org/W4300980246', 'https://openalex.org/W2963799213', 'https://openalex.org/W4282958795', 'https://openalex.org/W4226120743', 'https://openalex.org/W4287854499', 'https://openalex.org/W3093579165', 'https://openalex.org/W1494198834', 'https://openalex.org/W2995181338', 'https://openalex.org/W4287329822', 'https://openalex.org/W3175963743', 'https://openalex.org/W4223622550', 'https://openalex.org/W3209984917', 'https://openalex.org/W4283324001', 'https://openalex.org/W2979476256', 'https://openalex.org/W3034999214', 'https://openalex.org/W2127141656', 'https://openalex.org/W4287079508', 'https://openalex.org/W4287890956', 'https://openalex.org/W3209371554', 'https://openalex.org/W3176382501', 'https://openalex.org/W3148001440', 'https://openalex.org/W3161302809', 'https://openalex.org/W4221155340', 'https://openalex.org/W4226033575', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963341956']",2022-01-01
https://openalex.org/W4392979802,https://doi.org/10.1109/taslp.2024.3379877,SpeechLM: Enhanced Speech Pre-Training With Unpaired Textual Data,"How to boost speech pre-training with textual data is an unsolved problem due to the fact that speech and text are very different modalities with distinct characteristics. In this paper, we propose a cross-modal <bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">Speech</b> and <bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">L</b> anguage <bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">M</b> odel ( <bold xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">SpeechLM</b> ) to explicitly align speech and text pre-training with a pre-defined unified discrete representation. Specifically, we introduce two alternative discrete tokenizers to bridge the speech and text modalities, including phoneme-unit and hidden-unit tokenizers, which can be trained using unpaired speech or a small amount of paired speech-text data. Based on the trained tokenizers, we convert the unlabeled speech and text data into tokens of phoneme units or hidden units. The pre-training objective is designed to unify the speech and the text into the same discrete semantic space with a unified Transformer network. We evaluate SpeechLM on various spoken language processing tasks including speech recognition, speech translation, and universal representation evaluation framework SUPERB, demonstrating significant improvements on content-related tasks. Code and models are available at <uri xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">https://aka.ms/SpeechLM.</uri>","['https://openalex.org/W2896457183', 'https://openalex.org/W6762122294', 'https://openalex.org/W2973049979', 'https://openalex.org/W6796554684', 'https://openalex.org/W2509528580', 'https://openalex.org/W6803092890', 'https://openalex.org/W6810259195', 'https://openalex.org/W4226120743', 'https://openalex.org/W3011411500', 'https://openalex.org/W3197580070', 'https://openalex.org/W6769196770', 'https://openalex.org/W4226033575', 'https://openalex.org/W4224934179', 'https://openalex.org/W4283324001', 'https://openalex.org/W3161302809', 'https://openalex.org/W3162313915', 'https://openalex.org/W6802465204', 'https://openalex.org/W6810252168', 'https://openalex.org/W6846290820', 'https://openalex.org/W4319862474', 'https://openalex.org/W6847568899', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963925437', 'https://openalex.org/W2127141656', 'https://openalex.org/W3209371554', 'https://openalex.org/W2046932483', 'https://openalex.org/W6631362777', 'https://openalex.org/W6763832098', 'https://openalex.org/W1494198834', 'https://openalex.org/W2995181338', 'https://openalex.org/W3196509775', 'https://openalex.org/W6771467084', 'https://openalex.org/W6795952400', 'https://openalex.org/W4392909760', 'https://openalex.org/W3209984917', 'https://openalex.org/W6631190155', 'https://openalex.org/W6780218876', 'https://openalex.org/W6810007534', 'https://openalex.org/W4297841852', 'https://openalex.org/W3198299542', 'https://openalex.org/W2101105183', 'https://openalex.org/W3015213852', 'https://openalex.org/W2972943112', 'https://openalex.org/W3097286738', 'https://openalex.org/W3198858531', 'https://openalex.org/W2982223350', 'https://openalex.org/W3041561163', 'https://openalex.org/W3015265920', 'https://openalex.org/W3016181583', 'https://openalex.org/W3163142165', 'https://openalex.org/W2033875152', 'https://openalex.org/W1524333225', 'https://openalex.org/W1522301498', 'https://openalex.org/W3207222250', 'https://openalex.org/W2187089797', 'https://openalex.org/W4221155340', 'https://openalex.org/W4221145109']",2024-01-01
https://openalex.org/W4375869113,https://doi.org/10.1109/icassp49357.2023.10094922,CTCBERT: Advancing Hidden-Unit Bert with CTC Objectives,"In this work, we present a simple but effective method, CTCBERT, for advancing hidden-unit BERT (HuBERT). HuBERT applies a frame-level cross-entropy (CE) loss, which is similar to most acoustic model training. However, CTCBERT performs the model training with the Connectionist Temporal Classification (CTC) objective after removing duplicated IDs in each masked region. The idea stems from the observation that there can be significant errors in alignments when using clustered or aligned IDs. CTC learns alignments implicitly, indicating that learning with CTC can be more flexible when misalignment exists. We examine CTCBERT on IDs from HuBERT Iter1, HuBERT Iter2, and PBERT. The CTC training brings consistent improvements compared to the CE training. Furthermore, when loading blank-related parameters during finetuning, slight improvements are observed. Evaluated on the Librispeech 960-100h setting, the relative WER improvements of CTCBERT are 2%-11% over HuBERT and PERT on test-other data.","['https://openalex.org/W2972943112', 'https://openalex.org/W6755207826', 'https://openalex.org/W3097286738', 'https://openalex.org/W3016011332', 'https://openalex.org/W2933138175', 'https://openalex.org/W2953190524', 'https://openalex.org/W4293793697', 'https://openalex.org/W4226507725', 'https://openalex.org/W4226278833', 'https://openalex.org/W2982223350', 'https://openalex.org/W3015265920', 'https://openalex.org/W3160345865', 'https://openalex.org/W3041561163', 'https://openalex.org/W2127141656', 'https://openalex.org/W6838909421', 'https://openalex.org/W1494198834', 'https://openalex.org/W2889068726', 'https://openalex.org/W2973049979', 'https://openalex.org/W3209059054', 'https://openalex.org/W6780218876', 'https://openalex.org/W2514741789', 'https://openalex.org/W2888867175', 'https://openalex.org/W6757817989', 'https://openalex.org/W3204696009', 'https://openalex.org/W4297841844', 'https://openalex.org/W3209984917', 'https://openalex.org/W4224934179', 'https://openalex.org/W4281672148', 'https://openalex.org/W4281492411', 'https://openalex.org/W4287889722', 'https://openalex.org/W2908510526', 'https://openalex.org/W4283324001', 'https://openalex.org/W2896457183', 'https://openalex.org/W3036601975']",2023-05-05
https://openalex.org/W4391021652,https://doi.org/10.1109/asru57964.2023.10389778,Fast-Hubert: an Efficient Training Framework for Self-Supervised Speech Representation Learning,"Recent years have witnessed significant advancements in self-supervised learning (SSL) methods for speech-processing tasks. Various speech-based SSL models have been developed and present promising performance on a range of downstream tasks including speech recognition. However, existing speech-based SSL models face a common dilemma in terms of computational cost, which might hinder their potential application and in-depth academic research. To address this issue, we first analyze the computational cost of different modules during HuBERT pre-training and then introduce a stack of efficiency optimizations, which is named Fast-HuBERT in this paper. The proposed Fast-HuBERT can be trained in 1.1 days with 8 V100 GPUs on the Librispeech 960 h benchmark, without performance degradation, resulting in a 5.2x speedup, compared to the original implementation. Moreover, we explore two well-studied techniques in the Fast-HuBERT and demonstrate consistent improvements as reported in previous work. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> The code for Fast-HuBERT training is available at https://github.com/yanghaha0908/FastHuBERT","['https://openalex.org/W3211278025', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W4385823092', 'https://openalex.org/W4385822668', 'https://openalex.org/W3197580070', 'https://openalex.org/W4372260292', 'https://openalex.org/W3206189675', 'https://openalex.org/W6856240984', 'https://openalex.org/W4385823152', 'https://openalex.org/W4224934179', 'https://openalex.org/W4226507725', 'https://openalex.org/W4283324001', 'https://openalex.org/W4375869113', 'https://openalex.org/W4319862670', 'https://openalex.org/W6795952400', 'https://openalex.org/W1494198834', 'https://openalex.org/W6846807982', 'https://openalex.org/W6810007534', 'https://openalex.org/W2936774411', 'https://openalex.org/W6739901393', 'https://openalex.org/W2127141656', 'https://openalex.org/W6601894380', 'https://openalex.org/W2933138175', 'https://openalex.org/W4392904157', 'https://openalex.org/W3036601975', 'https://openalex.org/W46679369', 'https://openalex.org/W4287173589', 'https://openalex.org/W4385245566']",2023-12-16
https://openalex.org/W4372348115,https://doi.org/10.1109/icassp49357.2023.10094787,Self-Supervised Learning with Bi-Label Masked Speech Prediction for Streaming Multi-Talker Speech Recognition,"Self-supervised learning (SSL), which utilizes the input data itself for representation learning, has achieved state-of-the-art results for various downstream speech tasks. However, most of the previous studies focused on offline single-talker applications, with limited investigations in multi-talker cases, especially for streaming scenarios. In this paper, we investigate SSL for streaming multi-talker speech recognition, which generates transcriptions of overlapping speakers in a streaming fashion. Firstly, we observe that conventional SSL techniques do not work well on this task due to the poor representation of overlapping speech. We then propose a novel SSL training objective, referred to as bi-label masked speech prediction, which explicitly preserves representations of all speakers in overlapping speech. We investigate various aspects of the proposed system, including data configuration and quantizer selection. The proposed SSL setup achieves substantially better word error rates on the LibriSpeechMix dataset.","['https://openalex.org/W2973049979', 'https://openalex.org/W2972943112', 'https://openalex.org/W4226491018', 'https://openalex.org/W6786669483', 'https://openalex.org/W3016181583', 'https://openalex.org/W2842511635', 'https://openalex.org/W3197580070', 'https://openalex.org/W4281492411', 'https://openalex.org/W2896457183', 'https://openalex.org/W6775489429', 'https://openalex.org/W3016010032', 'https://openalex.org/W3211278025', 'https://openalex.org/W4283324001', 'https://openalex.org/W3116025434', 'https://openalex.org/W1494198834', 'https://openalex.org/W6784418856', 'https://openalex.org/W3000358149', 'https://openalex.org/W6718827390', 'https://openalex.org/W3197642003', 'https://openalex.org/W3206189675', 'https://openalex.org/W262275730', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209984917', 'https://openalex.org/W3198275944', 'https://openalex.org/W3209059054', 'https://openalex.org/W2460742184', 'https://openalex.org/W3036601975', 'https://openalex.org/W4297808394', 'https://openalex.org/W3094821064', 'https://openalex.org/W3112034174', 'https://openalex.org/W3161873870']",2023-05-05
https://openalex.org/W4319323883,https://doi.org/10.1109/slt54892.2023.10022676,Guided Contrastive Self-Supervised Pre-Training for Automatic Speech Recognition,"Contrastive Predictive Coding (CPC) is a representation learning method that maximizes the mutual information between intermediate latent representations and the output of a given model. It can be used to effectively initialize the encoder of an Automatic Speech Recognition (ASR) model. We present a novel modification of CPC called Guided Contrastive Predictive Coding (GCPC). Our proposed method maximizes the mutual information between representations from a prior-knowledge model and the output of the model being pre-trained, allowing prior knowledge injection during pre-training. We validate our method on 3 ASR tasks: German, French and English. Our method outperforms CPC pre-training on all three datasets, reducing the Word Error Rate (WER) by 4.44%, 6.55% and 15.43% relative on the German, French and English (Librispeech) tasks respectively, compared to training from scratch, while CPC pre-training only brings 2.96%, 1.01% and 14.39% relative WER reduction respectively.","['https://openalex.org/W6755207826', 'https://openalex.org/W2962739339', 'https://openalex.org/W2951974815', 'https://openalex.org/W3100270690', 'https://openalex.org/W2972943112', 'https://openalex.org/W2982223350', 'https://openalex.org/W3015265920', 'https://openalex.org/W3024182269', 'https://openalex.org/W3209059054', 'https://openalex.org/W2842511635', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W6780218876', 'https://openalex.org/W4226033575', 'https://openalex.org/W4283324001', 'https://openalex.org/W2127141656', 'https://openalex.org/W6727336983', 'https://openalex.org/W6788335241', 'https://openalex.org/W6797992114', 'https://openalex.org/W3160235762', 'https://openalex.org/W3212799896', 'https://openalex.org/W6638749077', 'https://openalex.org/W6682948231', 'https://openalex.org/W1494198834', 'https://openalex.org/W2995181338', 'https://openalex.org/W2963250244', 'https://openalex.org/W4297808394', 'https://openalex.org/W2187089797', 'https://openalex.org/W2520160253', 'https://openalex.org/W3178203035', 'https://openalex.org/W1828163288', 'https://openalex.org/W3036601975']",2023-01-09
https://openalex.org/W4319862631,https://doi.org/10.1109/slt54892.2023.10023214,ASBERT: ASR-Specific Self-Supervised Learning with Self-Training,"Pre-training of self-supervised learning (SSL) generally shows a good performance on various speech processing tasks. However, this pre-training scheme may lead to a sub-optimal solution for fine-tuning a specific task, such as automatic speech recognition (ASR). In order to provide a more optimal pre-trained model for ASR, we introduce an ASR-Specific hidden-unit BERT with self-training, namely ASBERT. Motivated by self-training, we extract linguistic-related pseudo labels from the fine-tuned model, and these labels are used in the next pre-training procedure. Experimental results on LibriSpeech test-clean and test-other datasets show that ASBERT without language model (LM) outperforms the conventional SSL and self-training model, achieving a 6.3/2.0% and 15.4/13.2% relatively word error rate reduction (RERR). Moreover, without using pseudo-transcription, ASBERT yields comparable performance to the conventional self-training method.","['https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W6755207826', 'https://openalex.org/W3197580070', 'https://openalex.org/W4226033575', 'https://openalex.org/W4221145109', 'https://openalex.org/W6844194202', 'https://openalex.org/W6769196770', 'https://openalex.org/W2973049979', 'https://openalex.org/W6810673746', 'https://openalex.org/W3026041220', 'https://openalex.org/W6784614252', 'https://openalex.org/W4297841337', 'https://openalex.org/W2127141656', 'https://openalex.org/W4224934179', 'https://openalex.org/W4283324001', 'https://openalex.org/W6629717138', 'https://openalex.org/W6771812881', 'https://openalex.org/W6631190155', 'https://openalex.org/W6947929050', 'https://openalex.org/W2933138175', 'https://openalex.org/W3163169798', 'https://openalex.org/W2979476256', 'https://openalex.org/W1522301498', 'https://openalex.org/W2896457183']",2023-01-09
https://openalex.org/W4404938578,https://doi.org/10.32388/2c9tpu,k2SSL: A Faster and Better Framework for Self-Supervised Speech Representation Learning,"Self-supervised learning (SSL) has achieved great success in speech-related tasks, driven by advancements in speech encoder architectures and the expansion of datasets. While Transformer and Conformer architectures have dominated SSL backbones, encoders like Zipformer, which excel in automatic speech recognition (ASR), remain unexplored in SSL. Concurrently, inefficiencies in data processing within existing SSL training frameworks, such as fairseq, pose challenges in managing the growing volumes of training data. To address these issues, we propose k2SSL, an open-source framework that offers faster, more memory-efficient, and better-performing self-supervised speech representation learning, with a focus on downstream ASR tasks. The optimized HuBERT and proposed Zipformer-based SSL systems exhibit substantial reductions in both training time and memory usage during SSL training. Experiments on LibriSpeech and Libri-Light demonstrate that Zipformer-based SSL systems significantly outperform comparable HuBERT and WavLM systems, achieving a relative WER reduction on dev-other/test-other of up to 34.8%/32.4% compared to HuBERT Base after supervised fine-tuning, along with a 3.5x pre-training speedup in total GPU hours.","['https://openalex.org/W2973049979', 'https://openalex.org/W3036601975', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W4221145109', 'https://openalex.org/W4311724836', 'https://openalex.org/W4281492411', 'https://openalex.org/W3211278025', 'https://openalex.org/W6739901393', 'https://openalex.org/W3197580070', 'https://openalex.org/W4391833199', 'https://openalex.org/W4382603054', 'https://openalex.org/W6852909395', 'https://openalex.org/W4311000453', 'https://openalex.org/W4323066695', 'https://openalex.org/W4392903704', 'https://openalex.org/W4399795275', 'https://openalex.org/W4402112280', 'https://openalex.org/W2933138175', 'https://openalex.org/W2962780374', 'https://openalex.org/W4387799863', 'https://openalex.org/W1494198834', 'https://openalex.org/W2995181338', 'https://openalex.org/W4385823192', 'https://openalex.org/W4391021746', 'https://openalex.org/W4391021652', 'https://openalex.org/W4226507725', 'https://openalex.org/W4392909760', 'https://openalex.org/W4375869113', 'https://openalex.org/W4283324001', 'https://openalex.org/W4385823152', 'https://openalex.org/W6849880362', 'https://openalex.org/W2127141656', 'https://openalex.org/W4283700324', 'https://openalex.org/W2143612262', 'https://openalex.org/W3208887030', 'https://openalex.org/W2962784628', 'https://openalex.org/W4375869165', 'https://openalex.org/W2953190524', 'https://openalex.org/W3016010032', 'https://openalex.org/W3097777922', 'https://openalex.org/W3093579165']",2024-12-01
https://openalex.org/W4321009928,https://doi.org/10.1145/3544548.3581465,LipLearner: Customizable Silent Speech Interactions on Mobile Devices,"Silent speech interface is a promising technology that enables private communications in natural language. However, previous approaches only support a small and inflexible vocabulary, which leads to limited expressiveness. We leverage contrastive learning to learn efficient lipreading representations, enabling few-shot command customization with minimal user effort. Our model exhibits high robustness to different lighting, posture, and gesture conditions on an in-the-wild dataset. For 25-command classification, an F1-score of 0.8947 is achievable only using one shot, and its performance can be further boosted by adaptively learning from more data. This generalizability allowed us to develop a mobile silent speech interface empowered with on-device fine-tuning and visual keyword spotting. A user study demonstrated that with LipLearner, users could define their own commands with high reliability guaranteed by an online incremental learning scheme. Subjective feedback indicated that our system provides essential functionalities for customizable silent speech interactions with high usability and learnability.","['https://openalex.org/W2952609044', 'https://openalex.org/W2963115079', 'https://openalex.org/W2032191478', 'https://openalex.org/W3093564344', 'https://openalex.org/W3145450063', 'https://openalex.org/W2108033344', 'https://openalex.org/W2099257899', 'https://openalex.org/W2896622783', 'https://openalex.org/W3182657421', 'https://openalex.org/W2322064186', 'https://openalex.org/W2110015077', 'https://openalex.org/W3177183117', 'https://openalex.org/W3206248339', 'https://openalex.org/W1995735739', 'https://openalex.org/W4220871331', 'https://openalex.org/W2963272864', 'https://openalex.org/W2793257307', 'https://openalex.org/W2087681821', 'https://openalex.org/W4225124540', 'https://openalex.org/W3091762102', 'https://openalex.org/W2941048526', 'https://openalex.org/W2115252128', 'https://openalex.org/W1578725541', 'https://openalex.org/W2921295135', 'https://openalex.org/W3016011581', 'https://openalex.org/W2806172474', 'https://openalex.org/W3160833628', 'https://openalex.org/W2963654155', 'https://openalex.org/W3035626590', 'https://openalex.org/W2909141918', 'https://openalex.org/W3135367836', 'https://openalex.org/W2981852735', 'https://openalex.org/W3007912670', 'https://openalex.org/W2013317861', 'https://openalex.org/W2808135558', 'https://openalex.org/W2963680395', 'https://openalex.org/W3207628370', 'https://openalex.org/W2897318954', 'https://openalex.org/W2056852181', 'https://openalex.org/W2156060621', 'https://openalex.org/W1540787848', 'https://openalex.org/W2149034627', 'https://openalex.org/W4224926225', 'https://openalex.org/W3031554797', 'https://openalex.org/W4220783478', 'https://openalex.org/W4200482297', 'https://openalex.org/W2039640471', 'https://openalex.org/W3164825066', 'https://openalex.org/W4292779060', 'https://openalex.org/W4288327876', 'https://openalex.org/W4287392530', 'https://openalex.org/W4288089799', 'https://openalex.org/W2394613688', 'https://openalex.org/W19399978', 'https://openalex.org/W3166396011', 'https://openalex.org/W3095897738', 'https://openalex.org/W1488659701', 'https://openalex.org/W3157582158', 'https://openalex.org/W4297808394', 'https://openalex.org/W3103322708', 'https://openalex.org/W2963741406']",2023-04-19
https://openalex.org/W4390943488,https://doi.org/10.3390/app14020798,Lip2Speech: Lightweight Multi-Speaker Speech Reconstruction with Gabor Features,"In environments characterised by noise or the absence of audio signals, visual cues, notably facial and lip movements, serve as valuable substitutes for missing or corrupted speech signals. In these scenarios, speech reconstruction can potentially generate speech from visual data. Recent advancements in this domain have predominantly relied on end-to-end deep learning models, like Convolutional Neural Networks (CNN) or Generative Adversarial Networks (GAN). However, these models are encumbered by their intricate and opaque architectures, coupled with their lack of speaker independence. Consequently, achieving multi-speaker speech reconstruction without supplementary information is challenging. This research introduces an innovative Gabor-based speech reconstruction system tailored for lightweight and efficient multi-speaker speech restoration. Using our Gabor feature extraction technique, we propose two novel models: GaborCNN2Speech and GaborFea2Speech. These models employ a rapid Gabor feature extraction method to derive lowdimensional mouth region features, encompassing filtered Gabor mouth images and low-dimensional Gabor features as visual inputs. An encoded spectrogram serves as the audio target, and a Long Short-Term Memory (LSTM)-based model is harnessed to generate coherent speech output. Through comprehensive experiments conducted on the GRID corpus, our proposed Gabor-based models have showcased superior performance in sentence and vocabulary reconstruction when compared to traditional end-to-end CNN models. These models stand out for their lightweight design and rapid processing capabilities. Notably, the GaborFea2Speech model presented in this study achieves robust multi-speaker speech reconstruction without necessitating supplementary information, thereby marking a significant milestone in the field of speech reconstruction.","['https://openalex.org/W2585824449', 'https://openalex.org/W2964352155', 'https://openalex.org/W2913906888', 'https://openalex.org/W2061698928', 'https://openalex.org/W2625027024', 'https://openalex.org/W3110258267', 'https://openalex.org/W2481971738', 'https://openalex.org/W2788241093', 'https://openalex.org/W3157356148', 'https://openalex.org/W3136499730', 'https://openalex.org/W4313188954', 'https://openalex.org/W2293856338', 'https://openalex.org/W2739009240', 'https://openalex.org/W1963897898', 'https://openalex.org/W3035626590', 'https://openalex.org/W6803359641', 'https://openalex.org/W4206204999', 'https://openalex.org/W4224926225', 'https://openalex.org/W2963019222', 'https://openalex.org/W2152859600', 'https://openalex.org/W2972563022', 'https://openalex.org/W3157840621', 'https://openalex.org/W3160305627', 'https://openalex.org/W3213322812', 'https://openalex.org/W6838983091', 'https://openalex.org/W4312433898', 'https://openalex.org/W4285019302', 'https://openalex.org/W4294435344', 'https://openalex.org/W2972852451', 'https://openalex.org/W4200635083', 'https://openalex.org/W4292773472', 'https://openalex.org/W4375868850', 'https://openalex.org/W2910267084', 'https://openalex.org/W2887437849', 'https://openalex.org/W6766619525', 'https://openalex.org/W6756520038', 'https://openalex.org/W2973068670', 'https://openalex.org/W2093680491', 'https://openalex.org/W2040851113', 'https://openalex.org/W2588908057', 'https://openalex.org/W2172803778', 'https://openalex.org/W2031614119', 'https://openalex.org/W2789876780', 'https://openalex.org/W4233692629', 'https://openalex.org/W6677618333', 'https://openalex.org/W1914401667', 'https://openalex.org/W6675227275', 'https://openalex.org/W2011282943', 'https://openalex.org/W3047238456', 'https://openalex.org/W2404926044', 'https://openalex.org/W2950965506', 'https://openalex.org/W2054139811', 'https://openalex.org/W3006908481', 'https://openalex.org/W2015143272', 'https://openalex.org/W2605516844', 'https://openalex.org/W1677182931', 'https://openalex.org/W6674330103', 'https://openalex.org/W2122669551', 'https://openalex.org/W1552314771', 'https://openalex.org/W2144404214', 'https://openalex.org/W2108305916', 'https://openalex.org/W3111737660', 'https://openalex.org/W2792641098', 'https://openalex.org/W2115252128', 'https://openalex.org/W3211862173', 'https://openalex.org/W4286432612', 'https://openalex.org/W2964404767', 'https://openalex.org/W4283809657', 'https://openalex.org/W2095705004', 'https://openalex.org/W3101481642']",2024-01-17
https://openalex.org/W4387969124,https://doi.org/10.1145/3581783.3613825,Face-Driven Zero-Shot Voice Conversion with Memory-based Face-Voice Alignment,"This paper presents a novel task, zero-shot voice conversion based on face images (zero-shot FaceVC), which aims at converting the voice characteristics of an utterance from any source speaker to a newly coming target speaker, solely relying on a single face image of the target speaker. To address this task, we propose a face-voice memory-based zero-shot FaceVC method. This method leverages a memory-based face-voice alignment module, in which slots act as the bridge to align these two modalities, allowing for the capture of voice characteristics from face images. A mixed supervision strategy is also introduced to mitigate the long-standing issue of the inconsistency between training and inference phases for voice conversion tasks. To obtain speaker-independent content-related representations, we transfer the knowledge from a pretrained zero-shot voice conversion model to our zero-shot FaceVC model. Considering the differences between FaceVC and traditional voice conversion tasks, systematic subjective and objective metrics are designed to thoroughly evaluate the homogeneity, diversity and consistency of voice characteristics controlled by face images. Through extensive experiments, we demonstrate the superiority of our proposed method on the zero-shot FaceVC task. Samples are presented on our demo website.","['https://openalex.org/W4287759306', 'https://openalex.org/W2935850529', 'https://openalex.org/W2944294033', 'https://openalex.org/W4224928197', 'https://openalex.org/W2800448574', 'https://openalex.org/W3096806995', 'https://openalex.org/W2066078528', 'https://openalex.org/W2963539064', 'https://openalex.org/W2946555236', 'https://openalex.org/W2902070858', 'https://openalex.org/W3211862173', 'https://openalex.org/W6804125026', 'https://openalex.org/W4225329057', 'https://openalex.org/W3206945595', 'https://openalex.org/W2077881910', 'https://openalex.org/W2576309025', 'https://openalex.org/W2972399707', 'https://openalex.org/W2979157532', 'https://openalex.org/W4283818626', 'https://openalex.org/W4210489562', 'https://openalex.org/W3035626590', 'https://openalex.org/W2804998325', 'https://openalex.org/W2096733369', 'https://openalex.org/W3098557217', 'https://openalex.org/W2266401008', 'https://openalex.org/W3000145882', 'https://openalex.org/W3095361818', 'https://openalex.org/W2962788625', 'https://openalex.org/W4224926225', 'https://openalex.org/W4224920206', 'https://openalex.org/W3015209900', 'https://openalex.org/W4221144911', 'https://openalex.org/W4220653738', 'https://openalex.org/W3015338123', 'https://openalex.org/W4296068587', 'https://openalex.org/W4313196646', 'https://openalex.org/W4382202825', 'https://openalex.org/W2996414377', 'https://openalex.org/W2341528187', 'https://openalex.org/W2963290645', 'https://openalex.org/W3186090335', 'https://openalex.org/W3101689408', 'https://openalex.org/W3101998545', 'https://openalex.org/W3214606349']",2023-10-26
https://openalex.org/W4372348072,https://doi.org/10.1109/icassp49357.2023.10096464,Zero-Shot Personalized Lip-To-Speech Synthesis with Face Image Based Voice Control,"Lip-to-Speech (Lip2Speech) synthesis, which predicts corresponding speech from talking face images, has witnessed significant progress with various models and training strategies in a series of independent studies. However, existing studies can not achieve voice control under zero-shot condition, because extra speaker embeddings need to be extracted from natural reference speech and are unavailable when only the silent video of an unseen speaker is given. In this paper, we propose a zero-shot personalized Lip2Speech synthesis method, in which face images control speaker identities. A variational autoencoder is adopted to disentangle the speaker identity and linguistic content representations, which enables speaker embeddings to control the voice characteristics of synthetic speech for unseen speakers. Furthermore, we propose associated cross-modal representation learning to promote the ability of face-based speaker embeddings (FSE) on voice control. Extensive experiments verify the effectiveness of the proposed method whose synthetic utterances are more natural and matching with the personality of input video than the compared methods. To our best knowledge, this paper makes the first attempt on zero-shot personalized Lip2Speech synthesis with a face image rather than reference audio to control voice characteristics.","['https://openalex.org/W3097777922', 'https://openalex.org/W3016011581', 'https://openalex.org/W3096650361', 'https://openalex.org/W2964350391', 'https://openalex.org/W2916104401', 'https://openalex.org/W6677618333', 'https://openalex.org/W3162707322', 'https://openalex.org/W3163271138', 'https://openalex.org/W4225299282', 'https://openalex.org/W3035626590', 'https://openalex.org/W4283809657', 'https://openalex.org/W2808631503', 'https://openalex.org/W2138621090', 'https://openalex.org/W6631190155', 'https://openalex.org/W2963839617', 'https://openalex.org/W2015143272', 'https://openalex.org/W4224926225', 'https://openalex.org/W2532494225', 'https://openalex.org/W3157840621', 'https://openalex.org/W6803359641', 'https://openalex.org/W4296069328', 'https://openalex.org/W2972563022', 'https://openalex.org/W2187089797', 'https://openalex.org/W1522301498', 'https://openalex.org/W3211862173', 'https://openalex.org/W2115252128']",2023-05-05
https://openalex.org/W4388117482,https://doi.org/10.23919/eusipco58844.2023.10290115,Facetron: A Multi-Speaker Face-to-Speech Model Based on Cross-Modal Latent Representations,"In this paper, we propose a multi-speaker face-to-speech waveform generation model that also works for unseen speaker conditions. Using a generative adversarial network (GAN) with linguistic and speaker characteristic features as auxiliary conditions, our method directly converts face images into speech waveforms under an end-to-end training framework. The linguistic features are extracted from lip movements using a lip-reading model, and the speaker characteristic features are predicted from face images using a face encoder trained through cross-modal learning with a pre-trained acoustic model. Since these two features are uncorrelated and controlled independently, we can flexibly synthesize speech waveforms whose speaker characteristics vary depending on the input face images. We show the superiority of our proposed model over conventional methods in terms of objective and subjective evaluation results.","['https://openalex.org/W6783867762', 'https://openalex.org/W2979157532', 'https://openalex.org/W2096733369', 'https://openalex.org/W2127141656', 'https://openalex.org/W6810209978', 'https://openalex.org/W2964352155', 'https://openalex.org/W2471520273', 'https://openalex.org/W2578229578', 'https://openalex.org/W2015143272', 'https://openalex.org/W3015841875', 'https://openalex.org/W6785471559', 'https://openalex.org/W2327501763', 'https://openalex.org/W2963936489', 'https://openalex.org/W3015645837', 'https://openalex.org/W3096650361', 'https://openalex.org/W3035626590', 'https://openalex.org/W2972563022', 'https://openalex.org/W2585824449', 'https://openalex.org/W3096806995', 'https://openalex.org/W3092028330', 'https://openalex.org/W3097206152', 'https://openalex.org/W4224926225', 'https://openalex.org/W4298112588', 'https://openalex.org/W2952746495']",2023-09-04
https://openalex.org/W4388821341,https://doi.org/10.1109/apsipaasc58517.2023.10317357,RobustL2S: Speaker-Specific Lip-to-Speech Synthesis exploiting Self-Supervised Representations,"Significant progress has been made in speaker-dependent Lip-to-Speech synthesis, which aims to generate speech from silent videos of talking faces. Current state-of-the-art approaches primarily employ non-autoregressive sequence-to-sequence architectures to directly predict mel-spectrograms or audio waveforms from lip representations. We hypothesize that the direct mel-prediction hampers training/model efficiency due to the entanglement of speech content with ambient information and speaker characteristics. To this end, we propose RobustL2S, a modularized framework for Lip-to-Speech synthesis. First, a non-autoregressive sequence-to-sequence model maps self-supervised visual features to a representation of disentangled speech content. A vocoder then converts the speech features into raw waveforms. Extensive evaluations confirm the effectiveness of our setup, achieving state-of-the-art performance on the unconstrained Lip2Wav dataset and the constrained GRID and TCD-TIMIT datasets. Speech samples from RobustL2S can be found at https://neha-sherin.github.io/RobustL2S/","['https://openalex.org/W4321009928', 'https://openalex.org/W4386057728', 'https://openalex.org/W4292266560', 'https://openalex.org/W2981905048', 'https://openalex.org/W3035626590', 'https://openalex.org/W2014621385', 'https://openalex.org/W4220668493', 'https://openalex.org/W4385823403', 'https://openalex.org/W2015143272', 'https://openalex.org/W2029199293', 'https://openalex.org/W4285019302', 'https://openalex.org/W4224926225', 'https://openalex.org/W2585824449', 'https://openalex.org/W2963019222', 'https://openalex.org/W2964352155', 'https://openalex.org/W3160305627', 'https://openalex.org/W2972563022', 'https://openalex.org/W3157840621', 'https://openalex.org/W6803359641', 'https://openalex.org/W4296069328', 'https://openalex.org/W4200635083', 'https://openalex.org/W4375868850', 'https://openalex.org/W4372348072', 'https://openalex.org/W2964243274', 'https://openalex.org/W4283809657', 'https://openalex.org/W6752910514', 'https://openalex.org/W4312433898', 'https://openalex.org/W3213322812', 'https://openalex.org/W3140429000', 'https://openalex.org/W4226132755', 'https://openalex.org/W4386076005', 'https://openalex.org/W3209059054', 'https://openalex.org/W6810168380', 'https://openalex.org/W2194775991', 'https://openalex.org/W2963785710', 'https://openalex.org/W4385245566', 'https://openalex.org/W6778823374', 'https://openalex.org/W6783867762', 'https://openalex.org/W3096650361', 'https://openalex.org/W2949662773', 'https://openalex.org/W6847363464', 'https://openalex.org/W3180374548', 'https://openalex.org/W2141998673', 'https://openalex.org/W2516001803', 'https://openalex.org/W3203789665', 'https://openalex.org/W2898035430', 'https://openalex.org/W3104792420', 'https://openalex.org/W3211862173', 'https://openalex.org/W4289761690', 'https://openalex.org/W4225511340', 'https://openalex.org/W4311000453', 'https://openalex.org/W3033411150', 'https://openalex.org/W4221153068', 'https://openalex.org/W3092028330']",2023-10-31
https://openalex.org/W4396804723,https://doi.org/10.14232/phd.11818,Improvements of Silent Speech Interface Algorithms,"Speech is a vital mode of communication, but for some individuals, speaking out loud may not be an option.Silent speech interfaces are a promising technology that allows for speech generation from articulatory signals, enabling individuals who are unable to speak to communicate.The focus of this topic is on the development and improvement of silent speech interfaces, which involves generating speech from data gathered from the movement of the tongue, lips, and jaw.To explore this topic, I have planned a comprehensive agenda that covers various aspects of silent speech interface development.The agenda includes the following topics:Preparing Data: This topic covers the collection and processing of data from the articulatory movements of a speaker.It includes data acquisition techniques such as Electromagnetic Articulography (EMA), Ultrasound Imaging, and Magnetic Resonance Imaging (MRI).Different Model Implementations: This topic covers various models that have been implemented for generating speech from articulatory signals, such as deep neural networks, support vector machines, and Hidden Markov Models.New Evaluation Metrics: This topic covers the development of new evaluation metrics for assessing the performance of silent speech interfaces.These metrics aim to provide a more accurate measure of speech quality and intelligibility, as traditional metrics may not be suitable for evaluating speech generated from articulatory signals.Model Improvement: This topic covers various techniques for improving the performance of silent speech interfaces, such as regularization, transfer learning, and data augmentation.Generalization of the Model for Unseen Data: This topic covers the development of models that can generalize well to new data and unseen speakers, which is essential for practical applications of silent speech interfaces.Throughout this topic, I have relied on extensive research and consultations with experts in the field.Their insights and guidance have been invaluable in developing a comprehensive understanding of silent speech interfaces.I am grateful to my professor and colleagues who have provided invaluable support and guidance throughout the research process.Their feedback and insights have been instrumental in shaping this topic, and I am grateful for their contributions.Last i but not least, I would like to express my gratitude to my family, friends, colleagues, and anyone who played a significant role in helping me reach this point in my life with their support.""As one","['https://openalex.org/W2135933868', 'https://openalex.org/W6743292536', 'https://openalex.org/W3126721948', 'https://openalex.org/W1520188839', 'https://openalex.org/W6764398373', 'https://openalex.org/W4291824895', 'https://openalex.org/W2157331557', 'https://openalex.org/W2944467544', 'https://openalex.org/W2746109435', 'https://openalex.org/W3047140788', 'https://openalex.org/W3047902412', 'https://openalex.org/W111477576', 'https://openalex.org/W6652300196', 'https://openalex.org/W2111261267', 'https://openalex.org/W2039640471', 'https://openalex.org/W6756911974', 'https://openalex.org/W2770785043', 'https://openalex.org/W3083438180', 'https://openalex.org/W3086311074', 'https://openalex.org/W2972882294', 'https://openalex.org/W2889853672', 'https://openalex.org/W3169801369', 'https://openalex.org/W4283709992', 'https://openalex.org/W2291998724', 'https://openalex.org/W1995735739', 'https://openalex.org/W2552465644', 'https://openalex.org/W6618372016', 'https://openalex.org/W2419247625', 'https://openalex.org/W6679176703', 'https://openalex.org/W2515755543', 'https://openalex.org/W1983364832', 'https://openalex.org/W2955236751', 'https://openalex.org/W2965685620', 'https://openalex.org/W2769531941', 'https://openalex.org/W6910636768', 'https://openalex.org/W2911721563', 'https://openalex.org/W2107860279', 'https://openalex.org/W2970006822', 'https://openalex.org/W3109961563', 'https://openalex.org/W2240622083', 'https://openalex.org/W2793399581', 'https://openalex.org/W2152158029', 'https://openalex.org/W2117193121', 'https://openalex.org/W2892110446', 'https://openalex.org/W2142283335', 'https://openalex.org/W2545603892', 'https://openalex.org/W2794506738', 'https://openalex.org/W3163320179', 'https://openalex.org/W2402146185', 'https://openalex.org/W2898847420', 'https://openalex.org/W2132470613', 'https://openalex.org/W2130382378', 'https://openalex.org/W6668030345', 'https://openalex.org/W6745378982', 'https://openalex.org/W6713645886', 'https://openalex.org/W2939800755', 'https://openalex.org/W3135435917', 'https://openalex.org/W3027415810', 'https://openalex.org/W3039319660', 'https://openalex.org/W4210396727', 'https://openalex.org/W6745481807', 'https://openalex.org/W1985371235', 'https://openalex.org/W2890704021', 'https://openalex.org/W6752688541', 'https://openalex.org/W2890964092', 'https://openalex.org/W3016246676', 'https://openalex.org/W2141998673', 'https://openalex.org/W2888815552', 'https://openalex.org/W2603597171', 'https://openalex.org/W1987841215', 'https://openalex.org/W2772114784', 'https://openalex.org/W2735559731', 'https://openalex.org/W2889007195', 'https://openalex.org/W4224926225', 'https://openalex.org/W4244175094', 'https://openalex.org/W6603616073', 'https://openalex.org/W2742947407', 'https://openalex.org/W3109261583', 'https://openalex.org/W3186178610', 'https://openalex.org/W3119709263', 'https://openalex.org/W2791396024', 'https://openalex.org/W3045728954', 'https://openalex.org/W2917649276', 'https://openalex.org/W2963119563', 'https://openalex.org/W6755013148', 'https://openalex.org/W2891607145', 'https://openalex.org/W2605287558', 'https://openalex.org/W4308506949', 'https://openalex.org/W3094463005', 'https://openalex.org/W3158448163', 'https://openalex.org/W3164409797', 'https://openalex.org/W3157356148', 'https://openalex.org/W4379086794', 'https://openalex.org/W4206204999', 'https://openalex.org/W3094929307', 'https://openalex.org/W2128989405', 'https://openalex.org/W2964058413', 'https://openalex.org/W1994371401', 'https://openalex.org/W3096158177', 'https://openalex.org/W2891446678', 'https://openalex.org/W2972667718', 'https://openalex.org/W4301185652', 'https://openalex.org/W3096008106', 'https://openalex.org/W4293450891', 'https://openalex.org/W2405756170', 'https://openalex.org/W3196749493', 'https://openalex.org/W3094114204', 'https://openalex.org/W4388297464', 'https://openalex.org/W3195797859', 'https://openalex.org/W2952436057', 'https://openalex.org/W3196919683', 'https://openalex.org/W2990503944', 'https://openalex.org/W2889519245', 'https://openalex.org/W3038656636', 'https://openalex.org/W3095614175', 'https://openalex.org/W2768153200', 'https://openalex.org/W2612434969', 'https://openalex.org/W4385822782', 'https://openalex.org/W4221166189', 'https://openalex.org/W3143787022', 'https://openalex.org/W603908379', 'https://openalex.org/W3203059235', 'https://openalex.org/W2125389028', 'https://openalex.org/W2535438430', 'https://openalex.org/W2963073614', 'https://openalex.org/W2160815625', 'https://openalex.org/W2963300588', 'https://openalex.org/W2163605009', 'https://openalex.org/W3095801312', 'https://openalex.org/W1485009520', 'https://openalex.org/W2748448865', 'https://openalex.org/W4298289240', 'https://openalex.org/W4205227948', 'https://openalex.org/W2962793481', 'https://openalex.org/W2971241095', 'https://openalex.org/W2008120082', 'https://openalex.org/W2963308316', 'https://openalex.org/W2807627734', 'https://openalex.org/W2963155035', 'https://openalex.org/W2765833400', 'https://openalex.org/W4298071410']",2024-02-05
https://openalex.org/W4319862412,https://doi.org/10.1109/slt54892.2023.10022892,Learning Invariant Representation and Risk Minimized for Unsupervised Accent Domain Adaptation,"Unsupervised representation learning for speech audios attained impressive performances for speech recognition tasks, particularly when annotated speech is limited. However, the unsupervised paradigm needs to be carefully designed and little is known about what properties these representations acquire. There is no guarantee that the model learns meaningful representations for valuable information for recognition. Moreover, the adaptation ability of the learned representations to other domains still needs to be estimated. In this work, we explore learning domain-invariant representations via a direct mapping of speech representations to their corresponding high-level linguistic informations. Results prove that the learned latents not only capture the articulatory feature of each phoneme but also enhance the adaptation ability, outperforming the baseline largely on accented benchmarks.","['https://openalex.org/W3112702554', 'https://openalex.org/W4319586927', 'https://openalex.org/W3209059054', 'https://openalex.org/W2973049979', 'https://openalex.org/W6780218876', 'https://openalex.org/W3044481399', 'https://openalex.org/W3163464943', 'https://openalex.org/W3162812479', 'https://openalex.org/W2982223350', 'https://openalex.org/W108866686', 'https://openalex.org/W3161411634', 'https://openalex.org/W6788335241', 'https://openalex.org/W4221160950', 'https://openalex.org/W3196787717', 'https://openalex.org/W6769196770', 'https://openalex.org/W6795952400', 'https://openalex.org/W2889494795', 'https://openalex.org/W3161223924', 'https://openalex.org/W2962850167', 'https://openalex.org/W3033038061', 'https://openalex.org/W2347098582', 'https://openalex.org/W6745117592', 'https://openalex.org/W3015213852', 'https://openalex.org/W6603838645', 'https://openalex.org/W6753579488', 'https://openalex.org/W3157503981', 'https://openalex.org/W3160366495', 'https://openalex.org/W1494198834', 'https://openalex.org/W1635512741', 'https://openalex.org/W6771467084', 'https://openalex.org/W2933138175', 'https://openalex.org/W2953190524', 'https://openalex.org/W2747874407', 'https://openalex.org/W6609836551', 'https://openalex.org/W2612690371', 'https://openalex.org/W2187089797', 'https://openalex.org/W2603276336', 'https://openalex.org/W2747329762', 'https://openalex.org/W267812377', 'https://openalex.org/W2560674852', 'https://openalex.org/W4212774754']",2023-01-09
https://openalex.org/W4372259842,https://doi.org/10.1109/icassp49357.2023.10094772,Learning Dependencies of Discrete Speech Representations with Neural Hidden Markov Models,"While discrete latent variable models have had great success in self-supervised learning, most models assume that frames are independent. Due to the segmental nature of phonemes in speech perception, modeling dependencies among latent variables at the frame level can potentially improve the learned representations on phonetic-related tasks. In this work, we assume Markovian dependencies among latent variables, and propose to learn speech representations with neural hidden Markov models. Our general framework allows us to compare to self-supervised models that assume independence, while keeping the number of parameters fixed. The added dependencies improve the accessibility of phonetic information, phonetic segmentation, and the cluster purity of phones, showcasing the benefit of the assumed dependencies.","['https://openalex.org/W2972943112', 'https://openalex.org/W3095361818', 'https://openalex.org/W2064218608', 'https://openalex.org/W4294068504', 'https://openalex.org/W2125838338', 'https://openalex.org/W6828034933', 'https://openalex.org/W6769196770', 'https://openalex.org/W3161411634', 'https://openalex.org/W2750248772', 'https://openalex.org/W2964140243', 'https://openalex.org/W4206192903', 'https://openalex.org/W2565877108', 'https://openalex.org/W3006094508', 'https://openalex.org/W6838949706', 'https://openalex.org/W3198134274', 'https://openalex.org/W6729448088', 'https://openalex.org/W3198782837', 'https://openalex.org/W3100075909', 'https://openalex.org/W3197974236', 'https://openalex.org/W6682396969', 'https://openalex.org/W4226033575', 'https://openalex.org/W6780218876', 'https://openalex.org/W3097286738', 'https://openalex.org/W4297841659', 'https://openalex.org/W3134881075', 'https://openalex.org/W4281771945', 'https://openalex.org/W3036601975', 'https://openalex.org/W2151457493', 'https://openalex.org/W2115979064', 'https://openalex.org/W2979476256', 'https://openalex.org/W2547875792', 'https://openalex.org/W4245655784']",2023-05-05
https://openalex.org/W4385822985,https://doi.org/10.21437/interspeech.2023-2297,DeCoR: Defy Knowledge Forgetting by Predicting Earlier Audio Codes,"Lifelong audio feature extraction involves learning new sound classes incrementally, which is essential for adapting to new data distributions over time.However, optimizing the model only on new data can lead to catastrophic forgetting of previously learned tasks, which undermines the model's ability to perform well over the long term.This paper introduces a new approach to continual audio representation learning called DeCoR.Unlike other methods that store previous data, features, or models, DeCoR indirectly distills knowledge from an earlier model to the latest by predicting quantization indices from a delayed codebook.We demonstrate that DeCoR improves acoustic scene classification accuracy and integrates well with continual self-supervised representation learning.Our approach introduces minimal storage and computation overhead, making it a lightweight and efficient solution for continual learning.","['https://openalex.org/W3159650566', 'https://openalex.org/W3167533889', 'https://openalex.org/W2936774411', 'https://openalex.org/W4296068589', 'https://openalex.org/W3106286430', 'https://openalex.org/W4375869398', 'https://openalex.org/W4225777901', 'https://openalex.org/W3034408737', 'https://openalex.org/W3209059054', 'https://openalex.org/W2997208223', 'https://openalex.org/W3094550259', 'https://openalex.org/W3030364939', 'https://openalex.org/W3184651026', 'https://openalex.org/W3097777922', 'https://openalex.org/W2902625698', 'https://openalex.org/W4319049883', 'https://openalex.org/W4319988532', 'https://openalex.org/W3209984917', 'https://openalex.org/W3139918052', 'https://openalex.org/W4312923322', 'https://openalex.org/W3209797185', 'https://openalex.org/W2964189064', 'https://openalex.org/W3198680251', 'https://openalex.org/W2473930607', 'https://openalex.org/W3005680577']",2023-08-14
https://openalex.org/W4366493008,https://doi.org/10.1109/taslp.2023.3268571,iEmoTTS: Toward Robust Cross-Speaker Emotion Transfer and Control for Speech Synthesis Based on Disentanglement Between Prosody and Timbre,"The capability of generating speech with a specific type of emotion is desired for many human-computer interaction applications. Cross-speaker emotion transfer is a common approach to generating emotional speech when speech data with emotion labels from target speakers is not available for model training. This paper presents a novel cross-speaker emotion transfer system named iEmoTTS. The system is composed of an emotion encoder, a prosody predictor, and a timbre encoder. The emotion encoder extracts the identity of emotion type and the respective emotion intensity from the mel-spectrogram of input speech. The emotion intensity is measured by the posterior probability that the input utterance carries that emotion. The prosody predictor is used to provide prosodic features for emotion transfer. The timbre encoder provides timbre-related information for the system. Unlike many other studies which focus on disentangling speaker and style factors of speech, the iEmoTTS is designed to achieve cross-speaker emotion transfer via disentanglement between prosody and timbre. Prosody is considered the primary carrier of emotion-related speech characteristics, and timbre accounts for the essential characteristics for speaker identification. Zero-shot emotion transfer, meaning that the speech of target speakers is not seen in model training, is also realized with iEmoTTS. Extensive experiments of subjective evaluation have been carried out. The results demonstrate the effectiveness of iEmoTTS compared with other recently proposed systems of cross-speaker emotion transfer. It is shown that iEmoTTS can produce speech with designated emotion types and controllable emotion intensity. With appropriate information bottleneck capacity, iEmoTTS is able to transfer emotional information to a new speaker effectively. Audio samples are publicly available.","['https://openalex.org/W2150658333', 'https://openalex.org/W1570629387', 'https://openalex.org/W2154920538', 'https://openalex.org/W2129142580', 'https://openalex.org/W2134973740', 'https://openalex.org/W2963609956', 'https://openalex.org/W2964243274', 'https://openalex.org/W6778823374', 'https://openalex.org/W6796464841', 'https://openalex.org/W3135547455', 'https://openalex.org/W2085013480', 'https://openalex.org/W6746238782', 'https://openalex.org/W2793479148', 'https://openalex.org/W3015841875', 'https://openalex.org/W2154611638', 'https://openalex.org/W6765987481', 'https://openalex.org/W2962788625', 'https://openalex.org/W6752888775', 'https://openalex.org/W6749555683', 'https://openalex.org/W3197304925', 'https://openalex.org/W2907262790', 'https://openalex.org/W6776390925', 'https://openalex.org/W3008691130', 'https://openalex.org/W3096457008', 'https://openalex.org/W3135644023', 'https://openalex.org/W6802591955', 'https://openalex.org/W4226421465', 'https://openalex.org/W6640215214', 'https://openalex.org/W2398561585', 'https://openalex.org/W2785364623', 'https://openalex.org/W1971670143', 'https://openalex.org/W2888976508', 'https://openalex.org/W3094785744', 'https://openalex.org/W3096830101', 'https://openalex.org/W3160329778', 'https://openalex.org/W6760861152', 'https://openalex.org/W3198712562', 'https://openalex.org/W3162791003', 'https://openalex.org/W3195366750', 'https://openalex.org/W4296069154', 'https://openalex.org/W3139170550', 'https://openalex.org/W2962793481', 'https://openalex.org/W6750489868', 'https://openalex.org/W4210433094', 'https://openalex.org/W6762533536', 'https://openalex.org/W2950689937', 'https://openalex.org/W3146550708', 'https://openalex.org/W2294130536', 'https://openalex.org/W2747664154', 'https://openalex.org/W2889326414', 'https://openalex.org/W6763832098', 'https://openalex.org/W3022876224', 'https://openalex.org/W2998115938', 'https://openalex.org/W6729448088', 'https://openalex.org/W6730091202', 'https://openalex.org/W3207961486', 'https://openalex.org/W2752796333', 'https://openalex.org/W6783867762', 'https://openalex.org/W6631190155', 'https://openalex.org/W4385245566', 'https://openalex.org/W349236604', 'https://openalex.org/W4234095459', 'https://openalex.org/W3150572638', 'https://openalex.org/W2950151997', 'https://openalex.org/W3206725777']",2023-01-01
https://openalex.org/W4389317789,https://doi.org/10.1109/taslp.2023.3337670,Representation Learning With Hidden Unit Clustering for Low Resource Speech Applications,"In this paper, we describe an approach to self-supervised representation learning from raw audio using a hidden unit clustering (HUC) framework. The input to the model consists of audio samples that are windowed and processed with 1-D convolutional layers. The learned ""time-frequency"" representations from the convolutional neural network (CNN) module are further processed with long short term memory (LSTM) layers which generate a contextual vector representation for every windowed segment. The HUC framework, allowing the categorization of the representations into a small number of phoneme-like units, is used to train the model for learning semantically rich speech representations. The targets consist of phoneme-like pseudo labels for each audio segment and these are generated with an iterative <inline-formula xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink""><tex-math notation=""LaTeX"">$k$</tex-math></inline-formula> -means algorithm. We explore techniques that improve the speaker invariance of the learned representations and illustrate the effectiveness of the proposed approach on two settings, i) completely unsupervised speech applications on the sub-tasks described as part of the ZeroSpeech 2021 challenge and ii) semi-supervised automatic speech recognition (ASR) applications on the TIMIT dataset and on the GramVaani challenge Hindi dataset. In these experiments, we achieve state-of-art results for various Zerospeech tasks. Further, on the ASR experiments, the HUC representations are shown to improve over other established benchmarks based on wav2vec, HuBERT and Best-RQ.","['https://openalex.org/W6790356757', 'https://openalex.org/W2114347655', 'https://openalex.org/W3197259906', 'https://openalex.org/W3035725276', 'https://openalex.org/W6769196770', 'https://openalex.org/W3197381195', 'https://openalex.org/W2972943112', 'https://openalex.org/W2394873997', 'https://openalex.org/W2756577849', 'https://openalex.org/W2972984069', 'https://openalex.org/W2889087444', 'https://openalex.org/W2973049979', 'https://openalex.org/W6780218876', 'https://openalex.org/W2842511635', 'https://openalex.org/W2752796333', 'https://openalex.org/W2896457183', 'https://openalex.org/W2786608204', 'https://openalex.org/W2963620343', 'https://openalex.org/W3093096176', 'https://openalex.org/W2787447541', 'https://openalex.org/W2785860501', 'https://openalex.org/W3095361818', 'https://openalex.org/W2787223168', 'https://openalex.org/W3100270690', 'https://openalex.org/W3015213852', 'https://openalex.org/W3041561163', 'https://openalex.org/W4385822823', 'https://openalex.org/W4385822567', 'https://openalex.org/W4224918488', 'https://openalex.org/W2883725317', 'https://openalex.org/W2295598076', 'https://openalex.org/W2124509324', 'https://openalex.org/W3209059054', 'https://openalex.org/W6739901393', 'https://openalex.org/W3011411500', 'https://openalex.org/W6810673746', 'https://openalex.org/W3097777922', 'https://openalex.org/W1494198834', 'https://openalex.org/W2097749765', 'https://openalex.org/W1973041621', 'https://openalex.org/W2395899413', 'https://openalex.org/W2995181338', 'https://openalex.org/W2741692265', 'https://openalex.org/W2996728628', 'https://openalex.org/W3198815374', 'https://openalex.org/W3197349023', 'https://openalex.org/W4385571911', 'https://openalex.org/W6636915900', 'https://openalex.org/W6688816777', 'https://openalex.org/W1635512741', 'https://openalex.org/W2766219058', 'https://openalex.org/W3163793923', 'https://openalex.org/W6996577779', 'https://openalex.org/W2107223151', 'https://openalex.org/W4239510810', 'https://openalex.org/W3006926732', 'https://openalex.org/W2726515241', 'https://openalex.org/W2030931454', 'https://openalex.org/W4297808394', 'https://openalex.org/W4394671563', 'https://openalex.org/W2029685080', 'https://openalex.org/W2187089797', 'https://openalex.org/W2219249508']",2023-12-04
https://openalex.org/W4385570599,https://doi.org/10.18653/v1/2023.findings-acl.309,BertNet: Harvesting Knowledge Graphs with Arbitrary Relations from Pretrained Language Models,"It is crucial to automatically construct knowledge graphs (KGs) of diverse new relations to support knowledge discovery and broad applications. Previous KG construction methods, based on either crowdsourcing or text mining, are often limited to a small predefined set of relations due to manual cost or restrictions in text corpus. Recent research proposed to use pretrained language models (LMs) as implicit knowledge bases that accept knowledge queries with prompts. Yet, the implicit knowledge lacks many desirable properties of a full-scale symbolic KG, such as easy access, navigation, editing, and quality assurance. In this paper, we propose a new approach of harvesting massive KGs of arbitrary relations from pretrained LMs. With minimal input of a relation definition (a prompt and a few shot of example entity pairs), the approach efficiently searches in the vast entity pair space to extract diverse accurate knowledge of the desired relation. We develop an effective search-and-rescore mechanism for improved efficiency and accuracy. We deploy the approach to harvest KGs of over 400 new relations, from LMs of varying capacities such as RoBERTaNet. Extensive human and automatic evaluations show our approach manages to extract diverse accurate knowledge, including tuples of complex relations (e.g., ""A is capable of but not good at B""). The resulting KGs as a symbolic interpretation of the source LMs also reveal new insights into the LMs' knowledge capacities.","['https://openalex.org/W2935052563', 'https://openalex.org/W3166986030', 'https://openalex.org/W2972167903', 'https://openalex.org/W3012590175', 'https://openalex.org/W3102049052', 'https://openalex.org/W4394672037', 'https://openalex.org/W4285258797', 'https://openalex.org/W2963101081', 'https://openalex.org/W2561529111', 'https://openalex.org/W2986213397', 'https://openalex.org/W2963687836', 'https://openalex.org/W2999524812', 'https://openalex.org/W4285255684', 'https://openalex.org/W2038721957', 'https://openalex.org/W3202712981', 'https://openalex.org/W4223974161', 'https://openalex.org/W3034999214', 'https://openalex.org/W4389520747', 'https://openalex.org/W3034918576', 'https://openalex.org/W3152979241', 'https://openalex.org/W2903721568', 'https://openalex.org/W2604165577', 'https://openalex.org/W4210706440', 'https://openalex.org/W4286903575', 'https://openalex.org/W4385572965', 'https://openalex.org/W2965373594', 'https://openalex.org/W4229038710', 'https://openalex.org/W2912351665', 'https://openalex.org/W3166846774', 'https://openalex.org/W4307407665', 'https://openalex.org/W4287660741', 'https://openalex.org/W3155001903', 'https://openalex.org/W2251913848', 'https://openalex.org/W2896457183', 'https://openalex.org/W4206118214', 'https://openalex.org/W2127795553', 'https://openalex.org/W4292779060', 'https://openalex.org/W3167906655', 'https://openalex.org/W2970161131', 'https://openalex.org/W3216037316', 'https://openalex.org/W3207166518', 'https://openalex.org/W3194243418', 'https://openalex.org/W3107969673', 'https://openalex.org/W2998557616', 'https://openalex.org/W3167136668', 'https://openalex.org/W2509019445']",2023-01-01
https://openalex.org/W4392904805,https://doi.org/10.1109/icassp48485.2024.10447112,"VoxtLM: Unified Decoder-Only Models for Consolidating Speech Recognition, Synthesis and Speech, Text Continuation Tasks","We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.","['https://openalex.org/W6778883912', 'https://openalex.org/W6790356757', 'https://openalex.org/W4381786045', 'https://openalex.org/W6777028661', 'https://openalex.org/W6848735303', 'https://openalex.org/W4388017359', 'https://openalex.org/W2964243274', 'https://openalex.org/W6778823374', 'https://openalex.org/W6802465204', 'https://openalex.org/W4226120743', 'https://openalex.org/W3209059054', 'https://openalex.org/W4226033575', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W6803092890', 'https://openalex.org/W6852381208', 'https://openalex.org/W6853611000', 'https://openalex.org/W4389524500', 'https://openalex.org/W6811340617', 'https://openalex.org/W6852781825', 'https://openalex.org/W2963250244', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963979492', 'https://openalex.org/W4385822683', 'https://openalex.org/W6783867762', 'https://openalex.org/W2890964092', 'https://openalex.org/W6786696081', 'https://openalex.org/W2972394484', 'https://openalex.org/W3202278141', 'https://openalex.org/W2995181338', 'https://openalex.org/W1494198834', 'https://openalex.org/W3095410713', 'https://openalex.org/W2972359262', 'https://openalex.org/W2998572311', 'https://openalex.org/W6631190155', 'https://openalex.org/W6796464841', 'https://openalex.org/W4319862255', 'https://openalex.org/W4313679638', 'https://openalex.org/W2899575547', 'https://openalex.org/W3024605872', 'https://openalex.org/W4378501656', 'https://openalex.org/W3207222250', 'https://openalex.org/W4379540238', 'https://openalex.org/W4394671563', 'https://openalex.org/W4229005866', 'https://openalex.org/W4377865046']",2024-03-18
https://openalex.org/W4206711328,https://doi.org/10.18653/v1/2021.emnlp-demo.17,fairseq Sˆ2: A Scalable and Integrable Speech Synthesis Toolkit,"Changhan Wang, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Ann Lee, Peng-Jen Chen, Jiatao Gu, Juan Pino. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2021.","['https://openalex.org/W2933138175', 'https://openalex.org/W2114925438', 'https://openalex.org/W2970295111', 'https://openalex.org/W2883586237', 'https://openalex.org/W2962691331', 'https://openalex.org/W3174311593', 'https://openalex.org/W2963799213', 'https://openalex.org/W2794490148', 'https://openalex.org/W3095545636', 'https://openalex.org/W4289383906', 'https://openalex.org/W2964277995', 'https://openalex.org/W2963691546', 'https://openalex.org/W2763421725', 'https://openalex.org/W2400063444', 'https://openalex.org/W3095948607', 'https://openalex.org/W2795935804', 'https://openalex.org/W2995680346', 'https://openalex.org/W2964243274', 'https://openalex.org/W2160473997', 'https://openalex.org/W2471520273', 'https://openalex.org/W2808706139', 'https://openalex.org/W3024605872', 'https://openalex.org/W3163906773', 'https://openalex.org/W4394671563', 'https://openalex.org/W4295312788', 'https://openalex.org/W4286784498', 'https://openalex.org/W2598638573', 'https://openalex.org/W3033411150', 'https://openalex.org/W3169320628', 'https://openalex.org/W2903739847', 'https://openalex.org/W2619368999', 'https://openalex.org/W2107740512', 'https://openalex.org/W2962699523', 'https://openalex.org/W3030437843', 'https://openalex.org/W3097945073', 'https://openalex.org/W3092028330', 'https://openalex.org/W2120847449', 'https://openalex.org/W3092424727', 'https://openalex.org/W1524333225', 'https://openalex.org/W3036601975', 'https://openalex.org/W3173767661', 'https://openalex.org/W2963300588', 'https://openalex.org/W3103029570', 'https://openalex.org/W3008480565', 'https://openalex.org/W2527729766', 'https://openalex.org/W3140429000', 'https://openalex.org/W2747874407', 'https://openalex.org/W2049686551', 'https://openalex.org/W2963739817', 'https://openalex.org/W2979476256', 'https://openalex.org/W3016160783', 'https://openalex.org/W3174758275', 'https://openalex.org/W2946200149', 'https://openalex.org/W2129142580', 'https://openalex.org/W2150658333', 'https://openalex.org/W116902681', 'https://openalex.org/W2130086727', 'https://openalex.org/W2950613790', 'https://openalex.org/W2595715041', 'https://openalex.org/W2972495969']",2021-01-01
https://openalex.org/W4386133927,https://doi.org/10.1109/taslp.2023.3308374,Speaker Adaptive Text-to-Speech With Timbre-Normalized Vector-Quantized Feature,"Achieving high fidelity and speaker similarity in text-to-speech speaker adaptation with limited amount of data is a challenging task. Most existing methods only consider adapting to the timbre of the target speakers but fail to capture their speaking styles from little data. In this work, we propose a novel TTS system, TN-VQTTS, which leverages timbre-normalized vector-quantized (TN-VQ) acoustic feature for speaker adaptation with little data. With the TN-VQ feature, speaking style and timbre can be effectively decomposed and controlled by the acoustic model and the vocoder separately of VQTTS. Such decomposition enables us to finely mimic both the two characteristics of the target speaker in adaptation with little data. Specifically, we first reduce the dimensionality of self-supervised VQ acoustic feature via PCA and normalize its timbre with a normalizing flow model. The feature is then quantized with k-means and used as the TN-VQ feature for a multi-speaker VQ-TTS system. Furthermore, we optimize timbre-independent style embeddings of the training speakers jointly with the acoustic model and store them in a lookup table. The embedding table later serves as a selectable codebook or a group of basis for representing the style of unseen speakers. Our experiments on LibriTTS dataset first show that the proposed model architecture for VQ feature achieves better performance in multi-speaker text-to-speech synthesis than several existing methods. We also find that the reconstruction performance and the naturalness are almost unchanged after applying timbre normalization and k-means quantization. Finally, we show that TN-VQTTS achieves better performance on speaker similarity in adaptation than both speaker embedding based adaptation method and fine-tuning based baseline AdaSpeech.","['https://openalex.org/W6752888775', 'https://openalex.org/W3163573274', 'https://openalex.org/W6745265071', 'https://openalex.org/W6796464841', 'https://openalex.org/W2063041593', 'https://openalex.org/W2748318213', 'https://openalex.org/W4372340947', 'https://openalex.org/W6785102375', 'https://openalex.org/W3097777922', 'https://openalex.org/W6790220310', 'https://openalex.org/W2088632109', 'https://openalex.org/W2890964092', 'https://openalex.org/W2963300588', 'https://openalex.org/W2973049979', 'https://openalex.org/W3209984917', 'https://openalex.org/W6755207826', 'https://openalex.org/W6769196770', 'https://openalex.org/W2085628288', 'https://openalex.org/W6803547063', 'https://openalex.org/W4296068820', 'https://openalex.org/W6846600677', 'https://openalex.org/W4285301843', 'https://openalex.org/W3097728852', 'https://openalex.org/W6745697700', 'https://openalex.org/W6765987481', 'https://openalex.org/W3016137096', 'https://openalex.org/W6755135894', 'https://openalex.org/W3140429000', 'https://openalex.org/W6748588790', 'https://openalex.org/W2973084242', 'https://openalex.org/W6748573829', 'https://openalex.org/W6795261426', 'https://openalex.org/W6777694618', 'https://openalex.org/W3015826515', 'https://openalex.org/W4226132755', 'https://openalex.org/W3196584150', 'https://openalex.org/W6779823529', 'https://openalex.org/W6770847983', 'https://openalex.org/W6777028661', 'https://openalex.org/W4297841714', 'https://openalex.org/W3162271107', 'https://openalex.org/W3015621018', 'https://openalex.org/W3197324626', 'https://openalex.org/W3202111322', 'https://openalex.org/W6778823374', 'https://openalex.org/W2964243274', 'https://openalex.org/W6805710207', 'https://openalex.org/W6848735303', 'https://openalex.org/W3197216873', 'https://openalex.org/W6736996214', 'https://openalex.org/W3216941316', 'https://openalex.org/W3016021263', 'https://openalex.org/W6780218876', 'https://openalex.org/W2129142580', 'https://openalex.org/W3095491807', 'https://openalex.org/W4221167022', 'https://openalex.org/W3151450932', 'https://openalex.org/W2979476256', 'https://openalex.org/W3150807214', 'https://openalex.org/W4298174729', 'https://openalex.org/W4301371414', 'https://openalex.org/W3172148458', 'https://openalex.org/W4307783813', 'https://openalex.org/W2808706139', 'https://openalex.org/W3033411150', 'https://openalex.org/W2762829962', 'https://openalex.org/W2519091744', 'https://openalex.org/W2619368999', 'https://openalex.org/W4225680573', 'https://openalex.org/W3128910262', 'https://openalex.org/W2892620417', 'https://openalex.org/W2896457183', 'https://openalex.org/W3026874504', 'https://openalex.org/W2788357188', 'https://openalex.org/W2963609956', 'https://openalex.org/W2766812927', 'https://openalex.org/W3169905056', 'https://openalex.org/W3036601975', 'https://openalex.org/W4313679638', 'https://openalex.org/W4297813370', 'https://openalex.org/W3036167779', 'https://openalex.org/W3024605872']",2023-01-01
https://openalex.org/W4200047557,https://doi.org/10.1109/ictc52510.2021.9621175,A Preliminary Study on Wav2Vec 2.0 Embeddings for Text-to-Speech,"Wav2Vec 2.0 (W2V), a self-supervised speech representation trained with massive unlabeled speech data, showed promising results on Automatic Speech Recognition (ASR). In spite of several evidences showing that W2V can generate unique acoustic features, it has been rarely utilized in Text-to-Speech (TTS) task. In this paper, we adapt W2V embed dings to TTS as feature vectors. Our TTS model consists of two components: Text2Vec, which converts a character-level text sequence into W2V embeddings, and GAN-based vocoder, which decodes a W2V embedding sequence to waveform signals. From the experiments, we observe that W2V embeddings have considerable potential as acoustic features for TTS.","['https://openalex.org/W3161627112', 'https://openalex.org/W2940544976', 'https://openalex.org/W2752796333', 'https://openalex.org/W3024605872', 'https://openalex.org/W2896457183', 'https://openalex.org/W3112092703', 'https://openalex.org/W3112616666', 'https://openalex.org/W2903739847', 'https://openalex.org/W6682918086', 'https://openalex.org/W3033411150', 'https://openalex.org/W2120847449', 'https://openalex.org/W2129142580', 'https://openalex.org/W2981728663', 'https://openalex.org/W2946200149', 'https://openalex.org/W2777302760', 'https://openalex.org/W3036601975', 'https://openalex.org/W2795935804', 'https://openalex.org/W1494198834', 'https://openalex.org/W2973026522', 'https://openalex.org/W3015338123', 'https://openalex.org/W3173767661', 'https://openalex.org/W2584505851', 'https://openalex.org/W2964243274', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963799213', 'https://openalex.org/W4287173589', 'https://openalex.org/W2962780374', 'https://openalex.org/W2154920538', 'https://openalex.org/W3198275944']",2021-10-20
https://openalex.org/W3163338468,https://doi.org/10.1109/icassp39728.2021.9414499,End-to-End Text-to-Speech Using Latent Duration Based on VQ-VAE,"Explicit duration modeling is a key to achieving robust and efficient alignment in text-to-speech synthesis (TTS). We propose a new TTS framework using explicit duration modeling that incorporates duration as a discrete latent variable to TTS and enables joint optimization of whole modules from scratch. We formulate our method based on conditional VQ-VAE to handle discrete duration in a variational autoencoder and provide a theoretical explanation to justify our method. In our framework, a connectionist temporal classification (CTC) -based force aligner acts as the approximate posterior, and text-to-duration works as the prior in the variational autoencoder. We evaluated our proposed method with a listening test and compared it with other TTS methods based on soft-attention or explicit duration modeling. The results showed that our systems rated between soft-attention-based methods (Transformer-TTS, Tacotron2) and explicit duration modeling-based methods (Fastspeech).","['https://openalex.org/W3016160783', 'https://openalex.org/W6631190155', 'https://openalex.org/W3096442195', 'https://openalex.org/W6778823374', 'https://openalex.org/W6640963894', 'https://openalex.org/W2752796333', 'https://openalex.org/W6777694618', 'https://openalex.org/W175280642', 'https://openalex.org/W6753540710', 'https://openalex.org/W6773153034', 'https://openalex.org/W6777028661', 'https://openalex.org/W2973158936', 'https://openalex.org/W6623517193', 'https://openalex.org/W2127141656', 'https://openalex.org/W6679434410', 'https://openalex.org/W3015922793', 'https://openalex.org/W2963609956', 'https://openalex.org/W2972702018', 'https://openalex.org/W6763832098', 'https://openalex.org/W2972746749', 'https://openalex.org/W2903739847', 'https://openalex.org/W3016136182', 'https://openalex.org/W2964243274', 'https://openalex.org/W2935711438', 'https://openalex.org/W3100270690', 'https://openalex.org/W2972849140', 'https://openalex.org/W6769328215', 'https://openalex.org/W2972374322', 'https://openalex.org/W6784520556', 'https://openalex.org/W3094917204', 'https://openalex.org/W2982602185', 'https://openalex.org/W3033411150', 'https://openalex.org/W2884607399', 'https://openalex.org/W2971753973', 'https://openalex.org/W2964121744', 'https://openalex.org/W3163338468', 'https://openalex.org/W3130016944', 'https://openalex.org/W2964308564', 'https://openalex.org/W2949382160', 'https://openalex.org/W2946200149', 'https://openalex.org/W3015440759', 'https://openalex.org/W2133564696', 'https://openalex.org/W1522301498', 'https://openalex.org/W3125709657', 'https://openalex.org/W2963799213', 'https://openalex.org/W3024605872', 'https://openalex.org/W3025793647', 'https://openalex.org/W1959608418', 'https://openalex.org/W854541894', 'https://openalex.org/W2519091744', 'https://openalex.org/W2970730223', 'https://openalex.org/W3026874504']",2021-05-13
https://openalex.org/W4393161149,https://doi.org/10.1609/aaai.v38i20.30271,Regeneration Learning: A Learning Paradigm for Data Generation,"Machine learning methods for conditional data generation usually build a mapping from source conditional data X to target data Y. The target Y (e.g., text, speech, music, image, video) is usually high-dimensional and complex, and contains information that does not exist in source data, which hinders effective and efficient learning on the source-target mapping. In this paper, we present a learning paradigm called regeneration learning for data generation, which first generates Y' (an abstraction/representation of Y) from X and then generates Y from Y'. During training, Y' is obtained from Y through either handcrafted rules or self-supervised learning and is used to learn X--&gt;Y' and Y'--&gt;Y. Regeneration learning extends the concept of representation learning to data generation tasks, and can be regarded as a counterpart of traditional representation learning, since 1) regeneration learning handles the abstraction (Y') of the target data Y for data generation while traditional representation learning handles the abstraction (X') of source data X for data understanding; 2) both the processes of Y'--&gt;Y in regeneration learning and X--&gt;X' in representation learning can be learned in a self-supervised way (e.g., pre-training); 3) both the mappings from X to Y' in regeneration learning and from X' to Y in representation learning are simpler than the direct mapping from X to Y. We show that regeneration learning can be a widely-used paradigm for data generation (e.g., text generation, speech recognition, speech synthesis, music composition, image generation, and video generation) and can provide valuable insights into developing data generation methods.","['https://openalex.org/W2163922914', 'https://openalex.org/W6658111725', 'https://openalex.org/W6690147562', 'https://openalex.org/W2327501763', 'https://openalex.org/W2944294033', 'https://openalex.org/W6683738474', 'https://openalex.org/W6676297131', 'https://openalex.org/W3207290297', 'https://openalex.org/W3111551570', 'https://openalex.org/W6630486035', 'https://openalex.org/W6631782140', 'https://openalex.org/W6758621670', 'https://openalex.org/W6687483927', 'https://openalex.org/W6779823529', 'https://openalex.org/W6794737794', 'https://openalex.org/W2163605009', 'https://openalex.org/W3171622771', 'https://openalex.org/W1574901103', 'https://openalex.org/W6634214284', 'https://openalex.org/W3217030260', 'https://openalex.org/W6762931180', 'https://openalex.org/W1483870316', 'https://openalex.org/W6610566761', 'https://openalex.org/W4226317937', 'https://openalex.org/W2129069237', 'https://openalex.org/W6802211943', 'https://openalex.org/W6678589667', 'https://openalex.org/W2995238198', 'https://openalex.org/W2752796333', 'https://openalex.org/W3030056709', 'https://openalex.org/W6798098866', 'https://openalex.org/W2605287558', 'https://openalex.org/W3097792222', 'https://openalex.org/W2962793481', 'https://openalex.org/W4313484371', 'https://openalex.org/W3152733922', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963090522', 'https://openalex.org/W1583912456', 'https://openalex.org/W4283388932', 'https://openalex.org/W2237250383', 'https://openalex.org/W2946200149', 'https://openalex.org/W1959608418', 'https://openalex.org/W2108598243', 'https://openalex.org/W4292779060', 'https://openalex.org/W1508960934', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963609956', 'https://openalex.org/W2963037989', 'https://openalex.org/W4280561221', 'https://openalex.org/W4225723749', 'https://openalex.org/W3174758275', 'https://openalex.org/W4287551327', 'https://openalex.org/W3165647589', 'https://openalex.org/W2160815625', 'https://openalex.org/W3174763799', 'https://openalex.org/W2963748441', 'https://openalex.org/W4281485151', 'https://openalex.org/W4312933868', 'https://openalex.org/W4388739184', 'https://openalex.org/W4224035735', 'https://openalex.org/W4303926680', 'https://openalex.org/W2896457183', 'https://openalex.org/W1828163288', 'https://openalex.org/W2031879798', 'https://openalex.org/W2125186487', 'https://openalex.org/W2952230511', 'https://openalex.org/W4320013936', 'https://openalex.org/W4210849719', 'https://openalex.org/W3205994442', 'https://openalex.org/W2971074500', 'https://openalex.org/W4307323391', 'https://openalex.org/W2794365787', 'https://openalex.org/W3198869563', 'https://openalex.org/W2194775991', 'https://openalex.org/W3200682172', 'https://openalex.org/W3215615641', 'https://openalex.org/W4297841605', 'https://openalex.org/W2963799213', 'https://openalex.org/W2926840633', 'https://openalex.org/W2964243274', 'https://openalex.org/W2133564696', 'https://openalex.org/W2963083845', 'https://openalex.org/W3036033610', 'https://openalex.org/W3175779516', 'https://openalex.org/W4300980117', 'https://openalex.org/W3024605872', 'https://openalex.org/W3036167779', 'https://openalex.org/W4382465386', 'https://openalex.org/W3010434693']",2024-03-24
https://openalex.org/W4296069123,https://doi.org/10.21437/interspeech.2022-10138,Speech Audio Corrector: using speech from non-target speakers for one-off correction of mispronunciations in grapheme-input text-to-speech,"Correct pronunciation is essential for text-to-speech (TTS) systems in production.Most production systems rely on pronouncing dictionaries to perform grapheme-to-phoneme conversion.Unlike end-to-end TTS, this enables pronunciation correction by manually altering the phoneme sequence, but the necessary dictionaries are labour-intensive to create and only exist in a few high-resourced languages.This work demonstrates that accurate TTS pronunciation control can be achieved without a dictionary.Moreover, we show that such control can be performed without requiring any model retraining or fine-tuning, merely by supplying a single correctly-pronounced reading of a word in a different voice and accent at synthesis time.Experimental results show that our proposed system successfully enables one-off correction of mispronunciations in graphemebased TTS with maintained synthesis quality.This opens the door to production-level TTS in languages and applications where pronunciation dictionaries are unavailable.","['https://openalex.org/W2763110165', 'https://openalex.org/W2150658333', 'https://openalex.org/W2747874407', 'https://openalex.org/W4214968481', 'https://openalex.org/W2884607399', 'https://openalex.org/W2964243274', 'https://openalex.org/W2972610613', 'https://openalex.org/W3180374548', 'https://openalex.org/W3024605872', 'https://openalex.org/W3197580070', 'https://openalex.org/W4288107125', 'https://openalex.org/W4287854499', 'https://openalex.org/W3150435369', 'https://openalex.org/W2997819339', 'https://openalex.org/W2903739847', 'https://openalex.org/W3169320628', 'https://openalex.org/W2963827314', 'https://openalex.org/W3036601975', 'https://openalex.org/W3092028330', 'https://openalex.org/W2963799213', 'https://openalex.org/W1494198834', 'https://openalex.org/W3197324626', 'https://openalex.org/W3096232484', 'https://openalex.org/W2767052532', 'https://openalex.org/W3140429000', 'https://openalex.org/W4286984129', 'https://openalex.org/W4287887366', 'https://openalex.org/W4394671563', 'https://openalex.org/W2933138175']",2022-09-16
https://openalex.org/W3206801991,https://doi.org/10.1109/icassp43922.2022.9747441,KaraSinger: Score-Free Singing Voice Synthesis with VQ-VAE Using Mel-Spectrograms,"In this paper, we propose a novel neural network model called KaraSinger for a less-studied singing voice synthesis (SVS) task named score-free SVS, in which the prosody and melody are spontaneously decided by machine. KaraSinger comprises a vector-quantized variational autoencoder (VQ-VAE) that compresses the Mel-spectrograms of singing audio to sequences of discrete codes, and a language model (LM) that learns to predict the discrete codes given the corresponding lyrics. For the VQ-VAE part, we employ a Connectionist Temporal Classification (CTC) loss to encourage the discrete codes to carry phoneme-related information. For the LM part, we use location-sensitive attention for learning a robust alignment between the input phoneme sequence and the output discrete code. We keep the architecture of both the VQ-VAE and LM light-weight for fast training and inference speed. We validate the effectiveness of the proposed design choices using a proprietary collection of 550 English pop songs sung by multiple amateur singers. The result of a listening test shows that KaraSinger achieves high scores in intelligibility, musicality, and the overall quality.","['https://openalex.org/W6761551260', 'https://openalex.org/W6767111847', 'https://openalex.org/W6779709467', 'https://openalex.org/W6777028661', 'https://openalex.org/W2127141656', 'https://openalex.org/W3015315843', 'https://openalex.org/W3197103763', 'https://openalex.org/W2972867623', 'https://openalex.org/W3081279708', 'https://openalex.org/W6783382068', 'https://openalex.org/W3097514409', 'https://openalex.org/W6796730497', 'https://openalex.org/W2963609956', 'https://openalex.org/W2964243274', 'https://openalex.org/W2973046048', 'https://openalex.org/W3096437652', 'https://openalex.org/W3015499232', 'https://openalex.org/W3133525064', 'https://openalex.org/W6739901393', 'https://openalex.org/W2516406502', 'https://openalex.org/W6790720088', 'https://openalex.org/W6678318511', 'https://openalex.org/W6623517193', 'https://openalex.org/W29794711', 'https://openalex.org/W2889244839', 'https://openalex.org/W2921576841', 'https://openalex.org/W3204116061', 'https://openalex.org/W3015437531', 'https://openalex.org/W2030149476', 'https://openalex.org/W6771665576', 'https://openalex.org/W6776218486', 'https://openalex.org/W3024973272', 'https://openalex.org/W6748381668', 'https://openalex.org/W2752796333', 'https://openalex.org/W3140429000', 'https://openalex.org/W6762931180', 'https://openalex.org/W2970006822', 'https://openalex.org/W4287802874', 'https://openalex.org/W2953022181', 'https://openalex.org/W3082910224', 'https://openalex.org/W2938704169', 'https://openalex.org/W3024605872', 'https://openalex.org/W3034573343', 'https://openalex.org/W2789543585', 'https://openalex.org/W3037798801', 'https://openalex.org/W2971074500', 'https://openalex.org/W2996287690', 'https://openalex.org/W2963403868', 'https://openalex.org/W3133405188', 'https://openalex.org/W2408435475', 'https://openalex.org/W3174758275', 'https://openalex.org/W2124097505', 'https://openalex.org/W3021164770', 'https://openalex.org/W2964026424', 'https://openalex.org/W3114301328', 'https://openalex.org/W2963799213', 'https://openalex.org/W854541894', 'https://openalex.org/W4385245566']",2022-04-27
https://openalex.org/W3027324582,https://doi.org/10.21437/interspeech.2020-1693,Vector-Quantized Neural Networks for Acoustic Unit Discovery in the ZeroSpeech 2020 Challenge,"In this paper, we explore vector quantization for acoustic unit discovery. Leveraging unlabelled data, we aim to learn discrete representations of speech that separate phonetic content from speaker-specific details. We propose two neural models to tackle this challenge - both use vector quantization to map continuous features to a finite set of codes. The first model is a type of vector-quantized variational autoencoder (VQ-VAE). The VQ-VAE encodes speech into a sequence of discrete units before reconstructing the audio waveform. Our second model combines vector quantization with contrastive predictive coding (VQ-CPC). The idea is to learn a representation of speech by predicting future acoustic units. We evaluate the models on English and Indonesian data for the ZeroSpeech 2020 challenge. In ABX phone discrimination tests, both models outperform all submissions to the 2019 and 2020 challenges, with a relative improvement of more than 30%. The models also perform competitively on a downstream voice conversion task. Of the two, VQ-CPC performs slightly better in general and is simpler and faster to train. Finally, probing experiments show that vector quantization is an effective bottleneck, forcing the models to discard speaker information.","['https://openalex.org/W2395899413', 'https://openalex.org/W2972374322', 'https://openalex.org/W2963620343', 'https://openalex.org/W2347098582', 'https://openalex.org/W2516890051', 'https://openalex.org/W2780786457', 'https://openalex.org/W2078769636', 'https://openalex.org/W2963112338', 'https://openalex.org/W2964121744', 'https://openalex.org/W2010188467', 'https://openalex.org/W2789543585', 'https://openalex.org/W2963830550', 'https://openalex.org/W2842511635', 'https://openalex.org/W2945769669', 'https://openalex.org/W2396043527', 'https://openalex.org/W2117041980', 'https://openalex.org/W2973026522', 'https://openalex.org/W1778492285', 'https://openalex.org/W2963799213', 'https://openalex.org/W2911249026', 'https://openalex.org/W2949382160', 'https://openalex.org/W3097692357', 'https://openalex.org/W2598638573', 'https://openalex.org/W2890983311', 'https://openalex.org/W2954386831', 'https://openalex.org/W2242818861', 'https://openalex.org/W2547039119', 'https://openalex.org/W3098361150', 'https://openalex.org/W2996383576', 'https://openalex.org/W2346964103', 'https://openalex.org/W3016181583', 'https://openalex.org/W2972867623', 'https://openalex.org/W2995680346', 'https://openalex.org/W2964026424', 'https://openalex.org/W3125709657', 'https://openalex.org/W2971775690', 'https://openalex.org/W3018535504', 'https://openalex.org/W2100768664', 'https://openalex.org/W2972943112', 'https://openalex.org/W3003875258']",2020-10-25
https://openalex.org/W3112613336,https://doi.org/10.21437/interspeech.2021-50,Towards Unsupervised Phone and Word Segmentation Using Self-Supervised Vector-Quantized Neural Networks,"We investigate segmenting and clustering speech into low-bitrate phone-like sequences without supervision. We specifically constrain pretrained self-supervised vector-quantized (VQ) neural networks so that blocks of contiguous feature vectors are assigned to the same code, thereby giving a variable-rate segmentation of the speech into discrete units. Two segmentation methods are considered. In the first, features are greedily merged until a prespecified number of segments are reached. The second uses dynamic programming to optimize a squared error with a penalty term to encourage fewer but longer segments. We show that these VQ segmentation methods can be used without alteration across a wide range of tasks: unsupervised phone segmentation, ABX phone discrimination, same-different word discrimination, and as inputs to a symbolic word segmentation algorithm. The penalized dynamic programming method generally performs best. While performance on individual tasks is only comparable to the state-of-the-art in some cases, in all tasks a reasonable competing approach is outperformed at a substantially lower bitrate.","['https://openalex.org/W2126377586', 'https://openalex.org/W130754613', 'https://openalex.org/W2145410271', 'https://openalex.org/W2478415332', 'https://openalex.org/W2407151108', 'https://openalex.org/W2971775690', 'https://openalex.org/W3097286738', 'https://openalex.org/W2972943112', 'https://openalex.org/W2242818861', 'https://openalex.org/W2400549570', 'https://openalex.org/W3008499099', 'https://openalex.org/W2842511635', 'https://openalex.org/W2516890051', 'https://openalex.org/W3093096176', 'https://openalex.org/W2010188467', 'https://openalex.org/W3096656254', 'https://openalex.org/W3095361818', 'https://openalex.org/W3018535504', 'https://openalex.org/W2780786457', 'https://openalex.org/W2117126688', 'https://openalex.org/W3096196861', 'https://openalex.org/W3102519966', 'https://openalex.org/W2345913943', 'https://openalex.org/W2962799131', 'https://openalex.org/W3006094508', 'https://openalex.org/W2964169922', 'https://openalex.org/W2996383576', 'https://openalex.org/W1796128977', 'https://openalex.org/W3125709657', 'https://openalex.org/W2396043527', 'https://openalex.org/W3097485645', 'https://openalex.org/W2972867623', 'https://openalex.org/W2963799213', 'https://openalex.org/W3097692357', 'https://openalex.org/W2483390977', 'https://openalex.org/W2972764223', 'https://openalex.org/W2404952642', 'https://openalex.org/W2395899413', 'https://openalex.org/W2620638943', 'https://openalex.org/W2963137467', 'https://openalex.org/W3098361150', 'https://openalex.org/W2052697931', 'https://openalex.org/W2973026522', 'https://openalex.org/W2468716020', 'https://openalex.org/W2025482506']",2021-08-27
https://openalex.org/W3213967396,https://doi.org/10.30746/978-91-519-5560-5,Proceedings of the 2020 Joint Conference on AI Music Creativity,"Modern approaches to sound synthesis using deep neural networks are hard to\ncontrol, especially when fine-grained conditioning information is not\navailable, hindering their adoption by musicians.\n In this paper, we cast the generation of individual instrumental notes as an\ninpainting-based task, introducing novel and unique ways to iteratively shape\nsounds. To this end, we propose a two-step approach: first, we adapt the\nVQ-VAE-2 image generation architecture to spectrograms in order to convert\nreal-valued spectrograms into compact discrete codemaps, we then implement\ntoken-masked Transformers for the inpainting-based generation of these\ncodemaps.\n We apply the proposed architecture on the NSynth dataset on masked resampling\ntasks. Most crucially, we open-source an interactive web interface to transform\nsounds by inpainting, for artists and practitioners alike, opening up to new,\ncreative uses.\n","['https://openalex.org/W2137619888', 'https://openalex.org/W6648289822', 'https://openalex.org/W6617288862', 'https://openalex.org/W2744457411', 'https://openalex.org/W2336031501', 'https://openalex.org/W1983627329', 'https://openalex.org/W6845202251', 'https://openalex.org/W6621314851', 'https://openalex.org/W2108032748', 'https://openalex.org/W6651938749', 'https://openalex.org/W2068627759', 'https://openalex.org/W52081385', 'https://openalex.org/W1976814248', 'https://openalex.org/W3128065392', 'https://openalex.org/W3158589047', 'https://openalex.org/W2948210185', 'https://openalex.org/W2530921900', 'https://openalex.org/W2963575853', 'https://openalex.org/W2606176153', 'https://openalex.org/W2164767140', 'https://openalex.org/W2115067604', 'https://openalex.org/W2143966220', 'https://openalex.org/W2026736703', 'https://openalex.org/W2169371330', 'https://openalex.org/W3018535504', 'https://openalex.org/W2910577860', 'https://openalex.org/W2971074500', 'https://openalex.org/W2963411769', 'https://openalex.org/W2099773255', 'https://openalex.org/W2794490148', 'https://openalex.org/W1998871699', 'https://openalex.org/W1981455444', 'https://openalex.org/W2901638613', 'https://openalex.org/W2519091744', 'https://openalex.org/W4287802874', 'https://openalex.org/W2955263139', 'https://openalex.org/W3037798801', 'https://openalex.org/W4294619240', 'https://openalex.org/W1581879603', 'https://openalex.org/W4288265053', 'https://openalex.org/W2152002570', 'https://openalex.org/W2811079561', 'https://openalex.org/W2100649345', 'https://openalex.org/W1984431516', 'https://openalex.org/W2963799213', 'https://openalex.org/W2000215628', 'https://openalex.org/W2938704169', 'https://openalex.org/W3029579848']",2021-01-01
https://openalex.org/W4407304302,https://doi.org/10.1109/access.2025.3540543,Probe-Assisted Fine-Grained Control for Non-Differentiable Features in Symbolic Music Generation,"As symbolic music generation evolves, research interest is shifting toward more controlled and steerable generative processes to support creative decisions. Previous methods focus on global conditioning or fine-grained control through input sequences but often limit flexibility for real-time interventions and require modifications to the model&#x2019;s architecture. We introduce a novel symbolic music generation framework by combining a Transformer encoder-decoder with probe models, which enable us to interpret the encoder hidden state using pre-defined non-differentiable musical features, and subsequently manipulate the hidden state to achieve a set of desired attributes in the generated music. This method allows fine-grained control over specific musical features without altering the underlying model architecture. Probes can be trained jointly with the generative model or applied post-training, enabling adaptable control without retraining the model. Our experiments demonstrate that this intervention effectively influences the model output without hindering the music quality. This approach enhances both the flexibility and interpretability of symbolic music generation, enabling better real-world applicability for music generation models.","['https://openalex.org/W4385245566', 'https://openalex.org/W4221148975', 'https://openalex.org/W1971928988', 'https://openalex.org/W2174443198', 'https://openalex.org/W6746363961', 'https://openalex.org/W6743002019', 'https://openalex.org/W6793578827', 'https://openalex.org/W4385763800', 'https://openalex.org/W6849635556', 'https://openalex.org/W6850818779', 'https://openalex.org/W6857617909', 'https://openalex.org/W6849517043', 'https://openalex.org/W6750803041', 'https://openalex.org/W6808277617', 'https://openalex.org/W4395093947', 'https://openalex.org/W6716060660', 'https://openalex.org/W6846234567', 'https://openalex.org/W6771747371', 'https://openalex.org/W6781972552', 'https://openalex.org/W6788307591', 'https://openalex.org/W6776739949', 'https://openalex.org/W3197762246', 'https://openalex.org/W2963408210', 'https://openalex.org/W3092879656', 'https://openalex.org/W3092850823', 'https://openalex.org/W6803953198', 'https://openalex.org/W6730558285', 'https://openalex.org/W6847841245', 'https://openalex.org/W6870158506', 'https://openalex.org/W4225570023', 'https://openalex.org/W6861452846', 'https://openalex.org/W6791320095', 'https://openalex.org/W6800240900', 'https://openalex.org/W4392120787', 'https://openalex.org/W6856990685', 'https://openalex.org/W6765645287', 'https://openalex.org/W6839669674', 'https://openalex.org/W6846011931', 'https://openalex.org/W6853260906', 'https://openalex.org/W6728271538', 'https://openalex.org/W3202070718', 'https://openalex.org/W2946417913', 'https://openalex.org/W2946359678', 'https://openalex.org/W6758532884', 'https://openalex.org/W6759455113', 'https://openalex.org/W2072170522', 'https://openalex.org/W2094639192', 'https://openalex.org/W6617826173', 'https://openalex.org/W2912563180', 'https://openalex.org/W2377213663', 'https://openalex.org/W2569872829', 'https://openalex.org/W2796068387', 'https://openalex.org/W6776218486', 'https://openalex.org/W4301785137', 'https://openalex.org/W4307413986', 'https://openalex.org/W3049272330', 'https://openalex.org/W3129912460', 'https://openalex.org/W4287250916', 'https://openalex.org/W3018535504', 'https://openalex.org/W4387561538', 'https://openalex.org/W4392120655', 'https://openalex.org/W4327669302', 'https://openalex.org/W2746068898', 'https://openalex.org/W4400434698', 'https://openalex.org/W3035602609', 'https://openalex.org/W4319989813', 'https://openalex.org/W4387928713', 'https://openalex.org/W4287802874', 'https://openalex.org/W4379928343', 'https://openalex.org/W4312083936', 'https://openalex.org/W4300180580', 'https://openalex.org/W3213549365']",2025-01-01
https://openalex.org/W3099378280,https://doi.org/10.48550/arxiv.2011.06801,"A Comprehensive Survey on Deep Music Generation: Multi-level Representations, Algorithms, Evaluations, and Future Directions","The utilization of deep learning techniques in generating various contents (such as image, text, etc.) has become a trend. Especially music, the topic of this paper, has attracted widespread attention of countless researchers.The whole process of producing music can be divided into three stages, corresponding to the three levels of music generation: score generation produces scores, performance generation adds performance characteristics to the scores, and audio generation converts scores with performance characteristics into audio by assigning timbre or generates music in audio format directly. Previous surveys have explored the network models employed in the field of automatic music generation. However, the development history, the model evolution, as well as the pros and cons of same music generation task have not been clearly illustrated. This paper attempts to provide an overview of various composition tasks under different music generation levels, covering most of the currently popular music generation tasks using deep learning. In addition, we summarize the datasets suitable for diverse tasks, discuss the music representations, the evaluation methods as well as the challenges under different levels, and finally point out several future directions.","['https://openalex.org/W2153579005', 'https://openalex.org/W2997586082', 'https://openalex.org/W1586966408', 'https://openalex.org/W3102568015', 'https://openalex.org/W2994781558', 'https://openalex.org/W2962721334', 'https://openalex.org/W3029866750', 'https://openalex.org/W2088807535', 'https://openalex.org/W2043003570', 'https://openalex.org/W2054623822', 'https://openalex.org/W2262643855', 'https://openalex.org/W1970183352', 'https://openalex.org/W2892051461', 'https://openalex.org/W2982059008', 'https://openalex.org/W2007842460', 'https://openalex.org/W1904711963', 'https://openalex.org/W2963276790', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963985773', 'https://openalex.org/W3187060660', 'https://openalex.org/W2794234266', 'https://openalex.org/W2945460072', 'https://openalex.org/W2767052532', 'https://openalex.org/W3014271217', 'https://openalex.org/W3105183525', 'https://openalex.org/W2903214502', 'https://openalex.org/W2150933458', 'https://openalex.org/W3010671658', 'https://openalex.org/W1543786473', 'https://openalex.org/W3016250102', 'https://openalex.org/W2963636093', 'https://openalex.org/W2771726741', 'https://openalex.org/W2020166869', 'https://openalex.org/W1544473997', 'https://openalex.org/W1662133657', 'https://openalex.org/W3098518646', 'https://openalex.org/W2963681776', 'https://openalex.org/W2900265519', 'https://openalex.org/W2899775901', 'https://openalex.org/W2890231126', 'https://openalex.org/W2343635552', 'https://openalex.org/W2962947361', 'https://openalex.org/W149236175', 'https://openalex.org/W2396304210', 'https://openalex.org/W2975687953', 'https://openalex.org/W2786254735', 'https://openalex.org/W3109050852', 'https://openalex.org/W2102870814', 'https://openalex.org/W2516406502', 'https://openalex.org/W2964110616', 'https://openalex.org/W2963237661', 'https://openalex.org/W3111868559', 'https://openalex.org/W2800106835', 'https://openalex.org/W2995416527', 'https://openalex.org/W2963889406', 'https://openalex.org/W2998706454', 'https://openalex.org/W2321639984', 'https://openalex.org/W2601110281', 'https://openalex.org/W2985518820', 'https://openalex.org/W2936858163', 'https://openalex.org/W2293272996', 'https://openalex.org/W3092850823', 'https://openalex.org/W2962942158', 'https://openalex.org/W2752134738', 'https://openalex.org/W3136272958', 'https://openalex.org/W2963411769', 'https://openalex.org/W2515336442', 'https://openalex.org/W2963466318', 'https://openalex.org/W2553749962', 'https://openalex.org/W2954914497', 'https://openalex.org/W2606176153', 'https://openalex.org/W2566969886', 'https://openalex.org/W3049272330', 'https://openalex.org/W2991421901', 'https://openalex.org/W2963358591', 'https://openalex.org/W2995670387', 'https://openalex.org/W2164498268', 'https://openalex.org/W2129142580', 'https://openalex.org/W2143129936', 'https://openalex.org/W2973046048', 'https://openalex.org/W2401489644', 'https://openalex.org/W2990617740', 'https://openalex.org/W2891815651', 'https://openalex.org/W2940405045', 'https://openalex.org/W2124539664', 'https://openalex.org/W2952480865', 'https://openalex.org/W2963032576', 'https://openalex.org/W2798113456', 'https://openalex.org/W3011892665', 'https://openalex.org/W1947206766', 'https://openalex.org/W3010943931', 'https://openalex.org/W2963011080', 'https://openalex.org/W2401283236', 'https://openalex.org/W2747023714', 'https://openalex.org/W3015499232', 'https://openalex.org/W2937242376', 'https://openalex.org/W2760038648', 'https://openalex.org/W2125389028', 'https://openalex.org/W1644402181', 'https://openalex.org/W2560662714', 'https://openalex.org/W2001358307', 'https://openalex.org/W2295485174', 'https://openalex.org/W3035430139', 'https://openalex.org/W2239199480', 'https://openalex.org/W3123961192', 'https://openalex.org/W2990692574', 'https://openalex.org/W3007429516', 'https://openalex.org/W2892428174', 'https://openalex.org/W3000168898', 'https://openalex.org/W3104194627', 'https://openalex.org/W2131686285', 'https://openalex.org/W2894749360', 'https://openalex.org/W2903035274', 'https://openalex.org/W2514141612', 'https://openalex.org/W3015516707', 'https://openalex.org/W2283344589', 'https://openalex.org/W2774077477', 'https://openalex.org/W2889244839', 'https://openalex.org/W3112895167', 'https://openalex.org/W2591984255', 'https://openalex.org/W3029579848', 'https://openalex.org/W2963408210', 'https://openalex.org/W2008209667', 'https://openalex.org/W2962046549', 'https://openalex.org/W3034498211', 'https://openalex.org/W2975414524', 'https://openalex.org/W2604567995', 'https://openalex.org/W1556624199', 'https://openalex.org/W3022195800', 'https://openalex.org/W1540596182', 'https://openalex.org/W3093121331', 'https://openalex.org/W2912259457', 'https://openalex.org/W2135341757', 'https://openalex.org/W2949950073', 'https://openalex.org/W2592737436', 'https://openalex.org/W3099425575', 'https://openalex.org/W2513234653', 'https://openalex.org/W2991178431', 'https://openalex.org/W1924770834', 'https://openalex.org/W2890043615', 'https://openalex.org/W2978471471', 'https://openalex.org/W2265822310', 'https://openalex.org/W2902922910', 'https://openalex.org/W2919624000', 'https://openalex.org/W2564034046', 'https://openalex.org/W2778460379', 'https://openalex.org/W3122518304', 'https://openalex.org/W2137619888', 'https://openalex.org/W2031410675', 'https://openalex.org/W2761241264', 'https://openalex.org/W2999344275', 'https://openalex.org/W2902235484', 'https://openalex.org/W2963253162', 'https://openalex.org/W3090619461', 'https://openalex.org/W2963782041', 'https://openalex.org/W3183478142', 'https://openalex.org/W2288593361', 'https://openalex.org/W3003673875', 'https://openalex.org/W107523896', 'https://openalex.org/W2118763712', 'https://openalex.org/W2805697608', 'https://openalex.org/W2899000566', 'https://openalex.org/W1987905533', 'https://openalex.org/W2962981281', 'https://openalex.org/W2921576841', 'https://openalex.org/W2160473997', 'https://openalex.org/W2994986888', 'https://openalex.org/W2978309747', 'https://openalex.org/W1819710477', 'https://openalex.org/W2963233633', 'https://openalex.org/W2052265952', 'https://openalex.org/W3010903955', 'https://openalex.org/W2753868141', 'https://openalex.org/W2892104732', 'https://openalex.org/W2946521317', 'https://openalex.org/W2982125965', 'https://openalex.org/W2990713144', 'https://openalex.org/W2984106626', 'https://openalex.org/W2962883485', 'https://openalex.org/W2288119145', 'https://openalex.org/W2970730223', 'https://openalex.org/W2575455538', 'https://openalex.org/W2899124505', 'https://openalex.org/W2043162293', 'https://openalex.org/W2962941684', 'https://openalex.org/W2475287302', 'https://openalex.org/W1946974084', 'https://openalex.org/W3012498027', 'https://openalex.org/W2067709094', 'https://openalex.org/W2809621972', 'https://openalex.org/W2964286535', 'https://openalex.org/W3049247973', 'https://openalex.org/W2963557407', 'https://openalex.org/W2165527202', 'https://openalex.org/W2989708046', 'https://openalex.org/W1977444015', 'https://openalex.org/W2725868244', 'https://openalex.org/W2898148140', 'https://openalex.org/W2898827701', 'https://openalex.org/W2129192849', 'https://openalex.org/W60920252', 'https://openalex.org/W3030187020', 'https://openalex.org/W2945053121', 'https://openalex.org/W2963656263', 'https://openalex.org/W3092135915', 'https://openalex.org/W2788612614', 'https://openalex.org/W37402343', 'https://openalex.org/W2991108091', 'https://openalex.org/W2327437958', 'https://openalex.org/W2963568710', 'https://openalex.org/W2940676016', 'https://openalex.org/W2981967511', 'https://openalex.org/W2948211236', 'https://openalex.org/W2049367655', 'https://openalex.org/W2964016415', 'https://openalex.org/W2789365424', 'https://openalex.org/W3015843452', 'https://openalex.org/W2112349754', 'https://openalex.org/W2584032004', 'https://openalex.org/W2120847449', 'https://openalex.org/W3024973272', 'https://openalex.org/W2064675550', 'https://openalex.org/W3094138986', 'https://openalex.org/W2903005299', 'https://openalex.org/W12861549', 'https://openalex.org/W2896788168', 'https://openalex.org/W2775473773', 'https://openalex.org/W2964307104', 'https://openalex.org/W2963493667', 'https://openalex.org/W2963551352', 'https://openalex.org/W2142980624', 'https://openalex.org/W2901024736', 'https://openalex.org/W2799213409', 'https://openalex.org/W3040218036', 'https://openalex.org/W2964289981', 'https://openalex.org/W2914745777', 'https://openalex.org/W2786651925', 'https://openalex.org/W2891757161', 'https://openalex.org/W3031000691', 'https://openalex.org/W2946488335', 'https://openalex.org/W2338312508', 'https://openalex.org/W2174291476', 'https://openalex.org/W2963575853', 'https://openalex.org/W2884558435', 'https://openalex.org/W3038344852', 'https://openalex.org/W3018535504', 'https://openalex.org/W2067621398', 'https://openalex.org/W2076769328', 'https://openalex.org/W1496920063', 'https://openalex.org/W3125080906', 'https://openalex.org/W2964268978', 'https://openalex.org/W3092879656', 'https://openalex.org/W2941633540', 'https://openalex.org/W3154236293', 'https://openalex.org/W2887720658', 'https://openalex.org/W3173531154', 'https://openalex.org/W2795201383', 'https://openalex.org/W2972812066', 'https://openalex.org/W2074310426', 'https://openalex.org/W2950577311', 'https://openalex.org/W2894295011', 'https://openalex.org/W3154171150', 'https://openalex.org/W3036051869', 'https://openalex.org/W2951535099', 'https://openalex.org/W2949382160', 'https://openalex.org/W2962714411', 'https://openalex.org/W3049174246', 'https://openalex.org/W3048830653', 'https://openalex.org/W3031005264', 'https://openalex.org/W3021164770', 'https://openalex.org/W2034161986', 'https://openalex.org/W3109510123', 'https://openalex.org/W2964105398', 'https://openalex.org/W763753110', 'https://openalex.org/W2899724567', 'https://openalex.org/W2997367363', 'https://openalex.org/W3016014797', 'https://openalex.org/W2963468922', 'https://openalex.org/W2145027674', 'https://openalex.org/W3036376755', 'https://openalex.org/W3048084370', 'https://openalex.org/W1990611383', 'https://openalex.org/W2912618358', 'https://openalex.org/W2964706117', 'https://openalex.org/W2963857374', 'https://openalex.org/W2592549691', 'https://openalex.org/W2582502747', 'https://openalex.org/W2086699924', 'https://openalex.org/W1556219185', 'https://openalex.org/W2921857201', 'https://openalex.org/W2788156829', 'https://openalex.org/W2808437126', 'https://openalex.org/W2972910332', 'https://openalex.org/W3100883077', 'https://openalex.org/W3108488605', 'https://openalex.org/W2995005087', 'https://openalex.org/W2327497238', 'https://openalex.org/W2989822430', 'https://openalex.org/W2963158134', 'https://openalex.org/W2579406683', 'https://openalex.org/W2739969482', 'https://openalex.org/W2978885571', 'https://openalex.org/W2572852026', 'https://openalex.org/W3048092838', 'https://openalex.org/W134527144', 'https://openalex.org/W3008434711', 'https://openalex.org/W3111037961', 'https://openalex.org/W2793183272', 'https://openalex.org/W2982431856', 'https://openalex.org/W2142996485', 'https://openalex.org/W2989919526', 'https://openalex.org/W3097679990', 'https://openalex.org/W2408435475', 'https://openalex.org/W3097469673', 'https://openalex.org/W3019084079', 'https://openalex.org/W2995233853', 'https://openalex.org/W2989955315', 'https://openalex.org/W1979251464', 'https://openalex.org/W2893613496', 'https://openalex.org/W3131643527', 'https://openalex.org/W2965526162', 'https://openalex.org/W2907543363', 'https://openalex.org/W2794719876', 'https://openalex.org/W2990885710', 'https://openalex.org/W2559110679', 'https://openalex.org/W3175179074', 'https://openalex.org/W2902184207']",2020-11-13
https://openalex.org/W3182466123,https://doi.org/10.48550/arxiv.2107.05944,The Piano Inpainting Application,"Autoregressive models are now capable of generating high-quality minute-long expressive MIDI piano performances. Even though this progress suggests new tools to assist music composition, we observe that generative algorithms are still not widely used by artists due to the limited control they offer, prohibitive inference times or the lack of integration within musicians' workflows. In this work, we present the Piano Inpainting Application (PIA), a generative model focused on inpainting piano performances, as we believe that this elementary operation (restoring missing parts of a piano performance) encourages human-machine interaction and opens up new ways to approach music composition. Our approach relies on an encoder-decoder Linear Transformer architecture trained on a novel representation for MIDI piano performances termed Structured MIDI Encoding. By uncovering an interesting synergy between Linear Transformers and our inpainting task, we are able to efficiently inpaint contiguous regions of a piano performance, which makes our model suitable for interactive and responsive A.I.-assisted composition. Finally, we introduce our freely-available Ableton Live PIA plugin, which allows musicians to smoothly generate or modify any MIDI clip using PIA within a widely-used professional Digital Audio Workstation.","['https://openalex.org/W3092879656', 'https://openalex.org/W3035435378', 'https://openalex.org/W2963411769', 'https://openalex.org/W2962212541', 'https://openalex.org/W2415924536', 'https://openalex.org/W3121043774', 'https://openalex.org/W2997347790', 'https://openalex.org/W3093121331', 'https://openalex.org/W3090619461', 'https://openalex.org/W3034573343', 'https://openalex.org/W2940744433', 'https://openalex.org/W3018535504', 'https://openalex.org/W2963408210', 'https://openalex.org/W2957258179', 'https://openalex.org/W2901638613', 'https://openalex.org/W2964110616', 'https://openalex.org/W2963925437', 'https://openalex.org/W2938704169', 'https://openalex.org/W2963575853', 'https://openalex.org/W2959020461', 'https://openalex.org/W2963253162', 'https://openalex.org/W3098518646', 'https://openalex.org/W2626778328', 'https://openalex.org/W2991421901', 'https://openalex.org/W3105938520']",2021-07-13
https://openalex.org/W3038090892,https://doi.org/10.48550/arxiv.2006.13331,Incorporating Music Knowledge in Continual Dataset Augmentation for Music Generation,"Deep learning has rapidly become the state-of-the-art approach for music generation. However, training a deep model typically requires a large training set, which is often not available for specific musical styles. In this paper, we present augmentative generation (Aug-Gen), a method of dataset augmentation for any music generation system trained on a resource-constrained domain. The key intuition of this method is that the training data for a generative system can be augmented by examples the system produces during the course of training, provided these examples are of sufficiently high quality and variety. We apply Aug-Gen to Transformer-based chorale generation in the style of J.S. Bach, and show that this allows for longer training and results in better generative output.","['https://openalex.org/W2752134738', 'https://openalex.org/W2891815651', 'https://openalex.org/W2965114873', 'https://openalex.org/W3018535504', 'https://openalex.org/W2963575853', 'https://openalex.org/W3037480493', 'https://openalex.org/W2626778328', 'https://openalex.org/W2129192849']",2020-06-23
https://openalex.org/W3212222439,https://doi.org/10.48550/arxiv.2111.07657,Symbolic Music Loop Generation with VQ-VAE,"Music is a repetition of patterns and rhythms. It can be composed by repeating a certain number of bars in a structured way. In this paper, the objective is to generate a loop of 8 bars that can be used as a building block of music. Even considering musical diversity, we assume that music patterns familiar to humans can be defined in a finite set. With explicit rules to extract loops from music, we found that discrete representations are sufficient to model symbolic music sequences. Among VAE family, musical properties from VQ-VAE are better observed rather than other models. Further, to emphasize musical structure, we have manipulated discrete latent features to be repetitive so that the properties are more strengthened. Quantitative and qualitative experiments are extensively conducted to verify our assumptions.","['https://openalex.org/W2805697608', 'https://openalex.org/W2963408210', 'https://openalex.org/W2908510526', 'https://openalex.org/W2971074500', 'https://openalex.org/W2963681776', 'https://openalex.org/W2919624000', 'https://openalex.org/W2963032576', 'https://openalex.org/W2946521317', 'https://openalex.org/W3119786062', 'https://openalex.org/W3034758887', 'https://openalex.org/W2475687244', 'https://openalex.org/W3021164770', 'https://openalex.org/W1959608418', 'https://openalex.org/W2753738274', 'https://openalex.org/W3018535504', 'https://openalex.org/W3099378280', 'https://openalex.org/W2963403868', 'https://openalex.org/W1677182931', 'https://openalex.org/W2963799213', 'https://openalex.org/W1884029234', 'https://openalex.org/W648786980']",2021-11-15
https://openalex.org/W4404503586,https://doi.org/10.3390/computers13110300,DiscHAR: A Discrete Approach to Enhance Human Activity Recognition in Cyber Physical Systems: Smart Homes,"The main challenges in smart home systems and cyber-physical systems come from not having enough data and unclear interpretation; thus, there is still a lot to be done in this field. In this work, we propose a practical approach called Discrete Human Activity Recognition (DiscHAR) based on prior research to enhance Human Activity Recognition (HAR). Our goal is to generate diverse data to build better models for activity classification. To tackle overfitting, which often occurs with small datasets, we generate data and convert them into discrete forms, improving classification accuracy. Our methodology includes advanced techniques like the R-Frame method for sampling and the Mixed-up approach for data generation. We apply K-means vector quantization to categorize the data, and through the elbow method, we determine the optimal number of clusters. The discrete sequences are converted into one-hot encoded vectors and fed into a CNN model to ensure precise recognition of human activities. Evaluations on the OPP79, PAMAP2, and WISDM datasets show that our approach outperforms existing models, achieving 89% accuracy for OPP79, 93.24% for PAMAP2, and 100% for WISDM. These results demonstrate the model’s effectiveness in identifying complex activities captured by wearable devices. Our work combines theory and practice to address ongoing challenges in this field, aiming to improve the reliability and performance of activity recognition systems in dynamic environments.","['https://openalex.org/W4296558744', 'https://openalex.org/W3162602085', 'https://openalex.org/W4378464606', 'https://openalex.org/W4391846608', 'https://openalex.org/W4381746737', 'https://openalex.org/W4390488752', 'https://openalex.org/W4285744644', 'https://openalex.org/W2926525016', 'https://openalex.org/W4394866263', 'https://openalex.org/W4293554011', 'https://openalex.org/W4285022937', 'https://openalex.org/W4321457716', 'https://openalex.org/W4366249647', 'https://openalex.org/W4383557812', 'https://openalex.org/W6795297288', 'https://openalex.org/W4387096227', 'https://openalex.org/W6842702491', 'https://openalex.org/W4280628094', 'https://openalex.org/W2054780155', 'https://openalex.org/W4206780588', 'https://openalex.org/W4200317051', 'https://openalex.org/W3186799104', 'https://openalex.org/W4381746838', 'https://openalex.org/W4387321395', 'https://openalex.org/W4372349107', 'https://openalex.org/W4386075606', 'https://openalex.org/W4386076211', 'https://openalex.org/W2026297770', 'https://openalex.org/W2126511896', 'https://openalex.org/W2017634428', 'https://openalex.org/W4399202664', 'https://openalex.org/W2073401630', 'https://openalex.org/W4399740865', 'https://openalex.org/W3209735728', 'https://openalex.org/W3133590696', 'https://openalex.org/W6888962972', 'https://openalex.org/W3161500668', 'https://openalex.org/W4294805216']",2024-11-19
https://openalex.org/W4394937522,https://doi.org/10.1109/ic-etite58242.2024.10493835,CallTran: Voice Translation for End-to-End Communication over the Internet,"The importance of language translation becomes evident when two individuals, each speaking a different mother tongue and lacking a common language, seek to communicate effectively. This article presents the development and evaluation of a mobile application for voice-to-voice translation over the Internet. The application employs three main technologies: speech recognition, machine translation, and speech synthesis. Google APIs (speech-to-text API, text-to-text-translator API, and text-to-speech synthesizer API) were used to implement the system. The evaluation showed that the system achieved an overall accuracy of 85% in recognizing and translating speech input from users. However, the accuracy varied across different languages. The system was also found to be effective in facilitating communication between users who speak different languages. The limitations of the system were identified in its performance in noisy or crowded environments and the handling of regional accents and dialects. Overall, the developed system has the potential to bridge language barriers and facilitate communication among people speaking different languages.","['https://openalex.org/W2154304575', 'https://openalex.org/W2051745966', 'https://openalex.org/W2477658439', 'https://openalex.org/W4372349107', 'https://openalex.org/W1984970186', 'https://openalex.org/W3133473264', 'https://openalex.org/W3099313647', 'https://openalex.org/W2274966938']",2024-02-22
https://openalex.org/W4408779662,https://doi.org/10.48175/ijarsct-23429,AI-Powered Heritage Exploration in Tamil Nadu Historical Wonders,"Heritage tourism is being transformed by AI-driven solutions that offer real-time, personalized, and interactive experiences. This project introduces an intelligent platform that integrates multilingual voice assistance, GPS-enabled navigation, and AI-generated historical content to enrich visitor engagement at cultural sites. The system delivers dynamic insights using advanced language models, making historical exploration more accessible, engaging, and informative. Unlike conventional approaches that rely on static information and manual translations, this platform provides instant voice-guided explanations and adaptive recommendations based on user preferences. Tourists can explore heritage sites with location-based storytelling and interactive itineraries, enhancing their cultural journey. The system ensures seamless accessibility for diverse audiences by supporting multiple languages and AI-driven narration. Beyond improving the tourist experience, this innovation contributes to heritage preservation and digital accessibility. Designed for scalability and adaptability, the platform can be extended to various historical locations, ensuring long-term sustainability and broader cultural education and tourism outreach...","['https://openalex.org/W2997731747', 'https://openalex.org/W4372349107', 'https://openalex.org/W4322731117']",2025-03-24
https://openalex.org/W4401702377,https://doi.org/10.1002/lary.31713,Quantification of Automatic Speech Recognition System Performance on d/Deaf and Hard of Hearing Speech,"Objective To evaluate the performance of commercial automatic speech recognition (ASR) systems on d/Deaf and hard‐of‐hearing (d/Dhh) speech. Methods A corpus containing 850 audio files of d/Dhh and normal hearing (NH) speech from the University of Memphis Speech Perception Assessment Laboratory was tested on four speech‐to‐text application program interfaces (APIs): Amazon Web Services, Microsoft Azure, Google Chirp, and OpenAI Whisper. We quantified the Word Error Rate (WER) of API transcriptions for 24 d/Dhh and nine NH participants and performed subgroup analysis by speech intelligibility classification (SIC), hearing loss (HL) onset, and primary communication mode. Results Mean WER averaged across APIs was 10 times higher for the d/Dhh group (52.6%) than the NH group (5.0%). APIs performed significantly worse for “low” and “medium” SIC (85.9% and 46.6% WER, respectively) as compared to “high” SIC group (9.5% WER, comparable to NH group). APIs performed significantly worse for speakers with prelingual HL relative to postlingual HL (80.5% and 37.1% WER, respectively). APIs performed significantly worse for speakers primarily communicating with sign language (70.2% WER) relative to speakers with both oral and sign language communication (51.5%) or oral communication only (19.7%). Conclusion Commercial ASR systems underperform for d/Dhh individuals, especially those with “low” and “medium” SIC, prelingual onset of HL, and sign language as primary communication mode. This contrasts with Big Tech companies' promises of accessibility, indicating the need for ASR systems ethically trained on heterogeneous d/Dhh speech data. Level of Evidence 3 Laryngoscope , 2024","['https://openalex.org/W4375959287', 'https://openalex.org/W3012624518', 'https://openalex.org/W3087246835', 'https://openalex.org/W4317830346', 'https://openalex.org/W2075699685', 'https://openalex.org/W2008012864', 'https://openalex.org/W2066537828', 'https://openalex.org/W2149477251', 'https://openalex.org/W2033379963', 'https://openalex.org/W14146365', 'https://openalex.org/W2765687795', 'https://openalex.org/W2018239554', 'https://openalex.org/W2585170615', 'https://openalex.org/W2737426676', 'https://openalex.org/W4288117694', 'https://openalex.org/W4392909068', 'https://openalex.org/W2314750237', 'https://openalex.org/W2048640028', 'https://openalex.org/W1998547550', 'https://openalex.org/W2021714242', 'https://openalex.org/W2123177271', 'https://openalex.org/W1985869155', 'https://openalex.org/W2953775953', 'https://openalex.org/W2068415541', 'https://openalex.org/W2015277205', 'https://openalex.org/W4311000453', 'https://openalex.org/W4391833135', 'https://openalex.org/W4392904549', 'https://openalex.org/W193308610', 'https://openalex.org/W2067241962']",2024-08-19
https://openalex.org/W4392904258,https://doi.org/10.1109/icassp48485.2024.10446625,AV2WAV: Diffusion-Based Re-Synthesis from Continuous Self-Supervised Features for Audio-Visual Speech Enhancement,"Speech enhancement systems are typically trained using pairs of clean and noisy speech. In audio-visual speech enhancement (AVSE), there is not as much ground-truth clean data available; most audio-visual datasets are collected in real-world environments with background noise and reverberation, hampering the development of AVSE. In this work, we introduce AV2Wav, a resynthesis-based audio-visual speech enhancement approach that can generate clean speech despite the challenges of real-world training data. We obtain a subset of nearly clean speech from an audio-visual corpus using a neural quality estimator, and then train a diffusion model on this subset to generate waveforms conditioned on continuous speech representations from AV-HuBERT with noise-robust training. We use continuous rather than discrete representations to retain prosody and speaker information. With this vocoding task alone, the model can perform speech enhancement better than a masking-based baseline. We further fine-tune the diffusion model on clean/noisy utterance pairs to improve the performance. Our approach outperforms a masking-based baseline in terms of both automatic metrics and a human listening test and is close in quality to the target speech in the listening test. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>","['https://openalex.org/W2788241093', 'https://openalex.org/W4385478414', 'https://openalex.org/W2964171275', 'https://openalex.org/W3182657421', 'https://openalex.org/W4312337341', 'https://openalex.org/W4386076005', 'https://openalex.org/W3140429000', 'https://openalex.org/W2015143272', 'https://openalex.org/W2808631503', 'https://openalex.org/W6754420807', 'https://openalex.org/W6810168380', 'https://openalex.org/W6782760101', 'https://openalex.org/W4372266927', 'https://openalex.org/W4226403810', 'https://openalex.org/W4224933800', 'https://openalex.org/W6853302197', 'https://openalex.org/W4297841641', 'https://openalex.org/W2963192057', 'https://openalex.org/W4372260337', 'https://openalex.org/W6847363464', 'https://openalex.org/W4319862437', 'https://openalex.org/W6780226713', 'https://openalex.org/W6631190155', 'https://openalex.org/W6936113694', 'https://openalex.org/W349236604', 'https://openalex.org/W4281820413', 'https://openalex.org/W6783713337', 'https://openalex.org/W1522301498', 'https://openalex.org/W4379474007', 'https://openalex.org/W4311000453', 'https://openalex.org/W2891205112', 'https://openalex.org/W4221153068', 'https://openalex.org/W3121370741', 'https://openalex.org/W2964058413']",2024-03-18
https://openalex.org/W4407843714,https://doi.org/10.3390/electronics14050844,Acoustic Feature Excitation-and-Aggregation Network Based on Multi-Task Learning for Speech Emotion Recognition,"In recent years, substantial research has focused on emotion recognition using multi-stream speech representations. In existing multi-stream speech emotion recognition (SER) approaches, effectively extracting and fusing speech features is crucial. To overcome the bottleneck in SER caused by the fusion of inter-feature information, including challenges like modeling complex feature relations and the inefficiency of fusion methods, this paper proposes an SER framework based on multi-task learning, named AFEA-Net. The framework consists of a speech emotion alignment learning (SEAL), an acoustic feature excitation-and-aggregation mechanism (AFEA), and a continuity learning. First, SEAL aligns sentiment information between WavLM and Fbank features. Then, we design an acoustic feature excitation-and-aggregation mechanism to adaptively calibrate and merge the two features. Furthermore, we introduce a continuity learning strategy to explore the distinctiveness and complementarity of dual-stream features from intra- and inter-speech. Experimental results on the publicly available IEMOCAP and RAVDESS sentiment datasets show that our proposed approach outperforms state-of-the-art SER approaches. Specifically, we achieve 75.1% WA, 75.3% UAR, 76% precision, and 75.4% F1-score on IEMOCAP, and 80.3%, 80.6%, 80.8%, and 80.4% on RAVDESS, respectively.","['https://openalex.org/W6686207219', 'https://openalex.org/W2032254851', 'https://openalex.org/W2080830759', 'https://openalex.org/W4285106979', 'https://openalex.org/W2074788634', 'https://openalex.org/W1844030040', 'https://openalex.org/W2111926505', 'https://openalex.org/W1995562189', 'https://openalex.org/W3161659450', 'https://openalex.org/W4226442948', 'https://openalex.org/W4385822457', 'https://openalex.org/W3197069310', 'https://openalex.org/W6790827927', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W4225635674', 'https://openalex.org/W4402669711', 'https://openalex.org/W4385822707', 'https://openalex.org/W4372266023', 'https://openalex.org/W4312857458', 'https://openalex.org/W4387247604', 'https://openalex.org/W4402112038', 'https://openalex.org/W4399322270', 'https://openalex.org/W2146334809', 'https://openalex.org/W2803193013', 'https://openalex.org/W2784665486', 'https://openalex.org/W2043152858', 'https://openalex.org/W3013363049', 'https://openalex.org/W2096352448', 'https://openalex.org/W2969889150', 'https://openalex.org/W4224234075', 'https://openalex.org/W2972452068', 'https://openalex.org/W3196735225', 'https://openalex.org/W4295957212', 'https://openalex.org/W4372269797', 'https://openalex.org/W4392904469', 'https://openalex.org/W4221162872', 'https://openalex.org/W4297841699', 'https://openalex.org/W4386158972', 'https://openalex.org/W4383551941', 'https://openalex.org/W4387055738', 'https://openalex.org/W3209984917', 'https://openalex.org/W3197642003', 'https://openalex.org/W4375869379', 'https://openalex.org/W4385822991', 'https://openalex.org/W2919250930', 'https://openalex.org/W6801377430', 'https://openalex.org/W4385823320', 'https://openalex.org/W4385822715', 'https://openalex.org/W3108601100', 'https://openalex.org/W3097149715', 'https://openalex.org/W3204087964', 'https://openalex.org/W4313887688', 'https://openalex.org/W4385823394', 'https://openalex.org/W4372341110', 'https://openalex.org/W4372266717', 'https://openalex.org/W4385823319', 'https://openalex.org/W4392903514', 'https://openalex.org/W3094805191', 'https://openalex.org/W3215440557', 'https://openalex.org/W4224920209', 'https://openalex.org/W4402112107', 'https://openalex.org/W3093863290', 'https://openalex.org/W4386933967', 'https://openalex.org/W4393193328', 'https://openalex.org/W3008039831', 'https://openalex.org/W4281760076', 'https://openalex.org/W4388303844', 'https://openalex.org/W3036601975', 'https://openalex.org/W3130875629']",2025-02-21
https://openalex.org/W4407900542,https://doi.org/10.1109/tnnls.2025.3534822,Disentanglement of Prosody Representations via Diffusion Models and Scheduled Gradient Reversal,"Prosody plays a fundamental role in human speech and communication, facilitating intelligibility and conveying emotional and cognitive states. Extracting accurate prosodic information from speech is vital for building assistive technology, such as controllable speech synthesis, speaking style transfer, and speech emotion recognition (SER). However, it is challenging to disentangle speaker-independent prosody representations since prosodic attributes, such as intonation, excessively entangle with speaker-specific attributes, e.g., pitch. In this article, we propose a novel model, called Diffsody, to disentangle and refine prosody representations: 1) to disentangle prosody representations, we leverage the expressive generative ability of a diffusion model by conditioning it on quantified semantic information and pretrained speaker embeddings. Additionally, a prosody encoder automatically learns prosody representations used for spectrogram reconstruction in an unsupervised fashion; and 2) to refine and learn speaker-invariant prosody representations, a scheduled gradient reversal layer (sGRL) is proposed and integrated into the prosody encoder of Diffsody. We extensively evaluate Diffsody through qualitative and quantitative means. t-SNE visualization and speaker verification experiments demonstrate the efficacy of the sGRL method in preventing speaker-specific information leakage. Experimental results on speaker-independent SER and automatic depression detection (ADD) tasks demonstrate that Diffsody can efficiently factorize speaker-independent prosody representations, resulting in a significant boost in SER and ADD. In addition, Diffsody synergistically integrates with the semantic representation model WavLM, which leads to a discernibly elevated performance, outperforming contemporary methods in both SER and ADD tasks. Furthermore, the Diffsody model exhibits promising potential for various practical applications, such as voice or style conversion. Some audio samples can be found on our https://leyuanqu.github.io/Diffsody/demo website.","['https://openalex.org/W2017561954', 'https://openalex.org/W2150791533', 'https://openalex.org/W4366493008', 'https://openalex.org/W6762533536', 'https://openalex.org/W6776390925', 'https://openalex.org/W6803547063', 'https://openalex.org/W3135547455', 'https://openalex.org/W6683017116', 'https://openalex.org/W2159570759', 'https://openalex.org/W2963447013', 'https://openalex.org/W4296068587', 'https://openalex.org/W4390872387', 'https://openalex.org/W4367359628', 'https://openalex.org/W4390873135', 'https://openalex.org/W6750489868', 'https://openalex.org/W4210777104', 'https://openalex.org/W4224926192', 'https://openalex.org/W6763832098', 'https://openalex.org/W2904459034', 'https://openalex.org/W4392904630', 'https://openalex.org/W3204457821', 'https://openalex.org/W3006705189', 'https://openalex.org/W3015308237', 'https://openalex.org/W3006926732', 'https://openalex.org/W4406462025', 'https://openalex.org/W4402669711', 'https://openalex.org/W6679045638', 'https://openalex.org/W6639480849', 'https://openalex.org/W2969985801', 'https://openalex.org/W3024869864', 'https://openalex.org/W2402146185', 'https://openalex.org/W2752782242', 'https://openalex.org/W2928165649', 'https://openalex.org/W6737575990', 'https://openalex.org/W2137639365', 'https://openalex.org/W3200314169', 'https://openalex.org/W3209059054', 'https://openalex.org/W4287887366', 'https://openalex.org/W4387247604', 'https://openalex.org/W6783182287', 'https://openalex.org/W6810926057', 'https://openalex.org/W4385245566', 'https://openalex.org/W2995181338', 'https://openalex.org/W2949175740', 'https://openalex.org/W6778823374', 'https://openalex.org/W2629461003', 'https://openalex.org/W6631190155', 'https://openalex.org/W2936774411', 'https://openalex.org/W1494198834', 'https://openalex.org/W2146334809', 'https://openalex.org/W2941086874', 'https://openalex.org/W3162475537', 'https://openalex.org/W3096723250', 'https://openalex.org/W2972498864', 'https://openalex.org/W4221162872', 'https://openalex.org/W3130293557', 'https://openalex.org/W4313887688', 'https://openalex.org/W4375869114', 'https://openalex.org/W4392909765', 'https://openalex.org/W2973049979', 'https://openalex.org/W6810007534', 'https://openalex.org/W3209984917', 'https://openalex.org/W4392172995', 'https://openalex.org/W4392903514', 'https://openalex.org/W4392904442', 'https://openalex.org/W6755207826', 'https://openalex.org/W6691669583', 'https://openalex.org/W6838923520', 'https://openalex.org/W3015554124', 'https://openalex.org/W3101080567', 'https://openalex.org/W2973133192', 'https://openalex.org/W2085662862']",2025-02-24
https://openalex.org/W4385570154,https://doi.org/10.18653/v1/2023.acl-long.436,CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation,"End-to-end speech translation (ST) is the task of translating speech signals in the source language into text in the target language. As a cross-modal task, end-to-end ST is difficult to train with limited data. Existing methods often try to transfer knowledge from machine translation (MT), but their performances are restricted by the modality gap between speech and text. In this paper, we propose Cross-modal Mixup via Optimal Transport (CMOT) to overcome the modality gap. We find the alignment between speech and text sequences via optimal transport and then mix up the sequences from different modalities at a token level using the alignment. Experiments on the MuST-C ST benchmark demonstrate that CMOT achieves an average BLEU of 30.0 in 8 translation directions, outperforming previous methods. Further analysis shows CMOT can adaptively find the alignment between modalities, which helps alleviate the modality gap between speech and text.","['https://openalex.org/W2933138175', 'https://openalex.org/W2765407302', 'https://openalex.org/W4285215858', 'https://openalex.org/W3090449556', 'https://openalex.org/W3034571331', 'https://openalex.org/W4226120743', 'https://openalex.org/W1522301498', 'https://openalex.org/W658020064', 'https://openalex.org/W3175809709', 'https://openalex.org/W3096490862', 'https://openalex.org/W1556750318', 'https://openalex.org/W2466918907', 'https://openalex.org/W385466589', 'https://openalex.org/W2966645965', 'https://openalex.org/W3169320628', 'https://openalex.org/W3198299542', 'https://openalex.org/W4288817190', 'https://openalex.org/W2949328740', 'https://openalex.org/W4385569716', 'https://openalex.org/W4280647381', 'https://openalex.org/W3162037819', 'https://openalex.org/W4221163209', 'https://openalex.org/W4307783940', 'https://openalex.org/W4206471589', 'https://openalex.org/W3168212167', 'https://openalex.org/W3037217258', 'https://openalex.org/W3017454464', 'https://openalex.org/W4287329822', 'https://openalex.org/W2945286432', 'https://openalex.org/W4367841185', 'https://openalex.org/W4385571004', 'https://openalex.org/W3105825505', 'https://openalex.org/W4300833946', 'https://openalex.org/W3163093788', 'https://openalex.org/W3006988520', 'https://openalex.org/W3196833881', 'https://openalex.org/W2962680099', 'https://openalex.org/W2972448360', 'https://openalex.org/W3176382501', 'https://openalex.org/W2963250244', 'https://openalex.org/W2973048981', 'https://openalex.org/W2901607128', 'https://openalex.org/W1494198834', 'https://openalex.org/W2964172053', 'https://openalex.org/W3176455679', 'https://openalex.org/W4382318914', 'https://openalex.org/W3037542581', 'https://openalex.org/W4319862442', 'https://openalex.org/W4303619686', 'https://openalex.org/W3118578889', 'https://openalex.org/W2963532001', 'https://openalex.org/W2978099976', 'https://openalex.org/W3036601975', 'https://openalex.org/W4298201654', 'https://openalex.org/W4304195479', 'https://openalex.org/W4287890956', 'https://openalex.org/W2896457183', 'https://openalex.org/W4226316887', 'https://openalex.org/W4385245566', 'https://openalex.org/W4214911019', 'https://openalex.org/W2512924740', 'https://openalex.org/W4224931728', 'https://openalex.org/W2997436923', 'https://openalex.org/W2747874407', 'https://openalex.org/W2158131535']",2023-01-01
https://openalex.org/W4385571004,https://doi.org/10.18653/v1/2023.acl-long.884,Understanding and Bridging the Modality Gap for Speech Translation,"How to achieve better end-to-end speech translation (ST) by leveraging (text) machine translation (MT) data? Among various existing techniques, multi-task learning is one of the effective ways to share knowledge between ST and MT in which additional MT data can help to learn source-to-target mapping. However, due to the differences between speech and text, there is always a gap between ST and MT. In this paper, we first aim to understand this modality gap from the target-side representation differences, and link the modality gap to another well-known problem in neural machine translation: exposure bias. We find that the modality gap is relatively small during training except for some difficult cases, but keeps increasing during inference due to the cascading effect. To address these problems, we propose the Cross-modal Regularization with Scheduled Sampling (Cress) method. Specifically, we regularize the output predictions of ST and MT, whose target-side contexts are derived by sampling between ground truth words and self-generated words with a varying probability. Furthermore, we introduce token-level adaptive training which assigns different training weights to target tokens to handle difficult cases with large modality gaps. Experiments and analysis show that our approach effectively bridges the modality gap, and achieves significant improvements over a strong baseline in all eight directions of the MuST-C dataset.","['https://openalex.org/W4221155340', 'https://openalex.org/W2933138175', 'https://openalex.org/W2964352247', 'https://openalex.org/W2978099976', 'https://openalex.org/W3113676066', 'https://openalex.org/W3095410713', 'https://openalex.org/W2995181338', 'https://openalex.org/W3007142233', 'https://openalex.org/W4385245566', 'https://openalex.org/W222053410', 'https://openalex.org/W1522301498', 'https://openalex.org/W2964345285', 'https://openalex.org/W4287890956', 'https://openalex.org/W3175995572', 'https://openalex.org/W2964161387', 'https://openalex.org/W4288021071', 'https://openalex.org/W4223622550', 'https://openalex.org/W2945700568', 'https://openalex.org/W3176382501', 'https://openalex.org/W4285215858', 'https://openalex.org/W3186200218', 'https://openalex.org/W4226120743', 'https://openalex.org/W2176263492', 'https://openalex.org/W2972448360', 'https://openalex.org/W648786980', 'https://openalex.org/W3176455679', 'https://openalex.org/W3006988520', 'https://openalex.org/W3034571331', 'https://openalex.org/W2963532001', 'https://openalex.org/W4385570154', 'https://openalex.org/W3172698324', 'https://openalex.org/W3168212167', 'https://openalex.org/W3030437843', 'https://openalex.org/W2964172053', 'https://openalex.org/W2758950307', 'https://openalex.org/W3034625919', 'https://openalex.org/W2963463964', 'https://openalex.org/W3174446152', 'https://openalex.org/W1494198834', 'https://openalex.org/W3173767661', 'https://openalex.org/W3213029956', 'https://openalex.org/W4385569716', 'https://openalex.org/W3162037819', 'https://openalex.org/W2973048981', 'https://openalex.org/W2945286432', 'https://openalex.org/W10548402', 'https://openalex.org/W2888442053', 'https://openalex.org/W4385573012', 'https://openalex.org/W2997436923', 'https://openalex.org/W3113908264', 'https://openalex.org/W3139878283', 'https://openalex.org/W3036601975', 'https://openalex.org/W3017454464', 'https://openalex.org/W4285112711', 'https://openalex.org/W3099417250', 'https://openalex.org/W2949328740', 'https://openalex.org/W3102811925', 'https://openalex.org/W2605131327', 'https://openalex.org/W4285158119', 'https://openalex.org/W3119308075', 'https://openalex.org/W4285115391', 'https://openalex.org/W2101105183', 'https://openalex.org/W2508809683', 'https://openalex.org/W3207222250', 'https://openalex.org/W4221163209', 'https://openalex.org/W3198299542', 'https://openalex.org/W3174864715', 'https://openalex.org/W3162471442', 'https://openalex.org/W3105825505', 'https://openalex.org/W3176711365', 'https://openalex.org/W2292087804', 'https://openalex.org/W4287329822', 'https://openalex.org/W4225470987', 'https://openalex.org/W3209059054', 'https://openalex.org/W3035214886']",2023-01-01
https://openalex.org/W2148437670,https://doi.org/10.18653/v1/2023.acl-long,Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Message from the Program ChairsIt's hard to believe that we're actually going to be seeing the program come together in Toronto.We're really looking forward to it and to seeing you all there!Most of the work of a program chair is behind the scenes: herding reviewers and chairs, wrangling data from various sources, and answering lots and lots of email.This is a volunteer position, so the only reward we get for this is our chance to make the process of submitting and reviewing papers to our conference better.This letter will outline some of those experiments.First, we asked reviewers for two scores: soundness and excitement.Our goal was that any sound paper would be accepted to some ACL affiliated venue, but that the ""main conference"" distinction (limited by space) would be focused on the most exciting papers.Our hope was that soundness would be less noisy than a single ""overall recommendation"" score, which would help reduce the randomness of decisions.Judging by the exit surveys, this change was well received: over 80% of the chairs, reviewers and authors either expressed support or did not object to this change.Next, we developed a new process for matching papers to reviewers based on keywords for not only the subject matter of the paper, but also its type of contribution and target language(s).This allowed more fine-grained control over the paper-reviewer matches, and we were also able to provide the chairs with context for the paper-reviewer matches.To improve review quality, we also updated the reviewer guidelines, and developed a system for the authors to flag specific types of issues with reviews.Finally, we have also proposed a new initiative for recognizing outstanding reviewers and chairs (73 awards at ACL'23).Finally, we have tried to give more options for presentations.Findings papers now have an in-person presentation spotlight slot and virtual posters in addition to recording videos.Virtual posters have portals to link in-person attendees to virtual posters.We have also brought back Miniconf and RocketChat to allow for better virtual communication between papers (regardless of where the authors are).This conference is a result of the joint efforts of over ten thousand people.We deeply thank them all, and apologize for the many nagging emails we had to send out.In particular: AreasTo ensure a smooth process, the submissions to ACL 2023 were divided into 26 areas.The areas mostly followed these of previous ACL, and more broadly *ACL conferences, reflecting the typical divisions in the field.Following EMNLP 2022, we split the ""Large Language Models"" track away from ""Machine learning in NLP"", reflecting the growth of submissions in the area.We also offered two new tracks (""Linguistic diversity"" and ""Multilingualism and Cross-Lingual NLP"").For the papers authored by SACs, the final recommendation decisions were made by a separate SAC team.The most popular areas (with over 250 submissions) were ""Dialogue and Interactive Systems"", ""Information Extraction"", ""Large Language Models"", ""Machine Learning for NLP"", and ""NLP Applications"". Best Paper AwardsACL'23 implemented the new ACL award policy, aiming to expand the pool of work that is recognized as outstanding.In total, 73 papers were nominated by the reviewers or area chairs for consideration for awards.These papers were assessed by the Best Paper Award Committee, and with their help we selected 4 best papers, 3 special awards (social impact, resource, reproduction), and several dozen outstanding papers.The best and outstanding papers will be announced in a dedicated plenary session for Best Paper Awards on July 10 2023. Presentation ModeIn ACL 2023, there is no meaningful distinction between oral and poster presentations in terms of paper quality.The composition of the oral sessions were proposed by the SACs of their respective tracks, so as to compose a thematically coherent set of papers on a shared topic or method, which would allow for an engaging discussion.The decisions were not based on the authors' virtual or on-site attendance.We hope you enjoy the program and the new elements we introduced (but let us know either way).We are looking forward to a great ACL 2023!","['https://openalex.org/W2951507555', 'https://openalex.org/W4385571737', 'https://openalex.org/W3009439632', 'https://openalex.org/W4399569456', 'https://openalex.org/W4385572079', 'https://openalex.org/W4385572399', 'https://openalex.org/W4367046651', 'https://openalex.org/W4385572029', 'https://openalex.org/W4385572485', 'https://openalex.org/W4385571549', 'https://openalex.org/W4385571543', 'https://openalex.org/W4385570942', 'https://openalex.org/W2289748525', 'https://openalex.org/W4385569716', 'https://openalex.org/W4385572624', 'https://openalex.org/W4235051855', 'https://openalex.org/W3201395296', 'https://openalex.org/W4385571309', 'https://openalex.org/W4385570874', 'https://openalex.org/W4385573189', 'https://openalex.org/W4385571338', 'https://openalex.org/W4320005767', 'https://openalex.org/W4385572155', 'https://openalex.org/W4385571647', 'https://openalex.org/W4385571232', 'https://openalex.org/W4385571411', 'https://openalex.org/W4385569933', 'https://openalex.org/W4385571451', 'https://openalex.org/W4385572037', 'https://openalex.org/W4385570058', 'https://openalex.org/W4385570512', 'https://openalex.org/W4385571282', 'https://openalex.org/W4287887165', 'https://openalex.org/W136732505', 'https://openalex.org/W4385571663', 'https://openalex.org/W4385572142', 'https://openalex.org/W3103573410', 'https://openalex.org/W2768281947', 'https://openalex.org/W4385570898', 'https://openalex.org/W273955616', 'https://openalex.org/W4385570225', 'https://openalex.org/W4385570601', 'https://openalex.org/W4385572490', 'https://openalex.org/W4385570560', 'https://openalex.org/W2143616301', 'https://openalex.org/W2911534755', 'https://openalex.org/W4385570100', 'https://openalex.org/W4295607477', 'https://openalex.org/W4385565474', 'https://openalex.org/W4385570895', 'https://openalex.org/W4206834956', 'https://openalex.org/W4385570731', 'https://openalex.org/W2250324679', 'https://openalex.org/W4385572362', 'https://openalex.org/W2091775941', 'https://openalex.org/W3164964801', 'https://openalex.org/W4385570959', 'https://openalex.org/W4385571795', 'https://openalex.org/W4385571224', 'https://openalex.org/W2924606678', 'https://openalex.org/W3134445280', 'https://openalex.org/W4385571291', 'https://openalex.org/W4385570894', 'https://openalex.org/W2970660466', 'https://openalex.org/W2159964404', 'https://openalex.org/W2964211782', 'https://openalex.org/W4292402161', 'https://openalex.org/W4385570028', 'https://openalex.org/W4385570847', 'https://openalex.org/W4385572338', 'https://openalex.org/W4385571423', 'https://openalex.org/W2081614222']",2023-01-01
https://openalex.org/W4388936667,https://doi.org/10.1109/taslp.2023.3335807,Gradformer: A Framework for Multi-Aspect Multi-Granularity Pronunciation Assessment,"Automatic pronunciation assessment is an indispensable technology in computer-assisted pronunciation training systems. To further evaluate the quality of pronunciation, multi-task learning with simultaneous output of multi-granularity and multi-aspect has become a mainstream solution. Existing methods either predict scores at all granularity levels simultaneously through a parallel structure, or predict individual granularity scores layer by layer through a hierarchical structure. However, these methods do not fully understand and take advantage of the correlation between the three granularity levels of phoneme, word, and utterance. To address this issue, we propose a novel method, Granularity-decoupled Transformer (Gradformer), which is able to model the relationships between multiple granularity levels. Specifically, we first use a convolution-augmented transformer encoder to encode acoustic features, where the convolution module helps the model better capture local information. The model outputs both phoneme- and word-level granularity scores with high correlation by the encoder. Then, we use utterance queries to interact with the output of the encoder through the transformer decoder, ultimately obtaining the utterance scores. Through unique encoder and decoder architecture, we achieve decoupling at three granularity levels, and handling the relationship between each granularity. Experiments on the speachocean762 dataset show that our model has advantages over state-of-the-art methods in various metrics, especially in key metrics such as phoneme accuracy, word accuracy, and total score.","['https://openalex.org/W2016114400', 'https://openalex.org/W2552635739', 'https://openalex.org/W2954234533', 'https://openalex.org/W2901251060', 'https://openalex.org/W3134065725', 'https://openalex.org/W3012416849', 'https://openalex.org/W2145155465', 'https://openalex.org/W2139008940', 'https://openalex.org/W3096674206', 'https://openalex.org/W3096544436', 'https://openalex.org/W2167642311', 'https://openalex.org/W1988687075', 'https://openalex.org/W1967624650', 'https://openalex.org/W4224928163', 'https://openalex.org/W6739901393', 'https://openalex.org/W4372262414', 'https://openalex.org/W3197742413', 'https://openalex.org/W4225317043', 'https://openalex.org/W2134124280', 'https://openalex.org/W2972347929', 'https://openalex.org/W2517552251', 'https://openalex.org/W2398741870', 'https://openalex.org/W2091856355', 'https://openalex.org/W2152334841', 'https://openalex.org/W1999114171', 'https://openalex.org/W2990413709', 'https://openalex.org/W6992206719', 'https://openalex.org/W6810257329', 'https://openalex.org/W3096270392', 'https://openalex.org/W3197938691', 'https://openalex.org/W2147768505', 'https://openalex.org/W3162227798', 'https://openalex.org/W2982223350', 'https://openalex.org/W3041561163', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W4307319566', 'https://openalex.org/W4223651314', 'https://openalex.org/W3196525293', 'https://openalex.org/W3198712976', 'https://openalex.org/W3197816268', 'https://openalex.org/W4372259940', 'https://openalex.org/W3015537910', 'https://openalex.org/W1995562189', 'https://openalex.org/W2963448630', 'https://openalex.org/W1494198834', 'https://openalex.org/W6631943919', 'https://openalex.org/W4224920457', 'https://openalex.org/W4319862666', 'https://openalex.org/W4372347650', 'https://openalex.org/W4221151578']",2023-11-23
https://openalex.org/W4391021541,https://doi.org/10.1109/asru57964.2023.10389777,Preserving Phonemic Distinctions For Ordinal Regression: A Novel Loss Function For Automatic Pronunciation Assessment,"Automatic pronunciation assessment (APA) manages to quantify the pronunciation proficiency of a second language (L2) learner in a language. Prevailing approaches to APA normally leverage neural models trained with a regression loss function, such as the mean-squared error (MSE) loss, for proficiency level prediction. Despite most regression models can effectively capture the ordinality of proficiency levels in the feature space, they are confronted with a primary obstacle that different phoneme categories with the same proficiency level are inevitably forced to be close to each other, retaining less phoneme-discriminative information. On account of this, we devise a phonemic contrast ordinal (PCO) loss for training regression-based APA models, which aims to preserve better phonemic distinctions between phoneme categories meanwhile considering ordinal relationships of the regression target output. Specifically, we introduce a phoneme-distinct regularizer into the MSE loss, which encourages feature representations of different phoneme categories to be far apart while simultaneously pulling closer the representations belonging to the same phoneme category by means of weighted distances. An extensive set of experiments carried out on the speechocean 762 benchmark dataset demonstrate the feasibility and effectiveness of our model in relation to some existing state-of-the-art models.","['https://openalex.org/W4319998451', 'https://openalex.org/W2966554590', 'https://openalex.org/W2130689396', 'https://openalex.org/W2401896499', 'https://openalex.org/W2071602138', 'https://openalex.org/W3096674206', 'https://openalex.org/W4283397032', 'https://openalex.org/W2559844874', 'https://openalex.org/W3081817774', 'https://openalex.org/W4319862719', 'https://openalex.org/W3205234329', 'https://openalex.org/W4375850654', 'https://openalex.org/W4375868817', 'https://openalex.org/W4372346583', 'https://openalex.org/W4224928163', 'https://openalex.org/W4312069033', 'https://openalex.org/W4372262414', 'https://openalex.org/W4385822603', 'https://openalex.org/W4385807490', 'https://openalex.org/W2139008940', 'https://openalex.org/W2091856355', 'https://openalex.org/W3197742413', 'https://openalex.org/W1494198834', 'https://openalex.org/W4224920457', 'https://openalex.org/W4385573574', 'https://openalex.org/W4372259940']",2023-12-16
https://openalex.org/W3216976702,https://doi.org/10.1109/cvpr52688.2022.01033,More than Words: In-the-Wild Visually-Driven Prosody for Text-to-Speech,"In this paper we present VDTTS, a Visually-Driven Text-to-Speech model. Motivated by dubbing, VDTTS takes ad-vantage of video frames as an additional input alongside text, and generates speech that matches the video signal. We demonstrate how this allows VDTTS to, unlike plain TTS models, generate speech that not only has prosodic variations like natural pauses and pitch, but is also synchronized to the input video. Experimentally, we show our model produces well-synchronized outputs, approaching the video-speech synchronization quality of the ground-truth, on several challenging benchmarks including ""in-the-wild"" content from VoxCeleb2. Supplementary demo videos demonstrating video-speech synchronization, robustness to speaker ID swapping, and prosody, presented at the project page. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> Project page: http://google-research.github.io/lingvo-lab/vdtts","['https://openalex.org/W6763093128', 'https://openalex.org/W6772994595', 'https://openalex.org/W6734491695', 'https://openalex.org/W3026041220', 'https://openalex.org/W6754420807', 'https://openalex.org/W2551572271', 'https://openalex.org/W2981767644', 'https://openalex.org/W6748181857', 'https://openalex.org/W2738406145', 'https://openalex.org/W3175779516', 'https://openalex.org/W2096733369', 'https://openalex.org/W2998657200', 'https://openalex.org/W6794378236', 'https://openalex.org/W2972359262', 'https://openalex.org/W2963936489', 'https://openalex.org/W6777575779', 'https://openalex.org/W6785947729', 'https://openalex.org/W6782760101', 'https://openalex.org/W6798098866', 'https://openalex.org/W2904459034', 'https://openalex.org/W6750489868', 'https://openalex.org/W2585824449', 'https://openalex.org/W6755300632', 'https://openalex.org/W2091425152', 'https://openalex.org/W6799081819', 'https://openalex.org/W1494198834', 'https://openalex.org/W6735927292', 'https://openalex.org/W6782491396', 'https://openalex.org/W2130086727', 'https://openalex.org/W2107740512', 'https://openalex.org/W6781662123', 'https://openalex.org/W6752888775', 'https://openalex.org/W6763832098', 'https://openalex.org/W6787353492', 'https://openalex.org/W6778823374', 'https://openalex.org/W6784545093', 'https://openalex.org/W6784809985', 'https://openalex.org/W3197294703', 'https://openalex.org/W2120847449', 'https://openalex.org/W6767111847', 'https://openalex.org/W2963300588', 'https://openalex.org/W6783182287', 'https://openalex.org/W2015143272', 'https://openalex.org/W6753038255', 'https://openalex.org/W6752581720', 'https://openalex.org/W3197324626', 'https://openalex.org/W6736996214', 'https://openalex.org/W2939131199', 'https://openalex.org/W2964243274', 'https://openalex.org/W6638273328', 'https://openalex.org/W2962788625', 'https://openalex.org/W2795109282', 'https://openalex.org/W2972702018', 'https://openalex.org/W6802779374', 'https://openalex.org/W2937909078', 'https://openalex.org/W6781924587', 'https://openalex.org/W6802411003', 'https://openalex.org/W3035626590', 'https://openalex.org/W2946200149', 'https://openalex.org/W2794490148', 'https://openalex.org/W3161296985', 'https://openalex.org/W2893436174', 'https://openalex.org/W3048487650', 'https://openalex.org/W2999966482', 'https://openalex.org/W2594690981', 'https://openalex.org/W2777302760', 'https://openalex.org/W1810943226', 'https://openalex.org/W3205685634', 'https://openalex.org/W3087665158', 'https://openalex.org/W2883383043', 'https://openalex.org/W3112809496', 'https://openalex.org/W2608207374', 'https://openalex.org/W2970006822', 'https://openalex.org/W3157840621', 'https://openalex.org/W2963927338', 'https://openalex.org/W3106532934', 'https://openalex.org/W3099206234', 'https://openalex.org/W3129651364', 'https://openalex.org/W2960274051', 'https://openalex.org/W2891205112', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963609956', 'https://openalex.org/W4289383906', 'https://openalex.org/W2604379605', 'https://openalex.org/W2952746495', 'https://openalex.org/W3204420730', 'https://openalex.org/W3091928890', 'https://openalex.org/W2949382160', 'https://openalex.org/W3098557217', 'https://openalex.org/W2980709326', 'https://openalex.org/W2782422271', 'https://openalex.org/W2808706139', 'https://openalex.org/W4295731579', 'https://openalex.org/W2963432880', 'https://openalex.org/W4210409334', 'https://openalex.org/W2808631503', 'https://openalex.org/W2963568578', 'https://openalex.org/W3215615641', 'https://openalex.org/W3033411150', 'https://openalex.org/W3081492798', 'https://openalex.org/W2949836599', 'https://openalex.org/W3186843219', 'https://openalex.org/W2970730223', 'https://openalex.org/W3178321840', 'https://openalex.org/W2972756321', 'https://openalex.org/W3105763085', 'https://openalex.org/W3123097577', 'https://openalex.org/W2107860279']",2022-06-01
https://openalex.org/W4388811481,https://doi.org/10.1101/2023.11.18.567666,Diffusion in a quantized vector space generates non-idealized protein structures and predicts conformational distributions,"Abstract The power of diffusion probabilistic models (DDPMs) in protein design was recently demonstrated by methods that performs three-dimensional protein backbone denoising. However, these DDPMs tend to generate protein backbones of idealized secondary structures and short loops, lacking diverse, non-idealized local structural elements which are essential for the rich conformational dynamics of natural proteins. Moreover, the sampling power of DDPMs have not yet been utilized for predicting the conformational distributions of natural proteins of dynamic structures. Aiming at these two needs, we developed a model named PVQD (protein vector quantization and diffusion), which used an auto-encoder with vector quantization and a generative diffusion model in the latent space to jointly performing the challenging task of modeling complicated protein structures within an end-to-end framework. Our study demonstrated that in design PVQD generated designable protein structures containing non-idealized elements, while in single sequence-based structure prediction PVQD reproduced experimentally observed conformational variations for a set of natural proteins of dynamic structures.","['https://openalex.org/W3186179742', 'https://openalex.org/W3177828909', 'https://openalex.org/W4286491305', 'https://openalex.org/W4296032638', 'https://openalex.org/W3216341763', 'https://openalex.org/W4383957026', 'https://openalex.org/W6824618908', 'https://openalex.org/W4311924788', 'https://openalex.org/W4281551093', 'https://openalex.org/W4214847177', 'https://openalex.org/W4385495389', 'https://openalex.org/W4306670540', 'https://openalex.org/W3036167779', 'https://openalex.org/W4224035735', 'https://openalex.org/W4304014526', 'https://openalex.org/W2153153865', 'https://openalex.org/W4386629205', 'https://openalex.org/W4327550249', 'https://openalex.org/W4313430582', 'https://openalex.org/W2152825437', 'https://openalex.org/W2998581726', 'https://openalex.org/W3134674457', 'https://openalex.org/W2063492240', 'https://openalex.org/W2001258946', 'https://openalex.org/W2043808245', 'https://openalex.org/W1981276685', 'https://openalex.org/W3178321840', 'https://openalex.org/W4307323391', 'https://openalex.org/W4381786045', 'https://openalex.org/W6814982423', 'https://openalex.org/W6739901393']",2023-11-18
https://openalex.org/W4388239851,https://doi.org/10.4108/eai.7-7-2023.2338069,Risk Analysis of AIGC in Market Regulation,"The rapid rise of AIGC technology has greatly promoted the development of the digital economy, but also brought a series of potential risks. AIGC technology has made great progress in the generation of text, pictures, audio, and video. And its products have been applied in various scenarios in the c","['https://openalex.org/W4224035735', 'https://openalex.org/W2949830468', 'https://openalex.org/W3005342969', 'https://openalex.org/W2743763476', 'https://openalex.org/W3089956691', 'https://openalex.org/W2896457183', 'https://openalex.org/W3030163527', 'https://openalex.org/W2125389028', 'https://openalex.org/W2564591810', 'https://openalex.org/W2783870679', 'https://openalex.org/W4226317937', 'https://openalex.org/W4245551996', 'https://openalex.org/W1493160505', 'https://openalex.org/W2765512343', 'https://openalex.org/W3157199281', 'https://openalex.org/W3118816476', 'https://openalex.org/W3178321840', 'https://openalex.org/W3211983881', 'https://openalex.org/W3206996142', 'https://openalex.org/W4297677272', 'https://openalex.org/W4300980117', 'https://openalex.org/W4281632497', 'https://openalex.org/W2130942839', 'https://openalex.org/W4226125322', 'https://openalex.org/W4286850872', 'https://openalex.org/W4313156423', 'https://openalex.org/W4327810158', 'https://openalex.org/W4292779060', 'https://openalex.org/W4353115070', 'https://openalex.org/W4286900267', 'https://openalex.org/W4312388283', 'https://openalex.org/W3036601975', 'https://openalex.org/W4381786045', 'https://openalex.org/W4310695675', 'https://openalex.org/W2962937198', 'https://openalex.org/W2964216930', 'https://openalex.org/W4225353277', 'https://openalex.org/W2606974598', 'https://openalex.org/W2952889708', 'https://openalex.org/W3166396011', 'https://openalex.org/W2964024144', 'https://openalex.org/W4287083215', 'https://openalex.org/W3129576130', 'https://openalex.org/W4385245566', 'https://openalex.org/W4302364879', 'https://openalex.org/W4320013936', 'https://openalex.org/W2978872486', 'https://openalex.org/W4287194430', 'https://openalex.org/W2933374552', 'https://openalex.org/W2558805089', 'https://openalex.org/W4285483774', 'https://openalex.org/W2116341502', 'https://openalex.org/W4288088047', 'https://openalex.org/W3036167779']",2023-01-01
https://openalex.org/W3213528868,https://doi.org/10.1109/icassp43922.2022.9746767,Bloom-Net: Blockwise Optimization for Masking Networks Toward Scalable and Efficient Speech Enhancement,"In this paper, we present a blockwise optimization method for masking-based networks (BLOOM-Net) for training scalable speech enhancement networks. Here, we design our network with a residual learning scheme and train the internal separator blocks sequentially to obtain a scalable masking-based deep neural network for speech enhancement. Its scalability lets it dynamically adjust the run-time complexity depending on the test time environment. To this end, we modularize our models in that they can flexibly accommodate varying needs for enhancement performance and constraints on the resources, incurring minimal memory or training overhead due to the added scalability. Our experiments on speech enhancement demonstrate that the proposed blockwise optimization method achieves the desired scalability with only a slight performance degradation compared to corresponding models trained end-to-end.","['https://openalex.org/W6677580257', 'https://openalex.org/W2938874599', 'https://openalex.org/W6737664043', 'https://openalex.org/W3110852964', 'https://openalex.org/W3042857426', 'https://openalex.org/W6638523607', 'https://openalex.org/W3162538144', 'https://openalex.org/W2972354707', 'https://openalex.org/W6798098866', 'https://openalex.org/W6767064347', 'https://openalex.org/W3095717210', 'https://openalex.org/W2952218014', 'https://openalex.org/W1677182931', 'https://openalex.org/W2221409856', 'https://openalex.org/W3096893582', 'https://openalex.org/W3015199127', 'https://openalex.org/W3163842642', 'https://openalex.org/W3163652268', 'https://openalex.org/W2291877678', 'https://openalex.org/W3177067699', 'https://openalex.org/W2141411743', 'https://openalex.org/W2890820256', 'https://openalex.org/W2194775991', 'https://openalex.org/W3015526955', 'https://openalex.org/W1494198834', 'https://openalex.org/W6676769703', 'https://openalex.org/W2962935966', 'https://openalex.org/W6688816777', 'https://openalex.org/W3028019732', 'https://openalex.org/W3094607766', 'https://openalex.org/W1821462560', 'https://openalex.org/W3099330747', 'https://openalex.org/W3215615641', 'https://openalex.org/W2964058413', 'https://openalex.org/W3162539493', 'https://openalex.org/W4287632494', 'https://openalex.org/W2994749257', 'https://openalex.org/W3178321840', 'https://openalex.org/W2112076978', 'https://openalex.org/W4297775537', 'https://openalex.org/W2612445135', 'https://openalex.org/W2964299589', 'https://openalex.org/W2219249508', 'https://openalex.org/W2119144962']",2022-04-27
https://openalex.org/W3210269866,https://doi.org/10.48550/arxiv.2111.02351,"Weight, Block or Unit? Exploring Sparsity Tradeoffs for Speech Enhancement on Tiny Neural Accelerators","We explore network sparsification strategies with the aim of compressing neural speech enhancement (SE) down to an optimal configuration for a new generation of low power microcontroller based neural accelerators (microNPU's). We examine three unique sparsity structures: weight pruning, block pruning and unit pruning; and discuss their benefits and drawbacks when applied to SE. We focus on the interplay between computational throughput, memory footprint and model quality. Our method supports all three structures above and jointly learns integer quantized weights along with sparsity. Additionally, we demonstrate offline magnitude based pruning of integer quantized models as a performance baseline. Although efficient speech enhancement is an active area of research, our work is the first to apply block pruning to SE and the first to address SE model compression in the context of microNPU's. Using weight pruning, we show that we are able to compress an already compact model's memory footprint by a factor of 42x from 3.7MB to 87kB while only losing 0.1 dB SDR in performance. We also show a computational speedup of 6.7x with a corresponding SDR drop of only 0.59 dB SDR using block pruning.","['https://openalex.org/W3185109982', 'https://openalex.org/W3164605550', 'https://openalex.org/W2153331367', 'https://openalex.org/W2057200980', 'https://openalex.org/W2510642588', 'https://openalex.org/W3178321840', 'https://openalex.org/W2608554408', 'https://openalex.org/W1482149378', 'https://openalex.org/W2963208781', 'https://openalex.org/W2127851351', 'https://openalex.org/W2767785892', 'https://openalex.org/W3094218199', 'https://openalex.org/W2964299589', 'https://openalex.org/W2963000224', 'https://openalex.org/W3028019732', 'https://openalex.org/W3162501355', 'https://openalex.org/W2962715207', 'https://openalex.org/W2964058413', 'https://openalex.org/W2519091744', 'https://openalex.org/W3094607766', 'https://openalex.org/W2626778328', 'https://openalex.org/W2134273960', 'https://openalex.org/W2963122961', 'https://openalex.org/W2734774145', 'https://openalex.org/W2900541487', 'https://openalex.org/W3095057960', 'https://openalex.org/W2996969697', 'https://openalex.org/W3163662330', 'https://openalex.org/W2114766824', 'https://openalex.org/W2101045344', 'https://openalex.org/W2067295501', 'https://openalex.org/W2064675550', 'https://openalex.org/W2891405874']",2021-11-03
https://openalex.org/W4393195893,https://doi.org/10.1111/cogs.13427,Computational Modeling of the Segmentation of Sentence Stimuli From an Infant Word‐Finding Study,"Abstract Computational models of infant word‐finding typically operate over transcriptions of infant‐directed speech corpora. It is now possible to test models of word segmentation on speech materials, rather than transcriptions of speech. We propose that such modeling efforts be conducted over the speech of the experimental stimuli used in studies measuring infants' capacity for learning from spoken sentences. Correspondence with infant outcomes in such experiments is an appropriate benchmark for models of infants. We demonstrate such an analysis by applying the DP‐Parser model of Algayres and colleagues to auditory stimuli used in infant psycholinguistic experiments by Pelucchi and colleagues. The DP‐Parser model takes speech as input, and creates multiple overlapping embeddings from each utterance. Prospective words are identified as clusters of similar embedded segments. This allows segmentation of each utterance into possible words, using a dynamic programming method that maximizes the frequency of constituent segments. We show that DP‐Parse mimics American English learners' performance in extracting words from Italian sentences, favoring the segmentation of words with high syllabic transitional probability. This kind of computational analysis over actual stimuli from infant experiments may be helpful in tuning future models to match human performance.","['https://openalex.org/W4223486244', 'https://openalex.org/W4283453659', 'https://openalex.org/W4296710617', 'https://openalex.org/W3044967013', 'https://openalex.org/W3036601975', 'https://openalex.org/W2106048795', 'https://openalex.org/W4320711885', 'https://openalex.org/W2895356663', 'https://openalex.org/W2063303346', 'https://openalex.org/W2935067899', 'https://openalex.org/W4200040515', 'https://openalex.org/W4319161892', 'https://openalex.org/W3187244867', 'https://openalex.org/W3197259906', 'https://openalex.org/W2483390977', 'https://openalex.org/W2107959623', 'https://openalex.org/W3199093330', 'https://openalex.org/W1979066498', 'https://openalex.org/W2126377586', 'https://openalex.org/W4307783259', 'https://openalex.org/W4385822676', 'https://openalex.org/W2037525070', 'https://openalex.org/W2556930864', 'https://openalex.org/W3106430258', 'https://openalex.org/W3130526202', 'https://openalex.org/W135984148', 'https://openalex.org/W2038056950', 'https://openalex.org/W2295297373', 'https://openalex.org/W2140661818', 'https://openalex.org/W4240788953', 'https://openalex.org/W1993768374', 'https://openalex.org/W4298147601', 'https://openalex.org/W4384626726', 'https://openalex.org/W4297690113', 'https://openalex.org/W2118020555', 'https://openalex.org/W2166061736', 'https://openalex.org/W1540332606', 'https://openalex.org/W2095908250', 'https://openalex.org/W6674725372', 'https://openalex.org/W2415378728', 'https://openalex.org/W2107917162', 'https://openalex.org/W4237938692', 'https://openalex.org/W2962736743', 'https://openalex.org/W4389549519', 'https://openalex.org/W3173403369', 'https://openalex.org/W2136549906', 'https://openalex.org/W1979898824', 'https://openalex.org/W2112593546', 'https://openalex.org/W2130450945', 'https://openalex.org/W2967164258', 'https://openalex.org/W2089883580', 'https://openalex.org/W2101509422', 'https://openalex.org/W3172924695', 'https://openalex.org/W4306642564', 'https://openalex.org/W3096196861']",2024-03-01
https://openalex.org/W4387877468,https://doi.org/10.31234/osf.io/5p8ge,Artificial neural networks to analyze and simulate language acquisition in children,"Lightweight child-worn recorders that collect audio across an entire day allow for a big-data approach to the study of language development. By collecting the child's production and linguistic environment, these recordings offer us a uniquely naturalistic view of everyday language uses. However, such recordings quickly accumulate thousands of hours of audio and require the use of automatic speech processing algorithms. Besides providing ecologically-valid measures of what children hear and say, these recordings can fuel computational models of early language acquisition with what infants truly hear. This opens up new opportunities for running realistic language learning simulations.A first aspect of my doctoral work is dedicated to developing automatic speech processing algorithms for child-centered long-form recordings. In this manuscript, I first show that current state-of-the-art automatic speech recognition systems fail to capture the complexity of naturalistic speech as found in long-forms. I then present our attempt to propose a free, open-source, and more accurate alternative to the LENA proprietary software, which is currently the standard tool for obtaining automatic analyses of long-forms. Using supervised learning methods, my collaborators and I built a suite of speech processing tools to detect voice activity, identify voice signal sources (child vocalizations, female or male speech), count the number of linguistic units (phonemes, syllables, or words), and estimate the quantity of background noise and reverberation. A second aspect of my doctoral work is dedicated to computational models of early language acquisition. I present a first modeling study showing that self-supervised learning algorithms trained on audiobooks can learn phonetic and lexical aspects of their training language. I then show that the same algorithm trained on ecological long-forms needs inductive biases to learn phonetic aspects of its training language reliably and reflect on whether similar inductive biases may guide language learning in infants. Interestingly, there is no evidence for lexical learning on long-forms, contrary to what has been shown in the literature on more curated data. This series of studies illustrates the importance of considering ecologically-valid input data when modeling language acquisition.","['https://openalex.org/W2885156775', 'https://openalex.org/W4407276585', 'https://openalex.org/W2888800758', 'https://openalex.org/W2989863749', 'https://openalex.org/W2406262283', 'https://openalex.org/W6632323398', 'https://openalex.org/W2919849250', 'https://openalex.org/W2887814324', 'https://openalex.org/W2187280882', 'https://openalex.org/W2561127898', 'https://openalex.org/W2048632248', 'https://openalex.org/W3004481867', 'https://openalex.org/W2131168675', 'https://openalex.org/W3045733287', 'https://openalex.org/W1269382222', 'https://openalex.org/W2070218750', 'https://openalex.org/W2010604004', 'https://openalex.org/W4394142399', 'https://openalex.org/W2115857751', 'https://openalex.org/W2750259098', 'https://openalex.org/W6688816777', 'https://openalex.org/W2544860310', 'https://openalex.org/W2985913104', 'https://openalex.org/W1499999342', 'https://openalex.org/W6769833506', 'https://openalex.org/W1982599741', 'https://openalex.org/W6786127183', 'https://openalex.org/W2611943505', 'https://openalex.org/W2696967604', 'https://openalex.org/W2768416973', 'https://openalex.org/W4319862721', 'https://openalex.org/W2296434735', 'https://openalex.org/W2805466703', 'https://openalex.org/W2982059757', 'https://openalex.org/W3099610051', 'https://openalex.org/W2410879554', 'https://openalex.org/W3015235644', 'https://openalex.org/W4311000453', 'https://openalex.org/W1494198834', 'https://openalex.org/W2593116425', 'https://openalex.org/W2555915854', 'https://openalex.org/W3031133340', 'https://openalex.org/W3161374022', 'https://openalex.org/W2901243971', 'https://openalex.org/W3038871978', 'https://openalex.org/W2895356663', 'https://openalex.org/W2516342150', 'https://openalex.org/W2063525438', 'https://openalex.org/W4220770602', 'https://openalex.org/W2103091632', 'https://openalex.org/W122673323', 'https://openalex.org/W6600387335', 'https://openalex.org/W3129009457', 'https://openalex.org/W2809193001', 'https://openalex.org/W2759573091', 'https://openalex.org/W1592295210', 'https://openalex.org/W6789725041', 'https://openalex.org/W2063303346', 'https://openalex.org/W6665016202', 'https://openalex.org/W6790730029', 'https://openalex.org/W2944539396', 'https://openalex.org/W2586148577', 'https://openalex.org/W4213306813', 'https://openalex.org/W3135377987', 'https://openalex.org/W2483390977', 'https://openalex.org/W2774051897', 'https://openalex.org/W3084297320', 'https://openalex.org/W2991557631', 'https://openalex.org/W2112883467', 'https://openalex.org/W2071591642', 'https://openalex.org/W2889102505', 'https://openalex.org/W1485633403', 'https://openalex.org/W6641916425', 'https://openalex.org/W2085478996', 'https://openalex.org/W2972476505', 'https://openalex.org/W3025683731', 'https://openalex.org/W3125043549', 'https://openalex.org/W6729411815', 'https://openalex.org/W3125087428', 'https://openalex.org/W2141994663', 'https://openalex.org/W2135563147', 'https://openalex.org/W3192452456', 'https://openalex.org/W2610616322', 'https://openalex.org/W3128683352', 'https://openalex.org/W2012125774', 'https://openalex.org/W6945130725', 'https://openalex.org/W2022042240', 'https://openalex.org/W7054993619', 'https://openalex.org/W3129048159', 'https://openalex.org/W1981826156', 'https://openalex.org/W3034612657', 'https://openalex.org/W2095458199', 'https://openalex.org/W1988747931', 'https://openalex.org/W2101048305', 'https://openalex.org/W2340774970', 'https://openalex.org/W2046338384', 'https://openalex.org/W2890174796', 'https://openalex.org/W4295309037', 'https://openalex.org/W6812385817', 'https://openalex.org/W1990351858', 'https://openalex.org/W6655910177', 'https://openalex.org/W1968703923', 'https://openalex.org/W2912762364', 'https://openalex.org/W2104752510', 'https://openalex.org/W4206953441', 'https://openalex.org/W6808618455', 'https://openalex.org/W2776941264', 'https://openalex.org/W6678463487', 'https://openalex.org/W2062956221', 'https://openalex.org/W2138930657', 'https://openalex.org/W2051676521', 'https://openalex.org/W3136507086', 'https://openalex.org/W2125766564', 'https://openalex.org/W6600721412', 'https://openalex.org/W2090314151', 'https://openalex.org/W4303629135', 'https://openalex.org/W2802302202', 'https://openalex.org/W6640227979', 'https://openalex.org/W2153767712', 'https://openalex.org/W2066213611', 'https://openalex.org/W6778883912', 'https://openalex.org/W3110458199', 'https://openalex.org/W3198217962', 'https://openalex.org/W4300021539', 'https://openalex.org/W3213014097', 'https://openalex.org/W2765364385', 'https://openalex.org/W3215376969', 'https://openalex.org/W4318621130', 'https://openalex.org/W2155042697', 'https://openalex.org/W2995181338', 'https://openalex.org/W4281492411', 'https://openalex.org/W2741692265', 'https://openalex.org/W305655093', 'https://openalex.org/W4394150413', 'https://openalex.org/W2014307400', 'https://openalex.org/W4200300291', 'https://openalex.org/W4390854704', 'https://openalex.org/W6810610419', 'https://openalex.org/W3135131435', 'https://openalex.org/W4225079082', 'https://openalex.org/W6718447644', 'https://openalex.org/W2137181162', 'https://openalex.org/W2980577029', 'https://openalex.org/W2767826640', 'https://openalex.org/W2169471344', 'https://openalex.org/W2118611915', 'https://openalex.org/W3011583491', 'https://openalex.org/W2995929068', 'https://openalex.org/W2098192529', 'https://openalex.org/W241613093', 'https://openalex.org/W2747060779', 'https://openalex.org/W2508684615', 'https://openalex.org/W2935067899', 'https://openalex.org/W2091746061', 'https://openalex.org/W4291746033', 'https://openalex.org/W3112495382', 'https://openalex.org/W1984586950', 'https://openalex.org/W3046659789', 'https://openalex.org/W6684012063', 'https://openalex.org/W2805407818', 'https://openalex.org/W2088377333', 'https://openalex.org/W2997253105', 'https://openalex.org/W6781919819', 'https://openalex.org/W4322766928', 'https://openalex.org/W2162693531', 'https://openalex.org/W6811685372', 'https://openalex.org/W7014979702', 'https://openalex.org/W4213430915', 'https://openalex.org/W3157861865', 'https://openalex.org/W2553608650', 'https://openalex.org/W153534061', 'https://openalex.org/W4281621399', 'https://openalex.org/W3016398799', 'https://openalex.org/W2767354245', 'https://openalex.org/W3005165546', 'https://openalex.org/W3036367741', 'https://openalex.org/W1530250655', 'https://openalex.org/W4378619943', 'https://openalex.org/W4321854743', 'https://openalex.org/W3090329631', 'https://openalex.org/W3187244867', 'https://openalex.org/W2026484633', 'https://openalex.org/W6676391964', 'https://openalex.org/W2107959623', 'https://openalex.org/W2762841950', 'https://openalex.org/W52412328', 'https://openalex.org/W4404957577', 'https://openalex.org/W2605959375', 'https://openalex.org/W6663865628', 'https://openalex.org/W2176118107', 'https://openalex.org/W2129705892', 'https://openalex.org/W2019922577', 'https://openalex.org/W122498424', 'https://openalex.org/W1740027839', 'https://openalex.org/W4318686658', 'https://openalex.org/W6983383401', 'https://openalex.org/W2118599126', 'https://openalex.org/W2160815625', 'https://openalex.org/W6661090959', 'https://openalex.org/W6644252350', 'https://openalex.org/W2999130843', 'https://openalex.org/W6779451415', 'https://openalex.org/W3162021500', 'https://openalex.org/W2133189729', 'https://openalex.org/W1989914796', 'https://openalex.org/W2149784807', 'https://openalex.org/W2742950837', 'https://openalex.org/W2038056950', 'https://openalex.org/W2045343524', 'https://openalex.org/W2068116204', 'https://openalex.org/W6659778137', 'https://openalex.org/W2059824090', 'https://openalex.org/W1975418600', 'https://openalex.org/W2898208057', 'https://openalex.org/W2780264041', 'https://openalex.org/W2132573777', 'https://openalex.org/W2488199425', 'https://openalex.org/W2005311247', 'https://openalex.org/W2140661818', 'https://openalex.org/W2989321827', 'https://openalex.org/W4311481261', 'https://openalex.org/W4379474370', 'https://openalex.org/W3211278025', 'https://openalex.org/W3008675085', 'https://openalex.org/W2114156501', 'https://openalex.org/W6673731931', 'https://openalex.org/W6610742289', 'https://openalex.org/W2074488330', 'https://openalex.org/W6664716616', 'https://openalex.org/W2136653392', 'https://openalex.org/W2165627680', 'https://openalex.org/W6643209160', 'https://openalex.org/W4304465621', 'https://openalex.org/W3093121832', 'https://openalex.org/W2978027383', 'https://openalex.org/W2102870983', 'https://openalex.org/W281094599', 'https://openalex.org/W7043347943', 'https://openalex.org/W4292997377', 'https://openalex.org/W2167834252', 'https://openalex.org/W3092318106', 'https://openalex.org/W2132566726', 'https://openalex.org/W2089720224', 'https://openalex.org/W4288360843', 'https://openalex.org/W6670256387', 'https://openalex.org/W2398490608', 'https://openalex.org/W2768381684', 'https://openalex.org/W2951943862', 'https://openalex.org/W2969145661', 'https://openalex.org/W3082004699', 'https://openalex.org/W4224979832', 'https://openalex.org/W6610556715', 'https://openalex.org/W2010980346', 'https://openalex.org/W6616763113', 'https://openalex.org/W6632820619', 'https://openalex.org/W2136412480', 'https://openalex.org/W6645640999', 'https://openalex.org/W6738652494', 'https://openalex.org/W2395225233', 'https://openalex.org/W2110638188', 'https://openalex.org/W2064135025', 'https://openalex.org/W2963962561', 'https://openalex.org/W4377220931', 'https://openalex.org/W6834868713', 'https://openalex.org/W2160527964', 'https://openalex.org/W2059168261', 'https://openalex.org/W2154600605', 'https://openalex.org/W3163700796', 'https://openalex.org/W6681399687', 'https://openalex.org/W2971977259', 'https://openalex.org/W3084747096', 'https://openalex.org/W6677458792', 'https://openalex.org/W6677158074', 'https://openalex.org/W7064937209', 'https://openalex.org/W6641355812', 'https://openalex.org/W2032476212', 'https://openalex.org/W6601893370', 'https://openalex.org/W2805234167', 'https://openalex.org/W6681346506', 'https://openalex.org/W3190032417', 'https://openalex.org/W6635139342', 'https://openalex.org/W3119308075', 'https://openalex.org/W2089452757', 'https://openalex.org/W2990241049', 'https://openalex.org/W2147740811', 'https://openalex.org/W2037751943', 'https://openalex.org/W2101509422', 'https://openalex.org/W2398830367', 'https://openalex.org/W2058616551', 'https://openalex.org/W6634417500', 'https://openalex.org/W2169868772', 'https://openalex.org/W2917235195', 'https://openalex.org/W1532494781', 'https://openalex.org/W3100385063', 'https://openalex.org/W4200598898', 'https://openalex.org/W6800751262', 'https://openalex.org/W4297677272', 'https://openalex.org/W140261823', 'https://openalex.org/W2162505970', 'https://openalex.org/W2060204180', 'https://openalex.org/W2095223181', 'https://openalex.org/W4221038855', 'https://openalex.org/W4225726571', 'https://openalex.org/W3197259906', 'https://openalex.org/W833103999', 'https://openalex.org/W1705150430', 'https://openalex.org/W2118373646', 'https://openalex.org/W2141038596', 'https://openalex.org/W2595479191', 'https://openalex.org/W3005081886', 'https://openalex.org/W2163999179', 'https://openalex.org/W6642493903', 'https://openalex.org/W6672662947', 'https://openalex.org/W7071849196', 'https://openalex.org/W135984148', 'https://openalex.org/W1540073772', 'https://openalex.org/W6635894473', 'https://openalex.org/W6637818730', 'https://openalex.org/W2784570041', 'https://openalex.org/W2937297214', 'https://openalex.org/W3166292821', 'https://openalex.org/W3134108900', 'https://openalex.org/W6676636377', 'https://openalex.org/W1968225092', 'https://openalex.org/W3023172065', 'https://openalex.org/W2913939165', 'https://openalex.org/W2016292361', 'https://openalex.org/W414516981', 'https://openalex.org/W2149932965', 'https://openalex.org/W2005592929', 'https://openalex.org/W1540332606', 'https://openalex.org/W6655970245', 'https://openalex.org/W1993984071', 'https://openalex.org/W2107917162', 'https://openalex.org/W2395899413', 'https://openalex.org/W4313484307', 'https://openalex.org/W2972337257', 'https://openalex.org/W1995403064', 'https://openalex.org/W2585893098', 'https://openalex.org/W6676456322', 'https://openalex.org/W6623095062', 'https://openalex.org/W3171477941', 'https://openalex.org/W4246559809', 'https://openalex.org/W2125566341', 'https://openalex.org/W2000977437', 'https://openalex.org/W2303872361', 'https://openalex.org/W4320013820', 'https://openalex.org/W4242257761', 'https://openalex.org/W3196595845', 'https://openalex.org/W4243223559', 'https://openalex.org/W2032442824', 'https://openalex.org/W2059146861', 'https://openalex.org/W2995609441', 'https://openalex.org/W2119165475', 'https://openalex.org/W4313255477', 'https://openalex.org/W1597121597', 'https://openalex.org/W2398326651', 'https://openalex.org/W4254816979', 'https://openalex.org/W4288279357', 'https://openalex.org/W1543397219', 'https://openalex.org/W2618602706', 'https://openalex.org/W4297798492', 'https://openalex.org/W821509450', 'https://openalex.org/W4307680525', 'https://openalex.org/W4243618065', 'https://openalex.org/W2120713167', 'https://openalex.org/W4394671563', 'https://openalex.org/W4246867089', 'https://openalex.org/W2131070395', 'https://openalex.org/W4249920809', 'https://openalex.org/W2160997109', 'https://openalex.org/W4297938313', 'https://openalex.org/W4287365906', 'https://openalex.org/W4308231222', 'https://openalex.org/W4237938692', 'https://openalex.org/W4247178956', 'https://openalex.org/W4298742451', 'https://openalex.org/W3036601975', 'https://openalex.org/W2252657604', 'https://openalex.org/W2099164611', 'https://openalex.org/W1559022555', 'https://openalex.org/W2143296986', 'https://openalex.org/W4300721020', 'https://openalex.org/W2973180715', 'https://openalex.org/W3151534266', 'https://openalex.org/W3034905892', 'https://openalex.org/W2163097816', 'https://openalex.org/W4255020641', 'https://openalex.org/W2435103813', 'https://openalex.org/W1981253069', 'https://openalex.org/W4240474221', 'https://openalex.org/W2799770360', 'https://openalex.org/W4210307751', 'https://openalex.org/W2963648280', 'https://openalex.org/W2163028944', 'https://openalex.org/W2996728628', 'https://openalex.org/W3202070718', 'https://openalex.org/W2010188467', 'https://openalex.org/W4319862635', 'https://openalex.org/W3162231828', 'https://openalex.org/W4294955557', 'https://openalex.org/W2533523411', 'https://openalex.org/W3080620940', 'https://openalex.org/W2933138175', 'https://openalex.org/W2973049979', 'https://openalex.org/W3199093330', 'https://openalex.org/W2959443032', 'https://openalex.org/W4292779060', 'https://openalex.org/W4249366912', 'https://openalex.org/W2219249508', 'https://openalex.org/W2041394569', 'https://openalex.org/W4251221781', 'https://openalex.org/W3130041921', 'https://openalex.org/W4225815933', 'https://openalex.org/W2801193150', 'https://openalex.org/W2333023345', 'https://openalex.org/W3126722376', 'https://openalex.org/W3198815374', 'https://openalex.org/W4362220304', 'https://openalex.org/W3133848337', 'https://openalex.org/W4385822336', 'https://openalex.org/W4253001367', 'https://openalex.org/W4245984049', 'https://openalex.org/W4376140088', 'https://openalex.org/W2029008609', 'https://openalex.org/W3212943633', 'https://openalex.org/W4255249122', 'https://openalex.org/W1524333225', 'https://openalex.org/W1925965306', 'https://openalex.org/W3144810982', 'https://openalex.org/W2621934507', 'https://openalex.org/W4245117732', 'https://openalex.org/W2037662195', 'https://openalex.org/W4256695500', 'https://openalex.org/W2182291882', 'https://openalex.org/W4375868953', 'https://openalex.org/W4307320512', 'https://openalex.org/W2164274485', 'https://openalex.org/W2032543155', 'https://openalex.org/W4309419356', 'https://openalex.org/W2803055582', 'https://openalex.org/W4387865047', 'https://openalex.org/W4236000557', 'https://openalex.org/W3036063182', 'https://openalex.org/W2995680346', 'https://openalex.org/W2040344088', 'https://openalex.org/W2963321191', 'https://openalex.org/W1984717236', 'https://openalex.org/W577928986', 'https://openalex.org/W4381786045', 'https://openalex.org/W4243341362', 'https://openalex.org/W2888777493', 'https://openalex.org/W2980877534', 'https://openalex.org/W4385822676', 'https://openalex.org/W3097945073', 'https://openalex.org/W4253926802', 'https://openalex.org/W4292825791', 'https://openalex.org/W1969005071', 'https://openalex.org/W4242738990', 'https://openalex.org/W2972449503', 'https://openalex.org/W166170480', 'https://openalex.org/W2127523122', 'https://openalex.org/W4251435902', 'https://openalex.org/W2058878924', 'https://openalex.org/W4238964169', 'https://openalex.org/W1577624656', 'https://openalex.org/W2065159495', 'https://openalex.org/W4253947715', 'https://openalex.org/W2296607128', 'https://openalex.org/W3197479040', 'https://openalex.org/W2546861836', 'https://openalex.org/W4237154886', 'https://openalex.org/W4288072840', 'https://openalex.org/W4301785137', 'https://openalex.org/W4231114907', 'https://openalex.org/W2519091744', 'https://openalex.org/W1719717336', 'https://openalex.org/W2044321477', 'https://openalex.org/W1972102750', 'https://openalex.org/W4297808394', 'https://openalex.org/W2020944885', 'https://openalex.org/W4323066695', 'https://openalex.org/W1558150890', 'https://openalex.org/W4283332789', 'https://openalex.org/W2323385789', 'https://openalex.org/W2115099665', 'https://openalex.org/W2123599888', 'https://openalex.org/W4230806239', 'https://openalex.org/W2160464066', 'https://openalex.org/W4232589384', 'https://openalex.org/W2964052309', 'https://openalex.org/W4230804341', 'https://openalex.org/W1533561824', 'https://openalex.org/W4230637005', 'https://openalex.org/W2187824139', 'https://openalex.org/W2108582985', 'https://openalex.org/W4252366034', 'https://openalex.org/W2058354688', 'https://openalex.org/W4366994024', 'https://openalex.org/W4385822936', 'https://openalex.org/W3015944949', 'https://openalex.org/W2070653320', 'https://openalex.org/W2003341094', 'https://openalex.org/W2963604492', 'https://openalex.org/W4385823426', 'https://openalex.org/W4235338493', 'https://openalex.org/W2126377586', 'https://openalex.org/W4283762111', 'https://openalex.org/W3146245645', 'https://openalex.org/W2060238187', 'https://openalex.org/W4250832892', 'https://openalex.org/W4246103655', 'https://openalex.org/W4242334097', 'https://openalex.org/W3030437843', 'https://openalex.org/W3015877095', 'https://openalex.org/W2015075592', 'https://openalex.org/W4251965084', 'https://openalex.org/W2110221456', 'https://openalex.org/W4307979480', 'https://openalex.org/W3095410713', 'https://openalex.org/W1964575515', 'https://openalex.org/W2618478924', 'https://openalex.org/W2115975321', 'https://openalex.org/W3195577433', 'https://openalex.org/W2037752261', 'https://openalex.org/W2343593471', 'https://openalex.org/W3177829661', 'https://openalex.org/W2571532437', 'https://openalex.org/W4290673677', 'https://openalex.org/W2169403152', 'https://openalex.org/W3016181583', 'https://openalex.org/W4286984129', 'https://openalex.org/W1967834254', 'https://openalex.org/W2964054038', 'https://openalex.org/W2950416202', 'https://openalex.org/W2615444509', 'https://openalex.org/W1980862600', 'https://openalex.org/W3047246203', 'https://openalex.org/W4253971549', 'https://openalex.org/W4287591426', 'https://openalex.org/W2054948443', 'https://openalex.org/W3015783745', 'https://openalex.org/W2728435982']",2023-10-22
https://openalex.org/W4226033575,https://doi.org/10.1109/asru51503.2021.9688253,w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training,"Motivated by the success of masked language modeling (MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks (the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light 60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec 2.0 and HuBERT, our model shows 5% to 10% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec 2.0 by more than 30% relatively.","['https://openalex.org/W2936774411', 'https://openalex.org/W6751104502', 'https://openalex.org/W6638667902', 'https://openalex.org/W6746023985', 'https://openalex.org/W2064675550', 'https://openalex.org/W6638749077', 'https://openalex.org/W6631190155', 'https://openalex.org/W2121879602', 'https://openalex.org/W1494198834', 'https://openalex.org/W6771812881', 'https://openalex.org/W2842511635', 'https://openalex.org/W3015995734', 'https://openalex.org/W2972943112', 'https://openalex.org/W2973049979', 'https://openalex.org/W3015265920', 'https://openalex.org/W3016011332', 'https://openalex.org/W2982223350', 'https://openalex.org/W3035202887', 'https://openalex.org/W3041561163', 'https://openalex.org/W3003875258', 'https://openalex.org/W6786669483', 'https://openalex.org/W6739901393', 'https://openalex.org/W3015522062', 'https://openalex.org/W6770514103', 'https://openalex.org/W2111316763', 'https://openalex.org/W2940322076', 'https://openalex.org/W3097777922', 'https://openalex.org/W6770506093', 'https://openalex.org/W165878654', 'https://openalex.org/W2928408492', 'https://openalex.org/W2101210369', 'https://openalex.org/W2088622183', 'https://openalex.org/W6784532283', 'https://openalex.org/W3160525311', 'https://openalex.org/W3093579165', 'https://openalex.org/W3157697407', 'https://openalex.org/W6755207826', 'https://openalex.org/W3026041220', 'https://openalex.org/W6780218876', 'https://openalex.org/W6769196770', 'https://openalex.org/W3209059054', 'https://openalex.org/W4385245566', 'https://openalex.org/W4293569541', 'https://openalex.org/W1522301498', 'https://openalex.org/W2988736778', 'https://openalex.org/W4320930577', 'https://openalex.org/W3128768055', 'https://openalex.org/W2979476256', 'https://openalex.org/W1836465849', 'https://openalex.org/W3112034174', 'https://openalex.org/W2991213871', 'https://openalex.org/W2962907457', 'https://openalex.org/W3036601975', 'https://openalex.org/W2995181338', 'https://openalex.org/W4210463634', 'https://openalex.org/W2896457183', 'https://openalex.org/W1828163288', 'https://openalex.org/W4297808394']",2021-12-13
https://openalex.org/W3203140070,https://doi.org/10.1109/icassp43922.2022.9747490,Distilhubert: Speech Representation Learning by Layer-Wise Distillation of Hidden-Unit Bert,"Self-supervised speech representation learning methods like wav2vec 2.0 and Hidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and offer good representations for numerous speech processing tasks. Despite the success of these methods, they require large memory and high pre-training costs, making them inaccessible for researchers in academia and small companies. Therefore, this paper introduces DistilHuBERT, a novel multi-task learning framework to distill hidden representations from a HuBERT model directly. This method reduces HuBERT's size by 75% and 73% faster while retaining most performance in ten different tasks. Moreover, DistilHuBERT required little training time and data, opening the possibilities of pre-training personal and on-device SSL models for speech.","['https://openalex.org/W2024490156', 'https://openalex.org/W1494198834', 'https://openalex.org/W2933138175', 'https://openalex.org/W6795952400', 'https://openalex.org/W3198094329', 'https://openalex.org/W2963242190', 'https://openalex.org/W3016181583', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W6780218876', 'https://openalex.org/W3197411683', 'https://openalex.org/W3209059054', 'https://openalex.org/W2973157397', 'https://openalex.org/W3015213852', 'https://openalex.org/W6770514103', 'https://openalex.org/W3197580070', 'https://openalex.org/W4226380987', 'https://openalex.org/W6786669483', 'https://openalex.org/W6739901393', 'https://openalex.org/W3015265920', 'https://openalex.org/W2972943112', 'https://openalex.org/W3207558756', 'https://openalex.org/W3198858531', 'https://openalex.org/W3035202887', 'https://openalex.org/W3097286738', 'https://openalex.org/W3041561163', 'https://openalex.org/W6844194202', 'https://openalex.org/W2982223350', 'https://openalex.org/W3189296823', 'https://openalex.org/W6638523607', 'https://openalex.org/W6798952882', 'https://openalex.org/W3105966348', 'https://openalex.org/W6768851824', 'https://openalex.org/W6796551075', 'https://openalex.org/W4206375145', 'https://openalex.org/W4287121455', 'https://openalex.org/W2988736778', 'https://openalex.org/W3179803166', 'https://openalex.org/W3112034174', 'https://openalex.org/W3157923770', 'https://openalex.org/W4297808394', 'https://openalex.org/W2963403868', 'https://openalex.org/W3036601975', 'https://openalex.org/W2842511635', 'https://openalex.org/W4285719527', 'https://openalex.org/W4287173589', 'https://openalex.org/W3169320628', 'https://openalex.org/W1821462560', 'https://openalex.org/W3096587983', 'https://openalex.org/W2979476256', 'https://openalex.org/W4385245566', 'https://openalex.org/W3213925038', 'https://openalex.org/W2978017171', 'https://openalex.org/W3165666670', 'https://openalex.org/W3099782249', 'https://openalex.org/W3187822143', 'https://openalex.org/W2996383576']",2022-04-27
https://openalex.org/W3148001440,https://doi.org/10.18653/v1/2021.naacl-main.152,SPLAT: Speech-Language Joint Pre-Training for Spoken Language Understanding,"Spoken language understanding (SLU) requires a model to analyze input acoustic signal to understand its linguistic content and make predictions. To boost the models’ performance, various pre-training methods have been proposed to learn rich representations from large-scale unannotated speech and text. However, the inherent disparities between the two modalities necessitate a mutual analysis. In this paper, we propose a novel semi-supervised learning framework, SPLAT, to jointly pre-train the speech and language modules. Besides conducting a self-supervised masked language modeling task on the two individual modules using unpaired speech and text, SPLAT aligns representations from the two modules in a shared latent space using a small amount of paired speech and text. Thus, during fine-tuning, the speech module alone can produce representations carrying both acoustic information and contextual semantic knowledge of an input acoustic signal. Experimental results verify the effectiveness of our approach on various SLU tasks. For example, SPLAT improves the previous state-of-the-art performance on the Spoken SQuAD dataset by more than 10%.","['https://openalex.org/W2979476256', 'https://openalex.org/W6813608150', 'https://openalex.org/W2939710050', 'https://openalex.org/W2998501909', 'https://openalex.org/W2996383576', 'https://openalex.org/W3003875258', 'https://openalex.org/W2962854302', 'https://openalex.org/W2904631866', 'https://openalex.org/W3198858531', 'https://openalex.org/W2786839803', 'https://openalex.org/W3096485810', 'https://openalex.org/W2936774411', 'https://openalex.org/W2089654579', 'https://openalex.org/W4297808394', 'https://openalex.org/W2972584841', 'https://openalex.org/W2885485938', 'https://openalex.org/W2842511635', 'https://openalex.org/W2965373594', 'https://openalex.org/W2972943112', 'https://openalex.org/W2889028433', 'https://openalex.org/W2996403597', 'https://openalex.org/W2883617287', 'https://openalex.org/W648947103', 'https://openalex.org/W3049256661', 'https://openalex.org/W2891229414', 'https://openalex.org/W2973049979', 'https://openalex.org/W2981458636', 'https://openalex.org/W2945260553', 'https://openalex.org/W2405622356', 'https://openalex.org/W3095292526', 'https://openalex.org/W2963288440', 'https://openalex.org/W2896457183', 'https://openalex.org/W3036601975', 'https://openalex.org/W2998230451', 'https://openalex.org/W1556470778', 'https://openalex.org/W3016011332', 'https://openalex.org/W3112034174', 'https://openalex.org/W2936695845', 'https://openalex.org/W2971274815', 'https://openalex.org/W2963748441', 'https://openalex.org/W4214784181', 'https://openalex.org/W3099782249', 'https://openalex.org/W3041561163', 'https://openalex.org/W2963341956', 'https://openalex.org/W1972595521', 'https://openalex.org/W3097286738', 'https://openalex.org/W2963403868', 'https://openalex.org/W2982223350', 'https://openalex.org/W2899197626', 'https://openalex.org/W3034999214', 'https://openalex.org/W3035202887', 'https://openalex.org/W2883409523', 'https://openalex.org/W1494198834', 'https://openalex.org/W3015265920', 'https://openalex.org/W3096589040', 'https://openalex.org/W2894164357', 'https://openalex.org/W4385245566', 'https://openalex.org/W3016181583']",2021-01-01
https://openalex.org/W4226403810,https://doi.org/10.21437/interspeech.2022-10002,Boosting Self-Supervised Embeddings for Speech Enhancement,"Self-supervised learning (SSL) representation for speech has achieved state-of-the-art (SOTA) performance on several downstream tasks.However, there remains room for improvement in speech enhancement (SE) tasks.In this study, we used a crossdomain feature to solve the problem that SSL embeddings may lack fine-grained information to regenerate speech signals.By integrating the SSL representation and spectrogram, the result can be significantly boosted.We further study the relationship between the noise robustness of SSL representation via clean-noisy distance (CN distance) and the layer importance for SE.Consequently, we found that SSL representations with lower noise robustness are more important.Furthermore, our experiments on the VCTK-DEMAND dataset demonstrated that fine-tuning an SSL representation with an SE model can outperform the SOTA SSL-based SE methods in PESQ, CSIG and COVL without invoking complicated network architectures.In later experiments, the CN distance in SSL embeddings was observed to increase after fine-tuning.These results verify our expectations and may help design SE-related SSL training in the future.","['https://openalex.org/W4210484003', 'https://openalex.org/W3049256661', 'https://openalex.org/W2972943112', 'https://openalex.org/W3112034174', 'https://openalex.org/W4244026860', 'https://openalex.org/W2962866211', 'https://openalex.org/W4297808394', 'https://openalex.org/W3193271183', 'https://openalex.org/W3169320628', 'https://openalex.org/W3035837245', 'https://openalex.org/W3015337486', 'https://openalex.org/W3016132433', 'https://openalex.org/W3209984917', 'https://openalex.org/W2603567530', 'https://openalex.org/W3196614065', 'https://openalex.org/W2935693977', 'https://openalex.org/W3202278141', 'https://openalex.org/W3132954452', 'https://openalex.org/W3163827866', 'https://openalex.org/W4311167834', 'https://openalex.org/W3095248373', 'https://openalex.org/W2405774341', 'https://openalex.org/W2982223350', 'https://openalex.org/W4311137818', 'https://openalex.org/W4285250921', 'https://openalex.org/W4226380987', 'https://openalex.org/W2973049979', 'https://openalex.org/W3197580070', 'https://openalex.org/W2949756029', 'https://openalex.org/W4224933800', 'https://openalex.org/W3015213852', 'https://openalex.org/W3211224152', 'https://openalex.org/W3197227964', 'https://openalex.org/W3197284240', 'https://openalex.org/W3015949486', 'https://openalex.org/W2044893557', 'https://openalex.org/W4221162932', 'https://openalex.org/W2962946126']",2022-09-16
https://openalex.org/W3163596720,https://doi.org/10.1109/icassp39728.2021.9414776,Probing Acoustic Representations for Phonetic Properties,"Pre-trained acoustic representations such as wav2vec and DeCoAR have attained impressive word error rates (WER) for speech recognition benchmarks, particularly when labeled data is limited. But little is known about what phonetic properties these various representations acquire, and how well they encode transferable features of speech. We compare features from two conventional and four pre-trained systems in some simple frame-level phonetic classification tasks, with classifiers trained on features from one version of the TIMIT dataset and tested on features from another. All contextualized representations offered some level of transferability across domains, and models pre-trained on more audio data give better results; but overall, DeCoAR, the system with the simplest architecture, performs best. This type of benchmarking analysis can thus uncover relative strengths of various proposed acoustic representations.","['https://openalex.org/W6786127183', 'https://openalex.org/W3020336359', 'https://openalex.org/W6788328058', 'https://openalex.org/W6786669483', 'https://openalex.org/W6675354045', 'https://openalex.org/W2982223350', 'https://openalex.org/W6777232839', 'https://openalex.org/W3024182269', 'https://openalex.org/W6780218876', 'https://openalex.org/W3015265920', 'https://openalex.org/W6629717138', 'https://openalex.org/W2973157397', 'https://openalex.org/W3015412285', 'https://openalex.org/W2964303116', 'https://openalex.org/W2190506272', 'https://openalex.org/W2077804127', 'https://openalex.org/W2296681920', 'https://openalex.org/W6731763572', 'https://openalex.org/W2963571336', 'https://openalex.org/W2973049979', 'https://openalex.org/W6844194202', 'https://openalex.org/W2250539671', 'https://openalex.org/W6769686700', 'https://openalex.org/W6682691769', 'https://openalex.org/W2964204621', 'https://openalex.org/W6769196770', 'https://openalex.org/W2933138175', 'https://openalex.org/W6755207826', 'https://openalex.org/W6687152286', 'https://openalex.org/W2962739339', 'https://openalex.org/W2896457183', 'https://openalex.org/W2191779130', 'https://openalex.org/W2101234009', 'https://openalex.org/W3148040514', 'https://openalex.org/W4294170691', 'https://openalex.org/W2979476256', 'https://openalex.org/W1494198834', 'https://openalex.org/W3036601975', 'https://openalex.org/W3112034174', 'https://openalex.org/W3196595845', 'https://openalex.org/W3025035610', 'https://openalex.org/W3121914243', 'https://openalex.org/W3096485810', 'https://openalex.org/W4297808394', 'https://openalex.org/W2951216052']",2021-05-13
https://openalex.org/W4319862642,https://doi.org/10.1109/slt54892.2023.10023274,Exploring Efficient-Tuning Methods in Self-Supervised Speech Models,"In this study, we aim to explore efficient tuning methods for speech self-supervised learning. Recent studies show that self-supervised learning (SSL) can learn powerful representations for different speech tasks. However, fine-tuning pre-trained models for each downstream task is parameter-inefficient since SSL models are notoriously large with millions of parameters. Adapters are lightweight modules commonly used in NLP to solve this problem. In downstream tasks, the parameters of SSL models are frozen, and only the adapters are trained. Given the lack of studies generally exploring the effectiveness of adapters for self-supervised speech tasks, we intend to fill this gap by adding various adapter modules in pre-trained speech SSL models. We show that the performance parity can be achieved with over 90% parameter reduction, and discussed the pros and cons of efficient tuning techniques. This is the first comprehensive investigation of various adapter types across speech tasks.","['https://openalex.org/W4281492411', 'https://openalex.org/W3209059054', 'https://openalex.org/W3197580070', 'https://openalex.org/W6759579507', 'https://openalex.org/W3174770825', 'https://openalex.org/W3200396895', 'https://openalex.org/W3101498587', 'https://openalex.org/W4225274946', 'https://openalex.org/W6780218876', 'https://openalex.org/W6786669483', 'https://openalex.org/W6778883912', 'https://openalex.org/W6796581206', 'https://openalex.org/W4225410153', 'https://openalex.org/W3176693010', 'https://openalex.org/W4283073456', 'https://openalex.org/W4224930323', 'https://openalex.org/W4226162428', 'https://openalex.org/W6790356757', 'https://openalex.org/W3176828726', 'https://openalex.org/W6802744804', 'https://openalex.org/W2127141656', 'https://openalex.org/W1494198834', 'https://openalex.org/W4226380987', 'https://openalex.org/W4394671563', 'https://openalex.org/W3036601975', 'https://openalex.org/W4292779060', 'https://openalex.org/W3112034174', 'https://openalex.org/W3168867926']",2023-01-09
https://openalex.org/W4221161839,https://doi.org/10.21437/interspeech.2022-141,Semi-FedSER: Semi-supervised Learning for Speech Emotion Recognition On Federated Learning using Multiview Pseudo-Labeling,"Speech Emotion Recognition (SER) application is frequently associated with privacy concerns as it often acquires and transmits speech data at the client-side to remote cloud platforms for further processing. These speech data can reveal not only speech content and affective information but the speaker's identity, demographic traits, and health status. Federated learning (FL) is a distributed machine learning algorithm that coordinates clients to train a model collaboratively without sharing local data. This algorithm shows enormous potential for SER applications as sharing raw speech or speech features from a user's device is vulnerable to privacy attacks. However, a major challenge in FL is limited availability of high-quality labeled data samples. In this work, we propose a semi-supervised federated learning framework, Semi-FedSER, that utilizes both labeled and unlabeled data samples to address the challenge of limited labeled data samples in FL. We show that our Semi-FedSER can generate desired SER performance even when the local label rate l=20 using two SER benchmark datasets: IEMOCAP and MSP-Improv.","['https://openalex.org/W2747664154', 'https://openalex.org/W2146334809', 'https://openalex.org/W4304206546', 'https://openalex.org/W3006555759', 'https://openalex.org/W2912728762', 'https://openalex.org/W3198858531', 'https://openalex.org/W3104696513', 'https://openalex.org/W4228996454', 'https://openalex.org/W2972943112', 'https://openalex.org/W3112034174', 'https://openalex.org/W2970408908', 'https://openalex.org/W2134707158', 'https://openalex.org/W2342475039', 'https://openalex.org/W2061068689', 'https://openalex.org/W4287388854', 'https://openalex.org/W3197580070', 'https://openalex.org/W3001197829', 'https://openalex.org/W3195062561', 'https://openalex.org/W3147359051', 'https://openalex.org/W4226255887', 'https://openalex.org/W2161073241', 'https://openalex.org/W3147788509', 'https://openalex.org/W3013045184', 'https://openalex.org/W2982223350', 'https://openalex.org/W3041561163', 'https://openalex.org/W4214611271', 'https://openalex.org/W4318619660']",2022-09-16
https://openalex.org/W4294068504,https://doi.org/10.1109/jstsp.2022.3203608,Autoregressive Predictive Coding: A Comprehensive Study,"We review autoregressive predictive coding (APC), an approach to learn speech representation by predicting a future frame given the past frames. We present three different views of interpreting APC, and provide a historical account to the approach. To study the speech representation learned by APC, we use common speech tasks, such as automatic speech recognition and speaker verification, to demonstrate the utility of the learned representation. In addition, we design a suite of fine-grained tasks, including frame classification, segment classification, fundamental frequency tracking, and duration prediction, to probe the phonetic and prosodic content of the representation. The three views of the APC objective welcome various generalizations and algorithms to learn speech representations. Probing on the suite of fine-grained tasks suggests that APC makes a wide range of high-level speech information accessible in its learned representation.","['https://openalex.org/W1995875735', 'https://openalex.org/W2112129677', 'https://openalex.org/W2124018923', 'https://openalex.org/W6602644328', 'https://openalex.org/W6728348090', 'https://openalex.org/W2962739339', 'https://openalex.org/W2896457183', 'https://openalex.org/W6763701032', 'https://openalex.org/W2973049979', 'https://openalex.org/W6780218876', 'https://openalex.org/W3015265920', 'https://openalex.org/W6786669483', 'https://openalex.org/W6769238691', 'https://openalex.org/W3160345865', 'https://openalex.org/W2972943112', 'https://openalex.org/W3204696009', 'https://openalex.org/W2963609956', 'https://openalex.org/W2842511635', 'https://openalex.org/W3198608154', 'https://openalex.org/W3041561163', 'https://openalex.org/W3035725276', 'https://openalex.org/W6634817459', 'https://openalex.org/W2752796333', 'https://openalex.org/W3003875258', 'https://openalex.org/W2025768430', 'https://openalex.org/W3097286738', 'https://openalex.org/W3197974236', 'https://openalex.org/W3204915839', 'https://openalex.org/W6748686604', 'https://openalex.org/W6768841368', 'https://openalex.org/W6784532283', 'https://openalex.org/W2479014774', 'https://openalex.org/W4252337780', 'https://openalex.org/W3100270690', 'https://openalex.org/W3016011332', 'https://openalex.org/W2117130368', 'https://openalex.org/W2145038566', 'https://openalex.org/W2163922914', 'https://openalex.org/W343636949', 'https://openalex.org/W6685158001', 'https://openalex.org/W6677326919', 'https://openalex.org/W2102605133', 'https://openalex.org/W2308529009', 'https://openalex.org/W2321533354', 'https://openalex.org/W2963420272', 'https://openalex.org/W6743489131', 'https://openalex.org/W6797408484', 'https://openalex.org/W6784614252', 'https://openalex.org/W4226033575', 'https://openalex.org/W6797992114', 'https://openalex.org/W6788335241', 'https://openalex.org/W3198447122', 'https://openalex.org/W6681302627', 'https://openalex.org/W1494198834', 'https://openalex.org/W2024490156', 'https://openalex.org/W2981087920', 'https://openalex.org/W6631362777', 'https://openalex.org/W6631943919', 'https://openalex.org/W6631190155', 'https://openalex.org/W2963920996', 'https://openalex.org/W2827410935', 'https://openalex.org/W3198858531', 'https://openalex.org/W2741480356', 'https://openalex.org/W2000633092', 'https://openalex.org/W2118774185', 'https://openalex.org/W2405144467', 'https://openalex.org/W2526425061', 'https://openalex.org/W6623517193', 'https://openalex.org/W2964199361', 'https://openalex.org/W3097747488', 'https://openalex.org/W2577366047', 'https://openalex.org/W2890704021', 'https://openalex.org/W3198275944', 'https://openalex.org/W2748488820', 'https://openalex.org/W6738543193', 'https://openalex.org/W6748455135', 'https://openalex.org/W6781476637', 'https://openalex.org/W6760212410', 'https://openalex.org/W3034723486', 'https://openalex.org/W4368755714', 'https://openalex.org/W2787149435', 'https://openalex.org/W1579853615', 'https://openalex.org/W3112034174', 'https://openalex.org/W3178203035', 'https://openalex.org/W2981991061', 'https://openalex.org/W65382156', 'https://openalex.org/W4297808394']",2022-09-01
https://openalex.org/W4224919704,https://doi.org/10.1109/icassp43922.2022.9747355,Contrastive Siamese Network for Semi-Supervised Speech Recognition,"This paper introduces contrastive siamese (c-siam) network, an architecture for leveraging unlabeled acoustic data in speech recognition. c-siam is the first network that extracts high-level linguistic information from speech by matching outputs of two identical transformer encoders. It contains augmented and target branches which are trained by: (1) masking inputs and matching outputs with a contrastive loss, (2) incorporating a stop gradient operation on the target branch, (3) using an extra learnable transformation on the augmented branch, (4) introducing new temporal augment functions to prevent the shortcut learning problem. We use the Libri-light 60k unsupervised data and the LibriSpeech 100hrs/960hrs supervised data to compare c-siam and other best-performing systems. Our experiments show that c-siam provides 20% relative word error rate improvement over wav2vec baselines. A c-siam network with 450M parameters achieves competitive results compared to the state-of-the-art networks with 600M parameters.","['https://openalex.org/W4226033575', 'https://openalex.org/W6682948231', 'https://openalex.org/W6753000030', 'https://openalex.org/W6774314701', 'https://openalex.org/W6784426525', 'https://openalex.org/W3016970897', 'https://openalex.org/W6779326418', 'https://openalex.org/W3035524453', 'https://openalex.org/W3171007011', 'https://openalex.org/W2964110616', 'https://openalex.org/W3041561163', 'https://openalex.org/W3097777922', 'https://openalex.org/W6786669483', 'https://openalex.org/W2842511635', 'https://openalex.org/W2982223350', 'https://openalex.org/W6769196770', 'https://openalex.org/W2973049979', 'https://openalex.org/W3015265920', 'https://openalex.org/W6780218876', 'https://openalex.org/W2972943112', 'https://openalex.org/W6784614252', 'https://openalex.org/W6638749077', 'https://openalex.org/W3016010032', 'https://openalex.org/W1494198834', 'https://openalex.org/W2168510624', 'https://openalex.org/W6631190155', 'https://openalex.org/W2936774411', 'https://openalex.org/W3005680577', 'https://openalex.org/W3036601975', 'https://openalex.org/W1522301498', 'https://openalex.org/W3093579165', 'https://openalex.org/W3035060554', 'https://openalex.org/W3112034174', 'https://openalex.org/W3198608154', 'https://openalex.org/W2152790380', 'https://openalex.org/W3103934428', 'https://openalex.org/W4297808394', 'https://openalex.org/W2979476256', 'https://openalex.org/W2883725317', 'https://openalex.org/W1828163288']",2022-04-27
https://openalex.org/W4386591026,https://doi.org/10.1109/taslp.2023.3313434,A Semi-Supervised Complementary Joint Training Approach for Low-Resource Speech Recognition,"Both unpaired speech and text have shown to be beneficial for low-resource automatic speech recognition (ASR), which, however were either separately used for pre-training, self-training and language model (LM) training, or jointly used for designing hybrid models in literature. In this work, we leverage both unpaired speech and text to train a general ASR model, which are used in the form of data pairs by generating the missing parts in prior to model training. We propose to train a model alternatively using the prepared speech-PseudoLabel and SynthesizedAudio-text pairs and reveal the complementary property in both acoustic and linguistic features. The proposed method is thus called complementary joint training (CJT). Based on the basic CJT, label masking for pseudo-labels and parallel layers for synthesized audio are then proposed for re-training to further cope with the deviations from real data, termed as CJT++. In addition, the proposed CJT is extended to the scenario with zero paired data by considering an iterative CJT for the training of seed ASR model. Experimental results on Libri-light show the efficacy of joint training as well as two second-round training strategies, and the superiority over recent models is validated, particularly in extreme low-resource cases.","['https://openalex.org/W2400208341', 'https://openalex.org/W4285144981', 'https://openalex.org/W6656414902', 'https://openalex.org/W3209059054', 'https://openalex.org/W6752726010', 'https://openalex.org/W2802248956', 'https://openalex.org/W6810327252', 'https://openalex.org/W2512655038', 'https://openalex.org/W3160525311', 'https://openalex.org/W4210690962', 'https://openalex.org/W6761563299', 'https://openalex.org/W6780218876', 'https://openalex.org/W2995181338', 'https://openalex.org/W2842511635', 'https://openalex.org/W3096338464', 'https://openalex.org/W3015522062', 'https://openalex.org/W6784436999', 'https://openalex.org/W3015237657', 'https://openalex.org/W3163605596', 'https://openalex.org/W6769365793', 'https://openalex.org/W2889213362', 'https://openalex.org/W2913851961', 'https://openalex.org/W3205644108', 'https://openalex.org/W2964012862', 'https://openalex.org/W2938947737', 'https://openalex.org/W6762242920', 'https://openalex.org/W3161143478', 'https://openalex.org/W2972889948', 'https://openalex.org/W4319862269', 'https://openalex.org/W2896457183', 'https://openalex.org/W2972943112', 'https://openalex.org/W3041561163', 'https://openalex.org/W2892009249', 'https://openalex.org/W2327501763', 'https://openalex.org/W2037034710', 'https://openalex.org/W3097777922', 'https://openalex.org/W2962699523', 'https://openalex.org/W6795952400', 'https://openalex.org/W4210758944', 'https://openalex.org/W6787141514', 'https://openalex.org/W3200601846', 'https://openalex.org/W6752630080', 'https://openalex.org/W3015280134', 'https://openalex.org/W2883586237', 'https://openalex.org/W3141100132', 'https://openalex.org/W3096485810', 'https://openalex.org/W3096297644', 'https://openalex.org/W3097647528', 'https://openalex.org/W3122931219', 'https://openalex.org/W4296068827', 'https://openalex.org/W2990906560', 'https://openalex.org/W6773139635', 'https://openalex.org/W2127141656', 'https://openalex.org/W3198836239', 'https://openalex.org/W3208049241', 'https://openalex.org/W6770514103', 'https://openalex.org/W2933138175', 'https://openalex.org/W3095350795', 'https://openalex.org/W6786669483', 'https://openalex.org/W2972981541', 'https://openalex.org/W2963925437', 'https://openalex.org/W6688816777', 'https://openalex.org/W2940322076', 'https://openalex.org/W2577366047', 'https://openalex.org/W6772883055', 'https://openalex.org/W2936774411', 'https://openalex.org/W1494198834', 'https://openalex.org/W3096273170', 'https://openalex.org/W6636915900', 'https://openalex.org/W6783867762', 'https://openalex.org/W2748795451', 'https://openalex.org/W6778823374', 'https://openalex.org/W3205080563', 'https://openalex.org/W6770528390', 'https://openalex.org/W2963240019', 'https://openalex.org/W2972991710', 'https://openalex.org/W2124558353', 'https://openalex.org/W3163464943', 'https://openalex.org/W2587275078', 'https://openalex.org/W3112034174', 'https://openalex.org/W3197223534', 'https://openalex.org/W2982095018', 'https://openalex.org/W3092028330', 'https://openalex.org/W3036601975', 'https://openalex.org/W3113594615', 'https://openalex.org/W4297808394', 'https://openalex.org/W1828163288', 'https://openalex.org/W2998532468', 'https://openalex.org/W2808640845', 'https://openalex.org/W1647671624', 'https://openalex.org/W3000915284', 'https://openalex.org/W2990391581', 'https://openalex.org/W3015208154', 'https://openalex.org/W3155427814', 'https://openalex.org/W4294103325', 'https://openalex.org/W2941814890', 'https://openalex.org/W4287173589', 'https://openalex.org/W2952711665', 'https://openalex.org/W4224902998', 'https://openalex.org/W2988736778', 'https://openalex.org/W3033411150', 'https://openalex.org/W2219249508', 'https://openalex.org/W2404126548']",2023-01-01
https://openalex.org/W4391021746,https://doi.org/10.1109/asru57964.2023.10389700,MelHuBERT: A Simplified Hubert on Mel Spectrograms,"Self-supervised models have had great success in learning speech representations that can generalize to various downstream tasks. However, most self-supervised models require a large amount of compute and multiple GPUs to train, significantly hampering the development of self-supervised learning. In an attempt to reduce the computation of training, we revisit the training of HuBERT, a highly successful self-supervised model. We improve and simplify several key components, including the loss function, input representation, and training in multiple stages. Our model, MelHuBERT, is able to achieve favorable performance on phone recognition, speaker identification, and automatic speech recognition against HuBERT, while saving 31.2% of the pre-training time, or equivalently 33.5% MACs per one second speech. The code and pretrained models are available in https://github.com/nervjack2/MelHuBERT.","['https://openalex.org/W2972943112', 'https://openalex.org/W2973049979', 'https://openalex.org/W6780218876', 'https://openalex.org/W3097286738', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W6840200333', 'https://openalex.org/W3204696009', 'https://openalex.org/W6769238691', 'https://openalex.org/W6786669483', 'https://openalex.org/W4297841853', 'https://openalex.org/W4375869340', 'https://openalex.org/W2398826216', 'https://openalex.org/W2964052309', 'https://openalex.org/W2842511635', 'https://openalex.org/W6682948231', 'https://openalex.org/W4294068504', 'https://openalex.org/W4297841659', 'https://openalex.org/W6769196770', 'https://openalex.org/W6768080748', 'https://openalex.org/W6745245109', 'https://openalex.org/W2933138175', 'https://openalex.org/W3197580070', 'https://openalex.org/W6849637795', 'https://openalex.org/W3200129129', 'https://openalex.org/W4375869065', 'https://openalex.org/W4385823182', 'https://openalex.org/W4385807463', 'https://openalex.org/W4375868755', 'https://openalex.org/W2982223350', 'https://openalex.org/W3041561163', 'https://openalex.org/W3206996142', 'https://openalex.org/W2747874407', 'https://openalex.org/W4226380987', 'https://openalex.org/W4375869259', 'https://openalex.org/W2127141656', 'https://openalex.org/W4319862652', 'https://openalex.org/W6810673746', 'https://openalex.org/W4385823192', 'https://openalex.org/W4297808394', 'https://openalex.org/W2975381464', 'https://openalex.org/W3112034174', 'https://openalex.org/W2979476256', 'https://openalex.org/W2152790380', 'https://openalex.org/W4319862404', 'https://openalex.org/W2981991061', 'https://openalex.org/W4221161761', 'https://openalex.org/W3036601975', 'https://openalex.org/W4285483774', 'https://openalex.org/W2763421725']",2023-12-16
https://openalex.org/W4387247604,https://doi.org/10.1109/taslp.2023.3320864,Disentangling Prosody Representations With Unsupervised Speech Reconstruction,"Human speech can be characterized by different components, including semantic content, speaker identity and prosodic information. Significant progress has been made in disentangling representations for semantic content and speaker identity in Automatic Speech Recognition (ASR) and speaker verification tasks respectively. However, it is still an open challenging research question to extract prosodic information because of the intrinsic association of different attributes, such as timbre and rhythm, and because of the need for supervised training schemes to achieve robust large-scale and speaker-independent ASR. The aim of this paper is to address the disentanglement of emotional prosody from speech based on unsupervised reconstruction. Specifically, we identify, design, implement and integrate three crucial components in our proposed speech reconstruction model Prosody2Vec: (1) a unit encoder that transforms speech signals into discrete units for semantic content, (2) a pretrained speaker verification model to generate speaker identity embeddings, and (3) a trainable prosody encoder to learn prosody representations. We first pretrain the Prosody2Vec representations on unlabelled emotional speech corpora, then fine-tune the model on specific datasets to perform Speech Emotion Recognition (SER) and Emotional Voice Conversion (EVC) tasks. Both objective (weighted and unweighted accuracies) and subjective (mean opinion score) evaluations on the EVC task suggest that Prosody2Vec effectively captures general prosodic features that can be smoothly transferred to other emotional speech. In addition, our SER experiments on the IEMOCAP dataset reveal that the prosody features learned by Prosody2Vec are complementary and beneficial for the performance of widely used speech pretraining models and surpass the state-of-the-art methods when combining Prosody2Vec with HuBERT representations. Some audio samples can be found on our demo website","['https://openalex.org/W3209059054', 'https://openalex.org/W2995181338', 'https://openalex.org/W2583542555', 'https://openalex.org/W3197580070', 'https://openalex.org/W4287887366', 'https://openalex.org/W3096723250', 'https://openalex.org/W4385574033', 'https://openalex.org/W2130821326', 'https://openalex.org/W2936774411', 'https://openalex.org/W4205742757', 'https://openalex.org/W6755207826', 'https://openalex.org/W6631190155', 'https://openalex.org/W3209984917', 'https://openalex.org/W6621543089', 'https://openalex.org/W2069924379', 'https://openalex.org/W6803547063', 'https://openalex.org/W4226487411', 'https://openalex.org/W2904459034', 'https://openalex.org/W2146334809', 'https://openalex.org/W2963199341', 'https://openalex.org/W4200635083', 'https://openalex.org/W6623517193', 'https://openalex.org/W2742542661', 'https://openalex.org/W6754420807', 'https://openalex.org/W2928165649', 'https://openalex.org/W2402146185', 'https://openalex.org/W2964243274', 'https://openalex.org/W2752782242', 'https://openalex.org/W2342475039', 'https://openalex.org/W6776390925', 'https://openalex.org/W6750489868', 'https://openalex.org/W3135547455', 'https://openalex.org/W2889374687', 'https://openalex.org/W2973181312', 'https://openalex.org/W2748654097', 'https://openalex.org/W2511640485', 'https://openalex.org/W2808631503', 'https://openalex.org/W3197993066', 'https://openalex.org/W3015241559', 'https://openalex.org/W3204457821', 'https://openalex.org/W4221147462', 'https://openalex.org/W6803378298', 'https://openalex.org/W6847363464', 'https://openalex.org/W4224918091', 'https://openalex.org/W2803193013', 'https://openalex.org/W3025680351', 'https://openalex.org/W2050681655', 'https://openalex.org/W2785978752', 'https://openalex.org/W3112594642', 'https://openalex.org/W2009375902', 'https://openalex.org/W3024869864', 'https://openalex.org/W1494198834', 'https://openalex.org/W3015884429', 'https://openalex.org/W3160039712', 'https://openalex.org/W3204087964', 'https://openalex.org/W3205428167', 'https://openalex.org/W3175161143', 'https://openalex.org/W4312120641', 'https://openalex.org/W2795109282', 'https://openalex.org/W6810585344', 'https://openalex.org/W6746468907', 'https://openalex.org/W6795949861', 'https://openalex.org/W4313887688', 'https://openalex.org/W6762533536', 'https://openalex.org/W3016181583', 'https://openalex.org/W2973049979', 'https://openalex.org/W6637108112', 'https://openalex.org/W6810007534', 'https://openalex.org/W6780218876', 'https://openalex.org/W6786669483', 'https://openalex.org/W2748702193', 'https://openalex.org/W6849896277', 'https://openalex.org/W4285251897', 'https://openalex.org/W4221162872', 'https://openalex.org/W3130293557', 'https://openalex.org/W2972498864', 'https://openalex.org/W2896457183', 'https://openalex.org/W648786980', 'https://openalex.org/W2187089797', 'https://openalex.org/W3211224152', 'https://openalex.org/W4311000453', 'https://openalex.org/W4301371414', 'https://openalex.org/W3036601975', 'https://openalex.org/W4287073476', 'https://openalex.org/W4221145109', 'https://openalex.org/W2794490148', 'https://openalex.org/W2945478979', 'https://openalex.org/W2891205112', 'https://openalex.org/W4319988532', 'https://openalex.org/W2774085128', 'https://openalex.org/W4295731579', 'https://openalex.org/W4225939199', 'https://openalex.org/W854541894', 'https://openalex.org/W1686946872', 'https://openalex.org/W1522301498', 'https://openalex.org/W3034794073', 'https://openalex.org/W3112034174']",2023-10-02
https://openalex.org/W4224934179,https://doi.org/10.1109/icassp43922.2022.9747022,Improving Self-Supervised Learning for Speech Recognition with Intermediate Layer Supervision,"Recently, pioneer work finds that self-supervised pre-training methods can improve multiple downstream speech tasks, because the model utilizes bottom layers to learn speaker-related information and top layers to encode content-related information. Since the network capacity is limited, we believe the speech recognition performance could be further improved if the model is dedicated to audio content information learning. To this end, we propose Intermediate Layer Supervision for Self-Supervised Learning (ILS-SSL), which forces the model to concentrate on content information as much as possible by adding an additional SSL loss on the intermediate layers. Experiments on LibriSpeech test-other set show that our method outperforms HuBERT significantly, which achieves a 23.5%/11.6% relative word error rate reduction in the w/o language model setting for Base/Large models. Detailed analysis shows the bottom layers of our model have a better correlation with phonetic units, which is consistent with our intuition and explains the success of our method for ASR. We will release our code and model at https://github.com/microsoft/UniSpeech.","['https://openalex.org/W6769196770', 'https://openalex.org/W6780218876', 'https://openalex.org/W6784614252', 'https://openalex.org/W3209059054', 'https://openalex.org/W2973157397', 'https://openalex.org/W3015213852', 'https://openalex.org/W4226033575', 'https://openalex.org/W6788335241', 'https://openalex.org/W6795952400', 'https://openalex.org/W3197580070', 'https://openalex.org/W6786669483', 'https://openalex.org/W3041561163', 'https://openalex.org/W3003875258', 'https://openalex.org/W3016011332', 'https://openalex.org/W2973049979', 'https://openalex.org/W2842511635', 'https://openalex.org/W6769593479', 'https://openalex.org/W3144810982', 'https://openalex.org/W2972943112', 'https://openalex.org/W6755207826', 'https://openalex.org/W6769627184', 'https://openalex.org/W6739901393', 'https://openalex.org/W2995181338', 'https://openalex.org/W1494198834', 'https://openalex.org/W6803164887', 'https://openalex.org/W6947929050', 'https://openalex.org/W3036601975', 'https://openalex.org/W2979476256', 'https://openalex.org/W4288089799', 'https://openalex.org/W3167207712', 'https://openalex.org/W2896457183', 'https://openalex.org/W4297808394', 'https://openalex.org/W4385245566', 'https://openalex.org/W2982223350', 'https://openalex.org/W3093579165', 'https://openalex.org/W3112034174', 'https://openalex.org/W3209984917', 'https://openalex.org/W4287173589']",2022-04-27
https://openalex.org/W4372348980,https://doi.org/10.1109/icassp49357.2023.10094711,Evidence of Vocal Tract Articulation in Self-Supervised Learning of Speech,"Recent self-supervised learning (SSL) models have proven to learn rich representations of speech, which can readily be utilized by diverse downstream tasks. To understand such utilities, various analyses have been done for speech SSL models to reveal which and how information is encoded in the learned representations. Although the scope of previous analyses is extensive in acoustic, phonetic, and semantic perspectives, the physical grounding by speech production has not yet received full attention. To bridge this gap, we conduct a comprehensive analysis to link speech representations to articulatory trajectories measured by electromagnetic articulography (EMA). Our analysis is based on a linear probing approach where we measure articulatory score as an average correlation of linear mapping to EMA. We analyze a set of SSL models selected from the leaderboard of the SUPERB benchmark [1] and perform further layer-wise analyses on two most successful models, Wav2Vec 2.0 [2] and HuBERT [3]. Surprisingly, representations from the recent speech SSL models are highly correlated with EMA traces (best: r =0.81), and only 5 minutes are sufficient to train a linear model with high performance (r =0.77). Our findings suggest that SSL models learn to align closely with continuous articulations, and provide a novel insight into speech SSL.","['https://openalex.org/W1494198834', 'https://openalex.org/W2804300206', 'https://openalex.org/W3209984917', 'https://openalex.org/W2995181338', 'https://openalex.org/W3041561163', 'https://openalex.org/W4297841848', 'https://openalex.org/W4225096077', 'https://openalex.org/W2231075402', 'https://openalex.org/W6786669483', 'https://openalex.org/W6780218876', 'https://openalex.org/W3197580070', 'https://openalex.org/W2973049979', 'https://openalex.org/W6784050962', 'https://openalex.org/W6810007534', 'https://openalex.org/W6769196770', 'https://openalex.org/W6788328058', 'https://openalex.org/W3163596720', 'https://openalex.org/W4281492411', 'https://openalex.org/W3209059054', 'https://openalex.org/W4226380987', 'https://openalex.org/W6795952400', 'https://openalex.org/W3121914243', 'https://openalex.org/W2979476256', 'https://openalex.org/W3036601975', 'https://openalex.org/W4221145109', 'https://openalex.org/W3118578889', 'https://openalex.org/W3112034174', 'https://openalex.org/W4287173589']",2023-05-05
https://openalex.org/W3097320994,https://doi.org/10.1145/3570161,Paralinguistic Privacy Protection at the Edge,"Voice user interfaces and digital assistants are rapidly entering our lives and becoming singular touch points spanning our devices. These always-on services capture and transmit our audio data to powerful cloud services for further processing and subsequent actions. Our voices and raw audio signals collected through these devices contain a host of sensitive paralinguistic information that is transmitted to service providers regardless of deliberate or false triggers. As our emotional patterns and sensitive attributes like our identity, gender, and well-being are easily inferred using deep acoustic models, we encounter a new generation of privacy risks by using these services. One approach to mitigate the risk of paralinguistic-based privacy breaches is to exploit a combination of cloud-based processing with privacy-preserving, on-device paralinguistic information learning and filtering before transmitting voice data. In this article we introduce EDGY , a configurable, lightweight, disentangled representation learning framework that transforms and filters high-dimensional voice data to identify and contain sensitive attributes at the edge prior to offloading to the cloud. We evaluate EDGY’s on-device performance and explore optimization techniques, including model quantization and knowledge distillation, to enable private, accurate, and efficient representation learning on resource-constrained devices. Our results show that EDGY runs in tens of milliseconds with 0.2% relative improvement in “zero-shot” ABX score or minimal performance penalties of approximately 5.95% word error rate (WER) in learning linguistic representations from raw voice signals, using a CPU and a single-core ARM processor without specialized hardware.","['https://openalex.org/W3047841241', 'https://openalex.org/W3045886598', 'https://openalex.org/W2995929068', 'https://openalex.org/W2979476256', 'https://openalex.org/W3036601975', 'https://openalex.org/W2242818861', 'https://openalex.org/W3165040079', 'https://openalex.org/W2030931454', 'https://openalex.org/W2327501763', 'https://openalex.org/W2972659941', 'https://openalex.org/W2963830550', 'https://openalex.org/W2962824709', 'https://openalex.org/W3100270690', 'https://openalex.org/W2885806496', 'https://openalex.org/W3025747610', 'https://openalex.org/W2593116425', 'https://openalex.org/W2887059375', 'https://openalex.org/W3047081942', 'https://openalex.org/W2048520715', 'https://openalex.org/W3100378519', 'https://openalex.org/W2982574661', 'https://openalex.org/W4287887366', 'https://openalex.org/W3130248090', 'https://openalex.org/W3007566156', 'https://openalex.org/W3098361150', 'https://openalex.org/W3012640291', 'https://openalex.org/W2998249245', 'https://openalex.org/W3112034174', 'https://openalex.org/W2898186212', 'https://openalex.org/W2986437614', 'https://openalex.org/W3101314853', 'https://openalex.org/W2972951327', 'https://openalex.org/W6736780897', 'https://openalex.org/W2951082691', 'https://openalex.org/W3110458199', 'https://openalex.org/W4297808394', 'https://openalex.org/W2596378825', 'https://openalex.org/W1494198834', 'https://openalex.org/W2935938411', 'https://openalex.org/W2898564584', 'https://openalex.org/W3020570669', 'https://openalex.org/W2906993533', 'https://openalex.org/W2057563799', 'https://openalex.org/W3006926732', 'https://openalex.org/W2953900859', 'https://openalex.org/W3024768724', 'https://openalex.org/W2948954216', 'https://openalex.org/W6776763299', 'https://openalex.org/W3027324582', 'https://openalex.org/W3150635893', 'https://openalex.org/W2964539095', 'https://openalex.org/W2962814013', 'https://openalex.org/W3122866338', 'https://openalex.org/W2795435272', 'https://openalex.org/W2999160446', 'https://openalex.org/W2971229607', 'https://openalex.org/W4287865702', 'https://openalex.org/W2964299589', 'https://openalex.org/W3082482205', 'https://openalex.org/W4206821167', 'https://openalex.org/W2612690371', 'https://openalex.org/W3099785009', 'https://openalex.org/W2963799213', 'https://openalex.org/W2193413348', 'https://openalex.org/W2963104724', 'https://openalex.org/W2949382160', 'https://openalex.org/W2730845691', 'https://openalex.org/W3009186400', 'https://openalex.org/W3024962219', 'https://openalex.org/W3016021263', 'https://openalex.org/W2965272715', 'https://openalex.org/W4287326402', 'https://openalex.org/W2995129880', 'https://openalex.org/W3082522567', 'https://openalex.org/W2964307104', 'https://openalex.org/W2962760690', 'https://openalex.org/W3111682954', 'https://openalex.org/W2608554408', 'https://openalex.org/W2915661750', 'https://openalex.org/W4287691744', 'https://openalex.org/W2904459034', 'https://openalex.org/W2953737218', 'https://openalex.org/W2981507750', 'https://openalex.org/W2515385951', 'https://openalex.org/W3104686647', 'https://openalex.org/W2995525544', 'https://openalex.org/W2916104401', 'https://openalex.org/W4212774754', 'https://openalex.org/W4288088095', 'https://openalex.org/W2963122961', 'https://openalex.org/W2294370754', 'https://openalex.org/W2726515241', 'https://openalex.org/W1493267010', 'https://openalex.org/W3042776162', 'https://openalex.org/W2963618559', 'https://openalex.org/W2763421725', 'https://openalex.org/W3016181583', 'https://openalex.org/W2937343983', 'https://openalex.org/W3021040286', 'https://openalex.org/W3103272945', 'https://openalex.org/W4297689207', 'https://openalex.org/W3098486933', 'https://openalex.org/W2912512634', 'https://openalex.org/W3005862564', 'https://openalex.org/W3125709657', 'https://openalex.org/W3030437843', 'https://openalex.org/W3034794073', 'https://openalex.org/W4310650179', 'https://openalex.org/W3098439673', 'https://openalex.org/W1821462560', 'https://openalex.org/W2996383576', 'https://openalex.org/W4287692175', 'https://openalex.org/W4236026035', 'https://openalex.org/W2913668833', 'https://openalex.org/W2954386831', 'https://openalex.org/W3099782249', 'https://openalex.org/W3210177631', 'https://openalex.org/W2928560789', 'https://openalex.org/W2795409001', 'https://openalex.org/W3043999252', 'https://openalex.org/W3015141382', 'https://openalex.org/W2914052719', 'https://openalex.org/W2787685498', 'https://openalex.org/W2803193013', 'https://openalex.org/W3015212100', 'https://openalex.org/W2519091744', 'https://openalex.org/W4287591426', 'https://openalex.org/W3095361818', 'https://openalex.org/W4246571396', 'https://openalex.org/W3029286132', 'https://openalex.org/W2842511635', 'https://openalex.org/W4234552385']",2022-11-03
https://openalex.org/W4283834483,https://doi.org/10.21437/interspeech.2022-592,M-Adapter: Modality Adaptation for End-to-End Speech-to-Text Translation,"End-to-end speech-to-text translation models are often initialized with pre-trained speech encoder and pre-trained text decoder.This leads to a significant training gap between pretraining and fine-tuning, largely due to the modality differences between speech outputs from the encoder and text inputs to the decoder.In this work, we aim to bridge the modality gap between speech and text to improve translation quality.We propose M-Adapter, a novel Transformer-based module, to adapt speech representations to text.While shrinking the speech sequence, M-Adapter produces features desired for speech-to-text translation via modelling global and local dependencies of a speech sequence.Our experimental results show that our model outperforms a strong baseline by up to 1 BLEU score on the Must-C En→DE dataset.","['https://openalex.org/W3035490255', 'https://openalex.org/W3092085609', 'https://openalex.org/W3176382501', 'https://openalex.org/W3169483174', 'https://openalex.org/W3200578235', 'https://openalex.org/W2605131327', 'https://openalex.org/W3092424727', 'https://openalex.org/W3125709657', 'https://openalex.org/W4226380987', 'https://openalex.org/W3107826490', 'https://openalex.org/W3183148055', 'https://openalex.org/W3198526264', 'https://openalex.org/W3036601975', 'https://openalex.org/W3153583341', 'https://openalex.org/W4297808394', 'https://openalex.org/W3105669983', 'https://openalex.org/W3097777922', 'https://openalex.org/W2951974815', 'https://openalex.org/W3176455679', 'https://openalex.org/W2997436923', 'https://openalex.org/W3173767661', 'https://openalex.org/W4287629556', 'https://openalex.org/W2079722985', 'https://openalex.org/W4287329822', 'https://openalex.org/W3196833881', 'https://openalex.org/W3015213852', 'https://openalex.org/W4385245566', 'https://openalex.org/W3176711365', 'https://openalex.org/W3112034174']",2022-09-16
https://openalex.org/W4319862404,https://doi.org/10.1109/slt54892.2023.10022991,On Compressing Sequences for Self-Supervised Speech Models,"Compressing self-supervised models has become increasingly necessary, as self-supervised models become larger. While previous approaches have primarily focused on compressing the model size, shortening sequences is also effective in reducing the computational cost. In this work, we study fixed-length and variable-length subsampling along the time axis in self-supervised learning. We explore how individual downstream tasks are sensitive to input frame rates. Subsampling while training self-supervised models not only improves the overall performance on downstream tasks under certain frame rates, but also brings significant speed-up in inference. Variable-length subsampling performs particularly well under low frame rates. In addition, if we have access to phonetic boundaries, we find no degradation in performance for an average frame rate as low as 10 Hz.","['https://openalex.org/W6780218876', 'https://openalex.org/W6786669483', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W6810007534', 'https://openalex.org/W3197580070', 'https://openalex.org/W6751979845', 'https://openalex.org/W6638523607', 'https://openalex.org/W6729956949', 'https://openalex.org/W2802023636', 'https://openalex.org/W6776048684', 'https://openalex.org/W6781533629', 'https://openalex.org/W3085139254', 'https://openalex.org/W2010291496', 'https://openalex.org/W2963211739', 'https://openalex.org/W2327501763', 'https://openalex.org/W2962826786', 'https://openalex.org/W2526425061', 'https://openalex.org/W2296681920', 'https://openalex.org/W2190506272', 'https://openalex.org/W2530876040', 'https://openalex.org/W2627092829', 'https://openalex.org/W3200129129', 'https://openalex.org/W4297841794', 'https://openalex.org/W4297841557', 'https://openalex.org/W3197974236', 'https://openalex.org/W3198782837', 'https://openalex.org/W6791299810', 'https://openalex.org/W3016167541', 'https://openalex.org/W3203140070', 'https://openalex.org/W2933138175', 'https://openalex.org/W1494198834', 'https://openalex.org/W2963250244', 'https://openalex.org/W2962784628', 'https://openalex.org/W6790356757', 'https://openalex.org/W3198134274', 'https://openalex.org/W6795952400', 'https://openalex.org/W6849880362', 'https://openalex.org/W2747874407', 'https://openalex.org/W2916113431', 'https://openalex.org/W4394671563', 'https://openalex.org/W3015468748', 'https://openalex.org/W3112034174', 'https://openalex.org/W4287173589', 'https://openalex.org/W4221145109', 'https://openalex.org/W3134881075', 'https://openalex.org/W3036601975', 'https://openalex.org/W4287704453', 'https://openalex.org/W4319862670']",2023-01-09
https://openalex.org/W4297841625,https://doi.org/10.21437/interspeech.2022-10752,DeToxy: A Large-Scale Multimodal Dataset for Toxicity Classification in Spoken Utterances,"Toxic speech, also known as hate speech, is regarded as one of the crucial issues plaguing online social media today.Most recent work on toxic speech detection is constrained to the modality of text and written conversations with very limited work on toxicity detection from spoken utterances or using the modality of speech.In this paper, we introduce a new dataset DeToxy, the first publicly available toxicity annotated dataset for the English language.DeToxy is sourced from various openly available speech databases and consists of over 2 million utterances.We believe that our dataset would act as a benchmark for the relatively new and un-explored Spoken Language Processing task of detecting toxicity from spoken utterances and boost further research in this space.Finally, we also provide strong unimodal baselines for our dataset and compare traditional two-step and E2E approaches.Our experiments show that in the case of spoken utterances, text-based approaches are largely dependent on gold human-annotated transcripts for their performance and also suffer from the problem of keyword bias.However, the presence of speech files in DeToxy helps facilitates the development of E2E speech models which alleviate both the abovestated problems by better capturing speech clues.","['https://openalex.org/W3015489952', 'https://openalex.org/W4287759860', 'https://openalex.org/W3198771897', 'https://openalex.org/W2968228919', 'https://openalex.org/W2465534249', 'https://openalex.org/W2982223350', 'https://openalex.org/W2742542661', 'https://openalex.org/W2949678053', 'https://openalex.org/W1494198834', 'https://openalex.org/W2959546144', 'https://openalex.org/W3030437843', 'https://openalex.org/W2146334809', 'https://openalex.org/W3127686677', 'https://openalex.org/W2595653137', 'https://openalex.org/W3101648800', 'https://openalex.org/W2785615365', 'https://openalex.org/W3166550823', 'https://openalex.org/W2963686995', 'https://openalex.org/W4301980136', 'https://openalex.org/W2892071465', 'https://openalex.org/W3177971705', 'https://openalex.org/W2342475039', 'https://openalex.org/W3112034174', 'https://openalex.org/W4317536030', 'https://openalex.org/W3121914243', 'https://openalex.org/W2921018690', 'https://openalex.org/W3095918555', 'https://openalex.org/W2920807444', 'https://openalex.org/W2883409523', 'https://openalex.org/W2964306921', 'https://openalex.org/W3118179615', 'https://openalex.org/W3036601975', 'https://openalex.org/W3161587393']",2022-09-16
https://openalex.org/W4385822823,https://doi.org/10.21437/interspeech.2023-1079,How to Estimate Model Transferability of Pre-Trained Speech Models?,"In this work, we introduce a ""score-based assessment"" framework for estimating the transferability of pre-trained speech models (PSMs) for fine-tuning target tasks.We leverage upon two representation theories, Bayesian likelihood estimation and optimal transport, to generate rank scores for the PSM candidates using the extracted representations.Our framework efficiently computes transferability scores without actual finetuning of candidate models or layers by making a temporal independent hypothesis.We evaluate some popular supervised speech models (e.g., Conformer RNN-Transducer) and selfsupervised speech models (e.g., HuBERT) in cross-layer and cross-model settings using public data.Experimental results show a high Spearman's rank correlation and low p-value between our estimation framework and fine-tuning ground truth.Our proposed transferability framework requires less computational time and resources, making it a resource-saving and timeefficient approach for tuning speech foundation models.","['https://openalex.org/W1828163288', 'https://openalex.org/W3015213852', 'https://openalex.org/W4372346241', 'https://openalex.org/W3209059054', 'https://openalex.org/W1494198834', 'https://openalex.org/W3196974791', 'https://openalex.org/W3007522628', 'https://openalex.org/W4292779060', 'https://openalex.org/W4311000453', 'https://openalex.org/W4288089799', 'https://openalex.org/W4385484947', 'https://openalex.org/W4285144981', 'https://openalex.org/W2593116425', 'https://openalex.org/W4375869211', 'https://openalex.org/W3133604157', 'https://openalex.org/W3197580070', 'https://openalex.org/W2982343573', 'https://openalex.org/W4309427288', 'https://openalex.org/W2970112944', 'https://openalex.org/W2797583228', 'https://openalex.org/W4226380987', 'https://openalex.org/W4319862642', 'https://openalex.org/W3154806625', 'https://openalex.org/W4256161595', 'https://openalex.org/W3204696009', 'https://openalex.org/W3112034174', 'https://openalex.org/W3097777922', 'https://openalex.org/W3005343217', 'https://openalex.org/W3042317849', 'https://openalex.org/W4312903743', 'https://openalex.org/W3172443934', 'https://openalex.org/W3215322745', 'https://openalex.org/W2896457183', 'https://openalex.org/W4287198862', 'https://openalex.org/W3186596101', 'https://openalex.org/W2108598243', 'https://openalex.org/W3095410713', 'https://openalex.org/W3195577433', 'https://openalex.org/W2981848390', 'https://openalex.org/W3036601975', 'https://openalex.org/W2187089797', 'https://openalex.org/W4225274946', 'https://openalex.org/W2526050071']",2023-08-14
https://openalex.org/W4297841508,https://doi.org/10.21437/interspeech.2022-105,The ZevoMOS entry to VoiceMOS Challenge 2022,"This paper introduces the ZevoMOS entry to the main track of the VoiceMOS Challenge 2022.The ZevoMOS submission is based on a two-step finetuning of pretrained self-supervised learning (SSL) speech models.The first step uses a task of classifying natural versus synthetic speech, while the second step's task is to predict the MOS scores associated with each training sample.The results of the finetuning process are then combined with the confidence scores extracted from an automatic speech recognition model, as well as the raw embeddings of the training samples obtained from a wav2vec SSL speech model.The team id assigned to the ZevoMOS system within the VoiceMOS Challenge is T01.The submission was placed on the 14th place with respect to the system-level SRCC, and on the 9th place with respect to the utterance-level MSE.The paper also introduces additional evaluations of the intermediate results.","['https://openalex.org/W2916104401', 'https://openalex.org/W3112034174', 'https://openalex.org/W3036601975', 'https://openalex.org/W4297808394', 'https://openalex.org/W3196568361', 'https://openalex.org/W2989571531', 'https://openalex.org/W3161558238', 'https://openalex.org/W2963163009', 'https://openalex.org/W3198270377', 'https://openalex.org/W3024752295', 'https://openalex.org/W2768348081', 'https://openalex.org/W3202278141', 'https://openalex.org/W3196225973', 'https://openalex.org/W2964243274', 'https://openalex.org/W4300816538', 'https://openalex.org/W3150572638', 'https://openalex.org/W2972394484', 'https://openalex.org/W3041561163', 'https://openalex.org/W4311829866', 'https://openalex.org/W2933138175', 'https://openalex.org/W2972943112']",2022-09-16
https://openalex.org/W4375868863,https://doi.org/10.1109/icassp49357.2023.10095929,Self-Supervised Speech Representation Learning for Keyword-Spotting With Light-Weight Transformers,"Self-supervised speech representation learning (S3RL) is revolutionizing the way we leverage the ever-growing availability of data. While S3RL related studies typically use large models, we employ light-weight networks to comply with tight memory of compute-constrained devices. We demonstrate the effectiveness of S3RL on a keyword-spotting (KS) problem by using transformers with 330k parameters and propose a mechanism to enhance utterance-wise distinction, which proves crucial for improving performance on classification tasks. On the Google speech commands v2 dataset, the proposed method applied to the Auto-Regressive Predictive Coding S3RL led to a 1.2% accuracy improvement compared to training from scratch. On an in-house KS dataset with four different keywords, it provided 6% to 23.7% relative false accept improvement at fixed false reject rate. We argue this demonstrates the applicability of S3RL approaches to light-weight models for KS and confirms S3RL is a powerful alternative to traditional supervised learning for resource-constrained applications.","['https://openalex.org/W6780218876', 'https://openalex.org/W6775396121', 'https://openalex.org/W3206996142', 'https://openalex.org/W6786669483', 'https://openalex.org/W2842511635', 'https://openalex.org/W3203140070', 'https://openalex.org/W2962739339', 'https://openalex.org/W6755207826', 'https://openalex.org/W6810007534', 'https://openalex.org/W6678775411', 'https://openalex.org/W6750665317', 'https://openalex.org/W3206252155', 'https://openalex.org/W4226020322', 'https://openalex.org/W3209984917', 'https://openalex.org/W1494198834', 'https://openalex.org/W6729448088', 'https://openalex.org/W6637373629', 'https://openalex.org/W6739901393', 'https://openalex.org/W6631190155', 'https://openalex.org/W2936774411', 'https://openalex.org/W6846177407', 'https://openalex.org/W2982223350', 'https://openalex.org/W6769238691', 'https://openalex.org/W3209059054', 'https://openalex.org/W4313156423', 'https://openalex.org/W3035524453', 'https://openalex.org/W3016011332', 'https://openalex.org/W3197580070', 'https://openalex.org/W2124509324', 'https://openalex.org/W3112034174', 'https://openalex.org/W2896457183', 'https://openalex.org/W2797583228', 'https://openalex.org/W3036601975', 'https://openalex.org/W2547875792', 'https://openalex.org/W3015265920', 'https://openalex.org/W4319862700', 'https://openalex.org/W1686810756', 'https://openalex.org/W4221145109', 'https://openalex.org/W1522301498', 'https://openalex.org/W4297808394', 'https://openalex.org/W4385245566', 'https://openalex.org/W2981991061']",2023-05-05
https://openalex.org/W4375869094,https://doi.org/10.1109/icassp49357.2023.10096248,Neural Architecture of Speech,"A vast literature on brain encoding has effectively harnessed deep neural network models for accurately predicting brain activations from visual or text stimuli. Unfortunately, there is not much work on brain encoding for speech stimuli. The few existing studies on brain encoding for speech stimuli transcribe speech to text and then leverage text-only models for encoding, thereby ignoring audio signals completely. However, recently several speech representation learning models have revolutionized the field of speech processing. Inspired by the recent progress on deep learning models for speech, we present a first systematic study on understanding human speech processing by probing neural speech models to predict both language and auditory brain region activations. In particular, we investigate 30 speech representation models grouped into four categories: (i) traditional feature engineering, (ii) generative, (iii) predictive, and (iv) contrastive, to study how these models encode the speech stimuli and align with human brain activity for the Moth Radio Hour fMRI (functional magnetic resonance imaging) dataset. We find that both contrastive (Wav2Vec2.0) and predictive models (HuBERT, Data2Vec) are very accurate. Specifically, Data2Vec aligns the best with both language and auditory brain regions among all investigated models. We make our code publicly available <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .","['https://openalex.org/W6788335241', 'https://openalex.org/W6846678071', 'https://openalex.org/W3201143670', 'https://openalex.org/W2945310593', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W6838662218', 'https://openalex.org/W2973049979', 'https://openalex.org/W3016181583', 'https://openalex.org/W3197411683', 'https://openalex.org/W4224927631', 'https://openalex.org/W2344975321', 'https://openalex.org/W6769196770', 'https://openalex.org/W2970648593', 'https://openalex.org/W6849003372', 'https://openalex.org/W4224821750', 'https://openalex.org/W6780218876', 'https://openalex.org/W3203140070', 'https://openalex.org/W6810007534', 'https://openalex.org/W3015213852', 'https://openalex.org/W2526050071', 'https://openalex.org/W2982223350', 'https://openalex.org/W6804298400', 'https://openalex.org/W3041561163', 'https://openalex.org/W3097286738', 'https://openalex.org/W2972943112', 'https://openalex.org/W6783041029', 'https://openalex.org/W3015265920', 'https://openalex.org/W3206532275', 'https://openalex.org/W6784776607', 'https://openalex.org/W4287854368', 'https://openalex.org/W6786669483', 'https://openalex.org/W4297841853', 'https://openalex.org/W6777232839', 'https://openalex.org/W3206996142', 'https://openalex.org/W3103949942', 'https://openalex.org/W3197580070', 'https://openalex.org/W6754825039', 'https://openalex.org/W6767096107', 'https://openalex.org/W6763027002', 'https://openalex.org/W2997066978', 'https://openalex.org/W6765524274', 'https://openalex.org/W1992570774', 'https://openalex.org/W3101022551', 'https://openalex.org/W3112034174', 'https://openalex.org/W3167207712', 'https://openalex.org/W3036601975', 'https://openalex.org/W2979476256', 'https://openalex.org/W3148040514', 'https://openalex.org/W2983144399', 'https://openalex.org/W3198858531', 'https://openalex.org/W4226399507', 'https://openalex.org/W4221145109', 'https://openalex.org/W2953945416', 'https://openalex.org/W4320086272', 'https://openalex.org/W2892147425', 'https://openalex.org/W3206626273', 'https://openalex.org/W3085600465', 'https://openalex.org/W2947012833', 'https://openalex.org/W4281765823', 'https://openalex.org/W4317536043']",2023-05-05
https://openalex.org/W4392911119,https://doi.org/10.1109/icassp48485.2024.10446075,Addressing Data Scarcity in Voice Disorder Detection with Self-Supervised Models,"Machine learning (ML) has shown promising results in the field of voice disorder detection over the past decade. However, the diversity of recording conditions, audio content, languages, and the scarcity of examples for each of these combinations pose a challenge in building ML models that can reliably detect voice disorders. Recent advancements in Self-Supervised Learning (SSL) offer hope by leveraging large datasets to pretrain models and extract audio features with high resilience for downstream tasks.In this paper, we fairly exhaustively explore commonly used SSL model representations to assess their suitability for addressing the downstream task of voice disorder detection. Using a combination of Support Vector Machines (SVM) and feedforward Deep Neural Networks (DNN) we show: i) that the combination of vowels /a/,/i/, and /u/ perform better than individual vowels; ii) SSL-based features generalize well to out-of-domain databases, and iii) that while spectral features like MFCC perform equally well compared to SSL-based features when trained and tested on the same database, performances seems to deteriorate when training and testing across different databases.","['https://openalex.org/W2113650738', 'https://openalex.org/W2398145559', 'https://openalex.org/W1772972328', 'https://openalex.org/W6604471452', 'https://openalex.org/W2806307414', 'https://openalex.org/W2972943112', 'https://openalex.org/W3112034174', 'https://openalex.org/W3015265920', 'https://openalex.org/W6773553514', 'https://openalex.org/W3198858531', 'https://openalex.org/W3097286738', 'https://openalex.org/W6780218876', 'https://openalex.org/W2973049979', 'https://openalex.org/W2339666751', 'https://openalex.org/W2506429203', 'https://openalex.org/W3184701424', 'https://openalex.org/W3164509420', 'https://openalex.org/W4372338343', 'https://openalex.org/W3203140070', 'https://openalex.org/W3206252155', 'https://openalex.org/W3209059054', 'https://openalex.org/W6769196770', 'https://openalex.org/W3209984917', 'https://openalex.org/W4296068785', 'https://openalex.org/W3035390927', 'https://openalex.org/W3208743843', 'https://openalex.org/W6794548583', 'https://openalex.org/W6766978945', 'https://openalex.org/W2101234009', 'https://openalex.org/W4293363567', 'https://openalex.org/W3036601975', 'https://openalex.org/W3213029956', 'https://openalex.org/W4221145109', 'https://openalex.org/W3016181583', 'https://openalex.org/W4295312788', 'https://openalex.org/W110017167']",2024-03-18
https://openalex.org/W3197842028,https://doi.org/10.21437/interspeech.2021-817,"Auto-KWS 2021 Challenge: Task, Datasets, and Baselines","Auto-KWS 2021 challenge calls for automated machine learning (AutoML) solutions to automate the process of applying machine learning to a customized keyword spotting task.Compared with other keyword spotting tasks, Auto-KWS challenge has the following three characteristics: 1) The challenge focuses on the problem of customized keyword spotting, where the target device can only be awakened by an enrolled speaker with his specified keyword.The speaker can use any language and accent to define his keyword.2) All dataset of the challenge is recorded in realistic environment.It is to simulate different user scenarios.3) Auto-KWS is a ""code competition"", where participants need to submit AutoML solutions, then the platform automatically runs the enrollment and prediction steps with the submitted code.This challenge aims at promoting the development of a more personalized and flexible keyword spotting system.Two baseline systems are provided to all participants as references.","['https://openalex.org/W4298277274', 'https://openalex.org/W2219249508', 'https://openalex.org/W2963571336', 'https://openalex.org/W2982223350', 'https://openalex.org/W3015574842', 'https://openalex.org/W2613303405', 'https://openalex.org/W2034940213', 'https://openalex.org/W2407023693', 'https://openalex.org/W3007913012', 'https://openalex.org/W2769912137', 'https://openalex.org/W2979476256', 'https://openalex.org/W3133600743', 'https://openalex.org/W3036601975', 'https://openalex.org/W2696967604', 'https://openalex.org/W3096606124', 'https://openalex.org/W2795183504', 'https://openalex.org/W3011966179', 'https://openalex.org/W3094791070', 'https://openalex.org/W3046052470', 'https://openalex.org/W3112034174', 'https://openalex.org/W2973157397', 'https://openalex.org/W3097810274', 'https://openalex.org/W3118331024', 'https://openalex.org/W2964187693', 'https://openalex.org/W2972943112', 'https://openalex.org/W4214794828', 'https://openalex.org/W1496120315', 'https://openalex.org/W1524333225', 'https://openalex.org/W2807071886', 'https://openalex.org/W2962959915', 'https://openalex.org/W4392271976', 'https://openalex.org/W4297808394', 'https://openalex.org/W2973049979']",2021-08-27
https://openalex.org/W3205080563,https://doi.org/10.1109/icassp43922.2022.9746249,Improving Pseudo-Label Training For End-To-End Speech Recognition Using Gradient Mask,"In the recent trend of semi-supervised speech recognition, both self-supervised representation learning and pseudo-labeling have shown promising results. In this paper, we propose a novel approach to combine their ideas for end-to-end speech recognition model. Without any extra loss function, we utilize the Gradient Mask to optimize the model when training on pseudo-label. This method forces the speech recognition model to predict from the masked in-put to learn strong acoustic representation and make training robust to label noise. In our semi-supervised experiments, the method can improve the model's performance when training on pseudo-label and our method achieved competitive results comparing with other semi-supervised approaches on the Librispeech 100 hours experiments.","['https://openalex.org/W2936774411', 'https://openalex.org/W6638575559', 'https://openalex.org/W6629717138', 'https://openalex.org/W3015315932', 'https://openalex.org/W6631362777', 'https://openalex.org/W3097777922', 'https://openalex.org/W6755207826', 'https://openalex.org/W2982223350', 'https://openalex.org/W6780361010', 'https://openalex.org/W6786669483', 'https://openalex.org/W3209059054', 'https://openalex.org/W6638749077', 'https://openalex.org/W2167460663', 'https://openalex.org/W6780530918', 'https://openalex.org/W2545319977', 'https://openalex.org/W3004534439', 'https://openalex.org/W2127141656', 'https://openalex.org/W2889213362', 'https://openalex.org/W6797992114', 'https://openalex.org/W2802248956', 'https://openalex.org/W3015522062', 'https://openalex.org/W2327501763', 'https://openalex.org/W2928408492', 'https://openalex.org/W3026041220', 'https://openalex.org/W3096338464', 'https://openalex.org/W2512655038', 'https://openalex.org/W6784436999', 'https://openalex.org/W1993660824', 'https://openalex.org/W2973049979', 'https://openalex.org/W3015265920', 'https://openalex.org/W2972943112', 'https://openalex.org/W3024182269', 'https://openalex.org/W6780218876', 'https://openalex.org/W6788335241', 'https://openalex.org/W3160235762', 'https://openalex.org/W3178203035', 'https://openalex.org/W4287716221', 'https://openalex.org/W3169320628', 'https://openalex.org/W3027083471', 'https://openalex.org/W3197223534', 'https://openalex.org/W3041561163', 'https://openalex.org/W3042609801', 'https://openalex.org/W3103005696', 'https://openalex.org/W3112034174', 'https://openalex.org/W3036601975', 'https://openalex.org/W1828163288', 'https://openalex.org/W2962907457', 'https://openalex.org/W3167207712', 'https://openalex.org/W2896457183', 'https://openalex.org/W1494198834', 'https://openalex.org/W3025165719', 'https://openalex.org/W3121299949', 'https://openalex.org/W3099782249', 'https://openalex.org/W3093788532', 'https://openalex.org/W2963341956', 'https://openalex.org/W1524333225', 'https://openalex.org/W2962784628']",2022-04-27
https://openalex.org/W4372259832,https://doi.org/10.1109/icassp49357.2023.10095392,FindAdaptNet: Find and Insert Adapters by Learned Layer Importance,"Adapters are lightweight bottleneck modules introduced to assist pre-trained self-supervised learning (SSL) models to be customized to new tasks. However, searching the appropriate layers to insert adapters on large models has become difficult due to the large number of possible layers and thus a vast search space (2 <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">N</sup> possibilities for N layers). In this paper, we propose a technique that achieves automatic insertion of adapters for downstream automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. Our approach is based on two-stage training. First, we train our model for a specific downstream task with additional shallow learnable layers and weight parameters to obtain the weighted summation over the output of each layer in SSL. This training method is established by the SUPERB baseline [1]. This first-stage training determines the most important layers given their respective weights. In the second stage, we proceed to insert adapters to the most important layers, retaining both performance and neural architecture search efficiency. On the CommonVoice dataset[2] we obtain 20.6% absolute improvement in Word Error Rate (WER) on the Welsh language against the conventional method, which inserts the adapter modules into the highest layers without search. In the SLURP SLU task, our method yields 4.0% intent accuracy improvement against the same conventional baseline.","['https://openalex.org/W3172698324', 'https://openalex.org/W4225274946', 'https://openalex.org/W3100460087', 'https://openalex.org/W6795498353', 'https://openalex.org/W6802744804', 'https://openalex.org/W3099793224', 'https://openalex.org/W6771467084', 'https://openalex.org/W3197580070', 'https://openalex.org/W3217767527', 'https://openalex.org/W3207558756', 'https://openalex.org/W2971840980', 'https://openalex.org/W4225534571', 'https://openalex.org/W6631943919', 'https://openalex.org/W1494198834', 'https://openalex.org/W6631190155', 'https://openalex.org/W3097777922', 'https://openalex.org/W6787191599', 'https://openalex.org/W6739901393', 'https://openalex.org/W2933138175', 'https://openalex.org/W2058094241', 'https://openalex.org/W2043701535', 'https://openalex.org/W6786669483', 'https://openalex.org/W6759579507', 'https://openalex.org/W6780218876', 'https://openalex.org/W4281492411', 'https://openalex.org/W3209984917', 'https://openalex.org/W3209059054', 'https://openalex.org/W3036601975', 'https://openalex.org/W1533861849', 'https://openalex.org/W2964303773', 'https://openalex.org/W3030437843', 'https://openalex.org/W3161686170', 'https://openalex.org/W1522301498', 'https://openalex.org/W2963211188', 'https://openalex.org/W4385245566', 'https://openalex.org/W3205949070', 'https://openalex.org/W3197845195', 'https://openalex.org/W3112034174']",2023-05-05
https://openalex.org/W4312120960,https://doi.org/10.23919/apsipaasc55919.2022.9979878,Exploring Speaker Age Estimation on Different Self-Supervised Learning Models,"Self-supervised learning (SSL) has played an important role in various tasks in the field of speech and audio processing. However, there is limited research on adapting these SSL models to predict the speaker's age and gender using speech signals. In this paper, we investigate seven SSL models, namely PASE+, NPC, wav2vec 2.0, XLSR, HuBERT, WavLM, and data2vec in the joint age estimation and gender classification task on the TIMIT corpus. Additionally, we also study the effect of using different hidden encoder layers within these models on the age estimation result. Furthermore, we evaluate how the performance of different SSL models varies in predicting the speaker's age under simulated noisy conditions. The simulated noisy speech is created by mixing the clean utterance from the TIMIT test set with random noises from the Music and Noise category of the MUSAN corpus on multiple levels of signal-to-noise ratio (SNR). Our findings confirm that a recent SSL model, namely WavLM can obtain better and more robust speech representation than wav2vec 2.0 SSL model used in the current state-of-the-art (SOTA) approach by achieving a 3.6% and 11.32% mean average error (MAE) reduction on the clean and 5dB SNR TIMIT test set.","['https://openalex.org/W6637373629', 'https://openalex.org/W2194775991', 'https://openalex.org/W1861492603', 'https://openalex.org/W2117539524', 'https://openalex.org/W2108598243', 'https://openalex.org/W6755207826', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3015213852', 'https://openalex.org/W6786669483', 'https://openalex.org/W4319862287', 'https://openalex.org/W3163571828', 'https://openalex.org/W2982223350', 'https://openalex.org/W3205635414', 'https://openalex.org/W4225635674', 'https://openalex.org/W4297841781', 'https://openalex.org/W3198858531', 'https://openalex.org/W3198429080', 'https://openalex.org/W3209984917', 'https://openalex.org/W6810007534', 'https://openalex.org/W4297841573', 'https://openalex.org/W3197580070', 'https://openalex.org/W1690817556', 'https://openalex.org/W58270994', 'https://openalex.org/W3021209107', 'https://openalex.org/W1856212627', 'https://openalex.org/W1838165847', 'https://openalex.org/W57879486', 'https://openalex.org/W2112465581', 'https://openalex.org/W2150769028', 'https://openalex.org/W2139749422', 'https://openalex.org/W2398521502', 'https://openalex.org/W2890964092', 'https://openalex.org/W3181310845', 'https://openalex.org/W6803423307', 'https://openalex.org/W1494198834', 'https://openalex.org/W1635512741', 'https://openalex.org/W4281492411', 'https://openalex.org/W6739901393', 'https://openalex.org/W6688816777', 'https://openalex.org/W4307823382', 'https://openalex.org/W2963677766', 'https://openalex.org/W4286895717', 'https://openalex.org/W2219249508', 'https://openalex.org/W4221145109', 'https://openalex.org/W3036601975', 'https://openalex.org/W2896457183', 'https://openalex.org/W3112034174', 'https://openalex.org/W4385245566']",2022-11-07
https://openalex.org/W4385484975,https://doi.org/10.1109/icasspw59220.2023.10193460,Phone and Speaker Spatial Organization in Self-Supervised Speech Representations,"Self-supervised representations of speech are currently being widely used for a large number of applications. Recently, some efforts have been made in trying to analyze the type of information present in each of these representations. Most such work uses downstream models to test whether the representations can be successfully used for a specific task. The downstream models, though, typically perform nonlinear operations on the representation extracting information that may not have been readily available in the original representation. In this work, we analyze the spatial organization of phone and speaker information in several state-of-the-art speech representations using methods that do not require a downstream model. We measure how different layers encode basic acoustic parameters such as formants and pitch using representation similarity analysis. Further, we study the extent to which each representation clusters the speech samples by phone or speaker classes using non-parametric statistical testing. Our results indicate that models represent these speech attributes differently depending on the target task used during pretraining.","['https://openalex.org/W4311137818', 'https://openalex.org/W3163596720', 'https://openalex.org/W4225726571', 'https://openalex.org/W6788328058', 'https://openalex.org/W6761472960', 'https://openalex.org/W4226380987', 'https://openalex.org/W3162133897', 'https://openalex.org/W2395899413', 'https://openalex.org/W3044967013', 'https://openalex.org/W2407151108', 'https://openalex.org/W2982223350', 'https://openalex.org/W6786669483', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W6780218876', 'https://openalex.org/W6810007534', 'https://openalex.org/W6755207826', 'https://openalex.org/W6729448088', 'https://openalex.org/W6631362777', 'https://openalex.org/W2889326414', 'https://openalex.org/W6679728998', 'https://openalex.org/W4206960538', 'https://openalex.org/W2888954148', 'https://openalex.org/W1635512741', 'https://openalex.org/W4205775916', 'https://openalex.org/W3197580070', 'https://openalex.org/W3036601975', 'https://openalex.org/W1482790282', 'https://openalex.org/W3096196861', 'https://openalex.org/W3121914243', 'https://openalex.org/W2942810103', 'https://openalex.org/W2547875792', 'https://openalex.org/W4221145109', 'https://openalex.org/W1524333225', 'https://openalex.org/W2129957127', 'https://openalex.org/W2896457183', 'https://openalex.org/W3112034174']",2023-06-04
https://openalex.org/W3209623737,https://doi.org/10.1109/icassp43922.2022.9747005,Combining Unsupervised and Text Augmented Semi-Supervised Learning For Low Resourced Autoregressive Speech Recognition,"Recent advances in unsupervised representation learning have demonstrated the impact of pretraining on large amounts of read speech. We adapt these techniques for domain adaptation in low-resource—both in terms of data and compute—conversational and broadcast domains. Moving beyond CTC, we pretrain state-of-the-art Conformer models in an unsupervised manner. While the unsupervised approach outperforms traditional semi-supervised training, the techniques are complementary. Combining the techniques is a 5% absolute improvement in WER, averaged over all conditions, compared to semi-supervised training alone. Additional text data is incorporated through external language models. By using CTC-based decoding, we are better able to take advantage of the additional text data. When used as a transcription model, it allows the Conformer model to better incorporate the knowledge from the language model through semi-supervised training than shallow fusion. Final performance is an additional 2% better absolute when using CTC-based decoding for semi-supervised training compared to shallow fusion.","['https://openalex.org/W6631362777', 'https://openalex.org/W2963537349', 'https://openalex.org/W2936451900', 'https://openalex.org/W3041561163', 'https://openalex.org/W2972943112', 'https://openalex.org/W6786669483', 'https://openalex.org/W3209059054', 'https://openalex.org/W6640059789', 'https://openalex.org/W6743477263', 'https://openalex.org/W3152221657', 'https://openalex.org/W3097777922', 'https://openalex.org/W2587275078', 'https://openalex.org/W2936774411', 'https://openalex.org/W2928408492', 'https://openalex.org/W2526425061', 'https://openalex.org/W2189391786', 'https://openalex.org/W2802248956', 'https://openalex.org/W2963211739', 'https://openalex.org/W3162334527', 'https://openalex.org/W6780218876', 'https://openalex.org/W2842511635', 'https://openalex.org/W6629717138', 'https://openalex.org/W6755207826', 'https://openalex.org/W2980423179', 'https://openalex.org/W3026041220', 'https://openalex.org/W2977458338', 'https://openalex.org/W6784614252', 'https://openalex.org/W6796059234', 'https://openalex.org/W2407922671', 'https://openalex.org/W3163793923', 'https://openalex.org/W2962780374', 'https://openalex.org/W4297808394', 'https://openalex.org/W3169320628', 'https://openalex.org/W2745596852', 'https://openalex.org/W3134774296', 'https://openalex.org/W2748679025', 'https://openalex.org/W2962907457', 'https://openalex.org/W1524333225', 'https://openalex.org/W3112034174', 'https://openalex.org/W3099782249', 'https://openalex.org/W3025165719', 'https://openalex.org/W3093579165', 'https://openalex.org/W2963341956', 'https://openalex.org/W2888779557', 'https://openalex.org/W1915251500', 'https://openalex.org/W2926827382', 'https://openalex.org/W3168982866', 'https://openalex.org/W1494198834', 'https://openalex.org/W2896457183', 'https://openalex.org/W3036601975', 'https://openalex.org/W2795935804']",2022-04-27
https://openalex.org/W3166129584,https://doi.org/10.21437/interspeech.2021-1644,Scaling Laws for Acoustic Models,"There is a recent trend in machine learning to increase model quality by growing models to sizes previously thought to be unreasonable. Recent work has shown that autoregressive generative models with cross-entropy objective functions exhibit smooth power-law relationships, or scaling laws, that predict model quality from model size, training set size, and the available compute budget. These scaling laws allow one to choose nearly optimal hyper-parameters given constraints on available training data, model parameter count, or training computation budget. In this paper, we demonstrate that acoustic models trained with an auto-predictive coding loss behave as if they are subject to similar scaling laws. We extend previous work to jointly predict loss due to model size, to training set size, and to the inherent ""irreducible loss"" of the task. We find that the scaling laws accurately match model performance over two orders of magnitude in both model size and training set size, and make predictions about the limits of model performance.","['https://openalex.org/W3015265920', 'https://openalex.org/W3001279689', 'https://openalex.org/W3119866685', 'https://openalex.org/W2842511635', 'https://openalex.org/W2963341956', 'https://openalex.org/W3099782249', 'https://openalex.org/W3016011332', 'https://openalex.org/W3095645723', 'https://openalex.org/W3112034174', 'https://openalex.org/W3093533780', 'https://openalex.org/W3030163527', 'https://openalex.org/W3097286738']",2021-08-27
https://openalex.org/W4294312977,https://doi.org/10.36227/techrxiv.20743012,Singing Beat Tracking With Self-supervised Front-end and Linear Transformers,"&lt;p&gt;Tracking beats of singing voices without the presence of musical accompaniment can find many applications in music production, automatic song arrangement, and social media interaction. Its main challenge is the lack of strong rhythmic and harmonic patterns that are important for music rhythmic analysis in general. Even for human listeners, this can be a challenging task. As a result, existing music beat tracking systems fail to deliver satisfactory performance on singing voices. In this paper, we propose singing beat tracking as a novel task, and propose the first approach to solving this task. Our approach leverages semantic information of singing voices by employing pre-trained self-supervised WavLM and DistilHuBERT speech representations as the front-end and uses a self-attention encoder layer to predict beats. To train and test the system, we obtain separated singing voices and their beat annotations using source separation and beat tracking on complete songs, followed by manual corrections. Experiments on the 741 separated vocal tracks of the GTZAN dataset show that the proposed system outperforms several state-of-the-art music beat tracking methods by a large margin in terms of beat tracking accuracy. Ablation studies also confirm the advantages of pre-trained self-supervised speech representations over generic spectral features.&lt;/p&gt;","['https://openalex.org/W2404509795', 'https://openalex.org/W3217401639', 'https://openalex.org/W4226300471', 'https://openalex.org/W3037798801', 'https://openalex.org/W2052872069', 'https://openalex.org/W2160484251', 'https://openalex.org/W4297854551', 'https://openalex.org/W4280498166', 'https://openalex.org/W3170881237', 'https://openalex.org/W4297854553', 'https://openalex.org/W3210646981', 'https://openalex.org/W4304206546', 'https://openalex.org/W4226380987', 'https://openalex.org/W4310304411', 'https://openalex.org/W2134679199', 'https://openalex.org/W3169320628', 'https://openalex.org/W3209984917', 'https://openalex.org/W16345605', 'https://openalex.org/W3197580070', 'https://openalex.org/W4286973758', 'https://openalex.org/W3033529678', 'https://openalex.org/W2117330939', 'https://openalex.org/W2964070952', 'https://openalex.org/W3036601975', 'https://openalex.org/W2126264921', 'https://openalex.org/W75292264', 'https://openalex.org/W4226075337', 'https://openalex.org/W2156063659', 'https://openalex.org/W2572771584', 'https://openalex.org/W3112034174', 'https://openalex.org/W4297809150', 'https://openalex.org/W3118825798', 'https://openalex.org/W4304135971', 'https://openalex.org/W1519655822', 'https://openalex.org/W2133824856']",2022-09-02
https://openalex.org/W4319862658,https://doi.org/10.1109/slt54892.2023.10022897,Combining Contrastive and Non-Contrastive Losses for Fine-Tuning Pretrained Models in Speech Analysis,"Embedding paralinguistic properties is a challenging task as there are only a few hours of training data available for domains such as emotional speech. One solution to this problem is to pretrain a general self-supervised speech representation model on large amounts of unlabeled speech. This pretrained model is then finetuned to a specific task. Paralinguistic properties however have notoriously high class variance, making the finetuning ineffective. In this work, we propose a two step approach to this. First we improve the embedding space, then we train an adapter to bridge the gap from the embedding space to a classification task. In order to improve the class invariance we use a combination of contrastive and non-contrastive losses to explicitly optimize for class invariant, yet discriminative features. Our approach consistently outperforms baselines that are finetuned end-to-end on multiple tasks and surpasses a benchmark on state-of-the-art emotion classification.","['https://openalex.org/W1962925405', 'https://openalex.org/W4210694145', 'https://openalex.org/W2899436735', 'https://openalex.org/W2754547322', 'https://openalex.org/W2530305026', 'https://openalex.org/W6752888775', 'https://openalex.org/W2972362771', 'https://openalex.org/W3024869864', 'https://openalex.org/W2807627734', 'https://openalex.org/W2962788625', 'https://openalex.org/W3209059054', 'https://openalex.org/W6786669483', 'https://openalex.org/W3015265920', 'https://openalex.org/W2982223350', 'https://openalex.org/W3041561163', 'https://openalex.org/W6780218876', 'https://openalex.org/W3198275944', 'https://openalex.org/W3197642003', 'https://openalex.org/W6787141514', 'https://openalex.org/W2171590421', 'https://openalex.org/W2842511635', 'https://openalex.org/W2963775347', 'https://openalex.org/W6791742336', 'https://openalex.org/W6753417141', 'https://openalex.org/W3001160332', 'https://openalex.org/W3119689200', 'https://openalex.org/W3197580070', 'https://openalex.org/W2964105864', 'https://openalex.org/W1983645263', 'https://openalex.org/W6735236233', 'https://openalex.org/W2146334809', 'https://openalex.org/W6771467084', 'https://openalex.org/W3127686677', 'https://openalex.org/W1494198834', 'https://openalex.org/W2726515241', 'https://openalex.org/W3181310845', 'https://openalex.org/W2187089797', 'https://openalex.org/W3112034174', 'https://openalex.org/W4297808394', 'https://openalex.org/W3113594615', 'https://openalex.org/W3089942512']",2023-01-09
https://openalex.org/W3166440012,https://doi.org/10.48550/arxiv.2106.05933,"PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition","Self-supervised speech representation learning (speech SSL) has demonstrated the benefit of scale in learning rich representations for Automatic Speech Recognition (ASR) with limited paired data, such as wav2vec 2.0. We investigate the existence of sparse subnetworks in pre-trained speech SSL models that achieve even better low-resource ASR results. However, directly applying widely adopted pruning methods such as the Lottery Ticket Hypothesis (LTH) is suboptimal in the computational cost needed. Moreover, we show that the discovered subnetworks yield minimal performance gain compared to the original dense network. We present Prune-Adjust-Re-Prune (PARP), which discovers and finetunes subnetworks for much better performance, while only requiring a single downstream ASR finetuning run. PARP is inspired by our surprising observation that subnetworks pruned for pre-training tasks need merely a slight adjustment to achieve a sizeable performance boost in downstream ASR tasks. Extensive experiments on low-resource ASR verify (1) sparse subnetworks exist in mono-lingual/multi-lingual pre-trained speech SSL, and (2) the computational advantage and performance gain of PARP over baseline pruning methods. In particular, on the 10min Librispeech split without LM decoding, PARP discovers subnetworks from wav2vec 2.0 with an absolute 10.9%/12.6% WER decrease compared to the full model. We further demonstrate the effectiveness of PARP via: cross-lingual pruning without any phone recognition degradation, the discovery of a multi-lingual subnetwork for 10 spoken languages in 1 finetuning run, and its applicability to pre-trained BERT/XLNet for natural language tasks.","['https://openalex.org/W3163842642', 'https://openalex.org/W3143035657', 'https://openalex.org/W3160399536', 'https://openalex.org/W3148001440', 'https://openalex.org/W2802201485', 'https://openalex.org/W2982223350', 'https://openalex.org/W2963340922', 'https://openalex.org/W3025165719', 'https://openalex.org/W2971237698', 'https://openalex.org/W2294543795', 'https://openalex.org/W3093579165', 'https://openalex.org/W3098680936', 'https://openalex.org/W3099782249', 'https://openalex.org/W3162665866', 'https://openalex.org/W3178203035', 'https://openalex.org/W3198217962', 'https://openalex.org/W3160405885', 'https://openalex.org/W3035615218', 'https://openalex.org/W1974205368', 'https://openalex.org/W3205032693', 'https://openalex.org/W3112034174', 'https://openalex.org/W2996383576', 'https://openalex.org/W2962965870', 'https://openalex.org/W2963813662', 'https://openalex.org/W3162391496', 'https://openalex.org/W2768501777', 'https://openalex.org/W3163600291', 'https://openalex.org/W3207300132', 'https://openalex.org/W2948130861', 'https://openalex.org/W2608554408', 'https://openalex.org/W2730658205', 'https://openalex.org/W3152519008', 'https://openalex.org/W3021469861', 'https://openalex.org/W3178584664', 'https://openalex.org/W3202278141', 'https://openalex.org/W3147962056', 'https://openalex.org/W2930682606', 'https://openalex.org/W3161223924', 'https://openalex.org/W3157923770', 'https://openalex.org/W3204123830', 'https://openalex.org/W3205533980', 'https://openalex.org/W3104263050', 'https://openalex.org/W2963310665', 'https://openalex.org/W3104136798', 'https://openalex.org/W2975044525', 'https://openalex.org/W3180374548', 'https://openalex.org/W2963981420', 'https://openalex.org/W2972808286', 'https://openalex.org/W2114766824', 'https://openalex.org/W3129009457', 'https://openalex.org/W2972943112', 'https://openalex.org/W3206559778', 'https://openalex.org/W2963674932', 'https://openalex.org/W3040454670', 'https://openalex.org/W3034487470', 'https://openalex.org/W2043422002', 'https://openalex.org/W2888867175', 'https://openalex.org/W3204696009', 'https://openalex.org/W2084910356', 'https://openalex.org/W2125389748', 'https://openalex.org/W3111265704', 'https://openalex.org/W2965862774', 'https://openalex.org/W3167207712', 'https://openalex.org/W3037057938', 'https://openalex.org/W2407115099', 'https://openalex.org/W3197974236', 'https://openalex.org/W3081179222', 'https://openalex.org/W3003875258', 'https://openalex.org/W3166035876', 'https://openalex.org/W3035081900', 'https://openalex.org/W104222852', 'https://openalex.org/W3165666670', 'https://openalex.org/W2963247446', 'https://openalex.org/W3206252155', 'https://openalex.org/W2951569836', 'https://openalex.org/W3144173820', 'https://openalex.org/W2894835365', 'https://openalex.org/W3094550259', 'https://openalex.org/W3162309234', 'https://openalex.org/W3204224625', 'https://openalex.org/W1515156256', 'https://openalex.org/W3104350794', 'https://openalex.org/W3095292526', 'https://openalex.org/W2936481169', 'https://openalex.org/W3193521535', 'https://openalex.org/W3207558756', 'https://openalex.org/W3162649911', 'https://openalex.org/W2963400886', 'https://openalex.org/W3173970713', 'https://openalex.org/W3169320628', 'https://openalex.org/W2988736778', 'https://openalex.org/W3197278374', 'https://openalex.org/W2112984492', 'https://openalex.org/W3140429000', 'https://openalex.org/W3147414526', 'https://openalex.org/W3152884768', 'https://openalex.org/W3038041907', 'https://openalex.org/W3111921445', 'https://openalex.org/W3128478537', 'https://openalex.org/W3160525311', 'https://openalex.org/W3157697407', 'https://openalex.org/W2962813140', 'https://openalex.org/W3041561163', 'https://openalex.org/W3205644108', 'https://openalex.org/W2963503967', 'https://openalex.org/W3001899777', 'https://openalex.org/W2915589364', 'https://openalex.org/W3034234149', 'https://openalex.org/W3022969335', 'https://openalex.org/W2842511635', 'https://openalex.org/W2156700117', 'https://openalex.org/W2291975472', 'https://openalex.org/W3139918052', 'https://openalex.org/W3205710300', 'https://openalex.org/W3087835661', 'https://openalex.org/W2995816250', 'https://openalex.org/W3162868000', 'https://openalex.org/W3024171804', 'https://openalex.org/W3162133897', 'https://openalex.org/W2970120757', 'https://openalex.org/W3096587983', 'https://openalex.org/W3016181583', 'https://openalex.org/W2292087804', 'https://openalex.org/W3093346109']",2021-06-10
https://openalex.org/W3137720654,https://doi.org/10.48550/arxiv.2103.08207,XLST: Cross-lingual Self-training to Learn Multilingual Representation for Low Resource Speech Recognition,"In this paper, we propose a weakly supervised multilingual representation learning framework, called cross-lingual self-training (XLST). XLST is able to utilize a small amount of annotated data from high-resource languages to improve the representation learning on multilingual un-annotated data. Specifically, XLST uses a supervised trained model to produce initial representations and another model to learn from them, by maximizing the similarity between output embeddings of these two models. Furthermore, the moving average mechanism and multi-view data augmentation are employed, which are experimentally shown to be crucial to XLST. Comprehensive experiments have been conducted on the CommonVoice corpus to evaluate the effectiveness of XLST. Results on 5 downstream low-resource ASR tasks shows that our multilingual pretrained model achieves relatively 18.6% PER reduction over the state-of-the-art self-supervised method, with leveraging additional 100 hours of annotated English data.","['https://openalex.org/W1970890968', 'https://openalex.org/W2127141656', 'https://openalex.org/W2894835365', 'https://openalex.org/W1993660824', 'https://openalex.org/W2025198378', 'https://openalex.org/W3041561163', 'https://openalex.org/W2106440210', 'https://openalex.org/W2091746061', 'https://openalex.org/W3099782249', 'https://openalex.org/W3037057938', 'https://openalex.org/W3125709657', 'https://openalex.org/W3107668149', 'https://openalex.org/W2982223350', 'https://openalex.org/W3095311338', 'https://openalex.org/W2891816510', 'https://openalex.org/W2131042651', 'https://openalex.org/W1994606281', 'https://openalex.org/W3034978746', 'https://openalex.org/W2941814890', 'https://openalex.org/W2402040300', 'https://openalex.org/W1796128977', 'https://openalex.org/W3121299949', 'https://openalex.org/W1494198834', 'https://openalex.org/W2996383576', 'https://openalex.org/W2972943112', 'https://openalex.org/W3026041220', 'https://openalex.org/W2921087533', 'https://openalex.org/W2963292011', 'https://openalex.org/W2079508481', 'https://openalex.org/W2127982613', 'https://openalex.org/W2936774411', 'https://openalex.org/W3112034174', 'https://openalex.org/W3016181583', 'https://openalex.org/W2407897255', 'https://openalex.org/W3030437843', 'https://openalex.org/W3015522062', 'https://openalex.org/W2972347614', 'https://openalex.org/W3101821705', 'https://openalex.org/W2842511635', 'https://openalex.org/W2973049979', 'https://openalex.org/W2765407302', 'https://openalex.org/W2198724430', 'https://openalex.org/W2981857663']",2021-03-15
https://openalex.org/W3207431783,,Speech Toxicity Analysis: A New Spoken Language Processing Task.,"Toxic speech, also known as hate speech, is regarded as one of the crucial issues plaguing online social media today. Most recent work on toxic speech detection is constrained to the modality of text with no existing work on toxicity detection from spoken utterances. In this paper, we propose a new Spoken Language Processing task of detecting toxicity from spoken speech. We introduce DeToxy, the first publicly available toxicity annotated dataset for English speech, sourced from various openly available speech databases, consisting of over 2 million utterances. Finally, we also provide analysis on how a spoken speech corpus annotated for toxicity can help facilitate the development of E2E models which better capture various prosodic cues in speech, thereby boosting toxicity classification on spoken utterances.","['https://openalex.org/W2982223350', 'https://openalex.org/W2968228919', 'https://openalex.org/W2995929068', 'https://openalex.org/W3112034174', 'https://openalex.org/W3028178035', 'https://openalex.org/W3015489952', 'https://openalex.org/W2342475039', 'https://openalex.org/W2959546144', 'https://openalex.org/W3198771897', 'https://openalex.org/W3170128710', 'https://openalex.org/W2892071465', 'https://openalex.org/W2953285128', 'https://openalex.org/W3099782249', 'https://openalex.org/W3118179615', 'https://openalex.org/W2895615590', 'https://openalex.org/W3015522062', 'https://openalex.org/W2595653137', 'https://openalex.org/W2785615365', 'https://openalex.org/W2742542661', 'https://openalex.org/W2949678053']",2021-10-14
https://openalex.org/W4313060917,https://doi.org/10.33103/uot.ijccce.21.4.8,Speaker Recognition System Based on Mel Frequency Cepstral Coefficient and Four Features,"Biometrics signs are the most important factor in the human recognition field and considered an effective technique for person authentication systems. Voice recognition is a popular method to use due to its ease of implementation and acceptable effectiveness. This research paper will introduce a speaker recognition system that consists of preprocessing techniques to eliminate noise and make the sound smoother. For the feature extraction stage, the method Mel Frequency Cepstral Coefficient (MFCC) is used, and in the second step, the four features (FF) Mean, Standard Division, Zero-Cross and Amplitude, which added to (MFCC) to improve the results. For data representation, vector quantization has been used. The evaluation method (k-fold cross-validation) has been used. Supervised machine learning (SML) is proposed using Quadratic Discriminant Analysis (QDA) classification algorithms. And the results obtained by the algorithm (QDA) varied between 98 percent and 98.43 percent, depending on the way of features extraction that was used. These results are satisfactory and reliable. Index Terms— SML, QDA, Voice Recognition, MFCC, FF.","['https://openalex.org/W3163334226', 'https://openalex.org/W3192420832', 'https://openalex.org/W3179880923', 'https://openalex.org/W6785197332', 'https://openalex.org/W2790428568', 'https://openalex.org/W2795207247', 'https://openalex.org/W2971926373', 'https://openalex.org/W3000388457', 'https://openalex.org/W3090835550', 'https://openalex.org/W6792042903', 'https://openalex.org/W3217760019', 'https://openalex.org/W2987791633', 'https://openalex.org/W3018137080', 'https://openalex.org/W3112034174', 'https://openalex.org/W3174146670', 'https://openalex.org/W3139283495', 'https://openalex.org/W4287758791', 'https://openalex.org/W4255890889', 'https://openalex.org/W3100732527']",2021-12-30
https://openalex.org/W4403094482,https://doi.org/10.1109/access.2024.3473743,Joint Speech-Text Embeddings for Multitask Speech Processing,"Devices that use speech as the communication medium between human and computer have been emerging for the past few years. The technologies behind this interface are called Automatic Speech Recognition (ASR) and Text-to-Speech (TTS). The two are distinct fields in speech signal processing that have independently made great strides in recent years. This paper proposes an architecture that takes advantage of the two modalities present in ASR and TTS, speech and text, while simultaneously training three tasks, adding speaker recognition to the underlying ASR and TTS tasks. This architecture not only reduces the memory footprint required to run all tasks, but also has performance comparable to single-task models. The dataset used to train and evaluate the model is the CSTR VCTK Corpus. Results show a 97.64% accuracy in the speaker recognition task, word and character error rates of 18.18% and 7.95% for the ASR task, a mel cepstral distortion of 4.31 and two predicted MOS of 2.98 and 3.28 for the TTS task. While voice conversion is not part of the training tasks, the architecture is capable of doing this and was evaluated to have 5.22, 2.98, and 2.73 for mel cepstral distortion and predicted MOS, respectively.","['https://openalex.org/W4319828443', 'https://openalex.org/W4312192581', 'https://openalex.org/W2973049979', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W6847363464', 'https://openalex.org/W2903739847', 'https://openalex.org/W6778823374', 'https://openalex.org/W3150572638', 'https://openalex.org/W3015282541', 'https://openalex.org/W6777694618', 'https://openalex.org/W3015338123', 'https://openalex.org/W6783867762', 'https://openalex.org/W3144035034', 'https://openalex.org/W4319585868', 'https://openalex.org/W3009565979', 'https://openalex.org/W2938947737', 'https://openalex.org/W6762242920', 'https://openalex.org/W4297841232', 'https://openalex.org/W4319862265', 'https://openalex.org/W6751433836', 'https://openalex.org/W3096589040', 'https://openalex.org/W6803675045', 'https://openalex.org/W3196833881', 'https://openalex.org/W6789693907', 'https://openalex.org/W6803092890', 'https://openalex.org/W2896457183', 'https://openalex.org/W4226033575', 'https://openalex.org/W4226120743', 'https://openalex.org/W4375869005', 'https://openalex.org/W4382935139', 'https://openalex.org/W2206072079', 'https://openalex.org/W2963739817', 'https://openalex.org/W4392904805', 'https://openalex.org/W3097777922', 'https://openalex.org/W3196001064', 'https://openalex.org/W6763832098', 'https://openalex.org/W4221154745', 'https://openalex.org/W2784163702', 'https://openalex.org/W2964243274', 'https://openalex.org/W3097206152', 'https://openalex.org/W3202278141', 'https://openalex.org/W6772349387', 'https://openalex.org/W2962780374', 'https://openalex.org/W6810007534', 'https://openalex.org/W2967479456', 'https://openalex.org/W2963386851', 'https://openalex.org/W4384695309', 'https://openalex.org/W6771876938']",2024-01-01
https://openalex.org/W4409294828,https://doi.org/10.1002/cpe.70076,Multiuser Hierarchical Authorization Using Sparsity Polarization Pruning for Model Active Protection,"ABSTRACT Currently, artificial intelligence technology is rapidly penetrating into various fields of socioeconomic development with increasing depth and breadth, becoming an important force driving innovation and development, empowering thousands of industries, while also bringing challenges such as security governance. The application of deep neural network models must implement hierarchical access based on user permissions to prevent unauthorized users from accessing and abusing the model, and to prevent malicious attackers from tampering or damaging the model, thereby reducing its vulnerabilities and security risks. To address this issue, the model provider must implement a hierarchical authorization policy for the model, which can grant users access to the model based on their specific needs, while ensuring that unauthorized users cannot use the model. Common methods for implementing hierarchical authorization of models include pruning and encryption, but existing technologies require high computational complexity and have unclear hierarchical effects. In this article, we propose a sparsity polarization pruning approach for layered authorization, which combines sparsity regularization to filter insignificant channels and a polarization technique to cluster critical channels into distinct intervals. By pruning channels based on polarized scaling factors from the batch normalization (BN) layer, our method dynamically adjusts model precision to match user authorization levels. Initially, we extract the scaling factor of the BN layer to assess the importance of each channel. A sparsity regularizer is then applied to filter out irrelevant scaling factors. To enhance the clarity and rationality of pruning intervals, we use a polarization technique to induce clustering of scaling factors. So we proposed multiuser hierarchical authorization using sparsity polarization pruning for model active protection. Based on the grading requirements, we prune channels corresponding to varying numbers of significant scaling factors. Access is granted at different levels depending on the precision key provided by the user, thereby ensuring a secure and efficient means of accessing the model's resources. Experimental results demonstrate that our approach achieves superior grading performance across three datasets and two different neural networks, showcasing its broad applicability. Moreover, our method achieves effective grading just by pruning a small portion of the channels, offering a high level of efficiency.","['https://openalex.org/W4400810737', 'https://openalex.org/W3198713693', 'https://openalex.org/W4388889456', 'https://openalex.org/W4400887755', 'https://openalex.org/W4372319207', 'https://openalex.org/W4392903956', 'https://openalex.org/W4392904805', 'https://openalex.org/W4206963665', 'https://openalex.org/W4362567371', 'https://openalex.org/W3186687747', 'https://openalex.org/W4292993844', 'https://openalex.org/W4200629745', 'https://openalex.org/W4319317939', 'https://openalex.org/W4391913151', 'https://openalex.org/W4400850895', 'https://openalex.org/W4403818393', 'https://openalex.org/W3174300150', 'https://openalex.org/W4402904225', 'https://openalex.org/W2962851801', 'https://openalex.org/W3104591140', 'https://openalex.org/W6783948878', 'https://openalex.org/W3204185301', 'https://openalex.org/W3169769781', 'https://openalex.org/W3214726878', 'https://openalex.org/W3128642798', 'https://openalex.org/W3209100754', 'https://openalex.org/W6803377907', 'https://openalex.org/W3128183952', 'https://openalex.org/W4402265834', 'https://openalex.org/W4396642831', 'https://openalex.org/W4400438454', 'https://openalex.org/W4392203282', 'https://openalex.org/W4387423731', 'https://openalex.org/W3187004776', 'https://openalex.org/W4401154288', 'https://openalex.org/W4394610786', 'https://openalex.org/W4393231758', 'https://openalex.org/W4396827130', 'https://openalex.org/W3211901356', 'https://openalex.org/W3212023986', 'https://openalex.org/W4287065492']",2025-04-09
https://openalex.org/W4376481237,https://doi.org/10.1109/tmm.2023.3275873,VatLM: Visual-Audio-Text Pre-Training With Unified Masked Prediction for Speech Representation Learning,"Although speech is a simple and effective way for humans to communicate with the outside world, a more realistic speech interaction contains multimodal information, e.g., vision, text. How to design a unified framework to integrate different modal information and leverage different resources (e.g., visual-audio pairs, audio-text pairs, unlabeled speech, and unlabeled text) to facilitate speech representation learning was not well explored. In this paper, we propose a unified cross-modal representation learning framework <sc xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">VatLM</small> (Visual-Audio-Text Language Model). The proposed <sc xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">VatLM</small> employs a unified backbone network to model the modality-independent information and utilizes three simple modality-dependent modules to preprocess visual, speech, and text inputs. In order to integrate these three modalities into one shared semantic space, <sc xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">VatLM</small> is optimized with a masked prediction task of unified tokens, given by our proposed unified tokenizer. We evaluate the pre-trained <sc xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">VatLM</small> on audio-visual related downstream tasks, including audio-visual speech recognition (AVSR), and visual speech recognition (VSR) tasks. Results show that the proposed <sc xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">VatLM</small> outperforms previous state-of-the-art models, such as the audio-visual pre-trained AV-HuBERT model, and analysis also demonstrates that <sc xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">VatLM</small> is capable of aligning different modalities into the same space. To facilitate future research, we release the code and pre-trained models at <uri xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">https://aka.ms/vatlm</uri> .","['https://openalex.org/W2121486117', 'https://openalex.org/W2032686202', 'https://openalex.org/W3154807520', 'https://openalex.org/W3205193540', 'https://openalex.org/W4281492411', 'https://openalex.org/W2973049979', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W6810007534', 'https://openalex.org/W6844194202', 'https://openalex.org/W3041561163', 'https://openalex.org/W2972943112', 'https://openalex.org/W4385245566', 'https://openalex.org/W4226033575', 'https://openalex.org/W4313156423', 'https://openalex.org/W3035042697', 'https://openalex.org/W3006974783', 'https://openalex.org/W6810259195', 'https://openalex.org/W4392979802', 'https://openalex.org/W2014621385', 'https://openalex.org/W2963654155', 'https://openalex.org/W2890952074', 'https://openalex.org/W3162293946', 'https://openalex.org/W3007589762', 'https://openalex.org/W2134867751', 'https://openalex.org/W6803092890', 'https://openalex.org/W3205644108', 'https://openalex.org/W6810168380', 'https://openalex.org/W4221153521', 'https://openalex.org/W4308236834', 'https://openalex.org/W4297841641', 'https://openalex.org/W4223622550', 'https://openalex.org/W3209371554', 'https://openalex.org/W4386071687', 'https://openalex.org/W6797307915', 'https://openalex.org/W4382202943', 'https://openalex.org/W3197580070', 'https://openalex.org/W3015830103', 'https://openalex.org/W3197567540', 'https://openalex.org/W3205715971', 'https://openalex.org/W4312638101', 'https://openalex.org/W4224319127', 'https://openalex.org/W4307286264', 'https://openalex.org/W3148001440', 'https://openalex.org/W3016010032', 'https://openalex.org/W4226120743', 'https://openalex.org/W6793736971', 'https://openalex.org/W6677618333', 'https://openalex.org/W3016011581', 'https://openalex.org/W4385573012', 'https://openalex.org/W6763832098', 'https://openalex.org/W6754420807', 'https://openalex.org/W2808631503', 'https://openalex.org/W3198694222', 'https://openalex.org/W2799473636', 'https://openalex.org/W1917432393', 'https://openalex.org/W6688816777', 'https://openalex.org/W2981501041', 'https://openalex.org/W2972756321', 'https://openalex.org/W2933138175', 'https://openalex.org/W2963250244', 'https://openalex.org/W2996970093', 'https://openalex.org/W3167917117', 'https://openalex.org/W3015383493', 'https://openalex.org/W3101648800', 'https://openalex.org/W3207222250', 'https://openalex.org/W2891205112', 'https://openalex.org/W4221153068', 'https://openalex.org/W4297808394', 'https://openalex.org/W3173978205', 'https://openalex.org/W2219249508', 'https://openalex.org/W2115252128', 'https://openalex.org/W4221155340']",2023-05-12
https://openalex.org/W4392903872,https://doi.org/10.1109/icassp48485.2024.10447553,SALM: Speech-Augmented Language Model with in-Context Learning for Speech Recognition and Translation,"We present a novel Speech Augmented Language Model (SALM) with multitask and in-context learning capabilities. SALM comprises a frozen text LLM, a audio encoder, a modality adapter module, and LoRA layers to accommodate speech input and associated task instructions. The unified SALM not only achieves performance on par with task-specific Conformer baselines for Automatic Speech Recognition (ASR) and Speech Translation (AST), but also exhibits zero-shot in-context learning capabilities, demonstrated through keyword-boosting task for ASR and AST. Moreover, speech supervised in-context training is proposed to bridge the gap between LLM training and downstream speech tasks, which further boosts the in-context learning ability of speech-to-text models. Proposed model is open-sourced via NeMo toolkit <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .","['https://openalex.org/W4393178509', 'https://openalex.org/W6852326057', 'https://openalex.org/W4372269772', 'https://openalex.org/W4385822949', 'https://openalex.org/W4389524500', 'https://openalex.org/W6853998256', 'https://openalex.org/W6853249747', 'https://openalex.org/W4392910583', 'https://openalex.org/W4391021666', 'https://openalex.org/W6855414158', 'https://openalex.org/W6767997687', 'https://openalex.org/W6767671539', 'https://openalex.org/W4385822953', 'https://openalex.org/W6848735303', 'https://openalex.org/W4252812408', 'https://openalex.org/W4391021773', 'https://openalex.org/W2889012072', 'https://openalex.org/W2886319145', 'https://openalex.org/W4225308107', 'https://openalex.org/W4226120743', 'https://openalex.org/W4392979802', 'https://openalex.org/W4391021542', 'https://openalex.org/W3097777922', 'https://openalex.org/W6796581206', 'https://openalex.org/W6800875267', 'https://openalex.org/W6846665566', 'https://openalex.org/W6761551260', 'https://openalex.org/W1494198834', 'https://openalex.org/W4385570170', 'https://openalex.org/W3092085609', 'https://openalex.org/W3005302685', 'https://openalex.org/W6857054612', 'https://openalex.org/W4387595589', 'https://openalex.org/W2963499882', 'https://openalex.org/W4387891768', 'https://openalex.org/W4375958083', 'https://openalex.org/W2938704169', 'https://openalex.org/W4392903956', 'https://openalex.org/W4381827575', 'https://openalex.org/W2974231335', 'https://openalex.org/W4313679638', 'https://openalex.org/W3168867926', 'https://openalex.org/W4310428868', 'https://openalex.org/W2973727699', 'https://openalex.org/W4377130946']",2024-03-18
https://openalex.org/W4392248135,https://doi.org/10.1109/aixvr59861.2024.00060,Typing on Any Surface: Real-Time Keystroke Detection in Augmented Reality,"The ineffectiveness of text entry interfaces remains a significant barrier to social engagement in augmented reality (AR). Popular options, such as mid-air keyboard interface, wireless keyboards or voice input face challenges such as ergonomic issues, limited accuracy, or social discomfort in public use. This paper introduces a deep-learning method allowing AR applications to predict keystrokes from the user perspective video stream captured by any AR headset. This enables users to type on flat surfaces without a physical or virtual keyboard. Our two-stage model combines an off-the-shelf hand landmark extractor with an innovative adaptive Convolutional Recurrent Neural Network (C-RNN). It was trained on a newly built dataset, enabling prediction of 27 keys (alphabet and space) at approximately 32 FPS. With practice, users can reach a 91.0% accuracy at 40 words per minute, comparable to typing on a physical keyboard. The encouraging results demonstrate our method's feasibility and potential for integration in diverse applications. We also explore limitations and future research directions for production system implementation.","['https://openalex.org/W6847363464', 'https://openalex.org/W4244024001', 'https://openalex.org/W2579504323', 'https://openalex.org/W4225089689', 'https://openalex.org/W3118105922', 'https://openalex.org/W2345101311', 'https://openalex.org/W3003221865', 'https://openalex.org/W4392979802', 'https://openalex.org/W3111120281', 'https://openalex.org/W3206237932', 'https://openalex.org/W2107655853', 'https://openalex.org/W77473773', 'https://openalex.org/W2903936639', 'https://openalex.org/W2792558628', 'https://openalex.org/W6679436768', 'https://openalex.org/W2962824709', 'https://openalex.org/W2951390634', 'https://openalex.org/W2064675550', 'https://openalex.org/W2157331557', 'https://openalex.org/W6749825310', 'https://openalex.org/W4385245566', 'https://openalex.org/W2792764867', 'https://openalex.org/W4311000453', 'https://openalex.org/W2559655401']",2024-01-17
https://openalex.org/W4394862800,https://doi.org/10.1109/taslp.2024.3389630,Conversational Speech Recognition by Learning Audio-Textual Cross-Modal Contextual Representation,"Automatic Speech Recognition (ASR) in conversational settings presents unique challenges, including extracting relevant contextual information from previous conversational turns. Due to irrelevant content, error propagation, and redundancy, existing methods struggle to extract longer and more effective contexts. To address this issue, we introduce a novel conversational ASR system, extending the Conformer encoder-decoder model with cross-modal conversational representation. Our approach leverages a cross-modal extractor that combines pre-trained speech and text models through a specialized encoder and a modal-level mask input. This enables the extraction of richer historical speech context without explicit error propagation. We also incorporate conditional latent variational modules to learn conversational-level attributes such as role preference and topic coherence. By introducing both cross-modal and conversational representations into the decoder, our model retains longer context without information loss, achieving relative accuracy improvements of 8.8% and 23% on Mandarin conversation datasets HKUST and MagicData-RAMC, respectively, compared to the standard Conformer model.","['https://openalex.org/W2160815625', 'https://openalex.org/W3211278025', 'https://openalex.org/W3173680274', 'https://openalex.org/W2759071281', 'https://openalex.org/W2972430654', 'https://openalex.org/W108866686', 'https://openalex.org/W2144499799', 'https://openalex.org/W4385245566', 'https://openalex.org/W3097777922', 'https://openalex.org/W2526425061', 'https://openalex.org/W1999965501', 'https://openalex.org/W179875071', 'https://openalex.org/W4221144554', 'https://openalex.org/W2886025712', 'https://openalex.org/W3096798607', 'https://openalex.org/W3197898596', 'https://openalex.org/W4372260611', 'https://openalex.org/W4372260432', 'https://openalex.org/W4297841248', 'https://openalex.org/W4385822425', 'https://openalex.org/W4283826357', 'https://openalex.org/W6810007534', 'https://openalex.org/W3209059054', 'https://openalex.org/W6766673545', 'https://openalex.org/W4283835659', 'https://openalex.org/W2892009249', 'https://openalex.org/W6746023985', 'https://openalex.org/W2964110616', 'https://openalex.org/W6731370813', 'https://openalex.org/W4372341362', 'https://openalex.org/W6803092890', 'https://openalex.org/W4392979802', 'https://openalex.org/W4385823039', 'https://openalex.org/W6780218876', 'https://openalex.org/W3197580070', 'https://openalex.org/W3203407300', 'https://openalex.org/W2952370363', 'https://openalex.org/W2896457183', 'https://openalex.org/W6846807982', 'https://openalex.org/W6847032292', 'https://openalex.org/W6797307915', 'https://openalex.org/W2963714898', 'https://openalex.org/W2964669873', 'https://openalex.org/W6687045409', 'https://openalex.org/W4297841547', 'https://openalex.org/W1526236009', 'https://openalex.org/W2933138175', 'https://openalex.org/W2936774411', 'https://openalex.org/W2962780374', 'https://openalex.org/W3163463193', 'https://openalex.org/W2891176389', 'https://openalex.org/W4311724836', 'https://openalex.org/W3173978205', 'https://openalex.org/W2965373594', 'https://openalex.org/W3207222250', 'https://openalex.org/W4221145109', 'https://openalex.org/W4320930577', 'https://openalex.org/W4310271076']",2024-01-01
https://openalex.org/W4405921783,https://doi.org/10.3390/electronics14010128,Multi-Head Attention-Enhanced Speech Recognition for Reduced Data Requirements,"Automatic speech recognition (ASR) technology has reached a mature level, and improving performance in data-scarce scenarios has become a key research focus. In this study, we propose a novel approach to building acoustic models by leveraging the multi-head attention mechanism to enhance various acoustic features extracted from audio data, combined with time-delay neural networks (TDNNs) using semi-orthogonal low-rank matrix factorization, resulting in significant performance improvements. To increase the robustness of the acoustic model, we introduce a small amount of data perturbation during the early stages of training. Additionally, during the decoding phase, we employ an external recurrent neural network language model (RNNLM) for rescoring, further enhancing the model’s accuracy. A comprehensive evaluation of the widely used LibriSpeech corpus shows that our method, using only half the training data, achieves word error rates (WERs) of 3.15 and 7.04 on the test_clean and test_other datasets, respectively. These results outperform models with similar architectures trained on the full dataset and demonstrate performance comparable to mainstream end-to-end models.","['https://openalex.org/W3100732527', 'https://openalex.org/W3211278025', 'https://openalex.org/W2188183693', 'https://openalex.org/W2514741789', 'https://openalex.org/W2143612262', 'https://openalex.org/W2293634267', 'https://openalex.org/W2973049979', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3204696009', 'https://openalex.org/W4391021666', 'https://openalex.org/W4392903956', 'https://openalex.org/W4387158707', 'https://openalex.org/W2144499799', 'https://openalex.org/W4283700324', 'https://openalex.org/W3097777922', 'https://openalex.org/W4391021736', 'https://openalex.org/W4382935270', 'https://openalex.org/W6838276489', 'https://openalex.org/W4226390724', 'https://openalex.org/W4392979802', 'https://openalex.org/W2962824709', 'https://openalex.org/W2169049489', 'https://openalex.org/W2001537489', 'https://openalex.org/W3146366485', 'https://openalex.org/W3139024197', 'https://openalex.org/W2888867175', 'https://openalex.org/W1494198834', 'https://openalex.org/W2888954148', 'https://openalex.org/W2107223151', 'https://openalex.org/W3198522318', 'https://openalex.org/W3162665866', 'https://openalex.org/W3207629995', 'https://openalex.org/W3205201903', 'https://openalex.org/W4387799863', 'https://openalex.org/W4281779489', 'https://openalex.org/W3036601975']",2024-12-31
https://openalex.org/W4308480316,https://doi.org/10.1109/jstsp.2022.3205434,Editorial Editorial of Special Issue on Self-Supervised Learning for Speech and Audio Processing,"The papers in this special section focus on self-supervised learning for speech and audio processing. A current trend in the machine learning community is the adoption of self-supervised approaches to pretrain deep networks. Self-supervised learning utilizes proxy-supervised learning tasks (or pretext tasks)—for example, distinguishing parts of the input signal from distractors or reconstructing masked input segments conditioned on unmasked segments—to obtain training data from unlabeled corpora. These approaches make it possible to use the tremendous amount of unlabeled data available on the web to train large neural models. Recent self-supervised approaches for speech and audio processing are also gaining attention.","['https://openalex.org/W4281492411', 'https://openalex.org/W4295308567', 'https://openalex.org/W4285144981', 'https://openalex.org/W4293793697', 'https://openalex.org/W4292976050', 'https://openalex.org/W4289792473', 'https://openalex.org/W4290995108', 'https://openalex.org/W4290712827', 'https://openalex.org/W4292968575', 'https://openalex.org/W4285069952', 'https://openalex.org/W4285412695', 'https://openalex.org/W4221160587', 'https://openalex.org/W4285258106', 'https://openalex.org/W4221141236', 'https://openalex.org/W4285821318', 'https://openalex.org/W4294068504', 'https://openalex.org/W4221161753', 'https://openalex.org/W4293370787', 'https://openalex.org/W4221140961', 'https://openalex.org/W4289824098', 'https://openalex.org/W4289821214', 'https://openalex.org/W4221152601', 'https://openalex.org/W4281663607', 'https://openalex.org/W4285225028', 'https://openalex.org/W4280619727', 'https://openalex.org/W3209984917', 'https://openalex.org/W3204696009', 'https://openalex.org/W3157923770', 'https://openalex.org/W3144173820', 'https://openalex.org/W3160397447', 'https://openalex.org/W3136810184', 'https://openalex.org/W3159481202', 'https://openalex.org/W3134652006', 'https://openalex.org/W4402490925', 'https://openalex.org/W3213029956', 'https://openalex.org/W6779941907', 'https://openalex.org/W4285192675', 'https://openalex.org/W4292825791', 'https://openalex.org/W4285110637', 'https://openalex.org/W4287889585', 'https://openalex.org/W3197580070', 'https://openalex.org/W4292969786', 'https://openalex.org/W3198771897', 'https://openalex.org/W4286359908', 'https://openalex.org/W4285414370', 'https://openalex.org/W3039695075']",2022-10-01
https://openalex.org/W4406132022,https://doi.org/10.1111/desc.13606,Simulating Early Phonetic and Word Learning Without Linguistic Categories,"ABSTRACT Before they even talk, infants become sensitive to the speech sounds of their native language and recognize the auditory form of an increasing number of words. Traditionally, these early perceptual changes are attributed to an emerging knowledge of linguistic categories such as phonemes or words. However, there is growing skepticism surrounding this interpretation due to limited evidence of category knowledge in infants. Previous modeling work has shown that a distributional learning algorithm could reproduce perceptual changes in infants' early phonetic learning without acquiring phonetic categories. Taking this inquiry further, we propose that linguistic categories may not be needed for early word learning. We introduce STELA, a predictive coding algorithm designed to extract statistical patterns from continuous raw speech data. Our findings demonstrate that STELA can reproduce some developmental patterns of phonetic and word form learning without relying on linguistic categories such as phonemes or words nor requiring explicit word segmentation. Through an analysis of the learned representations, we show evidence that linguistic categories may emerge as an end product of learning rather than being prerequisites during early language acquisition.","['https://openalex.org/W2529194139', 'https://openalex.org/W4283453659', 'https://openalex.org/W4296710617', 'https://openalex.org/W2099164611', 'https://openalex.org/W2995929068', 'https://openalex.org/W3193068792', 'https://openalex.org/W3202070718', 'https://openalex.org/W2063303346', 'https://openalex.org/W122673323', 'https://openalex.org/W3130041921', 'https://openalex.org/W4229781645', 'https://openalex.org/W6811685372', 'https://openalex.org/W2032442824', 'https://openalex.org/W4221038855', 'https://openalex.org/W4225815933', 'https://openalex.org/W1532704504', 'https://openalex.org/W4256675988', 'https://openalex.org/W4225726571', 'https://openalex.org/W4295308567', 'https://openalex.org/W2483390977', 'https://openalex.org/W2037662195', 'https://openalex.org/W2110485445', 'https://openalex.org/W6676391964', 'https://openalex.org/W2107038463', 'https://openalex.org/W3199093330', 'https://openalex.org/W6800775014', 'https://openalex.org/W2396361046', 'https://openalex.org/W2148764920', 'https://openalex.org/W6682122378', 'https://openalex.org/W4401117643', 'https://openalex.org/W2126377586', 'https://openalex.org/W2160727679', 'https://openalex.org/W2037525070', 'https://openalex.org/W2991557631', 'https://openalex.org/W1995422333', 'https://openalex.org/W1579929927', 'https://openalex.org/W3140196993', 'https://openalex.org/W1989914796', 'https://openalex.org/W2149784807', 'https://openalex.org/W2038056950', 'https://openalex.org/W2037752261', 'https://openalex.org/W4206039057', 'https://openalex.org/W1597121597', 'https://openalex.org/W6635894473', 'https://openalex.org/W2014307400', 'https://openalex.org/W6653853028', 'https://openalex.org/W2103091632', 'https://openalex.org/W1984717236', 'https://openalex.org/W7037316554', 'https://openalex.org/W2063525438', 'https://openalex.org/W3129009457', 'https://openalex.org/W4366994024', 'https://openalex.org/W4391669171', 'https://openalex.org/W4379474370', 'https://openalex.org/W4385822936', 'https://openalex.org/W2741692265', 'https://openalex.org/W4232693094', 'https://openalex.org/W6651343493', 'https://openalex.org/W2068247585', 'https://openalex.org/W6667887468', 'https://openalex.org/W2026992087', 'https://openalex.org/W2251025892', 'https://openalex.org/W2074488330', 'https://openalex.org/W2136653392', 'https://openalex.org/W4313255477', 'https://openalex.org/W6808618455', 'https://openalex.org/W2776941264', 'https://openalex.org/W2167834252', 'https://openalex.org/W3110458199', 'https://openalex.org/W4221140961', 'https://openalex.org/W4292825791', 'https://openalex.org/W2842511635', 'https://openalex.org/W4297808394', 'https://openalex.org/W2113332115', 'https://openalex.org/W2078828996', 'https://openalex.org/W6670256387', 'https://openalex.org/W4395036961', 'https://openalex.org/W6631362777', 'https://openalex.org/W2119885245', 'https://openalex.org/W3005511757', 'https://openalex.org/W3016181583', 'https://openalex.org/W1993984071', 'https://openalex.org/W1980862600', 'https://openalex.org/W6645640999', 'https://openalex.org/W2029008609', 'https://openalex.org/W3125087428', 'https://openalex.org/W2395899413', 'https://openalex.org/W1944583518', 'https://openalex.org/W6640915429', 'https://openalex.org/W2163097816', 'https://openalex.org/W2119165475', 'https://openalex.org/W6632622110', 'https://openalex.org/W2108582985', 'https://openalex.org/W2003341094', 'https://openalex.org/W1925965306', 'https://openalex.org/W6640227979', 'https://openalex.org/W4391428296', 'https://openalex.org/W2101509422', 'https://openalex.org/W3146245645', 'https://openalex.org/W4235199064', 'https://openalex.org/W1974052916', 'https://openalex.org/W2082256905', 'https://openalex.org/W593365102', 'https://openalex.org/W4394671563', 'https://openalex.org/W2468383091']",2025-01-06
https://openalex.org/W4408355354,https://doi.org/10.1109/icassp49660.2025.10890379,Speech Enhancement Using Continuous Embeddings of Neural Audio Codec,"Recent advancements in Neural Audio Codec (NAC) models have inspired their use in various speech processing tasks, including speech enhancement (SE). In this work, we propose a novel, efficient SE approach by leveraging the pre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE methods, which process discrete speech tokens using Language Models (LMs), we perform SE within the continuous embedding space of the pretrained NAC, which is highly compressed along the time dimension for efficient representation. Our lightweight SE model, optimized through an embedding-level loss, delivers results comparable to SE baselines trained on larger datasets, with a significantly lower real-time factor of 0.005. Additionally, our method achieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer in a simulated cloud-based audio transmission environment. This work highlights a new, efficient NAC-based SE solution, particularly suitable for cloud applications where NAC is used to compress audio before transmission. Copyright 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.","['https://openalex.org/W2963045393', 'https://openalex.org/W2962843322', 'https://openalex.org/W3206809722', 'https://openalex.org/W3129077738', 'https://openalex.org/W2900381824', 'https://openalex.org/W2774389566', 'https://openalex.org/W2972436155', 'https://openalex.org/W3011982609', 'https://openalex.org/W3131332223', 'https://openalex.org/W4402111962', 'https://openalex.org/W4221144097', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W6853515095', 'https://openalex.org/W6848735303', 'https://openalex.org/W4402112643', 'https://openalex.org/W4400111385', 'https://openalex.org/W6856434366', 'https://openalex.org/W4402111399', 'https://openalex.org/W4402115964', 'https://openalex.org/W6779577414', 'https://openalex.org/W4392909571', 'https://openalex.org/W4225905067', 'https://openalex.org/W6936113694', 'https://openalex.org/W3097906045', 'https://openalex.org/W3161480375', 'https://openalex.org/W1552314771', 'https://openalex.org/W4281820413', 'https://openalex.org/W2890964092', 'https://openalex.org/W3163652268', 'https://openalex.org/W3167533889', 'https://openalex.org/W4296069339', 'https://openalex.org/W4297841790', 'https://openalex.org/W4384080510', 'https://openalex.org/W4406859288', 'https://openalex.org/W1556611829', 'https://openalex.org/W4313679638', 'https://openalex.org/W4387323811', 'https://openalex.org/W4380551955', 'https://openalex.org/W3034302232']",2025-03-12
https://openalex.org/W4406858747,https://doi.org/10.1109/apsipaasc63619.2025.10849285,Enhancing Neural Speech Embeddings for Generative Speech Models,"We explore a speech enhancement framework where neural speech embeddings, derived from pre-trained self-supervised learning (SSL) models applied to noisy signals, are used as inputs to a neural vocoder to generate the corresponding clean speech. The primary innovation lies in enhancing these latent neural embeddings to mitigate distortions caused by noise and reverberation, resulting in a superior quality of the synthesized signal. By dividing the process into Separate phases for embedding enhancement and speech generation, the approach allows for greater flexibility in network design. We also examine the advantage of integrating hidden states from the SSL model in a learnable manner to create a more robust embedding for the vocoder input. Additionally, we investigate various loss functions for training the neural vocoder. Experimental results confirm the effectiveness of our proposed approach, particularly in environments with simultaneous background noise and reverberation.","['https://openalex.org/W6780218876', 'https://openalex.org/W3209984917', 'https://openalex.org/W3209059054', 'https://openalex.org/W3197580070', 'https://openalex.org/W4312095873', 'https://openalex.org/W4226403810', 'https://openalex.org/W4224933800', 'https://openalex.org/W6838843145', 'https://openalex.org/W6853937136', 'https://openalex.org/W3197273793', 'https://openalex.org/W4372266927', 'https://openalex.org/W6783867762', 'https://openalex.org/W4392909571', 'https://openalex.org/W4386764866', 'https://openalex.org/W3207860153', 'https://openalex.org/W3191448984', 'https://openalex.org/W2787894218', 'https://openalex.org/W4392944340', 'https://openalex.org/W3041561163', 'https://openalex.org/W3161480375', 'https://openalex.org/W1597121597', 'https://openalex.org/W2593116425', 'https://openalex.org/W6746278845', 'https://openalex.org/W4232282348', 'https://openalex.org/W2696967604', 'https://openalex.org/W3197042120', 'https://openalex.org/W1974387177', 'https://openalex.org/W2555915854', 'https://openalex.org/W6757817989', 'https://openalex.org/W3196475561', 'https://openalex.org/W38194800']",2024-12-03
https://openalex.org/W4409947375,https://doi.org/10.21203/rs.3.rs-6111294/v1,GD-Conformer: a Conformer-based gated dense encoder-decoder for monaural speech enhancement,"<title>Abstract</title> Speech enhancement improves speech quality by mitigating noise, dereverberation, and echo. Existing methods struggle with amplitude-phase compensation, capturing temporal-frequency features, and high complexity. To address these issues, a gated dense encoder-decoder architecture with a two-stage Conformer, abbreviated as GD-Conformer, is proposed. It integrates a gated dense module, a two-stage residual Conformer module, a mask decoder and a complex decoder. The gated dense module consists of two parts: a dilated dense convolution and a gated convolution, where the former captures both global and local dependencies features, while the latter refines these distinct features accordingly. The two-stage residual Conformer focuses on the time-frequency dependence of speech, it also reduces the computational complexity. The mask decoder and the complex decoder restore spectral resolution while preserving speech fidelity. The outcomes of experiments conducted on the public dataset VoiceBank+DEMAND and DNS Challenge 2020 demonstrate that, compared with those state-of-the-art methods, the proposed GD-Conformer achieves comparable performance in terms of denoising and generalization with fewer parameters and lower computation complexity.","['https://openalex.org/W4390604176', 'https://openalex.org/W4312313987', 'https://openalex.org/W2527611302', 'https://openalex.org/W4385872080', 'https://openalex.org/W2785898612', 'https://openalex.org/W3206131716', 'https://openalex.org/W3007348697', 'https://openalex.org/W4388469976', 'https://openalex.org/W4392909571', 'https://openalex.org/W4392903544', 'https://openalex.org/W4391486999', 'https://openalex.org/W4318833180', 'https://openalex.org/W3174609245', 'https://openalex.org/W4400409125', 'https://openalex.org/W4365420402', 'https://openalex.org/W4310059968', 'https://openalex.org/W2291877678', 'https://openalex.org/W2935693977', 'https://openalex.org/W4391582585', 'https://openalex.org/W2991361823', 'https://openalex.org/W4224934178', 'https://openalex.org/W3162493033', 'https://openalex.org/W4375947632', 'https://openalex.org/W4394611093', 'https://openalex.org/W4390224291', 'https://openalex.org/W3197729725', 'https://openalex.org/W4392904504', 'https://openalex.org/W3198968036', 'https://openalex.org/W4224917453', 'https://openalex.org/W4392904015', 'https://openalex.org/W3158779859', 'https://openalex.org/W4392909548', 'https://openalex.org/W3160973314', 'https://openalex.org/W4392902507', 'https://openalex.org/W3177067699', 'https://openalex.org/W4392902873', 'https://openalex.org/W4375869466', 'https://openalex.org/W4313174479', 'https://openalex.org/W3015679215', 'https://openalex.org/W3094938953', 'https://openalex.org/W3034763882', 'https://openalex.org/W4200309447', 'https://openalex.org/W4400525349', 'https://openalex.org/W4392909811', 'https://openalex.org/W4221144097', 'https://openalex.org/W3205770165', 'https://openalex.org/W4388107429', 'https://openalex.org/W3160324929', 'https://openalex.org/W4372266968', 'https://openalex.org/W3015791598', 'https://openalex.org/W3015372568', 'https://openalex.org/W3205004157', 'https://openalex.org/W3209472002', 'https://openalex.org/W4224919782', 'https://openalex.org/W4321608474']",2025-04-29
https://openalex.org/W4402259238,https://doi.org/10.1109/taslp.2024.3454964,Blind Identification of Binaural Room Impulse Responses From Smart Glasses,"Smart glasses are increasingly recognized as a key medium for augmented reality, offering a hands-free platform with integrated microphones and non-ear-occluding loudspeakers to seamlessly mix virtual sound sources into the real-world acoustic scene. To convincingly integrate virtual sound sources, the room acoustic rendering of the virtual sources must match the real-world acoustics. Information about a user's acoustic environment however is typically not available. This work uses a microphone array in a pair of smart glasses to blindly identify binaural room impulse responses (BRIRs) from a few seconds of speech in the real-world environment. The proposed method uses dereverberation and beamforming to generate a pseudo reference signal that is used by a multichannel Wiener filter to estimate room impulse responses which are then converted to BRIRs. The multichannel room impulse responses can be used to estimate room acoustic parameters which is shown to outperform baseline algorithms in the estimation of reverberation time and direct-To-reverberant energy ratio. Results from a listening experiment further indicate that the estimated BRIRs often reproduce the real-world room acoustics perceptually more convincingly than measured BRIRs from other rooms of similar size.","['https://openalex.org/W4229049812', 'https://openalex.org/W6636948892', 'https://openalex.org/W3119755526', 'https://openalex.org/W3213910403', 'https://openalex.org/W4401609602', 'https://openalex.org/W4312270126', 'https://openalex.org/W2046791564', 'https://openalex.org/W2110097273', 'https://openalex.org/W2122807474', 'https://openalex.org/W3115469537', 'https://openalex.org/W3113396598', 'https://openalex.org/W3178031393', 'https://openalex.org/W6852239215', 'https://openalex.org/W4385822504', 'https://openalex.org/W4372341113', 'https://openalex.org/W4386764631', 'https://openalex.org/W2410879554', 'https://openalex.org/W2900429685', 'https://openalex.org/W4382632381', 'https://openalex.org/W2324207125', 'https://openalex.org/W6792026763', 'https://openalex.org/W1983812858', 'https://openalex.org/W3083010206', 'https://openalex.org/W6629041830', 'https://openalex.org/W2530782473', 'https://openalex.org/W653761051', 'https://openalex.org/W2113638573', 'https://openalex.org/W2030535888', 'https://openalex.org/W2555915854', 'https://openalex.org/W2611709348', 'https://openalex.org/W3183773838', 'https://openalex.org/W4287510388', 'https://openalex.org/W4291142585', 'https://openalex.org/W2089586791', 'https://openalex.org/W2809642605', 'https://openalex.org/W1484839547', 'https://openalex.org/W4220707493', 'https://openalex.org/W6782501926', 'https://openalex.org/W3216728835', 'https://openalex.org/W4291464559', 'https://openalex.org/W4391218714', 'https://openalex.org/W2050551142', 'https://openalex.org/W2051345412', 'https://openalex.org/W6640103097', 'https://openalex.org/W6738854697', 'https://openalex.org/W7046848240', 'https://openalex.org/W1973667004', 'https://openalex.org/W7033776617', 'https://openalex.org/W4387250692', 'https://openalex.org/W6796327947', 'https://openalex.org/W7053199412', 'https://openalex.org/W1641915153']",2024-01-01
https://openalex.org/W4399775206,https://doi.org/10.1186/s13636-024-00352-8,MIRACLE—a microphone array impulse response dataset for acoustic learning,"Abstract This work introduces a large dataset comprising impulse responses of spatially distributed sources within a plane parallel to a planar microphone array. The dataset, named MIRACLE, encompasses 856,128 single-channel impulse responses and includes four different measurement scenarios. Three measurement scenarios were conducted under anechoic conditions. The fourth scenario includes an additional specular reflection from a reflective panel. The source positions were obtained by uniformly discretizing a rectangular source plane parallel to the microphone for each scenario. The dataset contains three scenarios with a spatial resolution of $$23\,\textrm{mm}$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mrow> <mml:mn>23</mml:mn> <mml:mspace/> <mml:mtext>mm</mml:mtext> </mml:mrow> </mml:math> at two different source-plane-to-array distances, as well as a scenario with a resolution of $$5\,\textrm{mm}$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mrow> <mml:mn>5</mml:mn> <mml:mspace/> <mml:mtext>mm</mml:mtext> </mml:mrow> </mml:math> for the shorter distance. In contrast to existing room impulse response datasets, the accuracy of the provided source location labels is assessed and additional metadata, such as the directivity of the loudspeaker used for excitation, is provided. The MIRACLE dataset can be used as a benchmark for data-driven modelling and interpolation methods as well as for various acoustic machine learning tasks, such as source separation, localization, and characterization. Two timely applications of the dataset are presented in this work: the generation of microphone array data for data-driven source localization and characterization tasks and data-driven model order reduction.","['https://openalex.org/W3103750408', 'https://openalex.org/W2086286498', 'https://openalex.org/W2989691494', 'https://openalex.org/W4320730519', 'https://openalex.org/W4372259768', 'https://openalex.org/W2147428175', 'https://openalex.org/W2900240603', 'https://openalex.org/W4281264304', 'https://openalex.org/W4386764631', 'https://openalex.org/W2982059757', 'https://openalex.org/W3197097128', 'https://openalex.org/W3212486120', 'https://openalex.org/W4225270933', 'https://openalex.org/W4221167652', 'https://openalex.org/W2803981468', 'https://openalex.org/W2900429685', 'https://openalex.org/W4280592948', 'https://openalex.org/W2028138594', 'https://openalex.org/W2568308529', 'https://openalex.org/W2972697221', 'https://openalex.org/W3162883221', 'https://openalex.org/W3047687894', 'https://openalex.org/W3199503812', 'https://openalex.org/W3111823819', 'https://openalex.org/W4282547902', 'https://openalex.org/W4308020544', 'https://openalex.org/W4362664992', 'https://openalex.org/W4385822950', 'https://openalex.org/W3043884016', 'https://openalex.org/W2763188033', 'https://openalex.org/W4224314935', 'https://openalex.org/W4283076784', 'https://openalex.org/W2898268964', 'https://openalex.org/W6927049668', 'https://openalex.org/W4393780318', 'https://openalex.org/W3213211078', 'https://openalex.org/W1996304098', 'https://openalex.org/W4367050859', 'https://openalex.org/W3217317857', 'https://openalex.org/W3209301195', 'https://openalex.org/W2885034036', 'https://openalex.org/W3176093030', 'https://openalex.org/W3117358809', 'https://openalex.org/W4312669081', 'https://openalex.org/W2899595090', 'https://openalex.org/W2144732021', 'https://openalex.org/W2901243971', 'https://openalex.org/W1968834101', 'https://openalex.org/W4206463025', 'https://openalex.org/W2410879554', 'https://openalex.org/W1989314204', 'https://openalex.org/W2136682440', 'https://openalex.org/W2058341666', 'https://openalex.org/W2226462370', 'https://openalex.org/W3133472919', 'https://openalex.org/W3142899349', 'https://openalex.org/W2759838493', 'https://openalex.org/W136129836', 'https://openalex.org/W2564778834', 'https://openalex.org/W2026759465', 'https://openalex.org/W1569827162', 'https://openalex.org/W3092469959', 'https://openalex.org/W196748274', 'https://openalex.org/W4220829897', 'https://openalex.org/W1594234351', 'https://openalex.org/W1973609478', 'https://openalex.org/W1996400044', 'https://openalex.org/W2075919272', 'https://openalex.org/W2118020555', 'https://openalex.org/W4387021735', 'https://openalex.org/W6892444216', 'https://openalex.org/W3189372451', 'https://openalex.org/W2050354342', 'https://openalex.org/W2168090960', 'https://openalex.org/W1551480811', 'https://openalex.org/W3164810036', 'https://openalex.org/W2470931065', 'https://openalex.org/W2789371860', 'https://openalex.org/W3175214549', 'https://openalex.org/W2117756735', 'https://openalex.org/W3103001801', 'https://openalex.org/W3104196160']",2024-06-18
https://openalex.org/W4411096143,https://doi.org/10.3390/electronics14122313,Research on a Neural Network-Based Method for Detecting the Concentration and Particle Size of Suspended Solids Based on Multi-Frequency Acoustic Information,"Suspended solids (SS) composed of micrometer-to-nanometer-scale particles, including silt and organic matter, significantly impact aquatic ecosystems through physicochemical interactions. Accurate monitoring of SS concentration and particle size is critical for environmental protection and pollution prevention. We constructed multiple datasets using received signals after propagation through different aqueous environments. Analysis of the performance of neural networks across different datasets revealed that high-frequency signals with rich spectra have high potential for detecting suspended solid information in complex aqueous environments. Our study explores the performance of two neural networks (Conv1dBGRU and TCN) in combination with channel attention mechanisms in classification tasks focused on the concentration of suspended solids and particle size. We also constructed neural networks for multi-task learning using both hard and soft parameter-sharing methods to simultaneously complete the classification tasks for concentration and particle size. The results show that multi-frequency acoustic signals in combination with neural networks can achieve simultaneous and accurate estimation of the concentration of suspended solids and particle size.","['https://openalex.org/W4281382653', 'https://openalex.org/W2001057425', 'https://openalex.org/W2979692898', 'https://openalex.org/W3014933076', 'https://openalex.org/W4317242137', 'https://openalex.org/W4313366917', 'https://openalex.org/W4403217771', 'https://openalex.org/W4398151337', 'https://openalex.org/W3119738858', 'https://openalex.org/W2986957258', 'https://openalex.org/W4390746199', 'https://openalex.org/W2037923889', 'https://openalex.org/W2019242070', 'https://openalex.org/W1973271736', 'https://openalex.org/W2032160695', 'https://openalex.org/W2062965122', 'https://openalex.org/W3039880962', 'https://openalex.org/W4281953789', 'https://openalex.org/W3125271087', 'https://openalex.org/W2913340405', 'https://openalex.org/W2809290718', 'https://openalex.org/W2963498646', 'https://openalex.org/W4392902778', 'https://openalex.org/W6861564866', 'https://openalex.org/W6838461096', 'https://openalex.org/W3215427016', 'https://openalex.org/W2752782242', 'https://openalex.org/W6754005058', 'https://openalex.org/W4410321483', 'https://openalex.org/W4403760075', 'https://openalex.org/W4282028729', 'https://openalex.org/W4391251745', 'https://openalex.org/W2950673314']",2025-06-06
https://openalex.org/W2995181338,https://doi.org/10.1109/icassp40776.2020.9052942,Libri-Light: A Benchmark for ASR with Limited or No Supervision,"We introduce a new collection of spoken English audio suitable for training\nspeech recognition systems under limited or no supervision. It is derived from\nopen-source audio books from the LibriVox project. It contains over 60K hours\nof audio, which is, to our knowledge, the largest freely-available corpus of\nspeech. The audio has been segmented using voice activity detection and is\ntagged with SNR, speaker ID and genre descriptions. Additionally, we provide\nbaseline systems and evaluation metrics working under three settings: (1) the\nzero resource/unsupervised setting (ABX), (2) the semi-supervised setting (PER,\nCER) and (3) the distant supervision setting (WER). Settings (2) and (3) use\nlimited textual resources (10 minutes to 10 hours) aligned with the speech.\nSetting (3) uses large amounts of unaligned text. They are evaluated on the\nstandard LibriSpeech dev and test sets for comparison with the supervised\nstate-of-the-art.\n","['https://openalex.org/W2963620343', 'https://openalex.org/W2940544976', 'https://openalex.org/W6629717138', 'https://openalex.org/W2937197076', 'https://openalex.org/W6712444837', 'https://openalex.org/W2592866267', 'https://openalex.org/W2953190524', 'https://openalex.org/W3005511757', 'https://openalex.org/W6751433836', 'https://openalex.org/W6770514103', 'https://openalex.org/W6756326128', 'https://openalex.org/W1970890968', 'https://openalex.org/W6656619859', 'https://openalex.org/W2127141656', 'https://openalex.org/W2973049979', 'https://openalex.org/W2842511635', 'https://openalex.org/W6775452034', 'https://openalex.org/W2346964103', 'https://openalex.org/W4234016251', 'https://openalex.org/W6747270024', 'https://openalex.org/W6679855610', 'https://openalex.org/W2944255943', 'https://openalex.org/W6748342566', 'https://openalex.org/W2972630480', 'https://openalex.org/W2963425185', 'https://openalex.org/W2161482971', 'https://openalex.org/W4288107125', 'https://openalex.org/W2979476256', 'https://openalex.org/W2963340922', 'https://openalex.org/W2899377381', 'https://openalex.org/W4297818305', 'https://openalex.org/W2134800885', 'https://openalex.org/W2593779438', 'https://openalex.org/W3103005696', 'https://openalex.org/W3016181583', 'https://openalex.org/W4297808394', 'https://openalex.org/W2988736778', 'https://openalex.org/W2804648901', 'https://openalex.org/W2025198378', 'https://openalex.org/W2161391345', 'https://openalex.org/W2787447541', 'https://openalex.org/W2926063217', 'https://openalex.org/W2794753807', 'https://openalex.org/W4300047444', 'https://openalex.org/W3015522062', 'https://openalex.org/W2395899413', 'https://openalex.org/W1494198834', 'https://openalex.org/W2996383576', 'https://openalex.org/W2973026522', 'https://openalex.org/W2781384251']",2020-04-09
https://openalex.org/W4379929801,https://doi.org/10.1007/s11633-022-1410-8,Large-scale Multi-modal Pre-trained Models: A Comprehensive Survey,"Abstract With the urgent demand for generalized deep models, many pre-trained big models are proposed, such as bidirectional encoder representations (BERT), vision transformer (ViT), generative pre-trained transformers (GPT), etc. Inspired by the success of these models in single domains (like computer vision and natural language processing), the multi-modal pre-trained big models have also drawn more and more attention in recent years. In this work, we give a comprehensive survey of these models and hope this paper could provide new insights and helps fresh researchers to track the most cutting-edge works. Specifically, we firstly introduce the background of multi-modal pre-training by reviewing the conventional deep learning, pre-training works in natural language process, computer vision, and speech. Then, we introduce the task definition, key challenges, and advantages of multi-modal pre-training models (MM-PTMs), and discuss the MM-PTMs with a focus on data, objectives, network architectures, and knowledge enhanced pre-training. After that, we introduce the downstream tasks used for the validation of large-scale MM-PTMs, including generative, classification, and regression tasks. We also give visualization and analysis of the model parameters and results on representative downstream tasks. Finally, we point out possible research directions for this topic that may benefit future works. In addition, we maintain a continuously updated paper list for large-scale pre-trained multi-modal big models: https://github.com/wangxiao5791509/MultiModal_BigModels_Survey .","['https://openalex.org/W2163605009', 'https://openalex.org/W2108598243', 'https://openalex.org/W2962835968', 'https://openalex.org/W2194775991', 'https://openalex.org/W2064675550', 'https://openalex.org/W6629028937', 'https://openalex.org/W2963341956', 'https://openalex.org/W3209274285', 'https://openalex.org/W6778883912', 'https://openalex.org/W2981852735', 'https://openalex.org/W2950813464', 'https://openalex.org/W3094502228', 'https://openalex.org/W3138516171', 'https://openalex.org/W3091588028', 'https://openalex.org/W3090449556', 'https://openalex.org/W3207750165', 'https://openalex.org/W3014611590', 'https://openalex.org/W3126337491', 'https://openalex.org/W3173978205', 'https://openalex.org/W3204762109', 'https://openalex.org/W4225832925', 'https://openalex.org/W4225683910', 'https://openalex.org/W3004304303', 'https://openalex.org/W4221141417', 'https://openalex.org/W6801962987', 'https://openalex.org/W4221141423', 'https://openalex.org/W4283815582', 'https://openalex.org/W6774952039', 'https://openalex.org/W3185341429', 'https://openalex.org/W3205235328', 'https://openalex.org/W3011574394', 'https://openalex.org/W3198659451', 'https://openalex.org/W4210352519', 'https://openalex.org/W4221167444', 'https://openalex.org/W4213019189', 'https://openalex.org/W4206706211', 'https://openalex.org/W6804036380', 'https://openalex.org/W6810263219', 'https://openalex.org/W4221165593', 'https://openalex.org/W3206487987', 'https://openalex.org/W3015851404', 'https://openalex.org/W4210473988', 'https://openalex.org/W6810882463', 'https://openalex.org/W3213351348', 'https://openalex.org/W6839710751', 'https://openalex.org/W2112796928', 'https://openalex.org/W2963446712', 'https://openalex.org/W3209721572', 'https://openalex.org/W2923014074', 'https://openalex.org/W3158631574', 'https://openalex.org/W2971871542', 'https://openalex.org/W6779879114', 'https://openalex.org/W3096609285', 'https://openalex.org/W3170841864', 'https://openalex.org/W3171125843', 'https://openalex.org/W6796761347', 'https://openalex.org/W3215434919', 'https://openalex.org/W2973049979', 'https://openalex.org/W2988736778', 'https://openalex.org/W3209059054', 'https://openalex.org/W4226033575', 'https://openalex.org/W6838888909', 'https://openalex.org/W3135367836', 'https://openalex.org/W2109586012', 'https://openalex.org/W2185175083', 'https://openalex.org/W68733909', 'https://openalex.org/W1889081078', 'https://openalex.org/W2277195237', 'https://openalex.org/W2560730294', 'https://openalex.org/W2808747415', 'https://openalex.org/W2886641317', 'https://openalex.org/W2963518342', 'https://openalex.org/W3001555892', 'https://openalex.org/W3176641147', 'https://openalex.org/W2963541336', 'https://openalex.org/W2984008963', 'https://openalex.org/W3204588463', 'https://openalex.org/W2250384498', 'https://openalex.org/W3209532394', 'https://openalex.org/W3198076979', 'https://openalex.org/W4221167912', 'https://openalex.org/W3155594712', 'https://openalex.org/W3202384916', 'https://openalex.org/W3156892778', 'https://openalex.org/W2962843773', 'https://openalex.org/W3176153963', 'https://openalex.org/W2963703197', 'https://openalex.org/W6791276965', 'https://openalex.org/W3197736584', 'https://openalex.org/W3095670406', 'https://openalex.org/W3133825286', 'https://openalex.org/W4221167911', 'https://openalex.org/W2737258237', 'https://openalex.org/W3173220247', 'https://openalex.org/W2998356391', 'https://openalex.org/W3035652667', 'https://openalex.org/W3193402170', 'https://openalex.org/W2970231061', 'https://openalex.org/W2962739339', 'https://openalex.org/W2945260553', 'https://openalex.org/W4206471589', 'https://openalex.org/W2807744099', 'https://openalex.org/W3034500398', 'https://openalex.org/W3173909648', 'https://openalex.org/W3165938948', 'https://openalex.org/W3176824248', 'https://openalex.org/W4224286930', 'https://openalex.org/W3207798279', 'https://openalex.org/W3212386989', 'https://openalex.org/W3184735396', 'https://openalex.org/W3187240237', 'https://openalex.org/W4229031382', 'https://openalex.org/W3035265375', 'https://openalex.org/W6801567822', 'https://openalex.org/W4312407537', 'https://openalex.org/W4221145554', 'https://openalex.org/W2968124245', 'https://openalex.org/W2966715458', 'https://openalex.org/W2970869018', 'https://openalex.org/W2969876226', 'https://openalex.org/W2997591391', 'https://openalex.org/W3116651605', 'https://openalex.org/W4225307291', 'https://openalex.org/W3035485997', 'https://openalex.org/W3035688398', 'https://openalex.org/W3194633557', 'https://openalex.org/W3126464137', 'https://openalex.org/W3126792443', 'https://openalex.org/W3159619744', 'https://openalex.org/W3184784418', 'https://openalex.org/W3212610063', 'https://openalex.org/W3213148312', 'https://openalex.org/W3208314443', 'https://openalex.org/W4312784228', 'https://openalex.org/W2981851019', 'https://openalex.org/W2975357369', 'https://openalex.org/W4394659899', 'https://openalex.org/W3100712674', 'https://openalex.org/W6804204055', 'https://openalex.org/W3177224328', 'https://openalex.org/W6790978476', 'https://openalex.org/W3165647589', 'https://openalex.org/W3154596443', 'https://openalex.org/W6803872405', 'https://openalex.org/W4321033098', 'https://openalex.org/W4313158203', 'https://openalex.org/W3182937942', 'https://openalex.org/W4313181088', 'https://openalex.org/W3155860693', 'https://openalex.org/W4225323055', 'https://openalex.org/W3177654849', 'https://openalex.org/W4226182655', 'https://openalex.org/W3215495615', 'https://openalex.org/W4312877428', 'https://openalex.org/W4283821415', 'https://openalex.org/W4221167472', 'https://openalex.org/W4312956471', 'https://openalex.org/W4312629998', 'https://openalex.org/W3212456749', 'https://openalex.org/W3116952214', 'https://openalex.org/W4229042118', 'https://openalex.org/W4281550812', 'https://openalex.org/W3176445421', 'https://openalex.org/W6838580846', 'https://openalex.org/W4312463400', 'https://openalex.org/W4320168466', 'https://openalex.org/W4282919422', 'https://openalex.org/W4282028729', 'https://openalex.org/W4283219931', 'https://openalex.org/W2127795553', 'https://openalex.org/W2283196293', 'https://openalex.org/W2250342289', 'https://openalex.org/W2184957013', 'https://openalex.org/W2433281745', 'https://openalex.org/W205829674', 'https://openalex.org/W2127426251', 'https://openalex.org/W1533230146', 'https://openalex.org/W68132019', 'https://openalex.org/W2949972983', 'https://openalex.org/W2964311892', 'https://openalex.org/W2519887557', 'https://openalex.org/W6730084236', 'https://openalex.org/W2624431344', 'https://openalex.org/W2604314403', 'https://openalex.org/W2950393809', 'https://openalex.org/W2951105272', 'https://openalex.org/W2984902757', 'https://openalex.org/W3035051781', 'https://openalex.org/W3105111366', 'https://openalex.org/W3102659883', 'https://openalex.org/W3153543512', 'https://openalex.org/W3105601320', 'https://openalex.org/W2953356739', 'https://openalex.org/W2970986510', 'https://openalex.org/W2963477107', 'https://openalex.org/W2964303913', 'https://openalex.org/W64813323', 'https://openalex.org/W2912924812', 'https://openalex.org/W2889787757', 'https://openalex.org/W2946659172', 'https://openalex.org/W2963961878', 'https://openalex.org/W2040916592', 'https://openalex.org/W2963995027', 'https://openalex.org/W6766937060', 'https://openalex.org/W3102187933', 'https://openalex.org/W2970062726', 'https://openalex.org/W2998617917', 'https://openalex.org/W2971236147', 'https://openalex.org/W3169993339', 'https://openalex.org/W2904565150', 'https://openalex.org/W4249013746', 'https://openalex.org/W2997908677', 'https://openalex.org/W1933349210', 'https://openalex.org/W3034188691', 'https://openalex.org/W2741631785', 'https://openalex.org/W2912371042', 'https://openalex.org/W2130158090', 'https://openalex.org/W2963115613', 'https://openalex.org/W3187415662', 'https://openalex.org/W2899197626', 'https://openalex.org/W2963449390', 'https://openalex.org/W6789883375', 'https://openalex.org/W4220736817', 'https://openalex.org/W2962764817', 'https://openalex.org/W2960655175', 'https://openalex.org/W4284696747', 'https://openalex.org/W3166712493', 'https://openalex.org/W3181069167', 'https://openalex.org/W2900474539', 'https://openalex.org/W3173871266', 'https://openalex.org/W4281479158', 'https://openalex.org/W4307008556', 'https://openalex.org/W4312651322', 'https://openalex.org/W3198377975', 'https://openalex.org/W4312310776', 'https://openalex.org/W2066190567', 'https://openalex.org/W3183152796', 'https://openalex.org/W2962964995', 'https://openalex.org/W2914304175', 'https://openalex.org/W3104279398', 'https://openalex.org/W3088409176', 'https://openalex.org/W2606555609']",2023-06-06
https://openalex.org/W3204696009,https://doi.org/10.1109/jstsp.2022.3182537,BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition,"We summarize the results of a host of efforts using giant automatic speech\nrecognition (ASR) models pre-trained using large, diverse unlabeled datasets\ncontaining approximately a million hours of audio. We find that the combination\nof pre-training, self-training and scaling up model size greatly increases data\nefficiency, even for extremely large tasks with tens of thousands of hours of\nlabeled data. In particular, on an ASR task with 34k hours of labeled data, by\nfine-tuning an 8 billion parameter pre-trained Conformer model we can match\nstate-of-the-art (SoTA) performance with only 3% of the training data and\nsignificantly improve SoTA with the full training set. We also report on the\nuniversal benefits gained from using big pre-trained and self-trained models\nfor a large set of downstream tasks that cover a wide range of speech domains\nand span multiple orders of magnitudes of dataset sizes, including obtaining\nSoTA performance on many public benchmarks. In addition, we utilize the learned\nrepresentation of pre-trained networks to achieve SoTA results on non-ASR\ntasks.\n","['https://openalex.org/W2964245029', 'https://openalex.org/W2963425185', 'https://openalex.org/W6844194202', 'https://openalex.org/W2972943112', 'https://openalex.org/W3100270690', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W3015265920', 'https://openalex.org/W6770514103', 'https://openalex.org/W3016181583', 'https://openalex.org/W3102342027', 'https://openalex.org/W6780218876', 'https://openalex.org/W4210690962', 'https://openalex.org/W6677717300', 'https://openalex.org/W165878654', 'https://openalex.org/W1993660824', 'https://openalex.org/W2940322076', 'https://openalex.org/W3015522062', 'https://openalex.org/W6770506093', 'https://openalex.org/W2962907457', 'https://openalex.org/W3141100132', 'https://openalex.org/W3026041220', 'https://openalex.org/W3096338464', 'https://openalex.org/W3198840231', 'https://openalex.org/W6784614252', 'https://openalex.org/W2995181338', 'https://openalex.org/W1494198834', 'https://openalex.org/W3160525311', 'https://openalex.org/W6791904447', 'https://openalex.org/W3160766462', 'https://openalex.org/W3097777922', 'https://openalex.org/W2144499799', 'https://openalex.org/W2127141656', 'https://openalex.org/W4210463634', 'https://openalex.org/W3006926732', 'https://openalex.org/W2593116425', 'https://openalex.org/W2111316763', 'https://openalex.org/W2101210369', 'https://openalex.org/W2088622183', 'https://openalex.org/W3035160371', 'https://openalex.org/W2936774411', 'https://openalex.org/W3015995734', 'https://openalex.org/W6747747266', 'https://openalex.org/W2964038834', 'https://openalex.org/W2964059644', 'https://openalex.org/W2892008152', 'https://openalex.org/W2913178639', 'https://openalex.org/W3096518646', 'https://openalex.org/W6784800133', 'https://openalex.org/W3160628828', 'https://openalex.org/W3196427171', 'https://openalex.org/W2963122170', 'https://openalex.org/W3198270883', 'https://openalex.org/W6755207826', 'https://openalex.org/W6756379755', 'https://openalex.org/W6767997687', 'https://openalex.org/W3129831491', 'https://openalex.org/W6776671032', 'https://openalex.org/W6780805062', 'https://openalex.org/W6795581714', 'https://openalex.org/W2964110616', 'https://openalex.org/W2121879602', 'https://openalex.org/W6739901393', 'https://openalex.org/W6751104502', 'https://openalex.org/W3149629662', 'https://openalex.org/W2033256038', 'https://openalex.org/W3020336359', 'https://openalex.org/W6640059789', 'https://openalex.org/W3181056718', 'https://openalex.org/W3198654230', 'https://openalex.org/W2125336414', 'https://openalex.org/W6771467084', 'https://openalex.org/W6691509046', 'https://openalex.org/W2799473636', 'https://openalex.org/W6631362777', 'https://openalex.org/W2900212944', 'https://openalex.org/W2962760690', 'https://openalex.org/W2513345070', 'https://openalex.org/W2963920996', 'https://openalex.org/W3016010032', 'https://openalex.org/W2030931454', 'https://openalex.org/W6712588427', 'https://openalex.org/W2726515241', 'https://openalex.org/W3196876847', 'https://openalex.org/W3096216346', 'https://openalex.org/W2052666245', 'https://openalex.org/W6750665317', 'https://openalex.org/W2991572650', 'https://openalex.org/W3163571828', 'https://openalex.org/W2977259558', 'https://openalex.org/W3096383643', 'https://openalex.org/W6746665609', 'https://openalex.org/W3198239978', 'https://openalex.org/W6675354045', 'https://openalex.org/W2767754137', 'https://openalex.org/W6791429434', 'https://openalex.org/W4214507759', 'https://openalex.org/W2963351145', 'https://openalex.org/W2996383576', 'https://openalex.org/W2979476256', 'https://openalex.org/W2767487732', 'https://openalex.org/W3125709657', 'https://openalex.org/W3139918052', 'https://openalex.org/W3151935374', 'https://openalex.org/W3157408717', 'https://openalex.org/W2962911098', 'https://openalex.org/W4297808394', 'https://openalex.org/W3157697407', 'https://openalex.org/W4293569541', 'https://openalex.org/W3099782249', 'https://openalex.org/W3134486096', 'https://openalex.org/W3093579165', 'https://openalex.org/W4287251377', 'https://openalex.org/W2963341956', 'https://openalex.org/W2926827382', 'https://openalex.org/W4285719527', 'https://openalex.org/W2963403868', 'https://openalex.org/W2794753807', 'https://openalex.org/W1828163288', 'https://openalex.org/W2250357346', 'https://openalex.org/W2991213871', 'https://openalex.org/W3036601975', 'https://openalex.org/W2797583228', 'https://openalex.org/W2980077696', 'https://openalex.org/W2900096133', 'https://openalex.org/W3093502935', 'https://openalex.org/W3093346109', 'https://openalex.org/W3163465001', 'https://openalex.org/W3101648800', 'https://openalex.org/W2988736778', 'https://openalex.org/W2396589722', 'https://openalex.org/W3021234081', 'https://openalex.org/W4385245566', 'https://openalex.org/W2998532468', 'https://openalex.org/W4237168004', 'https://openalex.org/W2101234009', 'https://openalex.org/W2973727699', 'https://openalex.org/W3016400019', 'https://openalex.org/W1915251500', 'https://openalex.org/W2117590177', 'https://openalex.org/W4287811302', 'https://openalex.org/W2896457183', 'https://openalex.org/W2842511635', 'https://openalex.org/W3025165719', 'https://openalex.org/W3157428800', 'https://openalex.org/W3030437843', 'https://openalex.org/W3197074955', 'https://openalex.org/W2796487725', 'https://openalex.org/W3003382064', 'https://openalex.org/W2783831488', 'https://openalex.org/W3040573126', 'https://openalex.org/W3147962056', 'https://openalex.org/W3027083471', 'https://openalex.org/W1524333225', 'https://openalex.org/W2773070064', 'https://openalex.org/W3131736947', 'https://openalex.org/W2911109671', 'https://openalex.org/W2995929068', 'https://openalex.org/W4398958419']",2022-06-13
https://openalex.org/W3160525311,https://doi.org/10.1109/icassp39728.2021.9414641,Self-Training and Pre-Training are Complementary for Speech Recognition,"Self-training and unsupervised pre-training have emerged as effective approaches to improve speech recognition systems using unlabeled data. However, it is not clear whether they learn similar patterns or if they can be effectively combined. In this paper, we show that pseudo-labeling and pre-training with wav2vec 2.0 are complementary in a variety of labeled data setups. Using just 10 minutes of labeled data from Libri-light as well as 53k hours of unlabeled data from LibriVox achieves word error rates (WER) of 2.8%/4.8% on the clean and other test sets of Librispeech – rivaling the best published systems trained on 960 hours of labeled data only a year ago. Training on all labeled data of Librispeech achieves WERs of 1.5%/3.1%.","['https://openalex.org/W2963250244', 'https://openalex.org/W2995181338', 'https://openalex.org/W1494198834', 'https://openalex.org/W6753018729', 'https://openalex.org/W6729448088', 'https://openalex.org/W2953190524', 'https://openalex.org/W6754905691', 'https://openalex.org/W2127141656', 'https://openalex.org/W2933138175', 'https://openalex.org/W2101210369', 'https://openalex.org/W6678794661', 'https://openalex.org/W6760911985', 'https://openalex.org/W3015522062', 'https://openalex.org/W3096338464', 'https://openalex.org/W3026041220', 'https://openalex.org/W2842511635', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W2972943112', 'https://openalex.org/W6739901393', 'https://openalex.org/W3097777922', 'https://openalex.org/W6770514103', 'https://openalex.org/W3095173472', 'https://openalex.org/W6755638308', 'https://openalex.org/W2124509324', 'https://openalex.org/W6772883055', 'https://openalex.org/W2972889948', 'https://openalex.org/W6770506093', 'https://openalex.org/W2111316763', 'https://openalex.org/W2936774411', 'https://openalex.org/W6769238691', 'https://openalex.org/W3016181583', 'https://openalex.org/W3102342027', 'https://openalex.org/W6780218876', 'https://openalex.org/W3003875258', 'https://openalex.org/W6755207826', 'https://openalex.org/W6783934668', 'https://openalex.org/W3027083471', 'https://openalex.org/W2896457183', 'https://openalex.org/W2811079561', 'https://openalex.org/W2962942158', 'https://openalex.org/W3090196146', 'https://openalex.org/W2124634352', 'https://openalex.org/W2998532468', 'https://openalex.org/W3036601975', 'https://openalex.org/W3168921237', 'https://openalex.org/W2963400424', 'https://openalex.org/W2996383576', 'https://openalex.org/W2547875792', 'https://openalex.org/W3021469861', 'https://openalex.org/W2963403868', 'https://openalex.org/W3099782249', 'https://openalex.org/W2963341956', 'https://openalex.org/W2981991061', 'https://openalex.org/W4385245566', 'https://openalex.org/W2991213871', 'https://openalex.org/W2962907457', 'https://openalex.org/W2988736778', 'https://openalex.org/W3025165719', 'https://openalex.org/W2963631907', 'https://openalex.org/W4297808394', 'https://openalex.org/W2979476256']",2021-05-13
https://openalex.org/W3160799772,https://doi.org/10.1109/icassp39728.2021.9414460,Hubert: How Much Can a Bad Teacher Benefit ASR Pre-Training?,"Compared to vision and language applications, self-supervised pre-training approaches for ASR are challenged by three unique problems: (1) There are multiple sound units in each input utterance, (2) With audio-only pre-training, there is no lexicon of sound units, and (3) Sound units have variable lengths with no explicit segmentation. In this paper, we propose the Hidden-Unit BERT (HUBERT) model which utilizes a cheap k-means clustering step to provide aligned target labels for pre-training of a BERT model. A key ingredient of our approach is applying the predictive loss over the masked regions only. This allows the pre-training stage to benefit from the consistency of the unsupervised teacher rather that its intrinsic quality. Starting with a simple k-means teacher of 100 cluster, and using two iterations of clustering, the HUBERT model matches the state-of-the-art wav2vec 2.0 performance on the ultra low-resource Libri-light 10h, 1h, 10min supervised subsets.","['https://openalex.org/W6675354045', 'https://openalex.org/W6631190155', 'https://openalex.org/W3003875258', 'https://openalex.org/W3015265920', 'https://openalex.org/W3035202887', 'https://openalex.org/W6769455919', 'https://openalex.org/W6783797576', 'https://openalex.org/W6780483730', 'https://openalex.org/W6777232839', 'https://openalex.org/W2982223350', 'https://openalex.org/W6770514103', 'https://openalex.org/W6766978945', 'https://openalex.org/W6780218876', 'https://openalex.org/W2110073835', 'https://openalex.org/W3015522062', 'https://openalex.org/W6772883055', 'https://openalex.org/W6753000030', 'https://openalex.org/W1494198834', 'https://openalex.org/W2995181338', 'https://openalex.org/W2981283774', 'https://openalex.org/W3033038061', 'https://openalex.org/W3035524453', 'https://openalex.org/W2750248772', 'https://openalex.org/W2962739339', 'https://openalex.org/W2973049979', 'https://openalex.org/W6760822226', 'https://openalex.org/W2842511635', 'https://openalex.org/W2752796333', 'https://openalex.org/W2146444479', 'https://openalex.org/W6771917389', 'https://openalex.org/W6769196770', 'https://openalex.org/W6755207826', 'https://openalex.org/W6675022971', 'https://openalex.org/W4254197176', 'https://openalex.org/W3011411500', 'https://openalex.org/W6947929050', 'https://openalex.org/W2962850167', 'https://openalex.org/W2933138175', 'https://openalex.org/W2127141656', 'https://openalex.org/W2946417913', 'https://openalex.org/W6745117592', 'https://openalex.org/W3008525923', 'https://openalex.org/W3100270690', 'https://openalex.org/W2963618559', 'https://openalex.org/W2998532468', 'https://openalex.org/W2758785877', 'https://openalex.org/W2953190524', 'https://openalex.org/W3144810982', 'https://openalex.org/W2963341956', 'https://openalex.org/W2988736778', 'https://openalex.org/W4297786395', 'https://openalex.org/W3148040514', 'https://openalex.org/W2964121744', 'https://openalex.org/W3125709657', 'https://openalex.org/W3013571468', 'https://openalex.org/W1522301498', 'https://openalex.org/W2100768664', 'https://openalex.org/W2970971581', 'https://openalex.org/W2896457183', 'https://openalex.org/W3126816608', 'https://openalex.org/W4295312788', 'https://openalex.org/W2979476256', 'https://openalex.org/W3099782249', 'https://openalex.org/W4287824654', 'https://openalex.org/W2996383576', 'https://openalex.org/W2950180292', 'https://openalex.org/W3025035610', 'https://openalex.org/W2972943112', 'https://openalex.org/W3036601975', 'https://openalex.org/W4297808394', 'https://openalex.org/W3016011332', 'https://openalex.org/W2963799213', 'https://openalex.org/W2101234009', 'https://openalex.org/W2883725317']",2021-05-13
https://openalex.org/W3205644108,https://doi.org/10.18653/v1/2022.acl-long.393,SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing,"Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.","['https://openalex.org/W3193521535', 'https://openalex.org/W2933138175', 'https://openalex.org/W2789541106', 'https://openalex.org/W2963979492', 'https://openalex.org/W3015338123', 'https://openalex.org/W3097206152', 'https://openalex.org/W2963925437', 'https://openalex.org/W4394671563', 'https://openalex.org/W2890964092', 'https://openalex.org/W2972359262', 'https://openalex.org/W3148001440', 'https://openalex.org/W3197580070', 'https://openalex.org/W3209059054', 'https://openalex.org/W2962739339', 'https://openalex.org/W3161302809', 'https://openalex.org/W3092424727', 'https://openalex.org/W2972541922', 'https://openalex.org/W2965373594', 'https://openalex.org/W3176455679', 'https://openalex.org/W3121299949', 'https://openalex.org/W2981852735', 'https://openalex.org/W1915251500', 'https://openalex.org/W3196833881', 'https://openalex.org/W2101105183', 'https://openalex.org/W2962780374', 'https://openalex.org/W2194775991', 'https://openalex.org/W3118753411', 'https://openalex.org/W3169320628', 'https://openalex.org/W2952509486', 'https://openalex.org/W1552314771', 'https://openalex.org/W2945700568', 'https://openalex.org/W2988736778', 'https://openalex.org/W2964243274', 'https://openalex.org/W3112092703', 'https://openalex.org/W4287329822', 'https://openalex.org/W4226033575', 'https://openalex.org/W3209984917', 'https://openalex.org/W3113687514', 'https://openalex.org/W2982399380', 'https://openalex.org/W2963403868', 'https://openalex.org/W3009565979', 'https://openalex.org/W2945260553', 'https://openalex.org/W3098403858', 'https://openalex.org/W3170863103', 'https://openalex.org/W3025075133', 'https://openalex.org/W3162037819', 'https://openalex.org/W2991213871', 'https://openalex.org/W2973217961', 'https://openalex.org/W2971274815', 'https://openalex.org/W2794753807', 'https://openalex.org/W2963799213', 'https://openalex.org/W2970597249', 'https://openalex.org/W2963341956', 'https://openalex.org/W3161940574', 'https://openalex.org/W4210690962', 'https://openalex.org/W4287689065', 'https://openalex.org/W2766219058', 'https://openalex.org/W2739883972', 'https://openalex.org/W3163464523', 'https://openalex.org/W3165666670', 'https://openalex.org/W2981363336', 'https://openalex.org/W2963425185', 'https://openalex.org/W4286984129', 'https://openalex.org/W3034999214', 'https://openalex.org/W3176711365', 'https://openalex.org/W2795935804', 'https://openalex.org/W2726515241', 'https://openalex.org/W95152782', 'https://openalex.org/W2516001803', 'https://openalex.org/W1494198834', 'https://openalex.org/W2903739847', 'https://openalex.org/W3162313915', 'https://openalex.org/W2914120296', 'https://openalex.org/W1522301498', 'https://openalex.org/W3096109555', 'https://openalex.org/W2787560479', 'https://openalex.org/W3037217258', 'https://openalex.org/W3092028330', 'https://openalex.org/W3160345865', 'https://openalex.org/W3175963743', 'https://openalex.org/W2767052532', 'https://openalex.org/W3096485810', 'https://openalex.org/W3099782249', 'https://openalex.org/W3026408381', 'https://openalex.org/W2963970792', 'https://openalex.org/W3036601975', 'https://openalex.org/W4287374065', 'https://openalex.org/W3172698324', 'https://openalex.org/W3157923770']",2022-01-01
https://openalex.org/W3096338464,https://doi.org/10.21437/interspeech.2020-1800,Iterative Pseudo-Labeling for Speech Recognition,"Pseudo-labeling has recently shown promise in end-to-end automatic speech recognition (ASR).We study Iterative Pseudo-Labeling (IPL), a semi-supervised algorithm which efficiently performs multiple iterations of pseudo-labeling on unlabeled data as the acoustic model evolves.In particular, IPL fine tunes an existing model at each iteration using both labeled data and a subset of unlabeled data.We study the main components of IPL: decoding with a language model and data augmentation.We then demonstrate the effectiveness of IPL by achieving state-of-the-art word-error rate on the LIBRISPEECH test sets in both standard and low-resource setting.We also study the effect of language models trained on different corpora to show IPL can effectively utilize additional text.Finally, we release a new large in-domain text corpus which does not overlap with the LIBRISPEECH training transcriptions to foster research in low-resource, semi-supervised ASR.","['https://openalex.org/W2963400424', 'https://openalex.org/W2095705004', 'https://openalex.org/W1975113979', 'https://openalex.org/W2981857663', 'https://openalex.org/W82886505', 'https://openalex.org/W3006827623', 'https://openalex.org/W3035160371', 'https://openalex.org/W3026041220', 'https://openalex.org/W3015265920', 'https://openalex.org/W4385245566', 'https://openalex.org/W2998532468', 'https://openalex.org/W2972818416', 'https://openalex.org/W2975381464', 'https://openalex.org/W2146502635', 'https://openalex.org/W2404463488', 'https://openalex.org/W3036601975', 'https://openalex.org/W2904818793', 'https://openalex.org/W2936774411', 'https://openalex.org/W2046932483', 'https://openalex.org/W2033256038', 'https://openalex.org/W2988736778', 'https://openalex.org/W2134800885', 'https://openalex.org/W4286784498', 'https://openalex.org/W2941814890', 'https://openalex.org/W2139453310', 'https://openalex.org/W1494198834', 'https://openalex.org/W3095350795', 'https://openalex.org/W2171761326', 'https://openalex.org/W2995181338', 'https://openalex.org/W2976223659', 'https://openalex.org/W3015522062', 'https://openalex.org/W2991213871', 'https://openalex.org/W2963250244']",2020-10-25
https://openalex.org/W3037057938,https://doi.org/10.21437/interspeech.2021-329,Unsupervised Cross-Lingual Representation Learning for Speech Recognition,"This paper presents XLSR which learns cross-lingual speech representations by pretraining a single model from the raw waveform of speech in multiple languages. We build on wav2vec 2.0 which is trained by solving a contrastive task over masked latent speech representations and jointly learns a quantization of the latents shared across languages. The resulting model is fine-tuned on labeled data and experiments show that cross-lingual pretraining significantly outperforms monolingual pretraining. On the CommonVoice benchmark, XLSR shows a relative phoneme error rate reduction of 72% compared to the best known results. On BABEL, our approach improves word error rate by 16% relative compared to a comparable system. Our approach enables a single multilingual speech recognition model which is competitive to strong individual models. Analysis shows that the latent discrete speech representations are shared across languages with increased sharing for related languages. We hope to catalyze research in low-resource speech understanding by releasing XLSR-53, a large model pretrained in 53 languages.","['https://openalex.org/W2963425185', 'https://openalex.org/W2898630520', 'https://openalex.org/W1994606281', 'https://openalex.org/W2127141656', 'https://openalex.org/W3005680577', 'https://openalex.org/W2970049541', 'https://openalex.org/W3095410713', 'https://openalex.org/W2124509324', 'https://openalex.org/W2963403868', 'https://openalex.org/W2996383576', 'https://openalex.org/W2292087804', 'https://openalex.org/W3099782249', 'https://openalex.org/W2981991061', 'https://openalex.org/W2973049979', 'https://openalex.org/W2025198378', 'https://openalex.org/W2896457183', 'https://openalex.org/W2141440284', 'https://openalex.org/W2995680346', 'https://openalex.org/W2696253854', 'https://openalex.org/W3005511757', 'https://openalex.org/W2958953787', 'https://openalex.org/W2671812860', 'https://openalex.org/W2972943112', 'https://openalex.org/W2964121744', 'https://openalex.org/W2947591107', 'https://openalex.org/W2141820854', 'https://openalex.org/W2842511635', 'https://openalex.org/W2891616026', 'https://openalex.org/W2811079561', 'https://openalex.org/W3032816972', 'https://openalex.org/W2971840980', 'https://openalex.org/W1524956127', 'https://openalex.org/W2988736778', 'https://openalex.org/W1978660892', 'https://openalex.org/W2964309797', 'https://openalex.org/W2987283559', 'https://openalex.org/W2547875792', 'https://openalex.org/W2894835365', 'https://openalex.org/W2983040767', 'https://openalex.org/W1846073453', 'https://openalex.org/W2123798005', 'https://openalex.org/W2962739339', 'https://openalex.org/W2936295285', 'https://openalex.org/W2963027641', 'https://openalex.org/W2730658205', 'https://openalex.org/W2941814890', 'https://openalex.org/W2933138175']",2021-08-27
https://openalex.org/W3035202887,https://doi.org/10.18653/v1/2020.acl-main.213,Improved Speech Representations with Multi-Target Autoregressive Predictive Coding,"Training objectives based on predictive coding have recently been shown to be very effective at learning meaningful representations from unlabeled speech. One example is Autoregressive Predictive Coding (Chung et al., 2019), which trains an autoregressive RNN to generate an unseen future frame given a context such as recent past frames. The basic hypothesis of these approaches is that hidden states that can accurately predict future frames are a useful representation for many downstream tasks. In this paper we extend this hypothesis and aim to enrich the information encoded in the hidden states by training the model to make more accurate future predictions. We propose an auxiliary objective that serves as a regularization to improve generalization of the future frame prediction task. Experimental results on phonetic classification, speech recognition, and speech translation not only support the hypothesis, but also demonstrate the effectiveness of our approach in learning representations that contain richer phonetic content.","['https://openalex.org/W2979476256', 'https://openalex.org/W2939710050', 'https://openalex.org/W2963618559', 'https://openalex.org/W3125709657', 'https://openalex.org/W3096485810', 'https://openalex.org/W2973049979', 'https://openalex.org/W2785350307', 'https://openalex.org/W2996383576', 'https://openalex.org/W2101105183', 'https://openalex.org/W2804648901', 'https://openalex.org/W2963317665', 'https://openalex.org/W2964121744', 'https://openalex.org/W2973157397', 'https://openalex.org/W2982223350', 'https://openalex.org/W2972943112', 'https://openalex.org/W4385245566', 'https://openalex.org/W2525778437', 'https://openalex.org/W2963045354', 'https://openalex.org/W2077804127', 'https://openalex.org/W2964199361', 'https://openalex.org/W3127686677', 'https://openalex.org/W2842511635', 'https://openalex.org/W2963340922', 'https://openalex.org/W1494198834', 'https://openalex.org/W854541894', 'https://openalex.org/W2024490156', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963571336', 'https://openalex.org/W2758785877', 'https://openalex.org/W2963425185', 'https://openalex.org/W2884305338', 'https://openalex.org/W4297808394', 'https://openalex.org/W2988736778', 'https://openalex.org/W2981991061', 'https://openalex.org/W2963720603', 'https://openalex.org/W3016011332', 'https://openalex.org/W179875071']",2020-01-01
https://openalex.org/W3189296823,https://doi.org/10.21437/interspeech.2021-556,LeBenchmark: A Reproducible Framework for Assessing Self-Supervised\n Representation Learning from Speech,"Self-Supervised Learning (SSL) using huge unlabeled data has been\nsuccessfully explored for image and natural language processing. Recent works\nalso investigated SSL from speech. They were notably successful to improve\nperformance on downstream tasks such as automatic speech recognition (ASR).\nWhile these works suggest it is possible to reduce dependence on labeled data\nfor building efficient speech systems, their evaluation was mostly made on ASR\nand using multiple and heterogeneous experimental settings (most of them for\nEnglish). This questions the objective comparison of SSL approaches and the\nevaluation of their impact on building speech systems. In this paper, we\npropose LeBenchmark: a reproducible framework for assessing SSL from speech. It\nnot only includes ASR (high and low resource) tasks but also spoken language\nunderstanding, speech translation and emotion recognition. We also focus on\nspeech technologies in a language different than English: French. SSL models of\ndifferent sizes are trained from carefully sourced and documented datasets.\nExperiments show that SSL is beneficial for most but not all tasks which\nconfirms the need for exhaustive and reliable benchmarks to evaluate its real\nimpact. LeBenchmark is shared with the scientific community for reproducible\nresearch in SSL from speech.\n","['https://openalex.org/W3095410713', 'https://openalex.org/W2988736778', 'https://openalex.org/W3198429080', 'https://openalex.org/W4385245566', 'https://openalex.org/W72302491', 'https://openalex.org/W2888867175', 'https://openalex.org/W2133564696', 'https://openalex.org/W2313339984', 'https://openalex.org/W2810556878', 'https://openalex.org/W3049256661', 'https://openalex.org/W3016011332', 'https://openalex.org/W2514741789', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962739339', 'https://openalex.org/W3016181583', 'https://openalex.org/W3015867372', 'https://openalex.org/W2576530755', 'https://openalex.org/W157724941', 'https://openalex.org/W3005680577', 'https://openalex.org/W3030437843', 'https://openalex.org/W3102342027', 'https://openalex.org/W2933138175', 'https://openalex.org/W2896457183', 'https://openalex.org/W2327501763', 'https://openalex.org/W3099944122', 'https://openalex.org/W2972943112', 'https://openalex.org/W2948012107', 'https://openalex.org/W3054645415', 'https://openalex.org/W3092424727', 'https://openalex.org/W3015935472', 'https://openalex.org/W2973049979', 'https://openalex.org/W3021934733', 'https://openalex.org/W2045528981', 'https://openalex.org/W1524333225', 'https://openalex.org/W3035202887', 'https://openalex.org/W2249819665', 'https://openalex.org/W3197771105', 'https://openalex.org/W2972327934', 'https://openalex.org/W2329093554', 'https://openalex.org/W3036601975', 'https://openalex.org/W2402146185', 'https://openalex.org/W3119308075', 'https://openalex.org/W2399733683']",2021-04-23
https://openalex.org/W3198608154,https://doi.org/10.21437/interspeech.2021-391,Speech SimCLR: Combining Contrastive and Reconstruction Objective for Self-Supervised Speech Representation Learning,"Self-supervised visual pretraining has shown significant progress recently.Among those methods, SimCLR greatly advanced the state of the art in self-supervised and semisupervised learning on ImageNet.The input feature representations for speech and visual tasks are both continuous, so it is natural to consider applying similar objective on speech representation learning.In this paper, we propose Speech SimCLR, a new self-supervised objective for speech representation learning.During training, Speech SimCLR applies augmentation on raw speech and its spectrogram.Its objective is the combination of contrastive loss that maximizes agreement between differently augmented samples in the latent space and reconstruction loss of input representation.The proposed method achieved competitive results on speech emotion recognition and speech recognition.","['https://openalex.org/W2963127222', 'https://openalex.org/W4214784181', 'https://openalex.org/W1522301498', 'https://openalex.org/W4398958419', 'https://openalex.org/W2973049979', 'https://openalex.org/W3160345865', 'https://openalex.org/W3015265920', 'https://openalex.org/W1568716973', 'https://openalex.org/W2146334809', 'https://openalex.org/W2404126548', 'https://openalex.org/W2972943112', 'https://openalex.org/W2936774411', 'https://openalex.org/W3102342027', 'https://openalex.org/W2964227577', 'https://openalex.org/W3046125265', 'https://openalex.org/W2973157397', 'https://openalex.org/W2995181338', 'https://openalex.org/W4385245566', 'https://openalex.org/W3148040514', 'https://openalex.org/W2982223350', 'https://openalex.org/W3035524453', 'https://openalex.org/W3034774681', 'https://openalex.org/W3016011332', 'https://openalex.org/W3036601975', 'https://openalex.org/W3041561163', 'https://openalex.org/W2972818416', 'https://openalex.org/W2219249508', 'https://openalex.org/W2979476256', 'https://openalex.org/W3035202887', 'https://openalex.org/W2944828972', 'https://openalex.org/W2251321385', 'https://openalex.org/W3005680577', 'https://openalex.org/W2981991061', 'https://openalex.org/W3144810982', 'https://openalex.org/W2936451900', 'https://openalex.org/W2988736778']",2021-08-27
https://openalex.org/W4286370724,https://doi.org/10.1109/taslp.2022.3192728,The Weighted Cross-Modal Attention Mechanism With Sentiment Prediction Auxiliary Task for Multimodal Sentiment Analysis,"Human brain extracts the spatial and temporal semantic information by processing the multi-modalities, which has contextually meaningful for perceiving and understanding the emotional state of an individual. However, there are two main challenges in modeling multimodal sequences: 1) the different sampling rates from multimodal data make the cross-modal interactions very difficult; 2) how to efficiently fuse unimodal representations and effectively capture relationships among multimodal data. In this paper, we design the weighted cross-modal attention mechanism, which not only captures the temporal correlation information and the spatial dependence information of each modality, but also dynamically adjusts the weight of each modality across different time steps. And the unimodal subtasks are led for assisting the representation learning of specific modality to jointly train the multimodal tasks and unimodal subtasks to explore the complementary relationships of each modality. Our model gets a new state-of-the-art record on the CMU-MOSI dataset and brings noticeable performance improvements on all the metrics. For the CMU-MOSEI dataset, the F1 score of the binary classification, the 7-class task, and the regression task of our model are still the highest among all models and the proposed model is only lower than the multimodal split attention fusion (MSAF) model with aligned data on the accuracy of the binary classification, showing the great performance of the suggested method.","['https://openalex.org/W2123442489', 'https://openalex.org/W2964266095', 'https://openalex.org/W3034897750', 'https://openalex.org/W2029996593', 'https://openalex.org/W2079725295', 'https://openalex.org/W2583643061', 'https://openalex.org/W2964010806', 'https://openalex.org/W2546919788', 'https://openalex.org/W2963128932', 'https://openalex.org/W2964216663', 'https://openalex.org/W3093051361', 'https://openalex.org/W2787581402', 'https://openalex.org/W2964346351', 'https://openalex.org/W3141688548', 'https://openalex.org/W3116238789', 'https://openalex.org/W3128412859', 'https://openalex.org/W6770514103', 'https://openalex.org/W2095176743', 'https://openalex.org/W2157331557', 'https://openalex.org/W6682137061', 'https://openalex.org/W6783497617', 'https://openalex.org/W3128687455', 'https://openalex.org/W2950331846', 'https://openalex.org/W4205932476', 'https://openalex.org/W6739365718', 'https://openalex.org/W2556418146', 'https://openalex.org/W2883409523', 'https://openalex.org/W2964051877', 'https://openalex.org/W3039263585', 'https://openalex.org/W3167098825', 'https://openalex.org/W3048195943', 'https://openalex.org/W3194765442', 'https://openalex.org/W6787666870', 'https://openalex.org/W2624871570', 'https://openalex.org/W2988736778', 'https://openalex.org/W4207082935', 'https://openalex.org/W3112864462', 'https://openalex.org/W1595126664']",2022-01-01
https://openalex.org/W3199093330,https://doi.org/10.1162/opmi_a_00046,Do Infants Really Learn Phonetic Categories?,"Abstract Early changes in infants’ ability to perceive native and nonnative speech sound contrasts are typically attributed to their developing knowledge of phonetic categories. We critically examine this hypothesis and argue that there is little direct evidence of category knowledge in infancy. We then propose an alternative account in which infants’ perception changes because they are learning a perceptual space that is appropriate to represent speech, without yet carving up that space into phonetic categories. If correct, this new account has substantial implications for understanding early language development.","['https://openalex.org/W6713887236', 'https://openalex.org/W2610616322', 'https://openalex.org/W2138621768', 'https://openalex.org/W2099385247', 'https://openalex.org/W6769196770', 'https://openalex.org/W6780218876', 'https://openalex.org/W2255122597', 'https://openalex.org/W2077739821', 'https://openalex.org/W2063303346', 'https://openalex.org/W2051882640', 'https://openalex.org/W2001365213', 'https://openalex.org/W4238515972', 'https://openalex.org/W2022042240', 'https://openalex.org/W2092189137', 'https://openalex.org/W2157427027', 'https://openalex.org/W2165454511', 'https://openalex.org/W2157423901', 'https://openalex.org/W1965689173', 'https://openalex.org/W2115113802', 'https://openalex.org/W2560653257', 'https://openalex.org/W3038687092', 'https://openalex.org/W3100270690', 'https://openalex.org/W2972943112', 'https://openalex.org/W1851153584', 'https://openalex.org/W2142633459', 'https://openalex.org/W2766298282', 'https://openalex.org/W2133514592', 'https://openalex.org/W2166686315', 'https://openalex.org/W2095458199', 'https://openalex.org/W6755207826', 'https://openalex.org/W2110627398', 'https://openalex.org/W2940544976', 'https://openalex.org/W2963620343', 'https://openalex.org/W2963648280', 'https://openalex.org/W4245984049', 'https://openalex.org/W2011238950', 'https://openalex.org/W6680300913', 'https://openalex.org/W1993755070', 'https://openalex.org/W2146163948', 'https://openalex.org/W2143022183', 'https://openalex.org/W2076343712', 'https://openalex.org/W2036486610', 'https://openalex.org/W2095120597', 'https://openalex.org/W1973733679', 'https://openalex.org/W4243791828', 'https://openalex.org/W2250874882', 'https://openalex.org/W2002714598', 'https://openalex.org/W2108145097', 'https://openalex.org/W4248547900', 'https://openalex.org/W1964922256', 'https://openalex.org/W2073161619', 'https://openalex.org/W2096563020', 'https://openalex.org/W1995422333', 'https://openalex.org/W1865482220', 'https://openalex.org/W3012276739', 'https://openalex.org/W2167294409', 'https://openalex.org/W2047509453', 'https://openalex.org/W1991355509', 'https://openalex.org/W1978549941', 'https://openalex.org/W2097181372', 'https://openalex.org/W2161012068', 'https://openalex.org/W1993678946', 'https://openalex.org/W2057007397', 'https://openalex.org/W1900890596', 'https://openalex.org/W2038056950', 'https://openalex.org/W2059824090', 'https://openalex.org/W1545920196', 'https://openalex.org/W2085834206', 'https://openalex.org/W1991848143', 'https://openalex.org/W4245176872', 'https://openalex.org/W1970727472', 'https://openalex.org/W2399904909', 'https://openalex.org/W2018124860', 'https://openalex.org/W1990351858', 'https://openalex.org/W2031282998', 'https://openalex.org/W2031142793', 'https://openalex.org/W2150389998', 'https://openalex.org/W2988972814', 'https://openalex.org/W2086880169', 'https://openalex.org/W2022465033', 'https://openalex.org/W2012714827', 'https://openalex.org/W2063525438', 'https://openalex.org/W2005311247', 'https://openalex.org/W2140661818', 'https://openalex.org/W2075142360', 'https://openalex.org/W6603402226', 'https://openalex.org/W1778492285', 'https://openalex.org/W2266656832', 'https://openalex.org/W2523066912', 'https://openalex.org/W3033791316', 'https://openalex.org/W2111376597', 'https://openalex.org/W2915375556', 'https://openalex.org/W2011569856', 'https://openalex.org/W2169703238', 'https://openalex.org/W91681889', 'https://openalex.org/W1994097638', 'https://openalex.org/W2318786919', 'https://openalex.org/W2418317743', 'https://openalex.org/W2170014483', 'https://openalex.org/W2894827659', 'https://openalex.org/W2118743044', 'https://openalex.org/W6781819162', 'https://openalex.org/W2136653392', 'https://openalex.org/W2104752510', 'https://openalex.org/W1576931943', 'https://openalex.org/W6632788291', 'https://openalex.org/W2169991335', 'https://openalex.org/W2776941264', 'https://openalex.org/W2131070395', 'https://openalex.org/W2073083335', 'https://openalex.org/W281094599', 'https://openalex.org/W2394503152', 'https://openalex.org/W2011145053', 'https://openalex.org/W2091143423', 'https://openalex.org/W2168266939', 'https://openalex.org/W1993173377', 'https://openalex.org/W2982653307', 'https://openalex.org/W2047204160', 'https://openalex.org/W1964111140', 'https://openalex.org/W3149842901', 'https://openalex.org/W4242558598', 'https://openalex.org/W1996669300', 'https://openalex.org/W2070791196', 'https://openalex.org/W6675881246', 'https://openalex.org/W2114347655', 'https://openalex.org/W2012380112', 'https://openalex.org/W2962739339', 'https://openalex.org/W1981980357', 'https://openalex.org/W1970688873', 'https://openalex.org/W1483736876', 'https://openalex.org/W2172174689', 'https://openalex.org/W1991274470', 'https://openalex.org/W3097485645', 'https://openalex.org/W2415378728', 'https://openalex.org/W1796128977', 'https://openalex.org/W2962850179', 'https://openalex.org/W2914978042', 'https://openalex.org/W6790935419', 'https://openalex.org/W2053679926', 'https://openalex.org/W1980862600', 'https://openalex.org/W2142968525', 'https://openalex.org/W2803143571', 'https://openalex.org/W2911249026', 'https://openalex.org/W3125087428', 'https://openalex.org/W2395899413', 'https://openalex.org/W2940759342', 'https://openalex.org/W2214526267', 'https://openalex.org/W2134145060', 'https://openalex.org/W2128566521', 'https://openalex.org/W1968447180', 'https://openalex.org/W2014337885', 'https://openalex.org/W1628323995', 'https://openalex.org/W2029735223', 'https://openalex.org/W2014695834', 'https://openalex.org/W1996861825', 'https://openalex.org/W2119165475', 'https://openalex.org/W2160464066', 'https://openalex.org/W2911464213', 'https://openalex.org/W3102667484', 'https://openalex.org/W2404799143', 'https://openalex.org/W2105000456', 'https://openalex.org/W2129115667', 'https://openalex.org/W3165249266', 'https://openalex.org/W2003341094', 'https://openalex.org/W1925965306', 'https://openalex.org/W2032443763', 'https://openalex.org/W2153767712', 'https://openalex.org/W3095361818', 'https://openalex.org/W2786608204', 'https://openalex.org/W2120396402', 'https://openalex.org/W2166037490', 'https://openalex.org/W2089883580', 'https://openalex.org/W2156909242', 'https://openalex.org/W2019548241', 'https://openalex.org/W2101509422', 'https://openalex.org/W2138193907', 'https://openalex.org/W2945619922', 'https://openalex.org/W2028399768', 'https://openalex.org/W2135563147', 'https://openalex.org/W2159901619', 'https://openalex.org/W2096736596', 'https://openalex.org/W2071316865', 'https://openalex.org/W2145638643', 'https://openalex.org/W2031609871', 'https://openalex.org/W4297808394', 'https://openalex.org/W2132089731', 'https://openalex.org/W3125709657', 'https://openalex.org/W3169320628', 'https://openalex.org/W217970951', 'https://openalex.org/W4246559809', 'https://openalex.org/W3036192984', 'https://openalex.org/W3036512766', 'https://openalex.org/W2138857742', 'https://openalex.org/W4212774754', 'https://openalex.org/W2082256905', 'https://openalex.org/W2973026522', 'https://openalex.org/W2132672710', 'https://openalex.org/W2615322227', 'https://openalex.org/W2154095608', 'https://openalex.org/W2026484633', 'https://openalex.org/W2727795052', 'https://openalex.org/W2842511635', 'https://openalex.org/W2151960659', 'https://openalex.org/W4289564011', 'https://openalex.org/W1519956846', 'https://openalex.org/W2760669695', 'https://openalex.org/W2988736778', 'https://openalex.org/W2973049979', 'https://openalex.org/W2039396020', 'https://openalex.org/W4385773817', 'https://openalex.org/W2939710050', 'https://openalex.org/W2604846069']",2021-01-01
https://openalex.org/W3213618310,https://doi.org/10.1609/aaai.v36i10.21327,Towards Building ASR Systems for the Next Billion Users,"Recent methods in speech and language technology pretrain very large models which are fine-tuned for specific tasks. However, the benefits of such large models are often limited to a few resource rich languages of the world. In this work, we make multiple contributions towards building ASR systems for low resource languages from the Indian subcontinent. First, we curate 17,000 hours of raw speech data for 40 Indian languages from a wide variety of domains including education, news, technology, and finance. Second, using this raw speech data we pretrain several variants of wav2vec style models for 40 Indian languages. Third, we analyze the pretrained models to find key features: codebook vectors of similar sounding phonemes are shared across languages, representations across layers are discriminative of the language family, and attention heads often pay attention within small local windows. Fourth, we fine-tune this model for downstream ASR for 9 languages and obtain state-of-the-art results on 3 public datasets, including on very low-resource languages such as Sinhala and Nepali. Our work establishes that multilingual pretraining is an effective strategy for building ASR systems for the linguistically diverse speakers of the Indian subcontinent.","['https://openalex.org/W2958953787', 'https://openalex.org/W2889169007', 'https://openalex.org/W2983040767', 'https://openalex.org/W6767737316', 'https://openalex.org/W2896457183', 'https://openalex.org/W6699243985', 'https://openalex.org/W2889427787', 'https://openalex.org/W7027429494', 'https://openalex.org/W3177999760', 'https://openalex.org/W2134800885', 'https://openalex.org/W2124509324', 'https://openalex.org/W3099919888', 'https://openalex.org/W1574170747', 'https://openalex.org/W2895676041', 'https://openalex.org/W2402146185', 'https://openalex.org/W2953190524', 'https://openalex.org/W6753742708', 'https://openalex.org/W3097106378', 'https://openalex.org/W3142385198', 'https://openalex.org/W3160186953', 'https://openalex.org/W2895103375', 'https://openalex.org/W572280432', 'https://openalex.org/W3046368065', 'https://openalex.org/W3121299949', 'https://openalex.org/W1494198834', 'https://openalex.org/W3147984406', 'https://openalex.org/W2781384251', 'https://openalex.org/W3036601975', 'https://openalex.org/W2963341956', 'https://openalex.org/W2933138175', 'https://openalex.org/W4287694131', 'https://openalex.org/W4287374065', 'https://openalex.org/W2936774411', 'https://openalex.org/W3035390927', 'https://openalex.org/W4394642489', 'https://openalex.org/W3198429080', 'https://openalex.org/W3099782249', 'https://openalex.org/W4297818305', 'https://openalex.org/W4287075881', 'https://openalex.org/W2127141656', 'https://openalex.org/W2889275521', 'https://openalex.org/W2952809536', 'https://openalex.org/W3037057938', 'https://openalex.org/W3173217749', 'https://openalex.org/W2914120296', 'https://openalex.org/W3012541776', 'https://openalex.org/W3167207712', 'https://openalex.org/W2941814890', 'https://openalex.org/W2991213871', 'https://openalex.org/W3195874849', 'https://openalex.org/W2316579313', 'https://openalex.org/W2988736778', 'https://openalex.org/W3027083471', 'https://openalex.org/W2908336025']",2022-06-28
https://openalex.org/W3049256661,https://doi.org/10.21437/interspeech.2020-1835,Investigating Self-Supervised Pre-Training for End-to-End Speech Translation,"Self-supervised learning from raw speech has been proven beneficial to improve automatic speech recognition (ASR). We investigate here its impact on end-to-end automatic speech translation (AST) performance. We use a contrastive predic-tive coding (CPC) model pre-trained from unlabeled speech as a feature extractor for a downstream AST task. We show that self-supervised pre-training is particularly efficient in low resource settings and that fine-tuning CPC models on the AST training data further improves performance. Even in higher resource settings, ensembling AST models trained with filter-bank and CPC representations leads to near state-of-the-art models without using any ASR pre-training. This might be particularly beneficial when one needs to develop a system that translates from speech in a language with poorly standardized orthography or even from speech in an unwritten language. Index Terms: self-supervised learning from speech, automatic speech translation, end-to-end models, low resource settings.","['https://openalex.org/W1997913719', 'https://openalex.org/W3015213852', 'https://openalex.org/W1494198834', 'https://openalex.org/W2972943112', 'https://openalex.org/W2972495969', 'https://openalex.org/W1686810756', 'https://openalex.org/W2949328740', 'https://openalex.org/W2964308564', 'https://openalex.org/W3043999252', 'https://openalex.org/W2973049979', 'https://openalex.org/W2995181338', 'https://openalex.org/W3005680577', 'https://openalex.org/W4285719527', 'https://openalex.org/W3016181583', 'https://openalex.org/W3016011332', 'https://openalex.org/W2964172053', 'https://openalex.org/W2945700568', 'https://openalex.org/W2964161387', 'https://openalex.org/W2605131327', 'https://openalex.org/W2973157397', 'https://openalex.org/W2899274165', 'https://openalex.org/W3035202887', 'https://openalex.org/W2995233853', 'https://openalex.org/W2890964092', 'https://openalex.org/W2963341956', 'https://openalex.org/W3015703505', 'https://openalex.org/W3127686677', 'https://openalex.org/W2899134946', 'https://openalex.org/W4300558631', 'https://openalex.org/W3102342027', 'https://openalex.org/W1524333225', 'https://openalex.org/W2988736778']",2020-10-25
https://openalex.org/W3110458199,https://doi.org/10.48550/arxiv.2011.11588,The Zero Resource Speech Benchmark 2021: Metrics and baselines for\n unsupervised spoken language modeling,"We introduce a new unsupervised task, spoken language modeling: the learning\nof linguistic representations from raw audio signals without any labels, along\nwith the Zero Resource Speech Benchmark 2021: a suite of 4 black-box, zero-shot\nmetrics probing for the quality of the learned models at 4 linguistic levels:\nphonetics, lexicon, syntax and semantics. We present the results and analyses\nof a composite baseline made of the concatenation of three unsupervised\nsystems: self-supervised contrastive representation learning (CPC), clustering\n(k-means) and language modeling (LSTM or BERT). The language models learn on\nthe basis of the pseudo-text derived from clustering the learned\nrepresentations. This simple pipeline shows better than chance performance on\nall four metrics, demonstrating the feasibility of spoken language modeling\nfrom raw speech. It also yields worse performance compared to text-based\n'topline' systems trained on the same data, delineating the space to be\nexplored by more sophisticated end-to-end models.\n","['https://openalex.org/W2963341956', 'https://openalex.org/W2972447203', 'https://openalex.org/W3099782249', 'https://openalex.org/W2252211741', 'https://openalex.org/W3016011332', 'https://openalex.org/W2137735870', 'https://openalex.org/W2963419157', 'https://openalex.org/W3003875258', 'https://openalex.org/W2963425185', 'https://openalex.org/W2809981375', 'https://openalex.org/W2142625445', 'https://openalex.org/W3034775979', 'https://openalex.org/W2103318667', 'https://openalex.org/W3093096176', 'https://openalex.org/W2973026522', 'https://openalex.org/W2933138175', 'https://openalex.org/W3129289122', 'https://openalex.org/W2395899413', 'https://openalex.org/W2176085882', 'https://openalex.org/W2741692265', 'https://openalex.org/W3102342027', 'https://openalex.org/W2014307400', 'https://openalex.org/W2965373594', 'https://openalex.org/W2080100102', 'https://openalex.org/W2995181338', 'https://openalex.org/W2346964103', 'https://openalex.org/W2251012068', 'https://openalex.org/W2251025892', 'https://openalex.org/W3005511757', 'https://openalex.org/W2963366649', 'https://openalex.org/W2963403868', 'https://openalex.org/W2549835527', 'https://openalex.org/W2593779438', 'https://openalex.org/W2973049979', 'https://openalex.org/W2132631284', 'https://openalex.org/W2026487812', 'https://openalex.org/W2963620343', 'https://openalex.org/W1494198834', 'https://openalex.org/W1854884267', 'https://openalex.org/W2988736778', 'https://openalex.org/W2889947987', 'https://openalex.org/W2996383576', 'https://openalex.org/W2963751529', 'https://openalex.org/W2170682101', 'https://openalex.org/W2996728628', 'https://openalex.org/W2842511635', 'https://openalex.org/W2910243263']",2020-11-23
https://openalex.org/W3160345865,https://doi.org/10.1109/icassp39728.2021.9414539,A Further Study of Unsupervised Pretraining for Transformer Based Speech Recognition,"The construction of an effective good speech recognition system typically requires large amounts of transcribed data, which is expensive to collect. To overcome this problem, many unsupervised pretraining methods have been proposed. Among these methods, Masked Predictive Coding achieved significant improvements on various speech recognition datasets with BERT-like Masked Reconstruction loss and transformer backbone. However, many aspects of MPC have yet to be fully investigated. In this paper, we conduct a further study on MPC and focus on three important aspects: the effect of pretraining data speaking style, its extension on streaming model, and strategies for better transferring learned knowledge from pretraining stage to downstream tasks. The experimental results demonstrated that pretraining data with a matching speaking style is more useful on downstream recognition tasks. A unified training objective with APC and MPC provided an 8.46% relative error reduction on the streaming model trained on HKUST. Additionally, the combination of target data adaption and layerwise discriminative training facilitated the knowledge transfer of MPC, which realized 3.99% relative error reduction on AISHELL over a strong baseline.","['https://openalex.org/W2911291251', 'https://openalex.org/W2972818416', 'https://openalex.org/W6726295259', 'https://openalex.org/W2563574619', 'https://openalex.org/W2964303116', 'https://openalex.org/W6682132143', 'https://openalex.org/W2963226322', 'https://openalex.org/W2963939538', 'https://openalex.org/W6603931906', 'https://openalex.org/W1494198834', 'https://openalex.org/W2926827382', 'https://openalex.org/W6769806307', 'https://openalex.org/W3016011332', 'https://openalex.org/W2982223350', 'https://openalex.org/W3003875258', 'https://openalex.org/W6769686700', 'https://openalex.org/W6769238691', 'https://openalex.org/W6777232839', 'https://openalex.org/W6780361010', 'https://openalex.org/W2168961642', 'https://openalex.org/W2902368158', 'https://openalex.org/W6762122294', 'https://openalex.org/W6770514103', 'https://openalex.org/W2935542736', 'https://openalex.org/W2560647685', 'https://openalex.org/W2939710050', 'https://openalex.org/W3016181583', 'https://openalex.org/W6773205534', 'https://openalex.org/W6769196770', 'https://openalex.org/W2973042454', 'https://openalex.org/W2842511635', 'https://openalex.org/W2963026768', 'https://openalex.org/W2980708516', 'https://openalex.org/W2927746189', 'https://openalex.org/W2936774411', 'https://openalex.org/W2327501763', 'https://openalex.org/W6739901393', 'https://openalex.org/W6766673545', 'https://openalex.org/W3015457435', 'https://openalex.org/W2526425061', 'https://openalex.org/W2962742956', 'https://openalex.org/W3096485810', 'https://openalex.org/W3041561163', 'https://openalex.org/W3007328579', 'https://openalex.org/W4385245566', 'https://openalex.org/W4299518610', 'https://openalex.org/W4297808394', 'https://openalex.org/W2149933564', 'https://openalex.org/W2965373594', 'https://openalex.org/W2996383576', 'https://openalex.org/W2945260553', 'https://openalex.org/W2973157397', 'https://openalex.org/W3148040514', 'https://openalex.org/W2988736778', 'https://openalex.org/W2963403868', 'https://openalex.org/W2515741950', 'https://openalex.org/W2972894903', 'https://openalex.org/W2979476256', 'https://openalex.org/W97072897', 'https://openalex.org/W2964067969', 'https://openalex.org/W3102342027', 'https://openalex.org/W2973049979', 'https://openalex.org/W2981991061', 'https://openalex.org/W4288088457', 'https://openalex.org/W2972943112', 'https://openalex.org/W2971274815']",2021-05-13
https://openalex.org/W3211264909,https://doi.org/10.1109/icassp43922.2022.9747699,Tunet: A Block-Online Bandwidth Extension Model Based On Transformers And Self-Supervised Pretraining,We introduce a block-online variant of the temporal feature-wise linear modulation (TFiLM) model to achieve bandwidth extension. The proposed architecture simplifies the UNet backbone of the TFiLM to reduce inference time and employs an efficient transformer at the bottleneck to alleviate performance degradation. We also utilize self-supervised pretraining and data augmentation to enhance the quality of bandwidth extended signals and reduce the sensitivity with respect to downsampling methods. Experiment results on the VCTK dataset show that the proposed method outperforms several recent baselines in both intrusive and non-intrusive metrics. Pretraining and filter augmentation also help stabilize and enhance the overall performance.,"['https://openalex.org/W3102195007', 'https://openalex.org/W6755207826', 'https://openalex.org/W6770514103', 'https://openalex.org/W6783944145', 'https://openalex.org/W6739901393', 'https://openalex.org/W3035414321', 'https://openalex.org/W6773206665', 'https://openalex.org/W2892110446', 'https://openalex.org/W6769767169', 'https://openalex.org/W6639824700', 'https://openalex.org/W2034562896', 'https://openalex.org/W6767367760', 'https://openalex.org/W6741681139', 'https://openalex.org/W3197990672', 'https://openalex.org/W3197334236', 'https://openalex.org/W26348862', 'https://openalex.org/W2914105075', 'https://openalex.org/W6772349387', 'https://openalex.org/W2964058413', 'https://openalex.org/W6748150159', 'https://openalex.org/W6785764544', 'https://openalex.org/W3161480375', 'https://openalex.org/W1901129140', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963952344', 'https://openalex.org/W3003875258', 'https://openalex.org/W4214784181', 'https://openalex.org/W2998572311', 'https://openalex.org/W2787300193', 'https://openalex.org/W2972745527', 'https://openalex.org/W2988736778', 'https://openalex.org/W4385245566', 'https://openalex.org/W2970844204', 'https://openalex.org/W2964176953', 'https://openalex.org/W4287118891', 'https://openalex.org/W3015338123', 'https://openalex.org/W2896457183', 'https://openalex.org/W4309793872', 'https://openalex.org/W2963403868', 'https://openalex.org/W3125056032']",2022-04-27
https://openalex.org/W3096017728,https://doi.org/10.21437/interspeech.2020-2231,Understanding Self-Attention of Self-Supervised Audio Transformers,"Self-supervised Audio Transformers (SAT) enable great success in many downstream speech applications like ASR, but how they work has not been widely explored yet.In this work, we present multiple strategies for the analysis of attention mechanisms in SAT.We categorize attentions into explainable categories, where we discover each category possesses its own unique functionality.We provide a visualization tool for understanding multi-head self-attention, importance ranking strategies for identifying critical attention, and attention refinement techniques to improve model performance.","['https://openalex.org/W2894279617', 'https://openalex.org/W4385245566', 'https://openalex.org/W4288351520', 'https://openalex.org/W2026858810', 'https://openalex.org/W2964089206', 'https://openalex.org/W3021934057', 'https://openalex.org/W2946417913', 'https://openalex.org/W2896457183', 'https://openalex.org/W3173787059', 'https://openalex.org/W2972794572', 'https://openalex.org/W2965373594', 'https://openalex.org/W3096485810', 'https://openalex.org/W2972324944', 'https://openalex.org/W2586482422', 'https://openalex.org/W1494198834', 'https://openalex.org/W2971033911', 'https://openalex.org/W2803005441', 'https://openalex.org/W2972451902', 'https://openalex.org/W2988736778', 'https://openalex.org/W130754613', 'https://openalex.org/W2747874407', 'https://openalex.org/W2979476256', 'https://openalex.org/W2970597249', 'https://openalex.org/W1986410788', 'https://openalex.org/W2982223350', 'https://openalex.org/W2981991061', 'https://openalex.org/W2996428491', 'https://openalex.org/W3015412890', 'https://openalex.org/W2970120757']",2020-10-25
https://openalex.org/W3049723069,https://doi.org/10.21437/interspeech.2020-1212,Jointly Fine-Tuning “BERT-Like” Self Supervised Models to Improve Multimodal Speech Emotion Recognition,"Multimodal emotion recognition from speech is an important area in affective computing. Fusing multiple data modalities and learning representations with limited amounts of labeled data is a challenging task. In this paper, we explore the use of modality-specific ""BERT-like"" pretrained Self Supervised Learning (SSL) architectures to represent both speech and text modalities for the task of multimodal speech emotion recognition. By conducting experiments on three publicly available datasets (IEMOCAP, CMU-MOSEI, and CMU-MOSI), we show that jointly fine-tuning ""BERT-like"" SSL architectures achieve state-of-the-art (SOTA) results. We also evaluate two methods of fusing speech and text modalities and show that a simple fusion mechanism can outperform more complex ones when using SSL models that have similar architectural properties to BERT.","['https://openalex.org/W2980109192', 'https://openalex.org/W2842511635', 'https://openalex.org/W2952509486', 'https://openalex.org/W2146334809', 'https://openalex.org/W2095176743', 'https://openalex.org/W2973049979', 'https://openalex.org/W2962686539', 'https://openalex.org/W2970608575', 'https://openalex.org/W2913939497', 'https://openalex.org/W2972495317', 'https://openalex.org/W3015489952', 'https://openalex.org/W2936173226', 'https://openalex.org/W2947476638', 'https://openalex.org/W2985076077', 'https://openalex.org/W2965373594', 'https://openalex.org/W3007708573', 'https://openalex.org/W2795642794', 'https://openalex.org/W2935542736', 'https://openalex.org/W2985862548', 'https://openalex.org/W2971250720', 'https://openalex.org/W2808359495', 'https://openalex.org/W2963341956', 'https://openalex.org/W2996383576', 'https://openalex.org/W2465534249', 'https://openalex.org/W2971222961', 'https://openalex.org/W2250539671', 'https://openalex.org/W2883409523', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963799213', 'https://openalex.org/W1494198834', 'https://openalex.org/W2339343773', 'https://openalex.org/W2158419735', 'https://openalex.org/W2889325879', 'https://openalex.org/W2963710346', 'https://openalex.org/W2784665486', 'https://openalex.org/W2903538854', 'https://openalex.org/W2988736778']",2020-10-25
https://openalex.org/W4391021811,https://doi.org/10.1109/asru57964.2023.10389642,Av-Data2Vec: Self-Supervised Learning of Audio-Visual Speech Representations with Contextualized Target Representations,"Self-supervision has shown great potential for audio-visual speech recognition by vastly reducing the amount of labeled data required to build good systems. However, existing methods are either not entirely end-to-end or do not train joint representations of both modalities. In this paper, we introduce AV-data2vec which addresses these challenges and builds audio-visual representations based on predicting contextualized representations which has been successful in the uni-modal case. The model uses a shared transformer encoder for both audio and video and can combine both modalities to improve speech recognition. Results on LRS3 show that AV-data2vec consistently outperforms existing methods under all settings with the same amount of data and model size.","['https://openalex.org/W4213171936', 'https://openalex.org/W4321428117', 'https://openalex.org/W6810168380', 'https://openalex.org/W4297841641', 'https://openalex.org/W4297841719', 'https://openalex.org/W6847652939', 'https://openalex.org/W6810007534', 'https://openalex.org/W6839936984', 'https://openalex.org/W2752796333', 'https://openalex.org/W3016011332', 'https://openalex.org/W3097286738', 'https://openalex.org/W3015265920', 'https://openalex.org/W3196919915', 'https://openalex.org/W3198858531', 'https://openalex.org/W6770514103', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W6846807982', 'https://openalex.org/W2890952074', 'https://openalex.org/W3035042697', 'https://openalex.org/W2972756321', 'https://openalex.org/W4307286264', 'https://openalex.org/W3006974783', 'https://openalex.org/W4312638101', 'https://openalex.org/W3199527474', 'https://openalex.org/W3015830103', 'https://openalex.org/W4376481237', 'https://openalex.org/W2194775991', 'https://openalex.org/W2963785710', 'https://openalex.org/W2212242068', 'https://openalex.org/W1677182931', 'https://openalex.org/W6780218876', 'https://openalex.org/W2962826786', 'https://openalex.org/W6754420807', 'https://openalex.org/W2808631503', 'https://openalex.org/W6677618333', 'https://openalex.org/W2933138175', 'https://openalex.org/W2963979492', 'https://openalex.org/W2766219058', 'https://openalex.org/W3203711169', 'https://openalex.org/W4223430326', 'https://openalex.org/W4372260274', 'https://openalex.org/W4372340876', 'https://openalex.org/W4385823207', 'https://openalex.org/W3036601975', 'https://openalex.org/W2988736778', 'https://openalex.org/W4285595742', 'https://openalex.org/W2891205112', 'https://openalex.org/W398859631', 'https://openalex.org/W2115252128', 'https://openalex.org/W2963799213']",2023-12-16
https://openalex.org/W3144173820,https://doi.org/10.21437/interspeech.2021-236,Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training,"Self-supervised learning of speech representations has been a very active research area but most work is focused on a single domain such as read audio books for which there exist large quantities of labeled and unlabeled data. In this paper, we explore more general setups where the domain of the unlabeled data for pre-training data differs from the domain of the labeled data for fine-tuning, which in turn may differ from the test data domain. Our experiments show that using target domain data during pre-training leads to large performance improvements across a variety of setups. On a large-scale competitive setup, we show that pre-training on unlabeled in-domain data reduces the gap between models trained on in-domain and out-of-domain labeled data by 66%-73%. This has obvious practical implications since it is much easier to obtain unlabeled target domain data than labeled data. Moreover, we find that pre-training on multiple domains improves generalization performance on domains not seen during training. Code and models will be made available at https://github.com/pytorch/fairseq.","['https://openalex.org/W2547875792', 'https://openalex.org/W1494198834', 'https://openalex.org/W2758785877', 'https://openalex.org/W2062164080', 'https://openalex.org/W2995929068', 'https://openalex.org/W2617258110', 'https://openalex.org/W2996383576', 'https://openalex.org/W3093502935', 'https://openalex.org/W1992272902', 'https://openalex.org/W2962946733', 'https://openalex.org/W3037057938', 'https://openalex.org/W2964245029', 'https://openalex.org/W1571339265', 'https://openalex.org/W3099782249', 'https://openalex.org/W3015726069', 'https://openalex.org/W2973157397', 'https://openalex.org/W3025165719', 'https://openalex.org/W2134800885', 'https://openalex.org/W3102342027', 'https://openalex.org/W2996159613', 'https://openalex.org/W2972943112', 'https://openalex.org/W2896457183', 'https://openalex.org/W2995181338', 'https://openalex.org/W2842511635', 'https://openalex.org/W2953190524', 'https://openalex.org/W2124509324', 'https://openalex.org/W2973049979', 'https://openalex.org/W3005511757', 'https://openalex.org/W2963403868', 'https://openalex.org/W3119308075', 'https://openalex.org/W2127141656', 'https://openalex.org/W3015349902', 'https://openalex.org/W2988736778', 'https://openalex.org/W3101648800', 'https://openalex.org/W3163464943']",2021-08-27
https://openalex.org/W4392909760,https://doi.org/10.1109/icassp48485.2024.10447751,Towards Universal Speech Discrete Tokens: A Case Study for ASR and TTS,"Self-supervised learning (SSL) proficiency in speech-related tasks has driven research into utilizing discrete tokens for speech tasks like recognition and translation, which offer lower storage requirements and great potential to employ natural language processing techniques. However, these studies, mainly single-task focused, faced challenges like overfitting and performance degradation in speech recognition tasks, often at the cost of sacrificing performance in multi-task scenarios. This study presents a comprehensive comparison and optimization of discrete tokens generated by various leading SSL models in speech recognition and synthesis tasks. We aim to explore the universality of speech discrete tokens across multiple speech tasks. Experimental results demonstrate that discrete tokens achieve comparable results against systems trained on FBank features in speech recognition tasks and outperform mel-spectrogram features in speech synthesis in subjective and objective metrics. These findings suggest that universal discrete tokens have enormous potential in various speech-related tasks. Our work is open-source and publicly available at https://github.com/k2-fsa/icefall.","['https://openalex.org/W4281492411', 'https://openalex.org/W6780218876', 'https://openalex.org/W6769196770', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W4385823092', 'https://openalex.org/W4385823152', 'https://openalex.org/W4381786045', 'https://openalex.org/W3215615641', 'https://openalex.org/W4307323391', 'https://openalex.org/W6770514103', 'https://openalex.org/W6755207826', 'https://openalex.org/W4385822683', 'https://openalex.org/W4226132755', 'https://openalex.org/W6853244311', 'https://openalex.org/W4385823130', 'https://openalex.org/W6853611000', 'https://openalex.org/W3211278025', 'https://openalex.org/W3197580070', 'https://openalex.org/W2936774411', 'https://openalex.org/W3096159803', 'https://openalex.org/W3097777922', 'https://openalex.org/W6778823374', 'https://openalex.org/W1494198834', 'https://openalex.org/W3198694222', 'https://openalex.org/W2963242190', 'https://openalex.org/W2962784628', 'https://openalex.org/W4283700324', 'https://openalex.org/W6857062747', 'https://openalex.org/W3015686596', 'https://openalex.org/W4372260432', 'https://openalex.org/W2972359262', 'https://openalex.org/W6853515095', 'https://openalex.org/W4380551955', 'https://openalex.org/W4387799863', 'https://openalex.org/W4380714544', 'https://openalex.org/W2988736778', 'https://openalex.org/W4378501656']",2024-03-18
https://openalex.org/W3114436296,https://doi.org/10.18653/v1/2021.acl-long.411,Text-Free Image-to-Speech Synthesis Using Learned Segmental Units,"In this paper we present the first model for directly synthesizing fluent, natural-sounding spoken audio captions for images that does not require natural language text as an intermediate representation or source of supervision. Instead, we connect the image captioning module and the speech synthesis module with a set of discrete, sub-word speech units that are discovered with a self-supervised visual grounding task. We conduct experiments on the Flickr8k spoken caption dataset in addition to a novel corpus of spoken audio captions collected for the popular MSCOCO dataset, demonstrating that our generated captions also capture diverse visual semantics of the images they describe. We investigate several different intermediate speech representations, and empirically find that the representation must satisfy several important properties to serve as drop-in replacements for text.","['https://openalex.org/W2979476256', 'https://openalex.org/W1499360075', 'https://openalex.org/W2939710050', 'https://openalex.org/W2963799213', 'https://openalex.org/W2988907666', 'https://openalex.org/W2963618559', 'https://openalex.org/W2963568578', 'https://openalex.org/W2926827382', 'https://openalex.org/W2964001192', 'https://openalex.org/W2964121744', 'https://openalex.org/W2962706528', 'https://openalex.org/W2995969307', 'https://openalex.org/W2120605154', 'https://openalex.org/W2953114965', 'https://openalex.org/W2963300588', 'https://openalex.org/W1861492603', 'https://openalex.org/W2788277448', 'https://openalex.org/W2108598243', 'https://openalex.org/W2736900972', 'https://openalex.org/W2842511635', 'https://openalex.org/W2995480165', 'https://openalex.org/W2962862718', 'https://openalex.org/W2973049979', 'https://openalex.org/W2963360726', 'https://openalex.org/W2971709506', 'https://openalex.org/W2996287690', 'https://openalex.org/W2947591107', 'https://openalex.org/W2995680346', 'https://openalex.org/W2575842049', 'https://openalex.org/W2950178297', 'https://openalex.org/W2794490148', 'https://openalex.org/W2964308564', 'https://openalex.org/W2949382160', 'https://openalex.org/W2940544976', 'https://openalex.org/W3039910566', 'https://openalex.org/W2963534259', 'https://openalex.org/W2970351109', 'https://openalex.org/W2962691331', 'https://openalex.org/W2608338293', 'https://openalex.org/W2964243274', 'https://openalex.org/W2532494225', 'https://openalex.org/W2963830550', 'https://openalex.org/W2586148577', 'https://openalex.org/W2154652894', 'https://openalex.org/W2013996527', 'https://openalex.org/W3125709657', 'https://openalex.org/W854541894', 'https://openalex.org/W2988736778', 'https://openalex.org/W3007068036', 'https://openalex.org/W2758849341', 'https://openalex.org/W2805122419', 'https://openalex.org/W2963417023', 'https://openalex.org/W2964069186', 'https://openalex.org/W3003875258', 'https://openalex.org/W2962850167', 'https://openalex.org/W2927673779', 'https://openalex.org/W3045485643', 'https://openalex.org/W2527729766', 'https://openalex.org/W3093845497', 'https://openalex.org/W2787779284', 'https://openalex.org/W2995404354', 'https://openalex.org/W2963804033', 'https://openalex.org/W2963902314', 'https://openalex.org/W2106053110', 'https://openalex.org/W3009205145', 'https://openalex.org/W2506483933', 'https://openalex.org/W2120847449', 'https://openalex.org/W1956340063', 'https://openalex.org/W1514535095', 'https://openalex.org/W2795151422', 'https://openalex.org/W2194775991', 'https://openalex.org/W385555557', 'https://openalex.org/W2996383576', 'https://openalex.org/W2101105183', 'https://openalex.org/W3105148948', 'https://openalex.org/W2736876693', 'https://openalex.org/W2745461083', 'https://openalex.org/W2792995953', 'https://openalex.org/W2156142001', 'https://openalex.org/W2963084599', 'https://openalex.org/W2884607399', 'https://openalex.org/W2989358187', 'https://openalex.org/W2796495654', 'https://openalex.org/W2920166246', 'https://openalex.org/W2937090315', 'https://openalex.org/W2119775030', 'https://openalex.org/W2171361956', 'https://openalex.org/W2134670479', 'https://openalex.org/W1895577753', 'https://openalex.org/W2963283805', 'https://openalex.org/W2963096510', 'https://openalex.org/W2964249784', 'https://openalex.org/W2133459682', 'https://openalex.org/W2556930864', 'https://openalex.org/W2950133079']",2021-01-01
https://openalex.org/W3047866127,https://doi.org/10.48550/arxiv.2008.03687,LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition,"Speech synthesis (text to speech, TTS) and recognition (automatic speech recognition, ASR) are important speech tasks, and require a large amount of text and speech pairs for model training. However, there are more than 6,000 languages in the world and most languages are lack of speech training data, which poses significant challenges when building TTS and ASR systems for extremely low-resource languages. In this paper, we develop LRSpeech, a TTS and ASR system under the extremely low-resource setting, which can support rare languages with low data cost. LRSpeech consists of three key techniques: 1) pre-training on rich-resource languages and fine-tuning on low-resource languages; 2) dual transformation between TTS and ASR to iteratively boost the accuracy of each other; 3) knowledge distillation to customize the TTS model on a high-quality target-speaker voice and improve the ASR model on multiple voices. We conduct experiments on an experimental language (English) and a truly low-resource language (Lithuanian) to verify the effectiveness of LRSpeech. Experimental results show that LRSpeech 1) achieves high quality for TTS in terms of both intelligibility (more than 98% intelligibility rate) and naturalness (above 3.5 mean opinion score (MOS)) of the synthesized speech, which satisfy the requirements for industrial deployment, 2) achieves promising recognition accuracy for ASR, and 3) last but not least, uses extremely low-resource training data. We also conduct comprehensive analyses on LRSpeech with different amounts of data resources, and provide valuable insights and guidances for industrial deployment. We are currently deploying LRSpeech into a commercialized cloud speech service to support TTS on more rare languages.","['https://openalex.org/W2905933322', 'https://openalex.org/W2973034126', 'https://openalex.org/W2963403868', 'https://openalex.org/W1902237438', 'https://openalex.org/W2808201675', 'https://openalex.org/W2103091632', 'https://openalex.org/W2963739817', 'https://openalex.org/W2889028433', 'https://openalex.org/W2963216553', 'https://openalex.org/W3177035667', 'https://openalex.org/W2972677740', 'https://openalex.org/W2945078028', 'https://openalex.org/W2888810455', 'https://openalex.org/W2963609956', 'https://openalex.org/W2988736778', 'https://openalex.org/W3015419784', 'https://openalex.org/W1494198834', 'https://openalex.org/W2755682845', 'https://openalex.org/W2790255275', 'https://openalex.org/W2964541115', 'https://openalex.org/W2964308564', 'https://openalex.org/W2794739275', 'https://openalex.org/W2962799225', 'https://openalex.org/W2327501763', 'https://openalex.org/W2962699523', 'https://openalex.org/W2970730223', 'https://openalex.org/W2165143604', 'https://openalex.org/W2963736842', 'https://openalex.org/W2901937222', 'https://openalex.org/W2973049979', 'https://openalex.org/W2982180741', 'https://openalex.org/W2962824709', 'https://openalex.org/W2493353997', 'https://openalex.org/W2964079874', 'https://openalex.org/W2964243274', 'https://openalex.org/W2981728663', 'https://openalex.org/W2951418500', 'https://openalex.org/W2766812927', 'https://openalex.org/W2963347649', 'https://openalex.org/W1586532344', 'https://openalex.org/W2605287558']",2020-08-09
https://openalex.org/W3198446739,https://doi.org/10.21437/interspeech.2021-1710,Self-Supervised End-to-End ASR for Low Resource L2 Swedish,"Funding Information: This work is part of Digitala project which is funded by the Academy of Finland (grant numbers 322619, 322625, 322965). The computational resources were provided by Aalto ScienceIT. Funding Information: This work is part of Digitala project which is funded by the Academy of Finland (grant numbers 322619, 322625, 322965). The computational resources were provided by Aalto Scien-ceIT. Publisher Copyright: Copyright © 2021 ISCA.","['https://openalex.org/W2896457183', 'https://openalex.org/W2979476256', 'https://openalex.org/W3015419784', 'https://openalex.org/W2990413709', 'https://openalex.org/W3099782249', 'https://openalex.org/W2184343439', 'https://openalex.org/W3030437843', 'https://openalex.org/W2936774411', 'https://openalex.org/W2292087804', 'https://openalex.org/W3036601975', 'https://openalex.org/W1979651826', 'https://openalex.org/W3015810689', 'https://openalex.org/W2291975472', 'https://openalex.org/W1614298861', 'https://openalex.org/W3119308075', 'https://openalex.org/W2519224033', 'https://openalex.org/W2963341956', 'https://openalex.org/W2973049979', 'https://openalex.org/W1524333225', 'https://openalex.org/W3095410713', 'https://openalex.org/W2317802258', 'https://openalex.org/W3198429080', 'https://openalex.org/W2950577311', 'https://openalex.org/W2988736778', 'https://openalex.org/W2963217176', 'https://openalex.org/W2981536480', 'https://openalex.org/W2963540679', 'https://openalex.org/W3106470464', 'https://openalex.org/W4285719527', 'https://openalex.org/W3019581400', 'https://openalex.org/W2940578750', 'https://openalex.org/W3016168927']",2021-08-27
https://openalex.org/W3184415155,https://doi.org/10.1109/taslp.2021.3098764,Guided Generative Adversarial Neural Network for Representation Learning and Audio Generation Using Fewer Labelled Audio Data,"The Generation power of Generative Adversarial Neural Networks (GANs) has shown great promise to learn representations from unlabelled data while guided by a small amount of labelled data. We aim to utilise the generation power of GANs to learn Audio Representations. Most existing studies are, however, focused on images. Some studies use GANs for speech generation, but they are conditioned on text or acoustic features, limiting their use for other audio, such as instruments, and even for speech where transcripts are limited. This paper proposes a novel GAN-based model that we named Guided Generative Adversarial Neural Network (GGAN), which can learn powerful representations and generate good-quality samples using a small amount of labelled data as guidance. Experimental results based on a speech [Speech Command Dataset (S09)] and a non-speech [Musical Instrument Sound dataset (Nsyth)] dataset demonstrate that using only 5\% of labelled data as guidance, GGAN learns significantly better representations than the state-of-the-art models.","['https://openalex.org/W2078179989', 'https://openalex.org/W2097117768', 'https://openalex.org/W2160473997', 'https://openalex.org/W2884805522', 'https://openalex.org/W3095118468', 'https://openalex.org/W2108598243', 'https://openalex.org/W2962927978', 'https://openalex.org/W6745245109', 'https://openalex.org/W6757068045', 'https://openalex.org/W2962896155', 'https://openalex.org/W3095950035', 'https://openalex.org/W2951974815', 'https://openalex.org/W2996889020', 'https://openalex.org/W2295707189', 'https://openalex.org/W6701655646', 'https://openalex.org/W3100270690', 'https://openalex.org/W3138521398', 'https://openalex.org/W2618553051', 'https://openalex.org/W6743338426', 'https://openalex.org/W2963306805', 'https://openalex.org/W6715501732', 'https://openalex.org/W6718140377', 'https://openalex.org/W2972667718', 'https://openalex.org/W6750665317', 'https://openalex.org/W2902070858', 'https://openalex.org/W1494198834', 'https://openalex.org/W2260602646', 'https://openalex.org/W6736723571', 'https://openalex.org/W6718379498', 'https://openalex.org/W6765779288', 'https://openalex.org/W6747733185', 'https://openalex.org/W6745560452', 'https://openalex.org/W2964243274', 'https://openalex.org/W2120847449', 'https://openalex.org/W6756197946', 'https://openalex.org/W2751205669', 'https://openalex.org/W6732429163', 'https://openalex.org/W2784918340', 'https://openalex.org/W2951310434', 'https://openalex.org/W7043259713', 'https://openalex.org/W2561826558', 'https://openalex.org/W6685777725', 'https://openalex.org/W6787972765', 'https://openalex.org/W1834627138', 'https://openalex.org/W6703116779', 'https://openalex.org/W2963480200', 'https://openalex.org/W2962914040', 'https://openalex.org/W2963569749', 'https://openalex.org/W6640963894', 'https://openalex.org/W6758708508', 'https://openalex.org/W2948467580', 'https://openalex.org/W6685352114', 'https://openalex.org/W6759880871', 'https://openalex.org/W6756663807', 'https://openalex.org/W6767111847', 'https://openalex.org/W3144035034', 'https://openalex.org/W6843673214', 'https://openalex.org/W6753855596', 'https://openalex.org/W2963609956', 'https://openalex.org/W6755312952', 'https://openalex.org/W2962770929', 'https://openalex.org/W3015338123', 'https://openalex.org/W6765052341', 'https://openalex.org/W6755257315', 'https://openalex.org/W2975414524', 'https://openalex.org/W6772230580', 'https://openalex.org/W3102342027', 'https://openalex.org/W2973049979', 'https://openalex.org/W6770514103', 'https://openalex.org/W3016181583', 'https://openalex.org/W6844194202', 'https://openalex.org/W6747899497', 'https://openalex.org/W3015734344', 'https://openalex.org/W2982039329', 'https://openalex.org/W2749812777', 'https://openalex.org/W2326925005', 'https://openalex.org/W2519091744', 'https://openalex.org/W2173520492', 'https://openalex.org/W2782980316', 'https://openalex.org/W2963684088', 'https://openalex.org/W4301206121', 'https://openalex.org/W2903538854', 'https://openalex.org/W2178768799', 'https://openalex.org/W2962760235', 'https://openalex.org/W3118608800', 'https://openalex.org/W4294619240', 'https://openalex.org/W4294643831', 'https://openalex.org/W2970006822', 'https://openalex.org/W2906424389', 'https://openalex.org/W2963237661', 'https://openalex.org/W2988736778', 'https://openalex.org/W2963226019', 'https://openalex.org/W3122318757', 'https://openalex.org/W2963373786', 'https://openalex.org/W4297808394', 'https://openalex.org/W3104557543', 'https://openalex.org/W2335728318', 'https://openalex.org/W2797583228', 'https://openalex.org/W3104866538', 'https://openalex.org/W2996286887', 'https://openalex.org/W2763421725', 'https://openalex.org/W2963250052', 'https://openalex.org/W2963975282', 'https://openalex.org/W2963782041', 'https://openalex.org/W2963981733', 'https://openalex.org/W2842511635', 'https://openalex.org/W2920684403', 'https://openalex.org/W2963041308', 'https://openalex.org/W2099471712', 'https://openalex.org/W2950662112', 'https://openalex.org/W4294568686', 'https://openalex.org/W2998249245', 'https://openalex.org/W1959608418', 'https://openalex.org/W2412320034', 'https://openalex.org/W2953469440', 'https://openalex.org/W2187089797', 'https://openalex.org/W2584032004', 'https://openalex.org/W2606176153', 'https://openalex.org/W2785325870', 'https://openalex.org/W2901997113', 'https://openalex.org/W2952716587', 'https://openalex.org/W3125709657', 'https://openalex.org/W4297817572', 'https://openalex.org/W2894295011', 'https://openalex.org/W4320013936', 'https://openalex.org/W2893749619', 'https://openalex.org/W3096891716', 'https://openalex.org/W2970241862', 'https://openalex.org/W4288594364', 'https://openalex.org/W2970893544']",2021-01-01
https://openalex.org/W4285145418,https://doi.org/10.18653/v1/2022.acl-long.542,Learning Adaptive Segmentation Policy for End-to-End Simultaneous Translation,"End-to-end simultaneous speech-to-text translation aims to directly perform translation from streaming source speech to target text with high translation quality and low latency. A typical simultaneous translation (ST) system consists of a speech translation model and a policy module, which determines when to wait and when to translate. Thus the policy is crucial to balance translation quality and latency. Conventional methods usually adopt fixed policies, e.g. segmenting the source speech with a fixed length and generating translation. However, this method ignores contextual information and suffers from low translation quality. This paper proposes an adaptive segmentation policy for end-to-end ST. Inspired by human interpreters, the policy learns to segment the source streaming speech into meaningful units by considering both acoustic features and translation history, maintaining consistency between the segmentation and translation. Experimental results on English-German and Chinese-English show that our method achieves a good accuracy-latency trade-off over recently proposed state-of-the-art methods.","['https://openalex.org/W3092424727', 'https://openalex.org/W2975711469', 'https://openalex.org/W3105626768', 'https://openalex.org/W2964172053', 'https://openalex.org/W2896457183', 'https://openalex.org/W3161781432', 'https://openalex.org/W2990461750', 'https://openalex.org/W2950613790', 'https://openalex.org/W2988736778', 'https://openalex.org/W3162000275', 'https://openalex.org/W3034586846', 'https://openalex.org/W2945700568', 'https://openalex.org/W3186200218', 'https://openalex.org/W3037469336', 'https://openalex.org/W2963532001', 'https://openalex.org/W2193413348', 'https://openalex.org/W3021515889', 'https://openalex.org/W2529548870', 'https://openalex.org/W3015927303', 'https://openalex.org/W3115075512', 'https://openalex.org/W2419292002', 'https://openalex.org/W3037337508', 'https://openalex.org/W3094002217', 'https://openalex.org/W2963242190', 'https://openalex.org/W2941814890', 'https://openalex.org/W2952992734', 'https://openalex.org/W3036601975', 'https://openalex.org/W3174501695', 'https://openalex.org/W3015974384', 'https://openalex.org/W2964078338', 'https://openalex.org/W2121457870', 'https://openalex.org/W3173767661', 'https://openalex.org/W3168461629', 'https://openalex.org/W3166908602', 'https://openalex.org/W2899663614', 'https://openalex.org/W2978099976', 'https://openalex.org/W2896234185', 'https://openalex.org/W4287694131', 'https://openalex.org/W4385245566', 'https://openalex.org/W3037698816', 'https://openalex.org/W2949328740', 'https://openalex.org/W3174032041', 'https://openalex.org/W2964089206']",2022-01-01
https://openalex.org/W3161005563,https://doi.org/10.1109/icassp39728.2021.9414970,Bi-APC: Bidirectional Autoregressive Predictive Coding for Unsupervised Pre-Training and its Application to Children’s ASR,"We present a bidirectional unsupervised model pre-training (UPT) method and apply it to children's automatic speech recognition (ASR). An obstacle to improving child ASR is the scarcity of child speech databases. A common approach to alleviate this problem is model pre-training using data from adult speech. Pre-training can be done using supervised (SPT) or unsupervised methods, depending on the availability of annotations. Typically, SPT performs better. In this paper, we focus on UPT to address the situations when pre-training data are unlabeled. Autoregressive predictive coding (APC), a UPT method, predicts frames from only one direction, limiting its use to uni-directional pre-training. Conventional bidirectional UPT methods, however, predict only a small portion of frames. To extend the benefits of APC to bi-directional pre-training, Bi-APC is proposed. We then use adaptation techniques to transfer knowledge learned from adult speech (using the Librispeech corpus) to child speech (OGI Kids corpus). LSTM-based hybrid systems are investigated. For the uni-LSTM structure, APC obtains similar WER improvements to SPT over the baseline. When applied to BLSTM, however, APC is not as competitive as SPT, but our proposed Bi-APC has comparable improvements to SPT.","['https://openalex.org/W2972943112', 'https://openalex.org/W3097053302', 'https://openalex.org/W2842511635', 'https://openalex.org/W6769238691', 'https://openalex.org/W6769593479', 'https://openalex.org/W3096485810', 'https://openalex.org/W6770514103', 'https://openalex.org/W6773206665', 'https://openalex.org/W2471933213', 'https://openalex.org/W6697215464', 'https://openalex.org/W2889212027', 'https://openalex.org/W6615059388', 'https://openalex.org/W6762122294', 'https://openalex.org/W2972560782', 'https://openalex.org/W3007928779', 'https://openalex.org/W2025198378', 'https://openalex.org/W1994888226', 'https://openalex.org/W2593216051', 'https://openalex.org/W2972725946', 'https://openalex.org/W6631362777', 'https://openalex.org/W1494198834', 'https://openalex.org/W6765757276', 'https://openalex.org/W2981857663', 'https://openalex.org/W3097973766', 'https://openalex.org/W3003875258', 'https://openalex.org/W2792175982', 'https://openalex.org/W1524333225', 'https://openalex.org/W2960140743', 'https://openalex.org/W2962732406', 'https://openalex.org/W434092020', 'https://openalex.org/W2988736778', 'https://openalex.org/W2945260553', 'https://openalex.org/W4297808394', 'https://openalex.org/W17704661', 'https://openalex.org/W2295983515', 'https://openalex.org/W2982223350', 'https://openalex.org/W2981991061', 'https://openalex.org/W2971274815']",2021-05-13
https://openalex.org/W4293691212,https://doi.org/10.1109/hsi55341.2022.9869453,ScSer: Supervised Contrastive Learning for Speech Emotion Recognition using Transformers,"Emotion recognition from the speech is a key challenging task and an active area of research in effective Human-Computer Interaction (HCI). Though many deep learning and machine learning approaches have been proposed to tackle the problem, they lack in both accuracy and learning robust representations agnostic to changes in voice. Additionally, there is a lack of sufficient labelled speech data for bigger models. To overcome these issues, we propose supervised contrastive learning with transformers for the task of speech emotion recognition (ScSer) and evaluate it on different standard datasets. Further, we experiment the supervised contrastive setting with different augmentations from WavAugment library and some custom augmentations. Finally, we propose a custom augmentation random cyclic shift with which ScSer outperforms other competitive methods and produce a state of the art accuracy of 96% on RAVDESS dataset with 7600 samples (Big-Ravdess) and a 2-4% boost over other wav2vec methods.","['https://openalex.org/W2750666523', 'https://openalex.org/W6780483730', 'https://openalex.org/W6776700526', 'https://openalex.org/W6750364050', 'https://openalex.org/W2803193013', 'https://openalex.org/W3213879871', 'https://openalex.org/W6846678071', 'https://openalex.org/W3197642003', 'https://openalex.org/W2061900096', 'https://openalex.org/W6780218876', 'https://openalex.org/W6769196770', 'https://openalex.org/W175750906', 'https://openalex.org/W6778883912', 'https://openalex.org/W3035524453', 'https://openalex.org/W6770514103', 'https://openalex.org/W3209059054', 'https://openalex.org/W6753049143', 'https://openalex.org/W2973049979', 'https://openalex.org/W2130162821', 'https://openalex.org/W2070960031', 'https://openalex.org/W2777468850', 'https://openalex.org/W2963710346', 'https://openalex.org/W4287812705', 'https://openalex.org/W2988736778', 'https://openalex.org/W2979476256', 'https://openalex.org/W4320086272', 'https://openalex.org/W4320013936', 'https://openalex.org/W2810914326', 'https://openalex.org/W2963087613', 'https://openalex.org/W2810637193', 'https://openalex.org/W4292779060', 'https://openalex.org/W3144810982', 'https://openalex.org/W3036601975']",2022-07-28
https://openalex.org/W4205377807,https://doi.org/10.1109/access.2021.3133200,Self-Supervised Representation Learning for Document Image Classification,"Supervised learning, despite being extremely effective, relies on expensive, time-consuming, and error-prone annotations. Self-supervised learning has recently emerged as a strong alternate to supervised learning in a range of different domains as collecting a large amount of unlabeled data can be achieved by simply crawling the internet. These self-supervised methods automatically discover features relevant to represent an input example by using self-defined proxy tasks. In this paper, we question the potential of commonly employed purely supervised training (starting either from ImageNet pretrained networks or pure random initialization) in contrast to self-supervised representations that can be learned directly using self-supervised representation learning methods on large document image datasets. For this purpose, we leverage a large-scale document image collection (RVL-CDIP) to train ResNet-50 image encoder using two different self-supervision methods (SimCLR and Barlow Twins). Employing a linear classifier on top of self-supervised embeddings from ResNet-50 results in an accuracy of 86.75&#x0025; as compared to 71.43&#x0025; from the corresponding ImageNet pretrained embeddings. Similarly, evaluating on Tobacco-3482 dataset using self-supervised embeddings from ResNet-50 yields an accuracy of 88.52&#x0025; in contrast to 74.16&#x0025; from the corresponding ImageNet pretrained embeddings. We show that in the case of limited labeled data, this wide gap in performance between self-supervised and fully supervised models persists even after fine-tuning pretrained models. However, a significant reduction in this gap is observed with an increasing amount of data including the case where the model is trained from scratch. Our results show that representations learned using self-supervised representation learning techniques are a viable option for document image classification, specifically in the context of limited labeled data, which is a usual restriction in industrial use cases.","['https://openalex.org/W2944828972', 'https://openalex.org/W6747899497', 'https://openalex.org/W6777265123', 'https://openalex.org/W2842511635', 'https://openalex.org/W6784015424', 'https://openalex.org/W6763442200', 'https://openalex.org/W6701655646', 'https://openalex.org/W2321533354', 'https://openalex.org/W343636949', 'https://openalex.org/W3110446398', 'https://openalex.org/W2964137095', 'https://openalex.org/W6694260854', 'https://openalex.org/W3080523870', 'https://openalex.org/W6778883912', 'https://openalex.org/W2162262658', 'https://openalex.org/W2117539524', 'https://openalex.org/W6791742336', 'https://openalex.org/W1649885719', 'https://openalex.org/W2618530766', 'https://openalex.org/W2962772269', 'https://openalex.org/W2963446712', 'https://openalex.org/W2194775991', 'https://openalex.org/W2769937543', 'https://openalex.org/W6637373629', 'https://openalex.org/W6739575509', 'https://openalex.org/W6734194636', 'https://openalex.org/W2144796873', 'https://openalex.org/W3035524453', 'https://openalex.org/W2098345386', 'https://openalex.org/W2901890385', 'https://openalex.org/W2605976347', 'https://openalex.org/W3004127423', 'https://openalex.org/W3004240503', 'https://openalex.org/W3003981162', 'https://openalex.org/W3035251701', 'https://openalex.org/W2156332201', 'https://openalex.org/W2786162033', 'https://openalex.org/W2493916176', 'https://openalex.org/W6682691769', 'https://openalex.org/W6774314701', 'https://openalex.org/W6755207826', 'https://openalex.org/W6770514103', 'https://openalex.org/W6779326418', 'https://openalex.org/W2515936484', 'https://openalex.org/W6776700526', 'https://openalex.org/W2031408949', 'https://openalex.org/W6630471269', 'https://openalex.org/W2028571532', 'https://openalex.org/W2004665841', 'https://openalex.org/W6608326786', 'https://openalex.org/W2079017068', 'https://openalex.org/W2097624458', 'https://openalex.org/W4294170691', 'https://openalex.org/W4293861706', 'https://openalex.org/W205301110', 'https://openalex.org/W2964350391', 'https://openalex.org/W3108655343', 'https://openalex.org/W3035060554', 'https://openalex.org/W3101483505', 'https://openalex.org/W4287812705', 'https://openalex.org/W2326925005', 'https://openalex.org/W1510710711', 'https://openalex.org/W1686810756', 'https://openalex.org/W4297808394', 'https://openalex.org/W3093122194', 'https://openalex.org/W2896457183', 'https://openalex.org/W3005680577', 'https://openalex.org/W2988736778', 'https://openalex.org/W4292779060', 'https://openalex.org/W3134652006', 'https://openalex.org/W2785325870', 'https://openalex.org/W3022061250', 'https://openalex.org/W2594633041']",2021-01-01
https://openalex.org/W4285257396,https://doi.org/10.18653/v1/2022.iwslt-1.28,ON-TRAC Consortium Systems for the IWSLT 2022 Dialect and Low-resource Speech Translation Tasks,"Marcely Zanon Boito, John Ortega, Hugo Riguidel, Antoine Laurent, Loïc Barrault, Fethi Bougares, Firas Chaabani, Ha Nguyen, Florentin Barbier, Souhir Gahbiche, Yannick Estève. Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022). 2022.","['https://openalex.org/W2933138175', 'https://openalex.org/W3198429080', 'https://openalex.org/W2963077089', 'https://openalex.org/W1524333225', 'https://openalex.org/W3035032094', 'https://openalex.org/W3169320628', 'https://openalex.org/W163811496', 'https://openalex.org/W3167533889', 'https://openalex.org/W1631260214', 'https://openalex.org/W3037217258', 'https://openalex.org/W2949328740', 'https://openalex.org/W3197580070', 'https://openalex.org/W3092424727', 'https://openalex.org/W4288817190', 'https://openalex.org/W4224308495', 'https://openalex.org/W2988736778', 'https://openalex.org/W4385570170', 'https://openalex.org/W3213029956', 'https://openalex.org/W2963250244', 'https://openalex.org/W2605131327', 'https://openalex.org/W3207583491', 'https://openalex.org/W2973049979', 'https://openalex.org/W3107826490', 'https://openalex.org/W3174446152', 'https://openalex.org/W2250976578', 'https://openalex.org/W3102342027', 'https://openalex.org/W3189296823', 'https://openalex.org/W3186200218', 'https://openalex.org/W3105681039', 'https://openalex.org/W602866007', 'https://openalex.org/W2250641567', 'https://openalex.org/W2613904329', 'https://openalex.org/W1902237438', 'https://openalex.org/W2962780374', 'https://openalex.org/W2936774411', 'https://openalex.org/W4385245566', 'https://openalex.org/W3036601975', 'https://openalex.org/W1522301498', 'https://openalex.org/W3173767661', 'https://openalex.org/W4226380987', 'https://openalex.org/W4296068815', 'https://openalex.org/W3097777922']",2022-01-01
https://openalex.org/W4297841287,https://doi.org/10.21437/interspeech.2022-353,A Study of Gender Impact in Self-supervised Models for Speech-to-Text Systems,"Self-supervised models for speech processing emerged recently as popular foundation blocks in speech processing pipelines.These models are pre-trained on unlabeled audio data and then used in speech processing downstream tasks such as automatic speech recognition (ASR) or speech translation (ST).Since these models are now used in research and industrial systems alike, it becomes necessary to understand the impact caused by some features such as gender distribution within pre-training data.Using French as our investigation language, we train and compare gender-specific wav2vec 2.0 models against models containing different degrees of gender balance in their pretraining data.The comparison is performed by applying these models to two speech-to-text downstream tasks: ASR and ST.Results show the type of downstream integration matters.We observe lower overall performance using gender-specific pretraining before fine-tuning an end-to-end ASR system.However, when self-supervised models are used as feature extractors, the overall ASR and ST results follow more complex patterns in which the balanced pre-trained model does not necessarily lead to the best results.Lastly, our crude 'fairness' metric, the relative performance difference measured between female and male test sets, does not display a strong variation from balanced to gender-specific pre-trained wav2vec 2.0 models.","['https://openalex.org/W4285242522', 'https://openalex.org/W2607719644', 'https://openalex.org/W3015935472', 'https://openalex.org/W2329093554', 'https://openalex.org/W2973049979', 'https://openalex.org/W3197771105', 'https://openalex.org/W3169320628', 'https://openalex.org/W2402146185', 'https://openalex.org/W4226380987', 'https://openalex.org/W2047279267', 'https://openalex.org/W4221152851', 'https://openalex.org/W3186528847', 'https://openalex.org/W10987127', 'https://openalex.org/W3092424727', 'https://openalex.org/W2988736778', 'https://openalex.org/W2963532001', 'https://openalex.org/W2888867175', 'https://openalex.org/W3036601975', 'https://openalex.org/W3197580070', 'https://openalex.org/W4298646754', 'https://openalex.org/W2933138175', 'https://openalex.org/W3198429080', 'https://openalex.org/W4287630764', 'https://openalex.org/W4385245566', 'https://openalex.org/W2983036158', 'https://openalex.org/W1524333225', 'https://openalex.org/W3147107444', 'https://openalex.org/W3167533889', 'https://openalex.org/W3189296823', 'https://openalex.org/W2514741789', 'https://openalex.org/W3049256661', 'https://openalex.org/W3213029956', 'https://openalex.org/W4320169525', 'https://openalex.org/W3037217258', 'https://openalex.org/W3207583491', 'https://openalex.org/W2810556878', 'https://openalex.org/W157724941', 'https://openalex.org/W3095410713', 'https://openalex.org/W1522301498', 'https://openalex.org/W3102342027']",2022-09-16
https://openalex.org/W4315645596,https://doi.org/10.18280/isi.270614,Indonesian Automatic Speech Recognition with XLSR-53,"This study focuses on the development of Indonesian Automatic Speech Recognition (ASR) using the XLSR-53 pre-trained model, the XLSR stands for cross-lingual speech representations. The use of this XLSR-53 pre-trained model is to significantly reduce the amount of training data in non-English languages required to achieve a competitive Word Error Rate (WER). The total amount of data used in this study is 24 hours, 18 minutes, and 1 second: (1) TITML-IDN 14 hours and 31 minutes; (2) Magic Data 3 hours and 33 minutes; and (3) Common Voice 6 hours, 14 minutes, and 1 second. With a WER of 20%, the model built in this study can compete with similar models using the Common Voice dataset split test. WER can be decreased by around 8% using a language model, resulting in a WER of 12%. Thus, the results of this study have succeeded in perfecting previous research in contributing to the creation of a better Indonesian ASR with a smaller amount of data.","['https://openalex.org/W4310183467', 'https://openalex.org/W330298975', 'https://openalex.org/W4291810460', 'https://openalex.org/W3198429080', 'https://openalex.org/W2973049979', 'https://openalex.org/W7039088390', 'https://openalex.org/W1549285799', 'https://openalex.org/W2547039119', 'https://openalex.org/W2601477764', 'https://openalex.org/W3030437843', 'https://openalex.org/W2127141656', 'https://openalex.org/W2988736778', 'https://openalex.org/W2134800885', 'https://openalex.org/W2954470341', 'https://openalex.org/W2962619756', 'https://openalex.org/W2896457183', 'https://openalex.org/W2979476256', 'https://openalex.org/W2981991061', 'https://openalex.org/W4297808394', 'https://openalex.org/W3036601975']",2022-12-31
https://openalex.org/W4372347505,https://doi.org/10.1109/icassp49357.2023.10096827,Bridging Speech and Textual Pre-Trained Models With Unsupervised ASR,"Spoken language understanding (SLU) is a task aiming to extract high-level semantics from spoken utterances. Previous works have investigated the use of speech self-supervised models and textual pre-trained models, which have shown reasonable improvements to various SLU tasks. However, because of the mismatched modalities between speech signals and text tokens, previous methods usually need complex designs of the frameworks. This work proposes a simple yet efficient unsupervised paradigm that connects speech and textual pre-trained models, resulting in an unsupervised speech-to-semantic pre-trained model for various tasks in SLU. To be specific, we propose to use unsupervised automatic speech recognition (ASR) as a connector that bridges different modalities used in speech and textual pre-trained models. Our experiments show that unsupervised ASR itself can improve the representations from speech self-supervised models. More importantly, it is shown as an efficient connector between speech and textual pre-trained models, improving the performances of five different SLU tasks. Notably, on spoken question answering, we reach the state-of-the-art result over the challenging NMSQA benchmark.","['https://openalex.org/W4297841398', 'https://openalex.org/W2888302696', 'https://openalex.org/W6810259195', 'https://openalex.org/W2557764419', 'https://openalex.org/W4226162428', 'https://openalex.org/W3164045210', 'https://openalex.org/W6804414493', 'https://openalex.org/W2995181338', 'https://openalex.org/W2146334809', 'https://openalex.org/W3037217258', 'https://openalex.org/W6803092890', 'https://openalex.org/W2963748441', 'https://openalex.org/W3100460087', 'https://openalex.org/W3196509775', 'https://openalex.org/W6780218876', 'https://openalex.org/W6755207826', 'https://openalex.org/W3217767527', 'https://openalex.org/W2766219058', 'https://openalex.org/W3097787369', 'https://openalex.org/W3163793923', 'https://openalex.org/W3148001440', 'https://openalex.org/W4297841571', 'https://openalex.org/W6849880362', 'https://openalex.org/W6849453057', 'https://openalex.org/W6757699909', 'https://openalex.org/W2972706021', 'https://openalex.org/W3016006013', 'https://openalex.org/W4221146627', 'https://openalex.org/W3161223924', 'https://openalex.org/W6795952400', 'https://openalex.org/W3015419784', 'https://openalex.org/W4285250921', 'https://openalex.org/W6770514103', 'https://openalex.org/W3207558756', 'https://openalex.org/W6630875275', 'https://openalex.org/W3197580070', 'https://openalex.org/W4224925047', 'https://openalex.org/W2058094241', 'https://openalex.org/W4221155340', 'https://openalex.org/W2981022124', 'https://openalex.org/W2964079874', 'https://openalex.org/W2988736778', 'https://openalex.org/W4372349162', 'https://openalex.org/W3207222250', 'https://openalex.org/W1514535095', 'https://openalex.org/W2896457183', 'https://openalex.org/W4286849919', 'https://openalex.org/W4287173589', 'https://openalex.org/W3036601975', 'https://openalex.org/W4319862670']",2023-05-05
https://openalex.org/W4392902656,https://doi.org/10.1109/icassp48485.2024.10447296,Loss Masking Is Not Needed In Decoder-Only Transformer For Discrete-Token-Based ASR,"Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for decoder-only Transformers in ASR tasks with different speech discretization methods <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .","['https://openalex.org/W6852800892', 'https://openalex.org/W6850625674', 'https://openalex.org/W4389524500', 'https://openalex.org/W6853611000', 'https://openalex.org/W6853998256', 'https://openalex.org/W6857968694', 'https://openalex.org/W6739901393', 'https://openalex.org/W6769196770', 'https://openalex.org/W6770514103', 'https://openalex.org/W4385822683', 'https://openalex.org/W4319862255', 'https://openalex.org/W3209984917', 'https://openalex.org/W3209059054', 'https://openalex.org/W4307323391', 'https://openalex.org/W6850218400', 'https://openalex.org/W2963979492', 'https://openalex.org/W2962784628', 'https://openalex.org/W2113158412', 'https://openalex.org/W6638523607', 'https://openalex.org/W2183341477', 'https://openalex.org/W1494198834', 'https://openalex.org/W2407080277', 'https://openalex.org/W4323066695', 'https://openalex.org/W4381827575', 'https://openalex.org/W2747329762', 'https://openalex.org/W4387595589', 'https://openalex.org/W4385245566', 'https://openalex.org/W4378501656', 'https://openalex.org/W1821462560', 'https://openalex.org/W4322718191', 'https://openalex.org/W2988736778']",2024-03-18
https://openalex.org/W4287890480,https://doi.org/10.18653/v1/2022.findings-naacl.15,Lacuna Reconstruction: Self-Supervised Pre-Training for Low-Resource Historical Document Transcription,"We present a self-supervised pre-training approach for learning rich visual language representations for both handwritten and printed historical document transcription. After supervised fine-tuning of our pre-trained encoder representations for low-resource document transcription on two languages, (1) a heterogeneous set of handwritten Islamicate manuscript images and (2) early modern English printed documents, we show a meaningful improvement in recognition accuracy over the same supervised model trained from scratch with as few as 30 line image transcriptions for training. Our masked language model-style pre-training strategy, where the model is trained to be able to identify the true masked visual representation from distractors sampled from within the same line, encourages learning robust contextualized language representations invariant to scribal writing style and printing noise present across documents.","['https://openalex.org/W3106274667', 'https://openalex.org/W2988736778', 'https://openalex.org/W2798485145', 'https://openalex.org/W2170942820', 'https://openalex.org/W3003915459', 'https://openalex.org/W3034742054', 'https://openalex.org/W3166396011', 'https://openalex.org/W2291291686', 'https://openalex.org/W2772781956', 'https://openalex.org/W2470345318', 'https://openalex.org/W2933138175', 'https://openalex.org/W3208625967', 'https://openalex.org/W2981991061', 'https://openalex.org/W4214784181', 'https://openalex.org/W2970597249', 'https://openalex.org/W4287824654', 'https://openalex.org/W3096485810', 'https://openalex.org/W4295312788', 'https://openalex.org/W2599351107', 'https://openalex.org/W3036601975', 'https://openalex.org/W2252269695', 'https://openalex.org/W2980063486', 'https://openalex.org/W3106061119', 'https://openalex.org/W2896457183', 'https://openalex.org/W3107720207', 'https://openalex.org/W3108950225', 'https://openalex.org/W2906024304', 'https://openalex.org/W1495007976', 'https://openalex.org/W2946759265', 'https://openalex.org/W2133485975', 'https://openalex.org/W2964057904', 'https://openalex.org/W3160235762', 'https://openalex.org/W2270070752', 'https://openalex.org/W3005680577', 'https://openalex.org/W2963868896', 'https://openalex.org/W4297808394']",2022-01-01
https://openalex.org/W4360989748,https://doi.org/10.20944/preprints202303.0460.v1,An Ensemble Learning based Technique for Bimodal Sentiment Analysis,"Communication is a key method of expressing one's thoughts and opinions. Amongst many modalities, speech and writing are the most powerful and common forms of human communication. Analysing what and how people think has inherently been an interesting and progressive research domain. This includes bimodal sentiment analysis which is an emerging area in natural language processing (NLP) and has received a great deal of attention in recent years in a variety of areas including social opinion mining, health care, banking, and so on. At present, there are limited studies on bimodal conversational sentiment analysis as it proves to be a challenging area given the complex nature of the way humans express sentiment cues across various modalities. To address this gap, a comparison of the performance of multiple data modality models has been conducted on the MELD dataset, a widely-used dataset for benchmarking sentiment analysis within the research community. Our work then demonstrates the results of combining acoustic and linguistic representations. Lastly, our proposed neural network-based ensemble learning technique is employed over six transformer and deep learning-based models, achieving a State-Of-The-Art (SOTA) accuracy.","['https://openalex.org/W1549518189', 'https://openalex.org/W3143835353', 'https://openalex.org/W4360989748', 'https://openalex.org/W6600376255', 'https://openalex.org/W6702248584', 'https://openalex.org/W6600424091', 'https://openalex.org/W6600655081', 'https://openalex.org/W6600721412', 'https://openalex.org/W2183159415', 'https://openalex.org/W6674691379', 'https://openalex.org/W6600234944', 'https://openalex.org/W6685518012', 'https://openalex.org/W6600566992', 'https://openalex.org/W4406406874', 'https://openalex.org/W2747664154', 'https://openalex.org/W2963182768', 'https://openalex.org/W2972463723', 'https://openalex.org/W3036601975', 'https://openalex.org/W2973049979', 'https://openalex.org/W2964010806', 'https://openalex.org/W2923014074', 'https://openalex.org/W2980927909', 'https://openalex.org/W2963686995', 'https://openalex.org/W2295001676', 'https://openalex.org/W2399733683', 'https://openalex.org/W2735532973', 'https://openalex.org/W2985882473', 'https://openalex.org/W2740550900', 'https://openalex.org/W2964051877', 'https://openalex.org/W2980282514', 'https://openalex.org/W1494198834', 'https://openalex.org/W2170938075', 'https://openalex.org/W2143350951', 'https://openalex.org/W2963873807', 'https://openalex.org/W2963647655', 'https://openalex.org/W4289551071', 'https://openalex.org/W2965373594', 'https://openalex.org/W1687157824', 'https://openalex.org/W2962739339', 'https://openalex.org/W2889462515', 'https://openalex.org/W4235305945', 'https://openalex.org/W3209072429', 'https://openalex.org/W3047079185', 'https://openalex.org/W4288009632', 'https://openalex.org/W2584561145', 'https://openalex.org/W2097726431', 'https://openalex.org/W3015489952', 'https://openalex.org/W2566785176', 'https://openalex.org/W2965453734', 'https://openalex.org/W2801842676', 'https://openalex.org/W3014475539', 'https://openalex.org/W99294916', 'https://openalex.org/W3122100772', 'https://openalex.org/W2979476256', 'https://openalex.org/W1614298861', 'https://openalex.org/W2461769152', 'https://openalex.org/W2149940198', 'https://openalex.org/W2787581402', 'https://openalex.org/W2896457183', 'https://openalex.org/W2055911634', 'https://openalex.org/W2898651222', 'https://openalex.org/W2805662932', 'https://openalex.org/W3088631780', 'https://openalex.org/W2577905684', 'https://openalex.org/W2988736778', 'https://openalex.org/W2962770129', 'https://openalex.org/W2141599568', 'https://openalex.org/W3178830508', 'https://openalex.org/W2891359673']",2023-03-27
https://openalex.org/W4226026089,https://doi.org/10.18653/v1/2022.findings-acl.197,Automatic Speech Recognition and Query By Example for Creole Languages Documentation,"We investigate the exploitation of self-supervised models for two Creole languages with few resources: Gwadloupéyen and Morisien. Automatic language processing tools are almost non-existent for these two languages. We propose to use about one hour of annotated data to design an automatic speech recognition system for each language. We evaluate how much data is needed to obtain a query-by-example system that is usable by linguists. Moreover, our experiments show that multilingual self-supervised models are not necessarily the most efficient for Creole languages.","['https://openalex.org/W8373060', 'https://openalex.org/W2002492105', 'https://openalex.org/W2331796052', 'https://openalex.org/W2006036304', 'https://openalex.org/W3036601975', 'https://openalex.org/W3102342027', 'https://openalex.org/W2227167115', 'https://openalex.org/W183032947', 'https://openalex.org/W1968409674', 'https://openalex.org/W2988736778', 'https://openalex.org/W2161278536', 'https://openalex.org/W365339437', 'https://openalex.org/W3198429080', 'https://openalex.org/W1752514392', 'https://openalex.org/W2134800885', 'https://openalex.org/W2087064593', 'https://openalex.org/W3189296823', 'https://openalex.org/W2796874292', 'https://openalex.org/W3207583491', 'https://openalex.org/W2039205693', 'https://openalex.org/W2076048613', 'https://openalex.org/W1633472765', 'https://openalex.org/W2906807996']",2022-01-01
https://openalex.org/W3136219906,https://doi.org/10.48550/arxiv.2103.07762,OkwuGbé: End-to-End Speech Recognition for Fon and Igbo,"Language is inherent and compulsory for human communication. Whether expressed in a written or spoken way, it ensures understanding between people of the same and different regions. With the growing awareness and effort to include more low-resourced languages in NLP research, African languages have recently been a major subject of research in machine translation, and other text-based areas of NLP. However, there is still very little comparable research in speech recognition for African languages. Interestingly, some of the unique properties of African languages affecting NLP, like their diacritical and tonal complexities, have a major root in their speech, suggesting that careful speech interpretation could provide more intuition on how to deal with the linguistic complexities of African languages for text-based NLP. OkwuGbé is a step towards building speech recognition systems for African low-resourced languages. Using Fon and Igbo as our case study, we conduct a comprehensive linguistic analysis of each language and describe the creation of end-to-end, deep neural network-based speech recognition models for both languages. We present a state-of-art ASR model for Fon, as well as benchmark ASR model results for Igbo. Our linguistic analyses (for Fon and Igbo) provide valuable insights and guidance into the creation of speech recognition models for other African low-resourced languages, as well as guide future NLP research for Fon and Igbo. The Fon and Igbo models source code have been made publicly available.","['https://openalex.org/W1861537833', 'https://openalex.org/W3044684492', 'https://openalex.org/W3092791109', 'https://openalex.org/W2949117887', 'https://openalex.org/W3134606166', 'https://openalex.org/W2127141656', 'https://openalex.org/W1922655562', 'https://openalex.org/W2949640717', 'https://openalex.org/W2750167318', 'https://openalex.org/W2061272101', 'https://openalex.org/W2041113956', 'https://openalex.org/W2143612262', 'https://openalex.org/W2973049979', 'https://openalex.org/W3015419784', 'https://openalex.org/W3013122861', 'https://openalex.org/W2901616036', 'https://openalex.org/W2795877110', 'https://openalex.org/W2095705004', 'https://openalex.org/W3035032094', 'https://openalex.org/W2988736778', 'https://openalex.org/W2936774411', 'https://openalex.org/W3149448910', 'https://openalex.org/W2745596852', 'https://openalex.org/W3081416955', 'https://openalex.org/W2970971581', 'https://openalex.org/W2949650786', 'https://openalex.org/W571987451', 'https://openalex.org/W2016095526', 'https://openalex.org/W3177035667', 'https://openalex.org/W3037057938', 'https://openalex.org/W2489997055', 'https://openalex.org/W2327501763', 'https://openalex.org/W2903739847', 'https://openalex.org/W2899663614', 'https://openalex.org/W2734531544', 'https://openalex.org/W3009096638', 'https://openalex.org/W2969945254', 'https://openalex.org/W2103869314', 'https://openalex.org/W2131774270', 'https://openalex.org/W2964308564', 'https://openalex.org/W570440754', 'https://openalex.org/W2908510526', 'https://openalex.org/W1586532344', 'https://openalex.org/W2302255633', 'https://openalex.org/W2963739817', 'https://openalex.org/W3034854768', 'https://openalex.org/W2963500086', 'https://openalex.org/W2125610452', 'https://openalex.org/W1647671624', 'https://openalex.org/W2945078028', 'https://openalex.org/W2524544624']",2021-03-13
https://openalex.org/W4385571192,https://doi.org/10.18653/v1/2023.iwslt-1.18,ON-TRAC Consortium Systems for the IWSLT 2023 Dialectal and Low-resource Speech Translation Tasks,"Antoine Laurent, Souhir Gahbiche, Ha Nguyen, Haroun Elleuch, Fethi Bougares, Antoine Thiol, Hugo Riguidel, Salima Mdhaffar, Gaëlle Laperrière, Lucas Maison, Sameer Khurana, Yannick Estève. Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023). 2023.","['https://openalex.org/W3030437843', 'https://openalex.org/W4288817190', 'https://openalex.org/W4285158119', 'https://openalex.org/W3119308075', 'https://openalex.org/W4286359908', 'https://openalex.org/W4296068815', 'https://openalex.org/W3112092703', 'https://openalex.org/W3105681039', 'https://openalex.org/W3102342027', 'https://openalex.org/W4285077564', 'https://openalex.org/W4288089799', 'https://openalex.org/W3197580070', 'https://openalex.org/W2605131327', 'https://openalex.org/W2988736778', 'https://openalex.org/W4372267440', 'https://openalex.org/W2964303773', 'https://openalex.org/W2989384871', 'https://openalex.org/W2949328740', 'https://openalex.org/W3174446152', 'https://openalex.org/W3207583491', 'https://openalex.org/W4382202628', 'https://openalex.org/W4311000453', 'https://openalex.org/W3039695075', 'https://openalex.org/W4224308495', 'https://openalex.org/W2963077089', 'https://openalex.org/W3032433061', 'https://openalex.org/W3186200218', 'https://openalex.org/W2973049979', 'https://openalex.org/W4291566970', 'https://openalex.org/W3095410713', 'https://openalex.org/W3169320628', 'https://openalex.org/W3198429080', 'https://openalex.org/W3189296823', 'https://openalex.org/W3213029956', 'https://openalex.org/W4385245566', 'https://openalex.org/W4309427880', 'https://openalex.org/W2613904329', 'https://openalex.org/W3036601975']",2023-01-01
https://openalex.org/W3204316016,https://doi.org/10.1109/asru51503.2021.9688061,Comparison of Self-Supervised Speech Pre-Training Methods on Flemish Dutch,"Recent research in speech processing exhibits a growing interest in unsupervised and self-supervised representation learning from unlabelled data to alleviate the need for large amounts of annotated data. We investigate several popular pre-training methods and apply them to Flemish Dutch. We compare off-the-shelf English pre-trained models to models trained on an increasing amount of Flemish data. We find that the most important factors for positive transfer to downstream speech recognition tasks include a substantial amount of data and a matching pre-training domain. Ideally, we also finetune on an annotated subset in the target language. All pre-trained models improve linear phone separability in Flemish, but not all methods improve Automatic Speech Recognition. We experience superior performance with wav2vec 2.0 and we obtain a 30% WER improvement by finetuning the multilingually pre-trained XLSR-53 model on Flemish Dutch, after integration into an HMM-DNN acoustic model.","['https://openalex.org/W2995181338', 'https://openalex.org/W1494198834', 'https://openalex.org/W6631362777', 'https://openalex.org/W3163596720', 'https://openalex.org/W2888867175', 'https://openalex.org/W6601192135', 'https://openalex.org/W6628009067', 'https://openalex.org/W2933138175', 'https://openalex.org/W3097286738', 'https://openalex.org/W3095410713', 'https://openalex.org/W3035202887', 'https://openalex.org/W2982223350', 'https://openalex.org/W3148040514', 'https://openalex.org/W2973049979', 'https://openalex.org/W3102342027', 'https://openalex.org/W2158081297', 'https://openalex.org/W3015265920', 'https://openalex.org/W3016181583', 'https://openalex.org/W3198771897', 'https://openalex.org/W2972943112', 'https://openalex.org/W3162133897', 'https://openalex.org/W3100270690', 'https://openalex.org/W3033038061', 'https://openalex.org/W6696449567', 'https://openalex.org/W3015213852', 'https://openalex.org/W3119308075', 'https://openalex.org/W2973157397', 'https://openalex.org/W2626778328', 'https://openalex.org/W2896457183', 'https://openalex.org/W2842511635', 'https://openalex.org/W3095292526', 'https://openalex.org/W4287173589', 'https://openalex.org/W1524333225', 'https://openalex.org/W2926827382', 'https://openalex.org/W2939710050', 'https://openalex.org/W3125709657', 'https://openalex.org/W2292087804', 'https://openalex.org/W3002741552', 'https://openalex.org/W2996383576', 'https://openalex.org/W3160525311', 'https://openalex.org/W3198608154', 'https://openalex.org/W3198858531', 'https://openalex.org/W2979476256', 'https://openalex.org/W3016011332', 'https://openalex.org/W3030437843', 'https://openalex.org/W1269315465', 'https://openalex.org/W4297808394', 'https://openalex.org/W3198429080', 'https://openalex.org/W3041561163', 'https://openalex.org/W3144173820', 'https://openalex.org/W3099782249', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963341956', 'https://openalex.org/W2988736778', 'https://openalex.org/W3093579165', 'https://openalex.org/W29952999', 'https://openalex.org/W3036601975', 'https://openalex.org/W3037057938', 'https://openalex.org/W3093533780', 'https://openalex.org/W3165666670', 'https://openalex.org/W3096587983']",2021-12-13
https://openalex.org/W4309224965,https://doi.org/10.1109/globconpt57482.2022.9938222,Recognition of Fricative Phoneme based Hindi Words in Speech-to-Text System using Wav2Vec2.0 Model,"In this work, we have discussed issues with Microsoft's state-of-the-art Speech-to-Text (STT) system. Two key issues have been identified: recognition of Hindi words starting with the fricative phoneme (/ha/) and recognition power of the system with background noise. The solution for correctly identifying the unrecognized Hindi fricative phoneme is by training the Wav2Vec2.0 model on the OpenSLR Hindi dataset. The evaluation of the proposed model is given by the performance metric Char-acter Error Rate (CER). To test the performance of the proposed model, 20 fricative words in both clean and noisy conditions are fed to the trained model. The second issue of handling noisy speech samples is resolved using an amplitude-based automatic noise detection method. The results achieved from the proposed model are observed to be better than the state-of-the-art STT model when trained with and without the language model in terms of CER in clean conditions.","['https://openalex.org/W2842511635', 'https://openalex.org/W3102342027', 'https://openalex.org/W2752796333', 'https://openalex.org/W6769313972', 'https://openalex.org/W6770596778', 'https://openalex.org/W6729448088', 'https://openalex.org/W2127141656', 'https://openalex.org/W6770514103', 'https://openalex.org/W6798423582', 'https://openalex.org/W2166754483', 'https://openalex.org/W3202357458', 'https://openalex.org/W6780218876', 'https://openalex.org/W6691779875', 'https://openalex.org/W6769238691', 'https://openalex.org/W1494198834', 'https://openalex.org/W3173737740', 'https://openalex.org/W2132262700', 'https://openalex.org/W3003875258', 'https://openalex.org/W2981991061', 'https://openalex.org/W2250319793', 'https://openalex.org/W2963799213', 'https://openalex.org/W3036601975', 'https://openalex.org/W4288088421', 'https://openalex.org/W4297808394', 'https://openalex.org/W2547875792', 'https://openalex.org/W2988736778', 'https://openalex.org/W4287075881', 'https://openalex.org/W2979476256', 'https://openalex.org/W2995680346']",2022-09-23
https://openalex.org/W3030187020,https://doi.org/10.1109/access.2020.3040797,High-Fidelity Audio Generation and Representation Learning With Guided Adversarial Autoencoder,"Unsupervised disentangled representation learning from the unlabelled audio data, and high fidelity audio generation have become two linchpins in the machine learning research fields. However, the representation learned from an unsupervised setting does not guarantee its' usability for any downstream task at hand, which can be a wastage of the resources, if the training was conducted for that particular posterior job. Also, during the representation learning, if the model is highly biased towards the downstream task, it losses its generalisation capability which directly benefits the downstream job but the ability to scale it to other related task is lost. Therefore, to fill this gap, we propose a new autoencoder based model named ""Guided Adversarial Autoencoder (GAAE)"", which can learn both post-task-specific representations and the general representation capturing the factors of variation in the training data leveraging a small percentage of labelled samples; thus, makes it suitable for future related tasks. Furthermore, our proposed model can generate audio with superior quality, which is indistinguishable from the real audio samples. Hence, with the extensive experimental results, we have demonstrated that by harnessing the power of the high-fidelity audio generation, the proposed GAAE model can learn powerful representation from unlabelled dataset leveraging a fewer percentage of labelled data as supervision/guidance.","['https://openalex.org/W2026653933', 'https://openalex.org/W2108598243', 'https://openalex.org/W2097117768', 'https://openalex.org/W2884805522', 'https://openalex.org/W6745245109', 'https://openalex.org/W6755312952', 'https://openalex.org/W6772230580', 'https://openalex.org/W2078179989', 'https://openalex.org/W6770514103', 'https://openalex.org/W6631190155', 'https://openalex.org/W2936774411', 'https://openalex.org/W2963425185', 'https://openalex.org/W2973049979', 'https://openalex.org/W3015734344', 'https://openalex.org/W2982039329', 'https://openalex.org/W6769196770', 'https://openalex.org/W3016181583', 'https://openalex.org/W6773205534', 'https://openalex.org/W2963571336', 'https://openalex.org/W2178768799', 'https://openalex.org/W6766978945', 'https://openalex.org/W6743338426', 'https://openalex.org/W6747899497', 'https://openalex.org/W6750665317', 'https://openalex.org/W2952938873', 'https://openalex.org/W1494198834', 'https://openalex.org/W6736723571', 'https://openalex.org/W2842511635', 'https://openalex.org/W6758708508', 'https://openalex.org/W6718379498', 'https://openalex.org/W6765779288', 'https://openalex.org/W6747733185', 'https://openalex.org/W6756663807', 'https://openalex.org/W2782663486', 'https://openalex.org/W2948467580', 'https://openalex.org/W2962927978', 'https://openalex.org/W6755207826', 'https://openalex.org/W6757068045', 'https://openalex.org/W6767211374', 'https://openalex.org/W2964046296', 'https://openalex.org/W6743587472', 'https://openalex.org/W2736090641', 'https://openalex.org/W2963480200', 'https://openalex.org/W2260602646', 'https://openalex.org/W2120847449', 'https://openalex.org/W3035574324', 'https://openalex.org/W6715501732', 'https://openalex.org/W6714590955', 'https://openalex.org/W2963609956', 'https://openalex.org/W6756197946', 'https://openalex.org/W6758675244', 'https://openalex.org/W6755257315', 'https://openalex.org/W7043259713', 'https://openalex.org/W3009716751', 'https://openalex.org/W6759880871', 'https://openalex.org/W6680067488', 'https://openalex.org/W2326925005', 'https://openalex.org/W2308529009', 'https://openalex.org/W343636949', 'https://openalex.org/W6745560452', 'https://openalex.org/W6718140377', 'https://openalex.org/W6685352114', 'https://openalex.org/W6765052341', 'https://openalex.org/W3100270690', 'https://openalex.org/W6780248173', 'https://openalex.org/W6738560588', 'https://openalex.org/W2962914040', 'https://openalex.org/W2962770929', 'https://openalex.org/W2907262790', 'https://openalex.org/W2963364041', 'https://openalex.org/W2963569749', 'https://openalex.org/W6640963894', 'https://openalex.org/W2561826558', 'https://openalex.org/W6676071220', 'https://openalex.org/W6689606951', 'https://openalex.org/W2936451900', 'https://openalex.org/W66627554', 'https://openalex.org/W2963684088', 'https://openalex.org/W2963265008', 'https://openalex.org/W2749812777', 'https://openalex.org/W2293255527', 'https://openalex.org/W2253429366', 'https://openalex.org/W2963981733', 'https://openalex.org/W1556219185', 'https://openalex.org/W2951001854', 'https://openalex.org/W2920684403', 'https://openalex.org/W2930753566', 'https://openalex.org/W2950230678', 'https://openalex.org/W2910577860', 'https://openalex.org/W2803098682', 'https://openalex.org/W2962686539', 'https://openalex.org/W2893749619', 'https://openalex.org/W2962756039', 'https://openalex.org/W2988736778', 'https://openalex.org/W2782980316', 'https://openalex.org/W2593116425', 'https://openalex.org/W2963022647', 'https://openalex.org/W2979476256', 'https://openalex.org/W3003382064', 'https://openalex.org/W2906424389', 'https://openalex.org/W2170607218', 'https://openalex.org/W2963226019', 'https://openalex.org/W2099471712', 'https://openalex.org/W2995460200', 'https://openalex.org/W2797583228', 'https://openalex.org/W2970241862', 'https://openalex.org/W2411541852', 'https://openalex.org/W2107789863', 'https://openalex.org/W2625917632', 'https://openalex.org/W2914132458', 'https://openalex.org/W2232901134', 'https://openalex.org/W2134842679', 'https://openalex.org/W2964121744', 'https://openalex.org/W2187089797', 'https://openalex.org/W2963373786', 'https://openalex.org/W2963341956', 'https://openalex.org/W3125709657', 'https://openalex.org/W2951535099', 'https://openalex.org/W2622563070', 'https://openalex.org/W2794753807', 'https://openalex.org/W2608207374', 'https://openalex.org/W2998249245', 'https://openalex.org/W2901997113', 'https://openalex.org/W2962910554', 'https://openalex.org/W2962742544', 'https://openalex.org/W2511956680', 'https://openalex.org/W2274287116', 'https://openalex.org/W2995327724', 'https://openalex.org/W2951004968', 'https://openalex.org/W2970971581', 'https://openalex.org/W2963237661', 'https://openalex.org/W3012640291', 'https://openalex.org/W2763421725', 'https://openalex.org/W2510153535', 'https://openalex.org/W2903538854', 'https://openalex.org/W3038022805', 'https://openalex.org/W2894295011']",2020-01-01
https://openalex.org/W3153492756,https://doi.org/10.1109/icassp39728.2021.9413375,Eat: Enhanced ASR-TTS for Self-Supervised Speech Recognition,"Self-supervised ASR-TTS models suffer in out-of-domain data conditions. Here we propose an enhanced ASR-TTS (EAT) model that incorporates two main features: 1) The ASR$\rightarrow$TTS direction is equipped with a language model reward to penalize the ASR hypotheses before forwarding it to TTS. 2) In the TTS$\rightarrow$ASR direction, a hyper-parameter is introduced to scale the attention context from synthesized speech before sending it to ASR to handle out-of-domain data. Training strategies and the effectiveness of the EAT model are explored under out-of-domain data conditions. The results show that EAT reduces the performance gap between supervised and self-supervised training significantly by absolute 2.6\% and 2.7\% on Librispeech and BABEL respectively.","['https://openalex.org/W3015522062', 'https://openalex.org/W6770514103', 'https://openalex.org/W6780218876', 'https://openalex.org/W3015737168', 'https://openalex.org/W6640963894', 'https://openalex.org/W2119717200', 'https://openalex.org/W2972818416', 'https://openalex.org/W3016160783', 'https://openalex.org/W2903739847', 'https://openalex.org/W2890964092', 'https://openalex.org/W2972889948', 'https://openalex.org/W2963739817', 'https://openalex.org/W6755639519', 'https://openalex.org/W6753186555', 'https://openalex.org/W3015440759', 'https://openalex.org/W3015449694', 'https://openalex.org/W2962699523', 'https://openalex.org/W3015654635', 'https://openalex.org/W6679434410', 'https://openalex.org/W2936774411', 'https://openalex.org/W1494198834', 'https://openalex.org/W6765939562', 'https://openalex.org/W6770506093', 'https://openalex.org/W2586412610', 'https://openalex.org/W3015280134', 'https://openalex.org/W6772883055', 'https://openalex.org/W2898974134', 'https://openalex.org/W2991213871', 'https://openalex.org/W2964308564', 'https://openalex.org/W2998532468', 'https://openalex.org/W2988736778', 'https://openalex.org/W2962369866', 'https://openalex.org/W3036601975', 'https://openalex.org/W3099782249', 'https://openalex.org/W2943008652', 'https://openalex.org/W2883586237', 'https://openalex.org/W2951004968']",2021-05-13
https://openalex.org/W3036601975,https://doi.org/10.48550/arxiv.2006.11477,wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations,"We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.","['https://openalex.org/W273093436', 'https://openalex.org/W3005680577', 'https://openalex.org/W3107298252', 'https://openalex.org/W3016181583', 'https://openalex.org/W2908336025', 'https://openalex.org/W2936295285', 'https://openalex.org/W2973049979', 'https://openalex.org/W2953190524', 'https://openalex.org/W2941814890', 'https://openalex.org/W2962901777', 'https://openalex.org/W2971155163', 'https://openalex.org/W2936774411', 'https://openalex.org/W3025165719', 'https://openalex.org/W2127141656', 'https://openalex.org/W2996383576', 'https://openalex.org/W2995181338', 'https://openalex.org/W2996159613', 'https://openalex.org/W3004728855', 'https://openalex.org/W2124509324', 'https://openalex.org/W2995680346', 'https://openalex.org/W10548402', 'https://openalex.org/W3026041220', 'https://openalex.org/W2896457183', 'https://openalex.org/W3103005696', 'https://openalex.org/W2121879602', 'https://openalex.org/W2794209590', 'https://openalex.org/W2991213871', 'https://openalex.org/W2944828972', 'https://openalex.org/W2949892913', 'https://openalex.org/W2994536315', 'https://openalex.org/W3021469861', 'https://openalex.org/W2152790380', 'https://openalex.org/W2964121744', 'https://openalex.org/W2547875792', 'https://openalex.org/W2962942158', 'https://openalex.org/W3127686677', 'https://openalex.org/W2963799213', 'https://openalex.org/W3003875258', 'https://openalex.org/W2963807318', 'https://openalex.org/W3027083471', 'https://openalex.org/W2899663614', 'https://openalex.org/W2296701362', 'https://openalex.org/W2965373594', 'https://openalex.org/W2963403868', 'https://openalex.org/W3002741552', 'https://openalex.org/W2981991061', 'https://openalex.org/W2962739339', 'https://openalex.org/W2952509486', 'https://openalex.org/W2988736778', 'https://openalex.org/W3035524453', 'https://openalex.org/W3037932933', 'https://openalex.org/W2963631907', 'https://openalex.org/W2842511635', 'https://openalex.org/W1494198834', 'https://openalex.org/W3015419784', 'https://openalex.org/W2972943112', 'https://openalex.org/W2972374322']",2020-06-20
https://openalex.org/W3180663620,https://doi.org/10.48550/arxiv.2107.05677,Codified audio language modeling learns useful representations for music information retrieval,"We demonstrate that language models pre-trained on codified (discretely-encoded) music audio learn representations that are useful for downstream MIR tasks. Specifically, we explore representations from Jukebox (Dhariwal et al. 2020): a music generation system containing a language model trained on codified audio from 1M songs. To determine if Jukebox's representations contain useful information for MIR, we use them as input features to train shallow models on several MIR tasks. Relative to representations from conventional MIR models which are pre-trained on tagging, we find that using representations from Jukebox as input features yields 30% stronger performance on average across four MIR tasks: tagging, genre classification, emotion recognition, and key detection. For key detection, we observe that representations from Jukebox are considerably stronger than those from models pre-trained on tagging, suggesting that pre-training via codified audio language modeling may address blind spots in conventional approaches. We interpret the strength of Jukebox's representations as evidence that modeling audio instead of tags provides richer representations for MIR.","['https://openalex.org/W1556219185', 'https://openalex.org/W2191779130', 'https://openalex.org/W2626778328', 'https://openalex.org/W2133824856', 'https://openalex.org/W2962942158', 'https://openalex.org/W2019360207', 'https://openalex.org/W1960352584', 'https://openalex.org/W3036601975', 'https://openalex.org/W2952502547', 'https://openalex.org/W3000400453', 'https://openalex.org/W3102568015', 'https://openalex.org/W3021164770', 'https://openalex.org/W3139080614', 'https://openalex.org/W2963341956', 'https://openalex.org/W3035137491', 'https://openalex.org/W2940744433', 'https://openalex.org/W2946359678', 'https://openalex.org/W2964274466', 'https://openalex.org/W2765119701', 'https://openalex.org/W3099782249', 'https://openalex.org/W2955142818', 'https://openalex.org/W2963430224', 'https://openalex.org/W2765325162', 'https://openalex.org/W2962739339', 'https://openalex.org/W3155776249', 'https://openalex.org/W2964307104', 'https://openalex.org/W3118485687', 'https://openalex.org/W3095513901', 'https://openalex.org/W2962904371', 'https://openalex.org/W2296751288', 'https://openalex.org/W3143852976', 'https://openalex.org/W2293255527', 'https://openalex.org/W3093494400', 'https://openalex.org/W2991108091', 'https://openalex.org/W2949382160', 'https://openalex.org/W2295991281', 'https://openalex.org/W2988736778', 'https://openalex.org/W3001279689', 'https://openalex.org/W2136129419', 'https://openalex.org/W335809467', 'https://openalex.org/W2137619888', 'https://openalex.org/W2990244497', 'https://openalex.org/W3034978746', 'https://openalex.org/W1849277567', 'https://openalex.org/W3029858316', 'https://openalex.org/W2794150026', 'https://openalex.org/W2964204621', 'https://openalex.org/W2964051853', 'https://openalex.org/W2407685581', 'https://openalex.org/W1989445502', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963550089', 'https://openalex.org/W3034445277', 'https://openalex.org/W2922565841', 'https://openalex.org/W3139211892', 'https://openalex.org/W3010903955', 'https://openalex.org/W2127870748', 'https://openalex.org/W2584032004', 'https://openalex.org/W2896197082', 'https://openalex.org/W2906658932', 'https://openalex.org/W2023001347']",2021-07-12
https://openalex.org/W3026957705,https://doi.org/10.48550/arxiv.2005.09862,A Further Study of Unsupervised Pre-training for Transformer Based Speech Recognition,"Building a good speech recognition system usually requires large amounts of transcribed data, which is expensive to collect. To tackle this problem, many unsupervised pre-training methods have been proposed. Among these methods, Masked Predictive Coding achieved significant improvements on various speech recognition datasets with BERT-like Masked Reconstruction loss and Transformer backbone. However, many aspects of MPC have not been fully investigated. In this paper, we conduct a further study on MPC and focus on three important aspects: the effect of pre-training data speaking style, its extension on streaming model, and how to better transfer learned knowledge from pre-training stage to downstream tasks. Experiments reveled that pre-training data with matching speaking style is more useful on downstream recognition tasks. A unified training objective with APC and MPC provided 8.46% relative error reduction on streaming model trained on HKUST. Also, the combination of target data adaption and layer-wise discriminative training helped the knowledge transfer of MPC, which achieved 3.99% relative error reduction on AISHELL over a strong baseline.","['https://openalex.org/W2962742956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2563574619', 'https://openalex.org/W343636949', 'https://openalex.org/W2963341956', 'https://openalex.org/W2842511635', 'https://openalex.org/W2936774411', 'https://openalex.org/W2963226322', 'https://openalex.org/W3035202887', 'https://openalex.org/W2981687423', 'https://openalex.org/W2168961642', 'https://openalex.org/W2996383576', 'https://openalex.org/W2911291251', 'https://openalex.org/W2972818416', 'https://openalex.org/W2927746189', 'https://openalex.org/W2327501763', 'https://openalex.org/W2972894903', 'https://openalex.org/W2982223350', 'https://openalex.org/W2973049979', 'https://openalex.org/W3003382064', 'https://openalex.org/W2560647685', 'https://openalex.org/W2981991061', 'https://openalex.org/W2981363336', 'https://openalex.org/W2113839990', 'https://openalex.org/W2526425061', 'https://openalex.org/W2949667497', 'https://openalex.org/W2971274815', 'https://openalex.org/W2922565841', 'https://openalex.org/W1494198834', 'https://openalex.org/W3003875258', 'https://openalex.org/W3005511757', 'https://openalex.org/W2988736778', 'https://openalex.org/W97072897', 'https://openalex.org/W2963939538', 'https://openalex.org/W2973157397', 'https://openalex.org/W2900898015', 'https://openalex.org/W2515741950', 'https://openalex.org/W2963026768', 'https://openalex.org/W2982413405', 'https://openalex.org/W2965373594', 'https://openalex.org/W2972943112', 'https://openalex.org/W2945824677']",2020-05-20
https://openalex.org/W3098598562,https://doi.org/10.48550/arxiv.2011.06195,Towards Semi-Supervised Semantics Understanding from Speech,"Much recent work on Spoken Language Understanding (SLU) falls short in at least one of three ways: models were trained on oracle text input and neglected the Automatics Speech Recognition (ASR) outputs, models were trained to predict only intents without the slot values, or models were trained on a large amount of in-house data. We proposed a clean and general framework to learn semantics directly from speech with semi-supervision from transcribed speech to address these. Our framework is built upon pretrained end-to-end (E2E) ASR and self-supervised language models, such as BERT, and fine-tuned on a limited amount of target SLU corpus. In parallel, we identified two inadequate settings under which SLU models have been tested: noise-robustness and E2E semantics evaluation. We tested the proposed framework under realistic environmental noises and with a new metric, the slots edit F1 score, on two public SLU corpora. Experiments show that our SLU framework with speech as input can perform on par with those with oracle text as input in semantics understanding, while environmental noises are present, and a limited amount of labeled semantics data is available.","['https://openalex.org/W2077302143', 'https://openalex.org/W3015267417', 'https://openalex.org/W3001770030', 'https://openalex.org/W3027083471', 'https://openalex.org/W3041561163', 'https://openalex.org/W3093312917', 'https://openalex.org/W2219249508', 'https://openalex.org/W2930682606', 'https://openalex.org/W3035202887', 'https://openalex.org/W2803609229', 'https://openalex.org/W2963446094', 'https://openalex.org/W2963340922', 'https://openalex.org/W2327501763', 'https://openalex.org/W2806429264', 'https://openalex.org/W2973049979', 'https://openalex.org/W1494198834', 'https://openalex.org/W3125709657', 'https://openalex.org/W3026041220', 'https://openalex.org/W2556930864', 'https://openalex.org/W3099782249', 'https://openalex.org/W2917128112', 'https://openalex.org/W2982223350', 'https://openalex.org/W2097550833', 'https://openalex.org/W2991557631', 'https://openalex.org/W3036601975', 'https://openalex.org/W2981458636', 'https://openalex.org/W3037057938', 'https://openalex.org/W2926827382', 'https://openalex.org/W2936774411', 'https://openalex.org/W3026408381', 'https://openalex.org/W2988736778', 'https://openalex.org/W3080993257', 'https://openalex.org/W2894164357', 'https://openalex.org/W2766219058', 'https://openalex.org/W3092630929', 'https://openalex.org/W2963288440', 'https://openalex.org/W2974831423', 'https://openalex.org/W2963403868', 'https://openalex.org/W3039910566', 'https://openalex.org/W2890964092', 'https://openalex.org/W3025035610', 'https://openalex.org/W2998616931', 'https://openalex.org/W2795935804', 'https://openalex.org/W2980282514', 'https://openalex.org/W3016011332', 'https://openalex.org/W2153501885', 'https://openalex.org/W2964117975', 'https://openalex.org/W3032892481', 'https://openalex.org/W3025429027', 'https://openalex.org/W2963720603', 'https://openalex.org/W3034875620', 'https://openalex.org/W2842511635', 'https://openalex.org/W2019116789', 'https://openalex.org/W2981991061', 'https://openalex.org/W3025324327', 'https://openalex.org/W2963341956', 'https://openalex.org/W2998532468', 'https://openalex.org/W2935542736', 'https://openalex.org/W3049038774', 'https://openalex.org/W3015412890', 'https://openalex.org/W2885185669', 'https://openalex.org/W2972818416', 'https://openalex.org/W2093973850', 'https://openalex.org/W2889201969', 'https://openalex.org/W2928075308', 'https://openalex.org/W2979722627', 'https://openalex.org/W2979476256', 'https://openalex.org/W3015522062', 'https://openalex.org/W1936920915']",2020-11-11
https://openalex.org/W3166633154,,Adversarial Defense for Automatic Speaker Verification by Self-Supervised Learning,"Previous works have shown that automatic speaker verification (ASV) is seriously vulnerable to malicious spoofing attacks, such as replay, synthetic speech, and recently emerged adversarial attacks. Great efforts have been dedicated to defending ASV against replay and synthetic speech; however, only a few approaches have been explored to deal with adversarial attacks. All the existing approaches to tackle adversarial attacks for ASV require the knowledge for adversarial samples generation, but it is impractical for defenders to know the exact attack algorithms that are applied by the in-the-wild attackers. This work is among the first to perform adversarial defense for ASV without knowing the specific attack algorithms. Inspired by self-supervised learning models (SSLMs) that possess the merits of alleviating the superficial noise in the inputs and reconstructing clean samples from the interrupted ones, this work regards adversarial perturbations as one kind of noise and conducts adversarial defense for ASV by SSLMs. Specifically, we propose to perform adversarial defense from two perspectives: 1) adversarial perturbation purification and 2) adversarial perturbation detection. Experimental results show that our detection module effectively shields the ASV by detecting adversarial samples with an accuracy of around 80%. Moreover, since there is no common metric for evaluating the adversarial defense performance for ASV, this work also formalizes evaluation metrics for adversarial defense considering both purification and detection based approaches into account. We sincerely encourage future works to benchmark their approaches based on the proposed evaluation framework.","['https://openalex.org/W2988736778', 'https://openalex.org/W1873522754', 'https://openalex.org/W2745896134', 'https://openalex.org/W2587150483', 'https://openalex.org/W1522301498', 'https://openalex.org/W3024195404', 'https://openalex.org/W2726515241', 'https://openalex.org/W2981461916', 'https://openalex.org/W2842511635', 'https://openalex.org/W3041561163', 'https://openalex.org/W2963058500', 'https://openalex.org/W3007679772', 'https://openalex.org/W2782403400', 'https://openalex.org/W3161011913', 'https://openalex.org/W3039832962', 'https://openalex.org/W3095498208', 'https://openalex.org/W2963389226', 'https://openalex.org/W2926827382', 'https://openalex.org/W2783113218', 'https://openalex.org/W3005511757', 'https://openalex.org/W2963207607', 'https://openalex.org/W2896457183', 'https://openalex.org/W3016518327', 'https://openalex.org/W2936774411', 'https://openalex.org/W3026494492', 'https://openalex.org/W2963420686', 'https://openalex.org/W2979476256', 'https://openalex.org/W2808631503', 'https://openalex.org/W2965373594', 'https://openalex.org/W2982223350', 'https://openalex.org/W2951534110', 'https://openalex.org/W2767951891', 'https://openalex.org/W2964153729', 'https://openalex.org/W2936697195', 'https://openalex.org/W2121812409', 'https://openalex.org/W3091838242', 'https://openalex.org/W2973252307', 'https://openalex.org/W2046056978', 'https://openalex.org/W3033893956', 'https://openalex.org/W3035501253', 'https://openalex.org/W2975059944', 'https://openalex.org/W3016011332', 'https://openalex.org/W2972678295', 'https://openalex.org/W2404874347', 'https://openalex.org/W2952730822', 'https://openalex.org/W2985489290', 'https://openalex.org/W2962904371', 'https://openalex.org/W2039057510', 'https://openalex.org/W3009053050', 'https://openalex.org/W2626778328', 'https://openalex.org/W3009810649', 'https://openalex.org/W2898435086', 'https://openalex.org/W2064364374', 'https://openalex.org/W2890964092', 'https://openalex.org/W2980486495', 'https://openalex.org/W3025035610', 'https://openalex.org/W1922655562', 'https://openalex.org/W2963242190']",2021-06-01
https://openalex.org/W3092123657,https://doi.org/10.48550/arxiv.2010.03135,Representation Learning for Sequence Data with Deep Autoencoding Predictive Components,"We propose Deep Autoencoding Predictive Components (DAPC) -- a self-supervised representation learning method for sequence data, based on the intuition that useful representations of sequence data should exhibit a simple structure in the latent space. We encourage this latent structure by maximizing an estimate of predictive information of latent feature sequences, which is the mutual information between past and future windows at each time step. In contrast to the mutual information lower bound commonly used by contrastive learning, the estimate of predictive information we adopt is exact under a Gaussian assumption. Additionally, it can be computed without negative sampling. To reduce the degeneracy of the latent space extracted by powerful encoders and keep useful information from the inputs, we regularize predictive information learning with a challenging masked reconstruction loss. We demonstrate that our method recovers the latent space of noisy dynamical systems, extracts predictive features for forecasting tasks, and improves automatic speech recognition when used to pretrain the encoder on large amounts of unlabeled data.","['https://openalex.org/W2808697642', 'https://openalex.org/W2963618559', 'https://openalex.org/W2092939357', 'https://openalex.org/W3041561163', 'https://openalex.org/W2145094598', 'https://openalex.org/W2842511635', 'https://openalex.org/W2127141656', 'https://openalex.org/W2988736778', 'https://openalex.org/W2981991061', 'https://openalex.org/W3003875258', 'https://openalex.org/W2939710050', 'https://openalex.org/W3005680577', 'https://openalex.org/W2945478251', 'https://openalex.org/W2108384452', 'https://openalex.org/W3026842484', 'https://openalex.org/W2024490156', 'https://openalex.org/W2962739339', 'https://openalex.org/W1523385540', 'https://openalex.org/W854541894', 'https://openalex.org/W1924770834', 'https://openalex.org/W3034781633', 'https://openalex.org/W3035060554', 'https://openalex.org/W2925140953', 'https://openalex.org/W3016011332', 'https://openalex.org/W1494198834', 'https://openalex.org/W3015213852', 'https://openalex.org/W3030163527', 'https://openalex.org/W2963341956', 'https://openalex.org/W2962780374', 'https://openalex.org/W3015265920', 'https://openalex.org/W2973157397', 'https://openalex.org/W2766219058', 'https://openalex.org/W2895352417', 'https://openalex.org/W2962738009', 'https://openalex.org/W2176412452', 'https://openalex.org/W3046749939', 'https://openalex.org/W3099782249', 'https://openalex.org/W1512746852', 'https://openalex.org/W2936774411', 'https://openalex.org/W2088035110', 'https://openalex.org/W2146444479', 'https://openalex.org/W3003382064', 'https://openalex.org/W2972818416', 'https://openalex.org/W2152790380', 'https://openalex.org/W1959608418', 'https://openalex.org/W1883346539', 'https://openalex.org/W2972943112', 'https://openalex.org/W592244745', 'https://openalex.org/W3037428746', 'https://openalex.org/W3035524453', 'https://openalex.org/W3125709657', 'https://openalex.org/W2327501763', 'https://openalex.org/W2981363336', 'https://openalex.org/W1638203394', 'https://openalex.org/W2887997457', 'https://openalex.org/W2962964508', 'https://openalex.org/W2626778328', 'https://openalex.org/W2995480165', 'https://openalex.org/W2160815625', 'https://openalex.org/W3036601975', 'https://openalex.org/W2886180730', 'https://openalex.org/W2128957129', 'https://openalex.org/W2949517790']",2020-10-07
https://openalex.org/W3132108706,https://doi.org/10.48550/arxiv.2102.06816,Bi-APC: Bidirectional Autoregressive Predictive Coding for Unsupervised Pre-training and Its Application to Children's ASR,"We present a bidirectional unsupervised model pre-training (UPT) method and apply it to children's automatic speech recognition (ASR). An obstacle to improving child ASR is the scarcity of child speech databases. A common approach to alleviate this problem is model pre-training using data from adult speech. Pre-training can be done using supervised (SPT) or unsupervised methods, depending on the availability of annotations. Typically, SPT performs better. In this paper, we focus on UPT to address the situations when pre-training data are unlabeled. Autoregressive predictive coding (APC), a UPT method, predicts frames from only one direction, limiting its use to uni-directional pre-training. Conventional bidirectional UPT methods, however, predict only a small portion of frames. To extend the benefits of APC to bi-directional pre-training, Bi-APC is proposed. We then use adaptation techniques to transfer knowledge learned from adult speech (using the Librispeech corpus) to child speech (OGI Kids corpus). LSTM-based hybrid systems are investigated. For the uni-LSTM structure, APC obtains similar WER improvements to SPT over the baseline. When applied to BLSTM, however, APC is not as competitive as SPT, but our proposed Bi-APC has comparable improvements to SPT.","['https://openalex.org/W2025198378', 'https://openalex.org/W2972943112', 'https://openalex.org/W1994888226', 'https://openalex.org/W2792175982', 'https://openalex.org/W2988736778', 'https://openalex.org/W2400702965', 'https://openalex.org/W17704661', 'https://openalex.org/W434092020', 'https://openalex.org/W2972725946', 'https://openalex.org/W3047949962', 'https://openalex.org/W2842511635', 'https://openalex.org/W2981857663', 'https://openalex.org/W2295983515', 'https://openalex.org/W3003875258', 'https://openalex.org/W2889212027', 'https://openalex.org/W2971274815', 'https://openalex.org/W2960140743', 'https://openalex.org/W3096485810', 'https://openalex.org/W2593216051', 'https://openalex.org/W1524333225', 'https://openalex.org/W3097973766', 'https://openalex.org/W2471933213', 'https://openalex.org/W2982223350', 'https://openalex.org/W2972560782', 'https://openalex.org/W2981991061', 'https://openalex.org/W3007928779', 'https://openalex.org/W1494198834']",2021-02-12
https://openalex.org/W3025896989,https://doi.org/10.21437/interspeech.2020-1917,Large Scale Weakly and Semi-Supervised Learning for Low-Resource Video ASR,"Many semi- and weakly-supervised approaches have been investigated for overcoming the labeling cost of building high quality speech recognition systems. On the challenging task of transcribing social media videos in low-resource conditions, we conduct a large scale systematic comparison between two self-labeling methods on one hand, and weakly-supervised pretraining using contextual metadata on the other. We investigate distillation methods at the frame level and the sequence level for hybrid, encoder-only CTC-based, and encoder-decoder speech recognition systems on Dutch and Romanian languages using 27,000 and 58,000 hours of unlabeled audio respectively. Although all approaches improved upon their respective baseline WERs by more than 8%, sequence-level distillation for encoder-decoder models provided the largest relative WER reduction of 20% compared to the strongest data-augmented supervised baseline.","['https://openalex.org/W15497043', 'https://openalex.org/W2842511635', 'https://openalex.org/W2056786202', 'https://openalex.org/W2120358855', 'https://openalex.org/W2963681135', 'https://openalex.org/W2892009249', 'https://openalex.org/W2972818416', 'https://openalex.org/W2514741789', 'https://openalex.org/W2193413348', 'https://openalex.org/W2988736778', 'https://openalex.org/W2936993002', 'https://openalex.org/W2139698650', 'https://openalex.org/W2507699225', 'https://openalex.org/W2963739817', 'https://openalex.org/W1975113979', 'https://openalex.org/W1494198834', 'https://openalex.org/W2604132379', 'https://openalex.org/W2533523411', 'https://openalex.org/W2962907457', 'https://openalex.org/W2143577772', 'https://openalex.org/W3015737168', 'https://openalex.org/W2110073835', 'https://openalex.org/W2404463488', 'https://openalex.org/W2767206889', 'https://openalex.org/W3008525923', 'https://openalex.org/W1555037511', 'https://openalex.org/W2033256038', 'https://openalex.org/W2936774411', 'https://openalex.org/W2154887136', 'https://openalex.org/W2964001192', 'https://openalex.org/W2963250244', 'https://openalex.org/W2985963903', 'https://openalex.org/W2941814890', 'https://openalex.org/W3015522062', 'https://openalex.org/W2134797427', 'https://openalex.org/W854541894', 'https://openalex.org/W2407080277', 'https://openalex.org/W2998532468', 'https://openalex.org/W2294370754', 'https://openalex.org/W2964121744', 'https://openalex.org/W2405883473', 'https://openalex.org/W2981857663', 'https://openalex.org/W2981536480', 'https://openalex.org/W2972974544', 'https://openalex.org/W2025482506', 'https://openalex.org/W2995181338', 'https://openalex.org/W2327501763', 'https://openalex.org/W2940322076', 'https://openalex.org/W1821462560', 'https://openalex.org/W2556930864']",2020-10-25
https://openalex.org/W3096703500,https://doi.org/10.48550/arxiv.2006.05174,Input-independent Attention Weights Are Expressive Enough: A Study of Attention in Self-supervised Audio Transformers,"In this paper, we seek solutions for reducing the computation complexity of transformer-based models for speech representation learning. We evaluate 10 attention algorithms; then, we pre-train the transformer-based model with those attention algorithms in a self-supervised fashion and treat them as feature extractors on downstream tasks, including phoneme classification and speaker classification. With the assistance of t-SNE, PCA and some observation, the attention weights in self-supervised audio transformers can be categorized into four general cases. Based on these cases and some analyses, we are able to use a specific set of attention weights to initialize the model. Our approach shows comparable performance to the typical self-attention yet requires 20% less time in both training and inference.","['https://openalex.org/W2981991061', 'https://openalex.org/W2981363336', 'https://openalex.org/W2940744433', 'https://openalex.org/W2963403868', 'https://openalex.org/W3021293129', 'https://openalex.org/W2842511635', 'https://openalex.org/W1555148682', 'https://openalex.org/W2126754439', 'https://openalex.org/W2187089797', 'https://openalex.org/W2973049979', 'https://openalex.org/W3006801027', 'https://openalex.org/W1575370549', 'https://openalex.org/W3025035610', 'https://openalex.org/W2982223350', 'https://openalex.org/W2808891190', 'https://openalex.org/W3019527251', 'https://openalex.org/W1494198834', 'https://openalex.org/W2973157397', 'https://openalex.org/W2597655663', 'https://openalex.org/W2963056065', 'https://openalex.org/W3023371261', 'https://openalex.org/W3015468748', 'https://openalex.org/W2946567085', 'https://openalex.org/W2949382160', 'https://openalex.org/W2997517014', 'https://openalex.org/W2988736778', 'https://openalex.org/W3000514857', 'https://openalex.org/W1974627246', 'https://openalex.org/W2995435108', 'https://openalex.org/W2963341956']",2020-06-09
https://openalex.org/W3126050678,,Fusing Wav2vec2.0 and BERT into End-to-end Model for Low-resource Speech Recognition,"Self-supervised acoustic pre-training has achieved impressive results on low-resource speech recognition tasks. It indicates that the pretrain-and-finetune paradigm is a promising direction. In this work, we propose an end-to-end model for the low-resource speech recognition, which fuses a pre-trained audio encoder (wav2vec2.0) and a pre-trained text decoder (BERT). The two modules are connected by a linear attention mechanism without parameters. A fully connected layer is introduced for hidden mapping between speech and language modalities. Besides, we design an effective fine-tuning strategy to preserve and utilize the text context modeling ability of the pre-trained decoder. Armed with this strategy, our model exhibits distinct faster convergence and better performance. Our model achieves approaching recognition performance in CALLHOME corpus (15h) as the SOTA pipeline modeling.","['https://openalex.org/W1526236009', 'https://openalex.org/W3025417467', 'https://openalex.org/W3099782249', 'https://openalex.org/W2973180718', 'https://openalex.org/W3028382961', 'https://openalex.org/W2808640845', 'https://openalex.org/W3001434439', 'https://openalex.org/W2962925243', 'https://openalex.org/W2936078256', 'https://openalex.org/W2102113734', 'https://openalex.org/W2996428491', 'https://openalex.org/W2125113755', 'https://openalex.org/W2996383576', 'https://openalex.org/W2995181338', 'https://openalex.org/W2803399609', 'https://openalex.org/W2767206889', 'https://openalex.org/W2750545698', 'https://openalex.org/W3113594615', 'https://openalex.org/W2988736778', 'https://openalex.org/W2888779557', 'https://openalex.org/W2981991061', 'https://openalex.org/W2963341956', 'https://openalex.org/W3036601975', 'https://openalex.org/W3112092703', 'https://openalex.org/W3014413043', 'https://openalex.org/W2963400424', 'https://openalex.org/W3037057938', 'https://openalex.org/W2127141656', 'https://openalex.org/W3016167541', 'https://openalex.org/W2953190524', 'https://openalex.org/W2952509486', 'https://openalex.org/W2939111082']",2021-01-17
https://openalex.org/W3166405250,https://doi.org/10.1109/icme51207.2021.9428111,Spiker-Converter: A Semi-Supervised Framework for Low-Resource Speech Recognition with Stable Adversarial Training,"Labeling large amounts of speech is laborious and expensive. The scarcity of speech with the accent or in specific scenes hangs the further applications of the ASR system in practice. On the contrary, collecting speech and domain-related text corpus is more achievable. In this work, we propose an end-to-end model called Spiker-Converter for the low-resource speech recognition task. It decomposes the ASR task by introducing additional acoustic supervision, dramatically reduce the demand for labeled samples. Besides, we provide a semi-supervised training method, which consumes a few labeled speech samples but large amounts of unlabeled speech and domain-related text. Specifically, we use a Discriminator to produce learning signals for the ASR model with unlabeled speech as input. Note that we apply adversarial training to part of the ASR model, ensuring stability. Experiments show the significant effectiveness of our semi-supervised training method. For now, our method can only be used for Chinese-like languages, but it shows a potential direction to solve low-resource speech recognition tasks.","['https://openalex.org/W2972706021', 'https://openalex.org/W6732447497', 'https://openalex.org/W6746574493', 'https://openalex.org/W6743477263', 'https://openalex.org/W6770514103', 'https://openalex.org/W2972981541', 'https://openalex.org/W6775452034', 'https://openalex.org/W2913851961', 'https://openalex.org/W2127141656', 'https://openalex.org/W6631362777', 'https://openalex.org/W6603760306', 'https://openalex.org/W2746791238', 'https://openalex.org/W6903921188', 'https://openalex.org/W2889213362', 'https://openalex.org/W2963739817', 'https://openalex.org/W2963400424', 'https://openalex.org/W2962824709', 'https://openalex.org/W6757699909', 'https://openalex.org/W2973180718', 'https://openalex.org/W6735913928', 'https://openalex.org/W6779669310', 'https://openalex.org/W6640090968', 'https://openalex.org/W2963242190', 'https://openalex.org/W2988736778', 'https://openalex.org/W2963240019', 'https://openalex.org/W1922655562', 'https://openalex.org/W3015522062', 'https://openalex.org/W2577366047', 'https://openalex.org/W4295521014', 'https://openalex.org/W1524333225', 'https://openalex.org/W2888779557', 'https://openalex.org/W2964079874', 'https://openalex.org/W92894758']",2021-06-09
https://openalex.org/W4319862240,https://doi.org/10.1109/slt54892.2023.10022417,Improved Noisy Iterative Pseudo-Labeling for Semi-Supervised Speech Recognition,"Due to the high annotation cost in ASR, the implementation of semi-supervised training has been a hot issue in research and industry. In a multitude of recent investigations, it has been established that pseudo-labeling, a fundamental sub-direction of semi-supervised learning, is effective in ASR. However, if the iterative PL is utilized, the expense of doing data experiments is prohibitively high, making the promotion to diverse situations of ASR tasks problematic. In this paper, we propose an empirical scoring method based on hypothesis distribution testing to guide iterative PL training, therefore lowering the cost of data experiments and boosting ASR performance. Meanwhile, we conducted extensive experiments to determine the necessity and limitation of model perturbation in the initial training and the PL stages. On the Librispeech 100/860 task, our method improves the 12+6 transformer-based CTC+S2S architecture performance from 4.8%/10.1 % to 3.9%/9.6% on test-clean and test-other.","['https://openalex.org/W6678975374', 'https://openalex.org/W6717772578', 'https://openalex.org/W6764051988', 'https://openalex.org/W6733814495', 'https://openalex.org/W2963558289', 'https://openalex.org/W2966415767', 'https://openalex.org/W6765939562', 'https://openalex.org/W6770578729', 'https://openalex.org/W6773005947', 'https://openalex.org/W3035160371', 'https://openalex.org/W3015522062', 'https://openalex.org/W6770506093', 'https://openalex.org/W3096338464', 'https://openalex.org/W3026041220', 'https://openalex.org/W3096273170', 'https://openalex.org/W3198098585', 'https://openalex.org/W3095350795', 'https://openalex.org/W3197223534', 'https://openalex.org/W2936774411', 'https://openalex.org/W6793078711', 'https://openalex.org/W3205405669', 'https://openalex.org/W3160525311', 'https://openalex.org/W6770514103', 'https://openalex.org/W6772883055', 'https://openalex.org/W3015265920', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962780374', 'https://openalex.org/W2331143823', 'https://openalex.org/W6631362777', 'https://openalex.org/W1977286842', 'https://openalex.org/W2129068307', 'https://openalex.org/W2991213871', 'https://openalex.org/W1524333225', 'https://openalex.org/W2431080869', 'https://openalex.org/W3198860978', 'https://openalex.org/W4288020585', 'https://openalex.org/W2962369866', 'https://openalex.org/W3001197829', 'https://openalex.org/W2988736778', 'https://openalex.org/W2953070460', 'https://openalex.org/W2951970475', 'https://openalex.org/W2998532468']",2023-01-09
https://openalex.org/W4380737765,https://doi.org/10.18653/v1/2023.fieldmatters-1.2,Application of Speech Processes for the Documentation of Kréyòl Gwadloupéyen,"In recent times, there has been a growing number of research studies focused on addressing the challenges posed by low-resource languages and the transcription bottleneck phenomenon. This phenomenon has driven the development of speech recognition methods to transcribe regional and Indigenous languages automatically. Although there is much talk about bridging the gap between speech technologies and field linguistics, there is a lack of documented efficient communication between NLP experts and documentary linguists. The models created for low-resource languages often remain within the confines of computer science departments, while documentary linguistics remain attached to traditional transcription workflows. This paper presents the early stage of a collaboration between NLP experts and field linguists, resulting in the successful transcription of Kréyòl Gwadloupéyen using speech recognition technology.","['https://openalex.org/W2790028675', 'https://openalex.org/W2988736778', 'https://openalex.org/W3112967339', 'https://openalex.org/W3030509225', 'https://openalex.org/W4211128534', 'https://openalex.org/W2251408482', 'https://openalex.org/W1593934207', 'https://openalex.org/W4285173613', 'https://openalex.org/W2585654394', 'https://openalex.org/W2014840417', 'https://openalex.org/W3029693316', 'https://openalex.org/W4226026089', 'https://openalex.org/W2895097770', 'https://openalex.org/W3189296823', 'https://openalex.org/W3197380209', 'https://openalex.org/W2742033697', 'https://openalex.org/W3167702107', 'https://openalex.org/W3198429080', 'https://openalex.org/W3102342027', 'https://openalex.org/W3036601975', 'https://openalex.org/W4323273779']",2023-01-01
https://openalex.org/W4386576757,https://doi.org/10.18653/v1/2023.fieldmatters-1,Proceedings of the Second Workshop on NLP Applications to Field Linguistics,"Experiments to fine-tune large multilingual models with limited data from a specific domain or setting has potential to improve automatic speech recognition (ASR) outcomes.This paper reports on the use of the Elpis ASR pipeline to fine-tune two pre-trained base models, Wav2Vec2-XLSR-53 and Wav2Vec2-Large-XLSR-Indonesian, with various mixes of data from 3 YouTube channels teaching Indonesian with English as the language of instruction.We discuss our results inferring new lesson audio (22-46% word error rate) in the context of speeding data collection in diverse and specialised settings.This study is an example of how ASR can be used to accelerate natural language research, expanding ethically sourced data in low-resource settings.","['https://openalex.org/W3118611680', 'https://openalex.org/W3113200168', 'https://openalex.org/W3154420890', 'https://openalex.org/W1632114991', 'https://openalex.org/W4323273779', 'https://openalex.org/W2167689496', 'https://openalex.org/W3108950052', 'https://openalex.org/W4285259294', 'https://openalex.org/W3135324665', 'https://openalex.org/W2895097770', 'https://openalex.org/W2021508351', 'https://openalex.org/W2798685342', 'https://openalex.org/W4287646732', 'https://openalex.org/W4253215990', 'https://openalex.org/W2792435655', 'https://openalex.org/W1538253570', 'https://openalex.org/W2068555210', 'https://openalex.org/W3045510483', 'https://openalex.org/W2284757298', 'https://openalex.org/W2499632556', 'https://openalex.org/W2514582887', 'https://openalex.org/W4312239536', 'https://openalex.org/W2250350054', 'https://openalex.org/W4306955484', 'https://openalex.org/W2049831997', 'https://openalex.org/W2903031159', 'https://openalex.org/W3136016591', 'https://openalex.org/W2115487165', 'https://openalex.org/W3135606662', 'https://openalex.org/W4248970100', 'https://openalex.org/W1529271041', 'https://openalex.org/W4211124943', 'https://openalex.org/W2188152546', 'https://openalex.org/W2279316390', 'https://openalex.org/W2989869204', 'https://openalex.org/W3155116954', 'https://openalex.org/W3103525021', 'https://openalex.org/W2069046433', 'https://openalex.org/W3110317399', 'https://openalex.org/W2485094346', 'https://openalex.org/W3037720825', 'https://openalex.org/W4245669781', 'https://openalex.org/W3213029956', 'https://openalex.org/W2330205722', 'https://openalex.org/W2900737918', 'https://openalex.org/W2600999069', 'https://openalex.org/W2730121257', 'https://openalex.org/W2341824394', 'https://openalex.org/W3112967339', 'https://openalex.org/W3214248866', 'https://openalex.org/W611951739', 'https://openalex.org/W2572569512', 'https://openalex.org/W4295874941', 'https://openalex.org/W2972401849', 'https://openalex.org/W2049108250', 'https://openalex.org/W2055963237', 'https://openalex.org/W3204132594', 'https://openalex.org/W3198429080', 'https://openalex.org/W4300191749', 'https://openalex.org/W3037593119', 'https://openalex.org/W4200300291', 'https://openalex.org/W2000110837', 'https://openalex.org/W3036601975', 'https://openalex.org/W3174220540', 'https://openalex.org/W3169369929', 'https://openalex.org/W2044340178', 'https://openalex.org/W14996564', 'https://openalex.org/W3170946593', 'https://openalex.org/W4390911892', 'https://openalex.org/W3169320628', 'https://openalex.org/W2105769030', 'https://openalex.org/W4300427681', 'https://openalex.org/W3134155512', 'https://openalex.org/W3214933191', 'https://openalex.org/W3176988670', 'https://openalex.org/W2490776483', 'https://openalex.org/W3092557781', 'https://openalex.org/W3196361957', 'https://openalex.org/W2914180170', 'https://openalex.org/W2741654545', 'https://openalex.org/W4283321345', 'https://openalex.org/W1524333225', 'https://openalex.org/W4286905107', 'https://openalex.org/W2548740357', 'https://openalex.org/W201432381', 'https://openalex.org/W3033057983', 'https://openalex.org/W1619735507', 'https://openalex.org/W4211034951', 'https://openalex.org/W3012624518', 'https://openalex.org/W2790378804', 'https://openalex.org/W3120612945', 'https://openalex.org/W2975250190', 'https://openalex.org/W2493916176', 'https://openalex.org/W2962680795', 'https://openalex.org/W2912742782', 'https://openalex.org/W2981169085', 'https://openalex.org/W3168656614', 'https://openalex.org/W4285077564', 'https://openalex.org/W2799410366', 'https://openalex.org/W168564468', 'https://openalex.org/W4299613993', 'https://openalex.org/W4311000453', 'https://openalex.org/W1986543644', 'https://openalex.org/W4327576533', 'https://openalex.org/W2145583812', 'https://openalex.org/W2170434840', 'https://openalex.org/W2962958105', 'https://openalex.org/W3197380209', 'https://openalex.org/W3087775410', 'https://openalex.org/W4254272999', 'https://openalex.org/W3030350046', 'https://openalex.org/W2014840417', 'https://openalex.org/W2053782355', 'https://openalex.org/W2000553825', 'https://openalex.org/W3030464057', 'https://openalex.org/W1855497951', 'https://openalex.org/W2882319491', 'https://openalex.org/W3081931604', 'https://openalex.org/W3198214208', 'https://openalex.org/W2075847010', 'https://openalex.org/W2591649529', 'https://openalex.org/W2988736778', 'https://openalex.org/W4226026089', 'https://openalex.org/W583684607', 'https://openalex.org/W2963555443', 'https://openalex.org/W2025260316', 'https://openalex.org/W4361200963', 'https://openalex.org/W3014577203', 'https://openalex.org/W3037202634', 'https://openalex.org/W4300963525', 'https://openalex.org/W2625209726', 'https://openalex.org/W3153224075', 'https://openalex.org/W2785413227', 'https://openalex.org/W2742033697', 'https://openalex.org/W2786234940', 'https://openalex.org/W2063248502', 'https://openalex.org/W3102342027', 'https://openalex.org/W2915977242', 'https://openalex.org/W2890953081', 'https://openalex.org/W4385245566', 'https://openalex.org/W1990501283', 'https://openalex.org/W1565050997', 'https://openalex.org/W3167702107', 'https://openalex.org/W2996714322', 'https://openalex.org/W2105412253', 'https://openalex.org/W3043783436', 'https://openalex.org/W2262393948', 'https://openalex.org/W3135142735', 'https://openalex.org/W3147792899', 'https://openalex.org/W2991461215', 'https://openalex.org/W2219577084', 'https://openalex.org/W2251408482', 'https://openalex.org/W2201578021', 'https://openalex.org/W2018789714', 'https://openalex.org/W2102381086', 'https://openalex.org/W2155870214', 'https://openalex.org/W2069781236', 'https://openalex.org/W4287888444', 'https://openalex.org/W2785782119', 'https://openalex.org/W3095315336', 'https://openalex.org/W2963265201', 'https://openalex.org/W4231037951', 'https://openalex.org/W2119320261', 'https://openalex.org/W4285188440', 'https://openalex.org/W2053590082', 'https://openalex.org/W3157070662', 'https://openalex.org/W2152692515', 'https://openalex.org/W1630657132', 'https://openalex.org/W2787420813', 'https://openalex.org/W4300009529', 'https://openalex.org/W2761336777', 'https://openalex.org/W782259221', 'https://openalex.org/W4248440877', 'https://openalex.org/W3008374555', 'https://openalex.org/W2766653511', 'https://openalex.org/W3003257820', 'https://openalex.org/W2756127416', 'https://openalex.org/W2804315663', 'https://openalex.org/W2983791227', 'https://openalex.org/W2724394450']",2023-01-01
https://openalex.org/W4387010942,https://doi.org/10.2528/pierl23073102,LFM Signal Sources Classification Based on Self-supervised Learning,"Linear Frequency Modulation (LFM) signals are widely used in radar and sonar technology.Many applications are interested in determining the source of an LFM signal.In recent years, the rapid development of machine learning has facilitated research in various fields, including signal recognition.The neural networks can extract the implicit features of the signals, which can help the system to sort and recognize the signal sources quickly and accurately.High performance of neural networks requires large amounts of high-quality labeled data.However, it is difficult and expensive to obtain a large amount of high-quality labeled data.Simultaneously, some features will be lost during data preprocessing, and feature extraction and classification tasks will be inefficient.The self-supervised network is proposed in this paper for pre-training the signal waveform and fine-tuning the classification with a small amount of labeled data.The proposed method can extract more signal waveform features, save labeling costs, and has higher precision.This method can provide up to 99.7% recognition accuracy at 20 dB.","['https://openalex.org/W3101130542', 'https://openalex.org/W2936043767', 'https://openalex.org/W2391735301', 'https://openalex.org/W2531162019', 'https://openalex.org/W2979177924', 'https://openalex.org/W2168744889', 'https://openalex.org/W4285817329', 'https://openalex.org/W4312387642', 'https://openalex.org/W4321845138', 'https://openalex.org/W3091289896', 'https://openalex.org/W3134652006', 'https://openalex.org/W6780226713', 'https://openalex.org/W6755977528', 'https://openalex.org/W6761563299', 'https://openalex.org/W2988736778', 'https://openalex.org/W3173061219', 'https://openalex.org/W2896457183', 'https://openalex.org/W3202385447', 'https://openalex.org/W3213723758', 'https://openalex.org/W2966569158', 'https://openalex.org/W4394666973', 'https://openalex.org/W4239741500', 'https://openalex.org/W2899663614', 'https://openalex.org/W2941814890', 'https://openalex.org/W2272847350']",2023-01-01
https://openalex.org/W4387942868,https://doi.org/10.14209/sbrt.2023.1570916069,Bilingual ASR model with language identification for Brazilian Portuguese and South-American Spanish,"Creating accurate and reliable low-resource automatic speech recognition (ASR) models remains challenging due to limited curated data.This work proposes a bilingual ASR model for Brazilian Portuguese and Latin-American Spanish implemented with the Wav2Vec2.0architecture and trained on multiple speech datasets.It combines Language Identification and Speech Recognition, employing a joint feature encoder and task-specific context encoders.Evaluation in the Multilingual Librispeech dataset demonstrates promising results, with an average accuracy of 75.98% for language identification and a competitive Word Error Rate of 30.45% in a bilingual setting, comparable to the Whisper model.","['https://openalex.org/W3037057938', 'https://openalex.org/W3213029956', 'https://openalex.org/W2091746061', 'https://openalex.org/W2995929068', 'https://openalex.org/W3095410713', 'https://openalex.org/W2992847832', 'https://openalex.org/W2997787503', 'https://openalex.org/W2888922844', 'https://openalex.org/W2805087519', 'https://openalex.org/W2894835365', 'https://openalex.org/W3040454670', 'https://openalex.org/W3031533975', 'https://openalex.org/W3042007685', 'https://openalex.org/W2988736778', 'https://openalex.org/W3036601975', 'https://openalex.org/W3112616666', 'https://openalex.org/W4221161148', 'https://openalex.org/W3144073776', 'https://openalex.org/W2139851371', 'https://openalex.org/W2154740140', 'https://openalex.org/W3028785944', 'https://openalex.org/W3106807794', 'https://openalex.org/W38194800', 'https://openalex.org/W4311000453', 'https://openalex.org/W4287726212', 'https://openalex.org/W2316579313', 'https://openalex.org/W3198429080', 'https://openalex.org/W3030437843', 'https://openalex.org/W2315995013', 'https://openalex.org/W3139878283', 'https://openalex.org/W3096032230', 'https://openalex.org/W3197642003', 'https://openalex.org/W4287553982']",2023-01-01
https://openalex.org/W3081424945,https://doi.org/10.1109/access.2020.3019084,nnAudio: An on-the-Fly GPU Audio to Spectrogram Conversion Toolbox Using 1D Convolutional Neural Networks,"In this paper, we present nnAudio, a new neural network-based audio processing framework with graphics processing unit (GPU) support that leverages 1D convolutional neural networks to perform time domain to frequency domain conversion. It allows on-the-fly spectrogram extraction due to its fast speed, without the need to store any spectrograms on the disk. Moreover, this approach also allows back-propagation on the waveforms-to-spectrograms transformation layer, and hence, the transformation process can be made trainable, further optimizing the waveform-to-spectrogram transformation for the specific task that the neural network is trained on. All spectrogram implementations scale as Big-O of linear time with respect to the input length. nnAudio, however, leverages the compute unified device architecture (CUDA) of 1D convolutional neural network from PyTorch, its short-time Fourier transform (STFT), Mel spectrogram, and constant-Q transform (CQT) implementations are an order of magnitude faster than other implementations using only the central processing unit (CPU). We tested our framework on three different machines with NVIDIA GPUs, and our framework significantly reduces the spectrogram extraction time from the order of seconds (using a popular python library librosa) to the order of milliseconds, given that the audio recordings are of the same length. When applying nnAudio to variable input audio lengths, an average of 11.5 hours are required to extract 34 spectrogram types with different parameters from the MusicNet dataset using librosa. An average of 2.8 hours is required for nnAudio, which is still four times faster than librosa. Our proposed framework also outperforms existing GPU processing libraries such as Kapre and torchaudio in terms of processing speed.","['https://openalex.org/W2798530805', 'https://openalex.org/W2904843390', 'https://openalex.org/W2889191349', 'https://openalex.org/W2753779507', 'https://openalex.org/W6733936739', 'https://openalex.org/W2950733131', 'https://openalex.org/W6756545897', 'https://openalex.org/W6740333300', 'https://openalex.org/W2922288583', 'https://openalex.org/W6776981032', 'https://openalex.org/W2963045359', 'https://openalex.org/W2105143211', 'https://openalex.org/W6769896225', 'https://openalex.org/W6773722262', 'https://openalex.org/W2112565646', 'https://openalex.org/W2106271918', 'https://openalex.org/W2921083967', 'https://openalex.org/W6730401039', 'https://openalex.org/W6767470613', 'https://openalex.org/W2191779130', 'https://openalex.org/W6756040250', 'https://openalex.org/W6740352302', 'https://openalex.org/W2106710516', 'https://openalex.org/W1838323663', 'https://openalex.org/W2168350281', 'https://openalex.org/W6603616073', 'https://openalex.org/W2931364255', 'https://openalex.org/W2794150026', 'https://openalex.org/W2963985474', 'https://openalex.org/W3008587939', 'https://openalex.org/W2889717020', 'https://openalex.org/W3007068036', 'https://openalex.org/W2940247716', 'https://openalex.org/W2902808043', 'https://openalex.org/W3016243847', 'https://openalex.org/W6765174816', 'https://openalex.org/W6764992649', 'https://openalex.org/W2103387126', 'https://openalex.org/W2076608692', 'https://openalex.org/W2148154194', 'https://openalex.org/W2042105302', 'https://openalex.org/W2935162632', 'https://openalex.org/W4232336823', 'https://openalex.org/W2317919972', 'https://openalex.org/W6632100814', 'https://openalex.org/W2086393337', 'https://openalex.org/W6601150824', 'https://openalex.org/W2112796928', 'https://openalex.org/W2971591236', 'https://openalex.org/W1975461858', 'https://openalex.org/W2721931776', 'https://openalex.org/W2519091744', 'https://openalex.org/W2954224973', 'https://openalex.org/W2986842658', 'https://openalex.org/W2725868244', 'https://openalex.org/W4287813281', 'https://openalex.org/W2991402284', 'https://openalex.org/W2485688913', 'https://openalex.org/W2950335938', 'https://openalex.org/W2559688696', 'https://openalex.org/W1604034532', 'https://openalex.org/W608300865', 'https://openalex.org/W1538131130', 'https://openalex.org/W3091273176', 'https://openalex.org/W4297791000', 'https://openalex.org/W2899771611']",2020-01-01
https://openalex.org/W3017258074,https://doi.org/10.18653/v1/2020.acl-main.661,Speech Translation and the End-to-End Promise: Taking Stock of Where We Are,"Over its three decade history, speech translation has experienced several shifts in its primary research themes; moving from loosely coupled cascades of speech recognition and machine translation, to exploring questions of tight coupling, and finally to end-to-end models that have recently attracted much attention. This paper provides a brief survey of these developments, along with a discussion of the main challenges of traditional approaches which stem from committing to intermediate representations from the speech recognizer, and from training cascaded models separately towards different objectives. Recent end-to-end modeling techniques promise a principled way of overcoming these issues by allowing joint training of all model components and removing the need for explicit intermediate representations. However, a closer look reveals that many end-to-end models fall short of solving these issues, due to compromises made to address data scarcity. This paper provides a unifying categorization and nomenclature that covers both traditional and recent approaches and that may help researchers by highlighting both trade-offs and open research questions.","['https://openalex.org/W2936969148', 'https://openalex.org/W3184351855', 'https://openalex.org/W399167303', 'https://openalex.org/W2795581297', 'https://openalex.org/W2977183928', 'https://openalex.org/W2964161387', 'https://openalex.org/W3015698636', 'https://openalex.org/W3012492057', 'https://openalex.org/W2952079278', 'https://openalex.org/W2113106066', 'https://openalex.org/W2593011301', 'https://openalex.org/W2103123519', 'https://openalex.org/W2493041683', 'https://openalex.org/W2210498294', 'https://openalex.org/W2402827806', 'https://openalex.org/W2952167535', 'https://openalex.org/W2406260785', 'https://openalex.org/W2997436923', 'https://openalex.org/W2978099976', 'https://openalex.org/W2938973646', 'https://openalex.org/W2964048171', 'https://openalex.org/W2168995909', 'https://openalex.org/W1920064753', 'https://openalex.org/W2116912781', 'https://openalex.org/W2945700568', 'https://openalex.org/W2963779652', 'https://openalex.org/W2404511972', 'https://openalex.org/W124561860', 'https://openalex.org/W2135708429', 'https://openalex.org/W3008125272', 'https://openalex.org/W2401330834', 'https://openalex.org/W2762715843', 'https://openalex.org/W2907380995', 'https://openalex.org/W2186089609', 'https://openalex.org/W2971345947', 'https://openalex.org/W2115340555', 'https://openalex.org/W2982727400', 'https://openalex.org/W2011783148', 'https://openalex.org/W3007068036', 'https://openalex.org/W2123663313', 'https://openalex.org/W2605202026', 'https://openalex.org/W2961103279', 'https://openalex.org/W3008549139', 'https://openalex.org/W2397318268', 'https://openalex.org/W2991293848', 'https://openalex.org/W2408724534', 'https://openalex.org/W3016137827', 'https://openalex.org/W2461231802', 'https://openalex.org/W2793214147', 'https://openalex.org/W2152834109', 'https://openalex.org/W53604701', 'https://openalex.org/W2970780025', 'https://openalex.org/W2945286432', 'https://openalex.org/W3006988520', 'https://openalex.org/W2986963494', 'https://openalex.org/W2242221029', 'https://openalex.org/W1486199320', 'https://openalex.org/W2294868251', 'https://openalex.org/W2063692070', 'https://openalex.org/W3098307012', 'https://openalex.org/W2400169135', 'https://openalex.org/W2962680099', 'https://openalex.org/W3015703505', 'https://openalex.org/W2089886978', 'https://openalex.org/W2582956876', 'https://openalex.org/W2972448360', 'https://openalex.org/W2398009384', 'https://openalex.org/W2466918907', 'https://openalex.org/W2951289103', 'https://openalex.org/W183182694', 'https://openalex.org/W2951635603', 'https://openalex.org/W2139647714', 'https://openalex.org/W2161772224', 'https://openalex.org/W3002201054', 'https://openalex.org/W2982129078', 'https://openalex.org/W2972495969', 'https://openalex.org/W2114483840', 'https://openalex.org/W3029944303', 'https://openalex.org/W2242818861', 'https://openalex.org/W3032598645', 'https://openalex.org/W2962775040', 'https://openalex.org/W2121495627', 'https://openalex.org/W3016139774', 'https://openalex.org/W3203055519']",2020-01-01
https://openalex.org/W4313203218,https://doi.org/10.1109/o-cocosda202257103.2022.9997945,Analysis of Layer-Wise Training in Direct Speech to Speech Translation Using BI-LSTM,"Speech-to-speech translation (S2ST) is the process of translation of speech from one language to another. Traditional S2ST systems follow a cascaded approach, where three modules automatics speech recognition (ASR), machine translation (MT), and text-to-speech translation (TTS) are concatenated to obtain the final translated speech utterance. The cascaded nature of the system results in the propagation of errors from one module to another. This, in turn, leads to the degradation in the overall performance of the S2ST task. With the evolution of the deep learning approaches to speech processing, many attempts have been made to perform end-to-end and direct speech-to-speech translation (DS2ST). But most of these approaches rely on language transcripts in one way or the other. In this work, we aim to perform the DS2ST task without using language transcripts. In this direction we have performed three experiments: First, we have investigated the direct learning of mapping function from source to target language with the increase in the number of utterances. Second, we have analyzed how the learning function improves with an increase in the number of Bi-LSTM layers. Third, we have observed how the system behaves with the unknown speakers (not used during training) during inference. From the experiments, it has been observed that with the increase in the number of utterances and layers, the quality of translation improves. And also, with a speaker and text-dependent training of approximately 4.4 hrs of speech, the model can generate the target language utterance even for unknown speakers. Though the generated utterance quality is not that good, but intelligent to some extent to be perceived.","['https://openalex.org/W2051745966', 'https://openalex.org/W2048458228', 'https://openalex.org/W2136545725', 'https://openalex.org/W1510746193', 'https://openalex.org/W1538023239', 'https://openalex.org/W2081592734', 'https://openalex.org/W2146173057', 'https://openalex.org/W6894251942', 'https://openalex.org/W1579847400', 'https://openalex.org/W2135708429', 'https://openalex.org/W1969443802', 'https://openalex.org/W2972448360', 'https://openalex.org/W3017535695', 'https://openalex.org/W2145095987', 'https://openalex.org/W2972495969', 'https://openalex.org/W3180374548', 'https://openalex.org/W3142316150', 'https://openalex.org/W3197659778', 'https://openalex.org/W3007068036', 'https://openalex.org/W3098557217', 'https://openalex.org/W1509691205', 'https://openalex.org/W2915253508', 'https://openalex.org/W2131774270', 'https://openalex.org/W2150355110', 'https://openalex.org/W2040882540', 'https://openalex.org/W6610843619', 'https://openalex.org/W3208643357', 'https://openalex.org/W1978264303', 'https://openalex.org/W2187089797', 'https://openalex.org/W304834817']",2022-11-01
https://openalex.org/W4287848452,https://doi.org/10.1109/wintechcon55229.2022.9832314,A Faster Approach For Direct Speech to Speech Translation,"As the world is pacing towards globalization, the demand for automatic language translators is increasing rapidly. Traditional translation systems consist of multiple steps like speech recognition, text to text machine translation, and speech generation. Issue with these systems are, latency due to multiple steps and error propagation from first steps toward last steps. Another challenge is that many spoken languages do not have text representation, so traditional system involving speech to text and text to text translation do not work. In this paper, we are presenting a recurrent neural network (RNN) based translation system that can generate a direct waveform of target language audio. We have used the sparse coding technique for the extraction and inversion of audio features. An attention-based multi-layered sequence to sequence model is trained using a novel technique on a dataset of Spanish to English audio and no intermediate text representation is used while training or inference. We have done performance comparison of proposed approaches using latency, bilingual evaluation understudy (BLEU) score and Perceptual Evaluation of Speech Quality PESQ score analysis. The resulting system provides a very fast translation with good translation accuracy and audio quality.","['https://openalex.org/W2136545725', 'https://openalex.org/W1966206566', 'https://openalex.org/W3036878939', 'https://openalex.org/W6652311901', 'https://openalex.org/W3142087749', 'https://openalex.org/W2972495969', 'https://openalex.org/W3007068036', 'https://openalex.org/W2752796333', 'https://openalex.org/W6799081819', 'https://openalex.org/W3142316150', 'https://openalex.org/W6773238793', 'https://openalex.org/W6679434410', 'https://openalex.org/W6728610325', 'https://openalex.org/W6621543089', 'https://openalex.org/W2296073425', 'https://openalex.org/W2016589492', 'https://openalex.org/W2005876975', 'https://openalex.org/W2167439752', 'https://openalex.org/W2085400714', 'https://openalex.org/W2063978378', 'https://openalex.org/W2135046866', 'https://openalex.org/W3160085755', 'https://openalex.org/W2101105183', 'https://openalex.org/W1552314771', 'https://openalex.org/W1902237438', 'https://openalex.org/W2964108850', 'https://openalex.org/W648786980', 'https://openalex.org/W4294149591', 'https://openalex.org/W2519091744', 'https://openalex.org/W3186843219', 'https://openalex.org/W3032433061', 'https://openalex.org/W1924770834', 'https://openalex.org/W2006969979', 'https://openalex.org/W2133564696', 'https://openalex.org/W4310895557', 'https://openalex.org/W3119000810']",2022-06-02
https://openalex.org/W4328054436,https://doi.org/10.1109/ncc56989.2023.10067896,Investigation Of Data Augmentation Techniques For Bi-LSTM Based Direct Speech To Speech Translation,"Direct speech-to-speech translation (DS2ST) system translates the speech in the source language directly to the speech in the target language. It has been shown in the literature that deep learning systems trained using parallel datasets have given a good translation of the speech. However, getting large datasets to train deep learning networks for DS2ST tasks extensively is not easy. Also, the parallel data might not capture the variabilities like session, gender, speaker, and domain variation that might be present in the real-world dataset. In this work, we explore the data-augmentation techniques such that the pool of the training data can be increased and the DS2ST task can be generalized for all the variations. This work uses noise injection, speed perturbation, pitch perturbation, and vocal tract modification-based data-augmentation approaches as an initial attempt. From the experimental results, it has been found that these augmentation approaches improve the performance of the DS2ST system when compared with the clean/original data. Mel-cepstral distortion (MCD) and intelligibility score (IS) are used as metrics to compare the translated speech with the target language speech. Among the augmentation approaches explored, speed perturbation provides the best improvement of 6.125 in terms of MCD. Vocal tract modification improves the performance of the speaker variability in the dataset. This study shows the robustness of the DS2ST system trained on augmented data.","['https://openalex.org/W2051745966', 'https://openalex.org/W1978264303', 'https://openalex.org/W2048458228', 'https://openalex.org/W6799081819', 'https://openalex.org/W2972495969', 'https://openalex.org/W3017535695', 'https://openalex.org/W3142316150', 'https://openalex.org/W3180374548', 'https://openalex.org/W2145095987', 'https://openalex.org/W2972448360', 'https://openalex.org/W2057853719', 'https://openalex.org/W1579847400', 'https://openalex.org/W1510746193', 'https://openalex.org/W2136545725', 'https://openalex.org/W2144081027', 'https://openalex.org/W3208643357', 'https://openalex.org/W2131774270', 'https://openalex.org/W2998376432', 'https://openalex.org/W2958622154', 'https://openalex.org/W1509691205', 'https://openalex.org/W3007068036', 'https://openalex.org/W83932636', 'https://openalex.org/W1538023239', 'https://openalex.org/W2135708429', 'https://openalex.org/W6610843619', 'https://openalex.org/W2150355110', 'https://openalex.org/W304834817', 'https://openalex.org/W3186843219', 'https://openalex.org/W3095123370']",2023-02-23
https://openalex.org/W4225319245,https://doi.org/10.23919/indiacom54597.2022.9763300,Using Machine Learning for Speech Extraction and Translation: HiTEK Languages,"Speech processing deals with retrieving and vocalizing conversational words/sentences i.e. articulatory phonetics, manner of articulation, place of articulation, articulatory gestures, articulatory phonology, articulatory speech recognition, and articulatory synthesis from Multilingual Source to Target language. The research focuses on multilingual speech recording in a single utterance and translation to a target language. Greedy method is used for fetching speech from the user. It consists of the grammatical structures of the speech in the dictionary using cohesion based method for term similarity. It translates speech to text then maps text to a set of phones resulting in target language speech. Multilingual Supervised Speech Dictionary is built for speech to speech translation, currently consisting of four languages Hindi, Telugu, English and Kannada with 100 phones for each language.","['https://openalex.org/W2120223668', 'https://openalex.org/W2294084151', 'https://openalex.org/W1510746193', 'https://openalex.org/W3007068036', 'https://openalex.org/W2519334175', 'https://openalex.org/W6729288640', 'https://openalex.org/W2434696539', 'https://openalex.org/W1580178421', 'https://openalex.org/W2535443429', 'https://openalex.org/W2240349907', 'https://openalex.org/W2032369741', 'https://openalex.org/W2081021839', 'https://openalex.org/W2501108756', 'https://openalex.org/W2549640480', 'https://openalex.org/W2010800472']",2022-03-23
https://openalex.org/W4297841754,https://doi.org/10.21437/interspeech.2022-10568,From Start to Finish: Latency Reduction Strategies for Incremental Speech Synthesis in Simultaneous Speech-to-Speech Translation,"Speech-to-speech translation (S2ST) converts input speech to speech in another language. A challenge of delivering S2ST in real time is the accumulated delay between the translation and speech synthesis modules. While recently incremental text-to-speech (iTTS) models have shown large quality improvements, they typically require additional future text inputs to reach optimal performance. In this work, we minimize the initial waiting time of iTTS by adapting the upstream speech translator to generate high-quality pseudo lookahead for the speech synthesizer. After mitigating the initial delay, we demonstrate that the duration of synthesized speech also plays a crucial role on latency. We formalize this as a latency metric and then present a simple yet effective duration-scaling approach for latency reduction. Our approaches consistently reduce latency by 0.2-0.5 second without sacrificing speech translation quality.","['https://openalex.org/W2964243274', 'https://openalex.org/W4226021270', 'https://openalex.org/W2396366106', 'https://openalex.org/W3115075512', 'https://openalex.org/W3197407562', 'https://openalex.org/W2963532001', 'https://openalex.org/W3092424727', 'https://openalex.org/W3007068036', 'https://openalex.org/W2972802841', 'https://openalex.org/W3048023795', 'https://openalex.org/W2963609956', 'https://openalex.org/W2097203679', 'https://openalex.org/W3033411150', 'https://openalex.org/W2747874407', 'https://openalex.org/W2951456627', 'https://openalex.org/W3012492057', 'https://openalex.org/W2963250244', 'https://openalex.org/W2986976179', 'https://openalex.org/W3030437843', 'https://openalex.org/W3198429080', 'https://openalex.org/W2962784628', 'https://openalex.org/W2250981492', 'https://openalex.org/W2136545725', 'https://openalex.org/W3083224111', 'https://openalex.org/W1964771471', 'https://openalex.org/W4206534379', 'https://openalex.org/W3161782335', 'https://openalex.org/W3186843219', 'https://openalex.org/W3092028330', 'https://openalex.org/W3036601975']",2022-09-16
https://openalex.org/W3205328248,,Incremental Speech Synthesis For Speech-To-Speech Translation.,"In a speech-to-speech translation (S2ST) pipeline, the text-to-speech (TTS) module is an important component for delivering the translated speech to users. To enable incremental S2ST, the TTS module must be capable of synthesizing and playing utterances while its input text is still streaming in. In this work, we focus on improving the incremental synthesis performance of TTS models. With a simple data augmentation strategy based on prefixes, we are able to improve the incremental TTS quality to approach offline performance. Furthermore, we bring our incremental TTS system to the practical scenario in combination with an upstream simultaneous speech translation system, and show the gains also carry over to this use-case. In addition, we propose latency metrics tailored to S2ST applications, and investigate methods for latency reduction in this context.","['https://openalex.org/W3099782249', 'https://openalex.org/W3211696375', 'https://openalex.org/W3037057938', 'https://openalex.org/W3180374548', 'https://openalex.org/W2972895078', 'https://openalex.org/W2972495969', 'https://openalex.org/W3162000275', 'https://openalex.org/W3012492057', 'https://openalex.org/W3130016944', 'https://openalex.org/W2973048981', 'https://openalex.org/W2936969148', 'https://openalex.org/W3037793211', 'https://openalex.org/W2889095150', 'https://openalex.org/W3118578889', 'https://openalex.org/W3098403858', 'https://openalex.org/W2517566275', 'https://openalex.org/W3100608856', 'https://openalex.org/W3174758275', 'https://openalex.org/W1964771471', 'https://openalex.org/W3048023795', 'https://openalex.org/W3092316169', 'https://openalex.org/W3197407562', 'https://openalex.org/W3037465386', 'https://openalex.org/W3115075512', 'https://openalex.org/W2747874407', 'https://openalex.org/W3007068036', 'https://openalex.org/W2986976179', 'https://openalex.org/W2972802841', 'https://openalex.org/W3169320628', 'https://openalex.org/W3186843219', 'https://openalex.org/W44352932', 'https://openalex.org/W3186200218', 'https://openalex.org/W2056890381', 'https://openalex.org/W3104081910']",2021-10-15
https://openalex.org/W3019991683,https://doi.org/10.4108/eai.13-7-2018.164109,A Direct Speech-to-Speech Neural Network Methodology for Spanish-English Translation,"In this work, a novel direct speech-to-speech methodology for translation is proposed; it is based on an LSTMneural network structure which has proven useful for translation in the classical way, i.e., the one consistingof three stages: speech-to-text conversion, text-to-text translation, and text-to-speech synthesis. In contrastwith traditional approaches, the one in this work belongs to the recently appeared idea of direct translationwithout text representation, as this sort of training better corresponds to the way oral language learning takesplace in humans. As a proof of concept digits are translated from an audio source in Spanish and pronouncedas an audio signal in English. Advantages and disadvantages of the proposal when compared with traditionalmethodologies are discussed.","['https://openalex.org/W6600881165', 'https://openalex.org/W2150355110', 'https://openalex.org/W6601659037', 'https://openalex.org/W4200501717', 'https://openalex.org/W1423339008', 'https://openalex.org/W1491238342', 'https://openalex.org/W4234904293', 'https://openalex.org/W6609809004', 'https://openalex.org/W1539670134', 'https://openalex.org/W6733584515', 'https://openalex.org/W2804792817', 'https://openalex.org/W6682391572', 'https://openalex.org/W126441537', 'https://openalex.org/W1569512666', 'https://openalex.org/W2296283641', 'https://openalex.org/W2000171511', 'https://openalex.org/W296599067', 'https://openalex.org/W2251805006', 'https://openalex.org/W2909502535', 'https://openalex.org/W2138997758', 'https://openalex.org/W3007068036', 'https://openalex.org/W2721931776', 'https://openalex.org/W2148603752', 'https://openalex.org/W2143630575', 'https://openalex.org/W2544767710', 'https://openalex.org/W2742947407', 'https://openalex.org/W4233045210', 'https://openalex.org/W2964285380', 'https://openalex.org/W1637570796', 'https://openalex.org/W2964075209', 'https://openalex.org/W2939855790', 'https://openalex.org/W120161273', 'https://openalex.org/W3099313647', 'https://openalex.org/W1522301498', 'https://openalex.org/W2103819988', 'https://openalex.org/W2152859600', 'https://openalex.org/W1992272902', 'https://openalex.org/W2972495969', 'https://openalex.org/W2017560935', 'https://openalex.org/W1981025032']",2018-07-13
https://openalex.org/W3092674239,https://doi.org/10.48550/arxiv.2010.05959,Towards Induction of Structured Phoneme Inventories,"This extended abstract surveying the work on phonological typology was prepared for ""SIGTYP 2020: The Second Workshop on Computational Research in Linguistic Typology"" to be held at EMNLP 2020.","['https://openalex.org/W2746555167', 'https://openalex.org/W2572670101', 'https://openalex.org/W2972708855', 'https://openalex.org/W3017568157', 'https://openalex.org/W2973034126', 'https://openalex.org/W3037580942', 'https://openalex.org/W3032617810', 'https://openalex.org/W3015877095', 'https://openalex.org/W2978709484', 'https://openalex.org/W2739967986', 'https://openalex.org/W2945774221', 'https://openalex.org/W3035490255', 'https://openalex.org/W2398528382', 'https://openalex.org/W3024040651', 'https://openalex.org/W2895651543', 'https://openalex.org/W2936123380', 'https://openalex.org/W2127846433', 'https://openalex.org/W3027324582', 'https://openalex.org/W1778492285', 'https://openalex.org/W2668277942', 'https://openalex.org/W2935756752', 'https://openalex.org/W3007068036', 'https://openalex.org/W2963069394', 'https://openalex.org/W2402366697']",2020-10-12
https://openalex.org/W3170928308,https://doi.org/10.48550/arxiv.2106.04298,Unsupervised Word Segmentation from Discrete Speech Units in Low-Resource Settings,"Documenting languages helps to prevent the extinction of endangered dialects, many of which are otherwise expected to disappear by the end of the century. When documenting oral languages, unsupervised word segmentation (UWS) from speech is a useful, yet challenging, task. It consists in producing time-stamps for slicing utterances into smaller segments corresponding to words, being performed from phonetic transcriptions, or in the absence of these, from the output of unsupervised speech discretization models. These discretization models are trained using raw speech only, producing discrete speech units that can be applied for downstream (text-based) tasks. In this paper we compare five of these models: three Bayesian and two neural approaches, with regards to the exploitability of the produced units for UWS. For the UWS task, we experiment with two models, using as our target language the Mboshi (Bantu C25), an unwritten language from Congo-Brazzaville. Additionally, we report results for Finnish, Hungarian, Romanian and Russian in equally low-resource settings, using only 4 hours of speech. Our results suggest that neural models for speech discretization are difficult to exploit in our setting, and that it might be necessary to adapt them to limit sequence length. We obtain our best UWS results by using Bayesian models that produce high quality, yet compressed, discrete representations of the input speech signal.","['https://openalex.org/W3029422373', 'https://openalex.org/W3100202343', 'https://openalex.org/W3097485645', 'https://openalex.org/W2466918907', 'https://openalex.org/W2895097770', 'https://openalex.org/W2347098582', 'https://openalex.org/W2808682925', 'https://openalex.org/W2401271873', 'https://openalex.org/W2407614114', 'https://openalex.org/W2996383576', 'https://openalex.org/W2111668269', 'https://openalex.org/W2972574141', 'https://openalex.org/W2973026522', 'https://openalex.org/W2401396251', 'https://openalex.org/W2195354', 'https://openalex.org/W1563026167', 'https://openalex.org/W2515167330', 'https://openalex.org/W2963378435', 'https://openalex.org/W2572097499', 'https://openalex.org/W2055408826', 'https://openalex.org/W2963819008', 'https://openalex.org/W2985777044', 'https://openalex.org/W2964169922', 'https://openalex.org/W2963799213', 'https://openalex.org/W2547875792', 'https://openalex.org/W2173413395', 'https://openalex.org/W2242818861', 'https://openalex.org/W2346964103', 'https://openalex.org/W2483390977', 'https://openalex.org/W2126377586', 'https://openalex.org/W3125709657', 'https://openalex.org/W2962784628', 'https://openalex.org/W2586232309', 'https://openalex.org/W3007068036', 'https://openalex.org/W2963620343', 'https://openalex.org/W2883972335', 'https://openalex.org/W3112613336', 'https://openalex.org/W2025482506', 'https://openalex.org/W2101281673', 'https://openalex.org/W2963678298', 'https://openalex.org/W2126449874', 'https://openalex.org/W1778492285', 'https://openalex.org/W175497273', 'https://openalex.org/W3093096176', 'https://openalex.org/W2972704741', 'https://openalex.org/W2084534958', 'https://openalex.org/W2582956876', 'https://openalex.org/W3032598645']",2021-06-08
https://openalex.org/W4393657127,https://doi.org/10.1109/o-cocosda60357.2023.10482964,Comparative Analysis of Direct Speech-to-Speech Translation and Voice Conversion Using Bi-LSTM,"This study aims to conduct a comparative analysis between Direct Speech to Speech Translation (DS2ST) and Voice Conversion (VC) utilizing a Bi-Directional Long Short-Term Memory (Bi-LSTM) network. The process in VC entails mapping speaker-specific characteristics between the target and source speakers. On the other hand, DS2ST aims to map linguistic information of speech of two different spoken languages, which may be for the same speaker. Since both involve similar strategies and there is a rich literature on VC, it may be appropriate to do a comparative study between the VC and DS2ST frameworks. Accordingly, this work develops the VC and DS2ST model using the Bi-LSTM and analyzes the research. The findings reveal that VC can be performed with less number of layers compared to DS2ST using the Bi-LSTM model. This may be attributed to the differences among the speaker specific information and sound unit information. We have evaluated the system up to four layers and found that the MCD value improves from 7.484 to 6.061 in the case of DS2ST, while in the case of VC, it is from 5.549 to 5.370.","['https://openalex.org/W2051745966', 'https://openalex.org/W1510746193', 'https://openalex.org/W2136545725', 'https://openalex.org/W2081592734', 'https://openalex.org/W2146173057', 'https://openalex.org/W1579847400', 'https://openalex.org/W2135708429', 'https://openalex.org/W2145095987', 'https://openalex.org/W1969443802', 'https://openalex.org/W3017535695', 'https://openalex.org/W3142316150', 'https://openalex.org/W4313203218', 'https://openalex.org/W2972495969', 'https://openalex.org/W4328054436', 'https://openalex.org/W3180374548', 'https://openalex.org/W3007068036', 'https://openalex.org/W4213198175', 'https://openalex.org/W2121387787', 'https://openalex.org/W2136922672', 'https://openalex.org/W1993882792', 'https://openalex.org/W2120605154', 'https://openalex.org/W2126143605', 'https://openalex.org/W2086796102', 'https://openalex.org/W1509691205', 'https://openalex.org/W2005708641', 'https://openalex.org/W2605131327', 'https://openalex.org/W2064675550', 'https://openalex.org/W2131774270']",2023-12-04
https://openalex.org/W4226132755,https://doi.org/10.21437/interspeech.2022-489,VQTTS: High-Fidelity Text-to-Speech Synthesis with Self-Supervised VQ Acoustic Feature,"The mainstream neural text-to-speech(TTS) pipeline is a cascade system,\nincluding an acoustic model(AM) that predicts acoustic feature from the input\ntranscript and a vocoder that generates waveform according to the given\nacoustic feature. However, the acoustic feature in current TTS systems is\ntypically mel-spectrogram, which is highly correlated along both time and\nfrequency axes in a complicated way, leading to a great difficulty for the AM\nto predict. Although high-fidelity audio can be generated by recent neural\nvocoders from ground-truth(GT) mel-spectrogram, the gap between the GT and the\npredicted mel-spectrogram from AM degrades the performance of the entire TTS\nsystem. In this work, we propose VQTTS, consisting of an AM txt2vec and a\nvocoder vec2wav, which uses self-supervised vector-quantized(VQ) acoustic\nfeature rather than mel-spectrogram. We redesign both the AM and the vocoder\naccordingly. In particular, txt2vec basically becomes a classification model\ninstead of a traditional regression model while vec2wav uses an additional\nfeature encoder before HifiGAN generator for smoothing the discontinuous\nquantized feature. Our experiments show that vec2wav achieves better\nreconstruction performance than HifiGAN when using self-supervised VQ acoustic\nfeature. Moreover, our entire TTS system VQTTS achieves state-of-the-art\nperformance in terms of naturalness among all current publicly available TTS\nsystems.\n","['https://openalex.org/W2982602185', 'https://openalex.org/W4301371414', 'https://openalex.org/W3162271107', 'https://openalex.org/W3095491807', 'https://openalex.org/W3097777922', 'https://openalex.org/W2129142580', 'https://openalex.org/W2990440871', 'https://openalex.org/W2973049979', 'https://openalex.org/W4221167022', 'https://openalex.org/W3016021263', 'https://openalex.org/W3180374548', 'https://openalex.org/W3033411150', 'https://openalex.org/W3015282541', 'https://openalex.org/W2963609956', 'https://openalex.org/W4320013936', 'https://openalex.org/W4287360867', 'https://openalex.org/W4200635400', 'https://openalex.org/W2519091744', 'https://openalex.org/W3150807214', 'https://openalex.org/W3026874504', 'https://openalex.org/W4308170756', 'https://openalex.org/W3197324626', 'https://openalex.org/W4287121924', 'https://openalex.org/W2896457183', 'https://openalex.org/W3151450932', 'https://openalex.org/W3092028330', 'https://openalex.org/W2085628288', 'https://openalex.org/W3169320628', 'https://openalex.org/W3036601975', 'https://openalex.org/W2088632109', 'https://openalex.org/W2979476256', 'https://openalex.org/W4214968481', 'https://openalex.org/W1552314771', 'https://openalex.org/W3096429957', 'https://openalex.org/W2964243274', 'https://openalex.org/W3140429000']",2022-09-16
https://openalex.org/W4393152865,https://doi.org/10.1609/aaai.v38i16.29769,MM-TTS: Multi-Modal Prompt Based Style Transfer for Expressive Text-to-Speech Synthesis,"The style transfer task in Text-to-Speech (TTS) refers to the process of transferring style information into text content to generate corresponding speech with a specific style. However, most existing style transfer approaches are either based on fixed emotional labels or reference speech clips, which cannot achieve flexible style transfer. Recently, some methods have adopted text descriptions to guide style transfer. In this paper, we propose a more flexible multi-modal and style controllable TTS framework named MM-TTS. It can utilize any modality as the prompt in unified multi-modal prompt space, including reference speech, emotional facial images, and text descriptions, to control the style of the generated speech in a system. The challenges of modeling such a multi-modal style controllable TTS mainly lie in two aspects: 1) aligning the multi-modal information into a unified style space to enable the input of arbitrary modality as the style prompt in a single system, and 2) efficiently transferring the unified style representation into the given text content, thereby empowering the ability to generate prompt style-related voice. To address these problems, we propose an aligned multi-modal prompt encoder that embeds different modalities into a unified style space, supporting style transfer for different modalities. Additionally, we present a new adaptive style transfer method named Style Adaptive Convolutions (SAConv) to achieve a better style representation. Furthermore, we design a Rectified Flow based Refiner to solve the problem of over-smoothing Mel-spectrogram and generate audio of higher fidelity. Since there is no public dataset for multi-modal TTS, we construct a dataset named MEAD-TTS, which is related to the field of expressive talking head. Our experiments on the MEAD-TTS dataset and out-of-domain datasets demonstrate that MM-TTS can achieve satisfactory results based on multi-modal prompts. The audio samples and constructed dataset are available at https://multimodal-tts.github.io.","['https://openalex.org/W3177097896', 'https://openalex.org/W3033194228', 'https://openalex.org/W3096806995', 'https://openalex.org/W4379933641', 'https://openalex.org/W4309873268', 'https://openalex.org/W6779823529', 'https://openalex.org/W4285483538', 'https://openalex.org/W6917585676', 'https://openalex.org/W3026874504', 'https://openalex.org/W6793030661', 'https://openalex.org/W3092028330', 'https://openalex.org/W4322717304', 'https://openalex.org/W6757079273', 'https://openalex.org/W3162046328', 'https://openalex.org/W3140294857', 'https://openalex.org/W4379251367', 'https://openalex.org/W2793479148', 'https://openalex.org/W3168527213', 'https://openalex.org/W4311000453', 'https://openalex.org/W2946200149', 'https://openalex.org/W6811003946', 'https://openalex.org/W3034038336', 'https://openalex.org/W3004823919', 'https://openalex.org/W4224920206', 'https://openalex.org/W3099284785', 'https://openalex.org/W2794490148', 'https://openalex.org/W6761768988', 'https://openalex.org/W2904459034', 'https://openalex.org/W2035372623', 'https://openalex.org/W3036167779', 'https://openalex.org/W3196584150', 'https://openalex.org/W3166396011', 'https://openalex.org/W4380714544', 'https://openalex.org/W4287248095', 'https://openalex.org/W1522301498', 'https://openalex.org/W2770743791', 'https://openalex.org/W1959608418', 'https://openalex.org/W2964243274', 'https://openalex.org/W4318907788', 'https://openalex.org/W2972359262', 'https://openalex.org/W4375869257', 'https://openalex.org/W4214968481', 'https://openalex.org/W4385822757', 'https://openalex.org/W4280561221', 'https://openalex.org/W3095545636', 'https://openalex.org/W3036601975', 'https://openalex.org/W4226332109', 'https://openalex.org/W4390040941', 'https://openalex.org/W4385822787', 'https://openalex.org/W4287236468', 'https://openalex.org/W2951418500', 'https://openalex.org/W4372263908', 'https://openalex.org/W4313679638', 'https://openalex.org/W4297676498', 'https://openalex.org/W1924770834']",2024-03-24
https://openalex.org/W3197216873,https://doi.org/10.21437/interspeech.2021-802,Rich Prosody Diversity Modelling with Phone-Level Mixture Density Network,"Generating natural speech with diverse and smooth prosody pattern is a challenging task.Although random sampling with phone-level prosody distribution has been investigated to generate different prosody patterns, the diversity of the generated speech is still very limited and far from what can be achieved by human.This is largely due to the use of uni-modal distribution, such as single Gaussian, in the prior works of phonelevel prosody modelling.In this work, we propose a novel approach that models phone-level prosodies with GMM based mixture density network (GMM-MDN).Experiments on the LJSpeech dataset demonstrate that phone-level prosodies can precisely control the synthetic speech and GMM-MDN can generate more natural and smooth prosody pattern than a single Gaussian.Subjective evaluations further show that the proposed approach not only achieves better naturalness, but also significantly improves the prosody diversity in synthetic speech without the need of manual control.","['https://openalex.org/W2892140764', 'https://openalex.org/W2964243274', 'https://openalex.org/W4295731579', 'https://openalex.org/W2964138190', 'https://openalex.org/W2100969003', 'https://openalex.org/W2794490148', 'https://openalex.org/W2107860279', 'https://openalex.org/W4214968481', 'https://openalex.org/W2962691331', 'https://openalex.org/W2595110011', 'https://openalex.org/W2946200149', 'https://openalex.org/W2396366106', 'https://openalex.org/W1494198834', 'https://openalex.org/W2970006822', 'https://openalex.org/W4385245566', 'https://openalex.org/W3095491807', 'https://openalex.org/W3033411150', 'https://openalex.org/W3016021263', 'https://openalex.org/W142684478', 'https://openalex.org/W1990505856', 'https://openalex.org/W1579853615', 'https://openalex.org/W1522301498', 'https://openalex.org/W2002342963', 'https://openalex.org/W3151450932', 'https://openalex.org/W2963609956']",2021-08-27
https://openalex.org/W3194613004,https://doi.org/10.21437/interspeech.2021-1407,Improving Performance of Seen and Unseen Speech Style Transfer in End-to-End Neural TTS,"End-to-end neural TTS training has shown improved performance in speech style transfer.However, the improvement is still limited by the training data in both target styles and speakers.Inadequate style transfer performance occurs when the trained TTS tries to transfer the speech to a target style from a new speaker with an unknown, arbitrary style.In this paper, we propose a new approach to style transfer for both seen and unseen styles, with disjoint, multi-style datasets, i.e., datasets of different styles are recorded, each individual style is by one speaker with multiple utterances.To encode the style information, we adopt an inverse autoregressive flow (IAF) structure to improve the variational inference.The whole system is optimized to minimize a weighed sum of four different loss functions: 1) a reconstruction loss to measure the distortions in both source and target reconstructions; 2) an adversarial loss to ""fool"" a well-trained discriminator; 3) a style distortion loss to measure the expected style loss after the transfer; 4) a cycle consistency loss to preserve the speaker identity of the source after the transfer.Experiments demonstrate, both objectively and subjectively, the effectiveness of the proposed approach for seen and unseen style transfer tasks.The performance of the new approach is better and more robust than those of four baseline systems of the prior art.","['https://openalex.org/W2963091184', 'https://openalex.org/W2962691331', 'https://openalex.org/W4293411471', 'https://openalex.org/W4320013936', 'https://openalex.org/W4214968481', 'https://openalex.org/W4297663084', 'https://openalex.org/W3094785744', 'https://openalex.org/W3139170550', 'https://openalex.org/W3007580377', 'https://openalex.org/W3146550708', 'https://openalex.org/W2130942839', 'https://openalex.org/W2964243274', 'https://openalex.org/W2133564696', 'https://openalex.org/W4295731579', 'https://openalex.org/W2932022923', 'https://openalex.org/W3015796413', 'https://openalex.org/W2794490148', 'https://openalex.org/W3007067948', 'https://openalex.org/W3135644023', 'https://openalex.org/W2766812927', 'https://openalex.org/W2904459034', 'https://openalex.org/W2996573371', 'https://openalex.org/W4289383906', 'https://openalex.org/W3168542456', 'https://openalex.org/W2963609956', 'https://openalex.org/W2885800352', 'https://openalex.org/W2808706139']",2021-08-27
https://openalex.org/W3214634826,https://doi.org/10.18653/v1/2021.conll-1.42,Controlling Prosody in End-to-End TTS: A Case Study on Contrastive Focus Generation,"While End-2-End Text-to-Speech (TTS) has made significant progresses over the past few years, these systems still lack intuitive user controls over prosody. For instance, generating speech with fine-grained prosody control (prosodic prominence, contextually appropriate emotions) is still an open challenge. In this paper, we investigate whether we can control prosody directly from the input text, in order to code information related to contrastive focus which emphasizes a specific word that is contrary to the presuppositions of the interlocutor. We build and share a specific dataset for this purpose and show that it allows to train a TTS system were this fine-grained prosodic feature can be correctly conveyed using control tokens. Our evaluation compares synthetic and natural utterances and shows that prosodic patterns of contrastive focus (variations of Fo, Intensity and Duration) can be learnt accurately. Such a milestone is important to allow, for example, smart speakers to be programmatically controlled in terms of output prosody.","['https://openalex.org/W2970730223', 'https://openalex.org/W1570629387', 'https://openalex.org/W4237528249', 'https://openalex.org/W2946200149', 'https://openalex.org/W3004823919', 'https://openalex.org/W2404915168', 'https://openalex.org/W3025528898', 'https://openalex.org/W3097003111', 'https://openalex.org/W2081539155', 'https://openalex.org/W2918595871', 'https://openalex.org/W2963300588', 'https://openalex.org/W2102146461', 'https://openalex.org/W2971134989', 'https://openalex.org/W2766812927', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963122608', 'https://openalex.org/W3034949308', 'https://openalex.org/W397522103', 'https://openalex.org/W2964243274', 'https://openalex.org/W2747874407', 'https://openalex.org/W2166481425', 'https://openalex.org/W2996573371', 'https://openalex.org/W3033411150', 'https://openalex.org/W2976532777', 'https://openalex.org/W3197587869', 'https://openalex.org/W3096442195', 'https://openalex.org/W2019597205', 'https://openalex.org/W2242221029', 'https://openalex.org/W2995435108', 'https://openalex.org/W2128160875', 'https://openalex.org/W2482230372', 'https://openalex.org/W2896457183', 'https://openalex.org/W4214968481', 'https://openalex.org/W2980282514', 'https://openalex.org/W2963272440', 'https://openalex.org/W3016021263', 'https://openalex.org/W2035108931', 'https://openalex.org/W2964138190', 'https://openalex.org/W2794490148', 'https://openalex.org/W2152834109', 'https://openalex.org/W3144667183']",2021-01-01
https://openalex.org/W3163003432,https://doi.org/10.1109/icassp39728.2021.9413604,Prosodic Clustering for Phoneme-Level Prosody Control in End-to-End Speech Synthesis,"This paper presents a method for controlling the prosody at the phoneme level\nin an autoregressive attention-based text-to-speech system. Instead of learning\nlatent prosodic features with a variational framework as is commonly done, we\ndirectly extract phoneme-level F0 and duration features from the speech data in\nthe training set. Each prosodic feature is discretized using unsupervised\nclustering in order to produce a sequence of prosodic labels for each\nutterance. This sequence is used in parallel to the phoneme sequence in order\nto condition the decoder with the utilization of a prosodic encoder and a\ncorresponding attention module. Experimental results show that the proposed\nmethod retains the high quality of generated speech, while allowing\nphoneme-level control of F0 and duration. By replacing the F0 cluster centroids\nwith musical notes, the model can also provide control over the note and octave\nwithin the range of the speaker.\n","['https://openalex.org/W6631190155', 'https://openalex.org/W3015440759', 'https://openalex.org/W2974194285', 'https://openalex.org/W6770943558', 'https://openalex.org/W3097892637', 'https://openalex.org/W6764068927', 'https://openalex.org/W2938102059', 'https://openalex.org/W6768443183', 'https://openalex.org/W2973158936', 'https://openalex.org/W3095389792', 'https://openalex.org/W3015922793', 'https://openalex.org/W6674330103', 'https://openalex.org/W6750489868', 'https://openalex.org/W6703356863', 'https://openalex.org/W2795109282', 'https://openalex.org/W6763316926', 'https://openalex.org/W6714142977', 'https://openalex.org/W6755300632', 'https://openalex.org/W2964138190', 'https://openalex.org/W2904459034', 'https://openalex.org/W2964243274', 'https://openalex.org/W3016021263', 'https://openalex.org/W2963609956', 'https://openalex.org/W6866227203', 'https://openalex.org/W6676245417', 'https://openalex.org/W6603616073', 'https://openalex.org/W3095459301', 'https://openalex.org/W2963091184', 'https://openalex.org/W6763100362', 'https://openalex.org/W6638273328', 'https://openalex.org/W2962970071', 'https://openalex.org/W2963568578', 'https://openalex.org/W3101882441', 'https://openalex.org/W88081813', 'https://openalex.org/W2336378194', 'https://openalex.org/W4295731579', 'https://openalex.org/W2948211236', 'https://openalex.org/W2996573371', 'https://openalex.org/W2107831318', 'https://openalex.org/W2095705004', 'https://openalex.org/W4214968481', 'https://openalex.org/W2794490148', 'https://openalex.org/W2977311057', 'https://openalex.org/W2945544731', 'https://openalex.org/W2963927338', 'https://openalex.org/W2964121744', 'https://openalex.org/W2991417167', 'https://openalex.org/W4285719527', 'https://openalex.org/W4293714597', 'https://openalex.org/W2952269766', 'https://openalex.org/W2963272440', 'https://openalex.org/W1810943226', 'https://openalex.org/W1522301498', 'https://openalex.org/W2948238043', 'https://openalex.org/W4395958166', 'https://openalex.org/W4289383906']",2021-05-13
https://openalex.org/W4280604450,https://doi.org/10.18653/v1/2022.acl-long.30,Cross-Utterance Conditioned VAE for Non-Autoregressive Text-to-Speech,"Yang Li, Cheng Yu, Guangzhi Sun, Hua Jiang, Fanglei Sun, Weiqin Zu, Ying Wen, Yang Yang, Jun Wang. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.","['https://openalex.org/W6600995197', 'https://openalex.org/W3197957589', 'https://openalex.org/W3038172701', 'https://openalex.org/W2964138190', 'https://openalex.org/W3016136182', 'https://openalex.org/W3163339651', 'https://openalex.org/W4385245566', 'https://openalex.org/W3155768880', 'https://openalex.org/W2973217961', 'https://openalex.org/W2885800352', 'https://openalex.org/W3092028330', 'https://openalex.org/W2107860279', 'https://openalex.org/W2963609956', 'https://openalex.org/W2946200149', 'https://openalex.org/W2519091744', 'https://openalex.org/W2907262790', 'https://openalex.org/W1959608418', 'https://openalex.org/W3197294703', 'https://openalex.org/W2107740512', 'https://openalex.org/W4289383906', 'https://openalex.org/W3198152857', 'https://openalex.org/W2962691331', 'https://openalex.org/W2952127920', 'https://openalex.org/W4287199948', 'https://openalex.org/W3163338468', 'https://openalex.org/W4288107051', 'https://openalex.org/W3162948689', 'https://openalex.org/W3197324626', 'https://openalex.org/W3033411150', 'https://openalex.org/W2896457183', 'https://openalex.org/W2188365844', 'https://openalex.org/W2964243274', 'https://openalex.org/W3016021263', 'https://openalex.org/W4214968481']",2022-01-01
https://openalex.org/W4386074659,https://doi.org/10.11159/mhci23.111,GAN-Based Fine-Grained Feature Modeling For Zero-Shot Voice Cloning,"With the continuous development of deep learning and speech signal processing, speech synthesis technology has greatly improved in naturalness and comprehensibility, and many application technologies such as artificial intelligence voice assistant and personalized navigation have been widely used in real life, and the demand for personalized speech synthesis is increasing.Personalized speech synthesis requires models that can achieve speech timbre migration, also known as speech reproduction, with only a small number of target speaker speech samples.However, since human speech is highly expressive and contains rich information, including speaker identity information, prosody, rhythm, emotion and other factors, the limited speech data will lead to poor similarity and rhythmic performance of the model-generated speech, and the model needs to be fine-tuned to improve the quality of the synthesized speech.Therefore, personalized speech synthesis with few samples is a very challenging task.To achieve the goal of speech cloning, this paper proposes a personalized speech synthesis method based on FastSpeech2.By using fine-grained feature modeling module containing prosody extractor and prosody predictor, and a training strategy based on Generative adversarial network (GAN) and meta-learning, it is realized that personalized speech with high similarity and naturalness can be generated with a very short reference audio.The subjective and objective experiments also demonstrate that the model proposed in this paper can achieve high quality speech replication without fine-tuning the model under a few or even a single reference audio of the target speaker.","['https://openalex.org/W6755135894', 'https://openalex.org/W6748588790', 'https://openalex.org/W6790220310', 'https://openalex.org/W1492383498', 'https://openalex.org/W6792650190', 'https://openalex.org/W6791415481', 'https://openalex.org/W3016159759', 'https://openalex.org/W3004823919', 'https://openalex.org/W3116834994', 'https://openalex.org/W6796730497', 'https://openalex.org/W3046670326', 'https://openalex.org/W6601083237', 'https://openalex.org/W1561109978', 'https://openalex.org/W2946200149', 'https://openalex.org/W6778823374', 'https://openalex.org/W2945078028', 'https://openalex.org/W6798096123', 'https://openalex.org/W2795109282', 'https://openalex.org/W6745697700', 'https://openalex.org/W6738277540', 'https://openalex.org/W6793783828', 'https://openalex.org/W2794490148', 'https://openalex.org/W2556467266', 'https://openalex.org/W3125481789', 'https://openalex.org/W2906797124', 'https://openalex.org/W4308273487', 'https://openalex.org/W4295731579', 'https://openalex.org/W4214968481', 'https://openalex.org/W2957543415', 'https://openalex.org/W3162794600', 'https://openalex.org/W3141350488', 'https://openalex.org/W2766812927', 'https://openalex.org/W3178839419', 'https://openalex.org/W2788357188', 'https://openalex.org/W3095012670', 'https://openalex.org/W28194048', 'https://openalex.org/W3174758275', 'https://openalex.org/W2892620417', 'https://openalex.org/W4287280083', 'https://openalex.org/W3161109662', 'https://openalex.org/W3033411150', 'https://openalex.org/W2619368999', 'https://openalex.org/W3128910262', 'https://openalex.org/W3163906773', 'https://openalex.org/W2952711665']",2023-08-01
https://openalex.org/W4387707208,https://doi.org/10.3233/atde230552,CAMP: A Unified Data Solution for Mandarin Speech Recognition Tasks,"Speech recognition, the transformation of spoken language into written text, is becoming increasingly vital across a broad range of applications. Despite the advancements in end-to-end Neural Network (NN) based speech recognition systems, the requirement for large volumes of annotated audio data tailored to specific scenarios remains a significant challenge. To address this, we introduce a novel approach, the Character Audio Mix-up (CAMP), which synthesizes scenario-specific audio data for Mandarin at a significantly reduced cost and effort. This method concatenates the audio segments of each character’s Pinyin in the text, obtained through force alignment on an existing annotated dataset, to synthesize the audio. These synthesized audios are then used to train the Automatic Speech Recognition (ASR) models. Experiments conducted on the AISHELL-3, and AIDATATANG datasets validate the effectiveness of CAMP, with ASR models trained on CAMP synthesized data performing relatively well compared to those trained with actual data from these datasets. Further, our ablation study reveals that while synthesized audio data can significantly reduce the need for real annotated audio specific to each scenario, it cannot entirely replace real audio. Thus, the importance of real annotated audio data in specific application scenarios is emphasized.","['https://openalex.org/W6631362777', 'https://openalex.org/W6631814027', 'https://openalex.org/W2914584698', 'https://openalex.org/W6601462914', 'https://openalex.org/W3025165719', 'https://openalex.org/W2739427748', 'https://openalex.org/W2939111082', 'https://openalex.org/W6834582571', 'https://openalex.org/W4200416755', 'https://openalex.org/W3004823919', 'https://openalex.org/W6600234944', 'https://openalex.org/W2975100556', 'https://openalex.org/W4289824098', 'https://openalex.org/W3097777922', 'https://openalex.org/W2898727538', 'https://openalex.org/W3008480565', 'https://openalex.org/W2962699523', 'https://openalex.org/W4309044936', 'https://openalex.org/W4226379831', 'https://openalex.org/W2936774411', 'https://openalex.org/W2963827914', 'https://openalex.org/W2888779557', 'https://openalex.org/W2962780374', 'https://openalex.org/W3094002217', 'https://openalex.org/W1855892484', 'https://openalex.org/W3107298252', 'https://openalex.org/W3015449694', 'https://openalex.org/W3197478142', 'https://openalex.org/W1524333225', 'https://openalex.org/W4319586610', 'https://openalex.org/W4210327373', 'https://openalex.org/W2892009249', 'https://openalex.org/W4214968481', 'https://openalex.org/W2407080277', 'https://openalex.org/W1828163288']",2023-10-17
https://openalex.org/W4389519824,https://doi.org/10.18653/v1/2023.findings-emnlp.327,Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis,"Training a high performance end-to-end speech (E2E) processing model requires an enormous amount of labeled speech data, especially in the era of data-centric artificial intelligence. However, labeled speech data are usually scarcer and more expensive for collection, compared to textual data. We propose Latent Synthesis (LaSyn), an efficient textual data utilization framework for E2E speech processing models. We train a latent synthesizer to convert textual data into an intermediate latent representation of a pre-trained speech model. These pseudo acoustic representations of textual data augment acoustic data for model training. We evaluate LaSyn on low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an E2E baseline trained on LibriSpeech train-clean-100, with relative word error rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our E2E baseline by absolute 4.1% for intent classification accuracy and 3.8% for slot filling SLU-F1 on SLURP, and absolute 4.49% and 2.25% for exact match (EM) and EM-Tree accuracies on STOP respectively. With fewer parameters, the results of LaSyn are competitive to published state-of-the-art works. The results demonstrate the quality of the augmented training data.","['https://openalex.org/W2997771882', 'https://openalex.org/W3090702983', 'https://openalex.org/W4225272718', 'https://openalex.org/W2915816387', 'https://openalex.org/W3096251052', 'https://openalex.org/W3161302809', 'https://openalex.org/W4226120743', 'https://openalex.org/W4319653946', 'https://openalex.org/W4221162932', 'https://openalex.org/W3173156538', 'https://openalex.org/W3211224152', 'https://openalex.org/W2804648901', 'https://openalex.org/W4300980246', 'https://openalex.org/W3016010032', 'https://openalex.org/W4288089799', 'https://openalex.org/W2976556660', 'https://openalex.org/W3173110665', 'https://openalex.org/W3207222250', 'https://openalex.org/W2729190387', 'https://openalex.org/W3034999214', 'https://openalex.org/W4225308107', 'https://openalex.org/W3015412890', 'https://openalex.org/W3175976195', 'https://openalex.org/W3107298252', 'https://openalex.org/W3045703328', 'https://openalex.org/W4385573012', 'https://openalex.org/W4319862713', 'https://openalex.org/W4214968481', 'https://openalex.org/W1901129140', 'https://openalex.org/W3036167779', 'https://openalex.org/W3100460087', 'https://openalex.org/W3217767527', 'https://openalex.org/W3026041220', 'https://openalex.org/W4389524500', 'https://openalex.org/W3102854726', 'https://openalex.org/W2896457183', 'https://openalex.org/W3096686110', 'https://openalex.org/W4312933868', 'https://openalex.org/W2952087486', 'https://openalex.org/W4281485151', 'https://openalex.org/W3138431698', 'https://openalex.org/W3095173472', 'https://openalex.org/W4385245566', 'https://openalex.org/W1828163288', 'https://openalex.org/W4288099666', 'https://openalex.org/W1494198834', 'https://openalex.org/W2786860129', 'https://openalex.org/W2986193249', 'https://openalex.org/W4226125322', 'https://openalex.org/W2963718112', 'https://openalex.org/W3082619646', 'https://openalex.org/W3004786215', 'https://openalex.org/W2097550833', 'https://openalex.org/W3163044982', 'https://openalex.org/W3103005696', 'https://openalex.org/W2760505947', 'https://openalex.org/W4389009452', 'https://openalex.org/W4281969232', 'https://openalex.org/W3045492832', 'https://openalex.org/W3162313915', 'https://openalex.org/W3148338017', 'https://openalex.org/W3172148458', 'https://openalex.org/W4297841852', 'https://openalex.org/W4308349017', 'https://openalex.org/W4213178287', 'https://openalex.org/W3015995734', 'https://openalex.org/W3110257065', 'https://openalex.org/W3169320628', 'https://openalex.org/W3015885816', 'https://openalex.org/W4288088457', 'https://openalex.org/W3036601975']",2023-01-01
https://openalex.org/W4225287045,https://doi.org/10.1109/icassp43922.2022.9746296,End-to-End Neural Speech Coding for Real-Time Communications,"Deep-learning based methods have shown their advantages in audio coding over traditional ones but limited attention has been paid on real-time communications (RTC). This paper proposes the TFNet, an end-to-end neural speech codec with low latency for RTC. It takes an encoder-temporal filtering-decoder paradigm that has seldom been investigated in audio coding. An interleaved structure is proposed for temporal filtering to capture both short-term and long-term temporal dependencies. Furthermore, with end-to-end optimization, the TFNet is jointly optimized with speech enhancement and packet loss concealment, yielding a one-for-all network for three tasks. Both subjective and objective results demonstrate the efficiency of the proposed TFNet.","['https://openalex.org/W2972354707', 'https://openalex.org/W2752796333', 'https://openalex.org/W2937484199', 'https://openalex.org/W2889442120', 'https://openalex.org/W2952218014', 'https://openalex.org/W4289665794', 'https://openalex.org/W2963189033', 'https://openalex.org/W6787607767', 'https://openalex.org/W6782481672', 'https://openalex.org/W208085512', 'https://openalex.org/W3016003977', 'https://openalex.org/W2963208781', 'https://openalex.org/W2963091184', 'https://openalex.org/W3096468295', 'https://openalex.org/W3160584619', 'https://openalex.org/W2935711438', 'https://openalex.org/W3163662330', 'https://openalex.org/W2775336875', 'https://openalex.org/W6798098866', 'https://openalex.org/W3161480375', 'https://openalex.org/W2067295501', 'https://openalex.org/W2963799213', 'https://openalex.org/W3163464523', 'https://openalex.org/W3113290170', 'https://openalex.org/W3099330747', 'https://openalex.org/W3215615641']",2022-04-27
https://openalex.org/W4313506319,https://doi.org/10.1109/tifs.2022.3229583,Dictionary Attacks on Speaker Verification,"In this paper, we propose dictionary attacks against speaker verification - a novel attack vector that aims to match a large fraction of speaker population by chance. We introduce a generic formulation of the attack that can be used with various speech representations and threat models. The attacker uses adversarial optimization to maximize raw similarity of speaker embeddings between a seed speech sample and a proxy population. The resulting master voice successfully matches a non-trivial fraction of people in an unknown population. Adversarial waveforms obtained with our approach can match on average 69% of females and 38% of males enrolled in the target system at a strict decision threshold calibrated to yield false alarm rate of 1%. By using the attack with a black-box voice cloning system, we obtain master voices that are effective in the most challenging conditions and transferable between speaker encoders. We also show that, combined with multiple attempts, this attack opens even more to serious issues on the security of these systems.","['https://openalex.org/W3207211939', 'https://openalex.org/W2797558164', 'https://openalex.org/W4230579564', 'https://openalex.org/W1936725236', 'https://openalex.org/W2943686508', 'https://openalex.org/W2783089003', 'https://openalex.org/W6782072790', 'https://openalex.org/W2141074280', 'https://openalex.org/W3015811740', 'https://openalex.org/W2121750345', 'https://openalex.org/W2973149417', 'https://openalex.org/W2612435500', 'https://openalex.org/W2114925438', 'https://openalex.org/W2046056978', 'https://openalex.org/W3193234023', 'https://openalex.org/W2605024226', 'https://openalex.org/W2897177022', 'https://openalex.org/W3119277739', 'https://openalex.org/W2129312524', 'https://openalex.org/W6640963894', 'https://openalex.org/W2972532204', 'https://openalex.org/W3137249133', 'https://openalex.org/W2041823554', 'https://openalex.org/W2150769028', 'https://openalex.org/W2134706965', 'https://openalex.org/W2787051072', 'https://openalex.org/W6796461952', 'https://openalex.org/W2964052309', 'https://openalex.org/W2981087920', 'https://openalex.org/W2194775991', 'https://openalex.org/W3015197287', 'https://openalex.org/W3015261361', 'https://openalex.org/W2962788625', 'https://openalex.org/W2889519245', 'https://openalex.org/W2748488820', 'https://openalex.org/W2938358845', 'https://openalex.org/W2890964092', 'https://openalex.org/W2916104401', 'https://openalex.org/W6722479552', 'https://openalex.org/W2964301649', 'https://openalex.org/W6760326341', 'https://openalex.org/W2972706975', 'https://openalex.org/W3095570773', 'https://openalex.org/W3128911095', 'https://openalex.org/W3153453329', 'https://openalex.org/W2123299109', 'https://openalex.org/W4302034295', 'https://openalex.org/W2584505851', 'https://openalex.org/W6752888775', 'https://openalex.org/W3015338123', 'https://openalex.org/W3160584619', 'https://openalex.org/W6762533536', 'https://openalex.org/W6682262322', 'https://openalex.org/W1494198834', 'https://openalex.org/W1552314771', 'https://openalex.org/W2963609956', 'https://openalex.org/W6748409065', 'https://openalex.org/W3197340960', 'https://openalex.org/W3024869864', 'https://openalex.org/W3048784519', 'https://openalex.org/W3007384386', 'https://openalex.org/W3153402868']",2022-12-15
https://openalex.org/W4200027410,https://doi.org/10.1109/waspaa52581.2021.9632754,End-to-End Zero-Shot Voice Conversion Using a DDSP Vocoder,"In this paper, we propose a zero-shot voice conversion algorithm using a neural vocoder based on differential digital signal processing. The vocoder does not require auto-regression, and its lightweight, differentiable nature allows the proposed system to be trained in an end-to-end fashion. This enables the use of more perceptually relevant objective functions for model training, and allows feature conversion and vocoder sub-networks to internally learn their own acoustic representation in a data-driven manner. We illustrate the effectiveness of the proposed algorithm by both qualitative and quantitative means, with comparisons to some of our previous works.","['https://openalex.org/W6781581198', 'https://openalex.org/W6781288810', 'https://openalex.org/W2471520273', 'https://openalex.org/W2963300588', 'https://openalex.org/W2964243274', 'https://openalex.org/W2972812066', 'https://openalex.org/W2963175743', 'https://openalex.org/W3095448311', 'https://openalex.org/W6771763809', 'https://openalex.org/W6745415975', 'https://openalex.org/W2984106626', 'https://openalex.org/W6936113694', 'https://openalex.org/W6728445508', 'https://openalex.org/W6762533536', 'https://openalex.org/W3160584619', 'https://openalex.org/W2752796333', 'https://openalex.org/W6726528559', 'https://openalex.org/W3096697361', 'https://openalex.org/W2962883485', 'https://openalex.org/W6697285287', 'https://openalex.org/W6784488536', 'https://openalex.org/W6770046961', 'https://openalex.org/W2989708046', 'https://openalex.org/W2519091744', 'https://openalex.org/W2805669069', 'https://openalex.org/W3000389243', 'https://openalex.org/W3047334337', 'https://openalex.org/W2962788625', 'https://openalex.org/W3095212898', 'https://openalex.org/W2945478979', 'https://openalex.org/W3046412268', 'https://openalex.org/W2518172956', 'https://openalex.org/W3046330735', 'https://openalex.org/W2296704011', 'https://openalex.org/W4298580827', 'https://openalex.org/W3095617022', 'https://openalex.org/W2963799213', 'https://openalex.org/W2532494225']",2021-10-17
https://openalex.org/W4372259964,https://doi.org/10.1109/icassp49357.2023.10094723,Disentangled Feature Learning for Real-Time Neural Speech Coding,"Recently end-to-end neural audio/speech coding has shown its great potential to outperform traditional signal analysis based audio codecs. This is mostly achieved by following the VQ-VAE paradigm where blind features are learned, vector-quantized and coded. In this paper, instead of blind end-to-end learning, we propose to learn disentangled features for real-time neural speech coding. Specifically, more global-like speaker identity and local content features are learned with disentanglement to represent speech. Such a compact feature decomposition not only achieves better coding efficiency by exploiting bit allocation among different features but also provides the flexibility to do audio editing in embedding space, such as voice conversion in real-time communications. Both subjective and objective results demonstrate its coding efficiency and we find that the learned disentangled features show comparable performance on any-to-any voice conversion with modern self-supervised speech representation learning models with far less parameters and low latency, showing the potential of our neural coding framework.","['https://openalex.org/W2775336875', 'https://openalex.org/W2963208781', 'https://openalex.org/W3163662330', 'https://openalex.org/W2935711438', 'https://openalex.org/W3215615641', 'https://openalex.org/W4225287045', 'https://openalex.org/W3214758449', 'https://openalex.org/W2752796333', 'https://openalex.org/W3160584619', 'https://openalex.org/W3015434413', 'https://openalex.org/W3197659778', 'https://openalex.org/W2972659941', 'https://openalex.org/W3140429000', 'https://openalex.org/W3207018606', 'https://openalex.org/W6769196770', 'https://openalex.org/W1494198834', 'https://openalex.org/W3207300132', 'https://openalex.org/W3197580070', 'https://openalex.org/W4205788663', 'https://openalex.org/W2187089797', 'https://openalex.org/W2963799213']",2023-05-05
https://openalex.org/W4389223778,https://doi.org/10.3389/frobt.2023.1202306,Working with troubles and failures in conversation between humans and robots: workshop report,"This paper summarizes the structure and findings from the first Workshop on Troubles and Failures in Conversations between Humans and Robots . The workshop was organized to bring together a small, interdisciplinary group of researchers working on miscommunication from two complementary perspectives. One group of technology-oriented researchers was made up of roboticists, Human-Robot Interaction (HRI) researchers and dialogue system experts. The second group involved experts from conversation analysis, cognitive science, and linguistics. Uniting both groups of researchers is the belief that communication failures between humans and machines need to be taken seriously and that a systematic analysis of such failures may open fruitful avenues in research beyond current practices to improve such systems, including both speech-centric and multimodal interfaces. This workshop represents a starting point for this endeavour. The aim of the workshop was threefold: Firstly, to establish an interdisciplinary network of researchers that share a common interest in investigating communicative failures with a particular view towards robotic speech interfaces; secondly, to gain a partial overview of the “failure landscape” as experienced by roboticists and HRI researchers; and thirdly, to determine the potential for creating a robotic benchmark scenario for testing future speech interfaces with respect to the identified failures. The present article summarizes both the “failure landscape” surveyed during the workshop as well as the outcomes of the attempt to define a benchmark scenario.","['https://openalex.org/W7070847186', 'https://openalex.org/W2801274502', 'https://openalex.org/W3200318528', 'https://openalex.org/W4205976400', 'https://openalex.org/W4323871435', 'https://openalex.org/W6785043770', 'https://openalex.org/W6755232636', 'https://openalex.org/W6770833198', 'https://openalex.org/W4301357669', 'https://openalex.org/W6732114347', 'https://openalex.org/W3159016082', 'https://openalex.org/W6769197323', 'https://openalex.org/W1916508083', 'https://openalex.org/W2116645326', 'https://openalex.org/W2968725073', 'https://openalex.org/W3197100919', 'https://openalex.org/W2986406870', 'https://openalex.org/W1488076994', 'https://openalex.org/W6683094677', 'https://openalex.org/W2144061622', 'https://openalex.org/W6637366565', 'https://openalex.org/W6847289272', 'https://openalex.org/W6843770860', 'https://openalex.org/W2135907989', 'https://openalex.org/W6729508826', 'https://openalex.org/W2799472783', 'https://openalex.org/W2799665858', 'https://openalex.org/W2001292406', 'https://openalex.org/W1998245987', 'https://openalex.org/W4229935648', 'https://openalex.org/W6947944706', 'https://openalex.org/W2808048733', 'https://openalex.org/W6622869322', 'https://openalex.org/W6728501297', 'https://openalex.org/W3176680735', 'https://openalex.org/W6732220701', 'https://openalex.org/W4296280786', 'https://openalex.org/W6783874057', 'https://openalex.org/W1212387563', 'https://openalex.org/W6774720599', 'https://openalex.org/W6802503896', 'https://openalex.org/W6774910488', 'https://openalex.org/W2128967821', 'https://openalex.org/W6786090126', 'https://openalex.org/W2899225656', 'https://openalex.org/W4238722546', 'https://openalex.org/W6780905834', 'https://openalex.org/W3095990227', 'https://openalex.org/W3182074706', 'https://openalex.org/W6694931738', 'https://openalex.org/W2620350838', 'https://openalex.org/W2078325788', 'https://openalex.org/W2964030743', 'https://openalex.org/W3193579463', 'https://openalex.org/W6847263603', 'https://openalex.org/W4323897161', 'https://openalex.org/W6758825785', 'https://openalex.org/W3162109690', 'https://openalex.org/W7062180850', 'https://openalex.org/W2811209743', 'https://openalex.org/W6748353932', 'https://openalex.org/W6691425627', 'https://openalex.org/W6730179945', 'https://openalex.org/W1976729764', 'https://openalex.org/W4212955756', 'https://openalex.org/W1980298376', 'https://openalex.org/W6848732800', 'https://openalex.org/W2063241222', 'https://openalex.org/W6850827683', 'https://openalex.org/W6802287092', 'https://openalex.org/W7071637872', 'https://openalex.org/W3128510346', 'https://openalex.org/W4236142606', 'https://openalex.org/W6745633135', 'https://openalex.org/W2940637054', 'https://openalex.org/W6763515994', 'https://openalex.org/W3036069040', 'https://openalex.org/W6795082736', 'https://openalex.org/W6846243110', 'https://openalex.org/W6784615201', 'https://openalex.org/W2990906560', 'https://openalex.org/W2541568006', 'https://openalex.org/W3204149091', 'https://openalex.org/W4312401956', 'https://openalex.org/W2575638006', 'https://openalex.org/W4306867207', 'https://openalex.org/W2991104449', 'https://openalex.org/W2911446408', 'https://openalex.org/W2767580168', 'https://openalex.org/W2787712888', 'https://openalex.org/W808224890', 'https://openalex.org/W4389010463', 'https://openalex.org/W3102870387', 'https://openalex.org/W2896226941', 'https://openalex.org/W2277838876', 'https://openalex.org/W3045542624', 'https://openalex.org/W4323870627', 'https://openalex.org/W3160584619', 'https://openalex.org/W3205843018', 'https://openalex.org/W3139567692', 'https://openalex.org/W2532841597', 'https://openalex.org/W2557118676', 'https://openalex.org/W2254658406', 'https://openalex.org/W4308223481', 'https://openalex.org/W4298112270', 'https://openalex.org/W3194172310', 'https://openalex.org/W2972345724', 'https://openalex.org/W4226030369', 'https://openalex.org/W3015208154', 'https://openalex.org/W3009763971', 'https://openalex.org/W4313492246', 'https://openalex.org/W2978495070', 'https://openalex.org/W3090654727', 'https://openalex.org/W2950483141', 'https://openalex.org/W3010290680', 'https://openalex.org/W3094545784', 'https://openalex.org/W2155572870', 'https://openalex.org/W2783549597', 'https://openalex.org/W2574884745']",2023-12-01
https://openalex.org/W4392903361,https://doi.org/10.1109/icassp48485.2024.10446352,Stylespeech: Self-Supervised Style Enhancing with VQ-VAE-Based Pre-Training for Expressive Audiobook Speech Synthesis,"The expressive quality of synthesized speech for audiobooks is limited by generalized model architecture and unbalanced style distribution in the training data. To address these issues, in this paper, we propose a self-supervised style enhancing method with VQ-VAE-based pre-training for expressive audiobook speech synthesis. Firstly, a text style encoder is pre-trained with a large amount of unlabeled text-only data. Secondly, a spectrogram style extractor based on VQ-VAE is pre-trained in a self-supervised manner, with plenty of audio data that covers complex style variations. Then a novel architecture with two encoder-decoder paths is specially designed to model the pronunciation and high-level style expressiveness respectively, with the guidance of the style extractor. Both objective and subjective evaluations demonstrate that our proposed method can effectively improve the naturalness and expressiveness of the synthesized speech in audiobook synthesis especially for the role and out-of-domain scenarios. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>","['https://openalex.org/W2964243274', 'https://openalex.org/W2903739847', 'https://openalex.org/W6778823374', 'https://openalex.org/W6750489868', 'https://openalex.org/W4225326644', 'https://openalex.org/W6749555683', 'https://openalex.org/W4319586244', 'https://openalex.org/W2407139314', 'https://openalex.org/W2885800352', 'https://openalex.org/W6751878434', 'https://openalex.org/W3163339651', 'https://openalex.org/W4226230180', 'https://openalex.org/W4224928640', 'https://openalex.org/W4283689139', 'https://openalex.org/W2752796333', 'https://openalex.org/W6774314701', 'https://openalex.org/W3171153522', 'https://openalex.org/W2896457183', 'https://openalex.org/W3094917204', 'https://openalex.org/W3160584619', 'https://openalex.org/W3035323028', 'https://openalex.org/W2194775991', 'https://openalex.org/W6783867762', 'https://openalex.org/W4391156274', 'https://openalex.org/W4365799947', 'https://openalex.org/W2963799213', 'https://openalex.org/W3092028330']",2024-03-18
https://openalex.org/W3159799236,https://doi.org/10.1109/tpds.2022.3157258,OCTOPUS: Overcoming Performance and Privatization Bottlenecks in Distributed Learning,"The diversity and quantity of data warehouses, gathering data from distributed devices such as mobile devices, can enhance the success and robustness of machine learning algorithms. Federated learning enables distributed participants to collaboratively learn a commonly-shared model while holding data locally. However, it is also faced with expensive communication and limitations due to the heterogeneity of distributed data sources and lack of access to global data. In this paper, we investigate a practical distributed learning scenario where multiple downstream tasks (e.g., classifiers) could be efficiently learned from dynamically-updated and non-iid distributed data sources while providing local data privatization. We introduce a new distributed/collaborative learning scheme to address communication overhead via latent compression, leveraging global data while providing privatization of local data without additional cost due to encryption or perturbation. This scheme divides learning into (1) informative feature encoding, and transmitting the latent representation of local data to address communication overhead; (2) downstream tasks centralized at the server using the encoded codes gathered from each node to address computing overhead. Besides, a disentanglement strategy is applied to address the privatization of sensitive components of local data. Extensive experiments are conducted on image and speech datasets. The results demonstrate that downstream tasks on the compact latent representations with the privatization of local data can achieve comparable accuracy to centralized learning.","['https://openalex.org/W6637373629', 'https://openalex.org/W6728757088', 'https://openalex.org/W6752029299', 'https://openalex.org/W6768234139', 'https://openalex.org/W6771536673', 'https://openalex.org/W3111009493', 'https://openalex.org/W3152952112', 'https://openalex.org/W6745560452', 'https://openalex.org/W1834627138', 'https://openalex.org/W6776649238', 'https://openalex.org/W6752600739', 'https://openalex.org/W3087391814', 'https://openalex.org/W6774607975', 'https://openalex.org/W2135599859', 'https://openalex.org/W3091404410', 'https://openalex.org/W1999602050', 'https://openalex.org/W3145145918', 'https://openalex.org/W2989289980', 'https://openalex.org/W3091075746', 'https://openalex.org/W2963209930', 'https://openalex.org/W6756436328', 'https://openalex.org/W2963297095', 'https://openalex.org/W6763138067', 'https://openalex.org/W6755487116', 'https://openalex.org/W6738840075', 'https://openalex.org/W2752796333', 'https://openalex.org/W3100270690', 'https://openalex.org/W2475287302', 'https://openalex.org/W2572730214', 'https://openalex.org/W2583638424', 'https://openalex.org/W2275363859', 'https://openalex.org/W6788246492', 'https://openalex.org/W2535838896', 'https://openalex.org/W3185987447', 'https://openalex.org/W2767079719', 'https://openalex.org/W4205687573', 'https://openalex.org/W6746720608', 'https://openalex.org/W3010852232', 'https://openalex.org/W6759226220', 'https://openalex.org/W2982464076', 'https://openalex.org/W6736413256', 'https://openalex.org/W6741986022', 'https://openalex.org/W3176364684', 'https://openalex.org/W6759238902', 'https://openalex.org/W3186051974', 'https://openalex.org/W2137937911', 'https://openalex.org/W6748381668', 'https://openalex.org/W6779731457', 'https://openalex.org/W2603777577', 'https://openalex.org/W3015434413', 'https://openalex.org/W3096524539', 'https://openalex.org/W3163475957', 'https://openalex.org/W2530417694', 'https://openalex.org/W6752191696', 'https://openalex.org/W3126528290', 'https://openalex.org/W3015640161', 'https://openalex.org/W6773520829', 'https://openalex.org/W2912213068', 'https://openalex.org/W6746839373', 'https://openalex.org/W3021654819', 'https://openalex.org/W6780507278', 'https://openalex.org/W3091870957', 'https://openalex.org/W3046653923', 'https://openalex.org/W6731905404', 'https://openalex.org/W2549401308', 'https://openalex.org/W6737664043', 'https://openalex.org/W3160584619', 'https://openalex.org/W2946900926', 'https://openalex.org/W4287553941', 'https://openalex.org/W3036033610', 'https://openalex.org/W2541884796', 'https://openalex.org/W2789543585', 'https://openalex.org/W4318619660', 'https://openalex.org/W2803867449', 'https://openalex.org/W3038028469', 'https://openalex.org/W2949382160', 'https://openalex.org/W4288110990', 'https://openalex.org/W3120121071', 'https://openalex.org/W4297775537', 'https://openalex.org/W3101220048', 'https://openalex.org/W1686810756', 'https://openalex.org/W2579186979', 'https://openalex.org/W3037674069', 'https://openalex.org/W2807006176', 'https://openalex.org/W4289147263', 'https://openalex.org/W2963803379', 'https://openalex.org/W3007559909', 'https://openalex.org/W3103802018', 'https://openalex.org/W2575849723', 'https://openalex.org/W4297779784', 'https://openalex.org/W4220779061', 'https://openalex.org/W2963799213', 'https://openalex.org/W3015636663', 'https://openalex.org/W2970289928', 'https://openalex.org/W4289382653', 'https://openalex.org/W2604392022', 'https://openalex.org/W2741269719', 'https://openalex.org/W3099088591', 'https://openalex.org/W2620364083', 'https://openalex.org/W2774000609', 'https://openalex.org/W3105229232', 'https://openalex.org/W4294106961', 'https://openalex.org/W2971064744', 'https://openalex.org/W2963834323', 'https://openalex.org/W3020784329', 'https://openalex.org/W3125709657', 'https://openalex.org/W3038022836', 'https://openalex.org/W3213330817', 'https://openalex.org/W2604783387', 'https://openalex.org/W2810065831', 'https://openalex.org/W4298221930', 'https://openalex.org/W4297687186', 'https://openalex.org/W2971074500', 'https://openalex.org/W2519091744', 'https://openalex.org/W2949522309', 'https://openalex.org/W2963179579', 'https://openalex.org/W3133428285', 'https://openalex.org/W4294643831', 'https://openalex.org/W2913570153', 'https://openalex.org/W2777914285', 'https://openalex.org/W3022800800']",2022-03-07
https://openalex.org/W4385764366,https://doi.org/10.24963/ijcai.2023/361,Disentanglement of Latent Representations via Causal Interventions,"The process of generating data such as images is controlled by independent and unknown factors of variation. The retrieval of these variables has been studied extensively in the disentanglement, causal representation learning, and independent component analysis fields. Recently, approaches merging these domains together have shown great success. Instead of directly representing the factors of variation, the problem of disentanglement can be seen as finding the interventions on one image that yield a change to a single factor. Following this assumption, we introduce a new method for disentanglement inspired by causal dynamics that combines causality theory with vector-quantized variational autoencoders. Our model considers the quantized vectors as causal variables and links them in a causal graph. It performs causal interventions on the graph and generates atomic transitions affecting a unique factor of variation in the image. We also introduce a new task of action retrieval that consists of finding the action responsible for the transition between two images. We test our method on standard synthetic and real-world disentanglement datasets. We show that it can effectively disentangle the factors of variation and perform precise interventions on high-level semantic attributes of an image without affecting its quality, even with imbalanced data distributions.","['https://openalex.org/W4287070285', 'https://openalex.org/W3160584619', 'https://openalex.org/W2753738274', 'https://openalex.org/W4225434639', 'https://openalex.org/W4287121985', 'https://openalex.org/W4295112158', 'https://openalex.org/W3135588948', 'https://openalex.org/W4286795917', 'https://openalex.org/W4288043168', 'https://openalex.org/W3176726917', 'https://openalex.org/W4286981685', 'https://openalex.org/W3005031238', 'https://openalex.org/W1834627138', 'https://openalex.org/W4285723986', 'https://openalex.org/W4287111106', 'https://openalex.org/W4283798426', 'https://openalex.org/W2902476877', 'https://openalex.org/W4220858968', 'https://openalex.org/W4294558607', 'https://openalex.org/W2964127395', 'https://openalex.org/W2547875792', 'https://openalex.org/W2950662112', 'https://openalex.org/W3094502228', 'https://openalex.org/W2785961484', 'https://openalex.org/W4391602018', 'https://openalex.org/W2964015378', 'https://openalex.org/W2963799213', 'https://openalex.org/W4297411720', 'https://openalex.org/W3129576130', 'https://openalex.org/W2951428711', 'https://openalex.org/W2184218725', 'https://openalex.org/W3175260249', 'https://openalex.org/W4289129346', 'https://openalex.org/W2971074500']",2023-08-01
https://openalex.org/W3159257553,https://doi.org/10.48550/arxiv.2105.01573,Exploring Disentanglement with Multilingual and Monolingual VQ-VAE,"This work examines the content and usefulness of disentangled phone and speaker representations from two separately trained VQ-VAE systems: one trained on multilingual data and another trained on monolingual data. We explore the multi- and monolingual models using four small proof-of-concept tasks: copy-synthesis, voice transformation, linguistic code-switching, and content-based privacy masking. From these tasks, we reflect on how disentangled phone and speaker representations can be used to manipulate speech in a meaningful way. Our experiments demonstrate that the VQ representations are suitable for these tasks, including creating new voices by mixing speaker representations together. We also present our novel technique to conceal the content of targeted words within an utterance by manipulating phone VQ codes, while retaining speaker identity and intelligibility of surrounding words. Finally, we discuss recommendations for further increasing the viability of disentangled representations.","['https://openalex.org/W2998572311', 'https://openalex.org/W3008391559', 'https://openalex.org/W2199505332', 'https://openalex.org/W2982290890', 'https://openalex.org/W3015484365', 'https://openalex.org/W2073495057', 'https://openalex.org/W3163338468', 'https://openalex.org/W2470254032', 'https://openalex.org/W2972921407', 'https://openalex.org/W2963799213', 'https://openalex.org/W2972473628', 'https://openalex.org/W3082522567', 'https://openalex.org/W2972849140', 'https://openalex.org/W3015434413', 'https://openalex.org/W3160584619', 'https://openalex.org/W3043999252', 'https://openalex.org/W3021164770', 'https://openalex.org/W3097297926', 'https://openalex.org/W2747874407', 'https://openalex.org/W2150769028', 'https://openalex.org/W3094917204', 'https://openalex.org/W2790857402', 'https://openalex.org/W2964002616', 'https://openalex.org/W3024962219', 'https://openalex.org/W3162390194', 'https://openalex.org/W3097286738', 'https://openalex.org/W2407075323']",2021-05-04
https://openalex.org/W3177416682,https://doi.org/10.48550/arxiv.2106.13479,Preliminary study on using vector quantization latent spaces for TTS/VC systems with consistent performance,"Generally speaking, the main objective when training a neural speech synthesis system is to synthesize natural and expressive speech from the output layer of the neural network without much attention given to the hidden layers. However, by learning useful latent representation, the system can be used for many more practical scenarios. In this paper, we investigate the use of quantized vectors to model the latent linguistic embedding and compare it with the continuous counterpart. By enforcing different policies over the latent spaces in the training, we are able to obtain a latent linguistic embedding that takes on different properties while having a similar performance in terms of quality and speaker similarity. Our experiments show that the voice cloning system built with vector quantization has only a small degradation in terms of perceptive evaluations, but has a discrete latent space that is useful for reducing the representation bit-rate, which is desirable for data transferring, or limiting the information leaking, which is important for speaker anonymization and other tasks of that nature.","['https://openalex.org/W3015440759', 'https://openalex.org/W2949416428', 'https://openalex.org/W2963912679', 'https://openalex.org/W2963272440', 'https://openalex.org/W2963799213', 'https://openalex.org/W2973026522', 'https://openalex.org/W3144044466', 'https://openalex.org/W3118753411', 'https://openalex.org/W2946555236', 'https://openalex.org/W3043999252', 'https://openalex.org/W2938947737', 'https://openalex.org/W2108501770', 'https://openalex.org/W2972848589', 'https://openalex.org/W3199367817', 'https://openalex.org/W2962896155', 'https://openalex.org/W2901997113', 'https://openalex.org/W3015434413', 'https://openalex.org/W2962981281', 'https://openalex.org/W2532494225', 'https://openalex.org/W2547875792', 'https://openalex.org/W2972849140', 'https://openalex.org/W3092496982', 'https://openalex.org/W3091928890', 'https://openalex.org/W2949281321', 'https://openalex.org/W2759925408', 'https://openalex.org/W3160584619', 'https://openalex.org/W2963828549', 'https://openalex.org/W3095990227', 'https://openalex.org/W37526647', 'https://openalex.org/W3154451338', 'https://openalex.org/W2972595148']",2021-06-25
https://openalex.org/W2996383576,,vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations,We propose vq-wav2vec to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The algorithm uses either a gumbel softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition.,"['https://openalex.org/W2346964103', 'https://openalex.org/W2518108298', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963403868', 'https://openalex.org/W2962901777', 'https://openalex.org/W10548402', 'https://openalex.org/W2842511635', 'https://openalex.org/W2547875792', 'https://openalex.org/W2520160253', 'https://openalex.org/W2962784628', 'https://openalex.org/W3011411500', 'https://openalex.org/W3127686677', 'https://openalex.org/W2947591107', 'https://openalex.org/W2124509324', 'https://openalex.org/W2941814890', 'https://openalex.org/W2963425185', 'https://openalex.org/W2965373594', 'https://openalex.org/W2936295285', 'https://openalex.org/W2926827382', 'https://openalex.org/W2973049979', 'https://openalex.org/W2153579005', 'https://openalex.org/W2889282842', 'https://openalex.org/W2936774411', 'https://openalex.org/W2940180244', 'https://openalex.org/W2987741655', 'https://openalex.org/W2963382687', 'https://openalex.org/W2933138175', 'https://openalex.org/W2786459654', 'https://openalex.org/W1494198834', 'https://openalex.org/W2794209590', 'https://openalex.org/W1885680957', 'https://openalex.org/W2963807318', 'https://openalex.org/W2963620343', 'https://openalex.org/W2141440284']",2020-04-30
https://openalex.org/W3175871055,https://doi.org/10.1609/aaai.v35i16.17684,UWSpeech: Speech to Speech Translation for Unwritten Languages,"Existing speech to speech translation systems heavily rely on the text of target language: they usually translate source language either to target text and then synthesize target speech from text, or directly to target speech with target text for auxiliary training. However, those methods cannot be applied to unwritten target languages, which have no written text or phoneme available. In this paper, we develop a translation system for unwritten languages, named as UWSpeech, which converts target unwritten speech into discrete tokens with a converter, and then translates source-language speech into target discrete tokens with a translator, and finally synthesizes target speech from target discrete tokens with an inverter. We propose a method called XL-VAE, which enhances vector quantized variational autoencoder (VQ-VAE) with cross-lingual (XL) speech recognition, to train the converter and inverter of UWSpeech jointly. Experiments on Fisher Spanish-English conversation translation dataset show that UWSpeech outperforms direct translation and VQ-VAE baseline by about 16 and 10 BLEU points respectively, which demonstrate the advantages and potentials of UWSpeech.","['https://openalex.org/W2755682845', 'https://openalex.org/W2399576818', 'https://openalex.org/W3125709657', 'https://openalex.org/W2940544976', 'https://openalex.org/W6697293080', 'https://openalex.org/W2466918907', 'https://openalex.org/W2936295285', 'https://openalex.org/W7027429494', 'https://openalex.org/W2120847449', 'https://openalex.org/W2936184970', 'https://openalex.org/W2599585580', 'https://openalex.org/W2103091632', 'https://openalex.org/W2765961751', 'https://openalex.org/W2097203679', 'https://openalex.org/W2998284473', 'https://openalex.org/W2949335953', 'https://openalex.org/W1537859740', 'https://openalex.org/W2134202996', 'https://openalex.org/W2113106066', 'https://openalex.org/W6898505805', 'https://openalex.org/W2945078028', 'https://openalex.org/W2936969148', 'https://openalex.org/W2947591107', 'https://openalex.org/W2346964103', 'https://openalex.org/W1993812374', 'https://openalex.org/W6756627502', 'https://openalex.org/W2605131327', 'https://openalex.org/W2507959295', 'https://openalex.org/W6723123444', 'https://openalex.org/W1902237438', 'https://openalex.org/W4297747548', 'https://openalex.org/W4298393544', 'https://openalex.org/W4385245566', 'https://openalex.org/W2972495969', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963799213', 'https://openalex.org/W2964169922', 'https://openalex.org/W2762715843', 'https://openalex.org/W2963242190', 'https://openalex.org/W2979476256', 'https://openalex.org/W4300191749', 'https://openalex.org/W2304648132', 'https://openalex.org/W2952468927', 'https://openalex.org/W2972374322', 'https://openalex.org/W2973026522', 'https://openalex.org/W2136545725', 'https://openalex.org/W2972867623', 'https://openalex.org/W2952167535', 'https://openalex.org/W3015419784', 'https://openalex.org/W2901607128', 'https://openalex.org/W2949328740', 'https://openalex.org/W2952711665', 'https://openalex.org/W2133564696', 'https://openalex.org/W3005578234', 'https://openalex.org/W2493353997', 'https://openalex.org/W2977997709', 'https://openalex.org/W2316803017', 'https://openalex.org/W4300047444']",2021-05-18
https://openalex.org/W3162850270,https://doi.org/10.1016/j.csl.2021.101244,Local and non-local dependency learning and emergence of rule-like representations in speech data by Deep Convolutional Generative Adversarial Networks,"This paper argues that training GANs on local and non-local dependencies in speech data offers insights into how deep neural networks discretize continuous data and how symbolic-like rule-based morphophonological processes emerge in a deep convolutional architecture. Acquisition of speech has recently been modeled as a dependency between latent space and data generated by GANs in Begu\v{s} (2020b; arXiv:2006.03965), who models learning of a simple local allophonic distribution. We extend this approach to test learning of local and non-local phonological processes that include approximations of morphological processes. We further parallel outputs of the model to results of a behavioral experiment where human subjects are trained on the data used for training the GAN network. Four main conclusions emerge: (i) the networks provide useful information for computational models of speech acquisition even if trained on a comparatively small dataset of an artificial grammar learning experiment; (ii) local processes are easier to learn than non-local processes, which matches both behavioral data in human subjects and typology in the world's languages. This paper also proposes (iii) how we can actively observe the network's progress in learning and explore the effect of training steps on learning representations by keeping latent space constant across different training steps. Finally, this paper shows that (iv) the network learns to encode the presence of a prefix with a single latent variable; by interpolating this variable, we can actively observe the operation of a non-local phonological process. The proposed technique for retrieving learning representations has general implications for our understanding of how GANs discretize continuous speech data and suggests that rule-like generalizations in the training data are represented as an interaction between variables in the network's latent space.","['https://openalex.org/W3097286738', 'https://openalex.org/W2029949252', 'https://openalex.org/W2945769669', 'https://openalex.org/W4246991306', 'https://openalex.org/W3127686677', 'https://openalex.org/W2963226019', 'https://openalex.org/W2135704565', 'https://openalex.org/W2739748921', 'https://openalex.org/W2966379269', 'https://openalex.org/W2950299304', 'https://openalex.org/W1979539191', 'https://openalex.org/W2173520492', 'https://openalex.org/W2936295285', 'https://openalex.org/W1699946128', 'https://openalex.org/W2621649611', 'https://openalex.org/W3209383001', 'https://openalex.org/W2627060831', 'https://openalex.org/W1598851216', 'https://openalex.org/W2119160928', 'https://openalex.org/W4295521014', 'https://openalex.org/W3033010920', 'https://openalex.org/W4235866814', 'https://openalex.org/W2946697451', 'https://openalex.org/W2605135824', 'https://openalex.org/W3106255532', 'https://openalex.org/W2157076315', 'https://openalex.org/W3105148948', 'https://openalex.org/W2586193078', 'https://openalex.org/W2164409231', 'https://openalex.org/W4288087796', 'https://openalex.org/W2099471712', 'https://openalex.org/W4239019441', 'https://openalex.org/W2434741482', 'https://openalex.org/W3025429027', 'https://openalex.org/W4214933144', 'https://openalex.org/W2789185601', 'https://openalex.org/W1951724000', 'https://openalex.org/W3022729974', 'https://openalex.org/W2067885406', 'https://openalex.org/W4320013936', 'https://openalex.org/W4285687965', 'https://openalex.org/W4206210768', 'https://openalex.org/W2982339091', 'https://openalex.org/W2971016963', 'https://openalex.org/W2725023556', 'https://openalex.org/W2007251877', 'https://openalex.org/W2752294398', 'https://openalex.org/W2112308533', 'https://openalex.org/W2972867623', 'https://openalex.org/W4297817572', 'https://openalex.org/W2582743722', 'https://openalex.org/W2963684088', 'https://openalex.org/W4299687266', 'https://openalex.org/W3207342693', 'https://openalex.org/W2066122504', 'https://openalex.org/W1485785480', 'https://openalex.org/W1969066628', 'https://openalex.org/W2111476268', 'https://openalex.org/W1983351226', 'https://openalex.org/W2011183451']",2020-09-26
https://openalex.org/W3207222250,https://doi.org/10.48550/arxiv.2110.10329,SLAM: A Unified Encoder for Speech and Language Modeling via Speech-Text Joint Pre-Training,"Unsupervised pre-training is now the predominant approach for both text and speech understanding. Self-attention models pre-trained on large amounts of unannotated data have been hugely successful when fine-tuned on downstream tasks from a variety of domains and languages. This paper takes the universality of unsupervised language pre-training one step further, by unifying speech and text pre-training within a single model. We build a single encoder with the BERT objective on unlabeled text together with the w2v-BERT objective on unlabeled speech. To further align our model representations across modalities, we leverage alignment losses, specifically Translation Language Modeling (TLM) and Speech Text Matching (STM) that make use of supervised speech-text recognition data. We demonstrate that incorporating both speech and text data during pre-training can significantly improve downstream quality on CoVoST~2 speech translation, by around 1 BLEU compared to single-modality pre-trained models, while retaining close to SotA performance on LibriSpeech and SpeechStew ASR tasks. On four GLUE tasks and text-normalization, we observe evidence of capacity limitations and interference between the two modalities, leading to degraded performance compared to an equivalent text-only model, while still being competitive with BERT. Through extensive empirical analysis we also demonstrate the importance of the choice of objective function for speech pre-training, and the beneficial effect of adding additional supervised signals on the quality of the learned representations.","['https://openalex.org/W2995181338', 'https://openalex.org/W2963341956', 'https://openalex.org/W3122317902', 'https://openalex.org/W2970597249', 'https://openalex.org/W3030163527', 'https://openalex.org/W11314411', 'https://openalex.org/W2336585117', 'https://openalex.org/W3204696009', 'https://openalex.org/W3126261584', 'https://openalex.org/W2963846996', 'https://openalex.org/W3035579820', 'https://openalex.org/W2251939518', 'https://openalex.org/W3093579165', 'https://openalex.org/W2965373594', 'https://openalex.org/W3152609875', 'https://openalex.org/W2153579005', 'https://openalex.org/W2250357346', 'https://openalex.org/W3119866685', 'https://openalex.org/W3034999214', 'https://openalex.org/W2787560479', 'https://openalex.org/W2125336414', 'https://openalex.org/W3144173820', 'https://openalex.org/W3082274269', 'https://openalex.org/W1494198834', 'https://openalex.org/W2936295285', 'https://openalex.org/W2963250244', 'https://openalex.org/W2546744831', 'https://openalex.org/W3104681546', 'https://openalex.org/W2995929068', 'https://openalex.org/W2138204974', 'https://openalex.org/W2270070752', 'https://openalex.org/W2943552823', 'https://openalex.org/W2547875792', 'https://openalex.org/W2132339004', 'https://openalex.org/W3165666670', 'https://openalex.org/W2842511635', 'https://openalex.org/W3154596443', 'https://openalex.org/W2963799213', 'https://openalex.org/W3193521535', 'https://openalex.org/W2958953787', 'https://openalex.org/W2950797609', 'https://openalex.org/W2973049979', 'https://openalex.org/W3099782249', 'https://openalex.org/W2963748441', 'https://openalex.org/W2983040767', 'https://openalex.org/W2963310665', 'https://openalex.org/W2970049541', 'https://openalex.org/W3054645415', 'https://openalex.org/W2996383576', 'https://openalex.org/W3169483174', 'https://openalex.org/W2626778328', 'https://openalex.org/W3093502935', 'https://openalex.org/W3037057938', 'https://openalex.org/W3032816972', 'https://openalex.org/W3153287399', 'https://openalex.org/W3197324626', 'https://openalex.org/W3211483028', 'https://openalex.org/W3011411500', 'https://openalex.org/W3097777922', 'https://openalex.org/W3139918052']",2021-10-20
https://openalex.org/W2972805867,https://doi.org/10.48550/arxiv.1909.06532,Bootstrapping non-parallel voice conversion from speaker-adaptive text-to-speech,"Voice conversion (VC) and text-to-speech (TTS) are two tasks that share a similar objective, generating speech with a target voice. However, they are usually developed independently under vastly different frameworks. In this paper, we propose a methodology to bootstrap a VC system from a pretrained speaker-adaptive TTS model and unify the techniques as well as the interpretations of these two tasks. Moreover by offloading the heavy data demand to the training stage of the TTS model, our VC system can be built using a small amount of target speaker speech data. It also opens up the possibility of using speech in a foreign unseen language to build the system. Our subjective evaluations show that the proposed framework is able to not only achieve competitive performance in the standard intra-language scenario but also adapt and convert using speech utterances in an unseen language.","['https://openalex.org/W1974745215', 'https://openalex.org/W2013996527', 'https://openalex.org/W2938583109', 'https://openalex.org/W1981457580', 'https://openalex.org/W2774848319', 'https://openalex.org/W2507912506', 'https://openalex.org/W2964069186', 'https://openalex.org/W2120605154', 'https://openalex.org/W2889329491', 'https://openalex.org/W2518172956', 'https://openalex.org/W113498433', 'https://openalex.org/W2527729766', 'https://openalex.org/W2899877258', 'https://openalex.org/W2947445680', 'https://openalex.org/W1588266896', 'https://openalex.org/W2940544976', 'https://openalex.org/W2788266530', 'https://openalex.org/W2977798327', 'https://openalex.org/W2950414763', 'https://openalex.org/W2963035245', 'https://openalex.org/W33533989', 'https://openalex.org/W3101689408', 'https://openalex.org/W2800289214', 'https://openalex.org/W2086796102', 'https://openalex.org/W2929315483', 'https://openalex.org/W49412823', 'https://openalex.org/W2936295285', 'https://openalex.org/W2887264325', 'https://openalex.org/W2156142001', 'https://openalex.org/W2532494225', 'https://openalex.org/W2134202996', 'https://openalex.org/W2950224550', 'https://openalex.org/W2161476805', 'https://openalex.org/W2947591107', 'https://openalex.org/W2963808252', 'https://openalex.org/W2973034126', 'https://openalex.org/W2808706139', 'https://openalex.org/W2885820941', 'https://openalex.org/W1538607601']",2019-09-14
https://openalex.org/W3155477605,https://doi.org/10.48550/arxiv.2104.08614,Cetacean Translation Initiative: a roadmap to deciphering the communication of sperm whales,"The past decade has witnessed a groundbreaking rise of machine learning for human language analysis, with current methods capable of automatically accurately recovering various aspects of syntax and semantics - including sentence structure and grounded word meaning - from large data collections. Recent research showed the promise of such tools for analyzing acoustic communication in nonhuman species. We posit that machine learning will be the cornerstone of future collection, processing, and analysis of multimodal streams of data in animal communication studies, including bioacoustic, behavioral, biological, and environmental data. Cetaceans are unique non-human model species as they possess sophisticated acoustic communications, but utilize a very different encoding system that evolved in an aquatic rather than terrestrial medium. Sperm whales, in particular, with their highly-developed neuroanatomical features, cognitive abilities, social structures, and discrete click-based encoding make for an excellent starting point for advanced machine learning tools that can be applied to other animals in the future. This paper details a roadmap toward this goal based on currently existing technology and multidisciplinary scientific community effort. We outline the key elements required for the collection and processing of massive bioacoustic data of sperm whales, detecting their basic communication units and language-like higher-level structures, and validating these models through interactive playback experiments. The technological capabilities developed by such an undertaking are likely to yield cross-applications and advancements in broader communities investigating non-human communication and animal behavioral research.","['https://openalex.org/W2016661916', 'https://openalex.org/W2963571336', 'https://openalex.org/W17120685', 'https://openalex.org/W2032408259', 'https://openalex.org/W2048911549', 'https://openalex.org/W2086193061', 'https://openalex.org/W2074874566', 'https://openalex.org/W2069138632', 'https://openalex.org/W2264742718', 'https://openalex.org/W1997505733', 'https://openalex.org/W1894835849', 'https://openalex.org/W2074361287', 'https://openalex.org/W2028008134', 'https://openalex.org/W2936295285', 'https://openalex.org/W2094555011', 'https://openalex.org/W3087638917', 'https://openalex.org/W2794753807', 'https://openalex.org/W2074452946', 'https://openalex.org/W2088152161', 'https://openalex.org/W2990874303', 'https://openalex.org/W2066908170', 'https://openalex.org/W2465328155', 'https://openalex.org/W1828958803', 'https://openalex.org/W2032914003', 'https://openalex.org/W2768572194', 'https://openalex.org/W2900428170', 'https://openalex.org/W3037314059', 'https://openalex.org/W3095361818', 'https://openalex.org/W2127419931', 'https://openalex.org/W2518136372', 'https://openalex.org/W2061280432', 'https://openalex.org/W2808427125', 'https://openalex.org/W2027409010', 'https://openalex.org/W2739497626', 'https://openalex.org/W2166715144', 'https://openalex.org/W2034217745', 'https://openalex.org/W1598934683', 'https://openalex.org/W2063140472', 'https://openalex.org/W2970608575', 'https://openalex.org/W2035110320', 'https://openalex.org/W2065116459', 'https://openalex.org/W2004775819', 'https://openalex.org/W2089908537', 'https://openalex.org/W2755569859', 'https://openalex.org/W2152036362', 'https://openalex.org/W2108112408', 'https://openalex.org/W2079145130', 'https://openalex.org/W3007096206', 'https://openalex.org/W1993533333', 'https://openalex.org/W2810840719', 'https://openalex.org/W2473499653', 'https://openalex.org/W2793128170', 'https://openalex.org/W2055408826', 'https://openalex.org/W2107959623', 'https://openalex.org/W2079214866', 'https://openalex.org/W2119286107', 'https://openalex.org/W1976252333', 'https://openalex.org/W3025429027', 'https://openalex.org/W3210434492', 'https://openalex.org/W2464664036', 'https://openalex.org/W2124479173', 'https://openalex.org/W2086648308', 'https://openalex.org/W2059539229', 'https://openalex.org/W2517592094', 'https://openalex.org/W2011238950', 'https://openalex.org/W2408060352', 'https://openalex.org/W1974388862', 'https://openalex.org/W2970226357', 'https://openalex.org/W2418992898', 'https://openalex.org/W2062248806', 'https://openalex.org/W2161394327', 'https://openalex.org/W2103505394', 'https://openalex.org/W3047188629', 'https://openalex.org/W2013832123', 'https://openalex.org/W2048570735', 'https://openalex.org/W2236233024', 'https://openalex.org/W2136479563', 'https://openalex.org/W2102224657', 'https://openalex.org/W3120387374', 'https://openalex.org/W1984891176', 'https://openalex.org/W2078484176', 'https://openalex.org/W2106318382', 'https://openalex.org/W2271388808', 'https://openalex.org/W2153190547', 'https://openalex.org/W2131462529', 'https://openalex.org/W1593341158', 'https://openalex.org/W2013579586', 'https://openalex.org/W2919115771', 'https://openalex.org/W2107191910', 'https://openalex.org/W2050688900', 'https://openalex.org/W2413716136', 'https://openalex.org/W1531752105', 'https://openalex.org/W2017677081', 'https://openalex.org/W2104137851', 'https://openalex.org/W2061596145', 'https://openalex.org/W2074719795', 'https://openalex.org/W1994717818', 'https://openalex.org/W2300894760', 'https://openalex.org/W2168201256', 'https://openalex.org/W3033010920', 'https://openalex.org/W2071196664', 'https://openalex.org/W2965465245', 'https://openalex.org/W2153568660', 'https://openalex.org/W1608215728', 'https://openalex.org/W2056263517', 'https://openalex.org/W1963933074', 'https://openalex.org/W2159906077', 'https://openalex.org/W2023314508', 'https://openalex.org/W2037460931', 'https://openalex.org/W2954366630', 'https://openalex.org/W2048139184', 'https://openalex.org/W2149557440', 'https://openalex.org/W2066974403', 'https://openalex.org/W2964118342', 'https://openalex.org/W1876949284', 'https://openalex.org/W2906586541', 'https://openalex.org/W2107101015']",2021-04-17
https://openalex.org/W4224875474,https://doi.org/10.21437/interspeech.2022-10652,"Word Discovery in Visually Grounded, Self-Supervised Speech Models",OursFigure 1: HuBERT: sum of attention weights each frame receives from other frames.Ours (VG-HuBERT3): attention weights each frame receives from the [CLS A] token.Attention weights from different attention heads are coded with different colors.,"['https://openalex.org/W4230640548', 'https://openalex.org/W2963620343', 'https://openalex.org/W2988907666', 'https://openalex.org/W2057007397', 'https://openalex.org/W3174311593', 'https://openalex.org/W3157861865', 'https://openalex.org/W3097159218', 'https://openalex.org/W2964169922', 'https://openalex.org/W2145410271', 'https://openalex.org/W1861492603', 'https://openalex.org/W2995680346', 'https://openalex.org/W3036601975', 'https://openalex.org/W3209993061', 'https://openalex.org/W2025482506', 'https://openalex.org/W2483390977', 'https://openalex.org/W30845872', 'https://openalex.org/W3097485645', 'https://openalex.org/W3198411039', 'https://openalex.org/W3094502228', 'https://openalex.org/W4385245566', 'https://openalex.org/W2398490608', 'https://openalex.org/W4286973758', 'https://openalex.org/W4214813806', 'https://openalex.org/W2481240925', 'https://openalex.org/W3198782837', 'https://openalex.org/W4226380987', 'https://openalex.org/W4297808394', 'https://openalex.org/W2468716020', 'https://openalex.org/W2556930864', 'https://openalex.org/W3159476814', 'https://openalex.org/W3198134274', 'https://openalex.org/W2972892814', 'https://openalex.org/W2137010615', 'https://openalex.org/W3169320628', 'https://openalex.org/W2796315435', 'https://openalex.org/W3159481202', 'https://openalex.org/W4313182775', 'https://openalex.org/W2989358187', 'https://openalex.org/W2114347655', 'https://openalex.org/W4394453761', 'https://openalex.org/W2896457183', 'https://openalex.org/W3171345413', 'https://openalex.org/W1778492285', 'https://openalex.org/W4294955557', 'https://openalex.org/W4221161768']",2022-09-16
https://openalex.org/W3033038061,https://doi.org/10.21437/interspeech.2020-3084,A Convolutional Deep Markov Model for Unsupervised Speech Representation Learning,"Probabilistic Latent Variable Models (LVMs) provide an alternative to self-supervised learning approaches for linguistic representation learning from speech. LVMs admit an intuitive probabilistic interpretation where the latent structure shapes the information extracted from the signal. Even though LVMs have recently seen a renewed interest due to the introduction of Variational Autoencoders (VAEs), their use for speech representation learning remains largely unexplored. In this work, we propose Convolutional Deep Markov Model (ConvDMM), a Gaussian state-space model with non-linear emission and transition functions modelled by deep neural networks. This unsupervised model is trained using black box variational inference. A deep convolutional neural network is used as an inference network for structured variational approximation. When trained on a large scale speech dataset (LibriSpeech), ConvDMM produces features that significantly outperform multiple self-supervised feature extracting methods on linear phone classification and recognition on the Wall Street Journal dataset. Furthermore, we found that ConvDMM complements self-supervised methods like Wav2Vec and PASE, improving on the results achieved with any of the methods alone. Lastly, we find that ConvDMM features enable learning better phone recognizers than any other features in an extreme low-resource regime with few labeled training examples.","['https://openalex.org/W2951004968', 'https://openalex.org/W2127141656', 'https://openalex.org/W2953331651', 'https://openalex.org/W2787698019', 'https://openalex.org/W2949382160', 'https://openalex.org/W2995680346', 'https://openalex.org/W3101749733', 'https://openalex.org/W2210838531', 'https://openalex.org/W2807947437', 'https://openalex.org/W2949517790', 'https://openalex.org/W2926063217', 'https://openalex.org/W2808697642', 'https://openalex.org/W2750248772', 'https://openalex.org/W2937090315', 'https://openalex.org/W2944828972', 'https://openalex.org/W1494198834', 'https://openalex.org/W2935542736', 'https://openalex.org/W2941814890', 'https://openalex.org/W2963300588', 'https://openalex.org/W2347098582', 'https://openalex.org/W2793111190', 'https://openalex.org/W2995085126', 'https://openalex.org/W2973049979', 'https://openalex.org/W2556930864', 'https://openalex.org/W2134952451', 'https://openalex.org/W2926827382', 'https://openalex.org/W2982223350', 'https://openalex.org/W2153185114', 'https://openalex.org/W2126377586', 'https://openalex.org/W2068247585', 'https://openalex.org/W2100768664', 'https://openalex.org/W3125709657', 'https://openalex.org/W2483390977', 'https://openalex.org/W2962695963', 'https://openalex.org/W2948211236', 'https://openalex.org/W2964232608', 'https://openalex.org/W2963618559', 'https://openalex.org/W2970014727', 'https://openalex.org/W2962990490']",2020-10-25
https://openalex.org/W4225166170,https://doi.org/10.1109/cvprw56347.2022.00504,Improving Multimodal Speech Recognition by Data Augmentation and Speech Representations,"Multimodal speech recognition aims to improve the performance of automatic speech recognition (ASR) systems by leveraging additional visual information that is usually associated to the audio input. While previous approaches make crucial use of strong visual representations, e.g. by finetuning pretrained image recognition networks, significantly less attention has been paid to its counterpart: the speech component. In this work, we investigate ways of improving the base speech recognition system by following similar techniques to the ones used for the visual encoder, namely, transferring representations and data augmentation. First, we show that starting from a pretrained ASR significantly improves the state-of-the-art performance; remarkably, even when building upon a strong unimodal system, we still find gains by including the visual modality. Second, we employ speech data augmentation techniques to encourage the multimodal system to attend to the visual stimuli. This technique replaces previously used word masking and comes with the benefits of being conceptually simpler and yielding consistent improvements in the multimodal setting. We provide empirical results on three multimodal datasets, including the newly introduced Localized Narratives.","['https://openalex.org/W3099041253', 'https://openalex.org/W6773991922', 'https://openalex.org/W3197828817', 'https://openalex.org/W3095670406', 'https://openalex.org/W2963654155', 'https://openalex.org/W2936774411', 'https://openalex.org/W6764984290', 'https://openalex.org/W6755559483', 'https://openalex.org/W3132191748', 'https://openalex.org/W2117539524', 'https://openalex.org/W2194775991', 'https://openalex.org/W3099414366', 'https://openalex.org/W68733909', 'https://openalex.org/W2972892814', 'https://openalex.org/W3148438775', 'https://openalex.org/W2950133079', 'https://openalex.org/W6783213970', 'https://openalex.org/W2963979492', 'https://openalex.org/W4288083516', 'https://openalex.org/W6639102338', 'https://openalex.org/W6796417832', 'https://openalex.org/W6629717138', 'https://openalex.org/W3148906368', 'https://openalex.org/W6750570362', 'https://openalex.org/W6678704754', 'https://openalex.org/W2962862718', 'https://openalex.org/W6739955027', 'https://openalex.org/W3035299099', 'https://openalex.org/W6750651883', 'https://openalex.org/W6770596778', 'https://openalex.org/W6756145441', 'https://openalex.org/W6729977899', 'https://openalex.org/W6754392867', 'https://openalex.org/W6790727354', 'https://openalex.org/W2507296351', 'https://openalex.org/W2185175083', 'https://openalex.org/W2914781455', 'https://openalex.org/W2512544816', 'https://openalex.org/W6629976795', 'https://openalex.org/W6864391120', 'https://openalex.org/W6761546396', 'https://openalex.org/W2586850765', 'https://openalex.org/W3116544769', 'https://openalex.org/W3176395632', 'https://openalex.org/W6634498817', 'https://openalex.org/W2962780374', 'https://openalex.org/W2964182350', 'https://openalex.org/W2714726990', 'https://openalex.org/W3134751001', 'https://openalex.org/W2899274165', 'https://openalex.org/W1574972348', 'https://openalex.org/W2126618487', 'https://openalex.org/W2556930864', 'https://openalex.org/W4288297863', 'https://openalex.org/W2962866381', 'https://openalex.org/W1503933356', 'https://openalex.org/W1861492603', 'https://openalex.org/W3015678833', 'https://openalex.org/W1494198834', 'https://openalex.org/W3102219307', 'https://openalex.org/W4394453761', 'https://openalex.org/W2995680346', 'https://openalex.org/W2890952074', 'https://openalex.org/W3085380432', 'https://openalex.org/W3162293946', 'https://openalex.org/W2796315435', 'https://openalex.org/W2939311817']",2022-06-01
https://openalex.org/W3161204797,https://doi.org/10.1109/icassp39728.2021.9414418,Align or attend? Toward More Efficient and Accurate Spoken Word Discovery Using Speech-to-Image Retrieval,"Multimodal word discovery (MWD) is often treated as a byproduct of the speech-to-image retrieval problem. However, our theoretical analysis shows that some kind of alignment/attention mechanism is crucial for a MWD system to learn meaningful word-level representation. We verify our theory by conducting retrieval and word discovery experiments on MSCOCO and Flickr8k, and empirically demonstrate that both neural MT with self-attention and statistical MT achieve word discovery scores that are superior to those of a state-of-the-art neural retrieval system, outperforming it by 2% and 5% alignment F1 scores respectively.","['https://openalex.org/W2752796333', 'https://openalex.org/W6652311901', 'https://openalex.org/W6679434410', 'https://openalex.org/W3095293218', 'https://openalex.org/W6739901393', 'https://openalex.org/W6712720595', 'https://openalex.org/W6639102338', 'https://openalex.org/W6680628865', 'https://openalex.org/W2736876693', 'https://openalex.org/W2122228338', 'https://openalex.org/W2963902314', 'https://openalex.org/W2962780374', 'https://openalex.org/W6729977899', 'https://openalex.org/W2971709506', 'https://openalex.org/W2963966654', 'https://openalex.org/W2920166246', 'https://openalex.org/W2950133079', 'https://openalex.org/W2988907666', 'https://openalex.org/W6770596778', 'https://openalex.org/W6676647902', 'https://openalex.org/W2108598243', 'https://openalex.org/W6620707391', 'https://openalex.org/W2745461083', 'https://openalex.org/W2277195237', 'https://openalex.org/W2064675550', 'https://openalex.org/W2148154194', 'https://openalex.org/W2613718673', 'https://openalex.org/W2995680346', 'https://openalex.org/W1861492603', 'https://openalex.org/W2556930864', 'https://openalex.org/W2963799213', 'https://openalex.org/W2112912048', 'https://openalex.org/W3102219307', 'https://openalex.org/W2119775030', 'https://openalex.org/W2962862718', 'https://openalex.org/W2796315435', 'https://openalex.org/W4385245566', 'https://openalex.org/W639708223', 'https://openalex.org/W2963403868', 'https://openalex.org/W2962832640', 'https://openalex.org/W2964308564', 'https://openalex.org/W2133564696', 'https://openalex.org/W2137010615', 'https://openalex.org/W2398490608', 'https://openalex.org/W2006969979']",2021-05-13
https://openalex.org/W3204267711,https://doi.org/10.1109/icassp43922.2022.9747336,Unsupervised Audio-Caption Aligning Learns Correspondences Between Individual Sound Events and Textual Phrases,"We investigate unsupervised learning of correspondences between sound events and textual phrases through aligning audio clips with textual captions describing the content of a whole audio clip. We align originally unaligned and unannotated audio clips and their captions by scoring the similarities between audio frames and words, as encoded by modality-specific encoders and using a ranking-loss criterion to optimize the model. After training, we obtain clip-caption similarity by averaging frame-word similarities and estimate event-phrase correspondences by calculating frame-phrase similarities. We evaluate the method with two cross-modal tasks: audio-caption retrieval, and phrase-based sound event detection (SED). Experimental results show that the proposed method can globally associate audio clips with captions as well as locally learn correspondences between individual sound events and textual phrases in an unsupervised manner.","['https://openalex.org/W2997525715', 'https://openalex.org/W3005971801', 'https://openalex.org/W2796315435', 'https://openalex.org/W3197467690', 'https://openalex.org/W3161204797', 'https://openalex.org/W3163843406', 'https://openalex.org/W2171590421', 'https://openalex.org/W6770596778', 'https://openalex.org/W3157861865', 'https://openalex.org/W3044495139', 'https://openalex.org/W3015591594', 'https://openalex.org/W6729977899', 'https://openalex.org/W2619383789', 'https://openalex.org/W2964213897', 'https://openalex.org/W2940092410', 'https://openalex.org/W2965809241', 'https://openalex.org/W6636510571', 'https://openalex.org/W2945761034', 'https://openalex.org/W2995680346', 'https://openalex.org/W2963902314', 'https://openalex.org/W2127589108', 'https://openalex.org/W2556930864', 'https://openalex.org/W1614298861']",2022-04-27
https://openalex.org/W4247178956,https://doi.org/10.31234/osf.io/pt9xq,Reverse engineering language acquisition with child-centered long-form recordings,"Language use in everyday life can be studied using lightweight, wearable recorders that collect long-form recordings - that is, audio (including speech) over whole days. The hardware and software underlying this technique is increasingly accessible and inexpensive, and these data are revolutionizing the language acquisition field. We first place this technique into the broader context of the current ways of studying both the input being received by children and children’s own language production, laying out the main advantages and drawbacks of long-form recordings. We then go on to argue that a unique advantage of long-form recordings is that they can fuel realistic models of early language acquisition that use speech to represent children's input and/or to establish production benchmarks. To enable the field to make the most of this unique empirical and conceptual contribution, we outline what this reverse engineering approach from long-form recordings entails, why it is useful, and how to evaluate success.","['https://openalex.org/W2759573091', 'https://openalex.org/W3177829661', 'https://openalex.org/W1592295210', 'https://openalex.org/W1532494781', 'https://openalex.org/W6789725041', 'https://openalex.org/W2885156775', 'https://openalex.org/W2063303346', 'https://openalex.org/W2772732614', 'https://openalex.org/W3112495382', 'https://openalex.org/W6665016202', 'https://openalex.org/W1984586950', 'https://openalex.org/W3046659789', 'https://openalex.org/W6790730029', 'https://openalex.org/W6781919819', 'https://openalex.org/W2997253105', 'https://openalex.org/W2944539396', 'https://openalex.org/W2586148577', 'https://openalex.org/W4213306813', 'https://openalex.org/W3123651923', 'https://openalex.org/W3005165546', 'https://openalex.org/W2015075592', 'https://openalex.org/W3135377987', 'https://openalex.org/W2483390977', 'https://openalex.org/W2160997109', 'https://openalex.org/W2774051897', 'https://openalex.org/W6685399832', 'https://openalex.org/W2991557631', 'https://openalex.org/W2112883467', 'https://openalex.org/W6633336849', 'https://openalex.org/W2071591642', 'https://openalex.org/W2889102505', 'https://openalex.org/W6781576031', 'https://openalex.org/W6629325461', 'https://openalex.org/W6641916425', 'https://openalex.org/W2085478996', 'https://openalex.org/W3110458199', 'https://openalex.org/W2618478924', 'https://openalex.org/W2132566726', 'https://openalex.org/W2972476505', 'https://openalex.org/W3025683731', 'https://openalex.org/W3125043549', 'https://openalex.org/W6729411815', 'https://openalex.org/W3129957462', 'https://openalex.org/W3125087428', 'https://openalex.org/W2801193150', 'https://openalex.org/W2141994663', 'https://openalex.org/W6658483803', 'https://openalex.org/W2799770360', 'https://openalex.org/W2805234167', 'https://openalex.org/W6681346506', 'https://openalex.org/W4407276585', 'https://openalex.org/W2065159495', 'https://openalex.org/W2621934507', 'https://openalex.org/W2252657604', 'https://openalex.org/W2101509422', 'https://openalex.org/W2803055582', 'https://openalex.org/W2058616551', 'https://openalex.org/W2135563147', 'https://openalex.org/W2060238187', 'https://openalex.org/W4253971549', 'https://openalex.org/W4246103655', 'https://openalex.org/W2058354688', 'https://openalex.org/W2343593471', 'https://openalex.org/W4255020641', 'https://openalex.org/W4251221781', 'https://openalex.org/W1559022555', 'https://openalex.org/W4232589384', 'https://openalex.org/W1558150890', 'https://openalex.org/W4297612016', 'https://openalex.org/W4245117732', 'https://openalex.org/W2546861836', 'https://openalex.org/W4253947715', 'https://openalex.org/W2032543155', 'https://openalex.org/W4287591426', 'https://openalex.org/W3080620940', 'https://openalex.org/W1967834254', 'https://openalex.org/W1485633403', 'https://openalex.org/W4242334097', 'https://openalex.org/W4236000557', 'https://openalex.org/W3047246203', 'https://openalex.org/W4251435902', 'https://openalex.org/W2995680346', 'https://openalex.org/W3126722376', 'https://openalex.org/W4252366034']",2021-03-31
https://openalex.org/W3096372900,https://doi.org/10.21437/interspeech.2020-3024,Speech-Image Semantic Alignment Does Not Depend on Any Prior Classification Tasks,"Semantically-aligned $(speech, image)$ datasets can be used to explore visually-grounded speech. In a majority of existing investigations, features of an image signal are extracted using neural networks on other tasks (e.g., classification on ImageNet). In still others, pre-trained networks are used to extract audio features prior to semantic embedding. Without transfer learning through pre-trained initialization or pre-trained feature extraction, previous results have tended to show low rates of recall in $speech \rightarrow image$ and $image \rightarrow speech$ queries. 
Choosing appropriate neural architectures for encoders in the speech and image branches and using large datasets, one can obtain competitive recall rates without any reliance on any pre-trained initialization or feature extraction: $(speech,image)$ semantic alignment and $speech \rightarrow image$ and $image \rightarrow speech$ retrieval are canonical tasks worthy of independent investigation of their own and allow one to explore other questions---e.g., the size of the audio embedder can be reduced significantly with little loss of recall rates in $speech \rightarrow image$ and $image \rightarrow speech$ queries.","['https://openalex.org/W2972345028', 'https://openalex.org/W2123081785', 'https://openalex.org/W385555557', 'https://openalex.org/W2016076298', 'https://openalex.org/W2971709506', 'https://openalex.org/W2556930864', 'https://openalex.org/W2483390977', 'https://openalex.org/W2963902314', 'https://openalex.org/W2970971581', 'https://openalex.org/W2950133079', 'https://openalex.org/W2964115348', 'https://openalex.org/W2964001192', 'https://openalex.org/W2134670479', 'https://openalex.org/W2995680346', 'https://openalex.org/W1522301498', 'https://openalex.org/W1895577753', 'https://openalex.org/W2586148577', 'https://openalex.org/W2962753610', 'https://openalex.org/W2963966654', 'https://openalex.org/W1905882502', 'https://openalex.org/W2963263347', 'https://openalex.org/W2988907666', 'https://openalex.org/W2949999304', 'https://openalex.org/W68733909', 'https://openalex.org/W2730658205', 'https://openalex.org/W1527935828', 'https://openalex.org/W2964054038', 'https://openalex.org/W2920166246', 'https://openalex.org/W2194775991', 'https://openalex.org/W2962862718', 'https://openalex.org/W2963446712', 'https://openalex.org/W2963702144', 'https://openalex.org/W1514535095']",2020-10-25
https://openalex.org/W4319862278,https://doi.org/10.1109/slt54892.2023.10023079,Towards Visually Prompted Keyword Localisation for Zero-Resource Spoken Languages,"Imagine being able to show a system a visual depiction of a keyword and finding spoken utterances that contain this keyword from a zero-resource speech corpus. We formalise this task and call it visually prompted keyword localisation (VPKL): given an image of a keyword, detect and predict where in an utterance the keyword occurs. To do VPKL, we propose a speech-vision model with a novel localising attention mechanism which we train with a new keyword sampling scheme. We show that these innovations give improvements in VPKL over an existing speech-vision model. We also compare to a visual bag-of-words (BoW) model where images are automatically tagged with visual labels and paired with unlabelled speech. Although this visual BoW can be queried directly with a written keyword (while our's takes image queries), our new model still outperforms the visual BoW in both detection and localisation, giving a 16% relative improvement in localisation F1.","['https://openalex.org/W3157861865', 'https://openalex.org/W2963902314', 'https://openalex.org/W2962753610', 'https://openalex.org/W6729977899', 'https://openalex.org/W4224875474', 'https://openalex.org/W3200287550', 'https://openalex.org/W2973135958', 'https://openalex.org/W3198411039', 'https://openalex.org/W3016087077', 'https://openalex.org/W6786915337', 'https://openalex.org/W4285110637', 'https://openalex.org/W1987959440', 'https://openalex.org/W2896342372', 'https://openalex.org/W4239171064', 'https://openalex.org/W2132921748', 'https://openalex.org/W6728167234', 'https://openalex.org/W2973180715', 'https://openalex.org/W2013596317', 'https://openalex.org/W2099415988', 'https://openalex.org/W2405666970', 'https://openalex.org/W2101346879', 'https://openalex.org/W2964001192', 'https://openalex.org/W2964099072', 'https://openalex.org/W2194775991', 'https://openalex.org/W3095361818', 'https://openalex.org/W2508907749', 'https://openalex.org/W6771812881', 'https://openalex.org/W2962862718', 'https://openalex.org/W6637373629', 'https://openalex.org/W2108598243', 'https://openalex.org/W6770596778', 'https://openalex.org/W6631190155', 'https://openalex.org/W2950133079', 'https://openalex.org/W6756623301', 'https://openalex.org/W4297841895', 'https://openalex.org/W3049010983', 'https://openalex.org/W3198749384', 'https://openalex.org/W3102219307', 'https://openalex.org/W2995680346']",2023-01-09
https://openalex.org/W4389519569,https://doi.org/10.18653/v1/2023.emnlp-main.140,Towards Noise-Tolerant Speech-Referring Video Object Segmentation: Bridging Speech and Text,"Linguistic communication is prevalent in Human-Computer Interaction (HCI). Speech (spoken language) serves as a convenient yet potentially ambiguous form due to noise and accents, exposing a gap compared to text. In this study, we investigate the prominent HCI task, Referring Video Object Segmentation (R-VOS), which aims to segment and track objects using linguistic references. While text input is well-investigated, speech input is under-explored. Our objective is to bridge the gap between speech and text, enabling the adaptation of existing text-input R-VOS models to accommodate noisy speech input effectively. Specifically, we propose a method to align the semantic spaces between speech and text by incorporating two key modules: 1) Noise-Aware Semantic Adjustment (NSA) for clear semantics extraction from noisy speech; and 2) Semantic Jitter Suppression (SJS) enabling R-VOS models to tolerate noisy queries. Comprehensive experiments conducted on the challenging AVOS benchmarks reveal that our proposed method outperforms state-of-the-art approaches.","['https://openalex.org/W3036601975', 'https://openalex.org/W4384811751', 'https://openalex.org/W4382467756', 'https://openalex.org/W3104844437', 'https://openalex.org/W4312655706', 'https://openalex.org/W2995680346', 'https://openalex.org/W3183673520', 'https://openalex.org/W2916743882', 'https://openalex.org/W3157861865', 'https://openalex.org/W4214622647', 'https://openalex.org/W3034328552', 'https://openalex.org/W2593116425', 'https://openalex.org/W4292692470', 'https://openalex.org/W4386113246', 'https://openalex.org/W4387323208', 'https://openalex.org/W3110109236', 'https://openalex.org/W4385823188', 'https://openalex.org/W3166396011', 'https://openalex.org/W3142316150', 'https://openalex.org/W2990205821', 'https://openalex.org/W3172917640', 'https://openalex.org/W4226024706', 'https://openalex.org/W4287330664', 'https://openalex.org/W3200949949', 'https://openalex.org/W1905722737', 'https://openalex.org/W3175132347', 'https://openalex.org/W2194775991', 'https://openalex.org/W2034014085', 'https://openalex.org/W2137010615', 'https://openalex.org/W2251512949', 'https://openalex.org/W4386075877', 'https://openalex.org/W3093016409', 'https://openalex.org/W4372263374', 'https://openalex.org/W3094664776', 'https://openalex.org/W4378760036', 'https://openalex.org/W3146550708', 'https://openalex.org/W4309431384', 'https://openalex.org/W2908510526', 'https://openalex.org/W4312837597', 'https://openalex.org/W2884561390', 'https://openalex.org/W3215899623', 'https://openalex.org/W3177322837', 'https://openalex.org/W4386076133', 'https://openalex.org/W3108819577', 'https://openalex.org/W4385823052', 'https://openalex.org/W4312689520', 'https://openalex.org/W4312852845', 'https://openalex.org/W4287392530', 'https://openalex.org/W2565639579', 'https://openalex.org/W2962766617', 'https://openalex.org/W3159619744', 'https://openalex.org/W2993182889', 'https://openalex.org/W4288072840', 'https://openalex.org/W3211993051', 'https://openalex.org/W4390872515', 'https://openalex.org/W3106546328', 'https://openalex.org/W3171516518']",2023-01-01
https://openalex.org/W3196698946,https://doi.org/10.21437/interspeech.2021-96,"Talk, Don&amp;#8217;t Write: A Study of Direct Speech-Based Image Retrieval","Speech-based image retrieval has been studied as a proxy for joint representation learning, usually without emphasis on retrieval itself.As such, it is unclear how well speech-based retrieval can work in practice -both in an absolute sense and versus alternative strategies that combine automatic speech recognition (ASR) with strong text encoders.In this work, we extensively study and expand choices of encoder architectures, training methodology (including unimodal and multimodal pretraining), and other factors.Our experiments cover different types of speech in three datasets: Flickr Audio, Places Audio, and Localized Narratives.Our best model configuration achieves large gains over state of the art, e.g., pushing recall-atone from 21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio.We also show our best speech-based models can match or exceed cascaded ASR-to-text encoding when speech is spontaneous, accented, or otherwise hard to automatically transcribe.","['https://openalex.org/W2988823324', 'https://openalex.org/W2972345028', 'https://openalex.org/W2119775030', 'https://openalex.org/W3015300171', 'https://openalex.org/W2923959898', 'https://openalex.org/W2896457183', 'https://openalex.org/W2964099072', 'https://openalex.org/W2901894078', 'https://openalex.org/W3100813302', 'https://openalex.org/W2971709506', 'https://openalex.org/W2998649947', 'https://openalex.org/W3005680577', 'https://openalex.org/W2593116425', 'https://openalex.org/W4287757663', 'https://openalex.org/W2556930864', 'https://openalex.org/W3197828817', 'https://openalex.org/W2927673779', 'https://openalex.org/W4297826211', 'https://openalex.org/W2137010615', 'https://openalex.org/W2963525826', 'https://openalex.org/W3155594712', 'https://openalex.org/W2973049979', 'https://openalex.org/W2886641317', 'https://openalex.org/W2995680346', 'https://openalex.org/W1905882502', 'https://openalex.org/W1931639407', 'https://openalex.org/W3095670406', 'https://openalex.org/W2955425717', 'https://openalex.org/W2962753610', 'https://openalex.org/W4394453761', 'https://openalex.org/W2988907666', 'https://openalex.org/W2586148577', 'https://openalex.org/W2989358187', 'https://openalex.org/W2964001192', 'https://openalex.org/W2796315435']",2021-08-27
https://openalex.org/W4294533720,https://doi.org/10.1145/3503161.3547996,Video-Guided Curriculum Learning for Spoken Video Grounding,"In this paper, we introduce a new task, spoken video grounding (SVG), which aims to localize the desired video fragments from spoken language descriptions. Compared with using text, employing audio requires the model to directly exploit the useful phonemes and syllables related to the video from raw speech. Moreover, we randomly add environmental noises to this speech audio, further increasing the difficulty of this task and better simulating real applications. To rectify the discriminative phonemes and extract video-related information from noisy audio, we develop a novel video-guided curriculum learning (VGCL) during the audio pre-training process, which can make use of the vital visual perceptions to help understand the spoken language and suppress the external noise. Considering during inference the model can not obtain ground truth video segments, we design a curriculum strategy that gradually shifts the input video from the ground truth to the entire video content during pre-training. Finally, the model can learn how to extract critical visual information from the entire video clip to help understand the spoken language. In addition, we collect the first large-scale spoken video grounding dataset based on ActivityNet, which is named as ActivityNet Speech dataset. Extensive experiments demonstrate our proposed video-guided curriculum learning can facilitate the pre-training process to obtain a mutual audio encoder, significantly promoting the performance of spoken video grounding tasks. Moreover, we prove that in the case of noisy sound, our model outperforms the method that grounding video with ASR transcripts, further demonstrating the effectiveness of our curriculum strategy.","['https://openalex.org/W2296073425', 'https://openalex.org/W2964089981', 'https://openalex.org/W2796315435', 'https://openalex.org/W2964232540', 'https://openalex.org/W1927052826', 'https://openalex.org/W3174026887', 'https://openalex.org/W2885485938', 'https://openalex.org/W3035339529', 'https://openalex.org/W3161945002', 'https://openalex.org/W2052666245', 'https://openalex.org/W2005874308', 'https://openalex.org/W1522734439', 'https://openalex.org/W3191850102', 'https://openalex.org/W2948958195', 'https://openalex.org/W3178087530', 'https://openalex.org/W2904824998', 'https://openalex.org/W3035490389', 'https://openalex.org/W2962869524', 'https://openalex.org/W3170972077', 'https://openalex.org/W2963017553', 'https://openalex.org/W3174311593', 'https://openalex.org/W1924762813', 'https://openalex.org/W3198749384', 'https://openalex.org/W2973132572', 'https://openalex.org/W2964023933', 'https://openalex.org/W2250539671', 'https://openalex.org/W2578908617', 'https://openalex.org/W4294643831', 'https://openalex.org/W3197032408', 'https://openalex.org/W2132730112', 'https://openalex.org/W3093174808', 'https://openalex.org/W3157861865', 'https://openalex.org/W2559655401', 'https://openalex.org/W3197828817', 'https://openalex.org/W3092945658', 'https://openalex.org/W4287116649', 'https://openalex.org/W2962800562', 'https://openalex.org/W3208327830', 'https://openalex.org/W2962854302', 'https://openalex.org/W3196863408', 'https://openalex.org/W2903901502', 'https://openalex.org/W3034743747', 'https://openalex.org/W4287635695', 'https://openalex.org/W2963916161', 'https://openalex.org/W2100913937', 'https://openalex.org/W2965147078', 'https://openalex.org/W2923622379', 'https://openalex.org/W2995680346', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963902314', 'https://openalex.org/W4297808394', 'https://openalex.org/W3101429639']",2022-10-10
https://openalex.org/W3161411634,https://doi.org/10.1109/icassp39728.2021.9413680,A Comparison of Discrete Latent Variable Models for Speech Representation Learning,Neural latent variable models enable the discovery of interesting structure in speech audio data. This paper presents a comparison of two different approaches which are broadly based on predicting future time-steps or auto-encoding the input signal. Our study compares the representations learned by vq-vae and vq-wav2vec in terms of sub-word unit discovery and phoneme recognition performance. Results show that future time-step prediction with vq-wav2vec achieves better performance. The best system achieves an error rate of 13.22 on the ZeroSpeech 2019 ABX phoneme discrimination challenge.,"['https://openalex.org/W6770596778', 'https://openalex.org/W6769196770', 'https://openalex.org/W6729448088', 'https://openalex.org/W6748381668', 'https://openalex.org/W2124509324', 'https://openalex.org/W6753018729', 'https://openalex.org/W6780218876', 'https://openalex.org/W1494198834', 'https://openalex.org/W6917638038', 'https://openalex.org/W2752796333', 'https://openalex.org/W6734376636', 'https://openalex.org/W3100270690', 'https://openalex.org/W2973049979', 'https://openalex.org/W6761553608', 'https://openalex.org/W6769313972', 'https://openalex.org/W2118154032', 'https://openalex.org/W2933138175', 'https://openalex.org/W2953190524', 'https://openalex.org/W2970119519', 'https://openalex.org/W6755207826', 'https://openalex.org/W2842511635', 'https://openalex.org/W3095361818', 'https://openalex.org/W2930682606', 'https://openalex.org/W2963263347', 'https://openalex.org/W2519091744', 'https://openalex.org/W2996383576', 'https://openalex.org/W3099782249', 'https://openalex.org/W2789543585', 'https://openalex.org/W2950365520', 'https://openalex.org/W2963799213', 'https://openalex.org/W2979476256', 'https://openalex.org/W2995680346', 'https://openalex.org/W2547875792', 'https://openalex.org/W3127686677', 'https://openalex.org/W2963341956', 'https://openalex.org/W3015419784', 'https://openalex.org/W2811079561', 'https://openalex.org/W3036601975', 'https://openalex.org/W2962942158', 'https://openalex.org/W4297808394', 'https://openalex.org/W2896457183', 'https://openalex.org/W3125709657']",2021-05-13
https://openalex.org/W4313053756,https://doi.org/10.1109/tcds.2022.3210751,Double Articulation Analyzer With Prosody for Unsupervised Word and Phone Discovery,"Infants acquire words and phonemes from unsegmented speech signals using segmentation cues, such as distributional, prosodic, and co-occurrence cues. Many pre-existing computational models that represent the process tend to focus on distributional or prosodic cues. This paper proposes a nonparametric Bayesian probabilistic generative model called the prosodic hierarchical Dirichlet process-hidden language model (Prosodic HDP-HLM). Prosodic HDP-HLM, an extension of HDP-HLM, considers both prosodic and distributional cues within a single integrative generative model. We conducted three experiments on different types of datasets, and demonstrate the validity of the proposed method. The results show that the Prosodic DAA successfully uses prosodic cues and outperforms a method that solely uses distributional cues. The main contributions of this study are as follows: 1) We develop a probabilistic generative model for time series data including prosody that potentially has a double articulation structure; 2) We propose the Prosodic DAA by deriving the inference procedure for Prosodic HDP-HLM and show that Prosodic DAA can discover words directly from continuous human speech signals using statistical information and prosodic information in an unsupervised manner; 3) We show that prosodic cues contribute to word segmentation more in naturally distributed case words, i.e., they follow Zipf's law.","['https://openalex.org/W2017449852', 'https://openalex.org/W6634986005', 'https://openalex.org/W2142111485', 'https://openalex.org/W2115867364', 'https://openalex.org/W2335112305', 'https://openalex.org/W2073685522', 'https://openalex.org/W4247883378', 'https://openalex.org/W2148764920', 'https://openalex.org/W2157149948', 'https://openalex.org/W4220814065', 'https://openalex.org/W1980862600', 'https://openalex.org/W3154005821', 'https://openalex.org/W2294206829', 'https://openalex.org/W2126377586', 'https://openalex.org/W2055226328', 'https://openalex.org/W2465308763', 'https://openalex.org/W2153791616', 'https://openalex.org/W3161215977', 'https://openalex.org/W3095361818', 'https://openalex.org/W2468716020', 'https://openalex.org/W3046603443', 'https://openalex.org/W3198134274', 'https://openalex.org/W2963620343', 'https://openalex.org/W3102667484', 'https://openalex.org/W3093096176', 'https://openalex.org/W1796128977', 'https://openalex.org/W3096656254', 'https://openalex.org/W2099626219', 'https://openalex.org/W2165470078', 'https://openalex.org/W2908005395', 'https://openalex.org/W1540332606', 'https://openalex.org/W2972937794', 'https://openalex.org/W2059824090', 'https://openalex.org/W4239549861', 'https://openalex.org/W1993755070', 'https://openalex.org/W2153767712', 'https://openalex.org/W2963720603', 'https://openalex.org/W6675022971', 'https://openalex.org/W2110614779', 'https://openalex.org/W2121137195', 'https://openalex.org/W2256409625', 'https://openalex.org/W1778492285', 'https://openalex.org/W2022058071', 'https://openalex.org/W2024079589', 'https://openalex.org/W6602703493', 'https://openalex.org/W2078769636', 'https://openalex.org/W6685161094', 'https://openalex.org/W6770596778', 'https://openalex.org/W2752552296', 'https://openalex.org/W2938991416', 'https://openalex.org/W3026278778', 'https://openalex.org/W6729977899', 'https://openalex.org/W6786915337', 'https://openalex.org/W2767009175', 'https://openalex.org/W6640963894', 'https://openalex.org/W2962206103', 'https://openalex.org/W2586148577', 'https://openalex.org/W4312334695', 'https://openalex.org/W3004441278', 'https://openalex.org/W3138660618', 'https://openalex.org/W2912021853', 'https://openalex.org/W2066006005', 'https://openalex.org/W6746134036', 'https://openalex.org/W2795648103', 'https://openalex.org/W2052011466', 'https://openalex.org/W1502984613', 'https://openalex.org/W1545504645', 'https://openalex.org/W3098643042', 'https://openalex.org/W3111013239', 'https://openalex.org/W3097485645', 'https://openalex.org/W2321470647', 'https://openalex.org/W1959608418', 'https://openalex.org/W3100945159', 'https://openalex.org/W3099142230', 'https://openalex.org/W2995680346', 'https://openalex.org/W2170353620', 'https://openalex.org/W2772857214', 'https://openalex.org/W2556930864', 'https://openalex.org/W3102739662', 'https://openalex.org/W66464469', 'https://openalex.org/W2100768664', 'https://openalex.org/W2949115596', 'https://openalex.org/W3099484702']",2022-09-29
https://openalex.org/W4372266917,https://doi.org/10.1109/icassp49357.2023.10095091,Hindi as a Second Language: Improving Visually Grounded Speech with Semantically Similar Samples,"The objective of this work is to explore the learning of visually grounded speech models (VGS) from multilingual perspective. Bilingual VGS models are generally trained with an equal number of spoken captions from both languages. However, in reality, there can be an imbalance among the languages for the available spoken captions. Our key contribution in this work is to leverage the power of a high-resource language in a bilingual visually grounded speech model to improve the performance of a low-resource language. We introduce two methods to distill the knowledge of high-resource language into low-resource languages: (1) incorporating a strong pre-trained high-resource language encoder and (2) using semantically similar spoken captions. Our experiments show that combining these two approaches effectively enables the low-resource language to surpass the performances of monolingual and bilingual counterparts for cross-modal retrieval tasks.","['https://openalex.org/W6770596778', 'https://openalex.org/W2963525826', 'https://openalex.org/W2964099072', 'https://openalex.org/W3198411039', 'https://openalex.org/W2962753610', 'https://openalex.org/W4224875474', 'https://openalex.org/W2586850765', 'https://openalex.org/W2962862718', 'https://openalex.org/W3168851777', 'https://openalex.org/W2963330681', 'https://openalex.org/W4312528757', 'https://openalex.org/W3196694757', 'https://openalex.org/W6803728867', 'https://openalex.org/W6784660784', 'https://openalex.org/W4224925617', 'https://openalex.org/W3158714121', 'https://openalex.org/W3015300171', 'https://openalex.org/W6776700526', 'https://openalex.org/W2842511635', 'https://openalex.org/W6729977899', 'https://openalex.org/W3095881291', 'https://openalex.org/W6779340355', 'https://openalex.org/W2938991416', 'https://openalex.org/W3100813302', 'https://openalex.org/W2586148577', 'https://openalex.org/W2963902314', 'https://openalex.org/W2964001192', 'https://openalex.org/W2995680346', 'https://openalex.org/W3207750165', 'https://openalex.org/W4287812705', 'https://openalex.org/W4297808394', 'https://openalex.org/W3197828817', 'https://openalex.org/W3094454579', 'https://openalex.org/W2556930864']",2023-05-05
https://openalex.org/W3100202343,https://doi.org/10.1109/icassp39728.2021.9414899,A Hierarchical Subspace Model for Language-Attuned Acoustic Unit Discovery,"In this work, we propose a hierarchical subspace model for acoustic unit discovery. In this approach, we frame the task as one of learning embeddings on a low-dimensional phonetic subspace, and simultaneously specify the subspace itself as an embedding on a hyper-subspace. We train the hyper-subspace on a set of transcribed languages and transfer it to the target language. In the target language, we infer both the language and unit embeddings in an unsupervised manner, and in so doing, we simultaneously learn a subspace of units specific to that language and the units that dwell on it. We conduct our experiments on TIMIT and two low-resource languages: Mboshi and Yoruba. Results show that our model outperforms major acoustic unit discovery techniques, both in terms of clustering quality and segmentation accuracy.","['https://openalex.org/W6679792166', 'https://openalex.org/W1494198834', 'https://openalex.org/W2786902352', 'https://openalex.org/W2888911345', 'https://openalex.org/W2927191280', 'https://openalex.org/W6865393803', 'https://openalex.org/W2962693497', 'https://openalex.org/W2641832364', 'https://openalex.org/W6640963894', 'https://openalex.org/W1981706894', 'https://openalex.org/W6744702808', 'https://openalex.org/W2940544976', 'https://openalex.org/W2127498532', 'https://openalex.org/W2963620343', 'https://openalex.org/W6769196770', 'https://openalex.org/W3100270690', 'https://openalex.org/W6770596778', 'https://openalex.org/W6675022971', 'https://openalex.org/W2752796333', 'https://openalex.org/W6973666849', 'https://openalex.org/W2347098582', 'https://openalex.org/W2483390977', 'https://openalex.org/W3092791109', 'https://openalex.org/W2084534958', 'https://openalex.org/W6712757354', 'https://openalex.org/W2195354', 'https://openalex.org/W6712648922', 'https://openalex.org/W6731521493', 'https://openalex.org/W2973026522', 'https://openalex.org/W1522301498', 'https://openalex.org/W2100768664', 'https://openalex.org/W2401396251', 'https://openalex.org/W2972574141', 'https://openalex.org/W2963799213', 'https://openalex.org/W2996383576', 'https://openalex.org/W2401271873', 'https://openalex.org/W2762715843', 'https://openalex.org/W2995680346', 'https://openalex.org/W2572097499', 'https://openalex.org/W2786608204', 'https://openalex.org/W1959608418', 'https://openalex.org/W2134670479']",2021-05-13
https://openalex.org/W3161215977,https://doi.org/10.48550/arxiv.2011.03115,A Hierarchical Subspace Model for Language-Attuned Acoustic Unit\n Discovery,"In this work, we propose a hierarchical subspace model for acoustic unit\ndiscovery. In this approach, we frame the task as one of learning embeddings on\na low-dimensional phonetic subspace, and simultaneously specify the subspace\nitself as an embedding on a hyper-subspace. We train the hyper-subspace on a\nset of transcribed languages and transfer it to the target language. In the\ntarget language, we infer both the language and unit embeddings in an\nunsupervised manner, and in so doing, we simultaneously learn a subspace of\nunits specific to that language and the units that dwell on it. We conduct our\nexperiments on TIMIT and two low-resource languages: Mboshi and Yoruba. Results\nshow that our model outperforms major acoustic unit discovery techniques, both\nin terms of clustering quality and segmentation accuracy.\n","['https://openalex.org/W3092791109', 'https://openalex.org/W2084534958', 'https://openalex.org/W2963799213', 'https://openalex.org/W2195354', 'https://openalex.org/W1981706894', 'https://openalex.org/W2401396251', 'https://openalex.org/W2100768664', 'https://openalex.org/W3100270690', 'https://openalex.org/W2979476256', 'https://openalex.org/W1959608418', 'https://openalex.org/W2962693497', 'https://openalex.org/W2963620343', 'https://openalex.org/W2888911345', 'https://openalex.org/W2572097499', 'https://openalex.org/W2927191280', 'https://openalex.org/W2134670479', 'https://openalex.org/W2641832364', 'https://openalex.org/W2973026522', 'https://openalex.org/W2786608204', 'https://openalex.org/W2347098582', 'https://openalex.org/W2995680346', 'https://openalex.org/W4394867155', 'https://openalex.org/W2401271873', 'https://openalex.org/W2996383576', 'https://openalex.org/W2972574141', 'https://openalex.org/W2786902352', 'https://openalex.org/W2127498532', 'https://openalex.org/W1494198834', 'https://openalex.org/W2483390977', 'https://openalex.org/W2940544976', 'https://openalex.org/W1522301498', 'https://openalex.org/W3125709657', 'https://openalex.org/W2762715843']",2020-11-04
https://openalex.org/W3158565912,https://doi.org/10.1109/iscas51556.2021.9401232,Learning Fine-Grained Semantics in Spoken Language Using Visual Grounding,"&lt;p&gt;In the case of unwritten languages, acoustic models cannot be trained in the standard way, i.e., using speech and textual transcriptions. Recently, several methods have been proposed to learn speech representations using images, i.e., using visual grounding. Existing studies have focused on scene images. Here, we investigate whether fine-grained semantic information, reflecting the relationship between attributes and objects, can be learned from spoken language. To this end, a Fine-grained Semantic Embedding Network (FSEN) for learning semantic representations of spoken language grounded by fine-grained images is proposed. For training, we propose an efficient objective function, which includes a matching constraint, an adversarial objective, and a classification constraint. The learned speech representations are evaluated using two tasks, i.e., speech-image cross-modal retrieval and speech-to-image generation. On the retrieval task, FSEN outperforms other state-of-the-art methods on both a scene image dataset and two fine-grained datasets. The image generation task shows that the learned speech representations can be used to generate high-quality and semantic-consistent fine-grained images. Learning fine-grained semantics from spoken language via visual grounding is thus possible.&lt;/p&gt;","['https://openalex.org/W6718379498', 'https://openalex.org/W2962845008', 'https://openalex.org/W2963163163', 'https://openalex.org/W6713645886', 'https://openalex.org/W6765779288', 'https://openalex.org/W6638319203', 'https://openalex.org/W2533598788', 'https://openalex.org/W2398118205', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963680395', 'https://openalex.org/W2765440071', 'https://openalex.org/W2967957126', 'https://openalex.org/W2327501763', 'https://openalex.org/W2157331557', 'https://openalex.org/W2194775991', 'https://openalex.org/W2962862718', 'https://openalex.org/W2988907666', 'https://openalex.org/W68733909', 'https://openalex.org/W2971709506', 'https://openalex.org/W2950133079', 'https://openalex.org/W2894786240', 'https://openalex.org/W2964001192', 'https://openalex.org/W6750651883', 'https://openalex.org/W2963902314', 'https://openalex.org/W6729977899', 'https://openalex.org/W6770596778', 'https://openalex.org/W2347145335', 'https://openalex.org/W2117539524', 'https://openalex.org/W2985951359', 'https://openalex.org/W2963966654', 'https://openalex.org/W6779669310', 'https://openalex.org/W6631190155', 'https://openalex.org/W2962793481', 'https://openalex.org/W2556930864', 'https://openalex.org/W4301206121', 'https://openalex.org/W2995680346', 'https://openalex.org/W1522301498', 'https://openalex.org/W3102219307', 'https://openalex.org/W1797268635', 'https://openalex.org/W2405756170', 'https://openalex.org/W2964121744', 'https://openalex.org/W2187089797', 'https://openalex.org/W2963373786', 'https://openalex.org/W2099471712', 'https://openalex.org/W4320013936', 'https://openalex.org/W2963981733', 'https://openalex.org/W2796315435']",2021-04-27
https://openalex.org/W3196694757,https://doi.org/10.21437/interspeech.2021-1352,Cascaded Multilingual Audio-Visual Learning from Videos,"In this paper, we explore self-supervised audio-visual models that learn from instructional videos.Prior work has shown that these models can relate spoken words and sounds to visual content after training on a large-scale dataset of videos, but they were only trained and evaluated on videos in English.To learn multilingual audio-visual representations, we propose a cascaded approach that leverages a model trained on English videos and applies it to audio-visual data in other languages, such as Japanese videos.With our cascaded approach, we show an improvement in retrieval performance of nearly 10x compared to training on the Japanese videos solely.We also apply the model trained on English videos to Japanese and Hindi spoken captions of images, achieving state-of-the-art performance.","['https://openalex.org/W2134670479', 'https://openalex.org/W2953114965', 'https://openalex.org/W2972345028', 'https://openalex.org/W3015300171', 'https://openalex.org/W2973132572', 'https://openalex.org/W2899274165', 'https://openalex.org/W2995680346', 'https://openalex.org/W2586148577', 'https://openalex.org/W2963330681', 'https://openalex.org/W3161204797', 'https://openalex.org/W2962934715', 'https://openalex.org/W4287591698', 'https://openalex.org/W3035276082', 'https://openalex.org/W2988907666', 'https://openalex.org/W4288595436', 'https://openalex.org/W3198429080', 'https://openalex.org/W2962978519', 'https://openalex.org/W2984008963', 'https://openalex.org/W3095881291', 'https://openalex.org/W2800448574', 'https://openalex.org/W2971709506', 'https://openalex.org/W2989322838', 'https://openalex.org/W2962753610', 'https://openalex.org/W2127982613', 'https://openalex.org/W2799316004', 'https://openalex.org/W2964099072', 'https://openalex.org/W2965147078', 'https://openalex.org/W4394453761', 'https://openalex.org/W2962862718', 'https://openalex.org/W1994606281', 'https://openalex.org/W4287240590', 'https://openalex.org/W2556930864', 'https://openalex.org/W2952132648', 'https://openalex.org/W2194775991', 'https://openalex.org/W3037309139', 'https://openalex.org/W3170972077', 'https://openalex.org/W3157861865', 'https://openalex.org/W2972073579', 'https://openalex.org/W3197828817', 'https://openalex.org/W4287199820', 'https://openalex.org/W3096372900', 'https://openalex.org/W2025198378', 'https://openalex.org/W2796315435']",2021-08-27
https://openalex.org/W3197064454,https://doi.org/10.21437/interspeech.2021-245,Spoken ObjectNet: A Bias-Controlled Spoken Caption Dataset,"Visually-grounded spoken language datasets can enable models to learn cross-modal correspondences with very weak supervision.However, modern audio-visual datasets contain biases that undermine the real-world performance of models trained on that data.We introduce Spoken ObjectNet, which is designed to remove some of these biases and provide a way to better evaluate how effectively models will perform in real-world scenarios.This dataset expands upon ObjectNet, which is a biascontrolled image dataset that features similar image classes to those present in ImageNet.We detail our data collection pipeline, which features several methods to improve caption quality, including automated language model checks.Lastly, we show baseline results on image retrieval and audio retrieval tasks.These results show that models trained on other datasets and then evaluated on Spoken ObjectNet tend to perform poorly due to biases in other datasets that the models have learned.We also show evidence that the performance decrease is due to the dataset controls, and not the transfer setting.","['https://openalex.org/W2988907666', 'https://openalex.org/W2586148577', 'https://openalex.org/W2971709506', 'https://openalex.org/W2593116425', 'https://openalex.org/W2962711930', 'https://openalex.org/W2970692043', 'https://openalex.org/W4239072543', 'https://openalex.org/W2796315435', 'https://openalex.org/W3015371781', 'https://openalex.org/W3095670406', 'https://openalex.org/W2134670479', 'https://openalex.org/W1522301498', 'https://openalex.org/W4288595436', 'https://openalex.org/W1889081078', 'https://openalex.org/W2553474785', 'https://openalex.org/W2899274165', 'https://openalex.org/W3170972077', 'https://openalex.org/W2980282514', 'https://openalex.org/W2556930864', 'https://openalex.org/W2963330681', 'https://openalex.org/W4287240590', 'https://openalex.org/W2984008963', 'https://openalex.org/W2137010615', 'https://openalex.org/W3161204797', 'https://openalex.org/W2119775030', 'https://openalex.org/W3015300171', 'https://openalex.org/W3197828817', 'https://openalex.org/W2965147078', 'https://openalex.org/W2995680346', 'https://openalex.org/W2962753610', 'https://openalex.org/W2953114965', 'https://openalex.org/W3161945002', 'https://openalex.org/W2736876693']",2021-08-27
https://openalex.org/W4372348373,https://doi.org/10.1109/icassp49357.2023.10096250,Continuous Action Space-Based Spoken Language Acquisition Agent Using Residual Sentence Embedding and Transformer Decoder,"Studies on spoken language acquisition agents aim to understand the mechanism of human language learning and to realize it on computers. Existing open vocabulary agents first perform unsupervised word learning from speech signals to construct a word dictionary as a discrete action space and then conduct reinforcement learning to understand the use of the words in the dictionary through interaction with dialogue partners. A limitation is that they have difficulty pronouncing multi-word utterances. This study proposes an agent that generates multi-word waveform utterances using a continuous action space. The conventional agent uses a vision-focusing mechanism to accelerate dialogue-based learning by guiding the agent’s attention to those concepts in its eyesight. In contrast, the proposed agent replaces it with residual sentence embedding combined with vision features used as the action space. The agent consists of speech and image input front-ends, a transformer language model of pseudo-action space. Experimental results show that the agent learns multi-word utterances assisted by unsupervised learning algorithms using unlabeled speech and image data sets.","['https://openalex.org/W98451546', 'https://openalex.org/W6679792166', 'https://openalex.org/W2142927041', 'https://openalex.org/W6782760101', 'https://openalex.org/W3016208931', 'https://openalex.org/W2407753819', 'https://openalex.org/W2978613765', 'https://openalex.org/W6640963894', 'https://openalex.org/W6768238010', 'https://openalex.org/W6783867762', 'https://openalex.org/W6753101812', 'https://openalex.org/W2964243274', 'https://openalex.org/W2020944885', 'https://openalex.org/W2483390977', 'https://openalex.org/W4285225028', 'https://openalex.org/W3096020106', 'https://openalex.org/W6739901393', 'https://openalex.org/W3174311593', 'https://openalex.org/W2145339207', 'https://openalex.org/W2108598243', 'https://openalex.org/W6784333009', 'https://openalex.org/W6684921986', 'https://openalex.org/W6729977899', 'https://openalex.org/W6735305794', 'https://openalex.org/W6770596778', 'https://openalex.org/W3159481202', 'https://openalex.org/W2194775991', 'https://openalex.org/W6780218876', 'https://openalex.org/W2019055305', 'https://openalex.org/W4253682481', 'https://openalex.org/W2112366626', 'https://openalex.org/W2766365052', 'https://openalex.org/W2038594453', 'https://openalex.org/W2963864421', 'https://openalex.org/W3092028330', 'https://openalex.org/W2810346659', 'https://openalex.org/W2964169922', 'https://openalex.org/W3123097577', 'https://openalex.org/W2998557583', 'https://openalex.org/W3036601975', 'https://openalex.org/W3094502228', 'https://openalex.org/W4385245566', 'https://openalex.org/W2519091744', 'https://openalex.org/W2995680346', 'https://openalex.org/W2556930864', 'https://openalex.org/W1959608418', 'https://openalex.org/W2134670479']",2023-05-05
https://openalex.org/W4392904409,https://doi.org/10.1109/icassp48485.2024.10446062,SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic Organization in Hubert,"Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space and speech units beyond phonemes are largely underexplored. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt ""self-distillation"" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames exhibit salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling.","['https://openalex.org/W3197580070', 'https://openalex.org/W4281492411', 'https://openalex.org/W6790356757', 'https://openalex.org/W3198217962', 'https://openalex.org/W4381786045', 'https://openalex.org/W6848735303', 'https://openalex.org/W3140429000', 'https://openalex.org/W4372348980', 'https://openalex.org/W4375868953', 'https://openalex.org/W4385823003', 'https://openalex.org/W3159481202', 'https://openalex.org/W2114347655', 'https://openalex.org/W1778492285', 'https://openalex.org/W2964169922', 'https://openalex.org/W2963720603', 'https://openalex.org/W3209059054', 'https://openalex.org/W3095361818', 'https://openalex.org/W2963902314', 'https://openalex.org/W6770596778', 'https://openalex.org/W4224875474', 'https://openalex.org/W4385823277', 'https://openalex.org/W6803675045', 'https://openalex.org/W2936774411', 'https://openalex.org/W1494198834', 'https://openalex.org/W6757817989', 'https://openalex.org/W2747874407', 'https://openalex.org/W2462305634', 'https://openalex.org/W3156636935', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209984917', 'https://openalex.org/W2250539671', 'https://openalex.org/W3036601975', 'https://openalex.org/W3213018012', 'https://openalex.org/W2908510526', 'https://openalex.org/W2995680346', 'https://openalex.org/W4394671563', 'https://openalex.org/W4313679638']",2024-03-18
https://openalex.org/W4287757663,https://doi.org/10.48550/arxiv.2006.08387,Catplayinginthesnow: Impact of Prior Segmentation on a Model of Visually\n Grounded Speech,"The language acquisition literature shows that children do not build their\nlexicon by segmenting the spoken input into phonemes and then building up words\nfrom them, but rather adopt a top-down approach and start by segmenting\nword-like units and then break them down into smaller units. This suggests that\nthe ideal way of learning a language is by starting from full semantic units.\nIn this paper, we investigate if this is also the case for a neural model of\nVisually Grounded Speech trained on a speech-image retrieval task. We evaluated\nhow well such a network is able to learn a reliable speech-to-image mapping\nwhen provided with phone, syllable, or word boundary information. We present a\nsimple way to introduce such information into an RNN-based model and\ninvestigate which type of boundary is the most efficient. We also explore at\nwhich level of the network's architecture such information should be introduced\nso as to maximise its performances. Finally, we show that using multiple\nboundary types at once in a hierarchical structure, by which low-level segments\nare used to recompose high-level segments, is beneficial and yields better\nresults than using low-level or high-level segments in isolation.\n","['https://openalex.org/W2971709506', 'https://openalex.org/W2972345028', 'https://openalex.org/W68733909', 'https://openalex.org/W2989358187', 'https://openalex.org/W3100113836', 'https://openalex.org/W3099142230', 'https://openalex.org/W2951974815', 'https://openalex.org/W2963330681', 'https://openalex.org/W646130444', 'https://openalex.org/W2556930864', 'https://openalex.org/W2137010615', 'https://openalex.org/W2157331557', 'https://openalex.org/W3213502289', 'https://openalex.org/W4237938692', 'https://openalex.org/W2149735195', 'https://openalex.org/W3015300171', 'https://openalex.org/W2586602577', 'https://openalex.org/W2964001192', 'https://openalex.org/W2157427027', 'https://openalex.org/W4289440907', 'https://openalex.org/W2114790177', 'https://openalex.org/W1861492603', 'https://openalex.org/W2038056950', 'https://openalex.org/W2920166246', 'https://openalex.org/W2995680346']",2020-06-15
https://openalex.org/W3035242764,https://doi.org/10.18653/v1/2020.conll-1.22,Catplayinginthesnow: Impact of Prior Segmentation on a Model of Visually Grounded Speech,"The language acquisition literature shows that children do not build their lexicon by segmenting the spoken input into phonemes and then building up words from them, but rather adopt a top-down approach and start by segmenting word-like units and then break them down into smaller units. This suggests that the ideal way of learning a language is by starting from full semantic units. In this paper, we investigate if this is also the case for a neural model of Visually Grounded Speech trained on a speech-image retrieval task. We evaluated how well such a network is able to learn a reliable speech-to-image mapping when provided with phone, syllable, or word boundary information. We present a simple way to introduce such information into an RNN-based model and investigate which type of boundary is the most efficient. We also explore at which level of the network's architecture such information should be introduced so as to maximise its performances. Finally, we show that using multiple boundary types at once in a hierarchical structure, by which low-level segments are used to recompose high-level segments, is beneficial and yields better results than using low-level or high-level segments in isolation.","['https://openalex.org/W3213502289', 'https://openalex.org/W2556930864', 'https://openalex.org/W2586148577', 'https://openalex.org/W2114790177', 'https://openalex.org/W2972345028', 'https://openalex.org/W2923959898', 'https://openalex.org/W68733909', 'https://openalex.org/W2149735195', 'https://openalex.org/W2896691342', 'https://openalex.org/W2951974815', 'https://openalex.org/W2964001192', 'https://openalex.org/W2995680346', 'https://openalex.org/W3015300171', 'https://openalex.org/W2989358187', 'https://openalex.org/W2107917162', 'https://openalex.org/W1861492603', 'https://openalex.org/W2963330681', 'https://openalex.org/W2971709506', 'https://openalex.org/W2920166246', 'https://openalex.org/W2157331557', 'https://openalex.org/W2962862718', 'https://openalex.org/W646130444', 'https://openalex.org/W2586602577', 'https://openalex.org/W2157427027']",2020-01-01
https://openalex.org/W3113159494,https://doi.org/10.21437/interspeech.2021-49,Direct Multimodal Few-Shot Learning of Speech and Images,"We propose direct multimodal few-shot models that learn a shared embedding space of spoken words and images from only a few paired examples. Imagine an agent is shown an image along with a spoken word describing the object in the picture, e.g. pen, book and eraser. After observing a few paired examples of each class, the model is asked to identify the ""book"" in a set of unseen pictures. Previous work used a two-step indirect approach relying on learned unimodal representations: speech-speech and image-image comparisons are performed across the support set of given speech-image pairs. We propose two direct models which instead learn a single multimodal space where inputs from different modalities are directly comparable: a multimodal triplet network (MTriplet) and a multimodal correspondence autoencoder (MCAE). To train these direct models, we mine speech-image pairs: the support set is used to pair up unlabelled in-domain speech and images. In a speech-to-image digit matching task, direct models outperform indirect models, with the MTriplet achieving the best multimodal five-shot accuracy. We show that the improvements are due to the combination of unsupervised and transfer learning in the direct models, and the absence of two-step compounding errors.","['https://openalex.org/W3095938409', 'https://openalex.org/W2964249784', 'https://openalex.org/W2165698076', 'https://openalex.org/W2250742840', 'https://openalex.org/W1964073652', 'https://openalex.org/W1545920196', 'https://openalex.org/W3046052470', 'https://openalex.org/W2184188583', 'https://openalex.org/W2556930864', 'https://openalex.org/W2124033848', 'https://openalex.org/W3131934726', 'https://openalex.org/W2971709506', 'https://openalex.org/W3110761489', 'https://openalex.org/W2102765684', 'https://openalex.org/W2123024445', 'https://openalex.org/W2995680346', 'https://openalex.org/W3035242764', 'https://openalex.org/W1975517671', 'https://openalex.org/W2408712009', 'https://openalex.org/W2127589108', 'https://openalex.org/W2802557066', 'https://openalex.org/W2132730112', 'https://openalex.org/W2963720603', 'https://openalex.org/W2112796928', 'https://openalex.org/W3099206234', 'https://openalex.org/W3120387732', 'https://openalex.org/W2114168642', 'https://openalex.org/W2013596317', 'https://openalex.org/W3036947827', 'https://openalex.org/W2145410271', 'https://openalex.org/W2156406284', 'https://openalex.org/W2586148577', 'https://openalex.org/W2963902314', 'https://openalex.org/W3091905774', 'https://openalex.org/W2115733720', 'https://openalex.org/W2010991551', 'https://openalex.org/W2979736514', 'https://openalex.org/W2964115348', 'https://openalex.org/W2598634450', 'https://openalex.org/W2194321275', 'https://openalex.org/W2963341924', 'https://openalex.org/W2963775347', 'https://openalex.org/W2962732076', 'https://openalex.org/W2100799972', 'https://openalex.org/W2291770225', 'https://openalex.org/W2979304729', 'https://openalex.org/W2964222437', 'https://openalex.org/W2162708558', 'https://openalex.org/W2767754137', 'https://openalex.org/W2950276680', 'https://openalex.org/W2889313720', 'https://openalex.org/W2415378728', 'https://openalex.org/W2964121744']",2021-08-27
https://openalex.org/W3163069788,https://doi.org/10.1109/icpr48806.2021.9412233,Unsupervised Co-Segmentation for Athlete Movements and Live Commentaries Using Crossmodal Temporal Proximity,"Audio-visual co-segmentation is a task to extract segments and regions corresponding to specific events on unlabeled audio and video signals. It is particularly important to accomplish it in an unsupervised way, since it is generally very difficult to manually label all the objects and events appearing in audio-visual signals for supervised learning. Here, we propose to take advantage of the temporal proximity of corresponding audio and video entities included in the signals. For this purpose, we newly employ a guided attention scheme to this task to efficiently detect and utilize temporal co-occurrences of audio and video information. Experiments using a real TV broadcasts of sumo wrestling, a sport event, with live commentaries show that our model can automatically extract specific athlete movements and its spoken descriptions in an unsupervised manner.","['https://openalex.org/W2895651543', 'https://openalex.org/W2964001192', 'https://openalex.org/W6759204928', 'https://openalex.org/W2957775769', 'https://openalex.org/W6747866816', 'https://openalex.org/W2964249784', 'https://openalex.org/W2962732076', 'https://openalex.org/W2920166246', 'https://openalex.org/W2586148577', 'https://openalex.org/W2962795934', 'https://openalex.org/W2425121537', 'https://openalex.org/W2964345931', 'https://openalex.org/W6729831399', 'https://openalex.org/W2808399042', 'https://openalex.org/W2885775891', 'https://openalex.org/W2984008963', 'https://openalex.org/W2972073579', 'https://openalex.org/W6772077125', 'https://openalex.org/W2988907666', 'https://openalex.org/W6631190155', 'https://openalex.org/W6750542987', 'https://openalex.org/W6761029826', 'https://openalex.org/W2962865004', 'https://openalex.org/W2963801643', 'https://openalex.org/W2938991416', 'https://openalex.org/W6750169759', 'https://openalex.org/W2981851635', 'https://openalex.org/W2962699416', 'https://openalex.org/W6729977899', 'https://openalex.org/W6750651883', 'https://openalex.org/W6770596778', 'https://openalex.org/W2767052532', 'https://openalex.org/W2899877258', 'https://openalex.org/W6725104640', 'https://openalex.org/W2963680395', 'https://openalex.org/W6750591037', 'https://openalex.org/W2963807156', 'https://openalex.org/W2963218389', 'https://openalex.org/W6766153511', 'https://openalex.org/W2963115079', 'https://openalex.org/W6765690770', 'https://openalex.org/W2964115348', 'https://openalex.org/W2963525826', 'https://openalex.org/W2962978519', 'https://openalex.org/W2972345028', 'https://openalex.org/W2963330681', 'https://openalex.org/W4206865574', 'https://openalex.org/W3015300171', 'https://openalex.org/W1522301498', 'https://openalex.org/W2972892814', 'https://openalex.org/W2556930864', 'https://openalex.org/W2619947201', 'https://openalex.org/W3009380496', 'https://openalex.org/W2963902314', 'https://openalex.org/W2965147078', 'https://openalex.org/W2965538726', 'https://openalex.org/W2995680346', 'https://openalex.org/W4293665662', 'https://openalex.org/W2899274165', 'https://openalex.org/W2965458216', 'https://openalex.org/W2796992393', 'https://openalex.org/W2914699769', 'https://openalex.org/W2611029872', 'https://openalex.org/W2952132648', 'https://openalex.org/W3099142230', 'https://openalex.org/W2799176631', 'https://openalex.org/W2964121744', 'https://openalex.org/W2784025607', 'https://openalex.org/W3197828817', 'https://openalex.org/W2962960500', 'https://openalex.org/W3123318516', 'https://openalex.org/W2619697695', 'https://openalex.org/W2964099072', 'https://openalex.org/W3034875620', 'https://openalex.org/W2962756039', 'https://openalex.org/W3035635319', 'https://openalex.org/W2511428026', 'https://openalex.org/W3176398504']",2021-01-10
https://openalex.org/W4214813806,https://doi.org/10.1109/ieeeconf53345.2021.9723367,A Translation Framework for Visually Grounded Spoken Unit Discovery,"Multimodal acoustic unit discovery (MAUD) is a key task in self-supervised spoken language learning and low-resource speech recognition. In this paper, we proposed two models for MAUD inspired by machine translation models where we treat speech and image as source and target languages. Our word discovery model outperforms previous state-of-the-art approach by 5.3% alignment F1 on SpeechCOCO dataset and our phoneme discovery model outperforms previous state-of-the-art approach by 7% normalized mutual information on TIMIT dataset.","['https://openalex.org/W2963620343', 'https://openalex.org/W6917638038', 'https://openalex.org/W1494198834', 'https://openalex.org/W6770596778', 'https://openalex.org/W2736876693', 'https://openalex.org/W6620707391', 'https://openalex.org/W6652311901', 'https://openalex.org/W7048738093', 'https://openalex.org/W3095293218', 'https://openalex.org/W3096656254', 'https://openalex.org/W2963902314', 'https://openalex.org/W6785385881', 'https://openalex.org/W3196459653', 'https://openalex.org/W6639102338', 'https://openalex.org/W2148876114', 'https://openalex.org/W2568262903', 'https://openalex.org/W6602812831', 'https://openalex.org/W2962862718', 'https://openalex.org/W2962753610', 'https://openalex.org/W6729977899', 'https://openalex.org/W6750194919', 'https://openalex.org/W2963966654', 'https://openalex.org/W2064675550', 'https://openalex.org/W2988907666', 'https://openalex.org/W3161204797', 'https://openalex.org/W6739901393', 'https://openalex.org/W6631190155', 'https://openalex.org/W6786696081', 'https://openalex.org/W2962832640', 'https://openalex.org/W1522301498', 'https://openalex.org/W2995680346', 'https://openalex.org/W4385245566', 'https://openalex.org/W1861492603', 'https://openalex.org/W4287591426', 'https://openalex.org/W2963330681', 'https://openalex.org/W2006969979', 'https://openalex.org/W639708223', 'https://openalex.org/W3161215977', 'https://openalex.org/W68733909', 'https://openalex.org/W2556930864']",2021-10-31
https://openalex.org/W4285258797,https://doi.org/10.18653/v1/2022.acl-long.553,Self-supervised Semantic-driven Phoneme Discovery for Zero-resource Speech Recognition,"Phonemes are defined by their relationship to words: changing a phoneme changes the word. Learning a phoneme inventory with little supervision has been a longstanding challenge with important applications to under-resourced speech technology. In this paper, we bridge the gap between the linguistic and statistical definition of phonemes and propose a novel neural discrete representation learning model for self-supervised learning of phoneme inventory with raw speech and word labels. Under mild assumptions, we prove that the phoneme inventory learned by our approach converges to the true one with an exponentially low error rate. Moreover, in experiments on TIMIT and Mboshi benchmarks, our approach consistently learns a better phoneme-level representation and achieves a lower error rate in a zero-resource phoneme recognition task than previous state-of-the-art self-supervised representation learning algorithms.","['https://openalex.org/W2979476256', 'https://openalex.org/W2100768664', 'https://openalex.org/W2797548502', 'https://openalex.org/W2295297373', 'https://openalex.org/W2116648050', 'https://openalex.org/W2118841860', 'https://openalex.org/W47568227', 'https://openalex.org/W1522301498', 'https://openalex.org/W2025722797', 'https://openalex.org/W2950414763', 'https://openalex.org/W2347098582', 'https://openalex.org/W68733909', 'https://openalex.org/W2963691697', 'https://openalex.org/W4297808394', 'https://openalex.org/W2995680346', 'https://openalex.org/W2787248994', 'https://openalex.org/W1967924372', 'https://openalex.org/W2934852845', 'https://openalex.org/W4287173589', 'https://openalex.org/W2164505566', 'https://openalex.org/W2964079874', 'https://openalex.org/W2187089797', 'https://openalex.org/W3098824274', 'https://openalex.org/W3125709657', 'https://openalex.org/W1494198834', 'https://openalex.org/W2120209245', 'https://openalex.org/W3095361818', 'https://openalex.org/W2963799213', 'https://openalex.org/W2973049979', 'https://openalex.org/W4236362309', 'https://openalex.org/W2478708596', 'https://openalex.org/W4293469690', 'https://openalex.org/W3036601975', 'https://openalex.org/W2137983211', 'https://openalex.org/W2972867623', 'https://openalex.org/W2897013370', 'https://openalex.org/W3096656254', 'https://openalex.org/W4287591426', 'https://openalex.org/W2950057603', 'https://openalex.org/W4300047444', 'https://openalex.org/W2547875792', 'https://openalex.org/W2137010615', 'https://openalex.org/W2927673779', 'https://openalex.org/W2762715843', 'https://openalex.org/W2927191280', 'https://openalex.org/W4287535613', 'https://openalex.org/W2972943112', 'https://openalex.org/W4287241729', 'https://openalex.org/W3095732712']",2022-01-01
https://openalex.org/W4205876710,https://doi.org/10.1109/o-cocosda202152914.2021.9660413,Self-Supervised Spoken Question Understanding and Speaking with Automatic Vocabulary Learning,"Spoken language acquisition involves automatically developing symbolic word concepts grounding their meaning to the world, recognizing the words in spoken utterances, and pronouncing them. Previous research only partly covered these aspects. One of the most comprehensive agent systems supported the word concept acquisition from pairs of raw speech and image and utterance pronunciation. However, the agent listened to nothing when interacting with the world, only pronouncing a food name to choose a favorite one among two shown images. In this work, we add a function to the agent to recognize a verbal question. Namely, we design a task where the agent must recognize a question in a sound utterance and understand the logical ""not"" concept. Experimental results show that the agent successfully learns the task. It appropriately behaves even for unseen combinations of images, correctly answering the food names it wants or the opposite one according to the question.","['https://openalex.org/W1771552368', 'https://openalex.org/W3016208931', 'https://openalex.org/W6678914141', 'https://openalex.org/W2194775991', 'https://openalex.org/W6766978945', 'https://openalex.org/W3096215352', 'https://openalex.org/W2962780374', 'https://openalex.org/W3096020106', 'https://openalex.org/W6637967152', 'https://openalex.org/W6770596778', 'https://openalex.org/W6770805772', 'https://openalex.org/W4251072654', 'https://openalex.org/W2107019937', 'https://openalex.org/W32403112', 'https://openalex.org/W2964169922', 'https://openalex.org/W2591896182', 'https://openalex.org/W4295312788', 'https://openalex.org/W2995680346', 'https://openalex.org/W2990408345', 'https://openalex.org/W4298857966', 'https://openalex.org/W2127218421']",2021-11-18
https://openalex.org/W4225297864,https://doi.org/10.1109/icassp43922.2022.9746436,Adversarial Input Ablation for Audio-Visual Learning,"We present an adversarial data augmentation strategy for speech spectrograms, within the context of training a model to semantically ground spoken audio captions to the images they describe. Our approach uses a two-pass strategy during training: first, a forward pass through the model is performed in order to identify segments of the input utterance that have the highest similarity score to their corresponding image. These segments are then ablated from the speech signal, producing a new and more challenging training example. Our experiments on the SpokenCOCO dataset demonstrate that when using this strategy: 1) content-carrying words tend to be ablated, forcing the model to focus on other regions of the speech; 2) the resulting model achieves improved speech-to-image retrieval accuracy; 3) the number of words that can be accurately detected by the model increases.","['https://openalex.org/W3157923770', 'https://openalex.org/W3157861865', 'https://openalex.org/W2963902314', 'https://openalex.org/W2953114965', 'https://openalex.org/W2964001192', 'https://openalex.org/W2920166246', 'https://openalex.org/W2972892814', 'https://openalex.org/W3099414366', 'https://openalex.org/W6797832685', 'https://openalex.org/W6769593479', 'https://openalex.org/W1861492603', 'https://openalex.org/W3003875258', 'https://openalex.org/W6780218876', 'https://openalex.org/W6755207826', 'https://openalex.org/W6729977899', 'https://openalex.org/W3209059054', 'https://openalex.org/W3016011332', 'https://openalex.org/W6786696081', 'https://openalex.org/W6844194202', 'https://openalex.org/W6770596778', 'https://openalex.org/W2963917611', 'https://openalex.org/W2936774411', 'https://openalex.org/W3174311593', 'https://openalex.org/W6750651883', 'https://openalex.org/W6631362777', 'https://openalex.org/W2988907666', 'https://openalex.org/W4287591426', 'https://openalex.org/W3197580070', 'https://openalex.org/W2896457183', 'https://openalex.org/W2995680346', 'https://openalex.org/W3036601975', 'https://openalex.org/W1524333225', 'https://openalex.org/W2982223350', 'https://openalex.org/W3177829661', 'https://openalex.org/W2556930864', 'https://openalex.org/W4297808394']",2022-04-27
https://openalex.org/W3112034174,https://doi.org/10.48550/arxiv.2012.06659,DeCoAR 2.0: Deep Contextualized Acoustic Representations with Vector Quantization,"Recent success in speech representation learning enables a new way to leverage unlabeled data to train speech recognition model. In speech representation learning, a large amount of unlabeled data is used in a self-supervised manner to learn a feature representation. Then a smaller amount of labeled data is used to train a downstream ASR system using the new feature representations. Based on our previous work DeCoAR and inspirations from other speech representation learning, we propose DeCoAR 2.0, a Deep Contextualized Acoustic Representation with vector quantization. We introduce several modifications over the DeCoAR: first, we use Transformers in encoding module instead of LSTMs; second, we introduce a vector quantization layer between encoder and reconstruction modules; third, we propose an objective that combines the reconstructive loss with vector quantization diversity loss to train speech representations. Our experiments show consistent improvements over other speech representations in different data-sparse scenarios. Without fine-tuning, a light-weight ASR model trained on 10 hours of LibriSpeech labeled data with DeCoAR 2.0 features outperforms the model trained on the full 960-hour dataset with filterbank features.","['https://openalex.org/W2766219058', 'https://openalex.org/W2972943112', 'https://openalex.org/W1821462560', 'https://openalex.org/W3096485810', 'https://openalex.org/W2995680346', 'https://openalex.org/W2981857663', 'https://openalex.org/W3035202887', 'https://openalex.org/W2991213871', 'https://openalex.org/W2982223350', 'https://openalex.org/W3099782249', 'https://openalex.org/W2970597249', 'https://openalex.org/W2963799213', 'https://openalex.org/W2110798204', 'https://openalex.org/W3097286738', 'https://openalex.org/W2996383576', 'https://openalex.org/W2963341956', 'https://openalex.org/W2512655038', 'https://openalex.org/W2962739339', 'https://openalex.org/W2973049979', 'https://openalex.org/W2981991061', 'https://openalex.org/W3036601975', 'https://openalex.org/W2842511635', 'https://openalex.org/W3015265920', 'https://openalex.org/W2124558353', 'https://openalex.org/W2941814890', 'https://openalex.org/W2184045248', 'https://openalex.org/W3007227084', 'https://openalex.org/W2327501763', 'https://openalex.org/W2963414781', 'https://openalex.org/W2963403868', 'https://openalex.org/W2547875792', 'https://openalex.org/W3015419784', 'https://openalex.org/W3025035610', 'https://openalex.org/W3041561163']",2020-12-11
https://openalex.org/W3032892481,https://doi.org/10.48550/arxiv.2006.02814,CSTNet: Contrastive Speech Translation Network for Self-Supervised Speech Representation Learning,"More than half of the 7,000 languages in the world are in imminent danger of going extinct. Traditional methods of documenting language proceed by collecting audio data followed by manual annotation by trained linguists at different levels of granularity. This time consuming and painstaking process could benefit from machine learning. Many endangered languages do not have any orthographic form but usually have speakers that are bi-lingual and trained in a high resource language. It is relatively easy to obtain textual translations corresponding to speech. In this work, we provide a multimodal machine learning framework for speech representation learning by exploiting the correlations between the two modalities namely speech and its corresponding text translation. Here, we construct a convolutional neural network audio encoder capable of extracting linguistic representations from speech. The audio encoder is trained to perform a speech-translation retrieval task in a contrastive learning framework. By evaluating the learned representations on a phone recognition task, we demonstrate that linguistic representations emerge in the audio encoder's internal representations as a by-product of learning to perform the retrieval task.","['https://openalex.org/W2964052309', 'https://openalex.org/W2750248772', 'https://openalex.org/W2347145335', 'https://openalex.org/W2963620343', 'https://openalex.org/W1836465849', 'https://openalex.org/W2793111190', 'https://openalex.org/W2346964103', 'https://openalex.org/W2100768664', 'https://openalex.org/W2995680346', 'https://openalex.org/W2808697642', 'https://openalex.org/W2950299304', 'https://openalex.org/W2949382160', 'https://openalex.org/W2973049979', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963618559', 'https://openalex.org/W2948211236', 'https://openalex.org/W2963902314', 'https://openalex.org/W2556930864', 'https://openalex.org/W2927673779', 'https://openalex.org/W2940544976', 'https://openalex.org/W3125709657', 'https://openalex.org/W2395899413', 'https://openalex.org/W2106053110', 'https://openalex.org/W2945700568', 'https://openalex.org/W2978709484', 'https://openalex.org/W2937090315', 'https://openalex.org/W2951004968', 'https://openalex.org/W2963300588', 'https://openalex.org/W2953331651', 'https://openalex.org/W2926827382', 'https://openalex.org/W2982223350', 'https://openalex.org/W2963702081', 'https://openalex.org/W2962990490', 'https://openalex.org/W2935542736', 'https://openalex.org/W2113896236', 'https://openalex.org/W2347098582']",2020-06-04
https://openalex.org/W3159476814,https://doi.org/10.1109/iscas51556.2021.9401692,\nLearning to Recognise Words Using Visually Grounded Speech,"We investigated word recognition in a Visually Grounded Speech model. The model has been trained on pairs of images and spoken captions to create visually grounded embeddings which can be used for speech to image retrieval and vice versa. We investigate whether such a model can be used to recognise words by embedding isolated words and using them to retrieve images of their visual referents. We investigate the time- course of word recognition using a gating paradigm and perform a statistical analysis to see whether well known word competition effects in human speech processing influence word recognition. Our experiments show that the model is able to recognise words, and the gating paradigm reveals that words can be recognised from partial input as well and that recognition is negatively influenced by word competition from the word initial cohort.","['https://openalex.org/W68733909', 'https://openalex.org/W1951724000', 'https://openalex.org/W2127802090', 'https://openalex.org/W1905882502', 'https://openalex.org/W2415378728', 'https://openalex.org/W1979656938', 'https://openalex.org/W2963902314', 'https://openalex.org/W2950133079', 'https://openalex.org/W2971709506', 'https://openalex.org/W3035750922', 'https://openalex.org/W2586148577', 'https://openalex.org/W3005578234', 'https://openalex.org/W2194775991', 'https://openalex.org/W2964099072', 'https://openalex.org/W2895651543', 'https://openalex.org/W2143417482', 'https://openalex.org/W2236615147', 'https://openalex.org/W817169058', 'https://openalex.org/W2760888555', 'https://openalex.org/W2989358187', 'https://openalex.org/W4213145363', 'https://openalex.org/W2556930864', 'https://openalex.org/W2127474935', 'https://openalex.org/W2043013057', 'https://openalex.org/W3099142230', 'https://openalex.org/W2995680346', 'https://openalex.org/W3102219307', 'https://openalex.org/W2962862718']",2021-01-01
https://openalex.org/W3143035657,https://doi.org/10.48550/arxiv.2104.01894,"Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval","Speech-based image retrieval has been studied as a proxy for joint representation learning, usually without emphasis on retrieval itself. As such, it is unclear how well speech-based retrieval can work in practice -- both in an absolute sense and versus alternative strategies that combine automatic speech recognition (ASR) with strong text encoders. In this work, we extensively study and expand choices of encoder architectures, training methodology (including unimodal and multimodal pretraining), and other factors. Our experiments cover different types of speech in three datasets: Flickr Audio, Places Audio, and Localized Narratives. Our best model configuration achieves large gains over state of the art, e.g., pushing recall-at-one from 21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio. We also show our best speech-based models can match or exceed cascaded ASR-to-text encoding when speech is spontaneous, accented, or otherwise hard to automatically transcribe.","['https://openalex.org/W3100813302', 'https://openalex.org/W2988823324', 'https://openalex.org/W1905882502', 'https://openalex.org/W2988907666', 'https://openalex.org/W2901894078', 'https://openalex.org/W2593116425', 'https://openalex.org/W2927673779', 'https://openalex.org/W2998649947', 'https://openalex.org/W2886641317', 'https://openalex.org/W2962862718', 'https://openalex.org/W3021471522', 'https://openalex.org/W2964099072', 'https://openalex.org/W2995680346', 'https://openalex.org/W3095670406', 'https://openalex.org/W3034875620', 'https://openalex.org/W1931639407', 'https://openalex.org/W2972345028', 'https://openalex.org/W3094626712', 'https://openalex.org/W2119775030', 'https://openalex.org/W385555557', 'https://openalex.org/W2586148577', 'https://openalex.org/W2971709506', 'https://openalex.org/W2955425717', 'https://openalex.org/W2923959898', 'https://openalex.org/W3197828817', 'https://openalex.org/W2989358187', 'https://openalex.org/W3034978746', 'https://openalex.org/W2963525826', 'https://openalex.org/W2531381952', 'https://openalex.org/W2963341956', 'https://openalex.org/W2964001192', 'https://openalex.org/W3015300171', 'https://openalex.org/W2963902314', 'https://openalex.org/W2962753610', 'https://openalex.org/W2973049979', 'https://openalex.org/W2556930864']",2021-04-05
https://openalex.org/W3177999760,https://doi.org/10.48550/arxiv.2107.07402,CLSRIL-23: Cross Lingual Speech Representations for Indic Languages,"We present a CLSRIL-23, a self supervised learning based audio pre-trained model which learns cross lingual speech representations from raw audio across 23 Indic languages. It is built on top of wav2vec 2.0 which is solved by training a contrastive task over masked latent speech representations and jointly learns the quantization of latents shared across all languages. We compare the language wise loss during pretraining to compare effects of monolingual and multilingual pretraining. Performance on some downstream fine-tuning tasks for speech recognition is also compared and our experiments show that multilingual pretraining outperforms monolingual training, in terms of learning speech representations which encodes phonetic similarity of languages and also in terms of performance on down stream tasks. A decrease of 5% is observed in WER and 9.5% in CER when a multilingual pretrained model is used for finetuning in Hindi. All the code models are also open sourced. CLSRIL-23 is a model trained on $23$ languages and almost 10,000 hours of audio data to facilitate research in speech recognition for Indic languages. We hope that new state of the art systems will be created using the self supervised approach, especially for low resources Indic languages.","['https://openalex.org/W3037057938', 'https://openalex.org/W3016181583', 'https://openalex.org/W2995680346', 'https://openalex.org/W1574170747', 'https://openalex.org/W3099782249', 'https://openalex.org/W2091746061', 'https://openalex.org/W2547875792', 'https://openalex.org/W2127141656', 'https://openalex.org/W2160815625', 'https://openalex.org/W2134800885', 'https://openalex.org/W3036601975', 'https://openalex.org/W2963425185', 'https://openalex.org/W2962788625', 'https://openalex.org/W2787560479', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963341956']",2021-07-15
https://openalex.org/W3167119498,https://doi.org/10.21437/interspeech.2021-435,Attention-Based Keyword Localisation in Speech Using Visual Grounding,"Visually grounded speech models learn from images paired with spoken captions. By tagging images with soft text labels using a trained visual classifier with a fixed vocabulary, previous work has shown that it is possible to train a model that can detect whether a particular text keyword occurs in speech utterances or not. Here we investigate whether visually grounded speech models can also do keyword localisation: predicting where, within an utterance, a given textual keyword occurs without any explicit text-based or alignment supervision. We specifically consider whether incorporating attention into a convolutional model is beneficial for localisation. Although absolute localisation performance with visually supervised models is still modest (compared to using unordered bag-of-word text labels for supervision), we show that attention provides a large gain in performance over previous visually grounded models. As in many other speech-image studies, we find that many of the incorrect localisations are due to semantic confusions, e.g. locating the word 'backstroke' for the query keyword 'swimming'.","['https://openalex.org/W385555557', 'https://openalex.org/W2995680346', 'https://openalex.org/W2920166246', 'https://openalex.org/W2950133079', 'https://openalex.org/W2125165590', 'https://openalex.org/W2108598243', 'https://openalex.org/W2964099072', 'https://openalex.org/W1686810756', 'https://openalex.org/W2327501763', 'https://openalex.org/W2973135958', 'https://openalex.org/W2963330681', 'https://openalex.org/W2586148577', 'https://openalex.org/W2140916511', 'https://openalex.org/W2466918907', 'https://openalex.org/W3016087077', 'https://openalex.org/W2163605009', 'https://openalex.org/W1902237438', 'https://openalex.org/W3095938409', 'https://openalex.org/W2927673779', 'https://openalex.org/W2102605133', 'https://openalex.org/W2964121744', 'https://openalex.org/W3111013239', 'https://openalex.org/W2962753610', 'https://openalex.org/W2964249784', 'https://openalex.org/W2127716180', 'https://openalex.org/W1987959440', 'https://openalex.org/W1993750641', 'https://openalex.org/W2962862718', 'https://openalex.org/W2104247845', 'https://openalex.org/W2605131327', 'https://openalex.org/W2508907749', 'https://openalex.org/W2963902314', 'https://openalex.org/W2531381952', 'https://openalex.org/W2132921748', 'https://openalex.org/W2950527759', 'https://openalex.org/W2962858109', 'https://openalex.org/W2032943813', 'https://openalex.org/W2964001192', 'https://openalex.org/W2556930864']",2021-08-27
https://openalex.org/W4387445713,https://doi.org/10.1109/icivc58118.2023.10270664,SAliCLIP: Generating Speech-Conditioned Images by Aligning Speech With CLIP Latents,"The advent of the Transformer architecture, followed by the development of contrastive models like OpenAI's CLIP, has enabled learning robust latent space representations of data in multiple modalities such as text and image. In this work, we present Speech-Aligned CLIP (SAliCLIP), developed by employing knowledge distillation to adapt an audio encoder to the embedding space of CLIP. Aligning the embedding spaces of image, speech, and text imparts additional functionality to CLIP, which can be leveraged for various downstream tasks. We utilize this framework of encoders to guide the images generated by a GAN-based network to represent any given speech signal. The proposed architecture achieves new state-of-the-art cross-modal retrieval and classification accuracies on the Spoken-COCO dataset, the Spoken ObjectNet dataset and the Flickr Audio Captions Corpus dataset which includes zero-shot learning and fine-tuning on these datasets.","['https://openalex.org/W2102166818', 'https://openalex.org/W4281492411', 'https://openalex.org/W6739901393', 'https://openalex.org/W3114632476', 'https://openalex.org/W1982471090', 'https://openalex.org/W2752796333', 'https://openalex.org/W6729831399', 'https://openalex.org/W6729977899', 'https://openalex.org/W4293665662', 'https://openalex.org/W3196974791', 'https://openalex.org/W2556930864', 'https://openalex.org/W3174311593', 'https://openalex.org/W2511428026', 'https://openalex.org/W2970692043', 'https://openalex.org/W1861492603', 'https://openalex.org/W3094502228', 'https://openalex.org/W2951611190', 'https://openalex.org/W2965147078', 'https://openalex.org/W3166396011', 'https://openalex.org/W2137010615', 'https://openalex.org/W2119775030', 'https://openalex.org/W3210447534', 'https://openalex.org/W4312282373', 'https://openalex.org/W3197064454', 'https://openalex.org/W4320013936', 'https://openalex.org/W4287240590', 'https://openalex.org/W2963902314', 'https://openalex.org/W2971709506', 'https://openalex.org/W2586148577', 'https://openalex.org/W2593116425', 'https://openalex.org/W3197828817', 'https://openalex.org/W2963799213', 'https://openalex.org/W4385245566', 'https://openalex.org/W2964345931', 'https://openalex.org/W3176445421', 'https://openalex.org/W2108598243', 'https://openalex.org/W2995680346', 'https://openalex.org/W3180355996', 'https://openalex.org/W3161204797']",2023-07-27
https://openalex.org/W4400880465,https://doi.org/10.22201/icat.24486736e.2024.22.3.2417,Cross-modal learning representation using new margin combination for speech recognition task,"Cross-modal retrieval aims to elucidate the fusion of information, mimic human learning, and advance the field. The main challenge in cross-modal matching is to build a shared subspace reflecting semantic proximity. Previous works fail to capture asymmetric relevance by adopting symmetric similarity computations. To overcome these shortcomings, an efficient approach called quaternion representation learning (QRL) is introduced for better cross-modal matching. Thus, a better representation of the shared semantics is offered by virtue of its richer representation capacity of the quaternionic space and its strong expressive power. Transfer learning is a crucial aspect in this context. By leveraging pre-trained models, the knowledge gained from one task or domain can be effectively transferred to another, allowing for improved performance and generalization. In this study, transfer learning is employed to enhance the cross-modal retrieval system. Specifically, a pre-trained ResNet-512 model is utilized in conjunction with the proposed total margin (TM) loss function, which combines the QRL approach with the novel adaptive mean margin (AMM) methodology. The TM loss function, coupled with the pre-trained ResNet-512 model, is evaluated on the Audio-Visual Arabic Speech Database (AVAS) and the Arabic Visual Speech Database (AVSD), along with other audio-visual datasets. Experimental results demonstrate the effectiveness of the TM loss function in consistently improving performance on both databases. The recall scores (R@k) and mean average precision (mAP) values achieved on the AVAS Database are as follows: R@1: 42.1±0.7, R@2: 70.2±0.1, R@5: 78.5±1.0, and mAP: 53.0±1.1. Similarly, on the AVSD Database, the results are R@1: 41.7±0.3, R@2: 69.2±1.1, R@5: 78.0±0.3, and mAP: 52.7±0.5. By incorporating transfer learning and the TM loss function into the cross-modal retrieval framework, this study demonstrates the potential for improving clustering efficiency and enhancing visual and speech understanding. The combination of pre-trained models and the TM loss function offers a promising avenue for advancing crossmodal matching techniques and achieving state-of-the-art performance.","['https://openalex.org/W3015312544', 'https://openalex.org/W2971074500', 'https://openalex.org/W2901145677', 'https://openalex.org/W3120074043', 'https://openalex.org/W3005680577', 'https://openalex.org/W3205715971', 'https://openalex.org/W1531883353', 'https://openalex.org/W3187443122', 'https://openalex.org/W3180327660', 'https://openalex.org/W4226391640', 'https://openalex.org/W3014475539', 'https://openalex.org/W4287018780', 'https://openalex.org/W4321439619', 'https://openalex.org/W3188692592', 'https://openalex.org/W4221161768', 'https://openalex.org/W3134006559', 'https://openalex.org/W2982078236', 'https://openalex.org/W3169472988', 'https://openalex.org/W4226033575', 'https://openalex.org/W4286794082', 'https://openalex.org/W1974783905', 'https://openalex.org/W2984008963', 'https://openalex.org/W2778100917', 'https://openalex.org/W2130942839', 'https://openalex.org/W4286797312', 'https://openalex.org/W2956018683', 'https://openalex.org/W2891205112', 'https://openalex.org/W3006119204', 'https://openalex.org/W3167886702', 'https://openalex.org/W3205488134', 'https://openalex.org/W3038716110', 'https://openalex.org/W4388692863', 'https://openalex.org/W3168433561', 'https://openalex.org/W4297808394', 'https://openalex.org/W2096391593', 'https://openalex.org/W3127988379', 'https://openalex.org/W4287240590', 'https://openalex.org/W3002094346', 'https://openalex.org/W4221153068', 'https://openalex.org/W2533523411', 'https://openalex.org/W3095293218', 'https://openalex.org/W2964171275', 'https://openalex.org/W4221161145', 'https://openalex.org/W4386076222', 'https://openalex.org/W2805730776', 'https://openalex.org/W2995680346', 'https://openalex.org/W3170972077', 'https://openalex.org/W4385337322', 'https://openalex.org/W2979476256', 'https://openalex.org/W4226226726', 'https://openalex.org/W3035042697', 'https://openalex.org/W4287902615', 'https://openalex.org/W3003990132', 'https://openalex.org/W3187541079', 'https://openalex.org/W4322588738', 'https://openalex.org/W4391407478', 'https://openalex.org/W2962802184', 'https://openalex.org/W2594690981', 'https://openalex.org/W3134451506', 'https://openalex.org/W3131251978', 'https://openalex.org/W2954058703', 'https://openalex.org/W4394641246', 'https://openalex.org/W4225166170', 'https://openalex.org/W4385822729', 'https://openalex.org/W2070246124', 'https://openalex.org/W3200287550', 'https://openalex.org/W2604230289', 'https://openalex.org/W2964266061', 'https://openalex.org/W2990152177', 'https://openalex.org/W3099206234', 'https://openalex.org/W4287631799', 'https://openalex.org/W3207409917', 'https://openalex.org/W2138621090', 'https://openalex.org/W3111882316', 'https://openalex.org/W2055828689', 'https://openalex.org/W2988907666', 'https://openalex.org/W4391770236', 'https://openalex.org/W4281492411', 'https://openalex.org/W3016181583', 'https://openalex.org/W2963074118', 'https://openalex.org/W4293495374', 'https://openalex.org/W3207222250', 'https://openalex.org/W2891267443', 'https://openalex.org/W3119510203', 'https://openalex.org/W4285118104', 'https://openalex.org/W3087110941', 'https://openalex.org/W3102342027', 'https://openalex.org/W4287645148', 'https://openalex.org/W2296654356', 'https://openalex.org/W2963330681']",2024-06-28
https://openalex.org/W3080764280,https://doi.org/10.1109/access.2020.3018151,Variations in Variational Autoencoders - A Comparative Evaluation,"Variational Auto-Encoders (VAEs) are deep latent space generative models which have been immensely successful in many applications such as image generation, image captioning, protein design, mutation prediction, and language models among others. The fundamental idea in VAEs is to learn the distribution of data in such a way that new meaningful data can be generated from the encoded distribution. This concept has led to tremendous research and variations in the design of VAEs in the last few years creating a field of its own, referred to as unsupervised representation learning. This paper provides a much-needed comprehensive evaluation of the variations of the VAEs based on their end goals and resulting architectures. It further provides intuition as well as mathematical formulation and quantitative results of each popular variation, presents a concise comparison of these variations, and concludes with challenges and future opportunities for research in VAEs.","['https://openalex.org/W6681096077', 'https://openalex.org/W2962935987', 'https://openalex.org/W6738560588', 'https://openalex.org/W2884001105', 'https://openalex.org/W6729831399', 'https://openalex.org/W2964012239', 'https://openalex.org/W6748391871', 'https://openalex.org/W6744627333', 'https://openalex.org/W6757140150', 'https://openalex.org/W6688325169', 'https://openalex.org/W2002454301', 'https://openalex.org/W1608549042', 'https://openalex.org/W2327501763', 'https://openalex.org/W2030760474', 'https://openalex.org/W2100495367', 'https://openalex.org/W6685352114', 'https://openalex.org/W2794284562', 'https://openalex.org/W6607775107', 'https://openalex.org/W6771719626', 'https://openalex.org/W6639732818', 'https://openalex.org/W6610566761', 'https://openalex.org/W2140405352', 'https://openalex.org/W2292917848', 'https://openalex.org/W6739625446', 'https://openalex.org/W6687506355', 'https://openalex.org/W6730333270', 'https://openalex.org/W2884851420', 'https://openalex.org/W2999248185', 'https://openalex.org/W4299345493', 'https://openalex.org/W6740364378', 'https://openalex.org/W6685380521', 'https://openalex.org/W6718140377', 'https://openalex.org/W6720208624', 'https://openalex.org/W6757185480', 'https://openalex.org/W6732248266', 'https://openalex.org/W2470142083', 'https://openalex.org/W6668990524', 'https://openalex.org/W2163922914', 'https://openalex.org/W2963538198', 'https://openalex.org/W3100270690', 'https://openalex.org/W2752796333', 'https://openalex.org/W6637131181', 'https://openalex.org/W6753018729', 'https://openalex.org/W6677929280', 'https://openalex.org/W6730200498', 'https://openalex.org/W6695087160', 'https://openalex.org/W6696469593', 'https://openalex.org/W1522734439', 'https://openalex.org/W2963446712', 'https://openalex.org/W2194775991', 'https://openalex.org/W2097117768', 'https://openalex.org/W2395611524', 'https://openalex.org/W2102605133', 'https://openalex.org/W2963470893', 'https://openalex.org/W1895577753', 'https://openalex.org/W2109317801', 'https://openalex.org/W2611632661', 'https://openalex.org/W1977001422', 'https://openalex.org/W2575671312', 'https://openalex.org/W3023371261', 'https://openalex.org/W6753000030', 'https://openalex.org/W6715501732', 'https://openalex.org/W2563705555', 'https://openalex.org/W2963420272', 'https://openalex.org/W2924485953', 'https://openalex.org/W6684191040', 'https://openalex.org/W6640963894', 'https://openalex.org/W6637373629', 'https://openalex.org/W1901616594', 'https://openalex.org/W6756663807', 'https://openalex.org/W6749820799', 'https://openalex.org/W6640170880', 'https://openalex.org/W6738819434', 'https://openalex.org/W6748713143', 'https://openalex.org/W6777915622', 'https://openalex.org/W6730306933', 'https://openalex.org/W6687045409', 'https://openalex.org/W6759252000', 'https://openalex.org/W2963283377', 'https://openalex.org/W6746240808', 'https://openalex.org/W2537458122', 'https://openalex.org/W6687214662', 'https://openalex.org/W2963917315', 'https://openalex.org/W6752571885', 'https://openalex.org/W6780248173', 'https://openalex.org/W2981091784', 'https://openalex.org/W2283340597', 'https://openalex.org/W2552383788', 'https://openalex.org/W1576462183', 'https://openalex.org/W3046698617', 'https://openalex.org/W6717697761', 'https://openalex.org/W652269744', 'https://openalex.org/W2154468968', 'https://openalex.org/W6743661861', 'https://openalex.org/W2796346823', 'https://openalex.org/W2924476266', 'https://openalex.org/W3000538487', 'https://openalex.org/W2556467266', 'https://openalex.org/W4294568686', 'https://openalex.org/W1673310716', 'https://openalex.org/W2963226019', 'https://openalex.org/W4293568373', 'https://openalex.org/W2995875017', 'https://openalex.org/W2622563070', 'https://openalex.org/W2753738274', 'https://openalex.org/W2145094598', 'https://openalex.org/W2601450892', 'https://openalex.org/W2753160622', 'https://openalex.org/W1959608418', 'https://openalex.org/W2964248207', 'https://openalex.org/W2952264928', 'https://openalex.org/W2964167449', 'https://openalex.org/W2963341924', 'https://openalex.org/W4294410884', 'https://openalex.org/W2962942158', 'https://openalex.org/W1686810756', 'https://openalex.org/W2625715238', 'https://openalex.org/W3034603995', 'https://openalex.org/W2963864522', 'https://openalex.org/W2624918875', 'https://openalex.org/W2919115771', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963351648', 'https://openalex.org/W1909320841', 'https://openalex.org/W189596042', 'https://openalex.org/W2963090522', 'https://openalex.org/W2903538854', 'https://openalex.org/W2963684088', 'https://openalex.org/W2289330286', 'https://openalex.org/W2811079561', 'https://openalex.org/W2163605009', 'https://openalex.org/W2173520492', 'https://openalex.org/W2904312275', 'https://openalex.org/W2950180292', 'https://openalex.org/W2950662112', 'https://openalex.org/W2950560720', 'https://openalex.org/W2073459066', 'https://openalex.org/W2964074409', 'https://openalex.org/W2904849495', 'https://openalex.org/W1903029394', 'https://openalex.org/W4297940714', 'https://openalex.org/W4293577678', 'https://openalex.org/W4288562426', 'https://openalex.org/W2962849408', 'https://openalex.org/W4300222729', 'https://openalex.org/W4293665662', 'https://openalex.org/W2808007053', 'https://openalex.org/W2997574889', 'https://openalex.org/W1923243475', 'https://openalex.org/W2188365844', 'https://openalex.org/W2467604901', 'https://openalex.org/W2964127395', 'https://openalex.org/W2796649821', 'https://openalex.org/W2577946330', 'https://openalex.org/W2730106296', 'https://openalex.org/W4320013936', 'https://openalex.org/W4214958790', 'https://openalex.org/W2769182847', 'https://openalex.org/W2883725317']",2020-01-01
https://openalex.org/W3022195800,https://doi.org/10.1109/jstsp.2020.3037506,GACELA: A Generative Adversarial Context Encoder for Long Audio Inpainting of Music,"We introduce GACELA, a generative adversarial network (GAN) designed to restore missing musical audio data with a duration ranging between hundreds of milliseconds to a few seconds, i.e., to perform long-gap audio inpainting. While previous work either addressed shorter gaps or relied on exemplars by copying available information from other signal parts, GACELA addresses the inpainting of long gaps in two aspects. First, it considers various time scales of audio information by relying on five parallel discriminators with increasing resolution of receptive fields. Second, it is conditioned not only on the available information surrounding the gap, i.e., the context, but also on the latent variable of the conditional GAN. This addresses the inherent multi-modality of audio inpainting at such long gaps and provides the option of user-defined inpainting. GACELA was tested in listening tests on music signals of varying complexity and gap durations ranging from 375~ms to 1500~ms. While our subjects were often able to detect the inpaintings, the severity of the artifacts decreased from unacceptable to mildly disturbing. GACELA represents a framework capable to integrate future improvements such as processing of more auditory-related features or more explicit musical features.","['https://openalex.org/W1605417594', 'https://openalex.org/W6687152286', 'https://openalex.org/W2963800363', 'https://openalex.org/W2120847449', 'https://openalex.org/W6736723571', 'https://openalex.org/W2981283774', 'https://openalex.org/W6721020019', 'https://openalex.org/W2771705696', 'https://openalex.org/W2161304220', 'https://openalex.org/W6720661750', 'https://openalex.org/W2981905048', 'https://openalex.org/W6638849899', 'https://openalex.org/W6635978374', 'https://openalex.org/W6694576955', 'https://openalex.org/W1819710477', 'https://openalex.org/W6754333143', 'https://openalex.org/W2516594951', 'https://openalex.org/W6794920904', 'https://openalex.org/W2610073446', 'https://openalex.org/W2404728592', 'https://openalex.org/W2152859600', 'https://openalex.org/W2741913599', 'https://openalex.org/W2112464782', 'https://openalex.org/W6766978945', 'https://openalex.org/W6632548451', 'https://openalex.org/W2296153915', 'https://openalex.org/W2110027626', 'https://openalex.org/W1970497840', 'https://openalex.org/W2015471578', 'https://openalex.org/W2891148495', 'https://openalex.org/W2155731523', 'https://openalex.org/W2172909489', 'https://openalex.org/W2401922685', 'https://openalex.org/W2890983311', 'https://openalex.org/W2964243274', 'https://openalex.org/W6765362075', 'https://openalex.org/W2962883485', 'https://openalex.org/W2758804652', 'https://openalex.org/W6763100362', 'https://openalex.org/W6758708508', 'https://openalex.org/W6758675244', 'https://openalex.org/W6757028937', 'https://openalex.org/W6748655329', 'https://openalex.org/W6741832134', 'https://openalex.org/W6753018729', 'https://openalex.org/W2963420272', 'https://openalex.org/W2097479740', 'https://openalex.org/W2786105342', 'https://openalex.org/W2222562092', 'https://openalex.org/W2624890766', 'https://openalex.org/W2112390905', 'https://openalex.org/W2963073614', 'https://openalex.org/W2884436603', 'https://openalex.org/W6691096134', 'https://openalex.org/W2999782137', 'https://openalex.org/W6732646663', 'https://openalex.org/W2118763712', 'https://openalex.org/W2131686285', 'https://openalex.org/W2963276790', 'https://openalex.org/W2899000566', 'https://openalex.org/W4234482113', 'https://openalex.org/W2784225997', 'https://openalex.org/W2898371500', 'https://openalex.org/W2807065297', 'https://openalex.org/W6713645886', 'https://openalex.org/W2059652044', 'https://openalex.org/W6748409065', 'https://openalex.org/W6802658689', 'https://openalex.org/W6745878906', 'https://openalex.org/W2963300588', 'https://openalex.org/W6732429163', 'https://openalex.org/W2975414524', 'https://openalex.org/W6767111847', 'https://openalex.org/W3105000799', 'https://openalex.org/W2405756170', 'https://openalex.org/W2985518820', 'https://openalex.org/W1832373861', 'https://openalex.org/W2584032004', 'https://openalex.org/W4295312788', 'https://openalex.org/W1604034532', 'https://openalex.org/W2739748921', 'https://openalex.org/W2963125871', 'https://openalex.org/W2475687244', 'https://openalex.org/W3103301587', 'https://openalex.org/W2970971581', 'https://openalex.org/W2963358591', 'https://openalex.org/W2887720658', 'https://openalex.org/W2962754210', 'https://openalex.org/W3206935648', 'https://openalex.org/W2951535099', 'https://openalex.org/W2962968839', 'https://openalex.org/W4297772798', 'https://openalex.org/W2811079561', 'https://openalex.org/W2972919410', 'https://openalex.org/W4289362584', 'https://openalex.org/W3105183525', 'https://openalex.org/W2963889406', 'https://openalex.org/W3122518304', 'https://openalex.org/W2787579267', 'https://openalex.org/W4292170079', 'https://openalex.org/W2989650386', 'https://openalex.org/W4297772864', 'https://openalex.org/W282666689', 'https://openalex.org/W2948211236', 'https://openalex.org/W2606176153', 'https://openalex.org/W2962942158', 'https://openalex.org/W2276883112', 'https://openalex.org/W2191779130', 'https://openalex.org/W2970006822', 'https://openalex.org/W2160719354', 'https://openalex.org/W4288286335', 'https://openalex.org/W2996286887', 'https://openalex.org/W2519091744', 'https://openalex.org/W4288594364', 'https://openalex.org/W2580221632', 'https://openalex.org/W2119337797', 'https://openalex.org/W1597922189', 'https://openalex.org/W2099471712', 'https://openalex.org/W2767858146', 'https://openalex.org/W1556219185', 'https://openalex.org/W2963237661', 'https://openalex.org/W2962981281', 'https://openalex.org/W3159973907', 'https://openalex.org/W2910577860', 'https://openalex.org/W4298580827', 'https://openalex.org/W2964307104', 'https://openalex.org/W2898148140', 'https://openalex.org/W4320013936', 'https://openalex.org/W1543786473', 'https://openalex.org/W2962813390', 'https://openalex.org/W1481866092']",2020-11-11
https://openalex.org/W3041738652,https://doi.org/10.1109/taslp.2021.3061245,Quasi-Periodic WaveNet: An Autoregressive Raw Waveform Generative Model With Pitch-Dependent Dilated Convolution Neural Network,"In this paper, a pitch-adaptive waveform generative model named\nQuasi-Periodic WaveNet (QPNet) is proposed to improve the limited pitch\ncontrollability of vanilla WaveNet (WN) using pitch-dependent dilated\nconvolution neural networks (PDCNNs). Specifically, as a probabilistic\nautoregressive generation model with stacked dilated convolution layers, WN\nachieves high-fidelity audio waveform generation. However, the pure-data-driven\nnature and the lack of prior knowledge of audio signals degrade the pitch\ncontrollability of WN. For instance, it is difficult for WN to precisely\ngenerate the periodic components of audio signals when the given auxiliary\nfundamental frequency ($F_{0}$) features are outside the $F_{0}$ range observed\nin the training data. To address this problem, QPNet with two novel designs is\nproposed. First, the PDCNN component is applied to dynamically change the\nnetwork architecture of WN according to the given auxiliary $F_{0}$ features.\nSecond, a cascaded network structure is utilized to simultaneously model the\nlong- and short-term dependencies of quasi-periodic signals such as speech. The\nperformances of single-tone sinusoid and speech generations are evaluated. The\nexperimental results show the effectiveness of the PDCNNs for unseen auxiliary\n$F_{0}$ features and the effectiveness of the cascaded structure for speech\ngeneration.\n","['https://openalex.org/W2097645910', 'https://openalex.org/W2972939746', 'https://openalex.org/W2797310469', 'https://openalex.org/W2786868129', 'https://openalex.org/W2749651610', 'https://openalex.org/W6696085341', 'https://openalex.org/W2973115941', 'https://openalex.org/W2151626637', 'https://openalex.org/W2099856211', 'https://openalex.org/W2803714261', 'https://openalex.org/W2963411216', 'https://openalex.org/W2802935216', 'https://openalex.org/W3014786045', 'https://openalex.org/W1969526148', 'https://openalex.org/W4245284779', 'https://openalex.org/W6749489859', 'https://openalex.org/W2775336875', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963103134', 'https://openalex.org/W2747161606', 'https://openalex.org/W2802455234', 'https://openalex.org/W2746474733', 'https://openalex.org/W2811001736', 'https://openalex.org/W2963035245', 'https://openalex.org/W4252713891', 'https://openalex.org/W6631190155', 'https://openalex.org/W6637242042', 'https://openalex.org/W6748409065', 'https://openalex.org/W2963091184', 'https://openalex.org/W2150418026', 'https://openalex.org/W6843673214', 'https://openalex.org/W6753855596', 'https://openalex.org/W2963300588', 'https://openalex.org/W6756159577', 'https://openalex.org/W2963175743', 'https://openalex.org/W2990440871', 'https://openalex.org/W2972504514', 'https://openalex.org/W6753018729', 'https://openalex.org/W2164764235', 'https://openalex.org/W4235716345', 'https://openalex.org/W2471520273', 'https://openalex.org/W2049686551', 'https://openalex.org/W6732429163', 'https://openalex.org/W6733471323', 'https://openalex.org/W2890983311', 'https://openalex.org/W2963589210', 'https://openalex.org/W2889044226', 'https://openalex.org/W6610566761', 'https://openalex.org/W6635084905', 'https://openalex.org/W2018458134', 'https://openalex.org/W2154291215', 'https://openalex.org/W2803595463', 'https://openalex.org/W2012086895', 'https://openalex.org/W4289761690', 'https://openalex.org/W2135479785', 'https://openalex.org/W2963975282', 'https://openalex.org/W2587284713', 'https://openalex.org/W2556247670', 'https://openalex.org/W2963139417', 'https://openalex.org/W4320013936', 'https://openalex.org/W2962882868', 'https://openalex.org/W2584032004', 'https://openalex.org/W2963840672', 'https://openalex.org/W2811079561', 'https://openalex.org/W2955616469', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963090522', 'https://openalex.org/W2800289214', 'https://openalex.org/W2099471712', 'https://openalex.org/W4298580827', 'https://openalex.org/W2963782041', 'https://openalex.org/W2962942158', 'https://openalex.org/W4238674002', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963691546', 'https://openalex.org/W1522301498', 'https://openalex.org/W4294619240', 'https://openalex.org/W1665214252', 'https://openalex.org/W1583912456', 'https://openalex.org/W4289305009', 'https://openalex.org/W2964307104']",2021-01-01
https://openalex.org/W3127854286,https://doi.org/10.1109/icassp39728.2021.9414235,Self-Supervised VQ-VAE for One-Shot Music Style Transfer,"Neural style transfer, allowing to apply the artistic style of one image to\nanother, has become one of the most widely showcased computer vision\napplications shortly after its introduction. In contrast, related tasks in the\nmusic audio domain remained, until recently, largely untackled. While several\nstyle conversion methods tailored to musical signals have been proposed, most\nlack the 'one-shot' capability of classical image style transfer algorithms. On\nthe other hand, the results of existing one-shot audio style transfer methods\non musical inputs are not as compelling. In this work, we are specifically\ninterested in the problem of one-shot timbre transfer. We present a novel\nmethod for this task, based on an extension of the vector-quantized variational\nautoencoder (VQ-VAE), along with a simple self-supervised learning strategy\ndesigned to obtain disentangled representations of timbre and pitch. We\nevaluate the method using a set of objective metrics and show that it is able\nto outperform selected baselines.\n","['https://openalex.org/W6753018729', 'https://openalex.org/W6600110733', 'https://openalex.org/W1976069042', 'https://openalex.org/W2954914497', 'https://openalex.org/W6785363610', 'https://openalex.org/W2963775347', 'https://openalex.org/W2015433306', 'https://openalex.org/W2752796333', 'https://openalex.org/W6752901844', 'https://openalex.org/W2766465839', 'https://openalex.org/W6756038284', 'https://openalex.org/W6765147365', 'https://openalex.org/W6762464424', 'https://openalex.org/W2157331557', 'https://openalex.org/W6757028937', 'https://openalex.org/W2120847449', 'https://openalex.org/W2803963372', 'https://openalex.org/W6780323268', 'https://openalex.org/W6631190155', 'https://openalex.org/W6771763809', 'https://openalex.org/W6782602824', 'https://openalex.org/W3080857286', 'https://openalex.org/W2603777577', 'https://openalex.org/W6787084273', 'https://openalex.org/W2475287302', 'https://openalex.org/W6784348365', 'https://openalex.org/W2963420272', 'https://openalex.org/W6636510571', 'https://openalex.org/W2799087757', 'https://openalex.org/W6747899497', 'https://openalex.org/W3000389243', 'https://openalex.org/W2989955315', 'https://openalex.org/W1522301498', 'https://openalex.org/W3040787347', 'https://openalex.org/W2962942158', 'https://openalex.org/W2941810372', 'https://openalex.org/W3112895167', 'https://openalex.org/W2964121744', 'https://openalex.org/W3097059217', 'https://openalex.org/W3049637270', 'https://openalex.org/W4287666817', 'https://openalex.org/W2385545', 'https://openalex.org/W2963485424', 'https://openalex.org/W3109050852', 'https://openalex.org/W2963889406', 'https://openalex.org/W2519091744', 'https://openalex.org/W2811079561', 'https://openalex.org/W2963233633', 'https://openalex.org/W1614298861', 'https://openalex.org/W4293568215', 'https://openalex.org/W3162366763', 'https://openalex.org/W2963799213', 'https://openalex.org/W2475687244', 'https://openalex.org/W2405581548', 'https://openalex.org/W2995233853', 'https://openalex.org/W2909535979', 'https://openalex.org/W2903005299', 'https://openalex.org/W2785325870']",2021-05-13
https://openalex.org/W3015710813,https://doi.org/10.1109/icassp40776.2020.9053047,Transferring Neural Speech Waveform Synthesizers to Musical Instrument Sounds Generation,"Recent neural waveform synthesizers such as WaveNet, WaveG-low, and the neural-source-filter (NSF) model have shown good performance in speech synthesis despite their different methods of waveform generation. The similarity between speech and music audio synthesis techniques suggests interesting avenues to explore in terms of the best way to apply speech synthesizers in the music domain. This work compares three neural synthesizers used for musical instrument sounds generation under three scenarios: training from scratch on music data, zero-shot learning from the speech domain, and fine-tuning-based adaptation from the speech to the music domain. The results of a large-scale perceptual test demonstrated that the performance of three synthesizers improved when they were pre-trained on speech data and fine-tuned on music data, which indicates the usefulness of knowledge from speech data for music audio generation. Among the synthesizers, WaveGlow showed the best potential in zero-shot learning while NSF performed best in the other scenarios and could generate samples that were perceptually close to natural audio.","['https://openalex.org/W6736723571', 'https://openalex.org/W6758675244', 'https://openalex.org/W2962721334', 'https://openalex.org/W2963568710', 'https://openalex.org/W2972882294', 'https://openalex.org/W2963175743', 'https://openalex.org/W2972309641', 'https://openalex.org/W6752910514', 'https://openalex.org/W6733471323', 'https://openalex.org/W6632561901', 'https://openalex.org/W2129142580', 'https://openalex.org/W2749651610', 'https://openalex.org/W117352590', 'https://openalex.org/W6732429163', 'https://openalex.org/W2963300588', 'https://openalex.org/W1994806620', 'https://openalex.org/W2150658333', 'https://openalex.org/W6753018729', 'https://openalex.org/W2759171953', 'https://openalex.org/W2963522141', 'https://openalex.org/W6936113694', 'https://openalex.org/W6755182157', 'https://openalex.org/W304834817', 'https://openalex.org/W2949382160', 'https://openalex.org/W2898148140', 'https://openalex.org/W2910577860', 'https://openalex.org/W2606176153', 'https://openalex.org/W2963139417', 'https://openalex.org/W2584032004', 'https://openalex.org/W3101943858', 'https://openalex.org/W2587284713', 'https://openalex.org/W4289761690', 'https://openalex.org/W2811079561', 'https://openalex.org/W2962942158', 'https://openalex.org/W2519091744', 'https://openalex.org/W2950547518', 'https://openalex.org/W2962981281', 'https://openalex.org/W1542948701', 'https://openalex.org/W2527729766']",2020-04-09
https://openalex.org/W3024973272,https://doi.org/10.21437/interspeech.2020-1137,Unconditional Audio Generation with Generative Adversarial Networks and Cycle Regularization,"In a recent paper, we have presented a generative adversarial network\n(GAN)-based model for unconditional generation of the mel-spectrograms of\nsinging voices. As the generator of the model is designed to take a\nvariable-length sequence of noise vectors as input, it can generate\nmel-spectrograms of variable length. However, our previous listening test shows\nthat the quality of the generated audio leaves room for improvement. The\npresent paper extends and expands that previous work in the following aspects.\nFirst, we employ a hierarchical architecture in the generator to induce some\nstructure in the temporal dimension. Second, we introduce a cycle\nregularization mechanism to the generator to avoid mode collapse. Third, we\nevaluate the performance of the new model not only for generating singing\nvoices, but also for generating speech voices. Evaluation result shows that new\nmodel outperforms the prior one both objectively and subjectively. We also\nemploy the model to unconditionally generate sequences of piano and violin\nmusic and find the result promising. Audio examples, as well as the code for\nimplementing our model, will be publicly available online upon paper\npublication.\n","['https://openalex.org/W1522301498', 'https://openalex.org/W4295274059', 'https://openalex.org/W4288107125', 'https://openalex.org/W3125709657', 'https://openalex.org/W2950547518', 'https://openalex.org/W2932319787', 'https://openalex.org/W3020570669', 'https://openalex.org/W2964243274', 'https://openalex.org/W2099471712', 'https://openalex.org/W2973046048', 'https://openalex.org/W2423557781', 'https://openalex.org/W2995005087', 'https://openalex.org/W2962793481', 'https://openalex.org/W2777302760', 'https://openalex.org/W1924770834', 'https://openalex.org/W2963373786', 'https://openalex.org/W2605195953', 'https://openalex.org/W3034794073', 'https://openalex.org/W2970006822', 'https://openalex.org/W2120605154', 'https://openalex.org/W3114301328', 'https://openalex.org/W2811079561', 'https://openalex.org/W2964607787', 'https://openalex.org/W2898148140', 'https://openalex.org/W2962942158', 'https://openalex.org/W4320013936', 'https://openalex.org/W2919624000', 'https://openalex.org/W2963804063', 'https://openalex.org/W3000389243', 'https://openalex.org/W2972706021', 'https://openalex.org/W3005991431', 'https://openalex.org/W2995233853', 'https://openalex.org/W3003673875', 'https://openalex.org/W2899724567', 'https://openalex.org/W2948211236', 'https://openalex.org/W2125389028', 'https://openalex.org/W2963306805', 'https://openalex.org/W3015843452', 'https://openalex.org/W3046715528', 'https://openalex.org/W2267126114', 'https://openalex.org/W2940544976', 'https://openalex.org/W2754229890', 'https://openalex.org/W2962706768', 'https://openalex.org/W2953318193', 'https://openalex.org/W2964199361', 'https://openalex.org/W3035574324', 'https://openalex.org/W2910577860', 'https://openalex.org/W2962981281', 'https://openalex.org/W2972867623']",2020-10-25
https://openalex.org/W3205226109,https://doi.org/10.18653/v1/2021.emnlp-main.347,DiscoDVT: Generating Long Text with Discourse-Aware Discrete Variational Transformer,"Despite the recent advances in applying pre-trained language models to generate high-quality texts, generating long passages that maintain long-range coherence is yet challenging for these models. In this paper, we propose DiscoDVT, a discourse-aware discrete variational Transformer to tackle the incoherence issue. DiscoDVT learns a discrete variable sequence that summarizes the global structure of the text and then applies it to guide the generation process at each decoding step. To further embed discourse-aware information into the discrete latent representations, we introduce an auxiliary objective to model the discourse relations within the text. We conduct extensive experiments on two open story generation datasets and demonstrate that the latent codes learn meaningful correspondence to the discourse structures that guide the model to generate long texts with better long-range coherence.","['https://openalex.org/W4294623686', 'https://openalex.org/W2166957049', 'https://openalex.org/W4391602018', 'https://openalex.org/W2797585226', 'https://openalex.org/W2983962589', 'https://openalex.org/W2938704169', 'https://openalex.org/W1975879668', 'https://openalex.org/W2996287690', 'https://openalex.org/W2548228487', 'https://openalex.org/W3098824823', 'https://openalex.org/W3037337776', 'https://openalex.org/W1566289585', 'https://openalex.org/W2968297680', 'https://openalex.org/W2979826702', 'https://openalex.org/W2995404354', 'https://openalex.org/W2962717182', 'https://openalex.org/W3014521650', 'https://openalex.org/W2963730239', 'https://openalex.org/W3100714086', 'https://openalex.org/W1579838312', 'https://openalex.org/W2914855263', 'https://openalex.org/W2962942158', 'https://openalex.org/W2963993699', 'https://openalex.org/W2963411289', 'https://openalex.org/W2889009749', 'https://openalex.org/W2971074500', 'https://openalex.org/W2789543585', 'https://openalex.org/W2963790827', 'https://openalex.org/W3115433372', 'https://openalex.org/W2101105183', 'https://openalex.org/W3102401511', 'https://openalex.org/W2587284713', 'https://openalex.org/W3034999214', 'https://openalex.org/W2115613106', 'https://openalex.org/W2963799213', 'https://openalex.org/W2962821399', 'https://openalex.org/W2952750383', 'https://openalex.org/W3167880383', 'https://openalex.org/W2908510526', 'https://openalex.org/W1497300277', 'https://openalex.org/W2963878748', 'https://openalex.org/W2963096510', 'https://openalex.org/W2982399380', 'https://openalex.org/W2970574558', 'https://openalex.org/W2123442489', 'https://openalex.org/W2963206148', 'https://openalex.org/W2547875792', 'https://openalex.org/W3098708719', 'https://openalex.org/W3022187094', 'https://openalex.org/W1605174196']",2021-01-01
https://openalex.org/W2989725337,https://doi.org/10.1109/mcas.2019.2945210,Applications of Deep Learning to Audio Generation,"In the recent past years, deep learning based machine learning systems have demonstrated remarkable success for a wide range of learning tasks in multiple domains such as computer vision, speech recognition and other pattern recognition based applications. The purpose of this article is to contribute a timely review and introduction of state-of-the-art deep learning techniques and their effectiveness in speech/acoustic signal processing. Thorough investigations of various deep learning architectures are provided under the categories of discriminative and generative algorithms, including the up-to-date Generative Adversarial Networks (GANs) as an integrated model. A comprehensive overview of applications in audio generation is highlighted. Based on understandings from these approaches, we discuss how deep learning methods can benefit the field of speech/acoustic signal synthesis and the potential issues that need to be addressed for prospective real-world scenarios. We hope this survey provides a valuable reference for practitioners seeking to innovate in the usage of deep learning approaches for speech/acoustic signal generation.","['https://openalex.org/W6638896900', 'https://openalex.org/W6681096077', 'https://openalex.org/W6637373629', 'https://openalex.org/W2618530766', 'https://openalex.org/W2117539524', 'https://openalex.org/W2025768430', 'https://openalex.org/W2100495367', 'https://openalex.org/W2194775991', 'https://openalex.org/W2097117768', 'https://openalex.org/W2172174689', 'https://openalex.org/W6680300913', 'https://openalex.org/W2112796928', 'https://openalex.org/W2803380720', 'https://openalex.org/W2040870580', 'https://openalex.org/W1995341919', 'https://openalex.org/W2076063813', 'https://openalex.org/W2766736793', 'https://openalex.org/W6736723571', 'https://openalex.org/W2136922672', 'https://openalex.org/W6754333143', 'https://openalex.org/W6788718338', 'https://openalex.org/W6744516482', 'https://openalex.org/W6697058068', 'https://openalex.org/W2116261113', 'https://openalex.org/W2064675550', 'https://openalex.org/W6637242042', 'https://openalex.org/W6682889407', 'https://openalex.org/W6635679246', 'https://openalex.org/W6638444622', 'https://openalex.org/W2962949934', 'https://openalex.org/W6640963894', 'https://openalex.org/W2150658333', 'https://openalex.org/W2129142580', 'https://openalex.org/W2399816321', 'https://openalex.org/W2100819376', 'https://openalex.org/W2509534099', 'https://openalex.org/W6950332740', 'https://openalex.org/W2136166660', 'https://openalex.org/W2045158511', 'https://openalex.org/W2020024436', 'https://openalex.org/W2010833326', 'https://openalex.org/W2102003408', 'https://openalex.org/W2481322208', 'https://openalex.org/W6688386640', 'https://openalex.org/W4205947740', 'https://openalex.org/W6743002019', 'https://openalex.org/W2791178280', 'https://openalex.org/W2601110281', 'https://openalex.org/W2809621972', 'https://openalex.org/W1509691205', 'https://openalex.org/W6743446608', 'https://openalex.org/W6696843773', 'https://openalex.org/W2423557781', 'https://openalex.org/W2515336442', 'https://openalex.org/W2964089573', 'https://openalex.org/W2847144333', 'https://openalex.org/W2157331557', 'https://openalex.org/W2963608065', 'https://openalex.org/W6640212811', 'https://openalex.org/W2043003570', 'https://openalex.org/W1990505856', 'https://openalex.org/W2793479148', 'https://openalex.org/W2963804033', 'https://openalex.org/W2964060510', 'https://openalex.org/W2494654097', 'https://openalex.org/W2086384421', 'https://openalex.org/W6732429163', 'https://openalex.org/W3119535344', 'https://openalex.org/W6747317711', 'https://openalex.org/W2798253173', 'https://openalex.org/W6729482032', 'https://openalex.org/W2964243274', 'https://openalex.org/W6713645886', 'https://openalex.org/W2963609956', 'https://openalex.org/W2557449848', 'https://openalex.org/W6745697700', 'https://openalex.org/W6685352114', 'https://openalex.org/W6765987481', 'https://openalex.org/W6718379498', 'https://openalex.org/W2591927543', 'https://openalex.org/W2778460379', 'https://openalex.org/W6753018729', 'https://openalex.org/W6720691552', 'https://openalex.org/W6729856380', 'https://openalex.org/W2963073614', 'https://openalex.org/W2746474733', 'https://openalex.org/W6912282855', 'https://openalex.org/W2783657448', 'https://openalex.org/W2473388484', 'https://openalex.org/W2102737569', 'https://openalex.org/W2123299109', 'https://openalex.org/W2590129515', 'https://openalex.org/W2787324986', 'https://openalex.org/W2576165910', 'https://openalex.org/W6741832134', 'https://openalex.org/W2680548194', 'https://openalex.org/W6729767818', 'https://openalex.org/W6744222783', 'https://openalex.org/W2105099419', 'https://openalex.org/W6717789980', 'https://openalex.org/W6727654133', 'https://openalex.org/W6735913928', 'https://openalex.org/W2919115771', 'https://openalex.org/W2395849284', 'https://openalex.org/W2607037079', 'https://openalex.org/W2579275111', 'https://openalex.org/W6732249622', 'https://openalex.org/W2163922914', 'https://openalex.org/W2625219738', 'https://openalex.org/W6715189028', 'https://openalex.org/W6843673214', 'https://openalex.org/W6717255582', 'https://openalex.org/W2962793481', 'https://openalex.org/W6748048626', 'https://openalex.org/W2470142083', 'https://openalex.org/W6687045409', 'https://openalex.org/W44815768', 'https://openalex.org/W3121926921', 'https://openalex.org/W2950253601', 'https://openalex.org/W2405756170', 'https://openalex.org/W2949382160', 'https://openalex.org/W2739969482', 'https://openalex.org/W1853900790', 'https://openalex.org/W1686810756', 'https://openalex.org/W2099471712', 'https://openalex.org/W2150341604', 'https://openalex.org/W2554423077', 'https://openalex.org/W2138857742', 'https://openalex.org/W2963681776', 'https://openalex.org/W2156387975', 'https://openalex.org/W2519091744', 'https://openalex.org/W4297752781', 'https://openalex.org/W4295521014', 'https://openalex.org/W2963464195', 'https://openalex.org/W2606176153', 'https://openalex.org/W2674088828', 'https://openalex.org/W1959608418', 'https://openalex.org/W2997574889', 'https://openalex.org/W2951326654', 'https://openalex.org/W1849277567', 'https://openalex.org/W2412510955', 'https://openalex.org/W2963557407', 'https://openalex.org/W1924770834', 'https://openalex.org/W2963373786', 'https://openalex.org/W2964201867', 'https://openalex.org/W2786254735', 'https://openalex.org/W2218318129', 'https://openalex.org/W2163605009', 'https://openalex.org/W1799366690', 'https://openalex.org/W1598796236', 'https://openalex.org/W2739748921', 'https://openalex.org/W2746068898', 'https://openalex.org/W2432840504', 'https://openalex.org/W2527569769', 'https://openalex.org/W2173520492', 'https://openalex.org/W2181671747', 'https://openalex.org/W2145094598', 'https://openalex.org/W2772474126', 'https://openalex.org/W2554314924', 'https://openalex.org/W2963703618', 'https://openalex.org/W2811079561', 'https://openalex.org/W2752134738', 'https://openalex.org/W2619368999', 'https://openalex.org/W2766812927', 'https://openalex.org/W2548275288', 'https://openalex.org/W2963784072', 'https://openalex.org/W2964195110', 'https://openalex.org/W4294619240', 'https://openalex.org/W2293272996', 'https://openalex.org/W2188365844', 'https://openalex.org/W1665214252', 'https://openalex.org/W4320013936', 'https://openalex.org/W2962942158', 'https://openalex.org/W4297772864', 'https://openalex.org/W2584032004', 'https://openalex.org/W2962879692', 'https://openalex.org/W4293320219', 'https://openalex.org/W2964281804', 'https://openalex.org/W2782892391', 'https://openalex.org/W2963782041', 'https://openalex.org/W2294797155', 'https://openalex.org/W2950776302', 'https://openalex.org/W2963684088', 'https://openalex.org/W2761241264', 'https://openalex.org/W2951986497']",2019-01-01
https://openalex.org/W3035481475,https://doi.org/10.1109/cvpr42600.2020.00844,Normalizing Flows With Multi-Scale Autoregressive Priors,"Flow-based generative models are an important class of exact inference models that admit efficient inference and sampling for image synthesis. Owing to the efficiency constraints on the design of the flow layers, e.g. split coupling flow layers in which approximately half the pixels do not undergo further transformations, they have limited expressiveness for modeling long-range data dependencies compared to autoregressive models that rely on conditional pixel-wise generation. In this work, we improve the representational power of flow-based models by introducing channel-wise dependencies in their latent space through multi-scale autoregressive priors (mAR). Our mAR prior for models with split coupling flow layers (mAR-SCF) can better capture dependencies in complex multimodal data. The resulting model achieves state-of-the-art density estimation results on MNIST, CIFAR-10, and ImageNet. Furthermore, we show that mAR-SCF allows for improved image generation quality, with gains in FID and Inception scores compared to state-of-the-art flow-based models.","['https://openalex.org/W2769810959', 'https://openalex.org/W6732492507', 'https://openalex.org/W2117539524', 'https://openalex.org/W6639732818', 'https://openalex.org/W2423557781', 'https://openalex.org/W6738543193', 'https://openalex.org/W6674887261', 'https://openalex.org/W6739112683', 'https://openalex.org/W2065585567', 'https://openalex.org/W6749097180', 'https://openalex.org/W6765779288', 'https://openalex.org/W6758706709', 'https://openalex.org/W2064675550', 'https://openalex.org/W6758845805', 'https://openalex.org/W6748409065', 'https://openalex.org/W6733471323', 'https://openalex.org/W6762931180', 'https://openalex.org/W6763437871', 'https://openalex.org/W6685352114', 'https://openalex.org/W6755312952', 'https://openalex.org/W6747491877', 'https://openalex.org/W6734831415', 'https://openalex.org/W6635084905', 'https://openalex.org/W6753018729', 'https://openalex.org/W6714644935', 'https://openalex.org/W6640963894', 'https://openalex.org/W2112796928', 'https://openalex.org/W6787972765', 'https://openalex.org/W6751576797', 'https://openalex.org/W6759142241', 'https://openalex.org/W6756533288', 'https://openalex.org/W6763509872', 'https://openalex.org/W6738536549', 'https://openalex.org/W4298580827', 'https://openalex.org/W2963428348', 'https://openalex.org/W2963047245', 'https://openalex.org/W4297798428', 'https://openalex.org/W2953046278', 'https://openalex.org/W2963684088', 'https://openalex.org/W4288616984', 'https://openalex.org/W2560512785', 'https://openalex.org/W2962750131', 'https://openalex.org/W1583912456', 'https://openalex.org/W4301206121', 'https://openalex.org/W2964307104', 'https://openalex.org/W4293849739', 'https://openalex.org/W2971074500', 'https://openalex.org/W2587284713', 'https://openalex.org/W2963373786', 'https://openalex.org/W2963139417', 'https://openalex.org/W2099471712', 'https://openalex.org/W4320013936', 'https://openalex.org/W4288617998', 'https://openalex.org/W4294619240', 'https://openalex.org/W2902630600', 'https://openalex.org/W2625614184', 'https://openalex.org/W2962695743', 'https://openalex.org/W2099057450', 'https://openalex.org/W2962942158', 'https://openalex.org/W2963540976', 'https://openalex.org/W2963981733', 'https://openalex.org/W2964020555', 'https://openalex.org/W2963527611', 'https://openalex.org/W2911827218', 'https://openalex.org/W1810943226', 'https://openalex.org/W2963636093', 'https://openalex.org/W2964122153', 'https://openalex.org/W2942415903', 'https://openalex.org/W2953318193', 'https://openalex.org/W1959608418', 'https://openalex.org/W2970641149', 'https://openalex.org/W2963857374', 'https://openalex.org/W2952716587', 'https://openalex.org/W1909320841', 'https://openalex.org/W2962738009', 'https://openalex.org/W2912298597', 'https://openalex.org/W4297712886', 'https://openalex.org/W2963866658', 'https://openalex.org/W2893749619', 'https://openalex.org/W2964057425', 'https://openalex.org/W2811079561', 'https://openalex.org/W2962897886', 'https://openalex.org/W2267126114', 'https://openalex.org/W2948700496', 'https://openalex.org/W3118608800', 'https://openalex.org/W2950946978', 'https://openalex.org/W2912618358', 'https://openalex.org/W2151391352']",2020-06-01
https://openalex.org/W3042610466,https://doi.org/10.23919/fruct49677.2020.9210988,A Deep Learning Approach for Low-Latency Packet Loss Concealment of Audio Signals in Networked Music Performance Applications,"Networked Music Performance (NMP) is envisioned as a potential game changer among Internet applications: it aims at revolutionizing the traditional concept of musical interaction by enabling remote musicians to interact and perform together through a telecommunication network. Ensuring realistic conditions for music performance, however, constitutes a significant engineering challenge due to extremely strict requirements in terms of audio quality and, most importantly, network delay. To minimize the end-to-end delay experienced by the musicians, typical implementations of NMP applications use un-compressed, bidirectional audio streams and leverage UDP as transport protocol. Being connection less and unreliable,audio packets transmitted via UDP which become lost in transit are not re-transmitted and thus cause glitches in the receiver audio playout. This article describes a technique for predicting lost packet content in real-time using a deep learning approach. The ability of concealing errors in real time can help mitigate audio impairments caused by packet losses, thus improving the quality of audio playout in real-world scenarios.","['https://openalex.org/W2071631271', 'https://openalex.org/W2268820445', 'https://openalex.org/W2902184804', 'https://openalex.org/W2025758844', 'https://openalex.org/W2312700074', 'https://openalex.org/W202885422', 'https://openalex.org/W2068646282', 'https://openalex.org/W2560180317', 'https://openalex.org/W2591938480', 'https://openalex.org/W2243752967', 'https://openalex.org/W2799789537', 'https://openalex.org/W2134215714', 'https://openalex.org/W6696952141', 'https://openalex.org/W2106885754', 'https://openalex.org/W2163324046', 'https://openalex.org/W2916113431', 'https://openalex.org/W2963011080', 'https://openalex.org/W2752796333', 'https://openalex.org/W2106748815', 'https://openalex.org/W2137539246', 'https://openalex.org/W6753018729', 'https://openalex.org/W6631190155', 'https://openalex.org/W2089394015', 'https://openalex.org/W2181523240', 'https://openalex.org/W6636045042', 'https://openalex.org/W6742331043', 'https://openalex.org/W2793211626', 'https://openalex.org/W2962942158', 'https://openalex.org/W2293172604', 'https://openalex.org/W2741420472', 'https://openalex.org/W1956260418', 'https://openalex.org/W2795935131', 'https://openalex.org/W1480024680', 'https://openalex.org/W2953318193', 'https://openalex.org/W1606487971', 'https://openalex.org/W2963799213', 'https://openalex.org/W1522301498', 'https://openalex.org/W2282093144', 'https://openalex.org/W2782490852']",2020-09-01
https://openalex.org/W3160235471,https://doi.org/10.1109/icassp39728.2021.9415047,Loopnet: Musical Loop Synthesis Conditioned on Intuitive Musical Parameters,"Loops, seamlessly repeatable musical segments, are a cornerstone of modern music production. Contemporary artists often mix and match various sampled or pre-recorded loops based on musical criteria such as rhythm, harmony and timbral texture to create compositions. Taking such criteria into account, we present LoopNet, a feed-forward generative model for creating loops conditioned on intuitive parameters. We leverage Music Information Retrieval (MIR) models as well as a large collection of public loop samples in our study and use the Wave-U-Net architecture to map control parameters to audio. We also evaluate the quality of the generated audio and propose intuitive controls for composers to map the ideas in their minds to an audio loop.","['https://openalex.org/W2593116425', 'https://openalex.org/W2984106626', 'https://openalex.org/W2778460379', 'https://openalex.org/W2963568710', 'https://openalex.org/W6782757012', 'https://openalex.org/W6755257315', 'https://openalex.org/W6732429163', 'https://openalex.org/W2769810959', 'https://openalex.org/W6751512325', 'https://openalex.org/W2972478942', 'https://openalex.org/W6771763809', 'https://openalex.org/W6631190155', 'https://openalex.org/W6753018729', 'https://openalex.org/W3015287975', 'https://openalex.org/W6718379498', 'https://openalex.org/W6756584662', 'https://openalex.org/W6758675244', 'https://openalex.org/W6756630480', 'https://openalex.org/W1819710477', 'https://openalex.org/W6736723571', 'https://openalex.org/W6720665447', 'https://openalex.org/W6742070039', 'https://openalex.org/W1976075345', 'https://openalex.org/W6738878785', 'https://openalex.org/W2120847449', 'https://openalex.org/W2990440871', 'https://openalex.org/W1522301498', 'https://openalex.org/W2811079561', 'https://openalex.org/W2894295011', 'https://openalex.org/W2962919088', 'https://openalex.org/W2584032004', 'https://openalex.org/W2963373786', 'https://openalex.org/W2963782041', 'https://openalex.org/W2901024736', 'https://openalex.org/W2606176153', 'https://openalex.org/W4294619240', 'https://openalex.org/W3136272958', 'https://openalex.org/W4297817572', 'https://openalex.org/W2962942158', 'https://openalex.org/W2099471712', 'https://openalex.org/W2995233853', 'https://openalex.org/W2962981281', 'https://openalex.org/W3000389243', 'https://openalex.org/W2963452667', 'https://openalex.org/W2473712276', 'https://openalex.org/W2519091744', 'https://openalex.org/W1518164296', 'https://openalex.org/W2910577860', 'https://openalex.org/W4289287927', 'https://openalex.org/W2752134738', 'https://openalex.org/W2962968839', 'https://openalex.org/W4320013936', 'https://openalex.org/W2738329169', 'https://openalex.org/W2624765617', 'https://openalex.org/W2964121744', 'https://openalex.org/W2901601067']",2021-05-13
https://openalex.org/W3093209529,https://doi.org/10.1145/3394171.3413519,Drum Synthesis and Rhythmic Transformation with Adversarial Autoencoders,"Creative rhythmic transformations of musical audio refer to automated methods for manipulation of temporally-relevant sounds in time. This paper presents a method for joint synthesis and rhythm transformation of drum sounds through the use of adversarial autoencoders (AAE). Users may navigate both the timbre and rhythm of drum patterns in audio recordings through expressive control over a low-dimensional latent space. The model is based on an AAE with Gaussian mixture latent distributions that introduce rhythmic pattern conditioning to represent a wide variety of drum performances. The AAE is trained on a dataset of bar-length segments of percussion recordings, along with their clustered rhythmic pattern labels. The decoder is conditioned during adversarial training for mixing of data-driven rhythmic and timbral properties. The system is trained with over 500000 bars from 5418 tracks in popular datasets covering various musical genres. In an evaluation using real percussion recordings, the reconstruction accuracy and latent space interpolation between drum performances are investigated for audio generation conditioned by target rhythmic patterns.","['https://openalex.org/W2405656250', 'https://openalex.org/W1978421240', 'https://openalex.org/W2120847449', 'https://openalex.org/W2992790584', 'https://openalex.org/W1991135550', 'https://openalex.org/W2950299304', 'https://openalex.org/W1501078361', 'https://openalex.org/W2397818963', 'https://openalex.org/W2964008635', 'https://openalex.org/W2963568578', 'https://openalex.org/W2990861008', 'https://openalex.org/W2951535099', 'https://openalex.org/W3153361515', 'https://openalex.org/W2099471712', 'https://openalex.org/W2893613496', 'https://openalex.org/W1585610988', 'https://openalex.org/W2949117887', 'https://openalex.org/W841503396', 'https://openalex.org/W3038022805', 'https://openalex.org/W2404509795', 'https://openalex.org/W156361483', 'https://openalex.org/W2999344275', 'https://openalex.org/W168712871', 'https://openalex.org/W2903006902', 'https://openalex.org/W1591658817', 'https://openalex.org/W2949382160', 'https://openalex.org/W2964126461', 'https://openalex.org/W2963032576', 'https://openalex.org/W2746068898', 'https://openalex.org/W2963226019', 'https://openalex.org/W2971074500', 'https://openalex.org/W182494600', 'https://openalex.org/W2989955315', 'https://openalex.org/W2295819747', 'https://openalex.org/W2962942158', 'https://openalex.org/W2572771584', 'https://openalex.org/W1959608418', 'https://openalex.org/W2962879692', 'https://openalex.org/W2910577860', 'https://openalex.org/W2963746531', 'https://openalex.org/W3029579848']",2020-10-12
https://openalex.org/W2986830070,https://doi.org/10.24963/ijcai.2020/400,Seq-U-Net: A One-Dimensional Causal U-Net for Efficient Sequence Modelling,"Convolutional neural networks (CNNs) with dilated filters such as the Wavenet or the Temporal Convolutional Network (TCN) have shown good results in a variety of sequence modelling tasks. While their receptive field grows exponentially with the number of layers, computing the convolutions over very long sequences of features in each layer is time and memory-intensive, and prohibits the use of longer receptive fields in practice. To increase efficiency, we make use of the ""slow feature"" hypothesis stating that many features of interest are slowly varying over time. For this, we use a U-Net architecture that computes features at multiple time-scales and adapt it to our auto-regressive scenario by making convolutions causal. We apply our model (""Seq-U-Net"") to a variety of tasks including language and audio generation. In comparison to TCN and Wavenet, our network consistently saves memory and computation time, with speed-ups for training and inference of over 4x in the audio generation experiment in particular, while achieving a comparable performance on real-world tasks.","['https://openalex.org/W6683885649', 'https://openalex.org/W2786228682', 'https://openalex.org/W2107878631', 'https://openalex.org/W1819710477', 'https://openalex.org/W2749590927', 'https://openalex.org/W6752333115', 'https://openalex.org/W2510842514', 'https://openalex.org/W2811079561', 'https://openalex.org/W2099257174', 'https://openalex.org/W2143612262', 'https://openalex.org/W2194775991', 'https://openalex.org/W2890983311', 'https://openalex.org/W2540404261', 'https://openalex.org/W1632114991', 'https://openalex.org/W2584032004', 'https://openalex.org/W2547418827', 'https://openalex.org/W1901129140', 'https://openalex.org/W2516114310', 'https://openalex.org/W2793273050', 'https://openalex.org/W2769810959', 'https://openalex.org/W2948211236', 'https://openalex.org/W2146444479', 'https://openalex.org/W2523469089', 'https://openalex.org/W2949650786', 'https://openalex.org/W2962942158', 'https://openalex.org/W2953331651', 'https://openalex.org/W2952276042', 'https://openalex.org/W2951305674', 'https://openalex.org/W2940744433', 'https://openalex.org/W2964307104', 'https://openalex.org/W2161850243', 'https://openalex.org/W2949382160', 'https://openalex.org/W2964268978', 'https://openalex.org/W2174424190', 'https://openalex.org/W2626778328']",2020-07-01
https://openalex.org/W3217752380,,Deep Generative Models for Synthetic Data,"Growing interest in synthetic data has stimulated development and advancement of a large variety of deep generative models for a wide range of applications. However, as this research has progressed, its streams have become more specialized and disconnected from each other. For example, models for synthesizing text data for natural language processing cannot readily be compared to models for synthesizing health records. To mitigate this isolation, we propose a data-driven evaluation framework for generative models for synthetic data based on five high-level criteria: representativeness, novelty, realism, diversity and coherence of a synthetic data sample relative to the original data-set regardless of the models' internal structures. The criteria reflect requirements different domains impose on synthetic data and allow model users to assess the quality of synthetic data across models. In a critical review of generative models for sequential data, we examine and compare the importance of each performance criterion in numerous domains. For example, we find that realism and coherence are more important for synthetic data for natural language, speech and audio processing, while novelty and representativeness are more important for healthcare and mobility data. We also find that measurement of representativeness is often accomplished using statistical metrics, realism by using human judgement, and novelty using privacy tests.","['https://openalex.org/W2963373786', 'https://openalex.org/W2136848157', 'https://openalex.org/W3042890712', 'https://openalex.org/W2293078015', 'https://openalex.org/W1975929202', 'https://openalex.org/W2510642588', 'https://openalex.org/W2963175743', 'https://openalex.org/W2941466173', 'https://openalex.org/W2805089815', 'https://openalex.org/W2147800946', 'https://openalex.org/W2964024144', 'https://openalex.org/W2963942586', 'https://openalex.org/W2962942158', 'https://openalex.org/W2964268978', 'https://openalex.org/W3004056434', 'https://openalex.org/W2265846598', 'https://openalex.org/W2992005611', 'https://openalex.org/W3080911033', 'https://openalex.org/W2775808131', 'https://openalex.org/W3016014797', 'https://openalex.org/W1893706364', 'https://openalex.org/W2593414223', 'https://openalex.org/W2601110281', 'https://openalex.org/W2058373514', 'https://openalex.org/W2963575853', 'https://openalex.org/W2112796928', 'https://openalex.org/W2963408210', 'https://openalex.org/W2792210438', 'https://openalex.org/W1909320841', 'https://openalex.org/W2905027213', 'https://openalex.org/W2093302200', 'https://openalex.org/W2892341857', 'https://openalex.org/W3103346379', 'https://openalex.org/W2564034046', 'https://openalex.org/W2792130717', 'https://openalex.org/W2144354855', 'https://openalex.org/W2622068151', 'https://openalex.org/W2765811365', 'https://openalex.org/W2904931021', 'https://openalex.org/W2163605009', 'https://openalex.org/W1959608418', 'https://openalex.org/W2963018920', 'https://openalex.org/W2962879692', 'https://openalex.org/W2963034797', 'https://openalex.org/W2594990650', 'https://openalex.org/W1832693441', 'https://openalex.org/W2102003408', 'https://openalex.org/W2962990490', 'https://openalex.org/W3125952450', 'https://openalex.org/W2950299304', 'https://openalex.org/W2784823820', 'https://openalex.org/W2594685110', 'https://openalex.org/W2963857374', 'https://openalex.org/W2964245526', 'https://openalex.org/W2525778437', 'https://openalex.org/W1610060839', 'https://openalex.org/W1985258458', 'https://openalex.org/W2100495367', 'https://openalex.org/W2101926813', 'https://openalex.org/W2753868141', 'https://openalex.org/W2072334238', 'https://openalex.org/W3175134484', 'https://openalex.org/W2129142580', 'https://openalex.org/W2898827701', 'https://openalex.org/W2526050071', 'https://openalex.org/W3016399951', 'https://openalex.org/W2559110679', 'https://openalex.org/W3124770945', 'https://openalex.org/W2805183640', 'https://openalex.org/W2125389028', 'https://openalex.org/W2899901572', 'https://openalex.org/W2949346884', 'https://openalex.org/W2511693736', 'https://openalex.org/W2899141320', 'https://openalex.org/W2963185411', 'https://openalex.org/W2952673310', 'https://openalex.org/W2173520492', 'https://openalex.org/W1498436455', 'https://openalex.org/W2395639500', 'https://openalex.org/W2963092440', 'https://openalex.org/W2804078698', 'https://openalex.org/W201313393', 'https://openalex.org/W2561662441', 'https://openalex.org/W2604918751', 'https://openalex.org/W2949099979', 'https://openalex.org/W2964308564', 'https://openalex.org/W2775288145', 'https://openalex.org/W2963290659', 'https://openalex.org/W3138478053', 'https://openalex.org/W2808478781', 'https://openalex.org/W2799397159', 'https://openalex.org/W2963300588']",2021-11-01
https://openalex.org/W2940744433,https://doi.org/10.48550/arxiv.1904.10509,Generating Long Sequences with Sparse Transformers,"Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.","['https://openalex.org/W2949427019', 'https://openalex.org/W2962942158', 'https://openalex.org/W1793121960', 'https://openalex.org/W2613904329', 'https://openalex.org/W2787214294', 'https://openalex.org/W2338908902', 'https://openalex.org/W2763421725', 'https://openalex.org/W2953331651', 'https://openalex.org/W2519091744', 'https://openalex.org/W2891815651', 'https://openalex.org/W2953318193', 'https://openalex.org/W2962776038', 'https://openalex.org/W2952276042', 'https://openalex.org/W2724346673', 'https://openalex.org/W2950946978', 'https://openalex.org/W2773781902', 'https://openalex.org/W2594961016', 'https://openalex.org/W2259472270', 'https://openalex.org/W2906625520', 'https://openalex.org/W2462831000', 'https://openalex.org/W2963139417', 'https://openalex.org/W2778792233', 'https://openalex.org/W2886490473', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964122153']",2019-04-23
https://openalex.org/W3041956526,https://doi.org/10.48550/arxiv.2007.03898,NVAE: A Deep Hierarchical Variational Autoencoder,"Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256$\times$256 pixels. The source code is available at https://github.com/NVlabs/NVAE .","['https://openalex.org/W2531409750', 'https://openalex.org/W2970014727', 'https://openalex.org/W2964318004', 'https://openalex.org/W2963147844', 'https://openalex.org/W3007345776', 'https://openalex.org/W2950739196', 'https://openalex.org/W1836465849', 'https://openalex.org/W2963319176', 'https://openalex.org/W2949899814', 'https://openalex.org/W2962844293', 'https://openalex.org/W2963163009', 'https://openalex.org/W2963446624', 'https://openalex.org/W2963851840', 'https://openalex.org/W2962738009', 'https://openalex.org/W2519430864', 'https://openalex.org/W2964160479', 'https://openalex.org/W2964167449', 'https://openalex.org/W2953046278', 'https://openalex.org/W2409550820', 'https://openalex.org/W2970641149', 'https://openalex.org/W2743472713', 'https://openalex.org/W2963836885', 'https://openalex.org/W2593634001', 'https://openalex.org/W3118608800', 'https://openalex.org/W1834627138', 'https://openalex.org/W2148989240', 'https://openalex.org/W2962897886', 'https://openalex.org/W2752782242', 'https://openalex.org/W2962942158', 'https://openalex.org/W2267126114', 'https://openalex.org/W2547875792', 'https://openalex.org/W2767286248', 'https://openalex.org/W2962770929', 'https://openalex.org/W2176412452', 'https://openalex.org/W2397262406', 'https://openalex.org/W2173520492', 'https://openalex.org/W2108598243', 'https://openalex.org/W2893749619', 'https://openalex.org/W299440670', 'https://openalex.org/W2955425717', 'https://openalex.org/W2618017917', 'https://openalex.org/W2963655973', 'https://openalex.org/W2963043971', 'https://openalex.org/W2963408680', 'https://openalex.org/W2962741254', 'https://openalex.org/W2561050497', 'https://openalex.org/W2971074500', 'https://openalex.org/W2912298597', 'https://openalex.org/W2902630600', 'https://openalex.org/W2963685250', 'https://openalex.org/W2963223306', 'https://openalex.org/W2963275229', 'https://openalex.org/W2971089687', 'https://openalex.org/W2970324680', 'https://openalex.org/W2587284713', 'https://openalex.org/W2952165242', 'https://openalex.org/W2970945269', 'https://openalex.org/W2963622136', 'https://openalex.org/W2194775991', 'https://openalex.org/W2963139417', 'https://openalex.org/W2963812505', 'https://openalex.org/W3006076983', 'https://openalex.org/W2962760235', 'https://openalex.org/W2963135265', 'https://openalex.org/W71081281', 'https://openalex.org/W2302255633', 'https://openalex.org/W2963522047', 'https://openalex.org/W2963428348', 'https://openalex.org/W3035564323', 'https://openalex.org/W2963636093', 'https://openalex.org/W2964052395', 'https://openalex.org/W2964122153', 'https://openalex.org/W1959608418', 'https://openalex.org/W1522301498', 'https://openalex.org/W2602076750', 'https://openalex.org/W2990155616', 'https://openalex.org/W2965850471', 'https://openalex.org/W2338908902', 'https://openalex.org/W2970554285']",2020-07-08
https://openalex.org/W2948211236,https://doi.org/10.48550/arxiv.1906.01083,MelNet: A Generative Model for Audio in the Frequency Domain,"Capturing high-level structure in audio waveforms is challenging because a single second of audio spans tens of thousands of timesteps. While long-range dependencies are difficult to model directly in the time domain, we show that they can be more tractably modelled in two-dimensional time-frequency representations such as spectrograms. By leveraging this representational advantage, in conjunction with a highly expressive probabilistic model and a multiscale generation procedure, we design a model capable of generating high-fidelity audio samples which capture structure at timescales that time-domain models have yet to achieve. We apply our model to a variety of audio generation tasks, including unconditional speech generation, music generation, and text-to-speech synthesis---showing improvements over previous approaches in both density estimates and human judgments.","['https://openalex.org/W2883853252', 'https://openalex.org/W2064675550', 'https://openalex.org/W1664573881', 'https://openalex.org/W2963139417', 'https://openalex.org/W2608207374', 'https://openalex.org/W2901997113', 'https://openalex.org/W2099471712', 'https://openalex.org/W2892620417', 'https://openalex.org/W1771459135', 'https://openalex.org/W2396144419', 'https://openalex.org/W2004453603', 'https://openalex.org/W2953331651', 'https://openalex.org/W2795485067', 'https://openalex.org/W2962942158', 'https://openalex.org/W2778792233', 'https://openalex.org/W1810943226', 'https://openalex.org/W1579853615', 'https://openalex.org/W3101648800', 'https://openalex.org/W2769810959', 'https://openalex.org/W2594961016', 'https://openalex.org/W2950547518', 'https://openalex.org/W2897548994', 'https://openalex.org/W2950946978', 'https://openalex.org/W2949382160', 'https://openalex.org/W2919624000', 'https://openalex.org/W2963799213', 'https://openalex.org/W2788851830', 'https://openalex.org/W2963300588', 'https://openalex.org/W2940744433', 'https://openalex.org/W2766527293', 'https://openalex.org/W2950299304', 'https://openalex.org/W1685006559', 'https://openalex.org/W2910577860', 'https://openalex.org/W2194775991', 'https://openalex.org/W1522301498', 'https://openalex.org/W2963636093', 'https://openalex.org/W2097039814', 'https://openalex.org/W2951986497', 'https://openalex.org/W2591927543', 'https://openalex.org/W2953318193', 'https://openalex.org/W2808631503', 'https://openalex.org/W2888169323', 'https://openalex.org/W2475988411', 'https://openalex.org/W2170942820', 'https://openalex.org/W2786254735', 'https://openalex.org/W2951004968', 'https://openalex.org/W2581236139', 'https://openalex.org/W2964243274', 'https://openalex.org/W2120847449', 'https://openalex.org/W2963186101']",2019-06-04
https://openalex.org/W3021164770,https://doi.org/10.48550/arxiv.2005.00341,Jukebox: A Generative Model for Music,"We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox","['https://openalex.org/W2991040477', 'https://openalex.org/W2888169323', 'https://openalex.org/W3015338123', 'https://openalex.org/W2963272440', 'https://openalex.org/W2962990490', 'https://openalex.org/W2963575853', 'https://openalex.org/W2099471712', 'https://openalex.org/W2963975282', 'https://openalex.org/W1512123845', 'https://openalex.org/W2792210438', 'https://openalex.org/W2129142580', 'https://openalex.org/W2963192573', 'https://openalex.org/W2907121943', 'https://openalex.org/W1959608418', 'https://openalex.org/W2766812927', 'https://openalex.org/W2963636093', 'https://openalex.org/W2963090522', 'https://openalex.org/W2971074500', 'https://openalex.org/W2962981281', 'https://openalex.org/W2102870814', 'https://openalex.org/W2401343111', 'https://openalex.org/W2962695743', 'https://openalex.org/W2963047245', 'https://openalex.org/W2962897886', 'https://openalex.org/W2964307104', 'https://openalex.org/W29794711', 'https://openalex.org/W1510436408', 'https://openalex.org/W2150769028', 'https://openalex.org/W2549139847', 'https://openalex.org/W2963799213', 'https://openalex.org/W2320263333', 'https://openalex.org/W2962883485', 'https://openalex.org/W2963568578', 'https://openalex.org/W2951535099', 'https://openalex.org/W2893749619', 'https://openalex.org/W2150658333', 'https://openalex.org/W2963557407', 'https://openalex.org/W2970006822', 'https://openalex.org/W2963403868', 'https://openalex.org/W189596042', 'https://openalex.org/W2116973068', 'https://openalex.org/W2962721334', 'https://openalex.org/W2991108091', 'https://openalex.org/W2940744433', 'https://openalex.org/W2963432880', 'https://openalex.org/W2996037775', 'https://openalex.org/W2922386270', 'https://openalex.org/W2408435475', 'https://openalex.org/W2998108143', 'https://openalex.org/W2949382160', 'https://openalex.org/W1997640156', 'https://openalex.org/W2606176153', 'https://openalex.org/W2964243274', 'https://openalex.org/W2949888546', 'https://openalex.org/W2919624000', 'https://openalex.org/W2973975824', 'https://openalex.org/W2804078698', 'https://openalex.org/W2898148140', 'https://openalex.org/W2964020555', 'https://openalex.org/W2892104732', 'https://openalex.org/W2948211236', 'https://openalex.org/W1999885698', 'https://openalex.org/W2753868141', 'https://openalex.org/W2962942158', 'https://openalex.org/W2584032004', 'https://openalex.org/W2963139417', 'https://openalex.org/W2963534259', 'https://openalex.org/W2964281804', 'https://openalex.org/W2587284713', 'https://openalex.org/W2962691331', 'https://openalex.org/W2963300588', 'https://openalex.org/W2992790584', 'https://openalex.org/W2963681776']",2020-04-30
https://openalex.org/W2947590261,https://doi.org/10.48550/arxiv.1906.00446,Generating Diverse High-Fidelity Images with VQ-VAE-2,"We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.","['https://openalex.org/W2962750131', 'https://openalex.org/W2945027741', 'https://openalex.org/W299440670', 'https://openalex.org/W2963373786', 'https://openalex.org/W2097039814', 'https://openalex.org/W1959608418', 'https://openalex.org/W2962942158', 'https://openalex.org/W2135181320', 'https://openalex.org/W2963981733', 'https://openalex.org/W2962793481', 'https://openalex.org/W1909320841', 'https://openalex.org/W2409550820', 'https://openalex.org/W2267126114', 'https://openalex.org/W2523714292', 'https://openalex.org/W2904367110', 'https://openalex.org/W2897371726', 'https://openalex.org/W2893749619', 'https://openalex.org/W2099471712', 'https://openalex.org/W2963292439', 'https://openalex.org/W2805984778', 'https://openalex.org/W2626778328', 'https://openalex.org/W2778792233', 'https://openalex.org/W2782980316', 'https://openalex.org/W1583912456', 'https://openalex.org/W2553897675', 'https://openalex.org/W2902630600', 'https://openalex.org/W2937274663', 'https://openalex.org/W2963799213', 'https://openalex.org/W2949382160', 'https://openalex.org/W2963139417', 'https://openalex.org/W2560512785', 'https://openalex.org/W2963082441', 'https://openalex.org/W2922386270', 'https://openalex.org/W2963857374', 'https://openalex.org/W2140196014']",2019-06-02
https://openalex.org/W2993118648,https://doi.org/10.48550/arxiv.1912.01219,WaveFlow: A Compact Flow-based Model for Raw Audio,"In this work, we propose WaveFlow, a small-footprint generative flow for raw audio, which is directly trained with maximum likelihood. It handles the long-range structure of 1-D waveform with a dilated 2-D convolutional architecture, while modeling the local variations using expressive autoregressive functions. WaveFlow provides a unified view of likelihood-based models for 1-D data, including WaveNet and WaveGlow as special cases. It generates high-fidelity speech as WaveNet, while synthesizing several orders of magnitude faster as it only requires a few sequential steps to generate very long waveforms with hundreds of thousands of time-steps. Furthermore, it can significantly reduce the likelihood gap that has existed between autoregressive models and flow-based models for efficient synthesis. Finally, our small-footprint WaveFlow has only 5.91M parameters, which is 15$\times$ smaller than WaveGlow. It can generate 22.05 kHz high-fidelity audio 42.6$\times$ faster than real-time (at a rate of 939.3 kHz) on a V100 GPU without engineered inference kernels.","['https://openalex.org/W2962942158', 'https://openalex.org/W3015338123', 'https://openalex.org/W2895434480', 'https://openalex.org/W2951682695', 'https://openalex.org/W2963609956', 'https://openalex.org/W2945613576', 'https://openalex.org/W2964307104', 'https://openalex.org/W2951418500', 'https://openalex.org/W2945278040', 'https://openalex.org/W2964243274', 'https://openalex.org/W2791004381', 'https://openalex.org/W2950299304', 'https://openalex.org/W2938410884', 'https://openalex.org/W2963139417', 'https://openalex.org/W2963840672', 'https://openalex.org/W2901997113', 'https://openalex.org/W2963636093', 'https://openalex.org/W2975414524', 'https://openalex.org/W2963685250', 'https://openalex.org/W2950946978', 'https://openalex.org/W3035068567', 'https://openalex.org/W2962695743', 'https://openalex.org/W2559246505', 'https://openalex.org/W2949382160', 'https://openalex.org/W2964121744', 'https://openalex.org/W2584032004', 'https://openalex.org/W2963090522', 'https://openalex.org/W2963975282', 'https://openalex.org/W2173520492', 'https://openalex.org/W2963047245', 'https://openalex.org/W2903739847', 'https://openalex.org/W1583912456', 'https://openalex.org/W3038172701', 'https://openalex.org/W2964281804', 'https://openalex.org/W2947196194', 'https://openalex.org/W2160473997', 'https://openalex.org/W2963534259', 'https://openalex.org/W2949847548', 'https://openalex.org/W2963712897', 'https://openalex.org/W2963300588', 'https://openalex.org/W2962882868', 'https://openalex.org/W2893749619', 'https://openalex.org/W2963691546', 'https://openalex.org/W2946200149', 'https://openalex.org/W2963782041', 'https://openalex.org/W2431962807', 'https://openalex.org/W2970006822']",2019-12-03
https://openalex.org/W2959020461,https://doi.org/10.48550/arxiv.1907.04868,LakhNES: Improving multi-instrumental music generation with cross-domain pre-training,"We are interested in the task of generating multi-instrumental music scores. The Transformer architecture has recently shown great promise for the task of piano score generation; here we adapt it to the multi-instrumental setting. Transformers are complex, high-dimensional language models which are capable of capturing long-term structure in sequence data, but require large amounts of data to fit. Their success on piano score generation is partially explained by the large volumes of symbolic data readily available for that domain. We leverage the recently-introduced NES-MDB dataset of four-instrument scores from an early video game sound synthesis chip (the NES), which we find to be well-suited to training with the Transformer architecture. To further improve the performance of our model, we propose a pre-training technique to leverage the information in a large collection of heterogeneous music, namely the Lakh MIDI dataset. Despite differences between the two corpora, we find that this transfer learning procedure improves both quantitative and qualitative performance for our primary task.","['https://openalex.org/W2919624000', 'https://openalex.org/W2137619888', 'https://openalex.org/W2963403868', 'https://openalex.org/W2894295011', 'https://openalex.org/W2118730391', 'https://openalex.org/W3104194627', 'https://openalex.org/W2161850243', 'https://openalex.org/W2174443198', 'https://openalex.org/W2963096510', 'https://openalex.org/W2165698076', 'https://openalex.org/W2963550089', 'https://openalex.org/W2963681776', 'https://openalex.org/W2753868141', 'https://openalex.org/W2601110281', 'https://openalex.org/W2911109671', 'https://openalex.org/W2902184207', 'https://openalex.org/W2962942158', 'https://openalex.org/W1556624199', 'https://openalex.org/W2067621398', 'https://openalex.org/W2963575853', 'https://openalex.org/W2808002141', 'https://openalex.org/W2963557407', 'https://openalex.org/W2962968839', 'https://openalex.org/W2898148140', 'https://openalex.org/W2963341956', 'https://openalex.org/W2964289981', 'https://openalex.org/W2475687244', 'https://openalex.org/W2963032576']",2019-07-10
https://openalex.org/W2922386270,https://doi.org/10.48550/arxiv.1903.04933,Hierarchical Autoregressive Image Models with Auxiliary Decoders,"Autoregressive generative models of images tend to be biased towards capturing local structure, and as a result they often produce samples which are lacking in terms of large-scale coherence. To address this, we propose two methods to learn discrete representations of images which abstract away local detail. We show that autoregressive models conditioned on these representations can produce high-fidelity reconstructions of images, and that we can train autoregressive priors on these representations that produce samples with large-scale coherence. We can recursively apply the learning procedure, yielding a hierarchy of progressively more abstract image representations. We train hierarchical class-conditional autoregressive models on the ImageNet dataset and demonstrate that they are able to generate realistic images at resolutions of 128$\times$128 and 256$\times$256 pixels. We also perform a human evaluation study comparing our models with both adversarial and likelihood-based state-of-the-art generative models.","['https://openalex.org/W2593915460', 'https://openalex.org/W2962919088', 'https://openalex.org/W2962739339', 'https://openalex.org/W2559246505', 'https://openalex.org/W343636949', 'https://openalex.org/W2267126114', 'https://openalex.org/W1959608418', 'https://openalex.org/W2952583441', 'https://openalex.org/W2902630600', 'https://openalex.org/W2194775991', 'https://openalex.org/W2904367110', 'https://openalex.org/W2963373786', 'https://openalex.org/W2086161653', 'https://openalex.org/W2963799213', 'https://openalex.org/W2173520492', 'https://openalex.org/W2097039814', 'https://openalex.org/W2893749619', 'https://openalex.org/W2963341956', 'https://openalex.org/W2962942158', 'https://openalex.org/W2242818861', 'https://openalex.org/W2145094598', 'https://openalex.org/W2963223306', 'https://openalex.org/W2751118800', 'https://openalex.org/W2476548250', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963522047', 'https://openalex.org/W2560512785', 'https://openalex.org/W2963981733', 'https://openalex.org/W2099471712', 'https://openalex.org/W2962897886', 'https://openalex.org/W2963139417', 'https://openalex.org/W2964020555', 'https://openalex.org/W2770298516', 'https://openalex.org/W2949899814', 'https://openalex.org/W2963420272', 'https://openalex.org/W2963428348', 'https://openalex.org/W2782980316', 'https://openalex.org/W2153579005', 'https://openalex.org/W1821462560', 'https://openalex.org/W2962835968', 'https://openalex.org/W2962750131', 'https://openalex.org/W2016589492', 'https://openalex.org/W2963636093', 'https://openalex.org/W2934761288', 'https://openalex.org/W2108598243', 'https://openalex.org/W2962760235', 'https://openalex.org/W2097117768', 'https://openalex.org/W2963857374', 'https://openalex.org/W2606176153', 'https://openalex.org/W2842511635', 'https://openalex.org/W2409550820', 'https://openalex.org/W2110798204', 'https://openalex.org/W2302255633', 'https://openalex.org/W2963527611', 'https://openalex.org/W2136922672']",2019-03-06
https://openalex.org/W3133405188,https://doi.org/10.48550/arxiv.2103.01950,Predicting Video with VQVAE,"In recent years, the task of video prediction-forecasting future video given past video frames-has attracted attention in the research community. In this paper we propose a novel approach to this problem with Vector Quantized Variational AutoEncoders (VQ-VAE). With VQ-VAE we compress high-resolution videos into a hierarchical set of multi-scale discrete latent variables. Compared to pixels, this compressed latent space has dramatically reduced dimensionality, allowing us to apply scalable autoregressive generative models to predict video. In contrast to previous work that has largely emphasized highly constrained datasets, we focus on very diverse, large-scale datasets such as Kinetics-600. We predict video at a higher resolution on unconstrained videos, 256x256, than any other previous method to our knowledge. We further validate our approach against prior work via a crowdsourced human evaluation.","['https://openalex.org/W2242818861', 'https://openalex.org/W2963428348', 'https://openalex.org/W2802701183', 'https://openalex.org/W2982494373', 'https://openalex.org/W2964318715', 'https://openalex.org/W2989708447', 'https://openalex.org/W2607738331', 'https://openalex.org/W2902630600', 'https://openalex.org/W2994760783', 'https://openalex.org/W2962839378', 'https://openalex.org/W2949382160', 'https://openalex.org/W2795093135', 'https://openalex.org/W2996375490', 'https://openalex.org/W1568514080', 'https://openalex.org/W2951108950', 'https://openalex.org/W2963092440', 'https://openalex.org/W2470142083', 'https://openalex.org/W2963435596', 'https://openalex.org/W2963482775', 'https://openalex.org/W2796303840', 'https://openalex.org/W2963857374', 'https://openalex.org/W2952453038', 'https://openalex.org/W2971074500', 'https://openalex.org/W2902437806', 'https://openalex.org/W2981803355', 'https://openalex.org/W2891145043', 'https://openalex.org/W2901599654', 'https://openalex.org/W2963125871', 'https://openalex.org/W2922386270', 'https://openalex.org/W2728326942', 'https://openalex.org/W2963799213', 'https://openalex.org/W2034328688', 'https://openalex.org/W2963402657', 'https://openalex.org/W2962876561', 'https://openalex.org/W2964327849', 'https://openalex.org/W2964270168', 'https://openalex.org/W2976617189', 'https://openalex.org/W2764019261', 'https://openalex.org/W2949099979', 'https://openalex.org/W1544092585', 'https://openalex.org/W2962942158', 'https://openalex.org/W2953133772', 'https://openalex.org/W2962750131', 'https://openalex.org/W3010151642', 'https://openalex.org/W2766527293', 'https://openalex.org/W2963669520', 'https://openalex.org/W2175030374', 'https://openalex.org/W2949527053', 'https://openalex.org/W2963547393', 'https://openalex.org/W2887051120', 'https://openalex.org/W3031246127', 'https://openalex.org/W2963406904', 'https://openalex.org/W2962841471', 'https://openalex.org/W3021164770', 'https://openalex.org/W2893749619', 'https://openalex.org/W2953318193', 'https://openalex.org/W2964295739', 'https://openalex.org/W2952595815', 'https://openalex.org/W2799055999']",2021-03-02
https://openalex.org/W2980709326,,MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis,"Previous works (Donahue et al., 2018a; Engel et al., 2019a) have found that generating coherent raw audio waveforms with GANs is challenging. In this paper, we show that it is possible to train GANs reliably to generate high quality coherent waveforms by introducing a set of architectural changes and simple training techniques. Subjective evaluation metric (Mean Opinion Score, or MOS) shows the effectiveness of the proposed approach for high quality mel-spectrogram inversion. To establish the generality of the proposed techniques, we show qualitative results of our model in speech synthesis, music domain translation and unconditional music synthesis. We evaluate the various components of the model through ablation studies and suggest a set of guidelines to design general purpose discriminators and generators for conditional sequence synthesis tasks. Our model is non-autoregressive, fully convolutional, with significantly fewer parameters than competing models and generalizes to unseen speakers for mel-spectrogram inversion. Our pytorch implementation runs at more than 100x faster than realtime on GTX 1080Ti GPU and more than 2x faster than real-time on CPU, without any hardware specific optimization tricks.","['https://openalex.org/W2519091744', 'https://openalex.org/W2963800363', 'https://openalex.org/W2785678896', 'https://openalex.org/W2888164449', 'https://openalex.org/W2963073614', 'https://openalex.org/W2963685250', 'https://openalex.org/W2883853252', 'https://openalex.org/W2963139417', 'https://openalex.org/W2886748926', 'https://openalex.org/W2963174698', 'https://openalex.org/W2796649226', 'https://openalex.org/W2964281804', 'https://openalex.org/W2950893734', 'https://openalex.org/W2962793481', 'https://openalex.org/W2099471712', 'https://openalex.org/W2973203693', 'https://openalex.org/W2938410884', 'https://openalex.org/W2120847449', 'https://openalex.org/W2331128040', 'https://openalex.org/W2202109488', 'https://openalex.org/W2527729766', 'https://openalex.org/W2953331651', 'https://openalex.org/W2962879692', 'https://openalex.org/W2963125871', 'https://openalex.org/W2788851830', 'https://openalex.org/W2951535099', 'https://openalex.org/W2792995953', 'https://openalex.org/W2950299304', 'https://openalex.org/W2769810959', 'https://openalex.org/W2606176153', 'https://openalex.org/W2963799213', 'https://openalex.org/W2475287302', 'https://openalex.org/W2535388113', 'https://openalex.org/W2593414223', 'https://openalex.org/W2910577860', 'https://openalex.org/W2963712897', 'https://openalex.org/W2963300588', 'https://openalex.org/W2920879895', 'https://openalex.org/W2766527293', 'https://openalex.org/W2471520273', 'https://openalex.org/W2962942158', 'https://openalex.org/W2963609956', 'https://openalex.org/W2904367110', 'https://openalex.org/W2964243274', 'https://openalex.org/W2901997113', 'https://openalex.org/W2502312327', 'https://openalex.org/W2963045359']",2019-10-08
https://openalex.org/W3111853169,https://doi.org/10.48550/arxiv.2012.03478,Multi-Instrumentalist Net: Unsupervised Generation of Music from Body Movements,"We propose a novel system that takes as an input body movements of a musician playing a musical instrument and generates music in an unsupervised setting. Learning to generate multi-instrumental music from videos without labeling the instruments is a challenging problem. To achieve the transformation, we built a pipeline named 'Multi-instrumentalistNet' (MI Net). At its base, the pipeline learns a discrete latent representation of various instruments music from log-spectrogram using a Vector Quantized Variational Autoencoder (VQ-VAE) with multi-band residual blocks. The pipeline is then trained along with an autoregressive prior conditioned on the musician's body keypoints movements encoded by a recurrent neural network. Joint training of the prior with the body movements encoder succeeds in the disentanglement of the music into latent features indicating the musical components and the instrumental features. The latent space results in distributions that are clustered into distinct instruments from which new music can be generated. Furthermore, the VQ-VAE architecture supports detailed music generation with additional conditioning. We show that a Midi can further condition the latent space such that the pipeline will generate the exact content of the music being played by the instrument in the video. We evaluate MI Net on two datasets containing videos of 13 instruments and obtain generated music of reasonable audio quality, easily associated with the corresponding instrument, and consistent with the music audio content.","['https://openalex.org/W2919624000', 'https://openalex.org/W2948211236', 'https://openalex.org/W2963799213', 'https://openalex.org/W2883853252', 'https://openalex.org/W3101943858', 'https://openalex.org/W2962795401', 'https://openalex.org/W2962865004', 'https://openalex.org/W2963782041', 'https://openalex.org/W2964109005', 'https://openalex.org/W2910577860', 'https://openalex.org/W2963804063', 'https://openalex.org/W2951004968', 'https://openalex.org/W3035250874', 'https://openalex.org/W2769811909', 'https://openalex.org/W3017343282', 'https://openalex.org/W2950547518', 'https://openalex.org/W2784435047', 'https://openalex.org/W2963403868', 'https://openalex.org/W2903831537', 'https://openalex.org/W3037391061', 'https://openalex.org/W2981851635', 'https://openalex.org/W2738406145', 'https://openalex.org/W2979157532', 'https://openalex.org/W2556930864', 'https://openalex.org/W3045082460', 'https://openalex.org/W3034548564', 'https://openalex.org/W2964345931', 'https://openalex.org/W2914217321', 'https://openalex.org/W3021164770', 'https://openalex.org/W2619697695', 'https://openalex.org/W3046890131', 'https://openalex.org/W2787919227', 'https://openalex.org/W2970006822', 'https://openalex.org/W2963218389', 'https://openalex.org/W2511428026', 'https://openalex.org/W2584032004', 'https://openalex.org/W2962942158', 'https://openalex.org/W2963066677', 'https://openalex.org/W2963807156', 'https://openalex.org/W2949382160', 'https://openalex.org/W3102619627', 'https://openalex.org/W3016096605', 'https://openalex.org/W2899724567', 'https://openalex.org/W2995416527', 'https://openalex.org/W2962756039']",2020-12-07
https://openalex.org/W3036013631,https://doi.org/10.48550/arxiv.2006.10553,Artificial Musical Intelligence: A Survey,"Computers have been used to analyze and create music since they were first introduced in the 1950s and 1960s. Beginning in the late 1990s, the rise of the Internet and large scale platforms for music recommendation and retrieval have made music an increasingly prevalent domain of machine learning and artificial intelligence research. While still nascent, several different approaches have been employed to tackle what may broadly be referred to as ""musical intelligence."" This article provides a definition of musical intelligence, introduces a taxonomy of its constituent components, and surveys the wide range of AI methods that can be, and have been, brought to bear in its pursuit, with a particular emphasis on machine learning methods.","['https://openalex.org/W2099760476', 'https://openalex.org/W2397090180', 'https://openalex.org/W2296610835', 'https://openalex.org/W2054698853', 'https://openalex.org/W1566941822', 'https://openalex.org/W2165766797', 'https://openalex.org/W1490654608', 'https://openalex.org/W166844666', 'https://openalex.org/W360531859', 'https://openalex.org/W2405967445', 'https://openalex.org/W2401800292', 'https://openalex.org/W2166010241', 'https://openalex.org/W2395462034', 'https://openalex.org/W2128838222', 'https://openalex.org/W2294414398', 'https://openalex.org/W2572771584', 'https://openalex.org/W2131471382', 'https://openalex.org/W2399222761', 'https://openalex.org/W1982295531', 'https://openalex.org/W774637286', 'https://openalex.org/W107407649', 'https://openalex.org/W1797428404', 'https://openalex.org/W146927993', 'https://openalex.org/W1980754243', 'https://openalex.org/W1819710477', 'https://openalex.org/W2153628411', 'https://openalex.org/W2071133755', 'https://openalex.org/W171045637', 'https://openalex.org/W2404633046', 'https://openalex.org/W2072828027', 'https://openalex.org/W2008765103', 'https://openalex.org/W1556624199', 'https://openalex.org/W2891317861', 'https://openalex.org/W1982446897', 'https://openalex.org/W2187193317', 'https://openalex.org/W2124386111', 'https://openalex.org/W2400422027', 'https://openalex.org/W2124867748', 'https://openalex.org/W2015108573', 'https://openalex.org/W2296587773', 'https://openalex.org/W2949408457', 'https://openalex.org/W104309722', 'https://openalex.org/W2027576066', 'https://openalex.org/W2087137043', 'https://openalex.org/W2103662883', 'https://openalex.org/W2117939210', 'https://openalex.org/W2399467590', 'https://openalex.org/W2396376630', 'https://openalex.org/W2096178388', 'https://openalex.org/W1882839442', 'https://openalex.org/W1526676615', 'https://openalex.org/W2978495754', 'https://openalex.org/W1499204086', 'https://openalex.org/W2414894569', 'https://openalex.org/W1586966408', 'https://openalex.org/W1835965142', 'https://openalex.org/W93815243', 'https://openalex.org/W2405190343', 'https://openalex.org/W2395935897', 'https://openalex.org/W2340829226', 'https://openalex.org/W179611734', 'https://openalex.org/W2145027674', 'https://openalex.org/W2186196505', 'https://openalex.org/W1863372652', 'https://openalex.org/W1508756421', 'https://openalex.org/W2394782790', 'https://openalex.org/W15066456', 'https://openalex.org/W2125838338', 'https://openalex.org/W2128309385', 'https://openalex.org/W2118104219', 'https://openalex.org/W2054464225', 'https://openalex.org/W1531663008', 'https://openalex.org/W30579322', 'https://openalex.org/W190299453', 'https://openalex.org/W2295137881', 'https://openalex.org/W2296557244', 'https://openalex.org/W2403402716', 'https://openalex.org/W2394843640', 'https://openalex.org/W2579424589', 'https://openalex.org/W1724148608', 'https://openalex.org/W1866281937', 'https://openalex.org/W2398809928', 'https://openalex.org/W1510343741', 'https://openalex.org/W2140304104', 'https://openalex.org/W1792626010', 'https://openalex.org/W2407116345', 'https://openalex.org/W2463338764', 'https://openalex.org/W1486009449', 'https://openalex.org/W1650097435', 'https://openalex.org/W60920252', 'https://openalex.org/W2144383041', 'https://openalex.org/W159705460', 'https://openalex.org/W2109507664', 'https://openalex.org/W2007523307', 'https://openalex.org/W150213279', 'https://openalex.org/W2170191458', 'https://openalex.org/W2133261681', 'https://openalex.org/W2130887554', 'https://openalex.org/W2579254592', 'https://openalex.org/W1994584159', 'https://openalex.org/W2293498979', 'https://openalex.org/W2402396424', 'https://openalex.org/W2295991281', 'https://openalex.org/W87172748', 'https://openalex.org/W2137319814', 'https://openalex.org/W2016215128', 'https://openalex.org/W1996533415', 'https://openalex.org/W1600581496', 'https://openalex.org/W2396767506', 'https://openalex.org/W170769362', 'https://openalex.org/W1942724189', 'https://openalex.org/W2573660531', 'https://openalex.org/W1973961215', 'https://openalex.org/W2106330465', 'https://openalex.org/W2104299881', 'https://openalex.org/W2293269068', 'https://openalex.org/W2174291476', 'https://openalex.org/W2891815651', 'https://openalex.org/W2112446559', 'https://openalex.org/W2746068898', 'https://openalex.org/W2963985773', 'https://openalex.org/W179394430', 'https://openalex.org/W1528140509', 'https://openalex.org/W2398851682', 'https://openalex.org/W2188492526', 'https://openalex.org/W2153603270', 'https://openalex.org/W2396658366', 'https://openalex.org/W2061912997', 'https://openalex.org/W2296751288', 'https://openalex.org/W2108672713', 'https://openalex.org/W2394827912', 'https://openalex.org/W31231921', 'https://openalex.org/W2097807493', 'https://openalex.org/W85317909', 'https://openalex.org/W2111066885', 'https://openalex.org/W2294938120', 'https://openalex.org/W2767858146', 'https://openalex.org/W1501340791', 'https://openalex.org/W2295104115', 'https://openalex.org/W1989445502', 'https://openalex.org/W2407888529', 'https://openalex.org/W1209758405', 'https://openalex.org/W2405905263', 'https://openalex.org/W1977970897', 'https://openalex.org/W2398352100', 'https://openalex.org/W2400416460', 'https://openalex.org/W112407144', 'https://openalex.org/W2293027038', 'https://openalex.org/W2407813070', 'https://openalex.org/W2405180489', 'https://openalex.org/W2577917979', 'https://openalex.org/W113342895', 'https://openalex.org/W291184432', 'https://openalex.org/W1880262756', 'https://openalex.org/W1519835462', 'https://openalex.org/W133467950', 'https://openalex.org/W3123961192', 'https://openalex.org/W2395752715', 'https://openalex.org/W2401749995', 'https://openalex.org/W72567820', 'https://openalex.org/W1891885937', 'https://openalex.org/W2406901689', 'https://openalex.org/W2018717593', 'https://openalex.org/W2295807339', 'https://openalex.org/W2140013053', 'https://openalex.org/W2951986497', 'https://openalex.org/W2147880316', 'https://openalex.org/W2396406079', 'https://openalex.org/W1940617452', 'https://openalex.org/W2159870523', 'https://openalex.org/W2408401389', 'https://openalex.org/W2108480209', 'https://openalex.org/W2152756914', 'https://openalex.org/W2165326485', 'https://openalex.org/W2171960770', 'https://openalex.org/W2398741234', 'https://openalex.org/W2295762748', 'https://openalex.org/W2295460171', 'https://openalex.org/W2898887637', 'https://openalex.org/W38176230', 'https://openalex.org/W1977444015', 'https://openalex.org/W1597258651', 'https://openalex.org/W2342458310', 'https://openalex.org/W1880726444', 'https://openalex.org/W2117160730', 'https://openalex.org/W24420621', 'https://openalex.org/W128872657', 'https://openalex.org/W2398080269', 'https://openalex.org/W145820976', 'https://openalex.org/W91413436', 'https://openalex.org/W180758017', 'https://openalex.org/W2138445519', 'https://openalex.org/W2294451090', 'https://openalex.org/W129695050', 'https://openalex.org/W2962942158', 'https://openalex.org/W2883727669', 'https://openalex.org/W2401733840', 'https://openalex.org/W93221271', 'https://openalex.org/W2293188872', 'https://openalex.org/W2008056655', 'https://openalex.org/W2400840148', 'https://openalex.org/W2407187122', 'https://openalex.org/W2213382594', 'https://openalex.org/W2151159559', 'https://openalex.org/W106639881', 'https://openalex.org/W2293626685', 'https://openalex.org/W2552472274', 'https://openalex.org/W2108754343', 'https://openalex.org/W13237084', 'https://openalex.org/W1503560067', 'https://openalex.org/W1972484488', 'https://openalex.org/W1861596447', 'https://openalex.org/W2159423903', 'https://openalex.org/W2295247772', 'https://openalex.org/W2006796819', 'https://openalex.org/W1989318262', 'https://openalex.org/W2183080703', 'https://openalex.org/W1518164296', 'https://openalex.org/W2885802111', 'https://openalex.org/W2144414181', 'https://openalex.org/W76706658', 'https://openalex.org/W2294160770', 'https://openalex.org/W2963681776', 'https://openalex.org/W2419128894', 'https://openalex.org/W2294328484', 'https://openalex.org/W2106624428', 'https://openalex.org/W2902583091', 'https://openalex.org/W2101252291', 'https://openalex.org/W2026684966', 'https://openalex.org/W119492206', 'https://openalex.org/W2571082394', 'https://openalex.org/W2126410803', 'https://openalex.org/W2156012518', 'https://openalex.org/W2293251668', 'https://openalex.org/W2408350387', 'https://openalex.org/W2412242051', 'https://openalex.org/W2186445378', 'https://openalex.org/W2294891765', 'https://openalex.org/W1861213619', 'https://openalex.org/W87329255', 'https://openalex.org/W2407904607', 'https://openalex.org/W2001014535', 'https://openalex.org/W2063205814', 'https://openalex.org/W180220758', 'https://openalex.org/W2164877274', 'https://openalex.org/W1761485880', 'https://openalex.org/W2296447391', 'https://openalex.org/W2161969291', 'https://openalex.org/W2110880609', 'https://openalex.org/W94439923', 'https://openalex.org/W2169850486', 'https://openalex.org/W1971928988', 'https://openalex.org/W116902681', 'https://openalex.org/W2209061031', 'https://openalex.org/W2152397575', 'https://openalex.org/W2215054329', 'https://openalex.org/W2024595034', 'https://openalex.org/W189934255', 'https://openalex.org/W1221120433', 'https://openalex.org/W2137884954', 'https://openalex.org/W2024636109', 'https://openalex.org/W2395036407', 'https://openalex.org/W2407740810', 'https://openalex.org/W1494080199', 'https://openalex.org/W179928807', 'https://openalex.org/W1979474593', 'https://openalex.org/W2022475078', 'https://openalex.org/W90018894', 'https://openalex.org/W2060482235', 'https://openalex.org/W1587694965', 'https://openalex.org/W1967957118', 'https://openalex.org/W2055132753', 'https://openalex.org/W2126485319', 'https://openalex.org/W1506021957', 'https://openalex.org/W2020901563', 'https://openalex.org/W2407544556', 'https://openalex.org/W2404274215', 'https://openalex.org/W92099993']",2020-06-17
https://openalex.org/W3110624927,https://doi.org/10.48550/arxiv.2011.12596,MTCRNN: A multi-scale RNN for directed audio texture synthesis,"Audio textures are a subset of environmental sounds, often defined as having stable statistical characteristics within an adequately large window of time but may be unstructured locally. They include common everyday sounds such as from rain, wind, and engines. Given that these complex sounds contain patterns on multiple timescales, they are a challenge to model with traditional methods. We introduce a novel modelling approach for textures, combining recurrent neural networks trained at different levels of abstraction with a conditioning strategy that allows for user-directed synthesis. We demonstrate the model's performance on a variety of datasets, examine its performance on various metrics, and discuss some potential applications.","['https://openalex.org/W2950547518', 'https://openalex.org/W2905488776', 'https://openalex.org/W2962942158', 'https://openalex.org/W3021164770', 'https://openalex.org/W2953331651', 'https://openalex.org/W2963889406', 'https://openalex.org/W1579853615', 'https://openalex.org/W2924273262', 'https://openalex.org/W2949382160', 'https://openalex.org/W2951986497', 'https://openalex.org/W2512517145']",2020-11-25
https://openalex.org/W3216433667,https://doi.org/10.48550/arxiv.2111.12701,Unleashing Transformers: Parallel Token Prediction with Discrete Absorbing Diffusion for Fast High-Resolution Image Generation from Vector-Quantized Codes,"Whilst diffusion probabilistic models can generate high quality image content, key limitations remain in terms of both generating high-resolution imagery and their associated high computational requirements. Recent Vector-Quantized image models have overcome this limitation of image resolution but are prohibitively slow and unidirectional as they generate tokens via element-wise autoregressive sampling from the prior. By contrast, in this paper we propose a novel discrete diffusion probabilistic model prior which enables parallel prediction of Vector-Quantized tokens by using an unconstrained Transformer architecture as the backbone. During training, tokens are randomly masked in an order-agnostic manner and the Transformer learns to predict the original tokens. This parallelism of Vector-Quantized token prediction in turn facilitates unconditional generation of globally consistent high-resolution and diverse imagery at a fraction of the computational expense. In this manner, we can generate image resolutions exceeding that of the original training set samples whilst additionally provisioning per-image likelihood estimates (in a departure from generative adversarial approaches). Our approach achieves state-of-the-art results in terms of Density (LSUN Bedroom: 1.51; LSUN Churches: 1.12; FFHQ: 1.20) and Coverage (LSUN Bedroom: 0.83; LSUN Churches: 0.73; FFHQ: 0.80), and performs competitively on FID (LSUN Bedroom: 3.64; LSUN Churches: 4.07; FFHQ: 6.11) whilst offering advantages in terms of both computation and reduced training set requirements.","['https://openalex.org/W3176276772', 'https://openalex.org/W2962942158', 'https://openalex.org/W3098510582', 'https://openalex.org/W2963121829', 'https://openalex.org/W2963799213', 'https://openalex.org/W3153819643', 'https://openalex.org/W3177150392', 'https://openalex.org/W3168470634', 'https://openalex.org/W3169733862', 'https://openalex.org/W2964122153', 'https://openalex.org/W2940744433', 'https://openalex.org/W3114951884', 'https://openalex.org/W2116064496', 'https://openalex.org/W2267126114', 'https://openalex.org/W2970774025', 'https://openalex.org/W2963836885', 'https://openalex.org/W2971128425', 'https://openalex.org/W3035574324', 'https://openalex.org/W2805984778', 'https://openalex.org/W3175528029', 'https://openalex.org/W2893749619', 'https://openalex.org/W3034573343', 'https://openalex.org/W3102021454', 'https://openalex.org/W3036669348', 'https://openalex.org/W3128876955', 'https://openalex.org/W3118605064', 'https://openalex.org/W3118552741', 'https://openalex.org/W3100572490', 'https://openalex.org/W3190965961', 'https://openalex.org/W3135058862', 'https://openalex.org/W2962897886', 'https://openalex.org/W2963223306', 'https://openalex.org/W2962736171', 'https://openalex.org/W2922772346', 'https://openalex.org/W3173562028', 'https://openalex.org/W3166862086', 'https://openalex.org/W3138478053', 'https://openalex.org/W3167002466', 'https://openalex.org/W1959608418', 'https://openalex.org/W2963636093', 'https://openalex.org/W3166164913', 'https://openalex.org/W3117450517', 'https://openalex.org/W2587284713', 'https://openalex.org/W3111551570', 'https://openalex.org/W3167710116', 'https://openalex.org/W2962750131', 'https://openalex.org/W1583776211', 'https://openalex.org/W2994964253', 'https://openalex.org/W2971074500', 'https://openalex.org/W3131922516', 'https://openalex.org/W3034723908', 'https://openalex.org/W2962785568', 'https://openalex.org/W2962760235', 'https://openalex.org/W2963341956', 'https://openalex.org/W3129576130', 'https://openalex.org/W3204937802', 'https://openalex.org/W3121345697', 'https://openalex.org/W2963403868', 'https://openalex.org/W2548228487', 'https://openalex.org/W2902630600', 'https://openalex.org/W3127792964', 'https://openalex.org/W3135367836', 'https://openalex.org/W3088059392', 'https://openalex.org/W3196163807', 'https://openalex.org/W2547875792', 'https://openalex.org/W2964121744', 'https://openalex.org/W2962770929', 'https://openalex.org/W2963641970', 'https://openalex.org/W967544008', 'https://openalex.org/W3120243996', 'https://openalex.org/W2099471712', 'https://openalex.org/W2908747729', 'https://openalex.org/W3181640983', 'https://openalex.org/W2963139417']",2021-11-24
https://openalex.org/W2958816042,https://doi.org/10.5281/zenodo.2883724,Exploring Conditioning for Generative Music Systems with Human-Interpretable Controls,"Performance RNN is a machine-learning system designed primarily for the generation of solo piano performances using an event-based (rather than audio) representation. More specifically, Performance RNN is a long short-term memory (LSTM) based recurrent neural network that models polyphonic music with expressive timing and dynamics (Oore et al., 2018). The neural network uses a simple language model based on the Musical Instrument Digital Interface (MIDI) file format. Performance RNN is trained on the <em>e-Piano Junior Competition Dataset </em>(International Piano e-Competition 2018), a collection of solo piano performances by expert pianists. As an artistic tool, one of the limitations of the original model has been the lack of useable controls. The standard form of Performance RNN can generate interesting pieces, but little control is provided over <em>what </em>specifically is generated. This paper explores a set of conditioning-based controls used to influence the generation process. Below, a brief description of the samples that accompany the paper are given: <strong>sample_01.mp3: </strong>Composer latitude, longitude, and birth year conditioning. Conditioned on Germany in 1685. <strong>sample_02.mp3: </strong>Composer latitude, longitude, and birth year conditioning. Conditioned on Poland in 1810. <strong>sample_03.mp3: </strong>Composer latitude, longitude, and birth year conditioning. Conditioned on Poland in 1810. <strong>sample_04.mp3: </strong>Tempo keyword conditioning. Conditioned to begin at adagio (very slow) and end at presto (very fast). <strong>sample_05.mp3: </strong>Velocity conditioning. Conditioned to begin quiet, grow loud, and then end quietly. <strong>sample_06.mp3: </strong>Velocity conditioning. Conditioned to begin quiet, grow loud, and then end quietly. <strong>sample_07.mp3: </strong>Relative-position and composer conditioning. Conditioned to be in the style of Debussy and the beginning of a piece. <strong>sample_08.mp3: </strong>Relative-position and composer conditioning. Conditioned to be in the style of Bach and the ending of a piece. <strong>sample_09.mp3: </strong>Tempo keyword and velocity conditioning. Conditioned to begin slow and quiet and then become fast and loud. <strong>sample_10.mp3: </strong>Composer conditioning. Conditioned on Ravel. <strong>sample_11.mp3: </strong>Composer conditioning. Conditioned on Schumann. <strong>sample_12.mp3: </strong>Velocity conditioning. Conditioned to begin quiet, grow loud, and then end quietly. <strong>sample_13.mp3: </strong>Birth year conditioning. Conditioned on the 1600s. <strong>sample_14.mp3: </strong>Composer latitude, longitude, and birth year conditioning. Conditioned on Spain in 1860.","['https://openalex.org/W3098518646', 'https://openalex.org/W2963408210', 'https://openalex.org/W3122518304', 'https://openalex.org/W2963032576', 'https://openalex.org/W2747023714', 'https://openalex.org/W2963889406', 'https://openalex.org/W2962942158', 'https://openalex.org/W2031410675']",2019-05-17
https://openalex.org/W2996019936,,"Deep Learning for Music Composition: Generation, Recommendation and Control","Technology has always helped expand the range of musical expression, from the fortepiano to synthesizers to electronic sequencers. Could machine learning further extend human creativity? We explore three ways deep learning supports the creative process: generation, recommendation, and control. Generative models can synthesize stylistic idioms, enabling artists to explore a wider palette of possibilities. Recommendation tools can assist artists in curation. Better model control helps artists stay in the creative loop. Furthermore, this control could take place at one or more musically-meaningful levels -- the score, the performance, or timbre -- or on a non-musical level, such as a subjective quality like “scary.” This dissertation posits that deep learning models designed to better match the structure of music can generate, recommend and provide control in the creative process, making music composition more accessible. I describe four projects to support this statement. AdaptiveKnobs uses Gaussian Processes to capture the nonlinear multimodal relationship between low-level sound synthesis parameters and perceived sound qualities. By using active learning, we assist sound designers in defining their own intuitive knobs by querying them on sounds that the model expects to improve the controls most. ChordRipple uses Chord2Vec to learn chord embeddings for recommending creative substitutions and a Ripple mechanism to propagate changes, allowing novices to compose more adventurous chord progressions. Music Transformer uses self-attention mechanisms to capture the self-similarity structure of music, generating coherent expressive piano music from scratch. As the model processes composition and performance as one, improvisers can play an initial motif and have the model develop it in a coherent fashion. Coconet uses convolutions to capture pitch and temporal invariance. The generative model fills in arbitrarily-partial musical scores, allowing it to perform a wide range of musical tasks. The model uses Gibbs sampling to approximate how human composers improve their music through rewriting. Recently, Coconet powered the Bach Doodle, harmonizing more than 50 million melodies composed by users. We hope machine learning can enable new ways of approaching the creative process for both novices and musicians.","['https://openalex.org/W2962883855', 'https://openalex.org/W2288593361', 'https://openalex.org/W2115351511', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963857374', 'https://openalex.org/W2795278834', 'https://openalex.org/W1931432374', 'https://openalex.org/W2007144263', 'https://openalex.org/W2268617045', 'https://openalex.org/W2142996485', 'https://openalex.org/W2170111110', 'https://openalex.org/W2098617596', 'https://openalex.org/W2579406683', 'https://openalex.org/W2963575853', 'https://openalex.org/W2413794162', 'https://openalex.org/W2194775991', 'https://openalex.org/W2962990490', 'https://openalex.org/W2953318193', 'https://openalex.org/W2951263002', 'https://openalex.org/W2963341956', 'https://openalex.org/W2399706197', 'https://openalex.org/W2287082121', 'https://openalex.org/W2071638895', 'https://openalex.org/W1485787479', 'https://openalex.org/W2135181320', 'https://openalex.org/W648786980', 'https://openalex.org/W2898148140', 'https://openalex.org/W2038443646', 'https://openalex.org/W2753868141', 'https://openalex.org/W2964121744', 'https://openalex.org/W2047680133', 'https://openalex.org/W2115305054', 'https://openalex.org/W2962942158', 'https://openalex.org/W2963045354', 'https://openalex.org/W2314567018', 'https://openalex.org/W123301997', 'https://openalex.org/W2033765726', 'https://openalex.org/W2406069903', 'https://openalex.org/W2153628411', 'https://openalex.org/W2099471712', 'https://openalex.org/W2963403868', 'https://openalex.org/W2131241448', 'https://openalex.org/W2401203361', 'https://openalex.org/W1597199547', 'https://openalex.org/W147294073', 'https://openalex.org/W2963681776', 'https://openalex.org/W2019378856', 'https://openalex.org/W2115387076', 'https://openalex.org/W2898727200', 'https://openalex.org/W2951308445', 'https://openalex.org/W1591699755', 'https://openalex.org/W2397098640', 'https://openalex.org/W2053952057', 'https://openalex.org/W1746819321', 'https://openalex.org/W2140539195', 'https://openalex.org/W2408284391', 'https://openalex.org/W1498436455', 'https://openalex.org/W1490794914', 'https://openalex.org/W2129192849', 'https://openalex.org/W2153579005', 'https://openalex.org/W1976355379', 'https://openalex.org/W90651201', 'https://openalex.org/W2963418779', 'https://openalex.org/W2126485319', 'https://openalex.org/W2059927004', 'https://openalex.org/W2802023636', 'https://openalex.org/W2949117887', 'https://openalex.org/W2169003314', 'https://openalex.org/W2475287302', 'https://openalex.org/W2171186197', 'https://openalex.org/W2404198203', 'https://openalex.org/W2137619888', 'https://openalex.org/W3123961192']",2019-05-21
https://openalex.org/W3096431806,https://doi.org/10.48550/arxiv.2011.00773,Using a Bi-directional LSTM Model with Attention Mechanism trained on MIDI Data for Generating Unique Music,"Generating music is an interesting and challenging problem in the field of machine learning. Mimicking human creativity has been popular in recent years, especially in the field of computer vision and image processing. With the advent of GANs, it is possible to generate new similar images, based on trained data. But this cannot be done for music similarly, as music has an extra temporal dimension. So it is necessary to understand how music is represented in digital form. When building models that perform this generative task, the learning and generation part is done in some high-level representation such as MIDI (Musical Instrument Digital Interface) or scores. This paper proposes a bi-directional LSTM (Long short-term memory) model with attention mechanism capable of generating similar type of music based on MIDI data. The music generated by the model follows the theme/style of the music the model is trained on. Also, due to the nature of MIDI, the tempo, instrument, and other parameters can be defined, and changed, post generation.","['https://openalex.org/W2952008036', 'https://openalex.org/W2962942158', 'https://openalex.org/W2461230277', 'https://openalex.org/W2949382160', 'https://openalex.org/W1556624199', 'https://openalex.org/W2772418616', 'https://openalex.org/W3036981371', 'https://openalex.org/W2111273501', 'https://openalex.org/W152093514', 'https://openalex.org/W1556219185', 'https://openalex.org/W2984106626', 'https://openalex.org/W2067329295', 'https://openalex.org/W2096027770', 'https://openalex.org/W2072463679', 'https://openalex.org/W1973401750', 'https://openalex.org/W1982571850', 'https://openalex.org/W1577013761', 'https://openalex.org/W2778460379', 'https://openalex.org/W2031410675', 'https://openalex.org/W2396726671', 'https://openalex.org/W2333735244', 'https://openalex.org/W1572665971', 'https://openalex.org/W2963782041', 'https://openalex.org/W1977731574', 'https://openalex.org/W1991994073', 'https://openalex.org/W2898148140', 'https://openalex.org/W2079283699', 'https://openalex.org/W2035032881', 'https://openalex.org/W2328988907', 'https://openalex.org/W113578986', 'https://openalex.org/W2558151185']",2020-11-02
https://openalex.org/W2990991171,https://doi.org/10.48550/arxiv.1911.11879,SchrödingeRNN: Generative Modeling of Raw Audio as a Continuously Observed Quantum State,"We introduce SchrödingeRNN, a quantum inspired generative model for raw audio. Audio data is wave-like and is sampled from a continuous signal. Although generative modelling of raw audio has made great strides lately, relational inductive biases relevant to these two characteristics are mostly absent from models explored to date. Quantum Mechanics is a natural source of probabilistic models of wave behaviour. Our model takes the form of a stochastic Schrödinger equation describing the continuous time measurement of a quantum system, and is equivalent to the continuous Matrix Product State (cMPS) representation of wavefunctions in one dimensional many-body systems. This constitutes a deep autoregressive architecture in which the systems state is a latent representation of the past observations. We test our model on synthetic data sets of stationary and non-stationary signals. This is the first time cMPS are used in machine learning.","['https://openalex.org/W2949382160', 'https://openalex.org/W2900208959', 'https://openalex.org/W3100068159', 'https://openalex.org/W2030299272', 'https://openalex.org/W2963139417', 'https://openalex.org/W2951535099', 'https://openalex.org/W2551156993', 'https://openalex.org/W1530682388', 'https://openalex.org/W2000215650', 'https://openalex.org/W2910259907', 'https://openalex.org/W149300422', 'https://openalex.org/W592244745', 'https://openalex.org/W2584032004', 'https://openalex.org/W2962942158', 'https://openalex.org/W3016208849', 'https://openalex.org/W2617524636', 'https://openalex.org/W3099497510', 'https://openalex.org/W2917217327', 'https://openalex.org/W2904390264', 'https://openalex.org/W2086731084', 'https://openalex.org/W2963755523', 'https://openalex.org/W3099956647', 'https://openalex.org/W2753545915', 'https://openalex.org/W2953511997', 'https://openalex.org/W2951542569']",2019-11-26
https://openalex.org/W3163985612,https://doi.org/10.5626/jok.2021.48.8.940,Deep Neural Networks and End-to-End Learning for Audio Compression,"Recent achievements in end-to-end deep learning have encouraged the exploration of tasks dealing with highly structured data with unified deep network models. Having such models for compressing audio signals has been challenging since it requires discrete representations that are not easy to train with end-to-end backpropagation. In this paper, we present an end-to-end deep learning approach that combines recurrent neural networks (RNNs) within the training strategy of variational autoencoders (VAEs) with a binary representation of the latent space. We apply a reparametrization trick for the Bernoulli distribution for the discrete representations, which allows smooth backpropagation. In addition, our approach allows the separation of the encoder and decoder, which is necessary for compression tasks. To our best knowledge, this is the first end-to-end learning for a single audio compression model with RNNs, and our model achieves a Signal to Distortion Ratio (SDR) of 20.54.","['https://openalex.org/W2963091184', 'https://openalex.org/W3021164770', 'https://openalex.org/W2800934755', 'https://openalex.org/W1884859883', 'https://openalex.org/W2753738274', 'https://openalex.org/W3093390800', 'https://openalex.org/W2064675550', 'https://openalex.org/W2950237361', 'https://openalex.org/W3118215210', 'https://openalex.org/W592244745', 'https://openalex.org/W2963182577', 'https://openalex.org/W2962942158', 'https://openalex.org/W2160815625', 'https://openalex.org/W2963619462', 'https://openalex.org/W2963799213', 'https://openalex.org/W2808041683', 'https://openalex.org/W2964339599', 'https://openalex.org/W1959608418', 'https://openalex.org/W2157331557', 'https://openalex.org/W2963279312', 'https://openalex.org/W1498436455']",2021-08-31
https://openalex.org/W3192483709,https://doi.org/10.48550/arxiv.2108.01043,Musical Speech: A Transformer-based Composition Tool,"In this paper, we propose a new compositional tool that will generate a musical outline of speech recorded/provided by the user for use as a musical building block in their compositions. The tool allows any user to use their own speech to generate musical material, while still being able to hear the direct connection between their recorded speech and the resulting music. The tool is built on our proposed pipeline. This pipeline begins with speech-based signal processing, after which some simple musical heuristics are applied, and finally these pre-processed signals are passed through Transformer models trained on new musical tasks. We illustrate the effectiveness of our pipeline -- which does not require a paired dataset for training -- through examples of music created by musicians making use of our tool.","['https://openalex.org/W2891794946', 'https://openalex.org/W3122518304', 'https://openalex.org/W2943532091', 'https://openalex.org/W3036120435', 'https://openalex.org/W2049515993', 'https://openalex.org/W3040087772', 'https://openalex.org/W3098518646', 'https://openalex.org/W1560013842', 'https://openalex.org/W3082274269', 'https://openalex.org/W2611369375', 'https://openalex.org/W2009254313', 'https://openalex.org/W2096774922', 'https://openalex.org/W2753868141', 'https://openalex.org/W1875231349', 'https://openalex.org/W2139301061', 'https://openalex.org/W3000389243', 'https://openalex.org/W3031000691', 'https://openalex.org/W2803963372', 'https://openalex.org/W2588767398', 'https://openalex.org/W2963408210', 'https://openalex.org/W2963403868', 'https://openalex.org/W3021164770', 'https://openalex.org/W2919624000', 'https://openalex.org/W1563089615', 'https://openalex.org/W2963341956', 'https://openalex.org/W2898148140', 'https://openalex.org/W1992153276', 'https://openalex.org/W2281488037', 'https://openalex.org/W3011411500', 'https://openalex.org/W2566129194', 'https://openalex.org/W2950813464', 'https://openalex.org/W1494198834', 'https://openalex.org/W2091425152', 'https://openalex.org/W3092135915', 'https://openalex.org/W3016714241', 'https://openalex.org/W3161779466', 'https://openalex.org/W2142518024', 'https://openalex.org/W2794719876', 'https://openalex.org/W3029764700', 'https://openalex.org/W1965635292', 'https://openalex.org/W2744457411', 'https://openalex.org/W5537492', 'https://openalex.org/W2948211236', 'https://openalex.org/W2962942158', 'https://openalex.org/W2471520273', 'https://openalex.org/W1997876077', 'https://openalex.org/W1966264494', 'https://openalex.org/W1986845175', 'https://openalex.org/W2604567995', 'https://openalex.org/W2131864930']",2021-08-02
https://openalex.org/W3207217026,https://doi.org/10.48550/arxiv.2110.08787,PixelPyramids: Exact Inference Models from Lossless Image Pyramids,"Autoregressive models are a class of exact inference approaches with highly flexible functional forms, yielding state-of-the-art density estimates for natural images. Yet, the sequential ordering on the dimensions makes these models computationally expensive and limits their applicability to low-resolution imagery. In this work, we propose Pixel-Pyramids, a block-autoregressive approach employing a lossless pyramid decomposition with scale-specific representations to encode the joint distribution of image pixels. Crucially, it affords a sparser dependency structure compared to fully autoregressive approaches. Our PixelPyramids yield state-of-the-art results for density estimation on various image datasets, especially for high-resolution data. For CelebA-HQ 1024 x 1024, we observe that the density estimates (in terms of bits/dim) are improved to ~44% of the baseline despite sampling speeds superior even to easily parallelizable flow-based models.","['https://openalex.org/W2963428348', 'https://openalex.org/W2964020555', 'https://openalex.org/W3095495550', 'https://openalex.org/W2970945269', 'https://openalex.org/W2963540976', 'https://openalex.org/W2902630600', 'https://openalex.org/W2963447011', 'https://openalex.org/W2911827218', 'https://openalex.org/W2964121744', 'https://openalex.org/W3137078226', 'https://openalex.org/W1810943226', 'https://openalex.org/W2064675550', 'https://openalex.org/W2267126114', 'https://openalex.org/W2962942158', 'https://openalex.org/W2065585567', 'https://openalex.org/W2963292439', 'https://openalex.org/W3172084791', 'https://openalex.org/W2963139417', 'https://openalex.org/W2103504761', 'https://openalex.org/W1901129140', 'https://openalex.org/W2963066125', 'https://openalex.org/W648143168', 'https://openalex.org/W2117539524', 'https://openalex.org/W2912298597', 'https://openalex.org/W2099471712', 'https://openalex.org/W2962897886', 'https://openalex.org/W2963636093', 'https://openalex.org/W2769810959', 'https://openalex.org/W3035586199', 'https://openalex.org/W967544008', 'https://openalex.org/W2097039814', 'https://openalex.org/W2587284713', 'https://openalex.org/W1959608418', 'https://openalex.org/W2962695743', 'https://openalex.org/W2962750131', 'https://openalex.org/W3118608800', 'https://openalex.org/W2963568578', 'https://openalex.org/W2971074500', 'https://openalex.org/W2963866658', 'https://openalex.org/W2962760235', 'https://openalex.org/W2964122153', 'https://openalex.org/W3015028689', 'https://openalex.org/W2772211716', 'https://openalex.org/W2893749619', 'https://openalex.org/W2964307104', 'https://openalex.org/W2067077161']",2021-10-17
https://openalex.org/W2940544976,https://doi.org/10.21437/interspeech.2019-2904,The Zero Resource Speech Challenge 2019: TTS Without T,"We present the Zero Resource Speech Challenge 2019, which proposes to build a\nspeech synthesizer without any text or phonetic labels: hence, TTS without T\n(text-to-speech without text). We provide raw audio for a target voice in an\nunknown language (the Voice dataset), but no alignment, text or labels.\nParticipants must discover subword units in an unsupervised way (using the Unit\nDiscovery dataset) and align them to the voice recordings in a way that works\nbest for the purpose of synthesizing novel utterances from novel speakers,\nsimilar to the target speaker's voice. We describe the metrics used for\nevaluation, a baseline system consisting of unsupervised subword unit discovery\nplus a standard TTS system, and a topline TTS using gold phoneme\ntranscriptions. We present an overview of the 19 submitted systems from 10\nteams and discuss the main results.\n","['https://openalex.org/W3125709657', 'https://openalex.org/W2962693497', 'https://openalex.org/W2963830550', 'https://openalex.org/W1524333225', 'https://openalex.org/W2892140764', 'https://openalex.org/W2134202996', 'https://openalex.org/W2964243274', 'https://openalex.org/W2972867623', 'https://openalex.org/W2963799213', 'https://openalex.org/W2774848319', 'https://openalex.org/W2962699523', 'https://openalex.org/W2973013862', 'https://openalex.org/W2947445680', 'https://openalex.org/W2766812927', 'https://openalex.org/W2963620343', 'https://openalex.org/W2950414763', 'https://openalex.org/W2547039119', 'https://openalex.org/W2598638573', 'https://openalex.org/W2745710152', 'https://openalex.org/W2519091744', 'https://openalex.org/W2972374322', 'https://openalex.org/W2584032004', 'https://openalex.org/W2964135678', 'https://openalex.org/W2532494225', 'https://openalex.org/W2347098582', 'https://openalex.org/W2972964185', 'https://openalex.org/W2346964103', 'https://openalex.org/W2787447541', 'https://openalex.org/W2964115348', 'https://openalex.org/W2020607164']",2019-09-13
https://openalex.org/W3118753411,https://doi.org/10.1109/taslp.2021.3049336,Pretraining Techniques for Sequence-to-Sequence Voice Conversion,"Sequence-to-sequence (seq2seq) voice conversion (VC) models are attractive owing to their ability to convert prosody. Nonetheless, without sufficient data, seq2seq VC models can suffer from unstable training and mispronunciation problems in the converted speech, thus far from practical. To tackle these shortcomings, we propose to transfer knowledge from other speech processing tasks where large-scale corpora are easily available, typically text-to-speech (TTS) and automatic speech recognition (ASR). We argue that VC models initialized with such pretrained ASR or TTS model parameters can generate effective hidden representations for high-fidelity, highly intelligible converted speech. In this work, we examine our proposed method in a parallel, one-to-one setting. We employed recurrent neural network (RNN)-based and Transformer based models, and through systematical experiments, we demonstrate the effectiveness of the pretraining scheme and the superiority of Transformer based models over RNN-based models in terms of intelligibility, naturalness, and similarity.","['https://openalex.org/W6762287338', 'https://openalex.org/W2962780374', 'https://openalex.org/W1494198834', 'https://openalex.org/W2749651610', 'https://openalex.org/W2786868129', 'https://openalex.org/W3015338123', 'https://openalex.org/W2147768505', 'https://openalex.org/W3034420534', 'https://openalex.org/W2107860279', 'https://openalex.org/W3025844872', 'https://openalex.org/W2518172956', 'https://openalex.org/W2963035245', 'https://openalex.org/W2473388484', 'https://openalex.org/W2972818416', 'https://openalex.org/W2996414377', 'https://openalex.org/W2972970915', 'https://openalex.org/W6761051834', 'https://openalex.org/W2973142754', 'https://openalex.org/W2963609956', 'https://openalex.org/W6623517193', 'https://openalex.org/W2964243274', 'https://openalex.org/W2767052532', 'https://openalex.org/W2972894903', 'https://openalex.org/W6780226713', 'https://openalex.org/W2973049979', 'https://openalex.org/W6737778391', 'https://openalex.org/W2903739847', 'https://openalex.org/W3095990227', 'https://openalex.org/W2892009249', 'https://openalex.org/W3016160783', 'https://openalex.org/W6603838645', 'https://openalex.org/W2120605154', 'https://openalex.org/W2156142001', 'https://openalex.org/W2475287302', 'https://openalex.org/W2962739339', 'https://openalex.org/W6702130928', 'https://openalex.org/W2963026768', 'https://openalex.org/W6755207826', 'https://openalex.org/W2972313557', 'https://openalex.org/W2938583109', 'https://openalex.org/W6739901393', 'https://openalex.org/W6765579056', 'https://openalex.org/W2963808252', 'https://openalex.org/W3096864844', 'https://openalex.org/W3006777338', 'https://openalex.org/W2972999331', 'https://openalex.org/W2892734764', 'https://openalex.org/W6775882176', 'https://openalex.org/W6771070734', 'https://openalex.org/W2901254300', 'https://openalex.org/W2160815625', 'https://openalex.org/W2117539524', 'https://openalex.org/W6682132143', 'https://openalex.org/W6682778277', 'https://openalex.org/W2102605133', 'https://openalex.org/W6777080322', 'https://openalex.org/W1536680647', 'https://openalex.org/W639708223', 'https://openalex.org/W1903029394', 'https://openalex.org/W2963150697', 'https://openalex.org/W2049686551', 'https://openalex.org/W2471520273', 'https://openalex.org/W6679434410', 'https://openalex.org/W6679436768', 'https://openalex.org/W2899877258', 'https://openalex.org/W1902237438', 'https://openalex.org/W2574092538', 'https://openalex.org/W2897353073', 'https://openalex.org/W6844194202', 'https://openalex.org/W3035202887', 'https://openalex.org/W3098557217', 'https://openalex.org/W6769196770', 'https://openalex.org/W2973157397', 'https://openalex.org/W3100270690', 'https://openalex.org/W3016011332', 'https://openalex.org/W2972374322', 'https://openalex.org/W2331128040', 'https://openalex.org/W2964308564', 'https://openalex.org/W2155541015', 'https://openalex.org/W2996383576', 'https://openalex.org/W3096567388', 'https://openalex.org/W2130942839', 'https://openalex.org/W3113687514', 'https://openalex.org/W3015669407', 'https://openalex.org/W2149933564', 'https://openalex.org/W2995435108', 'https://openalex.org/W4297808394', 'https://openalex.org/W2842511635', 'https://openalex.org/W2979476256', 'https://openalex.org/W3007328579', 'https://openalex.org/W2133564696', 'https://openalex.org/W2187089797', 'https://openalex.org/W3027343259', 'https://openalex.org/W2959758584', 'https://openalex.org/W2964265128', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963341956', 'https://openalex.org/W3102905810', 'https://openalex.org/W3101689408', 'https://openalex.org/W4294375521', 'https://openalex.org/W3114691655', 'https://openalex.org/W2896457183', 'https://openalex.org/W4299518610', 'https://openalex.org/W854541894', 'https://openalex.org/W4385245566', 'https://openalex.org/W2613904329', 'https://openalex.org/W2949382160', 'https://openalex.org/W3099078140', 'https://openalex.org/W2613718673', 'https://openalex.org/W95152782', 'https://openalex.org/W3125709657']",2021-01-01
https://openalex.org/W3007068036,https://doi.org/10.1109/asru46091.2019.9003853,Speech-to-Speech Translation Between Untranscribed Unknown Languages,"In this paper, we explore a method for training speech-to-speech translation tasks without any transcription or linguistic supervision. Our proposed method consists of two steps: First, we train and generate discrete representation with unsupervised term discovery with a discrete quantized autoencoder. Second, we train a sequence-to-sequence model that directly maps the source language speech to the target languages discrete representation. Our proposed method can directly generate target speech without any auxiliary or pre-training steps with a source or target transcription. To the best of our knowledge, this is the first work that performed pure speech-to-speech translation between untranscribed unknown languages.","['https://openalex.org/W2963581463', 'https://openalex.org/W2963796886', 'https://openalex.org/W2962699523', 'https://openalex.org/W6681644459', 'https://openalex.org/W6678262379', 'https://openalex.org/W6898505805', 'https://openalex.org/W2963620343', 'https://openalex.org/W2940544976', 'https://openalex.org/W2947445680', 'https://openalex.org/W6763445112', 'https://openalex.org/W2752796333', 'https://openalex.org/W2025768430', 'https://openalex.org/W6748381668', 'https://openalex.org/W6757152577', 'https://openalex.org/W6608432165', 'https://openalex.org/W2466918907', 'https://openalex.org/W2120847449', 'https://openalex.org/W6736180185', 'https://openalex.org/W2161742089', 'https://openalex.org/W6732953234', 'https://openalex.org/W2962680099', 'https://openalex.org/W6679434410', 'https://openalex.org/W6973666849', 'https://openalex.org/W6623517193', 'https://openalex.org/W1902237438', 'https://openalex.org/W6679436768', 'https://openalex.org/W2064675550', 'https://openalex.org/W2884852625', 'https://openalex.org/W2949328740', 'https://openalex.org/W206967138', 'https://openalex.org/W1836465849', 'https://openalex.org/W2605131327', 'https://openalex.org/W2130942839', 'https://openalex.org/W2972495969', 'https://openalex.org/W2123301721', 'https://openalex.org/W2963609956', 'https://openalex.org/W2144600658', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964308564', 'https://openalex.org/W2133564696', 'https://openalex.org/W2949117887', 'https://openalex.org/W1921523184', 'https://openalex.org/W2101105183', 'https://openalex.org/W1522301498', 'https://openalex.org/W2789543585', 'https://openalex.org/W2963799213', 'https://openalex.org/W2964026424', 'https://openalex.org/W2786608204', 'https://openalex.org/W2972374322', 'https://openalex.org/W2906111771', 'https://openalex.org/W2191779130', 'https://openalex.org/W2973026522', 'https://openalex.org/W854541894']",2019-12-01
https://openalex.org/W3089425003,https://doi.org/10.1109/ijcnn48605.2020.9207145,Robust Training of Vector Quantized Bottleneck Models,"In this paper we demonstrate methods for reliable and efficient training of discrete representation using Vector-Quantized Variational Auto-Encoder models (VQ-VAEs). Discrete latent variable models have been shown to learn nontrivial representations of speech, applicable to unsupervised voice conversion and reaching state-of-the-art performance on unit discovery tasks. For unsupervised representation learning, they became viable alternatives to continuous latent variable models such as the Variational Auto-Encoder (VAE). However, training deep discrete variable models is challenging, due to the inherent non-differentiability of the discretization operation. In this paper we focus on VQ-VAE, a state-of-the-art discrete bottleneck model shown to perform on par with its continuous counterparts. It quantizes encoder outputs with on-line $k$-means clustering. We show that the codebook learning can suffer from poor initialization and non-stationarity of clustered encoder outputs. We demonstrate that these can be successfully overcome by increasing the learning rate for the codebook and periodic date-dependent codeword re-initialization. As a result, we achieve more robust training across different tasks, and significantly increase the usage of latent codewords even for large codebooks. This has practical benefit, for instance, in unsupervised representation learning, where large codebooks may lead to disentanglement of latent representations.","['https://openalex.org/W6675022971', 'https://openalex.org/W6735799286', 'https://openalex.org/W2119717200', 'https://openalex.org/W6719357382', 'https://openalex.org/W2888911345', 'https://openalex.org/W2750248772', 'https://openalex.org/W2347098582', 'https://openalex.org/W6729448088', 'https://openalex.org/W6634851332', 'https://openalex.org/W6774456908', 'https://openalex.org/W6638667902', 'https://openalex.org/W6696934422', 'https://openalex.org/W6668990524', 'https://openalex.org/W2119885577', 'https://openalex.org/W6751821244', 'https://openalex.org/W6762931180', 'https://openalex.org/W6729906282', 'https://openalex.org/W6640963894', 'https://openalex.org/W1902027874', 'https://openalex.org/W2123237149', 'https://openalex.org/W3100270690', 'https://openalex.org/W6753966407', 'https://openalex.org/W2752796333', 'https://openalex.org/W2972374322', 'https://openalex.org/W2940544976', 'https://openalex.org/W1970890968', 'https://openalex.org/W2972867623', 'https://openalex.org/W6637108112', 'https://openalex.org/W6687566353', 'https://openalex.org/W6631362777', 'https://openalex.org/W2127141656', 'https://openalex.org/W6631190155', 'https://openalex.org/W2145889472', 'https://openalex.org/W2086161653', 'https://openalex.org/W2547875792', 'https://openalex.org/W2964121744', 'https://openalex.org/W2602076750', 'https://openalex.org/W1522301498', 'https://openalex.org/W1583776211', 'https://openalex.org/W2973026522', 'https://openalex.org/W2464234964', 'https://openalex.org/W1524333225', 'https://openalex.org/W2962695963', 'https://openalex.org/W1686946872', 'https://openalex.org/W3049315473', 'https://openalex.org/W3136591341', 'https://openalex.org/W3125709657', 'https://openalex.org/W3008499099', 'https://openalex.org/W2193413348', 'https://openalex.org/W2804145368', 'https://openalex.org/W2949117887', 'https://openalex.org/W2100768664', 'https://openalex.org/W4293469690', 'https://openalex.org/W2951004968', 'https://openalex.org/W2979454998', 'https://openalex.org/W2267126114', 'https://openalex.org/W2887927938', 'https://openalex.org/W2971074500', 'https://openalex.org/W2963799213', 'https://openalex.org/W1959608418', 'https://openalex.org/W2293634267', 'https://openalex.org/W2073459066', 'https://openalex.org/W2557579533', 'https://openalex.org/W2953318193', 'https://openalex.org/W2947590261', 'https://openalex.org/W1836465849']",2020-07-01
https://openalex.org/W3096216486,https://doi.org/10.21437/interspeech.2020-3033,Transformer VQ-VAE for Unsupervised Unit Discovery and Speech Synthesis: ZeroSpeech 2020 Challenge,"In this paper, we report our submitted system for the ZeroSpeech 2020 challenge on Track 2019.The main theme in this challenge is to build a speech synthesizer without any textual information or phonetic labels.In order to tackle those challenges, we build a system that must address two major components such as 1) given speech audio, extract subword units in an unsupervised way and 2) resynthesize the audio from novel speakers.The system also needs to balance the codebook performance between the ABX error rate and the bitrate compression rate.Our main contribution here is we proposed Transformer-based VQ-VAE for unsupervised unit discovery and Transformerbased inverter for the speech synthesis given the extracted codebook.Additionally, we also explored several regularization methods to improve performance even further.","['https://openalex.org/W3125709657', 'https://openalex.org/W2979476256', 'https://openalex.org/W2963799213', 'https://openalex.org/W4288107125', 'https://openalex.org/W2120847449', 'https://openalex.org/W1522301498', 'https://openalex.org/W2191779130', 'https://openalex.org/W2789543585', 'https://openalex.org/W2547039119', 'https://openalex.org/W2972374322', 'https://openalex.org/W4385245566', 'https://openalex.org/W4394666973', 'https://openalex.org/W2972867623', 'https://openalex.org/W2940544976', 'https://openalex.org/W2906111771', 'https://openalex.org/W2025768430', 'https://openalex.org/W4295312788']",2020-10-25
https://openalex.org/W3097692357,https://doi.org/10.21437/interspeech.2020-1785,Unsupervised Acoustic Unit Representation Learning for Voice Conversion Using WaveNet Auto-Encoders,This is a repository copy of Unsupervised acoustic unit representation learning for voice conversion using WaveNet auto-encoders.,"['https://openalex.org/W1959608418', 'https://openalex.org/W2005708641', 'https://openalex.org/W2786608204', 'https://openalex.org/W2963609956', 'https://openalex.org/W2964243274', 'https://openalex.org/W2972867623', 'https://openalex.org/W2963799213', 'https://openalex.org/W2603777577', 'https://openalex.org/W2785860501', 'https://openalex.org/W2972841524', 'https://openalex.org/W2519091744', 'https://openalex.org/W2395899413', 'https://openalex.org/W2758785877', 'https://openalex.org/W2346964103', 'https://openalex.org/W2963691546', 'https://openalex.org/W2972943112', 'https://openalex.org/W1665214252', 'https://openalex.org/W2787447541', 'https://openalex.org/W2402146185', 'https://openalex.org/W2128032727', 'https://openalex.org/W2598638573', 'https://openalex.org/W3095361818', 'https://openalex.org/W2963830550', 'https://openalex.org/W2502312327', 'https://openalex.org/W2789543585', 'https://openalex.org/W2972659941', 'https://openalex.org/W4288107125', 'https://openalex.org/W1522301498', 'https://openalex.org/W2547039119', 'https://openalex.org/W4295312788', 'https://openalex.org/W3125709657', 'https://openalex.org/W2952161038', 'https://openalex.org/W2514741789', 'https://openalex.org/W2242818861', 'https://openalex.org/W2972374322', 'https://openalex.org/W2979476256', 'https://openalex.org/W2786902352', 'https://openalex.org/W2950414763']",2020-10-25
https://openalex.org/W3089767655,https://doi.org/10.1109/icip40778.2020.9191228,Learning Cross-Modal Representations for Language-Based Image Manipulation,"In this paper, we propose a generative architecture for manipulating images/scenes with natural language descriptions. This is a challenging task as the generative network is expected to perform the given text instruction without changing the non-affiliating contents of the input image. Two main drawbacks of the existing methods are their limitation of performing changes that would affect only a limited region and the inability of handling complex instructions. The proposed approach, designed to address these limitations initially uses two sets of networks to extract the image and text features respectively. Rather than a simple combination of these two modalities during the image manipulation process, we use an improved technique to compose image and text features. Additionally, the generative network utilizes similarity learning to improve text manipulation which also enforces only the text-relevant changes on the input image. Our experiments on CSS and Fashion Synthesis datasets show that the proposed approach performs remarkably well and outperforms the baseline frameworks in terms of R-precision and FID.","['https://openalex.org/W2963966654', 'https://openalex.org/W2963800363', 'https://openalex.org/W3008297462', 'https://openalex.org/W3012498027', 'https://openalex.org/W3025192522', 'https://openalex.org/W6744040789', 'https://openalex.org/W2905544595', 'https://openalex.org/W2972374322', 'https://openalex.org/W2911340057', 'https://openalex.org/W2963767194', 'https://openalex.org/W2964313012', 'https://openalex.org/W2993158499', 'https://openalex.org/W2885537606', 'https://openalex.org/W2964050021', 'https://openalex.org/W2194775991', 'https://openalex.org/W6765779288', 'https://openalex.org/W2561715562', 'https://openalex.org/W6631190155', 'https://openalex.org/W6713645886', 'https://openalex.org/W2798951647', 'https://openalex.org/W6685352114', 'https://openalex.org/W6758494947', 'https://openalex.org/W2884870985', 'https://openalex.org/W6728889164', 'https://openalex.org/W2802650881', 'https://openalex.org/W3012970712', 'https://openalex.org/W2984809863', 'https://openalex.org/W3012404734', 'https://openalex.org/W6755312952', 'https://openalex.org/W2964318046', 'https://openalex.org/W6631516269', 'https://openalex.org/W2962793481', 'https://openalex.org/W6678815747', 'https://openalex.org/W6779841522', 'https://openalex.org/W6752378368', 'https://openalex.org/W6755102824', 'https://openalex.org/W2970067499', 'https://openalex.org/W2964024144', 'https://openalex.org/W2962845008', 'https://openalex.org/W2150856297', 'https://openalex.org/W4301206121', 'https://openalex.org/W3037695135', 'https://openalex.org/W2963971656', 'https://openalex.org/W2893749619', 'https://openalex.org/W2125389028', 'https://openalex.org/W2949999304', 'https://openalex.org/W2405756170', 'https://openalex.org/W4320013936', 'https://openalex.org/W2530372461', 'https://openalex.org/W2963163163', 'https://openalex.org/W2964121744', 'https://openalex.org/W2173520492', 'https://openalex.org/W2952716587', 'https://openalex.org/W2804078698', 'https://openalex.org/W2963981733', 'https://openalex.org/W2963684088', 'https://openalex.org/W2963836885', 'https://openalex.org/W2891417743', 'https://openalex.org/W1797268635', 'https://openalex.org/W1522301498', 'https://openalex.org/W4300838842', 'https://openalex.org/W2099471712', 'https://openalex.org/W2950404765', 'https://openalex.org/W1527575280', 'https://openalex.org/W2911599361']",2020-09-30
https://openalex.org/W4210560636,https://doi.org/10.1109/access.2022.3147670,NSVQ: Noise Substitution in Vector Quantization for Machine Learning,"Machine learning algorithms have been shown to be highly effective in solving optimization problems in a wide range of applications. Such algorithms typically use gradient descent with backpropagation and the chain rule. Hence, the backpropagation fails if intermediate gradients are zero for some functions in the computational graph, because it causes the gradients to collapse when multiplying with zero. Vector quantization is one of those challenging functions for machine learning algorithms, since it is a piece-wise constant function and its gradient is zero almost everywhere. A typical solution is to apply the straight through estimator which simply copies the gradients over the vector quantization function in the backpropagation. Other solutions are based on smooth or stochastic approximation. This study proposes a vector quantization technique called NSVQ, which approximates the vector quantization behavior by substituting a multiplicative noise so that it can be used for machine learning problems. Specifically, the vector quantization error is replaced by product of the original error and a normalized noise vector, the samples of which are drawn from a zero-mean, unit-variance normal distribution. We test our proposed NSVQ in three scenarios with various types of applications. Based on the experiments, the proposed NSVQ achieves more accuracy and faster convergence in comparison to the straight through estimator, exponential moving averages, and the MiniBatchKmeans approaches.","['https://openalex.org/W3135028703', 'https://openalex.org/W2968923792', 'https://openalex.org/W2963341071', 'https://openalex.org/W2160815625', 'https://openalex.org/W2112739286', 'https://openalex.org/W6679436768', 'https://openalex.org/W2105482032', 'https://openalex.org/W2097117768', 'https://openalex.org/W6684191040', 'https://openalex.org/W2194775991', 'https://openalex.org/W2963446712', 'https://openalex.org/W1634005169', 'https://openalex.org/W2752796333', 'https://openalex.org/W6762931180', 'https://openalex.org/W2935711438', 'https://openalex.org/W3215615641', 'https://openalex.org/W4200219715', 'https://openalex.org/W3197590976', 'https://openalex.org/W2972849140', 'https://openalex.org/W3096524539', 'https://openalex.org/W6776218486', 'https://openalex.org/W6753540710', 'https://openalex.org/W2972374322', 'https://openalex.org/W2982602185', 'https://openalex.org/W6690026940', 'https://openalex.org/W6727208969', 'https://openalex.org/W3100270690', 'https://openalex.org/W3094917204', 'https://openalex.org/W3004061291', 'https://openalex.org/W6741057705', 'https://openalex.org/W6754241195', 'https://openalex.org/W2889119508', 'https://openalex.org/W2999803881', 'https://openalex.org/W6734035190', 'https://openalex.org/W2119717200', 'https://openalex.org/W2552465432', 'https://openalex.org/W2972875719', 'https://openalex.org/W6694251005', 'https://openalex.org/W6639703010', 'https://openalex.org/W6767283756', 'https://openalex.org/W3192421036', 'https://openalex.org/W6628950865', 'https://openalex.org/W6753018729', 'https://openalex.org/W4244017338', 'https://openalex.org/W1494198834', 'https://openalex.org/W1552314771', 'https://openalex.org/W2141998673', 'https://openalex.org/W2888465851', 'https://openalex.org/W2616492649', 'https://openalex.org/W2242818861', 'https://openalex.org/W1481935768', 'https://openalex.org/W2524428287', 'https://openalex.org/W4287802874', 'https://openalex.org/W4294567867', 'https://openalex.org/W4297659253']",2022-01-01
https://openalex.org/W3044483536,https://doi.org/10.21437/interspeech.2020-1170,Unsupervised Subword Modeling Using Autoregressive Pretraining and Cross-Lingual Phone-Aware Modeling,"This study addresses unsupervised subword modeling, i.e., learning feature\nrepresentations that can distinguish subword units of a language. The proposed\napproach adopts a two-stage bottleneck feature (BNF) learning framework,\nconsisting of autoregressive predictive coding (APC) as a front-end and a\nDNN-BNF model as a back-end. APC pretrained features are set as input features\nto a DNN-BNF model. A language-mismatched ASR system is used to provide\ncross-lingual phone labels for DNN-BNF model training. Finally, BNFs are\nextracted as the subword-discriminative feature representation. A second aim of\nthis work is to investigate the robustness of our approach's effectiveness to\ndifferent amounts of training data. The results on Libri-light and the\nZeroSpeech 2017 databases show that APC is effective in front-end feature\npretraining. Our whole system outperforms the state of the art on both\ndatabases. Cross-lingual phone labels for English data by a Dutch ASR\noutperform those by a Mandarin ASR, possibly linked to the larger similarity of\nDutch compared to Mandarin with English. Our system is less sensitive to\ntraining data amount when the training data is over 50 hours. APC pretraining\nleads to a reduction of needed training material from over 5,000 hours to\naround 200 hours with little performance degradation.\n","['https://openalex.org/W2012897754', 'https://openalex.org/W2064675550', 'https://openalex.org/W2995181338', 'https://openalex.org/W4297808394', 'https://openalex.org/W2963618559', 'https://openalex.org/W2842511635', 'https://openalex.org/W29952999', 'https://openalex.org/W2786902352', 'https://openalex.org/W162588823', 'https://openalex.org/W3016181583', 'https://openalex.org/W2964121744', 'https://openalex.org/W2927191280', 'https://openalex.org/W2468716020', 'https://openalex.org/W1522301498', 'https://openalex.org/W2950414763', 'https://openalex.org/W2972943112', 'https://openalex.org/W2963799213', 'https://openalex.org/W1524333225', 'https://openalex.org/W2787426069', 'https://openalex.org/W3104842308', 'https://openalex.org/W2949510815', 'https://openalex.org/W2758785877', 'https://openalex.org/W2085628288', 'https://openalex.org/W1494198834', 'https://openalex.org/W2293634267', 'https://openalex.org/W3016011332', 'https://openalex.org/W2399576818', 'https://openalex.org/W2786608204', 'https://openalex.org/W3125709657', 'https://openalex.org/W2972374322', 'https://openalex.org/W2963620343', 'https://openalex.org/W4300047444', 'https://openalex.org/W2514741789', 'https://openalex.org/W2787447541']",2020-10-25
https://openalex.org/W3196229461,https://doi.org/10.1109/icip42928.2021.9506508,Robust Multi-Frame Future Prediction By Leveraging View Synthesis,"In this paper, we focus on the problem of video prediction, i.e., future frame prediction. Most state-of-the-art techniques focus on synthesizing a single future frame at each step. However, this leads to utilizing the model's own predicted frames when synthesizing multi-step prediction, resulting in gradual performance degradation due to accumulating errors in pixels. To alleviate this issue, we propose a model that can handle multi-step prediction. Additionally, we employ techniques to leverage from view synthesis for future frame prediction, where both problems are treated independently in the literature. Our proposed method employs multiview camera pose prediction and depth-prediction networks to project the last available frame to desired future frames via differentiable point cloud renderer. For the synthesis of moving objects, we utilize an additional refinement stage. In experiments, we show that the proposed framework outperforms state-of-theart methods in both KITTI and Cityscapes datasets.","['https://openalex.org/W3163573274', 'https://openalex.org/W3154443551', 'https://openalex.org/W6744040789', 'https://openalex.org/W6781662123', 'https://openalex.org/W6639824700', 'https://openalex.org/W6637373629', 'https://openalex.org/W2972374322', 'https://openalex.org/W3096939667', 'https://openalex.org/W6703848168', 'https://openalex.org/W6677326919', 'https://openalex.org/W2064675550', 'https://openalex.org/W6728805014', 'https://openalex.org/W6684191040', 'https://openalex.org/W2798951647', 'https://openalex.org/W2423557781', 'https://openalex.org/W2962793481', 'https://openalex.org/W2984809863', 'https://openalex.org/W6782924149', 'https://openalex.org/W2964313012', 'https://openalex.org/W2993447238', 'https://openalex.org/W3012970712', 'https://openalex.org/W2966687987', 'https://openalex.org/W6713563955', 'https://openalex.org/W2963767194', 'https://openalex.org/W6767467511', 'https://openalex.org/W3035524985', 'https://openalex.org/W2963115613', 'https://openalex.org/W2115579991', 'https://openalex.org/W6723816956', 'https://openalex.org/W2963610939', 'https://openalex.org/W6755143522', 'https://openalex.org/W2963800363', 'https://openalex.org/W6678815747', 'https://openalex.org/W3090052686', 'https://openalex.org/W3042732446', 'https://openalex.org/W3025044797', 'https://openalex.org/W4320013936', 'https://openalex.org/W3107061944', 'https://openalex.org/W2340897893', 'https://openalex.org/W2963841322', 'https://openalex.org/W1901129140', 'https://openalex.org/W2963026643', 'https://openalex.org/W2982966540', 'https://openalex.org/W2116435618', 'https://openalex.org/W4294554810', 'https://openalex.org/W2125389028', 'https://openalex.org/W1686810756', 'https://openalex.org/W2497039038', 'https://openalex.org/W2163605009', 'https://openalex.org/W3098557217', 'https://openalex.org/W2963971656', 'https://openalex.org/W3082559925']",2021-08-23
https://openalex.org/W4389524018,https://doi.org/10.18653/v1/2023.emnlp-main.182,Generative Spoken Language Model based on continuous word-sized audio tokens,"In NLP, text language models based on words or subwords are known to outperform their character-based counterparts. Yet, in the speech community, the standard input of spoken LMs are 20ms or 40ms-long discrete units (shorter than a phoneme). Taking inspiration from word-based LM, we introduce a Generative Spoken Language Model (GSLM) based on word-size continuous-valued audio tokens that can generate diverse and expressive language output. This is obtained by replacing lookup table for lexical types with a Lexical Embedding function, the cross entropy loss by a contrastive loss, and multinomial sampling by k-NN sampling. The resulting model is the first generative language model based on word-size continuous tokens. Its performance is on par with discrete unit GSLMs regarding generation quality as measured by automatic metrics and subjective human judgements. Moreover, it is five times more memory efficient thanks to its large 200ms units. In addition, the embeddings before and after the Lexical Embedder are phonetically and semantically interpretable.","['https://openalex.org/W2979476256', 'https://openalex.org/W4221164184', 'https://openalex.org/W4299585995', 'https://openalex.org/W2121227244', 'https://openalex.org/W4385245566', 'https://openalex.org/W4292825791', 'https://openalex.org/W2145410271', 'https://openalex.org/W2963456134', 'https://openalex.org/W4286984129', 'https://openalex.org/W2519091744', 'https://openalex.org/W3092028330', 'https://openalex.org/W2079656678', 'https://openalex.org/W2970006822', 'https://openalex.org/W3148101939', 'https://openalex.org/W4295308567', 'https://openalex.org/W2972374322', 'https://openalex.org/W1614298861', 'https://openalex.org/W3096656254', 'https://openalex.org/W4313182775', 'https://openalex.org/W101045393', 'https://openalex.org/W2346964103', 'https://openalex.org/W2785896739', 'https://openalex.org/W4224875474', 'https://openalex.org/W4381786045', 'https://openalex.org/W2766812927', 'https://openalex.org/W4224308101', 'https://openalex.org/W2913062184', 'https://openalex.org/W4372260156', 'https://openalex.org/W3133702157', 'https://openalex.org/W2777302760', 'https://openalex.org/W4303649106', 'https://openalex.org/W2768381684', 'https://openalex.org/W3140429000', 'https://openalex.org/W2131738223', 'https://openalex.org/W2947445680', 'https://openalex.org/W4214633470', 'https://openalex.org/W3146777637', 'https://openalex.org/W3036601975', 'https://openalex.org/W2126377586', 'https://openalex.org/W4286902103', 'https://openalex.org/W3110761489', 'https://openalex.org/W3197580070', 'https://openalex.org/W1810943226', 'https://openalex.org/W1494198834', 'https://openalex.org/W3215615641', 'https://openalex.org/W2963720603', 'https://openalex.org/W2963425185', 'https://openalex.org/W3097692357', 'https://openalex.org/W46679369', 'https://openalex.org/W4223486244', 'https://openalex.org/W3213873715', 'https://openalex.org/W4303519914', 'https://openalex.org/W2160473997', 'https://openalex.org/W2152790380', 'https://openalex.org/W2950414763', 'https://openalex.org/W4287887366', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963979492']",2023-01-01
https://openalex.org/W4372260084,https://doi.org/10.1109/icassp49357.2023.10096204,Stochastic Optimization of Vector Quantization Methods in Application to Speech and Image Processing,"Vector quantization (VQ) methods have been used in a wide range of applications for speech, image, and video data. While classic VQ methods often use expectation maximization, in this paper, we investigate the use of stochastic optimization employing our recently proposed noise substitution in vector quantization technique. We consider three variants of VQ including additive VQ, residual VQ, and product VQ, and evaluate their quality, complexity and bitrate in speech coding, image compression, approximate nearest neighbor search, and a selection of toy examples. Our experimental results demonstrate the trade-offs in accuracy, complexity, and bitrate such that using our open source implementations and complexity calculator, the best vector quantization method can be chosen for a particular problem.","['https://openalex.org/W4210560636', 'https://openalex.org/W2124509324', 'https://openalex.org/W3004061291', 'https://openalex.org/W6690026940', 'https://openalex.org/W2119913432', 'https://openalex.org/W1977182282', 'https://openalex.org/W2935711438', 'https://openalex.org/W6762931180', 'https://openalex.org/W1494198834', 'https://openalex.org/W6630743200', 'https://openalex.org/W2752796333', 'https://openalex.org/W2972374322', 'https://openalex.org/W6753540710', 'https://openalex.org/W2982602185', 'https://openalex.org/W2972849140', 'https://openalex.org/W3197590976', 'https://openalex.org/W6776218486', 'https://openalex.org/W3096524539', 'https://openalex.org/W2242818861', 'https://openalex.org/W4287802874', 'https://openalex.org/W1514583064', 'https://openalex.org/W2963799213', 'https://openalex.org/W2884607399', 'https://openalex.org/W2971074500']",2023-05-05
https://openalex.org/W4226177016,https://doi.org/10.1109/taslp.2022.3171975,Non-Parametric Bayesian Subspace Models for Acoustic Unit Discovery,"This work investigates subspace non-parametric models for the task of learning a set of acoustic units from unlabeled speech recordings. We constrain the base-measure of a Dirichlet-Process mixture with a phonetic subspace---estimated from other source languages---to build an \emph{educated prior}, thereby forcing the learned acoustic units to resemble phones of known source languages. Two types of models are proposed: (i) the Subspace HMM (SHMM) which assumes that the phonetic subspace is the same for every language, (ii) the Hierarchical-Subspace HMM (H-SHMM) which relaxes this assumption and allows to have a language-specific subspace estimated on the unlabeled target data. These models are applied on 3 languages: English, Yoruba and Mboshi and they are compared with various competitive acoustic units discovery baselines. Experimental results show that both subspace models outperform other systems in terms of clustering quality and segmentation accuracy. Moreover, we observe that the H-SHMM provides results superior to the SHMM supporting the idea that language-specific priors are preferable to language-agnostic priors for acoustic unit discovery.","['https://openalex.org/W2928408492', 'https://openalex.org/W2962907457', 'https://openalex.org/W2193413348', 'https://openalex.org/W6687566353', 'https://openalex.org/W2545177271', 'https://openalex.org/W2483390977', 'https://openalex.org/W2786608204', 'https://openalex.org/W6973666849', 'https://openalex.org/W2963620343', 'https://openalex.org/W6697293080', 'https://openalex.org/W2940544976', 'https://openalex.org/W2100768664', 'https://openalex.org/W6675022971', 'https://openalex.org/W2399576818', 'https://openalex.org/W2586754519', 'https://openalex.org/W2347098582', 'https://openalex.org/W6704752648', 'https://openalex.org/W3100270690', 'https://openalex.org/W3125709657', 'https://openalex.org/W2979476256', 'https://openalex.org/W6769196770', 'https://openalex.org/W2991557631', 'https://openalex.org/W6770596778', 'https://openalex.org/W2750248772', 'https://openalex.org/W2888911345', 'https://openalex.org/W2927191280', 'https://openalex.org/W3100202343', 'https://openalex.org/W3161215977', 'https://openalex.org/W2109176692', 'https://openalex.org/W6676755231', 'https://openalex.org/W1503207992', 'https://openalex.org/W1990005915', 'https://openalex.org/W2612661529', 'https://openalex.org/W4241943628', 'https://openalex.org/W4212863985', 'https://openalex.org/W2077804127', 'https://openalex.org/W4394867155', 'https://openalex.org/W6865393803', 'https://openalex.org/W2107638917', 'https://openalex.org/W4237840503', 'https://openalex.org/W6645634967', 'https://openalex.org/W7048738093', 'https://openalex.org/W2408460974', 'https://openalex.org/W2762715843', 'https://openalex.org/W1516111018', 'https://openalex.org/W6784355959', 'https://openalex.org/W6744702808', 'https://openalex.org/W2195354', 'https://openalex.org/W3092791109', 'https://openalex.org/W2401271873', 'https://openalex.org/W2084534958', 'https://openalex.org/W2572097499', 'https://openalex.org/W2401396251', 'https://openalex.org/W6600087822', 'https://openalex.org/W6712757354', 'https://openalex.org/W2752796333', 'https://openalex.org/W6731521493', 'https://openalex.org/W2972374322', 'https://openalex.org/W6712648922', 'https://openalex.org/W2099111195', 'https://openalex.org/W3093096176', 'https://openalex.org/W3112613336', 'https://openalex.org/W1494198834', 'https://openalex.org/W2125838338', 'https://openalex.org/W2115979064', 'https://openalex.org/W3198134274', 'https://openalex.org/W2072169887', 'https://openalex.org/W2805607737', 'https://openalex.org/W3209059054', 'https://openalex.org/W6804030475', 'https://openalex.org/W3198429080', 'https://openalex.org/W6677308353', 'https://openalex.org/W6640963894', 'https://openalex.org/W6639732818']",2022-01-01
https://openalex.org/W4382465907,https://doi.org/10.1609/aaai.v37i2.25361,ECO-3D: Equivariant Contrastive Learning for Pre-training on Perturbed 3D Point Cloud,"In this work, we investigate contrastive learning on perturbed point clouds and find that the contrasting process may widen the domain gap caused by random perturbations, making the pre-trained network fail to generalize on testing data. To this end, we propose the Equivariant COntrastive framework which closes the domain gap before contrasting, further introduces the equivariance property, and enables pre-training networks under more perturbation types to obtain meaningful features. Specifically, to close the domain gap, a pre-trained VAE is adopted to convert perturbed point clouds into less perturbed point embedding of similar domains and separated perturbation embedding. The contrastive pairs can then be generated by mixing the point embedding with different perturbation embedding. Moreover, to pursue the equivariance property, a Vector Quantizer is adopted during VAE training, discretizing the perturbation embedding into one-hot tokens which indicate the perturbation labels. By correctly predicting the perturbation labels from the perturbed point cloud, the property of equivariance can be encouraged in the learned features. Experiments on synthesized and real-world perturbed datasets show that ECO-3D outperforms most existing pre-training strategies under various downstream tasks, achieving SOTA performance for lots of perturbations.","['https://openalex.org/W2784996692', 'https://openalex.org/W3146508414', 'https://openalex.org/W2971081162', 'https://openalex.org/W6779879114', 'https://openalex.org/W3005680577', 'https://openalex.org/W2950187998', 'https://openalex.org/W3167071962', 'https://openalex.org/W2950996604', 'https://openalex.org/W2964609265', 'https://openalex.org/W2987283559', 'https://openalex.org/W6741121127', 'https://openalex.org/W2911779594', 'https://openalex.org/W6734074887', 'https://openalex.org/W2321533354', 'https://openalex.org/W3046762217', 'https://openalex.org/W2950642167', 'https://openalex.org/W3013748088', 'https://openalex.org/W3033437302', 'https://openalex.org/W2947713925', 'https://openalex.org/W6783272290', 'https://openalex.org/W2555897561', 'https://openalex.org/W3036560856', 'https://openalex.org/W6763442200', 'https://openalex.org/W2968609420', 'https://openalex.org/W3156629209', 'https://openalex.org/W6747904511', 'https://openalex.org/W6640300118', 'https://openalex.org/W2798991696', 'https://openalex.org/W3045125647', 'https://openalex.org/W2796426482', 'https://openalex.org/W2594538354', 'https://openalex.org/W2553307952', 'https://openalex.org/W3108655343', 'https://openalex.org/W3034459762', 'https://openalex.org/W4286891192', 'https://openalex.org/W2994633389', 'https://openalex.org/W4226335479', 'https://openalex.org/W2964222296', 'https://openalex.org/W3202611145', 'https://openalex.org/W4312317653', 'https://openalex.org/W4287591058', 'https://openalex.org/W3023371261', 'https://openalex.org/W3097823560', 'https://openalex.org/W2560609797', 'https://openalex.org/W3048911294', 'https://openalex.org/W4288623616', 'https://openalex.org/W3086441265', 'https://openalex.org/W2753738274', 'https://openalex.org/W2963600562', 'https://openalex.org/W3128716822', 'https://openalex.org/W343636949', 'https://openalex.org/W4313156423', 'https://openalex.org/W1959608418', 'https://openalex.org/W3009750677', 'https://openalex.org/W3091112259', 'https://openalex.org/W2971074500', 'https://openalex.org/W3116959466', 'https://openalex.org/W2963799213', 'https://openalex.org/W2972374322', 'https://openalex.org/W2187089797', 'https://openalex.org/W3188579662', 'https://openalex.org/W1920022804', 'https://openalex.org/W2785325870', 'https://openalex.org/W3035524453', 'https://openalex.org/W4297808394', 'https://openalex.org/W4312270234', 'https://openalex.org/W2972867623', 'https://openalex.org/W2977001478', 'https://openalex.org/W2962947361', 'https://openalex.org/W2979750740', 'https://openalex.org/W2949671016', 'https://openalex.org/W2981440248', 'https://openalex.org/W3034445277']",2023-06-26
https://openalex.org/W4391647131,https://doi.org/10.1007/s40747-023-01333-8,Retinal spike train decoder using vector quantization for visual scene reconstruction,"Abstract The retinal impulse signal is the basic carrier of visual information. It records the distribution of light on the retina. However, its direct conversion to a scene image is difficult due to the nonlinear characteristics of its distribution. Therefore, the use of artificial neural network to reconstruct the scene from retinal spikes has become an important research area. This paper proposes the architecture of a neural network based on vector quantization, where the feature vectors of spike trains are extracted, compressed, and stored using a feature extraction and compression network. During the decoding process, the nearest neighbour search method is used to find the nearest feature vector corresponding to each feature vector in the feature map. Finally, a reconstruction network is used to decode a new feature map composed of matching feature vectors to obtain a visual scene. This paper also verifies the impact of vector quantization on the characteristics of pulse signals by comparing experiments and visualizing the characteristics before and after vector quantization. The network delivers promising performance when evaluated on different datasets, demonstrating that this research is of great significance for improving relevant applications in the fields of retinal image processing and artificial intelligence.","['https://openalex.org/W3165362804', 'https://openalex.org/W2063640158', 'https://openalex.org/W2069519142', 'https://openalex.org/W4240629939', 'https://openalex.org/W2805069612', 'https://openalex.org/W2022800322', 'https://openalex.org/W2099616257', 'https://openalex.org/W2946822178', 'https://openalex.org/W2791741309', 'https://openalex.org/W2962949934', 'https://openalex.org/W2910639395', 'https://openalex.org/W2071941483', 'https://openalex.org/W2136610140', 'https://openalex.org/W3011209639', 'https://openalex.org/W1989786687', 'https://openalex.org/W2142978081', 'https://openalex.org/W2283006256', 'https://openalex.org/W4285056750', 'https://openalex.org/W3157324122', 'https://openalex.org/W3005431840', 'https://openalex.org/W2775143585', 'https://openalex.org/W4220806792', 'https://openalex.org/W2747898905', 'https://openalex.org/W3012740297', 'https://openalex.org/W3006075760', 'https://openalex.org/W4312739039', 'https://openalex.org/W1890834058', 'https://openalex.org/W2036329542', 'https://openalex.org/W2000474946', 'https://openalex.org/W2972374322', 'https://openalex.org/W4220894702', 'https://openalex.org/W2752480826', 'https://openalex.org/W2099474155', 'https://openalex.org/W4243711516', 'https://openalex.org/W1981501272', 'https://openalex.org/W2798878556']",2024-02-08
https://openalex.org/W3110371022,https://doi.org/10.1109/taslp.2020.3042016,Tackling Perception Bias in Unsupervised Phoneme Discovery Using DPGMM-RNN Hybrid Model and Functional Load,"The human perception of phonemes is biased against speech sounds. The lack of correspondence between perceptual phonemes and acoustic signals forms a big challenge in designing unsupervised algorithms to distinguish phonemes from sound. We propose the DPGMM-RNN hybrid model that improves phoneme categorization by relieving the fragmentation problem. We also merge segments with low functional load, which is the work done by segment contrasts to differentiate between utterances, just like humans who convert unambiguous segments into phonemes as units for immediate perception. Our results show that the DPGMM-RNN hybrid model relieves the fragmentation problem and improves phoneme discriminability. The minimal functional load merge compresses a segment system, preserves information and keeps phoneme discriminability.","['https://openalex.org/W2579555219', 'https://openalex.org/W2051512859', 'https://openalex.org/W6711866534', 'https://openalex.org/W2252172689', 'https://openalex.org/W1994158173', 'https://openalex.org/W6824062597', 'https://openalex.org/W2586754519', 'https://openalex.org/W6674922813', 'https://openalex.org/W6677461952', 'https://openalex.org/W6602180557', 'https://openalex.org/W6713016582', 'https://openalex.org/W2038101708', 'https://openalex.org/W6678947187', 'https://openalex.org/W2940544976', 'https://openalex.org/W4254499902', 'https://openalex.org/W2117041980', 'https://openalex.org/W6675022971', 'https://openalex.org/W91681889', 'https://openalex.org/W2068247585', 'https://openalex.org/W1977531436', 'https://openalex.org/W2015394094', 'https://openalex.org/W2011334394', 'https://openalex.org/W4246695671', 'https://openalex.org/W6682569104', 'https://openalex.org/W1600008395', 'https://openalex.org/W6713593416', 'https://openalex.org/W2950414763', 'https://openalex.org/W6745117592', 'https://openalex.org/W2752796333', 'https://openalex.org/W2095089846', 'https://openalex.org/W6675354045', 'https://openalex.org/W6734901337', 'https://openalex.org/W6680970901', 'https://openalex.org/W6713256719', 'https://openalex.org/W2972374322', 'https://openalex.org/W2963830550', 'https://openalex.org/W2759889345', 'https://openalex.org/W2347098582', 'https://openalex.org/W2787447541', 'https://openalex.org/W6712553779', 'https://openalex.org/W2056133372', 'https://openalex.org/W2057007397', 'https://openalex.org/W2170659185', 'https://openalex.org/W6973666849', 'https://openalex.org/W2114347655', 'https://openalex.org/W2020607164', 'https://openalex.org/W6638159135', 'https://openalex.org/W6712444837', 'https://openalex.org/W6712202099', 'https://openalex.org/W2010548701', 'https://openalex.org/W2017339534', 'https://openalex.org/W2506406911', 'https://openalex.org/W2509930204', 'https://openalex.org/W2826003142', 'https://openalex.org/W2089381378', 'https://openalex.org/W2345811097', 'https://openalex.org/W2750248772', 'https://openalex.org/W2403015869', 'https://openalex.org/W3145269374', 'https://openalex.org/W2151967501', 'https://openalex.org/W2395899413', 'https://openalex.org/W2396043527', 'https://openalex.org/W1984314602', 'https://openalex.org/W1796128977', 'https://openalex.org/W2396733358', 'https://openalex.org/W2054496875', 'https://openalex.org/W52412328', 'https://openalex.org/W2101234009', 'https://openalex.org/W2796346319', 'https://openalex.org/W2963799213', 'https://openalex.org/W2054168569', 'https://openalex.org/W2963620343', 'https://openalex.org/W2786608204', 'https://openalex.org/W2100768664', 'https://openalex.org/W1635512741', 'https://openalex.org/W2758785877', 'https://openalex.org/W2973026522', 'https://openalex.org/W2138615112', 'https://openalex.org/W2963618559', 'https://openalex.org/W2100163972', 'https://openalex.org/W2895297209', 'https://openalex.org/W2402366697', 'https://openalex.org/W2117126688', 'https://openalex.org/W2399576818', 'https://openalex.org/W2404799143', 'https://openalex.org/W2593779438', 'https://openalex.org/W2128032727']",2020-12-02
https://openalex.org/W3143569452,https://doi.org/10.1109/slt48900.2021.9383597,Incorporating Discriminative DPGMM Posteriorgrams for Low-Resource ASR,"The first step in building an ASR system is to extract proper speech features. The ideal speech features for ASR must also have high discriminabilities between linguistic units and be robust to such non-linguistic factors as gender, age, emotions, or noise. The discriminabilities of various features have been compared in several Zerospeech challenges to discover linguistic units without any transcriptions, in which the posteriorgrams of DPGMM clustering show strong discriminability and get several top results of ABX discrimination scores between phonemes. This paper appends DPGMM posteriorgrams to increase the discriminability of acoustic features to enhance ASR systems. To the best of our knowledge, DPGMM features, which are usually applied to such tasks as spoken term detection and zero resources tasks, have not been applied to large vocabulary continuous speech recognition (LVCSR) before. DPGMM clustering can dynamically change the number of Gaussians until each one fits one segmental pattern of the whole speech corpus with the highest probability such that the linguistic units of different segmental patterns are clearly discriminated. Our experimental results on the WSJ corpora show our proposal stably improves ASR systems and provides even more improvement for smaller datasets with fewer resources.","['https://openalex.org/W2345811097', 'https://openalex.org/W6726320248', 'https://openalex.org/W2327501763', 'https://openalex.org/W1902237438', 'https://openalex.org/W2526425061', 'https://openalex.org/W2587741066', 'https://openalex.org/W6638545294', 'https://openalex.org/W6631190155', 'https://openalex.org/W2183341477', 'https://openalex.org/W2963620343', 'https://openalex.org/W3009565979', 'https://openalex.org/W2940544976', 'https://openalex.org/W6638159135', 'https://openalex.org/W2020607164', 'https://openalex.org/W6712202099', 'https://openalex.org/W6713256719', 'https://openalex.org/W2972374322', 'https://openalex.org/W2759889345', 'https://openalex.org/W6675022971', 'https://openalex.org/W2347098582', 'https://openalex.org/W6631362777', 'https://openalex.org/W6682569104', 'https://openalex.org/W6633431331', 'https://openalex.org/W2787447541', 'https://openalex.org/W6712444837', 'https://openalex.org/W6712553779', 'https://openalex.org/W2090861223', 'https://openalex.org/W6973666849', 'https://openalex.org/W2148154194', 'https://openalex.org/W2750248772', 'https://openalex.org/W2103387126', 'https://openalex.org/W2950414763', 'https://openalex.org/W6824062597', 'https://openalex.org/W2156983866', 'https://openalex.org/W2024490156', 'https://openalex.org/W3015213852', 'https://openalex.org/W2165712214', 'https://openalex.org/W193119429', 'https://openalex.org/W1524333225', 'https://openalex.org/W2396043527', 'https://openalex.org/W3102516861', 'https://openalex.org/W2404799143', 'https://openalex.org/W1522301498', 'https://openalex.org/W1560013842', 'https://openalex.org/W2786608204', 'https://openalex.org/W2964121744', 'https://openalex.org/W2399576818', 'https://openalex.org/W1635512741', 'https://openalex.org/W1796128977', 'https://openalex.org/W2395899413', 'https://openalex.org/W2151967501', 'https://openalex.org/W2973026522', 'https://openalex.org/W2963347649', 'https://openalex.org/W2525778437', 'https://openalex.org/W1815076433', 'https://openalex.org/W2100768664']",2021-01-19
https://openalex.org/W4210997382,https://doi.org/10.1109/taslp.2022.3150220,Modeling Unsupervised Empirical Adaptation by DPGMM and DPGMM-RNN Hybrid Model to Extract Perceptual Features for Low-Resource ASR,"Speech feature extraction is critical for ASR systems. Such successful features as MFCC and PLP use filterbank techniques to model log-scaled speech perception but fail to model the adaptation of human speech perception by hearing experiences. Infant perception that is adapted by hearing speech without text may cause permanent brain state modifications (engrams) that serve as a physical fundamental basis for lifetime speech perception formation. This realization motivates us to propose to model such an unsupervised adaptation process, where adaptation denotes perception that is affected or changed by the history of experiences, with the Dirichlet Process Gaussian Mixture Model (DPGMM) and the DPGMM-RNN hybrid model to extract perceptual features to improve ASR. Our proposed features extend MFCC features with posteriorgrams extracted from the DPGMM algorithm or the DPGMM-RNN hybrid model. Our analysis shows that the DPGMM and DPGMM-RNN model perplexities agree with infant auditory perplexity to support that the proposed features are perceptual. Our ASR results verify the effectiveness of the proposed unsupervised features in such tasks as LVCSR on WSJ and ASR on noisy low-resource telephone conversations, compared with the supervised bottleneck features from Kaldi in ASR performance.","['https://openalex.org/W2148154194', 'https://openalex.org/W2090861223', 'https://openalex.org/W2101509422', 'https://openalex.org/W4255383621', 'https://openalex.org/W2998470023', 'https://openalex.org/W2610261218', 'https://openalex.org/W2082235945', 'https://openalex.org/W2155505956', 'https://openalex.org/W2083750191', 'https://openalex.org/W1980006923', 'https://openalex.org/W2943518292', 'https://openalex.org/W2148389554', 'https://openalex.org/W2041018266', 'https://openalex.org/W2040455912', 'https://openalex.org/W625960733', 'https://openalex.org/W4206040902', 'https://openalex.org/W2466905317', 'https://openalex.org/W2082396705', 'https://openalex.org/W2096162685', 'https://openalex.org/W4300166480', 'https://openalex.org/W6808634324', 'https://openalex.org/W198924044', 'https://openalex.org/W2165712214', 'https://openalex.org/W179875071', 'https://openalex.org/W2069429561', 'https://openalex.org/W2787447541', 'https://openalex.org/W2399576818', 'https://openalex.org/W2950414763', 'https://openalex.org/W2395899413', 'https://openalex.org/W1796128977', 'https://openalex.org/W2404799143', 'https://openalex.org/W2972374322', 'https://openalex.org/W2759889345', 'https://openalex.org/W2347098582', 'https://openalex.org/W2750248772', 'https://openalex.org/W6675022971', 'https://openalex.org/W3143569452', 'https://openalex.org/W6683650587', 'https://openalex.org/W6602180557', 'https://openalex.org/W2143022183', 'https://openalex.org/W2104752510', 'https://openalex.org/W2095458199', 'https://openalex.org/W2169991335', 'https://openalex.org/W4254499902', 'https://openalex.org/W4251668937', 'https://openalex.org/W3110371022', 'https://openalex.org/W2895297209', 'https://openalex.org/W2099111195', 'https://openalex.org/W6680970901', 'https://openalex.org/W1635512741', 'https://openalex.org/W2404126548', 'https://openalex.org/W6744702808', 'https://openalex.org/W6631362777', 'https://openalex.org/W2151967501', 'https://openalex.org/W6842558766', 'https://openalex.org/W2345811097', 'https://openalex.org/W2327501763', 'https://openalex.org/W1902237438', 'https://openalex.org/W3095361818', 'https://openalex.org/W4245838404', 'https://openalex.org/W2526425061', 'https://openalex.org/W3009565979', 'https://openalex.org/W2786608204', 'https://openalex.org/W2963620343', 'https://openalex.org/W2940544976', 'https://openalex.org/W2037662195', 'https://openalex.org/W2011238950', 'https://openalex.org/W2076493153', 'https://openalex.org/W2077382402', 'https://openalex.org/W2038142680', 'https://openalex.org/W2070095866', 'https://openalex.org/W2018567671', 'https://openalex.org/W6751796006', 'https://openalex.org/W2019200248', 'https://openalex.org/W1991923060', 'https://openalex.org/W2007105567', 'https://openalex.org/W1497856780', 'https://openalex.org/W4320013820', 'https://openalex.org/W1608461387', 'https://openalex.org/W2973026522', 'https://openalex.org/W2804109700']",2022-01-01
https://openalex.org/W3003883423,https://doi.org/10.48550/arxiv.2002.00417,WaveTTS: Tacotron-based TTS with Joint Time-Frequency Domain Loss,"Tacotron-based text-to-speech (TTS) systems directly synthesize speech from text input. Such frameworks typically consist of a feature prediction network that maps character sequences to frequency-domain acoustic features, followed by a waveform reconstruction algorithm or a neural vocoder that generates the time-domain waveform from acoustic features. As the loss function is usually calculated only for frequency-domain acoustic features, that doesn't directly control the quality of the generated time-domain waveform. To address this problem, we propose a new training scheme for Tacotron-based TTS, referred to as WaveTTS, that has 2 loss functions: 1) time-domain loss, denoted as the waveform loss, that measures the distortion between the natural and generated waveform; and 2) frequency-domain loss, that measures the Mel-scale acoustic feature loss between the natural and generated acoustic features. WaveTTS ensures both the quality of the acoustic features and the resulting speech waveform. To our best knowledge, this is the first implementation of Tacotron with joint time-frequency domain loss. Experimental results show that the proposed framework outperforms the baselines and achieves high-quality synthesized speech.","['https://openalex.org/W3016151052', 'https://openalex.org/W2111284386', 'https://openalex.org/W2889064624', 'https://openalex.org/W2941094131', 'https://openalex.org/W2964243274', 'https://openalex.org/W2996573371', 'https://openalex.org/W2977311057', 'https://openalex.org/W2150658333', 'https://openalex.org/W2550821151', 'https://openalex.org/W2962826786', 'https://openalex.org/W2962935966', 'https://openalex.org/W2102113734', 'https://openalex.org/W2964058413', 'https://openalex.org/W2964060510', 'https://openalex.org/W2903250132', 'https://openalex.org/W2963828919', 'https://openalex.org/W2984809863', 'https://openalex.org/W2790783922', 'https://openalex.org/W2963568578', 'https://openalex.org/W2120847449', 'https://openalex.org/W2994396107', 'https://openalex.org/W2884236563', 'https://openalex.org/W2519091744', 'https://openalex.org/W2786868129', 'https://openalex.org/W2889026195', 'https://openalex.org/W2102003408', 'https://openalex.org/W2963403868', 'https://openalex.org/W2962866211', 'https://openalex.org/W3098606562', 'https://openalex.org/W2813877859', 'https://openalex.org/W2129142580', 'https://openalex.org/W2964308564', 'https://openalex.org/W1516630152', 'https://openalex.org/W2975960378', 'https://openalex.org/W2963927338', 'https://openalex.org/W2193413348', 'https://openalex.org/W2972374322', 'https://openalex.org/W2604184139', 'https://openalex.org/W2911340057', 'https://openalex.org/W2972693890', 'https://openalex.org/W2887511658', 'https://openalex.org/W2944622002', 'https://openalex.org/W2962970934', 'https://openalex.org/W2962911378']",2020-02-02
https://openalex.org/W3084014658,https://doi.org/10.48550/arxiv.2009.04983,Exploration of End-to-end Synthesisers forZero Resource Speech Challenge 2020,"A Spoken dialogue system for an unseen language is referred to as Zero resource speech. It is especially beneficial for developing applications for languages that have low digital resources. Zero resource speech synthesis is the task of building text-to-speech (TTS) models in the absence of transcriptions. In this work, speech is modelled as a sequence of transient and steady-state acoustic units, and a unique set of acoustic units is discovered by iterative training. Using the acoustic unit sequence, TTS models are trained. The main goal of this work is to improve the synthesis quality of zero resource TTS system. Four different systems are proposed. All the systems consist of three stages: unit discovery, followed by unit sequence to spectrogram mapping, and finally spectrogram to speech inversion. Modifications are proposed to the spectrogram mapping stage. These modifications include training the mapping on voice data, using x-vectors to improve the mapping, two-stage learning, and gender-specific modelling. Evaluation of the proposed systems in the Zerospeech 2020 challenge shows that quite good quality synthesis can be achieved.","['https://openalex.org/W2964243274', 'https://openalex.org/W2402146185', 'https://openalex.org/W2335906338', 'https://openalex.org/W1796128977', 'https://openalex.org/W2972374322', 'https://openalex.org/W3016160783', 'https://openalex.org/W2963300588', 'https://openalex.org/W2547039119', 'https://openalex.org/W2972964185', 'https://openalex.org/W2519091744', 'https://openalex.org/W2015876361', 'https://openalex.org/W1967924372', 'https://openalex.org/W2892140764', 'https://openalex.org/W2020607164', 'https://openalex.org/W2748488820', 'https://openalex.org/W2347098582', 'https://openalex.org/W2598638573', 'https://openalex.org/W2947445680', 'https://openalex.org/W2890964092', 'https://openalex.org/W1524333225', 'https://openalex.org/W2973026522', 'https://openalex.org/W2100768664', 'https://openalex.org/W2191779130', 'https://openalex.org/W2972867623', 'https://openalex.org/W2963139417']",2020-09-10
https://openalex.org/W3026505300,https://doi.org/10.48550/arxiv.2005.09282,Bayesian Subspace HMM for the Zerospeech 2020 Challenge,"In this paper we describe our submission to the Zerospeech 2020 challenge, where the participants are required to discover latent representations from unannotated speech, and to use those representations to perform speech synthesis, with synthesis quality used as a proxy metric for the unit quality. In our system, we use the Bayesian Subspace Hidden Markov Model (SHMM) for unit discovery. The SHMM models each unit as an HMM whose parameters are constrained to lie in a low dimensional subspace of the total parameter space which is trained to model phonetic variability. Our system compares favorably with the baseline on the human-evaluated character error rate while maintaining significantly lower unit bitrate.","['https://openalex.org/W2972374322', 'https://openalex.org/W2641832364', 'https://openalex.org/W2962693497', 'https://openalex.org/W2787447541', 'https://openalex.org/W2786608204', 'https://openalex.org/W2395899413', 'https://openalex.org/W2406349064', 'https://openalex.org/W3125709657', 'https://openalex.org/W2972867623', 'https://openalex.org/W1959608418', 'https://openalex.org/W2125838338', 'https://openalex.org/W2972574141', 'https://openalex.org/W2127498532', 'https://openalex.org/W2963620343', 'https://openalex.org/W2973026522', 'https://openalex.org/W2400549570', 'https://openalex.org/W2598638573', 'https://openalex.org/W2547039119', 'https://openalex.org/W1545920196', 'https://openalex.org/W2963720603', 'https://openalex.org/W2468716020', 'https://openalex.org/W2347098582', 'https://openalex.org/W2973013862', 'https://openalex.org/W66627554', 'https://openalex.org/W2750248772', 'https://openalex.org/W2100768664', 'https://openalex.org/W2404799143', 'https://openalex.org/W2888911345']",2020-05-19
https://openalex.org/W3003875258,https://doi.org/10.1109/icassp40776.2020.9053541,Unsupervised Pre-Training of Bidirectional Speech Encoders via Masked Reconstruction,"We propose an approach for pre-training speech representations via a masked reconstruction loss. Our pre-trained encoder networks are bidirectional and can therefore be used directly in typical bidirectional speech recognition models. The pre-trained networks can then be fine-tuned on a smaller amount of supervised data for speech recognition. Experiments with this approach on the LibriSpeech and Wall Street Journal corpora show promising results. We find that the main factors that lead to speech recognition improvements are: masking segments of sufficient width in both time and frequency, pre-training on a much larger amount of unlabeled data than the labeled data, and domain adaptation when the unlabeled and labeled data come from different domains. The gain from pre-training is additive to that of supervised data augmentation.","['https://openalex.org/W2963211739', 'https://openalex.org/W2087006792', 'https://openalex.org/W1519855838', 'https://openalex.org/W6755207826', 'https://openalex.org/W6750615492', 'https://openalex.org/W2936774411', 'https://openalex.org/W2940544976', 'https://openalex.org/W3100270690', 'https://openalex.org/W2468716020', 'https://openalex.org/W6729977899', 'https://openalex.org/W6745117592', 'https://openalex.org/W2963317665', 'https://openalex.org/W6631190155', 'https://openalex.org/W6637242042', 'https://openalex.org/W2064675550', 'https://openalex.org/W6631943919', 'https://openalex.org/W2973157397', 'https://openalex.org/W1513862252', 'https://openalex.org/W6674330103', 'https://openalex.org/W2972943112', 'https://openalex.org/W2160815625', 'https://openalex.org/W6748634344', 'https://openalex.org/W2116064496', 'https://openalex.org/W2979476256', 'https://openalex.org/W6754278344', 'https://openalex.org/W2127141656', 'https://openalex.org/W1494198834', 'https://openalex.org/W6631362777', 'https://openalex.org/W4297808394', 'https://openalex.org/W2964121744', 'https://openalex.org/W2973026522', 'https://openalex.org/W2923014074', 'https://openalex.org/W2996383576', 'https://openalex.org/W1665214252', 'https://openalex.org/W2963341956', 'https://openalex.org/W2896457183', 'https://openalex.org/W2962739339', 'https://openalex.org/W2963310665', 'https://openalex.org/W2556930864', 'https://openalex.org/W1533861849', 'https://openalex.org/W3125709657', 'https://openalex.org/W2758785877', 'https://openalex.org/W2963618559', 'https://openalex.org/W1522301498', 'https://openalex.org/W2887997457', 'https://openalex.org/W1524333225', 'https://openalex.org/W2842511635', 'https://openalex.org/W2973049979', 'https://openalex.org/W3098643042', 'https://openalex.org/W2095705004']",2020-04-09
https://openalex.org/W3015265920,https://doi.org/10.1109/icassp40776.2020.9053176,Deep Contextualized Acoustic Representations for Semi-Supervised Speech Recognition,"We propose a novel approach to semi-supervised automatic speech recognition (ASR). We first exploit a large amount of unlabeled audio data via representation learning, where we reconstruct a temporal slice of filterbank features from past and future context frames. The resulting deep contextualized acoustic representations (DeCoAR) are then used to train a CTC-based end-to-end ASR system using a smaller amount of labeled audio data. In our experiments, we show that systems trained on DeCoAR consistently outperform ones trained on conventional filterbank features, giving 42% and 19% relative improvement over the baseline on WSJ eval92 and LibriSpeech test-clean, respectively. Our approach can drastically reduce the amount of labeled data required; unsupervised training on LibriSpeech then supervision with 100 hours of labeled data achieves performance on par with training on all 960 hours directly. Pre-trained models and code will be released online.","['https://openalex.org/W2486109868', 'https://openalex.org/W2962739339', 'https://openalex.org/W6755207826', 'https://openalex.org/W6763701032', 'https://openalex.org/W2972889948', 'https://openalex.org/W2972981541', 'https://openalex.org/W2136922672', 'https://openalex.org/W2160815625', 'https://openalex.org/W6676481782', 'https://openalex.org/W2964245029', 'https://openalex.org/W2944255943', 'https://openalex.org/W2889213362', 'https://openalex.org/W2936774411', 'https://openalex.org/W2802248956', 'https://openalex.org/W2124558353', 'https://openalex.org/W6765807869', 'https://openalex.org/W6760911985', 'https://openalex.org/W176510440', 'https://openalex.org/W6681588610', 'https://openalex.org/W2512655038', 'https://openalex.org/W2127499922', 'https://openalex.org/W1993660824', 'https://openalex.org/W2963425185', 'https://openalex.org/W3100270690', 'https://openalex.org/W2972943112', 'https://openalex.org/W6769196770', 'https://openalex.org/W2973049979', 'https://openalex.org/W2963211739', 'https://openalex.org/W2127141656', 'https://openalex.org/W2184045248', 'https://openalex.org/W2787560479', 'https://openalex.org/W3103005696', 'https://openalex.org/W2896457183', 'https://openalex.org/W2963341956', 'https://openalex.org/W2145494108', 'https://openalex.org/W2293363371', 'https://openalex.org/W2996383576', 'https://openalex.org/W2110798204', 'https://openalex.org/W2962753370', 'https://openalex.org/W2962907457', 'https://openalex.org/W3011411500', 'https://openalex.org/W2970597249', 'https://openalex.org/W2979476256', 'https://openalex.org/W3125709657', 'https://openalex.org/W2950813464']",2020-04-09
https://openalex.org/W2982223350,https://doi.org/10.1109/icassp40776.2020.9054458,Mockingjay: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders,"We present Mockingjay as a new speech representation learning approach, where\nbidirectional Transformer encoders are pre-trained on a large amount of\nunlabeled speech. Previous speech representation methods learn through\nconditioning on past frames and predicting information about future frames.\nWhereas Mockingjay is designed to predict the current frame through jointly\nconditioning on both past and future contexts. The Mockingjay representation\nimproves performance for a wide range of downstream tasks, including phoneme\nclassification, speaker recognition, and sentiment classification on spoken\ncontent, while outperforming other approaches. Mockingjay is empirically\npowerful and can be fine-tuned with downstream models, with only 2 epochs we\nfurther improve performance dramatically. In a low resource setting with only\n0.1% of labeled data, we outperform the result of Mel-features that uses all\n100% labeled data.\n","['https://openalex.org/W2947445680', 'https://openalex.org/W6607333740', 'https://openalex.org/W6755207826', 'https://openalex.org/W6780226713', 'https://openalex.org/W2964089206', 'https://openalex.org/W2972451902', 'https://openalex.org/W6737778391', 'https://openalex.org/W2270070752', 'https://openalex.org/W6766673545', 'https://openalex.org/W6768021236', 'https://openalex.org/W2963571336', 'https://openalex.org/W2963425185', 'https://openalex.org/W2972943112', 'https://openalex.org/W2842511635', 'https://openalex.org/W2979476256', 'https://openalex.org/W2973049979', 'https://openalex.org/W3100270690', 'https://openalex.org/W2998649947', 'https://openalex.org/W6739901393', 'https://openalex.org/W2962739339', 'https://openalex.org/W6631190155', 'https://openalex.org/W1494198834', 'https://openalex.org/W2747874407', 'https://openalex.org/W6674330103', 'https://openalex.org/W2883409523', 'https://openalex.org/W2996428491', 'https://openalex.org/W2943493972', 'https://openalex.org/W4297808394', 'https://openalex.org/W3125709657', 'https://openalex.org/W2613904329', 'https://openalex.org/W2336585117', 'https://openalex.org/W2975059944', 'https://openalex.org/W2787560479', 'https://openalex.org/W2965373594', 'https://openalex.org/W1522301498', 'https://openalex.org/W4385245566', 'https://openalex.org/W2996383576', 'https://openalex.org/W179875071', 'https://openalex.org/W2896457183', 'https://openalex.org/W2626778328', 'https://openalex.org/W3037932933', 'https://openalex.org/W2095705004']",2020-04-09
https://openalex.org/W2110073835,https://doi.org/10.1109/icassp.2006.1660839,Unsupervised Training on Large Amounts of Broadcast News Data,"This paper presents our recent effort that aims at improving our Arabic broadcast news (BN) recognition system by using thousands of hours of un-transcribed Arabic audio in the way of unsupervised training. Unsupervised training is first carried out on the 1,900-hour English topic detection and tracking (TDT) data and is compared with the lightly-supervised training method that we have used for the DARPA EARS evaluations. The comparison shows that unsupervised training produces a 21.7% relative reduction in word error rate (WER), which is comparable to the gain obtained with light supervision methods. The same unsupervised training strategy carried out on a similar amount of Arabic BN data produces an 11.6% relative gain. The gain, though considerable, is substantially smaller than what is observed on the English data. Our initial work towards understanding the reasons for this difference is also described","['https://openalex.org/W1654858490', 'https://openalex.org/W3148201686', 'https://openalex.org/W6871885761', 'https://openalex.org/W2112006116', 'https://openalex.org/W6601596142', 'https://openalex.org/W2143577772', 'https://openalex.org/W1555037511', 'https://openalex.org/W38848722', 'https://openalex.org/W4402490925']",2006-08-03
https://openalex.org/W3015522062,https://doi.org/10.1109/icassp40776.2020.9054295,Self-Training for End-to-End Speech Recognition,"We revisit self-training in the context of end-to-end speech recognition. We\ndemonstrate that training with pseudo-labels can substantially improve the\naccuracy of a baseline model. Key to our approach are a strong baseline\nacoustic and language model used to generate the pseudo-labels, filtering\nmechanisms tailored to common errors from sequence-to-sequence models, and a\nnovel ensemble approach to increase pseudo-label diversity. Experiments on the\nLibriSpeech corpus show that with an ensemble of four models and label\nfiltering, self-training yields a 33.9% relative improvement in WER compared\nwith a baseline trained on 100 hours of labelled data in the noisy speech\nsetting. In the clean speech setting, self-training recovers 59.3% of the gap\nbetween the baseline and an oracle model, which is at least 93.8% relatively\nhigher than what previous approaches can achieve.\n","['https://openalex.org/W2972889948', 'https://openalex.org/W82886505', 'https://openalex.org/W6679434410', 'https://openalex.org/W2157331557', 'https://openalex.org/W2577366047', 'https://openalex.org/W2143017621', 'https://openalex.org/W2963250244', 'https://openalex.org/W6764730917', 'https://openalex.org/W6757424787', 'https://openalex.org/W1555037511', 'https://openalex.org/W1588359339', 'https://openalex.org/W2972630480', 'https://openalex.org/W2124558353', 'https://openalex.org/W2056786202', 'https://openalex.org/W1494198834', 'https://openalex.org/W2748795451', 'https://openalex.org/W2111316763', 'https://openalex.org/W2944255943', 'https://openalex.org/W2963739817', 'https://openalex.org/W2587275078', 'https://openalex.org/W6753186555', 'https://openalex.org/W2962907457', 'https://openalex.org/W2889213362', 'https://openalex.org/W2799800213', 'https://openalex.org/W2471138382', 'https://openalex.org/W2964308564', 'https://openalex.org/W2953320089', 'https://openalex.org/W3103005696', 'https://openalex.org/W2183341477', 'https://openalex.org/W2897105542', 'https://openalex.org/W2079057609', 'https://openalex.org/W2883586237', 'https://openalex.org/W1493379323', 'https://openalex.org/W2963400424', 'https://openalex.org/W2953190524', 'https://openalex.org/W2101210369', 'https://openalex.org/W2963216553', 'https://openalex.org/W2769320958', 'https://openalex.org/W2163568299', 'https://openalex.org/W2963956526', 'https://openalex.org/W2294962864', 'https://openalex.org/W2133564696', 'https://openalex.org/W2520160253', 'https://openalex.org/W2963970792', 'https://openalex.org/W1493009343', 'https://openalex.org/W2130942839', 'https://openalex.org/W2904818793', 'https://openalex.org/W2121764873']",2020-04-09
https://openalex.org/W1494198834,https://doi.org/10.1109/icassp.2015.7178964,Librispeech: An ASR corpus based on public domain audio books,"This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.","['https://openalex.org/W2113641473', 'https://openalex.org/W2090755665', 'https://openalex.org/W2148154194', 'https://openalex.org/W2106554350', 'https://openalex.org/W2087064593', 'https://openalex.org/W1647671624', 'https://openalex.org/W1517939602', 'https://openalex.org/W2037740282', 'https://openalex.org/W1599512239', 'https://openalex.org/W6631362777', 'https://openalex.org/W6603477829', 'https://openalex.org/W2024490156', 'https://openalex.org/W2164107060', 'https://openalex.org/W6712802073', 'https://openalex.org/W6677973343', 'https://openalex.org/W6636811518', 'https://openalex.org/W6738902873', 'https://openalex.org/W2097927681', 'https://openalex.org/W2026369565', 'https://openalex.org/W2330075180', 'https://openalex.org/W2950186769', 'https://openalex.org/W2125234026', 'https://openalex.org/W1524333225', 'https://openalex.org/W1934041838', 'https://openalex.org/W1631260214', 'https://openalex.org/W2397159106', 'https://openalex.org/W2620757702', 'https://openalex.org/W85707815', 'https://openalex.org/W2916535084']",2015-04-01
https://openalex.org/W2981283774,https://doi.org/10.21437/interspeech.2020-1532,Deep Speech Inpainting of Time-Frequency Masks,"Transient loud intrusions, often occurring in noisy environments, can\ncompletely overpower speech signal and lead to an inevitable loss of\ninformation. While existing algorithms for noise suppression can yield\nimpressive results, their efficacy remains limited for very low signal-to-noise\nratios or when parts of the signal are missing. To address these limitations,\nhere we propose an end-to-end framework for speech inpainting, the\ncontext-based retrieval of missing or severely distorted parts of\ntime-frequency representation of speech. The framework is based on a\nconvolutional U-Net trained via deep feature losses, obtained using speechVGG,\na deep speech feature extractor pre-trained on an auxiliary word classification\ntask. Our evaluation results demonstrate that the proposed framework can\nrecover large portions of missing or distorted time-frequency representation of\nspeech, up to 400 ms and 3.2 kHz in bandwidth. In particular, our approach\nprovided a substantial increase in STOI &amp; PESQ objective metrics of the\ninitially corrupted speech samples. Notably, using deep feature losses to train\nthe framework led to the best results, as compared to conventional approaches.\n","['https://openalex.org/W2044893557', 'https://openalex.org/W1522301498', 'https://openalex.org/W2962946126', 'https://openalex.org/W2963868408', 'https://openalex.org/W2738588019', 'https://openalex.org/W2972460025', 'https://openalex.org/W2108598243', 'https://openalex.org/W2936774411', 'https://openalex.org/W2972443522', 'https://openalex.org/W2394461504', 'https://openalex.org/W2741913599', 'https://openalex.org/W2972785266', 'https://openalex.org/W2078528584', 'https://openalex.org/W4286800003', 'https://openalex.org/W1494198834', 'https://openalex.org/W1921523184', 'https://openalex.org/W2963276790', 'https://openalex.org/W1901129140', 'https://openalex.org/W2988157778', 'https://openalex.org/W2798365772', 'https://openalex.org/W2507424718', 'https://openalex.org/W3043547428', 'https://openalex.org/W2243752967', 'https://openalex.org/W2982136612', 'https://openalex.org/W3105183525', 'https://openalex.org/W1836465849', 'https://openalex.org/W2963341071', 'https://openalex.org/W2069681747', 'https://openalex.org/W1686810756', 'https://openalex.org/W2972542211', 'https://openalex.org/W204144752', 'https://openalex.org/W2137539246', 'https://openalex.org/W1897240248', 'https://openalex.org/W2962866211', 'https://openalex.org/W2067295501', 'https://openalex.org/W2146423068', 'https://openalex.org/W2964121744', 'https://openalex.org/W1825675169', 'https://openalex.org/W2962835968', 'https://openalex.org/W2296153915']",2020-10-25
https://openalex.org/W3035524453,https://doi.org/10.1109/cvpr42600.2020.00975,Momentum Contrast for Unsupervised Visual Representation Learning,"We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.","['https://openalex.org/W2147800946', 'https://openalex.org/W2913939497', 'https://openalex.org/W2194775991', 'https://openalex.org/W2963150697', 'https://openalex.org/W2991391304', 'https://openalex.org/W2144794286', 'https://openalex.org/W6638667902', 'https://openalex.org/W6754278344', 'https://openalex.org/W6698183232', 'https://openalex.org/W2549139847', 'https://openalex.org/W2798991696', 'https://openalex.org/W2933502442', 'https://openalex.org/W6682948231', 'https://openalex.org/W6701655646', 'https://openalex.org/W2948672349', 'https://openalex.org/W2558661413', 'https://openalex.org/W2998388430', 'https://openalex.org/W2138621090', 'https://openalex.org/W2785694322', 'https://openalex.org/W1536680647', 'https://openalex.org/W2102605133', 'https://openalex.org/W6741529945', 'https://openalex.org/W2990873191', 'https://openalex.org/W219040644', 'https://openalex.org/W2025768430', 'https://openalex.org/W2797977484', 'https://openalex.org/W2131846894', 'https://openalex.org/W6637373629', 'https://openalex.org/W6620707391', 'https://openalex.org/W2340897893', 'https://openalex.org/W2108598243', 'https://openalex.org/W6755207826', 'https://openalex.org/W343636949', 'https://openalex.org/W2962824366', 'https://openalex.org/W6715501732', 'https://openalex.org/W6682250724', 'https://openalex.org/W2031489346', 'https://openalex.org/W6747899497', 'https://openalex.org/W2987741655', 'https://openalex.org/W6753000030', 'https://openalex.org/W6715287400', 'https://openalex.org/W1976921161', 'https://openalex.org/W2963016543', 'https://openalex.org/W6686418764', 'https://openalex.org/W6700872662', 'https://openalex.org/W2963420272', 'https://openalex.org/W2575671312', 'https://openalex.org/W6639102338', 'https://openalex.org/W2565639579', 'https://openalex.org/W6751037545', 'https://openalex.org/W1903029394', 'https://openalex.org/W1861492603', 'https://openalex.org/W2250384498', 'https://openalex.org/W2963876278', 'https://openalex.org/W2184852195', 'https://openalex.org/W2342877626', 'https://openalex.org/W2737740651', 'https://openalex.org/W639708223', 'https://openalex.org/W2979579363', 'https://openalex.org/W4297808394', 'https://openalex.org/W3108655343', 'https://openalex.org/W3005680577', 'https://openalex.org/W2148349024', 'https://openalex.org/W2963341956', 'https://openalex.org/W2948012107', 'https://openalex.org/W2963265008', 'https://openalex.org/W2099471712', 'https://openalex.org/W2953469440', 'https://openalex.org/W2887997457', 'https://openalex.org/W2842511635', 'https://openalex.org/W4294568686', 'https://openalex.org/W2412782625', 'https://openalex.org/W2949517790', 'https://openalex.org/W2613718673', 'https://openalex.org/W1686810756', 'https://openalex.org/W3009561768', 'https://openalex.org/W1836465849', 'https://openalex.org/W2944828972', 'https://openalex.org/W2963074118', 'https://openalex.org/W2302255633', 'https://openalex.org/W2799269579', 'https://openalex.org/W2896457183', 'https://openalex.org/W2952865063', 'https://openalex.org/W2622263826', 'https://openalex.org/W2883725317', 'https://openalex.org/W2970241862', 'https://openalex.org/W2785325870', 'https://openalex.org/W2321533354', 'https://openalex.org/W2326925005', 'https://openalex.org/W3103455452', 'https://openalex.org/W2962835968', 'https://openalex.org/W2963703197', 'https://openalex.org/W2953139137', 'https://openalex.org/W4320013936', 'https://openalex.org/W2963684275', 'https://openalex.org/W2152790380']",2020-06-01
https://openalex.org/W2750248772,https://doi.org/10.21437/interspeech.2017-1160,Hidden Markov Model Variational Autoencoder for Acoustic Unit Discovery,"Variational Autoencoders (VAEs) have been shown to provide efficient neural-network-based approximate Bayesian inference for observation models for which exact inference is intractable. Its extension, the so-called Structured VAE (SVAE) allows inference in the presence of both discrete and continuous latent variables. Inspired by this extension, we developed a VAE with Hidden Markov Models (HMMs) as latent models. We applied the resulting HMM-VAE to the task of acoustic unit discovery in a zero resource scenario. Starting from an initial model based on variational inference in an HMM with Gaussian Mixture Model (GMM) emission probabilities, the accuracy of the acoustic unit discovery could be significantly improved by the HMM-VAE. In doing so we were able to demonstrate for an unsupervised learning task what is well-known in the supervised learning case: Neural networks provide superior modeling power compared to GMMs.","['https://openalex.org/W1796128977', 'https://openalex.org/W2468716020', 'https://openalex.org/W2159283619', 'https://openalex.org/W2078769636', 'https://openalex.org/W2347098582', 'https://openalex.org/W2962695963', 'https://openalex.org/W2117041980', 'https://openalex.org/W1971081490', 'https://openalex.org/W2142384583', 'https://openalex.org/W2556467266', 'https://openalex.org/W2100768664', 'https://openalex.org/W2077804127', 'https://openalex.org/W2949416428']",2017-08-16
https://openalex.org/W2842511635,,Representation Learning with Contrastive Predictive Coding,"While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.","['https://openalex.org/W2412320034', 'https://openalex.org/W2187089797', 'https://openalex.org/W2160660844', 'https://openalex.org/W2152808281', 'https://openalex.org/W2964127152', 'https://openalex.org/W2163605009', 'https://openalex.org/W2302255633', 'https://openalex.org/W2014902591', 'https://openalex.org/W343636949', 'https://openalex.org/W2157331557', 'https://openalex.org/W2950726992', 'https://openalex.org/W1494198834', 'https://openalex.org/W2106053110', 'https://openalex.org/W2160815625', 'https://openalex.org/W2606347107', 'https://openalex.org/W1522301498', 'https://openalex.org/W2321533354', 'https://openalex.org/W2963403868', 'https://openalex.org/W2127958135', 'https://openalex.org/W3099206234', 'https://openalex.org/W219040644', 'https://openalex.org/W2964043796', 'https://openalex.org/W2326925005', 'https://openalex.org/W1895577753', 'https://openalex.org/W2259472270', 'https://openalex.org/W2786036274', 'https://openalex.org/W2952186591', 'https://openalex.org/W1836465849', 'https://openalex.org/W2949382160', 'https://openalex.org/W2962824366', 'https://openalex.org/W3037932933', 'https://openalex.org/W2114524997', 'https://openalex.org/W1681397005', 'https://openalex.org/W2130942839', 'https://openalex.org/W2070246124', 'https://openalex.org/W2037034710', 'https://openalex.org/W2950577311', 'https://openalex.org/W2112129677', 'https://openalex.org/W2119885245', 'https://openalex.org/W2950797609', 'https://openalex.org/W2963762683', 'https://openalex.org/W2146444479', 'https://openalex.org/W2152790380', 'https://openalex.org/W2131744502', 'https://openalex.org/W3189092450', 'https://openalex.org/W1566289585', 'https://openalex.org/W1524333225', 'https://openalex.org/W2949536664', 'https://openalex.org/W2157364932', 'https://openalex.org/W2117539524']",2018-07-10
https://openalex.org/W2146444479,https://doi.org/10.1162/089976602317318938,Slow Feature Analysis: Unsupervised Learning of Invariances,"Invariant features of temporally varying signals are useful for analysis and classification. Slow feature analysis (SFA) is a new method for learning invariant or slowly varying features from a vectorial input signal. It is based on a nonlinear expansion of the input signal and application of principal component analysis to this expanded signal and its time derivative. It is guaranteed to find the optimal solution within a family of functions directly and can learn to extract a large number of decor-related features, which are ordered by their degree of invariance. SFA can be applied hierarchically to process high-dimensional input signals and extract complex features. SFA is applied first to complex cell tuning properties based on simple cell output, including disparity and motion. Then more complicated input-output functions are learned by repeated application of SFA. Finally, a hierarchical network of SFA modules is presented as a simple model of the visual system. The same unstructured network can learn translation, size, rotation, contrast, or, to a lesser degree, illumination invariance for one-dimensional objects, depending on only the training stimulus. Surprisingly, only a few training objects suffice to achieve good generalization to new objects. The generated representation is suitable for object recognition. Performance degrades if the network is trained to learn multiple invariances simultaneously.","['https://openalex.org/W4230610536', 'https://openalex.org/W2063971957', 'https://openalex.org/W2056415038', 'https://openalex.org/W2108384452', 'https://openalex.org/W2031912957', 'https://openalex.org/W2096388912', 'https://openalex.org/W2152996925', 'https://openalex.org/W2170302354', 'https://openalex.org/W1914401667', 'https://openalex.org/W2008780668', 'https://openalex.org/W2147800946', 'https://openalex.org/W2035353864', 'https://openalex.org/W2115441154', 'https://openalex.org/W2067570568', 'https://openalex.org/W2009236066', 'https://openalex.org/W2105884502', 'https://openalex.org/W4237768773', 'https://openalex.org/W2079210701', 'https://openalex.org/W2021234908', 'https://openalex.org/W2012432354', 'https://openalex.org/W1963597874', 'https://openalex.org/W2256679588', 'https://openalex.org/W1943381760', 'https://openalex.org/W1996417565', 'https://openalex.org/W2156909104', 'https://openalex.org/W2042561991', 'https://openalex.org/W1554663460', 'https://openalex.org/W2037492543', 'https://openalex.org/W2157349214', 'https://openalex.org/W2153944708', 'https://openalex.org/W81573328', 'https://openalex.org/W2092016629', 'https://openalex.org/W97083571', 'https://openalex.org/W2142150270', 'https://openalex.org/W1886989851', 'https://openalex.org/W2116574848', 'https://openalex.org/W18965947', 'https://openalex.org/W2079096572', 'https://openalex.org/W162059533', 'https://openalex.org/W2075418588', 'https://openalex.org/W2340087731', 'https://openalex.org/W2142457795', 'https://openalex.org/W2166676140', 'https://openalex.org/W25772085', 'https://openalex.org/W2087984924', 'https://openalex.org/W2039556170', 'https://openalex.org/W2106451016']",2002-04-01
https://openalex.org/W2933138175,https://doi.org/10.18653/v1/n19-4009,"fairseq: A Fast, Extensible Toolkit for Sequence Modeling","Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations). 2019.","['https://openalex.org/W2633911929', 'https://openalex.org/W2964190861', 'https://openalex.org/W2519314406', 'https://openalex.org/W4293718192', 'https://openalex.org/W2973088264', 'https://openalex.org/W2612675303', 'https://openalex.org/W4385245566', 'https://openalex.org/W2888482885', 'https://openalex.org/W2963112338', 'https://openalex.org/W2792376130', 'https://openalex.org/W2889518897', 'https://openalex.org/W2963366552', 'https://openalex.org/W2964258094', 'https://openalex.org/W2613904329', 'https://openalex.org/W2963500743', 'https://openalex.org/W4297747548', 'https://openalex.org/W2889326796', 'https://openalex.org/W2963403868', 'https://openalex.org/W2962805889', 'https://openalex.org/W2963631907', 'https://openalex.org/W2963475460', 'https://openalex.org/W2898875342', 'https://openalex.org/W2963418779', 'https://openalex.org/W2952339051', 'https://openalex.org/W2963034893', 'https://openalex.org/W2963773505', 'https://openalex.org/W2963096510', 'https://openalex.org/W1544827683', 'https://openalex.org/W2963970792', 'https://openalex.org/W2797328513', 'https://openalex.org/W2914855263', 'https://openalex.org/W2890914727', 'https://openalex.org/W2962974452', 'https://openalex.org/W2606974598', 'https://openalex.org/W2950513705', 'https://openalex.org/W2963212250', 'https://openalex.org/W3082674894', 'https://openalex.org/W2567070169', 'https://openalex.org/W4293569541', 'https://openalex.org/W2963206679', 'https://openalex.org/W2963263347', 'https://openalex.org/W2963532001', 'https://openalex.org/W2804704270', 'https://openalex.org/W2964265128', 'https://openalex.org/W2154652894', 'https://openalex.org/W2767989436', 'https://openalex.org/W2259472270', 'https://openalex.org/W2785047343', 'https://openalex.org/W2962784628', 'https://openalex.org/W2763421725', 'https://openalex.org/W2798931235', 'https://openalex.org/W2807964741', 'https://openalex.org/W2949555952', 'https://openalex.org/W2963929190', 'https://openalex.org/W2963807318', 'https://openalex.org/W4297788867', 'https://openalex.org/W2795285343', 'https://openalex.org/W2963925437', 'https://openalex.org/W2951672049', 'https://openalex.org/W2951559648', 'https://openalex.org/W2964343359', 'https://openalex.org/W1938755728', 'https://openalex.org/W1902237438', 'https://openalex.org/W2778814079', 'https://openalex.org/W2949615363', 'https://openalex.org/W4297801368']",2019-01-01
https://openalex.org/W2946417913,https://doi.org/10.18653/v1/p19-1452,BERT Rediscovers the Classical NLP Pipeline,"Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.","['https://openalex.org/W2963651521', 'https://openalex.org/W2896667998', 'https://openalex.org/W2888922637', 'https://openalex.org/W2964204621', 'https://openalex.org/W2153579005', 'https://openalex.org/W2123442489', 'https://openalex.org/W4294170691', 'https://openalex.org/W2605221307', 'https://openalex.org/W2962739339', 'https://openalex.org/W2896457183', 'https://openalex.org/W2181042685', 'https://openalex.org/W2180093461', 'https://openalex.org/W2963854351', 'https://openalex.org/W4385245566', 'https://openalex.org/W2563574619', 'https://openalex.org/W2889468083', 'https://openalex.org/W2963341956', 'https://openalex.org/W2799124508', 'https://openalex.org/W4288351520', 'https://openalex.org/W2250263931', 'https://openalex.org/W2888329843']",2019-01-01
https://openalex.org/W3008525923,https://doi.org/10.1109/asru46091.2019.9003972,From Senones to Chenones: Tied Context-Dependent Graphemes for Hybrid Speech Recognition,"There is an implicit assumption that traditional hybrid approaches for automatic speech recognition (ASR) cannot directly model graphemes and need to rely on phonetic lexicons to get competitive performance, especially on English which has poor grapheme-phoneme correspondence. In this work, we show for the first time that, on English, hybrid ASR systems can in fact model graphemes effectively by leveraging tied context-dependent graphemes, i.e., chenones. Our chenone-based systems significantly outperform equivalent senone baselines by 4.5% to 11.1% relative on three different English datasets. Our results on Librispeech are state-of-the-art compared to other hybrid approaches and competitive with previously published end-to-end numbers. Further analysis shows that chenones can better utilize powerful acoustic models and large training data, and require context- and position-dependent modeling to work well. Chenone-based systems also outperform senone baselines on proper noun and rare word recognition, an area where the latter is traditionally thought to have an advantage. Our work provides an alternative for end-to-end ASR and establishes that hybrid systems can be improved by dropping the reliance on phonetic knowledge.","['https://openalex.org/W2890197052', 'https://openalex.org/W2963970535', 'https://openalex.org/W6679901520', 'https://openalex.org/W2117541506', 'https://openalex.org/W2090755665', 'https://openalex.org/W1494198834', 'https://openalex.org/W2963070863', 'https://openalex.org/W2545177271', 'https://openalex.org/W2964107261', 'https://openalex.org/W2936774411', 'https://openalex.org/W6741807409', 'https://openalex.org/W6638749077', 'https://openalex.org/W2962760690', 'https://openalex.org/W1993882792', 'https://openalex.org/W2125964738', 'https://openalex.org/W6629052376', 'https://openalex.org/W6623517193', 'https://openalex.org/W6687566353', 'https://openalex.org/W2750499125', 'https://openalex.org/W2799800213', 'https://openalex.org/W2962824709', 'https://openalex.org/W2888867175', 'https://openalex.org/W2158069733', 'https://openalex.org/W2964084166', 'https://openalex.org/W1510837697', 'https://openalex.org/W6697471272', 'https://openalex.org/W1506752962', 'https://openalex.org/W2889282842', 'https://openalex.org/W2963540920', 'https://openalex.org/W2127141656', 'https://openalex.org/W2005708641', 'https://openalex.org/W6675365184', 'https://openalex.org/W2160815625', 'https://openalex.org/W2147768505', 'https://openalex.org/W6696934422', 'https://openalex.org/W2143612262', 'https://openalex.org/W1600744878', 'https://openalex.org/W6696982659', 'https://openalex.org/W2973215447', 'https://openalex.org/W2729190387', 'https://openalex.org/W6786243342', 'https://openalex.org/W2972630480', 'https://openalex.org/W6631190155', 'https://openalex.org/W6713762819', 'https://openalex.org/W2514741789', 'https://openalex.org/W2405883473', 'https://openalex.org/W2293634267', 'https://openalex.org/W2327501763', 'https://openalex.org/W1522301498', 'https://openalex.org/W3150637114', 'https://openalex.org/W3103005696', 'https://openalex.org/W4297818305', 'https://openalex.org/W2407080277', 'https://openalex.org/W2963920996', 'https://openalex.org/W2102113734', 'https://openalex.org/W854541894', 'https://openalex.org/W2184045248', 'https://openalex.org/W2782451907', 'https://openalex.org/W2193413348', 'https://openalex.org/W1489125746', 'https://openalex.org/W2293009711', 'https://openalex.org/W2294752925', 'https://openalex.org/W2964121744', 'https://openalex.org/W1828163288', 'https://openalex.org/W2963827914']",2019-12-01
https://openalex.org/W2963618559,,Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data,"We present a factorized hierarchical variational autoencoder, which learns disentangled and interpretable representations from sequential data without supervision. Specifically, we exploit the multi-scale nature of information in sequential data by formulating it explicitly within a factorized hierarchical graphical model that imposes sequence-dependent priors and sequence-independent priors to different sets of latent variables. The model is evaluated on two speech corpora to demonstrate, qualitatively, its ability to transform speakers or linguistic content by manipulating different sets of latent variables; and quantitatively, its ability to outperform an i-vector baseline for speaker verification and reduce the word error rate by as much as 35% in mismatched train/test scenarios for automatic speech recognition tasks.",[],2017-01-01
https://openalex.org/W2953190524,https://doi.org/10.1109/icassp.2019.8683535,Wav2Letter++: A Fast Open-source Speech Recognition System,"This paper introduces wav2letter++, the fastest open-source deep learning\nspeech recognition framework. wav2letter++ is written entirely in C++, and uses\nthe ArrayFire tensor library for maximum efficiency. Here we explain the\narchitecture and design of the wav2letter++ system and compare it to other\nmajor open-source speech recognition systems. In some cases wav2letter++ is\nmore than 2x faster than other optimized frameworks for training end-to-end\nneural networks for speech recognition. We also show that wav2letter++'s\ntraining times scale linearly to 64 GPUs, the highest we tested, for models\nwith 100 million parameters. High-performance frameworks enable fast iteration,\nwhich is often a crucial factor in successful research and model tuning on new\ndatasets and tasks.\n","['https://openalex.org/W6727336983', 'https://openalex.org/W6679434410', 'https://openalex.org/W6623517193', 'https://openalex.org/W6640090968', 'https://openalex.org/W6747270024', 'https://openalex.org/W6679855610', 'https://openalex.org/W6756040250', 'https://openalex.org/W6713134421', 'https://openalex.org/W2024490156', 'https://openalex.org/W2963211739', 'https://openalex.org/W2964277995', 'https://openalex.org/W1988057723', 'https://openalex.org/W2102182691', 'https://openalex.org/W6637151318', 'https://openalex.org/W2962780374', 'https://openalex.org/W6631362777', 'https://openalex.org/W2127141656', 'https://openalex.org/W2514741789', 'https://openalex.org/W1524333225', 'https://openalex.org/W2964308564', 'https://openalex.org/W2133564696', 'https://openalex.org/W2134800885', 'https://openalex.org/W2795935804', 'https://openalex.org/W2901739041', 'https://openalex.org/W2520160253', 'https://openalex.org/W2953384591', 'https://openalex.org/W854541894', 'https://openalex.org/W2781384251', 'https://openalex.org/W1922655562', 'https://openalex.org/W2899771611', 'https://openalex.org/W2804704270', 'https://openalex.org/W4302296459', 'https://openalex.org/W2404126548', 'https://openalex.org/W4297818305', 'https://openalex.org/W1667652561', 'https://openalex.org/W2402144811']",2019-04-17
https://openalex.org/W2988736778,https://doi.org/10.48550/arxiv.1911.03912,Effectiveness of self-supervised pre-training for speech recognition,"We compare self-supervised representation learning algorithms which either explicitly quantize the audio data or learn representations without quantization. We find the former to be more accurate since it builds a good vocabulary of the data through vq-wav2vec [1] to enable learning of effective representations in subsequent BERT training. Different to previous work, we directly fine-tune the pre-trained BERT models on transcribed speech using a Connectionist Temporal Classification (CTC) loss instead of feeding the representations into a task-specific model. We also propose a BERT-style model learning directly from the continuous audio data and compare pre-training on raw audio to spectral features. Fine-tuning a BERT model on 10 hour of labeled Librispeech data with a vq-wav2vec vocabulary is almost as good as the best known reported system trained on 100 hours of labeled data on testclean, while achieving a 25% WER reduction on test-other. When using only 10 minutes of labeled data, WER is 25.2 on test-other and 16.3 on test-clean. This demonstrates that self-supervision can enable speech recognition systems trained on a near-zero amount of transcribed data.","['https://openalex.org/W2933138175', 'https://openalex.org/W1494198834', 'https://openalex.org/W2972943112', 'https://openalex.org/W1994606281', 'https://openalex.org/W2970119519', 'https://openalex.org/W2964110616', 'https://openalex.org/W2025482506', 'https://openalex.org/W2025198378', 'https://openalex.org/W2114347655', 'https://openalex.org/W2932675979', 'https://openalex.org/W2153579005', 'https://openalex.org/W3103005696', 'https://openalex.org/W2963807318', 'https://openalex.org/W3207342693', 'https://openalex.org/W2962907457', 'https://openalex.org/W2127141656', 'https://openalex.org/W2586148577', 'https://openalex.org/W2941814890', 'https://openalex.org/W2940322076', 'https://openalex.org/W2296681920', 'https://openalex.org/W2965373594', 'https://openalex.org/W2953190524', 'https://openalex.org/W2996383576', 'https://openalex.org/W2963341956', 'https://openalex.org/W2842511635', 'https://openalex.org/W2998649947', 'https://openalex.org/W1545920196', 'https://openalex.org/W3015522062', 'https://openalex.org/W2995181338', 'https://openalex.org/W2787779284', 'https://openalex.org/W2963425185', 'https://openalex.org/W2055408826', 'https://openalex.org/W1978660892', 'https://openalex.org/W2291975472', 'https://openalex.org/W2059652594', 'https://openalex.org/W30845872', 'https://openalex.org/W2973049979', 'https://openalex.org/W2163922914', 'https://openalex.org/W2667408400', 'https://openalex.org/W2964115348', 'https://openalex.org/W2514608284', 'https://openalex.org/W2566587499', 'https://openalex.org/W2106440210', 'https://openalex.org/W2963571336', 'https://openalex.org/W2963382687', 'https://openalex.org/W2936774411', 'https://openalex.org/W1524333225', 'https://openalex.org/W2981857663', 'https://openalex.org/W2964001192', 'https://openalex.org/W2948012107', 'https://openalex.org/W2963799213', 'https://openalex.org/W2172097686', 'https://openalex.org/W2557283755', 'https://openalex.org/W2124558353', 'https://openalex.org/W3127686677', 'https://openalex.org/W2520160253', 'https://openalex.org/W2608612081']",2019-11-10
https://openalex.org/W3148040514,https://doi.org/10.1109/slt48900.2021.9383575,Audio Albert: A Lite Bert for Self-Supervised Learning of Audio Representation,"Self-supervised speech models are powerful speech representation extractors for downstream applications. Recently, larger models have been utilized in acoustic model training to achieve better performance. We propose Audio ALBERT, a lite version of the self-supervised speech representation model. We apply the lightweight representation extractor to two downstream tasks, speaker classification and phoneme classification. We show that Audio ALBERT achieves performance comparable with massive pre-trained networks in the downstream tasks while having 91% fewer parameters. Moreover, we design probing models to measure how much the latent representations can encode the speaker's and phoneme's information. We find that the representations encoded in internal layers of Audio ALBERT contain more information for both phoneme and speaker than the last layer, which is generally used for downstream tasks. Our findings provide a new avenue for using self-supervised networks to achieve better performance and efficiency.","['https://openalex.org/W6768021236', 'https://openalex.org/W6774314701', 'https://openalex.org/W3035524453', 'https://openalex.org/W6763701032', 'https://openalex.org/W6766673545', 'https://openalex.org/W2842511635', 'https://openalex.org/W2972943112', 'https://openalex.org/W6769686700', 'https://openalex.org/W6769196770', 'https://openalex.org/W2963347649', 'https://openalex.org/W2747874407', 'https://openalex.org/W1494198834', 'https://openalex.org/W6769238691', 'https://openalex.org/W2982223350', 'https://openalex.org/W2972889948', 'https://openalex.org/W3015265920', 'https://openalex.org/W2962739339', 'https://openalex.org/W2973049979', 'https://openalex.org/W6755207826', 'https://openalex.org/W2965046076', 'https://openalex.org/W6753640285', 'https://openalex.org/W2963928591', 'https://openalex.org/W3015412285', 'https://openalex.org/W2948947170', 'https://openalex.org/W6757817989', 'https://openalex.org/W2972808286', 'https://openalex.org/W2187089797', 'https://openalex.org/W2908510526', 'https://openalex.org/W2970597249', 'https://openalex.org/W2996428491', 'https://openalex.org/W2963341956', 'https://openalex.org/W2896457183', 'https://openalex.org/W2965373594', 'https://openalex.org/W2981991061', 'https://openalex.org/W3096485810', 'https://openalex.org/W2866343820', 'https://openalex.org/W2979476256', 'https://openalex.org/W3005680577', 'https://openalex.org/W2996383576', 'https://openalex.org/W4297808394']",2021-01-19
https://openalex.org/W1522301498,https://doi.org/10.48550/arxiv.1412.6980,Adam: A Method for Stochastic Optimization,"We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.","['https://openalex.org/W1984541135', 'https://openalex.org/W6908809', 'https://openalex.org/W2100495367', 'https://openalex.org/W1810943226', 'https://openalex.org/W35527955', 'https://openalex.org/W2143612262', 'https://openalex.org/W104184427', 'https://openalex.org/W1904365287', 'https://openalex.org/W2113459411', 'https://openalex.org/W2950351588', 'https://openalex.org/W59018853', 'https://openalex.org/W2146502635', 'https://openalex.org/W2160815625', 'https://openalex.org/W3176583243', 'https://openalex.org/W2156779765', 'https://openalex.org/W2148825261', 'https://openalex.org/W2951004968', 'https://openalex.org/W1598497354', 'https://openalex.org/W1491468723', 'https://openalex.org/W2086161653']",2014-12-22
https://openalex.org/W2100768664,,A Nonparametric Bayesian Approach to Acoustic Model Discovery,"We investigate the problem of acoustic modeling in which prior language-specific knowledge and transcribed data are unavailable. We present an unsupervised model that simultaneously segments the speech, discovers a proper set of sub-word units (e.g., phones) and learns a Hidden Markov Model (HMM) for each induced acoustic unit. Our approach is formulated as a Dirichlet process mixture model in which each mixture is an HMM that represents a sub-word unit. We apply our model to the TIMIT corpus, and the results demonstrate that our model discovers sub-word units that are highly correlated with English phones and also produces better segmentation than the state-of-the-art unsupervised baseline. We test the quality of the learned acoustic models on a spoken term detection task. Compared to the baselines, our model improves the relative precision of top hits by at least 22.1 % and outperforms a language-mismatched acoustic model. 1","['https://openalex.org/W2099415988', 'https://openalex.org/W2080972498', 'https://openalex.org/W53570656', 'https://openalex.org/W2117041980', 'https://openalex.org/W2120636621', 'https://openalex.org/W2154093685', 'https://openalex.org/W2077804127', 'https://openalex.org/W2401464865', 'https://openalex.org/W2121997342', 'https://openalex.org/W2026858810', 'https://openalex.org/W1957665339', 'https://openalex.org/W1990005915', 'https://openalex.org/W2083904075', 'https://openalex.org/W3127686677', 'https://openalex.org/W2045656233', 'https://openalex.org/W2126377586', 'https://openalex.org/W2148154194', 'https://openalex.org/W2111732304', 'https://openalex.org/W2171752983', 'https://openalex.org/W2126203737', 'https://openalex.org/W3104490327', 'https://openalex.org/W2403642609']",2012-07-08
https://openalex.org/W2970971581,https://doi.org/10.48550/arxiv.1912.01703,"PyTorch: An Imperative Style, High-Performance Deep Learning Library","Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.",[],2019-12-03
https://openalex.org/W4287824654,https://doi.org/10.4230/oasics.slate.2022.3,Question Answering For Toxicological Information Extraction,"Working with large amounts of text data has become hectic and time-consuming. In order to reduce human effort, costs, and make the process more efficient, companies and organizations resort to intelligent algorithms to automate and assist the manual work. This problem is also present in the field of toxicological analysis of chemical substances, where information needs to be searched from multiple documents. That said, we propose an approach that relies on Question Answering for acquiring information from unstructured data, in our case, English PDF documents containing information about physicochemical and toxicological properties of chemical substances. Experimental results confirm that our approach achieves promising results which can be applicable in the business scenario, especially if further revised by humans.",[],2022-01-01
https://openalex.org/W2950180292,https://doi.org/10.48550/arxiv.1807.05520,Deep Clustering for Unsupervised Learning of Visual Features,"Clustering is a class of unsupervised learning methods that has been extensively applied and studied in computer vision. Little work has been done to adapt it to the end-to-end training of visual features on large scale datasets. In this work, we present DeepCluster, a clustering method that jointly learns the parameters of a neural network and the cluster assignments of the resulting features. DeepCluster iteratively groups the features with a standard clustering algorithm, k-means, and uses the subsequent assignments as supervision to update the weights of the network. We apply DeepCluster to the unsupervised training of convolutional neural networks on large datasets like ImageNet and YFCC100M. The resulting model outperforms the current state of the art by a significant margin on all the standard benchmarks.","['https://openalex.org/W1989684337', 'https://openalex.org/W1836465849', 'https://openalex.org/W2108282816', 'https://openalex.org/W1480376833', 'https://openalex.org/W2949971241', 'https://openalex.org/W2326925005', 'https://openalex.org/W2750912449', 'https://openalex.org/W1520997877', 'https://openalex.org/W180242331', 'https://openalex.org/W1677182931', 'https://openalex.org/W1849277567', 'https://openalex.org/W2113221323', 'https://openalex.org/W2963495051', 'https://openalex.org/W2613718673', 'https://openalex.org/W1625255723', 'https://openalex.org/W2112796928', 'https://openalex.org/W2951004968', 'https://openalex.org/W2963474899', 'https://openalex.org/W2097820631', 'https://openalex.org/W2554692997', 'https://openalex.org/W2212123867', 'https://openalex.org/W2137145201', 'https://openalex.org/W343636949', 'https://openalex.org/W2737057113', 'https://openalex.org/W2575671312', 'https://openalex.org/W2117539524', 'https://openalex.org/W2963420272', 'https://openalex.org/W2964074409', 'https://openalex.org/W1955857676', 'https://openalex.org/W2950043193', 'https://openalex.org/W2168912077', 'https://openalex.org/W2412782625', 'https://openalex.org/W2103716973', 'https://openalex.org/W2134670479', 'https://openalex.org/W2412320034', 'https://openalex.org/W2511730936', 'https://openalex.org/W1825675169', 'https://openalex.org/W2287334441', 'https://openalex.org/W2121947440', 'https://openalex.org/W2511428026', 'https://openalex.org/W219040644', 'https://openalex.org/W2108598243', 'https://openalex.org/W2163605009', 'https://openalex.org/W2949532563', 'https://openalex.org/W2123872146', 'https://openalex.org/W2132820034', 'https://openalex.org/W2593864460', 'https://openalex.org/W2774008708', 'https://openalex.org/W1544092585', 'https://openalex.org/W2560977758', 'https://openalex.org/W2113896236', 'https://openalex.org/W2212363941', 'https://openalex.org/W2949667497', 'https://openalex.org/W2139427956', 'https://openalex.org/W2321533354', 'https://openalex.org/W2110798204', 'https://openalex.org/W2100031962', 'https://openalex.org/W2743157634', 'https://openalex.org/W2162762921', 'https://openalex.org/W2145094598', 'https://openalex.org/W1686810756', 'https://openalex.org/W2148349024', 'https://openalex.org/W2411541852', 'https://openalex.org/W2095705004', 'https://openalex.org/W2962749380', 'https://openalex.org/W2962852342', 'https://openalex.org/W2308529009', 'https://openalex.org/W2148809531', 'https://openalex.org/W2174726731', 'https://openalex.org/W2099471712', 'https://openalex.org/W2098693229', 'https://openalex.org/W2141362318', 'https://openalex.org/W2750549109']",2018-07-15
https://openalex.org/W2972943112,https://doi.org/10.21437/interspeech.2019-1473,An Unsupervised Autoregressive Model for Speech Representation Learning,"This paper proposes a novel unsupervised autoregressive neural model for learning generic speech representations.In contrast to other speech representation learning methods that aim to remove noise or speaker variabilities, ours is designed to preserve information for a wide range of downstream tasks.In addition, the proposed model does not require any phonetic or word boundary labels, allowing the model to benefit from large quantities of unlabeled data.Speech representations learned by our model significantly improve performance on both phone classification and speaker verification over the surface features and other supervised and unsupervised approaches.Further analysis shows that different levels of speech information are captured by our model at different layers.In particular, the lower layers tend to be more discriminative for speakers, while the upper layers provide more phonetic content.","['https://openalex.org/W1522301498', 'https://openalex.org/W2962850167', 'https://openalex.org/W179875071', 'https://openalex.org/W2963571336', 'https://openalex.org/W2525778437', 'https://openalex.org/W2951585248', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962739339', 'https://openalex.org/W2997574889', 'https://openalex.org/W343636949', 'https://openalex.org/W2599837529', 'https://openalex.org/W2550241133', 'https://openalex.org/W2758785877', 'https://openalex.org/W2150769028', 'https://openalex.org/W2888329843', 'https://openalex.org/W2519091744', 'https://openalex.org/W219040644', 'https://openalex.org/W2963425185', 'https://openalex.org/W4297808394', 'https://openalex.org/W2963026768', 'https://openalex.org/W2963317665', 'https://openalex.org/W2963609956', 'https://openalex.org/W1686946872', 'https://openalex.org/W2190506272', 'https://openalex.org/W3125709657', 'https://openalex.org/W2024490156', 'https://openalex.org/W2194775991', 'https://openalex.org/W2896457183', 'https://openalex.org/W2827410935', 'https://openalex.org/W2395899413']",2019-09-13
https://openalex.org/W3016011332,https://doi.org/10.1109/icassp40776.2020.9054438,Generative Pre-Training for Speech with Autoregressive Predictive Coding,"Learning meaningful and general representations from unannotated speech that are applicable to a wide range of tasks remains challenging. In this paper we propose to use autoregressive predictive coding (APC), a recently proposed self-supervised objective, as a generative pre-training approach for learning meaningful, non-specific, and transferable speech representations. We pre-train APC on large-scale unlabeled data and conduct transfer learning experiments on three speech applications that require different information about speech characteristics to perform well: speech recognition, speech translation, and speaker identification. Extensive experiments show that APC not only outperforms surface features (e.g., log Mel spectrograms) and other popular representation learning methods on all three tasks, but is also effective at reducing downstream labeled data size and model parameters. We also investigate the use of Transformers for modeling APC and find it superior to RNNs.","['https://openalex.org/W2889028433', 'https://openalex.org/W2943845043', 'https://openalex.org/W6761563299', 'https://openalex.org/W2963026768', 'https://openalex.org/W6752888775', 'https://openalex.org/W6764498146', 'https://openalex.org/W2973217961', 'https://openalex.org/W2973034126', 'https://openalex.org/W6745117592', 'https://openalex.org/W179875071', 'https://openalex.org/W6739901393', 'https://openalex.org/W2964199361', 'https://openalex.org/W6748148878', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962739339', 'https://openalex.org/W2194775991', 'https://openalex.org/W6755977528', 'https://openalex.org/W4300558631', 'https://openalex.org/W2973049979', 'https://openalex.org/W2973157397', 'https://openalex.org/W6898505805', 'https://openalex.org/W2842511635', 'https://openalex.org/W2973048981', 'https://openalex.org/W2963720603', 'https://openalex.org/W2963425185', 'https://openalex.org/W2963317665', 'https://openalex.org/W3100270690', 'https://openalex.org/W2972943112', 'https://openalex.org/W2884305338', 'https://openalex.org/W6631190155', 'https://openalex.org/W6623517193', 'https://openalex.org/W2024490156', 'https://openalex.org/W6766673545', 'https://openalex.org/W6755207826', 'https://openalex.org/W2899134946', 'https://openalex.org/W6748215858', 'https://openalex.org/W1522301498', 'https://openalex.org/W3125709657', 'https://openalex.org/W2963432880', 'https://openalex.org/W2785350307', 'https://openalex.org/W2941814890', 'https://openalex.org/W2808706139', 'https://openalex.org/W854541894', 'https://openalex.org/W2896457183', 'https://openalex.org/W2963045354', 'https://openalex.org/W2758785877', 'https://openalex.org/W2101105183', 'https://openalex.org/W2952127920', 'https://openalex.org/W2963341956', 'https://openalex.org/W4385245566', 'https://openalex.org/W2965373594', 'https://openalex.org/W2963403868', 'https://openalex.org/W2899663614', 'https://openalex.org/W4297808394', 'https://openalex.org/W2963618559', 'https://openalex.org/W2964121744']",2020-04-09
https://openalex.org/W2101234009,https://doi.org/10.48550/arxiv.1201.0490,Scikit-learn: Machine Learning in Python,"Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.org.","['https://openalex.org/W2118585731', 'https://openalex.org/W2040387238', 'https://openalex.org/W1496508106', 'https://openalex.org/W2153635508', 'https://openalex.org/W2146292423', 'https://openalex.org/W2047804403', 'https://openalex.org/W2024933578', 'https://openalex.org/W2035776949', 'https://openalex.org/W2097360283', 'https://openalex.org/W2152799677', 'https://openalex.org/W2063978378', 'https://openalex.org/W2097850441', 'https://openalex.org/W1571024744']",2012-01-02
https://openalex.org/W2936774411,https://doi.org/10.21437/interspeech.2019-2680,SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,"We present SpecAugment, a simple data augmentation method for speech\nrecognition. SpecAugment is applied directly to the feature inputs of a neural\nnetwork (i.e., filter bank coefficients). The augmentation policy consists of\nwarping the features, masking blocks of frequency channels, and masking blocks\nof time steps. We apply SpecAugment on Listen, Attend and Spell networks for\nend-to-end speech recognition tasks. We achieve state-of-the-art performance on\nthe LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work.\nOn LibriSpeech, we achieve 6.8% WER on test-other without the use of a language\nmodel, and 5.8% WER with shallow fusion with a language model. This compares to\nthe previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5'00 test set\nwithout the use of a language model, and 6.8%/14.1% with shallow fusion, which\ncompares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER.\n","['https://openalex.org/W2782451907', 'https://openalex.org/W2963742216', 'https://openalex.org/W2746314669', 'https://openalex.org/W2166637769', 'https://openalex.org/W2898181186', 'https://openalex.org/W1922655562', 'https://openalex.org/W4295727797', 'https://openalex.org/W2904818793', 'https://openalex.org/W1485222997', 'https://openalex.org/W1494198834', 'https://openalex.org/W2912492482', 'https://openalex.org/W2514741789', 'https://openalex.org/W2781384251', 'https://openalex.org/W2889163603', 'https://openalex.org/W2889282842', 'https://openalex.org/W2520160253', 'https://openalex.org/W2608712415', 'https://openalex.org/W2121879602', 'https://openalex.org/W4297818305', 'https://openalex.org/W1915251500', 'https://openalex.org/W2887528618', 'https://openalex.org/W2147768505', 'https://openalex.org/W2888493875', 'https://openalex.org/W2963070863', 'https://openalex.org/W1989674786', 'https://openalex.org/W2804047946', 'https://openalex.org/W2183341477', 'https://openalex.org/W2099621636', 'https://openalex.org/W2759921250', 'https://openalex.org/W2112739286', 'https://openalex.org/W2973215447', 'https://openalex.org/W2963727906', 'https://openalex.org/W2327501763', 'https://openalex.org/W2521999726', 'https://openalex.org/W2184343439', 'https://openalex.org/W1524333225', 'https://openalex.org/W2577366047', 'https://openalex.org/W2617258110', 'https://openalex.org/W2102113734', 'https://openalex.org/W2143612262', 'https://openalex.org/W2962826786', 'https://openalex.org/W2108677974', 'https://openalex.org/W2397147568', 'https://openalex.org/W2799800213', 'https://openalex.org/W2597757402', 'https://openalex.org/W2962824709', 'https://openalex.org/W2131342762', 'https://openalex.org/W4289440825', 'https://openalex.org/W2963303028', 'https://openalex.org/W97072897', 'https://openalex.org/W2905263927', 'https://openalex.org/W2407080277', 'https://openalex.org/W2184045248']",2019-09-13
https://openalex.org/W2963382687,https://doi.org/10.21437/interspeech.2019-2277,On the Choice of Modeling Unit for Sequence-to-Sequence Speech Recognition,"In conventional speech recognition, phoneme-based models outperform\ngrapheme-based models for non-phonetic languages such as English. The\nperformance gap between the two typically reduces as the amount of training\ndata is increased. In this work, we examine the impact of the choice of\nmodeling unit for attention-based encoder-decoder models. We conduct\nexperiments on the LibriSpeech 100hr, 460hr, and 960hr tasks, using various\ntarget units (phoneme, grapheme, and word-piece); across all tasks, we find\nthat grapheme or word-piece models consistently outperform phoneme-based\nmodels, even though they are evaluated without a lexicon or an external\nlanguage model. We also investigate model complementarity: we find that we can\nimprove WERs by up to 9% relative by rescoring N-best lists generated from a\nstrong word-piece based baseline with either the phoneme or the grapheme model.\nRescoring an N-best list generated by the phonemic system, however, provides\nlimited improvements. Further analysis shows that the word-piece-based models\nproduce more diverse N-best hypotheses, and thus lower oracle WERs, than\nphonemic models.\n","['https://openalex.org/W2526425061', 'https://openalex.org/W4289440825', 'https://openalex.org/W1553004968', 'https://openalex.org/W2402268235', 'https://openalex.org/W2121879602', 'https://openalex.org/W2912492482', 'https://openalex.org/W2525778437', 'https://openalex.org/W2327501763', 'https://openalex.org/W2963747784', 'https://openalex.org/W2131968858', 'https://openalex.org/W2121227244', 'https://openalex.org/W2530876040', 'https://openalex.org/W2964308564', 'https://openalex.org/W2936774411', 'https://openalex.org/W2964121744', 'https://openalex.org/W2962824709', 'https://openalex.org/W2928941594', 'https://openalex.org/W2963850025', 'https://openalex.org/W1828163288', 'https://openalex.org/W2803399609', 'https://openalex.org/W2949888546', 'https://openalex.org/W2605131327', 'https://openalex.org/W1510837697', 'https://openalex.org/W2889163603', 'https://openalex.org/W2739427748', 'https://openalex.org/W2782451907', 'https://openalex.org/W2964107261', 'https://openalex.org/W2133564696', 'https://openalex.org/W2799800213', 'https://openalex.org/W2064675550', 'https://openalex.org/W2131774270', 'https://openalex.org/W1522301498', 'https://openalex.org/W2577366047', 'https://openalex.org/W2130942839', 'https://openalex.org/W1494198834', 'https://openalex.org/W3150637114', 'https://openalex.org/W2937402758', 'https://openalex.org/W2963362078', 'https://openalex.org/W2962784628', 'https://openalex.org/W2117541506', 'https://openalex.org/W2963414781', 'https://openalex.org/W2963742216', 'https://openalex.org/W2963827914', 'https://openalex.org/W2750499125', 'https://openalex.org/W1895481600']",2019-09-13
https://openalex.org/W2964110616,https://doi.org/10.18653/v1/p19-1285,Transformer-XL: Attentive Language Models beyond a Fixed-Length Context,"Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.","['https://openalex.org/W2963304263', 'https://openalex.org/W4293718192', 'https://openalex.org/W2962739339', 'https://openalex.org/W2951714314', 'https://openalex.org/W2891369367', 'https://openalex.org/W2964059481', 'https://openalex.org/W2952723479', 'https://openalex.org/W2963983719', 'https://openalex.org/W4298422451', 'https://openalex.org/W1800356822', 'https://openalex.org/W2963351145', 'https://openalex.org/W1999965501', 'https://openalex.org/W2170973209', 'https://openalex.org/W2804845563', 'https://openalex.org/W2896457183', 'https://openalex.org/W2778817245', 'https://openalex.org/W2964348070', 'https://openalex.org/W2553303224', 'https://openalex.org/W2963403868', 'https://openalex.org/W4300687381', 'https://openalex.org/W2259472270', 'https://openalex.org/W2963341956', 'https://openalex.org/W2525246036', 'https://openalex.org/W4303633609', 'https://openalex.org/W2549416390', 'https://openalex.org/W2963970792', 'https://openalex.org/W2064675550', 'https://openalex.org/W1810943226', 'https://openalex.org/W4299838440', 'https://openalex.org/W2207587218', 'https://openalex.org/W2964269252', 'https://openalex.org/W2964308564', 'https://openalex.org/W2963088785', 'https://openalex.org/W2510842514', 'https://openalex.org/W2963266340', 'https://openalex.org/W2963034893', 'https://openalex.org/W2962746461', 'https://openalex.org/W2810075754', 'https://openalex.org/W2900096133', 'https://openalex.org/W2963735467', 'https://openalex.org/W2525332836', 'https://openalex.org/W4302375066', 'https://openalex.org/W4385245566', 'https://openalex.org/W2212703438', 'https://openalex.org/W2964019776', 'https://openalex.org/W4297788867', 'https://openalex.org/W2962964385', 'https://openalex.org/W2891815651', 'https://openalex.org/W2792764867', 'https://openalex.org/W2140679639', 'https://openalex.org/W2605203995', 'https://openalex.org/W2549476280', 'https://openalex.org/W2145543707', 'https://openalex.org/W2743945814', 'https://openalex.org/W2952729433', 'https://openalex.org/W2963573053', 'https://openalex.org/W2133564696', 'https://openalex.org/W2963925437', 'https://openalex.org/W2519314406', 'https://openalex.org/W2567070169', 'https://openalex.org/W1899504021', 'https://openalex.org/W4294555862', 'https://openalex.org/W2952339051', 'https://openalex.org/W2964347220', 'https://openalex.org/W2963951265', 'https://openalex.org/W4285719527', 'https://openalex.org/W2951672049', 'https://openalex.org/W2611669587', 'https://openalex.org/W2618854269', 'https://openalex.org/W2795285343', 'https://openalex.org/W2793273050', 'https://openalex.org/W4394642966', 'https://openalex.org/W2197913429', 'https://openalex.org/W2951104886', 'https://openalex.org/W179875071', 'https://openalex.org/W2950527759', 'https://openalex.org/W2792376130', 'https://openalex.org/W2963631907', 'https://openalex.org/W2962754271', 'https://openalex.org/W2963537482']",2019-01-01
https://openalex.org/W2963425185,https://doi.org/10.21437/interspeech.2018-2341,Speech2Vec: A Sequence-to-Sequence Framework for Learning Word Embeddings from Speech,"In this paper, we propose a novel deep neural network architecture, Speech2Vec, for learning fixed-length vector representations of audio segments excised from a speech corpus, where the vectors contain semantic information pertaining to the underlying spoken words, and are close to other vectors in the embedding space if their corresponding underlying spoken words are semantically similar.The proposed model can be viewed as a speech version of Word2Vec [1].Its design is based on a RNN Encoder-Decoder framework, and borrows the methodology of skipgrams or continuous bag-of-words for training.Learning word embeddings directly from speech enables Speech2Vec to make use of the semantic information carried by speech that does not exist in plain text.The learned word embeddings are evaluated and analyzed on 13 widely used word similarity benchmarks, and outperform word embeddings learned by Word2Vec from the transcriptions.","['https://openalex.org/W1494198834', 'https://openalex.org/W2132631284', 'https://openalex.org/W2103318667', 'https://openalex.org/W2187089797', 'https://openalex.org/W2556930864', 'https://openalex.org/W2133564696', 'https://openalex.org/W2611470828', 'https://openalex.org/W2139501017', 'https://openalex.org/W1854884267', 'https://openalex.org/W2964116568', 'https://openalex.org/W2951559648', 'https://openalex.org/W2962904995', 'https://openalex.org/W2493916176', 'https://openalex.org/W2962925243', 'https://openalex.org/W2251012068', 'https://openalex.org/W2059652594', 'https://openalex.org/W4294170691', 'https://openalex.org/W1860935423', 'https://openalex.org/W2130942839', 'https://openalex.org/W2137735870', 'https://openalex.org/W2176085882', 'https://openalex.org/W4300822525', 'https://openalex.org/W2142625445', 'https://openalex.org/W2296681920', 'https://openalex.org/W2468716020', 'https://openalex.org/W2252211741', 'https://openalex.org/W2170682101', 'https://openalex.org/W2190506272', 'https://openalex.org/W2951216052', 'https://openalex.org/W2137010615', 'https://openalex.org/W2296283641', 'https://openalex.org/W1902237438', 'https://openalex.org/W2964169922', 'https://openalex.org/W2963366649', 'https://openalex.org/W2767224889', 'https://openalex.org/W2251253014', 'https://openalex.org/W2963296402', 'https://openalex.org/W2157331557', 'https://openalex.org/W2962753610', 'https://openalex.org/W2080100102', 'https://openalex.org/W2891197287', 'https://openalex.org/W2250539671', 'https://openalex.org/W2963571336', 'https://openalex.org/W2899771611', 'https://openalex.org/W2550241133']",2018-08-28
https://openalex.org/W2932675979,https://doi.org/10.1109/icassp.2019.8682903,Acoustically Grounded Word Embeddings for Improved Acoustics-to-word Speech Recognition,"Direct acoustics-to-word (A2W) systems for end-to-end automatic speech recognition are simpler to train, and more efficient to decode with, than sub-word systems. However, A2W systems can have difficulties at training time when data is limited, and at decoding time when recognizing words outside the training vocabulary. To address these shortcomings, we investigate the use of recently proposed acoustic and acoustically grounded word embedding techniques in A2W systems. The idea is based on treating the final pre-softmax weight matrix of an AWE recognizer as a matrix of word embedding vectors, and using an externally trained set of word embeddings to improve the quality of this matrix. In particular we introduce two ideas: (1) Enforcing similarity at training time between the external embeddings and the recognizer weights, and (2) using the word embeddings at test time for predicting out-of-vocabulary words. Our word embedding model is acoustically grounded, that is it is learned jointly with acoustic embeddings so as to encode the words' acoustic-phonetic content; and it is parametric, so that it can embed any arbitrary (potentially out-of-vocabulary) sequence of characters. We find that both techniques improve the performance of an A2W recognizer on conversational telephone speech.","['https://openalex.org/W6756040250', 'https://openalex.org/W6761030284', 'https://openalex.org/W6631190155', 'https://openalex.org/W2190506272', 'https://openalex.org/W2962736743', 'https://openalex.org/W6731763572', 'https://openalex.org/W2962980711', 'https://openalex.org/W2963483399', 'https://openalex.org/W2147152072', 'https://openalex.org/W6680532216', 'https://openalex.org/W2091812280', 'https://openalex.org/W6682691769', 'https://openalex.org/W2250539671', 'https://openalex.org/W2513848549', 'https://openalex.org/W6752958021', 'https://openalex.org/W6697456849', 'https://openalex.org/W6753411874', 'https://openalex.org/W2597757402', 'https://openalex.org/W2096733369', 'https://openalex.org/W2545177271', 'https://openalex.org/W2963970535', 'https://openalex.org/W2963070863', 'https://openalex.org/W2962824709', 'https://openalex.org/W2963414781', 'https://openalex.org/W1496120315', 'https://openalex.org/W2059652594', 'https://openalex.org/W2578392894', 'https://openalex.org/W2963571336', 'https://openalex.org/W2884305338', 'https://openalex.org/W2963425185', 'https://openalex.org/W1522301498', 'https://openalex.org/W2936995161', 'https://openalex.org/W2951216052', 'https://openalex.org/W2899771611', 'https://openalex.org/W2884254529', 'https://openalex.org/W2998704965', 'https://openalex.org/W2566587499', 'https://openalex.org/W2153579005', 'https://openalex.org/W4294170691', 'https://openalex.org/W3099206234', 'https://openalex.org/W2296681920', 'https://openalex.org/W2964121744', 'https://openalex.org/W2884975363', 'https://openalex.org/W4285719527', 'https://openalex.org/W2291770225']",2019-04-17
https://openalex.org/W2124558353,https://doi.org/10.1109/asru.2013.6707741,Semi-supervised training of Deep Neural Networks,"In this paper we search for an optimal strategy for semi-supervised Deep Neural Network (DNN) training. We assume that a small part of the data is transcribed, while the majority of the data is untranscribed. We explore self-training strategies with data selection based on both the utterance-level and frame-level confidences. Further on, we study the interactions between semi-supervised frame-discriminative training and sequence-discriminative sMBR training. We found it beneficial to reduce the disproportion in amounts of transcribed and untranscribed data by including the transcribed data several times, as well as to do a frame-selection based on per-frame confidences derived from confusion in a lattice. For the experiments, we used the Limited language pack condition for the Surprise language task (Vietnamese) from the IARPA Babel program. The absolute Word Error Rate (WER) improvement for frame cross-entropy training is 2.2%, this corresponds to WER recovery of 36% when compared to the identical system, where the DNN is built on the fully transcribed data.","['https://openalex.org/W1987563958', 'https://openalex.org/W2403186097', 'https://openalex.org/W2131342762', 'https://openalex.org/W1965842648', 'https://openalex.org/W38527073', 'https://openalex.org/W2114016253', 'https://openalex.org/W811578723', 'https://openalex.org/W2394932179', 'https://openalex.org/W72757270', 'https://openalex.org/W2106554350', 'https://openalex.org/W6631362777', 'https://openalex.org/W2070737455', 'https://openalex.org/W2160650576', 'https://openalex.org/W6608710415', 'https://openalex.org/W2120209245', 'https://openalex.org/W6687284999', 'https://openalex.org/W165878654', 'https://openalex.org/W2103933358', 'https://openalex.org/W2076794394', 'https://openalex.org/W206571999', 'https://openalex.org/W2127499922', 'https://openalex.org/W176510440', 'https://openalex.org/W6681588610', 'https://openalex.org/W2106051978', 'https://openalex.org/W2136922672', 'https://openalex.org/W2189391786', 'https://openalex.org/W1524333225', 'https://openalex.org/W2145494108', 'https://openalex.org/W2293363371', 'https://openalex.org/W1553004968', 'https://openalex.org/W217970951', 'https://openalex.org/W2144792281']",2013-12-01
https://openalex.org/W2940322076,https://doi.org/10.1109/icassp.2019.8682172,Semi-supervised Training for End-to-end Models via Weak Distillation,"End-to-end (E2E) models are a promising research direction in speech recognition, as the single all-neural E2E system offers a much simpler and more compact solution compared to a conventional model, which has a separate acoustic (AM), pronunciation (PM) and language model (LM). However, it has been noted that E2E models perform poorly on tail words and proper nouns, likely because the end-to-end optimization requires joint audio-text pairs, and does not take advantage of additional lexicons and large amounts of text-only data used to train the LMs in conventional models. There has been numerous efforts in training an RNN-LM on text-only data and fusing it into the end-to-end model. In this work, we contrast this approach to training the E2E model with audio-text pairs generated from unsupervised speech data. To target the proper noun issue specifically, we adopt a Part-of-Speech (POS) tagger to filter the unsupervised data to use only those with proper nouns. We show that training with filtered unsupervised-data provides up to a 13% relative reduction in word-error-rate (WER), and when used in conjunction with a cold-fusion RNN-LM, up to a 17% relative improvement.","['https://openalex.org/W6698490408', 'https://openalex.org/W6640362995', 'https://openalex.org/W2515439472', 'https://openalex.org/W2962760690', 'https://openalex.org/W1975113979', 'https://openalex.org/W6603381559', 'https://openalex.org/W2139453310', 'https://openalex.org/W6687284999', 'https://openalex.org/W6697031438', 'https://openalex.org/W1993660824', 'https://openalex.org/W2033256038', 'https://openalex.org/W6713395095', 'https://openalex.org/W6638523607', 'https://openalex.org/W6679434410', 'https://openalex.org/W6746574493', 'https://openalex.org/W6780226713', 'https://openalex.org/W2963362078', 'https://openalex.org/W6640059789', 'https://openalex.org/W6694517276', 'https://openalex.org/W6729503108', 'https://openalex.org/W6743477263', 'https://openalex.org/W6732447497', 'https://openalex.org/W6746892368', 'https://openalex.org/W6695909211', 'https://openalex.org/W6639156005', 'https://openalex.org/W2963736842', 'https://openalex.org/W2769810959', 'https://openalex.org/W2617258110', 'https://openalex.org/W2064675550', 'https://openalex.org/W6629052376', 'https://openalex.org/W1855892484', 'https://openalex.org/W2271840356', 'https://openalex.org/W2963240019', 'https://openalex.org/W2962824709', 'https://openalex.org/W1915251500', 'https://openalex.org/W2294962864', 'https://openalex.org/W2403440562', 'https://openalex.org/W82886505', 'https://openalex.org/W2963782041', 'https://openalex.org/W2189391786', 'https://openalex.org/W1821462560', 'https://openalex.org/W2577366047', 'https://openalex.org/W2963216553', 'https://openalex.org/W1940872118', 'https://openalex.org/W2963572611', 'https://openalex.org/W4294619240', 'https://openalex.org/W2964308564', 'https://openalex.org/W2133564696', 'https://openalex.org/W2963920996', 'https://openalex.org/W2555428947', 'https://openalex.org/W2888779557']",2019-04-17
https://openalex.org/W2586148577,https://doi.org/10.18653/v1/p17-1057,Representations of language in a model of visually grounded speech signal,"We present a visually grounded model of speech perception which projects spoken utterances and images to a joint semantic space. We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaning-based linguistic knowledge from the input signal. We carry out an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of form-related aspects of the language input tends to initially increase and then plateau or decrease.","['https://openalex.org/W3213502289', 'https://openalex.org/W2155889433', 'https://openalex.org/W2282219577', 'https://openalex.org/W2292919134', 'https://openalex.org/W2953318193', 'https://openalex.org/W2123815913', 'https://openalex.org/W2531381952', 'https://openalex.org/W2471839888', 'https://openalex.org/W2194775991', 'https://openalex.org/W2172888184', 'https://openalex.org/W4245833664', 'https://openalex.org/W68733909', 'https://openalex.org/W1522301498', 'https://openalex.org/W2964121744', 'https://openalex.org/W2962776659', 'https://openalex.org/W2962835968', 'https://openalex.org/W2143623448', 'https://openalex.org/W4299801216', 'https://openalex.org/W2524611247', 'https://openalex.org/W385555557', 'https://openalex.org/W1905882502', 'https://openalex.org/W2580178245', 'https://openalex.org/W2515741950', 'https://openalex.org/W4394453761', 'https://openalex.org/W2951674897', 'https://openalex.org/W2952020226', 'https://openalex.org/W2251861449', 'https://openalex.org/W2962753610', 'https://openalex.org/W2953177656', 'https://openalex.org/W2953188482', 'https://openalex.org/W4294555862', 'https://openalex.org/W2160783091', 'https://openalex.org/W2963899908', 'https://openalex.org/W2134670479', 'https://openalex.org/W2562979205', 'https://openalex.org/W1686810756', 'https://openalex.org/W2396384435', 'https://openalex.org/W2107917162', 'https://openalex.org/W2556930864', 'https://openalex.org/W4237938692', 'https://openalex.org/W4297826211', 'https://openalex.org/W1861492603', 'https://openalex.org/W2127438782', 'https://openalex.org/W2962862718', 'https://openalex.org/W2250790822', 'https://openalex.org/W2117539524', 'https://openalex.org/W2137010615', 'https://openalex.org/W2473934411']",2017-01-01
https://openalex.org/W30845872,https://doi.org/10.21437/interspeech.2010-483,Towards spoken term discovery at scale with zero resources,"The spoken term discovery task takes speech as input and identifies terms of possible interest. The challenge is to perform this task efficiently on large amounts of speech with zero resources (no training data and no dictionaries), where we must fall back to more basic properties of language. We find that long (∼ 1 s) repetitions tend to be contentful phrases (e.g. University of Pennsylvania) and propose an algorithm to search for these long repetitions without first recognizing the speech. To address efficiency concerns, we take advantage of (i) sparse feature representations and (ii) inherent low occurrence frequency of long content terms to achieve orders-of-magnitude speedup relative to the prior art. We frame our evaluation in the context of spoken document information retrieval, and demonstrate our method’s competence at identifying repeated terms in conversational telephone speech. Index Terms: spoken term discovery, zero resource speech recognition, dotplots","['https://openalex.org/W2118841860', 'https://openalex.org/W2114347655', 'https://openalex.org/W2117041980', 'https://openalex.org/W2170580867', 'https://openalex.org/W1974930421', 'https://openalex.org/W1978394996', 'https://openalex.org/W2045664041', 'https://openalex.org/W2079460648']",2010-09-26
https://openalex.org/W2059652594,https://doi.org/10.1109/asru.2013.6707765,Fixed-dimensional acoustic embeddings of variable-length segments in low-resource settings,"Measures of acoustic similarity between words or other units are critical for segmental exemplar-based acoustic models, spoken term discovery, and query-by-example search. Dynamic time warping (DTW) alignment cost has been the most commonly used measure, but it has well-known inadequacies. Some recently proposed alternatives require large amounts of training data. In the interest of finding more efficient, accurate, and low-resource alternatives, we consider the problem of embedding speech segments of arbitrary length into fixed-dimensional spaces in which simple distances (such as cosine or Euclidean) serve as a proxy for linguistically meaningful (phonetic, lexical, etc.) dissimilarities. Such embeddings would enable efficient audio indexing and permit application of standard distance learning techniques to segmental acoustic modeling. In this paper, we explore several supervised and unsupervised approaches to this problem and evaluate them on an acoustic word discrimination task. We identify several embedding algorithms that match or improve upon the DTW baseline in low-resource settings.","['https://openalex.org/W1579848672', 'https://openalex.org/W2170580867', 'https://openalex.org/W137541753', 'https://openalex.org/W2395342389', 'https://openalex.org/W1974540032', 'https://openalex.org/W2111732304', 'https://openalex.org/W2126203737', 'https://openalex.org/W2799046698', 'https://openalex.org/W2110889574', 'https://openalex.org/W2097527229', 'https://openalex.org/W2147717514', 'https://openalex.org/W1583581687', 'https://openalex.org/W6683338658', 'https://openalex.org/W2053186076', 'https://openalex.org/W6683161245', 'https://openalex.org/W2097308346', 'https://openalex.org/W6675747103', 'https://openalex.org/W1984857732', 'https://openalex.org/W2097207027', 'https://openalex.org/W2023952145', 'https://openalex.org/W2407964052', 'https://openalex.org/W1978741356', 'https://openalex.org/W6634500723', 'https://openalex.org/W2114347655', 'https://openalex.org/W2397535009', 'https://openalex.org/W6714100551', 'https://openalex.org/W3148981562', 'https://openalex.org/W2117459613', 'https://openalex.org/W2057007397', 'https://openalex.org/W30845872', 'https://openalex.org/W2997701990', 'https://openalex.org/W2569190383', 'https://openalex.org/W2407151108', 'https://openalex.org/W2158139921', 'https://openalex.org/W2104290444', 'https://openalex.org/W2025482506', 'https://openalex.org/W2157444450', 'https://openalex.org/W2151660570', 'https://openalex.org/W1576613474', 'https://openalex.org/W2164365067']",2013-12-01
https://openalex.org/W2025482506,https://doi.org/10.1109/icassp.2013.6639245,A summary of the 2012 JHU CLSP workshop on zero resource speech technologies and models of early language acquisition,"We summarize the accomplishments of a multi-disciplinary workshop exploring the computational and scientific issues surrounding zero resource (unsupervised) speech technologies and related models of early language acquisition. Centered around the tasks of phonetic and lexical discovery, we consider unified evaluation metrics, present two new approaches for improving speaker independence in the absence of supervision, and evaluate the application of Bayesian word segmentation algorithms to automatic subword unit tokenizations. Finally, we present two strategies for integrating zero resource techniques into supervised settings, demonstrating the potential of unsupervised methods to improve mainstream technologies.","['https://openalex.org/W2126377586', 'https://openalex.org/W2160306971', 'https://openalex.org/W2054948443', 'https://openalex.org/W1993660824', 'https://openalex.org/W2142152793', 'https://openalex.org/W1526995323', 'https://openalex.org/W2161952424', 'https://openalex.org/W2110073835', 'https://openalex.org/W6602180557', 'https://openalex.org/W6677207036', 'https://openalex.org/W2166391802', 'https://openalex.org/W2162638453', 'https://openalex.org/W2057007397', 'https://openalex.org/W2074546930', 'https://openalex.org/W2126449874', 'https://openalex.org/W6678998471', 'https://openalex.org/W6714100551', 'https://openalex.org/W1967924372', 'https://openalex.org/W1857273500', 'https://openalex.org/W2121947440', 'https://openalex.org/W2170580867', 'https://openalex.org/W6677461952', 'https://openalex.org/W30845872', 'https://openalex.org/W2114347655', 'https://openalex.org/W2406820985', 'https://openalex.org/W2401464865', 'https://openalex.org/W6607928866', 'https://openalex.org/W2117041980', 'https://openalex.org/W2062914951', 'https://openalex.org/W2142390309', 'https://openalex.org/W2399869768', 'https://openalex.org/W6676025551', 'https://openalex.org/W2126203737', 'https://openalex.org/W6675022971', 'https://openalex.org/W2114478143', 'https://openalex.org/W2407151108', 'https://openalex.org/W1779834323', 'https://openalex.org/W2126953647', 'https://openalex.org/W196692374', 'https://openalex.org/W2107959623', 'https://openalex.org/W3136512150', 'https://openalex.org/W2952343510', 'https://openalex.org/W2100768664', 'https://openalex.org/W2117786207', 'https://openalex.org/W52412328', 'https://openalex.org/W2117126688']",2013-05-01
https://openalex.org/W2963571336,https://doi.org/10.21437/interspeech.2016-82,Audio Word2Vec: Unsupervised Learning of Audio Segment Representations Using Sequence-to-Sequence Autoencoder,"The vector representations of fixed dimensionality for words (in text) offered by Word2Vec have been shown to be very useful in many application scenarios, in particular due to the semantic information they carry.This paper proposes a parallel version, the Audio Word2Vec.It offers the vector representations of fixed dimensionality for variable-length audio segments.These vector representations are shown to describe the sequential phonetic structures of the audio segments to a good degree, with very attractive real world applications such as query-by-example Spoken Term Detection (STD).In this STD application, the proposed approach significantly outperformed the conventional Dynamic Time Warping (DTW) based approaches at significantly lower computation requirements.We propose unsupervised learning of Audio Word2Vec from audio data without human annotation using Sequence-to-sequence Autoencoder (SA).SA consists of two RNNs equipped with Long Short-Term Memory (LSTM) units: the first RNN (encoder) maps the input audio sequence into a vector representation of fixed dimensionality, and the second RNN (decoder) maps the representation back to the input audio sequence.The two RNNs are jointly trained by minimizing the reconstruction error.Denoising Sequence-to-sequence Autoencoder (DSA) is further proposed offering more robust learning.","['https://openalex.org/W4294170691', 'https://openalex.org/W2059652594', 'https://openalex.org/W1689711448', 'https://openalex.org/W2100649405', 'https://openalex.org/W1494198834', 'https://openalex.org/W2293634267', 'https://openalex.org/W2030422732', 'https://openalex.org/W2128160875', 'https://openalex.org/W2295088914', 'https://openalex.org/W1496120315', 'https://openalex.org/W1606347560', 'https://openalex.org/W2107878631', 'https://openalex.org/W4302400662', 'https://openalex.org/W2025768430', 'https://openalex.org/W2401725913', 'https://openalex.org/W2181347294', 'https://openalex.org/W2130942839', 'https://openalex.org/W2131744502', 'https://openalex.org/W2157331557', 'https://openalex.org/W111477576', 'https://openalex.org/W1577418252', 'https://openalex.org/W2296681920', 'https://openalex.org/W139960808', 'https://openalex.org/W1924770834', 'https://openalex.org/W2105016867', 'https://openalex.org/W2116435618', 'https://openalex.org/W1501669607', 'https://openalex.org/W4213009331', 'https://openalex.org/W2018970719', 'https://openalex.org/W2115613106', 'https://openalex.org/W2100495367', 'https://openalex.org/W2997574889', 'https://openalex.org/W1614298861', 'https://openalex.org/W1810943226', 'https://openalex.org/W2147107577', 'https://openalex.org/W2190506272']",2016-08-29
https://openalex.org/W2114347655,https://doi.org/10.1109/tasl.2007.909282,Unsupervised Pattern Discovery in Speech,"We present a novel approach to speech processing based on the principle of pattern discovery. Our work represents a departure from traditional models of speech recognition, where the end goal is to classify speech into categories defined by a prespecified inventory of lexical units (i.e., phones or words). Instead, we attempt to discover such an inventory in an unsupervised manner by exploiting the structure of repeating patterns within the speech signal. We show how pattern discovery can be used to automatically acquire lexical entities directly from an untranscribed audio stream. Our approach to unsupervised word acquisition utilizes a segmental variant of a widely used dynamic programming technique, which allows us to find matching acoustic patterns between spoken utterances. By aggregating information about these matching patterns across audio streams, we demonstrate how to group similar acoustic sequences together to form clusters corresponding to lexical entities such as words and short multiword phrases. On a corpus of academic lecture material, we demonstrate that clusters found using this technique exhibit high purity and that many of the corresponding lexical identities are relevant to the underlying audio stream.","['https://openalex.org/W2111732304', 'https://openalex.org/W2095293504', 'https://openalex.org/W4251670979', 'https://openalex.org/W2086891622', 'https://openalex.org/W2089458547', 'https://openalex.org/W6635336695', 'https://openalex.org/W2099402246', 'https://openalex.org/W3021051756', 'https://openalex.org/W2143235280', 'https://openalex.org/W4237938692', 'https://openalex.org/W2074546930', 'https://openalex.org/W2161952424', 'https://openalex.org/W6678164431', 'https://openalex.org/W2112006116', 'https://openalex.org/W2028903194', 'https://openalex.org/W2114510609', 'https://openalex.org/W1980862600', 'https://openalex.org/W2103447044', 'https://openalex.org/W2121947440', 'https://openalex.org/W2009566340', 'https://openalex.org/W2074231493', 'https://openalex.org/W4245668478', 'https://openalex.org/W2054849588', 'https://openalex.org/W2016243284', 'https://openalex.org/W1610605641', 'https://openalex.org/W2138370049', 'https://openalex.org/W1964917299', 'https://openalex.org/W2140277151', 'https://openalex.org/W2118841860', 'https://openalex.org/W2128160875', 'https://openalex.org/W2164463707', 'https://openalex.org/W2009570821', 'https://openalex.org/W3036063182', 'https://openalex.org/W2122228338', 'https://openalex.org/W2999905431', 'https://openalex.org/W2165874743', 'https://openalex.org/W1589182518', 'https://openalex.org/W2107917162', 'https://openalex.org/W1499245496', 'https://openalex.org/W1483126227', 'https://openalex.org/W1978394996', 'https://openalex.org/W2171009857', 'https://openalex.org/W1530250655', 'https://openalex.org/W1207633162']",2007-12-20
https://openalex.org/W2964115348,,"Linguistic unit discovery from multimodal inputs in unwritten languages: Summary of the ""Speaking Rosetta"" JSALT 2017 Workshop",We summarize the accomplishments of a multi-disciplinary workshop exploring the computational and scientific issues surrounding the discovery of linguistic units (subwords and words) in a language without orthography. We study the replacement of orthographic transcriptions by images and/or translated text in a well-resourced language to help unsupervised discovery from raw speech.,"['https://openalex.org/W2117041980', 'https://openalex.org/W2963620343', 'https://openalex.org/W2395052932', 'https://openalex.org/W2057007397', 'https://openalex.org/W1833498382', 'https://openalex.org/W2962862718', 'https://openalex.org/W2325972120', 'https://openalex.org/W2114347655', 'https://openalex.org/W4206864474', 'https://openalex.org/W2164505566', 'https://openalex.org/W2762715843', 'https://openalex.org/W2079460648', 'https://openalex.org/W2251025892', 'https://openalex.org/W1861492603', 'https://openalex.org/W2464234964', 'https://openalex.org/W2406349064', 'https://openalex.org/W1970890968', 'https://openalex.org/W2345799635', 'https://openalex.org/W2949328740', 'https://openalex.org/W1520968739', 'https://openalex.org/W2406324447', 'https://openalex.org/W1855892484', 'https://openalex.org/W2963819008', 'https://openalex.org/W2556930864', 'https://openalex.org/W2347098582', 'https://openalex.org/W2586148577', 'https://openalex.org/W4206865574', 'https://openalex.org/W2962832640', 'https://openalex.org/W3217769081']",2018-04-15
https://openalex.org/W2944255943,https://doi.org/10.21437/interspeech.2019-1780,RWTH ASR Systems for LibriSpeech: Hybrid vs Attention,"We present state-of-the-art automatic speech recognition (ASR) systems\nemploying a standard hybrid DNN/HMM architecture compared to an attention-based\nencoder-decoder design for the LibriSpeech task. Detailed descriptions of the\nsystem development, including model design, pretraining schemes, training\nschedules, and optimization approaches are provided for both system\narchitectures. Both hybrid DNN/HMM and attention-based systems employ\nbi-directional LSTMs for acoustic modeling/encoding. For language modeling, we\nemploy both LSTM and Transformer based architectures. All our systems are built\nusing RWTHs open-source toolkits RASR and RETURNN. To the best knowledge of the\nauthors, the results obtained when training on the full LibriSpeech training\nset, are the best published currently, both for the hybrid DNN/HMM and the\nattention-based systems. Our single hybrid system even outperforms previous\nresults obtained from combining eight single systems. Our comparison shows that\non the LibriSpeech 960h task, the hybrid DNN/HMM system outperforms the\nattention-based system by 15% relative on the clean and 40% relative on the\nother test sets in terms of word error rate. Moreover, experiments on a reduced\n100h-subset of the LibriSpeech training corpus even show a more pronounced\nmargin between the hybrid DNN/HMM and attention-based architectures.\n",[],2019-09-13
https://openalex.org/W2971155163,,Learning Representations by Maximizing Mutual Information Across Views,"We propose an approach to self-supervised representation learning based on maximizing mutual information between features extracted from multiple views of a shared context. For example, one could produce multiple views of a local spatio-temporal context by observing it from different locations (e.g., camera positions within a scene), and via different modalities (e.g., tactile, auditory, or visual). Or, an ImageNet image could provide a context from which one produces multiple views by repeatedly applying data augmentation. Maximizing mutual information between features extracted from these views requires capturing information about high-level factors whose influence spans multiple views – e.g., presence of certain objects or occurrence of certain events. Following our proposed approach, we develop a model which learns image representations that significantly outperform prior methods on the tasks we consider. Most notably, using self-supervised learning, our model learns representations which achieve 68.1% accuracy on ImageNet using standard linear evaluation. This beats prior results by over 12% and concurrent results by 7%. When we extend our model to use mixture-based representations, segmentation behaviour emerges as a natural side-effect. Our code is available online: https://github.com/Philip-Bachman/amdim-public.",[],2019-06-03
https://openalex.org/W2981857663,https://doi.org/10.1109/icassp40776.2020.9054345,Transformer-Based Acoustic Modeling for Hybrid Speech Recognition,"We propose and evaluate transformer-based acoustic models (AMs) for hybrid\nspeech recognition. Several modeling choices are discussed in this work,\nincluding various positional embedding methods and an iterated loss to enable\ntraining deep transformers. We also present a preliminary study of using\nlimited right context in transformer models, which makes it possible for\nstreaming applications. We demonstrate that on the widely used Librispeech\nbenchmark, our transformer-based AM outperforms the best published hybrid\nresult by 19% to 26% relative when the standard n-gram language model (LM) is\nused. Combined with neural network LM for rescoring, our proposed approach\nachieves state-of-the-art results on Librispeech. Our findings are also\nconfirmed on a much larger internal dataset.\n","['https://openalex.org/W6631362777', 'https://openalex.org/W6629717138', 'https://openalex.org/W2963088785', 'https://openalex.org/W2933138175', 'https://openalex.org/W2131342762', 'https://openalex.org/W6713762819', 'https://openalex.org/W2964084166', 'https://openalex.org/W2962760690', 'https://openalex.org/W2107878631', 'https://openalex.org/W6739901393', 'https://openalex.org/W2892009249', 'https://openalex.org/W1995562189', 'https://openalex.org/W2530876040', 'https://openalex.org/W6696934422', 'https://openalex.org/W6712930963', 'https://openalex.org/W6769495571', 'https://openalex.org/W2962826786', 'https://openalex.org/W2064675550', 'https://openalex.org/W2394932179', 'https://openalex.org/W2962824709', 'https://openalex.org/W2160815625', 'https://openalex.org/W2802023636', 'https://openalex.org/W2911291251', 'https://openalex.org/W2097268427', 'https://openalex.org/W2407080277', 'https://openalex.org/W1920942766', 'https://openalex.org/W3015960524', 'https://openalex.org/W2977728428', 'https://openalex.org/W4394666973', 'https://openalex.org/W2798657914', 'https://openalex.org/W2520160253', 'https://openalex.org/W1522301498', 'https://openalex.org/W2979286696', 'https://openalex.org/W2184045248', 'https://openalex.org/W2899663614', 'https://openalex.org/W2896457183', 'https://openalex.org/W2293634267', 'https://openalex.org/W2963341956', 'https://openalex.org/W4288400010', 'https://openalex.org/W3008525923', 'https://openalex.org/W3006827623', 'https://openalex.org/W2964045208', 'https://openalex.org/W2402146185', 'https://openalex.org/W1524333225', 'https://openalex.org/W2963403868', 'https://openalex.org/W3103005696', 'https://openalex.org/W2972818416', 'https://openalex.org/W1686810756', 'https://openalex.org/W2795138957', 'https://openalex.org/W2981740924', 'https://openalex.org/W2941814890', 'https://openalex.org/W2896060389', 'https://openalex.org/W4385245566', 'https://openalex.org/W2944255943', 'https://openalex.org/W2964089206', 'https://openalex.org/W2462831000', 'https://openalex.org/W2962778134', 'https://openalex.org/W2936774411', 'https://openalex.org/W1494198834', 'https://openalex.org/W1553004968']",2020-04-09
https://openalex.org/W2296681920,https://doi.org/10.21437/interspeech.2014-273,Word embeddings for speech recognition,"Speech recognition systems have used the concept of states as a way to decompose words into sub-word units for decades. As the number of such states now reaches the number of words used to train acoustic models, it is interesting to consider approaches that relax the assumption that words are made of states. We present here an alternative construction, where words are projected into a continuous embedding space where words that sound alike are nearby in the Euclidean sense. We show how embeddings can still allow to score words that were not in the training dictionary. Initial experiments using a lattice rescoring approach and model combination on a large realistic dataset show improvements in word error rate.","['https://openalex.org/W2105594594', 'https://openalex.org/W2131033001', 'https://openalex.org/W21006490', 'https://openalex.org/W2394932179', 'https://openalex.org/W2166004480', 'https://openalex.org/W2117459613', 'https://openalex.org/W2170353620', 'https://openalex.org/W1614298861', 'https://openalex.org/W1964917299', 'https://openalex.org/W2160815625', 'https://openalex.org/W2090755665', 'https://openalex.org/W2136189984', 'https://openalex.org/W1974974326', 'https://openalex.org/W2139406248', 'https://openalex.org/W2147768505', 'https://openalex.org/W2296748324', 'https://openalex.org/W2158069733']",2014-09-14
https://openalex.org/W2965373594,https://doi.org/10.4230/oasics.ldk.2021.22,Towards Learning Terminological Concept Systems from Multilingual Natural Language Text,"Terminological Concept Systems (TCS) provide a means of organizing, structuring and representing domain-specific multilingual information and are important to ensure terminological consistency in many tasks, such as translation and cross-border communication. While several approaches to (semi-)automatic term extraction exist, learning their interrelations is vastly underexplored. We propose an automated method to extract terms and relations across natural languages and specialized domains. To this end, we adapt pretrained multilingual neural language models, which we evaluate on term extraction standard datasets with best performing results and a combination of relation extraction standard datasets with competitive results. Code and dataset are publicly available.","['https://openalex.org/W2170973209', 'https://openalex.org/W2462831000', 'https://openalex.org/W2963807318', 'https://openalex.org/W1599016936', 'https://openalex.org/W2933138175', 'https://openalex.org/W2963748441', 'https://openalex.org/W2805206884', 'https://openalex.org/W2937297214', 'https://openalex.org/W2963846996', 'https://openalex.org/W2525127255', 'https://openalex.org/W2950501607', 'https://openalex.org/W2963112338', 'https://openalex.org/W2947813521', 'https://openalex.org/W2945785363', 'https://openalex.org/W2930786691', 'https://openalex.org/W2945290257', 'https://openalex.org/W2920812691', 'https://openalex.org/W2944815030', 'https://openalex.org/W2963403868', 'https://openalex.org/W2948629866', 'https://openalex.org/W2962753370', 'https://openalex.org/W2914526845', 'https://openalex.org/W2963341956', 'https://openalex.org/W2978670439', 'https://openalex.org/W2793353489', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963310665', 'https://openalex.org/W2396767181', 'https://openalex.org/W2962784628', 'https://openalex.org/W2130158090', 'https://openalex.org/W1566289585', 'https://openalex.org/W2899771611', 'https://openalex.org/W131533222', 'https://openalex.org/W2963756346', 'https://openalex.org/W2251939518', 'https://openalex.org/W2970597249', 'https://openalex.org/W2990704537', 'https://openalex.org/W2963026768', 'https://openalex.org/W2914120296', 'https://openalex.org/W2938830017', 'https://openalex.org/W2945260553', 'https://openalex.org/W2899663614', 'https://openalex.org/W2963323070', 'https://openalex.org/W2784121710', 'https://openalex.org/W1840435438']",2021-01-01
https://openalex.org/W2941814890,https://doi.org/10.48550/arxiv.1904.11660,Transformers with convolutional context for ASR,"The recent success of transformer networks for neural machine translation and other NLP tasks has led to a surge in research work trying to apply it for speech recognition. Recent efforts studied key research questions around ways of combining positional embedding with speech features, and stability of optimization for large scale learning of transformer networks. In this paper, we propose replacing the sinusoidal positional embedding for transformers with convolutionally learned input representations. These contextual representations provide subsequent transformer blocks with relative positional information needed for discovering long-range relationships between local concepts. The proposed system has favorable optimization characteristics where our reported results are produced with fixed learning rate of 1.0 and no warmup steps. The proposed model achieves a competitive 4.7% and 12.9% WER on the Librispeech ``test clean'' and ``test other'' subsets when no extra LM text is provided.","['https://openalex.org/W2802023636', 'https://openalex.org/W2102113734', 'https://openalex.org/W2130942839', 'https://openalex.org/W2963341956', 'https://openalex.org/W2964089206', 'https://openalex.org/W2799800213', 'https://openalex.org/W2782451907', 'https://openalex.org/W2520160253', 'https://openalex.org/W6908809', 'https://openalex.org/W2963742216', 'https://openalex.org/W2127141656', 'https://openalex.org/W2005708641', 'https://openalex.org/W2184045248', 'https://openalex.org/W2962778134', 'https://openalex.org/W2327501763', 'https://openalex.org/W2911291251', 'https://openalex.org/W179875071', 'https://openalex.org/W2963250244', 'https://openalex.org/W1922655562', 'https://openalex.org/W854541894', 'https://openalex.org/W2963403868', 'https://openalex.org/W2904818793', 'https://openalex.org/W2157331557', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962780374']",2019-04-26
https://openalex.org/W1524333225,,The Kaldi Speech Recognition Toolkit,"Abstract—We describe the design of Kaldi, a free, open-source toolkit for speech recognition research. Kaldi provides a speech recognition system based on finite-state transducers (using the freely available OpenFst), together with detailed documentation and scripts for building complete recognition systems. Kaldi is written is C++, and the core library supports modeling of arbitrary phonetic-context sizes, acoustic modeling with subspace Gaussian mixture models (SGMM) as well as standard Gaussian mixture models, together with all commonly used linear and affine transforms. Kaldi is released under the Apache License v2.0, which is highly nonrestrictive, making it suitable for a wide community of users. I.","['https://openalex.org/W38163879', 'https://openalex.org/W1582482241', 'https://openalex.org/W1981706894', 'https://openalex.org/W2045644151', 'https://openalex.org/W2122028591', 'https://openalex.org/W2002342963', 'https://openalex.org/W2046932483', 'https://openalex.org/W31925794', 'https://openalex.org/W2106554350', 'https://openalex.org/W2148428486', 'https://openalex.org/W2124629003', 'https://openalex.org/W2157421955', 'https://openalex.org/W1491238342', 'https://openalex.org/W2146871184', 'https://openalex.org/W173010698']",2011-01-01
https://openalex.org/W2395718496,https://doi.org/10.1109/icassp.2016.7471749,Fast and easy crowdsourced perceptual audio evaluation,"Automated objective methods of audio evaluation are fast, cheap, and require little effort by the investigator. However, objective evaluation methods do not exist for the output of all audio processing algorithms, often have output that correlates poorly with human quality assessments, and require ground truth data in their calculation. Subjective human ratings of audio quality are the gold standard for many tasks, but are expensive, slow, and require a great deal of effort to recruit subjects and run listening tests. Moving listening tests from the lab to the micro-task labor market of Amazon Mechanical Turk speeds data collection and reduces investigator effort. However, it also reduces the amount of control investigators have over the testing environment, adding new variability and potential biases to the data. In this work, we compare multiple stimulus listening tests performed in a lab environment to multiple stimulus listening tests performed in web environment on a population drawn from Mechanical Turk.","['https://openalex.org/W2170931744', 'https://openalex.org/W1993219564', 'https://openalex.org/W2061844755', 'https://openalex.org/W2135626977', 'https://openalex.org/W2106568252', 'https://openalex.org/W1999854131', 'https://openalex.org/W2105921478', 'https://openalex.org/W1604577397', 'https://openalex.org/W2108708552', 'https://openalex.org/W1606487971', 'https://openalex.org/W2160473997', 'https://openalex.org/W2127851351', 'https://openalex.org/W2406060094', 'https://openalex.org/W2049208969', 'https://openalex.org/W3122078363', 'https://openalex.org/W2157985198']",2016-03-01
https://openalex.org/W2890964092,https://doi.org/10.1109/icassp.2018.8461375,X-Vectors: Robust DNN Embeddings for Speaker Recognition,"In this paper, we use data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition. The DNN, which is trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings that we call x-vectors. Prior studies have found that embeddings leverage large-scale training datasets better than i-vectors. However, it can be challenging to collect substantial quantities of labeled data for training. We use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese. We find that while augmentation is beneficial in the PLDA classifier, it is not helpful in the i-vector extractor. However, the x-vector DNN effectively exploits data augmentation, due to its supervised training. As a result, the x-vectors achieve superior performance on the evaluation datasets.","['https://openalex.org/W2004497042', 'https://openalex.org/W2290689761', 'https://openalex.org/W2964228006', 'https://openalex.org/W2114925438', 'https://openalex.org/W6601610752', 'https://openalex.org/W2138547132', 'https://openalex.org/W2587150483', 'https://openalex.org/W2584329820', 'https://openalex.org/W2726515241', 'https://openalex.org/W2129066450', 'https://openalex.org/W6713727690', 'https://openalex.org/W2586741466', 'https://openalex.org/W1589137271', 'https://openalex.org/W2395750323', 'https://openalex.org/W2129379984', 'https://openalex.org/W2183016404', 'https://openalex.org/W2039057510', 'https://openalex.org/W2150769028', 'https://openalex.org/W1528954144', 'https://openalex.org/W2748488820', 'https://openalex.org/W2747238065', 'https://openalex.org/W6675123034', 'https://openalex.org/W6631362777', 'https://openalex.org/W6725862793', 'https://openalex.org/W2696967604', 'https://openalex.org/W2516631658', 'https://openalex.org/W2406312423', 'https://openalex.org/W2219249508', 'https://openalex.org/W2294814385', 'https://openalex.org/W1524333225', 'https://openalex.org/W3186851455', 'https://openalex.org/W2101556109', 'https://openalex.org/W2109761419', 'https://openalex.org/W40124310']",2018-04-01
https://openalex.org/W3161695192,https://doi.org/10.1109/icassp39728.2021.9415079,Any-to-One Sequence-to-Sequence Voice Conversion Using Self-Supervised Discrete Speech Representations,"We present a novel approach to any-to-one (A2O) voice conversion (VC) in a sequence-to-sequence (seq2seq) framework. A2O VC aims to convert any speaker, including those unseen during training, to a fixed target speaker. We utilize vq-wav2vec (VQW2V), a discretized self-supervised speech representation that was learned from massive unlabeled data, which is assumed to be speaker-independent and well corresponds to underlying linguistic contents. Given a training dataset of the target speaker, we extract VQW2V and acoustic features to estimate a seq2seq mapping function from the former to the latter. With the help of a pretraining method and a newly designed postprocessing technique, our model can be generalized to only 5 min of data, even outperforming the same model trained with parallel data.","['https://openalex.org/W3016011332', 'https://openalex.org/W3035202887', 'https://openalex.org/W2982223350', 'https://openalex.org/W6844194202', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W6754299077', 'https://openalex.org/W6729448088', 'https://openalex.org/W6603838645', 'https://openalex.org/W6629717138', 'https://openalex.org/W3034420534', 'https://openalex.org/W3025844872', 'https://openalex.org/W2899877258', 'https://openalex.org/W2996414377', 'https://openalex.org/W6779919476', 'https://openalex.org/W2897353073', 'https://openalex.org/W3096567388', 'https://openalex.org/W2972970915', 'https://openalex.org/W6679436768', 'https://openalex.org/W3100270690', 'https://openalex.org/W2156142001', 'https://openalex.org/W3016160783', 'https://openalex.org/W2933138175', 'https://openalex.org/W2903739847', 'https://openalex.org/W2962780374', 'https://openalex.org/W3015338123', 'https://openalex.org/W3118753411', 'https://openalex.org/W2892009249', 'https://openalex.org/W2130942839', 'https://openalex.org/W4297808394', 'https://openalex.org/W3099078140', 'https://openalex.org/W3125709657', 'https://openalex.org/W2996383576', 'https://openalex.org/W2842511635', 'https://openalex.org/W3037057938', 'https://openalex.org/W3101689408', 'https://openalex.org/W2979476256', 'https://openalex.org/W95152782', 'https://openalex.org/W2547875792', 'https://openalex.org/W1494198834', 'https://openalex.org/W3198429080']",2021-05-13
https://openalex.org/W2964243274,https://doi.org/10.1109/icassp.2018.8461368,Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions,"This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.","['https://openalex.org/W2964301388', 'https://openalex.org/W2507771204', 'https://openalex.org/W6738277540', 'https://openalex.org/W6745697700', 'https://openalex.org/W2963609956', 'https://openalex.org/W6679436768', 'https://openalex.org/W2120847449', 'https://openalex.org/W2749651610', 'https://openalex.org/W6756197946', 'https://openalex.org/W2148154194', 'https://openalex.org/W6638667902', 'https://openalex.org/W2131774270', 'https://openalex.org/W2769810959', 'https://openalex.org/W2154920538', 'https://openalex.org/W6635953567', 'https://openalex.org/W6675380101', 'https://openalex.org/W6631190155', 'https://openalex.org/W2129142580', 'https://openalex.org/W2111284386', 'https://openalex.org/W2150658333', 'https://openalex.org/W6734815144', 'https://openalex.org/W1570629387', 'https://openalex.org/W2064675550', 'https://openalex.org/W6679434410', 'https://openalex.org/W6623517193', 'https://openalex.org/W6634817459', 'https://openalex.org/W6714142977', 'https://openalex.org/W6674330103', 'https://openalex.org/W2095705004', 'https://openalex.org/W1836465849', 'https://openalex.org/W2591927543', 'https://openalex.org/W4293714597', 'https://openalex.org/W2519091744', 'https://openalex.org/W2619368999', 'https://openalex.org/W854541894', 'https://openalex.org/W2102003408', 'https://openalex.org/W1579853615', 'https://openalex.org/W2766812927', 'https://openalex.org/W2901997113', 'https://openalex.org/W4294619240', 'https://openalex.org/W1599623585', 'https://openalex.org/W1522301498', 'https://openalex.org/W2133564696', 'https://openalex.org/W2130942839']",2018-04-01
https://openalex.org/W3015877095,https://doi.org/10.1109/icassp40776.2020.9054362,Universal Phone Recognition with a Multilingual Allophone System,"Multilingual models can improve language processing, particularly for low resource situations, by sharing parameters across languages. Multilingual acoustic models, however, generally ignore the difference between phonemes (sounds that can support lexical contrasts in a particular language) and their corresponding phones (the sounds that are actually spoken, which are language independent). This can lead to performance degradation when combining a variety of training languages, as identically annotated phonemes can actually correspond to several different underlying phonetic realizations. In this work, we propose a joint model of both language-independent phone and language-dependent phoneme distributions. In multilingual ASR experiments over 11 languages, we find that this model improves testing performance by 2% phoneme error rate absolute in low-resource conditions. Additionally, because we are explicitly modeling language-independent phones, we can build a (nearly-)universal phone recognizer that, when combined with the PHOIBLE [1] large, manually curated database of phone inventories, can be customized into 2,000 language dependent recognizers. Experiments on two low-resourced indigenous languages, Inuktitut and Tusom, show that our recognizer achieves phone accuracy improvements of more than 17%, moving a step closer to speech recognition for all languages in the world. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>","['https://openalex.org/W2910959333', 'https://openalex.org/W2025198378', 'https://openalex.org/W2972581290', 'https://openalex.org/W1963727751', 'https://openalex.org/W6753999702', 'https://openalex.org/W6606306245', 'https://openalex.org/W6757499777', 'https://openalex.org/W6691509046', 'https://openalex.org/W2166637769', 'https://openalex.org/W6779469704', 'https://openalex.org/W2170101319', 'https://openalex.org/W2130414229', 'https://openalex.org/W6602987424', 'https://openalex.org/W2033436836', 'https://openalex.org/W2786835190', 'https://openalex.org/W2998284473', 'https://openalex.org/W2963292011', 'https://openalex.org/W2964309797', 'https://openalex.org/W1526236009', 'https://openalex.org/W2755682845', 'https://openalex.org/W2127141656', 'https://openalex.org/W2195354', 'https://openalex.org/W6752124048', 'https://openalex.org/W6631362777', 'https://openalex.org/W2964227218', 'https://openalex.org/W2883972335', 'https://openalex.org/W3034729383', 'https://openalex.org/W2963242190', 'https://openalex.org/W2250357346', 'https://openalex.org/W73572011', 'https://openalex.org/W2805993470', 'https://openalex.org/W2284628133', 'https://openalex.org/W1524333225', 'https://openalex.org/W156922528', 'https://openalex.org/W2904840750']",2020-04-09
https://openalex.org/W2899877258,https://doi.org/10.1109/icassp.2019.8683282,ATTS2S-VC: Sequence-to-sequence Voice Conversion with Attention and Context Preservation Mechanisms,"This paper describes a method based on a sequence-to-sequence learning (Seq2Seq) with attention and context preservation mechanism for voice conversion (VC) tasks. Seq2Seq has been outstanding at numerous tasks involving sequence modeling such as speech synthesis and recognition, machine translation, and image captioning. In contrast to current VC techniques, our method 1) stabilizes and accelerates the training procedure by considering guided attention and proposed context preservation losses, 2) allows not only spectral envelopes but also fundamental frequency contours and durations of speech to be converted, 3) requires no context information such as phoneme labels, and 4) requires no time-aligned source and target speech data in advance. In our experiment, the proposed VC framework can be trained in only one day, using only one GPU of an NVIDIA Tesla K80, while the quality of the synthesized speech is higher than that of speech converted by Gaussian mixture model-based VC and is comparable to that of speech generated by recurrent neural network-based text-to-speech synthesis, which can be regarded as an upper limit on VC performance.","['https://openalex.org/W624848986', 'https://openalex.org/W2153057929', 'https://openalex.org/W6785635027', 'https://openalex.org/W2963808252', 'https://openalex.org/W2962793481', 'https://openalex.org/W6729383884', 'https://openalex.org/W2889329491', 'https://openalex.org/W2518172956', 'https://openalex.org/W2105160541', 'https://openalex.org/W2156142001', 'https://openalex.org/W6735706088', 'https://openalex.org/W2962824709', 'https://openalex.org/W2962699523', 'https://openalex.org/W2964135678', 'https://openalex.org/W6746801104', 'https://openalex.org/W2804998325', 'https://openalex.org/W6679434410', 'https://openalex.org/W6679436768', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963609956', 'https://openalex.org/W2327501763', 'https://openalex.org/W2767052532', 'https://openalex.org/W6631190155', 'https://openalex.org/W6739901393', 'https://openalex.org/W6756102363', 'https://openalex.org/W2120605154', 'https://openalex.org/W1974745215', 'https://openalex.org/W1588266896', 'https://openalex.org/W1977362459', 'https://openalex.org/W2086796102', 'https://openalex.org/W2127520494', 'https://openalex.org/W2157412983', 'https://openalex.org/W2739735615', 'https://openalex.org/W1509691205', 'https://openalex.org/W6711854987', 'https://openalex.org/W6728445508', 'https://openalex.org/W2142300631', 'https://openalex.org/W2963539064', 'https://openalex.org/W2148846882', 'https://openalex.org/W2022125261', 'https://openalex.org/W2017742648', 'https://openalex.org/W2056852181', 'https://openalex.org/W2964301388', 'https://openalex.org/W2747744257', 'https://openalex.org/W2471520273', 'https://openalex.org/W6731370813', 'https://openalex.org/W6637242042', 'https://openalex.org/W6603838645', 'https://openalex.org/W6600438624', 'https://openalex.org/W2598638573', 'https://openalex.org/W2800289214', 'https://openalex.org/W95152782', 'https://openalex.org/W2964308564', 'https://openalex.org/W2898654681', 'https://openalex.org/W2605141709', 'https://openalex.org/W2532494225', 'https://openalex.org/W4385245566', 'https://openalex.org/W1522301498', 'https://openalex.org/W2963403868', 'https://openalex.org/W2130942839', 'https://openalex.org/W2396025094', 'https://openalex.org/W3099078140', 'https://openalex.org/W49412823', 'https://openalex.org/W10800834', 'https://openalex.org/W2133564696', 'https://openalex.org/W2546938941', 'https://openalex.org/W2567070169', 'https://openalex.org/W2774848319', 'https://openalex.org/W3034420534', 'https://openalex.org/W1665214252', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963970792']",2019-04-16
https://openalex.org/W2120605154,https://doi.org/10.1109/tasl.2007.907344,Voice Conversion Based on Maximum-Likelihood Estimation of Spectral Parameter Trajectory,"In this paper, we describe a novel spectral conversion method for voice conversion (VC). A Gaussian mixture model (GMM) of the joint probability density of source and target features is employed for performing spectral conversion between speakers. The conventional method converts spectral parameters frame by frame based on the minimum mean square error. Although it is reasonably effective, the deterioration of speech quality is caused by some problems: 1) appropriate spectral movements are not always caused by the frame-based conversion process, and 2) the converted spectra are excessively smoothed by statistical modeling. In order to address those problems, we propose a conversion method based on the maximum-likelihood estimation of a spectral parameter trajectory. Not only static but also dynamic feature statistics are used for realizing the appropriate converted spectrum sequence. Moreover, the oversmoothing effect is alleviated by considering a global variance feature of the converted spectra. Experimental results indicate that the performance of VC can be dramatically improved by the proposed method in view of both speech quality and conversion accuracy for speaker individuality.","['https://openalex.org/W1927394876', 'https://openalex.org/W2049686551', 'https://openalex.org/W2153057929', 'https://openalex.org/W2000513720', 'https://openalex.org/W109008040', 'https://openalex.org/W1979449467', 'https://openalex.org/W2042691334', 'https://openalex.org/W6635216677', 'https://openalex.org/W2163039187', 'https://openalex.org/W6632127171', 'https://openalex.org/W1526513105', 'https://openalex.org/W6632967009', 'https://openalex.org/W6713852859', 'https://openalex.org/W6633899882', 'https://openalex.org/W2011916518', 'https://openalex.org/W2132363718', 'https://openalex.org/W2093898552', 'https://openalex.org/W1963778986', 'https://openalex.org/W2033516035', 'https://openalex.org/W6682013673', 'https://openalex.org/W4301420498', 'https://openalex.org/W6693024853', 'https://openalex.org/W6611766843', 'https://openalex.org/W6634400515', 'https://openalex.org/W6679966330', 'https://openalex.org/W2123003832', 'https://openalex.org/W1990394889', 'https://openalex.org/W6711753470', 'https://openalex.org/W2035962301', 'https://openalex.org/W46785905', 'https://openalex.org/W2013996527', 'https://openalex.org/W2114543868', 'https://openalex.org/W2007023536', 'https://openalex.org/W2136166660', 'https://openalex.org/W2156142001', 'https://openalex.org/W1935012542', 'https://openalex.org/W2128766084', 'https://openalex.org/W2114659828', 'https://openalex.org/W2145130307', 'https://openalex.org/W2154920538', 'https://openalex.org/W2152974894', 'https://openalex.org/W22889032', 'https://openalex.org/W1550001449', 'https://openalex.org/W2264797740', 'https://openalex.org/W1566744681', 'https://openalex.org/W1588266896', 'https://openalex.org/W2406750268', 'https://openalex.org/W1540787848', 'https://openalex.org/W2147152002', 'https://openalex.org/W2395052932', 'https://openalex.org/W344150399', 'https://openalex.org/W1600722501', 'https://openalex.org/W2132462584', 'https://openalex.org/W3177989406', 'https://openalex.org/W1577231161']",2007-10-15
https://openalex.org/W3092368332,https://doi.org/10.21437/vccbc.2020-24,The Sequence-to-Sequence Baseline for the Voice Conversion Challenge 2020: Cascading ASR and TTS,"This paper presents the sequence-to-sequence (seq2seq) baseline system for the voice conversion challenge (VCC) 2020. We consider a naive approach for voice conversion (VC), which is to first transcribe the input speech with an automatic speech recognition (ASR) model, followed using the transcriptions to generate the voice of the target with a text-to-speech (TTS) model. We revisit this method under a sequence-to-sequence (seq2seq) framework by utilizing ESPnet, an open-source end-to-end speech processing toolkit, and the many well-configured pretrained models provided by the community. Official evaluation results show that our system comes out top among the participating systems in terms of conversion similarity, demonstrating the promising ability of seq2seq models to convert speaker identity. The implementation is made open-source at: this https URL.","['https://openalex.org/W2120605154', 'https://openalex.org/W4288079962', 'https://openalex.org/W2518172956', 'https://openalex.org/W2889329491', 'https://openalex.org/W2532494225', 'https://openalex.org/W2608338293', 'https://openalex.org/W2795409001', 'https://openalex.org/W2945478979', 'https://openalex.org/W3002433751', 'https://openalex.org/W6679436768', 'https://openalex.org/W2899877258', 'https://openalex.org/W2994715919', 'https://openalex.org/W6750226204', 'https://openalex.org/W2981728663', 'https://openalex.org/W1494198834', 'https://openalex.org/W6739901393', 'https://openalex.org/W2892009249', 'https://openalex.org/W2972389417', 'https://openalex.org/W2766219058', 'https://openalex.org/W2957543415', 'https://openalex.org/W2890964092', 'https://openalex.org/W2925134954', 'https://openalex.org/W6761768988', 'https://openalex.org/W2931508530', 'https://openalex.org/W2981087920', 'https://openalex.org/W2808706139', 'https://openalex.org/W6757079273', 'https://openalex.org/W2897548994', 'https://openalex.org/W2972802841', 'https://openalex.org/W2982055294', 'https://openalex.org/W2130942839', 'https://openalex.org/W2973177731', 'https://openalex.org/W2963403868', 'https://openalex.org/W2972595148', 'https://openalex.org/W2156142001', 'https://openalex.org/W2972359262', 'https://openalex.org/W2949281321', 'https://openalex.org/W2963830550', 'https://openalex.org/W2963432880', 'https://openalex.org/W3101689408', 'https://openalex.org/W3099078140', 'https://openalex.org/W3100378519', 'https://openalex.org/W2972473628', 'https://openalex.org/W2963568578', 'https://openalex.org/W2903739847', 'https://openalex.org/W3015338123', 'https://openalex.org/W2962780374']",2020-10-30
https://openalex.org/W2518172956,https://doi.org/10.1109/icme.2016.7552917,Phonetic posteriorgrams for many-to-one voice conversion without parallel data training,"This paper proposes a novel approach to voice conversion with non-parallel training data. The idea is to bridge between speakers by means of Phonetic PosteriorGrams (PPGs) obtained from a speaker-independent automatic speech recognition (SI-ASR) system. It is assumed that these PPGs can represent articulation of speech sounds in a speaker-normalized space and correspond to spoken content speaker-independently. The proposed approach first obtains PPGs of target speech. Then, a Deep Bidirectional Long Short-Term Memory based Recurrent Neural Network (DBLSTM) structure is used to model the relationships between the PPGs and acoustic features of the target speech. To convert arbitrary source speech, we obtain its PPGs from the same SI-ASR and feed them into the trained DBLSTM for generating converted speech. Our approach has two main advantages: 1) no parallel training data is required; 2) a trained model can be applied to any other source speaker for a fixed target speaker (i.e., many-to-one conversion). Experiments show that our approach performs equally well or better than state-of-the-art systems in both speech quality and speaker similarity.","['https://openalex.org/W6711872879', 'https://openalex.org/W2171019095', 'https://openalex.org/W2401207139', 'https://openalex.org/W2115055618', 'https://openalex.org/W2049686551', 'https://openalex.org/W2096980176', 'https://openalex.org/W6603838645', 'https://openalex.org/W6631362777', 'https://openalex.org/W6789826613', 'https://openalex.org/W6610843619', 'https://openalex.org/W6696767757', 'https://openalex.org/W6732251480', 'https://openalex.org/W2161476805', 'https://openalex.org/W1509691205', 'https://openalex.org/W2404109308', 'https://openalex.org/W2135832479', 'https://openalex.org/W2120605154', 'https://openalex.org/W2156142001', 'https://openalex.org/W2032130465', 'https://openalex.org/W2294351487', 'https://openalex.org/W95152782', 'https://openalex.org/W3127686677', 'https://openalex.org/W2395403337', 'https://openalex.org/W2577042574', 'https://openalex.org/W1524333225', 'https://openalex.org/W304834817']",2016-07-01
https://openalex.org/W2963539064,https://doi.org/10.1109/slt.2018.8639535,StarGAN-VC: non-parallel many-to-many Voice Conversion Using Star Generative Adversarial Networks,"This paper proposes a method that allows non-parallel many-to-many voice conversion (VC) by using a variant of a generative adversarial network (GAN) called StarGAN. Our method, which we call StarGAN-VC, is noteworthy in that it (1) requires no parallel utterances, transcriptions, or time alignment procedures for speech generator training, (2) simultaneously learns many-to-many mappings across different attribute domains using a single generator network, (3) is able to generate converted speech signals quickly enough to allow real-time implementations and (4) requires only several minutes of training examples to generate reasonably realistic sounding speech. Subjective evaluation experiments on a non-parallel many-to-many speaker identity conversion task revealed that the proposed method obtained higher sound quality and speaker similarity than a state-of-the-art method based on variational autoencoding GANs.","['https://openalex.org/W2962793481', 'https://openalex.org/W2964328256', 'https://openalex.org/W6675944832', 'https://openalex.org/W6640963894', 'https://openalex.org/W2150769028', 'https://openalex.org/W2746654391', 'https://openalex.org/W6735429107', 'https://openalex.org/W2963971656', 'https://openalex.org/W2666408839', 'https://openalex.org/W2748561347', 'https://openalex.org/W2752796333', 'https://openalex.org/W2804998325', 'https://openalex.org/W2142300631', 'https://openalex.org/W2123003832', 'https://openalex.org/W2532494225', 'https://openalex.org/W6726273514', 'https://openalex.org/W2518312472', 'https://openalex.org/W2962896155', 'https://openalex.org/W6746801104', 'https://openalex.org/W2651834199', 'https://openalex.org/W2796495654', 'https://openalex.org/W2123771434', 'https://openalex.org/W6751928472', 'https://openalex.org/W2105160541', 'https://openalex.org/W2157412983', 'https://openalex.org/W6735204497', 'https://openalex.org/W2057609679', 'https://openalex.org/W2739735615', 'https://openalex.org/W1509691205', 'https://openalex.org/W1974745215', 'https://openalex.org/W1977362459', 'https://openalex.org/W2086796102', 'https://openalex.org/W2127520494', 'https://openalex.org/W6711854987', 'https://openalex.org/W2148846882', 'https://openalex.org/W2022125261', 'https://openalex.org/W2056852181', 'https://openalex.org/W2161727827', 'https://openalex.org/W2156142001', 'https://openalex.org/W2747744257', 'https://openalex.org/W2963073614', 'https://openalex.org/W2120605154', 'https://openalex.org/W2471520273', 'https://openalex.org/W2769810959', 'https://openalex.org/W2804644188', 'https://openalex.org/W6731370813', 'https://openalex.org/W6745718552', 'https://openalex.org/W2963444790', 'https://openalex.org/W6687506355', 'https://openalex.org/W2515020857', 'https://openalex.org/W2598581049', 'https://openalex.org/W2963341071', 'https://openalex.org/W2805669069', 'https://openalex.org/W1959608418', 'https://openalex.org/W2963767194', 'https://openalex.org/W2774848319', 'https://openalex.org/W2963970792', 'https://openalex.org/W4320013936', 'https://openalex.org/W2964167449', 'https://openalex.org/W2108501770', 'https://openalex.org/W2963035245', 'https://openalex.org/W2396025094', 'https://openalex.org/W2099471712', 'https://openalex.org/W2519091744', 'https://openalex.org/W4294619240', 'https://openalex.org/W2951939904', 'https://openalex.org/W2949382160', 'https://openalex.org/W2567070169', 'https://openalex.org/W2963799213']",2018-12-01
https://openalex.org/W3188160682,,Daft-Exprt: Robust Prosody Transfer Across Speakers for Expressive Speech Synthesis.,"This paper presents Daft-Exprt, a multi-speaker acoustic model advancing the state-of-the-art on inter-speaker and inter-text prosody transfer. This improvement is achieved using FiLM conditioning layers, alongside adversarial training that encourages disentanglement between prosodic information and speaker identity. The acoustic model inherits attractive qualities from FastSpeech 2, such as fast inference and local prosody attributes prediction for finer grained control over generation. Experimental results show that Daft-Exprt significantly outperforms strong baselines on prosody transfer tasks, while yielding naturalness comparable to state-of-the-art expressive models. Moreover, results indicate that adversarial training effectively discards speaker identity information from the prosody representation, which ensures Daft-Exprt will consistently generate speech with the desired voice. We publicly release our code and provide speech samples from our experiments.","['https://openalex.org/W3130016944', 'https://openalex.org/W2963272440', 'https://openalex.org/W2962723986', 'https://openalex.org/W2603777577', 'https://openalex.org/W3128910262', 'https://openalex.org/W2545656684', 'https://openalex.org/W2972359262', 'https://openalex.org/W2903739847', 'https://openalex.org/W2963609956', 'https://openalex.org/W2963927338', 'https://openalex.org/W2567658225', 'https://openalex.org/W2796339975', 'https://openalex.org/W2948238043', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963830550', 'https://openalex.org/W3163475957', 'https://openalex.org/W3150572638', 'https://openalex.org/W2917688842', 'https://openalex.org/W2963596039', 'https://openalex.org/W2963826681', 'https://openalex.org/W3025528898', 'https://openalex.org/W3091928890', 'https://openalex.org/W2972569067', 'https://openalex.org/W2954386831', 'https://openalex.org/W2803193013', 'https://openalex.org/W2963921132', 'https://openalex.org/W3098403858', 'https://openalex.org/W2904459034', 'https://openalex.org/W3168527213']",2021-08-04
https://openalex.org/W4288079962,https://doi.org/10.48550/arxiv.2008.12527,Voice Conversion Challenge 2020: Intra-lingual semi-parallel and cross-lingual voice conversion,"The voice conversion challenge is a bi-annual scientific event held to compare and understand different voice conversion (VC) systems built on a common dataset. In 2020, we organized the third edition of the challenge and constructed and distributed a new database for two tasks, intra-lingual semi-parallel and cross-lingual VC. After a two-month challenge period, we received 33 submissions, including 3 baselines built on the database. From the results of crowd-sourced listening tests, we observed that VC methods have progressed rapidly thanks to advanced deep learning methods. In particular, speaker similarity scores of several systems turned out to be as high as target speakers in the intra-lingual semi-parallel VC task. However, we confirmed that none of them have achieved human-level naturalness yet for the same task. The cross-lingual conversion task is, as expected, a more difficult task, and the overall naturalness and similarity scores were lower than those for the intra-lingual conversion task. However, we observed encouraging results, and the MOS scores of the best systems were higher than 4.0. We also show a few additional analysis results to aid in understanding cross-lingual VC better.",[],2020-08-28
https://openalex.org/W2949382160,,WaveNet: A Generative Model for Raw Audio,"This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.","['https://openalex.org/W2963175699', 'https://openalex.org/W181056519', 'https://openalex.org/W1610060839', 'https://openalex.org/W2139576349', 'https://openalex.org/W2507771204', 'https://openalex.org/W2964301388', 'https://openalex.org/W2408093180', 'https://openalex.org/W2293049663', 'https://openalex.org/W1923697677', 'https://openalex.org/W1979449467', 'https://openalex.org/W1527535554', 'https://openalex.org/W2259472270', 'https://openalex.org/W1542280630', 'https://openalex.org/W2953037075', 'https://openalex.org/W3035139526', 'https://openalex.org/W2064675550', 'https://openalex.org/W2112197391', 'https://openalex.org/W2150658333', 'https://openalex.org/W2133035145', 'https://openalex.org/W2395849284', 'https://openalex.org/W2471520273', 'https://openalex.org/W2000513720', 'https://openalex.org/W2063105057', 'https://openalex.org/W1579853615', 'https://openalex.org/W1560013842', 'https://openalex.org/W2285182995', 'https://openalex.org/W2423557781', 'https://openalex.org/W2428180336', 'https://openalex.org/W2953318193', 'https://openalex.org/W2294797155', 'https://openalex.org/W2338186431', 'https://openalex.org/W1877553482', 'https://openalex.org/W2949650786', 'https://openalex.org/W2527729766', 'https://openalex.org/W2286929393', 'https://openalex.org/W2067329295', 'https://openalex.org/W2102003408', 'https://openalex.org/W2953250761', 'https://openalex.org/W2049686551', 'https://openalex.org/W2337335398', 'https://openalex.org/W2798983173', 'https://openalex.org/W1594494252', 'https://openalex.org/W3142087749', 'https://openalex.org/W233059834', 'https://openalex.org/W2395578248']",2016-09-12
https://openalex.org/W2949281321,,AutoVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss,"Non-parallel many-to-many voice conversion, as well as zero-shot voice conversion, remain under-explored areas. Deep style transfer algorithms, such as generative adversarial networks (GAN) and conditional variational autoencoder (CVAE), are being applied as new solutions in this field. However, GAN training is sophisticated and difficult, and there is no strong evidence that its generated speech is of good perceptual quality. On the other hand, CVAE training is simple but does not come with the distribution-matching property of a GAN. In this paper, we propose a new style transfer scheme that involves only an autoencoder with a carefully designed bottleneck. We formally show that this scheme can achieve distribution-matching style transfer by training only on a self-reconstruction loss. Based on this scheme, we proposed AUTOVC, which achieves state-of-the-art results in many-to-many voice conversion with non-parallel data, and which is the first to perform zero-shot voice conversion.",[],2019-05-24
https://openalex.org/W3082130377,https://doi.org/10.21437/vcc_bc.2020-14,Voice Conversion Challenge 2020 –- Intra-lingual semi-parallel and cross-lingual voice conversion –-,"The voice conversion challenge is a bi-annual scientific event held to compare and understand different voice conversion (VC) systems built on a common dataset.In 2020, we organized the third edition of the challenge and constructed and distributed a new database for two tasks, intra-lingual semiparallel and cross-lingual VC.After a two-month challenge period, we received 33 submissions, including 3 baselines built on the database.From the results of crowd-sourced listening tests, we observed that VC methods have progressed rapidly thanks to advanced deep learning methods.In particular, speaker similarity scores of several systems turned out to be as high as target speakers in the intra-lingual semi-parallel VC task.However, we confirmed that none of them have achieved human-level naturalness yet for the same task.The cross-lingual conversion task is, as expected, a more difficult task, and the overall naturalness and similarity scores were lower than those for the intra-lingual conversion task.However, we observed encouraging results, and the MOS scores of the best systems were higher than 4.0.We also show a few additional analysis results to aid in understanding cross-lingual VC better.","['https://openalex.org/W6681167449', 'https://openalex.org/W2017742648', 'https://openalex.org/W2161727827', 'https://openalex.org/W1590808459', 'https://openalex.org/W2056852181', 'https://openalex.org/W6720742747', 'https://openalex.org/W2963035245', 'https://openalex.org/W2515028311', 'https://openalex.org/W6634186343', 'https://openalex.org/W3047769339', 'https://openalex.org/W2963048608', 'https://openalex.org/W2518172956', 'https://openalex.org/W2806000759', 'https://openalex.org/W2889329491', 'https://openalex.org/W2945478979', 'https://openalex.org/W2752796333', 'https://openalex.org/W2937343983', 'https://openalex.org/W2608338293', 'https://openalex.org/W6682600690', 'https://openalex.org/W2890964092', 'https://openalex.org/W2938583109', 'https://openalex.org/W6736996214', 'https://openalex.org/W6739901393', 'https://openalex.org/W6757079273', 'https://openalex.org/W2587284713', 'https://openalex.org/W2928664166', 'https://openalex.org/W3020975377', 'https://openalex.org/W2994715919', 'https://openalex.org/W3027343259', 'https://openalex.org/W2902070858', 'https://openalex.org/W2963539064', 'https://openalex.org/W2995124259', 'https://openalex.org/W2794725088', 'https://openalex.org/W2161476805', 'https://openalex.org/W6748409065', 'https://openalex.org/W2898322826', 'https://openalex.org/W2981728663', 'https://openalex.org/W2898847420', 'https://openalex.org/W2970006822', 'https://openalex.org/W2940620867', 'https://openalex.org/W2120847449', 'https://openalex.org/W2471520273', 'https://openalex.org/W1963637322', 'https://openalex.org/W3083776549', 'https://openalex.org/W6682699007', 'https://openalex.org/W3095990227', 'https://openalex.org/W2946809691', 'https://openalex.org/W3016160783', 'https://openalex.org/W2963609956', 'https://openalex.org/W2972659941', 'https://openalex.org/W2972970915', 'https://openalex.org/W4385245566', 'https://openalex.org/W2519091744', 'https://openalex.org/W2608207374', 'https://openalex.org/W2150769028', 'https://openalex.org/W2788851830', 'https://openalex.org/W2963799213', 'https://openalex.org/W2962896155', 'https://openalex.org/W2990440871', 'https://openalex.org/W3096864844', 'https://openalex.org/W3015338123', 'https://openalex.org/W2963403868', 'https://openalex.org/W1599112982', 'https://openalex.org/W3098557217', 'https://openalex.org/W2949382160', 'https://openalex.org/W2473388484', 'https://openalex.org/W1572989473', 'https://openalex.org/W2963091184', 'https://openalex.org/W2972544500', 'https://openalex.org/W2962793481', 'https://openalex.org/W2903739847', 'https://openalex.org/W3096567388', 'https://openalex.org/W2963300588', 'https://openalex.org/W2152573073', 'https://openalex.org/W2142300631', 'https://openalex.org/W4298580827']",2020-10-16
https://openalex.org/W3098403858,,HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis,"Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.",[],2020-10-12
https://openalex.org/W4289299319,https://doi.org/10.48550/arxiv.1811.04076,AttS2S-VC: Sequence-to-Sequence Voice Conversion with Attention and\n Context Preservation Mechanisms,"This paper describes a method based on a sequence-to-sequence learning\n(Seq2Seq) with attention and context preservation mechanism for voice\nconversion (VC) tasks. Seq2Seq has been outstanding at numerous tasks involving\nsequence modeling such as speech synthesis and recognition, machine\ntranslation, and image captioning. In contrast to current VC techniques, our\nmethod 1) stabilizes and accelerates the training procedure by considering\nguided attention and proposed context preservation losses, 2) allows not only\nspectral envelopes but also fundamental frequency contours and durations of\nspeech to be converted, 3) requires no context information such as phoneme\nlabels, and 4) requires no time-aligned source and target speech data in\nadvance. In our experiment, the proposed VC framework can be trained in only\none day, using only one GPU of an NVIDIA Tesla K80, while the quality of the\nsynthesized speech is higher than that of speech converted by Gaussian mixture\nmodel-based VC and is comparable to that of speech generated by recurrent\nneural network-based text-to-speech synthesis, which can be regarded as an\nupper limit on VC performance.\n",[],2018-11-09
https://openalex.org/W2947445680,https://doi.org/10.21437/interspeech.2019-2048,Unsupervised End-to-End Learning of Discrete Linguistic Units for Voice Conversion,"We present an unsupervised end-to-end training scheme where we discover\ndiscrete subword units from speech without using any labels. The discrete\nsubword units are learned under an ASR-TTS autoencoder reconstruction setting,\nwhere an ASR-Encoder is trained to discover a set of common linguistic units\ngiven a variety of speakers, and a TTS-Decoder trained to project the\ndiscovered units back to the designated speech. We propose a discrete encoding\nmethod, Multilabel-Binary Vectors (MBV), to make the ASR-TTS autoencoder\ndifferentiable. We found that the proposed encoding method offers automatic\nextraction of speech content from speaker style, and is sufficient to cover\nfull linguistic content in a given language. Therefore, the TTS-Decoder can\nsynthesize speech with the same content as the input of ASR-Encoder but with\ndifferent speaker characteristics, which achieves voice conversion (VC). We\nfurther improve the quality of VC using adversarial training, where we train a\nTTS-Patcher that augments the output of TTS-Decoder. Objective and subjective\nevaluations show that the proposed approach offers strong VC results as it\neliminates speaker identity while preserving content within speech. In the\nZeroSpeech 2019 Challenge, we achieved outstanding performance in terms of low\nbitrate.\n","['https://openalex.org/W4295521014', 'https://openalex.org/W2547039119', 'https://openalex.org/W2963691546', 'https://openalex.org/W2899518769', 'https://openalex.org/W2963609956', 'https://openalex.org/W2550241133', 'https://openalex.org/W1522301498', 'https://openalex.org/W2792995953', 'https://openalex.org/W2020607164', 'https://openalex.org/W2767754137', 'https://openalex.org/W2940544976', 'https://openalex.org/W2963073614', 'https://openalex.org/W2963618559', 'https://openalex.org/W2964135678', 'https://openalex.org/W2962974898', 'https://openalex.org/W2532494225', 'https://openalex.org/W4320013936', 'https://openalex.org/W2964243274', 'https://openalex.org/W2347098582', 'https://openalex.org/W4394670483', 'https://openalex.org/W2598638573', 'https://openalex.org/W2099471712', 'https://openalex.org/W2950776302', 'https://openalex.org/W2963830550', 'https://openalex.org/W2963684067', 'https://openalex.org/W2395899413', 'https://openalex.org/W3125709657', 'https://openalex.org/W4288107125', 'https://openalex.org/W2962736743', 'https://openalex.org/W2608207374', 'https://openalex.org/W2476548250', 'https://openalex.org/W2547875792', 'https://openalex.org/W2548275288', 'https://openalex.org/W2963799213', 'https://openalex.org/W2951216052', 'https://openalex.org/W2962879692', 'https://openalex.org/W2758785877', 'https://openalex.org/W2059652594', 'https://openalex.org/W2963571336']",2019-09-13
https://openalex.org/W2953114965,https://doi.org/10.1109/cvpr.2019.00213,Learning Words by Drawing Images,"We propose a framework for learning through drawing. Our goal is to learn the correspondence between spoken words and abstract visual attributes, from a dataset of spoken descriptions of images. Building upon recent findings that GAN representations can be manipulated to edit semantic concepts in the generated output, we propose a new method to use such GAN-generated images to train a model using a triplet loss. To apply the method, we develop Audio CLEVRGAN, a new dataset of audio descriptions of GAN-generated CLEVR images, and we describe a training procedure that creates a curriculum of GAN-generated images that focuses training on image pairs that differ in a specific, informative way. Training is done without additional supervision beyond the spoken captions and the GAN. We find that training that takes advantage of GAN-generated edited examples results in improvements in the model's ability to learn attributes compared to previous results. Our proposed learning framework also results in models that can associate spoken words with some abstract visual concepts such as color and size.","['https://openalex.org/W2963798744', 'https://openalex.org/W6685352114', 'https://openalex.org/W2793546384', 'https://openalex.org/W6745560452', 'https://openalex.org/W2964001192', 'https://openalex.org/W2962716332', 'https://openalex.org/W2964345931', 'https://openalex.org/W6750591037', 'https://openalex.org/W6749394738', 'https://openalex.org/W2296073425', 'https://openalex.org/W4237938692', 'https://openalex.org/W1899185266', 'https://openalex.org/W1971941906', 'https://openalex.org/W2586148577', 'https://openalex.org/W6621378261', 'https://openalex.org/W2758849341', 'https://openalex.org/W2151834591', 'https://openalex.org/W4289665794', 'https://openalex.org/W2963218389', 'https://openalex.org/W2345837149', 'https://openalex.org/W6691895530', 'https://openalex.org/W2619697695', 'https://openalex.org/W6720210739', 'https://openalex.org/W2963143606', 'https://openalex.org/W6729831399', 'https://openalex.org/W2561715562', 'https://openalex.org/W2963115079', 'https://openalex.org/W2963749936', 'https://openalex.org/W6752056529', 'https://openalex.org/W2625455707', 'https://openalex.org/W2964171275', 'https://openalex.org/W6687239747', 'https://openalex.org/W6746078580', 'https://openalex.org/W2962753610', 'https://openalex.org/W6736021936', 'https://openalex.org/W2963680395', 'https://openalex.org/W2963902314', 'https://openalex.org/W6738893770', 'https://openalex.org/W2962865004', 'https://openalex.org/W2560474170', 'https://openalex.org/W6752378368', 'https://openalex.org/W6729977899', 'https://openalex.org/W2951523806', 'https://openalex.org/W2107917162', 'https://openalex.org/W2962756039', 'https://openalex.org/W2964327384', 'https://openalex.org/W3099142230', 'https://openalex.org/W648143168', 'https://openalex.org/W2963907629', 'https://openalex.org/W4320013936', 'https://openalex.org/W2952828155', 'https://openalex.org/W2256388387', 'https://openalex.org/W2796992393', 'https://openalex.org/W2556930864', 'https://openalex.org/W4298018319', 'https://openalex.org/W2963081790', 'https://openalex.org/W2804180102', 'https://openalex.org/W2190656909', 'https://openalex.org/W4294643831', 'https://openalex.org/W2962760235', 'https://openalex.org/W3099849198', 'https://openalex.org/W2797032258', 'https://openalex.org/W2610018085', 'https://openalex.org/W3105148948', 'https://openalex.org/W2963996492', 'https://openalex.org/W4293665662', 'https://openalex.org/W2962960500', 'https://openalex.org/W2471094925', 'https://openalex.org/W2804078698', 'https://openalex.org/W2794529570', 'https://openalex.org/W2963684088', 'https://openalex.org/W2099471712', 'https://openalex.org/W2511428026']",2019-06-01
https://openalex.org/W2950414763,https://doi.org/10.21437/interspeech.2019-1337,Combining Adversarial Training and Disentangled Speech Representation for Robust Zero-Resource Subword Modeling,"This study addresses the problem of unsupervised subword unit discovery from untranscribed speech. It forms the basis of the ultimate goal of ZeroSpeech 2019, building text-to-speech systems without text labels. In this work, unit discovery is formulated as a pipeline of phonetically discriminative feature learning and unit inference. One major difficulty in robust unsupervised feature learning is dealing with speaker variation. Here the robustness towards speaker variation is achieved by applying adversarial training and FHVAE based disentangled speech representation learning. A comparison of the two approaches as well as their combination is studied in a DNN-bottleneck feature (DNN-BNF) architecture. Experiments are conducted on ZeroSpeech 2019 and 2017. Experimental results on ZeroSpeech 2017 show that both approaches are effective while the latter is more prominent, and that their combination brings further marginal improvement in across-speaker condition. Results on ZeroSpeech 2019 show that in the ABX discriminability task, our approaches significantly outperform the official baseline, and are competitive to or even outperform the official topline. The proposed unit sequence smoothing algorithm improves synthesis quality, at a cost of slight decrease in ABX discriminability.","['https://openalex.org/W2516890051', 'https://openalex.org/W1882958252', 'https://openalex.org/W2826003142', 'https://openalex.org/W2128032727', 'https://openalex.org/W2786608204', 'https://openalex.org/W1545920196', 'https://openalex.org/W2399576818', 'https://openalex.org/W2963620343', 'https://openalex.org/W2758785877', 'https://openalex.org/W2598638573', 'https://openalex.org/W2964121744', 'https://openalex.org/W2889228998', 'https://openalex.org/W2940544976', 'https://openalex.org/W1522301498', 'https://openalex.org/W2786902352', 'https://openalex.org/W4288107125', 'https://openalex.org/W2949510815', 'https://openalex.org/W2796339975', 'https://openalex.org/W4289564011', 'https://openalex.org/W2587088898', 'https://openalex.org/W2906459023', 'https://openalex.org/W2402144811', 'https://openalex.org/W1796128977', 'https://openalex.org/W2963618559', 'https://openalex.org/W3125709657', 'https://openalex.org/W2547039119', 'https://openalex.org/W2787426069', 'https://openalex.org/W1524333225', 'https://openalex.org/W2785860501', 'https://openalex.org/W2936120996', 'https://openalex.org/W2787447541', 'https://openalex.org/W2404799143', 'https://openalex.org/W2953384591', 'https://openalex.org/W4300047444', 'https://openalex.org/W2963826681']",2019-09-13
https://openalex.org/W2295297373,https://doi.org/10.1109/taslp.2016.2517567,Unsupervised Word Segmentation and Lexicon Discovery Using Acoustic Word Embeddings,"In settings where only unlabelled speech data is available, speech technology needs to be developed without transcriptions, pronunciation dictionaries, or language modelling text. A similar problem is faced when modelling infant language acquisition. In these cases, categorical linguistic structure needs to be discovered directly from speech audio. We present a novel unsupervised Bayesian model that segments unlabelled speech and clusters the segments into hypothesized word groupings. The result is a complete unsupervised tokenization of the input speech in terms of discovered word types. In our approach, a potential word segment (of arbitrary length) is embedded in a fixed-dimensional acoustic vector space. The model, implemented as a Gibbs sampler, then builds a whole-word acoustic model in this space while jointly performing segmentation. We report word error rates in a small-vocabulary connected digit recognition task by mapping the unsupervised decoded output to ground truth transcriptions. The model achieves around 20% error rate, outperforming a previous HMM-based system by about 10% absolute. Moreover, in contrast to the baseline, our model does not require a pre-specified vocabulary size.","['https://openalex.org/W2152131029', 'https://openalex.org/W1539673959', 'https://openalex.org/W1590183771', 'https://openalex.org/W1997505733', 'https://openalex.org/W6675747103', 'https://openalex.org/W2097308346', 'https://openalex.org/W6675002571', 'https://openalex.org/W2116422968', 'https://openalex.org/W1606268232', 'https://openalex.org/W2022058071', 'https://openalex.org/W6607261905', 'https://openalex.org/W6640777149', 'https://openalex.org/W2074546930', 'https://openalex.org/W2166391802', 'https://openalex.org/W2161952424', 'https://openalex.org/W2140991203', 'https://openalex.org/W2142390309', 'https://openalex.org/W6687508285', 'https://openalex.org/W1577418252', 'https://openalex.org/W6602180557', 'https://openalex.org/W2799046698', 'https://openalex.org/W2057007397', 'https://openalex.org/W2059652594', 'https://openalex.org/W2114347655', 'https://openalex.org/W2126377586', 'https://openalex.org/W2010188467', 'https://openalex.org/W2154093685', 'https://openalex.org/W2032943813', 'https://openalex.org/W2126203737', 'https://openalex.org/W6676028815', 'https://openalex.org/W6678007500', 'https://openalex.org/W2036964623', 'https://openalex.org/W2033413759', 'https://openalex.org/W2168277610', 'https://openalex.org/W6675022971', 'https://openalex.org/W2072396742', 'https://openalex.org/W6638059883', 'https://openalex.org/W2398490608', 'https://openalex.org/W2157427027', 'https://openalex.org/W2152483743', 'https://openalex.org/W1967924372', 'https://openalex.org/W2107038463', 'https://openalex.org/W1778492285', 'https://openalex.org/W52412328', 'https://openalex.org/W2166270474', 'https://openalex.org/W2190506272', 'https://openalex.org/W1525748279', 'https://openalex.org/W2099873701', 'https://openalex.org/W1942713348', 'https://openalex.org/W2997701990', 'https://openalex.org/W2952155034', 'https://openalex.org/W2104290444', 'https://openalex.org/W178496478', 'https://openalex.org/W2952343510', 'https://openalex.org/W2100768664', 'https://openalex.org/W1503398984', 'https://openalex.org/W2116330964', 'https://openalex.org/W2120636621']",2016-01-12
https://openalex.org/W2106053110,https://doi.org/10.5555/1577069.1577078,Distance Metric Learning for Large Margin Nearest Neighbor Classification,"The accuracy of k-nearest neighbor (kNN) classification depends significantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Mahalanobis distance metric for kNN classification from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classification using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. As in support vector machines (SVMs), the margin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach requires no modification or extension for problems in multiway (as opposed to binary) classification. In our framework, the Mahalanobis distance metric is obtained as the solution to a semidefinite program. On several data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner.","['https://openalex.org/W1823600381', 'https://openalex.org/W2136261952', 'https://openalex.org/W1596398496', 'https://openalex.org/W2138451337', 'https://openalex.org/W2108995755', 'https://openalex.org/W2122111042', 'https://openalex.org/W2117154949', 'https://openalex.org/W66926763', 'https://openalex.org/W1620759536', 'https://openalex.org/W2003677307', 'https://openalex.org/W2104752854', 'https://openalex.org/W1588635063', 'https://openalex.org/W2095979710', 'https://openalex.org/W2140095548', 'https://openalex.org/W2148694408', 'https://openalex.org/W2091632079', 'https://openalex.org/W2133296809', 'https://openalex.org/W2144935315', 'https://openalex.org/W2057175746', 'https://openalex.org/W2170605888', 'https://openalex.org/W2296319761', 'https://openalex.org/W2912522929', 'https://openalex.org/W2121947440', 'https://openalex.org/W2054498688', 'https://openalex.org/W2147717514', 'https://openalex.org/W2157791002', 'https://openalex.org/W1667072054', 'https://openalex.org/W2122808326', 'https://openalex.org/W1676820704', 'https://openalex.org/W2137291015']",2009-12-01
https://openalex.org/W2107917162,https://doi.org/10.1016/s0364-0213(01)00061-1,Learning words from sights and sounds: a computational model,"This paper presents an implemented computational model of word acquisition which learns directly from raw multimodal sensory input. Set in an information theoretic framework, the model acquires a lexicon by finding and statistically modeling consistent cross-modal structure. The model has been implemented in a system using novel speech processing, computer vision, and machine learning algorithms. In evaluations the model successfully performed speech segmentation, word discovery and visual categorization from spontaneous infant-directed speech paired with video images of single objects. These results demonstrate the possibility of using state-of-the-art techniques from sensory pattern recognition and machine learning to implement cognitive models which can process raw sensor data without the need for human transcription or labeling.","['https://openalex.org/W1554663460', 'https://openalex.org/W2333916262', 'https://openalex.org/W2137075158', 'https://openalex.org/W1499050190', 'https://openalex.org/W2135884851', 'https://openalex.org/W2126493631', 'https://openalex.org/W2110871230', 'https://openalex.org/W4250234291', 'https://openalex.org/W2141622386', 'https://openalex.org/W2022378810', 'https://openalex.org/W4237938692', 'https://openalex.org/W2140661818', 'https://openalex.org/W2045830397', 'https://openalex.org/W4253573210', 'https://openalex.org/W2509276541', 'https://openalex.org/W322227076', 'https://openalex.org/W1942647574', 'https://openalex.org/W2952343510', 'https://openalex.org/W2073022807', 'https://openalex.org/W2054658115', 'https://openalex.org/W4238354054', 'https://openalex.org/W2566861030', 'https://openalex.org/W2149492821', 'https://openalex.org/W2165470078', 'https://openalex.org/W1982374456', 'https://openalex.org/W2060491487', 'https://openalex.org/W2029948425', 'https://openalex.org/W2099111195', 'https://openalex.org/W170174517', 'https://openalex.org/W1535702289', 'https://openalex.org/W2112166433', 'https://openalex.org/W4245122798', 'https://openalex.org/W2065114842', 'https://openalex.org/W1992119553', 'https://openalex.org/W2125838338', 'https://openalex.org/W3036063182', 'https://openalex.org/W1980862600', 'https://openalex.org/W2074546930', 'https://openalex.org/W2074855560', 'https://openalex.org/W4388297464', 'https://openalex.org/W3128655779', 'https://openalex.org/W1971771135', 'https://openalex.org/W285471286', 'https://openalex.org/W1980491396', 'https://openalex.org/W2162922114', 'https://openalex.org/W1855262058', 'https://openalex.org/W2882319491', 'https://openalex.org/W2067242602', 'https://openalex.org/W2124973255', 'https://openalex.org/W2090072485', 'https://openalex.org/W1628323995', 'https://openalex.org/W2123758925', 'https://openalex.org/W1534064252']",2002-02-01
https://openalex.org/W2920166246,https://doi.org/10.1109/icassp.2019.8683069,Models of Visually Grounded Speech Signal Pay Attention to Nouns: A Bilingual Experiment on English and Japanese,"We investigate the behaviour of attention in neural models of visually\ngrounded speech trained on two languages: English and Japanese. Experimental\nresults show that attention focuses on nouns and this behaviour holds true for\ntwo very typologically different languages. We also draw parallels between\nartificial neural attention and human attention and show that neural attention\nfocuses on word endings as it has been theorised for human attention. Finally,\nwe investigate how two visually grounded monolingual models can be used to\nperform cross-lingual speech-to-speech retrieval. For both languages, the\nenriched bilingual (speech-image) corpora with part-of-speech tags and forced\nalignments are distributed to the community for reproducible research.\n","['https://openalex.org/W2963496089', 'https://openalex.org/W2292919134', 'https://openalex.org/W2962753610', 'https://openalex.org/W2964001192', 'https://openalex.org/W6729977899', 'https://openalex.org/W2483390977', 'https://openalex.org/W6637373629', 'https://openalex.org/W6743149223', 'https://openalex.org/W2586602577', 'https://openalex.org/W2586148577', 'https://openalex.org/W2962862718', 'https://openalex.org/W2963778889', 'https://openalex.org/W2736876693', 'https://openalex.org/W2963330681', 'https://openalex.org/W2625455707', 'https://openalex.org/W1861492603', 'https://openalex.org/W2185175083', 'https://openalex.org/W2963902314', 'https://openalex.org/W6681262047', 'https://openalex.org/W6679101406', 'https://openalex.org/W2216809352', 'https://openalex.org/W6681871837', 'https://openalex.org/W2143995218', 'https://openalex.org/W2796156786', 'https://openalex.org/W2962835968', 'https://openalex.org/W2580178245', 'https://openalex.org/W2962832640', 'https://openalex.org/W2137010615', 'https://openalex.org/W2747874407', 'https://openalex.org/W2147682057', 'https://openalex.org/W2796315435', 'https://openalex.org/W2611064105', 'https://openalex.org/W2127863960', 'https://openalex.org/W2601713192', 'https://openalex.org/W3099142230', 'https://openalex.org/W2556930864', 'https://openalex.org/W2023182894', 'https://openalex.org/W3105148948', 'https://openalex.org/W1686810756', 'https://openalex.org/W2738919465']",2019-04-17
https://openalex.org/W2118841860,https://doi.org/10.1109/asru.2005.1566529,Towards unsupervised pattern discovery in speech,"We present an unsupervised algorithm for discovering acoustic patterns in speech by finding matching subsequences between pairs of utterances. The approach we describe is, in theory, language and topic independent, and is particularly well suited for processing large amounts of speech from a single speaker. A variation of dynamic time warping (DTW), which we call segmental DTW, is used to performing the pairwise utterance comparison. Using academic lecture data, we describe two potentially useful applications for the segmental DTW output: augmenting speech recognition transcriptions for information retrieval and speech segment clustering for unsupervised word discovery. Some preliminary qualitative results for both experiments are shown and the implications for future work and applications are discussed","['https://openalex.org/W2009566340', 'https://openalex.org/W2028903194', 'https://openalex.org/W2159344129', 'https://openalex.org/W2114510609', 'https://openalex.org/W2122686984', 'https://openalex.org/W156258841', 'https://openalex.org/W2046134527', 'https://openalex.org/W2099402246', 'https://openalex.org/W1560013842']",2005-01-01
https://openalex.org/W2938991416,https://doi.org/10.1109/icassp.2019.8683275,Semantic Query-by-example Speech Search Using Visual Grounding,"A number of recent studies have started to investigate how speech systems can be trained on untranscribed speech by leveraging accompanying images at training time. Examples of tasks include keyword prediction and within- and across-mode retrieval. Here we consider how such models can be used for query-by-example (QbE) search, the task of retrieving utterances relevant to a given spoken query. We are particularly interested in semantic QbE, where the task is not only to retrieve utterances containing exact instances of the query, but also utterances whose meaning is relevant to the query. We follow a segmental QbE approach where variable-duration speech segments (queries, search utterances) are mapped to fixed-dimensional embedding vectors. We show that a QbE system using an embedding function trained on visually grounded speech data outperforms a purely acoustic QbE system in terms of both exact and semantic retrieval performance.","['https://openalex.org/W2114347655', 'https://openalex.org/W2962862718', 'https://openalex.org/W6751433836', 'https://openalex.org/W2889313720', 'https://openalex.org/W2884305338', 'https://openalex.org/W1854884267', 'https://openalex.org/W2170682101', 'https://openalex.org/W2963902314', 'https://openalex.org/W2964001192', 'https://openalex.org/W2950133079', 'https://openalex.org/W1984076147', 'https://openalex.org/W1577418252', 'https://openalex.org/W1496120315', 'https://openalex.org/W2962980711', 'https://openalex.org/W2802557066', 'https://openalex.org/W2578392894', 'https://openalex.org/W2347145335', 'https://openalex.org/W6729855024', 'https://openalex.org/W6864391120', 'https://openalex.org/W2963571336', 'https://openalex.org/W2963834942', 'https://openalex.org/W2586148577', 'https://openalex.org/W2963425185', 'https://openalex.org/W6729977899', 'https://openalex.org/W2895651543', 'https://openalex.org/W2508907749', 'https://openalex.org/W2962753610', 'https://openalex.org/W2052697931', 'https://openalex.org/W2171019095', 'https://openalex.org/W2407964052', 'https://openalex.org/W2126203737', 'https://openalex.org/W2059652594', 'https://openalex.org/W2190506272', 'https://openalex.org/W6697456849', 'https://openalex.org/W2556930864', 'https://openalex.org/W2550241133', 'https://openalex.org/W2296681920', 'https://openalex.org/W2963340922', 'https://openalex.org/W3102219307', 'https://openalex.org/W2804648901', 'https://openalex.org/W2291770225', 'https://openalex.org/W385555557', 'https://openalex.org/W2964099072', 'https://openalex.org/W4394453761', 'https://openalex.org/W2962736743', 'https://openalex.org/W2964115348']",2019-04-17
https://openalex.org/W2113896236,,Learning Classification with Unlabeled Data,"One of the advantages of supervised learning is that the final error met-ric is available during training. For classifiers, the algorithm can directly reduce the number of misclassifications on the training set. Unfortu-nately, when modeling human learning or constructing classifiers for au-tonomous robots, supervisory labels are often not available or too ex-pensive. In this paper we show that we can substitute for the labels by making use of structure between the pattern distributions to different sen-sory modalities. We show that minimizing the disagreement between the outputs of networks processing patterns from these different modalities is a sensible approximation to minimizing the number of misclassifications in each modality, and leads to similar results. Using the Peterson-Barney vowel dataset we show that the algorithm performs well in finding ap-propriate placement for the codebook vectors particularly when the con-fuseable classes are different for the two modalities. 1","['https://openalex.org/W1990863955', 'https://openalex.org/W65738273', 'https://openalex.org/W2096388912', 'https://openalex.org/W2115020216', 'https://openalex.org/W1929733467', 'https://openalex.org/W1490454746', 'https://openalex.org/W1967011375']",1993-11-29
https://openalex.org/W2395899413,https://doi.org/10.21437/interspeech.2013-441,Evaluating speech features with the minimal-pair ABX task: analysis of the classical MFC/PLP pipeline,"We present a new framework for the evaluation of speech representations in zero-resource settings, that extends and complements previous work by Carlin, Jansen and Hermansky [1].In particular, we replace their Same/Different discrimination task by several Minimal-Pair ABX (MP-ABX) tasks.We explain the analytical advantages of this new framework and apply it to decompose the standard signal processing pipelines for computing PLP and MFC coefficients.This method enables us to confirm and quantify a variety of well-known and not-so-well-known results in a single framework.","['https://openalex.org/W2407151108', 'https://openalex.org/W2232131953', 'https://openalex.org/W2980286501', 'https://openalex.org/W2137075158', 'https://openalex.org/W2090861223', 'https://openalex.org/W4285719527', 'https://openalex.org/W2096765209', 'https://openalex.org/W2986535481', 'https://openalex.org/W4247807440', 'https://openalex.org/W2054139811', 'https://openalex.org/W168991039', 'https://openalex.org/W2406820985', 'https://openalex.org/W2089177488', 'https://openalex.org/W2025482506', 'https://openalex.org/W2160719354', 'https://openalex.org/W156237177', 'https://openalex.org/W48303286', 'https://openalex.org/W2400113920', 'https://openalex.org/W2117041980', 'https://openalex.org/W2100768664', 'https://openalex.org/W282666689', 'https://openalex.org/W1480485976']",2013-08-25
https://openalex.org/W2767754137,https://doi.org/10.1109/icassp.2018.8461684,Unsupervised Learning of Semantic Audio Representations,"Even in the absence of any explicit semantic annotation, vast collections of audio recordings provide valuable information for learning the categorical structure of sounds. We consider several class-agnostic semantic constraints that apply to unlabeled nonspeech audio: (i) noise and translations in time do not change the underlying sound category, (ii) a mixture of two sound events inherits the categories of the constituents, and (iii) the categories of events in close temporal proximity are likely to be the same or related. Without labels to ground them, these constraints are incompatible with classification loss functions. However, they may still be leveraged to identify geometric inequalities needed for triplet loss-based training of convolutional neural networks. The result is low-dimensional embeddings of the input spectrograms that recover 41% and 84% of the performance of their fully-supervised counterparts when applied to downstream query-by-example sound retrieval and sound event classification tasks, respectively. Moreover, in limited-supervision settings, our unsupervised embeddings double the state-of-the-art classification performance.","['https://openalex.org/W6676071220', 'https://openalex.org/W2561826558', 'https://openalex.org/W6631829595', 'https://openalex.org/W6973666849', 'https://openalex.org/W1967924372', 'https://openalex.org/W6691725507', 'https://openalex.org/W6663645435', 'https://openalex.org/W6632653590', 'https://openalex.org/W2516890051', 'https://openalex.org/W2096733369', 'https://openalex.org/W6738531280', 'https://openalex.org/W219040644', 'https://openalex.org/W2136655611', 'https://openalex.org/W6701655646', 'https://openalex.org/W1975517671', 'https://openalex.org/W6675751002', 'https://openalex.org/W2526050071', 'https://openalex.org/W2963775347', 'https://openalex.org/W2190506272', 'https://openalex.org/W2963420272', 'https://openalex.org/W1520997877', 'https://openalex.org/W343636949', 'https://openalex.org/W6729977899', 'https://openalex.org/W6729831399', 'https://openalex.org/W2844632039', 'https://openalex.org/W2964001192', 'https://openalex.org/W1545920196', 'https://openalex.org/W2618269622', 'https://openalex.org/W2846592055', 'https://openalex.org/W3104866538', 'https://openalex.org/W2326925005', 'https://openalex.org/W2107789863', 'https://openalex.org/W2342547278', 'https://openalex.org/W2052697931', 'https://openalex.org/W2250874882', 'https://openalex.org/W2786608204', 'https://openalex.org/W2106053110', 'https://openalex.org/W4293665662', 'https://openalex.org/W1531663008', 'https://openalex.org/W2591013610', 'https://openalex.org/W2556930864']",2018-04-01
https://openalex.org/W2134670479,,Learning Deep Features for Scene Recognition using Places Database,"Scene recognition is one of the hallmark tasks of computer vision, allowing defi-nition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level fea-tures, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competi-tive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers ’ responses al-lows us to show differences in the internal representations of object-centric and scene-centric networks. 1","['https://openalex.org/W2162915993', 'https://openalex.org/W2155893237', 'https://openalex.org/W2118101390', 'https://openalex.org/W2062118960', 'https://openalex.org/W1576445103', 'https://openalex.org/W2115628259', 'https://openalex.org/W2087189381', 'https://openalex.org/W2131759478', 'https://openalex.org/W1566135517', 'https://openalex.org/W2155541015', 'https://openalex.org/W2108598243', 'https://openalex.org/W2163605009', 'https://openalex.org/W1966385142', 'https://openalex.org/W2152161678', 'https://openalex.org/W2070148066', 'https://openalex.org/W2160921898', 'https://openalex.org/W2147625498', 'https://openalex.org/W2166049352', 'https://openalex.org/W2017814585', 'https://openalex.org/W2072128103', 'https://openalex.org/W2031342017', 'https://openalex.org/W2118585731', 'https://openalex.org/W2038765747', 'https://openalex.org/W2147800946']",2014-12-08
https://openalex.org/W2117041980,https://doi.org/10.3115/1557690.1557736,Unsupervised learning of acoustic sub-word units,"Accurate unsupervised learning of phonemes of a language directly from speech is demonstrated via an algorithm for joint unsupervised learning of the topology and parameters of a hidden Markov model (HMM); states and short state-sequences through this HMM correspond to the learnt sub-word units. The algorithm, originally proposed for unsupervised learning of allophonic variations within a given phoneme set, has been adapted to learn without any knowledge of the phonemes. An evaluation methodology is also proposed, whereby the state-sequence that aligns to a test utterance is transduced in an automatic manner to a phoneme-sequence and compared to its manual transcription. Over 85% phoneme recognition accuracy is demonstrated for speaker-dependent learning from fluent, large-vocabulary speech.","['https://openalex.org/W4298166701', 'https://openalex.org/W3034729383', 'https://openalex.org/W2101536075', 'https://openalex.org/W1971081490', 'https://openalex.org/W1949782964', 'https://openalex.org/W1918599710']",2008-01-01
https://openalex.org/W2971709506,https://doi.org/10.21437/interspeech.2019-3067,Language Learning Using Speech to Image Retrieval,"Humans learn language by interaction with their environment and listening to\nother humans. It should also be possible for computational models to learn\nlanguage directly from speech but so far most approaches require text. We\nimprove on existing neural network approaches to create visually grounded\nembeddings for spoken utterances. Using a combination of a multi-layer GRU,\nimportance sampling, cyclic learning rates, ensembling and vectorial\nself-attention our results show a remarkable increase in image-caption\nretrieval performance over previous work. Furthermore, we investigate which\nlayers in the model learn to recognise words in the input. We find that deeper\nnetwork layers are better at encoding word presence, although the final layer\nhas slightly lower performance. This shows that our visually grounded sentence\nencoder learns to recognise words from the input even though it is not\nexplicitly trained for word recognition.\n","['https://openalex.org/W2963902314', 'https://openalex.org/W2556930864', 'https://openalex.org/W2730658205', 'https://openalex.org/W2758849341', 'https://openalex.org/W2137010615', 'https://openalex.org/W2462305634', 'https://openalex.org/W2481240925', 'https://openalex.org/W2075446875', 'https://openalex.org/W2164757945', 'https://openalex.org/W2153579005', 'https://openalex.org/W2923959898', 'https://openalex.org/W2612983688', 'https://openalex.org/W2194775991', 'https://openalex.org/W2964001192', 'https://openalex.org/W1905882502', 'https://openalex.org/W2296167893', 'https://openalex.org/W2586148577', 'https://openalex.org/W2058354688', 'https://openalex.org/W1924770834', 'https://openalex.org/W2039762981', 'https://openalex.org/W68733909', 'https://openalex.org/W4294170691', 'https://openalex.org/W2889404673', 'https://openalex.org/W1522301498', 'https://openalex.org/W1486649854', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963264012', 'https://openalex.org/W2250539671', 'https://openalex.org/W2016076298', 'https://openalex.org/W2962862718', 'https://openalex.org/W2739181657', 'https://openalex.org/W2964054038', 'https://openalex.org/W2951696358', 'https://openalex.org/W2963918774']",2019-09-13
https://openalex.org/W2556930864,,Unsupervised learning of spoken language with visual context,"Humans learn to speak before they can read or write, so why can't computers do the same? In this paper, we present a deep neural network model capable of rudimentary spoken language acquisition using untranscribed audio training data, whose only supervision comes in the form of contextually relevant visual images. We describe the collection of our data comprised of over 120,000 spoken audio captions for the Places image dataset and evaluate our model on an image search and annotation task. We also provide some visualizations which suggest that our model is learning to recognize meaningful words within the caption spectrograms.","['https://openalex.org/W2100768664', 'https://openalex.org/W1905882502', 'https://openalex.org/W2314664620', 'https://openalex.org/W2134670479', 'https://openalex.org/W1969608442', 'https://openalex.org/W2112912048', 'https://openalex.org/W1931639407', 'https://openalex.org/W66167291', 'https://openalex.org/W2009388533', 'https://openalex.org/W2102605133', 'https://openalex.org/W2114347655', 'https://openalex.org/W2062955551', 'https://openalex.org/W1895577753', 'https://openalex.org/W2137471889', 'https://openalex.org/W2119775030', 'https://openalex.org/W2055408826', 'https://openalex.org/W2119187236', 'https://openalex.org/W1524333225', 'https://openalex.org/W2123024445', 'https://openalex.org/W2187089797', 'https://openalex.org/W1778492285', 'https://openalex.org/W2962862718', 'https://openalex.org/W2149557440', 'https://openalex.org/W3127686677', 'https://openalex.org/W2126203737', 'https://openalex.org/W30845872', 'https://openalex.org/W1686810756']",2016-01-01
https://openalex.org/W1942713348,https://doi.org/10.21437/interspeech.2015-239,Fully unsupervised small-vocabulary speech recognition using a segmental Bayesian model,"Current supervised speech technology relies heavily on transcribed speech and pronunciation dictionaries.In settings where unlabelled speech data alone is available, unsupervised methods are required to discover categorical linguistic structure directly from the audio.We present a novel Bayesian model which segments unlabelled input speech into word-like units, resulting in a complete unsupervised transcription of the speech in terms of discovered word types.In our approach, a potential word segment (of arbitrary length) is embedded in a fixed-dimensional space; the model (implemented as a Gibbs sampler) then builds a whole-word acoustic model in this space while jointly doing segmentation.We report word error rates in a connected digit recognition task by mapping the unsupervised output to ground truth transcriptions.Our model outperforms a previously developed HMM-based system, even when the model is not constrained to discover only the 11 word types present in the data.","['https://openalex.org/W2097308346', 'https://openalex.org/W2166270474', 'https://openalex.org/W1577418252', 'https://openalex.org/W2140991203', 'https://openalex.org/W2052697931', 'https://openalex.org/W1545920196', 'https://openalex.org/W2116330964', 'https://openalex.org/W1539673959', 'https://openalex.org/W2107038463', 'https://openalex.org/W2022058071', 'https://openalex.org/W2152483743', 'https://openalex.org/W30845872', 'https://openalex.org/W2799046698', 'https://openalex.org/W2116422968', 'https://openalex.org/W2100768664', 'https://openalex.org/W2059652594', 'https://openalex.org/W2126203737', 'https://openalex.org/W2168277610', 'https://openalex.org/W2152131029', 'https://openalex.org/W2997701990', 'https://openalex.org/W1997505733', 'https://openalex.org/W2114347655', 'https://openalex.org/W1525748279', 'https://openalex.org/W1503398984', 'https://openalex.org/W51277926', 'https://openalex.org/W2126377586', 'https://openalex.org/W2057007397', 'https://openalex.org/W2010188467', 'https://openalex.org/W2020607164', 'https://openalex.org/W2104290444']",2015-09-06
https://openalex.org/W1778492285,https://doi.org/10.1162/tacl_a_00146,Unsupervised Lexicon Discovery from Acoustic Input,"We present a model of unsupervised phonological lexicon discovery—the problem of simultaneously learning phoneme-like and word-like units from acoustic input. Our model builds on earlier models of unsupervised phone-like unit discovery from acoustic data (Lee and Glass, 2012), and unsupervised symbolic lexicon discovery using the Adaptor Grammar framework (Johnson et al., 2006), integrating these earlier approaches using a probabilistic model of phonological variation. We show that the model is competitive with state-of-the-art spoken term discovery systems, and present analyses exploring the model’s behavior and the kinds of linguistic structures it learns.","['https://openalex.org/W1964917299', 'https://openalex.org/W2074546930', 'https://openalex.org/W2162505970', 'https://openalex.org/W2108207895', 'https://openalex.org/W2148154194', 'https://openalex.org/W2069429561', 'https://openalex.org/W2130518563', 'https://openalex.org/W2111732304', 'https://openalex.org/W2101711363', 'https://openalex.org/W2042143122', 'https://openalex.org/W2126377586', 'https://openalex.org/W4251556668', 'https://openalex.org/W1990005915', 'https://openalex.org/W2142390309', 'https://openalex.org/W1980862600', 'https://openalex.org/W2115867364', 'https://openalex.org/W2026858810', 'https://openalex.org/W1969608442', 'https://openalex.org/W2030118973', 'https://openalex.org/W4301623764', 'https://openalex.org/W2111668269', 'https://openalex.org/W2154093685', 'https://openalex.org/W2164151151', 'https://openalex.org/W2126203737', 'https://openalex.org/W1992613273', 'https://openalex.org/W2087309226', 'https://openalex.org/W2114347655', 'https://openalex.org/W2107038463', 'https://openalex.org/W1548450879', 'https://openalex.org/W3036063182', 'https://openalex.org/W2408712009', 'https://openalex.org/W2083195487', 'https://openalex.org/W2140991203', 'https://openalex.org/W2022201897', 'https://openalex.org/W2025482506', 'https://openalex.org/W2053306448', 'https://openalex.org/W2126953647', 'https://openalex.org/W2952343510', 'https://openalex.org/W2116422968', 'https://openalex.org/W2170353620', 'https://openalex.org/W4237961478', 'https://openalex.org/W2100768664', 'https://openalex.org/W2070554026', 'https://openalex.org/W2033413759', 'https://openalex.org/W2117126688', 'https://openalex.org/W2086891622', 'https://openalex.org/W2130042265', 'https://openalex.org/W2188924248', 'https://openalex.org/W1650210997', 'https://openalex.org/W30845872', 'https://openalex.org/W2226889031', 'https://openalex.org/W1779834323', 'https://openalex.org/W2126449874', 'https://openalex.org/W2163245285', 'https://openalex.org/W1525748279', 'https://openalex.org/W2157381218', 'https://openalex.org/W1530250655', 'https://openalex.org/W1843673427', 'https://openalex.org/W1583697620', 'https://openalex.org/W1978470410']",2015-12-01
https://openalex.org/W2126203737,https://doi.org/10.1109/asru.2009.5372931,Unsupervised spoken keyword spotting via segmental DTW on Gaussian posteriorgrams,"In this paper, we present an unsupervised learning framework to address the problem of detecting spoken keywords. Without any transcription information, a Gaussian Mixture Model is trained to label speech frames with a Gaussian posteriorgram. Given one or more spoken examples of a keyword, we use segmental dynamic time warping to compare the Gaussian posteriorgrams between keyword samples and test utterances. The keyword detection result is then obtained by ranking the distortion scores of all the test utterances. We examine the TIMIT corpus as a development set to tune the parameters in our system, and the MIT Lecture corpus for more substantial evaluation. The results demonstrate the viability and effectiveness of our unsupervised learning framework on the keyword spotting task.","['https://openalex.org/W201187342', 'https://openalex.org/W6649874456', 'https://openalex.org/W2122797512', 'https://openalex.org/W2099415988', 'https://openalex.org/W2153426278', 'https://openalex.org/W2155427043', 'https://openalex.org/W2171019095', 'https://openalex.org/W2116969213', 'https://openalex.org/W2114347655', 'https://openalex.org/W2111732304', 'https://openalex.org/W1975113979', 'https://openalex.org/W2103371184', 'https://openalex.org/W2103933358', 'https://openalex.org/W2107076535', 'https://openalex.org/W2405666970', 'https://openalex.org/W2115220208', 'https://openalex.org/W1578200545', 'https://openalex.org/W2086891622', 'https://openalex.org/W2128160875', 'https://openalex.org/W2101536075', 'https://openalex.org/W6684930139', 'https://openalex.org/W1999121090', 'https://openalex.org/W2169384404', 'https://openalex.org/W2142570735']",2009-12-01
https://openalex.org/W385555557,https://doi.org/10.6084/m9.figshare.1277822.v1,Learning Words from Images and Speech,"This paper explores the possibility to learn a semantically-relevant lexicon from images and speech only. For this, we train a multi-modal neural network working both on image fragments and on speech features, by learning an embedding in which images and content words that co-occur together are close. Making no assumption on the acoustic model, this paper shows promising results on how multi-modality could help word learning.","['https://openalex.org/W2953276893', 'https://openalex.org/W2095705004', 'https://openalex.org/W2117154949', 'https://openalex.org/W2230076941', 'https://openalex.org/W2166571083', 'https://openalex.org/W6908809', 'https://openalex.org/W2119775030', 'https://openalex.org/W2052697931', 'https://openalex.org/W2140916511', 'https://openalex.org/W2138621090', 'https://openalex.org/W2102605133', 'https://openalex.org/W1987959440', 'https://openalex.org/W2123081785', 'https://openalex.org/W2163605009', 'https://openalex.org/W2149557440']",2014-01-01
https://openalex.org/W2347098582,https://doi.org/10.1016/j.procs.2016.04.033,Variational Inference for Acoustic Unit Discovery,"Recently, several nonparametric Bayesian models have been proposed to automatically discover acoustic units in unlabeled data. Most of them are trained using various versions of the Gibbs Sampling (GS) method. In this work, we consider Variational Bayes (VB) as alternative inference process. Even though VB yields an approximate solution of the posterior distribution it can be easily parallelized which makes it more suitable for large database. Results show that, notwithstanding VB inference is an order of magnitude faster, it outperforms GS in terms of accuracy.","['https://openalex.org/W2170353620', 'https://openalex.org/W1796128977', 'https://openalex.org/W6959456255', 'https://openalex.org/W1967687583', 'https://openalex.org/W2127498532', 'https://openalex.org/W1635512741', 'https://openalex.org/W6624852173', 'https://openalex.org/W2040891197', 'https://openalex.org/W2154099718', 'https://openalex.org/W1506806321', 'https://openalex.org/W2619993508', 'https://openalex.org/W2120636621', 'https://openalex.org/W1516111018', 'https://openalex.org/W2100768664']",2016-01-01
https://openalex.org/W2404799143,https://doi.org/10.21437/interspeech.2015-640,A hybrid dynamic time warping-deep neural network architecture for unsupervised acoustic modeling,"We report on an architecture for the unsupervised discovery of talker-invariant subword embeddings. It is made out of two components: a dynamic-time warping based spoken term discovery (STD) system and a Siamese deep neural network (DNN). The STD system clusters word-sized repeated fragments in the acoustic streams while the DNN is trained to minimize the distance between time aligned frames of tokens of the same cluster, and maximize the distance between tokens of different clusters. We use additional side information regarding the average duration of phonemic units, as well as talker identity tags. For evaluation we use the datasets and metrics of the Zero Resource Speech Challenge. The model shows improvement over the baseline in subword unit modeling.","['https://openalex.org/W2619993508', 'https://openalex.org/W2608958052', 'https://openalex.org/W2138621090', 'https://openalex.org/W1997460147', 'https://openalex.org/W3127686677', 'https://openalex.org/W1545920196', 'https://openalex.org/W1985371235', 'https://openalex.org/W2251545788', 'https://openalex.org/W2079623482', 'https://openalex.org/W2164770604', 'https://openalex.org/W1993755070', 'https://openalex.org/W2171590421', 'https://openalex.org/W1972159947', 'https://openalex.org/W148837159', 'https://openalex.org/W2048741136', 'https://openalex.org/W2052697931', 'https://openalex.org/W2127589108', 'https://openalex.org/W2054948443', 'https://openalex.org/W1606347560', 'https://openalex.org/W6908809', 'https://openalex.org/W904206136', 'https://openalex.org/W2114347655', 'https://openalex.org/W2057007397', 'https://openalex.org/W21006490']",2015-09-06
https://openalex.org/W2132921748,https://doi.org/10.1109/tmm.2003.811618,Grounded spoken language acquisition: experiments in word learning,"Language is grounded in sensory-motor experience. Grounding connects concepts to the physical world enabling humans to acquire and use words and sentences in context. Currently most machines which process language are not grounded. Instead, semantic representations are abstract, pre-specified, and have meaning only when interpreted by humans. We are interested in developing computational systems which represent words, utterances, and underlying concepts in terms of sensory-motor experiences leading to richer levels of machine understanding. A key element of this work is the development of effective architectures for processing multisensory data. Inspired by theories of infant cognition, we present a computational model which learns words from untranscribed acoustic and video input. Channels of input derived from different sensors are integrated in an information -theoretic framework. Acquired words are represented in terms of associations between acoustic and visual sensory experience. The model has been implemented in a real-time robotic system which performs interactive language learning and understanding. Successful learning has also been demonstrated using infant-directed speech and images.","['https://openalex.org/W2043220715', 'https://openalex.org/W266716723', 'https://openalex.org/W2107019937', 'https://openalex.org/W2150375089', 'https://openalex.org/W2896342372', 'https://openalex.org/W6678835735', 'https://openalex.org/W2060491487', 'https://openalex.org/W1963494713', 'https://openalex.org/W2063971957', 'https://openalex.org/W2127716180', 'https://openalex.org/W2074546930', 'https://openalex.org/W1991427832', 'https://openalex.org/W78796249', 'https://openalex.org/W1966089223', 'https://openalex.org/W6636547438', 'https://openalex.org/W4237938692', 'https://openalex.org/W1942647574', 'https://openalex.org/W2140661818', 'https://openalex.org/W2156909242', 'https://openalex.org/W2110871230', 'https://openalex.org/W2137075158', 'https://openalex.org/W2150355110', 'https://openalex.org/W1519957219', 'https://openalex.org/W2125838338', 'https://openalex.org/W2162922114', 'https://openalex.org/W1855262058', 'https://openalex.org/W4231261525', 'https://openalex.org/W2022378810', 'https://openalex.org/W2135884851', 'https://openalex.org/W2478708596', 'https://openalex.org/W2107917162', 'https://openalex.org/W1613892635', 'https://openalex.org/W2328091329', 'https://openalex.org/W2094249282', 'https://openalex.org/W3110909889', 'https://openalex.org/W1993750641', 'https://openalex.org/W1530250655', 'https://openalex.org/W3022135529', 'https://openalex.org/W2099111195', 'https://openalex.org/W2040487278', 'https://openalex.org/W3128655779', 'https://openalex.org/W3036063182', 'https://openalex.org/W2126493631', 'https://openalex.org/W1499050190']",2003-06-01
https://openalex.org/W2950133079,https://doi.org/10.1109/taslp.2018.2872106,Semantic speech retrieval with a visually grounded model of untranscribed speech,"There is growing interest in models that can learn from unlabelled speech paired with visual context. This setting is relevant for low-resource speech processing, robotics, and human language acquisition research. Here we study how a visually grounded speech model, trained on images of scenes paired with spoken captions, captures aspects of semantics. We use an external image tagger to generate soft text labels from images, which serve as targets for a neural model that maps untranscribed speech to (semantic) keyword labels. We introduce a newly collected data set of human semantic relevance judgements and an associated task, semantic speech retrieval, where the goal is to search for spoken utterances that are semantically relevant to a given text query. Without seeing any text, the model trained on parallel speech and images achieves a precision of almost 60% on its top ten semantic retrievals. Compared to a supervised model trained on transcriptions, our model matches human judgements better by some measures, especially in retrieving non-verbatim semantic matches. We perform an extensive analysis of the model and its resulting representations.","['https://openalex.org/W385555557', 'https://openalex.org/W1491206589', 'https://openalex.org/W1966387021', 'https://openalex.org/W4394453761', 'https://openalex.org/W4285719527', 'https://openalex.org/W2962980711', 'https://openalex.org/W2062955551', 'https://openalex.org/W2593011301', 'https://openalex.org/W2584414011', 'https://openalex.org/W210770835', 'https://openalex.org/W2059861538', 'https://openalex.org/W2033653351', 'https://openalex.org/W2544224704', 'https://openalex.org/W2099415988', 'https://openalex.org/W1897761818', 'https://openalex.org/W2187089797', 'https://openalex.org/W4293665662', 'https://openalex.org/W2580178245', 'https://openalex.org/W2626463069', 'https://openalex.org/W2166571083', 'https://openalex.org/W2168119002', 'https://openalex.org/W1854884267', 'https://openalex.org/W2160783091', 'https://openalex.org/W2963902314', 'https://openalex.org/W2114347655', 'https://openalex.org/W2531381952', 'https://openalex.org/W2136480620', 'https://openalex.org/W2962756039', 'https://openalex.org/W2123815913', 'https://openalex.org/W1814992895', 'https://openalex.org/W1606268232', 'https://openalex.org/W2112184938', 'https://openalex.org/W4307533274', 'https://openalex.org/W2714726990', 'https://openalex.org/W2950178297', 'https://openalex.org/W1895577753', 'https://openalex.org/W2009388533', 'https://openalex.org/W2081580037', 'https://openalex.org/W2029096624', 'https://openalex.org/W155596317', 'https://openalex.org/W2141038596', 'https://openalex.org/W2112912048', 'https://openalex.org/W2508429489', 'https://openalex.org/W2556930864', 'https://openalex.org/W4231856373', 'https://openalex.org/W2964115348', 'https://openalex.org/W2117041980', 'https://openalex.org/W2962732076', 'https://openalex.org/W2405884322', 'https://openalex.org/W1514535095', 'https://openalex.org/W2108598243', 'https://openalex.org/W2347145335', 'https://openalex.org/W2181691731', 'https://openalex.org/W1931639407', 'https://openalex.org/W2016850359', 'https://openalex.org/W2014012135', 'https://openalex.org/W2963330681', 'https://openalex.org/W1778492285', 'https://openalex.org/W2481240925', 'https://openalex.org/W2250742840', 'https://openalex.org/W1522301498', 'https://openalex.org/W1540618313', 'https://openalex.org/W1979459060', 'https://openalex.org/W1525264136', 'https://openalex.org/W1992613273', 'https://openalex.org/W2508907749', 'https://openalex.org/W2025482506', 'https://openalex.org/W68733909', 'https://openalex.org/W2126203737', 'https://openalex.org/W2951798058', 'https://openalex.org/W4297826211', 'https://openalex.org/W2220867547', 'https://openalex.org/W2052697931', 'https://openalex.org/W1905882502', 'https://openalex.org/W2346964103', 'https://openalex.org/W2586148577', 'https://openalex.org/W2586850765', 'https://openalex.org/W2101346879', 'https://openalex.org/W4386506836', 'https://openalex.org/W2148154194', 'https://openalex.org/W2137010615', 'https://openalex.org/W2295297373', 'https://openalex.org/W2004413798', 'https://openalex.org/W2251970440', 'https://openalex.org/W1895989618', 'https://openalex.org/W1861492603', 'https://openalex.org/W2148986421', 'https://openalex.org/W2171019095', 'https://openalex.org/W2758849341', 'https://openalex.org/W170967611', 'https://openalex.org/W1984076147', 'https://openalex.org/W2282219577', 'https://openalex.org/W1719717946', 'https://openalex.org/W2466445867', 'https://openalex.org/W4237938692', 'https://openalex.org/W2964121744', 'https://openalex.org/W2962862718', 'https://openalex.org/W2137735870', 'https://openalex.org/W2796315435', 'https://openalex.org/W2605131327', 'https://openalex.org/W2118440833', 'https://openalex.org/W2107917162', 'https://openalex.org/W2963499246', 'https://openalex.org/W2405666970', 'https://openalex.org/W2100768664', 'https://openalex.org/W2964001192', 'https://openalex.org/W2154093685', 'https://openalex.org/W2962753610', 'https://openalex.org/W2185175083', 'https://openalex.org/W137009889', 'https://openalex.org/W2796156786', 'https://openalex.org/W2158329300', 'https://openalex.org/W2787779284', 'https://openalex.org/W2536305071', 'https://openalex.org/W21006490', 'https://openalex.org/W2601713192', 'https://openalex.org/W2415378728', 'https://openalex.org/W1858383477', 'https://openalex.org/W1969616664', 'https://openalex.org/W4295803813', 'https://openalex.org/W2170682101', 'https://openalex.org/W1686810756', 'https://openalex.org/W2466918907', 'https://openalex.org/W1947481528']",2017-10-05
https://openalex.org/W2242818861,https://doi.org/10.48550/arxiv.1308.3432,Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation,"Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we ""back-propagate"" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of {\em conditional computation}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.","['https://openalex.org/W1498436455', 'https://openalex.org/W1904365287', 'https://openalex.org/W2951163624', 'https://openalex.org/W2913932916', 'https://openalex.org/W2124289529', 'https://openalex.org/W3037950864', 'https://openalex.org/W2025768430', 'https://openalex.org/W2156718681', 'https://openalex.org/W2099257174', 'https://openalex.org/W60493759', 'https://openalex.org/W2041176801']",2013-08-15
https://openalex.org/W2547039119,,Development of Indonesian Large Vocabulary Continuous Speech Recognition System within A-STAR Project.,"The paper outlines the development of a large vocabulary continuous speech recognition (LVCSR) system for the Indonesian language within the Asian speech translation (A-STAR) project. An overview of the A-STAR project and Indonesian language characteristics will be briefly described. We then focus on a discussion of the development of Indonesian LVCSR, including data resources issues, acoustic modeling, language modeling, the lexicon, and accuracy of recognition. There are three types of Indonesian data resources: daily news, telephone application, and BTEC tasks, which are used in this project. They are available in both text and speech forms. The Indonesian speech recognition engine was trained using the clean speech of both daily news and telephone application tasks. The optimum performance achieved on the BTEC task was 92.47 % word accuracy.","['https://openalex.org/W22876683', 'https://openalex.org/W1584533718', 'https://openalex.org/W1502737068', 'https://openalex.org/W206967138']",2008-01-01
https://openalex.org/W2399576818,https://doi.org/10.21437/interspeech.2015-642,Parallel inference of dirichlet process Gaussian mixture models for unsupervised acoustic modeling: a feasibility study,"We adopt a Dirichlet process Gaussian mixture model (DPGMM) for unsupervised acoustic modeling and represent speech frames with Gaussian posteriorgrams. The model performs unsupervised clustering on untranscribed data, and each Gaussian component can be considered as a cluster of sounds from various speakers. The model infers its model complexity (i.e. the number of Gaussian components) from the data. For computation efficiency, we use a parallel sampler for the model inference. Our experiments are conducted on the corpus provided by the zero resource speech challenge. Experimental results show that the unsupervised DPGMM posteriorgrams obviously outperformMFCC, and perform comparably to the posteriorgrams derived from language-mismatched phoneme recognizers in terms of the error rate of ABX discrimination test. The error rates can be further reduced by the fusion of these two kinds of posteriorgrams.","['https://openalex.org/W2406349064', 'https://openalex.org/W2119187236', 'https://openalex.org/W2106284094', 'https://openalex.org/W2114347655', 'https://openalex.org/W2049142189', 'https://openalex.org/W2125534887', 'https://openalex.org/W2127498532', 'https://openalex.org/W2151967501', 'https://openalex.org/W66167291', 'https://openalex.org/W2125247927', 'https://openalex.org/W2395899413', 'https://openalex.org/W2079460648', 'https://openalex.org/W1967924372', 'https://openalex.org/W2020607164', 'https://openalex.org/W2213520355', 'https://openalex.org/W2399363370', 'https://openalex.org/W2126203737', 'https://openalex.org/W2406820985', 'https://openalex.org/W2117041980', 'https://openalex.org/W2100768664', 'https://openalex.org/W1997505733', 'https://openalex.org/W2170659185', 'https://openalex.org/W2078769636', 'https://openalex.org/W2052697931', 'https://openalex.org/W2171019095', 'https://openalex.org/W2065136108', 'https://openalex.org/W1503398984', 'https://openalex.org/W2168319451', 'https://openalex.org/W2044138293', 'https://openalex.org/W2080972498', 'https://openalex.org/W1975728937', 'https://openalex.org/W2162021827', 'https://openalex.org/W1833498382', 'https://openalex.org/W1545920196', 'https://openalex.org/W2154085905', 'https://openalex.org/W2128032727', 'https://openalex.org/W2110589736']",2015-09-06
https://openalex.org/W2666408839,https://doi.org/10.1109/icassp.2017.7953090,Generative adversarial network-based postfilter for statistical parametric speech synthesis,"We propose a postfilter based on a generative adversarial network (GAN) to compensate for the differences between natural speech and speech synthesized by statistical parametric speech synthesis. In particular, we focus on the differences caused by over-smoothing, which makes the sounds muffled. Over-smoothing occurs in the time and frequency directions and is highly correlated in both directions, and conventional methods based on heuristics are too limited to cover all the factors (e.g., global variance was designed only to recover the dynamic range). To solve this problem, we focus on ""spectral texture"", i.e., the details of the time-frequency representation, and propose a learning-based postfilter that captures the structures directly from the data. To estimate the true distribution, we utilize a GAN composed of a generator and a discriminator. This optimizes the generator to produce samples imitating the dataset according to the adversarial discriminator. This adversarial process encourages the generator to fit the true data distribution, i.e., to generate realistic spectral texture. Objective evaluation of experimental results shows that the GAN-based postfilter can compensate for detailed spectral structures including modulation spectrum, and subjective evaluation shows that its generated speech is comparable to natural speech.","['https://openalex.org/W6696843773', 'https://openalex.org/W6676044216', 'https://openalex.org/W2000513720', 'https://openalex.org/W2396043161', 'https://openalex.org/W1987992317', 'https://openalex.org/W6638023308', 'https://openalex.org/W2049036695', 'https://openalex.org/W1502723613', 'https://openalex.org/W6621378261', 'https://openalex.org/W6685352114', 'https://openalex.org/W6631190155', 'https://openalex.org/W6639125025', 'https://openalex.org/W6640185926', 'https://openalex.org/W2150658333', 'https://openalex.org/W6602768981', 'https://openalex.org/W6674887261', 'https://openalex.org/W2039800941', 'https://openalex.org/W1927394876', 'https://openalex.org/W2129142580', 'https://openalex.org/W6675380101', 'https://openalex.org/W4251158933', 'https://openalex.org/W7075637324', 'https://openalex.org/W6687506355', 'https://openalex.org/W2194775991', 'https://openalex.org/W6678815747', 'https://openalex.org/W2049686551', 'https://openalex.org/W1903029394', 'https://openalex.org/W6637242042', 'https://openalex.org/W4320013936', 'https://openalex.org/W2108674328', 'https://openalex.org/W1921523184', 'https://openalex.org/W1600722501', 'https://openalex.org/W2963857374', 'https://openalex.org/W1778816975', 'https://openalex.org/W2125389028', 'https://openalex.org/W2963684088', 'https://openalex.org/W1522301498', 'https://openalex.org/W2964167449', 'https://openalex.org/W648143168', 'https://openalex.org/W2294797155', 'https://openalex.org/W2964121744', 'https://openalex.org/W2102003408', 'https://openalex.org/W2951523806', 'https://openalex.org/W2099471712', 'https://openalex.org/W68089216', 'https://openalex.org/W2099057450', 'https://openalex.org/W1861150963', 'https://openalex.org/W1665214252']",2017-03-01
https://openalex.org/W2964069186,https://doi.org/10.1109/icassp.2018.8462342,High-Quality Nonparallel Voice Conversion Based on Cycle-Consistent Adversarial Network,"Although voice conversion (VC) algorithms have achieved remarkable success along with the development of machine learning, superior performance is still difficult to achieve when using nonparallel data. In this paper, we propose using a cycle-consistent adversarial network (CycleGAN) for nonparallel data-based VC training. A CycleGAN is a generative adversarial network (GAN) originally developed for unpaired image-to-image translation. A subjective evaluation of inter-gender conversion demonstrated that the proposed method significantly outperformed a method based on the Merlin open source neural network speech synthesis system (a parallel VC system adapted for our setup) and a GAN-based parallel VC system. This is the first research to show that the performance of a nonparallel VC method can exceed that of state-of-the-art parallel VC methods.","['https://openalex.org/W4251158933', 'https://openalex.org/W6682918086', 'https://openalex.org/W2114543868', 'https://openalex.org/W6736210646', 'https://openalex.org/W2120605154', 'https://openalex.org/W2157412983', 'https://openalex.org/W6630475647', 'https://openalex.org/W2079735306', 'https://openalex.org/W2747744257', 'https://openalex.org/W6601306257', 'https://openalex.org/W2161476805', 'https://openalex.org/W2142300631', 'https://openalex.org/W6735509194', 'https://openalex.org/W2576309025', 'https://openalex.org/W2090830255', 'https://openalex.org/W2471520273', 'https://openalex.org/W6639486974', 'https://openalex.org/W2013996527', 'https://openalex.org/W2077865492', 'https://openalex.org/W4245885054', 'https://openalex.org/W2156142001', 'https://openalex.org/W2118850452', 'https://openalex.org/W6658769476', 'https://openalex.org/W2475998840', 'https://openalex.org/W2067175291', 'https://openalex.org/W6741832134', 'https://openalex.org/W2962896155', 'https://openalex.org/W6631309588', 'https://openalex.org/W2593414223', 'https://openalex.org/W2032130465', 'https://openalex.org/W33829787', 'https://openalex.org/W4320013936', 'https://openalex.org/W2962793481', 'https://openalex.org/W2598638573', 'https://openalex.org/W1523372075', 'https://openalex.org/W2739748921', 'https://openalex.org/W2154920538', 'https://openalex.org/W1892640180', 'https://openalex.org/W1509691205']",2018-04-01
https://openalex.org/W2962699523,https://doi.org/10.1109/asru.2017.8268950,Listening while speaking: Speech chain by deep learning,"Despite the close relationship between speech perception and production, research in automatic speech recognition (ASR) and text-to-speech synthesis (TTS) has progressed more or less independently without exerting much mutual influence on each other. In human communication, on the other hand, a closed-loop speech chain mechanism with auditory feedback from the speaker's mouth to her ear is crucial. In this paper, we take a step further and develop a closed-loop speech chain model based on deep learning. The sequence-to-sequence model in close-loop architecture allows us to train our model on the concatenation of both labeled and unlabeled data. While ASR transcribes the unlabeled speech features, TTS attempts to reconstruct the original speech waveform based on the text from ASR. In the opposite direction, ASR also attempts to reconstruct the original text transcription given the synthesized speech. To the best of our knowledge, this is the first deep learning model that integrates human speech perception and production behaviors. Our experimental results show that the proposed approach significantly improved the performance more than separate systems that were only trained with labeled data.","['https://openalex.org/W6696126599', 'https://openalex.org/W6608432165', 'https://openalex.org/W2042196737', 'https://openalex.org/W1608375737', 'https://openalex.org/W2150658333', 'https://openalex.org/W1600722501', 'https://openalex.org/W1935012542', 'https://openalex.org/W2143612262', 'https://openalex.org/W2168249605', 'https://openalex.org/W6712560600', 'https://openalex.org/W6675380101', 'https://openalex.org/W2128160875', 'https://openalex.org/W6670693114', 'https://openalex.org/W2101346879', 'https://openalex.org/W2120847449', 'https://openalex.org/W1990005915', 'https://openalex.org/W1861172732', 'https://openalex.org/W2133129980', 'https://openalex.org/W2063223471', 'https://openalex.org/W2115040572', 'https://openalex.org/W6679436768', 'https://openalex.org/W2327501763', 'https://openalex.org/W6630875275', 'https://openalex.org/W2546938941', 'https://openalex.org/W2102003408', 'https://openalex.org/W1514535095', 'https://openalex.org/W2130942839', 'https://openalex.org/W2949382160', 'https://openalex.org/W2398826216', 'https://openalex.org/W2963357083', 'https://openalex.org/W1921523184', 'https://openalex.org/W2157331557', 'https://openalex.org/W1810943226', 'https://openalex.org/W2078993594', 'https://openalex.org/W3210141620', 'https://openalex.org/W206967138', 'https://openalex.org/W2604184139', 'https://openalex.org/W2422843715', 'https://openalex.org/W2964308564', 'https://openalex.org/W1586532344', 'https://openalex.org/W2519091744', 'https://openalex.org/W2591927543', 'https://openalex.org/W2133564696', 'https://openalex.org/W1902237438']",2017-12-01
https://openalex.org/W2962896155,https://doi.org/10.21437/interspeech.2017-63,Voice Conversion from Unaligned Corpora Using Variational Autoencoding Wasserstein Generative Adversarial Networks,"Building a voice conversion (VC) system from non-parallel speech corpora is challenging but highly valuable in real application scenarios.In most situations, the source and the target speakers do not repeat the same texts or they may even speak different languages.In this case, one possible, although indirect, solution is to build a generative model for speech.Generative models focus on explaining the observations with latent variables instead of learning a pairwise transformation function, thereby bypassing the requirement of speech frame alignment.In this paper, we propose a non-parallel VC framework with a variational autoencoding Wasserstein generative adversarial network (VAW-GAN) that explicitly considers a VC objective when building the speech model.Experimental results corroborate the capability of our framework for building a VC system from unaligned data, and demonstrate improved conversion quality.","['https://openalex.org/W2574092538', 'https://openalex.org/W1520370180', 'https://openalex.org/W2086796102', 'https://openalex.org/W2512087624', 'https://openalex.org/W2515020857', 'https://openalex.org/W2407281753', 'https://openalex.org/W2518312472', 'https://openalex.org/W4394670483', 'https://openalex.org/W2583102335', 'https://openalex.org/W2290946177', 'https://openalex.org/W2161476805', 'https://openalex.org/W2964167449', 'https://openalex.org/W4298289240', 'https://openalex.org/W2120605154', 'https://openalex.org/W1959608418', 'https://openalex.org/W2290463584', 'https://openalex.org/W2067175291', 'https://openalex.org/W2049686551', 'https://openalex.org/W2532494225', 'https://openalex.org/W2473388484', 'https://openalex.org/W4235670393', 'https://openalex.org/W2515028311', 'https://openalex.org/W2169579015']",2017-08-16
https://openalex.org/W2120847449,https://doi.org/10.1109/tassp.1984.1164317,Signal estimation from modified short-time Fourier transform,"In this paper, we present an algorithm to estimate a signal from its modified short-time Fourier transform (STFT). This algorithm is computationally simple and is obtained by minimizing the mean squared error between the STFT of the estimated signal and the modified STFT. Using this algorithm, we also develop an iterative algorithm to estimate a signal from its modified STFT magnitude. The iterative algorithm is shown to decrease, in each iteration, the mean squared error between the STFT magnitude of the estimated signal and the modified STFT magnitude. The major computation involved in the iterative algorithm is the discrete Fourier transform (DFT) computation, and the algorithm appears to be real-time implementable with current hardware technology. The algorithm developed in this paper has been applied to the time-scale modification of speech. The resulting system generates very high-quality speech, and appears to be better in performance than any existing method.","['https://openalex.org/W2138196899', 'https://openalex.org/W6628955290', 'https://openalex.org/W2051049794', 'https://openalex.org/W2080145918', 'https://openalex.org/W2133541109', 'https://openalex.org/W2168274102', 'https://openalex.org/W2020997493', 'https://openalex.org/W1968939597', 'https://openalex.org/W1989344925', 'https://openalex.org/W1973119332', 'https://openalex.org/W2069501481', 'https://openalex.org/W2071609006', 'https://openalex.org/W1484412996', 'https://openalex.org/W3094114204']",1984-04-01
https://openalex.org/W2787447541,https://doi.org/10.1109/asru.2017.8269011,Feature optimized DPGMM clustering for unsupervised subword modeling: A contribution to zerospeech 2017,"This paper describes our unsupervised subword modeling pipeline for the zero resource speech challenge (ZeroSpeech) 2017. Our approach is built around the Dirichlet process Gaussian mixture model (DPGMM) that we use to cluster speech feature vectors into a dynamically sized set of classes. By considering each class an acoustic unit, speech can be represented as sequence of class posteriorgrams. We enhance this method by automatically optimizing the DPGMM sampler's input features in a multi-stage clustering framework, where we unsupervisedly learn transformations using LDA, MLLT and (basis) fMLLR to reduce variance in the features. We show that this optimization considerably boosts the subword modeling quality, according to the performance on the ABX phone discriminability task. For the first time, we apply inferred subword models to previously unseen data from a new set of speakers. We demonstrate our method's good generalization and the effectiveness of its blind speaker adaptation in extensive experiments on a multitude of datasets. Our pipeline has very little need for hyper-parameter adjustment and is entirely unsupervised, i.e., it only takes raw audio recordings as input, without requiring any pre-defined segmentation, explicit speaker IDs or other meta data.","['https://openalex.org/W6678947187', 'https://openalex.org/W2001619934', 'https://openalex.org/W2124629003', 'https://openalex.org/W2106554350', 'https://openalex.org/W1599512239', 'https://openalex.org/W2002342963', 'https://openalex.org/W2019042707', 'https://openalex.org/W2078769636', 'https://openalex.org/W6712444837', 'https://openalex.org/W6712202099', 'https://openalex.org/W6638159135', 'https://openalex.org/W6712553779', 'https://openalex.org/W6713256719', 'https://openalex.org/W2509930204', 'https://openalex.org/W6704305767', 'https://openalex.org/W2963620343', 'https://openalex.org/W6973666849', 'https://openalex.org/W2586754519', 'https://openalex.org/W6631362777', 'https://openalex.org/W2113641473', 'https://openalex.org/W1631260214', 'https://openalex.org/W2395899413', 'https://openalex.org/W2396043527', 'https://openalex.org/W2404799143', 'https://openalex.org/W2128032727', 'https://openalex.org/W1796128977', 'https://openalex.org/W2786608204', 'https://openalex.org/W1524333225', 'https://openalex.org/W2399576818', 'https://openalex.org/W2345811097']",2017-12-01
https://openalex.org/W2963796886,https://doi.org/10.21437/interspeech.2018-1558,Machine Speech Chain with One-shot Speaker Adaptation,"In previous work, we developed a closed-loop speech chain model based on deep learning, in which the architecture enabled the automatic speech recognition (ASR) and text-to-speech synthesis (TTS) components to mutually improve their performance.This was accomplished by the two parts teaching each other using both labeled and unlabeled data.This approach could significantly improve model performance within a single-speaker speech dataset, but only a slight increase could be gained in multi-speaker tasks.Furthermore, the model is still unable to handle unseen speakers.In this paper, we present a new speech chain mechanism by integrating a speaker recognition model inside the loop.We also propose extending the capability of TTS to handle unseen speakers by implementing one-shot speaker adaptation.This enables TTS to mimic voice characteristics from one speaker to another with only a one-shot speaker sample, even from a text without any speaker information.In the speech chain loop mechanism, ASR also benefits from the ability to further learn an arbitrary speakers characteristics from the generated speech waveform, resulting in a significant improvement in the recognition rate.","['https://openalex.org/W2612434969', 'https://openalex.org/W2150769028', 'https://openalex.org/W2962699523', 'https://openalex.org/W2526425061', 'https://openalex.org/W2024490156', 'https://openalex.org/W2094147890', 'https://openalex.org/W2899771611', 'https://openalex.org/W2133564696', 'https://openalex.org/W950853366', 'https://openalex.org/W1630959083', 'https://openalex.org/W2962759037', 'https://openalex.org/W1924770834', 'https://openalex.org/W2331128040', 'https://openalex.org/W2130942839', 'https://openalex.org/W2327501763', 'https://openalex.org/W2519091744', 'https://openalex.org/W2964167449', 'https://openalex.org/W4235154690', 'https://openalex.org/W1524333225', 'https://openalex.org/W1921523184', 'https://openalex.org/W2604184139', 'https://openalex.org/W2120605154']",2018-08-28
https://openalex.org/W2911340057,https://doi.org/10.1109/slt.2018.8639507,Adaptive Wavenet Vocoder for Residual Compensation in GAN-Based Voice Conversion,"In this paper, we propose to use generative adversarial networks (GAN) together with a WaveNet vocoder to address the over-smoothing problem arising from the deep learning approaches to voice conversion, and to improve the vocoding quality over the traditional vocoders. As GAN aims to minimize the divergence between the natural and converted speech parameters, it effectively alleviates the over-smoothing problem in the converted speech. On the other hand, WaveNet vocoder allows us to leverage from the human speech of a large speaker population, thus improving the naturalness of the synthetic voice. Furthermore, for the first time, we study how to use WaveNet vocoder for residual compensation to improve the voice conversion performance. The experiments show that the proposed voice conversion framework consistently outperforms the baselines.","['https://openalex.org/W2889064624', 'https://openalex.org/W2964243274', 'https://openalex.org/W2746474733', 'https://openalex.org/W2963539064', 'https://openalex.org/W2749371885', 'https://openalex.org/W2964024144', 'https://openalex.org/W6736494419', 'https://openalex.org/W6745117592', 'https://openalex.org/W79241043', 'https://openalex.org/W1965912016', 'https://openalex.org/W2107860279', 'https://openalex.org/W6748381320', 'https://openalex.org/W2086796102', 'https://openalex.org/W6711854987', 'https://openalex.org/W1509691205', 'https://openalex.org/W2518172956', 'https://openalex.org/W2532494225', 'https://openalex.org/W6737196085', 'https://openalex.org/W2293049663', 'https://openalex.org/W1517202054', 'https://openalex.org/W2105160541', 'https://openalex.org/W2786868129', 'https://openalex.org/W2120605154', 'https://openalex.org/W2017425464', 'https://openalex.org/W2749651610', 'https://openalex.org/W2005438552', 'https://openalex.org/W6722217012', 'https://openalex.org/W1977362459', 'https://openalex.org/W2151262064', 'https://openalex.org/W2402356521', 'https://openalex.org/W2118850452', 'https://openalex.org/W6714093102', 'https://openalex.org/W6746801104', 'https://openalex.org/W2747744257', 'https://openalex.org/W2785978752', 'https://openalex.org/W2746457594', 'https://openalex.org/W2269051851', 'https://openalex.org/W2963341071', 'https://openalex.org/W6744040789', 'https://openalex.org/W2666408839', 'https://openalex.org/W2963618559', 'https://openalex.org/W2962896155', 'https://openalex.org/W2962850167', 'https://openalex.org/W2396025094', 'https://openalex.org/W2963971656', 'https://openalex.org/W2608338293', 'https://openalex.org/W2949382160', 'https://openalex.org/W2907162034', 'https://openalex.org/W2484196375', 'https://openalex.org/W2758785877', 'https://openalex.org/W2519091744', 'https://openalex.org/W2774848319', 'https://openalex.org/W2785608393', 'https://openalex.org/W2407039802', 'https://openalex.org/W3142619648', 'https://openalex.org/W2605762339']",2018-12-01
https://openalex.org/W1836465849,https://doi.org/10.48550/arxiv.1502.03167,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,"Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.","['https://openalex.org/W1563686443', 'https://openalex.org/W2084894614', 'https://openalex.org/W1915968771', 'https://openalex.org/W2034368206', 'https://openalex.org/W2146502635', 'https://openalex.org/W2914484425', 'https://openalex.org/W2123649031', 'https://openalex.org/W2127230474', 'https://openalex.org/W104184427', 'https://openalex.org/W1762484328', 'https://openalex.org/W1815076433', 'https://openalex.org/W1814328102', 'https://openalex.org/W2952020226', 'https://openalex.org/W2152424459', 'https://openalex.org/W2963504252', 'https://openalex.org/W1533861849', 'https://openalex.org/W2950179405', 'https://openalex.org/W2168231600', 'https://openalex.org/W1665214252', 'https://openalex.org/W2095705004', 'https://openalex.org/W1677182931', 'https://openalex.org/W2112796928', 'https://openalex.org/W2134583763']",2015-02-11
https://openalex.org/W2191779130,https://doi.org/10.25080/majora-7b98e3ed-003,librosa: Audio and Music Signal Analysis in Python,"This document describes version 0.4.0 of librosa: a Python package for audio and music signal processing. At a high level, librosa provides implementations of a variety of common functions used throughout the field of music information retrieval. In this document, a brief overview of the library's functionality is provided, along with explanations of the design goals, software development practices, and notational conventions.","['https://openalex.org/W6675354045', 'https://openalex.org/W6691839725', 'https://openalex.org/W2146292423', 'https://openalex.org/W1916910924', 'https://openalex.org/W2103869314', 'https://openalex.org/W6603616073', 'https://openalex.org/W1975929202', 'https://openalex.org/W2125324924', 'https://openalex.org/W129413713', 'https://openalex.org/W2915624731', 'https://openalex.org/W2397818963', 'https://openalex.org/W78743328', 'https://openalex.org/W1902027874', 'https://openalex.org/W1561135842', 'https://openalex.org/W2016885049', 'https://openalex.org/W6697202309', 'https://openalex.org/W2385545', 'https://openalex.org/W2285540944', 'https://openalex.org/W2053347927', 'https://openalex.org/W2294459443', 'https://openalex.org/W2052872069', 'https://openalex.org/W2101234009', 'https://openalex.org/W4232336823', 'https://openalex.org/W4245436919', 'https://openalex.org/W2254715784', 'https://openalex.org/W2407685581', 'https://openalex.org/W3005347330', 'https://openalex.org/W2011301426', 'https://openalex.org/W2016381774']",2015-01-01
https://openalex.org/W4388017359,https://doi.org/10.1109/taslp.2023.3328283,End-to-End Speech Recognition: A Survey,"In the last decade of automatic speech recognition (ASR) research, the introduction of deep learning has brought considerable reductions in word error rate of more than 50% relative, compared to modeling without deep learning. In the wake of this transition, a number of all-neural ASR architectures have been introduced. These so-called <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">end-to-end</i> (E2E) models provide highly integrated, completely neural ASR models, which rely strongly on general machine learning knowledge, learn more consistently from data, with lower dependence on ASR domain-specific experience. The success and enthusiastic adoption of deep learning, accompanied by more generic model architectures has led to E2E models now becoming the prominent ASR approach. The goal of this survey is to provide a taxonomy of E2E ASR models and corresponding improvements, and to discuss their properties and their relationship to classical hidden Markov model (HMM) based ASR architectures. All relevant aspects of E2E ASR are covered in this work: modeling, training, decoding, and external language model integration, discussions of performance and deployment opportunities, as well as an outlook into potential future developments.","['https://openalex.org/W6715097780', 'https://openalex.org/W2125838338', 'https://openalex.org/W2394932179', 'https://openalex.org/W98857008', 'https://openalex.org/W2165712214', 'https://openalex.org/W1588735863', 'https://openalex.org/W6680532216', 'https://openalex.org/W2056590938', 'https://openalex.org/W2408093180', 'https://openalex.org/W2288217446', 'https://openalex.org/W2127141656', 'https://openalex.org/W6638749077', 'https://openalex.org/W6623517193', 'https://openalex.org/W2327501763', 'https://openalex.org/W2143564602', 'https://openalex.org/W6683738474', 'https://openalex.org/W6675365184', 'https://openalex.org/W2889129739', 'https://openalex.org/W2962760690', 'https://openalex.org/W2129545859', 'https://openalex.org/W2100180150', 'https://openalex.org/W6780218876', 'https://openalex.org/W3008762051', 'https://openalex.org/W2962699523', 'https://openalex.org/W2972889948', 'https://openalex.org/W2545177271', 'https://openalex.org/W2962784628', 'https://openalex.org/W6728811460', 'https://openalex.org/W6734588641', 'https://openalex.org/W2899879954', 'https://openalex.org/W3198455051', 'https://openalex.org/W2121879602', 'https://openalex.org/W2046932483', 'https://openalex.org/W2889187401', 'https://openalex.org/W3197991202', 'https://openalex.org/W2750499125', 'https://openalex.org/W2064675550', 'https://openalex.org/W2105482032', 'https://openalex.org/W4385245566', 'https://openalex.org/W3097777922', 'https://openalex.org/W2746192915', 'https://openalex.org/W3016234571', 'https://openalex.org/W2143612262', 'https://openalex.org/W3211040052', 'https://openalex.org/W6690026940', 'https://openalex.org/W3008174054', 'https://openalex.org/W3028545098', 'https://openalex.org/W2962826786', 'https://openalex.org/W6727690538', 'https://openalex.org/W2889504751', 'https://openalex.org/W2915977493', 'https://openalex.org/W2745439869', 'https://openalex.org/W3008898571', 'https://openalex.org/W6685711979', 'https://openalex.org/W6735706088', 'https://openalex.org/W6747158283', 'https://openalex.org/W2952992734', 'https://openalex.org/W2962742956', 'https://openalex.org/W2936123380', 'https://openalex.org/W2972995428', 'https://openalex.org/W6793472422', 'https://openalex.org/W4319862683', 'https://openalex.org/W3015671919', 'https://openalex.org/W2514741789', 'https://openalex.org/W6727336983', 'https://openalex.org/W66978610', 'https://openalex.org/W2748816379', 'https://openalex.org/W4319862408', 'https://openalex.org/W2963211739', 'https://openalex.org/W6640090968', 'https://openalex.org/W2608712415', 'https://openalex.org/W2577366047', 'https://openalex.org/W2530876040', 'https://openalex.org/W2963303028', 'https://openalex.org/W2964012862', 'https://openalex.org/W6754576867', 'https://openalex.org/W2889163603', 'https://openalex.org/W3008525923', 'https://openalex.org/W2131968858', 'https://openalex.org/W2606722458', 'https://openalex.org/W2766219058', 'https://openalex.org/W2973122799', 'https://openalex.org/W3011339933', 'https://openalex.org/W3163203022', 'https://openalex.org/W6784400248', 'https://openalex.org/W6784800133', 'https://openalex.org/W2972625221', 'https://openalex.org/W2886319145', 'https://openalex.org/W2886025712', 'https://openalex.org/W2937402758', 'https://openalex.org/W2889374926', 'https://openalex.org/W3095173472', 'https://openalex.org/W2892009249', 'https://openalex.org/W3016010032', 'https://openalex.org/W6839026989', 'https://openalex.org/W2972818416', 'https://openalex.org/W3163793923', 'https://openalex.org/W3197976839', 'https://openalex.org/W3015686596', 'https://openalex.org/W3160551958', 'https://openalex.org/W3161375121', 'https://openalex.org/W3202419788', 'https://openalex.org/W4319862418', 'https://openalex.org/W6853611000', 'https://openalex.org/W6853998256', 'https://openalex.org/W3008284571', 'https://openalex.org/W3015927303', 'https://openalex.org/W3015383801', 'https://openalex.org/W3198116002', 'https://openalex.org/W1806891645', 'https://openalex.org/W6910546390', 'https://openalex.org/W2105594594', 'https://openalex.org/W6679128827', 'https://openalex.org/W1991133427', 'https://openalex.org/W2155368638', 'https://openalex.org/W4224518768', 'https://openalex.org/W6796656850', 'https://openalex.org/W3202184514', 'https://openalex.org/W2024539680', 'https://openalex.org/W3198439131', 'https://openalex.org/W3206876927', 'https://openalex.org/W4372259859', 'https://openalex.org/W2033565080', 'https://openalex.org/W2157749010', 'https://openalex.org/W1979136262', 'https://openalex.org/W2114016253', 'https://openalex.org/W2001679125', 'https://openalex.org/W2953561564', 'https://openalex.org/W2963747784', 'https://openalex.org/W3095697114', 'https://openalex.org/W2900209846', 'https://openalex.org/W2963144852', 'https://openalex.org/W2892124901', 'https://openalex.org/W3095376166', 'https://openalex.org/W2136922672', 'https://openalex.org/W2110798204', 'https://openalex.org/W2471933213', 'https://openalex.org/W2799923439', 'https://openalex.org/W2799800213', 'https://openalex.org/W6757856092', 'https://openalex.org/W2963571336', 'https://openalex.org/W2951974815', 'https://openalex.org/W1501286448', 'https://openalex.org/W1975550806', 'https://openalex.org/W6752630080', 'https://openalex.org/W2963431393', 'https://openalex.org/W3096215352', 'https://openalex.org/W3096032230', 'https://openalex.org/W4210463634', 'https://openalex.org/W3204696009', 'https://openalex.org/W4226120743', 'https://openalex.org/W2033245860', 'https://openalex.org/W1587755118', 'https://openalex.org/W2000200144', 'https://openalex.org/W6757817989', 'https://openalex.org/W6745410505', 'https://openalex.org/W2963026768', 'https://openalex.org/W6863618527', 'https://openalex.org/W3163842339', 'https://openalex.org/W2151834591', 'https://openalex.org/W2296073425', 'https://openalex.org/W6687566353', 'https://openalex.org/W3097747488', 'https://openalex.org/W3017474798', 'https://openalex.org/W1988720110', 'https://openalex.org/W6604254268', 'https://openalex.org/W6631190155', 'https://openalex.org/W3198654230', 'https://openalex.org/W4206410067', 'https://openalex.org/W6681151457', 'https://openalex.org/W2145249131', 'https://openalex.org/W6692956712', 'https://openalex.org/W6640036494', 'https://openalex.org/W2618530766', 'https://openalex.org/W2331143823', 'https://openalex.org/W2972451902', 'https://openalex.org/W3162249256', 'https://openalex.org/W6600213771', 'https://openalex.org/W6714142977', 'https://openalex.org/W2183341477', 'https://openalex.org/W6621543089', 'https://openalex.org/W6749075489', 'https://openalex.org/W2057653135', 'https://openalex.org/W6749518548', 'https://openalex.org/W3163839574', 'https://openalex.org/W1989674786', 'https://openalex.org/W2407080277', 'https://openalex.org/W6675409298', 'https://openalex.org/W2937780860', 'https://openalex.org/W3015995734', 'https://openalex.org/W3095189764', 'https://openalex.org/W2883586237', 'https://openalex.org/W4225334634', 'https://openalex.org/W2964107261', 'https://openalex.org/W1583239513', 'https://openalex.org/W2050526637', 'https://openalex.org/W1986184096', 'https://openalex.org/W2516457973', 'https://openalex.org/W2808939837', 'https://openalex.org/W2127095586', 'https://openalex.org/W2914018192', 'https://openalex.org/W2888909726', 'https://openalex.org/W206545267', 'https://openalex.org/W4473315', 'https://openalex.org/W6793076252', 'https://openalex.org/W6785891320', 'https://openalex.org/W2944255943', 'https://openalex.org/W3026041220', 'https://openalex.org/W2936774411', 'https://openalex.org/W3015726069', 'https://openalex.org/W2291975472', 'https://openalex.org/W2971840980', 'https://openalex.org/W108866686', 'https://openalex.org/W3097882114', 'https://openalex.org/W6774835902', 'https://openalex.org/W3096160024', 'https://openalex.org/W3016167541', 'https://openalex.org/W3197140813', 'https://openalex.org/W3206573929', 'https://openalex.org/W3094713728', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963260202', 'https://openalex.org/W2886180730', 'https://openalex.org/W6854240803', 'https://openalex.org/W4299649720', 'https://openalex.org/W2151058131', 'https://openalex.org/W2008554732', 'https://openalex.org/W2066378046', 'https://openalex.org/W6765658108', 'https://openalex.org/W3015190365', 'https://openalex.org/W1710082047', 'https://openalex.org/W3015974384', 'https://openalex.org/W6770245836', 'https://openalex.org/W3097973766', 'https://openalex.org/W3197304116', 'https://openalex.org/W2972977747', 'https://openalex.org/W3148654612', 'https://openalex.org/W2963022149', 'https://openalex.org/W2627092829', 'https://openalex.org/W2963240019', 'https://openalex.org/W3163300396', 'https://openalex.org/W2972780808', 'https://openalex.org/W2787663903', 'https://openalex.org/W3008912312', 'https://openalex.org/W2972837679', 'https://openalex.org/W2938348542', 'https://openalex.org/W3197478142', 'https://openalex.org/W6797037654', 'https://openalex.org/W2972953886', 'https://openalex.org/W3005302685', 'https://openalex.org/W6713134421', 'https://openalex.org/W2933138175', 'https://openalex.org/W6760633627', 'https://openalex.org/W2962728618', 'https://openalex.org/W2972630480', 'https://openalex.org/W2949975180', 'https://openalex.org/W3015369343', 'https://openalex.org/W2091981305', 'https://openalex.org/W2972528057', 'https://openalex.org/W3016053754', 'https://openalex.org/W2964103964', 'https://openalex.org/W2963382396', 'https://openalex.org/W2970692082', 'https://openalex.org/W2078354939', 'https://openalex.org/W2136617108', 'https://openalex.org/W3015501067', 'https://openalex.org/W2972799770', 'https://openalex.org/W2972621414', 'https://openalex.org/W6752334204', 'https://openalex.org/W2097927681', 'https://openalex.org/W179875071', 'https://openalex.org/W2402268235', 'https://openalex.org/W2566563465', 'https://openalex.org/W6731370813', 'https://openalex.org/W6757424787', 'https://openalex.org/W2940180244', 'https://openalex.org/W2963088785', 'https://openalex.org/W2943845043', 'https://openalex.org/W2964110616', 'https://openalex.org/W2150355110', 'https://openalex.org/W6770251742', 'https://openalex.org/W6640059789', 'https://openalex.org/W2888779557', 'https://openalex.org/W2939111082', 'https://openalex.org/W3008037978', 'https://openalex.org/W3152221657', 'https://openalex.org/W3205201903', 'https://openalex.org/W6755207826', 'https://openalex.org/W3024308166', 'https://openalex.org/W2962745521', 'https://openalex.org/W3197507772', 'https://openalex.org/W1966812932', 'https://openalex.org/W2014151772', 'https://openalex.org/W2080213370', 'https://openalex.org/W1985258458', 'https://openalex.org/W2396464458', 'https://openalex.org/W2166637769', 'https://openalex.org/W1494198834', 'https://openalex.org/W3008191852', 'https://openalex.org/W3209059054', 'https://openalex.org/W6770506093', 'https://openalex.org/W3147414526', 'https://openalex.org/W2995181338', 'https://openalex.org/W2981857663', 'https://openalex.org/W4319862255', 'https://openalex.org/W2972692349', 'https://openalex.org/W2962824709', 'https://openalex.org/W3007528493', 'https://openalex.org/W3094667432', 'https://openalex.org/W3048407879', 'https://openalex.org/W3162665866', 'https://openalex.org/W3161873870', 'https://openalex.org/W3015194534', 'https://openalex.org/W3160766462', 'https://openalex.org/W3198442913', 'https://openalex.org/W6803092890', 'https://openalex.org/W6810259195', 'https://openalex.org/W4223622550', 'https://openalex.org/W3148001440', 'https://openalex.org/W3205644108', 'https://openalex.org/W4225319488', 'https://openalex.org/W4319862474', 'https://openalex.org/W2963739817', 'https://openalex.org/W6735168207', 'https://openalex.org/W3211278025', 'https://openalex.org/W4221155340', 'https://openalex.org/W2079656678', 'https://openalex.org/W2952230511', 'https://openalex.org/W3034775979', 'https://openalex.org/W4394662461', 'https://openalex.org/W2792376130', 'https://openalex.org/W4297781872', 'https://openalex.org/W1922655562', 'https://openalex.org/W3207222250', 'https://openalex.org/W3147187328', 'https://openalex.org/W3007328579', 'https://openalex.org/W3151269043', 'https://openalex.org/W1508165687', 'https://openalex.org/W1904365287', 'https://openalex.org/W3170405627', 'https://openalex.org/W4381827575', 'https://openalex.org/W4378501656', 'https://openalex.org/W4383605108', 'https://openalex.org/W3167895882', 'https://openalex.org/W2987019345', 'https://openalex.org/W3100910367', 'https://openalex.org/W2525778437', 'https://openalex.org/W1915251500', 'https://openalex.org/W2242818861', 'https://openalex.org/W2411921399', 'https://openalex.org/W2808640845', 'https://openalex.org/W2520160253', 'https://openalex.org/W3092122846', 'https://openalex.org/W3105532142', 'https://openalex.org/W2928941594', 'https://openalex.org/W4288290348', 'https://openalex.org/W1553004968', 'https://openalex.org/W2904818793', 'https://openalex.org/W3094957294', 'https://openalex.org/W3103005696']",2023-10-30
https://openalex.org/W4389524500,https://doi.org/10.18653/v1/2023.findings-emnlp.1055,SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities,"Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-modal content. With discrete speech representations, we construct SpeechInstruct, the first large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow cross-modal human instructions and highlight the potential of handling multiple modalities with one model. Code and models are available in https://github.com/0nutation/SpeechGPT. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.","['https://openalex.org/W4385807416', 'https://openalex.org/W4394671563', 'https://openalex.org/W3166396011', 'https://openalex.org/W4367061106', 'https://openalex.org/W4381786045', 'https://openalex.org/W4385570101', 'https://openalex.org/W4385572615', 'https://openalex.org/W4323572061', 'https://openalex.org/W4361866031', 'https://openalex.org/W3140429000', 'https://openalex.org/W4322718246', 'https://openalex.org/W3169320628', 'https://openalex.org/W4310924890', 'https://openalex.org/W4287120025', 'https://openalex.org/W2995181338', 'https://openalex.org/W4307680525', 'https://openalex.org/W4313679638', 'https://openalex.org/W3036601975', 'https://openalex.org/W4224308101', 'https://openalex.org/W3094502228', 'https://openalex.org/W1494198834', 'https://openalex.org/W3168867926', 'https://openalex.org/W4366330503', 'https://openalex.org/W4375958083', 'https://openalex.org/W4361229539', 'https://openalex.org/W3030437843', 'https://openalex.org/W4386384714', 'https://openalex.org/W4377297670', 'https://openalex.org/W4323651091', 'https://openalex.org/W4322718191']",2023-01-01
https://openalex.org/W3202278141,https://doi.org/10.1109/icassp43922.2022.9746395,Generalization Ability of MOS Prediction Networks,"Automatic methods to predict listener opinions of synthesized speech remain elusive since listeners, systems being evaluated, characteristics of the speech, and even the instructions given and the rating scale all vary from test to test. While automatic predictors for metrics such as mean opinion score (MOS) can achieve high prediction accuracy on samples from the same test, they typically fail to generalize well to new listening test contexts. In this paper, using a variety of networks for MOS prediction including MOSNet and self-supervised speech models such as wav2vec2, we investigate their performance on data from different listening tests in both zero-shot and fine-tuned settings. We find that wav2vec2 models fine-tuned for MOS prediction have good generalization capability to out-of-domain data even for the most challenging case of utterance-level predictions in the zero-shot setting, and that fine-tuning to in-domain data can improve predictions. We also observe that unseen systems are especially challenging for MOS prediction models.","['https://openalex.org/W3095410713', 'https://openalex.org/W6603931906', 'https://openalex.org/W2166637769', 'https://openalex.org/W4395699666', 'https://openalex.org/W4395699642', 'https://openalex.org/W4395958010', 'https://openalex.org/W4395958177', 'https://openalex.org/W2473388484', 'https://openalex.org/W2515028311', 'https://openalex.org/W2963035245', 'https://openalex.org/W3082130377', 'https://openalex.org/W3083776549', 'https://openalex.org/W2962780374', 'https://openalex.org/W2995181338', 'https://openalex.org/W3209059054', 'https://openalex.org/W1494198834', 'https://openalex.org/W6780218876', 'https://openalex.org/W3161558238', 'https://openalex.org/W6771467084', 'https://openalex.org/W3024752295', 'https://openalex.org/W6865107258', 'https://openalex.org/W3198270377', 'https://openalex.org/W2972394484', 'https://openalex.org/W3196225973', 'https://openalex.org/W3016160783', 'https://openalex.org/W2936802426', 'https://openalex.org/W3026777299', 'https://openalex.org/W2963522141', 'https://openalex.org/W6865530009', 'https://openalex.org/W3197580070', 'https://openalex.org/W6712208827', 'https://openalex.org/W2915960560', 'https://openalex.org/W3036601975', 'https://openalex.org/W2917688842', 'https://openalex.org/W3030437843', 'https://openalex.org/W2967606780', 'https://openalex.org/W3099782249', 'https://openalex.org/W2394921947', 'https://openalex.org/W97072897', 'https://openalex.org/W3169320628', 'https://openalex.org/W2917438849', 'https://openalex.org/W2796495654', 'https://openalex.org/W2917245127', 'https://openalex.org/W2119929864']",2022-04-27
https://openalex.org/W3095410713,https://doi.org/10.21437/interspeech.2020-2826,MLS: A Large-Scale Multilingual Dataset for Speech Research,"This paper introduces Multilingual LibriSpeech (MLS) dataset, a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages, including about 44.5K hours of English and a total of about 6K hours for other languages. Additionally, we provide Language Models (LM) and baseline Automatic Speech Recognition (ASR) models and for all the languages in our dataset. We believe such a large transcribed dataset will open new avenues in ASR and Text-To-Speech (TTS) research. The dataset will be made freely available for anyone at http://www.openslr.org.","['https://openalex.org/W2953190524', 'https://openalex.org/W4297818305', 'https://openalex.org/W2087347434', 'https://openalex.org/W2127141656', 'https://openalex.org/W4385245566', 'https://openalex.org/W2134800885', 'https://openalex.org/W2786234940', 'https://openalex.org/W2996159613', 'https://openalex.org/W3030437843', 'https://openalex.org/W3198270883', 'https://openalex.org/W2972630480', 'https://openalex.org/W2936774411', 'https://openalex.org/W3093502935', 'https://openalex.org/W1494198834', 'https://openalex.org/W3096104971', 'https://openalex.org/W2937197076', 'https://openalex.org/W2520160253', 'https://openalex.org/W2991213871', 'https://openalex.org/W2995181338', 'https://openalex.org/W2975381464', 'https://openalex.org/W3001899777', 'https://openalex.org/W2781384251', 'https://openalex.org/W2963979492', 'https://openalex.org/W2626778328', 'https://openalex.org/W2146502635', 'https://openalex.org/W2972359262', 'https://openalex.org/W2087064593']",2020-10-25
https://openalex.org/W2972359262,https://doi.org/10.21437/interspeech.2019-2441,LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech,"This paper introduces a new speech corpus called ""LibriTTS"" designed for text-to-speech use.It is derived from the original audio and text materials of the LibriSpeech corpus, which has been used for training and evaluating automatic speech recognition systems.The new corpus inherits desired properties of the LibriSpeech corpus while addressing a number of issues which make LibriSpeech less than ideal for text-to-speech work.The released corpus consists of 585 hours of speech data at 24kHz sampling rate from 2,456 speakers and the corresponding texts.Experimental results show that neural end-to-end TTS models trained from the LibriTTS corpus achieved above 4.0 in mean opinion scores in naturalness in five out of six evaluation speakers.","['https://openalex.org/W2892620417', 'https://openalex.org/W4232345992', 'https://openalex.org/W2794490148', 'https://openalex.org/W2182214061', 'https://openalex.org/W2808706139', 'https://openalex.org/W2129142580', 'https://openalex.org/W4298174729', 'https://openalex.org/W2105961775', 'https://openalex.org/W2103085228', 'https://openalex.org/W4294619240', 'https://openalex.org/W2963827314', 'https://openalex.org/W2604184139', 'https://openalex.org/W1597121597', 'https://openalex.org/W1522301498', 'https://openalex.org/W2964243274', 'https://openalex.org/W2788357188', 'https://openalex.org/W2736900972', 'https://openalex.org/W2901997113', 'https://openalex.org/W2885800352', 'https://openalex.org/W2033256038', 'https://openalex.org/W4289383906', 'https://openalex.org/W3177989406', 'https://openalex.org/W2747681982', 'https://openalex.org/W4298580827', 'https://openalex.org/W1574170747', 'https://openalex.org/W2892140764', 'https://openalex.org/W2293634267', 'https://openalex.org/W4298240696', 'https://openalex.org/W4295731579', 'https://openalex.org/W1494198834', 'https://openalex.org/W2889028433']",2019-09-13
https://openalex.org/W2998572311,https://doi.org/10.7488/ds/2645,CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92),"This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. The newspaper texts were taken from Herald Glasgow, with permission from Herald &amp; Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage. The details of the text selection algorithms are described in the following paper: C. Veaux, J. Yamagishi and S. King, ""The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,"" https://doi.org/10.1109/ICSDA.2013.6709856 The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf All speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed. This corpus was originally aimed for HMM-based text-to-speech synthesis systems, especially for speaker-adaptive HMM-based speech synthesis that uses average voice models trained on multiple speakers and speaker adaptation technologies. This corpus is also suitable for DNN-based multi-speaker text-to-speech synthesis systems and neural waveform modeling. The dataset was was referenced in the Google DeepMind work on WaveNet: https://arxiv.org/pdf/1609.03499.pdf . Please note while text files containing transcripts of the speech are provided for 109 of the 110 recordings, in the '/txt' folder, the 'p315' text was lost due to a hard disk error.",[],2019-01-01
https://openalex.org/W4319862255,https://doi.org/10.1109/slt54892.2023.10022656,E-Branchformer: Branchformer with Enhanced Merging for Speech Recognition,"Conformer, combining convolution and self-attention sequentially to capture both local and global information, has shown remarkable performance and is currently regarded as the state-of-the-art for automatic speech recognition (ASR). Several other studies have explored integrating convolution and self-attention but they have not managed to match Conformer's performance. The recently introduced Branchformer achieves comparable performance to Conformer by using dedicated branches of convolution and self-attention and merging local and global context from each branch. In this paper, we propose E-Branchformer, which enhances Branchformer by applying an effective merging method and stacking additional point-wise modules. E-Branchformer sets new state-of-the-art word error rates (WERs) 1.81% and 3.65% on LibriSpeech test-clean and test-other sets without using any external training data.","['https://openalex.org/W6800664022', 'https://openalex.org/W3100460087', 'https://openalex.org/W4226103796', 'https://openalex.org/W2327501763', 'https://openalex.org/W6638749077', 'https://openalex.org/W6773475747', 'https://openalex.org/W2127141656', 'https://openalex.org/W2407080277', 'https://openalex.org/W2936774411', 'https://openalex.org/W6770812743', 'https://openalex.org/W2963414781', 'https://openalex.org/W2799800213', 'https://openalex.org/W3007528493', 'https://openalex.org/W2973215447', 'https://openalex.org/W3015966793', 'https://openalex.org/W6776480786', 'https://openalex.org/W2892009249', 'https://openalex.org/W2972818416', 'https://openalex.org/W6772991762', 'https://openalex.org/W6783944145', 'https://openalex.org/W6779163297', 'https://openalex.org/W6789463137', 'https://openalex.org/W3085139254', 'https://openalex.org/W6800388583', 'https://openalex.org/W6810443081', 'https://openalex.org/W3207805159', 'https://openalex.org/W3097777922', 'https://openalex.org/W6839026989', 'https://openalex.org/W6640090968', 'https://openalex.org/W6751097180', 'https://openalex.org/W6758657797', 'https://openalex.org/W2963091558', 'https://openalex.org/W4214493665', 'https://openalex.org/W6796931752', 'https://openalex.org/W4315705623', 'https://openalex.org/W4312847199', 'https://openalex.org/W6774054309', 'https://openalex.org/W6757585730', 'https://openalex.org/W6781275321', 'https://openalex.org/W6771917389', 'https://openalex.org/W6755207826', 'https://openalex.org/W4226224676', 'https://openalex.org/W6838697126', 'https://openalex.org/W2302255633', 'https://openalex.org/W6780226713', 'https://openalex.org/W6674330103', 'https://openalex.org/W2963925437', 'https://openalex.org/W6755977528', 'https://openalex.org/W4226334005', 'https://openalex.org/W2752782242', 'https://openalex.org/W6746023985', 'https://openalex.org/W6770280828', 'https://openalex.org/W6629717138', 'https://openalex.org/W2962780374', 'https://openalex.org/W3152221657', 'https://openalex.org/W6784531339', 'https://openalex.org/W6770506093', 'https://openalex.org/W4226033575', 'https://openalex.org/W2995181338', 'https://openalex.org/W4226102207', 'https://openalex.org/W2899663614', 'https://openalex.org/W3047171714', 'https://openalex.org/W3163793923', 'https://openalex.org/W1922655562', 'https://openalex.org/W3035691519', 'https://openalex.org/W4285045102', 'https://openalex.org/W3006683367', 'https://openalex.org/W1494198834', 'https://openalex.org/W4295253143', 'https://openalex.org/W3015974384', 'https://openalex.org/W2952809536', 'https://openalex.org/W4284973286', 'https://openalex.org/W2896457183', 'https://openalex.org/W3194203264', 'https://openalex.org/W4320930577', 'https://openalex.org/W3016010032', 'https://openalex.org/W3197956630', 'https://openalex.org/W4308558335', 'https://openalex.org/W3095189764', 'https://openalex.org/W1828163288', 'https://openalex.org/W2991213871', 'https://openalex.org/W3033529678', 'https://openalex.org/W4309793872', 'https://openalex.org/W4287824654', 'https://openalex.org/W2095705004', 'https://openalex.org/W3123221884', 'https://openalex.org/W3171087525', 'https://openalex.org/W3007328579', 'https://openalex.org/W3095173472']",2023-01-09
https://openalex.org/W3024605872,https://doi.org/10.48550/arxiv.2005.05525,DiscreTalk: Text-to-Speech as a Machine Translation Problem,"This paper proposes a new end-to-end text-to-speech (E2E-TTS) model based on neural machine translation (NMT). The proposed model consists of two components; a non-autoregressive vector quantized variational autoencoder (VQ-VAE) model and an autoregressive Transformer-NMT model. The VQ-VAE model learns a mapping function from a speech waveform into a sequence of discrete symbols, and then the Transformer-NMT model is trained to estimate this discrete symbol sequence from a given input text. Since the VQ-VAE model can learn such a mapping in a fully-data-driven manner, we do not need to consider hyperparameters of the feature extraction required in the conventional E2E-TTS models. Thanks to the use of discrete symbols, we can use various techniques developed in NMT and automatic speech recognition (ASR) such as beam search, subword units, and fusions with a language model. Furthermore, we can avoid an over smoothing problem of predicted features, which is one of the common issues in TTS. The experimental evaluation with the JSUT corpus shows that the proposed method outperforms the conventional Transformer-TTS model with a non-autoregressive neural vocoder in naturalness, achieving the performance comparable to the reconstruction of the VQ-VAE model.","['https://openalex.org/W2963609956', 'https://openalex.org/W2111284386', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964243274', 'https://openalex.org/W3125709657', 'https://openalex.org/W2892140764', 'https://openalex.org/W2963975282', 'https://openalex.org/W2979476256', 'https://openalex.org/W2963620343', 'https://openalex.org/W2903739847', 'https://openalex.org/W2963300588', 'https://openalex.org/W2970206392', 'https://openalex.org/W3034729383', 'https://openalex.org/W2127141656', 'https://openalex.org/W2765486990', 'https://openalex.org/W2888169323', 'https://openalex.org/W2970730223', 'https://openalex.org/W2964060510', 'https://openalex.org/W2977997709', 'https://openalex.org/W2963799213', 'https://openalex.org/W2885185669', 'https://openalex.org/W2749651610', 'https://openalex.org/W2994689640', 'https://openalex.org/W2940544976', 'https://openalex.org/W3015338123', 'https://openalex.org/W72347498', 'https://openalex.org/W2970006822', 'https://openalex.org/W2963691546', 'https://openalex.org/W2129142580', 'https://openalex.org/W2949382160', 'https://openalex.org/W3016160783', 'https://openalex.org/W2884607399']",2020-05-12
https://openalex.org/W4378501656,https://doi.org/10.48550/arxiv.2305.16107,"VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation","Recent research shows a big convergence in model architecture, training objectives, and inference methods across various tasks for different modalities. In this paper, we propose VioLA, a single auto-regressive Transformer decoder-only network that unifies various cross-modal tasks involving speech and text, such as speech-to-text, text-to-text, text-to-speech, and speech-to-speech tasks, as a conditional codec language model task via multi-task learning framework. To accomplish this, we first convert all the speech utterances to discrete tokens (similar to the textual data) using an offline neural codec encoder. In such a way, all these tasks are converted to token-based sequence conversion problems, which can be naturally handled with one conditional language model. We further integrate task IDs (TID) and language IDs (LID) into the proposed model to enhance the modeling capability of handling different languages and tasks. Experimental results demonstrate that the proposed VioLA model can support both single-modal and cross-modal tasks well, and the decoder-only model achieves a comparable and even better performance than the strong baselines.",[],2023-05-25
https://openalex.org/W4379540238,https://doi.org/10.48550/arxiv.2306.02982,PolyVoice: Language Models for Speech to Speech Translation,"We propose PolyVoice, a language model-based framework for speech-to-speech translation (S2ST) system. Our framework consists of two language models: a translation language model and a speech synthesis language model. We use discretized speech units, which are generated in a fully unsupervised way, and thus our framework can be used for unwritten languages. For the speech synthesis part, we adopt the existing VALL-E X approach and build a unit-based audio language model. This grants our framework the ability to preserve the voice characteristics and the speaking style of the original speech. We examine our system on Chinese $\rightarrow$ English and English $\rightarrow$ Spanish pairs. Experimental results show that our system can generate speech with high translation quality and audio quality. Speech samples are available at https://speechtranslation.github.io/polyvoice.",[],2023-06-05
https://openalex.org/W4229005866,https://doi.org/10.4230/oasics.slate.2022.19,Analysing Off-The-Shelf Options for Question Answering with Portuguese FAQs,"Following the current interest in developing automatic question answering systems, we analyse alternative approaches for finding suitable answers from a list of Frequently Asked Questions (FAQs), in Portuguese. These rely on different technologies, some more established and others more recent, and are all easily adaptable to new lists of FAQs, on new domains. We analyse the effort required for their configuration, the accuracy of their answers, and the time they take to get such answers. We conclude that traditional Information Retrieval (IR) can be a solution for smaller lists of FAQs, but approaches based on deep neural networks for sentence encoding are at least as reliable and less dependent on the number and complexity of the FAQs. We also contribute with a small dataset of Portuguese FAQs on the domain of telecommunications, which was used in our experiments.",[],2022-01-01
https://openalex.org/W2963581463,https://doi.org/10.1109/icassp.2019.8683480,End-to-end Feedback Loss in Speech Chain Framework via Straight-through Estimator,"The speech chain mechanism integrates automatic speech recognition (ASR) and text-to-speech synthesis (TTS) modules into a single cycle during training. In our previous work, we applied a speech chain mechanism as a semi-supervised learning. It provides the ability for ASR and TTS to assist each other when they receive unpaired data and let them infer the missing pair and optimize the model with reconstruction loss. If we only have speech without transcription, ASR generates the most likely transcription from the speech data, and then TTS uses the generated transcription to reconstruct the original speech features. However, in previous papers, we just limited our back-propagation to the closest module, which is the TTS part. One reason is that back-propagating the error through the ASR is challenging due to the output of the ASR being discrete tokens, creating non-differentiability between the TTS and ASR. In this paper, we address this problem and describe how to thoroughly train a speech chain end-to-end for reconstruction loss using a straight-through estimator (ST). Experimental results revealed that, with sampling from ST-Gumbel-Softmax, we were able to update ASR parameters and improve the ASR performances by 11% relative CER reduction compared to the baseline.","['https://openalex.org/W2327501763', 'https://openalex.org/W1902237438', 'https://openalex.org/W6736356763', 'https://openalex.org/W2120847449', 'https://openalex.org/W2024490156', 'https://openalex.org/W6631362777', 'https://openalex.org/W1710082047', 'https://openalex.org/W950853366', 'https://openalex.org/W2962826786', 'https://openalex.org/W2884852625', 'https://openalex.org/W6690026940', 'https://openalex.org/W2963796886', 'https://openalex.org/W6638523607', 'https://openalex.org/W6730091202', 'https://openalex.org/W2962699523', 'https://openalex.org/W6679434410', 'https://openalex.org/W2526425061', 'https://openalex.org/W2963216553', 'https://openalex.org/W6745177358', 'https://openalex.org/W6729383884', 'https://openalex.org/W1524333225', 'https://openalex.org/W2547875792', 'https://openalex.org/W2604184139', 'https://openalex.org/W1821462560', 'https://openalex.org/W2133564696', 'https://openalex.org/W2964308564', 'https://openalex.org/W2962759037', 'https://openalex.org/W2548228487', 'https://openalex.org/W2242818861', 'https://openalex.org/W2952165242', 'https://openalex.org/W2546938941']",2019-04-17
https://openalex.org/W2025768430,https://doi.org/10.1145/1390156.1390294,Extracting and composing robust features with denoising autoencoders,"Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.","['https://openalex.org/W2176028050', 'https://openalex.org/W1498436455', 'https://openalex.org/W1526741802', 'https://openalex.org/W2153014441', 'https://openalex.org/W2128076038', 'https://openalex.org/W2110798204', 'https://openalex.org/W2112812053', 'https://openalex.org/W2172174689', 'https://openalex.org/W2149384588', 'https://openalex.org/W2128084896', 'https://openalex.org/W2613634265', 'https://openalex.org/W2133257461', 'https://openalex.org/W2153663612', 'https://openalex.org/W2108665656', 'https://openalex.org/W1994197834', 'https://openalex.org/W2131686571', 'https://openalex.org/W1586881595', 'https://openalex.org/W2072128103', 'https://openalex.org/W2100495367', 'https://openalex.org/W2136922672', 'https://openalex.org/W2609208295', 'https://openalex.org/W2111406701', 'https://openalex.org/W3207342693']",2008-01-01
https://openalex.org/W2466918907,https://doi.org/10.18653/v1/n16-1109,An Attentional Model for Speech Translation Without Transcription,"Long Duong, Antonios Anastasopoulos, David Chiang, Steven Bird, Trevor Cohn. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.","['https://openalex.org/W2143612262', 'https://openalex.org/W1994167151', 'https://openalex.org/W1885617648', 'https://openalex.org/W2038698865', 'https://openalex.org/W1973923101', 'https://openalex.org/W1902237438', 'https://openalex.org/W2107468211', 'https://openalex.org/W2327501763', 'https://openalex.org/W1556470778', 'https://openalex.org/W2184135559', 'https://openalex.org/W2064675550', 'https://openalex.org/W2063655091', 'https://openalex.org/W156805311', 'https://openalex.org/W1514535095', 'https://openalex.org/W2146939522', 'https://openalex.org/W2127141656', 'https://openalex.org/W2490163484', 'https://openalex.org/W854541894', 'https://openalex.org/W2148708890', 'https://openalex.org/W2090861223', 'https://openalex.org/W2963333747', 'https://openalex.org/W2595715041', 'https://openalex.org/W2168310280', 'https://openalex.org/W2964308564', 'https://openalex.org/W2095705004', 'https://openalex.org/W2170353620', 'https://openalex.org/W2006969979', 'https://openalex.org/W3012492057', 'https://openalex.org/W6908809', 'https://openalex.org/W2251001376', 'https://openalex.org/W1586532344', 'https://openalex.org/W2252212004', 'https://openalex.org/W2133564696', 'https://openalex.org/W2124807415', 'https://openalex.org/W2950178297', 'https://openalex.org/W2133444727', 'https://openalex.org/W2130942839', 'https://openalex.org/W2146502635', 'https://openalex.org/W2293858598']",2016-01-01
https://openalex.org/W2161742089,https://doi.org/10.1109/tasl.2006.878262,Comparative study on corpora for speech translation,"This paper investigates issues in preparing corpora for developing speech-to-speech translation (S2ST). It is impractical to create a broad-coverage parallel corpus only from dialog speech. An alternative approach is to have bilingual experts write conversational-style texts in the target domain, with translations. There is, however, a risk of losing fidelity to the actual utterances. This paper focuses on balancing a tradeoff between these two kinds of corpora through the analysis of two newly developed corpora in the travel domain: a bilingual parallel corpus with 420 K utterances and a collection of in-domain dialogs using actual S2ST systems. We found that the first corpus is effective for covering utterances in the second corpus if complimented with a small number of utterances taken from monolingual dialogs. We also found that characteristics of in-domain utterances become closer to those of the first corpus when more restrictive conditions and instructions to speakers are given. These results suggest the possibility of a bootstrap-style of development of corpora and S2ST systems, where an initial S2ST system is developed with parallel texts, and is then gradually improved with in-domain utterances collected by the system as restrictions are relaxed","['https://openalex.org/W146247575', 'https://openalex.org/W146602292', 'https://openalex.org/W2128876910', 'https://openalex.org/W2113106066', 'https://openalex.org/W2010917994', 'https://openalex.org/W80512364', 'https://openalex.org/W2101105183', 'https://openalex.org/W56244238', 'https://openalex.org/W37344644', 'https://openalex.org/W2262684420', 'https://openalex.org/W1577247080', 'https://openalex.org/W206967138', 'https://openalex.org/W92952441', 'https://openalex.org/W6680674784', 'https://openalex.org/W2042060232', 'https://openalex.org/W44443035', 'https://openalex.org/W2156104781', 'https://openalex.org/W42776112', 'https://openalex.org/W1991696363', 'https://openalex.org/W6712012081', 'https://openalex.org/W2398064412', 'https://openalex.org/W2394921947', 'https://openalex.org/W1508165687', 'https://openalex.org/W2263694371', 'https://openalex.org/W2138783964']",2006-08-23
https://openalex.org/W2962680099,https://doi.org/10.21437/interspeech.2017-944,Structured-Based Curriculum Learning for End-to-End English-Japanese Speech Translation,"Sequence-to-sequence attentional-based neural network architectures have been shown to provide a powerful model for machine translation and speech recognition.Recently, several works have attempted to extend the models for end-to-end speech translation task.However, the usefulness of these models were only investigated on language pairs with similar syntax and word order (e.g., English-French or English-Spanish).In this work, we focus on end-to-end speech translation tasks on syntactically distant language pairs (e.g., English-Japanese) that require distant word reordering.To guide the encoder-decoder attentional model to learn this difficult problem, we propose a structured-based curriculum learning strategy.Unlike conventional curriculum learning that gradually emphasizes difficult data examples, we formalize learning strategies from easier network structures to more difficult network structures.Here, we start the training with end-to-end encoder-decoder for speech recognition or text-based machine translation task then gradually move to end-to-end speech translation task.The experiment results show that the proposed approach could provide significant improvements in comparison with the one without curriculum learning.","['https://openalex.org/W2114483840', 'https://openalex.org/W2130942839', 'https://openalex.org/W2949328740', 'https://openalex.org/W2962965405', 'https://openalex.org/W1524333225', 'https://openalex.org/W1902237438', 'https://openalex.org/W2152834109', 'https://openalex.org/W1522301498', 'https://openalex.org/W2153653739', 'https://openalex.org/W2466918907', 'https://openalex.org/W854541894', 'https://openalex.org/W2327501763', 'https://openalex.org/W2161742089', 'https://openalex.org/W2136545725', 'https://openalex.org/W2345837149', 'https://openalex.org/W2295730220', 'https://openalex.org/W2064675550', 'https://openalex.org/W2133564696', 'https://openalex.org/W2507561499']",2017-08-16
https://openalex.org/W1902237438,https://doi.org/10.18653/v1/d15-1166,Effective Approaches to Attention-based Neural Machine Translation,"An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation.However, there has been little work exploring useful architectures for attention-based NMT.This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time.We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions.With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout.Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1","['https://openalex.org/W1591801644', 'https://openalex.org/W2086202918', 'https://openalex.org/W2101105183', 'https://openalex.org/W2950178297', 'https://openalex.org/W1514535095', 'https://openalex.org/W2130942839', 'https://openalex.org/W1586532344', 'https://openalex.org/W2100664567', 'https://openalex.org/W2250653840', 'https://openalex.org/W2147527908', 'https://openalex.org/W2169724380', 'https://openalex.org/W2962741254', 'https://openalex.org/W2157331557', 'https://openalex.org/W2964308564', 'https://openalex.org/W4241645538', 'https://openalex.org/W2133564696', 'https://openalex.org/W2118434577', 'https://openalex.org/W1753482797', 'https://openalex.org/W2153653739', 'https://openalex.org/W2951527505']",2015-01-01
https://openalex.org/W2884852625,https://doi.org/10.1109/slt.2018.8639528,Multi-Scale Alignment and Contextual History for Attention Mechanism in Sequence-to-Sequence Model,"A sequence-to-sequence model is a neural network module for mapping two sequences of different lengths. The sequence-to-sequence model has three core modules: encoder, decoder, and attention. Attention is the bridge that connects the encoder and decoder modules and improves model performance in many tasks. In this paper, we propose two ideas to improve sequence-to-sequence model performance by enhancing the attention module. First, we maintain the history of the location and the expected context from several previous time-steps. Second, we apply multiscale convolution from several previous attention vectors to the current decoder state. We utilized our proposed framework for sequence-to-sequence speech recognition and text-to-speech systems. The results reveal that our proposed extension can improve performance significantly compared to a standard attention baseline.","['https://openalex.org/W2963609956', 'https://openalex.org/W2962699523', 'https://openalex.org/W2963796886', 'https://openalex.org/W2024490156', 'https://openalex.org/W6631362777', 'https://openalex.org/W1710082047', 'https://openalex.org/W6640185926', 'https://openalex.org/W2144499799', 'https://openalex.org/W2962826786', 'https://openalex.org/W6631190155', 'https://openalex.org/W6623517193', 'https://openalex.org/W2327501763', 'https://openalex.org/W6745543685', 'https://openalex.org/W1902237438', 'https://openalex.org/W6630875275', 'https://openalex.org/W6735706088', 'https://openalex.org/W6679434410', 'https://openalex.org/W6679436768', 'https://openalex.org/W2526425061', 'https://openalex.org/W6745177358', 'https://openalex.org/W2964308564', 'https://openalex.org/W2963312585', 'https://openalex.org/W2962759037', 'https://openalex.org/W1514535095', 'https://openalex.org/W854541894', 'https://openalex.org/W2133564696', 'https://openalex.org/W2191779130', 'https://openalex.org/W1921523184', 'https://openalex.org/W1524333225', 'https://openalex.org/W2964121744', 'https://openalex.org/W2767601419', 'https://openalex.org/W1522301498', 'https://openalex.org/W2747920239', 'https://openalex.org/W2605141709', 'https://openalex.org/W2130942839']",2018-12-01
https://openalex.org/W2949328740,https://doi.org/10.48550/arxiv.1612.01744,Listen and Translate: A Proof of Concept for End-to-End Speech-to-Text\n Translation,"This paper proposes a first attempt to build an end-to-end speech-to-text\ntranslation system, which does not use source language transcription during\nlearning or decoding. We propose a model for direct speech-to-text translation,\nwhich gives promising results on a small French-English synthetic corpus.\nRelaxing the need for source language transcription would drastically change\nthe data collection methodology in speech translation, especially in\nunder-resourced scenarios. For instance, in the former project DARPA TRANSTAC\n(speech translation from spoken Arabic dialects), a large effort was devoted to\nthe collection of speech transcripts (and a prerequisite to obtain transcripts\nwas often a detailed transcription guide for languages with little standardized\nspelling). Now, if end-to-end approaches for speech-to-text translation are\nsuccessful, one might consider collecting data by asking bilingual speakers to\ndirectly utter speech in the source language from target language text\nutterances. Such an approach has the advantage to be applicable to any\nunwritten (source) language.\n","['https://openalex.org/W2963069010', 'https://openalex.org/W1522301498', 'https://openalex.org/W2953022181', 'https://openalex.org/W2327501763', 'https://openalex.org/W2949888546', 'https://openalex.org/W2466918907', 'https://openalex.org/W2953188895', 'https://openalex.org/W2101281673', 'https://openalex.org/W1591801644', 'https://openalex.org/W2964308564', 'https://openalex.org/W2222949842', 'https://openalex.org/W2963842982', 'https://openalex.org/W2525145252']",2016-12-06
https://openalex.org/W2605131327,https://doi.org/10.21437/interspeech.2017-503,Sequence-to-Sequence Models Can Directly Translate Foreign Speech,"We present a recurrent encoder-decoder deep neural network architecture that directly translates speech in one language into text in another.The model does not explicitly transcribe the speech into text in the source language, nor does it require supervision from the ground truth source language transcription during training.We apply a slightly modified sequence-to-sequence with attention architecture that has previously been used for speech recognition and show that it can be repurposed for this more complex task, illustrating the power of attention-based models.A single model trained end-to-end obtains state-of-the-art performance on the Fisher Callhome Spanish-English speech translation task, outperforming a cascade of independently trained sequence-to-sequence speech recognition and machine translation models by 1.8 BLEU points on the Fisher test set.In addition, we find that making use of the training data in both languages by multi-task training sequence-to-sequence speech translation and recognition models with a shared encoder network can improve performance by a further 1.4 BLEU points.","['https://openalex.org/W2327501763', 'https://openalex.org/W2577366047', 'https://openalex.org/W2136530135', 'https://openalex.org/W2095705004', 'https://openalex.org/W4294150926', 'https://openalex.org/W2139647714', 'https://openalex.org/W1537859740', 'https://openalex.org/W2466918907', 'https://openalex.org/W2964102148', 'https://openalex.org/W1869752048', 'https://openalex.org/W2251313925', 'https://openalex.org/W1514535095', 'https://openalex.org/W2130942839', 'https://openalex.org/W2525778437', 'https://openalex.org/W2133564696', 'https://openalex.org/W1836465849', 'https://openalex.org/W2011783148', 'https://openalex.org/W2550821151', 'https://openalex.org/W2949328740', 'https://openalex.org/W3012492057', 'https://openalex.org/W2530876040', 'https://openalex.org/W2953384591', 'https://openalex.org/W2375583958', 'https://openalex.org/W854541894', 'https://openalex.org/W1522301498', 'https://openalex.org/W2593011301', 'https://openalex.org/W2252212004', 'https://openalex.org/W1970987322', 'https://openalex.org/W1485009520', 'https://openalex.org/W2963842982', 'https://openalex.org/W1895577753', 'https://openalex.org/W2113106066']",2017-08-16
https://openalex.org/W2130942839,https://doi.org/10.48550/arxiv.1409.3215,Sequence to Sequence Learning with Neural Networks,"Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.","['https://openalex.org/W2112796928', 'https://openalex.org/W179875071', 'https://openalex.org/W2950635152', 'https://openalex.org/W2136939460', 'https://openalex.org/W2107878631', 'https://openalex.org/W1498436455', 'https://openalex.org/W2184045248', 'https://openalex.org/W1908388472', 'https://openalex.org/W194249466', 'https://openalex.org/W2964222437', 'https://openalex.org/W2251682575', 'https://openalex.org/W2316776689', 'https://openalex.org/W2253807446', 'https://openalex.org/W2964308564', 'https://openalex.org/W2141125852', 'https://openalex.org/W2132339004', 'https://openalex.org/W2163605009', 'https://openalex.org/W1525783482', 'https://openalex.org/W2150355110', 'https://openalex.org/W2127141656', 'https://openalex.org/W1753482797', 'https://openalex.org/W2064675550', 'https://openalex.org/W2402268235', 'https://openalex.org/W2101105183', 'https://openalex.org/W1578042930', 'https://openalex.org/W2250489405', 'https://openalex.org/W1810943226', 'https://openalex.org/W1815076433', 'https://openalex.org/W2147768505']",2014-09-10
https://openalex.org/W2972495969,https://doi.org/10.21437/interspeech.2019-1951,Direct Speech-to-Speech Translation with a Sequence-to-Sequence Model,"We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation.The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice).We further demonstrate the ability to synthesize translated speech using the voice of the source speaker.We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.","['https://openalex.org/W2963609956', 'https://openalex.org/W2133564696', 'https://openalex.org/W4298174729', 'https://openalex.org/W2747920239', 'https://openalex.org/W2972970915', 'https://openalex.org/W2892620417', 'https://openalex.org/W4293569541', 'https://openalex.org/W2133300417', 'https://openalex.org/W2964138190', 'https://openalex.org/W2963779652', 'https://openalex.org/W2097203679', 'https://openalex.org/W4293714597', 'https://openalex.org/W4385245566', 'https://openalex.org/W2113106066', 'https://openalex.org/W2963011080', 'https://openalex.org/W2964243274', 'https://openalex.org/W2962824709', 'https://openalex.org/W2139647714', 'https://openalex.org/W2525778437', 'https://openalex.org/W2120847449', 'https://openalex.org/W1962947832', 'https://openalex.org/W2896538040', 'https://openalex.org/W2928941594', 'https://openalex.org/W1494198834', 'https://openalex.org/W2011783148', 'https://openalex.org/W2808706139', 'https://openalex.org/W2795581297', 'https://openalex.org/W2912492482', 'https://openalex.org/W3012492057', 'https://openalex.org/W1538023239', 'https://openalex.org/W2605131327', 'https://openalex.org/W2152834109', 'https://openalex.org/W4289383906', 'https://openalex.org/W1537859740', 'https://openalex.org/W2136545725', 'https://openalex.org/W4294619240', 'https://openalex.org/W2788357188', 'https://openalex.org/W2949328740', 'https://openalex.org/W4298580827', 'https://openalex.org/W2941115821', 'https://openalex.org/W2794490148', 'https://openalex.org/W4300558631']",2019-09-13
https://openalex.org/W2123301721,,METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments,"We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalize concept of unigram match ing between th e mach inepro uce translation an h uman-pro uce reference translations. Unigrams can be match e base on th eir surface forms, stemme forms, an meanings; furth ermore, METEOR can be easily exten e to inclu e more a vance match ing strategies. Once all generalize unigram match es between th e two strings h ave been foun , METEOR computes a score for th is match ing using a combination of unigram-precision, unigram-recall, an a measure of fragmentation th at is esigne to irectly capture h ow well-or ere th e match e wor s in th e mach ine translation are in relation to th e reference. We evaluate METEOR by measuring th e correlation between th e metric scores an h uman judgments of translation quality. We compute th e Pearson R correlation value between its scores an human quality assessments of th e LDC TIDES 2003 Arabic-to-English and Chinese-to-English atasets. We perform segment-bysegment correlation, an show that METEOR gets an R correlation value of 0.347 on the Arabic data an 0.331 on the Chinese data. This is shown to be an improvement on using simply unigram-precision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules.","['https://openalex.org/W2078861931', 'https://openalex.org/W2101105183', 'https://openalex.org/W2118021410', 'https://openalex.org/W1588719663', 'https://openalex.org/W1489409710', 'https://openalex.org/W3197906698']",2005-06-01
https://openalex.org/W2963609956,https://doi.org/10.21437/interspeech.2017-1452,Tacotron: Towards End-to-End Speech Synthesis,"A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module.Building these components often requires extensive domain expertise and may contain brittle design choices.In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters.Given <text, audio> pairs, the model can be trained completely from scratch with random initialization.We present several key techniques to make the sequence-tosequence framework perform well for this challenging task.Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness.In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.","['https://openalex.org/W1869752048', 'https://openalex.org/W2327501763', 'https://openalex.org/W2130942839', 'https://openalex.org/W2525778437', 'https://openalex.org/W2584032004', 'https://openalex.org/W2133564696', 'https://openalex.org/W4295276571', 'https://openalex.org/W2507771204', 'https://openalex.org/W2519091744', 'https://openalex.org/W1924770834', 'https://openalex.org/W648786980', 'https://openalex.org/W4394643672', 'https://openalex.org/W2591927543', 'https://openalex.org/W2194775991', 'https://openalex.org/W2271840356', 'https://openalex.org/W1563460361', 'https://openalex.org/W2901997113', 'https://openalex.org/W4298261015', 'https://openalex.org/W2099057450', 'https://openalex.org/W1836465849', 'https://openalex.org/W2120847449', 'https://openalex.org/W1522301498', 'https://openalex.org/W2515943672', 'https://openalex.org/W2129142580', 'https://openalex.org/W2531207078']",2017-08-16
https://openalex.org/W2144600658,https://doi.org/10.1184/r1/6473090.v1,Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability,"In statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system. To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data. In this paper, we consider how to make such experiments more statistically reliable. We provide a systematic analysis of the effects of optimizer instability—an extraneous variable that is seldom controlled for—on experimental outcomes, and make recommendations for reporting results more accurately","['https://openalex.org/W2989631226', 'https://openalex.org/W2101105183', 'https://openalex.org/W2147258359', 'https://openalex.org/W1991903527', 'https://openalex.org/W2124807415', 'https://openalex.org/W2111142112', 'https://openalex.org/W2152382718', 'https://openalex.org/W1496357020', 'https://openalex.org/W1997859518', 'https://openalex.org/W2069196424', 'https://openalex.org/W2160218441', 'https://openalex.org/W2171421863', 'https://openalex.org/W2117897510', 'https://openalex.org/W2107223151', 'https://openalex.org/W2169755259', 'https://openalex.org/W2164766438', 'https://openalex.org/W2123301721', 'https://openalex.org/W2075601665', 'https://openalex.org/W2154590816', 'https://openalex.org/W3215037115', 'https://openalex.org/W2405762604', 'https://openalex.org/W2065883680', 'https://openalex.org/W222053410', 'https://openalex.org/W2159358338', 'https://openalex.org/W2146574666', 'https://openalex.org/W2110310640', 'https://openalex.org/W2437005631', 'https://openalex.org/W2149327368', 'https://openalex.org/W2155607551']",2011-01-01
https://openalex.org/W2133564696,https://doi.org/10.48550/arxiv.1409.0473,Neural Machine Translation by Jointly Learning to Align and Translate,"Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.","['https://openalex.org/W2172140247', 'https://openalex.org/W1810943226', 'https://openalex.org/W2103078213', 'https://openalex.org/W3037950864', 'https://openalex.org/W2950635152', 'https://openalex.org/W2964222437', 'https://openalex.org/W1916559533', 'https://openalex.org/W2964335273', 'https://openalex.org/W2949888546', 'https://openalex.org/W2132339004', 'https://openalex.org/W2131774270', 'https://openalex.org/W1908388472', 'https://openalex.org/W1753482797', 'https://openalex.org/W2395935897', 'https://openalex.org/W2251222643', 'https://openalex.org/W1571227886', 'https://openalex.org/W1828163288', 'https://openalex.org/W2005708641', 'https://openalex.org/W1905522558', 'https://openalex.org/W2251682575', 'https://openalex.org/W2153653739', 'https://openalex.org/W6908809']",2014-09-01
https://openalex.org/W1921523184,https://doi.org/10.48550/arxiv.1505.00853,Empirical Evaluation of Rectified Activations in Convolutional Network,"In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified linear unit (PReLU) and a new randomized leaky rectified linear units (RReLU). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in ReLU. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart. By using RReLU, we achieved 75.68\% accuracy on CIFAR-100 test set without multiple test or ensemble.","['https://openalex.org/W1497265063', 'https://openalex.org/W2095705004', 'https://openalex.org/W2179352600', 'https://openalex.org/W2949608135', 'https://openalex.org/W2949117887', 'https://openalex.org/W2102605133', 'https://openalex.org/W2952304308', 'https://openalex.org/W2950179405', 'https://openalex.org/W3118608800', 'https://openalex.org/W1799366690']",2015-05-05
https://openalex.org/W2789543585,https://doi.org/10.48550/arxiv.1803.03382,Fast Decoding in Sequence Models using Discrete Latent Variables,"Autoregressive sequence models based on deep neural networks, such as RNNs, Wavenet and the Transformer attain state-of-the-art results on many tasks. However, they are difficult to parallelize and are thus slow at processing long sequences. RNNs lack parallelism both during training and decoding, while architectures like WaveNet and Transformer are much more parallelizable during training, yet still operate sequentially during decoding. Inspired by [arxiv:1711.00937], we present a method to extend sequence models using discrete latent variables that makes decoding much more parallelizable. We first auto-encode the target sequence into a shorter sequence of discrete latent variables, which at inference time is generated autoregressively, and finally decode the output sequence from this shorter latent sequence in parallel. To this end, we introduce a novel method for constructing a sequence of discrete latent variables and compare it with previously introduced methods. Finally, we evaluate our model end-to-end on the task of neural machine translation, where it is an order of magnitude faster at decoding than comparable autoregressive models. While lower in BLEU than purely autoregressive models, our model achieves higher scores than previously proposed non-autoregressive translation models.","['https://openalex.org/W1863218161', 'https://openalex.org/W2602076750', 'https://openalex.org/W2952165242', 'https://openalex.org/W2547875792', 'https://openalex.org/W2157331557', 'https://openalex.org/W382779172', 'https://openalex.org/W2613904329', 'https://openalex.org/W2545625743', 'https://openalex.org/W2951004968', 'https://openalex.org/W2594538354', 'https://openalex.org/W1652807540', 'https://openalex.org/W2064675550', 'https://openalex.org/W1753482797', 'https://openalex.org/W2953333557', 'https://openalex.org/W2913932916', 'https://openalex.org/W2278112683', 'https://openalex.org/W2100495367', 'https://openalex.org/W2119717200', 'https://openalex.org/W2145094598', 'https://openalex.org/W2626778328', 'https://openalex.org/W2130942839', 'https://openalex.org/W2963069010', 'https://openalex.org/W2962997665', 'https://openalex.org/W1779483307', 'https://openalex.org/W2767002724', 'https://openalex.org/W2789649201', 'https://openalex.org/W2963187627', 'https://openalex.org/W189596042', 'https://openalex.org/W2962790997', 'https://openalex.org/W2785779000', 'https://openalex.org/W2133564696', 'https://openalex.org/W2963045354', 'https://openalex.org/W2952264928', 'https://openalex.org/W2540404261']",2018-03-09
https://openalex.org/W2972374322,https://doi.org/10.21437/interspeech.2019-3232,VQVAE Unsupervised Unit Discovery and Multi-Scale Code2Spec Inverter for Zerospeech Challenge 2019,"We describe our submitted system for the ZeroSpeech Challenge 2019.The current challenge theme addresses the difficulty of constructing a speech synthesizer without any text or phonetic labels and requires a system that can (1) discover subword units in an unsupervised way, and (2) synthesize the speech with a target speaker's voice.Moreover, the system should also balance the discrimination score ABX, the bit-rate compression rate, and the naturalness and the intelligibility of the constructed voice.To tackle these problems and achieve the best tradeoff, we utilize a vector quantized variational autoencoder (VQ-VAE) and a multi-scale codebook-tospectrogram (Code2Spec) inverter trained by mean square error and adversarial loss.The VQ-VAE extracts the speech to a latent space, forces itself to map it into the nearest codebook and produces compressed representation.Next, the inverter generates a magnitude spectrogram to the target voice, given the codebook vectors from VQ-VAE.In our experiments, we also investigated several other clustering algorithms, including K-Means and GMM, and compared them with the VQ-VAE result on ABX scores and bit rates.Our proposed approach significantly improved the intelligibility (in CER), the MOS, and discrimination ABX scores compared to the official ZeroSpeech 2019 baseline or even the topline.","['https://openalex.org/W2963971656', 'https://openalex.org/W2786608204', 'https://openalex.org/W2593414223', 'https://openalex.org/W2547039119', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963799213', 'https://openalex.org/W2101234009', 'https://openalex.org/W2399576818', 'https://openalex.org/W1959608418', 'https://openalex.org/W2666408839', 'https://openalex.org/W2964069186', 'https://openalex.org/W2962699523', 'https://openalex.org/W4300047444', 'https://openalex.org/W2962896155', 'https://openalex.org/W2940544976', 'https://openalex.org/W2120847449', 'https://openalex.org/W1522301498', 'https://openalex.org/W4320013936', 'https://openalex.org/W2787447541', 'https://openalex.org/W2888858245', 'https://openalex.org/W2963796886', 'https://openalex.org/W4394670483', 'https://openalex.org/W2911340057', 'https://openalex.org/W1836465849', 'https://openalex.org/W2899771611', 'https://openalex.org/W3125709657', 'https://openalex.org/W2191779130']",2019-09-13
https://openalex.org/W854541894,https://doi.org/10.48550/arxiv.1506.07503,Attention-Based Models for Speech Recognition,"Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks in- cluding machine translation, handwriting synthesis and image caption gen- eration. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the at- tention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level.","['https://openalex.org/W2127141656', 'https://openalex.org/W1872489089', 'https://openalex.org/W2157331557', 'https://openalex.org/W2209647458', 'https://openalex.org/W1987841215', 'https://openalex.org/W2112796928', 'https://openalex.org/W6908809', 'https://openalex.org/W1586532344', 'https://openalex.org/W2113021982', 'https://openalex.org/W2130942839', 'https://openalex.org/W2064675550', 'https://openalex.org/W1915251500', 'https://openalex.org/W1524333225', 'https://openalex.org/W1810943226', 'https://openalex.org/W1514535095', 'https://openalex.org/W2619993508', 'https://openalex.org/W2147527908', 'https://openalex.org/W2142416747', 'https://openalex.org/W2950178297', 'https://openalex.org/W2160815625', 'https://openalex.org/W1922655562', 'https://openalex.org/W2108677974', 'https://openalex.org/W2964308564', 'https://openalex.org/W2143612262', 'https://openalex.org/W2152175008', 'https://openalex.org/W1828163288', 'https://openalex.org/W3037881859', 'https://openalex.org/W1606347560', 'https://openalex.org/W2167839676', 'https://openalex.org/W2102113734', 'https://openalex.org/W1904365287']",2015-06-24
https://openalex.org/W2755682845,https://doi.org/10.48550/arxiv.1709.05522,AISHELL-1: An Open-Source Mandarin Speech Corpus and A Speech Recognition Baseline,"An open-source Mandarin speech corpus called AISHELL-1 is released. It is by far the largest corpus which is suitable for conducting the speech recognition research and building speech recognition systems for Mandarin. The recording procedure, including audio capturing devices and environments are presented in details. The preparation of the related resources, including transcriptions and lexicon are described. The corpus is released with a Kaldi recipe. Experimental results implies that the quality of audio recordings and transcriptions are promising.","['https://openalex.org/W1494198834', 'https://openalex.org/W2164240571', 'https://openalex.org/W1922655562', 'https://openalex.org/W2402146185', 'https://openalex.org/W2085628288', 'https://openalex.org/W2250357346']",2017-09-16
https://openalex.org/W2936295285,https://doi.org/10.21437/interspeech.2019-1518,Unsupervised Acoustic Unit Discovery for Speech Synthesis Using Discrete Latent-Variable Neural Networks,"For our submission to the ZeroSpeech 2019 challenge, we apply discrete latent-variable neural networks to unlabelled speech and use the discovered units for speech synthesis. Unsupervised discrete subword modelling could be useful for studies of phonetic category learning in infants or in low-resource speech technology requiring symbolic input. We use an autoencoder (AE) architecture with intermediate discretisation. We decouple acoustic unit discovery from speaker modelling by conditioning the AE's decoder on the training speaker identity. At test time, unit discovery is performed on speech from an unseen speaker, followed by unit decoding conditioned on a known target speaker to obtain reconstructed filterbanks. This output is fed to a neural vocoder to synthesise speech in the target speaker's voice. For discretisation, categorical variational autoencoders (CatVAEs), vector-quantised VAEs (VQ-VAEs) and straight-through estimation are compared at different compression levels on two languages. Our final model uses convolutional encoding, VQ-VAE discretisation, deconvolutional decoding and an FFTNet vocoder. We show that decoupled speaker conditioning intrinsically improves discrete acoustic representations, yielding competitive synthesis quality compared to the challenge baseline.","['https://openalex.org/W2951004968', 'https://openalex.org/W2025482506', 'https://openalex.org/W1577418252', 'https://openalex.org/W2547039119', 'https://openalex.org/W2911249026', 'https://openalex.org/W2964115348', 'https://openalex.org/W2949382160', 'https://openalex.org/W2347098582', 'https://openalex.org/W2476548250', 'https://openalex.org/W2346964103', 'https://openalex.org/W2963620343', 'https://openalex.org/W2964121744', 'https://openalex.org/W2091746061', 'https://openalex.org/W2963830550', 'https://openalex.org/W2242818861', 'https://openalex.org/W1967924372', 'https://openalex.org/W2963799213', 'https://openalex.org/W2890983311', 'https://openalex.org/W2395899413', 'https://openalex.org/W2134202996', 'https://openalex.org/W2945769669', 'https://openalex.org/W2548228487', 'https://openalex.org/W2963619462', 'https://openalex.org/W2598638573', 'https://openalex.org/W3125709657', 'https://openalex.org/W2826003142', 'https://openalex.org/W2148154194', 'https://openalex.org/W2404799143', 'https://openalex.org/W2035424729', 'https://openalex.org/W2126203737', 'https://openalex.org/W2020607164', 'https://openalex.org/W2962790638', 'https://openalex.org/W1778492285', 'https://openalex.org/W2963149687', 'https://openalex.org/W2973026522', 'https://openalex.org/W2963618559', 'https://openalex.org/W2547875792', 'https://openalex.org/W52412328', 'https://openalex.org/W1796128977', 'https://openalex.org/W2396043527', 'https://openalex.org/W2010188467', 'https://openalex.org/W2787447541']",2019-09-13
https://openalex.org/W2599585580,https://doi.org/10.1109/asru.2017.8269008,An embedded segmental K-means model for unsupervised segmentation and clustering of speech,"Unsupervised segmentation and clustering of unlabelled speech are core problems in zero-resource speech processing. Most approaches lie at methodological extremes: some use probabilistic Bayesian models with convergence guarantees, while others opt for more efficient heuristic techniques. Despite competitive performance in previous work, the full Bayesian approach is difficult to scale to large speech corpora. We introduce an approximation to a recent Bayesian model that still has a clear objective function but improves efficiency by using hard clustering and segmentation rather than full Bayesian inference. Like its Bayesian counterpart, this embedded segmental K-means model (ES-KMeans) represents arbitrary-length word segments as fixed-dimensional acoustic word embeddings. We first compare ES-KMeans to previous approaches on common English and Xitsonga data sets (5 and 2.5 hours of speech): ES-KMeans outperforms a leading heuristic method in word segmentation, giving similar scores to the Bayesian model while being 5 times faster with fewer hyperparameters. However, its clusters are less pure than those of the other models. We then show that ES-KMeans scales to larger corpora by applying it to the 5 languages of the Zero Resource Speech Challenge 2017 (up to 45 hours), where it performs competitively compared to the challenge baseline.","['https://openalex.org/W2962980711', 'https://openalex.org/W2190506272', 'https://openalex.org/W2059652594', 'https://openalex.org/W2463237750', 'https://openalex.org/W2161562001', 'https://openalex.org/W2963311389', 'https://openalex.org/W6731763572', 'https://openalex.org/W2962736743', 'https://openalex.org/W2963571336', 'https://openalex.org/W2154093685', 'https://openalex.org/W6677180724', 'https://openalex.org/W1577418252', 'https://openalex.org/W51277926', 'https://openalex.org/W2719865699', 'https://openalex.org/W2114347655', 'https://openalex.org/W2057007397', 'https://openalex.org/W2686360660', 'https://openalex.org/W2032943813', 'https://openalex.org/W2022058071', 'https://openalex.org/W6638059883', 'https://openalex.org/W2048648518', 'https://openalex.org/W6677620606', 'https://openalex.org/W3102667484', 'https://openalex.org/W2641832364', 'https://openalex.org/W2142775654', 'https://openalex.org/W6675022971', 'https://openalex.org/W6638159135', 'https://openalex.org/W2020607164', 'https://openalex.org/W2010188467', 'https://openalex.org/W2516890051', 'https://openalex.org/W2091746061', 'https://openalex.org/W2295297373', 'https://openalex.org/W6704726871', 'https://openalex.org/W2758697525', 'https://openalex.org/W6681031197', 'https://openalex.org/W6712720595', 'https://openalex.org/W2468716020', 'https://openalex.org/W6973666849', 'https://openalex.org/W2072396742', 'https://openalex.org/W6691362072', 'https://openalex.org/W113159538', 'https://openalex.org/W2100768664', 'https://openalex.org/W2117041980', 'https://openalex.org/W1796128977', 'https://openalex.org/W2291770225', 'https://openalex.org/W2116330964', 'https://openalex.org/W2483390977', 'https://openalex.org/W2951216052', 'https://openalex.org/W2398490608', 'https://openalex.org/W2346964103', 'https://openalex.org/W1590183771', 'https://openalex.org/W1778492285', 'https://openalex.org/W2251025892', 'https://openalex.org/W2143776582', 'https://openalex.org/W2550241133']",2017-12-01
https://openalex.org/W2103091632,https://doi.org/10.1098/rstb.2007.2154,Phonetic learning as a pathway to language: new data and native language magnet theory expanded (NLM-e),"Infants' speech perception skills show a dual change towards the end of the first year of life. Not only does non-native speech perception decline, as often shown, but native language speech perception skills show improvement, reflecting a facilitative effect of experience with native language. The mechanism underlying change at this point in development, and the relationship between the change in native and non-native speech perception, is of theoretical interest. As shown in new data presented here, at the cusp of this developmental change, infants' native and non-native phonetic perception skills predict later language ability, but in opposite directions. Better native language skill at 7.5 months of age predicts faster language advancement, whereas better non-native language skill predicts slower advancement. We suggest that native language phonetic performance is indicative of neural commitment to the native language, while non-native phonetic performance reveals un committed neural circuitry. This paper has three goals: (i) to review existing models of phonetic perception development, (ii) to present new event-related potential data showing that native and non-native phonetic perception at 7.5 months of age predicts language growth over the next 2 years, and (iii) to describe a revised version of our previous model, the native language magnet model, expanded (NLM-e). NLM-e incorporates five new principles. Specific testable predictions for future research programmes are described.","['https://openalex.org/W6805396450', 'https://openalex.org/W2137250991', 'https://openalex.org/W2061387296', 'https://openalex.org/W2991572845', 'https://openalex.org/W2165766103', 'https://openalex.org/W2001980829', 'https://openalex.org/W2051882640', 'https://openalex.org/W2020214791', 'https://openalex.org/W2067884299', 'https://openalex.org/W584551294', 'https://openalex.org/W1998703787', 'https://openalex.org/W2137416189', 'https://openalex.org/W2157427027', 'https://openalex.org/W2165454511', 'https://openalex.org/W2168249880', 'https://openalex.org/W2137354906', 'https://openalex.org/W2157423901', 'https://openalex.org/W2044707576', 'https://openalex.org/W2115113802', 'https://openalex.org/W2034952732', 'https://openalex.org/W2029862988', 'https://openalex.org/W1509639097', 'https://openalex.org/W2028520710', 'https://openalex.org/W1970547050', 'https://openalex.org/W2032442824', 'https://openalex.org/W2068785147', 'https://openalex.org/W2095458199', 'https://openalex.org/W2274070239', 'https://openalex.org/W2173628018', 'https://openalex.org/W2091746237', 'https://openalex.org/W2041874330', 'https://openalex.org/W2040247592', 'https://openalex.org/W2037662195', 'https://openalex.org/W2075371579', 'https://openalex.org/W2011238950', 'https://openalex.org/W4232516125', 'https://openalex.org/W2132530046', 'https://openalex.org/W4210416677', 'https://openalex.org/W2039948074', 'https://openalex.org/W2096156305', 'https://openalex.org/W2116137580', 'https://openalex.org/W2104143313', 'https://openalex.org/W2411353279', 'https://openalex.org/W2117099611', 'https://openalex.org/W2989728284', 'https://openalex.org/W2024797122', 'https://openalex.org/W1979066498', 'https://openalex.org/W2121531464', 'https://openalex.org/W2162207320', 'https://openalex.org/W2122143426', 'https://openalex.org/W2033739146', 'https://openalex.org/W2019922577', 'https://openalex.org/W2132730112', 'https://openalex.org/W2129928902', 'https://openalex.org/W2042816691', 'https://openalex.org/W1996678589', 'https://openalex.org/W4302454383', 'https://openalex.org/W2042055936', 'https://openalex.org/W2118599126', 'https://openalex.org/W2165545766', 'https://openalex.org/W2046840785', 'https://openalex.org/W2070696251', 'https://openalex.org/W2108449190', 'https://openalex.org/W1998550070', 'https://openalex.org/W1993678946', 'https://openalex.org/W2095459173', 'https://openalex.org/W2009836603', 'https://openalex.org/W2115639689', 'https://openalex.org/W2057513783', 'https://openalex.org/W2036747889', 'https://openalex.org/W2045343524', 'https://openalex.org/W2071591642', 'https://openalex.org/W4234331195', 'https://openalex.org/W2004518245', 'https://openalex.org/W2123940254', 'https://openalex.org/W2012607015', 'https://openalex.org/W2069880401', 'https://openalex.org/W2150389998', 'https://openalex.org/W2482127355', 'https://openalex.org/W2988972814', 'https://openalex.org/W1989769398', 'https://openalex.org/W2003810032', 'https://openalex.org/W1972159947', 'https://openalex.org/W2020944885', 'https://openalex.org/W2132852532', 'https://openalex.org/W2093111935', 'https://openalex.org/W2092655206', 'https://openalex.org/W2022465033', 'https://openalex.org/W2026219038', 'https://openalex.org/W2012714827', 'https://openalex.org/W2002681612', 'https://openalex.org/W2140661818', 'https://openalex.org/W2086880169', 'https://openalex.org/W2005311247', 'https://openalex.org/W2119669301', 'https://openalex.org/W1984717236', 'https://openalex.org/W2063525438', 'https://openalex.org/W1991819607', 'https://openalex.org/W2467997928', 'https://openalex.org/W2150423550', 'https://openalex.org/W2068247585', 'https://openalex.org/W2104381218', 'https://openalex.org/W2112119829', 'https://openalex.org/W1979539191', 'https://openalex.org/W1975527368', 'https://openalex.org/W1993768374', 'https://openalex.org/W2104752510', 'https://openalex.org/W2136653392', 'https://openalex.org/W2159222495', 'https://openalex.org/W2072947178', 'https://openalex.org/W2118769958', 'https://openalex.org/W2026593493', 'https://openalex.org/W2026352347', 'https://openalex.org/W2087826769', 'https://openalex.org/W2132875343', 'https://openalex.org/W2016344489', 'https://openalex.org/W2062956221', 'https://openalex.org/W1979521920', 'https://openalex.org/W1964576515', 'https://openalex.org/W2041708570', 'https://openalex.org/W2038421753', 'https://openalex.org/W2094012766', 'https://openalex.org/W2089239460', 'https://openalex.org/W1987812081', 'https://openalex.org/W2003208764', 'https://openalex.org/W2170124481', 'https://openalex.org/W2050864369', 'https://openalex.org/W2035256014', 'https://openalex.org/W1981922751', 'https://openalex.org/W1999587230', 'https://openalex.org/W2125811888', 'https://openalex.org/W2015112417', 'https://openalex.org/W2149676521', 'https://openalex.org/W4248274665', 'https://openalex.org/W1981980357', 'https://openalex.org/W2098123762', 'https://openalex.org/W1980044217', 'https://openalex.org/W1970688873', 'https://openalex.org/W2046356675', 'https://openalex.org/W2095737744', 'https://openalex.org/W2171016944', 'https://openalex.org/W2017650460', 'https://openalex.org/W1989477529', 'https://openalex.org/W1526915519', 'https://openalex.org/W2011962163', 'https://openalex.org/W2083270362', 'https://openalex.org/W2169578671', 'https://openalex.org/W1980862600', 'https://openalex.org/W2012557606', 'https://openalex.org/W2055208615', 'https://openalex.org/W2129370214', 'https://openalex.org/W4253682481', 'https://openalex.org/W1996861825', 'https://openalex.org/W2119165475', 'https://openalex.org/W2035791048', 'https://openalex.org/W2101483414', 'https://openalex.org/W2016434864', 'https://openalex.org/W1999287246', 'https://openalex.org/W2117404108', 'https://openalex.org/W4231737588', 'https://openalex.org/W2032476212', 'https://openalex.org/W2108582985', 'https://openalex.org/W2003341094', 'https://openalex.org/W2099227280', 'https://openalex.org/W2110170855', 'https://openalex.org/W2148005889', 'https://openalex.org/W2089883580', 'https://openalex.org/W2156909242', 'https://openalex.org/W2026070756', 'https://openalex.org/W2101509422', 'https://openalex.org/W2084579522', 'https://openalex.org/W2114952413', 'https://openalex.org/W2139380752', 'https://openalex.org/W2019548241', 'https://openalex.org/W2094736481', 'https://openalex.org/W2132106373', 'https://openalex.org/W2125566341', 'https://openalex.org/W2094512069', 'https://openalex.org/W3008317750', 'https://openalex.org/W2984678911', 'https://openalex.org/W1564039035', 'https://openalex.org/W2140961616', 'https://openalex.org/W2082256905', 'https://openalex.org/W1485243506', 'https://openalex.org/W4205096896', 'https://openalex.org/W1505035441', 'https://openalex.org/W1499947426', 'https://openalex.org/W4249060244', 'https://openalex.org/W2292765687', 'https://openalex.org/W4244412329', 'https://openalex.org/W608812506', 'https://openalex.org/W2163028944', 'https://openalex.org/W1200185189', 'https://openalex.org/W1917342115', 'https://openalex.org/W2560731476', 'https://openalex.org/W1993119641', 'https://openalex.org/W1964575515', 'https://openalex.org/W2951585632', 'https://openalex.org/W1983600564', 'https://openalex.org/W4246559809', 'https://openalex.org/W2089064206', 'https://openalex.org/W2135347537', 'https://openalex.org/W2130927703', 'https://openalex.org/W2122609529', 'https://openalex.org/W653904423', 'https://openalex.org/W2002103405', 'https://openalex.org/W598767079', 'https://openalex.org/W2066791380', 'https://openalex.org/W2166754847', 'https://openalex.org/W3036126562', 'https://openalex.org/W2146843281', 'https://openalex.org/W2994863418', 'https://openalex.org/W2014007333', 'https://openalex.org/W3108610876', 'https://openalex.org/W1525726177', 'https://openalex.org/W2019304146', 'https://openalex.org/W2164770604', 'https://openalex.org/W1519966925', 'https://openalex.org/W1998662048', 'https://openalex.org/W2619081900', 'https://openalex.org/W9610585', 'https://openalex.org/W2093985889', 'https://openalex.org/W2788324997']",2007-09-10
https://openalex.org/W2097203679,https://doi.org/10.1109/icassp.1997.599557,Janus-III: speech-to-speech translation in multiple languages,"This paper describes JANUS-III, our most recent version of the JANUS speech-to-speech translation system. We present an overview of the system and focus on how system design facilitates speech translation between multiple languages, and allows for easy adaptation to new source and target languages. We also describe our methodology for evaluation of end-to-end system performance with a variety of source and target languages. For system development and evaluation, we have experimented with both push-to-talk as well as cross-talk recording conditions. To date, our system has achieved performance levels of over 80% acceptable translations on transcribed input, and over 70% acceptable translations on speech input recognized with a 75-90% word accuracy. Our current major research is concentrated on enhancing the capabilities of the system to deal with input in broad and general domains.","['https://openalex.org/W6605806876', 'https://openalex.org/W1719940802', 'https://openalex.org/W2152439927', 'https://openalex.org/W2103972557', 'https://openalex.org/W2135045913', 'https://openalex.org/W6684567468', 'https://openalex.org/W6675988978', 'https://openalex.org/W6631372126', 'https://openalex.org/W2126924254', 'https://openalex.org/W2141404479', 'https://openalex.org/W1524863112', 'https://openalex.org/W2165871906', 'https://openalex.org/W3019806197', 'https://openalex.org/W143821927', 'https://openalex.org/W2105710547', 'https://openalex.org/W2406851047']",2002-11-22
https://openalex.org/W2998284473,https://doi.org/10.1609/aaai.v34i05.6341,Towards Zero-Shot Learning for Automatic Phonemic Transcription,"Automatic phonemic transcription tools are useful for low-resource language documentation. However, due to the lack of training sets, only a tiny fraction of languages have phonemic transcription tools. Fortunately, multilingual acoustic modeling provides a solution given limited audio training data. A more challenging problem is to build phonemic transcribers for languages with zero training data. The difficulty of this task is that phoneme inventories often differ between the training languages and the target language, making it infeasible to recognize unseen phonemes. In this work, we address this problem by adopting the idea of zero-shot learning. Our model is able to recognize unseen phonemes in the target language without any training data. In our model, we decompose phonemes into corresponding articulatory attributes such as vowel and consonant. Instead of predicting phonemes directly, we first predict distributions over articulatory attributes, and then compute phoneme distributions with a customized acoustic model. We evaluate our model by training it using 13 languages and testing it using 7 unseen languages. We find that it achieves 7.7% better phoneme error rate on average over a standard multilingual model.","['https://openalex.org/W2787760419', 'https://openalex.org/W2476082370', 'https://openalex.org/W2025401819', 'https://openalex.org/W2788381663', 'https://openalex.org/W6664307575', 'https://openalex.org/W2166637769', 'https://openalex.org/W7027429494', 'https://openalex.org/W2787447541', 'https://openalex.org/W2791647162', 'https://openalex.org/W2672120608', 'https://openalex.org/W1559678887', 'https://openalex.org/W2286443923', 'https://openalex.org/W6980779235', 'https://openalex.org/W2134270519', 'https://openalex.org/W2965549619', 'https://openalex.org/W1736701665', 'https://openalex.org/W2883972335', 'https://openalex.org/W2572670101', 'https://openalex.org/W6752124048', 'https://openalex.org/W3217769081', 'https://openalex.org/W6682222085', 'https://openalex.org/W1494198834', 'https://openalex.org/W2067392428', 'https://openalex.org/W652269744', 'https://openalex.org/W2250357346', 'https://openalex.org/W2894690744', 'https://openalex.org/W1980850109', 'https://openalex.org/W6678360021', 'https://openalex.org/W63916190', 'https://openalex.org/W2116648050', 'https://openalex.org/W2407897255', 'https://openalex.org/W2633221078', 'https://openalex.org/W2972797781', 'https://openalex.org/W6973666849', 'https://openalex.org/W2397721308', 'https://openalex.org/W2550821151', 'https://openalex.org/W2025482506', 'https://openalex.org/W2055408826', 'https://openalex.org/W2805993470', 'https://openalex.org/W2124033848', 'https://openalex.org/W2316803017', 'https://openalex.org/W3143107425', 'https://openalex.org/W2408712009', 'https://openalex.org/W1486697269', 'https://openalex.org/W2150295085', 'https://openalex.org/W2963292011', 'https://openalex.org/W2786608204']",2020-04-03
https://openalex.org/W1537859740,https://doi.org/10.21437/interspeech.2005-726,On the integration of speech recognition and statistical machine translation,"This paper focuses on the interface between speech recognition and machine translation in a speech translation system. Based on a thorough theoretical framework, we exploit word lattices of automatic speech recognition hypotheses as input to our translation system which is based on weighted finite-state transducers. We show that acoustic recognition scores of the recognized words in the lattices positively and significantly affect the translation quality. In experiments, we have found consistent improvements on three different corpora compared with translations of single best recognized results. In addition we build and evaluate a fully integrated speech translation model.","['https://openalex.org/W2403178545', 'https://openalex.org/W2107223151', 'https://openalex.org/W1500765663', 'https://openalex.org/W2156985047', 'https://openalex.org/W2139647714', 'https://openalex.org/W2336725036', 'https://openalex.org/W2046932483', 'https://openalex.org/W53604701', 'https://openalex.org/W2113106066', 'https://openalex.org/W2132109814', 'https://openalex.org/W2998215494', 'https://openalex.org/W2078861931', 'https://openalex.org/W2101105183', 'https://openalex.org/W2098750614']",2005-09-04
https://openalex.org/W2134202996,https://doi.org/10.1109/icassp.2014.6854069,Automatic discovery of a phonetic inventory for unwritten languages for statistical speech synthesis,"Speech synthesis systems are typically built with speech data and transcriptions. In this paper, we try to build synthesis systems when no transcriptions or knowledge about the language are available. It is usually necessary to at least possess phonetic knowledge about the language. In this paper, we propose an automated way of obtaining phones and phonetic knowledge about the corpus at hand by making use of Articulatory Features (AFs). An Articulatory Feature predictor is trained on a bootstrap corpus in an arbitrary other language using a three-hidden layer neural network. This neural network is run on the speech corpus to extract AFs. Hierarchical clustering is used to cluster the AFs into categories i.e. phones. Phonetic information about each of these inferred phones is obtained by computing the mean of the AFs in each cluster. Results of systems built with this framework in multiple languages are reported.","['https://openalex.org/W2404169761', 'https://openalex.org/W6603838645', 'https://openalex.org/W6608197479', 'https://openalex.org/W2150612204', 'https://openalex.org/W6697285287', 'https://openalex.org/W1986174057', 'https://openalex.org/W6730233244', 'https://openalex.org/W2116648050', 'https://openalex.org/W1981457580', 'https://openalex.org/W1966264494', 'https://openalex.org/W2401299519', 'https://openalex.org/W6602682705', 'https://openalex.org/W2128446656', 'https://openalex.org/W1490506669', 'https://openalex.org/W2296704011', 'https://openalex.org/W202879582', 'https://openalex.org/W2397987315', 'https://openalex.org/W66627554', 'https://openalex.org/W1560013842', 'https://openalex.org/W4389138872', 'https://openalex.org/W95152782', 'https://openalex.org/W2551677481']",2014-05-01
https://openalex.org/W2113106066,https://doi.org/10.1109/icassp.1999.758176,Speech translation: coupling of recognition and translation,"In speech translation, we are faced with the problem of how to couple the speech recognition process and the translation process. Starting from the Bayes decision rule for speech translation, we analyze how the interaction between the recognition process and the translation process can be modelled. In the light of this decision rule, we discuss the already existing approaches to speech translation. None of the existing approaches seems to have addressed this direct interaction. We suggest two new methods, the local averaging approximation and the monotone alignments.","['https://openalex.org/W6780303086', 'https://openalex.org/W2012511220', 'https://openalex.org/W2166810516', 'https://openalex.org/W2146418175', 'https://openalex.org/W6652311901', 'https://openalex.org/W2158164089', 'https://openalex.org/W2096312440', 'https://openalex.org/W2139647714', 'https://openalex.org/W1603508585', 'https://openalex.org/W2422872931', 'https://openalex.org/W6783671085', 'https://openalex.org/W6612447841', 'https://openalex.org/W3088213079', 'https://openalex.org/W368202463', 'https://openalex.org/W2006969979', 'https://openalex.org/W4285719527', 'https://openalex.org/W3037853454']",1999-01-01
https://openalex.org/W2945078028,https://doi.org/10.48550/arxiv.1905.06791,Almost Unsupervised Text to Speech and Automatic Speech Recognition,"Text to speech (TTS) and automatic speech recognition (ASR) are two dual tasks in speech processing and both achieve impressive performance thanks to the recent advance in deep learning and large amount of aligned speech and text data. However, the lack of aligned data poses a major practical problem for TTS and ASR on low-resource languages. In this paper, by leveraging the dual nature of the two tasks, we propose an almost unsupervised learning method that only leverages few hundreds of paired data and extra unpaired data for TTS and ASR. Our method consists of the following components: (1) a denoising auto-encoder, which reconstructs speech and text sequences respectively to develop the capability of language modeling both in speech and text domain; (2) dual transformation, where the TTS model transforms the text $y$ into speech $\hat{x}$, and the ASR model leverages the transformed pair $(\hat{x},y)$ for training, and vice versa, to boost the accuracy of the two tasks; (3) bidirectional sequence modeling, which addresses error propagation especially in the long speech and text sequence when training with few paired data; (4) a unified model structure, which combines all the above components for TTS and ASR based on Transformer model. Our method achieves 99.84% in terms of word level intelligible rate and 2.68 MOS for TTS, and 11.7% PER for ASR on LJSpeech dataset, by leveraging only 200 paired speech and text data (about 20 minutes audio), together with extra unpaired speech and text data.",[],2019-05-13
https://openalex.org/W2936969148,https://doi.org/10.1162/tacl_a_00270,Attention-Passing Models for Robust and Data-Efficient End-to-End Speech Translation,"Speech translation has traditionally been approached through cascaded models consisting of a speech recognizer trained on a corpus of transcribed speech, and a machine translation system trained on parallel texts. Several recent works have shown the feasibility of collapsing the cascade into a single, direct model that can be trained in an end-to-end fashion on a corpus of translated speech. However, experiments are inconclusive on whether the cascade or the direct model is stronger, and have only been conducted under the unrealistic assumption that both are trained on equal amounts of data, ignoring other available speech recognition and machine translation corpora. In this paper, we demonstrate that direct speech translation models require more data to perform well than cascaded models, and although they allow including auxiliary data through multi-task training, they are poor at exploiting such data, putting them at a severe disadvantage. As a remedy, we propose the use of end- to-end trainable models with two attention mechanisms, the first establishing source speech to source text alignments, the second modeling source to target text alignment. We show that such models naturally decompose into multi-task–trainable recognition and translation tasks and propose an attention-passing technique that alleviates error propagation issues in a previous formulation of a model with two attention stages. Our proposed model outperforms all examined baselines and is able to exploit auxiliary training data much more effectively than direct attentional models.","['https://openalex.org/W2964084097', 'https://openalex.org/W2962717763', 'https://openalex.org/W2964172053', 'https://openalex.org/W2964308564', 'https://openalex.org/W2963551569', 'https://openalex.org/W2792296443', 'https://openalex.org/W2183341477', 'https://openalex.org/W854541894', 'https://openalex.org/W3163547718', 'https://openalex.org/W2768066693', 'https://openalex.org/W1522301498', 'https://openalex.org/W2785350307', 'https://openalex.org/W97072897', 'https://openalex.org/W2419539795', 'https://openalex.org/W2620507731', 'https://openalex.org/W4300558631', 'https://openalex.org/W3012492057', 'https://openalex.org/W2963834942', 'https://openalex.org/W2605131327', 'https://openalex.org/W2327501763', 'https://openalex.org/W2963266340', 'https://openalex.org/W2466918907', 'https://openalex.org/W2963086938', 'https://openalex.org/W2963779652', 'https://openalex.org/W2620290161', 'https://openalex.org/W2530876040', 'https://openalex.org/W1494198834', 'https://openalex.org/W569478347', 'https://openalex.org/W2595840341', 'https://openalex.org/W2963303028', 'https://openalex.org/W2962680099']",2019-06-19
https://openalex.org/W2346964103,https://doi.org/10.1016/j.procs.2016.04.031,The Zero Resource Speech Challenge 2015: Proposed Approaches and Results,"This paper reports on the results of the Zero Resource Speech Challenge 2015, the first unified benchmark for zero resource speech technology, which aims at the unsupervised discovery of subword and word units from raw speech. This paper discusses the motivation for the challenge, its data sets, tasks and baseline systems. We outline the ideas behind the systems that were submitted for the two challenge tracks: unsupervised subword unit modeling and spoken term discovery, and summarize their results. The results obtained by participating teams show great promise; many systems beat the provided baselines and some even perform better than comparable supervised systems.","['https://openalex.org/W2025482506', 'https://openalex.org/W2346964103', 'https://openalex.org/W2400668844', 'https://openalex.org/W2396043527', 'https://openalex.org/W2402366697', 'https://openalex.org/W2399576818', 'https://openalex.org/W2407614114', 'https://openalex.org/W2398490608', 'https://openalex.org/W1796128977', 'https://openalex.org/W2404799143', 'https://openalex.org/W2044138293', 'https://openalex.org/W2238331496', 'https://openalex.org/W2247128061', 'https://openalex.org/W2395899413', 'https://openalex.org/W2406349064', 'https://openalex.org/W2251025892', 'https://openalex.org/W2057007397', 'https://openalex.org/W2117126688', 'https://openalex.org/W2052697931', 'https://openalex.org/W2011845089', 'https://openalex.org/W2786608204']",2016-01-01
https://openalex.org/W4385245566,https://doi.org/10.4230/lipics.itp.2023.19,MizAR 60 for Mizar 50,"As a present to Mizar on its 50th anniversary, we develop an AI/TP system that automatically proves about 60% of the Mizar theorems in the hammer setting. We also automatically prove 75% of the Mizar theorems when the automated provers are helped by using only the premises used in the human-written Mizar proofs. We describe the methods and large-scale experiments leading to these results. This includes in particular the E and Vampire provers, their ENIGMA and Deepire learning modifications, a number of learning-based premise selection methods, and the incremental loop that interleaves growing a corpus of millions of ATP proofs with training increasingly strong AI/TP systems on them. We also present a selection of Mizar problems that were proved automatically.",[],2023-01-01
https://openalex.org/W2762715843,https://doi.org/10.48550/arxiv.1710.03501,A Very Low Resource Language Speech Corpus for Computational Language Documentation Experiments,"Most speech and language technologies are trained with massive amounts of speech and text information. However, most of the world languages do not have such resources or stable orthography. Systems constructed under these almost zero resource conditions are not only promising for speech technology but also for computational language documentation. The goal of computational language documentation is to help field linguists to (semi-)automatically analyze and annotate audio recordings of endangered and unwritten languages. Example tasks are automatic phoneme discovery or lexicon discovery from the speech signal. This paper presents a speech corpus collected during a realistic language documentation process. It is made up of 5k speech utterances in Mboshi (Bantu C25) aligned to French text translations. Speech transcriptions are also made available: they correspond to a non-standard graphemic form close to the language phonology. We present how the data was collected, cleaned and processed and we illustrate its use through a zero-resource task: spoken term discovery. The dataset is made available to the community for reproducible computational language documentation experiments and their evaluation.","['https://openalex.org/W847516984', 'https://openalex.org/W2620638943', 'https://openalex.org/W1557247526', 'https://openalex.org/W2592914315', 'https://openalex.org/W2466918907', 'https://openalex.org/W2756778986', 'https://openalex.org/W2747287964', 'https://openalex.org/W2347145335', 'https://openalex.org/W2122228338', 'https://openalex.org/W2126377586', 'https://openalex.org/W2553206119', 'https://openalex.org/W2590585939', 'https://openalex.org/W2251408482', 'https://openalex.org/W2345799635', 'https://openalex.org/W2668277942']",2017-10-10
https://openalex.org/W2304648132,https://doi.org/10.48550/arxiv.1603.07285,A guide to convolution arithmetic for deep learning,"We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.","['https://openalex.org/W2964288706', 'https://openalex.org/W2173520492', 'https://openalex.org/W1849277567', 'https://openalex.org/W78356000', 'https://openalex.org/W2557283755']",2016-03-23
https://openalex.org/W2952468927,,MASS: Masked Sequence to Sequence Pre-training for Language Generation,"Pre-training and fine-tuning, e.g., BERT, have achieved great success in language understanding by transferring knowledge from rich-resource pre-training task to the low/zero-resource downstream tasks. Inspired by the success of BERT, we propose MAsked Sequence to Sequence pre-training (MASS) for the encoder-decoder based language generation tasks. MASS adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence: its encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and its decoder tries to predict this masked fragment. In this way, MASS can jointly train the encoder and decoder to develop the capability of representation extraction and language modeling. By further fine-tuning on a variety of zero/low-resource language generation tasks, including neural machine translation, text summarization and conversational response generation (3 tasks and totally 8 datasets), MASS achieves significant improvements over the baselines without pre-training or with other pre-training methods. Specially, we achieve the state-of-the-art accuracy (37.5 in terms of BLEU score) on the unsupervised English-French translation, even beating the early attention-based supervised model.","['https://openalex.org/W2952711665', 'https://openalex.org/W2963963856', 'https://openalex.org/W2251939518', 'https://openalex.org/W2025768430', 'https://openalex.org/W2794365787', 'https://openalex.org/W2427527485', 'https://openalex.org/W2963532104', 'https://openalex.org/W2964308564', 'https://openalex.org/W2610891036', 'https://openalex.org/W2890964657', 'https://openalex.org/W2951991713', 'https://openalex.org/W2158108973', 'https://openalex.org/W2905933322', 'https://openalex.org/W2784823820', 'https://openalex.org/W2170973209', 'https://openalex.org/W2144578941', 'https://openalex.org/W2250539671', 'https://openalex.org/W2121227244', 'https://openalex.org/W2117130368', 'https://openalex.org/W2341718173', 'https://openalex.org/W2964007535', 'https://openalex.org/W2201245532', 'https://openalex.org/W2097117768', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964265128', 'https://openalex.org/W2963756346', 'https://openalex.org/W2131744502', 'https://openalex.org/W2963088995', 'https://openalex.org/W1840435438', 'https://openalex.org/W2102605133', 'https://openalex.org/W2153579005', 'https://openalex.org/W2561274697', 'https://openalex.org/W2963500743', 'https://openalex.org/W2964121744', 'https://openalex.org/W179875071', 'https://openalex.org/W2130903752', 'https://openalex.org/W2963341956', 'https://openalex.org/W2799054028', 'https://openalex.org/W2786464815', 'https://openalex.org/W2962739339', 'https://openalex.org/W2914120296', 'https://openalex.org/W2766182427', 'https://openalex.org/W2736472496', 'https://openalex.org/W2194775991', 'https://openalex.org/W2964013027', 'https://openalex.org/W2125320996', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963206679', 'https://openalex.org/W2157331557', 'https://openalex.org/W4285719527']",2019-05-07
https://openalex.org/W2136545725,https://doi.org/10.1109/tsa.2005.860774,The ATR Multilingual Speech-to-Speech Translation System,"In this paper, we describe the ATR multilingual speech-to-speech translation (S2ST) system, which is mainly focused on translation between English and Asian languages (Japanese and Chinese). There are three main modules of our S2ST system: large-vocabulary continuous speech recognition, machine text-to-text (T2T) translation, and text-to-speech synthesis. All of them are multilingual and are designed using state-of-the-art technologies developed at ATR. A corpus-based statistical machine learning framework forms the basis of our system design. We use a parallel multilingual database consisting of over 600 000 sentences that cover a broad range of travel-related conversations. Recent evaluation of the overall system showed that speech-to-speech translation quality is high, being at the level of a person having a Test of English for International Communication (TOEIC) score of 750 out of the perfect score of 990.","['https://openalex.org/W1538023239', 'https://openalex.org/W6713649003', 'https://openalex.org/W2030599239', 'https://openalex.org/W60527148', 'https://openalex.org/W75668230', 'https://openalex.org/W6713857124', 'https://openalex.org/W1608375737', 'https://openalex.org/W2056191164', 'https://openalex.org/W2115040572', 'https://openalex.org/W6615979439', 'https://openalex.org/W2158069733', 'https://openalex.org/W1971081490', 'https://openalex.org/W2068970468', 'https://openalex.org/W42776112', 'https://openalex.org/W6678277124', 'https://openalex.org/W1991696363', 'https://openalex.org/W4245436919', 'https://openalex.org/W6629143899', 'https://openalex.org/W3162356162', 'https://openalex.org/W2035301451', 'https://openalex.org/W6679517586', 'https://openalex.org/W2124262992', 'https://openalex.org/W2396590899', 'https://openalex.org/W6631063652', 'https://openalex.org/W2114434620', 'https://openalex.org/W6660882514', 'https://openalex.org/W2135540090', 'https://openalex.org/W6652766275', 'https://openalex.org/W1972176569', 'https://openalex.org/W6800720845', 'https://openalex.org/W6637695085', 'https://openalex.org/W6676380348', 'https://openalex.org/W2154920538', 'https://openalex.org/W6677017728', 'https://openalex.org/W6608974995', 'https://openalex.org/W6608432165', 'https://openalex.org/W87685050', 'https://openalex.org/W6656414902', 'https://openalex.org/W6639371816', 'https://openalex.org/W6704302056', 'https://openalex.org/W2134742194', 'https://openalex.org/W80512364', 'https://openalex.org/W6674566583', 'https://openalex.org/W1489834179', 'https://openalex.org/W2068225931', 'https://openalex.org/W229329885', 'https://openalex.org/W1869993023', 'https://openalex.org/W165976194', 'https://openalex.org/W2024490156', 'https://openalex.org/W2010532441', 'https://openalex.org/W2016381774', 'https://openalex.org/W2111514866', 'https://openalex.org/W2097790277', 'https://openalex.org/W1878590289', 'https://openalex.org/W1738293921', 'https://openalex.org/W2407488922', 'https://openalex.org/W2097333193', 'https://openalex.org/W1517947178', 'https://openalex.org/W2116661712', 'https://openalex.org/W3196758762', 'https://openalex.org/W2339771386', 'https://openalex.org/W564913581', 'https://openalex.org/W2276283915', 'https://openalex.org/W2134031652', 'https://openalex.org/W2404832377', 'https://openalex.org/W1520858356', 'https://openalex.org/W4285719527', 'https://openalex.org/W2042060232', 'https://openalex.org/W1585177184', 'https://openalex.org/W2121227244', 'https://openalex.org/W206967138', 'https://openalex.org/W2242106140', 'https://openalex.org/W2138783964']",2006-02-21
https://openalex.org/W2901607128,https://doi.org/10.21437/iberspeech.2018-13,End-to-End Speech Translation with the Transformer,"Speech Translation has been traditionally addressed with the concatenation of two tasks: Speech Recognition and Machine Translation. This approach has the main drawback that errors are concatenated. Recently, neural approaches to Speech Recognition and Machine Translation have made possible facing the task by means of an End-to-End Speech Translation architecture. In this paper, we propose to use the architecture of the Transformer which is based solely on attention-based mechanisms to address the End-to-End Speech Translation system. As a contrastive architecture, we use the same Transformer to built the Speech Recognition and Machine Translation systems to perform Speech Translation through concatenation of systems. Results on a Spanish-to-English standard task show that the end-to-end architecture is able to outperform the concatenated systems by half point BLEU.","['https://openalex.org/W2150622104', 'https://openalex.org/W2964308564', 'https://openalex.org/W1855892484', 'https://openalex.org/W2963403868']",2018-11-19
https://openalex.org/W3005578234,https://doi.org/10.1109/taslp.2020.2973896,Speech Technology for Unwritten Languages,"&lt;p&gt;Speech technology plays an important role in our everyday life. Among others, speech is used for human-computer interaction, for instance for information retrieval and on-line shopping. In the case of an unwritten language, however, speech technology is unfortunately difficult to create, because it cannot be created by the standard combination of pre-trained speech-to-text and text-to-speech subsystems. The research presented in this article takes the first steps towards speech technology for unwritten languages. Specifically, the aim of this work was 1) to learn speech-to-meaning representations without using text as an intermediate representation, and 2) to test the sufficiency of the learned representations to regenerate speech or translated text, or to retrieve images that depict the meaning of an utterance in an unwritten language. The results suggest that building systems that go directly from speech-to-meaning and from meaning-to-speech, bypassing the need for text, is possible.&lt;/p&gt;","['https://openalex.org/W6727690538', 'https://openalex.org/W6635896687', 'https://openalex.org/W1970890968', 'https://openalex.org/W2079460648', 'https://openalex.org/W6713625570', 'https://openalex.org/W6637698695', 'https://openalex.org/W2302086703', 'https://openalex.org/W2134587001', 'https://openalex.org/W68733909', 'https://openalex.org/W2077302143', 'https://openalex.org/W2194775991', 'https://openalex.org/W2545319977', 'https://openalex.org/W1990005915', 'https://openalex.org/W6698656214', 'https://openalex.org/W6807204625', 'https://openalex.org/W2964243274', 'https://openalex.org/W2894690744', 'https://openalex.org/W4206865574', 'https://openalex.org/W2592914315', 'https://openalex.org/W6729977899', 'https://openalex.org/W6679436768', 'https://openalex.org/W2110405378', 'https://openalex.org/W2117041980', 'https://openalex.org/W2625455707', 'https://openalex.org/W2347145335', 'https://openalex.org/W2127141656', 'https://openalex.org/W6744702808', 'https://openalex.org/W2963330681', 'https://openalex.org/W2962862718', 'https://openalex.org/W2963902314', 'https://openalex.org/W2927673779', 'https://openalex.org/W6775587661', 'https://openalex.org/W2964115348', 'https://openalex.org/W1552207788', 'https://openalex.org/W2114347655', 'https://openalex.org/W2101105183', 'https://openalex.org/W6684449667', 'https://openalex.org/W2584852661', 'https://openalex.org/W6631194405', 'https://openalex.org/W6704752648', 'https://openalex.org/W6639156005', 'https://openalex.org/W6631637103', 'https://openalex.org/W6802821423', 'https://openalex.org/W6753507521', 'https://openalex.org/W2466918907', 'https://openalex.org/W6749352598', 'https://openalex.org/W2730658205', 'https://openalex.org/W103827355', 'https://openalex.org/W6754400763', 'https://openalex.org/W6679434410', 'https://openalex.org/W2101281673', 'https://openalex.org/W2582956876', 'https://openalex.org/W2345799635', 'https://openalex.org/W6734394903', 'https://openalex.org/W6712032203', 'https://openalex.org/W2134202996', 'https://openalex.org/W6604618392', 'https://openalex.org/W2129084675', 'https://openalex.org/W3148931049', 'https://openalex.org/W2164505566', 'https://openalex.org/W2592864539', 'https://openalex.org/W1855892484', 'https://openalex.org/W1520968739', 'https://openalex.org/W2025482506', 'https://openalex.org/W1985229197', 'https://openalex.org/W2964308564', 'https://openalex.org/W1753482797', 'https://openalex.org/W2396312253', 'https://openalex.org/W2949328740', 'https://openalex.org/W1589303317', 'https://openalex.org/W2307960051', 'https://openalex.org/W3105148948', 'https://openalex.org/W2100799972', 'https://openalex.org/W2347098582', 'https://openalex.org/W2955541912', 'https://openalex.org/W2133564696', 'https://openalex.org/W202879582', 'https://openalex.org/W2792296443', 'https://openalex.org/W4206864474', 'https://openalex.org/W2130942839', 'https://openalex.org/W3015703505', 'https://openalex.org/W2556930864', 'https://openalex.org/W4240549324', 'https://openalex.org/W2168322495', 'https://openalex.org/W2762715843', 'https://openalex.org/W2964172053', 'https://openalex.org/W2406324447', 'https://openalex.org/W2828845773', 'https://openalex.org/W1604771987', 'https://openalex.org/W114226241', 'https://openalex.org/W1532499126', 'https://openalex.org/W2963260927', 'https://openalex.org/W3204007566', 'https://openalex.org/W2525778437']",2020-01-01
https://openalex.org/W3034420534,https://doi.org/10.1109/taslp.2020.3001456,ConvS2S-VC: Fully Convolutional Sequence-to-Sequence Voice Conversion,"This article proposes a voice conversion (VC) method using sequence-to-sequence (seq2seq or S2S) learning, which flexibly converts not only the voice characteristics but also the pitch contour and duration of input speech. The proposed method, called ConvS2S-VC, has three key features. First, it uses a model with a fully convolutional architecture. This is particularly advantageous in that it is suitable for parallel computations using GPUs. It is also beneficial since it enables effective normalization techniques such as batch normalization to be used for all the hidden layers in the networks. Second, it achieves many-to-many conversion by simultaneously learning mappings among multiple speakers using only a single model instead of separately learning mappings between each speaker pair using a different model. This enables the model to fully utilize available training data collected from multiple speakers by capturing common latent features that can be shared across different speakers. Owing to this structure, our model works reasonably well even without source speaker information, thus making it able to handle any-to-many conversion tasks. Third, we introduce a mechanism, called the conditional batch normalization that switches batch normalization layers in accordance with the target speaker. This particular mechanism has been found to be extremely effective for our many-to-many conversion model. We conducted speaker identity conversion experiments and found that ConvS2S-VC obtained higher sound quality and speaker similarity than baseline methods. We also found from audio examples that it could perform well in various tasks including emotional expression conversion, electrolaryngeal speech enhancement, and English accent conversion.","['https://openalex.org/W2972970915', 'https://openalex.org/W2972999331', 'https://openalex.org/W6737778391', 'https://openalex.org/W6731370813', 'https://openalex.org/W1902237438', 'https://openalex.org/W2897353073', 'https://openalex.org/W2963808252', 'https://openalex.org/W2899877258', 'https://openalex.org/W6756102363', 'https://openalex.org/W2076258817', 'https://openalex.org/W2800289214', 'https://openalex.org/W2963035245', 'https://openalex.org/W6749489859', 'https://openalex.org/W6724804524', 'https://openalex.org/W2767052532', 'https://openalex.org/W6695676441', 'https://openalex.org/W6638667902', 'https://openalex.org/W2964243274', 'https://openalex.org/W2142300631', 'https://openalex.org/W2123003832', 'https://openalex.org/W2963539064', 'https://openalex.org/W6623517193', 'https://openalex.org/W6679436768', 'https://openalex.org/W2591927543', 'https://openalex.org/W2963609956', 'https://openalex.org/W6756197946', 'https://openalex.org/W2619368999', 'https://openalex.org/W2911708970', 'https://openalex.org/W2093450784', 'https://openalex.org/W6631190155', 'https://openalex.org/W6603838645', 'https://openalex.org/W2972667718', 'https://openalex.org/W6729110096', 'https://openalex.org/W6739901393', 'https://openalex.org/W2471520273', 'https://openalex.org/W2105160541', 'https://openalex.org/W2157412983', 'https://openalex.org/W2963011080', 'https://openalex.org/W2057609679', 'https://openalex.org/W2739735615', 'https://openalex.org/W1509691205', 'https://openalex.org/W2747744257', 'https://openalex.org/W2532494225', 'https://openalex.org/W2962896155', 'https://openalex.org/W2946555236', 'https://openalex.org/W2902070858', 'https://openalex.org/W2148846882', 'https://openalex.org/W2022125261', 'https://openalex.org/W2056852181', 'https://openalex.org/W2161727827', 'https://openalex.org/W2156142001', 'https://openalex.org/W2017742648', 'https://openalex.org/W2963175743', 'https://openalex.org/W2120605154', 'https://openalex.org/W6753855596', 'https://openalex.org/W6843673214', 'https://openalex.org/W6756159577', 'https://openalex.org/W2963300588', 'https://openalex.org/W6748409065', 'https://openalex.org/W2749651610', 'https://openalex.org/W2890983311', 'https://openalex.org/W6732429163', 'https://openalex.org/W2130942839', 'https://openalex.org/W4385245566', 'https://openalex.org/W2284050935', 'https://openalex.org/W4289305009', 'https://openalex.org/W854541894', 'https://openalex.org/W1836465849', 'https://openalex.org/W2963975282', 'https://openalex.org/W4298580827', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963782041', 'https://openalex.org/W2584032004', 'https://openalex.org/W2963712897', 'https://openalex.org/W1522301498', 'https://openalex.org/W2613904329', 'https://openalex.org/W2964265128', 'https://openalex.org/W2964281804', 'https://openalex.org/W2502312327', 'https://openalex.org/W2963691546', 'https://openalex.org/W2519091744', 'https://openalex.org/W2962882868', 'https://openalex.org/W3099078140', 'https://openalex.org/W2901997113', 'https://openalex.org/W4294619240', 'https://openalex.org/W2963970792', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963685250', 'https://openalex.org/W2545656684', 'https://openalex.org/W95152782', 'https://openalex.org/W2898654681', 'https://openalex.org/W2567070169', 'https://openalex.org/W2949382160', 'https://openalex.org/W2964307104']",2020-01-01
https://openalex.org/W3025844872,https://doi.org/10.21437/interspeech.2020-2665,An Open Source Implementation of ITU-T Recommendation P.808 with Validation,"The ITU-T Recommendation P.808 provides a crowdsourcing approach for\nconducting a subjective assessment of speech quality using the Absolute\nCategory Rating (ACR) method. We provide an open-source implementation of the\nITU-T Rec. P.808 that runs on the Amazon Mechanical Turk platform. We extended\nour implementation to include Degradation Category Ratings (DCR) and Comparison\nCategory Ratings (CCR) test methods. We also significantly speed up the test\nprocess by integrating the participant qualification step into the main rating\ntask compared to a two-stage qualification and rating solution. We provide\nprogram scripts for creating and executing the subjective test, and data\ncleansing and analyzing the answers to avoid operational errors. To validate\nthe implementation, we compare the Mean Opinion Scores (MOS) collected through\nour implementation with MOS values from a standard laboratory experiment\nconducted based on the ITU-T Rec. P.800. We also evaluate the reproducibility\nof the result of the subjective speech quality assessment through crowdsourcing\nusing our implementation. Finally, we quantify the impact of parts of the\nsystem designed to improve the reliability: environmental tests, gold and\ntrapping questions, rating patterns, and a headset usage test.\n","['https://openalex.org/W2408491799', 'https://openalex.org/W2799412098', 'https://openalex.org/W3037833165', 'https://openalex.org/W3002439978', 'https://openalex.org/W2049208969', 'https://openalex.org/W2791182566', 'https://openalex.org/W577318182', 'https://openalex.org/W2395622121', 'https://openalex.org/W2922332774', 'https://openalex.org/W2026538633', 'https://openalex.org/W2160473997', 'https://openalex.org/W3101528112', 'https://openalex.org/W1597438642', 'https://openalex.org/W3013102888', 'https://openalex.org/W3015717277', 'https://openalex.org/W2791686384']",2020-10-25
https://openalex.org/W2996414377,https://doi.org/10.1109/taslp.2019.2960721,Non-Parallel Sequence-to-Sequence Voice Conversion With Disentangled Linguistic and Speaker Representations,"This paper presents a method of sequence-to-sequence (seq2seq) voice\nconversion using non-parallel training data. In this method, disentangled\nlinguistic and speaker representations are extracted from acoustic features,\nand voice conversion is achieved by preserving the linguistic representations\nof source utterances while replacing the speaker representations with the\ntarget ones. Our model is built under the framework of encoder-decoder neural\nnetworks. A recognition encoder is designed to learn the disentangled\nlinguistic representations with two strategies. First, phoneme transcriptions\nof training data are introduced to provide the references for leaning\nlinguistic representations of audio signals. Second, an adversarial training\nstrategy is employed to further wipe out speaker information from the\nlinguistic representations. Meanwhile, speaker representations are extracted\nfrom audio signals by a speaker encoder. The model parameters are estimated by\ntwo-stage training, including a pretraining stage using a multi-speaker dataset\nand a fine-tuning stage using the dataset of a specific conversion pair. Since\nboth the recognition encoder and the decoder for recovering acoustic features\nare seq2seq neural networks, there are no constrains of frame alignment and\nframe-by-frame conversion in our proposed method. Experimental results showed\nthat our method obtained higher similarity and naturalness than the best\nnon-parallel voice conversion method in Voice Conversion Challenge 2018.\nBesides, the performance of our proposed method was closed to the\nstate-of-the-art parallel seq2seq voice conversion method.\n","['https://openalex.org/W2939131199', 'https://openalex.org/W2064675550', 'https://openalex.org/W2804998325', 'https://openalex.org/W2888922217', 'https://openalex.org/W2889329491', 'https://openalex.org/W6736811364', 'https://openalex.org/W2963830550', 'https://openalex.org/W6737196085', 'https://openalex.org/W2532494225', 'https://openalex.org/W2475998840', 'https://openalex.org/W2937579788', 'https://openalex.org/W2518172956', 'https://openalex.org/W4245885054', 'https://openalex.org/W2100819376', 'https://openalex.org/W6947763825', 'https://openalex.org/W2152974894', 'https://openalex.org/W156398200', 'https://openalex.org/W2161476805', 'https://openalex.org/W6602386084', 'https://openalex.org/W2964069186', 'https://openalex.org/W2902070858', 'https://openalex.org/W6798679566', 'https://openalex.org/W6936113694', 'https://openalex.org/W2049686551', 'https://openalex.org/W2153425803', 'https://openalex.org/W2471520273', 'https://openalex.org/W6735509194', 'https://openalex.org/W6631190155', 'https://openalex.org/W2886769154', 'https://openalex.org/W6685352114', 'https://openalex.org/W6623517193', 'https://openalex.org/W2157412983', 'https://openalex.org/W2086796102', 'https://openalex.org/W2937020545', 'https://openalex.org/W1509691205', 'https://openalex.org/W2009272428', 'https://openalex.org/W6679436768', 'https://openalex.org/W2157331557', 'https://openalex.org/W6679434410', 'https://openalex.org/W1902237438', 'https://openalex.org/W2897353073', 'https://openalex.org/W2899877258', 'https://openalex.org/W2067234399', 'https://openalex.org/W2123003832', 'https://openalex.org/W2576309025', 'https://openalex.org/W2161135987', 'https://openalex.org/W2120605154', 'https://openalex.org/W4240592325', 'https://openalex.org/W2963290645', 'https://openalex.org/W2126143605', 'https://openalex.org/W2964243274', 'https://openalex.org/W6736996214', 'https://openalex.org/W6735927292', 'https://openalex.org/W2157364932', 'https://openalex.org/W6752888775', 'https://openalex.org/W6748588790', 'https://openalex.org/W2327501763', 'https://openalex.org/W6748573829']",2019-12-19
https://openalex.org/W2897353073,https://doi.org/10.1109/taslp.2019.2892235,Sequence-to-Sequence Acoustic Modeling for Voice Conversion,"In this paper, a neural network named Sequence-to-sequence ConvErsion NeTwork (SCENT) is presented for acoustic modeling in voice conversion. At training stage, a SCENT model is estimated by aligning the feature sequences of source and target speakers implicitly using attention mechanism. At conversion stage, acoustic features and durations of source utterances are converted simultaneously using the unified acoustic model. Mel-scale spectrograms are adopted as acoustic features which contain both excitation and vocal tract descriptions of speech signals. The bottleneck features extracted from source speech using an automatic speech recognition (ASR) model are appended as auxiliary input. A WaveNet vocoder conditioned on Mel-spectrograms is built to reconstruct waveforms from the outputs of the SCENT model. It is worth noting that our proposed method can achieve appropriate duration conversion which is difficult in conventional methods. Experimental results show that our proposed method obtained better objective and subjective performance than the baseline methods using Gaussian mixture models (GMM) and deep neural networks (DNN) as acoustic models. This proposed method also outperformed our previous work which achieved the top rank in Voice Conversion Challenge 2018. Ablation tests further confirmed the effectiveness of several components in our proposed method.","['https://openalex.org/W2963522141', 'https://openalex.org/W2049686551', 'https://openalex.org/W6736811364', 'https://openalex.org/W6742837257', 'https://openalex.org/W6745569068', 'https://openalex.org/W2471520273', 'https://openalex.org/W2891813127', 'https://openalex.org/W6742995815', 'https://openalex.org/W6752888775', 'https://openalex.org/W2767052532', 'https://openalex.org/W6748573829', 'https://openalex.org/W4245885054', 'https://openalex.org/W2100819376', 'https://openalex.org/W1902237438', 'https://openalex.org/W2963830550', 'https://openalex.org/W2889329491', 'https://openalex.org/W6736996214', 'https://openalex.org/W6748588790', 'https://openalex.org/W6745697700', 'https://openalex.org/W2964243274', 'https://openalex.org/W6798679566', 'https://openalex.org/W6714142977', 'https://openalex.org/W2020024436', 'https://openalex.org/W6735509194', 'https://openalex.org/W6631309588', 'https://openalex.org/W2153425803', 'https://openalex.org/W6631190155', 'https://openalex.org/W2137983211', 'https://openalex.org/W2052274528', 'https://openalex.org/W2803595463', 'https://openalex.org/W2086796102', 'https://openalex.org/W1509691205', 'https://openalex.org/W2009272428', 'https://openalex.org/W2603810172', 'https://openalex.org/W2576309025', 'https://openalex.org/W6679436768', 'https://openalex.org/W2157331557', 'https://openalex.org/W6679434410', 'https://openalex.org/W2067234399', 'https://openalex.org/W2123003832', 'https://openalex.org/W2120605154', 'https://openalex.org/W4240592325', 'https://openalex.org/W2157412983', 'https://openalex.org/W2126143605', 'https://openalex.org/W6634817459', 'https://openalex.org/W2886769154', 'https://openalex.org/W1990505856', 'https://openalex.org/W2327501763', 'https://openalex.org/W2064675550', 'https://openalex.org/W6739901393', 'https://openalex.org/W6780226713']",2019-01-10
https://openalex.org/W3096567388,https://doi.org/10.21437/interspeech.2020-1066,Voice Transformer Network: Sequence-to-Sequence Voice Conversion Using Transformer with Text-to-Speech Pretraining,"We introduce a novel sequence-to-sequence (seq2seq) voice conversion (VC) model based on the Transformer architecture with text-to-speech (TTS) pretraining.Seq2seq VC models are attractive owing to their ability to convert prosody.While seq2seq models based on recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have been successfully applied to VC, the use of the Transformer network, which has shown promising results in various speech processing tasks, has not yet been investigated.Nonetheless, their data-hungry property and the mispronunciation of converted speech make seq2seq models far from practical.To this end, we propose a simple yet effective pretraining technique to transfer knowledge from learned TTS models, which benefit from large-scale, easily accessible TTS corpora.VC models initialized with such pretrained model parameters are able to generate effective hidden representations for high-fidelity, highly intelligible converted speech.Experimental results show that such a pretraining scheme can facilitate data-efficient training and outperform an RNN-based seq2seq VC model in terms of intelligibility, naturalness, and similarity.","['https://openalex.org/W2049686551', 'https://openalex.org/W3015338123', 'https://openalex.org/W2972999331', 'https://openalex.org/W2749651610', 'https://openalex.org/W2786868129', 'https://openalex.org/W95152782', 'https://openalex.org/W2903739847', 'https://openalex.org/W2471520273', 'https://openalex.org/W2962780374', 'https://openalex.org/W4289299319', 'https://openalex.org/W2901607128', 'https://openalex.org/W2973142754', 'https://openalex.org/W2898654681', 'https://openalex.org/W2889329491', 'https://openalex.org/W2120605154', 'https://openalex.org/W2959758584', 'https://openalex.org/W2519091744', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2963808252', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963609956', 'https://openalex.org/W2892620417', 'https://openalex.org/W2788357188', 'https://openalex.org/W3099078140', 'https://openalex.org/W2964243274', 'https://openalex.org/W1902237438', 'https://openalex.org/W2767052532', 'https://openalex.org/W1494198834', 'https://openalex.org/W3016160783', 'https://openalex.org/W2972818416', 'https://openalex.org/W2613904329', 'https://openalex.org/W2901254300', 'https://openalex.org/W2808706139', 'https://openalex.org/W2978099976', 'https://openalex.org/W2892009249', 'https://openalex.org/W2972970915', 'https://openalex.org/W2156142001', 'https://openalex.org/W3006777338']",2020-10-25
https://openalex.org/W2972970915,https://doi.org/10.21437/interspeech.2019-1789,Parrotron: An End-to-End Speech-to-Speech Conversion Model and its Applications to Hearing-Impaired Speech and Speech Separation,"We describe Parrotron, an end-to-end-trained speech-to-speech conversion model that maps an input spectrogram directly to another spectrogram, without utilizing any intermediate discrete representation.The network is composed of an encoder, spectrogram and phoneme decoders, followed by a vocoder to synthesize a time-domain waveform.We demonstrate that this model can be trained to normalize speech from any speaker regardless of accent, prosody, and background noise, into the voice of a single canonical target speaker with a fixed accent and consistent articulation and prosody.We further show that this normalization model can be adapted to normalize highly atypical speech from a deaf speaker, resulting in significant improvements in intelligibility and naturalness, measured via a speech recognizer and listening tests.Finally, demonstrating the utility of this model on other speech tasks, we show that the same model architecture can be trained to perform a speech separation task.","['https://openalex.org/W190138384', 'https://openalex.org/W2007908237', 'https://openalex.org/W854541894', 'https://openalex.org/W2013996527', 'https://openalex.org/W46785905', 'https://openalex.org/W1963778986', 'https://openalex.org/W2327501763', 'https://openalex.org/W2221409856', 'https://openalex.org/W4299563208', 'https://openalex.org/W2142300631', 'https://openalex.org/W2962824709', 'https://openalex.org/W2963609956', 'https://openalex.org/W2963011080', 'https://openalex.org/W2465174275', 'https://openalex.org/W2114543868', 'https://openalex.org/W2120847449', 'https://openalex.org/W2964243274', 'https://openalex.org/W2808706139', 'https://openalex.org/W1836465849', 'https://openalex.org/W4289299319', 'https://openalex.org/W2530876040', 'https://openalex.org/W2133564696', 'https://openalex.org/W2963902628', 'https://openalex.org/W4294619240', 'https://openalex.org/W2605131327', 'https://openalex.org/W4298580827', 'https://openalex.org/W87473629', 'https://openalex.org/W2017742648', 'https://openalex.org/W1485009520', 'https://openalex.org/W2005768155', 'https://openalex.org/W2131774270']",2019-09-13
https://openalex.org/W2156142001,https://doi.org/10.1109/89.661472,Continuous probabilistic transform for voice conversion,"Voice conversion, as considered in this paper, is defined as modifying the speech signal of one speaker (source speaker) so that it sounds as if it had been pronounced by a different speaker (target speaker). Our contribution includes the design of a new methodology for representing the relationship between two sets of spectral envelopes. The proposed method is based on the use of a Gaussian mixture model of the source speaker spectral envelopes. The conversion itself is represented by a continuous parametric function which takes into account the probabilistic classification provided by the mixture model. The parameters of the conversion function are estimated by least squares optimization on the training data. This conversion method is implemented in the context of the HNM (harmonic+noise model) system, which allows high-quality modifications of speech signals. Compared to earlier methods based on vector quantization, the proposed conversion scheme results in a much better match between the converted envelopes and the target envelopes. Evaluation by objective tests and formal listening tests shows that the proposed transform greatly improves the quality and naturalness of the converted speech signals compared with previous proposed conversion methods.","['https://openalex.org/W2165880886', 'https://openalex.org/W2129967803', 'https://openalex.org/W2110420312', 'https://openalex.org/W1981367467', 'https://openalex.org/W2114180153', 'https://openalex.org/W1925249181', 'https://openalex.org/W2071850795', 'https://openalex.org/W2122159244', 'https://openalex.org/W2013996527', 'https://openalex.org/W2118850452', 'https://openalex.org/W2033516035', 'https://openalex.org/W2518483182', 'https://openalex.org/W2109114466', 'https://openalex.org/W2134383396', 'https://openalex.org/W1992468098', 'https://openalex.org/W4300956553', 'https://openalex.org/W2169325501', 'https://openalex.org/W2130418280', 'https://openalex.org/W2088562567', 'https://openalex.org/W93048866', 'https://openalex.org/W2008957275', 'https://openalex.org/W2088632526', 'https://openalex.org/W334543337', 'https://openalex.org/W2111748458', 'https://openalex.org/W1970763740', 'https://openalex.org/W2136833314', 'https://openalex.org/W7048738093', 'https://openalex.org/W4300515658', 'https://openalex.org/W2053742104', 'https://openalex.org/W2040266692', 'https://openalex.org/W2070513494', 'https://openalex.org/W167578630', 'https://openalex.org/W1963778986', 'https://openalex.org/W6780562095', 'https://openalex.org/W1999405202', 'https://openalex.org/W2110007337', 'https://openalex.org/W174669002', 'https://openalex.org/W95551363', 'https://openalex.org/W2011916518', 'https://openalex.org/W1560013842', 'https://openalex.org/W2041326643', 'https://openalex.org/W3017143921', 'https://openalex.org/W2121407732', 'https://openalex.org/W2049633694', 'https://openalex.org/W2075665712', 'https://openalex.org/W2064844706', 'https://openalex.org/W2170120409', 'https://openalex.org/W2084134149', 'https://openalex.org/W1965392255', 'https://openalex.org/W2155199877', 'https://openalex.org/W3037213840', 'https://openalex.org/W2989457786', 'https://openalex.org/W250076511', 'https://openalex.org/W2016520419']",1998-03-01
https://openalex.org/W3016160783,https://doi.org/10.1109/icassp40776.2020.9053512,"Espnet-TTS: Unified, Reproducible, and Integratable Open Source End-to-End Text-to-Speech Toolkit","This paper introduces a new end-to-end text-to-speech (E2E-TTS) toolkit named ESPnet-TTS, which is an extension of the open-source speech processing toolkit ESPnet. The toolkit supports state-of- the-art E2E-TTS models, including Tacotron 2, Transformer TTS, and FastSpeech, and also provides recipes inspired by the Kaldi automatic speech recognition (ASR) toolkit. The recipes are based on the design unified with the ESPnet ASR recipe, providing high reproducibility. The toolkit also provides pre-trained models and samples of all of the recipes so that users can use it as a baseline. Furthermore, the unified design enables the integration of ASR functions with TTS, e.g., ASR-based objective evaluation and semi- supervised learning with both ASR and TTS models. This paper describes the design of the toolkit and experimental evaluation in comparison with other toolkits. The experimental results show that our models can achieve state-of-the-art performance comparable to the other latest toolkits, resulting in a mean opinion score (MOS) of 4.25 on the LJSpeech dataset. The toolkit is publicly available at https://github.com/espnet/espnet.","['https://openalex.org/W2886769154', 'https://openalex.org/W6623517193', 'https://openalex.org/W6756040250', 'https://openalex.org/W6762242920', 'https://openalex.org/W3015338123', 'https://openalex.org/W2152859600', 'https://openalex.org/W2767052532', 'https://openalex.org/W2890964092', 'https://openalex.org/W2938947737', 'https://openalex.org/W6753186555', 'https://openalex.org/W6903921188', 'https://openalex.org/W2598638573', 'https://openalex.org/W2964060510', 'https://openalex.org/W6756104738', 'https://openalex.org/W2963300588', 'https://openalex.org/W2471520273', 'https://openalex.org/W6629717138', 'https://openalex.org/W2749651610', 'https://openalex.org/W2797310469', 'https://openalex.org/W6748409065', 'https://openalex.org/W6755300632', 'https://openalex.org/W6750489868', 'https://openalex.org/W2962780374', 'https://openalex.org/W2972818416', 'https://openalex.org/W6631362777', 'https://openalex.org/W1990505856', 'https://openalex.org/W6640212811', 'https://openalex.org/W6675380101', 'https://openalex.org/W2111284386', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963609956', 'https://openalex.org/W6754925833', 'https://openalex.org/W6749489859', 'https://openalex.org/W6763832098', 'https://openalex.org/W6745289305', 'https://openalex.org/W6651937441', 'https://openalex.org/W2972359262', 'https://openalex.org/W6767245602', 'https://openalex.org/W6677973343', 'https://openalex.org/W6603838645', 'https://openalex.org/W2963568578', 'https://openalex.org/W854541894', 'https://openalex.org/W2946200149', 'https://openalex.org/W4298580827', 'https://openalex.org/W1924770834', 'https://openalex.org/W2972889948', 'https://openalex.org/W2766109623', 'https://openalex.org/W2899771611', 'https://openalex.org/W3007328579', 'https://openalex.org/W2892140764', 'https://openalex.org/W2765486990', 'https://openalex.org/W2963691546', 'https://openalex.org/W2969049672', 'https://openalex.org/W2883586237', 'https://openalex.org/W2794490148', 'https://openalex.org/W2964307104', 'https://openalex.org/W2970730223', 'https://openalex.org/W1494198834', 'https://openalex.org/W2901739041', 'https://openalex.org/W2005522781', 'https://openalex.org/W4289383906', 'https://openalex.org/W2949382160', 'https://openalex.org/W2945078028', 'https://openalex.org/W2519091744', 'https://openalex.org/W95152782', 'https://openalex.org/W1524333225', 'https://openalex.org/W2102003408', 'https://openalex.org/W2952711665']",2020-04-09
https://openalex.org/W2903739847,https://doi.org/10.1609/aaai.v33i01.33016706,Neural Speech Synthesis with Transformer Network,"Although end-to-end neural text-to-speech (TTS) methods (such as Tacotron2) are proposed and achieve state-of-theart performance, they still suffer from two problems: 1) low efficiency during training and inference; 2) hard to model long dependency using current recurrent neural networks (RNNs). Inspired by the success of Transformer network in neural machine translation (NMT), in this paper, we introduce and adapt the multi-head attention mechanism to replace the RNN structures and also the original attention mechanism in Tacotron2. With the help of multi-head self-attention, the hidden states in the encoder and decoder are constructed in parallel, which improves training efficiency. Meanwhile, any two inputs at different times are connected directly by a self-attention mechanism, which solves the long range dependency problem effectively. Using phoneme sequences as input, our Transformer TTS network generates mel spectrograms, followed by a WaveNet vocoder to output the final audio results. Experiments are conducted to test the efficiency and performance of our new network. For the efficiency, our Transformer TTS network can speed up the training about 4.25 times faster compared with Tacotron2. For the performance, rigorous human tests show that our proposed model achieves state-of-the-art performance (outperforms Tacotron2 with a gap of 0.048) and is very close to human quality (4.39 vs 4.44 in MOS).","['https://openalex.org/W1599623585', 'https://openalex.org/W2148228080', 'https://openalex.org/W2157331557', 'https://openalex.org/W6623517193', 'https://openalex.org/W2120847449', 'https://openalex.org/W6666761814', 'https://openalex.org/W2150658333', 'https://openalex.org/W2587284713', 'https://openalex.org/W6756197946', 'https://openalex.org/W6679436768', 'https://openalex.org/W2766557690', 'https://openalex.org/W2154920538', 'https://openalex.org/W6676641785', 'https://openalex.org/W2168510624', 'https://openalex.org/W1991133427', 'https://openalex.org/W2102003408', 'https://openalex.org/W6679146927', 'https://openalex.org/W2129142580', 'https://openalex.org/W2133564696', 'https://openalex.org/W2962778134', 'https://openalex.org/W2604184139', 'https://openalex.org/W2613904329', 'https://openalex.org/W2963975282', 'https://openalex.org/W2963850025', 'https://openalex.org/W2519091744', 'https://openalex.org/W4301368689', 'https://openalex.org/W4298857617', 'https://openalex.org/W2964243274', 'https://openalex.org/W4294619240', 'https://openalex.org/W2130942839', 'https://openalex.org/W2804078698', 'https://openalex.org/W4385245566', 'https://openalex.org/W2901997113', 'https://openalex.org/W2584032004', 'https://openalex.org/W854541894', 'https://openalex.org/W2111284386']",2019-07-17
https://openalex.org/W2962780374,https://doi.org/10.21437/interspeech.2018-1456,ESPnet: End-to-End Speech Processing Toolkit,"This paper introduces a new open source platform for end-to-end speech processing named ESPnet. ESPnet mainly focuses on end-to-end automatic speech recognition (ASR), and adopts widely-used dynamic neural network toolkits, Chainer and PyTorch, as a main deep learning engine. ESPnet also follows the Kaldi ASR toolkit style for data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments. This paper explains a major architecture of this software platform, several important functionalities, which differentiate ESPnet from other open source ASR toolkits, and experimental results with major ASR benchmarks.",[],2018-08-28
https://openalex.org/W3015338123,https://doi.org/10.1109/icassp40776.2020.9053795,Parallel Wavegan: A Fast Waveform Generation Model Based on Generative Adversarial Networks with Multi-Resolution Spectrogram,"We propose Parallel WaveGAN, a distillation-free, fast, and small-footprint waveform generation method using a generative adversarial network. In the proposed method, a non-autoregressive WaveNet is trained by jointly optimizing multi-resolution spectrogram and adversarial loss functions, which can effectively capture the time-frequency distribution of the realistic speech waveform. As our method does not require density distillation used in the conventional teacher-student framework, the entire model can be easily trained. Furthermore, our model is able to generate high-fidelity speech even with its compact architecture. In particular, the proposed Parallel WaveGAN has only 1.44 M parameters and can generate 24 kHz speech waveform 28.68 times faster than real-time on a single GPU environment. Perceptual listening test results verify that our proposed method achieves 4.16 mean opinion score within a Transformer-based text-to-speech framework, which is comparative to the best distillation-based Parallel WaveNet system.","['https://openalex.org/W6755879856', 'https://openalex.org/W6753855596', 'https://openalex.org/W2972597685', 'https://openalex.org/W6733471323', 'https://openalex.org/W6755257315', 'https://openalex.org/W6739901393', 'https://openalex.org/W2903739847', 'https://openalex.org/W6763832098', 'https://openalex.org/W2972333964', 'https://openalex.org/W2593414223', 'https://openalex.org/W2535388113', 'https://openalex.org/W6767164110', 'https://openalex.org/W2964243274', 'https://openalex.org/W2749651610', 'https://openalex.org/W6754850772', 'https://openalex.org/W2963522141', 'https://openalex.org/W2984862052', 'https://openalex.org/W2786868129', 'https://openalex.org/W2751205669', 'https://openalex.org/W6675380101', 'https://openalex.org/W2769810959', 'https://openalex.org/W2972824008', 'https://openalex.org/W2963341071', 'https://openalex.org/W2748379347', 'https://openalex.org/W1996021349', 'https://openalex.org/W2888169323', 'https://openalex.org/W6695676441', 'https://openalex.org/W2963175743', 'https://openalex.org/W2970730223', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963975282', 'https://openalex.org/W2284050935', 'https://openalex.org/W2102003408', 'https://openalex.org/W2099471712', 'https://openalex.org/W2946200149', 'https://openalex.org/W2963782041', 'https://openalex.org/W2994689640', 'https://openalex.org/W2963685250', 'https://openalex.org/W2895976713', 'https://openalex.org/W2949382160', 'https://openalex.org/W3103913581', 'https://openalex.org/W2894295011', 'https://openalex.org/W2519091744', 'https://openalex.org/W2587284713', 'https://openalex.org/W4297817572', 'https://openalex.org/W4320013936', 'https://openalex.org/W2968917279', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963945466', 'https://openalex.org/W4294619240']",2020-04-09
https://openalex.org/W2892009249,https://doi.org/10.1109/icassp.2018.8462506,Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition,"Recurrent sequence-to-sequence models using encoder-decoder architecture have made great progress in speech recognition task. However, they suffer from the drawback of slow training speed because the internal recurrence limits the training parallelization. In this paper, we present the Speech-Transformer, a no-recurrence sequence-to-sequence model entirely relies on attention mechanisms to learn the positional dependencies, which can be trained faster with more efficiency. We also propose a 2D-Attention mechanism, which can jointly attend to the time and frequency axes of the 2-dimensional speech inputs, thus providing more expressive representations for the Speech-Transformer. Evaluated on the Wall Street Journal (WSJ) speech recognition dataset, our best model achieves competitive word error rate (WER) of 10.9%, while the whole training process only takes 1.2 days on 1 GPU, significantly faster than the published results of recurrent sequence-to-sequence models.","['https://openalex.org/W2735006420', 'https://openalex.org/W2143612262', 'https://openalex.org/W2293634267', 'https://openalex.org/W2396144419', 'https://openalex.org/W2327501763', 'https://openalex.org/W2530876040', 'https://openalex.org/W2526425061', 'https://openalex.org/W2962826786', 'https://openalex.org/W6623517193', 'https://openalex.org/W2125838338', 'https://openalex.org/W6675365184', 'https://openalex.org/W1522301498', 'https://openalex.org/W1924770834', 'https://openalex.org/W854541894', 'https://openalex.org/W2577366047', 'https://openalex.org/W2952288254', 'https://openalex.org/W2605141709', 'https://openalex.org/W2102113734', 'https://openalex.org/W2525778437', 'https://openalex.org/W2963827914', 'https://openalex.org/W1828163288', 'https://openalex.org/W2620290161', 'https://openalex.org/W4385245566']",2018-04-01
https://openalex.org/W95152782,,The CMU Arctic speech databases.,"The CMU Arctic databases designed for the purpose of speech synthesis research. These single speaker speech databases have been carefully recorded under studio conditions and consist of approximately 1200 phonetically balanced English utterances. In addition to wavefiles, the databases provide complete support for the Festival Speech Synthesis System, including pre-built voices that may be used as is. The entire package is distributed as free software, without restriction on commercial or non-commercial use. 1.",['https://openalex.org/W183794703'],2004-01-01
https://openalex.org/W2547875792,https://doi.org/10.48550/arxiv.1611.01144,Categorical Reparameterization with Gumbel-Softmax,"Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.",[],2016-11-03
https://openalex.org/W3097777922,https://doi.org/10.21437/interspeech.2020-3015,Conformer: Convolution-augmented Transformer for Speech Recognition,"Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs).Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively.In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way.To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer.Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies.On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3%without using a language model and 1.9%/3.9%with an external language model on test/testother.We also observe competitive performance of 2.7%/6.3%with a small model of only 10M parameters.","['https://openalex.org/W1964175594', 'https://openalex.org/W2952809536', 'https://openalex.org/W2963542740', 'https://openalex.org/W3015537910', 'https://openalex.org/W1494198834', 'https://openalex.org/W3015194534', 'https://openalex.org/W4287812455', 'https://openalex.org/W2112739286', 'https://openalex.org/W4385245566', 'https://openalex.org/W4309845474', 'https://openalex.org/W3016010032', 'https://openalex.org/W2962760690', 'https://openalex.org/W4295253143', 'https://openalex.org/W1828163288', 'https://openalex.org/W2095705004', 'https://openalex.org/W2948981900', 'https://openalex.org/W2963414781', 'https://openalex.org/W2981413347', 'https://openalex.org/W2892009249', 'https://openalex.org/W3015995734', 'https://openalex.org/W1522301498', 'https://openalex.org/W2899423466', 'https://openalex.org/W2936774411', 'https://openalex.org/W4320930577', 'https://openalex.org/W3095173472', 'https://openalex.org/W2928941594', 'https://openalex.org/W2567070169', 'https://openalex.org/W2972818416', 'https://openalex.org/W2973215447', 'https://openalex.org/W1995562189', 'https://openalex.org/W2981857663', 'https://openalex.org/W2979636403', 'https://openalex.org/W2964110616']",2020-10-25
https://openalex.org/W3095173472,https://doi.org/10.21437/interspeech.2020-2059,ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context,"Convolutional neural networks (CNN) have shown promising results for end-to-end speech recognition, albeit still behind RNN/transformer based models in performance.In this paper, we study how to bridge this gap and go beyond with a novel CNN-RNN-transducer architecture, which we call ContextNet.ContextNet features a fully convolutional encoder that incorporates global context information into convolution layers by adding squeeze-and-excitation modules.In addition, we propose a simple scaling method that scales the widths of Con-textNet that achieves good trade-off between computation and accuracy.We demonstrate that on the widely used Librispeech benchmark, ContextNet achieves a word error rate (WER) of 2.1%/4.6%without external language model (LM), 1.9%/4.1% with LM and 2.9%/7.0%with only 10M parameters on the clean/noisy LibriSpeech test sets.This compares to the best previously published model of 2.0%/4.6%with LM and 3.9%/11.3%with 20M parameters.The superiority of the proposed ContextNet model is also verified on a much larger internal dataset.","['https://openalex.org/W4286800115', 'https://openalex.org/W2033256038', 'https://openalex.org/W3015537910', 'https://openalex.org/W2962760690', 'https://openalex.org/W2164240571', 'https://openalex.org/W2531409750', 'https://openalex.org/W2530876040', 'https://openalex.org/W2112021726', 'https://openalex.org/W2936774411', 'https://openalex.org/W3008191852', 'https://openalex.org/W2963163009', 'https://openalex.org/W2981857663', 'https://openalex.org/W2972818416', 'https://openalex.org/W1836465849', 'https://openalex.org/W4320930577', 'https://openalex.org/W1494198834', 'https://openalex.org/W3149629662', 'https://openalex.org/W2904818793', 'https://openalex.org/W3016010032', 'https://openalex.org/W2194775991', 'https://openalex.org/W2288817436', 'https://openalex.org/W3007904524', 'https://openalex.org/W1522301498', 'https://openalex.org/W4385245566', 'https://openalex.org/W2928941594', 'https://openalex.org/W2963414781', 'https://openalex.org/W3015995734', 'https://openalex.org/W2079623482', 'https://openalex.org/W1828163288', 'https://openalex.org/W2973215447', 'https://openalex.org/W2898732869', 'https://openalex.org/W4309845474', 'https://openalex.org/W3015194534']",2020-10-25
https://openalex.org/W2914417638,https://doi.org/10.1109/slt.2018.8639513,End-To-End Named Entity And Semantic Concept Extraction From Speech,"Named entity recognition (NER) is among SLU tasks that usually extract semantic information from textual documents. Until now, NER from speech is made through a pipeline process that consists in processing first an automatic speech recognition (ASR) on the audio and then processing a NER on the ASR outputs. Such approach has some disadvantages (error propagation, metric to tune ASR systems sub-optimal in regards to the final task, reduced space search at the ASR output level,...) and it is known that more integrated approaches outperform sequential ones, when they can be applied. In this paper, we explore an end-to-end approach that directly extracts named entities from speech, though a unique neural architecture. On a such way, a joint optimization is possible for both ASR and NER. Experiments are carried on French data easily accessible, composed of data distributed in several evaluation campaigns. The results are promising since this end-to-end approach provides similar results (F-measure=0.66 on test data) than a classical pipeline approach to detect named entity categories (F-measure=0.64). Last, we also explore this approach applied to semantic concept extraction, through a slot filling task known as a spoken language understanding problem, and also observe an improvement in comparison to a pipeline approach.","['https://openalex.org/W2127141656', 'https://openalex.org/W1710082047', 'https://openalex.org/W6683738474', 'https://openalex.org/W6685810106', 'https://openalex.org/W2296283641', 'https://openalex.org/W2962902328', 'https://openalex.org/W2033357968', 'https://openalex.org/W6636392717', 'https://openalex.org/W6647266308', 'https://openalex.org/W6606699392', 'https://openalex.org/W2618015847', 'https://openalex.org/W2007261869', 'https://openalex.org/W6751431506', 'https://openalex.org/W2748978660', 'https://openalex.org/W6712344656', 'https://openalex.org/W6606949179', 'https://openalex.org/W6687566353', 'https://openalex.org/W2963288440', 'https://openalex.org/W6751635672', 'https://openalex.org/W6625097472', 'https://openalex.org/W6602944679', 'https://openalex.org/W2150859660', 'https://openalex.org/W6682042142', 'https://openalex.org/W6608208975', 'https://openalex.org/W21503661', 'https://openalex.org/W6628258601', 'https://openalex.org/W6691675912', 'https://openalex.org/W2193413348', 'https://openalex.org/W1623072288', 'https://openalex.org/W2805328873', 'https://openalex.org/W72302491', 'https://openalex.org/W2149847229', 'https://openalex.org/W2803211626', 'https://openalex.org/W2951562155', 'https://openalex.org/W1988596541', 'https://openalex.org/W204182981', 'https://openalex.org/W2251580867', 'https://openalex.org/W947855104', 'https://openalex.org/W169943709', 'https://openalex.org/W1419993066', 'https://openalex.org/W163811496', 'https://openalex.org/W2902290872', 'https://openalex.org/W2397495166', 'https://openalex.org/W2158899491', 'https://openalex.org/W2952230511', 'https://openalex.org/W2963625095']",2018-12-01
https://openalex.org/W3198299542,https://doi.org/10.21437/interspeech.2021-1912,Large-Scale Self- and Semi-Supervised Learning for Speech Translation,"In this paper, we improve speech translation (ST) through effectively leveraging large quantities of unlabeled speech and text data in different and complementary ways. We explore both pretraining and self-training by using the large Libri-Light speech audio corpus and language modeling with CommonCrawl. Our experiments improve over the previous state of the art by 2.6 BLEU on average on all four considered CoVoST 2 language pairs via a simple recipe of combining wav2vec 2.0 pretraining, a single iteration of self-training and decoding with a language model. Different to existing work, our approach does not leverage any other supervision than ST data. Code and models will be publicly released.",[],2021-08-27
https://openalex.org/W2327501763,https://doi.org/10.1109/icassp.2016.7472621,"Listen, attend and spell: A neural network for large vocabulary conversational speech recognition","We present Listen, Attend and Spell (LAS), a neural speech recognizer that transcribes speech utterances directly to characters without pronunciation models, HMMs or other components of traditional speech recognizers. In LAS, the neural network architecture subsumes the acoustic, pronunciation and language models making it not only an end-to-end trained system but an end-to-end model. In contrast to DNN-HMM, CTC and most other models, LAS makes no independence assumptions about the probability distribution of the output character sequences given the acoustic sequence. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits each character conditioned on all previous characters, and the entire acoustic sequence. On a Google voice search task, LAS achieves a WER of 14.1% without a dictionary or an external language model and 10.3% with language model rescoring over the top 32 beams. In comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0% on the same set.","['https://openalex.org/W6679434410', 'https://openalex.org/W6635078382', 'https://openalex.org/W6623517193', 'https://openalex.org/W6630875275', 'https://openalex.org/W6676562027', 'https://openalex.org/W2064675550', 'https://openalex.org/W2005708641', 'https://openalex.org/W6674758992', 'https://openalex.org/W6680587008', 'https://openalex.org/W2112796928', 'https://openalex.org/W6679429981', 'https://openalex.org/W6629052376', 'https://openalex.org/W2160815625', 'https://openalex.org/W2963211739', 'https://openalex.org/W6696982659', 'https://openalex.org/W6679436768', 'https://openalex.org/W2399086392', 'https://openalex.org/W6675365184', 'https://openalex.org/W2157331557', 'https://openalex.org/W2143612262', 'https://openalex.org/W6621543089', 'https://openalex.org/W6635768407', 'https://openalex.org/W6631362777', 'https://openalex.org/W6640090968', 'https://openalex.org/W6684859321', 'https://openalex.org/W1533416326', 'https://openalex.org/W2293858598', 'https://openalex.org/W2963920996', 'https://openalex.org/W1922655562', 'https://openalex.org/W854541894', 'https://openalex.org/W2109886035', 'https://openalex.org/W2130942839', 'https://openalex.org/W1600744878', 'https://openalex.org/W1484868577', 'https://openalex.org/W1586532344', 'https://openalex.org/W2168231600', 'https://openalex.org/W1514535095', 'https://openalex.org/W2964308564', 'https://openalex.org/W2133564696', 'https://openalex.org/W2131342762', 'https://openalex.org/W2293009711', 'https://openalex.org/W2099257174', 'https://openalex.org/W2102113734', 'https://openalex.org/W2504108613', 'https://openalex.org/W648786980', 'https://openalex.org/W2138660131', 'https://openalex.org/W1524333225', 'https://openalex.org/W2962826786']",2016-03-01
https://openalex.org/W4226278833,https://doi.org/10.21437/interspeech.2022-10368,Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data,"This paper studies a novel pre-training technique with unpaired speech data, Speech2C, for encoder-decoder based automatic speech recognition (ASR). Within a multi-task learning framework, we introduce two pre-training tasks for the encoder-decoder network using acoustic units, i.e., pseudo codes, derived from an offline clustering model. One is to predict the pseudo codes via masked language modeling in encoder output, like HuBERT model, while the other lets the decoder learn to reconstruct pseudo codes autoregressively instead of generating textual scripts. In this way, the decoder learns to reconstruct original speech information with codes before learning to generate correct text. Comprehensive experiments on the LibriSpeech corpus show that the proposed Speech2C can relatively reduce the word error rate (WER) by 19.2% over the method without decoder pre-training, and also outperforms significantly the state-of-the-art wav2vec 2.0 and HuBERT on fine-tuning subsets of 10h and 100h. We release our code and model at https://github.com/microsoft/SpeechT5/tree/main/Speech2C.",[],2022-09-16
https://openalex.org/W3001434439,https://doi.org/10.1162/tacl_a_00343,Multilingual Denoising Pre-training for Neural Machine Translation,"This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART—a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019 ). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, whereas previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine-tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task- specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show that it enables transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training. 1","['https://openalex.org/W2888536529', 'https://openalex.org/W2971120622', 'https://openalex.org/W2550821151', 'https://openalex.org/W2963250244', 'https://openalex.org/W2891534142', 'https://openalex.org/W2963216553', 'https://openalex.org/W2962802109', 'https://openalex.org/W2963842551', 'https://openalex.org/W2988937804', 'https://openalex.org/W2989539713', 'https://openalex.org/W2766182427', 'https://openalex.org/W2905927205', 'https://openalex.org/W2126725946', 'https://openalex.org/W2933138175', 'https://openalex.org/W2970597249', 'https://openalex.org/W2987998469', 'https://openalex.org/W2101105183', 'https://openalex.org/W2945260553', 'https://openalex.org/W2891068404', 'https://openalex.org/W2982399380', 'https://openalex.org/W398859631', 'https://openalex.org/W2963993537', 'https://openalex.org/W2948735718', 'https://openalex.org/W2798931235', 'https://openalex.org/W2919290281', 'https://openalex.org/W2905027511', 'https://openalex.org/W2982180741', 'https://openalex.org/W2913659301', 'https://openalex.org/W2914120296', 'https://openalex.org/W2983040767', 'https://openalex.org/W2962739339', 'https://openalex.org/W2981852735', 'https://openalex.org/W2944815030', 'https://openalex.org/W2607303097', 'https://openalex.org/W2963403868', 'https://openalex.org/W2948697567', 'https://openalex.org/W2963532001', 'https://openalex.org/W2965373594', 'https://openalex.org/W2951560313', 'https://openalex.org/W2951184134', 'https://openalex.org/W2964093087', 'https://openalex.org/W2988739750', 'https://openalex.org/W2963602293', 'https://openalex.org/W2906155634', 'https://openalex.org/W2963247703', 'https://openalex.org/W2891555348', 'https://openalex.org/W2973049837', 'https://openalex.org/W2969740599', 'https://openalex.org/W3037026762', 'https://openalex.org/W2958953787', 'https://openalex.org/W2963331137', 'https://openalex.org/W2962801832', 'https://openalex.org/W4238634189', 'https://openalex.org/W2963341956', 'https://openalex.org/W2953109491', 'https://openalex.org/W2184135559', 'https://openalex.org/W2612690371']",2020-11-25
https://openalex.org/W3173767661,https://doi.org/10.18653/v1/2021.acl-long.68,Multilingual Speech Translation from Efficient Finetuning of Pretrained Models,"Xian Li, Changhan Wang, Yun Tang, Chau Tran, Yuqing Tang, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.","['https://openalex.org/W2933138175', 'https://openalex.org/W3092424727', 'https://openalex.org/W3082274269', 'https://openalex.org/W3102342027', 'https://openalex.org/W3034999214', 'https://openalex.org/W3035202887', 'https://openalex.org/W2963779652', 'https://openalex.org/W3118578889', 'https://openalex.org/W2970597249', 'https://openalex.org/W2971274815', 'https://openalex.org/W3016181583', 'https://openalex.org/W3046368065', 'https://openalex.org/W3099782249', 'https://openalex.org/W2983040767', 'https://openalex.org/W2973049979', 'https://openalex.org/W2964303773', 'https://openalex.org/W2966715458', 'https://openalex.org/W3037542581', 'https://openalex.org/W2989195139', 'https://openalex.org/W2972448360', 'https://openalex.org/W4287694131', 'https://openalex.org/W3034571331', 'https://openalex.org/W3037057938', 'https://openalex.org/W2963460174', 'https://openalex.org/W2981851019', 'https://openalex.org/W4288089799', 'https://openalex.org/W3036601975', 'https://openalex.org/W2970854433', 'https://openalex.org/W3007142233', 'https://openalex.org/W3015698636', 'https://openalex.org/W4288026527', 'https://openalex.org/W3015633994', 'https://openalex.org/W2945260553', 'https://openalex.org/W3198429080', 'https://openalex.org/W2963341956', 'https://openalex.org/W2970608575', 'https://openalex.org/W3023622314', 'https://openalex.org/W2896457183', 'https://openalex.org/W2964161387', 'https://openalex.org/W3008125272', 'https://openalex.org/W2914120296', 'https://openalex.org/W3176711365', 'https://openalex.org/W2997436923', 'https://openalex.org/W2969876226', 'https://openalex.org/W4287824654', 'https://openalex.org/W3153805297', 'https://openalex.org/W2970120757', 'https://openalex.org/W2964172053', 'https://openalex.org/W3107826490', 'https://openalex.org/W3101498587', 'https://openalex.org/W2593011301', 'https://openalex.org/W3037465386', 'https://openalex.org/W3035390927', 'https://openalex.org/W2963532001', 'https://openalex.org/W2936848022', 'https://openalex.org/W3054645415', 'https://openalex.org/W2949328740', 'https://openalex.org/W2958953787', 'https://openalex.org/W3162037819']",2021-01-01
https://openalex.org/W1828163288,https://doi.org/10.48550/arxiv.1211.3711,Sequence Transduction with Recurrent Neural Networks,"Many machine learning tasks can be expressed as the transformation---or \emph{transduction}---of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few. One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions such as shrinking, stretching and translating. Recurrent neural networks (RNNs) are a powerful sequence learning architecture that has proven capable of learning such representations. However RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transduction. This is a severe limitation since \emph{finding} the alignment is the most difficult aspect of many sequence transduction problems. Indeed, even determining the length of the output sequence is often challenging. This paper introduces an end-to-end, probabilistic sequence transduction system, based entirely on RNNs, that is in principle able to transform any input sequence into any finite, discrete output sequence. Experimental results for phoneme recognition are provided on the TIMIT speech corpus.","['https://openalex.org/W2170942820', 'https://openalex.org/W2131774270', 'https://openalex.org/W2064675550', 'https://openalex.org/W2147880316', 'https://openalex.org/W1964175594', 'https://openalex.org/W179875071', 'https://openalex.org/W1902568950', 'https://openalex.org/W2127141656', 'https://openalex.org/W2131283257', 'https://openalex.org/W2077804127', 'https://openalex.org/W3023071679', 'https://openalex.org/W2112796928', 'https://openalex.org/W196214544', 'https://openalex.org/W1525783482', 'https://openalex.org/W2167898728', 'https://openalex.org/W2079735306', 'https://openalex.org/W1674799117', 'https://openalex.org/W2103359087', 'https://openalex.org/W3127686677', 'https://openalex.org/W2053831280']",2012-11-14
https://openalex.org/W4221145109,https://doi.org/10.48550/arxiv.2202.03555,"data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language","While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches.",[],2022-02-07
https://openalex.org/W2520160253,https://doi.org/10.48550/arxiv.1609.03193,Wav2Letter: an End-to-End ConvNet-based Speech Recognition System,"This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC while being simpler. We show competitive results in word error rate on the Librispeech corpus with MFCC features, and promising results from raw waveform.","['https://openalex.org/W2147880316', 'https://openalex.org/W2028706510', 'https://openalex.org/W1902568950', 'https://openalex.org/W2160815625', 'https://openalex.org/W2402146185', 'https://openalex.org/W1736701665', 'https://openalex.org/W2898360541', 'https://openalex.org/W2193413348', 'https://openalex.org/W15592790', 'https://openalex.org/W1666984270', 'https://openalex.org/W2950051476', 'https://openalex.org/W2963175699', 'https://openalex.org/W2949640717', 'https://openalex.org/W38527073', 'https://openalex.org/W1993882792', 'https://openalex.org/W116332155', 'https://openalex.org/W1494198834', 'https://openalex.org/W2127141656', 'https://openalex.org/W2143612262', 'https://openalex.org/W2079623482', 'https://openalex.org/W1877570817', 'https://openalex.org/W2398826216', 'https://openalex.org/W2916986993', 'https://openalex.org/W1922655562']",2016-09-11
https://openalex.org/W3054645415,https://doi.org/10.48550/arxiv.2007.10310,CoVoST 2 and Massively Multilingual Speech-to-Text Translation,"Speech translation has recently become an increasingly popular topic of research, partly due to the development of benchmark datasets. Nevertheless, current datasets cover a limited number of languages. With the aim to foster research in massive multilingual speech translation and speech translation for low resource language pairs, we release CoVoST 2, a large-scale multilingual speech translation corpus covering translations from 21 languages into English and from English into 15 languages. This represents the largest open dataset available to date from total volume and language coverage perspective. Data sanity checks provide evidence about the quality of the data, which is released under CC0 license. We also provide extensive speech recognition, bilingual and multilingual machine translation and speech translation baselines with open-source implementation.","['https://openalex.org/W2327501763', 'https://openalex.org/W3008549139', 'https://openalex.org/W3092424727', 'https://openalex.org/W1524333225', 'https://openalex.org/W2933138175', 'https://openalex.org/W3103029570', 'https://openalex.org/W2962735107', 'https://openalex.org/W2101105183', 'https://openalex.org/W3015698636', 'https://openalex.org/W2963532001', 'https://openalex.org/W2945700568', 'https://openalex.org/W3008125272', 'https://openalex.org/W2963403868', 'https://openalex.org/W2970295111', 'https://openalex.org/W3101860695', 'https://openalex.org/W2962784628', 'https://openalex.org/W2991213871', 'https://openalex.org/W2582956876', 'https://openalex.org/W2964308564', 'https://openalex.org/W2963250244', 'https://openalex.org/W2958953787', 'https://openalex.org/W2936774411']",2020-07-20
https://openalex.org/W4288089799,https://doi.org/10.48550/arxiv.1910.10683,Exploring the Limits of Transfer Learning with a Unified Text-to-Text\n Transformer,"Transfer learning, where a model is first pre-trained on a data-rich task\nbefore being fine-tuned on a downstream task, has emerged as a powerful\ntechnique in natural language processing (NLP). The effectiveness of transfer\nlearning has given rise to a diversity of approaches, methodology, and\npractice. In this paper, we explore the landscape of transfer learning\ntechniques for NLP by introducing a unified framework that converts all\ntext-based language problems into a text-to-text format. Our systematic study\ncompares pre-training objectives, architectures, unlabeled data sets, transfer\napproaches, and other factors on dozens of language understanding tasks. By\ncombining the insights from our exploration with scale and our new ``Colossal\nClean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks\ncovering summarization, question answering, text classification, and more. To\nfacilitate future work on transfer learning for NLP, we release our data set,\npre-trained models, and code.\n",[],2019-10-23
https://openalex.org/W2073459066,https://doi.org/10.5555/1283383.1283494,k-means++: the advantages of careful seeding,"The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is Θ(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.","['https://openalex.org/W2028761382', 'https://openalex.org/W1998325344', 'https://openalex.org/W2091283109', 'https://openalex.org/W2045964207', 'https://openalex.org/W2033387072', 'https://openalex.org/W1501500081', 'https://openalex.org/W1970866964', 'https://openalex.org/W1529871791', 'https://openalex.org/W2110105238', 'https://openalex.org/W2151242668', 'https://openalex.org/W1998739300', 'https://openalex.org/W2091684877', 'https://openalex.org/W2004791924', 'https://openalex.org/W1489597983', 'https://openalex.org/W2123297508', 'https://openalex.org/W2199495299', 'https://openalex.org/W2058295780', 'https://openalex.org/W1998905999', 'https://openalex.org/W2150593711', 'https://openalex.org/W2050761777', 'https://openalex.org/W2118858274']",2007-01-07
https://openalex.org/W2549416390,https://doi.org/10.48550/arxiv.1611.01462,Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,"Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.","['https://openalex.org/W1821462560', 'https://openalex.org/W2950133940', 'https://openalex.org/W2473934411', 'https://openalex.org/W2951798229', 'https://openalex.org/W1591801644', 'https://openalex.org/W2250539671', 'https://openalex.org/W2229833550', 'https://openalex.org/W2172140247', 'https://openalex.org/W2514713644', 'https://openalex.org/W1843891098', 'https://openalex.org/W2525332836', 'https://openalex.org/W2064675550', 'https://openalex.org/W2212703438', 'https://openalex.org/W2140679639', 'https://openalex.org/W1632114991', 'https://openalex.org/W2251939518', 'https://openalex.org/W2951559648']",2016-11-04
https://openalex.org/W4394666973,https://doi.org/10.5281/zenodo.17851047,Neuromodulatory Control Networks (NCNs): A Biologically Inspired Architecture for Dynamic LLM Processing,"Large Language Models (LLMs) based on the Transformer architecture have achieved remarkable success, yet their core processing mechanisms remain largely static after training. While powerful, this static nature limits their ability to dynamically adapt their processing strategy based on nuanced contextual cues, task demands, or desired operational modes (e.g., shifting between exploration and exploitation). We propose Neuromodulatory Control Networks (NCNs), a novel architectural modification inspired by the neuromodulatory systems in the vertebrate brain (e.g., those utilizing dopamine, acetylcholine, norepinephrine). NCNs are small, parallel networks that receive contextual input, summarizing the global state, task information, or external control signals, and compute dynamic ""modulatory signals"". These signals are distributed as layer-specific control vectors to the main LLM to influence its computational properties during a forward pass, analogous to how neuromodulators alter neuronal gain, plasticity, and network states across different cortical depths. Instead of merely routing information, NCNs aim to change how information is processed throughout the base model by modulating key components like attention mechanisms (e.g., via precision scaling), layer gains, and activation functions. Crucially, the architecture allows the model to implicitly learn to self-regulate these parameters via backpropagation, effectively becoming its own ""tuning expert."" We further introduce formal stability mechanisms, including homeostatic regularization, to prevent control manifold collapse. This paper introduces the NCN architecture, details its components and implicit learning mechanism, discusses its conceptual advantages and potential failure modes (such as contextual stereotyping), and provides an open-source PyTorch implementation to facilitate community exploration and future empirical validation.",[],2025-04-25
https://openalex.org/W2752047430,,Deliberation Networks: Sequence Generation Beyond One-Pass Decoding,"The encoder-decoder framework has achieved promising progress for many sequence generation tasks, including machine translation, text summarization, dialog system, image captioning, etc. Such a framework adopts an one-pass forward process while decoding and generating a sequence, but lacks the deliberation process: A generated sequence is directly used as final output without further polishing. However, deliberation is a common behavior in human's daily life like reading news and writing papers/articles/books. In this work, we introduce the deliberation process into the encoder-decoder framework and propose deliberation networks for sequence generation. A deliberation network has two levels of decoders, where the first-pass decoder generates a raw sequence and the second-pass decoder polishes and refines the raw sentence with deliberation. Since the second-pass deliberation decoder has global information about what the sequence to be generated might be, it has the potential to generate a better sequence by looking into future words in the raw sentence. Experiments on neural machine translation and text summarization demonstrate the effectiveness of the proposed deliberation networks. On the WMT 2014 English-to-French translation task, our model establishes a new state-of-the-art BLEU score of 41.5.","['https://openalex.org/W1895577753', 'https://openalex.org/W2751527518', 'https://openalex.org/W2962965405', 'https://openalex.org/W2964308564', 'https://openalex.org/W2963774520', 'https://openalex.org/W2963551569', 'https://openalex.org/W2525778437', 'https://openalex.org/W2064675550', 'https://openalex.org/W1514535095', 'https://openalex.org/W2130942839', 'https://openalex.org/W2546938941', 'https://openalex.org/W2962780935', 'https://openalex.org/W2575583396', 'https://openalex.org/W2194775991', 'https://openalex.org/W2963620441', 'https://openalex.org/W2100664567', 'https://openalex.org/W2099471712', 'https://openalex.org/W2739810937', 'https://openalex.org/W2302255633', 'https://openalex.org/W2964049455', 'https://openalex.org/W2384495648', 'https://openalex.org/W2963403868', 'https://openalex.org/W2101105183', 'https://openalex.org/W2779385623', 'https://openalex.org/W2964265128', 'https://openalex.org/W2157331557', 'https://openalex.org/W2740707878']",2017-12-04
https://openalex.org/W3186200218,https://doi.org/10.18653/v1/2021.iwslt-1.1,FINDINGS OF THE IWSLT 2021 EVALUATION CAMPAIGN,"Antonios Anastasopoulos, Ondřej Bojar, Jacob Bremerman, Roldano Cattoni, Maha Elbayad, Marcello Federico, Xutai Ma, Satoshi Nakamura, Matteo Negri, Jan Niehues, Juan Pino, Elizabeth Salesky, Sebastian Stüker, Katsuhito Sudoh, Marco Turchi, Alexander Waibel, Changhan Wang, Matthew Wiesner. Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021). 2021.","['https://openalex.org/W2933138175', 'https://openalex.org/W2939710050', 'https://openalex.org/W2101105183', 'https://openalex.org/W2945700568', 'https://openalex.org/W3185028872', 'https://openalex.org/W3101648800', 'https://openalex.org/W2970925270', 'https://openalex.org/W3184159110', 'https://openalex.org/W3037542581', 'https://openalex.org/W3186312705', 'https://openalex.org/W2140818133', 'https://openalex.org/W3030437843', 'https://openalex.org/W2512848817', 'https://openalex.org/W2917807695', 'https://openalex.org/W4288345582', 'https://openalex.org/W3185037272', 'https://openalex.org/W2916456038', 'https://openalex.org/W3119308075', 'https://openalex.org/W3037625699', 'https://openalex.org/W3186761682', 'https://openalex.org/W2785350307', 'https://openalex.org/W2799800213', 'https://openalex.org/W3184912720', 'https://openalex.org/W3183514466', 'https://openalex.org/W4287114223', 'https://openalex.org/W3197771105', 'https://openalex.org/W3037187981', 'https://openalex.org/W3162157691', 'https://openalex.org/W3029944303', 'https://openalex.org/W2963403868', 'https://openalex.org/W2251321385', 'https://openalex.org/W3183454191', 'https://openalex.org/W2948981900', 'https://openalex.org/W3186513069', 'https://openalex.org/W3183897008', 'https://openalex.org/W2936774411', 'https://openalex.org/W3176382501', 'https://openalex.org/W3037465386', 'https://openalex.org/W2136630431', 'https://openalex.org/W2915395439', 'https://openalex.org/W3041866211', 'https://openalex.org/W1494198834', 'https://openalex.org/W3173767661', 'https://openalex.org/W2963532001', 'https://openalex.org/W2806412155', 'https://openalex.org/W2184135559', 'https://openalex.org/W3101727459', 'https://openalex.org/W2955541912', 'https://openalex.org/W3127012371', 'https://openalex.org/W3186921989', 'https://openalex.org/W3032433061', 'https://openalex.org/W2946888380', 'https://openalex.org/W3005389111', 'https://openalex.org/W3183880494', 'https://openalex.org/W2953830716', 'https://openalex.org/W3126420472', 'https://openalex.org/W3100370880', 'https://openalex.org/W3183341856', 'https://openalex.org/W3157233339', 'https://openalex.org/W3204130541', 'https://openalex.org/W2917604489', 'https://openalex.org/W2899274165', 'https://openalex.org/W3184309109', 'https://openalex.org/W2133564696', 'https://openalex.org/W3174446152', 'https://openalex.org/W3109864162', 'https://openalex.org/W3202218022', 'https://openalex.org/W3107826490', 'https://openalex.org/W3143377973', 'https://openalex.org/W2250875036', 'https://openalex.org/W3112092703', 'https://openalex.org/W2401271873', 'https://openalex.org/W4226513777', 'https://openalex.org/W3094903517', 'https://openalex.org/W3036601975', 'https://openalex.org/W3185041190', 'https://openalex.org/W2973049979', 'https://openalex.org/W3091991378', 'https://openalex.org/W3185743763', 'https://openalex.org/W3136598139', 'https://openalex.org/W3198587774', 'https://openalex.org/W3115075512', 'https://openalex.org/W3184037296', 'https://openalex.org/W2905927205', 'https://openalex.org/W4385245566', 'https://openalex.org/W3015703505', 'https://openalex.org/W4288817190', 'https://openalex.org/W2291598608', 'https://openalex.org/W3186672448', 'https://openalex.org/W3099757670', 'https://openalex.org/W3096686110', 'https://openalex.org/W3162037819', 'https://openalex.org/W2970279348', 'https://openalex.org/W2149327368', 'https://openalex.org/W3120929527', 'https://openalex.org/W3163239510', 'https://openalex.org/W3097030750', 'https://openalex.org/W3099782249', 'https://openalex.org/W3185995032', 'https://openalex.org/W2972451902', 'https://openalex.org/W3092085609', 'https://openalex.org/W2099655049', 'https://openalex.org/W3015698636', 'https://openalex.org/W3097777922', 'https://openalex.org/W2943493972', 'https://openalex.org/W2963925437', 'https://openalex.org/W2917668649']",2021-01-01
https://openalex.org/W4307770080,https://doi.org/10.48550/arxiv.2205.08993,Leveraging Pseudo-labeled Data to Improve Direct Speech-to-Speech Translation,"Direct Speech-to-speech translation (S2ST) has drawn more and more attention recently. The task is very challenging due to data scarcity and complex speech-to-speech mapping. In this paper, we report our recent achievements in S2ST. Firstly, we build a S2ST Transformer baseline which outperforms the original Translatotron. Secondly, we utilize the external data by pseudo-labeling and obtain a new state-of-the-art result on the Fisher English-to-Spanish test set. Indeed, we exploit the pseudo data with a combination of popular techniques which are not trivial when applied to S2ST. Moreover, we evaluate our approach on both syntactically similar (Spanish-English) and distant (English-Chinese) language pairs. Our implementation is available at https://github.com/fengpeng-yue/speech-to-speech-translation.",[],2022-05-18
https://openalex.org/W2292087804,,Speech recognition and keyword spotting for low-resource languages : Babel project research at CUED,"Recently there has been increased interest in Automatic Speech Recognition (ASR) and Key Word Spotting (KWS) systems for low resource languages. One of the driving forces for this research di-rection is the IARPA Babel project. This paper describes some of the research funded by this project at Cambridge University, as part of the Lorelei team co-ordinated by IBM. A range of topics are dis-cussed including: deep neural network based acoustic models; data augmentation; and zero acoustic model resource systems. Perfor-mance for all approaches is evaluated using the Limited (approx-imately 10 hours) and/or Full (approximately 80 hours) language packs distributed by IARPA. Both KWS and ASR performance fig-ures are given. Though absolute performance varies from language to language, and keyword list, the approaches described show con-sistent trends over the languages investigated to date. Using com-parable systems over the five Option Period 1 languages indicates a strong correlation between ASR performance and KWS perfor-mance.","['https://openalex.org/W2099621636', 'https://openalex.org/W2028956843', 'https://openalex.org/W2165921245', 'https://openalex.org/W2039285212', 'https://openalex.org/W2165712214', 'https://openalex.org/W1553004968', 'https://openalex.org/W1989674786', 'https://openalex.org/W2103933358', 'https://openalex.org/W1992912377', 'https://openalex.org/W2184045248', 'https://openalex.org/W1604771987', 'https://openalex.org/W2150907703', 'https://openalex.org/W23025778', 'https://openalex.org/W1975113979', 'https://openalex.org/W2160306971', 'https://openalex.org/W2096140469', 'https://openalex.org/W2106554350', 'https://openalex.org/W2139453310', 'https://openalex.org/W1993952617', 'https://openalex.org/W1994606281', 'https://openalex.org/W1904457459', 'https://openalex.org/W2023155336', 'https://openalex.org/W2169272853', 'https://openalex.org/W2442329935', 'https://openalex.org/W1975550806', 'https://openalex.org/W2103635001', 'https://openalex.org/W114193738', 'https://openalex.org/W2184343439', 'https://openalex.org/W1571931074', 'https://openalex.org/W2002342963', 'https://openalex.org/W2030694415', 'https://openalex.org/W2129334286', 'https://openalex.org/W2125610823', 'https://openalex.org/W2286443923', 'https://openalex.org/W2164505566', 'https://openalex.org/W2090764203', 'https://openalex.org/W2407897255', 'https://openalex.org/W2114016253']",2014-05-14
https://openalex.org/W2250357346,,TED-LIUM: an automatic speech recognition dedicated corpus,"This paper presents the corpus developed by the LIUM for Automatic Speech Recognition (ASR), based on the TED Talks. This corpus was built during the IWSLT 2011 Evaluation Campaign, and is composed of 118 hours of speech with its accompanying automatically aligned transcripts. We describe the content of the corpus, how the data was collected and processed, how it will be publicly available and how we built an ASR system using this data leading to a WER score of 17.4%. The official results we obtained at the IWSLT 2011 evaluation campaign are also discussed.","['https://openalex.org/W1631260214', 'https://openalex.org/W2187919308', 'https://openalex.org/W2916456038', 'https://openalex.org/W2159948109', 'https://openalex.org/W2620757702', 'https://openalex.org/W177378321']",2015-11-03
https://openalex.org/W3142316150,https://doi.org/10.1109/slt48900.2021.9383496,Transformer-Based Direct Speech-To-Speech Translation with Transcoder,"Traditional speech translation systems use a cascade manner that concatenates speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) synthesis to translate speech from one language to another language in a step-by-step manner. Unfortunately, since those components are trained separately, MT often struggles to handle ASR errors, resulting in unnatural translation results. Recently, one work attempted to construct direct speech translation in a single model. The model used a multi-task scheme that learns to predict not only the target speech spectrograms directly but also the source and target phoneme transcription as auxiliary tasks. However, that work was only evaluated Spanish-English language pairs with similar syntax and word order. With syntactically distant language pairs, speech translation requires distant word order, and thus direct speech frame-to-frame alignments become difficult. Another direction was to construct a single deep-learning framework while keeping the step-by-step translation process. However, such studies focused only on speech-to-text translation. Furthermore, all of these works were based on a recurrent neural net-work (RNN) model. In this work, we propose a step-by-step scheme to a complete end-to-end speech-to-speech translation and propose a Transformer-based speech translation using Transcoder. We compare our proposed and multi-task model using syntactically similar and distant language pairs.","['https://openalex.org/W6679434410', 'https://openalex.org/W4241645538', 'https://openalex.org/W6736996214', 'https://openalex.org/W3017535695', 'https://openalex.org/W2605131327', 'https://openalex.org/W2605202026', 'https://openalex.org/W6608432165', 'https://openalex.org/W2161742089', 'https://openalex.org/W6898505805', 'https://openalex.org/W6677328238', 'https://openalex.org/W2972495969', 'https://openalex.org/W4300558631', 'https://openalex.org/W2936969148', 'https://openalex.org/W2962680099', 'https://openalex.org/W3037217258', 'https://openalex.org/W6739901393', 'https://openalex.org/W2582956876', 'https://openalex.org/W2136545725', 'https://openalex.org/W6623517193', 'https://openalex.org/W854541894', 'https://openalex.org/W206967138', 'https://openalex.org/W2101105183', 'https://openalex.org/W2963403868', 'https://openalex.org/W2949328740', 'https://openalex.org/W2133564696', 'https://openalex.org/W2964308564', 'https://openalex.org/W2116492146', 'https://openalex.org/W2963609956', 'https://openalex.org/W2153653739', 'https://openalex.org/W4385245566']",2021-01-19
https://openalex.org/W3168212167,https://doi.org/10.18653/v1/2021.naacl-main.150,Source and Target Bidirectional Knowledge Distillation for End-to-end Speech Translation,"Hirofumi Inaguma, Tatsuya Kawahara, Shinji Watanabe. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.","['https://openalex.org/W2964089333', 'https://openalex.org/W3007142233', 'https://openalex.org/W3127901106', 'https://openalex.org/W1524333225', 'https://openalex.org/W3037217258', 'https://openalex.org/W3034571331', 'https://openalex.org/W3113715281', 'https://openalex.org/W3096700052', 'https://openalex.org/W3099942180', 'https://openalex.org/W2183341477', 'https://openalex.org/W2972448360', 'https://openalex.org/W2914120296', 'https://openalex.org/W2945700568', 'https://openalex.org/W3008549139', 'https://openalex.org/W3092424727', 'https://openalex.org/W3105962700', 'https://openalex.org/W2963001247', 'https://openalex.org/W2997436923', 'https://openalex.org/W2952443824', 'https://openalex.org/W2964172053', 'https://openalex.org/W2766219058', 'https://openalex.org/W4300558631', 'https://openalex.org/W2995999067', 'https://openalex.org/W2949328740', 'https://openalex.org/W3025165719', 'https://openalex.org/W2964121744', 'https://openalex.org/W3043665049', 'https://openalex.org/W3118578889', 'https://openalex.org/W2963341956', 'https://openalex.org/W3102811925', 'https://openalex.org/W2407080277', 'https://openalex.org/W2988975212', 'https://openalex.org/W2101105183', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963736842', 'https://openalex.org/W3034363136', 'https://openalex.org/W2972818416', 'https://openalex.org/W2998386507', 'https://openalex.org/W2767206889', 'https://openalex.org/W3095901788', 'https://openalex.org/W2964212550', 'https://openalex.org/W2148708890', 'https://openalex.org/W3097777922', 'https://openalex.org/W2987395887', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963779652', 'https://openalex.org/W2936969148', 'https://openalex.org/W2741049976', 'https://openalex.org/W2936774411', 'https://openalex.org/W3015633994', 'https://openalex.org/W1522301498', 'https://openalex.org/W2991786320', 'https://openalex.org/W3054645415', 'https://openalex.org/W2963979492', 'https://openalex.org/W2964213257', 'https://openalex.org/W3162919436', 'https://openalex.org/W2595715041', 'https://openalex.org/W3000840023', 'https://openalex.org/W2149327368']",2021-01-01
https://openalex.org/W4280617721,https://doi.org/10.48550/arxiv.2205.08533,Consistent Human Evaluation of Machine Translation across Language Pairs,"Obtaining meaningful quality scores for machine translation systems through human evaluation remains a challenge given the high variability between human evaluators, partly due to subjective expectations for translation quality for different language pairs. We propose a new metric called XSTS that is more focused on semantic equivalence and a cross-lingual calibration method that enables more consistent assessment. We demonstrate the effectiveness of these novel contributions in large scale evaluation studies across up to 14 language pairs, with translation both into and out of English.",[],2022-05-17
https://openalex.org/W2958953787,https://doi.org/10.48550/arxiv.1907.05019,Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges,"We introduce our efforts towards building a universal neural machine translation (NMT) system capable of translating between any language pair. We set a milestone towards this goal by building a single massively multilingual NMT model handling 103 languages trained on over 25 billion examples. Our system demonstrates effective transfer learning ability, significantly improving translation quality of low-resource languages, while keeping high-resource language translation quality on-par with competitive bilingual baselines. We provide in-depth analysis of various aspects of model building that are crucial to achieving quality and practicality in universal NMT. While we prototype a high-quality universal translation system, our extensive empirical analysis exposes issues that need to be further addressed, and we suggest directions for future research.","['https://openalex.org/W2952339051', 'https://openalex.org/W2797328513', 'https://openalex.org/W2936887742', 'https://openalex.org/W2550821151', 'https://openalex.org/W2552124255', 'https://openalex.org/W1594170634', 'https://openalex.org/W2101105183', 'https://openalex.org/W2417358553', 'https://openalex.org/W2962784628', 'https://openalex.org/W2145680191', 'https://openalex.org/W2561274697', 'https://openalex.org/W2933350699', 'https://openalex.org/W2557283755', 'https://openalex.org/W2908336025', 'https://openalex.org/W2963982496', 'https://openalex.org/W2887920589', 'https://openalex.org/W2525778437', 'https://openalex.org/W2921280978', 'https://openalex.org/W2512924740', 'https://openalex.org/W2251743902', 'https://openalex.org/W2117130368', 'https://openalex.org/W2964045208', 'https://openalex.org/W2809290718', 'https://openalex.org/W2963403868', 'https://openalex.org/W2785847164', 'https://openalex.org/W2932484476', 'https://openalex.org/W2597452529', 'https://openalex.org/W2512848817', 'https://openalex.org/W2760656271', 'https://openalex.org/W1753482797', 'https://openalex.org/W2922466325', 'https://openalex.org/W2983826605', 'https://openalex.org/W2185720331', 'https://openalex.org/W1738019091', 'https://openalex.org/W2788388592', 'https://openalex.org/W22168010', 'https://openalex.org/W2794365787', 'https://openalex.org/W2250875036', 'https://openalex.org/W2963250244', 'https://openalex.org/W2952036846', 'https://openalex.org/W2919188216', 'https://openalex.org/W2555745756', 'https://openalex.org/W2928941594', 'https://openalex.org/W2560647685', 'https://openalex.org/W2565989828', 'https://openalex.org/W2531207078', 'https://openalex.org/W2949454572', 'https://openalex.org/W2903193068', 'https://openalex.org/W2311921240', 'https://openalex.org/W2896691342', 'https://openalex.org/W2794363191', 'https://openalex.org/W2296073425', 'https://openalex.org/W2923622379', 'https://openalex.org/W2795900505', 'https://openalex.org/W2110518760', 'https://openalex.org/W2229833550', 'https://openalex.org/W2237537322', 'https://openalex.org/W630532510', 'https://openalex.org/W2903188467', 'https://openalex.org/W2948223045', 'https://openalex.org/W2091432990', 'https://openalex.org/W2946740027', 'https://openalex.org/W2657631929', 'https://openalex.org/W2952468927', 'https://openalex.org/W3035219538', 'https://openalex.org/W1924762813', 'https://openalex.org/W1857884451', 'https://openalex.org/W2962830144', 'https://openalex.org/W2537667581', 'https://openalex.org/W2426267443', 'https://openalex.org/W2626792426', 'https://openalex.org/W2963993537', 'https://openalex.org/W2572474373', 'https://openalex.org/W2766182427', 'https://openalex.org/W2613253298', 'https://openalex.org/W2401823607', 'https://openalex.org/W2184135559', 'https://openalex.org/W2994475016', 'https://openalex.org/W2963842982', 'https://openalex.org/W2095705004', 'https://openalex.org/W2950135462', 'https://openalex.org/W2963991316', 'https://openalex.org/W1533861849', 'https://openalex.org/W2199580741', 'https://openalex.org/W2624871570', 'https://openalex.org/W652269744', 'https://openalex.org/W2919290281', 'https://openalex.org/W2250342921', 'https://openalex.org/W2891089320', 'https://openalex.org/W2963216553', 'https://openalex.org/W2606722458', 'https://openalex.org/W2913340405', 'https://openalex.org/W2913946806', 'https://openalex.org/W2742079690', 'https://openalex.org/W2798362442', 'https://openalex.org/W2963351145', 'https://openalex.org/W2157331557', 'https://openalex.org/W2804145368', 'https://openalex.org/W2806311723', 'https://openalex.org/W2788190072', 'https://openalex.org/W2962982474', 'https://openalex.org/W2798926775', 'https://openalex.org/W2893749619', 'https://openalex.org/W2952650870', 'https://openalex.org/W2152477898', 'https://openalex.org/W2101096097', 'https://openalex.org/W2767206889', 'https://openalex.org/W2907121943', 'https://openalex.org/W2775461895', 'https://openalex.org/W2972909479', 'https://openalex.org/W2963633299', 'https://openalex.org/W2962724315', 'https://openalex.org/W2807535859', 'https://openalex.org/W1825672851', 'https://openalex.org/W1991564165', 'https://openalex.org/W2604763608', 'https://openalex.org/W2963983698', 'https://openalex.org/W2964007535', 'https://openalex.org/W2951882776', 'https://openalex.org/W2765961751', 'https://openalex.org/W2888541716', 'https://openalex.org/W2610245951', 'https://openalex.org/W2952626150', 'https://openalex.org/W2991040477', 'https://openalex.org/W2964327384', 'https://openalex.org/W2952518244', 'https://openalex.org/W2788800397', 'https://openalex.org/W2784231336', 'https://openalex.org/W2741787148', 'https://openalex.org/W2555428947', 'https://openalex.org/W2123301721', 'https://openalex.org/W2078861931', 'https://openalex.org/W2613904329', 'https://openalex.org/W2339995566', 'https://openalex.org/W99485931', 'https://openalex.org/W2130942839', 'https://openalex.org/W2964308564', 'https://openalex.org/W2120501001', 'https://openalex.org/W1602635394', 'https://openalex.org/W2952444318', 'https://openalex.org/W2165698076', 'https://openalex.org/W2963341924', 'https://openalex.org/W2963887123', 'https://openalex.org/W1572401739', 'https://openalex.org/W1915251500', 'https://openalex.org/W2948697567', 'https://openalex.org/W2953212265', 'https://openalex.org/W2891924676', 'https://openalex.org/W2220350356', 'https://openalex.org/W2963341956', 'https://openalex.org/W2748679025', 'https://openalex.org/W2463008967']",2019-07-11
https://openalex.org/W2972448360,https://doi.org/10.21437/interspeech.2019-2582,End-to-End Speech Translation with Knowledge Distillation,"End-to-end speech translation (ST), which directly translates from source language speech into target language text, has attracted intensive attentions in recent years.Compared to conventional pipepine systems, end-to-end ST models have advantages of lower latency, smaller model size and less error propagation.However, the combination of speech recognition and text translation in one model is more difficult than each of these two tasks.In this paper, we propose a knowledge distillation approach to improve ST model by transferring the knowledge from text translation model.Specifically, we first train a text translation model, regarded as a teacher model, and then ST model is trained to learn output probabilities from teacher model through knowledge distillation.Experiments on English-French Augmented LibriSpeech and English-Chinese TED corpus show that end-to-end ST is possible to implement on both similar and dissimilar language pairs.In addition, with the instruction of teacher model, end-to-end ST model can gain significant improvements by over 3.5 BLEU points.","['https://openalex.org/W2964172053', 'https://openalex.org/W2952650870', 'https://openalex.org/W2327501763', 'https://openalex.org/W4300558631', 'https://openalex.org/W1690739335', 'https://openalex.org/W2592335154', 'https://openalex.org/W2962784628', 'https://openalex.org/W1821462560', 'https://openalex.org/W2963927126', 'https://openalex.org/W2803985397', 'https://openalex.org/W2949328740', 'https://openalex.org/W1522301498', 'https://openalex.org/W1489125746', 'https://openalex.org/W2133564696', 'https://openalex.org/W2739879705', 'https://openalex.org/W4385245566', 'https://openalex.org/W2785350307', 'https://openalex.org/W2963736842', 'https://openalex.org/W2605131327', 'https://openalex.org/W2892009249', 'https://openalex.org/W2964102148', 'https://openalex.org/W4298159529', 'https://openalex.org/W2525778437', 'https://openalex.org/W2466918907', 'https://openalex.org/W2963240019', 'https://openalex.org/W2109886035']",2019-09-13
https://openalex.org/W4287072252,https://doi.org/10.48550/arxiv.2107.08661,Translatotron 2: High-quality direct speech-to-speech translation with voice preservation,"We present Translatotron 2, a neural direct speech-to-speech translation model that can be trained end-to-end. Translatotron 2 consists of a speech encoder, a linguistic decoder, an acoustic synthesizer, and a single attention module that connects them together. Experimental results on three datasets consistently show that Translatotron 2 outperforms the original Translatotron by a large margin on both translation quality (up to +15.5 BLEU) and speech generation quality, and approaches the same of cascade systems. In addition, we propose a simple method for preserving speakers' voices from the source speech to the translation speech in a different language. Unlike existing approaches, the proposed method is able to preserve each speaker's voice on speaker turns without requiring for speaker segmentation. Furthermore, compared to existing approaches, it better preserves speaker's privacy and mitigates potential misuse of voice cloning for creating spoofing audio artifacts.",[],2021-07-19
https://openalex.org/W3011339933,https://doi.org/10.1109/icassp40776.2020.9053606,Deliberation Model Based Two-Pass End-To-End Speech Recognition,"End-to-end (E2E) models have made rapid progress in automatic speech recognition (ASR) and perform competitively relative to conventional models. To further improve the quality, a two-pass model has been proposed to rescore streamed hypotheses using the non-streaming Listen, Attend and Spell (LAS) model while maintaining a reasonable latency. The model attends to acoustics to rescore hypotheses, as opposed to a class of neural correction models that use only first-pass text hypotheses. In this work, we propose to attend to both acoustics and first-pass hypotheses using a deliberation network. A bidirectional encoder is used to extract context information from first-pass hypotheses. The proposed deliberation model achieves 12% relative WER reduction compared to LAS rescoring in Google Voice Search (VS) tasks, and 23% reduction on a proper noun test set. Compared to a large conventional model, our best model performs 21% relatively better for VS. In terms of computational complexity, the deliberation decoder has a larger size than the LAS decoder, and hence requires more computations in second-pass decoding.","['https://openalex.org/W2973122799', 'https://openalex.org/W2924677654', 'https://openalex.org/W2972650231', 'https://openalex.org/W2916997151', 'https://openalex.org/W2963240019', 'https://openalex.org/W2972880214', 'https://openalex.org/W6743726175', 'https://openalex.org/W6749740488', 'https://openalex.org/W2938973646', 'https://openalex.org/W2121879602', 'https://openalex.org/W6713134421', 'https://openalex.org/W2962826786', 'https://openalex.org/W2327501763', 'https://openalex.org/W6739901393', 'https://openalex.org/W2962760690', 'https://openalex.org/W6760633627', 'https://openalex.org/W6728030952', 'https://openalex.org/W2515439472', 'https://openalex.org/W2962824709', 'https://openalex.org/W2963414781', 'https://openalex.org/W6638749077', 'https://openalex.org/W2944255943', 'https://openalex.org/W2963122170', 'https://openalex.org/W2963747784', 'https://openalex.org/W3008181812', 'https://openalex.org/W6688428952', 'https://openalex.org/W2617258110', 'https://openalex.org/W2064675550', 'https://openalex.org/W6843673214', 'https://openalex.org/W2752047430', 'https://openalex.org/W2963782041', 'https://openalex.org/W2964138484', 'https://openalex.org/W2526425061', 'https://openalex.org/W4294619240', 'https://openalex.org/W4385245566', 'https://openalex.org/W3103005696', 'https://openalex.org/W2963403868', 'https://openalex.org/W2928941594', 'https://openalex.org/W2402144811', 'https://openalex.org/W2794365787', 'https://openalex.org/W2953384591', 'https://openalex.org/W1828163288', 'https://openalex.org/W2213952365']",2020-04-09
https://openalex.org/W2938973646,https://doi.org/10.1109/icassp.2019.8682801,Towards End-to-end Speech-to-text Translation with Two-pass Decoding,"Speech-to-text translation (ST) refers to transforming the audio in source language to the text in target language. Mainstream solutions for such tasks are to cascade automatic speech recognition with machine translation, for which the transcriptions of the source language are needed in training. End-to-end approaches for ST tasks have been investigated because of not only technical interests such as to achieve globally optimized solution, but the need for ST tasks for the many source languages worldwide which do not have written form. In this paper, we propose a new end-to-end ST framework with two decoders to handle the relatively deeper relationships between the source language audio and target language text. The first-pass decoder generates some useful latent representations, and the second-pass decoder then integrates the output of both the encoder and the first-pass decoder to generate the text translation in target language. Only paired source language audio and target language text are used in training. Preliminary experiments on several language pairs showed improved performance, and offered some initial analysis.","['https://openalex.org/W6743726175', 'https://openalex.org/W6747302705', 'https://openalex.org/W6744702808', 'https://openalex.org/W6749669830', 'https://openalex.org/W2796108585', 'https://openalex.org/W6628877408', 'https://openalex.org/W6898505805', 'https://openalex.org/W6732953234', 'https://openalex.org/W2101281673', 'https://openalex.org/W4300558631', 'https://openalex.org/W2605131327', 'https://openalex.org/W6679434410', 'https://openalex.org/W2963834942', 'https://openalex.org/W2963378435', 'https://openalex.org/W2963779652', 'https://openalex.org/W6623517193', 'https://openalex.org/W179875071', 'https://openalex.org/W6720877245', 'https://openalex.org/W2963091079', 'https://openalex.org/W2101105183', 'https://openalex.org/W2762715843', 'https://openalex.org/W2963418779', 'https://openalex.org/W854541894', 'https://openalex.org/W2752047430', 'https://openalex.org/W4297747548', 'https://openalex.org/W2964308564', 'https://openalex.org/W3104636952', 'https://openalex.org/W2949328740', 'https://openalex.org/W2474824677', 'https://openalex.org/W1485009520', 'https://openalex.org/W2133564696', 'https://openalex.org/W2525778437', 'https://openalex.org/W3012492057']",2019-04-17
https://openalex.org/W4285158119,https://doi.org/10.18653/v1/2022.iwslt-1.10,Findings of the IWSLT 2022 Evaluation Campaign,"Antonios Anastasopoulos, Loïc Barrault, Luisa Bentivogli, Marcely Zanon Boito, Ondřej Bojar, Roldano Cattoni, Anna Currey, Georgiana Dinu, Kevin Duh, Maha Elbayad, Clara Emmanuel, Yannick Estève, Marcello Federico, Christian Federmann, Souhir Gahbiche, Hongyu Gong, Roman Grundkiewicz, Barry Haddow, Benjamin Hsu, Dávid Javorský, Vĕra Kloudová, Surafel Lakew, Xutai Ma, Prashant Mathur, Paul McNamee, Kenton Murray, Maria Nǎdejde, Satoshi Nakamura, Matteo Negri, Jan Niehues, Xing Niu, John Ortega, Juan Pino, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Sebastian Stüker, Katsuhito Sudoh, Marco Turchi, Yogesh Virkar, Alexander Waibel, Changhan Wang, Shinji Watanabe. Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022). 2022.","['https://openalex.org/W2933138175', 'https://openalex.org/W4285225130', 'https://openalex.org/W2467834614', 'https://openalex.org/W2902409908', 'https://openalex.org/W4287854445', 'https://openalex.org/W4285158790', 'https://openalex.org/W3015703505', 'https://openalex.org/W3115075512', 'https://openalex.org/W4385245566', 'https://openalex.org/W2136630431', 'https://openalex.org/W4285269614', 'https://openalex.org/W4285269970', 'https://openalex.org/W3097777922', 'https://openalex.org/W3204130541', 'https://openalex.org/W2941599692', 'https://openalex.org/W3015698636', 'https://openalex.org/W2972802841', 'https://openalex.org/W3119308075', 'https://openalex.org/W3153988736', 'https://openalex.org/W3084396630', 'https://openalex.org/W4224308495', 'https://openalex.org/W4285122949', 'https://openalex.org/W4285309242', 'https://openalex.org/W3214250531', 'https://openalex.org/W3175746962', 'https://openalex.org/W4221143967', 'https://openalex.org/W4297841689', 'https://openalex.org/W3158927431', 'https://openalex.org/W4285110390', 'https://openalex.org/W4385571063', 'https://openalex.org/W2251321385', 'https://openalex.org/W2915395439', 'https://openalex.org/W2184135559', 'https://openalex.org/W4224931655', 'https://openalex.org/W2252166243', 'https://openalex.org/W2899274165', 'https://openalex.org/W2917668649', 'https://openalex.org/W222053410', 'https://openalex.org/W3174446152', 'https://openalex.org/W2291598608', 'https://openalex.org/W3035390927', 'https://openalex.org/W2917604489', 'https://openalex.org/W2971345947', 'https://openalex.org/W3005389111', 'https://openalex.org/W3169483174', 'https://openalex.org/W4285232530', 'https://openalex.org/W4285118402', 'https://openalex.org/W4285257396', 'https://openalex.org/W3159892921', 'https://openalex.org/W3198429080', 'https://openalex.org/W2972791412', 'https://openalex.org/W1494198834', 'https://openalex.org/W2785350307', 'https://openalex.org/W3082928416', 'https://openalex.org/W2250342921', 'https://openalex.org/W2806412155', 'https://openalex.org/W4285108476', 'https://openalex.org/W4285239627', 'https://openalex.org/W3207583491', 'https://openalex.org/W4285201770', 'https://openalex.org/W4285169892', 'https://openalex.org/W2982026991', 'https://openalex.org/W4225302758', 'https://openalex.org/W3161049737', 'https://openalex.org/W3101648800', 'https://openalex.org/W3092085609', 'https://openalex.org/W4297841628', 'https://openalex.org/W4288345582', 'https://openalex.org/W4288089799', 'https://openalex.org/W4287694131', 'https://openalex.org/W2889330990', 'https://openalex.org/W3197273793', 'https://openalex.org/W3092424727', 'https://openalex.org/W2977725500', 'https://openalex.org/W2194775991', 'https://openalex.org/W3105214104', 'https://openalex.org/W3041866211', 'https://openalex.org/W4285187734', 'https://openalex.org/W2970780025', 'https://openalex.org/W3213993043', 'https://openalex.org/W4297756269', 'https://openalex.org/W2991213871', 'https://openalex.org/W2807895655', 'https://openalex.org/W2972664115', 'https://openalex.org/W2970752815', 'https://openalex.org/W2061504941', 'https://openalex.org/W3107826490', 'https://openalex.org/W2953830716', 'https://openalex.org/W4285107548', 'https://openalex.org/W3042199843', 'https://openalex.org/W2963532001', 'https://openalex.org/W3120929527', 'https://openalex.org/W2808631503', 'https://openalex.org/W4285163683', 'https://openalex.org/W4285242950', 'https://openalex.org/W4287121924', 'https://openalex.org/W3172862365', 'https://openalex.org/W4285134744', 'https://openalex.org/W2936695845', 'https://openalex.org/W4285188582', 'https://openalex.org/W3101727459', 'https://openalex.org/W3093871477', 'https://openalex.org/W3174864715', 'https://openalex.org/W4285140158']",2022-01-01
https://openalex.org/W4221155340,https://doi.org/10.48550/arxiv.2202.01374,mSLAM: Massively multilingual joint pre-training for speech and text,"We present mSLAM, a multilingual Speech and LAnguage Model that learns cross-lingual cross-modal representations of speech and text by pre-training jointly on large amounts of unlabeled speech and text in multiple languages. mSLAM combines w2v-BERT pre-training on speech with SpanBERT pre-training on character-level text, along with Connectionist Temporal Classification (CTC) losses on paired speech and transcript data, to learn a single model capable of learning from and representing both speech and text signals in a shared representation space. We evaluate mSLAM on several downstream speech understanding tasks and find that joint pre-training with text improves quality on speech translation, speech intent classification and speech language-ID while being competitive on multilingual ASR, when compared against speech-only pre-training. Our speech translation model demonstrates zero-shot text translation without seeing any text translation data, providing evidence for cross-modal alignment of representations. mSLAM also benefits from multi-modal fine-tuning, further improving the quality of speech translation by directly leveraging text translation data during the fine-tuning process. Our empirical analysis highlights several opportunities and challenges arising from large-scale multimodal pre-training, suggesting directions for future research.",[],2022-02-03
https://openalex.org/W4226444650,https://doi.org/10.48550/arxiv.2201.03713,CVSS Corpus and Massively Multilingual Speech-to-Speech Translation,"We introduce CVSS, a massively multilingual-to-English speech-to-speech translation (S2ST) corpus, covering sentence-level parallel S2ST pairs from 21 languages into English. CVSS is derived from the Common Voice speech corpus and the CoVoST 2 speech-to-text translation (ST) corpus, by synthesizing the translation text from CoVoST 2 into speech using state-of-the-art TTS systems. Two versions of translation speeches are provided: 1) CVSS-C: All the translation speeches are in a single high-quality canonical voice; 2) CVSS-T: The translation speeches are in voices transferred from the corresponding source speeches. In addition, CVSS provides normalized translation text which matches the pronunciation in the translation speech. On each version of CVSS, we built baseline multilingual direct S2ST models and cascade S2ST models, verifying the effectiveness of the corpus. To build strong cascade S2ST baselines, we trained an ST model on CoVoST 2, which outperforms the previous state-of-the-art trained on the corpus without extra data by 5.8 BLEU. Nevertheless, the performance of the direct S2ST models approaches the strong cascade baselines when trained from scratch, and with only 0.1 or 0.7 BLEU difference on ASR transcribed translation when initialized from matching ST models.",[],2022-01-11
https://openalex.org/W3172862365,https://doi.org/10.18653/v1/2021.naacl-main.151,Searchable Hidden Intermediates for End-to-End Models of Decomposable Sequence Tasks,"Siddharth Dalmia, Brian Yan, Vikas Raunak, Florian Metze, Shinji Watanabe. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.","['https://openalex.org/W2963250244', 'https://openalex.org/W1522301498', 'https://openalex.org/W2058094241', 'https://openalex.org/W2964103964', 'https://openalex.org/W1602395960', 'https://openalex.org/W1902568950', 'https://openalex.org/W4301259831', 'https://openalex.org/W2146390270', 'https://openalex.org/W4287647128', 'https://openalex.org/W3175809709', 'https://openalex.org/W2964121744', 'https://openalex.org/W2902031175', 'https://openalex.org/W2963665552', 'https://openalex.org/W3034625919', 'https://openalex.org/W2963551569', 'https://openalex.org/W2526425061', 'https://openalex.org/W2964308564', 'https://openalex.org/W2101105183', 'https://openalex.org/W2788277448', 'https://openalex.org/W2963403868', 'https://openalex.org/W2973122799', 'https://openalex.org/W2595715041', 'https://openalex.org/W3087091162', 'https://openalex.org/W2989224798', 'https://openalex.org/W2061819627', 'https://openalex.org/W2124807415', 'https://openalex.org/W2990379018', 'https://openalex.org/W2154124206', 'https://openalex.org/W2898972706', 'https://openalex.org/W2149327368', 'https://openalex.org/W3115021520', 'https://openalex.org/W2160815625', 'https://openalex.org/W3143377973', 'https://openalex.org/W4297683418', 'https://openalex.org/W1970987322', 'https://openalex.org/W2945700568', 'https://openalex.org/W3105718208', 'https://openalex.org/W2605131327', 'https://openalex.org/W2998386507', 'https://openalex.org/W2982727400', 'https://openalex.org/W2963620441', 'https://openalex.org/W2894164357', 'https://openalex.org/W3016137827', 'https://openalex.org/W2963267799', 'https://openalex.org/W4288331674', 'https://openalex.org/W4288817190', 'https://openalex.org/W2952167535', 'https://openalex.org/W2164948578', 'https://openalex.org/W2587464121', 'https://openalex.org/W2936774411', 'https://openalex.org/W2963779652', 'https://openalex.org/W2963532001', 'https://openalex.org/W2964118342', 'https://openalex.org/W2912668608', 'https://openalex.org/W1984511497', 'https://openalex.org/W2103237065', 'https://openalex.org/W2972780808', 'https://openalex.org/W2766219058', 'https://openalex.org/W2886090311', 'https://openalex.org/W2963303028', 'https://openalex.org/W2963382396', 'https://openalex.org/W2988249555', 'https://openalex.org/W3106274667', 'https://openalex.org/W2046932483', 'https://openalex.org/W2936969148', 'https://openalex.org/W2986378306', 'https://openalex.org/W2161227214', 'https://openalex.org/W3163793923', 'https://openalex.org/W3037217258', 'https://openalex.org/W2627092829', 'https://openalex.org/W3054645415', 'https://openalex.org/W3012492057', 'https://openalex.org/W2123301721', 'https://openalex.org/W3089828951', 'https://openalex.org/W4287329822', 'https://openalex.org/W2997436923', 'https://openalex.org/W4245734123', 'https://openalex.org/W2137387514', 'https://openalex.org/W2130942839', 'https://openalex.org/W2043701535', 'https://openalex.org/W2962780374', 'https://openalex.org/W4385245566', 'https://openalex.org/W2288817436', 'https://openalex.org/W2133564696', 'https://openalex.org/W3092424727', 'https://openalex.org/W2997244573', 'https://openalex.org/W3011339933']",2021-01-01
https://openalex.org/W3091928890,https://doi.org/10.48550/arxiv.2010.04301,Non-Attentive Tacotron: Robust and Controllable Neural TTS Synthesis Including Unsupervised Duration Modeling,"This paper presents Non-Attentive Tacotron based on the Tacotron 2 text-to-speech model, replacing the attention mechanism with an explicit duration predictor. This improves robustness significantly as measured by unaligned duration ratio and word deletion rate, two metrics introduced in this paper for large-scale robustness evaluation using a pre-trained speech recognition model. With the use of Gaussian upsampling, Non-Attentive Tacotron achieves a 5-scale mean opinion score for naturalness of 4.41, slightly outperforming Tacotron 2. The duration predictor enables both utterance-wide and per-phoneme control of duration at inference time. When accurate target durations are scarce or unavailable in the training data, we propose a method using a fine-grained variational auto-encoder to train the duration predictor in a semi-supervised or unsupervised manner, with results almost as good as supervised training.","['https://openalex.org/W2963403868', 'https://openalex.org/W1902237438', 'https://openalex.org/W2129142580', 'https://openalex.org/W3123097577', 'https://openalex.org/W2970006822', 'https://openalex.org/W2963590452', 'https://openalex.org/W3015922793', 'https://openalex.org/W3024747869', 'https://openalex.org/W2972951102', 'https://openalex.org/W2327501763', 'https://openalex.org/W2963927338', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963432880', 'https://openalex.org/W1810943226', 'https://openalex.org/W2972359262', 'https://openalex.org/W2963609956', 'https://openalex.org/W2995181338', 'https://openalex.org/W2402737981', 'https://openalex.org/W1494198834', 'https://openalex.org/W2971753973', 'https://openalex.org/W2945544731', 'https://openalex.org/W2970730223', 'https://openalex.org/W3048487650', 'https://openalex.org/W2963300588', 'https://openalex.org/W2952269766', 'https://openalex.org/W3033913438', 'https://openalex.org/W2886769154', 'https://openalex.org/W3016021263', 'https://openalex.org/W3025013833', 'https://openalex.org/W2949382160', 'https://openalex.org/W2767206889', 'https://openalex.org/W2901997113', 'https://openalex.org/W2016589492', 'https://openalex.org/W3025793647', 'https://openalex.org/W2972495969', 'https://openalex.org/W2964307104', 'https://openalex.org/W2972970915', 'https://openalex.org/W1959608418', 'https://openalex.org/W3016136182', 'https://openalex.org/W2964281804', 'https://openalex.org/W2964308564', 'https://openalex.org/W3026041220', 'https://openalex.org/W2972702018', 'https://openalex.org/W3130016944', 'https://openalex.org/W2969521066', 'https://openalex.org/W854541894', 'https://openalex.org/W2964138190', 'https://openalex.org/W2963691546', 'https://openalex.org/W2903739847', 'https://openalex.org/W2963712897', 'https://openalex.org/W3093733783', 'https://openalex.org/W2102003408']",2020-10-08
https://openalex.org/W3176711365,https://doi.org/10.18653/v1/2021.acl-long.328,Improving Speech Translation by Understanding and Learning from the Auxiliary Text Translation Task,"Yun Tang, Juan Pino, Xian Li, Changhan Wang, Dmitriy Genzel. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.","['https://openalex.org/W2933138175', 'https://openalex.org/W6784050962', 'https://openalex.org/W4297730150', 'https://openalex.org/W1522301498', 'https://openalex.org/W2936774411', 'https://openalex.org/W2963532001', 'https://openalex.org/W2963430933', 'https://openalex.org/W3092424727', 'https://openalex.org/W3112092703', 'https://openalex.org/W3015703505', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964012862', 'https://openalex.org/W2970925270', 'https://openalex.org/W1682403713', 'https://openalex.org/W3037542581', 'https://openalex.org/W4294103325', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963736842', 'https://openalex.org/W3035464238', 'https://openalex.org/W3035490255', 'https://openalex.org/W2995343314', 'https://openalex.org/W3101498587', 'https://openalex.org/W2901389167', 'https://openalex.org/W2963759780', 'https://openalex.org/W2962864421', 'https://openalex.org/W3015633994', 'https://openalex.org/W3118578889', 'https://openalex.org/W3085046840', 'https://openalex.org/W3034672970', 'https://openalex.org/W2807535859', 'https://openalex.org/W1821462560', 'https://openalex.org/W2972448360', 'https://openalex.org/W3173767661', 'https://openalex.org/W2963779652', 'https://openalex.org/W2767204723', 'https://openalex.org/W3162037819', 'https://openalex.org/W2991018369', 'https://openalex.org/W3007142233', 'https://openalex.org/W3017454464', 'https://openalex.org/W2962743139', 'https://openalex.org/W2767434619', 'https://openalex.org/W3096490862', 'https://openalex.org/W2997436923', 'https://openalex.org/W2945700568']",2021-01-01
https://openalex.org/W3015698636,https://doi.org/10.1109/icassp40776.2020.9054626,Europarl-ST: A Multilingual Corpus for Speech Translation of Parliamentary Debates,"[EN] Current research into spoken language translation (SLT), or speech-to-text translation, is often hampered by the lack of specific data resources for this task, as currently available SLT datasets are restricted to a limited set of language pairs. In this paper we present Europarl-ST, a novel multilingual SLT corpus containing paired audio-text samples for SLT from and into 6 European languages, for a total of 30 different translation directions. This corpus has been compiled using the debates held in the European Parliament in the period between 2008 and 2012. This paper describes the corpus creation process and presents a series of automatic speech recognition, machine translation and spoken language translation experiments that highlight the potential of this new resource. The corpus is released under a Creative Commons license and is freely accessible and downloadable.","['https://openalex.org/W6775053297', 'https://openalex.org/W6766240984', 'https://openalex.org/W2404270056', 'https://openalex.org/W6635144179', 'https://openalex.org/W280003846', 'https://openalex.org/W2124807415', 'https://openalex.org/W6682040602', 'https://openalex.org/W6682026795', 'https://openalex.org/W6898505805', 'https://openalex.org/W6628972281', 'https://openalex.org/W6686989717', 'https://openalex.org/W2605131327', 'https://openalex.org/W6739901393', 'https://openalex.org/W2952167535', 'https://openalex.org/W2936969148', 'https://openalex.org/W2972495969', 'https://openalex.org/W6775587661', 'https://openalex.org/W2745785989', 'https://openalex.org/W6931689928', 'https://openalex.org/W2963532001', 'https://openalex.org/W2401969231', 'https://openalex.org/W2962784628', 'https://openalex.org/W7051469422', 'https://openalex.org/W1484504603', 'https://openalex.org/W3015703505', 'https://openalex.org/W2148708890', 'https://openalex.org/W2945700568', 'https://openalex.org/W630532510', 'https://openalex.org/W1631260214', 'https://openalex.org/W2963403868', 'https://openalex.org/W2966095117', 'https://openalex.org/W4385245566', 'https://openalex.org/W4301980136', 'https://openalex.org/W1589339348', 'https://openalex.org/W2148334774', 'https://openalex.org/W2955541912', 'https://openalex.org/W2101105183', 'https://openalex.org/W2186089609', 'https://openalex.org/W3012492057']",2020-04-09
https://openalex.org/W2964161387,https://doi.org/10.1109/icassp.2019.8683343,Leveraging Weakly Supervised Data to Improve End-to-end Speech-to-text Translation,"End-to-end Speech Translation (ST) models have many potential advantages when compared to the cascade of Automatic Speech Recognition (ASR) and text Machine Translation (MT) models, including lowered inference latency and the avoidance of error compounding. However, the quality of end-to-end ST is often limited by a paucity of training data, since it is difficult to collect large parallel corpora of speech and translated transcript pairs. Previous studies have proposed the use of pre-trained components and multi-task learning in order to benefit from weakly supervised training data, such as speech-to-transcript or text-to-foreign-text pairs. In this paper, we demonstrate that using pre-trained MT or text-to-speech (TTS) synthesis models to convert weakly supervised data into speech-to-translation pairs for ST training can be more effective than multi-task learning. Furthermore, we demonstrate that a high quality end-to-end ST model can be trained using only weakly supervised datasets, and that synthetic data sourced from unlabeled monolingual text or speech can be used to improve performance. Finally, we discuss methods for avoiding overfitting to synthetic speech with a quantitative ablation study.","['https://openalex.org/W6755300632', 'https://openalex.org/W6750489868', 'https://openalex.org/W2795109282', 'https://openalex.org/W6898505805', 'https://openalex.org/W2963779652', 'https://openalex.org/W4300558631', 'https://openalex.org/W2113106066', 'https://openalex.org/W6632017195', 'https://openalex.org/W6754400763', 'https://openalex.org/W2962680099', 'https://openalex.org/W2963796886', 'https://openalex.org/W2964012862', 'https://openalex.org/W6753186555', 'https://openalex.org/W2120847449', 'https://openalex.org/W2964199361', 'https://openalex.org/W6629717138', 'https://openalex.org/W6679436768', 'https://openalex.org/W2963609956', 'https://openalex.org/W6679434410', 'https://openalex.org/W2582956876', 'https://openalex.org/W6739901393', 'https://openalex.org/W2962824709', 'https://openalex.org/W2605131327', 'https://openalex.org/W2327501763', 'https://openalex.org/W6638523607', 'https://openalex.org/W2121879602', 'https://openalex.org/W2963216553', 'https://openalex.org/W6746700228', 'https://openalex.org/W2964045208', 'https://openalex.org/W6752888775', 'https://openalex.org/W6745697700', 'https://openalex.org/W2808706139', 'https://openalex.org/W2794490148', 'https://openalex.org/W2963927338', 'https://openalex.org/W2525778437', 'https://openalex.org/W2766812927', 'https://openalex.org/W1537859740', 'https://openalex.org/W2963272440', 'https://openalex.org/W2964172053', 'https://openalex.org/W3012492057', 'https://openalex.org/W1494198834', 'https://openalex.org/W4289383906', 'https://openalex.org/W2963568578', 'https://openalex.org/W1821462560', 'https://openalex.org/W4295731579', 'https://openalex.org/W2883586237', 'https://openalex.org/W2130942839', 'https://openalex.org/W2949328740', 'https://openalex.org/W2963432880', 'https://openalex.org/W2133564696', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963403868', 'https://openalex.org/W2101105183', 'https://openalex.org/W2964308564', 'https://openalex.org/W4385245566']",2019-04-17
https://openalex.org/W2183341477,https://doi.org/10.1109/cvpr.2016.308,Rethinking the Inception Architecture for Computer Vision,"Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.","['https://openalex.org/W1903029394', 'https://openalex.org/W1944396096', 'https://openalex.org/W2111979893', 'https://openalex.org/W2096733369', 'https://openalex.org/W6604254268', 'https://openalex.org/W2068730032', 'https://openalex.org/W54257720', 'https://openalex.org/W1677182931', 'https://openalex.org/W2102605133', 'https://openalex.org/W2016053056', 'https://openalex.org/W6638667902', 'https://openalex.org/W6685405536', 'https://openalex.org/W6684191040', 'https://openalex.org/W2097117768', 'https://openalex.org/W2113325037', 'https://openalex.org/W6677907805', 'https://openalex.org/W2163605009', 'https://openalex.org/W2168894214', 'https://openalex.org/W2172166488', 'https://openalex.org/W2118097920', 'https://openalex.org/W1815076433', 'https://openalex.org/W2172654076', 'https://openalex.org/W104184427', 'https://openalex.org/W3099206234', 'https://openalex.org/W1686810756', 'https://openalex.org/W1836465849', 'https://openalex.org/W2117539524']",2016-06-01
https://openalex.org/W4300558631,,End-to-End Automatic Speech Translation of Audiobooks,"We investigate end-to-end speech-to-text translation on a corpus of audiobooks specifically augmented for this task. Previous works investigated the extreme case where source language transcription is not available during learning nor decoding, but we also study a midway case where source language transcription is available at training time only. In this case, a single model is trained to decode source speech into target text in a single pass. Experimental results show that it is possible to train compact and efficient end-to-end speech translation models in this setup. We also distribute the corpus and hope that our speech translation baseline on this corpus will be challenged in the future.","['https://openalex.org/W2605131327', 'https://openalex.org/W2466918907', 'https://openalex.org/W2963842982', 'https://openalex.org/W3012492057', 'https://openalex.org/W1855892484', 'https://openalex.org/W2271840356', 'https://openalex.org/W1591801644', 'https://openalex.org/W2949328740', 'https://openalex.org/W2785350307', 'https://openalex.org/W2962826786', 'https://openalex.org/W2963767893', 'https://openalex.org/W2594229957', 'https://openalex.org/W854541894', 'https://openalex.org/W2962784628', 'https://openalex.org/W2295300449', 'https://openalex.org/W2133564696', 'https://openalex.org/W1494198834', 'https://openalex.org/W2347145335', 'https://openalex.org/W2951595529', 'https://openalex.org/W1522301498']",2018-02-12
https://openalex.org/W22168010,,Europarl: A Parallel Corpus for Statistical Machine Translation,"We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web 1. This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT). We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead.","['https://openalex.org/W2153653739', 'https://openalex.org/W1490831147', 'https://openalex.org/W1497763703', 'https://openalex.org/W2153072229', 'https://openalex.org/W2018560257', 'https://openalex.org/W2998215494', 'https://openalex.org/W1489181569', 'https://openalex.org/W2101105183', 'https://openalex.org/W2145080939']",2005-09-13
https://openalex.org/W2157331557,https://doi.org/10.3115/v1/d14-1179,Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation,"Kyunghyun Cho, Bart van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2014.","['https://openalex.org/W4285719527', 'https://openalex.org/W2117278770', 'https://openalex.org/W2103305545', 'https://openalex.org/W1905522558', 'https://openalex.org/W1675954498', 'https://openalex.org/W2950682695', 'https://openalex.org/W2147768505', 'https://openalex.org/W2161792612', 'https://openalex.org/W2163605009', 'https://openalex.org/W2398269847', 'https://openalex.org/W1606347560', 'https://openalex.org/W1490600648', 'https://openalex.org/W1753482797', 'https://openalex.org/W2064675550', 'https://openalex.org/W2153579005', 'https://openalex.org/W2963504252', 'https://openalex.org/W22168010', 'https://openalex.org/W2251222643', 'https://openalex.org/W2251682575', 'https://openalex.org/W3037950864', 'https://openalex.org/W1970689298', 'https://openalex.org/W2250489405', 'https://openalex.org/W2118090838', 'https://openalex.org/W2251098065', 'https://openalex.org/W2153653739', 'https://openalex.org/W2964335273', 'https://openalex.org/W2156387975', 'https://openalex.org/W2108563286', 'https://openalex.org/W2144499799', 'https://openalex.org/W932413789', 'https://openalex.org/W6908809']",2014-01-01
https://openalex.org/W3139918052,https://doi.org/10.48550/arxiv.2104.02133,SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network,"We present SpeechStew, a speech recognition model that is trained on a combination of various publicly available speech recognition datasets: AMI, Broadcast News, Common Voice, LibriSpeech, Switchboard/Fisher, Tedlium, and Wall Street Journal. SpeechStew simply mixes all of these datasets together, without any special re-weighting or re-balancing of the datasets. SpeechStew achieves SoTA or near SoTA results across a variety of tasks, without the use of an external language model. Our results include 9.0\% WER on AMI-IHM, 4.7\% WER on Switchboard, 8.3\% WER on CallHome, and 1.3\% on WSJ, which significantly outperforms prior work with strong external language models. We also demonstrate that SpeechStew learns powerful transfer learning representations. We fine-tune SpeechStew on a noisy low resource speech dataset, CHiME-6. We achieve 38.9\% WER without a language model, which compares to 38.6\% WER to a strong HMM baseline with a language model.","['https://openalex.org/W2155893237', 'https://openalex.org/W2168231600', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962911098', 'https://openalex.org/W3098903812', 'https://openalex.org/W2155541015', 'https://openalex.org/W2102113734', 'https://openalex.org/W2963742216', 'https://openalex.org/W3030437843', 'https://openalex.org/W2936774411', 'https://openalex.org/W2995181338', 'https://openalex.org/W3131736947', 'https://openalex.org/W3036601975', 'https://openalex.org/W2996603747', 'https://openalex.org/W2327501763', 'https://openalex.org/W3026041220', 'https://openalex.org/W2125336414', 'https://openalex.org/W3015726069', 'https://openalex.org/W3100859887', 'https://openalex.org/W1828163288', 'https://openalex.org/W2963122170', 'https://openalex.org/W2108677974', 'https://openalex.org/W2250357346', 'https://openalex.org/W2900212944', 'https://openalex.org/W3094841848', 'https://openalex.org/W2123798005', 'https://openalex.org/W3099782249', 'https://openalex.org/W3093579165', 'https://openalex.org/W2973049979', 'https://openalex.org/W3016400019', 'https://openalex.org/W3035445001', 'https://openalex.org/W3097217077', 'https://openalex.org/W2804935296', 'https://openalex.org/W3097777922', 'https://openalex.org/W3093502935', 'https://openalex.org/W3001279689', 'https://openalex.org/W2964002616', 'https://openalex.org/W2530876040', 'https://openalex.org/W3101648800', 'https://openalex.org/W2991213871', 'https://openalex.org/W3149390355', 'https://openalex.org/W2163605009', 'https://openalex.org/W2962824709', 'https://openalex.org/W2889282842', 'https://openalex.org/W2939690918', 'https://openalex.org/W2963403868', 'https://openalex.org/W3147962056', 'https://openalex.org/W3018441253', 'https://openalex.org/W2963341956', 'https://openalex.org/W3131505732', 'https://openalex.org/W3015995734', 'https://openalex.org/W2194775991', 'https://openalex.org/W1978660892', 'https://openalex.org/W1524333225']",2021-04-05
https://openalex.org/W2095705004,,Dropout: a simple way to prevent neural networks from overfitting,"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned ” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.","['https://openalex.org/W2085040216', 'https://openalex.org/W2335728318', 'https://openalex.org/W2611675901', 'https://openalex.org/W2096873754', 'https://openalex.org/W1993882792', 'https://openalex.org/W189596042', 'https://openalex.org/W35527955', 'https://openalex.org/W2963574257', 'https://openalex.org/W2053229256', 'https://openalex.org/W2147800946', 'https://openalex.org/W2136922672', 'https://openalex.org/W2103359087', 'https://openalex.org/W2100495367', 'https://openalex.org/W2971788173', 'https://openalex.org/W1567512734', 'https://openalex.org/W2546302380', 'https://openalex.org/W1492459858', 'https://openalex.org/W2135046866', 'https://openalex.org/W2145094598', 'https://openalex.org/W2150717117', 'https://openalex.org/W1524333225', 'https://openalex.org/W2163605009', 'https://openalex.org/W137106866', 'https://openalex.org/W2183112036', 'https://openalex.org/W2294059674', 'https://openalex.org/W2156163116', 'https://openalex.org/W3118608800', 'https://openalex.org/W2156297475', 'https://openalex.org/W2962820688', 'https://openalex.org/W2152722485', 'https://openalex.org/W2025768430', 'https://openalex.org/W2114296159', 'https://openalex.org/W2158542502', 'https://openalex.org/W2114733238', 'https://openalex.org/W2949821452', 'https://openalex.org/W2131241448']",2014-01-01
https://openalex.org/W4226054021,https://doi.org/10.1109/asru51503.2021.9687894,Fast-MD: Fast Multi-Decoder End-to-End Speech Translation with Non-Autoregressive Hidden Intermediates,"The multi-decoder (MD) end-to-end speech translation model has demonstrated high translation quality by searching for better intermediate automatic speech recognition (ASR) decoder states as hidden intermediates (HI). It is a two-pass decoding model decomposing the overall task into ASR and machine translation sub-tasks. However, the decoding speed is not fast enough for real-world applications because it conducts beam search for both sub-tasks during inference. We propose Fast-MD, a fast MD model that generates HI by non-autoregressive (NAR) decoding based on connectionist temporal classification (CTC) outputs followed by an ASR decoder. We investigated two types of NAR HI: (1) parallel HI by using an autoregressive Transformer ASR decoder and (2) masked HI by using Mask-CTC, which combines CTC and the conditional masked language model. To reduce a mismatch in the ASR decoder between teacher-forcing during training and conditioning on CTC outputs during testing, we also propose sampling CTC outputs during training. Experimental evaluations on three corpora show that Fast-MD achieved about 2× and 4× faster decoding speed than that of the naïve MD model on GPU and CPU with comparable translation quality. Adopting the Conformer encoder and intermediate CTC loss further boosts its quality without sacrificing decoding speed.","['https://openalex.org/W2058094241', 'https://openalex.org/W3037217258', 'https://openalex.org/W3147414526', 'https://openalex.org/W6793772276', 'https://openalex.org/W6784364040', 'https://openalex.org/W3162431424', 'https://openalex.org/W3160622492', 'https://openalex.org/W3197140813', 'https://openalex.org/W3015960524', 'https://openalex.org/W3162899666', 'https://openalex.org/W6787040858', 'https://openalex.org/W6784050962', 'https://openalex.org/W3168212167', 'https://openalex.org/W3097030750', 'https://openalex.org/W3094903517', 'https://openalex.org/W2972818416', 'https://openalex.org/W3037625699', 'https://openalex.org/W3162249256', 'https://openalex.org/W6794247771', 'https://openalex.org/W3186672448', 'https://openalex.org/W2766219058', 'https://openalex.org/W3015783745', 'https://openalex.org/W2296073425', 'https://openalex.org/W2043701535', 'https://openalex.org/W2605131327', 'https://openalex.org/W2113106066', 'https://openalex.org/W3176382501', 'https://openalex.org/W3177132412', 'https://openalex.org/W3162919436', 'https://openalex.org/W2988975212', 'https://openalex.org/W3097882114', 'https://openalex.org/W3097777922', 'https://openalex.org/W2016589492', 'https://openalex.org/W3163793923', 'https://openalex.org/W3034571331', 'https://openalex.org/W3175809709', 'https://openalex.org/W2997436923', 'https://openalex.org/W2972448360', 'https://openalex.org/W2963532001', 'https://openalex.org/W2972780808', 'https://openalex.org/W6631190155', 'https://openalex.org/W2936969148', 'https://openalex.org/W2938973646', 'https://openalex.org/W6775053297', 'https://openalex.org/W3172862365', 'https://openalex.org/W6770250107', 'https://openalex.org/W3170787215', 'https://openalex.org/W2127141656', 'https://openalex.org/W6785130102', 'https://openalex.org/W3153583341', 'https://openalex.org/W3174501695', 'https://openalex.org/W3105669983', 'https://openalex.org/W2964172053', 'https://openalex.org/W2786891429', 'https://openalex.org/W2998386507', 'https://openalex.org/W3007142233', 'https://openalex.org/W6787135666', 'https://openalex.org/W3102811925', 'https://openalex.org/W2963250244', 'https://openalex.org/W2963779652', 'https://openalex.org/W2936774411', 'https://openalex.org/W2407080277', 'https://openalex.org/W2962784628', 'https://openalex.org/W2124807415', 'https://openalex.org/W6748215858', 'https://openalex.org/W6898505805', 'https://openalex.org/W6631362777', 'https://openalex.org/W2785350307', 'https://openalex.org/W1522301498', 'https://openalex.org/W1524333225', 'https://openalex.org/W3118578889', 'https://openalex.org/W3097301532', 'https://openalex.org/W3111562797', 'https://openalex.org/W3148986242', 'https://openalex.org/W4385245566', 'https://openalex.org/W4300558631', 'https://openalex.org/W3007328579', 'https://openalex.org/W3157233339', 'https://openalex.org/W2989224798', 'https://openalex.org/W3012492057', 'https://openalex.org/W3113676066', 'https://openalex.org/W3097438275']",2021-12-13
https://openalex.org/W4281789500,https://doi.org/10.48550/arxiv.2205.12523,TranSpeech: Speech-to-Speech Translation With Bilateral Perturbation,"Direct speech-to-speech translation (S2ST) with discrete units leverages recent progress in speech representation learning. Specifically, a sequence of discrete representations derived in a self-supervised manner are predicted from the model and passed to a vocoder for speech reconstruction, while still facing the following challenges: 1) Acoustic multimodality: the discrete units derived from speech with same content could be indeterministic due to the acoustic property (e.g., rhythm, pitch, and energy), which causes deterioration of translation accuracy; 2) high latency: current S2ST systems utilize autoregressive models which predict each unit conditioned on the sequence previously generated, failing to take full advantage of parallelism. In this work, we propose TranSpeech, a speech-to-speech translation model with bilateral perturbation. To alleviate the acoustic multimodal problem, we propose bilateral perturbation (BiP), which consists of the style normalization and information enhancement stages, to learn only the linguistic information from speech samples and generate more deterministic representations. With reduced multimodality, we step forward and become the first to establish a non-autoregressive S2ST technique, which repeatedly masks and predicts unit choices and produces high-accuracy results in just a few cycles. Experimental results on three language pairs demonstrate that BiP yields an improvement of 2.9 BLEU on average compared with a baseline textless S2ST model. Moreover, our parallel decoding shows a significant reduction of inference latency, enabling speedup up to 21.4x than autoregressive technique. Audio samples are available at \url{https://TranSpeech.github.io/}",[],2022-05-25
https://openalex.org/W3174864715,https://doi.org/10.48550/arxiv.2106.14448,R-Drop: Regularized Dropout for Neural Networks,"Dropout is a powerful and widely used technique to regularize the training of deep neural networks. In this paper, we introduce a simple regularization strategy upon dropout in model training, namely R-Drop, which forces the output distributions of different sub models generated by dropout to be consistent with each other. Specifically, for each training sample, R-Drop minimizes the bidirectional KL-divergence between the output distributions of two sub models sampled by dropout. Theoretical analysis reveals that R-Drop reduces the freedom of the model parameters and complements dropout. Experiments on $\bf{5}$ widely used deep learning tasks ($\bf{18}$ datasets in total), including neural machine translation, abstractive summarization, language understanding, language modeling, and image classification, show that R-Drop is universally effective. In particular, it yields substantial improvements when applied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large, and BART, and achieves state-of-the-art (SOTA) performances with the vanilla Transformer model on WMT14 English$\to$German translation ($\bf{30.91}$ BLEU) and WMT14 English$\to$French translation ($\bf{43.95}$ BLEU), even surpassing models trained with extra large-scale data and expert-designed advanced variants of Transformer models. Our code is available at GitHub{\url{https://github.com/dropreg/R-Drop}}.","['https://openalex.org/W2970597249', 'https://openalex.org/W2963532001', 'https://openalex.org/W2479750863', 'https://openalex.org/W2144513243', 'https://openalex.org/W2983128379', 'https://openalex.org/W2964059111', 'https://openalex.org/W1677182931', 'https://openalex.org/W3138516171', 'https://openalex.org/W3156636935', 'https://openalex.org/W2943008967', 'https://openalex.org/W2933138175', 'https://openalex.org/W1904365287', 'https://openalex.org/W3119716506', 'https://openalex.org/W1821462560', 'https://openalex.org/W2965373594', 'https://openalex.org/W2963403868', 'https://openalex.org/W2996035354', 'https://openalex.org/W2963543570', 'https://openalex.org/W3103361051', 'https://openalex.org/W3119786062', 'https://openalex.org/W2963266340', 'https://openalex.org/W2136836265', 'https://openalex.org/W2963744496', 'https://openalex.org/W2963685250', 'https://openalex.org/W2994928925', 'https://openalex.org/W2964222566', 'https://openalex.org/W2163605009', 'https://openalex.org/W2183341477', 'https://openalex.org/W2963341956', 'https://openalex.org/W2620998106', 'https://openalex.org/W3123681568', 'https://openalex.org/W1836465849', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963310665', 'https://openalex.org/W2525332836', 'https://openalex.org/W2987861506', 'https://openalex.org/W3124384256', 'https://openalex.org/W3100439847', 'https://openalex.org/W2962784628', 'https://openalex.org/W3098985395', 'https://openalex.org/W3113303810', 'https://openalex.org/W2143163787', 'https://openalex.org/W2993587506', 'https://openalex.org/W3034715004', 'https://openalex.org/W2523888555', 'https://openalex.org/W3102220759', 'https://openalex.org/W2964093309', 'https://openalex.org/W1519506695', 'https://openalex.org/W3097217077', 'https://openalex.org/W3155713635', 'https://openalex.org/W1544827683', 'https://openalex.org/W2582745083', 'https://openalex.org/W2160204597', 'https://openalex.org/W3066373881', 'https://openalex.org/W3129779966', 'https://openalex.org/W3089659770', 'https://openalex.org/W35527955', 'https://openalex.org/W2963117513', 'https://openalex.org/W2989571009', 'https://openalex.org/W2962686221', 'https://openalex.org/W2117499988', 'https://openalex.org/W2964293126', 'https://openalex.org/W3118608800', 'https://openalex.org/W2963807318', 'https://openalex.org/W3030163527', 'https://openalex.org/W2962832505', 'https://openalex.org/W2982399380', 'https://openalex.org/W2963000224', 'https://openalex.org/W2746314669', 'https://openalex.org/W3119866685', 'https://openalex.org/W3127622310', 'https://openalex.org/W1034159276', 'https://openalex.org/W2963631907', 'https://openalex.org/W4919037']",2021-06-28
https://openalex.org/W3118578889,https://doi.org/10.48550/arxiv.2010.05171,fairseq S2T: Fast Speech-to-Text Modeling with fairseq,"We introduce fairseq S2T, a fairseq extension for speech-to-text (S2T) modeling tasks such as end-to-end speech recognition and speech-to-text translation. It follows fairseq's careful design for scalability and extensibility. We provide end-to-end workflows from data pre-processing, model training to offline (online) inference. We implement state-of-the-art RNN-based, Transformer-based as well as Conformer-based models and open-source detailed training recipes. Fairseq's machine translation models and language models can be seamlessly integrated into S2T workflows for multi-task learning or transfer learning. Fairseq S2T documentation and examples are available at https://github.com/pytorch/fairseq/tree/master/examples/speech_to_text.","['https://openalex.org/W2250342921', 'https://openalex.org/W2945700568', 'https://openalex.org/W3101727459', 'https://openalex.org/W3035485128', 'https://openalex.org/W2890785942', 'https://openalex.org/W3037217258', 'https://openalex.org/W2466918907', 'https://openalex.org/W2958953787', 'https://openalex.org/W1524333225', 'https://openalex.org/W2890536590', 'https://openalex.org/W2963403868', 'https://openalex.org/W2995428172', 'https://openalex.org/W2963779652', 'https://openalex.org/W2101105183', 'https://openalex.org/W2936774411', 'https://openalex.org/W2952992734', 'https://openalex.org/W2978099976', 'https://openalex.org/W3099878876', 'https://openalex.org/W2582956876', 'https://openalex.org/W2973049979', 'https://openalex.org/W3035390927', 'https://openalex.org/W2953190524', 'https://openalex.org/W2970295111', 'https://openalex.org/W3054645415', 'https://openalex.org/W2946888380', 'https://openalex.org/W3008125272', 'https://openalex.org/W2933138175', 'https://openalex.org/W2963532001', 'https://openalex.org/W2127141656', 'https://openalex.org/W2963112338', 'https://openalex.org/W2970971581', 'https://openalex.org/W1494198834', 'https://openalex.org/W3103029570', 'https://openalex.org/W2941814890', 'https://openalex.org/W3097787369', 'https://openalex.org/W3037465386', 'https://openalex.org/W2998353611', 'https://openalex.org/W3096490862', 'https://openalex.org/W2991213871', 'https://openalex.org/W2799923439', 'https://openalex.org/W2998386507', 'https://openalex.org/W2928941594']",2020-10-11
https://openalex.org/W3161302809,https://doi.org/10.1109/icassp39728.2021.9414558,St-Bert: Cross-Modal Language Model Pre-Training for End-to-End Spoken Language Understanding,"Language model pre-training has shown promising results in various downstream tasks. In this context, we introduce a cross-modal pre-trained language model, called Speech-Text BERT (ST-BERT), to tackle end-to-end spoken language understanding (E2E SLU) tasks. Taking phoneme posterior and subword-level text as an input, ST-BERT learns a contextualized cross-modal alignment via our two proposed pre-training tasks: Cross-modal Masked Language Modeling (CM-MLM) and Cross-modal Conditioned Language Modeling (CM-CLM). Experimental results on three benchmarks present that our approach is effective for various SLU datasets and shows a surprisingly marginal performance degradation even when 1% of the training data are available. Also, our method shows further SLU performance gain via domain-adaptive pre-training with domain-specific speech-text pair data.","['https://openalex.org/W6776527912', 'https://openalex.org/W6754850772', 'https://openalex.org/W2891229414', 'https://openalex.org/W2972314145', 'https://openalex.org/W2964052309', 'https://openalex.org/W2998356391', 'https://openalex.org/W6766904570', 'https://openalex.org/W1933349210', 'https://openalex.org/W2963115613', 'https://openalex.org/W6751433836', 'https://openalex.org/W6638523607', 'https://openalex.org/W3015267417', 'https://openalex.org/W6755207826', 'https://openalex.org/W6751425476', 'https://openalex.org/W6776020682', 'https://openalex.org/W3096589040', 'https://openalex.org/W6755871579', 'https://openalex.org/W3097414768', 'https://openalex.org/W6759455113', 'https://openalex.org/W1494198834', 'https://openalex.org/W2972584841', 'https://openalex.org/W2977838803', 'https://openalex.org/W2963288440', 'https://openalex.org/W2971207485', 'https://openalex.org/W2747874407', 'https://openalex.org/W3034571331', 'https://openalex.org/W6631190155', 'https://openalex.org/W2296073425', 'https://openalex.org/W4297683418', 'https://openalex.org/W1522301498', 'https://openalex.org/W2971863715', 'https://openalex.org/W2963341956', 'https://openalex.org/W3176896803', 'https://openalex.org/W2963340922', 'https://openalex.org/W2525778437', 'https://openalex.org/W2898771141', 'https://openalex.org/W2804648901', 'https://openalex.org/W2966715458', 'https://openalex.org/W2970608575', 'https://openalex.org/W3015586639', 'https://openalex.org/W3034238904', 'https://openalex.org/W2896457183', 'https://openalex.org/W2895976713', 'https://openalex.org/W2803609229', 'https://openalex.org/W2981458636', 'https://openalex.org/W3015412890', 'https://openalex.org/W3099944122', 'https://openalex.org/W1821462560', 'https://openalex.org/W2914120296', 'https://openalex.org/W2964121744']",2021-05-13
https://openalex.org/W3162313915,https://doi.org/10.1109/icassp39728.2021.9414900,Speech-Language Pre-Training for End-to-End Spoken Language Understanding,"End-to-end (E2E) spoken language understanding (SLU) can infer semantics directly from speech signal without cascading an automatic speech recognizer (ASR) with a natural language understanding (NLU) module. However, paired utterance recordings and corresponding semantics may not always be available or sufficient to train an E2E SLU model in a real production environment. In this paper, we propose to unify a well-optimized E2E ASR encoder (speech) and a pre-trained language model encoder (language) into a transformer decoder. The unified speech-language pre-trained model (SLP) is continually enhanced on limited labeled data from a target domain by using a conditional masked language model (MLM) objective, and thus can effectively generate a sequence of intent, slot type, and slot value for given input speech in the inference. The experimental results on two public corpora show that our approach to E2E SLU is superior to the conventional cascaded method. It also outperforms the present state-of-the-art approaches to E2E SLU with much less paired data.","['https://openalex.org/W3096109555', 'https://openalex.org/W3035676545', 'https://openalex.org/W3096398645', 'https://openalex.org/W3016006013', 'https://openalex.org/W6783507577', 'https://openalex.org/W6769627184', 'https://openalex.org/W6759393636', 'https://openalex.org/W3102854726', 'https://openalex.org/W2945475330', 'https://openalex.org/W2988647680', 'https://openalex.org/W6780226713', 'https://openalex.org/W2891229414', 'https://openalex.org/W6623517193', 'https://openalex.org/W6748263443', 'https://openalex.org/W2972584841', 'https://openalex.org/W2894164357', 'https://openalex.org/W6776020682', 'https://openalex.org/W2963288440', 'https://openalex.org/W6768263079', 'https://openalex.org/W6755207826', 'https://openalex.org/W3015885816', 'https://openalex.org/W3096589040', 'https://openalex.org/W6739901393', 'https://openalex.org/W6762122294', 'https://openalex.org/W2803392141', 'https://openalex.org/W2077302143', 'https://openalex.org/W3038268361', 'https://openalex.org/W2971274815', 'https://openalex.org/W2945260553', 'https://openalex.org/W2786839803', 'https://openalex.org/W2963341956', 'https://openalex.org/W854541894', 'https://openalex.org/W3015412890', 'https://openalex.org/W2525778437', 'https://openalex.org/W3082274269', 'https://openalex.org/W4385245566', 'https://openalex.org/W3026408381', 'https://openalex.org/W2963403868', 'https://openalex.org/W2997591391', 'https://openalex.org/W2896457183', 'https://openalex.org/W2917128112', 'https://openalex.org/W3091835551', 'https://openalex.org/W4288089799']",2021-05-13
https://openalex.org/W4319862474,https://doi.org/10.1109/slt54892.2023.10022774,JOIST: A Joint Speech and Text Streaming Model for ASR,"We present JOIST, an algorithm to train a streaming, cascaded, encoder end-to-end (E2E) model with both speech-text paired inputs, and text-only unpaired inputs. Unlike previous works, we explore joint training with both modalities, rather than pre-training and fine-tuning. In addition, we explore JOIST using a streaming E2E model with an order of magnitude more data, which are also novelties compared to previous works. Through a series of ablation studies, we explore different types of text modeling, including how to model the length of the text sequence and the appropriate text subword unit representation. We find that best text representation for JOIST improves WER across a variety of search and rare-word test sets by 4-14% relative, compared to a model not trained with text. In addition, we quantitatively show that JOIST maintains streaming capabilities, which is important for good user-level experience.","['https://openalex.org/W3095311338', 'https://openalex.org/W2962760690', 'https://openalex.org/W2962824709', 'https://openalex.org/W2526425061', 'https://openalex.org/W3007227084', 'https://openalex.org/W3028545098', 'https://openalex.org/W3198442913', 'https://openalex.org/W2577366047', 'https://openalex.org/W2963240019', 'https://openalex.org/W4210690962', 'https://openalex.org/W2963739817', 'https://openalex.org/W2962699523', 'https://openalex.org/W4226093136', 'https://openalex.org/W3015556645', 'https://openalex.org/W6803092890', 'https://openalex.org/W6810259195', 'https://openalex.org/W4223622550', 'https://openalex.org/W4225319488', 'https://openalex.org/W4226120743', 'https://openalex.org/W6792221667', 'https://openalex.org/W3205644108', 'https://openalex.org/W6784572113', 'https://openalex.org/W4225307083', 'https://openalex.org/W3212799896', 'https://openalex.org/W6666758853', 'https://openalex.org/W2076794394', 'https://openalex.org/W2394932179', 'https://openalex.org/W2293634267', 'https://openalex.org/W2963747784', 'https://openalex.org/W2750499125', 'https://openalex.org/W2963382687', 'https://openalex.org/W3008181812', 'https://openalex.org/W2842511635', 'https://openalex.org/W2973049979', 'https://openalex.org/W3016011332', 'https://openalex.org/W2982223350', 'https://openalex.org/W6755207826', 'https://openalex.org/W3209059054', 'https://openalex.org/W6769196770', 'https://openalex.org/W6810673746', 'https://openalex.org/W6780218876', 'https://openalex.org/W4226033575', 'https://openalex.org/W3160235762', 'https://openalex.org/W1494198834', 'https://openalex.org/W4224916448', 'https://openalex.org/W3034999214', 'https://openalex.org/W2143612262', 'https://openalex.org/W3016234571', 'https://openalex.org/W2121879602', 'https://openalex.org/W2515439472', 'https://openalex.org/W10731371', 'https://openalex.org/W3094979069', 'https://openalex.org/W3095697114', 'https://openalex.org/W3160766462', 'https://openalex.org/W2935756939', 'https://openalex.org/W4225272718', 'https://openalex.org/W2033256038', 'https://openalex.org/W4225529283', 'https://openalex.org/W2617258110', 'https://openalex.org/W6688428952', 'https://openalex.org/W2013598660', 'https://openalex.org/W2936774411', 'https://openalex.org/W2296545762', 'https://openalex.org/W2296014541', 'https://openalex.org/W3096815019', 'https://openalex.org/W2507771204', 'https://openalex.org/W3097777922', 'https://openalex.org/W3011411500', 'https://openalex.org/W3197976839', 'https://openalex.org/W3163907627', 'https://openalex.org/W2937402758', 'https://openalex.org/W3207222250', 'https://openalex.org/W4221155340', 'https://openalex.org/W4297808394']",2023-01-09
https://openalex.org/W3209371554,https://doi.org/10.1109/icassp43922.2022.9747760,Optimizing Alignment of Speech and Language Latent Spaces for End-To-End Speech Recognition and Understanding,"The advances in attention-based encoder-decoder (AED) networks have brought great progress to end-to-end (E2E) automatic speech recognition (ASR). One way to further improve the performance of AED-based E2E ASR is to introduce an extra text encoder for leveraging extensive text data and thus capture more context-aware linguistic information. However, this approach brings a mismatch problem between the speech encoder and the text encoder due to the different units used for modeling. In this paper, we propose an embedding aligner and modality switch training to better align the speech and text latent spaces. The embedding aligner is a shared linear projection between text encoder and speech encoder trained by masked language modeling (MLM) loss and connectionist temporal classification (CTC), respectively. The modality switch training randomly swaps speech and text embeddings based on the forced alignment result to learn a joint representation space. Experimental results show that our proposed approach achieves a relative 14% to 19% word error rate (WER) reduction on Librispeech ASR task. We further verify its effectiveness on spoken language understanding (SLU), i.e., an absolute 2.5% to 2.8% F1 score improvement on SNIPS slot filling task.","['https://openalex.org/W2962961016', 'https://openalex.org/W4210690962', 'https://openalex.org/W2964161387', 'https://openalex.org/W3024464021', 'https://openalex.org/W6791205002', 'https://openalex.org/W6782964881', 'https://openalex.org/W3106321930', 'https://openalex.org/W2963979492', 'https://openalex.org/W6739901393', 'https://openalex.org/W2936774411', 'https://openalex.org/W2526425061', 'https://openalex.org/W6623517193', 'https://openalex.org/W2964012862', 'https://openalex.org/W3162037819', 'https://openalex.org/W3016006013', 'https://openalex.org/W3008370744', 'https://openalex.org/W2799800213', 'https://openalex.org/W2997436923', 'https://openalex.org/W2962824709', 'https://openalex.org/W6631190155', 'https://openalex.org/W2514741789', 'https://openalex.org/W6631362777', 'https://openalex.org/W3197580070', 'https://openalex.org/W2962780374', 'https://openalex.org/W6770812743', 'https://openalex.org/W3085104073', 'https://openalex.org/W2964121744', 'https://openalex.org/W4385245566', 'https://openalex.org/W3197744084', 'https://openalex.org/W1522301498', 'https://openalex.org/W1524333225', 'https://openalex.org/W2402146185', 'https://openalex.org/W3095189764', 'https://openalex.org/W3157923770', 'https://openalex.org/W2992632249', 'https://openalex.org/W2963403868', 'https://openalex.org/W3105912780', 'https://openalex.org/W3197074955', 'https://openalex.org/W3107298252', 'https://openalex.org/W854541894', 'https://openalex.org/W3092327118']",2022-04-27
https://openalex.org/W3169688220,https://doi.org/10.48550/arxiv.2106.06909,"GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio","This paper introduces GigaSpeech, an evolving, multi-domain English speech recognition corpus with 10,000 hours of high quality labeled audio suitable for supervised training, and 40,000 hours of total audio suitable for semi-supervised and unsupervised training. Around 40,000 hours of transcribed audio is first collected from audiobooks, podcasts and YouTube, covering both read and spontaneous speaking styles, and a variety of topics, such as arts, science, sports, etc. A new forced alignment and segmentation pipeline is proposed to create sentence segments suitable for speech recognition training, and to filter out segments with low-quality transcription. For system training, GigaSpeech provides five subsets of different sizes, 10h, 250h, 1000h, 2500h, and 10000h. For our 10,000-hour XL training subset, we cap the word error rate at 4% during the filtering/validation stage, and for all our other smaller training subsets, we cap it at 0%. The DEV and TEST evaluation sets, on the other hand, are re-processed by professional human transcribers to ensure high transcription quality. Baseline systems are provided for popular speech recognition toolkits, namely Athena, ESPnet, Kaldi and Pika.","['https://openalex.org/W1524333225', 'https://openalex.org/W2147768505', 'https://openalex.org/W3143186397', 'https://openalex.org/W2962780374', 'https://openalex.org/W2972818416', 'https://openalex.org/W2102113734', 'https://openalex.org/W2604132379', 'https://openalex.org/W2089499735', 'https://openalex.org/W3101648800', 'https://openalex.org/W2963403868', 'https://openalex.org/W3097777922', 'https://openalex.org/W3094800360', 'https://openalex.org/W3095697114', 'https://openalex.org/W2885185669', 'https://openalex.org/W2514741789', 'https://openalex.org/W1494198834', 'https://openalex.org/W2786234940']",2021-06-13
https://openalex.org/W3111551570,https://doi.org/10.1109/cvpr46437.2021.01268,Taming Transformers for High-Resolution Image Synthesis,"Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at https://github.com/CompVis/taming-transformers .","['https://openalex.org/W6724670942', 'https://openalex.org/W2962785568', 'https://openalex.org/W3034723751', 'https://openalex.org/W6767480857', 'https://openalex.org/W2981721547', 'https://openalex.org/W6752910514', 'https://openalex.org/W3035574324', 'https://openalex.org/W2903739847', 'https://openalex.org/W6693848384', 'https://openalex.org/W6640963894', 'https://openalex.org/W6780593937', 'https://openalex.org/W6779093361', 'https://openalex.org/W6739901393', 'https://openalex.org/W6745560452', 'https://openalex.org/W2963800363', 'https://openalex.org/W6763239785', 'https://openalex.org/W2962770929', 'https://openalex.org/W6762780510', 'https://openalex.org/W6625168331', 'https://openalex.org/W6784347278', 'https://openalex.org/W6782908428', 'https://openalex.org/W3025973238', 'https://openalex.org/W6753059488', 'https://openalex.org/W6772853553', 'https://openalex.org/W6702130928', 'https://openalex.org/W2963073614', 'https://openalex.org/W6767264202', 'https://openalex.org/W3035687950', 'https://openalex.org/W6767627451', 'https://openalex.org/W6785102375', 'https://openalex.org/W3108329879', 'https://openalex.org/W6639732818', 'https://openalex.org/W2955639361', 'https://openalex.org/W6747491877', 'https://openalex.org/W6786494455', 'https://openalex.org/W6748148878', 'https://openalex.org/W6758800702', 'https://openalex.org/W2108598243', 'https://openalex.org/W6692550842', 'https://openalex.org/W3034776267', 'https://openalex.org/W2922386270', 'https://openalex.org/W6755312952', 'https://openalex.org/W6690026940', 'https://openalex.org/W2561196672', 'https://openalex.org/W6778883912', 'https://openalex.org/W2412782625', 'https://openalex.org/W3034625979', 'https://openalex.org/W2963522749', 'https://openalex.org/W2962974533', 'https://openalex.org/W6763509872', 'https://openalex.org/W6756533288', 'https://openalex.org/W2471768434', 'https://openalex.org/W2171108400', 'https://openalex.org/W2963428348', 'https://openalex.org/W2971480596', 'https://openalex.org/W2348664362', 'https://openalex.org/W2963045354', 'https://openalex.org/W3098510582', 'https://openalex.org/W2962760235', 'https://openalex.org/W2881214865', 'https://openalex.org/W2963139417', 'https://openalex.org/W967544008', 'https://openalex.org/W2962820504', 'https://openalex.org/W3118605064', 'https://openalex.org/W3176823897', 'https://openalex.org/W2963174698', 'https://openalex.org/W2940744433', 'https://openalex.org/W2998108143', 'https://openalex.org/W3162926177', 'https://openalex.org/W2963799213', 'https://openalex.org/W3103781353', 'https://openalex.org/W3083914148', 'https://openalex.org/W2949847915', 'https://openalex.org/W3092033429', 'https://openalex.org/W1959608418', 'https://openalex.org/W2331128040', 'https://openalex.org/W2242818861', 'https://openalex.org/W2099471712', 'https://openalex.org/W2964308564', 'https://openalex.org/W3081167590', 'https://openalex.org/W2267126114', 'https://openalex.org/W2194775991', 'https://openalex.org/W3104876213', 'https://openalex.org/W2962897886', 'https://openalex.org/W2945924057', 'https://openalex.org/W2971074500', 'https://openalex.org/W2547875792', 'https://openalex.org/W2996287690', 'https://openalex.org/W2970718407', 'https://openalex.org/W3030163527', 'https://openalex.org/W2413794162', 'https://openalex.org/W2269752429', 'https://openalex.org/W2970315999', 'https://openalex.org/W3106570356', 'https://openalex.org/W2902630600', 'https://openalex.org/W2893749619', 'https://openalex.org/W3099188186', 'https://openalex.org/W2994760783', 'https://openalex.org/W2963403868', 'https://openalex.org/W2507296351']",2021-06-01
https://openalex.org/W4298277274,,Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package,"Dynamic time warping is a popular technique for comparing time series, providing both a distance measure that is insensitive to local compression and stretches and the warping which optimally deforms one of the two input series onto the other. A variety of algorithms and constraints have been discussed in the literature. The dtw package provides an unification of them; it allows R users to compute time series alignments mixing freely a variety of continuity constraints, restriction windows, endpoints, local distance definitions, and so on. The package also provides functions for visualizing alignments and constraints using several classic diagram types.",[],2013-08-12
https://openalex.org/W2963981733,https://doi.org/10.48550/arxiv.1706.08500,GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium,"Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the ""Fréchet Inception Distance"" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.",[],2017-06-26
https://openalex.org/W1574170747,https://doi.org/10.21437/interspeech.2008-644,Robust signal-to-noise ratio estimation based on waveform amplitude distribution analysis,"Abstract In this paper, we introduce a new algorithm for estimating the signal-to-noise ratio (SNR) of speech signals, called WADA-SNR (Waveform Amplitude Distribution Analysis). In this algorithm we assume that the amplitude distribution of clean speech can be approximated by the Gamma distribution with a shaping parameter of 0.4, and that an additive noise signal is Gaussian. Based on this assumption, we can estimate the SNR by examining the amplitude distribution of the noise-corrupted speech. We evaluate the performance of the WADA-SNR algorithm on databases corrupted by white noise, background music, and interfering speech. The WADA-SNR algorithm shows significantly less bias and less variability with respect to the type of noise compared to the standard NIST STNR algorithm. In addition, the algorithm is quite computationally efficient. Index Terms : SNR estimation, Gamma distribution, Gaussian distribution 1. Introduction The estimation of signal-to-noise ratios (SNRs) has been extensively investigated for decades and it is still an active field of research (","['https://openalex.org/W2138456300', 'https://openalex.org/W2158336491', 'https://openalex.org/W1631909651', 'https://openalex.org/W2153894152', 'https://openalex.org/W2166529462', 'https://openalex.org/W2056612043', 'https://openalex.org/W1976011531', 'https://openalex.org/W202024829', 'https://openalex.org/W2144242878', 'https://openalex.org/W2069501481', 'https://openalex.org/W70821729', 'https://openalex.org/W2144561273']",2008-09-22
https://openalex.org/W3026874504,https://doi.org/10.48550/arxiv.2005.11129,Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search,"Recently, text-to-speech (TTS) models such as FastSpeech and ParaNet have been proposed to generate mel-spectrograms from text in parallel. Despite the advantage, the parallel TTS models cannot be trained without guidance from autoregressive TTS models as their external aligners. In this work, we propose Glow-TTS, a flow-based generative model for parallel TTS that does not require any external aligner. By combining the properties of flows and dynamic programming, the proposed model searches for the most probable monotonic alignment between text and the latent representation of speech on its own. We demonstrate that enforcing hard monotonic alignments enables robust TTS, which generalizes to long utterances, and employing generative flows enables fast, diverse, and controllable speech synthesis. Glow-TTS obtains an order-of-magnitude speed-up over the autoregressive model, Tacotron 2, at synthesis with comparable speech quality. We further show that our model can be easily extended to a multi-speaker setting.","['https://openalex.org/W2463507112', 'https://openalex.org/W2788851830', 'https://openalex.org/W2911827218', 'https://openalex.org/W2963139417', 'https://openalex.org/W3015922793', 'https://openalex.org/W2127141656', 'https://openalex.org/W2962695743', 'https://openalex.org/W2963799213', 'https://openalex.org/W2125838338', 'https://openalex.org/W2970898247', 'https://openalex.org/W2769810959', 'https://openalex.org/W2964121744', 'https://openalex.org/W2767206889', 'https://openalex.org/W2794490148', 'https://openalex.org/W1583912456', 'https://openalex.org/W3025528898', 'https://openalex.org/W2903739847', 'https://openalex.org/W2970351109', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963712897', 'https://openalex.org/W2970730223', 'https://openalex.org/W2963927338', 'https://openalex.org/W2985856318', 'https://openalex.org/W2963403868', 'https://openalex.org/W3035083561', 'https://openalex.org/W2963300588', 'https://openalex.org/W2949382160', 'https://openalex.org/W2963432880', 'https://openalex.org/W2963691546', 'https://openalex.org/W2972359262', 'https://openalex.org/W2963925437', 'https://openalex.org/W2962882868', 'https://openalex.org/W2111284386']",2020-05-22
https://openalex.org/W3023126978,https://doi.org/10.48550/arxiv.2005.00205,Multi-head Monotonic Chunkwise Attention For Online Speech Recognition,"The attention mechanism of the Listen, Attend and Spell (LAS) model requires the whole input sequence to calculate the attention context and thus is not suitable for online speech recognition. To deal with this problem, we propose multi-head monotonic chunk-wise attention (MTH-MoChA), an improved version of MoChA. MTH-MoChA splits the input sequence into small chunks and computes multi-head attentions over the chunks. We also explore useful training strategies such as LSTM pooling, minimum world error rate training and SpecAugment to further improve the performance of MTH-MoChA. Experiments on AISHELL-1 data show that the proposed model, along with the training strategies, improve the character error rate (CER) of MoChA from 8.96% to 7.68% on test set. On another 18000 hours in-car speech data set, MTH-MoChA obtains 7.28% CER, which is significantly better than a state-of-the-art hybrid system.","['https://openalex.org/W2963747784', 'https://openalex.org/W2962824709', 'https://openalex.org/W2963382687', 'https://openalex.org/W2964272710', 'https://openalex.org/W2605131327', 'https://openalex.org/W2928941594', 'https://openalex.org/W2183341477', 'https://openalex.org/W2143612262', 'https://openalex.org/W1828163288', 'https://openalex.org/W2962760690', 'https://openalex.org/W2963242190', 'https://openalex.org/W2972818416', 'https://openalex.org/W2888957211', 'https://openalex.org/W2514741789', 'https://openalex.org/W2605141709', 'https://openalex.org/W2963236196', 'https://openalex.org/W2936078256', 'https://openalex.org/W2972991710', 'https://openalex.org/W2912492482', 'https://openalex.org/W2515439472', 'https://openalex.org/W2965116050', 'https://openalex.org/W2936774411']",2020-05-01
https://openalex.org/W2742542661,https://doi.org/10.1109/taffc.2017.2736999,Building Naturalistic Emotionally Balanced Speech Corpus by Retrieving Emotional Speech from Existing Podcast Recordings,"The lack of a large, natural emotional database is one of the key barriers to translate results on speech emotion recognition in controlled conditions into real-life applications. Collecting emotional databases is expensive and time demanding, which limits the size of existing corpora. Current approaches used to collect spontaneous databases tend to provide unbalanced emotional content, which is dictated by the given recording protocol (e.g., positive for colloquial conversations, negative for discussion or debates). The size and speaker diversity are also limited. This paper proposes a novel approach to effectively build a large, naturalistic emotional database with balanced emotional content, reduced cost and reduced manual labor. It relies on existing spontaneous recordings obtained from audio-sharing websites. The proposed approach combines machine learning algorithms to retrieve recordings conveying balanced emotional content with a cost effective annotation process using crowdsourcing, which make it possible to build a large scale speech emotional database. This approach provides natural emotional renditions from multiple speakers, with different channel conditions and conveying balanced emotional content that are difficult to obtain with alternative data collection protocols.","['https://openalex.org/W2158630797', 'https://openalex.org/W6603931906', 'https://openalex.org/W6712777154', 'https://openalex.org/W2659927845', 'https://openalex.org/W1604358271', 'https://openalex.org/W2014893602', 'https://openalex.org/W1574170747', 'https://openalex.org/W6685951362', 'https://openalex.org/W1965520710', 'https://openalex.org/W2239141610', 'https://openalex.org/W2085662862', 'https://openalex.org/W1563099953', 'https://openalex.org/W2404863510', 'https://openalex.org/W2747172199', 'https://openalex.org/W1551188655', 'https://openalex.org/W2030931454', 'https://openalex.org/W2550557083', 'https://openalex.org/W60557504', 'https://openalex.org/W2032382719', 'https://openalex.org/W129316862', 'https://openalex.org/W2020944977', 'https://openalex.org/W6990412439', 'https://openalex.org/W2058787788', 'https://openalex.org/W2138077241', 'https://openalex.org/W2035720976', 'https://openalex.org/W2058475745', 'https://openalex.org/W2673304402', 'https://openalex.org/W2026984028', 'https://openalex.org/W2400120934', 'https://openalex.org/W6607261757', 'https://openalex.org/W1993008008', 'https://openalex.org/W2146334809', 'https://openalex.org/W2045528981', 'https://openalex.org/W2170876097', 'https://openalex.org/W2132555391', 'https://openalex.org/W2097732741', 'https://openalex.org/W2129794709', 'https://openalex.org/W2122563357', 'https://openalex.org/W2525412388', 'https://openalex.org/W2342475039', 'https://openalex.org/W2127531292', 'https://openalex.org/W6741262016', 'https://openalex.org/W6607193717', 'https://openalex.org/W6683165025', 'https://openalex.org/W2102953093', 'https://openalex.org/W2527527138', 'https://openalex.org/W1825415099', 'https://openalex.org/W2133990480', 'https://openalex.org/W2064149108', 'https://openalex.org/W2404446881', 'https://openalex.org/W6681409903', 'https://openalex.org/W2518511005', 'https://openalex.org/W2508783453', 'https://openalex.org/W6677944028', 'https://openalex.org/W2491770404', 'https://openalex.org/W1530474158', 'https://openalex.org/W175750906', 'https://openalex.org/W2144005487', 'https://openalex.org/W2984960739', 'https://openalex.org/W2119417805', 'https://openalex.org/W97072897', 'https://openalex.org/W2182389375', 'https://openalex.org/W2158943324', 'https://openalex.org/W2291445757', 'https://openalex.org/W2401417847', 'https://openalex.org/W179852215', 'https://openalex.org/W2148603752', 'https://openalex.org/W2735400762']",2017-08-07
https://openalex.org/W2981087920,https://doi.org/10.1016/j.csl.2019.101027,Voxceleb: Large-scale speaker verification in the wild,"The objective of this work is speaker recognition under noisy and unconstrained conditions. We make two key contributions. First, we introduce a very large-scale audio-visual dataset collected from open source media using a fully automated pipeline. Most existing datasets for speaker identification contain samples obtained under quite constrained conditions, and usually require manual annotations, hence are limited in size. We propose a pipeline based on computer vision techniques to create the dataset from open-source media. Our pipeline involves obtaining videos from YouTube; performing active speaker verification using a two-stream synchronization Convolutional Neural Network (CNN), and confirming the identity of the speaker using CNN based facial recognition. We use this pipeline to curate VoxCeleb which contains contains over a million 'real-world' utterances from over 6000 speakers. This is several times larger than any publicly available speaker recognition dataset. Second, we develop and compare different CNN architectures with various aggregation methods and training loss functions that can effectively recognise identities from voice under various conditions. The models trained on our dataset surpass the performance of previous works by a significant margin.","['https://openalex.org/W6750558598', 'https://openalex.org/W2886300652', 'https://openalex.org/W2179042386', 'https://openalex.org/W6696055461', 'https://openalex.org/W6743352670', 'https://openalex.org/W2963077989', 'https://openalex.org/W2964216323', 'https://openalex.org/W6745206590', 'https://openalex.org/W6702266465', 'https://openalex.org/W6648737282', 'https://openalex.org/W6652992263', 'https://openalex.org/W6683390034', 'https://openalex.org/W6737896281', 'https://openalex.org/W6752581720', 'https://openalex.org/W2793493229', 'https://openalex.org/W6734491695', 'https://openalex.org/W6735927292', 'https://openalex.org/W7011482893', 'https://openalex.org/W2150769028', 'https://openalex.org/W6750169759', 'https://openalex.org/W6738297741', 'https://openalex.org/W1635512741', 'https://openalex.org/W6696652321', 'https://openalex.org/W6684440257', 'https://openalex.org/W2399210150', 'https://openalex.org/W6726453277', 'https://openalex.org/W6680272447', 'https://openalex.org/W1936725236', 'https://openalex.org/W6682320352', 'https://openalex.org/W6687483927', 'https://openalex.org/W6677390740', 'https://openalex.org/W2126854862', 'https://openalex.org/W6635152626', 'https://openalex.org/W6635314807', 'https://openalex.org/W2739192055', 'https://openalex.org/W6672279060', 'https://openalex.org/W6686659972', 'https://openalex.org/W6677618333', 'https://openalex.org/W6684191040', 'https://openalex.org/W2941320063', 'https://openalex.org/W6660130284', 'https://openalex.org/W6785652829', 'https://openalex.org/W6736780073', 'https://openalex.org/W6623210947', 'https://openalex.org/W6633847657', 'https://openalex.org/W2516764878', 'https://openalex.org/W6639530811', 'https://openalex.org/W6981834165', 'https://openalex.org/W6750736135', 'https://openalex.org/W6750148349', 'https://openalex.org/W6740167877', 'https://openalex.org/W6700903540', 'https://openalex.org/W2041823554', 'https://openalex.org/W2165880886', 'https://openalex.org/W6677651945', 'https://openalex.org/W6738893770', 'https://openalex.org/W2890704021', 'https://openalex.org/W6637373629', 'https://openalex.org/W6742911084', 'https://openalex.org/W6754496211', 'https://openalex.org/W2963026686', 'https://openalex.org/W86526702', 'https://openalex.org/W6662018943', 'https://openalex.org/W1963882359', 'https://openalex.org/W2101306910', 'https://openalex.org/W6745415975', 'https://openalex.org/W6657069055', 'https://openalex.org/W6760057544', 'https://openalex.org/W6753544016', 'https://openalex.org/W6600006618', 'https://openalex.org/W6755154125', 'https://openalex.org/W2882991827', 'https://openalex.org/W2025013471', 'https://openalex.org/W2292259253', 'https://openalex.org/W2604379605', 'https://openalex.org/W2186222003', 'https://openalex.org/W2726515241', 'https://openalex.org/W2796292145', 'https://openalex.org/W2952419167', 'https://openalex.org/W2746742816', 'https://openalex.org/W2157364932', 'https://openalex.org/W2087681821', 'https://openalex.org/W2624614404', 'https://openalex.org/W2046056978', 'https://openalex.org/W2604292070', 'https://openalex.org/W2620629206', 'https://openalex.org/W2039057510', 'https://openalex.org/W2963671154', 'https://openalex.org/W2613087992', 'https://openalex.org/W2152305730', 'https://openalex.org/W2962788625', 'https://openalex.org/W2596164567', 'https://openalex.org/W2748488820', 'https://openalex.org/W2288371874', 'https://openalex.org/W2952933449', 'https://openalex.org/W1581253957', 'https://openalex.org/W2893436174', 'https://openalex.org/W2114925438', 'https://openalex.org/W2166637769', 'https://openalex.org/W2798513620', 'https://openalex.org/W2115252128', 'https://openalex.org/W2808631503', 'https://openalex.org/W2138621090', 'https://openalex.org/W2896968054', 'https://openalex.org/W1893902159', 'https://openalex.org/W1994002998', 'https://openalex.org/W1569447338', 'https://openalex.org/W1591607137', 'https://openalex.org/W2010062708', 'https://openalex.org/W2609296554', 'https://openalex.org/W3143835353', 'https://openalex.org/W3106250896', 'https://openalex.org/W4236965008', 'https://openalex.org/W2797032258', 'https://openalex.org/W2950298192', 'https://openalex.org/W204053250', 'https://openalex.org/W2117539524', 'https://openalex.org/W2596989490', 'https://openalex.org/W2890964092', 'https://openalex.org/W2620275798', 'https://openalex.org/W2797090057', 'https://openalex.org/W2962835968', 'https://openalex.org/W2194775991', 'https://openalex.org/W819783837', 'https://openalex.org/W183860', 'https://openalex.org/W1589137271', 'https://openalex.org/W1554705102', 'https://openalex.org/W2325939864', 'https://openalex.org/W2163605009', 'https://openalex.org/W2963839617', 'https://openalex.org/W2594690981']",2019-10-16
https://openalex.org/W3196318247,https://doi.org/10.48550/arxiv.2108.12409,"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation","Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.","['https://openalex.org/W3131922516', 'https://openalex.org/W2965373594', 'https://openalex.org/W179875071', 'https://openalex.org/W3147874613', 'https://openalex.org/W1566289585', 'https://openalex.org/W2995154514', 'https://openalex.org/W3098824823', 'https://openalex.org/W1999965501', 'https://openalex.org/W2525332836', 'https://openalex.org/W2051840895', 'https://openalex.org/W3168847912', 'https://openalex.org/W2963807318', 'https://openalex.org/W2964110616', 'https://openalex.org/W3170261818', 'https://openalex.org/W1591801644', 'https://openalex.org/W2963341956', 'https://openalex.org/W3015468748', 'https://openalex.org/W3030163527', 'https://openalex.org/W2919624000', 'https://openalex.org/W2963347649', 'https://openalex.org/W3174401451', 'https://openalex.org/W2789541106', 'https://openalex.org/W3035691519', 'https://openalex.org/W2805206884', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963631907', 'https://openalex.org/W2962964385', 'https://openalex.org/W2995575179', 'https://openalex.org/W3082274269', 'https://openalex.org/W3035390927']",2021-08-27
https://openalex.org/W2946200149,https://doi.org/10.48550/arxiv.1905.09263,"FastSpeech: Fast, Robust and Controllable Text to Speech","Neural network based end-to-end text to speech (TTS) has significantly improved the quality of synthesized speech. Prominent methods (e.g., Tacotron 2) usually first generate mel-spectrogram from text, and then synthesize speech from the mel-spectrogram using vocoder such as WaveNet. Compared with traditional concatenative and statistical parametric approaches, neural network based end-to-end models suffer from slow inference speed, and the synthesized speech is usually not robust (i.e., some words are skipped or repeated) and lack of controllability (voice speed or prosody control). In this work, we propose a novel feed-forward network based on Transformer to generate mel-spectrogram in parallel for TTS. Specifically, we extract attention alignments from an encoder-decoder based teacher model for phoneme duration prediction, which is used by a length regulator to expand the source phoneme sequence to match the length of the target mel-spectrogram sequence for parallel mel-spectrogram generation. Experiments on the LJSpeech dataset show that our parallel model matches autoregressive models in terms of speech quality, nearly eliminates the problem of word skipping and repeating in particularly hard cases, and can adjust voice speed smoothly. Most importantly, compared with autoregressive Transformer TTS, our model speeds up mel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x. Therefore, we call our model FastSpeech.","['https://openalex.org/W2463507112', 'https://openalex.org/W2767206889', 'https://openalex.org/W2962936105', 'https://openalex.org/W2962969034', 'https://openalex.org/W2608207374', 'https://openalex.org/W2591927543', 'https://openalex.org/W2471520273', 'https://openalex.org/W2327501763', 'https://openalex.org/W3038172701', 'https://openalex.org/W2519091744', 'https://openalex.org/W2120847449', 'https://openalex.org/W2952711665', 'https://openalex.org/W2945613576', 'https://openalex.org/W2964265128', 'https://openalex.org/W2892140764', 'https://openalex.org/W2766812927', 'https://openalex.org/W2598638573', 'https://openalex.org/W2769810959', 'https://openalex.org/W2150658333', 'https://openalex.org/W2963975282', 'https://openalex.org/W2963300588', 'https://openalex.org/W2964243274', 'https://openalex.org/W648786980', 'https://openalex.org/W2963536265', 'https://openalex.org/W2964308564', 'https://openalex.org/W2963403868']",2019-05-22
https://openalex.org/W4221145199,https://doi.org/10.1109/tpami.2023.3263585,Dawn of the Transformer Era in Speech Emotion Recognition: Closing the Valence Gap,"Recent advances in transformer-based architectures which are pre-trained in self-supervised manner have shown great promise in several machine learning tasks. In the audio domain, such architectures have also been successfully utilised in the field of speech emotion recognition (SER). However, existing works have not evaluated the influence of model size and pre-training data on downstream performance, and have shown limited attention to generalisation, robustness, fairness, and efficiency. The present contribution conducts a thorough analysis of these aspects on several pre-trained variants of wav2vec 2.0 and HuBERT that we fine-tuned on the dimensions arousal, dominance, and valence of MSP-Podcast, while additionally using IEMOCAP and MOSI to test cross-corpus generalisation. To the best of our knowledge, we obtain the top performance for valence prediction without use of explicit linguistic information, with a concordance correlation coefficient (CCC) of .638 on MSP-Podcast. Furthermore, our investigations reveal that transformer-based architectures are more robust to small perturbations compared to a CNN-based baseline and fair with respect to biological sex groups, but not towards individual speakers. Finally, we are the first to show that their extraordinary success on valence is based on implicit linguistic information learnt during fine-tuning of the transformer layers, which explains why they perform on-par with recent multimodal approaches that explicitly utilise textual information. Our findings collectively paint the following picture: transformer-based architectures constitute the new state-of-the-art in SER, but further advances are needed to mitigate remaining robustness and individual speaker issues. To make our findings reproducible, we release the best performing model to the community.","['https://openalex.org/W3208152093', 'https://openalex.org/W4297841899', 'https://openalex.org/W3082167223', 'https://openalex.org/W3015988193', 'https://openalex.org/W6774314701', 'https://openalex.org/W2785722081', 'https://openalex.org/W6800751262', 'https://openalex.org/W2982223350', 'https://openalex.org/W2052666245', 'https://openalex.org/W2972852081', 'https://openalex.org/W6755207826', 'https://openalex.org/W2972372393', 'https://openalex.org/W2966384645', 'https://openalex.org/W6739901393', 'https://openalex.org/W4213019189', 'https://openalex.org/W3209059054', 'https://openalex.org/W6780218876', 'https://openalex.org/W2746763037', 'https://openalex.org/W2897444637', 'https://openalex.org/W2128837546', 'https://openalex.org/W4393695169', 'https://openalex.org/W3007157104', 'https://openalex.org/W2811466185', 'https://openalex.org/W3161663055', 'https://openalex.org/W2399733683', 'https://openalex.org/W2556418146', 'https://openalex.org/W3094550259', 'https://openalex.org/W2313339984', 'https://openalex.org/W2973034847', 'https://openalex.org/W2910165986', 'https://openalex.org/W6802103249', 'https://openalex.org/W4221089191', 'https://openalex.org/W2003653478', 'https://openalex.org/W2117645142', 'https://openalex.org/W2156503193', 'https://openalex.org/W6804030475', 'https://openalex.org/W6802301941', 'https://openalex.org/W3162890625', 'https://openalex.org/W1533303231', 'https://openalex.org/W2972935927', 'https://openalex.org/W2742542661', 'https://openalex.org/W6803979805', 'https://openalex.org/W3162811262', 'https://openalex.org/W1966797434', 'https://openalex.org/W2803098682', 'https://openalex.org/W3119308075', 'https://openalex.org/W3198771897', 'https://openalex.org/W6799856993', 'https://openalex.org/W3193714551', 'https://openalex.org/W2781692313', 'https://openalex.org/W3197642003', 'https://openalex.org/W3197580070', 'https://openalex.org/W6803378298', 'https://openalex.org/W6680300913', 'https://openalex.org/W6931421704', 'https://openalex.org/W6776129198', 'https://openalex.org/W6782436372', 'https://openalex.org/W3215440557', 'https://openalex.org/W6802546489', 'https://openalex.org/W2146334809', 'https://openalex.org/W3169022486', 'https://openalex.org/W3100511085', 'https://openalex.org/W4285111045']",2023-03-31
https://openalex.org/W2794490148,https://doi.org/10.48550/arxiv.1803.09017,"Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis","In this work, we propose ""global style tokens"" (GSTs), a bank of embeddings that are jointly trained within Tacotron, a state-of-the-art end-to-end speech synthesis system. The embeddings are trained with no explicit labels, yet learn to model a large range of acoustic expressiveness. GSTs lead to a rich set of significant results. The soft interpretable ""labels"" they generate can be used to control synthesis in novel ways, such as varying speed and speaking style - independently of the text content. They can also be used for style transfer, replicating the speaking style of a single audio clip across an entire long-form text corpus. When trained on noisy, unlabeled found data, GSTs learn to factorize noise and speaker identity, providing a path towards highly scalable but robust speech synthesis.","['https://openalex.org/W2792995953', 'https://openalex.org/W2964301388', 'https://openalex.org/W2475998840', 'https://openalex.org/W2964281804', 'https://openalex.org/W2099057450', 'https://openalex.org/W2950527759', 'https://openalex.org/W2758785877', 'https://openalex.org/W2759925408', 'https://openalex.org/W2187089797', 'https://openalex.org/W2156146072', 'https://openalex.org/W2651834199', 'https://openalex.org/W2949382160', 'https://openalex.org/W28194048', 'https://openalex.org/W2409027918', 'https://openalex.org/W2963609956', 'https://openalex.org/W2617258110', 'https://openalex.org/W2963799213', 'https://openalex.org/W2777302760', 'https://openalex.org/W2736900972', 'https://openalex.org/W162654330', 'https://openalex.org/W2626778328']",2018-03-23
https://openalex.org/W3178839419,https://doi.org/10.48550/arxiv.2107.02530,AdaSpeech 3: Adaptive Text to Speech for Spontaneous Style,"While recent text to speech (TTS) models perform very well in synthesizing reading-style (e.g., audiobook) speech, it is still challenging to synthesize spontaneous-style speech (e.g., podcast or conversation), mainly because of two reasons: 1) the lack of training data for spontaneous speech; 2) the difficulty in modeling the filled pauses (um and uh) and diverse rhythms in spontaneous speech. In this paper, we develop AdaSpeech 3, an adaptive TTS system that fine-tunes a well-trained reading-style TTS model for spontaneous-style speech. Specifically, 1) to insert filled pauses (FP) in the text sequence appropriately, we introduce an FP predictor to the TTS model; 2) to model the varying rhythms, we introduce a duration predictor based on mixture of experts (MoE), which contains three experts responsible for the generation of fast, medium and slow speech respectively, and fine-tune it as well as the pitch predictor for rhythm adaptation; 3) to adapt to other speaker timbre, we fine-tune some parameters in the decoder with few speech data. To address the challenge of lack of training data, we mine a spontaneous speech dataset to support our research this work and facilitate future research on spontaneous TTS. Experiments show that AdaSpeech 3 synthesizes speech with natural FP and rhythms in spontaneous styles, and achieves much better MOS and SMOS scores than previous adaptive TTS systems.","['https://openalex.org/W3130016944', 'https://openalex.org/W2068238590', 'https://openalex.org/W1992245461', 'https://openalex.org/W3016151952', 'https://openalex.org/W2795023788', 'https://openalex.org/W1984905644', 'https://openalex.org/W2748900206', 'https://openalex.org/W2498127116', 'https://openalex.org/W2608207374', 'https://openalex.org/W2766812927', 'https://openalex.org/W2973177710', 'https://openalex.org/W2570199661', 'https://openalex.org/W2317015831', 'https://openalex.org/W2970006822', 'https://openalex.org/W2747874407', 'https://openalex.org/W2963403868', 'https://openalex.org/W2115122836', 'https://openalex.org/W2972677740', 'https://openalex.org/W2970730223', 'https://openalex.org/W183448665', 'https://openalex.org/W3161109662', 'https://openalex.org/W3128910262', 'https://openalex.org/W2937881934', 'https://openalex.org/W2527729766', 'https://openalex.org/W2093107730', 'https://openalex.org/W2066334462', 'https://openalex.org/W2972359262', 'https://openalex.org/W3174758275']",2021-07-06
https://openalex.org/W3206375275,https://doi.org/10.48550/arxiv.2110.07840,ESPnet2-TTS: Extending the Edge of TTS Research,"This paper describes ESPnet2-TTS, an end-to-end text-to-speech (E2E-TTS) toolkit. ESPnet2-TTS extends our earlier version, ESPnet-TTS, by adding many new features, including: on-the-fly flexible pre-processing, joint training with neural vocoders, and state-of-the-art TTS models with extensions like full-band E2E text-to-waveform modeling, which simplify the training pipeline and further enhance TTS performance. The unified design of our recipes enables users to quickly reproduce state-of-the-art E2E-TTS results. We also provide many pre-trained models in a unified Python interface for inference, offering a quick means for users to generate baseline samples and build demos. Experimental evaluations with English and Japanese corpora demonstrate that our provided models synthesize utterances comparable to ground-truth ones, achieving state-of-the-art TTS performance. The toolkit is available online at https://github.com/espnet/espnet.","['https://openalex.org/W3169905056', 'https://openalex.org/W3092028330', 'https://openalex.org/W3034729383', 'https://openalex.org/W3128170746', 'https://openalex.org/W2890964092', 'https://openalex.org/W2970730223', 'https://openalex.org/W2970006822', 'https://openalex.org/W2795935804', 'https://openalex.org/W3033194228', 'https://openalex.org/W2974231335', 'https://openalex.org/W2903739847', 'https://openalex.org/W2964243274', 'https://openalex.org/W3098824823', 'https://openalex.org/W2998572311', 'https://openalex.org/W2765486990', 'https://openalex.org/W2791686384', 'https://openalex.org/W2767052532', 'https://openalex.org/W2804704270', 'https://openalex.org/W3016160783', 'https://openalex.org/W3198769980', 'https://openalex.org/W2963609956', 'https://openalex.org/W2975414524', 'https://openalex.org/W3015338123', 'https://openalex.org/W2144994235', 'https://openalex.org/W2527729766', 'https://openalex.org/W3144035034', 'https://openalex.org/W3211696375', 'https://openalex.org/W3016021263', 'https://openalex.org/W2951882630', 'https://openalex.org/W2994715919', 'https://openalex.org/W1990505856', 'https://openalex.org/W2111284386', 'https://openalex.org/W3025165719', 'https://openalex.org/W2933138175', 'https://openalex.org/W3130016944', 'https://openalex.org/W3163793923', 'https://openalex.org/W2797310469', 'https://openalex.org/W1494198834', 'https://openalex.org/W3082130377', 'https://openalex.org/W2892140764', 'https://openalex.org/W2963272440', 'https://openalex.org/W3095717210', 'https://openalex.org/W2969049672', 'https://openalex.org/W1524333225', 'https://openalex.org/W3150572638', 'https://openalex.org/W2970971581', 'https://openalex.org/W2963432880']",2021-10-15
https://openalex.org/W3151269043,https://doi.org/10.48550/arxiv.2103.16710,A study of latent monotonic attention variants,"End-to-end models reach state-of-the-art performance for speech recognition, but global soft attention is not monotonic, which might lead to convergence problems, to instability, to bad generalisation, cannot be used for online streaming, and is also inefficient in calculation. Monotonicity can potentially fix all of this. There are several ad-hoc solutions or heuristics to introduce monotonicity, but a principled introduction is rarely found in literature so far. In this paper, we present a mathematically clean solution to introduce monotonicity, by introducing a new latent variable which represents the audio position or segment boundaries. We compare several monotonic latent models to our global soft attention baseline such as a hard attention model, a local windowed soft attention model, and a segmental soft attention model. We can show that our monotonic models perform as good as the global soft attention model. We perform our experiments on Switchboard 300h. We carefully outline the details of our training and release our code and configs.","['https://openalex.org/W2147527908', 'https://openalex.org/W2739883972', 'https://openalex.org/W2799800213', 'https://openalex.org/W2905263927', 'https://openalex.org/W3028545098', 'https://openalex.org/W2739427748', 'https://openalex.org/W2964308564', 'https://openalex.org/W2740240682', 'https://openalex.org/W1484210532', 'https://openalex.org/W2766219058', 'https://openalex.org/W2972995428', 'https://openalex.org/W2631415506', 'https://openalex.org/W2907222274', 'https://openalex.org/W3013642574', 'https://openalex.org/W330298975', 'https://openalex.org/W2131033001', 'https://openalex.org/W1810943226', 'https://openalex.org/W2745439869', 'https://openalex.org/W2963827914', 'https://openalex.org/W1806891645', 'https://openalex.org/W2773781902', 'https://openalex.org/W3002595344', 'https://openalex.org/W2777929561', 'https://openalex.org/W2963499882', 'https://openalex.org/W2962784628', 'https://openalex.org/W2767052532', 'https://openalex.org/W2886769154', 'https://openalex.org/W2936774411', 'https://openalex.org/W2936123380', 'https://openalex.org/W1514535095', 'https://openalex.org/W2729190387', 'https://openalex.org/W2605141709', 'https://openalex.org/W2949335953', 'https://openalex.org/W2951997403', 'https://openalex.org/W2889187401', 'https://openalex.org/W2936706905', 'https://openalex.org/W2524011860', 'https://openalex.org/W2507756961', 'https://openalex.org/W2767601419', 'https://openalex.org/W2963236196', 'https://openalex.org/W2798474427', 'https://openalex.org/W2962824709', 'https://openalex.org/W2410539690', 'https://openalex.org/W2949418473', 'https://openalex.org/W2972702018', 'https://openalex.org/W2516649778', 'https://openalex.org/W2083393647', 'https://openalex.org/W2166637769', 'https://openalex.org/W2973122799', 'https://openalex.org/W2900676547', 'https://openalex.org/W2972977747', 'https://openalex.org/W2102988002', 'https://openalex.org/W2859444450', 'https://openalex.org/W2963747784', 'https://openalex.org/W2890909908', 'https://openalex.org/W2064675550', 'https://openalex.org/W2526425061', 'https://openalex.org/W2963729263', 'https://openalex.org/W2130942839', 'https://openalex.org/W2963061963', 'https://openalex.org/W2746192915', 'https://openalex.org/W2127141656', 'https://openalex.org/W2748816379', 'https://openalex.org/W1828163288', 'https://openalex.org/W2757154661', 'https://openalex.org/W2550147980', 'https://openalex.org/W3008191852', 'https://openalex.org/W2799923439', 'https://openalex.org/W2512608784', 'https://openalex.org/W2110871230', 'https://openalex.org/W2291022022']",2021-03-30
https://openalex.org/W4292779060,https://doi.org/10.4230/lipics.giscience.2023.43,Evaluating the Effectiveness of Large Language Models in Representing Textual Descriptions of Geometry and Spatial Relations (Short Paper),"This research focuses on assessing the ability of large language models (LLMs) in representing geometries and their spatial relations. We utilize LLMs including GPT-2 and BERT to encode the well-known text (WKT) format of geometries and then feed their embeddings into classifiers and regressors to evaluate the effectiveness of the LLMs-generated embeddings for geometric attributes. The experiments demonstrate that while the LLMs-generated embeddings can preserve geometry types and capture some spatial relations (up to 73% accuracy), challenges remain in estimating numeric values and retrieving spatially related objects. This research highlights the need for improvement in terms of capturing the nuances and complexities of the underlying geospatial data and integrating domain knowledge to support various GeoAI applications using foundation models.",[],2023-01-01
https://openalex.org/W2972702018,https://doi.org/10.21437/interspeech.2019-1972,Robust Sequence-to-Sequence Acoustic Modeling with Stepwise Monotonic Attention for Neural TTS,"Neural TTS has demonstrated strong capabilities to generate human-like speech with high quality and naturalness, while its generalization to out-of-domain texts is still a challenging task, with regard to the design of attention-based sequence-tosequence acoustic modeling.Various errors occur in those inputs with unseen context, including attention collapse, skipping, repeating, etc., which limits the broader applications.In this paper, we propose a novel stepwise monotonic attention method in sequence-to-sequence acoustic modeling to improve the robustness on out-of-domain inputs.The method utilizes the strict monotonic property in TTS with constraints on monotonic hard attention that the alignments between inputs and outputs sequence must be not only monotonic but allowing no skipping on inputs.Soft attention could be used to evade mismatch between training and inference.The experimental results show that the proposed method could achieve significant improvements in robustness on out-of-domain scenarios for phoneme-based models, without any regression on the in-domain naturalness test.","['https://openalex.org/W2625720409', 'https://openalex.org/W1514535095', 'https://openalex.org/W1810943226', 'https://openalex.org/W2767601419', 'https://openalex.org/W2253795368', 'https://openalex.org/W1902237438', 'https://openalex.org/W2964243274', 'https://openalex.org/W2133564696', 'https://openalex.org/W2745439869', 'https://openalex.org/W2619122421', 'https://openalex.org/W2125308790', 'https://openalex.org/W2758584285', 'https://openalex.org/W2767052532', 'https://openalex.org/W2963691546', 'https://openalex.org/W2519091744', 'https://openalex.org/W2886769154', 'https://openalex.org/W2963609956', 'https://openalex.org/W2605141709', 'https://openalex.org/W854541894', 'https://openalex.org/W2890909908', 'https://openalex.org/W2178654303']",2019-09-13
https://openalex.org/W2938704169,https://doi.org/10.48550/arxiv.1904.09751,The Curious Case of Neural Text Degeneration,"Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.","['https://openalex.org/W2964268978', 'https://openalex.org/W2963366196', 'https://openalex.org/W2805486818', 'https://openalex.org/W2807747378', 'https://openalex.org/W2402268235', 'https://openalex.org/W2099960657', 'https://openalex.org/W2557436004', 'https://openalex.org/W2995404354', 'https://openalex.org/W2892153332', 'https://openalex.org/W2788277448', 'https://openalex.org/W2963929190', 'https://openalex.org/W1902237438', 'https://openalex.org/W2963206148', 'https://openalex.org/W2995969307', 'https://openalex.org/W2410983263', 'https://openalex.org/W2172140247', 'https://openalex.org/W2963167310', 'https://openalex.org/W2963970792', 'https://openalex.org/W2550147980', 'https://openalex.org/W2951520714', 'https://openalex.org/W2963506925', 'https://openalex.org/W2963456134', 'https://openalex.org/W2803267010', 'https://openalex.org/W2799184518', 'https://openalex.org/W2963706817', 'https://openalex.org/W2962788902', 'https://openalex.org/W2898718449', 'https://openalex.org/W2996068536', 'https://openalex.org/W2048176942', 'https://openalex.org/W2739046565', 'https://openalex.org/W2963096510', 'https://openalex.org/W2963403868', 'https://openalex.org/W2051840895', 'https://openalex.org/W2970692082', 'https://openalex.org/W2042492924', 'https://openalex.org/W2264742718', 'https://openalex.org/W2963283805', 'https://openalex.org/W2963466651', 'https://openalex.org/W2964308564', 'https://openalex.org/W2557283755']",2019-04-22
https://openalex.org/W4391020683,https://doi.org/10.1109/tpami.2024.3356232,<i>NaturalSpeech</i>: End-to-End Text-to-Speech Synthesis With Human-Level Quality,"Text-to-speech (TTS) has made rapid progress in both academia and industry in recent years. Some questions naturally arise that whether a TTS system can achieve human-level quality, how to define/judge that quality, and how to achieve it. In this paper, we answer these questions by first defining the human-level quality based on the statistical significance of subjective measure and introducing appropriate guidelines to judge it, and then developing a TTS system called NaturalSpeech that achieves human-level quality on benchmark datasets. Specifically, we leverage a variational auto-encoder (VAE) for end-to-end text-to-waveform generation, with several key modules to enhance the capacity of the prior from text and reduce the complexity of the posterior from speech, including phoneme pre-training, differentiable duration modeling, bidirectional prior/posterior modeling, and a memory mechanism in VAE. Experimental evaluations on the popular LJSpeech dataset show that our proposed NaturalSpeech achieves -0.01 CMOS (comparative mean opinion score) to human recordings at the sentence level, with Wilcoxon signed rank test at p-level p >> 0.05, which demonstrates no statistically significant difference from human recordings for the first time.","['https://openalex.org/W6796730497', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963609956', 'https://openalex.org/W2591927543', 'https://openalex.org/W6765987481', 'https://openalex.org/W6745697700', 'https://openalex.org/W2767052532', 'https://openalex.org/W2903739847', 'https://openalex.org/W6763832098', 'https://openalex.org/W4395957972', 'https://openalex.org/W2963300588', 'https://openalex.org/W6777694618', 'https://openalex.org/W6795261426', 'https://openalex.org/W6796464841', 'https://openalex.org/W6748409065', 'https://openalex.org/W6783867762', 'https://openalex.org/W6778823374', 'https://openalex.org/W6779337556', 'https://openalex.org/W2752796333', 'https://openalex.org/W6790978476', 'https://openalex.org/W6640963894', 'https://openalex.org/W4296068816', 'https://openalex.org/W3197294703', 'https://openalex.org/W6635084905', 'https://openalex.org/W2587284713', 'https://openalex.org/W6917585676', 'https://openalex.org/W6727697161', 'https://openalex.org/W1552314771', 'https://openalex.org/W2141998673', 'https://openalex.org/W2964058413', 'https://openalex.org/W2972394484', 'https://openalex.org/W3161558238', 'https://openalex.org/W1857789879', 'https://openalex.org/W6796242362', 'https://openalex.org/W3215495615', 'https://openalex.org/W6793801364', 'https://openalex.org/W3133428285', 'https://openalex.org/W6762931180', 'https://openalex.org/W3180355996', 'https://openalex.org/W4393161149', 'https://openalex.org/W4385245566', 'https://openalex.org/W3016137096', 'https://openalex.org/W3197324626', 'https://openalex.org/W6755207826', 'https://openalex.org/W6746023985', 'https://openalex.org/W6610566761', 'https://openalex.org/W6734312481', 'https://openalex.org/W2593414223', 'https://openalex.org/W6750489868', 'https://openalex.org/W6790220310', 'https://openalex.org/W4225746985', 'https://openalex.org/W3161782335', 'https://openalex.org/W2972677740', 'https://openalex.org/W4200300291', 'https://openalex.org/W6714644935', 'https://openalex.org/W2962784628', 'https://openalex.org/W6757817989']",2024-01-19
https://openalex.org/W4385764360,https://doi.org/10.24963/ijcai.2023/575,Learning to Speak from Text: Zero-Shot Multilingual Text-to-Speech with Unsupervised Text Pretraining,"While neural text-to-speech (TTS) has achieved human-like natural synthetic speech, multilingual TTS systems are limited to resource-rich languages due to the need for paired text and studio-quality audio data. This paper proposes a method for zero-shot multilingual TTS using text-only data for the target language. The use of text-only data allows the development of TTS systems for low-resource languages for which only textual resources are available, making TTS accessible to thousands of languages. Inspired by the strong cross-lingual transferability of multilingual language models, our framework first performs masked language model pretraining with multilingual text-only data. Then we train this model with a paired data in a supervised manner, while freezing a language-aware embedding layer. This allows inference even for languages not included in the paired data but present in the text-only data. Evaluation results demonstrate highly intelligible zero-shot TTS with a character error rate of less than 12% for an unseen language.","['https://openalex.org/W4385245566', 'https://openalex.org/W2964002616', 'https://openalex.org/W3048217770', 'https://openalex.org/W3163339651', 'https://openalex.org/W2973213036', 'https://openalex.org/W4372272479', 'https://openalex.org/W2746132399', 'https://openalex.org/W1665214252', 'https://openalex.org/W2973217961', 'https://openalex.org/W2187089797', 'https://openalex.org/W2093450784', 'https://openalex.org/W2952037945', 'https://openalex.org/W3119308075', 'https://openalex.org/W2952638691', 'https://openalex.org/W3104723404', 'https://openalex.org/W2964243274', 'https://openalex.org/W2726515241', 'https://openalex.org/W3092028330', 'https://openalex.org/W4300191749', 'https://openalex.org/W4311000453', 'https://openalex.org/W4394666973', 'https://openalex.org/W2494654097', 'https://openalex.org/W2962780374', 'https://openalex.org/W4296069143', 'https://openalex.org/W2025638820', 'https://openalex.org/W3206375275', 'https://openalex.org/W2767052532', 'https://openalex.org/W2890964092', 'https://openalex.org/W2899663614', 'https://openalex.org/W4225956675', 'https://openalex.org/W3181257032', 'https://openalex.org/W2903739847', 'https://openalex.org/W2952711665', 'https://openalex.org/W4297841354', 'https://openalex.org/W4296068816', 'https://openalex.org/W4375869005', 'https://openalex.org/W2970925270', 'https://openalex.org/W3197324626', 'https://openalex.org/W3177252310', 'https://openalex.org/W3167533889', 'https://openalex.org/W2106568252', 'https://openalex.org/W3169905056', 'https://openalex.org/W3096303254']",2023-08-01
https://openalex.org/W2972473628,https://doi.org/10.21437/interspeech.2019-2668,Learning to Speak Fluently in a Foreign Language: Multilingual Speech Synthesis and Cross-Language Voice Cloning,"We present a multispeaker, multilingual text-to-speech (TTS) synthesis model based on Tacotron that is able to produce high quality speech in multiple languages.Moreover, the model is able to transfer voices across languages, e.g.synthesize fluent Spanish speech using an English speaker's voice, without training on any bilingual or parallel examples.Such transfer works across distantly related languages, e.g.English and Mandarin.Critical to achieving this result are: 1. using a phonemic input representation to encourage sharing of model capacity across languages, and 2. incorporating an adversarial loss term to encourage the model to disentangle its representation of speaker identity (which is perfectly correlated with language in the training data) from the speech content.Further scaling up the model by training on multiple speakers of each language, and incorporating an autoencoding input to help stabilize attention during training, results in a model which can be used to consistently synthesize intelligible speech for training speakers in all languages seen during training, and in native or foreign accents.","['https://openalex.org/W2963691546', 'https://openalex.org/W2794490148', 'https://openalex.org/W2964002616', 'https://openalex.org/W2892620417', 'https://openalex.org/W1959608418', 'https://openalex.org/W2024678752', 'https://openalex.org/W1731081199', 'https://openalex.org/W2025638820', 'https://openalex.org/W4298580827', 'https://openalex.org/W2808706139', 'https://openalex.org/W2471520273', 'https://openalex.org/W2519091744', 'https://openalex.org/W2962691331', 'https://openalex.org/W2964243274', 'https://openalex.org/W2907262790', 'https://openalex.org/W2494654097', 'https://openalex.org/W2963827314', 'https://openalex.org/W4289383906', 'https://openalex.org/W2794235490', 'https://openalex.org/W2963964591', 'https://openalex.org/W2901997113', 'https://openalex.org/W2788357188', 'https://openalex.org/W1544828620', 'https://openalex.org/W2901389167', 'https://openalex.org/W2604184139', 'https://openalex.org/W4298174729', 'https://openalex.org/W2884607399']",2019-09-13
https://openalex.org/W3197324626,https://doi.org/10.21437/interspeech.2021-1757,PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS,"This paper introduces PnG BERT, a new encoder model for neural TTS.This model is augmented from the original BERT model, by taking both phoneme and grapheme representations of text as input, as well as the word-level alignment between them.It can be pre-trained on a large text corpus in a selfsupervised manner, and fine-tuned in a TTS task.Experimental results show that a neural TTS model using a pre-trained PnG BERT as its encoder yields more natural prosody and more accurate pronunciation than a baseline model using only phoneme input with no pre-training.Subjective side-by-side preference evaluations show that raters have no statistically significant preference between the speech synthesized using a PnG BERT and ground truth recordings from professional speakers.","['https://openalex.org/W3163339651', 'https://openalex.org/W2952744660', 'https://openalex.org/W3081488690', 'https://openalex.org/W2591927543', 'https://openalex.org/W2808706139', 'https://openalex.org/W3161296985', 'https://openalex.org/W2914120296', 'https://openalex.org/W2901997113', 'https://openalex.org/W2619368999', 'https://openalex.org/W2103085228', 'https://openalex.org/W2519091744', 'https://openalex.org/W3161782335', 'https://openalex.org/W2963250244', 'https://openalex.org/W4299287062', 'https://openalex.org/W2946200149', 'https://openalex.org/W4385245566', 'https://openalex.org/W2121879602', 'https://openalex.org/W2896457183', 'https://openalex.org/W3091928890', 'https://openalex.org/W2963827314', 'https://openalex.org/W3033411150', 'https://openalex.org/W4287547221', 'https://openalex.org/W2964243274', 'https://openalex.org/W2962784628', 'https://openalex.org/W2794490148', 'https://openalex.org/W2866343820', 'https://openalex.org/W2963609956', 'https://openalex.org/W4298580827', 'https://openalex.org/W2907916773', 'https://openalex.org/W2903739847']",2021-08-27
https://openalex.org/W4372267432,https://doi.org/10.1109/icassp49357.2023.10097074,Phoneme-Level Bert for Enhanced Prosody of Text-To-Speech with Grapheme Predictions,"Large-scale pre-trained language models have been shown to be helpful in improving the naturalness of text-to-speech (TTS) models by enabling them to produce more naturalistic prosodic patterns. However, these models are usually word-level or sup-phoneme-level and jointly trained with phonemes, making them inefficient for the downstream TTS task where only phonemes are needed. In this work, we propose a phoneme-level BERT (PL-BERT) with a pretext task of predicting the corresponding graphemes along with the regular masked phoneme predictions. Subjective evaluations show that our phoneme-level BERT encoder has significantly improved the mean opinion scores (MOS) of rated naturalness of synthesized speech compared with the state-of-the-art (SOTA) StyleTTS baseline on out-of-distribution (OOD) texts.","['https://openalex.org/W6767671539', 'https://openalex.org/W6796464841', 'https://openalex.org/W6766673545', 'https://openalex.org/W6778823374', 'https://openalex.org/W6917585676', 'https://openalex.org/W4200300291', 'https://openalex.org/W3196667132', 'https://openalex.org/W2952370363', 'https://openalex.org/W2964243274', 'https://openalex.org/W6638575559', 'https://openalex.org/W6848461853', 'https://openalex.org/W6796730497', 'https://openalex.org/W6838721020', 'https://openalex.org/W6768021236', 'https://openalex.org/W6783867762', 'https://openalex.org/W3100806282', 'https://openalex.org/W2999635142', 'https://openalex.org/W3197324626', 'https://openalex.org/W3163339651', 'https://openalex.org/W4296068816', 'https://openalex.org/W3081488690', 'https://openalex.org/W6849600165', 'https://openalex.org/W6788095661', 'https://openalex.org/W3016137096', 'https://openalex.org/W3113850747', 'https://openalex.org/W3169905056', 'https://openalex.org/W2965373594', 'https://openalex.org/W4320451749', 'https://openalex.org/W2962784628', 'https://openalex.org/W3092028330', 'https://openalex.org/W2996428491', 'https://openalex.org/W4391020683', 'https://openalex.org/W4319862431', 'https://openalex.org/W4280561221', 'https://openalex.org/W3033411150', 'https://openalex.org/W2974231335', 'https://openalex.org/W3174758275']",2023-05-05
https://openalex.org/W3015826515,https://doi.org/10.1109/icassp40776.2020.9054535,Zero-Shot Multi-Speaker Text-To-Speech with State-Of-The-Art Neural Speaker Embeddings,"While speaker adaptation for end-to-end speech synthesis using speaker embeddings can produce good speaker similarity for speakers seen during training, there remains a gap for zero-shot adaptation to unseen speakers. We investigate multi-speaker modeling for end-to-end text-to-speech synthesis and study the effects of different types of state-of-the-art neural speaker embeddings on speaker similarity for unseen speakers. Learnable dictionary encoding-based speaker embeddings with angular softmax loss can improve equal error rates over x-vectors in a speaker verification task; these embeddings also improve speaker similarity and naturalness for unseen speakers when used for zero-shot adaptation to new speakers in end-to-end speech synthesis.","['https://openalex.org/W6677973343', 'https://openalex.org/W2886769154', 'https://openalex.org/W6739901393', 'https://openalex.org/W6631852945', 'https://openalex.org/W2972633940', 'https://openalex.org/W6936113694', 'https://openalex.org/W2023238506', 'https://openalex.org/W2064364374', 'https://openalex.org/W6757322325', 'https://openalex.org/W2972610613', 'https://openalex.org/W2972440097', 'https://openalex.org/W6752888775', 'https://openalex.org/W2973157397', 'https://openalex.org/W2973084242', 'https://openalex.org/W2972961496', 'https://openalex.org/W2890964092', 'https://openalex.org/W2150769028', 'https://openalex.org/W2963371159', 'https://openalex.org/W2963945466', 'https://openalex.org/W6765653190', 'https://openalex.org/W2808631503', 'https://openalex.org/W2964243274', 'https://openalex.org/W6755135894', 'https://openalex.org/W2041823554', 'https://openalex.org/W2972574864', 'https://openalex.org/W2972772214', 'https://openalex.org/W6753855596', 'https://openalex.org/W6748573829', 'https://openalex.org/W2963609956', 'https://openalex.org/W2888968865', 'https://openalex.org/W2916104401', 'https://openalex.org/W2972944317', 'https://openalex.org/W3010925296', 'https://openalex.org/W2963035245', 'https://openalex.org/W6753575415', 'https://openalex.org/W2726515241', 'https://openalex.org/W2951758756', 'https://openalex.org/W4298174729', 'https://openalex.org/W2963432880', 'https://openalex.org/W2963912924', 'https://openalex.org/W2884412522', 'https://openalex.org/W2963192573', 'https://openalex.org/W4385245566', 'https://openalex.org/W2917245127', 'https://openalex.org/W2892620417', 'https://openalex.org/W2788357188', 'https://openalex.org/W2960427821', 'https://openalex.org/W2963975282', 'https://openalex.org/W2963403868', 'https://openalex.org/W2527729766', 'https://openalex.org/W2519091744', 'https://openalex.org/W4289750118', 'https://openalex.org/W2808706139', 'https://openalex.org/W2903853691', 'https://openalex.org/W2962739369', 'https://openalex.org/W1529628403']",2020-04-09
https://openalex.org/W4225274946,https://doi.org/10.1109/icassp43922.2022.9746223,Efficient Adapter Transfer of Self-Supervised Speech Models for Automatic Speech Recognition,"Self-supervised learning (SSL) is a powerful tool that allows learning of underlying representations from unlabeled data. Transformer based models such as wav2vec 2.0 and HuBERT are leading the field in the speech domain. Generally these models are fine-tuned on a small amount of labeled data for a downstream task such as Automatic Speech Recognition (ASR). This involves re-training the majority of the model for each task. Adapters are small lightweight modules which are commonly used in Natural Language Processing (NLP) to adapt pre-trained models to new tasks. In this paper we propose applying adapters to wav2vec 2.0 to reduce the number of parameters required for downstream ASR tasks, and increase scalability of the model to multiple tasks or languages. Using adapters we can perform ASR while training fewer than 10% of parameters per task compared to full fine-tuning with little degradation of performance. Ablations show that applying adapters into just the top few layers of the pre-trained network gives similar performance to full transfer, supporting the theory that higher pre-trained layers encode more phonemic information, and further optimizing efficiency.","['https://openalex.org/W3172698324', 'https://openalex.org/W3160525311', 'https://openalex.org/W2970925270', 'https://openalex.org/W6738045163', 'https://openalex.org/W2971840980', 'https://openalex.org/W6787191599', 'https://openalex.org/W6799245484', 'https://openalex.org/W2933138175', 'https://openalex.org/W1494198834', 'https://openalex.org/W2995181338', 'https://openalex.org/W6759579507', 'https://openalex.org/W3209059054', 'https://openalex.org/W6769263558', 'https://openalex.org/W3101498587', 'https://openalex.org/W2972943112', 'https://openalex.org/W2842511635', 'https://openalex.org/W6780218876', 'https://openalex.org/W6755207826', 'https://openalex.org/W2973049979', 'https://openalex.org/W6771467084', 'https://openalex.org/W4297808394', 'https://openalex.org/W3030437843', 'https://openalex.org/W2963211188', 'https://openalex.org/W2964303773', 'https://openalex.org/W3036601975', 'https://openalex.org/W3035390927', 'https://openalex.org/W3197845195', 'https://openalex.org/W3186596101', 'https://openalex.org/W2896457183']",2022-04-27
https://openalex.org/W3206189675,https://doi.org/10.1109/icassp43922.2022.9747814,Large-Scale Self-Supervised Speech Representation Learning for Automatic Speaker Verification,"The speech representations learned from large-scale unlabeled data have shown better generalizability than those from supervised learning and thus attract a lot of interest to be applied for various downstream tasks. In this paper, we explore the limits of speech representations learned by different self-supervised objectives and datasets for automatic speaker verification (ASV), especially with a well-recognized SOTA ASV model, ECAPA-TDNN [1], as a downstream model. The representations from all hidden layers of the pre-trained model are firstly averaged with learnable weights and then fed into the ECAPA-TDNN as input features. The experimental results on Voxceleb dataset show that the weighted average representation is significantly superior to FBank, a conventional handcrafted feature for ASV. Our best single system achieves 0.537%, 0.569%, and 1.180% equal error rate (EER) on the three official trials of VoxCeleb1, separately. Accordingly, the ensemble system with three pre-trained models can further improve the EER to 0.479%, 0.536% and 1.023%. Among the three evaluation trials, our best system outperforms the winner system [2] of the VoxCeleb Speaker Recognition Challenge 2021 (VoxSRC2021) on the VoxCeleb1-E trial.","['https://openalex.org/W125553504', 'https://openalex.org/W2969985801', 'https://openalex.org/W3197642003', 'https://openalex.org/W6761176859', 'https://openalex.org/W2752782242', 'https://openalex.org/W2157161740', 'https://openalex.org/W3094374485', 'https://openalex.org/W6688816777', 'https://openalex.org/W2808631503', 'https://openalex.org/W2794506738', 'https://openalex.org/W2889519245', 'https://openalex.org/W6755207826', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3160397447', 'https://openalex.org/W3161606033', 'https://openalex.org/W3163187953', 'https://openalex.org/W2948947170', 'https://openalex.org/W6803394801', 'https://openalex.org/W2890964092', 'https://openalex.org/W2972909277', 'https://openalex.org/W1006777433', 'https://openalex.org/W3010925296', 'https://openalex.org/W6739901393', 'https://openalex.org/W6769178842', 'https://openalex.org/W2888968865', 'https://openalex.org/W2747238065', 'https://openalex.org/W6801723603', 'https://openalex.org/W2962788625', 'https://openalex.org/W3024869864', 'https://openalex.org/W4226380987', 'https://openalex.org/W3197580070', 'https://openalex.org/W3198275944', 'https://openalex.org/W2517995648', 'https://openalex.org/W2726515241', 'https://openalex.org/W2964052309', 'https://openalex.org/W2590129515', 'https://openalex.org/W2928165649', 'https://openalex.org/W2963341956', 'https://openalex.org/W3157923770', 'https://openalex.org/W2963420686', 'https://openalex.org/W3036601975', 'https://openalex.org/W2981461916', 'https://openalex.org/W3169320628', 'https://openalex.org/W2219249508', 'https://openalex.org/W3198698812', 'https://openalex.org/W3099782249', 'https://openalex.org/W3179803166', 'https://openalex.org/W2896457183', 'https://openalex.org/W2937142395', 'https://openalex.org/W3206252155', 'https://openalex.org/W4288091954', 'https://openalex.org/W2963403868', 'https://openalex.org/W4385245566']",2022-04-27
https://openalex.org/W4390075359,https://doi.org/10.1162/tacl_a_00618,"Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision","Abstract We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to “reading”) and from semantic tokens to low-level acoustic tokens (“speaking”). Decoupling these two tasks enables training of the “speaking” module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the “reading” component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in naturalness and acoustic quality.","['https://openalex.org/W6849105126', 'https://openalex.org/W3205644108', 'https://openalex.org/W4381786045', 'https://openalex.org/W6778883912', 'https://openalex.org/W6805710207', 'https://openalex.org/W3209984917', 'https://openalex.org/W4226033575', 'https://openalex.org/W6800767084', 'https://openalex.org/W2889326796', 'https://openalex.org/W6638667902', 'https://openalex.org/W6917585676', 'https://openalex.org/W4380874786', 'https://openalex.org/W2995181338', 'https://openalex.org/W3198217962', 'https://openalex.org/W6804184754', 'https://openalex.org/W6849956205', 'https://openalex.org/W6783867762', 'https://openalex.org/W4385574033', 'https://openalex.org/W6790356757', 'https://openalex.org/W4296068981', 'https://openalex.org/W3034999214', 'https://openalex.org/W6810406373', 'https://openalex.org/W3015419784', 'https://openalex.org/W4307680525', 'https://openalex.org/W1494198834', 'https://openalex.org/W4285182272', 'https://openalex.org/W3140429000', 'https://openalex.org/W6769627184', 'https://openalex.org/W3161480375', 'https://openalex.org/W3033411150', 'https://openalex.org/W3016181583', 'https://openalex.org/W6853998256', 'https://openalex.org/W2963216553', 'https://openalex.org/W6751104502', 'https://openalex.org/W2964243274', 'https://openalex.org/W6677920722', 'https://openalex.org/W6739901393', 'https://openalex.org/W6848735303', 'https://openalex.org/W3215615641', 'https://openalex.org/W2972359262', 'https://openalex.org/W4394671563', 'https://openalex.org/W4288089799', 'https://openalex.org/W4381827575', 'https://openalex.org/W4296070453', 'https://openalex.org/W4320459320', 'https://openalex.org/W4318351475', 'https://openalex.org/W4385245566', 'https://openalex.org/W3198123200', 'https://openalex.org/W3215895588', 'https://openalex.org/W3092028330', 'https://openalex.org/W4313679638', 'https://openalex.org/W4292779060']",2023-01-01
https://openalex.org/W3161436426,https://doi.org/10.1109/icassp39728.2021.9414226,Disentangled Speaker and Language Representations Using Mutual Information Minimization and Domain Adaptation for Cross-Lingual TTS,"We propose a method for obtaining disentangled speaker and language representations via mutual information minimization and domain adaptation for cross-lingual text-to-speech (TTS) synthesis. The proposed method extracts speaker and language embeddings from acoustic features by a speaker encoder and a language encoder. Then the proposed method applies domain adaptation on the two embeddings to obtain language-invariant speaker embedding and speaker-invariant language embedding. To get more disentangled representations, the proposed method further uses mutual information minimization between the two embeddings to remove entangled information within each embedding. Disentangled representations of speaker and language are critical for cross-lingual TTS synthesis since entangled representations make it difficult to maintain speaker identity information when changing the language representation and consequently causes performance degradation. We evaluate the proposed method using English and Japanese multi-speaker datasets with a total of 207 speakers. Experimental result demonstrates that the proposed method significantly improves the naturalness and speaker similarity of both intra-lingual and cross-lingual TTS synthesis. Furthermore, we show that the proposed method has a good capability of maintaining the speaker identity between languages.","['https://openalex.org/W2936780106', 'https://openalex.org/W3016090709', 'https://openalex.org/W2963964591', 'https://openalex.org/W3096514088', 'https://openalex.org/W3016139610', 'https://openalex.org/W6637618735', 'https://openalex.org/W6771146561', 'https://openalex.org/W6761687776', 'https://openalex.org/W3108400054', 'https://openalex.org/W2964243274', 'https://openalex.org/W6748573829', 'https://openalex.org/W6631190155', 'https://openalex.org/W2907262790', 'https://openalex.org/W2794235490', 'https://openalex.org/W2494654097', 'https://openalex.org/W2972473628', 'https://openalex.org/W6662018943', 'https://openalex.org/W6748588790', 'https://openalex.org/W2973084242', 'https://openalex.org/W6752888775', 'https://openalex.org/W6779459370', 'https://openalex.org/W6767245602', 'https://openalex.org/W2120847449', 'https://openalex.org/W6603576222', 'https://openalex.org/W6780226713', 'https://openalex.org/W6737575990', 'https://openalex.org/W3106587939', 'https://openalex.org/W2964121744', 'https://openalex.org/W2993842823', 'https://openalex.org/W4298174729', 'https://openalex.org/W2940579548', 'https://openalex.org/W2046056978', 'https://openalex.org/W3035060230', 'https://openalex.org/W2527729766', 'https://openalex.org/W2962739369', 'https://openalex.org/W1731081199', 'https://openalex.org/W3036928441', 'https://openalex.org/W2963432880', 'https://openalex.org/W2969049672', 'https://openalex.org/W2788357188', 'https://openalex.org/W2963192573', 'https://openalex.org/W89093025', 'https://openalex.org/W2612434969', 'https://openalex.org/W2808706139', 'https://openalex.org/W1522301498']",2021-05-13
https://openalex.org/W3150572638,https://doi.org/10.1109/icassp39728.2021.9413889,Fastpitch: Parallel Text-to-Speech with Pitch Prediction,"We present FastPitch, a fully-parallel text-to-speech model based on FastSpeech, conditioned on fundamental frequency contours. The model predicts pitch contours during inference. By altering these predictions, the generated speech can be more expressive, better match the semantic of the utterance, and in the end more engaging to the listener. Uniformly increasing or decreasing pitch with FastPitch generates speech that resembles the voluntary modulation of voice. Conditioning on frequency contours improves the overall quality of synthesized speech, making it comparable to state-of-the-art. It does not introduce an overhead, and FastPitch retains the favorable, fully-parallel Transformer architecture, with over 900× real-time factor for mel-spectrogram synthesis of a typical utterance.","['https://openalex.org/W6917585676', 'https://openalex.org/W6676245417', 'https://openalex.org/W2964243274', 'https://openalex.org/W2903739847', 'https://openalex.org/W2975414524', 'https://openalex.org/W6767453231', 'https://openalex.org/W3016136182', 'https://openalex.org/W2747874407', 'https://openalex.org/W2963827314', 'https://openalex.org/W6712324455', 'https://openalex.org/W6778083308', 'https://openalex.org/W6769050887', 'https://openalex.org/W2913271971', 'https://openalex.org/W2963300588', 'https://openalex.org/W6778823374', 'https://openalex.org/W6777694618', 'https://openalex.org/W6749489859', 'https://openalex.org/W6763832098', 'https://openalex.org/W6745245109', 'https://openalex.org/W6739901393', 'https://openalex.org/W6762287338', 'https://openalex.org/W2746896242', 'https://openalex.org/W2395718496', 'https://openalex.org/W6753640700', 'https://openalex.org/W2886824226', 'https://openalex.org/W2963112338', 'https://openalex.org/W2396931946', 'https://openalex.org/W2996286887', 'https://openalex.org/W2519091744', 'https://openalex.org/W3025528898', 'https://openalex.org/W2963691546', 'https://openalex.org/W2763421725', 'https://openalex.org/W3015645837', 'https://openalex.org/W3130016944', 'https://openalex.org/W2971753973', 'https://openalex.org/W2946200149', 'https://openalex.org/W2963403868', 'https://openalex.org/W4385245566', 'https://openalex.org/W2949382160', 'https://openalex.org/W2970730223', 'https://openalex.org/W3026874504', 'https://openalex.org/W3033411150', 'https://openalex.org/W2995435108', 'https://openalex.org/W3122115223', 'https://openalex.org/W2107831318']",2021-05-13
https://openalex.org/W4283640572,https://doi.org/10.21437/interspeech.2022-46,SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech,"In this paper, we present SANE-TTS, a stable and natural end-to-end\nmultilingual TTS model. By the difficulty of obtaining multilingual corpus for\ngiven speaker, training multilingual TTS model with monolingual corpora is\nunavoidable. We introduce speaker regularization loss that improves speech\nnaturalness during cross-lingual synthesis as well as domain adversarial\ntraining, which is applied in other multilingual TTS models. Furthermore, by\nadding speaker regularization loss, replacing speaker embedding with zero\nvector in duration predictor stabilizes cross-lingual inference. With this\nreplacement, our model generates speeches with moderate rhythm regardless of\nsource speaker in cross-lingual synthesis. In MOS evaluation, SANE-TTS achieves\nnaturalness score above 3.80 both in cross-lingual and intralingual synthesis,\nwhere the ground truth score is 3.99. Also, SANE-TTS maintains speaker\nsimilarity close to that of ground truth even in cross-lingual inference. Audio\nsamples are available on our web page.\n","['https://openalex.org/W2972473628', 'https://openalex.org/W2969049672', 'https://openalex.org/W2946200149', 'https://openalex.org/W4385245566', 'https://openalex.org/W4225680573', 'https://openalex.org/W3026874504', 'https://openalex.org/W2963925437', 'https://openalex.org/W2133564696', 'https://openalex.org/W2973084242', 'https://openalex.org/W2808706139', 'https://openalex.org/W3033411150', 'https://openalex.org/W3197294703', 'https://openalex.org/W1731081199', 'https://openalex.org/W2963090522', 'https://openalex.org/W3150572638', 'https://openalex.org/W4225276642', 'https://openalex.org/W2936832667', 'https://openalex.org/W2963609956', 'https://openalex.org/W3094002217', 'https://openalex.org/W2964243274', 'https://openalex.org/W4287111656', 'https://openalex.org/W3168527213', 'https://openalex.org/W2120847449', 'https://openalex.org/W2990124956', 'https://openalex.org/W2972359262', 'https://openalex.org/W2766812927', 'https://openalex.org/W3095012670', 'https://openalex.org/W4287121924']",2022-09-16
https://openalex.org/W4392114301,https://doi.org/10.1109/taslp.2024.3369537,Text-Inductive Graphone-Based Language Adaptation for Low-Resource Speech Synthesis,"Neural text-to-speech (TTS) systems have made significant progress in generating natural synthetic speech. However, neural TTS requires large amounts of paired training data, which limits its applicability to a small number of resource-rich languages. Previous work on low-resource TTS has addressed the data hungriness based on transfer learning from a multilingual model to low-resource languages, but it still relies heavily on the availability of paired data for the target languages. In this paper, we propose a text-inductive language adaptation framework for low-resource TTS to address the cost of collecting the paired data for low-resource languages. To inject textual knowledge during transfer learning, our framework employs a two-stage adaptation scheme that utilizes both text-only and supervised data for the target language. In the text-based adaptation stage, we update the language-aware embedding layer with a masked language model objective using text-only data for the target language. In the supervised adaptation stage, the entire TTS model is updated using paired data for the target language. We also propose a graphone-based multilingual training method that jointly uses graphemes and International Phonetic Alphabet symbols (referred to as graphones) for resource-rich languages, while using only graphemes for low-resource languages. This approach facilitates the transfer of pronunciation knowledge from resource-rich to low-resource languages. Through extensive evaluations, we demonstrate that 1) our framework with text-based adaptation outperforms the previous supervised transfer learning approach, 2) the proposed graphone-based training method further improves the performance of both multilingual TTS and low-resource language adaptation. With only 5 minutes of paired data for fine-tuning, our method achieved highly intelligible synthetic speech with the character error rates of around 6 % for a target language.","['https://openalex.org/W2058094241', 'https://openalex.org/W6796730497', 'https://openalex.org/W6610006008', 'https://openalex.org/W4296069143', 'https://openalex.org/W6757094361', 'https://openalex.org/W2973034126', 'https://openalex.org/W6798575157', 'https://openalex.org/W3096303254', 'https://openalex.org/W4375869005', 'https://openalex.org/W3197324626', 'https://openalex.org/W4385764360', 'https://openalex.org/W2896457183', 'https://openalex.org/W3034469191', 'https://openalex.org/W3101498587', 'https://openalex.org/W2090755665', 'https://openalex.org/W2140576685', 'https://openalex.org/W6731979058', 'https://openalex.org/W2952638691', 'https://openalex.org/W2025638820', 'https://openalex.org/W2494654097', 'https://openalex.org/W4285189120', 'https://openalex.org/W2889028433', 'https://openalex.org/W4297841354', 'https://openalex.org/W2964002616', 'https://openalex.org/W2746132399', 'https://openalex.org/W3048217770', 'https://openalex.org/W6638665372', 'https://openalex.org/W2769280657', 'https://openalex.org/W2970854433', 'https://openalex.org/W6767737316', 'https://openalex.org/W4230579319', 'https://openalex.org/W3105421296', 'https://openalex.org/W3177252310', 'https://openalex.org/W3114579299', 'https://openalex.org/W2973217961', 'https://openalex.org/W3163339651', 'https://openalex.org/W4372183701', 'https://openalex.org/W4296068816', 'https://openalex.org/W2903739847', 'https://openalex.org/W6778823374', 'https://openalex.org/W6796464841', 'https://openalex.org/W4385993876', 'https://openalex.org/W4285181910', 'https://openalex.org/W3119308075', 'https://openalex.org/W3035390927', 'https://openalex.org/W6770212971', 'https://openalex.org/W2972802841', 'https://openalex.org/W6846288075', 'https://openalex.org/W2182894058', 'https://openalex.org/W6783867762', 'https://openalex.org/W2972359262', 'https://openalex.org/W6936113694', 'https://openalex.org/W2726515241', 'https://openalex.org/W3167533889', 'https://openalex.org/W6802659129', 'https://openalex.org/W4385245566', 'https://openalex.org/W2962780374', 'https://openalex.org/W2093450784', 'https://openalex.org/W2144994235', 'https://openalex.org/W3016160783', 'https://openalex.org/W6847363464', 'https://openalex.org/W4319862635', 'https://openalex.org/W2106568252', 'https://openalex.org/W3082130377', 'https://openalex.org/W4292341621', 'https://openalex.org/W3181257032', 'https://openalex.org/W4311000453', 'https://openalex.org/W2901389167', 'https://openalex.org/W3156563027', 'https://openalex.org/W1526974435', 'https://openalex.org/W3104723404', 'https://openalex.org/W3206375275', 'https://openalex.org/W3174758275']",2024-01-01
https://openalex.org/W2964002616,https://doi.org/10.1109/icassp.2019.8682674,Bytes Are All You Need: End-to-end Multilingual Speech Recognition and Synthesis with Bytes,"We present two end-to-end models: Audio-to-Byte (A2B) and Byte-to-Audio (B2A), for multilingual speech recognition and synthesis. Prior work has predominantly used characters, sub-words or words as the unit of choice to model text. These units are difficult to scale to languages with large vocabularies, particularly in the case of multilingual processing. In this work, we model text via a sequence of Unicode bytes, specifically, the UTF-8 variable length byte sequence for each character. Bytes allow us to avoid large softmaxes in languages with large vocabularies, and share representations in multilingual models. We show that bytes are superior to grapheme characters over a wide variety of languages in monolingual end-to-end speech recognition. Additionally, our multilingual byte model outperform each respective single language baseline on average by 4.4% relatively. In Japanese-English code-switching speech, our multilingual byte model outperform our monolingual baseline by 38.6% relatively. Finally, we present an end-to-end multilingual speech synthesis model using byte representations which matches the performance of our monolingual baselines.","['https://openalex.org/W2617258110', 'https://openalex.org/W6748409065', 'https://openalex.org/W2084543186', 'https://openalex.org/W6629052376', 'https://openalex.org/W2962826786', 'https://openalex.org/W2514969556', 'https://openalex.org/W6728811460', 'https://openalex.org/W2962824709', 'https://openalex.org/W2799800213', 'https://openalex.org/W6747398299', 'https://openalex.org/W6728910023', 'https://openalex.org/W2963970535', 'https://openalex.org/W6756197946', 'https://openalex.org/W6736356763', 'https://openalex.org/W6679434410', 'https://openalex.org/W3150637114', 'https://openalex.org/W2064675550', 'https://openalex.org/W1506752962', 'https://openalex.org/W80018330', 'https://openalex.org/W2515439472', 'https://openalex.org/W6630406810', 'https://openalex.org/W6675365184', 'https://openalex.org/W6600206001', 'https://openalex.org/W2014869452', 'https://openalex.org/W2327501763', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963208801', 'https://openalex.org/W6743440867', 'https://openalex.org/W2962893195', 'https://openalex.org/W6749107185', 'https://openalex.org/W2962704885', 'https://openalex.org/W2964309797', 'https://openalex.org/W2545177271', 'https://openalex.org/W1489125746', 'https://openalex.org/W2133564696', 'https://openalex.org/W2964308564', 'https://openalex.org/W2964307104', 'https://openalex.org/W60702959', 'https://openalex.org/W2604184139', 'https://openalex.org/W2901997113', 'https://openalex.org/W2788760202', 'https://openalex.org/W2952288254', 'https://openalex.org/W2102113734', 'https://openalex.org/W2963414781', 'https://openalex.org/W5476443', 'https://openalex.org/W2750499125', 'https://openalex.org/W2963920996', 'https://openalex.org/W2530486890', 'https://openalex.org/W1510837697', 'https://openalex.org/W4298580827']",2019-04-17
https://openalex.org/W3048217770,https://doi.org/10.21437/interspeech.2020-1821,Phonological Features for 0-Shot Multilingual Speech Synthesis,"Code-switching---the intra-utterance use of multiple languages---is prevalent\nacross the world. Within text-to-speech (TTS), multilingual models have been\nfound to enable code-switching. By modifying the linguistic input to\nsequence-to-sequence TTS, we show that code-switching is possible for languages\nunseen during training, even within monolingual models. We use a small set of\nphonological features derived from the International Phonetic Alphabet (IPA),\nsuch as vowel height and frontness, consonant place and manner. This allows the\nmodel topology to stay unchanged for different languages, and enables new,\npreviously unseen feature combinations to be interpreted by the model. We show\nthat this allows us to generate intelligible, code-switched speech in a new\nlanguage at test time, including the approximation of sounds never seen in\ntraining.\n","['https://openalex.org/W3022351837', 'https://openalex.org/W2187089797', 'https://openalex.org/W2622010270', 'https://openalex.org/W2169203263', 'https://openalex.org/W1598851216', 'https://openalex.org/W2527729766', 'https://openalex.org/W2891730843', 'https://openalex.org/W2129142580', 'https://openalex.org/W2806482367', 'https://openalex.org/W2573877011', 'https://openalex.org/W2068772561', 'https://openalex.org/W2972489453', 'https://openalex.org/W1630569004', 'https://openalex.org/W2964170290', 'https://openalex.org/W2972708855', 'https://openalex.org/W2572670101', 'https://openalex.org/W2316803017', 'https://openalex.org/W2120847449', 'https://openalex.org/W2010800472', 'https://openalex.org/W2972473628', 'https://openalex.org/W1992153276', 'https://openalex.org/W2964243274', 'https://openalex.org/W2622265915', 'https://openalex.org/W2964002616', 'https://openalex.org/W2746132399', 'https://openalex.org/W2151311453']",2020-10-25
https://openalex.org/W3194464626,https://doi.org/10.21437/ssw.2021-28,Cross-lingual Transfer of Phonological Features for Low-resource Speech Synthesis,"Previous work on cross-lingual transfer learning in text-tospeech has shown the effectiveness of fine-tuning phonemic representations on small amounts of target language data.In other contexts, phonological features (PFs) have been suggested as a more suitable input representation than phonemes for sharing acoustic information between languages, for example in multilingual model training or for code-switching synthesis where an utterance may contain words from multiple languages.Starting from a model trained on 14 hours of English, we find that cross-lingual fine-tuning with 15 minutes of German data can produce speech with subjective naturalness ratings comparable to a model trained from scratch on 4 hours of German, using either phonemes or PFs.We also find a modest but statistically significant improvement in naturalness ratings using PFs over phonemes when training from scratch on 4 hours of German.","['https://openalex.org/W2293770964', 'https://openalex.org/W2806482367', 'https://openalex.org/W2972802841', 'https://openalex.org/W3015338123', 'https://openalex.org/W2572670101', 'https://openalex.org/W4288265053', 'https://openalex.org/W1597121597', 'https://openalex.org/W2963609956', 'https://openalex.org/W3029598321', 'https://openalex.org/W2963827314', 'https://openalex.org/W3022351837', 'https://openalex.org/W2972489453', 'https://openalex.org/W2973034126', 'https://openalex.org/W2972610613', 'https://openalex.org/W2151311453', 'https://openalex.org/W2786672974', 'https://openalex.org/W2970457158', 'https://openalex.org/W1630569004', 'https://openalex.org/W2964243274', 'https://openalex.org/W3048217770', 'https://openalex.org/W2746132399', 'https://openalex.org/W3015484365', 'https://openalex.org/W3037676302']",2021-08-24
https://openalex.org/W4297841714,https://doi.org/10.21437/interspeech.2022-10797,WavThruVec: Latent speech representation as intermediate features for neural speech synthesis,"Recent advances in neural text-to-speech research have been dominated by two-stage pipelines utilizing low-level intermediate speech representation such as mel-spectrograms. However, such predetermined features are fundamentally limited, because they do not allow to exploit the full potential of a data-driven approach through learning hidden representations. For this reason, several end-to-end methods have been proposed. However, such models are harder to train and require a large number of high-quality recordings with transcriptions. Here, we propose WavThruVec - a two-stage architecture that resolves the bottleneck by using high-dimensional Wav2Vec 2.0 embeddings as intermediate speech representation. Since these hidden activations provide high-level linguistic features, they are more robust to noise. That allows us to utilize annotated speech datasets of a lower quality to train the first-stage module. At the same time, the second-stage component can be trained on large-scale untranscribed audio corpora, as Wav2Vec 2.0 embeddings are already time-aligned. This results in an increased generalization capability to out-of-vocabulary words, as well as to a better generalization to unseen speakers. We show that the proposed model not only matches the quality of state-of-the-art neural models, but also presents useful properties enabling tasks like voice conversion or zero-shot synthesis.","['https://openalex.org/W2901997113', 'https://openalex.org/W3026874504', 'https://openalex.org/W2964243274', 'https://openalex.org/W4252978691', 'https://openalex.org/W2974231335', 'https://openalex.org/W2946200149', 'https://openalex.org/W4287761884', 'https://openalex.org/W3161782335', 'https://openalex.org/W3150572638', 'https://openalex.org/W3198020407', 'https://openalex.org/W4294619240', 'https://openalex.org/W3140429000', 'https://openalex.org/W4394671563', 'https://openalex.org/W3036601975', 'https://openalex.org/W2970006822', 'https://openalex.org/W2903739847', 'https://openalex.org/W3123318516', 'https://openalex.org/W2086381917', 'https://openalex.org/W3174758275', 'https://openalex.org/W3092028330', 'https://openalex.org/W2766812927', 'https://openalex.org/W2996286887', 'https://openalex.org/W3094550259', 'https://openalex.org/W3194000401', 'https://openalex.org/W3030437843', 'https://openalex.org/W3033411150', 'https://openalex.org/W2519091744', 'https://openalex.org/W4297817572', 'https://openalex.org/W1494198834', 'https://openalex.org/W3024869864', 'https://openalex.org/W4287121924', 'https://openalex.org/W2963300588', 'https://openalex.org/W3123097577']",2022-09-16
https://openalex.org/W4225096077,https://doi.org/10.21437/interspeech.2022-10019,Why does Self-Supervised Learning for Speech Recognition Benefit Speaker Recognition?,"Recently, self-supervised learning (SSL) has demonstrated strong performance in speaker recognition, even if the pretraining objective is designed for speech recognition.In this paper, we study which factor leads to the success of selfsupervised learning on speaker-related tasks, e.g.speaker verification (SV), through a series of carefully designed experiments.Our empirical results on the Voxceleb-1 dataset suggest that the benefit of SSL to SV task is from a combination of mask speech prediction loss, data scale, and model size, while the SSL quantizer has a minor impact.We further employ the integrated gradients attribution method and loss landscape visualization to understand the effectiveness of self-supervised learning for speaker recognition performance.","['https://openalex.org/W4221161761', 'https://openalex.org/W3142516134', 'https://openalex.org/W2971033911', 'https://openalex.org/W2747874407', 'https://openalex.org/W2936774411', 'https://openalex.org/W4287120025', 'https://openalex.org/W3204696009', 'https://openalex.org/W4285666836', 'https://openalex.org/W4226221575', 'https://openalex.org/W3209984917', 'https://openalex.org/W2777662428', 'https://openalex.org/W2969985801', 'https://openalex.org/W3024869864', 'https://openalex.org/W3094374485', 'https://openalex.org/W3197580070', 'https://openalex.org/W2928165649', 'https://openalex.org/W3169320628', 'https://openalex.org/W2995181338', 'https://openalex.org/W3119308075', 'https://openalex.org/W3198698812', 'https://openalex.org/W3036601975', 'https://openalex.org/W2594633041', 'https://openalex.org/W2963799213', 'https://openalex.org/W1494198834', 'https://openalex.org/W2981087920', 'https://openalex.org/W4306169301']",2022-09-16
https://openalex.org/W4385329631,https://doi.org/10.1109/taslp.2023.3278184,Prosody Modelling With Pre-Trained Cross-Utterance Representations for Improved Speech Synthesis,"When humans speak multiple utterances in a continuous manner, the prosodic features generated in each utterance are related to those in its neighbouring utterances. Such cross-utterance (CU) dependencies are often ignored by the current neural text-to-speech (TTS) systems, which reduces the naturalness and expressiveness of the synthesized speeches. In this paper, we propose to improve the prosody modelling ability of neural TTS systems using pre-trained CU acoustic and text representations. Such CU acoustic representations are derived using the Wav2Vec 2.0 model (W2V2) from the synthesized audios of the past utterances, while the CU text representations are extracted using the Bidirectional Encoder Representation from Transformers (BERT) model from the scripts of the future utterances. Experimental results on a Mandarin audiobook and an English audiobook showed the naturalness and expressiveness of the synthesized audios were significantly improved by incorporating such pre-trained W2V2 and BERT CU representations into the Fastspeech2 TTS framework.","['https://openalex.org/W6755300632', 'https://openalex.org/W2795109282', 'https://openalex.org/W4226380987', 'https://openalex.org/W3195592874', 'https://openalex.org/W2960585468', 'https://openalex.org/W6739901393', 'https://openalex.org/W3034949308', 'https://openalex.org/W2904459034', 'https://openalex.org/W3197642003', 'https://openalex.org/W6750489868', 'https://openalex.org/W2979826702', 'https://openalex.org/W3163339651', 'https://openalex.org/W3202191685', 'https://openalex.org/W3151309757', 'https://openalex.org/W3198152857', 'https://openalex.org/W2746419079', 'https://openalex.org/W2765860599', 'https://openalex.org/W2395980997', 'https://openalex.org/W4388277895', 'https://openalex.org/W59470279', 'https://openalex.org/W2168531172', 'https://openalex.org/W3162948689', 'https://openalex.org/W3016137096', 'https://openalex.org/W6677973343', 'https://openalex.org/W4221163652', 'https://openalex.org/W3158374895', 'https://openalex.org/W6749489859', 'https://openalex.org/W3015338123', 'https://openalex.org/W3161959914', 'https://openalex.org/W3097538987', 'https://openalex.org/W6778823374', 'https://openalex.org/W6763832098', 'https://openalex.org/W2973217961', 'https://openalex.org/W3204696009', 'https://openalex.org/W3197580070', 'https://openalex.org/W3163738228', 'https://openalex.org/W2972881244', 'https://openalex.org/W6796551075', 'https://openalex.org/W6769196770', 'https://openalex.org/W3094550259', 'https://openalex.org/W4281492411', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963609956', 'https://openalex.org/W4296068816', 'https://openalex.org/W3197324626', 'https://openalex.org/W3113076898', 'https://openalex.org/W3170320568', 'https://openalex.org/W4313156423', 'https://openalex.org/W6755207826', 'https://openalex.org/W2088009352', 'https://openalex.org/W3196843885', 'https://openalex.org/W2936103087', 'https://openalex.org/W3198429080', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3150572638', 'https://openalex.org/W4289383906', 'https://openalex.org/W4385245566', 'https://openalex.org/W2979476256', 'https://openalex.org/W2946200149', 'https://openalex.org/W3036601975', 'https://openalex.org/W4287121455', 'https://openalex.org/W3033411150', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963691546', 'https://openalex.org/W4295731579', 'https://openalex.org/W2794490148', 'https://openalex.org/W2896457183']",2023-01-01
https://openalex.org/W4382202703,https://doi.org/10.1609/aaai.v37i11.26488,A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech,"Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.","['https://openalex.org/W6778883912', 'https://openalex.org/W3169688220', 'https://openalex.org/W6747158283', 'https://openalex.org/W4226132755', 'https://openalex.org/W3111551570', 'https://openalex.org/W6750305986', 'https://openalex.org/W4298277274', 'https://openalex.org/W2982055294', 'https://openalex.org/W6802659129', 'https://openalex.org/W2963981733', 'https://openalex.org/W6761551260', 'https://openalex.org/W1574170747', 'https://openalex.org/W3026874504', 'https://openalex.org/W6631190155', 'https://openalex.org/W3092028330', 'https://openalex.org/W3129009457', 'https://openalex.org/W6757079273', 'https://openalex.org/W3023126978', 'https://openalex.org/W2742542661', 'https://openalex.org/W2981087920', 'https://openalex.org/W3196318247', 'https://openalex.org/W2946200149', 'https://openalex.org/W3091928890', 'https://openalex.org/W2777302760', 'https://openalex.org/W4221145199', 'https://openalex.org/W2794490148', 'https://openalex.org/W6750226204', 'https://openalex.org/W6749954789', 'https://openalex.org/W2998572311', 'https://openalex.org/W6798096123', 'https://openalex.org/W6793472422', 'https://openalex.org/W3116834994', 'https://openalex.org/W3163906773', 'https://openalex.org/W1522301498', 'https://openalex.org/W3180355996', 'https://openalex.org/W4301206121', 'https://openalex.org/W3016160783', 'https://openalex.org/W3178839419', 'https://openalex.org/W3206375275', 'https://openalex.org/W2979476256', 'https://openalex.org/W4287019748', 'https://openalex.org/W4287121924', 'https://openalex.org/W4287120025', 'https://openalex.org/W3151269043', 'https://openalex.org/W2962780374', 'https://openalex.org/W2964243274', 'https://openalex.org/W4292779060', 'https://openalex.org/W2963096510', 'https://openalex.org/W2903739847', 'https://openalex.org/W2972702018', 'https://openalex.org/W4294619417', 'https://openalex.org/W4361994820', 'https://openalex.org/W4394671563', 'https://openalex.org/W2938704169']",2023-06-26
https://openalex.org/W4281760581,https://doi.org/10.21437/interspeech.2022-10140,Pronunciation Dictionary-Free Multilingual Speech Synthesis by Combining Unsupervised and Supervised Phonetic Representations,"This paper proposes a multilingual speech synthesis method which combines unsupervised phonetic representations (UPR) and supervised phonetic representations (SPR) to avoid reliance on the pronunciation dictionaries of target languages.In this method, a pretrained wav2vec 2.0 model is adopted to extract UPRs and a language-independent automatic speech recognition (LI-ASR) model is built with a connectionist temporal classification (CTC) loss to extract segment-level SPRs from the audio data of target languages.Then, an acoustic model is designed, which first predicts UPRs and SPRs from texts separately and then combines the predicted UPRs and SPRs to generate mel-spectrograms.The results of our experiments on six languages show that the proposed method outperformed the methods that directly predicted mel-spectrograms from character or phoneme sequences and the ablated models that utilized only UPRs or SPRs.","['https://openalex.org/W3092028330', 'https://openalex.org/W3140429000', 'https://openalex.org/W2903739847', 'https://openalex.org/W4302213377', 'https://openalex.org/W4200300291', 'https://openalex.org/W2514457011', 'https://openalex.org/W2316803017', 'https://openalex.org/W2946200149', 'https://openalex.org/W4287173589', 'https://openalex.org/W3174758275', 'https://openalex.org/W2963799213', 'https://openalex.org/W2884873108', 'https://openalex.org/W4286905904', 'https://openalex.org/W4297808394', 'https://openalex.org/W2963609956', 'https://openalex.org/W2964243274', 'https://openalex.org/W3036601975', 'https://openalex.org/W3033411150', 'https://openalex.org/W2747874407', 'https://openalex.org/W2933138175']",2022-09-16
https://openalex.org/W4385822479,https://doi.org/10.21437/interspeech.2023-2056,A Low-Resource Pipeline for Text-to-Speech from Found Data With Application to Scottish Gaelic,"In this work we present an end-to-end pipeline for building a speech corpus and text-to-speech synthesis system for a new language without reference to any expert-defined linguistic resources. We segment and align over 85 hours of Scottish Gaelic recordings found online and select 2- and 8-hour subsets with comprehensive coverage of speech sounds based on self-supervised discrete acoustic unit sequences. We then compare FastPitch models trained on these relatively small data sets using character, acoustic unit and phone inputs. According to native speaker listening test judgements, characters serve well for Gaelic given its regular orthography, even in these limited data scenarios. We release our corpus building recipe so that others may easily apply our work to new languages.",[],2023-08-14
https://openalex.org/W4389600306,https://doi.org/10.48550/arxiv.2312.05187,Seamless: Multilingual Expressive and Streaming Speech Translation,"Large-scale automatic speech translation systems today lack key features that help machine-mediated communication feel seamless when compared to human-to-human dialogue. In this work, we introduce a family of models that enable end-to-end expressive and multilingual translations in a streaming fashion. First, we contribute an improved version of the massively multilingual and multimodal SeamlessM4T model-SeamlessM4T v2. This newer model, incorporating an updated UnitY2 framework, was trained on more low-resource language data. SeamlessM4T v2 provides the foundation on which our next two models are initiated. SeamlessExpressive enables translation that preserves vocal styles and prosody. Compared to previous efforts in expressive speech research, our work addresses certain underexplored aspects of prosody, such as speech rate and pauses, while also preserving the style of one's voice. As for SeamlessStreaming, our model leverages the Efficient Monotonic Multihead Attention mechanism to generate low-latency target translations without waiting for complete source utterances. As the first of its kind, SeamlessStreaming enables simultaneous speech-to-speech/text translation for multiple source and target languages. To ensure that our models can be used safely and responsibly, we implemented the first known red-teaming effort for multimodal machine translation, a system for the detection and mitigation of added toxicity, a systematic evaluation of gender bias, and an inaudible localized watermarking mechanism designed to dampen the impact of deepfakes. Consequently, we bring major components from SeamlessExpressive and SeamlessStreaming together to form Seamless, the first publicly available system that unlocks expressive cross-lingual communication in real-time. The contributions to this work are publicly released and accessible at https://github.com/facebookresearch/seamless_communication",[],2023-12-08
https://openalex.org/W3196001064,https://doi.org/10.1109/icassp43922.2022.9747707,One TTS Alignment to Rule Them All,"Speech-to-text alignment is a critical component of neural text-to-speech (TTS) models. Autoregressive TTS models typically use an attention mechanism to learn these alignments on-line. However, these alignments tend to be brittle and often fail to generalize to long utterances and out-of-domain text, leading to missing or repeating words. Most non-autoregressive end-to-end TTS models rely on durations extracted from external sources. In this paper we leverage the alignment mechanism proposed in RAD-TTS and demonstrate its applicability to wide variety of neural TTS models. The alignment learning framework combines the forward-sum algorithm, Viterbi algorithm, and an efficient static prior. In our experiments, the framework improves all tested TTS architectures, both autoregressive (Flowtron, Tacotron 2) and non-autoregressive (FastPitch, FastSpeech 2, RAD-TTS). Specifically, it improves alignment convergence speed, simplifies the training pipeline by eliminating need for external aligners, enhances robustness to errors on long utterances and improves the perceived speech synthesis quality, as judged by human evaluators.","['https://openalex.org/W2747874407', 'https://openalex.org/W6779871621', 'https://openalex.org/W6800393981', 'https://openalex.org/W6745317255', 'https://openalex.org/W2125838338', 'https://openalex.org/W6623517193', 'https://openalex.org/W3015922793', 'https://openalex.org/W6917585676', 'https://openalex.org/W2107860279', 'https://openalex.org/W6778823374', 'https://openalex.org/W6778083308', 'https://openalex.org/W6777694618', 'https://openalex.org/W6763832098', 'https://openalex.org/W6638273328', 'https://openalex.org/W3150572638', 'https://openalex.org/W6746700228', 'https://openalex.org/W6736356763', 'https://openalex.org/W6679434410', 'https://openalex.org/W2972359262', 'https://openalex.org/W2973215447', 'https://openalex.org/W2964243274', 'https://openalex.org/W3033411150', 'https://openalex.org/W3038172701', 'https://openalex.org/W2970730223', 'https://openalex.org/W2127141656', 'https://openalex.org/W3035083561', 'https://openalex.org/W1810943226', 'https://openalex.org/W3194000401', 'https://openalex.org/W2964308564', 'https://openalex.org/W2953022181', 'https://openalex.org/W3025528898', 'https://openalex.org/W2604184139', 'https://openalex.org/W2946200149', 'https://openalex.org/W2932319281', 'https://openalex.org/W854541894', 'https://openalex.org/W2767052532', 'https://openalex.org/W3130016944', 'https://openalex.org/W3034949308', 'https://openalex.org/W2133564696', 'https://openalex.org/W3026874504']",2022-04-27
https://openalex.org/W4225534571,https://doi.org/10.21437/interspeech.2022-10796,Combining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Translation,"Self-Supervised Learning (SSL) models have been successfully applied in various deep learning-based speech tasks, particularly those with a limited amount of data. However, the quality of SSL representations depends highly on the relatedness between the SSL training domain(s) and the target data domain. On the contrary, spectral feature (SF) extractors such as log Mel-filterbanks are hand-crafted non-learnable components, and could be more robust to domain shifts. The present work examines the assumption that combining non-learnable SF extractors to SSL models is an effective approach to low resource speech tasks. We propose a learnable and interpretable framework to combine SF and SSL representations. The proposed framework outperforms significantly both baseline and SSL models on Automatic Speech Recognition (ASR) and Speech Translation (ST) tasks on three low resource datasets. We additionally design a mixture of experts based combination model. This last model reveals that the relative contribution of SSL models over conventional SF extractors is very small in case of domain mismatch between SSL training set and the target language data.",[],2022-09-16
https://openalex.org/W4296068817,https://doi.org/10.21437/interspeech.2022-952,A Multi-Stage Multi-Codebook VQ-VAE Approach to High-Performance Neural TTS,"We propose a Multi-Stage, Multi-Codebook (MSMC) approach to high performance neural TTS synthesis.A vector-quantized, variational autoencoder (VQ-VAE) based feature analyzer is used to encode Mel spectrograms of speech training data by down-sampling progressively in multiple stages into MSMC Representations (MSMCRs) with different time resolutions, and quantizing them with multiple VQ codebooks, respectively.Multi-stage predictors are trained to map the input text sequence to MSMCRs progressively by minimizing a combined loss of the reconstruction Mean Square Error (MSE) and ""triplet loss"".In synthesis, the neural vocoder converts the predicted MSM-CRs into final speech waveforms.The proposed approach is trained and tested with an English TTS database of 16 hours by a female speaker.The proposed TTS achieves an MOS score of 4.41, which outperforms the baseline with an MOS of 3.62.Compact versions of the proposed TTS with much less parameters can still preserve high MOS scores.Ablation studies show that both multiple stages and multiple codebooks are effective for achieving high TTS performance.","['https://openalex.org/W4297841320', 'https://openalex.org/W4287111234', 'https://openalex.org/W3026874504', 'https://openalex.org/W1522301498', 'https://openalex.org/W2517788403', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963971656', 'https://openalex.org/W2039225946', 'https://openalex.org/W1680622244', 'https://openalex.org/W3120243996', 'https://openalex.org/W2963799213', 'https://openalex.org/W4286905415', 'https://openalex.org/W2971074500', 'https://openalex.org/W3197273793', 'https://openalex.org/W2946200149', 'https://openalex.org/W4385245566', 'https://openalex.org/W2894176037', 'https://openalex.org/W3092028330', 'https://openalex.org/W2903739847', 'https://openalex.org/W4200219715', 'https://openalex.org/W2937909162', 'https://openalex.org/W2191179271', 'https://openalex.org/W2972702018', 'https://openalex.org/W4287121924', 'https://openalex.org/W2935711438', 'https://openalex.org/W3095277453']",2022-09-16
https://openalex.org/W3197273793,https://doi.org/10.21437/interspeech.2021-1016,UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation,"Most neural vocoders employ band-limited mel-spectrograms to generate waveforms.If full-band spectral features are used as the input, the vocoder can be provided with as much acoustic information as possible.However, in some models employing full-band mel-spectrograms, an over-smoothing problem occurs as part of which non-sharp spectrograms are generated.To address this problem, we propose UnivNet, a neural vocoder that synthesizes high-fidelity waveforms in real time.Inspired by works in the field of voice activity detection, we added a multiresolution spectrogram discriminator that employs multiple linear spectrogram magnitudes computed using various parameter sets.Using full-band mel-spectrograms as input, we expect to generate high-resolution signals by adding a discriminator that employs spectrograms of multiple resolutions as the input.In an evaluation on a dataset containing information on hundreds of speakers, UnivNet obtained the best objective and subjective results among competing models for both seen and unseen speakers.These results, including the best subjective score for text-to-speech, demonstrate the potential for fast adaptation to new speakers without a need for training from scratch.","['https://openalex.org/W2197404611', 'https://openalex.org/W4252713891', 'https://openalex.org/W2049686551', 'https://openalex.org/W2972359262', 'https://openalex.org/W2970006822', 'https://openalex.org/W2963300588', 'https://openalex.org/W2120847449', 'https://openalex.org/W3015338123', 'https://openalex.org/W3150572638', 'https://openalex.org/W1552314771', 'https://openalex.org/W3092028330', 'https://openalex.org/W1522301498', 'https://openalex.org/W2964243274', 'https://openalex.org/W3161236344', 'https://openalex.org/W3033411150', 'https://openalex.org/W3130774171', 'https://openalex.org/W2593414223', 'https://openalex.org/W2993118648', 'https://openalex.org/W2423557781', 'https://openalex.org/W4298580827', 'https://openalex.org/W2471520273', 'https://openalex.org/W3144035034', 'https://openalex.org/W2889329491', 'https://openalex.org/W3096442195', 'https://openalex.org/W2284050935', 'https://openalex.org/W2519091744', 'https://openalex.org/W4320013936', 'https://openalex.org/W2963975282']",2021-08-27
https://openalex.org/W2084534958,https://doi.org/10.1109/icassp.2013.6639248,GlobalPhone: A multilingual text &amp;amp; speech database in 20 languages,"This paper describes the advances in the multilingual text and speech database GlobalPhone, a multilingual database of high-quality read speech with corresponding transcriptions and pronunciation dictionaries in 20 languages. GlobalPhone was designed to be uniform across languages with respect to the amount of data, speech quality, the collection scenario, the transcription and phone set conventions. With more than 400 hours of transcribed audio data from more than 2000 native speakers GlobalPhone supplies an excellent basis for research in the areas of multilingual speech recognition, rapid deployment of speech processing systems to yet unsupported languages, language identification tasks, speaker recognition in multiple languages, multilingual speech synthesis, as well as monolingual speech recognition in a large variety of languages.","['https://openalex.org/W2033436836', 'https://openalex.org/W6636811518', 'https://openalex.org/W2407441242', 'https://openalex.org/W198385923', 'https://openalex.org/W6605012280', 'https://openalex.org/W6602682705', 'https://openalex.org/W6675631943', 'https://openalex.org/W1631260214', 'https://openalex.org/W2104499423', 'https://openalex.org/W121945369', 'https://openalex.org/W273093436', 'https://openalex.org/W60702959', 'https://openalex.org/W66627554']",2013-05-01
https://openalex.org/W4225956675,https://doi.org/10.21437/interspeech.2022-439,UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022,"We present the UTokyo-SaruLab mean opinion score (MOS) prediction system submitted to VoiceMOS Challenge 2022.The challenge is to predict the MOS values of speech samples collected from previous Blizzard Challenges and Voice Conversion Challenges for two tracks: a main track for in-domain prediction and an out-of-domain (OOD) track for which there is less labeled data from different listening tests.Our system is based on ensemble learning of strong and weak learners.Strong learners incorporate several improvements to the previous finetuning models of self-supervised learning (SSL) models, while weak learners use basic machine-learning methods to predict scores from SSL features.In the Challenge, our system had the highest score on several metrics for both the main and OOD tracks.In addition, we conducted ablation studies to investigate the effectiveness of our proposed methods.","['https://openalex.org/W3196225973', 'https://openalex.org/W3209984917', 'https://openalex.org/W4300816538', 'https://openalex.org/W3090098535', 'https://openalex.org/W1673310716', 'https://openalex.org/W70888257', 'https://openalex.org/W3030437843', 'https://openalex.org/W3169320628', 'https://openalex.org/W3036601975', 'https://openalex.org/W2557915412', 'https://openalex.org/W3198429080', 'https://openalex.org/W3144810982', 'https://openalex.org/W4322714819', 'https://openalex.org/W2962897394', 'https://openalex.org/W2768348081', 'https://openalex.org/W2972394484', 'https://openalex.org/W3200245256', 'https://openalex.org/W1494198834', 'https://openalex.org/W3202278141', 'https://openalex.org/W4311829866', 'https://openalex.org/W3160506022', 'https://openalex.org/W1522301498', 'https://openalex.org/W28412257', 'https://openalex.org/W3161558238', 'https://openalex.org/W605727707']",2022-09-16
https://openalex.org/W4392538788,https://doi.org/10.48550/arxiv.2403.03100,NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models,"While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can effectively and efficiently model intricate speech with disentangled subspaces in a divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the state-of-the-art TTS systems on quality, similarity, prosody, and intelligibility, and achieves on-par quality with human recordings. Furthermore, we achieve better performance by scaling to 1B parameters and 200K hours of training data.",[],2024-03-05
https://openalex.org/W4226424742,https://doi.org/10.48550/arxiv.2201.08124,Cross-Lingual Text-to-Speech Using Multi-Task Learning and Speaker Classifier Joint Training,"In cross-lingual speech synthesis, the speech in various languages can be synthesized for a monoglot speaker. Normally, only the data of monoglot speakers are available for model training, thus the speaker similarity is relatively low between the synthesized cross-lingual speech and the native language recordings. Based on the multilingual transformer text-to-speech model, this paper studies a multi-task learning framework to improve the cross-lingual speaker similarity. To further improve the speaker similarity, joint training with a speaker classifier is proposed. Here, a scheme similar to parallel scheduled sampling is proposed to train the transformer model efficiently to avoid breaking the parallel training mechanism when introducing joint training. By using multi-task learning and speaker classifier joint training, in subjective and objective evaluations, the cross-lingual speaker similarity can be consistently improved for both the seen and unseen speakers in the training set.",[],2022-01-20
https://openalex.org/W4366460484,https://doi.org/10.48550/arxiv.2304.09116,NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers,"Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild datasets is important to capture the diversity in human speech such as speaker identities, prosodies, and styles (e.g., singing). Current large TTS systems usually quantize speech into discrete tokens and use language models to generate these tokens one by one, which suffer from unstable prosody, word skipping/repeating issue, and poor voice quality. In this paper, we develop NaturalSpeech 2, a TTS system that leverages a neural audio codec with residual vector quantizers to get the quantized latent vectors and uses a diffusion model to generate these latent vectors conditioned on text input. To enhance the zero-shot capability that is important to achieve diverse speech synthesis, we design a speech prompting mechanism to facilitate in-context learning in the diffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to large-scale datasets with 44K hours of speech and singing data and evaluate its voice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS systems by a large margin in terms of prosody/timbre similarity, robustness, and voice quality in a zero-shot setting, and performs novel zero-shot singing synthesis with only a speech prompt. Audio samples are available at https://speechresearch.github.io/naturalspeech2.",[],2023-04-18
https://openalex.org/W3090254849,https://doi.org/10.48550/arxiv.2009.14153,Clova Baseline System for the VoxCeleb Speaker Recognition Challenge 2020,"This report describes our submission to the VoxCeleb Speaker Recognition Challenge (VoxSRC) at Interspeech 2020. We perform a careful analysis of speaker recognition models based on the popular ResNet architecture, and train a number of variants using a range of loss functions. Our results show significant improvements over most existing works without the use of model ensemble or post-processing. We release the training code and pre-trained models as unofficial baselines for this year's challenge.","['https://openalex.org/W2969985801', 'https://openalex.org/W3025075133', 'https://openalex.org/W3015368919', 'https://openalex.org/W2970971581', 'https://openalex.org/W2779809129', 'https://openalex.org/W2962898354', 'https://openalex.org/W2963371159', 'https://openalex.org/W2890964092', 'https://openalex.org/W3010925296', 'https://openalex.org/W2194775991', 'https://openalex.org/W2696967604', 'https://openalex.org/W2219249508', 'https://openalex.org/W2808631503', 'https://openalex.org/W3013020904', 'https://openalex.org/W2981461916', 'https://openalex.org/W2794506738', 'https://openalex.org/W2502312327', 'https://openalex.org/W2991951409']",2020-09-29
https://openalex.org/W4388927799,https://doi.org/10.48550/arxiv.2311.12454,HierSpeech++: Bridging the Gap between Semantic and Acoustic Representation of Speech by Hierarchical Variational Inference for Zero-shot Speech Synthesis,"Large language models (LLM)-based speech synthesis has been widely adopted in zero-shot speech synthesis. However, they require a large-scale data and possess the same limitations as previous autoregressive speech models, including slow inference speed and lack of robustness. This paper proposes HierSpeech++, a fast and strong zero-shot speech synthesizer for text-to-speech (TTS) and voice conversion (VC). We verified that hierarchical speech synthesis frameworks could significantly improve the robustness and expressiveness of the synthetic speech. Furthermore, we significantly improve the naturalness and speaker similarity of synthetic speech even in zero-shot speech synthesis scenarios. For text-to-speech, we adopt the text-to-vec framework, which generates a self-supervised speech representation and an F0 representation based on text representations and prosody prompts. Then, HierSpeech++ generates speech from the generated vector, F0, and voice prompt. We further introduce a high-efficient speech super-resolution framework from 16 kHz to 48 kHz. The experimental results demonstrated that the hierarchical variational autoencoder could be a strong zero-shot speech synthesizer given that it outperforms LLM-based and diffusion-based models. Moreover, we achieved the first human-level quality zero-shot speech synthesis. Audio samples and source code are available at https://github.com/sh-lee-prml/HierSpeechpp.",[],2023-11-21
https://openalex.org/W4323651091,https://doi.org/10.48550/arxiv.2303.03926,Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling,"We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \url{https://aka.ms/vallex}.",[],2023-03-07
https://openalex.org/W2296607128,https://doi.org/10.1109/asru.2017.8268953,The zero resource speech challenge 2017,We describe a new challenge aimed at discovering subword and word units from raw speech. This challenge is the followup to the Zero Resource Speech Challenge 2015. It aims at constructing systems that generalize across languages and adapt to new speakers. The design features and evaluation metrics of the challenge are presented and the results of seventeen models are discussed.,"['https://openalex.org/W2964169922', 'https://openalex.org/W2126377586', 'https://openalex.org/W6748325621', 'https://openalex.org/W2057007397', 'https://openalex.org/W6973666849', 'https://openalex.org/W6704726871', 'https://openalex.org/W91681889', 'https://openalex.org/W2101509422', 'https://openalex.org/W2150389998', 'https://openalex.org/W6631362777', 'https://openalex.org/W6731521493', 'https://openalex.org/W2395899413', 'https://openalex.org/W6713745070', 'https://openalex.org/W6638159135', 'https://openalex.org/W6713256719', 'https://openalex.org/W2055408826', 'https://openalex.org/W2466918907', 'https://openalex.org/W2785415724', 'https://openalex.org/W2101281673', 'https://openalex.org/W2582956876', 'https://openalex.org/W2963211739', 'https://openalex.org/W6712553779', 'https://openalex.org/W6678947187', 'https://openalex.org/W2787447541', 'https://openalex.org/W2786902352', 'https://openalex.org/W2785860501', 'https://openalex.org/W2787223168', 'https://openalex.org/W2284628133', 'https://openalex.org/W2599585580', 'https://openalex.org/W2399576818', 'https://openalex.org/W2572097499', 'https://openalex.org/W2128032727', 'https://openalex.org/W2346964103', 'https://openalex.org/W2483390977', 'https://openalex.org/W2963620343']",2017-12-01
https://openalex.org/W3044386551,https://doi.org/10.21437/interspeech.2020-3000,Self-Expressing Autoencoders for Unsupervised Spoken Term Discovery,"Unsupervised spoken term discovery consists of two tasks: finding the acoustic segment boundaries and labeling acoustically similar segments with the same labels. We perform segmentation based on the assumption that the frame feature vectors are more similar within a segment than across the segments. Therefore, for strong segmentation performance, it is crucial that the features represent the phonetic properties of a frame more than other factors of variability. We achieve this via a self-expressing autoencoder framework. It consists of a single encoder and two decoders with shared weights. The encoder projects the input features into a latent representation. One of the decoders tries to reconstruct the input from these latent representations and the other from the self-expressed version of them. We use the obtained features to segment and cluster the speech data. We evaluate the performance of the proposed method in the Zero Resource 2020 challenge unit discovery task. The proposed system consistently outperforms the baseline, demonstrating the usefulness of the method in learning representations.","['https://openalex.org/W2972794572', 'https://openalex.org/W2160815625', 'https://openalex.org/W2803005441', 'https://openalex.org/W1545920196', 'https://openalex.org/W2166637769', 'https://openalex.org/W2786608204', 'https://openalex.org/W2398490608', 'https://openalex.org/W2078769636', 'https://openalex.org/W2100768664', 'https://openalex.org/W2170659185', 'https://openalex.org/W2404799143', 'https://openalex.org/W2963620343', 'https://openalex.org/W2747192917', 'https://openalex.org/W2020607164', 'https://openalex.org/W1994458317', 'https://openalex.org/W2057007397', 'https://openalex.org/W2468716020', 'https://openalex.org/W2964169922', 'https://openalex.org/W2126203737', 'https://openalex.org/W2150769028', 'https://openalex.org/W2991213871', 'https://openalex.org/W2890718354', 'https://openalex.org/W2400549570', 'https://openalex.org/W126222424', 'https://openalex.org/W1922655562', 'https://openalex.org/W2981857663', 'https://openalex.org/W2251025892', 'https://openalex.org/W2940544976', 'https://openalex.org/W1796128977']",2020-10-25
https://openalex.org/W2284628133,https://doi.org/10.48550/arxiv.1512.01882,THCHS-30 : A Free Chinese Speech Corpus,"Speech data is crucially important for speech recognition research. There are quite some speech databases that can be purchased at prices that are reasonable for most research institutes. However, for young people who just start research activities or those who just gain initial interest in this direction, the cost for data is still an annoying barrier. We support the `free data' movement in speech recognition: research institutes (particularly supported by public funds) publish their data freely so that new researchers can obtain sufficient data to kick of their career. In this paper, we follow this trend and release a free Chinese speech database THCHS-30 that can be used to build a full- edged Chinese speech recognition system. We report the baseline system established with this database, including the performance under highly noisy conditions.","['https://openalex.org/W1524333225', 'https://openalex.org/W2024539680', 'https://openalex.org/W2027197476', 'https://openalex.org/W1631260214', 'https://openalex.org/W1970663544', 'https://openalex.org/W2027499299', 'https://openalex.org/W2131342762', 'https://openalex.org/W2166637769', 'https://openalex.org/W330298975', 'https://openalex.org/W2213952365', 'https://openalex.org/W1984541135', 'https://openalex.org/W2060312364']",2015-12-07
https://openalex.org/W2598638573,https://doi.org/10.21437/ssw.2016-33,Merlin: An Open Source Neural Network Speech Synthesis System,"We introduce the Merlin speech synthesis toolkit for neural network-based speech synthesis.The system takes linguistic features as input, and employs neural networks to predict acoustic features, which are then passed to a vocoder to produce the speech waveform.Various neural network architectures are implemented, including a standard feedforward neural network, mixture density neural network, recurrent neural network (RNN), long short-term memory (LSTM) recurrent neural network, amongst others.The toolkit is Open Source, written in Python, and is extensible.This paper briefly describes the system, and provides some benchmarking results on a freelyavailable corpus.","['https://openalex.org/W2402539796', 'https://openalex.org/W2100167690', 'https://openalex.org/W2129142580', 'https://openalex.org/W2399853303', 'https://openalex.org/W1924770834', 'https://openalex.org/W2406990556', 'https://openalex.org/W2049686551', 'https://openalex.org/W1990505856', 'https://openalex.org/W2402103843', 'https://openalex.org/W2045158511', 'https://openalex.org/W2111284386', 'https://openalex.org/W2134973740', 'https://openalex.org/W1888924187', 'https://openalex.org/W1571950845', 'https://openalex.org/W2397670047', 'https://openalex.org/W2102003408', 'https://openalex.org/W3123963976', 'https://openalex.org/W1613141907', 'https://openalex.org/W133102907', 'https://openalex.org/W2964060510', 'https://openalex.org/W1576227399', 'https://openalex.org/W2294797155', 'https://openalex.org/W2806733747', 'https://openalex.org/W2079735306', 'https://openalex.org/W1499332833', 'https://openalex.org/W2239904444', 'https://openalex.org/W1502723613', 'https://openalex.org/W1120805016', 'https://openalex.org/W2020024436', 'https://openalex.org/W2170980774', 'https://openalex.org/W31154030', 'https://openalex.org/W2395700867', 'https://openalex.org/W1544516254', 'https://openalex.org/W2404881427', 'https://openalex.org/W2043003570', 'https://openalex.org/W2471520273', 'https://openalex.org/W1543299179', 'https://openalex.org/W2136374105']",2016-09-13
https://openalex.org/W2962693497,https://doi.org/10.1109/icassp.2018.8461545,Bayesian Models for Unit Discovery on a Very Low Resource Language,Accepted to ICASSP 2018,"['https://openalex.org/W6640963894', 'https://openalex.org/W1516111018', 'https://openalex.org/W6704885441', 'https://openalex.org/W6705006301', 'https://openalex.org/W2564058731', 'https://openalex.org/W2140991203', 'https://openalex.org/W2142390309', 'https://openalex.org/W6739631751', 'https://openalex.org/W6740233529', 'https://openalex.org/W2750248772', 'https://openalex.org/W6719357382', 'https://openalex.org/W6606665087', 'https://openalex.org/W2033413759', 'https://openalex.org/W2347098582', 'https://openalex.org/W6675022971', 'https://openalex.org/W6638813818', 'https://openalex.org/W2126377586', 'https://openalex.org/W2963620343', 'https://openalex.org/W6691362072', 'https://openalex.org/W2057007397', 'https://openalex.org/W2345799635', 'https://openalex.org/W2719865699', 'https://openalex.org/W2251025892', 'https://openalex.org/W1959608418', 'https://openalex.org/W4294562888', 'https://openalex.org/W4300047444', 'https://openalex.org/W166614460', 'https://openalex.org/W2347145335', 'https://openalex.org/W1833498382', 'https://openalex.org/W2762715843', 'https://openalex.org/W2464234964', 'https://openalex.org/W1506806321', 'https://openalex.org/W2100768664', 'https://openalex.org/W4237840503', 'https://openalex.org/W2641832364']",2018-04-01
https://openalex.org/W3026041220,https://doi.org/10.21437/interspeech.2020-1470,Improved Noisy Student Training for Automatic Speech Recognition,"Recently, a semi-supervised learning method known as ""noisy student training""\nhas been shown to improve image classification performance of deep networks\nsignificantly. Noisy student training is an iterative self-training method that\nleverages augmentation to improve network performance. In this work, we adapt\nand improve noisy student training for automatic speech recognition, employing\n(adaptive) SpecAugment as the augmentation method. We find effective methods to\nfilter, balance and augment the data generated in between self-training\niterations. By doing so, we are able to obtain word error rates (WERs)\n4.2%/8.6% on the clean/noisy LibriSpeech test sets by only using the clean 100h\nsubset of LibriSpeech as the supervised set and the rest (860h) as the\nunlabeled set. Furthermore, we are able to achieve WERs 1.7%/3.4% on the\nclean/noisy LibriSpeech test sets by using the unlab-60k subset of LibriLight\nas the unlabeled set for LibriSpeech 960h. We are thus able to improve upon the\nprevious state-of-the-art clean/noisy test WERs achieved on LibriSpeech 100h\n(4.74%/12.20%) and LibriSpeech (1.9%/4.1%).\n","['https://openalex.org/W2088622183', 'https://openalex.org/W1915251500', 'https://openalex.org/W3125709657', 'https://openalex.org/W1494198834', 'https://openalex.org/W2964245029', 'https://openalex.org/W2111316763', 'https://openalex.org/W2327501763', 'https://openalex.org/W165878654', 'https://openalex.org/W3103005696', 'https://openalex.org/W3015522062', 'https://openalex.org/W1828163288', 'https://openalex.org/W1489125746', 'https://openalex.org/W2976223659', 'https://openalex.org/W2998532468', 'https://openalex.org/W2995181338', 'https://openalex.org/W2962907457', 'https://openalex.org/W2973049979', 'https://openalex.org/W2963425185', 'https://openalex.org/W2101210369', 'https://openalex.org/W2996383576', 'https://openalex.org/W2995746049', 'https://openalex.org/W3015995734', 'https://openalex.org/W2940322076', 'https://openalex.org/W1993660824', 'https://openalex.org/W2963920996', 'https://openalex.org/W2962369866', 'https://openalex.org/W2121879602', 'https://openalex.org/W2577366047', 'https://openalex.org/W2117590177', 'https://openalex.org/W3001197829', 'https://openalex.org/W4285719527', 'https://openalex.org/W2936774411', 'https://openalex.org/W2981952041', 'https://openalex.org/W2944255943', 'https://openalex.org/W3016010032', 'https://openalex.org/W2972943112', 'https://openalex.org/W2912492482', 'https://openalex.org/W3035160371', 'https://openalex.org/W3015265920', 'https://openalex.org/W2062366414', 'https://openalex.org/W2979476256']",2020-10-25
https://openalex.org/W4285181910,https://doi.org/10.18653/v1/2022.findings-acl.166,Zero-shot Learning for Grapheme to Phoneme Conversion with Language Ensemble,"Grapheme-to-Phoneme (G2P) has many applications in NLP and speech fields. Most existing work focuses heavily on languages with abundant training datasets, which limits the scope of target languages to less than 100 languages. This work attempts to apply zero-shot learning to approximate G2P models for all low-resource and endangered languages in Glottolog (about 8k languages). For any unseen target language, we first build the phylogenetic tree (i.e. language family tree) to identify top-k nearest languages for which we have training sets. Then we run models of those languages to obtain a hypothesis set, which we combine into a confusion network to propose a most likely hypothesis as an approximation to the target language. We test our approach on over 600 unseen languages and demonstrate it significantly outperforms baselines.","['https://openalex.org/W2102443632', 'https://openalex.org/W2963913768', 'https://openalex.org/W1904457459', 'https://openalex.org/W3037676302', 'https://openalex.org/W1593247906', 'https://openalex.org/W1923658367', 'https://openalex.org/W2805993470', 'https://openalex.org/W2090755665', 'https://openalex.org/W3206375275', 'https://openalex.org/W2572721812', 'https://openalex.org/W2739967986', 'https://openalex.org/W3029344629', 'https://openalex.org/W2594610113', 'https://openalex.org/W1736701665', 'https://openalex.org/W2120321299', 'https://openalex.org/W2972528034', 'https://openalex.org/W1779680350', 'https://openalex.org/W3196485121', 'https://openalex.org/W2998284473', 'https://openalex.org/W3198105115', 'https://openalex.org/W622382977', 'https://openalex.org/W2920834369', 'https://openalex.org/W2572670101', 'https://openalex.org/W3015877095', 'https://openalex.org/W803770162', 'https://openalex.org/W2752573522', 'https://openalex.org/W1580142630', 'https://openalex.org/W1513168562']",2022-01-01
https://openalex.org/W3037465386,https://doi.org/10.18653/v1/2020.iwslt-1.1,FINDINGS OF THE IWSLT 2020 EVALUATION CAMPAIGN,"The evaluation campaign of the International Conference on Spoken Language Translation (IWSLT 2020) featured this year six challenge tracks: (i) Simultaneous speech translation, (ii) Video speech translation, (iii) Offline speech translation, (iv) Conversational speech translation, (v) Open domain translation, and (vi) Non-native speech translation. A total of 30 teams participated in at least one of the tracks. This paper introduces each track's goal, data and evaluation metrics, and reports the results of the received submissions.","['https://openalex.org/W2939710050', 'https://openalex.org/W2168103496', 'https://openalex.org/W3015583403', 'https://openalex.org/W3015633994', 'https://openalex.org/W2127141656', 'https://openalex.org/W2977725500', 'https://openalex.org/W2960374072', 'https://openalex.org/W3037231493', 'https://openalex.org/W3012492057', 'https://openalex.org/W2512848817', 'https://openalex.org/W3037344076', 'https://openalex.org/W3015703505', 'https://openalex.org/W2973048981', 'https://openalex.org/W2916456038', 'https://openalex.org/W3037542581', 'https://openalex.org/W3015889230', 'https://openalex.org/W3037277288', 'https://openalex.org/W3037625699', 'https://openalex.org/W2973049979', 'https://openalex.org/W3038129562', 'https://openalex.org/W2964161387', 'https://openalex.org/W2915395439', 'https://openalex.org/W3036994232', 'https://openalex.org/W1484504603', 'https://openalex.org/W3204130541', 'https://openalex.org/W3037025621', 'https://openalex.org/W2963418779', 'https://openalex.org/W3016137827', 'https://openalex.org/W3037292298', 'https://openalex.org/W3037440258', 'https://openalex.org/W3037255450', 'https://openalex.org/W2763856713', 'https://openalex.org/W2951456627', 'https://openalex.org/W3037298611', 'https://openalex.org/W3163093788', 'https://openalex.org/W3013713299', 'https://openalex.org/W3037462739', 'https://openalex.org/W3101648800', 'https://openalex.org/W2136630431', 'https://openalex.org/W2964110616', 'https://openalex.org/W2250875036', 'https://openalex.org/W2952992734', 'https://openalex.org/W3029944303', 'https://openalex.org/W2395447549', 'https://openalex.org/W2785350307', 'https://openalex.org/W4288345582', 'https://openalex.org/W2251321385', 'https://openalex.org/W2211796614', 'https://openalex.org/W3037186962', 'https://openalex.org/W2945286432', 'https://openalex.org/W2515295520', 'https://openalex.org/W2963779652', 'https://openalex.org/W2899274165', 'https://openalex.org/W2963532001', 'https://openalex.org/W2973215447', 'https://openalex.org/W2917668649', 'https://openalex.org/W2899773543', 'https://openalex.org/W2099655049', 'https://openalex.org/W2963212250', 'https://openalex.org/W2936774411', 'https://openalex.org/W2945700568', 'https://openalex.org/W3037187981', 'https://openalex.org/W1569447338', 'https://openalex.org/W3008125272', 'https://openalex.org/W3005389111', 'https://openalex.org/W2140818133', 'https://openalex.org/W3015698636', 'https://openalex.org/W3037338656', 'https://openalex.org/W2964343359', 'https://openalex.org/W2133512280', 'https://openalex.org/W3037322406', 'https://openalex.org/W2184135559', 'https://openalex.org/W3041866211', 'https://openalex.org/W3202218022', 'https://openalex.org/W4385245566', 'https://openalex.org/W3038114179', 'https://openalex.org/W2149327368', 'https://openalex.org/W2419539795', 'https://openalex.org/W1494198834', 'https://openalex.org/W2496235729', 'https://openalex.org/W1524333225', 'https://openalex.org/W2325237720', 'https://openalex.org/W4288284086', 'https://openalex.org/W2978099976', 'https://openalex.org/W2291598608', 'https://openalex.org/W2101105183', 'https://openalex.org/W3037627767', 'https://openalex.org/W630532510', 'https://openalex.org/W2806412155', 'https://openalex.org/W3038092046', 'https://openalex.org/W2963403868', 'https://openalex.org/W2972451902', 'https://openalex.org/W2183341477', 'https://openalex.org/W2962780374', 'https://openalex.org/W1686810756', 'https://openalex.org/W2955541912', 'https://openalex.org/W3032001786', 'https://openalex.org/W2905927205', 'https://openalex.org/W2251366179', 'https://openalex.org/W4297747548', 'https://openalex.org/W2953830716', 'https://openalex.org/W3208326910', 'https://openalex.org/W2917604489', 'https://openalex.org/W2917807695', 'https://openalex.org/W2962835968', 'https://openalex.org/W2946888380']",2020-01-01
https://openalex.org/W3207738474,,Direct simultaneous speech to speech translation,"We present the first direct simultaneous speech-to-speech translation (Simul-S2ST) model, with the ability to start generating translation in the target speech before consuming the full source speech content and independently from intermediate text representations. Our approach leverages recent progress on direct speech-to-speech translation with discrete units. Instead of continuous spectrogram features, a sequence of direct representations, which are learned in a unsupervised manner, are predicted from the model and passed directly to a vocoder for speech synthesis. The simultaneous policy then operates on source speech features and target discrete units. Finally, a vocoder synthesize the target speech from discrete units on-the-fly. We carry out numerical studies to compare cascaded and direct approach on Fisher Spanish-English dataset.","['https://openalex.org/W3098403858', 'https://openalex.org/W3162665866', 'https://openalex.org/W3162000275', 'https://openalex.org/W2963403868', 'https://openalex.org/W3180374548', 'https://openalex.org/W3034586846', 'https://openalex.org/W3105422997', 'https://openalex.org/W3104081910', 'https://openalex.org/W3186843219', 'https://openalex.org/W3118578889', 'https://openalex.org/W3197407562', 'https://openalex.org/W3169320628', 'https://openalex.org/W3035348852', 'https://openalex.org/W3115075512', 'https://openalex.org/W3130016944', 'https://openalex.org/W2952992734', 'https://openalex.org/W2995428172', 'https://openalex.org/W2936184970', 'https://openalex.org/W2964078338', 'https://openalex.org/W3046643869', 'https://openalex.org/W2529548870', 'https://openalex.org/W3100608856', 'https://openalex.org/W2419292002']",2021-10-15
https://openalex.org/W3094502228,https://doi.org/10.48550/arxiv.2010.11929,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.","['https://openalex.org/W2799269579', 'https://openalex.org/W2147800946', 'https://openalex.org/W2964080601', 'https://openalex.org/W2194775991', 'https://openalex.org/W2968124245', 'https://openalex.org/W3097065222', 'https://openalex.org/W2108598243', 'https://openalex.org/W2971315489', 'https://openalex.org/W3034445277', 'https://openalex.org/W2963091558', 'https://openalex.org/W3008526508', 'https://openalex.org/W2981689412', 'https://openalex.org/W3034885317', 'https://openalex.org/W2962843773', 'https://openalex.org/W3096609285', 'https://openalex.org/W3102696055', 'https://openalex.org/W2284050935', 'https://openalex.org/W2998108143', 'https://openalex.org/W3097217077', 'https://openalex.org/W2948798935', 'https://openalex.org/W2963341956', 'https://openalex.org/W2964121744', 'https://openalex.org/W2949117887', 'https://openalex.org/W2937843571', 'https://openalex.org/W3034978746', 'https://openalex.org/W3035058308', 'https://openalex.org/W2970608575', 'https://openalex.org/W2963631907', 'https://openalex.org/W2981413347', 'https://openalex.org/W2931316642', 'https://openalex.org/W3012324658', 'https://openalex.org/W3035422918', 'https://openalex.org/W3033210410', 'https://openalex.org/W3035282577', 'https://openalex.org/W2940744433', 'https://openalex.org/W2950739196', 'https://openalex.org/W2163605009', 'https://openalex.org/W3030163527', 'https://openalex.org/W1977295328', 'https://openalex.org/W2994760783', 'https://openalex.org/W3040573126', 'https://openalex.org/W2533598788', 'https://openalex.org/W3040304705', 'https://openalex.org/W2983446232', 'https://openalex.org/W2086161653', 'https://openalex.org/W3035524453', 'https://openalex.org/W2626778328', 'https://openalex.org/W3035160371', 'https://openalex.org/W2994759459', 'https://openalex.org/W3042608254', 'https://openalex.org/W2970389371', 'https://openalex.org/W3090449556', 'https://openalex.org/W3118608800']",2020-10-22
https://openalex.org/W1533861849,,Understanding the difficulty of training deep feedforward neural networks,"Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include","['https://openalex.org/W2172174689', 'https://openalex.org/W2110798204', 'https://openalex.org/W2140833774', 'https://openalex.org/W2159291644', 'https://openalex.org/W2131462252', 'https://openalex.org/W2117499988', 'https://openalex.org/W1994197834', 'https://openalex.org/W2072128103', 'https://openalex.org/W85575376', 'https://openalex.org/W2102324552', 'https://openalex.org/W2107878631', 'https://openalex.org/W1498436455', 'https://openalex.org/W2025768430', 'https://openalex.org/W1529808766', 'https://openalex.org/W2112796928', 'https://openalex.org/W2136922672', 'https://openalex.org/W2117130368']",2010-03-31
https://openalex.org/W4221161761,https://doi.org/10.48550/arxiv.2202.01855,Self-supervised Learning with Random-projection Quantizer for Speech Recognition,"We present a simple and effective self-supervised learning approach for speech recognition. The approach learns a model to predict the masked speech signals, in the form of discrete labels generated with a random-projection quantizer. In particular the quantizer projects speech inputs with a randomly initialized matrix, and does a nearest-neighbor lookup in a randomly-initialized codebook. Neither the matrix nor the codebook is updated during self-supervised learning. Since the random-projection quantizer is not trained and is separated from the speech recognition model, the design makes the approach flexible and is compatible with universal speech recognition architecture. On LibriSpeech our approach achieves similar word-error-rates as previous work using self-supervised learning with non-streaming models, and provides lower word-error-rates and latency than wav2vec 2.0 and w2v-BERT with streaming models. On multilingual tasks the approach also provides significant improvement over wav2vec 2.0 and w2v-BERT.",[],2022-02-03
https://openalex.org/W2928941594,https://doi.org/10.48550/arxiv.1902.08295,Lingvo: a Modular and Scalable Framework for Sequence-to-Sequence Modeling,"Lingvo is a Tensorflow framework offering a complete solution for collaborative deep learning research, with a particular focus towards sequence-to-sequence models. Lingvo models are composed of modular building blocks that are flexible and easily extensible, and experiment configurations are centralized and highly customizable. Distributed training and quantized inference are supported directly within the framework, and it contains existing implementations of a large number of utilities, helper functions, and the newest research ideas. Lingvo has been used in collaboration by dozens of researchers in more than 20 papers over the last two years. This document outlines the underlying design of Lingvo and serves as an introduction to the various pieces of the framework, while also offering examples of advanced features that showcase the capabilities of the framework.","['https://openalex.org/W2899716505', 'https://openalex.org/W2897548994', 'https://openalex.org/W2121879602']",2019-02-21
https://openalex.org/W2160815625,https://doi.org/10.1109/msp.2012.2205597,Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups,"Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.","['https://openalex.org/W6676071220', 'https://openalex.org/W1981678206', 'https://openalex.org/W6675321185', 'https://openalex.org/W2118595744', 'https://openalex.org/W6681419218', 'https://openalex.org/W6633713915', 'https://openalex.org/W2114016253', 'https://openalex.org/W2501000458', 'https://openalex.org/W2024539680', 'https://openalex.org/W2155273149', 'https://openalex.org/W2022011789', 'https://openalex.org/W2105099419', 'https://openalex.org/W2089177488', 'https://openalex.org/W2071489795', 'https://openalex.org/W1647054946', 'https://openalex.org/W2122514667', 'https://openalex.org/W2110871230', 'https://openalex.org/W2140372979', 'https://openalex.org/W2053280194', 'https://openalex.org/W1970996882', 'https://openalex.org/W2003123121', 'https://openalex.org/W6844641610', 'https://openalex.org/W2012897754', 'https://openalex.org/W2084514013', 'https://openalex.org/W2116064496', 'https://openalex.org/W2136922672', 'https://openalex.org/W6601785968', 'https://openalex.org/W2106051978', 'https://openalex.org/W2165119652', 'https://openalex.org/W2131700150', 'https://openalex.org/W2172097686', 'https://openalex.org/W6608133726', 'https://openalex.org/W2102017903', 'https://openalex.org/W2096635587', 'https://openalex.org/W6675994261', 'https://openalex.org/W6688386640', 'https://openalex.org/W6681096077', 'https://openalex.org/W2149600041', 'https://openalex.org/W42107399', 'https://openalex.org/W2027915610', 'https://openalex.org/W6630035193', 'https://openalex.org/W1993882792', 'https://openalex.org/W1498436455', 'https://openalex.org/W6631943919', 'https://openalex.org/W2132424367', 'https://openalex.org/W2100495367', 'https://openalex.org/W1994197834', 'https://openalex.org/W2090861223', 'https://openalex.org/W2168171912', 'https://openalex.org/W2032210463', 'https://openalex.org/W2069976350', 'https://openalex.org/W2165712214', 'https://openalex.org/W6617368339', 'https://openalex.org/W1877570817', 'https://openalex.org/W6678457041', 'https://openalex.org/W2160306971', 'https://openalex.org/W6697318756', 'https://openalex.org/W2147768505', 'https://openalex.org/W1964420823', 'https://openalex.org/W6608710415', 'https://openalex.org/W6711962127', 'https://openalex.org/W2107789863', 'https://openalex.org/W2997574889', 'https://openalex.org/W1553004968', 'https://openalex.org/W44815768', 'https://openalex.org/W2141778357', 'https://openalex.org/W2907162034', 'https://openalex.org/W2105153012', 'https://openalex.org/W3146320432', 'https://openalex.org/W587794757', 'https://openalex.org/W2151660570', 'https://openalex.org/W4299681030', 'https://openalex.org/W1504869774', 'https://openalex.org/W2125234026', 'https://openalex.org/W196761320', 'https://openalex.org/W1533861849', 'https://openalex.org/W2145094598', 'https://openalex.org/W299739017', 'https://openalex.org/W217970951', 'https://openalex.org/W2139622435', 'https://openalex.org/W2296748324', 'https://openalex.org/W2394932179', 'https://openalex.org/W2014641584', 'https://openalex.org/W2103359087', 'https://openalex.org/W1562289873', 'https://openalex.org/W2218318129']",2012-10-19
https://openalex.org/W2515753980,https://doi.org/10.1109/taslp.2016.2602884,Very Deep Convolutional Neural Networks for Noise Robust Speech Recognition,"Although great progress has been made in automatic speech recognition, significant performance degradation still exists in noisy environments. Recently, very deep convolutional neural networks (CNNs) have been successfully applied to computer vision and speech recognition tasks. Based on our previous work on very deep CNNs, in this paper this architecture is further developed to improve recognition accuracy for noise robust speech recognition. In the proposed very deep CNN architecture, we study the best configuration for the sizes of filters, pooling, and input feature maps: the sizes of filters and poolings are reduced and dimensions of input features are extended to allow for adding more convolutional layers. Then the appropriate pooling, padding, and input feature map selection strategies are investigated and applied to the very deep CNN to make it more robust for speech recognition. In addition, an in-depth analysis of the architecture reveals key characteristics, such as compact model scale, fast convergence speed, and noise robustness. The proposed new model is evaluated on two tasks: Aurora4 task with multiple additive noise types and channel mismatch, and the AMI meeting transcription task with significant reverberation. Experiments on both tasks show that the proposed very deep CNNs can significantly reduce word error rate (WER) for noise robust speech recognition. The best architecture obtains a 10.0% relative reduction over the traditional CNN on AMI, competitive with the long short-term memory recurrent neural networks (LSTM-RNN) acoustic model. On Aurora4, even without feature enhancement, model adaptation, and sequence training, it achieves a WER of 8.81%, a 17.0% relative improvement over the LSTM-RNN. To our knowledge, this is the best published result on Aurora4.","['https://openalex.org/W6696982659', 'https://openalex.org/W6696934422', 'https://openalex.org/W2024490156', 'https://openalex.org/W4256161595', 'https://openalex.org/W2160306971', 'https://openalex.org/W6639736908', 'https://openalex.org/W2181607856', 'https://openalex.org/W6631362777', 'https://openalex.org/W2408583661', 'https://openalex.org/W1600744878', 'https://openalex.org/W6714171909', 'https://openalex.org/W2020408662', 'https://openalex.org/W6632100814', 'https://openalex.org/W2112796928', 'https://openalex.org/W2155273149', 'https://openalex.org/W2033310064', 'https://openalex.org/W6684191040', 'https://openalex.org/W1995562189', 'https://openalex.org/W2103088716', 'https://openalex.org/W6674914833', 'https://openalex.org/W6637373629', 'https://openalex.org/W2402922286', 'https://openalex.org/W1976637928', 'https://openalex.org/W2062164080', 'https://openalex.org/W2147768505', 'https://openalex.org/W1992475611', 'https://openalex.org/W2239847623', 'https://openalex.org/W2119203697', 'https://openalex.org/W1973669708', 'https://openalex.org/W2397042577', 'https://openalex.org/W6711962127', 'https://openalex.org/W2112739286', 'https://openalex.org/W2160815625', 'https://openalex.org/W1677182931', 'https://openalex.org/W6712816420', 'https://openalex.org/W6687483927', 'https://openalex.org/W2148613904', 'https://openalex.org/W2402201734', 'https://openalex.org/W2198724430', 'https://openalex.org/W2039716246', 'https://openalex.org/W6677416784', 'https://openalex.org/W2963574257', 'https://openalex.org/W2194775991', 'https://openalex.org/W2407023693', 'https://openalex.org/W2097117768', 'https://openalex.org/W330298975', 'https://openalex.org/W2394932179', 'https://openalex.org/W2293634267', 'https://openalex.org/W1538131130', 'https://openalex.org/W1524333225', 'https://openalex.org/W3125118953', 'https://openalex.org/W2618530766', 'https://openalex.org/W1686810756', 'https://openalex.org/W2293009711', 'https://openalex.org/W2115086266', 'https://openalex.org/W2962835968', 'https://openalex.org/W2397947827']",2016-08-25
https://openalex.org/W3163793923,https://doi.org/10.1109/icassp39728.2021.9414858,Recent Developments on Espnet Toolkit Boosted By Conformer,"In this study, we present recent developments on ESPnet: End-to- End Speech Processing toolkit, which mainly involves a recently proposed architecture called Conformer, Convolution-augmented Transformer. This paper shows the results for a wide range of end- to-end speech processing applications, such as automatic speech recognition (ASR), speech translations (ST), speech separation (SS) and text-to-speech (TTS). Our experiments reveal various training tips and significant performance benefits obtained with the Conformer on different tasks. These results are competitive or even outperform the current state-of-art Transformer models. We are preparing to release all-in-one recipes using open source and publicly available corpora for all the above tasks with pre-trained models. Our aim for this work is to contribute to our research community by reducing the burden of preparing state-of-the-art research environments usually requiring high resources.","['https://openalex.org/W3015338123', 'https://openalex.org/W2903739847', 'https://openalex.org/W3016160783', 'https://openalex.org/W2963542740', 'https://openalex.org/W6679434410', 'https://openalex.org/W6783267081', 'https://openalex.org/W6731370813', 'https://openalex.org/W6763608318', 'https://openalex.org/W2963250244', 'https://openalex.org/W2127141656', 'https://openalex.org/W2526425061', 'https://openalex.org/W2886180730', 'https://openalex.org/W2963587345', 'https://openalex.org/W6754299077', 'https://openalex.org/W2972389417', 'https://openalex.org/W2964110616', 'https://openalex.org/W2972818416', 'https://openalex.org/W2127851351', 'https://openalex.org/W3008191852', 'https://openalex.org/W2962780374', 'https://openalex.org/W3097777922', 'https://openalex.org/W6755207826', 'https://openalex.org/W3037217258', 'https://openalex.org/W6739901393', 'https://openalex.org/W2734774145', 'https://openalex.org/W6778823374', 'https://openalex.org/W6763832098', 'https://openalex.org/W6631362777', 'https://openalex.org/W3034949308', 'https://openalex.org/W2936774411', 'https://openalex.org/W6713762819', 'https://openalex.org/W2892009249', 'https://openalex.org/W2963341956', 'https://openalex.org/W2133564696', 'https://openalex.org/W3085139254', 'https://openalex.org/W3115011379', 'https://openalex.org/W2896457183', 'https://openalex.org/W2407080277', 'https://openalex.org/W3130016944', 'https://openalex.org/W2963970792', 'https://openalex.org/W3033411150', 'https://openalex.org/W2970730223', 'https://openalex.org/W2964308564', 'https://openalex.org/W2567070169', 'https://openalex.org/W1524333225', 'https://openalex.org/W2948981900', 'https://openalex.org/W4385245566', 'https://openalex.org/W3007328579', 'https://openalex.org/W4287667694', 'https://openalex.org/W2963403868', 'https://openalex.org/W3150572638', 'https://openalex.org/W2946200149']",2021-05-13
https://openalex.org/W2398826216,https://doi.org/10.21437/interspeech.2015-1,Learning the speech front-end with raw waveform CLDNNs,"Learning an acoustic model directly from the raw waveform has been an active area of research.However, waveformbased models have not yet matched the performance of logmel trained neural networks.We will show that raw waveform features match the performance of log-mel filterbank energies when used with a state-of-the-art CLDNN acoustic model trained on over 2,000 hours of speech.Specifically, we will show the benefit of the CLDNN, namely the time convolution layer in reducing temporal variations, the frequency convolution layer for preserving locality and reducing frequency variations, as well as the LSTM layers for temporal modeling.In addition, by stacking raw waveform features with log-mel features, we achieve a 3% relative reduction in word error rate.","['https://openalex.org/W2112739286', 'https://openalex.org/W2168231600', 'https://openalex.org/W1542280630', 'https://openalex.org/W2172097686', 'https://openalex.org/W2132037657', 'https://openalex.org/W2408093180', 'https://openalex.org/W2148154194', 'https://openalex.org/W2009150118', 'https://openalex.org/W1600744878', 'https://openalex.org/W2962719052', 'https://openalex.org/W2293634267', 'https://openalex.org/W2035846950', 'https://openalex.org/W2165777135', 'https://openalex.org/W2963175699', 'https://openalex.org/W2064675550', 'https://openalex.org/W2050758723', 'https://openalex.org/W1533861849', 'https://openalex.org/W4245923654', 'https://openalex.org/W2155117693']",2015-09-06
https://openalex.org/W3015356564,https://doi.org/10.1109/icassp40776.2020.9054224,Effectiveness of Self-Supervised Pre-Training for ASR,"We compare self-supervised representation learning algorithms which either explicitly quantize the audio data or learn representations without quantization. We find the former to be more accurate since it builds a good vocabulary of the data through vq-wav2vec [1] self-supervision approach to enable learning of effective representations in subsequent BERT training. Different to previous work, we directly fine-tune the pre-trained BERT models on transcribed speech using a Connectionist Temporal Classification (CTC) loss instead of feeding the representations into a task-specific model. We also propose a BERT-style model learning directly from the continuous audio data and compare pre-training on raw audio to spectral features. Fine-tuning a BERT model on 10 hour of labeled Librispeech data with a vq-wav2vec vocabulary is almost as good as the best known reported system trained on 100 hours of labeled data on test-clean, while achieving a 25% WER reduction on test-other. When using only 10 minutes of labeled data, WER is 25.2 on test-other and 16.3 on test-clean. This demonstrates that self-supervision can enable speech recognition systems trained on a near-zero amount of transcribed data.","['https://openalex.org/W6772641181', 'https://openalex.org/W6775452034', 'https://openalex.org/W2963807318', 'https://openalex.org/W6631362777', 'https://openalex.org/W2933138175', 'https://openalex.org/W2936774411', 'https://openalex.org/W2963382687', 'https://openalex.org/W6769557084', 'https://openalex.org/W2953190524', 'https://openalex.org/W2964110616', 'https://openalex.org/W2963425185', 'https://openalex.org/W2842511635', 'https://openalex.org/W2972943112', 'https://openalex.org/W2932675979', 'https://openalex.org/W6761176036', 'https://openalex.org/W2124558353', 'https://openalex.org/W2667408400', 'https://openalex.org/W6760911985', 'https://openalex.org/W2940322076', 'https://openalex.org/W2586148577', 'https://openalex.org/W2752796333', 'https://openalex.org/W2055408826', 'https://openalex.org/W6761563299', 'https://openalex.org/W30845872', 'https://openalex.org/W2059652594', 'https://openalex.org/W6763416564', 'https://openalex.org/W2025482506', 'https://openalex.org/W2963571336', 'https://openalex.org/W6697456849', 'https://openalex.org/W2114347655', 'https://openalex.org/W2964115348', 'https://openalex.org/W6769196770', 'https://openalex.org/W2964001192', 'https://openalex.org/W2970119519', 'https://openalex.org/W6755207826', 'https://openalex.org/W6771812881', 'https://openalex.org/W1494198834', 'https://openalex.org/W6766673545', 'https://openalex.org/W2944255943', 'https://openalex.org/W2971155163', 'https://openalex.org/W2981857663', 'https://openalex.org/W2996383576', 'https://openalex.org/W2979476256', 'https://openalex.org/W2963799213', 'https://openalex.org/W2948012107', 'https://openalex.org/W2973049979', 'https://openalex.org/W2296681920', 'https://openalex.org/W2963341956', 'https://openalex.org/W3103005696', 'https://openalex.org/W2998649947', 'https://openalex.org/W2995181338', 'https://openalex.org/W2965373594', 'https://openalex.org/W2896457183', 'https://openalex.org/W3015522062', 'https://openalex.org/W2941814890', 'https://openalex.org/W2962907457', 'https://openalex.org/W1524333225', 'https://openalex.org/W4297808394']",2020-04-09
https://openalex.org/W4385570101,https://doi.org/10.18653/v1/2023.findings-acl.447,DUB: Discrete Unit Back-translation for Speech Translation,"How can speech-to-text translation (ST) perform as well as machine translation (MT)? The key point is to bridge the modality gap between speech and text so that useful MT techniques can be applied to ST.Recently, the approach of representing speech with unsupervised discrete units yields a new way to ease the modality problem. This motivates us to propose Discrete Unit Back-translation(DUB) to answer two questions (1) Is it better to represent speech with discrete units than with continuous features in direct ST? (2) How much benefit can useful MT techniques bring to ST? With DUB, the back-translation technique can successfully be applied on direct ST and obtains an average boost of 5.5 BLEU on MuST-C En-De/Fr/Es. In the low-resource language scenario, our method achieves comparable performance to existing methods that rely on large-scale external data. Code and models are available at https://anonymous.4open.science/r/DUB/.",[],2023-01-01
https://openalex.org/W2166637769,https://doi.org/10.1109/icassp.1992.225858,SWITCHBOARD: telephone speech corpus for research and development,"SWITCHBOARD is a large multispeaker corpus of conversational speech and text which should be of interest to researchers in speaker authentication and large vocabulary speech recognition. About 2500 conversations by 500 speakers from around the US were collected automatically over T1 lines at Texas Instruments. Designed for training and testing of a variety of speech processing algorithms, especially in speaker verification, it has over an 1 h of speech from each of 50 speakers, and several minutes each from hundreds of others. A time-aligned word for word transcription accompanies each recording.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>","['https://openalex.org/W2077302143', 'https://openalex.org/W1643320849', 'https://openalex.org/W1976349544']",1992-01-01
https://openalex.org/W3198587774,https://doi.org/10.21437/interspeech.2021-1860,"SPGISpeech: 5,000 Hours of Transcribed Financial Audio for Fully Formatted End-to-End Speech Recognition","In the English speech-to-text (STT) machine learning task, acoustic models are conventionally trained on uncased Latin characters, and any necessary orthography (such as capitalization, punctuation, and denormalization of non-standard words) is imputed by separate post-processing models.This adds complexity and limits performance, as many formatting tasks benefit from semantic information present in the acoustic signal but absent in transcription.Here we propose a new STT task: endto-end neural transcription with fully formatted text for target labels.We present baseline Conformer-based models trained on a corpus of 5,000 hours of professionally transcribed earnings calls, achieving a CER of 1.7.As a contribution to the STT research community, we release the corpus free for noncommercial use. 1","['https://openalex.org/W4295276571', 'https://openalex.org/W3153592532', 'https://openalex.org/W2741483887', 'https://openalex.org/W3011222885', 'https://openalex.org/W3023256384', 'https://openalex.org/W3101648800', 'https://openalex.org/W2406343628', 'https://openalex.org/W1995562189', 'https://openalex.org/W97072897', 'https://openalex.org/W3097777922', 'https://openalex.org/W2166637769', 'https://openalex.org/W2297949011', 'https://openalex.org/W2880875857', 'https://openalex.org/W2404126548', 'https://openalex.org/W2963250244', 'https://openalex.org/W1828163288', 'https://openalex.org/W3160789530', 'https://openalex.org/W2974231335', 'https://openalex.org/W2749051922', 'https://openalex.org/W4287120025', 'https://openalex.org/W2101926813', 'https://openalex.org/W2189162242', 'https://openalex.org/W1494198834', 'https://openalex.org/W4385245566', 'https://openalex.org/W3015752032', 'https://openalex.org/W3030437843']",2021-08-27
https://openalex.org/W4391021675,https://doi.org/10.1109/asru57964.2023.10389676,Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data,"Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pretrained models and training logs to promote open science. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> https://github.com/espnet/espnet","['https://openalex.org/W6739901393', 'https://openalex.org/W6778883912', 'https://openalex.org/W6810081322', 'https://openalex.org/W6811340617', 'https://openalex.org/W6850625674', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W3213029956', 'https://openalex.org/W3197580070', 'https://openalex.org/W4281492411', 'https://openalex.org/W3207558756', 'https://openalex.org/W4319862410', 'https://openalex.org/W6847363464', 'https://openalex.org/W6791904447', 'https://openalex.org/W4210463634', 'https://openalex.org/W6850218400', 'https://openalex.org/W6796715840', 'https://openalex.org/W6850068309', 'https://openalex.org/W6850936240', 'https://openalex.org/W2962780374', 'https://openalex.org/W2963242190', 'https://openalex.org/W3196509775', 'https://openalex.org/W3198694222', 'https://openalex.org/W1494198834', 'https://openalex.org/W3092085609', 'https://openalex.org/W3198587774', 'https://openalex.org/W2799473636', 'https://openalex.org/W4385823111', 'https://openalex.org/W3095410713', 'https://openalex.org/W3203407300', 'https://openalex.org/W2083751884', 'https://openalex.org/W6771467084', 'https://openalex.org/W2166637769', 'https://openalex.org/W6775053297', 'https://openalex.org/W4319862635', 'https://openalex.org/W3091427154', 'https://openalex.org/W4297841547', 'https://openalex.org/W3119308075', 'https://openalex.org/W2024490156', 'https://openalex.org/W2526425061', 'https://openalex.org/W2936774411', 'https://openalex.org/W2739883972', 'https://openalex.org/W6766978945', 'https://openalex.org/W3097777922', 'https://openalex.org/W6839026989', 'https://openalex.org/W4319862255', 'https://openalex.org/W6838276489', 'https://openalex.org/W2963250244', 'https://openalex.org/W4385823140', 'https://openalex.org/W4372260519', 'https://openalex.org/W3203140070', 'https://openalex.org/W3166440012', 'https://openalex.org/W4375869065', 'https://openalex.org/W4385823182', 'https://openalex.org/W3204647170', 'https://openalex.org/W4372262650', 'https://openalex.org/W2788388592', 'https://openalex.org/W4385822685', 'https://openalex.org/W6842738431', 'https://openalex.org/W4321854923', 'https://openalex.org/W4295312788', 'https://openalex.org/W4322718191', 'https://openalex.org/W3012492057', 'https://openalex.org/W3030437843', 'https://openalex.org/W4360836968', 'https://openalex.org/W4224308101', 'https://openalex.org/W4311000453', 'https://openalex.org/W4292779060', 'https://openalex.org/W4229005866', 'https://openalex.org/W3139918052', 'https://openalex.org/W3101648800', 'https://openalex.org/W4323066695', 'https://openalex.org/W4295108597']",2023-12-16
https://openalex.org/W3100460087,https://doi.org/10.18653/v1/2020.emnlp-main.588,SLURP: A Spoken Language Understanding Resource Package,"Spoken Language Understanding infers semantic meaning directly from audio data, and thus promises to reduce error propagation and misunderstandings in end-user applications. However, publicly available SLU resources are limited. In this paper, we release SLURP, a new SLU package containing the following: (1) A new challenging dataset in English spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets; (2) Competitive baselines based on state-of-the-art NLU and ASR systems; (3) A new transparent metric for entity labelling which enables a detailed error analysis for identifying potential areas of improvement. SLURP is available at https://github.com/pswietojanski/slurp.","['https://openalex.org/W2047237057', 'https://openalex.org/W97072897', 'https://openalex.org/W2140461783', 'https://openalex.org/W2239847623', 'https://openalex.org/W2124386177', 'https://openalex.org/W2696967604', 'https://openalex.org/W2898856000', 'https://openalex.org/W1524333225', 'https://openalex.org/W1817086658', 'https://openalex.org/W2972584841', 'https://openalex.org/W3004786215', 'https://openalex.org/W2402146185', 'https://openalex.org/W1591607137', 'https://openalex.org/W2077302143', 'https://openalex.org/W2972525948', 'https://openalex.org/W1494198834', 'https://openalex.org/W4297683418', 'https://openalex.org/W2963912046', 'https://openalex.org/W2964006684', 'https://openalex.org/W2963288440', 'https://openalex.org/W2894164357', 'https://openalex.org/W2399877729', 'https://openalex.org/W2054870958', 'https://openalex.org/W3022055203', 'https://openalex.org/W3005352739', 'https://openalex.org/W2921312604', 'https://openalex.org/W3030437843', 'https://openalex.org/W2979486337', 'https://openalex.org/W2083751884', 'https://openalex.org/W2966087730', 'https://openalex.org/W1976538119', 'https://openalex.org/W2514741789', 'https://openalex.org/W4285719527', 'https://openalex.org/W2166637769', 'https://openalex.org/W3015885816', 'https://openalex.org/W2981269614', 'https://openalex.org/W2963066655', 'https://openalex.org/W3049397270']",2020-01-01
https://openalex.org/W2899274165,https://doi.org/10.48550/arxiv.1811.00347,How2: A Large-scale Dataset for Multimodal Language Understanding,"In this paper, we introduce How2, a multimodal collection of instructional videos with English subtitles and crowdsourced Portuguese translations. We also present integrated sequence-to-sequence baselines for machine translation, automatic speech recognition, spoken language translation, and multimodal summarization. By making available data and code for several multimodal natural language tasks, we hope to stimulate more research on these and similar challenges, to obtain a deeper understanding of multimodality in language processing.","['https://openalex.org/W2087095333', 'https://openalex.org/W1880262756', 'https://openalex.org/W1522301498', 'https://openalex.org/W2903343986', 'https://openalex.org/W2963347649', 'https://openalex.org/W2568262903', 'https://openalex.org/W4300558631', 'https://openalex.org/W2885185669', 'https://openalex.org/W2964308564', 'https://openalex.org/W2962934715', 'https://openalex.org/W2327501763', 'https://openalex.org/W2095705004', 'https://openalex.org/W2512544816', 'https://openalex.org/W2963778889', 'https://openalex.org/W2015143272', 'https://openalex.org/W2185175083', 'https://openalex.org/W114341944', 'https://openalex.org/W628858056', 'https://openalex.org/W1524333225', 'https://openalex.org/W2949615363', 'https://openalex.org/W2714726990', 'https://openalex.org/W68733909', 'https://openalex.org/W2785350307', 'https://openalex.org/W2108325777', 'https://openalex.org/W2101105183', 'https://openalex.org/W2425121537', 'https://openalex.org/W2056609756', 'https://openalex.org/W2345720230', 'https://openalex.org/W2509282593', 'https://openalex.org/W2594690981', 'https://openalex.org/W2159527087', 'https://openalex.org/W2914699769', 'https://openalex.org/W2166637769', 'https://openalex.org/W2950613790', 'https://openalex.org/W2798400235']",2018-11-01
https://openalex.org/W4381827575,https://doi.org/10.48550/arxiv.2306.12925,AudioPaLM: A Large Language Model That Can Speak and Listen,"We introduce AudioPaLM, a large language model for speech understanding and generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2 [Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech-to-speech translation. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from AudioLM and the linguistic knowledge present only in text large language models such as PaLM-2. We demonstrate that initializing AudioPaLM with the weights of a text-only large language model improves speech processing, successfully leveraging the larger quantity of text training data used in pretraining to assist with the speech tasks. The resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform zero-shot speech-to-text translation for many languages for which input/target language combinations were not seen in training. AudioPaLM also demonstrates features of audio language models, such as transferring a voice across languages based on a short spoken prompt. We release examples of our method at https://google-research.github.io/seanet/audiopalm/examples",[],2023-06-22
https://openalex.org/W4200635400,https://doi.org/10.48550/arxiv.2112.09382,Discretization and Re-synthesis: an alternative method to solve the Cocktail Party Problem,"Deep learning based models have significantly improved the performance of speech separation with input mixtures like the cocktail party. Prominent methods (e.g., frequency-domain and time-domain speech separation) usually build regression models to predict the ground-truth speech from the mixture, using the masking-based design and the signal-level loss criterion (e.g., MSE or SI-SNR). This study demonstrates, for the first time, that the synthesis-based approach can also perform well on this problem, with great flexibility and strong potential. Specifically, we propose a novel speech separation/enhancement model based on the recognition of discrete symbols, and convert the paradigm of the speech separation/enhancement related tasks from regression to classification. By utilizing the synthesis model with the input of discrete symbols, after the prediction of discrete symbol sequence, each target speech could be re-synthesized. Evaluation results based on the WSJ0-2mix and VCTK-noisy corpora in various settings show that our proposed method can steadily synthesize the separated speech with high speech quality and without any interference, which is difficult to avoid in regression-based methods. In addition, with negligible loss of listening quality, the speaker conversion of enhanced/separated speech could be easily realized through our method.",[],2021-12-17
https://openalex.org/W1499360075,https://doi.org/10.1017/cbo9780511620645.014,The International Phonetic Association,"The following text is taken from the pamphlet entitled The Principles of the International Phonetic Association, published by the International Phonetics Association (1949 edition).",[],1987-12-25
https://openalex.org/W2939710050,https://doi.org/10.21437/interspeech.2019-1873,wav2vec: Unsupervised Pre-Training for Speech Recognition,"We explore unsupervised pre-training for speech recognition by learning representations of raw audio. wav2vec is trained on large amounts of unlabeled audio data and the resulting representations are then used to improve acoustic model training. We pre-train a simple multi-layer convolutional neural network optimized via a noise contrastive binary classification task. Our experiments on WSJ reduce WER of a strong character-based log-mel filterbank baseline by up to 36% when only a few hours of transcribed data is available. Our approach achieves 2.43% WER on the nov92 test set. This outperforms Deep Speech 2, the best reported character-based system in the literature while using two orders of magnitude less labeled training data.","['https://openalex.org/W2963303951', 'https://openalex.org/W2122028591', 'https://openalex.org/W2141440284', 'https://openalex.org/W2952230511', 'https://openalex.org/W2933138175', 'https://openalex.org/W2153579005', 'https://openalex.org/W2964121744', 'https://openalex.org/W2944828972', 'https://openalex.org/W2468716020', 'https://openalex.org/W2914120296', 'https://openalex.org/W2108598243', 'https://openalex.org/W3127686677', 'https://openalex.org/W2345968833', 'https://openalex.org/W2940180244', 'https://openalex.org/W2520160253', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962901777', 'https://openalex.org/W2786459654', 'https://openalex.org/W2889282842', 'https://openalex.org/W2518108298', 'https://openalex.org/W2972894903', 'https://openalex.org/W2193413348', 'https://openalex.org/W343636949', 'https://openalex.org/W2962896489', 'https://openalex.org/W2953190524', 'https://openalex.org/W2794209590', 'https://openalex.org/W2899377381', 'https://openalex.org/W1861492603', 'https://openalex.org/W2970119519', 'https://openalex.org/W2963970792', 'https://openalex.org/W2922709902', 'https://openalex.org/W2842511635']",2019-09-13
https://openalex.org/W2788277448,https://doi.org/10.1609/aaai.v32i1.12340,Diverse Beam Search for Improved Description of Complex Scenes,"A single image captures the appearance and position of multiple entities in a scene as well as their complex interactions. As a consequence, natural language grounded in visual contexts tends to be diverse---with utterances differing as focus shifts to specific objects, interactions, or levels of detail. Recently, neural sequence models such as RNNs and LSTMs have been employed to produce visually-grounded language. Beam Search, the standard work-horse for decoding sequences from these models, is an approximate inference algorithm that decodes the top-B sequences in a greedy left-to-right fashion. In practice, the resulting sequences are often minor rewordings of a common utterance, failing to capture the multimodal nature of source images. To address this shortcoming, we propose Diverse Beam Search (DBS), a diversity promoting alternative to BS for approximate inference. DBS produces sequences that are significantly different from each other by incorporating diversity constraints within groups of candidate sequences during decoding; moreover, it achieves this with minimal computational or memory overhead. We demonstrate that our method improves both diversity and quality of decoded sequences over existing techniques on two visually-grounded language generation tasks---image captioning and visual question generation---particularly on complex scenes containing diverse visual content. We also show similar improvements at language-only machine translation tasks, highlighting the generality of our approach.","['https://openalex.org/W2506483933', 'https://openalex.org/W2133564696', 'https://openalex.org/W6639086153', 'https://openalex.org/W6745764446', 'https://openalex.org/W2252065493', 'https://openalex.org/W6687483927', 'https://openalex.org/W2528287124', 'https://openalex.org/W1905882502', 'https://openalex.org/W6688760732', 'https://openalex.org/W2464988381', 'https://openalex.org/W1958706068', 'https://openalex.org/W6639102338', 'https://openalex.org/W2153579005', 'https://openalex.org/W2306876680', 'https://openalex.org/W6898505805', 'https://openalex.org/W2146861498', 'https://openalex.org/W1956340063', 'https://openalex.org/W6680375555', 'https://openalex.org/W6639657675', 'https://openalex.org/W1846689784', 'https://openalex.org/W2768661419', 'https://openalex.org/W2951289747', 'https://openalex.org/W2414484917', 'https://openalex.org/W1861492603', 'https://openalex.org/W2194775991', 'https://openalex.org/W2143612262', 'https://openalex.org/W2963351776', 'https://openalex.org/W2139501017', 'https://openalex.org/W2963243547', 'https://openalex.org/W2216735140', 'https://openalex.org/W1895577753', 'https://openalex.org/W2962838921', 'https://openalex.org/W2101105183', 'https://openalex.org/W4294170691', 'https://openalex.org/W1591706642', 'https://openalex.org/W2222235228', 'https://openalex.org/W2964308564', 'https://openalex.org/W2963206148']",2018-04-27
https://openalex.org/W2108598243,https://doi.org/10.1109/cvpr.2009.5206848,ImageNet: A large-scale hierarchical image database,"The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ""ImageNet"", a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.","['https://openalex.org/W2172191903', 'https://openalex.org/W2009606665', 'https://openalex.org/W6634343353', 'https://openalex.org/W2134135198', 'https://openalex.org/W2151103935', 'https://openalex.org/W2106097867', 'https://openalex.org/W1851597118', 'https://openalex.org/W2128017662', 'https://openalex.org/W1997011019', 'https://openalex.org/W2158535772', 'https://openalex.org/W1521539493', 'https://openalex.org/W6605172141', 'https://openalex.org/W6637774275', 'https://openalex.org/W2166742463', 'https://openalex.org/W2115733720', 'https://openalex.org/W6686583229', 'https://openalex.org/W6625122095', 'https://openalex.org/W6631412525', 'https://openalex.org/W2110764733', 'https://openalex.org/W2145607950', 'https://openalex.org/W2149489787', 'https://openalex.org/W6681704240', 'https://openalex.org/W2141282920', 'https://openalex.org/W3035258717', 'https://openalex.org/W1782590233', 'https://openalex.org/W2038721957', 'https://openalex.org/W1739260168', 'https://openalex.org/W1576445103', 'https://openalex.org/W2186094539', 'https://openalex.org/W1528789833', 'https://openalex.org/W4392159830', 'https://openalex.org/W2146719366', 'https://openalex.org/W128019999']",2009-06-01
https://openalex.org/W2736900972,,Voice Synthesis for in-the-Wild Speakers via a Phonological Loop.,"We present a new neural text to speech method that is able to transform text to speech in voices that are sampled in the wild. Unlike other text to speech systems, our solution is able to deal with unconstrained samples obtained from public speeches. The network architecture is simpler than those in the existing literature and is based on a novel shifting buffer working memory. The same buffer is used for estimating the attention, computing the output audio, and for updating the buffer itself. The input sentence is encoded using a context-free lookup table that contains one entry per character or phoneme. Lastly, the speakers are similarly represented by a short vector that can also be fitted to new speakers and variability in the generated speech is achieved by priming the buffer prior to generating the audio. Experimental results on two datasets demonstrate convincing multi-speaker and in-the-wild capabilities. In order to promote reproducibility, we release our source code and models: PyTorch code and sample audio files are available at ytaigman.github.io/loop.",[],2017-07-20
https://openalex.org/W2995480165,,On Mutual Information Maximization for Representation Learning,"Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.","['https://openalex.org/W2166944917', 'https://openalex.org/W2971155163', 'https://openalex.org/W1523385540', 'https://openalex.org/W2963800509', 'https://openalex.org/W2962947707', 'https://openalex.org/W1511891232', 'https://openalex.org/W2946006146', 'https://openalex.org/W2097482982', 'https://openalex.org/W2963683498', 'https://openalex.org/W2949178656', 'https://openalex.org/W2113307832', 'https://openalex.org/W2963145887', 'https://openalex.org/W2963721283', 'https://openalex.org/W115285041', 'https://openalex.org/W2970518055', 'https://openalex.org/W3099206234', 'https://openalex.org/W2963775347', 'https://openalex.org/W2883594813', 'https://openalex.org/W1959608418', 'https://openalex.org/W2555897561', 'https://openalex.org/W2122925692', 'https://openalex.org/W2409550820', 'https://openalex.org/W219040644', 'https://openalex.org/W2092939357', 'https://openalex.org/W2101234009', 'https://openalex.org/W2842511635', 'https://openalex.org/W2963341956', 'https://openalex.org/W2964121744', 'https://openalex.org/W2108384452', 'https://openalex.org/W2913939497', 'https://openalex.org/W2951585248', 'https://openalex.org/W3147183491', 'https://openalex.org/W2890487780', 'https://openalex.org/W2949517790', 'https://openalex.org/W2593814746', 'https://openalex.org/W2962739339', 'https://openalex.org/W2135482703', 'https://openalex.org/W2769112066', 'https://openalex.org/W2063971957', 'https://openalex.org/W2803832867', 'https://openalex.org/W2963858191', 'https://openalex.org/W2887997457', 'https://openalex.org/W2900148439', 'https://openalex.org/W2904849495', 'https://openalex.org/W2944828972']",2020-04-30
https://openalex.org/W2963360726,,Contrastive Learning for Image Captioning,"Image captioning, a popular topic in computer vision, has achieved substantial progress in recent years. However, the distinctiveness of natural descriptions is often overlooked in previous work. It is closely related to the quality of captions, as distinctive captions are more likely to describe images with their unique aspects. In this work, we propose a new learning method, Contrastive Learning (CL), for image captioning. Specifically, via two constraints formulated on top of a reference model, the proposed method can encourage distinctiveness, while maintaining the overall quality of the generated captions. We tested our method on two challenging datasets, where it improves the baseline model by significant margins. We also showed in our studies that the proposed method is generic and can be used for models with various structures.",[],2017-10-01
https://openalex.org/W2995680346,https://doi.org/10.48550/arxiv.1911.09602,Learning Hierarchical Discrete Linguistic Units from Visually-Grounded\n Speech,"In this paper, we present a method for learning discrete linguistic units by\nincorporating vector quantization layers into neural models of visually\ngrounded speech. We show that our method is capable of capturing both\nword-level and sub-word units, depending on how it is configured. What\ndifferentiates this paper from prior work on speech unit learning is the choice\nof training objective. Rather than using a reconstruction-based loss, we use a\ndiscriminative, multimodal grounding objective which forces the learned units\nto be useful for semantic image retrieval. We evaluate the sub-word units on\nthe ZeroSpeech 2019 challenge, achieving a 27.3\\% reduction in ABX error rate\nover the top-performing submission, while keeping the bitrate approximately the\nsame. We also present experiments demonstrating the noise robustness of these\nunits. Finally, we show that a model with multiple quantizers can\nsimultaneously learn phone-like detectors at a lower layer and word-like\ndetectors at a higher layer. We show that these detectors are highly accurate,\ndiscovering 279 words with an F1 score of greater than 0.5.\n","['https://openalex.org/W2586148577', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963330681', 'https://openalex.org/W2100768664', 'https://openalex.org/W2972345028', 'https://openalex.org/W3125709657', 'https://openalex.org/W2889313720', 'https://openalex.org/W2964249784', 'https://openalex.org/W2947445680', 'https://openalex.org/W2963317665', 'https://openalex.org/W2964001192', 'https://openalex.org/W2953114965', 'https://openalex.org/W2950414763', 'https://openalex.org/W30845872', 'https://openalex.org/W2295297373', 'https://openalex.org/W2972867623', 'https://openalex.org/W2468716020', 'https://openalex.org/W2964115348', 'https://openalex.org/W2106053110', 'https://openalex.org/W2107917162', 'https://openalex.org/W2962862718', 'https://openalex.org/W2963618559', 'https://openalex.org/W2078769636', 'https://openalex.org/W2920166246', 'https://openalex.org/W2118841860', 'https://openalex.org/W2963525826', 'https://openalex.org/W2938991416', 'https://openalex.org/W2113896236', 'https://openalex.org/W2989358187', 'https://openalex.org/W2962850167', 'https://openalex.org/W2395899413', 'https://openalex.org/W2962753610', 'https://openalex.org/W2973157397', 'https://openalex.org/W2962824709', 'https://openalex.org/W2767754137', 'https://openalex.org/W2194775991', 'https://openalex.org/W2963417023', 'https://openalex.org/W2134670479', 'https://openalex.org/W2117041980', 'https://openalex.org/W2971709506', 'https://openalex.org/W2988907666', 'https://openalex.org/W2556930864', 'https://openalex.org/W1942713348', 'https://openalex.org/W1778492285', 'https://openalex.org/W2114347655', 'https://openalex.org/W2126203737', 'https://openalex.org/W2971074500', 'https://openalex.org/W2962732076', 'https://openalex.org/W2973026522', 'https://openalex.org/W2483390977', 'https://openalex.org/W2758849341', 'https://openalex.org/W385555557', 'https://openalex.org/W2972943112', 'https://openalex.org/W2964099072', 'https://openalex.org/W2347098582', 'https://openalex.org/W2404799143', 'https://openalex.org/W2962978519', 'https://openalex.org/W2842511635', 'https://openalex.org/W2963902314', 'https://openalex.org/W2132921748', 'https://openalex.org/W2057007397', 'https://openalex.org/W2950133079', 'https://openalex.org/W2964169922', 'https://openalex.org/W2963799213', 'https://openalex.org/W2972892814', 'https://openalex.org/W2973135958', 'https://openalex.org/W2242818861', 'https://openalex.org/W2927673779']",2019-11-21
https://openalex.org/W2575842049,https://doi.org/10.1109/cvpr.2017.345,Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning,"Attention-based neural encoder-decoder frameworks have been widely adopted for image captioning. Most methods force visual attention to be active for every generated word. However, the decoder likely requires little to no visual information from the image to predict non-visual words such as the and of. Other words that may seem visual can often be predicted reliably just from the language model e.g., sign after behind a red stop or phone following talking on a cell. In this paper, we propose a novel adaptive attention model with a visual sentinel. At each time step, our model decides whether to attend to the image (and if so, to which regions) or to the visual sentinel. The model decides whether to attend to the image and where, in order to extract meaningful information for sequential word generation. We test our method on the COCO image captioning 2015 challenge dataset and Flickr30K. Our approach sets the new state-of-the-art by a significant margin.","['https://openalex.org/W6714077321', 'https://openalex.org/W2963954913', 'https://openalex.org/W6630875275', 'https://openalex.org/W6686423066', 'https://openalex.org/W2302086703', 'https://openalex.org/W2194775991', 'https://openalex.org/W1905882502', 'https://openalex.org/W6685230081', 'https://openalex.org/W6642839729', 'https://openalex.org/W6682086108', 'https://openalex.org/W6682631176', 'https://openalex.org/W6639102338', 'https://openalex.org/W6719057275', 'https://openalex.org/W6638742206', 'https://openalex.org/W2157331557', 'https://openalex.org/W1895577753', 'https://openalex.org/W1895989618', 'https://openalex.org/W6697449767', 'https://openalex.org/W2133459682', 'https://openalex.org/W1931639407', 'https://openalex.org/W1947481528', 'https://openalex.org/W6639694449', 'https://openalex.org/W6725318829', 'https://openalex.org/W6600334730', 'https://openalex.org/W6898505805', 'https://openalex.org/W6679436768', 'https://openalex.org/W1956340063', 'https://openalex.org/W2463565445', 'https://openalex.org/W2295107390', 'https://openalex.org/W2185175083', 'https://openalex.org/W2407414618', 'https://openalex.org/W2525332836', 'https://openalex.org/W2149172860', 'https://openalex.org/W4298422451', 'https://openalex.org/W2133564696', 'https://openalex.org/W2101105183', 'https://openalex.org/W2963668159', 'https://openalex.org/W2950477994', 'https://openalex.org/W2552161745', 'https://openalex.org/W1897761818', 'https://openalex.org/W2964018924', 'https://openalex.org/W2149557440', 'https://openalex.org/W2171361956', 'https://openalex.org/W1861492603', 'https://openalex.org/W8316075', 'https://openalex.org/W2963733622', 'https://openalex.org/W2964308564', 'https://openalex.org/W1706899115', 'https://openalex.org/W2293453011', 'https://openalex.org/W2963579811', 'https://openalex.org/W2154652894', 'https://openalex.org/W2183341477', 'https://openalex.org/W2506483933', 'https://openalex.org/W1811254738', 'https://openalex.org/W2130942839', 'https://openalex.org/W2066134726', 'https://openalex.org/W1514535095', 'https://openalex.org/W2962706528']",2017-07-01
https://openalex.org/W2950178297,https://doi.org/10.48550/arxiv.1502.03044,"Show, Attend and Tell: Neural Image Caption Generation with Visual\n Attention","Inspired by recent work in machine translation and object detection, we\nintroduce an attention based model that automatically learns to describe the\ncontent of images. We describe how we can train this model in a deterministic\nmanner using standard backpropagation techniques and stochastically by\nmaximizing a variational lower bound. We also show through visualization how\nthe model is able to automatically learn to fix its gaze on salient objects\nwhile generating the corresponding words in the output sequence. We validate\nthe use of attention with state-of-the-art performance on three benchmark\ndatasets: Flickr8k, Flickr30k and MS COCO.\n","['https://openalex.org/W2950635152', 'https://openalex.org/W2950179405', 'https://openalex.org/W1687846465', 'https://openalex.org/W2949769367', 'https://openalex.org/W2164370980', 'https://openalex.org/W2951912364', 'https://openalex.org/W2185175083', 'https://openalex.org/W2115595010', 'https://openalex.org/W2019370496', 'https://openalex.org/W2122180654', 'https://openalex.org/W2951527505', 'https://openalex.org/W2133564696', 'https://openalex.org/W1527575280', 'https://openalex.org/W2952020226', 'https://openalex.org/W1591801644', 'https://openalex.org/W2141399712', 'https://openalex.org/W2296385829', 'https://openalex.org/W1861492603', 'https://openalex.org/W2130942839', 'https://openalex.org/W2095705004', 'https://openalex.org/W2143449221', 'https://openalex.org/W1686810756', 'https://openalex.org/W2119717200', 'https://openalex.org/W1522301498', 'https://openalex.org/W2950182411', 'https://openalex.org/W1969616664', 'https://openalex.org/W1606347560', 'https://openalex.org/W2962706528', 'https://openalex.org/W1905882502', 'https://openalex.org/W2951183276', 'https://openalex.org/W1484210532', 'https://openalex.org/W2953308237', 'https://openalex.org/W2149172860', 'https://openalex.org/W2154071538', 'https://openalex.org/W68733909', 'https://openalex.org/W2156718681', 'https://openalex.org/W2122710056', 'https://openalex.org/W2171361956']",2015-02-10
https://openalex.org/W3039910566,https://doi.org/10.1109/slt48900.2021.9383605,Data Augmenting Contrastive Learning of Speech Representations in the Time Domain,"Contrastive Predictive Coding (CPC), based on predicting future segments of speech based on past segments is emerging as a powerful algorithm for representation learning of speech signal. However, it still under-performs other methods on unsupervised evaluation benchmarks. Here, we introduce WavAugment, a time-domain data augmentation library and find that applying augmentation in the past is generally more efficient and yields better performances than other methods. We find that a combination of pitch modification, additive noise and reverberation substantially increase the performance of CPC (relative improvement of 18-22%), beating the reference Libri-light results with 600 times less data. Using an out-of-domain dataset, time-domain data augmentation can push CPC to be on par with the state of the art on the Zero Speech Benchmark 2017. We also show that time-domain data augmentation consistently improves downstream limited-supervision phoneme classification tasks by a factor of 12-15% relative.","['https://openalex.org/W6762619590', 'https://openalex.org/W1494198834', 'https://openalex.org/W6774314701', 'https://openalex.org/W6682250724', 'https://openalex.org/W2883725317', 'https://openalex.org/W6688816777', 'https://openalex.org/W3015783745', 'https://openalex.org/W6766978945', 'https://openalex.org/W3102342027', 'https://openalex.org/W3003875258', 'https://openalex.org/W2509930204', 'https://openalex.org/W6748342566', 'https://openalex.org/W3100270690', 'https://openalex.org/W2940544976', 'https://openalex.org/W2020607164', 'https://openalex.org/W3015213852', 'https://openalex.org/W6677620606', 'https://openalex.org/W6675022971', 'https://openalex.org/W2696967604', 'https://openalex.org/W2346964103', 'https://openalex.org/W6713762819', 'https://openalex.org/W6844194202', 'https://openalex.org/W2936774411', 'https://openalex.org/W6697293080', 'https://openalex.org/W6771812881', 'https://openalex.org/W6761176036', 'https://openalex.org/W3093096176', 'https://openalex.org/W3005511757', 'https://openalex.org/W2055408826', 'https://openalex.org/W2347098582', 'https://openalex.org/W6761553608', 'https://openalex.org/W2983785920', 'https://openalex.org/W6769455919', 'https://openalex.org/W6760822226', 'https://openalex.org/W6686369942', 'https://openalex.org/W1989674786', 'https://openalex.org/W2995181338', 'https://openalex.org/W3016011332', 'https://openalex.org/W2963620343', 'https://openalex.org/W2842511635', 'https://openalex.org/W2148349024', 'https://openalex.org/W2593779438', 'https://openalex.org/W2930682606', 'https://openalex.org/W3005680577', 'https://openalex.org/W2973049979', 'https://openalex.org/W2219249508', 'https://openalex.org/W2973026522', 'https://openalex.org/W3016181583', 'https://openalex.org/W2787447541', 'https://openalex.org/W2407080277', 'https://openalex.org/W2990583358', 'https://openalex.org/W2970971581', 'https://openalex.org/W2972943112']",2021-01-19
https://openalex.org/W2962691331,https://doi.org/10.21437/interspeech.2018-1113,Expressive Speech Synthesis via Modeling Expressions with Variational Autoencoder,"Recent advances in neural autoregressive models have improve the performance of speech synthesis (SS).However, as they lack the ability to model global characteristics of speech (such as speaker individualities or speaking styles), particularly when these characteristics have not been labeled, making neural autoregressive SS systems more expressive is still an open issue.In this paper, we propose to combine VoiceLoop, an autoregressive SS model, with Variational Autoencoder (VAE).This approach, unlike traditional autoregressive SS systems, uses VAE to model the global characteristics explicitly, enabling the expressiveness of the synthesized speech to be controlled in an unsupervised manner.Experiments using the VCTK and Bliz-zard2012 datasets show the VAE helps VoiceLoop to generate higher quality speech and to control the expressions in its synthesized speech by incorporating global characteristics into the speech generating process.","['https://openalex.org/W2963799213', 'https://openalex.org/W2515020857', 'https://openalex.org/W2519091744', 'https://openalex.org/W1522301498', 'https://openalex.org/W2532494225', 'https://openalex.org/W2471520273', 'https://openalex.org/W2160473997', 'https://openalex.org/W2953046278', 'https://openalex.org/W4322588875', 'https://openalex.org/W1978136968', 'https://openalex.org/W2770743791', 'https://openalex.org/W2962896155', 'https://openalex.org/W2745595539', 'https://openalex.org/W4298857617', 'https://openalex.org/W2963609956', 'https://openalex.org/W1517939602', 'https://openalex.org/W2059721297', 'https://openalex.org/W2766406951', 'https://openalex.org/W2962850167', 'https://openalex.org/W2407139314', 'https://openalex.org/W1492383498', 'https://openalex.org/W2759925408', 'https://openalex.org/W4293411878', 'https://openalex.org/W2156146072', 'https://openalex.org/W2745387029', 'https://openalex.org/W2901997113', 'https://openalex.org/W1959608418', 'https://openalex.org/W2963223306']",2018-08-28
https://openalex.org/W2532494225,https://doi.org/10.1109/apsipa.2016.7820786,Voice conversion from non-parallel corpora using variational auto-encoder,"We propose a flexible framework for spectral conversion (SC) that facilitates training with unaligned corpora. Many SC frameworks require parallel corpora, phonetic alignments, or explicit frame-wise correspondence for learning conversion functions or for synthesizing a target spectrum with the aid of alignments. However, these requirements gravely limit the scope of practical applications of SC due to scarcity or even unavailability of parallel corpora. We propose an SC framework based on variational auto-encoder which enables us to exploit non-parallel corpora. The framework comprises an encoder that learns speaker-independent phonetic representations and a decoder that learns to reconstruct the designated speaker. It removes the requirement of parallel corpora or phonetic alignments to train a spectral conversion system. We report objective and subjective evaluations to validate our proposed method and compare it to SC methods that have access to aligned corpora.","['https://openalex.org/W2512087624', 'https://openalex.org/W2290946177', 'https://openalex.org/W2169579015', 'https://openalex.org/W2067175291', 'https://openalex.org/W2161476805', 'https://openalex.org/W2407281753', 'https://openalex.org/W6631184489', 'https://openalex.org/W6675944832', 'https://openalex.org/W6640963894', 'https://openalex.org/W2121387787', 'https://openalex.org/W2019849101', 'https://openalex.org/W2157412983', 'https://openalex.org/W2156477760', 'https://openalex.org/W2290463584', 'https://openalex.org/W6696767757', 'https://openalex.org/W1517202054', 'https://openalex.org/W2120605154', 'https://openalex.org/W1977362459', 'https://openalex.org/W2473388484', 'https://openalex.org/W2049686551', 'https://openalex.org/W6631190155', 'https://openalex.org/W6637242042', 'https://openalex.org/W1665214252', 'https://openalex.org/W2108501770', 'https://openalex.org/W2032130465', 'https://openalex.org/W2964121744', 'https://openalex.org/W2294351487', 'https://openalex.org/W1520370180', 'https://openalex.org/W2949416428', 'https://openalex.org/W1522301498', 'https://openalex.org/W1959608418']",2016-12-01
https://openalex.org/W2963830550,https://doi.org/10.21437/interspeech.2018-1830,Multi-target Voice Conversion without Parallel Data by Adversarially Learning Disentangled Audio Representations,"Recently, cycle-consistent adversarial network (Cycle-GAN) has been successfully applied to voice conversion to a different speaker without parallel data, although in those approaches an individual model is needed for each target speaker.In this paper, we propose an adversarial learning framework for voice conversion, with which a single model can be trained to convert the voice to many different speakers, all without parallel data, by separating the speaker characteristics from the linguistic content in speech signals.An autoencoder is first trained to extract speaker-independent latent representations and speaker embedding separately using another auxiliary speaker classifier to regularize the latent representation.The decoder then takes the speaker-independent latent representation and the target speaker embedding as the input to generate the voice of the target speaker with the linguistic content of the source utterance.The quality of decoder output is further improved by patching with the residual signal produced by another pair of generator and discriminator.A target speaker set size of 20 was tested in the preliminary experiments, and very good voice quality was obtained.Conventional voice conversion metrics are reported.We also show that the speaker information has been properly reduced from the latent representations.","['https://openalex.org/W2502312327', 'https://openalex.org/W2962974898', 'https://openalex.org/W2964135678', 'https://openalex.org/W2950776302', 'https://openalex.org/W2476548250', 'https://openalex.org/W2621350877', 'https://openalex.org/W2120605154', 'https://openalex.org/W2157412983', 'https://openalex.org/W2758785877', 'https://openalex.org/W1523372075', 'https://openalex.org/W2962896155', 'https://openalex.org/W4295521014', 'https://openalex.org/W2396025094', 'https://openalex.org/W2127520494', 'https://openalex.org/W2532494225', 'https://openalex.org/W4298426053', 'https://openalex.org/W2547364378', 'https://openalex.org/W1509691205', 'https://openalex.org/W1959608418', 'https://openalex.org/W4320013936', 'https://openalex.org/W2963073614', 'https://openalex.org/W2057609679', 'https://openalex.org/W2156142001', 'https://openalex.org/W2086796102', 'https://openalex.org/W2963808252', 'https://openalex.org/W2056852181', 'https://openalex.org/W2148846882', 'https://openalex.org/W2651834199', 'https://openalex.org/W2608207374', 'https://openalex.org/W2774848319', 'https://openalex.org/W1987992317', 'https://openalex.org/W2105160541', 'https://openalex.org/W2619034550']",2018-08-28
https://openalex.org/W2154652894,,ROUGE: A Package for Automatic Evaluation of Summaries,"ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluations. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST. 1","['https://openalex.org/W2101105183', 'https://openalex.org/W2963010813', 'https://openalex.org/W2112174218', 'https://openalex.org/W2752885492', 'https://openalex.org/W3145128584', 'https://openalex.org/W2108325777', 'https://openalex.org/W2097879961', 'https://openalex.org/W1493638625', 'https://openalex.org/W2150824314', 'https://openalex.org/W2088781183', 'https://openalex.org/W3151369355', 'https://openalex.org/W2117010802']",2004-07-25
https://openalex.org/W2805122419,https://doi.org/10.48550/arxiv.1805.11264,Disentangling by Partitioning: A Representation Learning Framework for Multimodal Sensory Data,"Multimodal sensory data resembles the form of information perceived by humans for learning, and are easy to obtain in large quantities. Compared to unimodal data, synchronization of concepts between modalities in such data provides supervision for disentangling the underlying explanatory factors of each modality. Previous work leveraging multimodal data has mainly focused on retaining only the modality-invariant factors while discarding the rest. In this paper, we present a partitioned variational autoencoder (PVAE) and several training objectives to learn disentangled representations, which encode not only the shared factors, but also modality-dependent ones, into separate latent variables. Specifically, PVAE integrates a variational inference framework and a multimodal generative model that partitions the explanatory factors and conditions only on the relevant subset of them for generation. We evaluate our model on two parallel speech/image datasets, and demonstrate its ability to learn disentangled representations by qualitatively exploring within-modality and cross-modality conditional generation with semantics and styles specified by examples. For quantitative analysis, we evaluate the classification accuracy of automatically discovered semantic units. Our PVAE can achieve over 99% accuracy on both modalities.","['https://openalex.org/W2963104724', 'https://openalex.org/W2962732076', 'https://openalex.org/W2580178245', 'https://openalex.org/W2963618559', 'https://openalex.org/W2163922914', 'https://openalex.org/W2753738274', 'https://openalex.org/W2963143316', 'https://openalex.org/W2793111190', 'https://openalex.org/W2963115079', 'https://openalex.org/W1524333225', 'https://openalex.org/W2964001192', 'https://openalex.org/W1503933356', 'https://openalex.org/W2963305465', 'https://openalex.org/W2963680395', 'https://openalex.org/W2964327849', 'https://openalex.org/W2964121744', 'https://openalex.org/W2962862718', 'https://openalex.org/W2605762339', 'https://openalex.org/W2556930864', 'https://openalex.org/W2964127395', 'https://openalex.org/W2556013083', 'https://openalex.org/W2794490148', 'https://openalex.org/W2963330681', 'https://openalex.org/W2962695963', 'https://openalex.org/W2963264829', 'https://openalex.org/W2964245029', 'https://openalex.org/W1959608418', 'https://openalex.org/W2593116425']",2018-05-29
https://openalex.org/W2527729766,https://doi.org/10.7488/ds/1495,SUPERSEDED - CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit,"# SUPERSEDED - This item has been replaced by the one which can be found at https://doi.org/10.7488/ds/1994 . # This CSTR VCTK Corpus (Centre for Speech Technology Voice Cloning Toolkit) includes speech data uttered by 109 native speakers of English with various accents. Each speaker reads out about 400 sentences, most of which were selected from a newspaper plus the Rainbow Passage and an elicitation paragraph intended to identify the speaker's accent. The newspaper texts were taken from The Herald (Glasgow), with permission from Herald &amp; Times Group. Each speaker reads a different set of the newspaper sentences, where each set was selected using a greedy algorithm designed to maximise the contextual and phonetic coverage. The Rainbow Passage and elicitation paragraph are the same for all speakers. The Rainbow Passage can be found in the International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf . All speech data were recorded using an identical recording setup: an omni-directional head-mounted microphone (DPA 4035), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. All recordings were converted into 16 bits, downsampled to 48 kHz based on STPK, and manually end-pointed. This corpus was recorded for the purpose of building HMM-based text-to-speech synthesis systems, especially for speaker-adaptive HMM-based speech synthesis using average voice models trained on multiple speakers and speaker adaptation technologies. The file was previously available on the CSTR website, and was referenced in the Google DeepMind work on WaveNet: https://arxiv.org/pdf/1609.03499.pdf .",[],2016-01-01
https://openalex.org/W3093845497,https://doi.org/10.5281/zenodo.4126934,Show and Speak: Directly Synthesize Spoken Description of Images,This database is for the image-to-speech task. (Flickr8k),"['https://openalex.org/W854541894', 'https://openalex.org/W3096249149', 'https://openalex.org/W2895420168', 'https://openalex.org/W2896348597', 'https://openalex.org/W2986670728', 'https://openalex.org/W1524333225', 'https://openalex.org/W2968831808', 'https://openalex.org/W2613718673', 'https://openalex.org/W3095030004', 'https://openalex.org/W2997591391', 'https://openalex.org/W1895577753', 'https://openalex.org/W1514535095', 'https://openalex.org/W2745461083', 'https://openalex.org/W2962862718', 'https://openalex.org/W2108598243', 'https://openalex.org/W2888867175', 'https://openalex.org/W1494198834', 'https://openalex.org/W2964243274', 'https://openalex.org/W648786980', 'https://openalex.org/W2949382160', 'https://openalex.org/W2277195237', 'https://openalex.org/W68733909', 'https://openalex.org/W2990818246', 'https://openalex.org/W2988907666', 'https://openalex.org/W2963992143']",2020-10-24
https://openalex.org/W2787779284,https://doi.org/10.1109/icassp.2018.8461761,Linguistic Unit Discovery from Multi-Modal Inputs in Unwritten Languages: Summary of the “Speaking Rosetta” JSALT 2017 Workshop,We summarize the accomplishments of a multi-disciplinary workshop exploring the computational and scientific issues surrounding the discovery of linguistic units (subwords and words) in a language without orthography. We study the replacement of orthographic transcriptions by images and/or translated text in a well-resourced language to help unsupervised discovery from raw speech.,"['https://openalex.org/W6701403061', 'https://openalex.org/W4206865574', 'https://openalex.org/W6711753470', 'https://openalex.org/W6807204625', 'https://openalex.org/W6664486393', 'https://openalex.org/W2756778986', 'https://openalex.org/W2582956876', 'https://openalex.org/W6713625570', 'https://openalex.org/W2137010615', 'https://openalex.org/W2586148577', 'https://openalex.org/W6729977899', 'https://openalex.org/W6705006301', 'https://openalex.org/W6680628865', 'https://openalex.org/W2736876693', 'https://openalex.org/W6639102338', 'https://openalex.org/W6719357382', 'https://openalex.org/W6677620606', 'https://openalex.org/W6638813818', 'https://openalex.org/W2347098582', 'https://openalex.org/W6670629611', 'https://openalex.org/W3217769081', 'https://openalex.org/W2114347655', 'https://openalex.org/W2164505566', 'https://openalex.org/W6643049754', 'https://openalex.org/W6631194405', 'https://openalex.org/W6691362072', 'https://openalex.org/W6713745070', 'https://openalex.org/W6697293080', 'https://openalex.org/W2117041980', 'https://openalex.org/W2395899413', 'https://openalex.org/W1861492603', 'https://openalex.org/W2950613790', 'https://openalex.org/W2025482506', 'https://openalex.org/W2762715843', 'https://openalex.org/W2395052932', 'https://openalex.org/W2963620343', 'https://openalex.org/W2079460648', 'https://openalex.org/W2406324447', 'https://openalex.org/W1970890968', 'https://openalex.org/W2325972120', 'https://openalex.org/W1833498382', 'https://openalex.org/W2949328740', 'https://openalex.org/W2464234964', 'https://openalex.org/W202879582', 'https://openalex.org/W2251025892', 'https://openalex.org/W2556930864', 'https://openalex.org/W2345799635', 'https://openalex.org/W1520968739']",2018-04-01
https://openalex.org/W2995404354,,Neural Text Generation With Unlikelihood Training,"Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-k and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques.","['https://openalex.org/W2963096510', 'https://openalex.org/W2963403868', 'https://openalex.org/W2525332836', 'https://openalex.org/W2913443447', 'https://openalex.org/W2964190861', 'https://openalex.org/W2916772188', 'https://openalex.org/W2898658996', 'https://openalex.org/W2963463964', 'https://openalex.org/W3035239386', 'https://openalex.org/W2964268978', 'https://openalex.org/W2996287690', 'https://openalex.org/W2788277448', 'https://openalex.org/W2158349948', 'https://openalex.org/W2933138175', 'https://openalex.org/W2963283805', 'https://openalex.org/W2008652694', 'https://openalex.org/W2963631907', 'https://openalex.org/W2557436004', 'https://openalex.org/W2612675303', 'https://openalex.org/W2885421725', 'https://openalex.org/W2909379646', 'https://openalex.org/W2963212250', 'https://openalex.org/W2971883198', 'https://openalex.org/W2962957031']",2020-04-30
https://openalex.org/W3009205145,https://doi.org/10.1109/iccv.2019.00769,Unpaired Image-to-Speech Synthesis With Multimodal Information Bottleneck,"Deep generative models have led to significant advances in cross-modal generation such as text-to-image synthesis. Training these models typically requires paired data with direct correspondence between modalities. We introduce the novel problem of translating instances from one modality to another without paired data by leveraging an intermediate modality shared by the two other modalities. To demonstrate this, we take the problem of translating images to speech. In this case, one could leverage disjoint datasets with one shared modality, e.g., image-text pairs and text-speech pairs, with text as the shared modality. We call this problem ""skip-modal generation"" because the shared modality is skipped during the generation process. We propose a multimodal information bottleneck approach that learns the correspondence between modalities from unpaired data (image and speech) by leveraging the shared modality (text). We address fundamental challenges of skip-modal generation: 1) learning multimodal representations using a single model, 2) bridging the domain gap between two unrelated datasets, and 3) learning the correspondence between modalities from unpaired data. We show qualitative results on image-to-speech synthesis; this is the first time such results have been reported in the literature. We also show that our approach improves performance on traditional cross-modal generation, suggesting that it improves data efficiency in solving individual tasks.","['https://openalex.org/W6756197946', 'https://openalex.org/W6746700228', 'https://openalex.org/W1963549852', 'https://openalex.org/W2607579284', 'https://openalex.org/W1494198834', 'https://openalex.org/W6748573829', 'https://openalex.org/W6620707391', 'https://openalex.org/W6713645886', 'https://openalex.org/W6745697700', 'https://openalex.org/W2963267809', 'https://openalex.org/W6757422803', 'https://openalex.org/W6691096134', 'https://openalex.org/W2745461083', 'https://openalex.org/W6687566353', 'https://openalex.org/W6753850902', 'https://openalex.org/W6631190155', 'https://openalex.org/W6735204497', 'https://openalex.org/W2277195237', 'https://openalex.org/W6640963894', 'https://openalex.org/W6747270024', 'https://openalex.org/W6750489868', 'https://openalex.org/W6732742072', 'https://openalex.org/W6745992979', 'https://openalex.org/W2962793481', 'https://openalex.org/W2963727906', 'https://openalex.org/W2963807156', 'https://openalex.org/W2964024144', 'https://openalex.org/W6698228248', 'https://openalex.org/W2963966654', 'https://openalex.org/W6630875275', 'https://openalex.org/W2108598243', 'https://openalex.org/W6639480849', 'https://openalex.org/W2097117768', 'https://openalex.org/W2040510368', 'https://openalex.org/W6765987481', 'https://openalex.org/W2120847449', 'https://openalex.org/W2194775991', 'https://openalex.org/W2160815625', 'https://openalex.org/W2963446712', 'https://openalex.org/W2963073614', 'https://openalex.org/W2591927543', 'https://openalex.org/W6639432524', 'https://openalex.org/W6739366716', 'https://openalex.org/W2157331557', 'https://openalex.org/W2963609956', 'https://openalex.org/W2963767194', 'https://openalex.org/W6739901393', 'https://openalex.org/W1895577753', 'https://openalex.org/W6639082767', 'https://openalex.org/W6745569068', 'https://openalex.org/W6730095352', 'https://openalex.org/W6637108112', 'https://openalex.org/W2613718673', 'https://openalex.org/W2963712897', 'https://openalex.org/W2619368999', 'https://openalex.org/W4298857617', 'https://openalex.org/W2901997113', 'https://openalex.org/W2777302760', 'https://openalex.org/W2781384251', 'https://openalex.org/W2885013662', 'https://openalex.org/W2963826681', 'https://openalex.org/W2794490148', 'https://openalex.org/W2964121744', 'https://openalex.org/W2405756170', 'https://openalex.org/W2962739369', 'https://openalex.org/W2949382160', 'https://openalex.org/W1522301498', 'https://openalex.org/W2184045248', 'https://openalex.org/W1686946872', 'https://openalex.org/W4298174729', 'https://openalex.org/W2964243274', 'https://openalex.org/W2626778328', 'https://openalex.org/W2766812927', 'https://openalex.org/W4297772798', 'https://openalex.org/W2519091744', 'https://openalex.org/W2964341837', 'https://openalex.org/W1869752048', 'https://openalex.org/W2963534259', 'https://openalex.org/W2906797124', 'https://openalex.org/W2768959015', 'https://openalex.org/W2962964479', 'https://openalex.org/W2963125871', 'https://openalex.org/W2951939904', 'https://openalex.org/W2953022248', 'https://openalex.org/W4297818305', 'https://openalex.org/W639708223', 'https://openalex.org/W2949999304', 'https://openalex.org/W1514535095', 'https://openalex.org/W2302086703', 'https://openalex.org/W2979454998', 'https://openalex.org/W2099471712', 'https://openalex.org/W1882958252', 'https://openalex.org/W4320013936', 'https://openalex.org/W1889081078', 'https://openalex.org/W2564591810', 'https://openalex.org/W4385245566', 'https://openalex.org/W4293398859', 'https://openalex.org/W1959608418', 'https://openalex.org/W2963330667', 'https://openalex.org/W2193413348', 'https://openalex.org/W2963069010']",2019-10-01
https://openalex.org/W2736876693,https://doi.org/10.21437/glu.2017-9,SPEECH-COCO: 600k Visually Grounded Spoken Captions Aligned to MSCOCO Data Set,"This paper presents an augmentation of MSCOCO dataset where speech is added\nto image and text. Speech captions are generated using text-to-speech (TTS)\nsynthesis resulting in 616,767 spoken captions (more than 600h) paired with\nimages. Disfluencies and speed perturbation are added to the signal in order to\nsound more natural. Each speech signal (WAV) is paired with a JSON file\ncontaining exact timecode for each word/syllable/phoneme in the spoken caption.\nSuch a corpus could be used for Language and Vision (LaVi) tasks including\nspeech input or output instead of text. Investigating multimodal learning\nschemes for unsupervised speech pattern discovery is also possible with this\ncorpus, as demonstrated by a preliminary study conducted on a subset of the\ncorpus (10h, 10k spoken captions). The dataset is available on Zenodo:\nhttps://zenodo.org/record/4282267\n","['https://openalex.org/W2282219577', 'https://openalex.org/W68733909', 'https://openalex.org/W2057007397', 'https://openalex.org/W1993617199', 'https://openalex.org/W2949474740', 'https://openalex.org/W2963389687', 'https://openalex.org/W2964001192', 'https://openalex.org/W1905882502', 'https://openalex.org/W2962753610', 'https://openalex.org/W2509490957', 'https://openalex.org/W2964138343', 'https://openalex.org/W2132921748', 'https://openalex.org/W2962862718', 'https://openalex.org/W2185175083', 'https://openalex.org/W854541894', 'https://openalex.org/W2251025892', 'https://openalex.org/W2586148577', 'https://openalex.org/W2580178245', 'https://openalex.org/W2496096353', 'https://openalex.org/W2277195237', 'https://openalex.org/W2601713192', 'https://openalex.org/W1889081078', 'https://openalex.org/W2137010615', 'https://openalex.org/W1861492603']",2017-08-25
https://openalex.org/W2745461083,https://doi.org/10.1109/cvpr.2018.00636,Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering,"Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.","['https://openalex.org/W1984512236', 'https://openalex.org/W6730088351', 'https://openalex.org/W2963084599', 'https://openalex.org/W6620707391', 'https://openalex.org/W2963037989', 'https://openalex.org/W2560645892', 'https://openalex.org/W2964345214', 'https://openalex.org/W6780493881', 'https://openalex.org/W2161469100', 'https://openalex.org/W2117539524', 'https://openalex.org/W6719057275', 'https://openalex.org/W2575842049', 'https://openalex.org/W6898505805', 'https://openalex.org/W1933349210', 'https://openalex.org/W6725318829', 'https://openalex.org/W6736769356', 'https://openalex.org/W6682631176', 'https://openalex.org/W6694395031', 'https://openalex.org/W2949376505', 'https://openalex.org/W6639102338', 'https://openalex.org/W6785652829', 'https://openalex.org/W6600313631', 'https://openalex.org/W4251019013', 'https://openalex.org/W2963383024', 'https://openalex.org/W2149095485', 'https://openalex.org/W2560730294', 'https://openalex.org/W2194775991', 'https://openalex.org/W2302255633', 'https://openalex.org/W2064675550', 'https://openalex.org/W2471094925', 'https://openalex.org/W6618372016', 'https://openalex.org/W6638167273', 'https://openalex.org/W1905882502', 'https://openalex.org/W6639432524', 'https://openalex.org/W1999567494', 'https://openalex.org/W2019370496', 'https://openalex.org/W2157331557', 'https://openalex.org/W2133459682', 'https://openalex.org/W6731370813', 'https://openalex.org/W2962749469', 'https://openalex.org/W1947481528', 'https://openalex.org/W2963954913', 'https://openalex.org/W6630875275', 'https://openalex.org/W2552161745', 'https://openalex.org/W6729046916', 'https://openalex.org/W1956340063', 'https://openalex.org/W2088049833', 'https://openalex.org/W6692004142', 'https://openalex.org/W2119717200', 'https://openalex.org/W2506483933', 'https://openalex.org/W2963668159', 'https://openalex.org/W3106250896', 'https://openalex.org/W2277195237', 'https://openalex.org/W2964049455', 'https://openalex.org/W2613718673', 'https://openalex.org/W2154652894', 'https://openalex.org/W1861492603', 'https://openalex.org/W639708223', 'https://openalex.org/W2963970792', 'https://openalex.org/W2567070169', 'https://openalex.org/W2463565445', 'https://openalex.org/W603908379', 'https://openalex.org/W2109326756', 'https://openalex.org/W2101105183', 'https://openalex.org/W1785460851', 'https://openalex.org/W2555661914', 'https://openalex.org/W2963656855', 'https://openalex.org/W1514535095', 'https://openalex.org/W7746136', 'https://openalex.org/W2608030593', 'https://openalex.org/W3103022576', 'https://openalex.org/W1889081078']",2018-06-01
https://openalex.org/W2792995953,https://doi.org/10.48550/arxiv.1710.07654,Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning,"We present Deep Voice 3, a fully-convolutional attention-based neural text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training ten times faster. We scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers. In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods. We also describe how to scale inference to ten million queries per day on one single-GPU server.","['https://openalex.org/W2049686551', 'https://openalex.org/W2962965405', 'https://openalex.org/W2157331557', 'https://openalex.org/W2120847449', 'https://openalex.org/W2117418893', 'https://openalex.org/W2951064684', 'https://openalex.org/W854541894', 'https://openalex.org/W1563460361', 'https://openalex.org/W2736900972', 'https://openalex.org/W2626778328', 'https://openalex.org/W2964308564', 'https://openalex.org/W2160473997', 'https://openalex.org/W2949382160', 'https://openalex.org/W2963712897', 'https://openalex.org/W2130942839', 'https://openalex.org/W2963970792', 'https://openalex.org/W2901997113', 'https://openalex.org/W2471520273', 'https://openalex.org/W2963685250', 'https://openalex.org/W2591927543', 'https://openalex.org/W2165143604', 'https://openalex.org/W2963609956', 'https://openalex.org/W2964265128', 'https://openalex.org/W2584032004', 'https://openalex.org/W2507771204', 'https://openalex.org/W1570629387', 'https://openalex.org/W1494198834']",2017-10-20
https://openalex.org/W2884607399,https://doi.org/10.48550/arxiv.1807.11470,Deep Encoder-Decoder Models for Unsupervised Learning of Controllable Speech Synthesis,"Generating versatile and appropriate synthetic speech requires control over the output expression separate from the spoken text. Important non-textual speech variation is seldom annotated, in which case output control must be learned in an unsupervised fashion. In this paper, we perform an in-depth study of methods for unsupervised learning of control in statistical speech synthesis. For example, we show that popular unsupervised training heuristics can be interpreted as variational inference in certain autoencoder models. We additionally connect these models to VQ-VAEs, another, recently-proposed class of deep variational autoencoders, which we show can be derived from a very similar mathematical argument. The implications of these new probabilistic interpretations are discussed. We illustrate the utility of the various approaches with an application to acoustic modelling for emotional speech synthesis, where the unsupervised methods for learning expression control (without access to emotional labels) are found to give results that in many aspects match or surpass the previous best supervised approach.","['https://openalex.org/W1783473872', 'https://openalex.org/W2745595539', 'https://openalex.org/W2242818861', 'https://openalex.org/W2129703931', 'https://openalex.org/W1975079546', 'https://openalex.org/W2794490148', 'https://openalex.org/W2531662911', 'https://openalex.org/W2793479148', 'https://openalex.org/W2605762339', 'https://openalex.org/W2510638983', 'https://openalex.org/W2151402979', 'https://openalex.org/W304834817', 'https://openalex.org/W2788266530', 'https://openalex.org/W2753738274', 'https://openalex.org/W2901997113', 'https://openalex.org/W2617620476', 'https://openalex.org/W2808706139', 'https://openalex.org/W1993409002', 'https://openalex.org/W2294797155', 'https://openalex.org/W2963691546', 'https://openalex.org/W2066452495', 'https://openalex.org/W2795370625', 'https://openalex.org/W2099471712', 'https://openalex.org/W2804397569', 'https://openalex.org/W2167510900', 'https://openalex.org/W2283817422', 'https://openalex.org/W2795109282', 'https://openalex.org/W1532325895', 'https://openalex.org/W172929083', 'https://openalex.org/W2632564668', 'https://openalex.org/W2801493797', 'https://openalex.org/W2085013480', 'https://openalex.org/W2515020857', 'https://openalex.org/W2129142580', 'https://openalex.org/W2887264325', 'https://openalex.org/W2187089797', 'https://openalex.org/W2584521635', 'https://openalex.org/W2400063444', 'https://openalex.org/W2012086895', 'https://openalex.org/W2577946330', 'https://openalex.org/W2963799213', 'https://openalex.org/W2116082100', 'https://openalex.org/W75704375', 'https://openalex.org/W2595110011', 'https://openalex.org/W2042360461', 'https://openalex.org/W2964121744', 'https://openalex.org/W2402539796', 'https://openalex.org/W2125838338', 'https://openalex.org/W2951298482', 'https://openalex.org/W2152205330', 'https://openalex.org/W1959608418', 'https://openalex.org/W2396566817', 'https://openalex.org/W2950067852', 'https://openalex.org/W2069631319', 'https://openalex.org/W2766406951', 'https://openalex.org/W2963618559', 'https://openalex.org/W2784067649', 'https://openalex.org/W1833977909', 'https://openalex.org/W2467604901', 'https://openalex.org/W2188827208', 'https://openalex.org/W2471520273', 'https://openalex.org/W2759925408', 'https://openalex.org/W2112021726', 'https://openalex.org/W2791489977', 'https://openalex.org/W2026799324', 'https://openalex.org/W2494654097', 'https://openalex.org/W2953046278', 'https://openalex.org/W1492383498', 'https://openalex.org/W2154920538', 'https://openalex.org/W1993267429', 'https://openalex.org/W2949547296', 'https://openalex.org/W2795485067', 'https://openalex.org/W2964243274', 'https://openalex.org/W2049633694', 'https://openalex.org/W2093632031', 'https://openalex.org/W2963223306', 'https://openalex.org/W2121044470', 'https://openalex.org/W2584032004', 'https://openalex.org/W1909320841']",2018-07-30
https://openalex.org/W2796495654,https://doi.org/10.48550/arxiv.1804.04262,The Voice Conversion Challenge 2018: Promoting Development of Parallel and Nonparallel Methods,"We present the Voice Conversion Challenge 2018, designed as a follow up to the 2016 edition with the aim of providing a common framework for evaluating and comparing different state-of-the-art voice conversion (VC) systems. The objective of the challenge was to perform speaker conversion (i.e. transform the vocal identity) of a source speaker to a target speaker while maintaining linguistic information. As an update to the previous challenge, we considered both parallel and non-parallel data to form the Hub and Spoke tasks, respectively. A total of 23 teams from around the world submitted their systems, 11 of them additionally participated in the optional Spoke task. A large-scale crowdsourced perceptual evaluation was then carried out to rate the submitted converted speech in terms of naturalness and similarity to the target speaker identity. In this paper, we present a brief summary of the state-of-the-art techniques for VC, followed by a detailed explanation of the challenge tasks and the results that were obtained.","['https://openalex.org/W2142300631', 'https://openalex.org/W1590808459', 'https://openalex.org/W2161727827']",2018-04-12
https://openalex.org/W2937090315,https://doi.org/10.1109/icassp.2019.8683131,A Factorial Deep Markov Model for Unsupervised Disentangled Representation Learning from Speech,"We present the Factorial Deep Markov Model (FDMM) for representation learning of speech. The FDMM learns disentangled, interpretable and lower dimensional latent representations from speech without supervision. We use a static and dynamic latent variable to exploit the fact that information in a speech signal evolves at different time scales. Latent representations learned by the FDMM outperform a baseline i-vector system on speaker verification and dialect identification while also reducing the error rate of a phone recognition system in a domain mismatch scenario.","['https://openalex.org/W6680137814', 'https://openalex.org/W6731701471', 'https://openalex.org/W2963499843', 'https://openalex.org/W6631362777', 'https://openalex.org/W6604441197', 'https://openalex.org/W2005708641', 'https://openalex.org/W6696934422', 'https://openalex.org/W2748528945', 'https://openalex.org/W6640963894', 'https://openalex.org/W6732782712', 'https://openalex.org/W2963417023', 'https://openalex.org/W6745117592', 'https://openalex.org/W6749693530', 'https://openalex.org/W6728354068', 'https://openalex.org/W6675022971', 'https://openalex.org/W2055408826', 'https://openalex.org/W2225156818', 'https://openalex.org/W6749758385', 'https://openalex.org/W2963134917', 'https://openalex.org/W2791874451', 'https://openalex.org/W2293634267', 'https://openalex.org/W111477576', 'https://openalex.org/W4297730691', 'https://openalex.org/W2758785877', 'https://openalex.org/W3211294292', 'https://openalex.org/W2964232608', 'https://openalex.org/W3101380508', 'https://openalex.org/W2962922562', 'https://openalex.org/W2963618559', 'https://openalex.org/W1635512741', 'https://openalex.org/W2793111190', 'https://openalex.org/W1959608418', 'https://openalex.org/W2134827050', 'https://openalex.org/W2572702041', 'https://openalex.org/W1524333225', 'https://openalex.org/W2584414011', 'https://openalex.org/W2100768664']",2019-04-17
https://openalex.org/W1895577753,https://doi.org/10.1109/cvpr.2015.7298935,Show and tell: A neural image caption generator,"Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.","['https://openalex.org/W6686423066', 'https://openalex.org/W1987835821', 'https://openalex.org/W6679436768', 'https://openalex.org/W6635446068', 'https://openalex.org/W2064675550', 'https://openalex.org/W68733909', 'https://openalex.org/W6638667902', 'https://openalex.org/W6676647902', 'https://openalex.org/W6631516269', 'https://openalex.org/W6685230081', 'https://openalex.org/W2066134726', 'https://openalex.org/W6682086108', 'https://openalex.org/W6637306801', 'https://openalex.org/W6629368666', 'https://openalex.org/W6682778277', 'https://openalex.org/W6677651945', 'https://openalex.org/W2157331557', 'https://openalex.org/W6639694449', 'https://openalex.org/W6682059829', 'https://openalex.org/W6681184217', 'https://openalex.org/W6603785906', 'https://openalex.org/W2098524862', 'https://openalex.org/W6679434410', 'https://openalex.org/W6638273328', 'https://openalex.org/W6677231853', 'https://openalex.org/W6639102338', 'https://openalex.org/W6636510571', 'https://openalex.org/W6683512859', 'https://openalex.org/W6676497082', 'https://openalex.org/W6677994088', 'https://openalex.org/W6898505805', 'https://openalex.org/W2149172860', 'https://openalex.org/W1614298861', 'https://openalex.org/W1810943226', 'https://openalex.org/W1687846465', 'https://openalex.org/W2143449221', 'https://openalex.org/W1527575280', 'https://openalex.org/W2159243025', 'https://openalex.org/W2949117887', 'https://openalex.org/W2130942839', 'https://openalex.org/W2296385829', 'https://openalex.org/W2964308564', 'https://openalex.org/W2149557440', 'https://openalex.org/W4294375521', 'https://openalex.org/W2963542991', 'https://openalex.org/W2109586012', 'https://openalex.org/W8316075', 'https://openalex.org/W1591801644', 'https://openalex.org/W2112912048', 'https://openalex.org/W1956340063', 'https://openalex.org/W2101105183', 'https://openalex.org/W2185175083', 'https://openalex.org/W2119775030', 'https://openalex.org/W2133564696', 'https://openalex.org/W2950179405', 'https://openalex.org/W1836465849', 'https://openalex.org/W2953276893', 'https://openalex.org/W2114841702', 'https://openalex.org/W1861492603', 'https://openalex.org/W2155541015', 'https://openalex.org/W92662927', 'https://openalex.org/W2171361956', 'https://openalex.org/W2117539524', 'https://openalex.org/W1897761818']",2015-06-01
https://openalex.org/W2133459682,https://doi.org/10.3115/v1/w14-3348,Meteor Universal: Language Specific Translation Evaluation for Any Target Language,"This paper describes Meteor Universal, released for the 2014 ACL Workshop on Statistical Machine Translation.Meteor Universal brings language specific evaluation to previously unsupported target languages by (1) automatically extracting linguistic resources (paraphrase tables and function word lists) from the bitext used to train MT systems and (2) using a universal parameter set learned from pooling human judgments of translation quality from several language directions.Meteor Universal is shown to significantly outperform baseline BLEU on two new languages, Russian (WMT13) and Hindi (WMT14).","['https://openalex.org/W2018869373', 'https://openalex.org/W2153653739', 'https://openalex.org/W2165213242', 'https://openalex.org/W2124807415', 'https://openalex.org/W2143927888', 'https://openalex.org/W2895810819', 'https://openalex.org/W4241645538', 'https://openalex.org/W2508316494', 'https://openalex.org/W2087735403', 'https://openalex.org/W2595715041', 'https://openalex.org/W2102692503', 'https://openalex.org/W2251810465', 'https://openalex.org/W2251994258', 'https://openalex.org/W2917452219', 'https://openalex.org/W2116492146']",2014-01-01
https://openalex.org/W1993660824,https://doi.org/10.1109/icassp.2013.6638959,Deep neural network features and semi-supervised training for low resource speech recognition,"We propose a new technique for training deep neural networks (DNNs) as data-driven feature front-ends for large vocabulary continuous speech recognition (LVCSR) in low resource settings. To circumvent the lack of sufficient training data for acoustic modeling in these scenarios, we use transcribed multilingual data and semi-supervised training to build the proposed feature front-ends. In our experiments, the proposed features provide an absolute improvement of 16% in a low-resource LVCSR setting with only one hour of in-domain training data. While close to three-fourths of these gains come from DNN-based features, the remaining are from semi-supervised training.","['https://openalex.org/W2105099419', 'https://openalex.org/W2110073835', 'https://openalex.org/W2147768505', 'https://openalex.org/W6606723699', 'https://openalex.org/W6678292227', 'https://openalex.org/W1555037511', 'https://openalex.org/W6680300913', 'https://openalex.org/W3148201686', 'https://openalex.org/W6608710415', 'https://openalex.org/W2160306971', 'https://openalex.org/W2160815625', 'https://openalex.org/W55333121', 'https://openalex.org/W2127982613', 'https://openalex.org/W2126315862', 'https://openalex.org/W6713135925', 'https://openalex.org/W2136922672', 'https://openalex.org/W2076794394', 'https://openalex.org/W6676481782', 'https://openalex.org/W2012897754', 'https://openalex.org/W6606983177', 'https://openalex.org/W6890491316', 'https://openalex.org/W2166637769', 'https://openalex.org/W2123798005', 'https://openalex.org/W6631781866', 'https://openalex.org/W2407441242', 'https://openalex.org/W2130414229', 'https://openalex.org/W2090861223', 'https://openalex.org/W2131042651', 'https://openalex.org/W2120137673', 'https://openalex.org/W6631419337', 'https://openalex.org/W6605668731', 'https://openalex.org/W2170580867', 'https://openalex.org/W1991180839', 'https://openalex.org/W6712585555', 'https://openalex.org/W6714284179', 'https://openalex.org/W2403234505', 'https://openalex.org/W2399153411', 'https://openalex.org/W217970951', 'https://openalex.org/W2184045248', 'https://openalex.org/W165878654', 'https://openalex.org/W1526995323', 'https://openalex.org/W2606321545', 'https://openalex.org/W1534441687', 'https://openalex.org/W139772320', 'https://openalex.org/W2110798204', 'https://openalex.org/W2123237149', 'https://openalex.org/W169882265', 'https://openalex.org/W2138857742', 'https://openalex.org/W2407897255']",2013-05-01
https://openalex.org/W2079623482,https://doi.org/10.1109/asru.2013.6707705,Speaker adaptation of neural network acoustic models using i-vectors,"We propose to adapt deep neural network (DNN) acoustic models to a target speaker by supplying speaker identity vectors (i-vectors) as input features to the network in parallel with the regular acoustic features for ASR. For both training and test, the i-vector for a given speaker is concatenated to every frame belonging to that speaker and changes across different speakers. Experimental results on a Switchboard 300 hours corpus show that DNNs trained on speaker independent features and i-vectors achieve a 10% relative improvement in word error rate (WER) over networks trained on speaker independent features only. These networks are comparable in performance to DNNs trained on speaker-adapted features (with VTLN and FMLLR) with the advantage that only one decoding pass is needed. Furthermore, networks trained on speaker-adapted features and i-vectors achieve a 5-6% relative improvement in WER after hessian-free sequence training over networks trained on speaker-adapted features only.","['https://openalex.org/W2403195671', 'https://openalex.org/W2058641082', 'https://openalex.org/W1979058968', 'https://openalex.org/W6608353291', 'https://openalex.org/W2022608894', 'https://openalex.org/W2087006792', 'https://openalex.org/W2160306971', 'https://openalex.org/W4230607426', 'https://openalex.org/W2150769028', 'https://openalex.org/W2062164080', 'https://openalex.org/W1989549063', 'https://openalex.org/W1993409002', 'https://openalex.org/W2164240571', 'https://openalex.org/W2146635662', 'https://openalex.org/W2289089770', 'https://openalex.org/W1523142923', 'https://openalex.org/W1482750548', 'https://openalex.org/W1985371235', 'https://openalex.org/W2024919734', 'https://openalex.org/W204053250', 'https://openalex.org/W2265396709', 'https://openalex.org/W2196568223', 'https://openalex.org/W2270033225', 'https://openalex.org/W2189248916']",2013-12-01
https://openalex.org/W2150769028,https://doi.org/10.1109/tasl.2010.2064307,Front-End Factor Analysis for Speaker Verification,"This paper presents an extension of our previous work which proposes a new speaker representation for speaker verification. In this modeling, a new low-dimensional speaker- and channel-dependent space is defined using a simple factor analysis. This space is named the total variability space because it models both speaker and channel variabilities. Two speaker verification systems are proposed which use this new representation. The first system is a support vector machine-based system that uses the cosine kernel to estimate the similarity between the input data. The second system directly uses the cosine similarity as the final decision score. We tested three channel compensation techniques in the total variability space, which are within-class covariance normalization (WCCN), linear discriminate analysis (LDA), and nuisance attribute projection (NAP). We found that the best results are obtained when LDA is followed by WCCN. We achieved an equal error rate (EER) of 1.12% and MinDCF of 0.0094 using the cosine distance scoring on the male English trials of the core condition of the NIST 2008 Speaker Recognition Evaluation dataset. We also obtained 4% absolute EER improvement for both-gender trials on the 10 s-10 s condition compared to the classical joint factor analysis scoring.","['https://openalex.org/W2170065313', 'https://openalex.org/W2121415728', 'https://openalex.org/W2156909104', 'https://openalex.org/W6712325649', 'https://openalex.org/W2129785530', 'https://openalex.org/W6712660717', 'https://openalex.org/W2041823554', 'https://openalex.org/W2136879537', 'https://openalex.org/W2144760012', 'https://openalex.org/W6640010188', 'https://openalex.org/W2130955819', 'https://openalex.org/W2112582577', 'https://openalex.org/W2107638917', 'https://openalex.org/W111477576', 'https://openalex.org/W1916834241', 'https://openalex.org/W740415', 'https://openalex.org/W2397634864', 'https://openalex.org/W2398362606']",2010-08-10
https://openalex.org/W2150907703,https://doi.org/10.1109/icassp.2002.5743665,Minimum Phone Error and I-smoothing for improved discriminative training,"In this paper we introduce the Minimum Phone Error (MPE) and Minimum Word Error (MWE) criteria for the discriminative training of HMM systems. The MPE/MWE criteria are smoothed approximations to the phone or word error rate respectively. We also discuss I-smoothing which is a novel technique for smoothing discriminative training criteria using statistics for maximum likelihood estimation (MLE). Experiments have been performed on the Switchboard/Call Home corpora of telephone conversations with up to 265 hours of training data. It is shown that for the maximum mutual information estimation (MMIE) criterion, I-smoothing reduces the word error rate (WER) by 0.4% absolute over the MMIE baseline. The combination of MPE and I-smoothing gives an improvement of 1 % over MMIE and a total reduction in WER of 4.8% absolute over the original MLE system.","['https://openalex.org/W2098601596', 'https://openalex.org/W6683602084', 'https://openalex.org/W2157749010', 'https://openalex.org/W2003123121', 'https://openalex.org/W2158289097', 'https://openalex.org/W2097635532', 'https://openalex.org/W1574530145', 'https://openalex.org/W1565453839', 'https://openalex.org/W1877570817', 'https://openalex.org/W2033565080']",2002-05-01
https://openalex.org/W1877570817,https://doi.org/10.1109/icassp.1986.1169179,Maximum mutual information estimation of hidden Markov model parameters for speech recognition,A method for estimating the parameters of hidden Markov models of speech is described. Parameter values are chosen to maximize the mutual information between an acoustic observation sequence and the corresponding word sequence. Recognition results are presented comparing this method with maximum likelihood estimation.,"['https://openalex.org/W1966812932', 'https://openalex.org/W2099074650', 'https://openalex.org/W1575431606']",2005-03-24
https://openalex.org/W3093579165,https://doi.org/10.48550/arxiv.2010.10504,Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition,"We employ a combination of recent developments in semi-supervised learning for automatic speech recognition to obtain state-of-the-art results on LibriSpeech utilizing the unlabeled audio of the Libri-Light dataset. More precisely, we carry out noisy student training with SpecAugment using giant Conformer models pre-trained using wav2vec 2.0 pre-training. By doing so, we are able to achieve word-error-rates (WERs) 1.4%/2.6% on the LibriSpeech test/test-other sets against the current state-of-the-art WERs 1.7%/3.3%.","['https://openalex.org/W2064675550', 'https://openalex.org/W3021469861', 'https://openalex.org/W3015522062', 'https://openalex.org/W3035160371', 'https://openalex.org/W2964245029', 'https://openalex.org/W2767286248', 'https://openalex.org/W3025165719', 'https://openalex.org/W2963403868', 'https://openalex.org/W1494198834', 'https://openalex.org/W2981952041', 'https://openalex.org/W2962907457', 'https://openalex.org/W2977728428', 'https://openalex.org/W165878654', 'https://openalex.org/W2976223659', 'https://openalex.org/W3125709657', 'https://openalex.org/W2911109671', 'https://openalex.org/W3035003500', 'https://openalex.org/W2794753807', 'https://openalex.org/W3006827623', 'https://openalex.org/W2121879602', 'https://openalex.org/W1993660824', 'https://openalex.org/W2946856970', 'https://openalex.org/W1915251500', 'https://openalex.org/W2998532468', 'https://openalex.org/W2940322076', 'https://openalex.org/W3036982689', 'https://openalex.org/W2117590177', 'https://openalex.org/W2932319281', 'https://openalex.org/W2802422770', 'https://openalex.org/W1489125746', 'https://openalex.org/W3099782249', 'https://openalex.org/W2973049979', 'https://openalex.org/W2995181338', 'https://openalex.org/W3026041220', 'https://openalex.org/W2936774411', 'https://openalex.org/W2781384251', 'https://openalex.org/W3015995734', 'https://openalex.org/W2088622183', 'https://openalex.org/W1828163288', 'https://openalex.org/W3001197829', 'https://openalex.org/W2962369866', 'https://openalex.org/W2962911098', 'https://openalex.org/W2111316763', 'https://openalex.org/W2991213871', 'https://openalex.org/W3016010032', 'https://openalex.org/W2193413348', 'https://openalex.org/W2987806402', 'https://openalex.org/W3103005696', 'https://openalex.org/W2963341956', 'https://openalex.org/W3015265920', 'https://openalex.org/W2926827382', 'https://openalex.org/W3036601975', 'https://openalex.org/W2979476256']",2020-10-20
https://openalex.org/W4225741214,https://doi.org/10.1109/asru51503.2021.9688056,On Lattice-Free Boosted MMI Training of HMM and CTC-Based Full-Context ASR Models,"Hybrid automatic speech recognition (ASR) models are typically sequentially trained with CTC or LF-MMI criteria. However, they have vastly different legacies and are usually implemented in different frameworks. In this paper, by decoupling the concepts of modeling units and label topologies and building proper numerator/denominator graphs accordingly, we establish a generalized framework for hybrid acoustic modeling (AM). In this framework, we show that LF-MMI is a powerful training criterion applicable to both limited-context and full-context models, for wordpiece/mono-char/bi-char/chenone units, with both HMM/CTC topologies. From this framework, we propose three novel training schemes: chenone(ch)/wordpiece(wp)-CTC-bMMI, and wordpiece(wp)-HMM-bMMI with different advantages in training performance, decoding efficiency and decoding time-stamp accuracy. The advantages of different training schemes are evaluated comprehensively on Librispeech, and wp-CTC-bMMI and ch-CTC-bMMI are evaluated on two real world ASR tasks to show their effectiveness. Besides, we also show bi-char(bc) HMM-MMI models can serve as better alignment models than traditional non-neural GMM-HMMs.","['https://openalex.org/W3162665866', 'https://openalex.org/W6761305474', 'https://openalex.org/W2404463488', 'https://openalex.org/W3097558625', 'https://openalex.org/W2963250244', 'https://openalex.org/W2402146185', 'https://openalex.org/W2048060899', 'https://openalex.org/W2064675550', 'https://openalex.org/W6629717138', 'https://openalex.org/W2327501763', 'https://openalex.org/W3097777922', 'https://openalex.org/W2962824709', 'https://openalex.org/W3146505093', 'https://openalex.org/W2143612262', 'https://openalex.org/W3144557079', 'https://openalex.org/W3149509723', 'https://openalex.org/W6778184438', 'https://openalex.org/W2802248956', 'https://openalex.org/W6762146994', 'https://openalex.org/W6793540458', 'https://openalex.org/W2406513788', 'https://openalex.org/W2400997536', 'https://openalex.org/W2124558353', 'https://openalex.org/W2808939837', 'https://openalex.org/W2158069733', 'https://openalex.org/W2514741789', 'https://openalex.org/W6638749077', 'https://openalex.org/W7027429494', 'https://openalex.org/W1877570817', 'https://openalex.org/W2962826786', 'https://openalex.org/W2147768505', 'https://openalex.org/W3008525923', 'https://openalex.org/W2125234026', 'https://openalex.org/W1533416326', 'https://openalex.org/W3003285202', 'https://openalex.org/W2963164596', 'https://openalex.org/W6631362777', 'https://openalex.org/W2886949521', 'https://openalex.org/W2900209846', 'https://openalex.org/W1828163288', 'https://openalex.org/W1494198834', 'https://openalex.org/W1524333225', 'https://openalex.org/W2936774411', 'https://openalex.org/W3028545098', 'https://openalex.org/W2939297570', 'https://openalex.org/W3198004110']",2021-12-13
https://openalex.org/W4312337341,https://doi.org/10.1109/cvpr52688.2022.00805,Audio-Visual Speech Codecs: Rethinking Audio-Visual Speech Enhancement by Re-Synthesis,"Since facial actions such as lip movements contain significant information about speech content, it is not surprising that audio-visual speech enhancement methods are more accurate than their audio-only counterparts. Yet, state-of-the-art approaches still struggle to generate clean, realistic speech without noise artifacts and unnatural distortions in challenging acoustic environments. In this paper, we propose a novel audio-visual speech enhancement framework for high-fidelity telecommunications in AR/VR. Our approach leverages audio-visual speech cues to generate the codes of a neural speech codec, enabling efficient synthesis of clean, realistic speech from noisy signals. Given the importance of speaker-specific cues in speech, we focus on developing personalized models that work well for individual speakers. We demonstrate the efficacy of our approach on a new audio-visual speech dataset collected in an unconstrained, large vocabulary setting, as well as existing audio-visual datasets, outperforming speech enhancement baselines on both quantitative metrics and human evaluation studies. Please see the supplemental video for qualitative results <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> https://github.com/facebookresearch/facestar/releases/download/paper_materials/video.mp4.","['https://openalex.org/W6750599028', 'https://openalex.org/W6750591037', 'https://openalex.org/W2752796333', 'https://openalex.org/W2903739847', 'https://openalex.org/W2963936489', 'https://openalex.org/W6767111847', 'https://openalex.org/W2962946126', 'https://openalex.org/W6774995033', 'https://openalex.org/W3136499730', 'https://openalex.org/W2811426698', 'https://openalex.org/W2069681747', 'https://openalex.org/W2957926190', 'https://openalex.org/W6736996214', 'https://openalex.org/W3163662330', 'https://openalex.org/W1897240248', 'https://openalex.org/W2242685705', 'https://openalex.org/W2981816492', 'https://openalex.org/W1987906574', 'https://openalex.org/W6783867762', 'https://openalex.org/W2460742184', 'https://openalex.org/W6798098866', 'https://openalex.org/W6743555153', 'https://openalex.org/W2981851635', 'https://openalex.org/W2972513594', 'https://openalex.org/W2964171275', 'https://openalex.org/W2593116425', 'https://openalex.org/W2194775991', 'https://openalex.org/W2029199293', 'https://openalex.org/W2050708608', 'https://openalex.org/W2031647436', 'https://openalex.org/W2963182577', 'https://openalex.org/W6729448088', 'https://openalex.org/W6684458083', 'https://openalex.org/W2964243274', 'https://openalex.org/W2131738223', 'https://openalex.org/W2555915854', 'https://openalex.org/W2014621385', 'https://openalex.org/W3096159803', 'https://openalex.org/W6751512325', 'https://openalex.org/W7071105756', 'https://openalex.org/W1504438288', 'https://openalex.org/W6684447540', 'https://openalex.org/W2963019222', 'https://openalex.org/W2644497536', 'https://openalex.org/W6750169759', 'https://openalex.org/W2585824449', 'https://openalex.org/W3180355996', 'https://openalex.org/W6675974536', 'https://openalex.org/W6749863746', 'https://openalex.org/W3017343282', 'https://openalex.org/W6757240503', 'https://openalex.org/W3182657421', 'https://openalex.org/W6761347342', 'https://openalex.org/W6755135894', 'https://openalex.org/W2105301089', 'https://openalex.org/W3024147341', 'https://openalex.org/W2808631503', 'https://openalex.org/W3097945073', 'https://openalex.org/W3154411171', 'https://openalex.org/W2015143272', 'https://openalex.org/W2143027228', 'https://openalex.org/W6755592152', 'https://openalex.org/W3035626590', 'https://openalex.org/W6762533536', 'https://openalex.org/W2660943524', 'https://openalex.org/W2963341071', 'https://openalex.org/W1882423120', 'https://openalex.org/W3140429000', 'https://openalex.org/W6749489859', 'https://openalex.org/W2963300588', 'https://openalex.org/W2945478979', 'https://openalex.org/W2962960500', 'https://openalex.org/W2547875792', 'https://openalex.org/W2519091744', 'https://openalex.org/W2168793898', 'https://openalex.org/W2106488367', 'https://openalex.org/W3092028330', 'https://openalex.org/W2168818958', 'https://openalex.org/W3215615641', 'https://openalex.org/W3123318516', 'https://openalex.org/W3035268204', 'https://openalex.org/W2935711438', 'https://openalex.org/W2970006822', 'https://openalex.org/W2963799213', 'https://openalex.org/W2892620417', 'https://openalex.org/W2963609956', 'https://openalex.org/W2963452667', 'https://openalex.org/W2963307811', 'https://openalex.org/W2963082324', 'https://openalex.org/W2963691546']",2022-06-01
https://openalex.org/W2585824449,https://doi.org/10.1109/icassp.2017.7953127,Vid2speech: Speech reconstruction from silent video,"Speechreading is a notoriously difficult task for humans to perform. In this paper we present an end-to-end model based on a convolutional neural network (CNN) for generating an intelligible acoustic speech signal from silent video frames of a speaking person. The proposed CNN generates sound features for each frame based on its neighboring frames. Waveforms are then synthesized from the learned speech features to produce intelligible speech. We show that by leveraging the automatic feature learning capabilities of a CNN, we can obtain state-of-the-art word intelligibility on the GRID dataset, and show promising results for learning out-of-vocabulary (OOV) words.","['https://openalex.org/W2293856338', 'https://openalex.org/W4206319965', 'https://openalex.org/W2009674825', 'https://openalex.org/W6637373629', 'https://openalex.org/W1677182931', 'https://openalex.org/W2365880764', 'https://openalex.org/W2060510034', 'https://openalex.org/W2578229578', 'https://openalex.org/W2267805933', 'https://openalex.org/W2079402412', 'https://openalex.org/W2113814270', 'https://openalex.org/W2068056889', 'https://openalex.org/W6631190155', 'https://openalex.org/W2095705004', 'https://openalex.org/W2015143272', 'https://openalex.org/W1686810756', 'https://openalex.org/W2964345931', 'https://openalex.org/W4298112588', 'https://openalex.org/W2952746495', 'https://openalex.org/W1522301498', 'https://openalex.org/W2964121744', 'https://openalex.org/W22517275']",2017-03-01
https://openalex.org/W2015143272,https://doi.org/10.1121/1.2229005,An audio-visual corpus for speech perception and automatic speech recognition,"An audio-visual corpus has been collected to support the use of common material in speech perception and automatic speech recognition studies. The corpus consists of high-quality audio and video recordings of 1000 sentences spoken by each of 34 talkers. Sentences are simple, syntactically identical phrases such as “place green at B 4 now.” Intelligibility tests using the audio signals suggest that the material is easily identifiable in quiet and low levels of stationary noise. The annotated corpus is available on the web for research use.","['https://openalex.org/W2019588966', 'https://openalex.org/W2046058153', 'https://openalex.org/W4250500702', 'https://openalex.org/W2075076415', 'https://openalex.org/W1985029311', 'https://openalex.org/W2022833114', 'https://openalex.org/W2086300622', 'https://openalex.org/W2096391593', 'https://openalex.org/W1987831012', 'https://openalex.org/W2072503695', 'https://openalex.org/W1991112797', 'https://openalex.org/W2152131029', 'https://openalex.org/W2051091181', 'https://openalex.org/W2166637769', 'https://openalex.org/W2619993508', 'https://openalex.org/W1985188306']",2006-11-01
https://openalex.org/W2609317876,https://doi.org/10.1109/taslp.2017.2696307,Time-Frequency Masking in the Complex Domain for Speech Dereverberation and Denoising,"In real-world situations, speech is masked by both background noise and reverberation, which negatively affect perceptual quality and intelligibility. In this paper, we address monaural speech separation in reverberant and noisy environments. We perform dereverberation and denoising using supervised learning with a deep neural network. Specifically, we enhance the magnitude and phase by performing separation with an estimate of the complex ideal ratio mask. We define the complex ideal ratio mask so that direct speech results after the mask is applied to reverberant and noisy speech. Our approach is evaluated using simulated and real room impulse responses, and with background noises. The proposed approach improves objective speech quality and intelligibility significantly. Evaluations and comparisons show that it outperforms related methods in many reverberant and noisy environments.","['https://openalex.org/W1516630152', 'https://openalex.org/W2069681747', 'https://openalex.org/W2078210373', 'https://openalex.org/W2128334000', 'https://openalex.org/W2070126272', 'https://openalex.org/W1989364685', 'https://openalex.org/W2141998673', 'https://openalex.org/W2026640571', 'https://openalex.org/W2054480978', 'https://openalex.org/W2135858797', 'https://openalex.org/W2042071805', 'https://openalex.org/W2095072097', 'https://openalex.org/W2006129368', 'https://openalex.org/W2090861223', 'https://openalex.org/W2137075158', 'https://openalex.org/W2114218864', 'https://openalex.org/W4231807801', 'https://openalex.org/W1992879732', 'https://openalex.org/W1991646723', 'https://openalex.org/W2913340405', 'https://openalex.org/W2243943790', 'https://openalex.org/W2040782121', 'https://openalex.org/W2153418894', 'https://openalex.org/W2052667477', 'https://openalex.org/W1482149378', 'https://openalex.org/W6681435938', 'https://openalex.org/W2084347883', 'https://openalex.org/W2117678320', 'https://openalex.org/W2122982914', 'https://openalex.org/W2055611715', 'https://openalex.org/W2097491464', 'https://openalex.org/W6679607613', 'https://openalex.org/W2291877678', 'https://openalex.org/W2058399474', 'https://openalex.org/W2711335087', 'https://openalex.org/W2041638389', 'https://openalex.org/W1983812858', 'https://openalex.org/W2045043668', 'https://openalex.org/W2110322414', 'https://openalex.org/W1998648683', 'https://openalex.org/W2146502635', 'https://openalex.org/W3127686677', 'https://openalex.org/W2131118158', 'https://openalex.org/W2478884216', 'https://openalex.org/W2165899180']",2017-04-24
https://openalex.org/W2075076415,https://doi.org/10.1121/1.2166600,A glimpsing model of speech perception in noise,"Do listeners process noisy speech by taking advantage of “glimpses”—spectrotemporal regions in which the target signal is least affected by the background? This study used an automatic speech recognition system, adapted for use with partially specified inputs, to identify consonants in noise. Twelve masking conditions were chosen to create a range of glimpse sizes. Several different glimpsing models were employed, differing in the local signal-to-noise ratio (SNR) used for detection, the minimum glimpse size, and the use of information in the masked regions. Recognition results were compared with behavioral data. A quantitative analysis demonstrated that the proportion of the time–frequency plane glimpsed is a good predictor of intelligibility. Recognition scores in each noise condition confirmed that sufficient information exists in glimpses to support consonant identification. Close fits to listeners’ performance were obtained at two local SNR thresholds: one at around 8dB and another in the range −5 to −2dB. A transmitted information analysis revealed that cues to voicing are degraded more in the model than in human auditory processing.","['https://openalex.org/W2019588966', 'https://openalex.org/W2064359032', 'https://openalex.org/W2025172651', 'https://openalex.org/W2047306924', 'https://openalex.org/W1498607327', 'https://openalex.org/W2106603795', 'https://openalex.org/W1500135552', 'https://openalex.org/W1969105061', 'https://openalex.org/W133022121', 'https://openalex.org/W4250500702', 'https://openalex.org/W2071070548', 'https://openalex.org/W2029723712', 'https://openalex.org/W1999060702', 'https://openalex.org/W2099741732', 'https://openalex.org/W4247750718', 'https://openalex.org/W2102865113', 'https://openalex.org/W203616620', 'https://openalex.org/W2074354966', 'https://openalex.org/W2037752682', 'https://openalex.org/W7011341127', 'https://openalex.org/W2074912325', 'https://openalex.org/W2075364277', 'https://openalex.org/W2091766358', 'https://openalex.org/W2001079512', 'https://openalex.org/W121116918', 'https://openalex.org/W2022833114', 'https://openalex.org/W1985784335', 'https://openalex.org/W2086300622', 'https://openalex.org/W2063110373', 'https://openalex.org/W2010851803', 'https://openalex.org/W2074658690', 'https://openalex.org/W2097349831', 'https://openalex.org/W1980459899', 'https://openalex.org/W1964600646', 'https://openalex.org/W1970533835', 'https://openalex.org/W2045691673', 'https://openalex.org/W1565658805', 'https://openalex.org/W2115995108', 'https://openalex.org/W2129800309', 'https://openalex.org/W6720003353', 'https://openalex.org/W2402362654', 'https://openalex.org/W2034040413', 'https://openalex.org/W2069256376', 'https://openalex.org/W1974932989', 'https://openalex.org/W2072503695', 'https://openalex.org/W2101529994', 'https://openalex.org/W2028429753', 'https://openalex.org/W1987831012', 'https://openalex.org/W2023562516', 'https://openalex.org/W1991473673', 'https://openalex.org/W1800365115', 'https://openalex.org/W2040958907', 'https://openalex.org/W1977531436', 'https://openalex.org/W1964223780', 'https://openalex.org/W2100611575', 'https://openalex.org/W1550027367', 'https://openalex.org/W2469991276', 'https://openalex.org/W4416539', 'https://openalex.org/W1548802052', 'https://openalex.org/W2057889776', 'https://openalex.org/W3023341514', 'https://openalex.org/W2395939774', 'https://openalex.org/W2128202590', 'https://openalex.org/W2619993508', 'https://openalex.org/W4205897042', 'https://openalex.org/W1991112797']",2006-03-01
https://openalex.org/W1482149378,https://doi.org/10.1109/icassp.2015.7178061,Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks,"Separation of speech embedded in non-stationary interference is a challenging problem that has recently seen dramatic improvements using deep network-based methods. Previous work has shown that estimating a masking function to be applied to the noisy spectrum is a viable approach that can be improved by using a signal-approximation based objective function. Better modeling of dynamics through deep recurrent networks has also been shown to improve performance. Here we pursue both of these directions. We develop a phase-sensitive objective function based on the signal-to-noise ratio (SNR) of the reconstructed signal, and show that in experiments it yields uniformly better results in terms of signal-to-distortion ratio (SDR). We also investigate improvements to the modeling of dynamics, using bidirectional recurrent networks, as well as by incorporating speech recognition outputs in the form of alignment vectors concatenated with the spectral input features. Both methods yield further improvements, pointing to tighter integration of recognition with separation as a promising future direction.","['https://openalex.org/W1980741455', 'https://openalex.org/W6668037159', 'https://openalex.org/W2141411743', 'https://openalex.org/W2090681206', 'https://openalex.org/W2094461119', 'https://openalex.org/W1976951404', 'https://openalex.org/W2149648698', 'https://openalex.org/W2147455188', 'https://openalex.org/W2101045344', 'https://openalex.org/W146976060', 'https://openalex.org/W2046869671', 'https://openalex.org/W1983133367', 'https://openalex.org/W2053165762', 'https://openalex.org/W2031647436', 'https://openalex.org/W6680012447', 'https://openalex.org/W2079362249', 'https://openalex.org/W2078528584', 'https://openalex.org/W2064675550', 'https://openalex.org/W2115055618', 'https://openalex.org/W2069681747', 'https://openalex.org/W3143596294', 'https://openalex.org/W1974774036', 'https://openalex.org/W2135029798']",2015-04-01
https://openalex.org/W3182657421,https://doi.org/10.1109/cvpr46437.2021.01524,VisualVoice: Audio-Visual Speech Separation with Cross-Modal Consistency,"We introduce a new approach for audio-visual speech separation. Given a video, the goal is to extract the speech associated with a face in spite of simultaneous back-ground sounds and/or other human speakers. Whereas existing methods focus on learning the alignment between the speaker’s lip movements and the sounds they generate, we propose to leverage the speaker’s face appearance as an additional prior to isolate the corresponding vocal qualities they are likely to produce. Our approach jointly learns audio-visual speech separation and cross-modal speaker embeddings from unlabeled video. It yields state-of-the-art results on five benchmark datasets for audio-visual speech separation and enhancement, and generalizes well to challenging real-world videos of diverse scenarios. Our video results and code: http://vision.cs.utexas.edu/projects/VisualVoice/.","['https://openalex.org/W6668037159', 'https://openalex.org/W6784429643', 'https://openalex.org/W6749633790', 'https://openalex.org/W2141998673', 'https://openalex.org/W6696676114', 'https://openalex.org/W2981816492', 'https://openalex.org/W6752791795', 'https://openalex.org/W6751966083', 'https://openalex.org/W1897240248', 'https://openalex.org/W2971680695', 'https://openalex.org/W1987906574', 'https://openalex.org/W2460742184', 'https://openalex.org/W2221409856', 'https://openalex.org/W2194775991', 'https://openalex.org/W2029199293', 'https://openalex.org/W2120847449', 'https://openalex.org/W2031647436', 'https://openalex.org/W6783539077', 'https://openalex.org/W2788241093', 'https://openalex.org/W6684193366', 'https://openalex.org/W2660943524', 'https://openalex.org/W2016165338', 'https://openalex.org/W6714030504', 'https://openalex.org/W1552314771', 'https://openalex.org/W2593116425', 'https://openalex.org/W6639824700', 'https://openalex.org/W3034658206', 'https://openalex.org/W6684458083', 'https://openalex.org/W2963680395', 'https://openalex.org/W2081144555', 'https://openalex.org/W6684447540', 'https://openalex.org/W1504438288', 'https://openalex.org/W7071105756', 'https://openalex.org/W2964171275', 'https://openalex.org/W6754392867', 'https://openalex.org/W6675974536', 'https://openalex.org/W3017343282', 'https://openalex.org/W2963082324', 'https://openalex.org/W6749767719', 'https://openalex.org/W6776723300', 'https://openalex.org/W2988200020', 'https://openalex.org/W2982624843', 'https://openalex.org/W2963887950', 'https://openalex.org/W3015734344', 'https://openalex.org/W3143803406', 'https://openalex.org/W2644497536', 'https://openalex.org/W6725104640', 'https://openalex.org/W2964345931', 'https://openalex.org/W6750591037', 'https://openalex.org/W2979157532', 'https://openalex.org/W2147871138', 'https://openalex.org/W2726515241', 'https://openalex.org/W2019111214', 'https://openalex.org/W2734774145', 'https://openalex.org/W6685010520', 'https://openalex.org/W3038871978', 'https://openalex.org/W2808631503', 'https://openalex.org/W6777214184', 'https://openalex.org/W2963290645', 'https://openalex.org/W2950864153', 'https://openalex.org/W6750599028', 'https://openalex.org/W3023351797', 'https://openalex.org/W2963807156', 'https://openalex.org/W2143027228', 'https://openalex.org/W3096780661', 'https://openalex.org/W4289665794', 'https://openalex.org/W2981851635', 'https://openalex.org/W6782004394', 'https://openalex.org/W2972513594', 'https://openalex.org/W2619697695', 'https://openalex.org/W2886300652', 'https://openalex.org/W6729831399', 'https://openalex.org/W6750736135', 'https://openalex.org/W2088692057', 'https://openalex.org/W3016011581', 'https://openalex.org/W6774995033', 'https://openalex.org/W6754782314', 'https://openalex.org/W2962946126', 'https://openalex.org/W6753054903', 'https://openalex.org/W6781171973', 'https://openalex.org/W6753767121', 'https://openalex.org/W3105352633', 'https://openalex.org/W2809716557', 'https://openalex.org/W3108655859', 'https://openalex.org/W2098923380', 'https://openalex.org/W3127985485', 'https://openalex.org/W2962960500', 'https://openalex.org/W2168793898', 'https://openalex.org/W3035268204', 'https://openalex.org/W2962756039', 'https://openalex.org/W3162707322', 'https://openalex.org/W2511428026', 'https://openalex.org/W2106488367', 'https://openalex.org/W2883780447', 'https://openalex.org/W2168010845', 'https://openalex.org/W2168818958', 'https://openalex.org/W2890855852', 'https://openalex.org/W2963115079', 'https://openalex.org/W2946601169', 'https://openalex.org/W3123318516', 'https://openalex.org/W2796292145', 'https://openalex.org/W4293665662', 'https://openalex.org/W2964109005', 'https://openalex.org/W3096431533', 'https://openalex.org/W2962918445', 'https://openalex.org/W2315268655', 'https://openalex.org/W2963801643', 'https://openalex.org/W2407685581', 'https://openalex.org/W2951237705', 'https://openalex.org/W2792330714', 'https://openalex.org/W2796992393', 'https://openalex.org/W2187089797', 'https://openalex.org/W2164899449', 'https://openalex.org/W2890952074', 'https://openalex.org/W2069681747', 'https://openalex.org/W3092603041', 'https://openalex.org/W2950388022', 'https://openalex.org/W1901129140', 'https://openalex.org/W2962865004', 'https://openalex.org/W3116298410', 'https://openalex.org/W3024147341', 'https://openalex.org/W2963218389', 'https://openalex.org/W2883222475', 'https://openalex.org/W2291877678']",2021-06-01
https://openalex.org/W2141998673,https://doi.org/10.1109/tasl.2011.2114881,An Algorithm for Intelligibility Prediction of Time–Frequency Weighted Noisy Speech,"In the development process of noise-reduction algorithms, an objective machine-driven intelligibility measure which shows high correlation with speech intelligibility is of great interest. Besides reducing time and costs compared to real listening experiments, an objective intelligibility measure could also help provide answers on how to improve the intelligibility of noisy unprocessed speech. In this paper, a short-time objective intelligibility measure (STOI) is presented, which shows high correlation with the intelligibility of noisy and time-frequency weighted noisy speech (e.g., resulting from noise reduction) of three different listening experiments. In general, STOI showed better correlation with speech intelligibility compared to five other reference objective intelligibility models. In contrast to other conventional intelligibility models which tend to rely on global statistics across entire sentences, STOI is based on shorter time segments (386 ms). Experiments indeed show that it is beneficial to take segment lengths of this order into account. In addition, a free Matlab implementation is provided.","['https://openalex.org/W246614603', 'https://openalex.org/W4298982457', 'https://openalex.org/W1965213123', 'https://openalex.org/W1972489323', 'https://openalex.org/W2030680983', 'https://openalex.org/W1494634987', 'https://openalex.org/W1998648683', 'https://openalex.org/W2015512175', 'https://openalex.org/W6635827916', 'https://openalex.org/W2075012882', 'https://openalex.org/W2053313164', 'https://openalex.org/W2087126002', 'https://openalex.org/W2041777218', 'https://openalex.org/W1987831012', 'https://openalex.org/W1985226152', 'https://openalex.org/W2048151864', 'https://openalex.org/W2084458166', 'https://openalex.org/W4245919820', 'https://openalex.org/W2047068213', 'https://openalex.org/W2074658690', 'https://openalex.org/W2057889776', 'https://openalex.org/W1985029311', 'https://openalex.org/W1979099822', 'https://openalex.org/W3147539069', 'https://openalex.org/W2007790905', 'https://openalex.org/W2158336491', 'https://openalex.org/W2109215269', 'https://openalex.org/W2144404214', 'https://openalex.org/W2131753991', 'https://openalex.org/W2419243463', 'https://openalex.org/W1599328249', 'https://openalex.org/W2009543464', 'https://openalex.org/W2121973264', 'https://openalex.org/W2495580237', 'https://openalex.org/W1495679096']",2011-02-16
https://openalex.org/W4200483526,https://doi.org/10.1109/waspaa52581.2021.9632770,HiFi-GAN-2: Studio-Quality Speech Enhancement via Generative Adversarial Networks Conditioned on Acoustic Features,"Modern speech content creation tasks such as podcasts, video voice-overs, and audio books require studio-quality audio with full bandwidth and balanced equalization (EQ). These goals pose a challenge for conventional speech enhancement methods, which typically focus on removing significant acoustic degradation such as noise and reverb so as to improve speech clarity and intelligibility. We present HiFi-GAN-2, a waveform-to-waveform enhancement method that improves the quality of real-world consumer-grade recordings, with moderate noise, reverb and EQ distortion, to sound like studio recordings. HiFi-GAN-2 has three components. First, given a noisy reverberant recording as input, a recurrent network predicts the acoustic features (MFCCs) of a clean signal. Second, given the same noisy input, and conditioned on the MFCCs output by the first network, a feed-forward WaveNet (modeled via multidomain multi-scale adversarial training) generates a clean 16kHz signal. Third, a pre-trained bandwidth extension network generates the final 48kHz studio-quality signal from the 16kHz output of the second network. The complete pipeline is trained via simulation of noise, reverb and EQ added to studio-quality speech. Objective and subjective evaluations show that the proposed method outperforms state-of-the-art baselines on both conventional denoising as well as joint dereverberation and denoising tasks. Listening tests also show that our method achieves close to studio quality on real-world speech content (TED Talks and the VoxCeleb dataset).","['https://openalex.org/W3097934054', 'https://openalex.org/W2410879554', 'https://openalex.org/W1989314204', 'https://openalex.org/W2555915854', 'https://openalex.org/W2030931454', 'https://openalex.org/W3160652646', 'https://openalex.org/W2940807449', 'https://openalex.org/W2940385941', 'https://openalex.org/W6756251360', 'https://openalex.org/W3097945073', 'https://openalex.org/W3097034112', 'https://openalex.org/W2726515241', 'https://openalex.org/W6788379279', 'https://openalex.org/W2963321191', 'https://openalex.org/W6762114000', 'https://openalex.org/W2963341071', 'https://openalex.org/W2889335976', 'https://openalex.org/W2711335087', 'https://openalex.org/W6779126078', 'https://openalex.org/W6770232298', 'https://openalex.org/W3096408984', 'https://openalex.org/W2044893557', 'https://openalex.org/W2963103134', 'https://openalex.org/W6651792446', 'https://openalex.org/W2972785266', 'https://openalex.org/W3015626488', 'https://openalex.org/W3096159803', 'https://openalex.org/W6767111847', 'https://openalex.org/W3163296124', 'https://openalex.org/W2964243274', 'https://openalex.org/W2970006822', 'https://openalex.org/W4289242435', 'https://openalex.org/W3160085755', 'https://openalex.org/W3197042120', 'https://openalex.org/W2519091744', 'https://openalex.org/W2006129368', 'https://openalex.org/W4297789187', 'https://openalex.org/W2949558265', 'https://openalex.org/W2805669069', 'https://openalex.org/W2757519008', 'https://openalex.org/W2991361823']",2021-10-17
https://openalex.org/W2160473997,https://doi.org/10.1109/icassp.2011.5946971,CROWDMOS: An approach for crowdsourcing mean opinion score studies,"MOS (mean opinion score) subjective quality studies are used to evaluate many signal processing methods. Since laboratory quality studies are time consuming and expensive, researchers often run small studies with less statistical significance or use objective measures which only approximate human perception. We propose a cost-effective and convenient measure called crowdMOS, obtained by having internet users participate in a MOS-like listening study. Workers listen and rate sentences at their leisure, using their own hardware, in an environment of their choice. Since these individuals cannot be supervised, we propose methods for detecting and discarding inaccurate scores. To automate crowdMOS testing, we offer a set of freely distributable, open-source tools for Amazon Mechanical Turk, a platform designed to facilitate crowdsourcing. These tools implement the MOS testing methodology described in this pa per, providing researchers with a user-friendly means of performing subjective quality evaluations without the overhead associated with laboratory studies. Finally, we demonstrate the use of crowdMOS using data from the Blizzard text-to-speech competition, showing that it delivers accurate and repeatable results.","['https://openalex.org/W2024714418', 'https://openalex.org/W6677973343', 'https://openalex.org/W3106889297', 'https://openalex.org/W2150285586', 'https://openalex.org/W2170931744', 'https://openalex.org/W2151401338', 'https://openalex.org/W1969235627', 'https://openalex.org/W2917438849', 'https://openalex.org/W2119929864', 'https://openalex.org/W1995945562']",2011-05-01
https://openalex.org/W2141411743,https://doi.org/10.1109/icassp.2013.6639038,Ideal ratio mask estimation using deep neural networks for robust speech recognition,"We propose a feature enhancement algorithm to improve robust automatic speech recognition (ASR). The algorithm estimates a smoothed ideal ratio mask (IRM) in the Mel frequency domain using deep neural networks and a set of time-frequency unit level features that has previously been used to estimate the ideal binary mask. The estimated IRM is used to filter out noise from a noisy Mel spectrogram before performing cepstral feature extraction for ASR. On the noisy subset of the Aurora-4 robust ASR corpus, the proposed enhancement obtains a relative improvement of over 38% in terms of word error rates using ASR models trained in clean conditions, and an improvement of over 14% when the models are trained using the multi-condition training data. In terms of instantaneous SNR estimation performance, the proposed system obtains a mean absolute error of less than 4 dB in most frequency channels.","['https://openalex.org/W2027701650', 'https://openalex.org/W2130020166', 'https://openalex.org/W2138456300', 'https://openalex.org/W2086139506', 'https://openalex.org/W6603045223', 'https://openalex.org/W2034040413', 'https://openalex.org/W2150866759', 'https://openalex.org/W160800111', 'https://openalex.org/W6684924409', 'https://openalex.org/W1989364685', 'https://openalex.org/W2136922672', 'https://openalex.org/W1635512741', 'https://openalex.org/W2137075158', 'https://openalex.org/W4233392025', 'https://openalex.org/W2490695385', 'https://openalex.org/W2146083413', 'https://openalex.org/W2155445312', 'https://openalex.org/W2126597753', 'https://openalex.org/W2002342963', 'https://openalex.org/W2163648514', 'https://openalex.org/W4245919820', 'https://openalex.org/W1495679096', 'https://openalex.org/W2168379380', 'https://openalex.org/W2619993508', 'https://openalex.org/W2334833135', 'https://openalex.org/W2561557072', 'https://openalex.org/W75153181']",2013-05-01
https://openalex.org/W4224934174,https://doi.org/10.1109/icassp43922.2022.9747583,Audio Signal Processing for Telepresence Based on Wearable Array in Noisy and Dynamic Scenes,"Telepresence for virtual meetings has gained interest due to recent travel limitations and the new reality of working from home. However, current literature supporting real-world microphone arrays for realistic telepresence in audio is very limited. This paper investigates a scenario of a distant participant joining virtually a meeting between two dynamic participants. The audio signal processing chain (i) starts by recording using an array mounted on glasses, (ii) with initial processing providing direction-of-arrival estimation of a desired speaker using a direct-path dominance test robust to reverberation, combined with speaker separation for improved dynamic localization, (iii) followed by speech enhancement against interfering speakers and noise, (iv) and ends with applying binaural signal matching for headphone listening. This paper compares model-based processing to learning-based processing in both noisy and dynamic scenarios, and presents a novel processing using data from a real wearable array, studied by simulation and a listening test.","['https://openalex.org/W6782501926', 'https://openalex.org/W6797761920', 'https://openalex.org/W2552071709', 'https://openalex.org/W4249314718', 'https://openalex.org/W2893728996', 'https://openalex.org/W2905517431', 'https://openalex.org/W2117678320', 'https://openalex.org/W2221409856', 'https://openalex.org/W3095717210', 'https://openalex.org/W6632964863', 'https://openalex.org/W49710499', 'https://openalex.org/W3030781580', 'https://openalex.org/W2115366770', 'https://openalex.org/W2899838214', 'https://openalex.org/W2006155012', 'https://openalex.org/W6634605788', 'https://openalex.org/W2127096736', 'https://openalex.org/W3021261768', 'https://openalex.org/W4236344233', 'https://openalex.org/W3102937397', 'https://openalex.org/W3079310208', 'https://openalex.org/W1550164471', 'https://openalex.org/W1575206345', 'https://openalex.org/W3179026300']",2022-04-27
https://openalex.org/W3162420637,https://doi.org/10.1109/icassp39728.2021.9414721,"Cascaded Time + Time-Frequency Unet For Speech Enhancement: Jointly Addressing Clipping, Codec Distortions, And Gaps","Speech enhancement aims to improve speech quality by eliminating noise and distortions. While most speech enhancement methods address signal independent additive sources of noise, several degradations to speech signals are signal dependent and non-additive, like speech clipping, codec distortions, and gaps in speech. In this work, we first systematically study and achieve state of the art results on each of these three distortions individually. Next, we demonstrate a neural network pipeline that cascades a time domain convolutional neural network with a time-frequency domain convolutional neural network to address all three distortions jointly. We observe that such a cascade achieves good performance while also keeping the action of each neural network component interpretable.","['https://openalex.org/W3015278429', 'https://openalex.org/W2963453742', 'https://openalex.org/W2711335087', 'https://openalex.org/W2070126272', 'https://openalex.org/W3043119899', 'https://openalex.org/W6769194633', 'https://openalex.org/W2997007695', 'https://openalex.org/W3015780049', 'https://openalex.org/W2809824582', 'https://openalex.org/W2944209089', 'https://openalex.org/W3011573174', 'https://openalex.org/W2128653836', 'https://openalex.org/W2191779130', 'https://openalex.org/W2962946126', 'https://openalex.org/W3147539069', 'https://openalex.org/W6751512325', 'https://openalex.org/W2963341071', 'https://openalex.org/W2952218014', 'https://openalex.org/W4253928870', 'https://openalex.org/W6777814698', 'https://openalex.org/W2929274168', 'https://openalex.org/W6639824700', 'https://openalex.org/W3097906045', 'https://openalex.org/W6631190155', 'https://openalex.org/W181032131', 'https://openalex.org/W2121973264', 'https://openalex.org/W2322799359', 'https://openalex.org/W3025293551', 'https://openalex.org/W2964121744', 'https://openalex.org/W1495679096', 'https://openalex.org/W1522301498', 'https://openalex.org/W1901129140', 'https://openalex.org/W2963452667', 'https://openalex.org/W2981560594', 'https://openalex.org/W1586645951', 'https://openalex.org/W3099330747', 'https://openalex.org/W1510355813', 'https://openalex.org/W2394461504']",2021-05-13
https://openalex.org/W3162293946,https://doi.org/10.1109/icassp39728.2021.9414567,End-To-End Audio-Visual Speech Recognition with Conformers,"In this work, we present a hybrid CTC/Attention model based on a ResNet-18 and Convolution-augmented transformer (Conformer), that can be trained in an end-to-end manner. In particular, the audio and visual encoders learn to extract features directly from raw pixels and audio waveforms, respectively, which are then fed to conformers and then fusion takes place via a Multi-Layer Perceptron (MLP). The model learns to recognise characters using a combination of CTC and an attention mechanism. We show that end-to-end training, instead of using pre-computed visual features which is common in the literature, the use of a conformer, instead of a recurrent network, and the use of a transformer-based language model, significantly improve the performance of our model. We present results on the largest publicly available datasets for sentence-level speech recognition, Lip Reading Sentences 2 (LRS2) and Lip Reading Sentences 3 (LRS3), respectively. The results show that our proposed models raise the state-of-the-art performance by a large margin in audio-only, visual-only, and audio-visual experiments.","['https://openalex.org/W3035042697', 'https://openalex.org/W2766219058', 'https://openalex.org/W6739901393', 'https://openalex.org/W1974387177', 'https://openalex.org/W2996970093', 'https://openalex.org/W2981501041', 'https://openalex.org/W2963071736', 'https://openalex.org/W3015383493', 'https://openalex.org/W3097777922', 'https://openalex.org/W2194775991', 'https://openalex.org/W2943845043', 'https://openalex.org/W6780483730', 'https://openalex.org/W6677618333', 'https://openalex.org/W6631190155', 'https://openalex.org/W3097450720', 'https://openalex.org/W2972504708', 'https://openalex.org/W6781171973', 'https://openalex.org/W3006974783', 'https://openalex.org/W2963528589', 'https://openalex.org/W6732872814', 'https://openalex.org/W2972756321', 'https://openalex.org/W6754420807', 'https://openalex.org/W2015143272', 'https://openalex.org/W3011234510', 'https://openalex.org/W2551572271', 'https://openalex.org/W6731370813', 'https://openalex.org/W2964110616', 'https://openalex.org/W3015830103', 'https://openalex.org/W2121486117', 'https://openalex.org/W6754392867', 'https://openalex.org/W3016011581', 'https://openalex.org/W3015579786', 'https://openalex.org/W1494198834', 'https://openalex.org/W2963654155', 'https://openalex.org/W2936774411', 'https://openalex.org/W2096391593', 'https://openalex.org/W2963785710', 'https://openalex.org/W2891205112', 'https://openalex.org/W2890952074', 'https://openalex.org/W2115252128', 'https://openalex.org/W2567070169', 'https://openalex.org/W2963970792', 'https://openalex.org/W3106423890', 'https://openalex.org/W2578229578', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963403868', 'https://openalex.org/W2952746495', 'https://openalex.org/W4298112588', 'https://openalex.org/W3144810982', 'https://openalex.org/W2964121744', 'https://openalex.org/W1522301498', 'https://openalex.org/W3162707322']",2021-05-13
https://openalex.org/W2952218014,https://doi.org/10.1109/taslp.2019.2915167,Conv-TasNet: Surpassing Ideal Time–Frequency Magnitude Masking for Speech Separation,"Single-channel, speaker-independent speech separation methods have recently seen great progress. However, the accuracy, latency, and computational cost of such methods remain insufficient. The majority of the previous methods have formulated the separation problem through the time-frequency representation of the mixed signal, which has several drawbacks, including the decoupling of the phase and magnitude of the signal, the suboptimality of time-frequency representation for speech separation, and the long latency of the entire system. To address these shortcomings, we propose a fully-convolutional time-domain audio separation network (Conv-TasNet), a deep learning framework for end-to-end time-domain speech separation. Conv-TasNet uses a linear encoder to generate a representation of the speech waveform optimized for separating individual speakers. Speaker separation is achieved by applying a set of weighting functions (masks) to the encoder output. The modified encoder representations are then inverted back to the waveforms using a linear decoder. The masks are found using a temporal convolutional network (TCN) consisting of stacked 1-D dilated convolutional blocks, which allows the network to model the long-term dependencies of the speech signal while maintaining a small model size. The proposed Conv-TasNet system significantly outperforms previous time-frequency masking methods in separating two- and three-speaker mixtures. Additionally, Conv-TasNet surpasses several ideal time-frequency magnitude masks in two-speaker speech separation as evaluated by both objective distortion measures and subjective quality assessment by human listeners. Finally, Conv-TasNet has a significantly smaller model size and a much shorter minimum latency, making it a suitable solution for both offline and real-time speech separation applications. This study therefore represents a major step toward the realization of speech separation systems for real-world speech processing technologies.","['https://openalex.org/W2531409750', 'https://openalex.org/W6737664043', 'https://openalex.org/W6749825310', 'https://openalex.org/W2550143307', 'https://openalex.org/W1482149378', 'https://openalex.org/W2069681747', 'https://openalex.org/W2147455188', 'https://openalex.org/W160800111', 'https://openalex.org/W2516722836', 'https://openalex.org/W1528954144', 'https://openalex.org/W4234330420', 'https://openalex.org/W2568308529', 'https://openalex.org/W2168103112', 'https://openalex.org/W2963301902', 'https://openalex.org/W2163202748', 'https://openalex.org/W2892163332', 'https://openalex.org/W2510642588', 'https://openalex.org/W2405774341', 'https://openalex.org/W2962866211', 'https://openalex.org/W6751512325', 'https://openalex.org/W2963045393', 'https://openalex.org/W2962935966', 'https://openalex.org/W6639824700', 'https://openalex.org/W2963341071', 'https://openalex.org/W2889540509', 'https://openalex.org/W2221409856', 'https://openalex.org/W2800022361', 'https://openalex.org/W2166159048', 'https://openalex.org/W2963843276', 'https://openalex.org/W2144763279', 'https://openalex.org/W2079362249', 'https://openalex.org/W1994514225', 'https://openalex.org/W1980051803', 'https://openalex.org/W2079724265', 'https://openalex.org/W2096980176', 'https://openalex.org/W2161219071', 'https://openalex.org/W2891405874', 'https://openalex.org/W6750842128', 'https://openalex.org/W1677182931', 'https://openalex.org/W2800664709', 'https://openalex.org/W2120847449', 'https://openalex.org/W2401387233', 'https://openalex.org/W2552071709', 'https://openalex.org/W6746914816', 'https://openalex.org/W1485161427', 'https://openalex.org/W6713339159', 'https://openalex.org/W2963443859', 'https://openalex.org/W2044893557', 'https://openalex.org/W2078528584', 'https://openalex.org/W2460742184', 'https://openalex.org/W2962715207', 'https://openalex.org/W2558649592', 'https://openalex.org/W2734774145', 'https://openalex.org/W2735663686', 'https://openalex.org/W1552314771', 'https://openalex.org/W2963163009', 'https://openalex.org/W6780226713', 'https://openalex.org/W2127851351', 'https://openalex.org/W6631190155', 'https://openalex.org/W1522301498', 'https://openalex.org/W2229908198', 'https://openalex.org/W2949117887', 'https://openalex.org/W2614024025', 'https://openalex.org/W4297775537', 'https://openalex.org/W2963452667', 'https://openalex.org/W3124972797', 'https://openalex.org/W2799119527', 'https://openalex.org/W1994923416', 'https://openalex.org/W2962905190', 'https://openalex.org/W2774707525', 'https://openalex.org/W1901129140', 'https://openalex.org/W153881393', 'https://openalex.org/W2519091744', 'https://openalex.org/W2792764867', 'https://openalex.org/W2953333557', 'https://openalex.org/W1845880232', 'https://openalex.org/W2039057510', 'https://openalex.org/W285277413', 'https://openalex.org/W2612445135', 'https://openalex.org/W2403380333', 'https://openalex.org/W2139896607']",2019-05-07
https://openalex.org/W2962935966,https://doi.org/10.1109/icassp.2018.8462116,"TaSNet: Time-Domain Audio Separation Network for Real-Time, Single-Channel Speech Separation","Robust speech processing in multi-talker environments requires effective speech separation. Recent deep learning systems have made significant progress toward solving this problem, yet it remains challenging particularly in real-time, short latency applications. Most methods attempt to construct a mask for each source in time-frequency representation of the mixture signal which is not necessarily an optimal representation for speech separation. In addition, time-frequency decomposition results in inherent problems such as phase/magnitude decoupling and long time window which is required to achieve sufficient frequency resolution. We propose Time-domain Audio Separation Network (TasNet) to overcome these limitations. We directly model the signal in the time-domain using an encoder-decoder framework and perform the source separation on nonnegative encoder outputs. This method removes the frequency decomposition step and reduces the separation problem to estimation of source masks on encoder outputs which is then synthesized by the decoder. Our system outperforms the current state-of-the-art causal and noncausal speech separation algorithms, reduces the computational cost of speech separation, and significantly reduces the minimum required latency of the output. This makes TasNet suitable for applications where low-power, real-time implementation is desirable such as in hearable and telecommunication devices.","['https://openalex.org/W6712560600', 'https://openalex.org/W2508048623', 'https://openalex.org/W2963341071', 'https://openalex.org/W2163202748', 'https://openalex.org/W2168103112', 'https://openalex.org/W2189422931', 'https://openalex.org/W2072599882', 'https://openalex.org/W2093522043', 'https://openalex.org/W6742082877', 'https://openalex.org/W2734774145', 'https://openalex.org/W2127851351', 'https://openalex.org/W2962715207', 'https://openalex.org/W2735663686', 'https://openalex.org/W2558649592', 'https://openalex.org/W2291877678', 'https://openalex.org/W1482149378', 'https://openalex.org/W2304609584', 'https://openalex.org/W2552071709', 'https://openalex.org/W1790748249', 'https://openalex.org/W2519994964', 'https://openalex.org/W6731370813', 'https://openalex.org/W6698183232', 'https://openalex.org/W2296073425', 'https://openalex.org/W2304648132', 'https://openalex.org/W2742004888', 'https://openalex.org/W2567070169', 'https://openalex.org/W2584032004', 'https://openalex.org/W1522301498', 'https://openalex.org/W2302255633', 'https://openalex.org/W2398826216', 'https://openalex.org/W3124972797', 'https://openalex.org/W2519091744', 'https://openalex.org/W3124794156']",2018-04-01
https://openalex.org/W2593116425,https://doi.org/10.1109/icassp.2017.7952261,Audio Set: An ontology and human-labeled dataset for audio events,"Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets - principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 632 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers.","['https://openalex.org/W6632100227', 'https://openalex.org/W2038484192', 'https://openalex.org/W2074188409', 'https://openalex.org/W6675872639', 'https://openalex.org/W2086384421', 'https://openalex.org/W2566935005', 'https://openalex.org/W2068737686', 'https://openalex.org/W2117539524', 'https://openalex.org/W4236923591', 'https://openalex.org/W4319290960', 'https://openalex.org/W2081913879', 'https://openalex.org/W2008142581', 'https://openalex.org/W6674914833', 'https://openalex.org/W6684191040', 'https://openalex.org/W2032337854', 'https://openalex.org/W2081580037', 'https://openalex.org/W2526050071', 'https://openalex.org/W2210407171', 'https://openalex.org/W2194775991', 'https://openalex.org/W2508979362', 'https://openalex.org/W2097117768', 'https://openalex.org/W2126422864', 'https://openalex.org/W2163605009', 'https://openalex.org/W1535031652', 'https://openalex.org/W2105068979']",2017-03-01
https://openalex.org/W2107860279,https://doi.org/10.1109/pacrim.1993.407206,Mel-cepstral distance measure for objective speech quality assessment,"The author proposes a perceptually motivated modification to the cepstral distance measure (CD) based on the mel frequency scale and critical-band filtering. The new objective parameter is referred to as the mel cepstral distance (MCD). The author measures and compares the performance of the CD and MCD algorithms by applying them to a dataset representing low-bit-rate code-excited linear prediction (CELP)-coded speech with simulated channel conditions. The improvement in correlation with subjective DAM scores indicates that critical band filtering (and frequency warping) allows better modeling of perceived quality.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>","['https://openalex.org/W2013139519', 'https://openalex.org/W2146283888', 'https://openalex.org/W2148154194', 'https://openalex.org/W2163181067', 'https://openalex.org/W2479702386', 'https://openalex.org/W6726809291', 'https://openalex.org/W2078911760', 'https://openalex.org/W4285719527', 'https://openalex.org/W2519954739']",2002-12-30
https://openalex.org/W2891205112,https://doi.org/10.48550/arxiv.1809.00496,LRS3-TED: a large-scale dataset for visual speech recognition,"This paper introduces a new multi-modal dataset for visual and audio-visual speech recognition. It includes face tracks from over 400 hours of TED and TEDx videos, along with the corresponding subtitles and word alignment boundaries. The new dataset is substantially larger in scale compared to other public datasets that are available for general research.","['https://openalex.org/W2578229578', 'https://openalex.org/W2594690981', 'https://openalex.org/W2570575067', 'https://openalex.org/W2952746495', 'https://openalex.org/W2015143272', 'https://openalex.org/W2883383043', 'https://openalex.org/W3106250896']",2018-09-03
https://openalex.org/W3179026300,https://doi.org/10.48550/arxiv.2107.04174,EasyCom: An Augmented Reality Dataset to Support Algorithms for Easy Communication in Noisy Environments,"Augmented Reality (AR) as a platform has the potential to facilitate the reduction of the cocktail party effect. Future AR headsets could potentially leverage information from an array of sensors spanning many different modalities. Training and testing signal processing and machine learning algorithms on tasks such as beam-forming and speech enhancement require high quality representative data. To the best of the author's knowledge, as of publication there are no available datasets that contain synchronized egocentric multi-channel audio and video with dynamic movement and conversations in a noisy environment. In this work, we describe, evaluate and release a dataset that contains over 5 hours of multi-modal data useful for training and testing algorithms for the application of improving conversations for an AR glasses wearer. We provide speech intelligibility, quality and signal-to-noise ratio improvement results for a baseline method and show improvements across all tested metrics. The dataset we are releasing contains AR glasses egocentric multi-channel microphone array audio, wide field-of-view RGB video, speech source pose, headset microphone audio, annotated voice activity, speech transcriptions, head bounding boxes, target of speech and source identification labels. We have created and are releasing this dataset to facilitate research in multi-modal AR solutions to the cocktail party problem.","['https://openalex.org/W2092644348', 'https://openalex.org/W3124666641', 'https://openalex.org/W2750452568', 'https://openalex.org/W3032727804', 'https://openalex.org/W2975704529', 'https://openalex.org/W2964242760', 'https://openalex.org/W1578985305', 'https://openalex.org/W3034552680', 'https://openalex.org/W1991139021', 'https://openalex.org/W1582976041', 'https://openalex.org/W1728888090', 'https://openalex.org/W3121480429', 'https://openalex.org/W2808631503', 'https://openalex.org/W2053913299', 'https://openalex.org/W3016400019', 'https://openalex.org/W2141998673', 'https://openalex.org/W2516001803', 'https://openalex.org/W3103840368', 'https://openalex.org/W2046317813', 'https://openalex.org/W3104792420', 'https://openalex.org/W3087144913', 'https://openalex.org/W3037916678', 'https://openalex.org/W2039846947', 'https://openalex.org/W2101045344', 'https://openalex.org/W3037038648', 'https://openalex.org/W2127851351', 'https://openalex.org/W2964058413', 'https://openalex.org/W2796347433']",2021-07-09
https://openalex.org/W3035268204,,Voice Separation with an Unknown Number of Multiple Speakers,"We present a new method for separating a mixed audio sequence, in which multiple voices speak simultaneously. The new method employs gated neural networks that are trained to separate the voices at multiple processing steps, while maintaining the speaker in each output channel fixed. A different model is trained for every number of possible speakers, and the model with the largest number of speakers is employed to select the actual number of speakers in a given sample. Our method greatly outperforms the current state of the art, which, as we show, is not competitive for more than two speakers.",[],2020-07-12
https://openalex.org/W4221153068,https://doi.org/10.48550/arxiv.2201.02184,Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction,"Video recordings of speech contain correlated audio and visual information, providing a strong signal for speech representation learning from the speaker's lip movements and the produced sound. We introduce Audio-Visual Hidden Unit BERT (AV-HuBERT), a self-supervised representation learning framework for audio-visual speech, which masks multi-stream video input and predicts automatically discovered and iteratively refined multimodal hidden units. AV-HuBERT learns powerful audio-visual speech representation benefiting both lip-reading and automatic speech recognition. On the largest public lip-reading benchmark LRS3 (433 hours), AV-HuBERT achieves 32.5% WER with only 30 hours of labeled data, outperforming the former state-of-the-art approach (33.6%) trained with a thousand times more transcribed video data (31K hours). The lip-reading WER is further reduced to 26.9% when using all 433 hours of labeled data from LRS3 and combined with self-training. Using our audio-visual representation on the same benchmark for audio-only speech recognition leads to a 40% relative WER reduction over the state-of-the-art performance (1.3% vs 2.3%). Our code and models are available at https://github.com/facebookresearch/av_hubert",[],2022-01-05
https://openalex.org/W4285595742,https://doi.org/10.48550/arxiv.2207.07036,u-HuBERT: Unified Mixed-Modal Speech Pretraining And Zero-Shot Transfer to Unlabeled Modality,"While audio-visual speech models can yield superior performance and robustness compared to audio-only models, their development and adoption are hindered by the lack of labeled and unlabeled audio-visual data and the cost to deploy one model per modality. In this paper, we present u-HuBERT, a self-supervised pre-training framework that can leverage both multimodal and unimodal speech with a unified masked cluster prediction objective. By utilizing modality dropout during pre-training, we demonstrate that a single fine-tuned model can achieve performance on par or better than the state-of-the-art modality-specific models. Moreover, our model fine-tuned only on audio can perform well with audio-visual and visual speech input, achieving zero-shot modality generalization for multiple speech processing tasks. In particular, our single model yields 1.2%/1.4%/27.2% speech recognition word error rate on LRS3 with audio-visual/audio/visual input. Codes and models are available at https://github.com/facebookresearch/av_hubert",[],2022-07-14
https://openalex.org/W3203491020,https://doi.org/10.48550/arxiv.2109.13731,VoiceFixer: Toward General Speech Restoration with Neural Vocoder,"Speech restoration aims to remove distortions in speech signals. Prior methods mainly focus on single-task speech restoration (SSR), such as speech denoising or speech declipping. However, SSR systems only focus on one task and do not address the general speech restoration problem. In addition, previous SSR systems show limited performance in some speech restoration tasks such as speech super-resolution. To overcome those limitations, we propose a general speech restoration (GSR) task that attempts to remove multiple distortions simultaneously. Furthermore, we propose VoiceFixer, a generative framework to address the GSR task. VoiceFixer consists of an analysis stage and a synthesis stage to mimic the speech analysis and comprehension of the human auditory system. We employ a ResUNet to model the analysis stage and a neural vocoder to model the synthesis stage. We evaluate VoiceFixer with additive noise, room reverberation, low-resolution, and clipping distortions. Our baseline GSR model achieves a 0.499 higher mean opinion score (MOS) than the speech enhancement SSR model. VoiceFixer further surpasses the GSR baseline model on the MOS score by 0.256. Moreover, we observe that VoiceFixer generalizes well to severely degraded real speech recordings, indicating its potential in restoring old movies and historical speeches. The source code is available at https://github.com/haoheliu/voicefixer_main.","['https://openalex.org/W3197624376', 'https://openalex.org/W3094893721', 'https://openalex.org/W2100285470', 'https://openalex.org/W2471520273', 'https://openalex.org/W2998639482', 'https://openalex.org/W1924770834', 'https://openalex.org/W2515943672', 'https://openalex.org/W3097945073', 'https://openalex.org/W2757519008', 'https://openalex.org/W2141411743', 'https://openalex.org/W914169429', 'https://openalex.org/W2222562092', 'https://openalex.org/W3102195007', 'https://openalex.org/W2122336353', 'https://openalex.org/W2963300588', 'https://openalex.org/W3093990297', 'https://openalex.org/W2058079016', 'https://openalex.org/W2928550135', 'https://openalex.org/W2952903195', 'https://openalex.org/W2133665775', 'https://openalex.org/W2121388655', 'https://openalex.org/W1901129140', 'https://openalex.org/W2006129368', 'https://openalex.org/W2279914180', 'https://openalex.org/W3163296124', 'https://openalex.org/W2527729766', 'https://openalex.org/W2952218014', 'https://openalex.org/W3015338123', 'https://openalex.org/W3146413998', 'https://openalex.org/W2964243274', 'https://openalex.org/W2997007695', 'https://openalex.org/W2973231102', 'https://openalex.org/W2883935097', 'https://openalex.org/W2963341071', 'https://openalex.org/W2183868163', 'https://openalex.org/W79317937', 'https://openalex.org/W2071383468', 'https://openalex.org/W2153894152', 'https://openalex.org/W1964538581', 'https://openalex.org/W2981560594', 'https://openalex.org/W2127057679', 'https://openalex.org/W1997109056', 'https://openalex.org/W2964058413', 'https://openalex.org/W3199384252', 'https://openalex.org/W1517841224', 'https://openalex.org/W1512725166', 'https://openalex.org/W2739619458', 'https://openalex.org/W2970006822', 'https://openalex.org/W2559260703', 'https://openalex.org/W3098403858', 'https://openalex.org/W3109064156', 'https://openalex.org/W2587215352', 'https://openalex.org/W2972745527', 'https://openalex.org/W3043119899', 'https://openalex.org/W1495679096', 'https://openalex.org/W2147585306', 'https://openalex.org/W3131695311', 'https://openalex.org/W2946200149', 'https://openalex.org/W1989314204', 'https://openalex.org/W2949382160', 'https://openalex.org/W3163243746', 'https://openalex.org/W2943470767', 'https://openalex.org/W3046669506', 'https://openalex.org/W2609317876', 'https://openalex.org/W1539860142', 'https://openalex.org/W3125868443', 'https://openalex.org/W3035068567', 'https://openalex.org/W2911394281', 'https://openalex.org/W3183654341', 'https://openalex.org/W2890820256', 'https://openalex.org/W1552314771', 'https://openalex.org/W3123940584', 'https://openalex.org/W2902132730', 'https://openalex.org/W2971753973', 'https://openalex.org/W3094002217', 'https://openalex.org/W2972666141']",2021-09-28
https://openalex.org/W4296069328,https://doi.org/10.21437/interspeech.2022-10770,SVTS: Scalable Video-to-Speech Synthesis,"Video-to-speech synthesis (also known as lip-to-speech) refers to the translation of silent lip movements into the corresponding audio. This task has received an increasing amount of attention due to its self-supervised nature (i.e., can be trained without manual labelling) combined with the ever-growing collection of audio-visual data available online. Despite these strong motivations, contemporary video-to-speech works focus mainly on small- to medium-sized corpora with substantial constraints in both vocabulary and setting. In this work, we introduce a scalable video-to-speech framework consisting of two components: a video-to-spectrogram predictor and a pre-trained neural vocoder, which converts the mel-frequency spectrograms into waveform audio. We achieve state-of-the art results for GRID and considerably outperform previous approaches on LRW. More importantly, by focusing on spectrogram prediction using a simple feedforward model, we can efficiently and effectively scale our method to very large and unconstrained datasets: To the best of our knowledge, we are the first to show intelligible results on the challenging LRS3 dataset.","['https://openalex.org/W2963263347', 'https://openalex.org/W3211862173', 'https://openalex.org/W3160305627', 'https://openalex.org/W3161236344', 'https://openalex.org/W2152859600', 'https://openalex.org/W1494198834', 'https://openalex.org/W2808631503', 'https://openalex.org/W2293856338', 'https://openalex.org/W2963019222', 'https://openalex.org/W2963654155', 'https://openalex.org/W2908510526', 'https://openalex.org/W3162293946', 'https://openalex.org/W2120847449', 'https://openalex.org/W2949662773', 'https://openalex.org/W2964243274', 'https://openalex.org/W4220782092', 'https://openalex.org/W2726515241', 'https://openalex.org/W3035626590', 'https://openalex.org/W4206204999', 'https://openalex.org/W2141998673', 'https://openalex.org/W2194775991', 'https://openalex.org/W2964352155', 'https://openalex.org/W2519091744', 'https://openalex.org/W2891205112', 'https://openalex.org/W3097777922', 'https://openalex.org/W3144035034', 'https://openalex.org/W4221153068', 'https://openalex.org/W3015338123', 'https://openalex.org/W1552314771', 'https://openalex.org/W2972359262', 'https://openalex.org/W2029199293', 'https://openalex.org/W2625027024', 'https://openalex.org/W3096650361', 'https://openalex.org/W2550980560', 'https://openalex.org/W3213322812', 'https://openalex.org/W2585824449', 'https://openalex.org/W2972563022', 'https://openalex.org/W3034552680', 'https://openalex.org/W2808706139']",2022-09-16
https://openalex.org/W4281820413,https://doi.org/10.48550/arxiv.2206.03065,Universal Speech Enhancement with Score-based Diffusion,"Removing background noise from speech audio has been the subject of considerable effort, especially in recent years due to the rise of virtual communication and amateur recordings. Yet background noise is not the only unpleasant disturbance that can prevent intelligibility: reverb, clipping, codec artifacts, problematic equalization, limited bandwidth, or inconsistent loudness are equally disturbing and ubiquitous. In this work, we propose to consider the task of speech enhancement as a holistic endeavor, and present a universal speech enhancement system that tackles 55 different distortions at the same time. Our approach consists of a generative model that employs score-based diffusion, together with a multi-resolution conditioning network that performs enhancement with mixture density networks. We show that this approach significantly outperforms the state of the art in a subjective test performed by expert listeners. We also show that it achieves competitive objective scores with just 4-8 diffusion steps, despite not considering any particular strategy for fast sampling. We hope that both our methodology and technical contributions encourage researchers and practitioners to adopt a universal approach to speech enhancement, possibly framing it as a generative task.",[],2022-06-07
https://openalex.org/W3020336359,https://doi.org/10.21437/chime.2020-1,CHiME-6 Challenge: Tackling Multispeaker Speech Recognition for Unsegmented Recordings,"Following the success of the 1st, 2nd, 3rd, 4th and 5th CHiME challenges we organize the 6th CHiME Speech Separation and Recognition Challenge (CHiME-6). The new challenge revisits the previous CHiME-5 challenge and further considers the problem of distant multi-microphone conversational speech diarization and recognition in everyday home environments. Speech material is the same as the previous CHiME-5 recordings except for accurate array synchronization. The material was elicited using a dinner party scenario with efforts taken to capture data that is representative of natural conversational speech. This paper provides a baseline description of the CHiME-6 challenge for both segmented multispeaker speech recognition (Track 1) and unsegmented multispeaker speech recognition (Track 2). Of note, Track 2 is the first challenge activity in the community to tackle an unsegmented multispeaker speech recognition scenario with a complete set of reproducible open source baselines providing speech enhancement, speaker diarization, and speech recognition modules.",[],2020-05-04
https://openalex.org/W3157840621,https://doi.org/10.1109/tcyb.2022.3162495,End-to-End Video-to-Speech Synthesis Using Generative Adversarial Networks,"Video-to-speech is the process of reconstructing the audio speech from a video of a spoken utterance. Previous approaches to this task have relied on a two-step process where an intermediate representation is inferred from the video and is then decoded into waveform audio using a vocoder or a waveform reconstruction algorithm. In this work, we propose a new end-to-end video-to-speech model based on generative adversarial networks (GANs) which translates spoken video to waveform end-to-end without using any intermediate representation or separate waveform synthesis algorithm. Our model consists of an encoder-decoder architecture that receives raw video as input and generates speech, which is then fed to a waveform critic and a power critic. The use of an adversarial loss based on these two critics enables the direct synthesis of the raw audio waveform and ensures its realism. In addition, the use of our three comparative losses helps establish direct correspondence between the generated audio and the input video. We show that this model is able to reconstruct speech with remarkable realism for constrained datasets such as GRID, and that it is the first end-to-end model to produce intelligible speech for Lip Reading in the Wild (LRW), featuring hundreds of speakers recorded entirely ""in the wild."" We evaluate the generated samples in two different scenarios-seen and unseen speakers-using four objective metrics which measure the quality and intelligibility of artificial speech. We demonstrate that the proposed approach outperforms all previous works in most metrics on GRID and LRW.","['https://openalex.org/W2585824449', 'https://openalex.org/W2963609956', 'https://openalex.org/W2963019222', 'https://openalex.org/W2979894294', 'https://openalex.org/W6735913928', 'https://openalex.org/W2136155248', 'https://openalex.org/W6637568146', 'https://openalex.org/W3015338123', 'https://openalex.org/W2102737569', 'https://openalex.org/W2141998673', 'https://openalex.org/W6779337556', 'https://openalex.org/W2972563022', 'https://openalex.org/W6755257315', 'https://openalex.org/W2973068670', 'https://openalex.org/W6632208634', 'https://openalex.org/W2124397839', 'https://openalex.org/W2194775991', 'https://openalex.org/W2029199293', 'https://openalex.org/W2035777533', 'https://openalex.org/W2963528589', 'https://openalex.org/W6713645886', 'https://openalex.org/W3015213852', 'https://openalex.org/W2964243274', 'https://openalex.org/W6633117090', 'https://openalex.org/W2962912639', 'https://openalex.org/W2963654155', 'https://openalex.org/W3035626590', 'https://openalex.org/W3003241499', 'https://openalex.org/W2972756321', 'https://openalex.org/W2293856338', 'https://openalex.org/W2015143272', 'https://openalex.org/W2625027024', 'https://openalex.org/W6732872814', 'https://openalex.org/W6779669310', 'https://openalex.org/W6734491695', 'https://openalex.org/W2963066677', 'https://openalex.org/W2963192365', 'https://openalex.org/W2471520273', 'https://openalex.org/W6678815747', 'https://openalex.org/W2769810959', 'https://openalex.org/W2972504708', 'https://openalex.org/W349236604', 'https://openalex.org/W3096650361', 'https://openalex.org/W2290318471', 'https://openalex.org/W1526392145', 'https://openalex.org/W2964352155', 'https://openalex.org/W2973157397', 'https://openalex.org/W2964345931', 'https://openalex.org/W2168744723', 'https://openalex.org/W2107860279', 'https://openalex.org/W2887437849', 'https://openalex.org/W6767111847', 'https://openalex.org/W2963073614', 'https://openalex.org/W1995663108', 'https://openalex.org/W6677618333', 'https://openalex.org/W2910267084', 'https://openalex.org/W2963936489', 'https://openalex.org/W2973215447', 'https://openalex.org/W2981905048', 'https://openalex.org/W2159300614', 'https://openalex.org/W2963807156', 'https://openalex.org/W4298112588', 'https://openalex.org/W278779762', 'https://openalex.org/W4298289240', 'https://openalex.org/W4295521014', 'https://openalex.org/W4294619240', 'https://openalex.org/W2550980560', 'https://openalex.org/W1552314771', 'https://openalex.org/W2405756170', 'https://openalex.org/W2963782041', 'https://openalex.org/W2125389028', 'https://openalex.org/W2594690981', 'https://openalex.org/W4361745739', 'https://openalex.org/W4297817572', 'https://openalex.org/W3101481642', 'https://openalex.org/W2962879692', 'https://openalex.org/W2970006822', 'https://openalex.org/W1710476689', 'https://openalex.org/W3125708547', 'https://openalex.org/W2519091744', 'https://openalex.org/W2120847449', 'https://openalex.org/W1538323930', 'https://openalex.org/W4287761884', 'https://openalex.org/W2894295011', 'https://openalex.org/W2115252128']",2022-04-19
https://openalex.org/W2516001803,https://doi.org/10.1109/taslp.2016.2585878,An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated Noise Maskers,"Intelligibility listening tests are necessary during development and evaluation of speech processing algorithms, despite the fact that they are expensive and time consuming. In this paper, we propose a monaural intelligibility prediction algorithm, which has the potential of replacing some of these listening tests. The proposed algorithm shows similarities to the short-time objective intelligibility (STOI) algorithm, but works for a larger range of input signals. In contrast to STOI, extended STOI (ESTOI) does not assume mutual independence between frequency bands. ESTOI also incorporates spectral correlation by comparing complete 400ms length spectrograms of the noisy/processed speech and the clean speech signals. As a consequence, ESTOI is also able to accurately predict the intelligibility of speech contaminated by temporally highly modulated noise sources in addition to noisy signals processed with time-frequency weighting. We show that ESTOI can be interpreted in terms of an orthogonal decomposition of short-time spectrograms into intelligibility subspaces, i.e., a ranking of spectrogram features according to their importance to intelligibility. A free MATLAB implementation of the algorithm is available for noncommercial use at http://kom.aau.dk/~jje/.","['https://openalex.org/W1998648683', 'https://openalex.org/W1494634987', 'https://openalex.org/W2007790905', 'https://openalex.org/W1974387177', 'https://openalex.org/W2199505332', 'https://openalex.org/W2156488408', 'https://openalex.org/W2057889776', 'https://openalex.org/W2087126002', 'https://openalex.org/W2093346567', 'https://openalex.org/W2047068213', 'https://openalex.org/W2035101257', 'https://openalex.org/W2037122044', 'https://openalex.org/W2013630629', 'https://openalex.org/W1974778974', 'https://openalex.org/W2141998673', 'https://openalex.org/W4255712431', 'https://openalex.org/W2153038597', 'https://openalex.org/W2014365762', 'https://openalex.org/W2117416982', 'https://openalex.org/W2086300622', 'https://openalex.org/W1947936853', 'https://openalex.org/W2075012882', 'https://openalex.org/W2084458166', 'https://openalex.org/W1987831012', 'https://openalex.org/W2075076415', 'https://openalex.org/W1985784335', 'https://openalex.org/W1985029311', 'https://openalex.org/W1985226152', 'https://openalex.org/W2034593061', 'https://openalex.org/W2092644348', 'https://openalex.org/W6635827916', 'https://openalex.org/W2078239626', 'https://openalex.org/W2070154210', 'https://openalex.org/W2097137621', 'https://openalex.org/W2159373586', 'https://openalex.org/W6726739341', 'https://openalex.org/W1965172014', 'https://openalex.org/W1999686891', 'https://openalex.org/W2056800251', 'https://openalex.org/W3127686677', 'https://openalex.org/W2520567684', 'https://openalex.org/W2592831219', 'https://openalex.org/W1599328249', 'https://openalex.org/W4239339398', 'https://openalex.org/W4205687621']",2016-08-10
https://openalex.org/W2067295501,https://doi.org/10.1109/icassp.2010.5495701,A short-time objective intelligibility measure for time-frequency weighted noisy speech,"Existing objective speech-intelligibility measures are suitable for several types of degradation, however, it turns out that they are less appropriate for methods where noisy speech is processed by a time-frequency (TF) weighting, e.g., noise reduction and speech separation. In this paper, we present an objective intelligibility measure, which shows high correlation (rho=0.95) with the intelligibility of both noisy, and TF-weighted noisy speech. The proposed method shows significantly better performance than three other, more sophisticated, objective measures. Furthermore, it is based on an intermediate intelligibility measure for short-time (approximately 400 ms) TF-regions, and uses a simple DFT-based TF-decomposition. In addition, a free Matlab implementation is provided.","['https://openalex.org/W6717306377', 'https://openalex.org/W2109452115', 'https://openalex.org/W6629499117', 'https://openalex.org/W6635827916', 'https://openalex.org/W2087126002', 'https://openalex.org/W2007790905', 'https://openalex.org/W2041777218', 'https://openalex.org/W1987831012', 'https://openalex.org/W2084458166', 'https://openalex.org/W1985226152', 'https://openalex.org/W3147539069', 'https://openalex.org/W2047068213', 'https://openalex.org/W2074658690', 'https://openalex.org/W1985029311', 'https://openalex.org/W2057889776', 'https://openalex.org/W1494634987', 'https://openalex.org/W2121973264', 'https://openalex.org/W1599328249', 'https://openalex.org/W2419243463']",2010-03-01
https://openalex.org/W1552314771,https://doi.org/10.1109/icassp.2001.941023,Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs,"Previous objective speech quality assessment models, such as bark spectral distortion (BSD), the perceptual speech quality measure (PSQM), and measuring normalizing blocks (MNB), have been found to be suitable for assessing only a limited range of distortions. A new model has therefore been developed for use across a wider range of network conditions, including analogue connections, codecs, packet loss and variable delay. Known as perceptual evaluation of speech quality (PESQ), it is the result of integration of the perceptual analysis measurement system (PAMS) and PSQM99, an enhanced version of PSQM. PESQ is expected to become a new ITU-T recommendation P.862, replacing P.861 which specified PSQM and MNB.","['https://openalex.org/W6634720128', 'https://openalex.org/W6640681790', 'https://openalex.org/W6696014804', 'https://openalex.org/W6631714932', 'https://openalex.org/W2145756175', 'https://openalex.org/W2163181067', 'https://openalex.org/W1992979025', 'https://openalex.org/W1939939164', 'https://openalex.org/W1531715242', 'https://openalex.org/W1567057050', 'https://openalex.org/W2287109931', 'https://openalex.org/W1598621380', 'https://openalex.org/W1578752358', 'https://openalex.org/W1606487971']",2002-11-13
https://openalex.org/W3035626590,https://doi.org/10.1109/cvpr42600.2020.01381,Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis,"Humans involuntarily tend to infer parts of the conversation from lip movements when the speech is absent or corrupted by external noise. In this work, we explore the task of lip to speech synthesis, i.e., learning to generate natural speech given only the lip movements of a speaker. Acknowledging the importance of contextual and speaker-specific cues for accurate lip-reading, we take a different path from existing works. We focus on learning accurate lip sequences to speech mappings for individual speakers in unconstrained, large vocabulary settings. To this end, we collect and release a large-scale benchmark dataset, the first of its kind, specifically to train and evaluate the single-speaker lip to speech task in natural settings. We propose a novel approach with key design choices to achieve accurate, natural lip to speech synthesis in such unconstrained scenarios for the first time. Extensive evaluation using quantitative, qualitative metrics and human evaluation shows that our method is four times more intelligible than previous works in this space.","['https://openalex.org/W2790322981', 'https://openalex.org/W1522734439', 'https://openalex.org/W2130942839', 'https://openalex.org/W2777302760', 'https://openalex.org/W2585824449', 'https://openalex.org/W2750317406', 'https://openalex.org/W2619368999', 'https://openalex.org/W2120847449', 'https://openalex.org/W2029199293', 'https://openalex.org/W2194775991', 'https://openalex.org/W2050708608', 'https://openalex.org/W1836465849', 'https://openalex.org/W2516001803', 'https://openalex.org/W237546731', 'https://openalex.org/W7043748487', 'https://openalex.org/W2972852451', 'https://openalex.org/W2766960583', 'https://openalex.org/W2085052862', 'https://openalex.org/W2015143272', 'https://openalex.org/W2808195542', 'https://openalex.org/W2744766693', 'https://openalex.org/W2079402412', 'https://openalex.org/W2963936489', 'https://openalex.org/W2093378872', 'https://openalex.org/W2981353300', 'https://openalex.org/W2625027024', 'https://openalex.org/W2115502413', 'https://openalex.org/W2963307811', 'https://openalex.org/W2519091744', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963019222', 'https://openalex.org/W2972563022', 'https://openalex.org/W1983364832', 'https://openalex.org/W2963030892', 'https://openalex.org/W2067295501', 'https://openalex.org/W2981905048', 'https://openalex.org/W2964352155', 'https://openalex.org/W2267805933', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963403868', 'https://openalex.org/W2891205112', 'https://openalex.org/W1552314771', 'https://openalex.org/W2594690981', 'https://openalex.org/W2808706139', 'https://openalex.org/W2964171275', 'https://openalex.org/W2963712897', 'https://openalex.org/W2964243274', 'https://openalex.org/W2952746495', 'https://openalex.org/W2963432880', 'https://openalex.org/W1574462636', 'https://openalex.org/W2972359262', 'https://openalex.org/W1522301498', 'https://openalex.org/W2963691546', 'https://openalex.org/W2964283370', 'https://openalex.org/W2584505851']",2020-06-01
https://openalex.org/W4206204999,https://doi.org/10.23919/eusipco54536.2021.9616266,Speaker disentanglement in video-to-speech conversion,"The task of video-to-speech aims to translate silent video of lip movement to its corresponding audio signal. Previous approaches to this task are generally limited to the case of a single speaker, but a method that accounts for multiple speakers is desirable as it allows to (i) leverage datasets with multiple speakers or few samples per speaker; and (ii) control speaker identity at inference time. In this paper, we introduce a new video-to-speech architecture and explore ways of extending it to the multi-speaker scenario: we augment the network with an additional speaker-related input, through which we feed either a discrete identity or a speaker embedding. Interestingly, we observe that the visual encoder of the network is capable of learning the speaker identity from the lip region of the face alone. To better disentangle the two inputs-linguistic content and speaker identity-we add adversarial losses that dispel the identity from the video embeddings. To the best of our knowledge, the proposed method is the first to provide important functionalities such as (i) control of the target voice and (ii) speech synthesis for unseen identities over the state-of-the-art, while still maintaining the intelligibility of the spoken output.","['https://openalex.org/W2068056889', 'https://openalex.org/W2293856338', 'https://openalex.org/W2964404767', 'https://openalex.org/W3024559545', 'https://openalex.org/W2964283370', 'https://openalex.org/W6746700228', 'https://openalex.org/W2194775991', 'https://openalex.org/W6745317255', 'https://openalex.org/W2584505851', 'https://openalex.org/W6760057544', 'https://openalex.org/W6746173266', 'https://openalex.org/W6765987481', 'https://openalex.org/W2972563022', 'https://openalex.org/W2963010714', 'https://openalex.org/W6752888775', 'https://openalex.org/W2887437849', 'https://openalex.org/W6757660528', 'https://openalex.org/W6732945439', 'https://openalex.org/W6639480849', 'https://openalex.org/W2402146185', 'https://openalex.org/W2015143272', 'https://openalex.org/W6768158615', 'https://openalex.org/W6691770337', 'https://openalex.org/W2964243274', 'https://openalex.org/W2971905065', 'https://openalex.org/W1882958252', 'https://openalex.org/W2767052532', 'https://openalex.org/W2585824449', 'https://openalex.org/W2907262790', 'https://openalex.org/W2964352155', 'https://openalex.org/W2916104401', 'https://openalex.org/W2808706139', 'https://openalex.org/W3101481642', 'https://openalex.org/W2251321385', 'https://openalex.org/W3035626590', 'https://openalex.org/W2619368999']",2021-08-23
https://openalex.org/W2739009240,https://doi.org/10.23919/mva.2017.7986914,Visual-to-speech conversion based on maximum likelihood estimation,"This paper proposes a visual-to-speech conversion method that converts voiceless lip movements into voiced utterances without recognizing text information. Inspired by a Gaussian Mixture Model (GMM)-based voice conversion method, GMM is estimated from jointed visual and audio features and input visual features are converted to audio features using maximum likelihood estimation. In order to capture lip movements whose frame rate data is smaller than the audio data, we construct long-term image features. The proposed method has been evaluated using large-vocabulary continuous speech and experimental results show that our proposed method effectively estimates spectral envelopes and fundamental frequencies of audio speech from voiceless lip movements.","['https://openalex.org/W2155658135', 'https://openalex.org/W6678322406', 'https://openalex.org/W2152205330', 'https://openalex.org/W2156142001', 'https://openalex.org/W2343023855', 'https://openalex.org/W2401544731', 'https://openalex.org/W2120605154', 'https://openalex.org/W2022125261', 'https://openalex.org/W2015394094', 'https://openalex.org/W2047618352', 'https://openalex.org/W2121922650', 'https://openalex.org/W2550980560', 'https://openalex.org/W2949382160', 'https://openalex.org/W2519091744']",2017-05-01
https://openalex.org/W2625027024,https://doi.org/10.1109/taslp.2017.2716178,Generating Intelligible Audio Speech From Visual Speech,"This work is concerned with generating intelligible audio speech from a video of a person talking. Regression and classification methods are proposed first to estimate static spectral envelope features from active appearance model (AAM) visual features. Two further methods are then developed to incorporate temporal information into the prediction - a feature-level method using multiple frames and a model-level method based on recurrent neural networks. Speech excitation information is not available from the visual signal, so methods to artificially generate aperiodicity and fundamental frequency are developed. These are combined within the STRAIGHT vocoder to produce a speech signal. The various systems are optimised through objective tests before applying subjective intelligibility tests that determine a word accuracy of 85% from a set of human listeners on the GRID audio-visual speech database. This compares favourably with a previous regression-based system that serves as a baseline which achieved a word accuracy of 33%.","['https://openalex.org/W2142838865', 'https://openalex.org/W2152826865', 'https://openalex.org/W2038952578', 'https://openalex.org/W2113814270', 'https://openalex.org/W6677131165', 'https://openalex.org/W6674385629', 'https://openalex.org/W2160815625', 'https://openalex.org/W2539331707', 'https://openalex.org/W6639968709', 'https://openalex.org/W6605722544', 'https://openalex.org/W2157867825', 'https://openalex.org/W2059761048', 'https://openalex.org/W2008120082', 'https://openalex.org/W2025515167', 'https://openalex.org/W1423512047', 'https://openalex.org/W2129142580', 'https://openalex.org/W6606709985', 'https://openalex.org/W2164764235', 'https://openalex.org/W2090861223', 'https://openalex.org/W2131062138', 'https://openalex.org/W2516001803', 'https://openalex.org/W1552314771', 'https://openalex.org/W1869491686', 'https://openalex.org/W2121486117', 'https://openalex.org/W2170942820', 'https://openalex.org/W2096263221', 'https://openalex.org/W2267805933', 'https://openalex.org/W6765296696', 'https://openalex.org/W2137400100', 'https://openalex.org/W6712456248', 'https://openalex.org/W2293856338', 'https://openalex.org/W6654041629', 'https://openalex.org/W2168348268', 'https://openalex.org/W2039640471', 'https://openalex.org/W1999673299', 'https://openalex.org/W6632994029', 'https://openalex.org/W6606983177', 'https://openalex.org/W6605737493', 'https://openalex.org/W6694381704', 'https://openalex.org/W6697389695', 'https://openalex.org/W2154354834', 'https://openalex.org/W2015143272', 'https://openalex.org/W6696934422', 'https://openalex.org/W2143612262', 'https://openalex.org/W6711777497', 'https://openalex.org/W2005708641', 'https://openalex.org/W2395578248', 'https://openalex.org/W2295518984', 'https://openalex.org/W4300458848', 'https://openalex.org/W1554064406', 'https://openalex.org/W2116529913', 'https://openalex.org/W2097998348', 'https://openalex.org/W1536972981', 'https://openalex.org/W2952746495', 'https://openalex.org/W2293634267', 'https://openalex.org/W2276139887', 'https://openalex.org/W167578630', 'https://openalex.org/W169882265', 'https://openalex.org/W139257441', 'https://openalex.org/W2397008871', 'https://openalex.org/W2014240681', 'https://openalex.org/W142803501', 'https://openalex.org/W4285719527', 'https://openalex.org/W1908325895']",2017-06-15
https://openalex.org/W2887437849,https://doi.org/10.1145/3240508.3241911,Harnessing AI for Speech Reconstruction using Multi-view Silent Video Feed,"Speechreading or lipreading is the technique of understanding and getting\nphonetic features from a speaker's visual features such as movement of lips,\nface, teeth and tongue. It has a wide range of multimedia applications such as\nin surveillance, Internet telephony, and as an aid to a person with hearing\nimpairments. However, most of the work in speechreading has been limited to\ntext generation from silent videos. Recently, research has started venturing\ninto generating (audio) speech from silent video sequences but there have been\nno developments thus far in dealing with divergent views and poses of a\nspeaker. Thus although, we have multiple camera feeds for the speech of a user,\nbut we have failed in using these multiple video feeds for dealing with the\ndifferent poses. To this end, this paper presents the world's first ever\nmulti-view speech reading and reconstruction system. This work encompasses the\nboundaries of multimedia research by putting forth a model which leverages\nsilent video feeds from multiple cameras recording the same subject to generate\nintelligent speech for a speaker. Initial results confirm the usefulness of\nexploiting multiple camera views in building an efficient speech reading and\nreconstruction system. It further shows the optimal placement of cameras which\nwould lead to the maximum intelligibility of speech. Next, it lays out various\ninnovative applications for the proposed system focusing on its potential\nprodigious impact in not just security arena but in many other multimedia\nanalytics problems.\n","['https://openalex.org/W1526392145', 'https://openalex.org/W2002591263', 'https://openalex.org/W2015143272', 'https://openalex.org/W2963019222', 'https://openalex.org/W2108708552', 'https://openalex.org/W2009674825', 'https://openalex.org/W2016067832', 'https://openalex.org/W2763858995', 'https://openalex.org/W2168744723', 'https://openalex.org/W2078711706', 'https://openalex.org/W2004143321', 'https://openalex.org/W1535545184', 'https://openalex.org/W2164764235', 'https://openalex.org/W2076462394', 'https://openalex.org/W2158003960', 'https://openalex.org/W2964345931', 'https://openalex.org/W2138527036', 'https://openalex.org/W36949327', 'https://openalex.org/W2405713570', 'https://openalex.org/W3006531343', 'https://openalex.org/W1552314771', 'https://openalex.org/W2116777898', 'https://openalex.org/W4250413588', 'https://openalex.org/W2129082420', 'https://openalex.org/W2208724044', 'https://openalex.org/W2021279213', 'https://openalex.org/W1606487971', 'https://openalex.org/W2267805933', 'https://openalex.org/W2136155248', 'https://openalex.org/W2051676197', 'https://openalex.org/W2045956438']",2018-10-15
https://openalex.org/W3197659778,https://doi.org/10.21437/interspeech.2021-283,VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-Shot Voice Conversion,"One-shot voice conversion (VC), which performs conversion across arbitrary speakers with only a single target-speaker utterance for reference, can be effectively achieved by speech representation disentanglement.Existing work generally ignores the correlation between different speech representations during training, which causes leakage of content information into the speaker representation and thus degrades VC performance.To alleviate this issue, we employ vector quantization (VQ) for content encoding and introduce mutual information (MI) as the correlation metric during training, to achieve proper disentanglement of content, speaker and pitch representations, by reducing their inter-dependencies in an unsupervised manner.Experimental results reflect the superiority of the proposed method in learning effective disentangled speech representations for retaining source linguistic content and intonation variations, while capturing target speaker characteristics.In doing so, the proposed approach achieves higher speech naturalness and speaker similarity than current state-of-the-art one-shot VC systems.Our code, pre-trained models and demo are available at https://github.com/Wendison/VQMIVC.","['https://openalex.org/W3015338123', 'https://openalex.org/W2962896155', 'https://openalex.org/W3015805741', 'https://openalex.org/W3097256596', 'https://openalex.org/W2152790380', 'https://openalex.org/W1522301498', 'https://openalex.org/W2124641009', 'https://openalex.org/W2105160541', 'https://openalex.org/W2972894903', 'https://openalex.org/W2973154337', 'https://openalex.org/W2963539064', 'https://openalex.org/W4297808394', 'https://openalex.org/W1862426464', 'https://openalex.org/W3034794073', 'https://openalex.org/W2803832867', 'https://openalex.org/W3126283728', 'https://openalex.org/W4235132546', 'https://openalex.org/W3125709657', 'https://openalex.org/W2979476256', 'https://openalex.org/W2527729766', 'https://openalex.org/W2774848319', 'https://openalex.org/W2888922217', 'https://openalex.org/W3096524539', 'https://openalex.org/W3163475957', 'https://openalex.org/W2945478979', 'https://openalex.org/W2972659941', 'https://openalex.org/W3095361818', 'https://openalex.org/W3015212100', 'https://openalex.org/W3036928441', 'https://openalex.org/W2576309025', 'https://openalex.org/W2120605154', 'https://openalex.org/W2166944917', 'https://openalex.org/W2518172956', 'https://openalex.org/W2973215447', 'https://openalex.org/W2161476805', 'https://openalex.org/W2787748320']",2021-08-27
https://openalex.org/W2115252128,https://doi.org/10.5555/1577069.1755843,Dlib-ml: A Machine Learning Toolkit,"There are many excellent toolkits which provide support for developing machine learning software in Python, R, Matlab, and similar environments. Dlib-ml is an open source library, targeted at both engineers and research scientists, which aims to provide a similarly rich environment for developing machine learning software in the C++ language. Towards this end, dlib-ml contains an extensible linear algebra toolkit with built in BLAS support. It also houses implementations of algorithms for performing inference in Bayesian networks and kernel-based methods for classification, regression, clustering, anomaly detection, and feature ranking. To enable easy use of these tools, the entire library has been developed with contract programming, which provides complete and precise documentation as well as powerful debugging tools.","['https://openalex.org/W2153290280', 'https://openalex.org/W1633751774', 'https://openalex.org/W2125993116', 'https://openalex.org/W1618905105', 'https://openalex.org/W1486089539', 'https://openalex.org/W2142623206', 'https://openalex.org/W2154462399', 'https://openalex.org/W2153635508', 'https://openalex.org/W1494068061']",2009-12-01
https://openalex.org/W2293856338,https://doi.org/10.21437/interspeech.2015-139,Reconstructing intelligible audio speech from visual speech features,"This work describes an investigation into the feasibility of producing intelligible audio speech from only visual speech fea- tures. The proposed method aims to estimate a spectral enve- lope from visual features which is then combined with an arti- ficial excitation signal and used within a model of speech pro- duction to reconstruct an audio signal. Different combinations of audio and visual features are considered, along with both a statistical method of estimation and a deep neural network. The intelligibility of the reconstructed audio speech is measured by human listeners, and then compared to the intelligibility of the video signal only and when combined with the reconstructed audio.","['https://openalex.org/W2015143272', 'https://openalex.org/W3143833948', 'https://openalex.org/W2062227835', 'https://openalex.org/W2121430128', 'https://openalex.org/W2049686551', 'https://openalex.org/W2018651852', 'https://openalex.org/W2160815625', 'https://openalex.org/W2104263160', 'https://openalex.org/W2129082420', 'https://openalex.org/W2038010270', 'https://openalex.org/W2069238044', 'https://openalex.org/W2164764235', 'https://openalex.org/W2165856656', 'https://openalex.org/W1875231349', 'https://openalex.org/W2127211243', 'https://openalex.org/W1999673299', 'https://openalex.org/W2116529913', 'https://openalex.org/W2172803778', 'https://openalex.org/W1588594383', 'https://openalex.org/W2397008871', 'https://openalex.org/W2143908786', 'https://openalex.org/W142803501', 'https://openalex.org/W1554064406']",2015-09-06
https://openalex.org/W3163573274,https://doi.org/10.1109/icassp39728.2021.9413391,Seen and Unseen Emotional Style Transfer for Voice Conversion with A New Emotional Speech Dataset,"Emotional voice conversion aims to transform emotional prosody in speech while preserving the linguistic content and speaker identity. Prior studies show that it is possible to disentangle emotional prosody using an encoder-decoder network conditioned on discrete representation, such as one-hot emotion labels. Such networks learn to remember a fixed set of emotional styles. In this paper, we propose a novel framework based on variational auto-encoding Wasserstein generative adversarial network (VAW-GAN), which makes use of a pre-trained speech emotion recognition (SER) model to transfer emotional style during training and at run-time inference. In this way, the network is able to transfer both seen and unseen emotional style to a new utterance. We show that the proposed framework achieves remarkable performance by consistently outperforming the baseline framework. This paper also marks the release of an emotional speech dataset (ESD) for voice conversion, which has multiple speakers and languages.","['https://openalex.org/W6781784828', 'https://openalex.org/W3015841875', 'https://openalex.org/W6773966927', 'https://openalex.org/W2471520273', 'https://openalex.org/W2885005742', 'https://openalex.org/W2146334809', 'https://openalex.org/W3008297462', 'https://openalex.org/W6683855838', 'https://openalex.org/W2040587156', 'https://openalex.org/W2511640485', 'https://openalex.org/W2517513811', 'https://openalex.org/W2938833595', 'https://openalex.org/W2883743124', 'https://openalex.org/W3025680351', 'https://openalex.org/W3095169545', 'https://openalex.org/W2899361462', 'https://openalex.org/W2149628368', 'https://openalex.org/W3098557217', 'https://openalex.org/W1966797434', 'https://openalex.org/W3016151052', 'https://openalex.org/W2105160541', 'https://openalex.org/W2937154351', 'https://openalex.org/W2120605154', 'https://openalex.org/W2086796102', 'https://openalex.org/W2941094131', 'https://openalex.org/W6711854987', 'https://openalex.org/W6634507583', 'https://openalex.org/W3096939667', 'https://openalex.org/W2759925408', 'https://openalex.org/W2962896155', 'https://openalex.org/W2795109282', 'https://openalex.org/W6750489868', 'https://openalex.org/W6772230580', 'https://openalex.org/W3014201970', 'https://openalex.org/W2161736993', 'https://openalex.org/W1581458799', 'https://openalex.org/W2187089797', 'https://openalex.org/W3044380931', 'https://openalex.org/W3168542456', 'https://openalex.org/W2080119116', 'https://openalex.org/W4295731579', 'https://openalex.org/W2319003035', 'https://openalex.org/W2714549561', 'https://openalex.org/W2963272440', 'https://openalex.org/W2998249245', 'https://openalex.org/W2963927338', 'https://openalex.org/W2396025094', 'https://openalex.org/W3047107405', 'https://openalex.org/W3006108364', 'https://openalex.org/W2608338293', 'https://openalex.org/W2794490148']",2021-05-13
https://openalex.org/W2063041593,https://doi.org/10.1121/1.400726,Virtual pitch and phase sensitivity of a computer model of the auditory periphery. II: Phase sensitivity,"In a companion article [Meddis and Hewitt, J. Acoust. Soc. Am. 89, 2866–2882 (1991)] it was shown that a computational model of the auditory periphery followed by a system of autocorrelation analyses was able to account for a wide range of human virtual pitch perception phenomena. In this article it is shown that the same model, with no substantial modification, can predict a number of results concerning human sensitivity to phase relationships among harmonic components of tone complexes. The model is successfully evaluated using (a) amplitude-modulated and quasifrequency-modulated stimuli, (b) harmonic complexes with alternating phase change and monotonic phase change across harmonic components, and (c) mistuned harmonics. The model is contrasted with phase-insensitive theories of low-level auditory processing and offered as further evidence in favor of the value of analysing time intervals among spikes in the auditory nerve when explaining psychophysical phenomena.","['https://openalex.org/W1972835707', 'https://openalex.org/W2044697983', 'https://openalex.org/W2002525568', 'https://openalex.org/W1966948181', 'https://openalex.org/W2033383069', 'https://openalex.org/W2013020033', 'https://openalex.org/W2028078738', 'https://openalex.org/W2083145261', 'https://openalex.org/W2048967079', 'https://openalex.org/W2096408429', 'https://openalex.org/W2018637815', 'https://openalex.org/W2059260101', 'https://openalex.org/W2049384489', 'https://openalex.org/W1989330747', 'https://openalex.org/W1971146038', 'https://openalex.org/W2304226179']",1991-06-01
https://openalex.org/W4372340947,https://doi.org/10.1109/icassp49357.2023.10096626,Wespeaker: A Research and Production Oriented Speaker Embedding Learning Toolkit,"Speaker modeling is essential for many related tasks, such as speaker recognition and speaker diarization. The dominant modeling approach is fixed-dimensional vector representation, i.e., speaker embedding. This paper introduces a research and production oriented speaker embedding learning toolkit, Wespeaker. Wespeaker contains the implementation of scalable data management, state-of-the-art speaker embedding models, loss functions, and scoring back-ends, with highly competitive results achieved by structured recipes which were adopted in the winning systems in several speaker verification challenges. The application to other downstream tasks such as speaker diarization is also exhibited in the related recipe. Moreover, CPU- and GPU-compatible deployment codes are integrated for production-oriented development. The toolkit is publicly available at https://github.com/wenet-e2e/wespeaker.","['https://openalex.org/W6771538394', 'https://openalex.org/W4221167707', 'https://openalex.org/W6809668307', 'https://openalex.org/W6787439801', 'https://openalex.org/W2981087920', 'https://openalex.org/W3197478142', 'https://openalex.org/W3038871978', 'https://openalex.org/W2962780374', 'https://openalex.org/W4206908380', 'https://openalex.org/W2890964092', 'https://openalex.org/W2046056978', 'https://openalex.org/W2936774411', 'https://openalex.org/W3163421828', 'https://openalex.org/W6839093550', 'https://openalex.org/W3006889321', 'https://openalex.org/W2784163702', 'https://openalex.org/W2888968865', 'https://openalex.org/W3094374485', 'https://openalex.org/W6748010250', 'https://openalex.org/W6753575415', 'https://openalex.org/W2963466847', 'https://openalex.org/W3010925296', 'https://openalex.org/W6801723603', 'https://openalex.org/W6743579824', 'https://openalex.org/W6694517276', 'https://openalex.org/W6766978945', 'https://openalex.org/W3167533889', 'https://openalex.org/W3024869864', 'https://openalex.org/W6769178842', 'https://openalex.org/W6631362777', 'https://openalex.org/W6603616073', 'https://openalex.org/W4283458492', 'https://openalex.org/W2991951409', 'https://openalex.org/W4289750118', 'https://openalex.org/W2969985801', 'https://openalex.org/W3111481301', 'https://openalex.org/W2271840356', 'https://openalex.org/W4295312788', 'https://openalex.org/W1524333225', 'https://openalex.org/W2747165665', 'https://openalex.org/W4288091954', 'https://openalex.org/W2219249508', 'https://openalex.org/W4221167533', 'https://openalex.org/W3103152812', 'https://openalex.org/W3198698812']",2023-05-05
https://openalex.org/W2088632109,https://doi.org/10.1109/tassp.1976.1162846,A comparative performance study of several pitch detection algorithms,"A comparative performance study of seven pitch detection algorithms was conducted. A speech data base, consisting of eight utterances spoken by three males, three females, and one child was constructed. Telephone, close talking microphone, and wideband recordings were made of each of the utterances. For each of the utterances in the data base; a ""standard"" pitch contour was semiautomatically measured using a highly sophisticated interactive pitch detection program. The ""standard"" pitch contour was then compared with the pitch contour that was obtained from each of the seven programmed pitch detectors. The algorithms used in this study were 1) a center clipping, infinite-peak clipping, modified autocorrelation method (AUTOC), 2) the cepstral method (CEP), 3) the simplified inverse filtering technique (SIFT) method, 4) the parallel processing time-domain method (PPROC), 5) the data reduction method (DARD), 6) a spectral flattening linear predictive coding (LPC) method, and 7) the average magnitude difference function (AMDF) method. A set of measurements was made on the pitch contours to quantify the various types of errors which occur in each of the above methods. Included among the error measurements were the average and standard deviation of the error in pitch period during voiced regions, the number of gross errors in the pitch period, and the average number of voiced-unvoiced classification errors. For each of the error measurements, the individual pitch detectors could be rank ordered as a measure of their relative performance as a function of recording condition, and pitch range of the various speakers. Performance scores are presented for each of the seven pitch detectors based on each of the categories of error.","['https://openalex.org/W2171321863', 'https://openalex.org/W2108771579', 'https://openalex.org/W2019183077', 'https://openalex.org/W2125062731', 'https://openalex.org/W2022554507', 'https://openalex.org/W2135933868', 'https://openalex.org/W2049377439', 'https://openalex.org/W2019977573', 'https://openalex.org/W2111132004', 'https://openalex.org/W1536990986', 'https://openalex.org/W2105113261', 'https://openalex.org/W2071914178', 'https://openalex.org/W2084044763', 'https://openalex.org/W2157225271', 'https://openalex.org/W2143039550', 'https://openalex.org/W2162256359', 'https://openalex.org/W1995966271', 'https://openalex.org/W2087038535', 'https://openalex.org/W2088632109', 'https://openalex.org/W2184798135']",1976-10-01
https://openalex.org/W2085628288,https://doi.org/10.1109/icassp.2014.6854049,A pitch extraction algorithm tuned for automatic speech recognition,"In this paper we present an algorithm that produces pitch and probability-of-voicing estimates for use as features in automatic speech recognition systems. These features give large performance improvements on tonal languages for ASR systems, and even substantial improvements for non-tonal languages. Our method, which we are calling the Kaldi pitch tracker (because we are adding it to the Kaldi ASR toolkit), is a highly modified version of the getf0 (RAPT) algorithm. Unlike the original getf0 we do not make a hard decision whether any given frame is voiced or unvoiced; instead, we assign a pitch even to unvoiced frames while constraining the pitch trajectory to be continuous. Our algorithm also produces a quantity that can be used as a probability of voicing measure; it is based on the normalized autocorrelation measure that our pitch extractor uses. We present results on data from various languages in the BABEL project, and show a large improvement over systems without tonal features and systems where pitch and POV information was obtained from SAcC or getf0.","['https://openalex.org/W2125234026', 'https://openalex.org/W6679097437', 'https://openalex.org/W6718561954', 'https://openalex.org/W1981706894', 'https://openalex.org/W2091425152', 'https://openalex.org/W6631362777', 'https://openalex.org/W2089537772', 'https://openalex.org/W3141523618', 'https://openalex.org/W1975079546', 'https://openalex.org/W2119599673', 'https://openalex.org/W2135431242', 'https://openalex.org/W23669922', 'https://openalex.org/W1524333225', 'https://openalex.org/W2442329935', 'https://openalex.org/W1571901760', 'https://openalex.org/W2129070077', 'https://openalex.org/W2115098197']",2014-05-01
https://openalex.org/W4285301843,https://doi.org/10.1109/taslp.2022.3171971,Neural Fusion for Voice Cloning,"Voice cloning is a technique to build text-to-speech applications for individuals. When only very limited training data is available, it is challenging to preserve both high speech quality and high speaker similarity. We propose a neural fusion architecture to incorporate a unit concatenation method into a parametric text-to-speech model to address this issue. Unlike the hybrid unit concatenation system, the proposed fusion architecture is still an end-to-end neural network model. It consists of a text encoder, an acoustic decoder, and a phoneme-level reference encoder. The reference encoder extracts phoneme-level embeddings corresponding to the cloning audio segments, and the text encoder infers phoneme-level embeddings from the input text. One of the two embeddings is then selected and sent to the decoder. We use auto-regressive distribution modeling and decoder refinement after the selection stage to overcome the concatenation discontinuity problem. Experimental results show that the neural fusion system significantly improves the speaker similarity using the selected units with the highest probability. The speech naturalness remains similar to the directly decoded systems.","['https://openalex.org/W3162794600', 'https://openalex.org/W3150572638', 'https://openalex.org/W3097892637', 'https://openalex.org/W6640963894', 'https://openalex.org/W2962691331', 'https://openalex.org/W6750489868', 'https://openalex.org/W6605232188', 'https://openalex.org/W1984905644', 'https://openalex.org/W4235154690', 'https://openalex.org/W2746498480', 'https://openalex.org/W3095035471', 'https://openalex.org/W2752796333', 'https://openalex.org/W6762931180', 'https://openalex.org/W1901129140', 'https://openalex.org/W6790220310', 'https://openalex.org/W3095990227', 'https://openalex.org/W6748588790', 'https://openalex.org/W2907262790', 'https://openalex.org/W3097728852', 'https://openalex.org/W3197216873', 'https://openalex.org/W6682594620', 'https://openalex.org/W2165848216', 'https://openalex.org/W1854592333', 'https://openalex.org/W2406990556', 'https://openalex.org/W2747681982', 'https://openalex.org/W3015607139', 'https://openalex.org/W3173323807', 'https://openalex.org/W6778823374', 'https://openalex.org/W3162771752', 'https://openalex.org/W2964098179', 'https://openalex.org/W2972359262', 'https://openalex.org/W2471520273', 'https://openalex.org/W2191779130', 'https://openalex.org/W6631362777', 'https://openalex.org/W6917585676', 'https://openalex.org/W2791686384', 'https://openalex.org/W2592497314']",2022-01-01
https://openalex.org/W3097728852,https://doi.org/10.21437/interspeech.2020-2530,Data Efficient Voice Cloning from Noisy Samples with Domain Adversarial Training,"Data efficient voice cloning aims at synthesizing target speaker's voice with only a few enrollment samples at hand.To this end, speaker adaptation and speaker encoding are two typical methods based on base model trained from multiple speakers.The former uses a small set of target speaker data to transfer the multi-speaker model to target speaker's voice through direct model update, while in the latter, only a few seconds of target speaker's audio directly goes through an extra speaker encoding model along with the multi-speaker model to synthesize target speaker's voice without model update.Nevertheless, the two methods need clean target speaker data.However, the samples provided by user may inevitably contain acoustic noise in real applications.It's still challenging to generating target voice with noisy data.In this paper, we study the data efficient voice cloning problem from noisy samples under the sequenceto-sequence based TTS paradigm.Specifically, we introduce domain adversarial training (DAT) to speaker adaptation and speaker encoding, which aims to disentangle noise from speechnoise mixture.Experiments show that for both speaker adaptation and encoding, the proposed approaches can consistently synthesize clean speech from noisy speaker samples, apparently outperforming the method adopting state-of-the-art speech enhancement module.","['https://openalex.org/W2963091184', 'https://openalex.org/W2467604901', 'https://openalex.org/W2574456902', 'https://openalex.org/W2892620417', 'https://openalex.org/W3015922793', 'https://openalex.org/W2801554275', 'https://openalex.org/W2559260703', 'https://openalex.org/W3016159759', 'https://openalex.org/W3016008406', 'https://openalex.org/W2949756029', 'https://openalex.org/W4288102668', 'https://openalex.org/W2187089797', 'https://openalex.org/W2907262790', 'https://openalex.org/W2963609956', 'https://openalex.org/W2801581493', 'https://openalex.org/W2973099347', 'https://openalex.org/W2890964092', 'https://openalex.org/W2808706139', 'https://openalex.org/W2769025471', 'https://openalex.org/W3104309246', 'https://openalex.org/W2788357188', 'https://openalex.org/W2964243274', 'https://openalex.org/W2587088898', 'https://openalex.org/W2972440097', 'https://openalex.org/W4298857617', 'https://openalex.org/W4320013936', 'https://openalex.org/W2962684181']",2020-10-25
https://openalex.org/W3016137096,https://doi.org/10.1109/icassp40776.2020.9054337,Improving Prosody with Linguistic and Bert Derived Features in Multi-Speaker Based Mandarin Chinese Neural TTS,"Recent advances of neural TTS have made ""human parity"" synthesized speech possible when a large amount of studio-quality training data from a voice talent is available. However, with only limited, casual recordings from an ordinary speaker, human-like TTS is still a big challenge, in addition to other artifacts like incomplete sentences, repetition of words, etc. Chinese, a language, of which the text is different from that of other roman-letter based languages like English, has no blank space between adjacent words, hence word segmentation errors can cause serious semantic confusions and unnatural prosody. In this study, with a multi-speaker TTS to accommodate the insufficient training data of a target speaker, we investigate linguistic features and Bert-derived information to improve the prosody of our Mandarin Chinese TTS. Three factors are studied: phone-related and prosody-related linguistic features; better predicted breaks with a refined Bert-CRF model; augmented phoneme sequence with character embedding derived from a Bert model. Subjective tests on in- and out-domain tasks of News, Chat and Audiobook, have shown that all factors are effective for improving prosody of our Mandarin TTS. The model with additional character embeddings from Bert is the best one, which outperforms the baseline by 0.17 MOS gain.","['https://openalex.org/W2515943672', 'https://openalex.org/W2963609956', 'https://openalex.org/W6738277540', 'https://openalex.org/W2749651610', 'https://openalex.org/W2964243274', 'https://openalex.org/W6752888775', 'https://openalex.org/W6757322325', 'https://openalex.org/W2936103087', 'https://openalex.org/W6755207826', 'https://openalex.org/W2973217961', 'https://openalex.org/W2043003570', 'https://openalex.org/W6675380101', 'https://openalex.org/W6756197946', 'https://openalex.org/W6734815144', 'https://openalex.org/W6623517193', 'https://openalex.org/W6679434410', 'https://openalex.org/W3142087749', 'https://openalex.org/W2327501763', 'https://openalex.org/W2952127920', 'https://openalex.org/W2962936105', 'https://openalex.org/W2985056549', 'https://openalex.org/W6768248635', 'https://openalex.org/W6757699683', 'https://openalex.org/W2619368999', 'https://openalex.org/W2102003408', 'https://openalex.org/W2808706139', 'https://openalex.org/W2896457183', 'https://openalex.org/W2907916773', 'https://openalex.org/W2963341956', 'https://openalex.org/W1570629387', 'https://openalex.org/W854541894', 'https://openalex.org/W2964308564', 'https://openalex.org/W2976444281', 'https://openalex.org/W2901997113', 'https://openalex.org/W2964281804', 'https://openalex.org/W2903853691', 'https://openalex.org/W2591927543', 'https://openalex.org/W2133564696']",2020-04-09
https://openalex.org/W3162271107,https://doi.org/10.1109/icassp39728.2021.9413513,Graphspeech: Syntax-Aware Graph Attention Network for Neural Speech Synthesis,"Attention-based end-to-end text-to-speech synthesis (TTS) is superior to conventional statistical methods in many ways. Transformer-based TTS is one of such successful implementations. While Transformer TTS models the speech frame sequence well with a self-attention mechanism, it does not associate input text with output utterances from a syntactic point of view at sentence level. We propose a novel neural TTS model, denoted as GraphSpeech, that is formulated under graph neural network framework. GraphSpeech encodes explicitly the syntactic relation of input lexical tokens in a sentence, and incorporates such information to derive syntactically motivated character embeddings for TTS attention mechanism. Experiments show that GraphSpeech consistently outperforms the Transformer TTS baseline in terms of spectrum and prosody rendering of utterances.","['https://openalex.org/W2889544410', 'https://openalex.org/W2107860279', 'https://openalex.org/W2120847449', 'https://openalex.org/W6781784828', 'https://openalex.org/W2889026195', 'https://openalex.org/W6753212667', 'https://openalex.org/W2157331557', 'https://openalex.org/W6739901393', 'https://openalex.org/W2903739847', 'https://openalex.org/W3005962597', 'https://openalex.org/W3016137096', 'https://openalex.org/W3106690208', 'https://openalex.org/W3088892837', 'https://openalex.org/W6917585676', 'https://openalex.org/W2150658333', 'https://openalex.org/W2998054581', 'https://openalex.org/W3098557217', 'https://openalex.org/W2963609956', 'https://openalex.org/W3037109418', 'https://openalex.org/W2102003408', 'https://openalex.org/W3016151052', 'https://openalex.org/W2964243274', 'https://openalex.org/W2129142580', 'https://openalex.org/W3048768961', 'https://openalex.org/W1570629387', 'https://openalex.org/W3015621018', 'https://openalex.org/W2116341502', 'https://openalex.org/W6784688130', 'https://openalex.org/W2998665041', 'https://openalex.org/W2798749466', 'https://openalex.org/W1568793342', 'https://openalex.org/W2998702685', 'https://openalex.org/W3047107405', 'https://openalex.org/W2604184139', 'https://openalex.org/W3049756574', 'https://openalex.org/W2963403868', 'https://openalex.org/W4385245566', 'https://openalex.org/W3163573274', 'https://openalex.org/W2813877859', 'https://openalex.org/W3168542456']",2021-05-13
https://openalex.org/W3015621018,https://doi.org/10.1109/icassp40776.2020.9053355,GraphTTS: Graph-to-Sequence Modelling in Neural Text-to-Speech,"This paper leverages the graph-to-sequence method in neural text-to-speech (GraphTTS), which maps the graph embedding of the input sequence to spectrograms. The graphical inputs consist of node and edge representations constructed from input texts. The encoding of these graphical inputs incorporates syntax information by a GNN encoder module. Besides, applying the encoder of GraphTTS as a graph auxiliary encoder (GAE) can analyse prosody information from the semantic structure of texts. This can remove the manual selection of reference audios process and makes prosody modelling an end-to-end procedure. Experimental analysis shows that GraphTTS outperforms the state-of-the-art sequence-to-sequence models by 0.24 in Mean Opinion Score (MOS). GAE can adjust the pause, ventilation and tones of synthesised audios automatically. This experimental conclusion may give some inspiration to researchers working on improving speech synthesis prosody.","['https://openalex.org/W6757374366', 'https://openalex.org/W2798749466', 'https://openalex.org/W2963653811', 'https://openalex.org/W6735019100', 'https://openalex.org/W2963374482', 'https://openalex.org/W6679436768', 'https://openalex.org/W2963964898', 'https://openalex.org/W2949952652', 'https://openalex.org/W6745232783', 'https://openalex.org/W6750489868', 'https://openalex.org/W2795109282', 'https://openalex.org/W2904459034', 'https://openalex.org/W6756328725', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963609956', 'https://openalex.org/W2116341502', 'https://openalex.org/W2766406951', 'https://openalex.org/W2905224888', 'https://openalex.org/W2130942839', 'https://openalex.org/W2964138190', 'https://openalex.org/W2595713599', 'https://openalex.org/W2963272440', 'https://openalex.org/W4295731579', 'https://openalex.org/W2794490148', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963927338']",2020-04-09
https://openalex.org/W3202111322,https://doi.org/10.3390/app11199010,Acoustic Word Embeddings for End-to-End Speech Synthesis,"The most recent end-to-end speech synthesis systems use phonemes as acoustic input tokens and ignore the information about which word the phonemes come from. However, many words have their specific prosody type, which may significantly affect the naturalness. Prior works have employed pre-trained linguistic word embeddings as TTS system input. However, since linguistic information is not directly relevant to how words are pronounced, TTS quality improvement of these systems is mild. In this paper, we propose a novel and effective way of jointly training acoustic phone and word embeddings for end-to-end TTS systems. Experiments on the LJSpeech dataset show that the acoustic word embeddings dramatically decrease both the training and validation loss in phone-level prosody prediction. Subjective evaluations on naturalness demonstrate that the incorporation of acoustic word embeddings can significantly outperform both pure phone-based system and the TTS system with pre-trained linguistic word embedding.","['https://openalex.org/W2963609956', 'https://openalex.org/W2964243274', 'https://openalex.org/W2946200149', 'https://openalex.org/W2102003408', 'https://openalex.org/W1578102511', 'https://openalex.org/W2896457183', 'https://openalex.org/W2973217961', 'https://openalex.org/W3016137096', 'https://openalex.org/W3081488690', 'https://openalex.org/W3037109418', 'https://openalex.org/W6739901393', 'https://openalex.org/W6917585676', 'https://openalex.org/W2972359262', 'https://openalex.org/W6631190155', 'https://openalex.org/W2970006822', 'https://openalex.org/W2250539671', 'https://openalex.org/W6762551027']",2021-09-27
https://openalex.org/W3216941316,https://doi.org/10.1109/taslp.2021.3133205,Phone-Level Prosody Modelling With GMM-Based MDN for Diverse and Controllable Speech Synthesis,"Generating natural speech with a diverse and smooth prosody pattern is a\nchallenging task. Although random sampling with phone-level prosody\ndistribution has been investigated to generate different prosody patterns, the\ndiversity of the generated speech is still very limited and far from what can\nbe achieved by humans. This is largely due to the use of uni-modal\ndistribution, such as single Gaussian, in the prior works of phone-level\nprosody modelling. In this work, we propose a novel approach that models\nphone-level prosodies with a GMM-based mixture density network(MDN) and then\nextend it for multi-speaker TTS using speaker adaptation transforms of Gaussian\nmeans and variances. Furthermore, we show that we can clone the prosodies from\na reference speech by sampling prosodies from the Gaussian components that\nproduce the reference prosodies. Our experiments on LJSpeech and LibriTTS\ndataset show that the proposed method with GMM-based MDN not only achieves\nsignificantly better diversity than using a single Gaussian in both\nsingle-speaker and multi-speaker TTS, but also provides better naturalness. The\nprosody cloning experiments demonstrate that the prosody similarity of the\nproposed method with GMM-based MDN is comparable to recent proposed\nfine-grained VAE while the target speaker similarity is better.\n","['https://openalex.org/W2129142580', 'https://openalex.org/W2102003408', 'https://openalex.org/W2964243274', 'https://openalex.org/W6754925833', 'https://openalex.org/W6778823374', 'https://openalex.org/W3197294703', 'https://openalex.org/W1072663298', 'https://openalex.org/W2139525902', 'https://openalex.org/W3095491807', 'https://openalex.org/W3151450932', 'https://openalex.org/W65791558', 'https://openalex.org/W1927421023', 'https://openalex.org/W6600433678', 'https://openalex.org/W2161855744', 'https://openalex.org/W2160464533', 'https://openalex.org/W6749555683', 'https://openalex.org/W6750489868', 'https://openalex.org/W2962691331', 'https://openalex.org/W2973158936', 'https://openalex.org/W3015440759', 'https://openalex.org/W3016021263', 'https://openalex.org/W3022876224', 'https://openalex.org/W142684478', 'https://openalex.org/W2100969003', 'https://openalex.org/W2002342963', 'https://openalex.org/W1990505856', 'https://openalex.org/W2595110011', 'https://openalex.org/W6634817459', 'https://openalex.org/W6917585676', 'https://openalex.org/W2972359262', 'https://openalex.org/W2964138190', 'https://openalex.org/W3197216873', 'https://openalex.org/W6763832098', 'https://openalex.org/W1494198834', 'https://openalex.org/W6631190155', 'https://openalex.org/W6739901393', 'https://openalex.org/W6767111847', 'https://openalex.org/W2107860279', 'https://openalex.org/W6712235671', 'https://openalex.org/W6631362777', 'https://openalex.org/W2890964092', 'https://openalex.org/W1579853615', 'https://openalex.org/W2396366106', 'https://openalex.org/W1524333225', 'https://openalex.org/W2963927338', 'https://openalex.org/W3130016944', 'https://openalex.org/W2892140764', 'https://openalex.org/W10581632', 'https://openalex.org/W2970730223', 'https://openalex.org/W3149617007', 'https://openalex.org/W2964121744']",2021-12-07
https://openalex.org/W3016021263,https://doi.org/10.1109/icassp40776.2020.9053520,Fully-Hierarchical Fine-Grained Prosody Modeling For Interpretable Speech Synthesis,"This paper proposes a hierarchical, fine-grained and interpretable latent variable model for prosody based on the Tacotron 2 text-to-speech model. It achieves multi-resolution modeling of prosody by conditioning finer level representations on coarser level ones. Additionally, it imposes hierarchical conditioning across all latent dimensions using a conditional variational auto-encoder (VAE) with an auto-regressive structure. Evaluation of reconstruction performance illustrates that the new structure does not degrade the model while allowing better interpretability. Interpretations of prosody attributes are provided together with the comparison between word-level and phone-level prosody representations. Moreover, both qualitative and quantitative evaluations are used to demonstrate the improvement in the disentanglement of the latent dimensions.","['https://openalex.org/W6675938391', 'https://openalex.org/W6680537413', 'https://openalex.org/W6739901393', 'https://openalex.org/W6751810238', 'https://openalex.org/W6730405185', 'https://openalex.org/W2091425152', 'https://openalex.org/W6677973343', 'https://openalex.org/W2972359262', 'https://openalex.org/W6738536549', 'https://openalex.org/W2907262790', 'https://openalex.org/W2795109282', 'https://openalex.org/W2964138190', 'https://openalex.org/W2964243274', 'https://openalex.org/W2904459034', 'https://openalex.org/W2962691331', 'https://openalex.org/W2889028433', 'https://openalex.org/W6745697700', 'https://openalex.org/W6755300632', 'https://openalex.org/W6718140377', 'https://openalex.org/W6679436768', 'https://openalex.org/W6750489868', 'https://openalex.org/W2963609956', 'https://openalex.org/W2069859485', 'https://openalex.org/W6756197946', 'https://openalex.org/W2962689740', 'https://openalex.org/W6744627333', 'https://openalex.org/W6745117592', 'https://openalex.org/W6745687250', 'https://openalex.org/W6748223763', 'https://openalex.org/W7075712485', 'https://openalex.org/W6756663807', 'https://openalex.org/W2758785877', 'https://openalex.org/W4293411471', 'https://openalex.org/W4289383906', 'https://openalex.org/W2963568578', 'https://openalex.org/W2963226019', 'https://openalex.org/W2130942839', 'https://openalex.org/W2962690557', 'https://openalex.org/W2963047245', 'https://openalex.org/W2963618559', 'https://openalex.org/W2794490148', 'https://openalex.org/W2948238043', 'https://openalex.org/W4295112158', 'https://openalex.org/W2971074500', 'https://openalex.org/W2805530975', 'https://openalex.org/W2950662112', 'https://openalex.org/W2903538854', 'https://openalex.org/W2901997113', 'https://openalex.org/W2981870653', 'https://openalex.org/W2621357189', 'https://openalex.org/W2977311057', 'https://openalex.org/W2884607399', 'https://openalex.org/W2963104724', 'https://openalex.org/W2963403868', 'https://openalex.org/W2753738274', 'https://openalex.org/W2963272440', 'https://openalex.org/W4295731579', 'https://openalex.org/W2785519580', 'https://openalex.org/W2963927338', 'https://openalex.org/W2917688842', 'https://openalex.org/W2963366547', 'https://openalex.org/W2963264829', 'https://openalex.org/W2559823555', 'https://openalex.org/W4293849739', 'https://openalex.org/W2140574335', 'https://openalex.org/W2107740512', 'https://openalex.org/W4385245566', 'https://openalex.org/W2766812927', 'https://openalex.org/W2964204277', 'https://openalex.org/W2996573371']",2020-04-09
https://openalex.org/W3095491807,https://doi.org/10.21437/interspeech.2020-2477,Hierarchical Multi-Grained Generative Model for Expressive Speech Synthesis,"This paper proposes a hierarchical generative model with a multi-grained latent variable to synthesize expressive speech. In recent years, fine-grained latent variables are introduced into the text-to-speech synthesis that enable the fine control of the prosody and speaking styles of synthesized speech. However, the naturalness of speech degrades when these latent variables are obtained by sampling from the standard Gaussian prior. To solve this problem, we propose a novel framework for modeling the fine-grained latent variables, considering the dependence on an input text, a hierarchical linguistic structure, and a temporal structure of latent variables. This framework consists of a multi-grained variational autoencoder, a conditional prior, and a multi-level auto-regressive latent converter to obtain the different time-resolution latent variables and sample the finer-level latent variables from the coarser-level ones by taking into account the input text. Experimental results indicate an appropriate method of sampling fine-grained latent variables without the reference signal at the synthesis stage. Our proposed framework also provides the controllability of speaking style in an entire utterance.",[],2020-10-25
https://openalex.org/W4221167022,https://doi.org/10.1109/icassp43922.2022.9746323,Unsupervised Word-Level Prosody Tagging for Controllable Speech Synthesis,"Although word-level prosody modeling in neural text-to-speech (TTS) has been investigated in recent research for diverse speech synthesis, it is still challenging to control speech synthesis manually without a specific reference. This is largely due to lack of word-level prosody tags. In this work, we propose a novel approach for unsupervised word-level prosody tagging with two stages, where we first group the words into different types with a decision tree according to their phonetic content and then cluster the prosodies using GMM within each type of words separately. This design is based on the assumption that the prosodies of different type of words, such as long or short words, should be tagged with different label sets. Furthermore, a TTS system with the derived word-level prosody tags is trained for controllable speech synthesis. Experiments on LJSpeech show that the TTS model trained with word-level prosody tags not only achieves better naturalness than a typical FastSpeech2 model, but also gains the ability to manipulate word-level prosody.","['https://openalex.org/W3163003432', 'https://openalex.org/W2962691331', 'https://openalex.org/W6755300632', 'https://openalex.org/W2973158936', 'https://openalex.org/W3015440759', 'https://openalex.org/W3151450932', 'https://openalex.org/W3095491807', 'https://openalex.org/W3016021263', 'https://openalex.org/W3216941316', 'https://openalex.org/W3197216873', 'https://openalex.org/W2964138190', 'https://openalex.org/W1072663298', 'https://openalex.org/W3160844600', 'https://openalex.org/W6768443183', 'https://openalex.org/W3170320568', 'https://openalex.org/W3196843885', 'https://openalex.org/W2537421476', 'https://openalex.org/W2161807040', 'https://openalex.org/W2124551647', 'https://openalex.org/W6712610176', 'https://openalex.org/W6603582338', 'https://openalex.org/W1494198834', 'https://openalex.org/W6917585676', 'https://openalex.org/W6767111847', 'https://openalex.org/W86969866', 'https://openalex.org/W2970006822', 'https://openalex.org/W2400063444', 'https://openalex.org/W4289383906', 'https://openalex.org/W2996573371']",2022-04-27
https://openalex.org/W4301371414,https://doi.org/10.48550/arxiv.2110.14513,Neural Analysis and Synthesis: Reconstructing Speech from\n Self-Supervised Representations,"We present a neural analysis and synthesis (NANSY) framework that can\nmanipulate voice, pitch, and speed of an arbitrary speech signal. Most of the\nprevious works have focused on using information bottleneck to disentangle\nanalysis features for controllable synthesis, which usually results in poor\nreconstruction quality. We address this issue by proposing a novel training\nstrategy based on information perturbation. The idea is to perturb information\nin the original input signal (e.g., formant, pitch, and frequency response),\nthereby letting synthesis networks selectively take essential attributes to\nreconstruct the input signal. Because NANSY does not need any bottleneck\nstructures, it enjoys both high reconstruction quality and controllability.\nFurthermore, NANSY does not require any labels associated with speech data such\nas text and speaker information, but rather uses a new set of analysis\nfeatures, i.e., wav2vec feature and newly proposed pitch feature, Yingram,\nwhich allows for fully self-supervised training. Taking advantage of fully\nself-supervised training, NANSY can be easily extended to a multilingual\nsetting by simply training it with a multilingual dataset. The experiments show\nthat NANSY can achieve significant improvement in performance in several\napplications such as zero-shot voice conversion, pitch shift, and time-scale\nmodification.\n",[],2021-10-27
https://openalex.org/W4307783813,https://doi.org/10.48550/arxiv.2210.15868,Residual Adapters for Few-Shot Text-to-Speech Speaker Adaptation,"Adapting a neural text-to-speech (TTS) model to a target speaker typically involves fine-tuning most if not all of the parameters of a pretrained multi-speaker backbone model. However, serving hundreds of fine-tuned neural TTS models is expensive as each of them requires significant footprint and separate computational resources (e.g., accelerators, memory). To scale speaker adapted neural TTS voices to hundreds of speakers while preserving the naturalness and speaker similarity, this paper proposes a parameter-efficient few-shot speaker adaptation, where the backbone model is augmented with trainable lightweight modules called residual adapters. This architecture allows the backbone model to be shared across different target speakers. Experimental results show that the proposed approach can achieve competitive naturalness and speaker similarity compared to the full fine-tuning approaches, while requiring only $\sim$0.1% of the backbone model parameters for each speaker.",[],2022-10-28
https://openalex.org/W2808706139,https://doi.org/10.48550/arxiv.1806.04558,Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis,"We describe a neural network-based system for text-to-speech (TTS) synthesis that is able to generate speech audio in the voice of many different speakers, including those unseen during training. Our system consists of three independently trained components: (1) a speaker encoder network, trained on a speaker verification task using an independent dataset of noisy speech from thousands of speakers without transcripts, to generate a fixed-dimensional embedding vector from seconds of reference speech from a target speaker; (2) a sequence-to-sequence synthesis network based on Tacotron 2, which generates a mel spectrogram from text, conditioned on the speaker embedding; (3) an auto-regressive WaveNet-based vocoder that converts the mel spectrogram into a sequence of time domain waveform samples. We demonstrate that the proposed model is able to transfer the knowledge of speaker variability learned by the discriminatively-trained speaker encoder to the new task, and is able to synthesize natural speech from speakers that were not seen during training. We quantify the importance of training the speaker encoder on a large and diverse speaker set in order to obtain the best generalization performance. Finally, we show that randomly sampled speaker embeddings can be used to synthesize speech in the voice of novel speakers dissimilar from those used in training, indicating that the model has learned a high quality speaker representation.","['https://openalex.org/W2766812927', 'https://openalex.org/W2794490148', 'https://openalex.org/W2519091744', 'https://openalex.org/W2808631503', 'https://openalex.org/W2726515241', 'https://openalex.org/W2527729766', 'https://openalex.org/W2963712897', 'https://openalex.org/W2046056978', 'https://openalex.org/W2963609956', 'https://openalex.org/W2964243274', 'https://openalex.org/W2114925438', 'https://openalex.org/W2892620417', 'https://openalex.org/W2128653836', 'https://openalex.org/W2788357188', 'https://openalex.org/W2963534259', 'https://openalex.org/W2795109282', 'https://openalex.org/W2788266530', 'https://openalex.org/W1494198834', 'https://openalex.org/W2964308564', 'https://openalex.org/W2962788625', 'https://openalex.org/W2901997113']",2018-06-12
https://openalex.org/W3033411150,https://doi.org/10.57702/v7a23pbp,FastSpeech 2: Fast and High-Quality End-to-End Text-to-Speech,"Non-autoregressive text to speech (TTS) models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even surpass autoregressive models. Audio samples are available at https://speechresearch.github.io/fastspeech2/.","['https://openalex.org/W2102003408', 'https://openalex.org/W1498897359', 'https://openalex.org/W3128910262', 'https://openalex.org/W3017654531', 'https://openalex.org/W2398973091', 'https://openalex.org/W2963636093', 'https://openalex.org/W3033913438', 'https://openalex.org/W2926840633', 'https://openalex.org/W2949382160', 'https://openalex.org/W2970006822', 'https://openalex.org/W2127589467', 'https://openalex.org/W3016136182', 'https://openalex.org/W2963403868', 'https://openalex.org/W2591927543', 'https://openalex.org/W3015338123', 'https://openalex.org/W2963712897', 'https://openalex.org/W2903739847', 'https://openalex.org/W2995233853', 'https://openalex.org/W3160919572', 'https://openalex.org/W1964668808', 'https://openalex.org/W2747874407', 'https://openalex.org/W2963609956', 'https://openalex.org/W2963782041', 'https://openalex.org/W2945613576', 'https://openalex.org/W652196294', 'https://openalex.org/W3025793647', 'https://openalex.org/W3034949308', 'https://openalex.org/W2963300588', 'https://openalex.org/W3026874504', 'https://openalex.org/W278779762', 'https://openalex.org/W2294797155', 'https://openalex.org/W2964243274', 'https://openalex.org/W2629461003', 'https://openalex.org/W2962882868', 'https://openalex.org/W3163906773', 'https://openalex.org/W2096684483', 'https://openalex.org/W2964121744', 'https://openalex.org/W2766812927', 'https://openalex.org/W2963330667', 'https://openalex.org/W2970730223']",2024-01-01
https://openalex.org/W2762829962,,The relationship between fundamental frequency variation and articulation in healthy speech production,"Fundamental Frequency (F0) and articulation are two factors of speech production that impact speech perception, and yet the potential interactions of these two factors are not well understood. Their relationship has potential theoretical as well as clinical implications. This Honors Project aims to better understand this relationship by examining changes in fundamental frequency (F0) and the acoustic vowel space as an index of articulatory behaviors with a within-speaker approach. Specifically, F0 variations were examined in relation to the acoustic vowel space for 10 male native speakers of American English. Two sets of acoustic measures were made to evaluate F0 and vowel space characteristics. For F0 variation, F0 trajectories were generated for 20 randomly selected spans of speech (i.e., speech runs) per speaker, per task. To index articulation, the acoustic vowel space for each speaker was calculated from formant frequencies measured at the temporal midpoint of vowels /i/, /æ/, /ɑ/, and /u/. Motivated by the construct of sufficient contrast, which states that spectral distinction must be maintained for sufficient acoustic contrast [Diehl et. al., J. Phon. 24(2) 187-208 (1996)], the hypothesis of the present study was that variations in F0 would be accompanied by adjustments in formant frequencies necessary for maintaining distinction. This study used a within-speaker design to study variation as a means of adaptation within a given speech mechanism. To evaluate the two production factors, correlational and distributional analysis methods were used. Results and directions of continuing work are discussed within the framework of the acoustic theory of vowel production, and potential clinical implications for motor speech disorders and hearing technology are considered.",[],2017-01-01
https://openalex.org/W2619368999,https://doi.org/10.48550/arxiv.1705.08947,Deep Voice 2: Multi-Speaker Neural Text-to-Speech,"We introduce a technique for augmenting neural text-to-speech (TTS) with lowdimensional trainable speaker embeddings to generate different voices from a single model. As a starting point, we show improvements over the two state-ofthe-art approaches for single-speaker neural TTS: Deep Voice 1 and Tacotron. We introduce Deep Voice 2, which is based on a similar pipeline with Deep Voice 1, but constructed with higher performance building blocks and demonstrates a significant audio quality improvement over Deep Voice 1. We improve Tacotron by introducing a post-processing neural vocoder, and demonstrate a significant audio quality improvement. We then demonstrate our technique for multi-speaker speech synthesis for both Deep Voice 2 and Tacotron on two multi-speaker TTS datasets. We show that a single neural TTS system can learn hundreds of unique voices from less than half an hour of data per speaker, while achieving high audio quality synthesis and preserving the speaker identities almost perfectly.","['https://openalex.org/W2608338293', 'https://openalex.org/W2963609956', 'https://openalex.org/W2127141656', 'https://openalex.org/W1576227399', 'https://openalex.org/W3124149531', 'https://openalex.org/W1522301498', 'https://openalex.org/W1993409002', 'https://openalex.org/W2584032004', 'https://openalex.org/W2432004435', 'https://openalex.org/W2160473997', 'https://openalex.org/W2428702538', 'https://openalex.org/W2117418893', 'https://openalex.org/W2949117887', 'https://openalex.org/W2901997113', 'https://openalex.org/W2950635152', 'https://openalex.org/W1492383498', 'https://openalex.org/W2605320104', 'https://openalex.org/W2952436057', 'https://openalex.org/W2574456902', 'https://openalex.org/W2296283641', 'https://openalex.org/W2591927543', 'https://openalex.org/W2612434969', 'https://openalex.org/W2949382160']",2017-05-24
https://openalex.org/W4225680573,https://doi.org/10.48550/arxiv.2112.02418,YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice\n Conversion for everyone,"YourTTS brings the power of a multilingual approach to the task of zero-shot\nmulti-speaker TTS. Our method builds upon the VITS model and adds several novel\nmodifications for zero-shot multi-speaker and multilingual training. We\nachieved state-of-the-art (SOTA) results in zero-shot multi-speaker TTS and\nresults comparable to SOTA in zero-shot voice conversion on the VCTK dataset.\nAdditionally, our approach achieves promising results in a target language with\na single-speaker dataset, opening possibilities for zero-shot multi-speaker TTS\nand zero-shot voice conversion systems in low-resource languages. Finally, it\nis possible to fine-tune the YourTTS model with less than 1 minute of speech\nand achieve state-of-the-art results in voice similarity and with reasonable\nquality. This is important to allow synthesis for speakers with a very\ndifferent voice or recording characteristics from those seen during training.\n",[],2021-12-04
https://openalex.org/W3128910262,https://doi.org/10.48550/arxiv.2103.00993,AdaSpeech: Adaptive Text to Speech for Custom Voice,"Custom voice, a specific text to speech (TTS) service in commercial speech platforms, aims to adapt a source TTS model to synthesize personal voice for a target speaker using few speech data. Custom voice presents two unique challenges for TTS adaptation: 1) to support diverse customers, the adaptation model needs to handle diverse acoustic conditions that could be very different from source speech data, and 2) to support a large number of customers, the adaptation parameters need to be small enough for each target speaker to reduce memory usage while maintaining high voice quality. In this work, we propose AdaSpeech, an adaptive TTS system for high-quality and efficient customization of new voices. We design several techniques in AdaSpeech to address the two challenges in custom voice: 1) To handle different acoustic conditions, we use two acoustic encoders to extract an utterance-level vector and a sequence of phoneme-level vectors from the target speech during training; in inference, we extract the utterance-level vector from a reference speech and use an acoustic predictor to predict the phoneme-level vectors. 2) To better trade off the adaptation parameters and voice quality, we introduce conditional layer normalization in the mel-spectrogram decoder of AdaSpeech, and fine-tune this part in addition to speaker embedding for adaptation. We pre-train the source TTS model on LibriTTS datasets and fine-tune it on VCTK and LJSpeech datasets (with different acoustic conditions from LibriTTS) with few adaptation data, e.g., 20 sentences, about 1 minute speech. Experiment results show that AdaSpeech achieves much better adaptation quality than baseline methods, with only about 5K specific parameters for each speaker, which demonstrates its effectiveness for custom voice. Audio samples are available at https://speechresearch.github.io/adaspeech/.","['https://openalex.org/W2963403868', 'https://openalex.org/W3035083561', 'https://openalex.org/W2970730223', 'https://openalex.org/W3163906773', 'https://openalex.org/W2591927543', 'https://openalex.org/W2612434969', 'https://openalex.org/W2972359262', 'https://openalex.org/W3130016944', 'https://openalex.org/W2766812927', 'https://openalex.org/W2962788625', 'https://openalex.org/W2964243274', 'https://openalex.org/W2187089797', 'https://openalex.org/W2943731990', 'https://openalex.org/W2527729766', 'https://openalex.org/W3097795905', 'https://openalex.org/W2963912924', 'https://openalex.org/W3160919572', 'https://openalex.org/W2963432880', 'https://openalex.org/W3026874504', 'https://openalex.org/W3016159759', 'https://openalex.org/W2926840633', 'https://openalex.org/W3015826515', 'https://openalex.org/W2963609956', 'https://openalex.org/W2963192573', 'https://openalex.org/W3161109662', 'https://openalex.org/W3015440759', 'https://openalex.org/W3024747869', 'https://openalex.org/W2970006822', 'https://openalex.org/W1494198834', 'https://openalex.org/W2747874407', 'https://openalex.org/W2963712897']",2021-03-01
https://openalex.org/W2892620417,https://doi.org/10.48550/arxiv.1809.10460,Sample Efficient Adaptive Text-to-Speech,"We present a meta-learning approach for adaptive text-to-speech (TTS) with few data. During training, we learn a multi-speaker model using a shared conditional WaveNet core and independent learned embeddings for each speaker. The aim of training is not to produce a neural network with fixed weights, which is then deployed as a TTS system. Instead, the aim is to produce a network that requires few data at deployment time to rapidly adapt to new speakers. We introduce and benchmark three strategies: (i) learning the speaker embedding while keeping the WaveNet core fixed, (ii) fine-tuning the entire architecture with stochastic gradient descent, and (iii) predicting the speaker embedding with a trained neural network encoder. The experiments show that these approaches are successful at adapting the multi-speaker neural network to new speakers, obtaining state-of-the-art results in both sample naturalness and voice similarity with merely a few minutes of audio data from new speakers.","['https://openalex.org/W2962732055', 'https://openalex.org/W2766812927', 'https://openalex.org/W2787053496', 'https://openalex.org/W2901997113', 'https://openalex.org/W1494198834', 'https://openalex.org/W2963609956', 'https://openalex.org/W2963775850', 'https://openalex.org/W2163605009', 'https://openalex.org/W99485931', 'https://openalex.org/W2949382160', 'https://openalex.org/W2964135650', 'https://openalex.org/W2963341924', 'https://openalex.org/W2788357188', 'https://openalex.org/W2962715211', 'https://openalex.org/W2963534259', 'https://openalex.org/W2267126114', 'https://openalex.org/W2769810959', 'https://openalex.org/W2964281804', 'https://openalex.org/W1993409002', 'https://openalex.org/W2604763608', 'https://openalex.org/W2964301388', 'https://openalex.org/W2472819217', 'https://openalex.org/W2160815625', 'https://openalex.org/W2805560727', 'https://openalex.org/W2097117768', 'https://openalex.org/W2624086852', 'https://openalex.org/W2962788625', 'https://openalex.org/W2963221401', 'https://openalex.org/W2527729766', 'https://openalex.org/W2962741254', 'https://openalex.org/W2962724383', 'https://openalex.org/W2808706139', 'https://openalex.org/W2771657877', 'https://openalex.org/W1510007267', 'https://openalex.org/W2963559821', 'https://openalex.org/W2091425152', 'https://openalex.org/W2753160622', 'https://openalex.org/W2092387347', 'https://openalex.org/W2962832278', 'https://openalex.org/W2525778437', 'https://openalex.org/W2103085228', 'https://openalex.org/W2795109282', 'https://openalex.org/W2471520273', 'https://openalex.org/W2507550121', 'https://openalex.org/W2788266530', 'https://openalex.org/W1570629387', 'https://openalex.org/W2298457129']",2018-09-27
https://openalex.org/W2788357188,https://doi.org/10.48550/arxiv.1802.06006,Neural Voice Cloning with a Few Samples,"Voice cloning is a highly desired feature for personalized speech interfaces. Neural network based speech synthesis has been shown to generate high quality speech for a large number of speakers. In this paper, we introduce a neural voice cloning system that takes a few audio samples as input. We study two approaches: speaker adaptation and speaker encoding. Speaker adaptation is based on fine-tuning a multi-speaker generative model with a few cloning samples. Speaker encoding is based on training a separate model to directly infer a new speaker embedding from cloning audios and to be used with a multi-speaker generative model. In terms of naturalness of the speech and its similarity to original speaker, both approaches can achieve good performance, even with very few cloning audios. While speaker adaptation can achieve better naturalness and similarity, the cloning time or required memory for the speaker encoding approach is significantly less, making it favorable for low-resource deployment.","['https://openalex.org/W2963691546', 'https://openalex.org/W2949247522', 'https://openalex.org/W2747616140', 'https://openalex.org/W2951535099', 'https://openalex.org/W2401812832', 'https://openalex.org/W2774796173', 'https://openalex.org/W2401822949', 'https://openalex.org/W2114168642', 'https://openalex.org/W2626778328', 'https://openalex.org/W2604184139', 'https://openalex.org/W2949382160', 'https://openalex.org/W2408712009', 'https://openalex.org/W1989549063', 'https://openalex.org/W2193413348', 'https://openalex.org/W2766812927', 'https://openalex.org/W2112021726', 'https://openalex.org/W2743945814', 'https://openalex.org/W2198484938', 'https://openalex.org/W2963712897', 'https://openalex.org/W2121812409', 'https://openalex.org/W2901997113', 'https://openalex.org/W2612434969', 'https://openalex.org/W2766669584', 'https://openalex.org/W2048526313', 'https://openalex.org/W2086796102', 'https://openalex.org/W2587150483', 'https://openalex.org/W2259472270', 'https://openalex.org/W2194321275', 'https://openalex.org/W2766527293', 'https://openalex.org/W2153914468', 'https://openalex.org/W2512087624', 'https://openalex.org/W2777302760', 'https://openalex.org/W2423557781', 'https://openalex.org/W2515028311', 'https://openalex.org/W2736900972', 'https://openalex.org/W2527729766', 'https://openalex.org/W2953331651', 'https://openalex.org/W1993409002', 'https://openalex.org/W2317898072', 'https://openalex.org/W2129142580', 'https://openalex.org/W2591927543', 'https://openalex.org/W2605320104', 'https://openalex.org/W1494198834']",2018-02-14
https://openalex.org/W2766812927,,Deep Voice 3: 2000-Speaker Neural Text-to-Speech,"We present Deep Voice 3, a fully-convolutional attention-based neural text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training ten times faster. We scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers. In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods. We also describe how to scale inference to ten million queries per day on one single-GPU server.","['https://openalex.org/W2964265128', 'https://openalex.org/W2964308564', 'https://openalex.org/W2507771204', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962965405', 'https://openalex.org/W2963970792', 'https://openalex.org/W2049686551', 'https://openalex.org/W2591927543', 'https://openalex.org/W2963685250', 'https://openalex.org/W2963403868', 'https://openalex.org/W2605141709', 'https://openalex.org/W1563460361', 'https://openalex.org/W1570629387', 'https://openalex.org/W2165143604']",2017-10-20
https://openalex.org/W3169905056,https://doi.org/10.48550/arxiv.2106.06103,Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech,"Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the natural one-to-many relationship in which a text input can be spoken in multiple ways with different pitches and rhythms. A subjective human evaluation (mean opinion score, or MOS) on the LJ Speech, a single speaker dataset, shows that our method outperforms the best publicly available TTS systems and achieves a MOS comparable to ground truth.","['https://openalex.org/W1959608418', 'https://openalex.org/W3122115223', 'https://openalex.org/W2963799213', 'https://openalex.org/W2527729766', 'https://openalex.org/W2593414223', 'https://openalex.org/W2970898247', 'https://openalex.org/W2912618358', 'https://openalex.org/W3034509000', 'https://openalex.org/W2996286887', 'https://openalex.org/W2963300588', 'https://openalex.org/W2908510526', 'https://openalex.org/W3016136182', 'https://openalex.org/W2964307104', 'https://openalex.org/W2964243274', 'https://openalex.org/W2903739847', 'https://openalex.org/W2970832665', 'https://openalex.org/W2962695743', 'https://openalex.org/W3125481789', 'https://openalex.org/W2963534259', 'https://openalex.org/W2970006822', 'https://openalex.org/W1810943226', 'https://openalex.org/W2912298597', 'https://openalex.org/W2946200149', 'https://openalex.org/W2963403868', 'https://openalex.org/W2789541106', 'https://openalex.org/W3098403858', 'https://openalex.org/W2963432880', 'https://openalex.org/W4285719527', 'https://openalex.org/W3015282541', 'https://openalex.org/W2949382160', 'https://openalex.org/W2091425152', 'https://openalex.org/W2560512785', 'https://openalex.org/W2587284713', 'https://openalex.org/W3130016944', 'https://openalex.org/W2904459034', 'https://openalex.org/W2963090522', 'https://openalex.org/W3125708547', 'https://openalex.org/W2963568578']",2021-06-11
https://openalex.org/W4297813370,https://doi.org/10.48550/arxiv.2005.13580,Network-to-Network Translation with Conditional Invertible Neural\n Networks,"Given the ever-increasing computational costs of modern machine learning\nmodels, we need to find new ways to reuse such expert models and thus tap into\nthe resources that have been invested in their creation. Recent work suggests\nthat the power of these massive models is captured by the representations they\nlearn. Therefore, we seek a model that can relate between different existing\nrepresentations and propose to solve this task with a conditionally invertible\nnetwork. This network demonstrates its capability by (i) providing generic\ntransfer between diverse domains, (ii) enabling controlled content synthesis by\nallowing modification in other domains, and (iii) facilitating diagnosis of\nexisting representations by translating them into interpretable domains such as\nimages. Our domain transfer network can translate between fixed representations\nwithout having to learn or finetune them. This allows users to utilize various\nexisting domain-specific expert models from the literature that had been\ntrained with extensive computational resources. Experiments on diverse\nconditional image synthesis tasks, competitive image modification results and\nexperiments on image-to-image and text-to-image generation demonstrate the\ngeneric applicability of our approach. For example, we translate between BERT\nand BigGAN, state-of-the-art text and image models to provide text-to-image\ngeneration, which neither of both experts can perform on their own.\n",[],2020-05-27
https://openalex.org/W2935711438,https://doi.org/10.1109/icassp.2019.8683277,Low Bit-rate Speech Coding with VQ-VAE and a WaveNet Decoder,"In order to efficiently transmit and store speech signals, speech codecs create a minimally redundant representation of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on VQ-VAE with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosody-transparent and speaker-independent model trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits perceptual quality which is around halfway between the MELP codec at 2.4 kbps and AMR-WB codec at 23.05 kbps. In addition, when training on high-quality recorded speech with the test speaker included in the training set, a model coding speech at 1.6 kbps produces output of similar perceptual quality to that generated by AMR-WB at 23.05 kbps.","['https://openalex.org/W2165291881', 'https://openalex.org/W2752796333', 'https://openalex.org/W6640963894', 'https://openalex.org/W1494198834', 'https://openalex.org/W6696768431', 'https://openalex.org/W2108532241', 'https://openalex.org/W6752888775', 'https://openalex.org/W6738494155', 'https://openalex.org/W6735849998', 'https://openalex.org/W2963182577', 'https://openalex.org/W2309400744', 'https://openalex.org/W2632564668', 'https://openalex.org/W2552465432', 'https://openalex.org/W6734035190', 'https://openalex.org/W2775336875', 'https://openalex.org/W6755135894', 'https://openalex.org/W2808706139', 'https://openalex.org/W2950237263', 'https://openalex.org/W2963799213', 'https://openalex.org/W1959608418', 'https://openalex.org/W2616159609', 'https://openalex.org/W4294567867', 'https://openalex.org/W2604231067', 'https://openalex.org/W2892620417', 'https://openalex.org/W2951004968', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963449488', 'https://openalex.org/W2962676454', 'https://openalex.org/W3124456579', 'https://openalex.org/W2292235217']",2019-04-16
https://openalex.org/W3037038648,https://doi.org/10.1109/qomex48832.2020.9123150,ViSQOL v3: An Open Source Production Ready Objective Speech and Audio Metric,"The 12th International Conference on Quality of Multimedia Experience (QoMEX), Athlone, Ireland (held online due to coronavirus outbreak), 26-28 May 2020","['https://openalex.org/W2775336875', 'https://openalex.org/W2963208781', 'https://openalex.org/W2963300588', 'https://openalex.org/W1976188834', 'https://openalex.org/W2922332774', 'https://openalex.org/W2111964411', 'https://openalex.org/W6638872167', 'https://openalex.org/W2469918093', 'https://openalex.org/W2623662528', 'https://openalex.org/W2972359262', 'https://openalex.org/W2153635508', 'https://openalex.org/W2108708552', 'https://openalex.org/W6636045042', 'https://openalex.org/W1496883935', 'https://openalex.org/W6636170946', 'https://openalex.org/W6754761153', 'https://openalex.org/W1552314771', 'https://openalex.org/W2040151471', 'https://openalex.org/W2990738305', 'https://openalex.org/W1606487971', 'https://openalex.org/W1849775568', 'https://openalex.org/W1607435270', 'https://openalex.org/W2892330131', 'https://openalex.org/W2103934944', 'https://openalex.org/W2987307811']",2020-05-01
https://openalex.org/W4372279529,https://doi.org/10.48550/arxiv.2305.02765,HiFi-Codec: Group-residual Vector quantization for High Fidelity Audio Codec,"Audio codec models are widely used in audio communication as a crucial technique for compressing audio into discrete representations. Nowadays, audio codec models are increasingly utilized in generation fields as intermediate representations. For instance, AudioLM is an audio generation model that uses the discrete representation of SoundStream as a training target, while VALL-E employs the Encodec model as an intermediate feature to aid TTS tasks. Despite their usefulness, two challenges persist: (1) training these audio codec models can be difficult due to the lack of publicly available training processes and the need for large-scale data and GPUs; (2) achieving good reconstruction performance requires many codebooks, which increases the burden on generation models. In this study, we propose a group-residual vector quantization (GRVQ) technique and use it to develop a novel \textbf{Hi}gh \textbf{Fi}delity Audio Codec model, HiFi-Codec, which only requires 4 codebooks. We train all the models using publicly available TTS data such as LibriTTS, VCTK, AISHELL, and more, with a total duration of over 1000 hours, using 8 GPUs. Our experimental results show that HiFi-Codec outperforms Encodec in terms of reconstruction performance despite requiring only 4 codebooks. To facilitate research in audio codec and generation, we introduce AcademiCodec, the first open-source audio codec toolkit that offers training codes and pre-trained models for Encodec, SoundStream, and HiFi-Codec. Code and pre-trained model can be found on: \href{https://github.com/yangdongchao/AcademiCodec}{https://github.com/yangdongchao/AcademiCodec}",[],2023-05-04
https://openalex.org/W3094002217,https://doi.org/10.48550/arxiv.2010.11567,AISHELL-3: A Multi-speaker Mandarin TTS Corpus and the Baselines,"In this paper, we present AISHELL-3, a large-scale and high-fidelity multi-speaker Mandarin speech corpus which could be used to train multi-speaker Text-to-Speech (TTS) systems. The corpus contains roughly 85 hours of emotion-neutral recordings spoken by 218 native Chinese mandarin speakers. Their auxiliary attributes such as gender, age group and native accents are explicitly marked and provided in the corpus. Accordingly, transcripts in Chinese character-level and pinyin-level are provided along with the recordings. We present a baseline system that uses AISHELL-3 for multi-speaker Madarin speech synthesis. The multi-speaker speech synthesis system is an extension on Tacotron-2 where a speaker verification model and a corresponding loss regarding voice similarity are incorporated as the feedback constraint. We aim to use the presented corpus to build a robust synthesis model that is able to achieve zero-shot voice cloning. The system trained on this dataset also generalizes well on speakers that are never seen in the training process. Objective evaluation results from our experiments show that the proposed multi-speaker synthesis system achieves high voice similarity concerning both speaker embedding similarity and equal error rate measurement. The dataset, baseline system code and generated samples are available online.","['https://openalex.org/W2963272440', 'https://openalex.org/W2963712897', 'https://openalex.org/W3016021263', 'https://openalex.org/W2963609956', 'https://openalex.org/W3212683077', 'https://openalex.org/W2903739847', 'https://openalex.org/W2405052056', 'https://openalex.org/W2972457126', 'https://openalex.org/W3011026433', 'https://openalex.org/W3015440759', 'https://openalex.org/W2963300588', 'https://openalex.org/W2970006822', 'https://openalex.org/W3021733045', 'https://openalex.org/W2963432880', 'https://openalex.org/W2949382160', 'https://openalex.org/W3096086473', 'https://openalex.org/W3015922793', 'https://openalex.org/W2527729766', 'https://openalex.org/W2964243274', 'https://openalex.org/W3015645837', 'https://openalex.org/W2972956685', 'https://openalex.org/W2969521066', 'https://openalex.org/W2962970071']",2020-10-22
https://openalex.org/W2990124956,https://doi.org/10.48550/arxiv.1911.11601,Cross-lingual Multi-speaker Text-to-speech Synthesis for Voice Cloning without Using Parallel Corpus for Unseen Speakers,"We investigate a novel cross-lingual multi-speaker text-to-speech synthesis approach for generating high-quality native or accented speech for native/foreign seen/unseen speakers in English and Mandarin. The system consists of three separately trained components: an x-vector speaker encoder, a Tacotron-based synthesizer and a WaveNet vocoder. It is conditioned on 3 kinds of embeddings: (1) speaker embedding so that the system can be trained with speech from many speakers will little data from each speaker; (2) language embedding with shared phoneme inputs; (3) stress and tone embedding which improves naturalness of synthesized speech, especially for a tonal language like Mandarin. By adjusting the various embeddings, MOS results show that our method can generate high-quality natural and intelligible native speech for native/foreign seen/unseen speakers. Intelligibility and naturalness of accented speech is low as expected. Speaker similarity is good for native speech from native speakers. Interestingly, speaker similarity is also good for accented speech from foreign speakers. We also find that normalizing speaker embedding x-vectors by L2-norm normalization or whitening improves output quality a lot in many cases, and the WaveNet performance seems to be language-independent: our WaveNet is trained with Cantonese speech and can be used to generate Mandarin and English speech very well.","['https://openalex.org/W2471520273', 'https://openalex.org/W2901997113', 'https://openalex.org/W1494198834', 'https://openalex.org/W2736900972', 'https://openalex.org/W2973084242', 'https://openalex.org/W1524333225', 'https://openalex.org/W2963534259', 'https://openalex.org/W2963192573', 'https://openalex.org/W2072563337', 'https://openalex.org/W2953331651', 'https://openalex.org/W2963712897', 'https://openalex.org/W2957543415', 'https://openalex.org/W2964243274', 'https://openalex.org/W2527729766', 'https://openalex.org/W2890964092', 'https://openalex.org/W2963432880', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963242190', 'https://openalex.org/W2120847449', 'https://openalex.org/W2792995953', 'https://openalex.org/W2591927543', 'https://openalex.org/W1510007267', 'https://openalex.org/W1492383498', 'https://openalex.org/W2963609956']",2019-11-26
https://openalex.org/W4380551955,https://doi.org/10.48550/arxiv.2306.06546,High-Fidelity Audio Compression with Improved RVQGAN,"Language models have been successfully used to model natural signals, such as images, speech, and music. A key component of these models is a high quality neural compression model that can compress high-dimensional natural signals into lower dimensional discrete tokens. To that end, we introduce a high-fidelity universal neural audio compression algorithm that achieves ~90x compression of 44.1 KHz audio into tokens at just 8kbps bandwidth. We achieve this by combining advances in high-fidelity audio generation with better vector quantization techniques from the image domain, along with improved adversarial and reconstruction losses. We compress all domains (speech, environment, music, etc.) with a single universal model, making it widely applicable to generative modeling of all audio. We compare with competing audio compression algorithms, and find our method outperforms them significantly. We provide thorough ablations for every design choice, as well as open-source code and trained model weights. We hope our work can lay the foundation for the next generation of high-fidelity audio modeling.",[],2023-06-11
https://openalex.org/W2980451698,https://doi.org/10.21437/interspeech.2019-1495,Bridging the Gap Between Monaural Speech Enhancement and Recognition with Distortion-Independent Acoustic Modeling,"Monaural speech enhancement has made dramatic advances since the introduction of deep learning a few years ago.Although enhanced speech has been demonstrated to have better intelligibility and quality for human listeners, feeding it directly to automatic speech recognition (ASR) systems trained with noisy speech has not produced expected improvements in ASR performance.The lack of an enhancement benefit on recognition, or the gap between monaural speech enhancement and recognition, is often attributed to speech distortions introduced in the enhancement process.In this study, we analyze the distortion problem, compare different acoustic models, and investigate a distortionindependent training scheme for monaural speech recognition.Experimental results suggest that distortion-independent acoustic modeling is able to overcome the distortion problem.Such an acoustic model can also work with speech enhancement models different from the one used during training.Moreover, the models investigated in this paper outperform the previous best system on the CHiME-2 corpus.","['https://openalex.org/W2035576074', 'https://openalex.org/W2966212213', 'https://openalex.org/W2889442120', 'https://openalex.org/W1897240248', 'https://openalex.org/W2897371647', 'https://openalex.org/W2078528584', 'https://openalex.org/W2408713104', 'https://openalex.org/W2516547830', 'https://openalex.org/W2678916739', 'https://openalex.org/W1524333225', 'https://openalex.org/W2893370043', 'https://openalex.org/W2044893557', 'https://openalex.org/W2399742709', 'https://openalex.org/W2963122170', 'https://openalex.org/W2962998773', 'https://openalex.org/W2296167893', 'https://openalex.org/W2550397165', 'https://openalex.org/W2023331723', 'https://openalex.org/W2889698889', 'https://openalex.org/W2950258612', 'https://openalex.org/W2057200980', 'https://openalex.org/W2141411743', 'https://openalex.org/W2069681747', 'https://openalex.org/W4298998200', 'https://openalex.org/W2891644395', 'https://openalex.org/W2364134690', 'https://openalex.org/W2237671050', 'https://openalex.org/W2962866211', 'https://openalex.org/W2403766732', 'https://openalex.org/W2398758402', 'https://openalex.org/W2405774341', 'https://openalex.org/W2138939691']",2019-09-13
https://openalex.org/W3131332223,https://doi.org/10.1109/icassp39728.2021.9414060,Variational Autoencoder for Speech Enhancement with a Noise-Aware Encoder,"Recently, a generative variational autoencoder (VAE) has been proposed for\nspeech enhancement to model speech statistics. However, this approach only uses\nclean speech in the training phase, making the estimation particularly\nsensitive to noise presence, especially in low signal-to-noise ratios (SNRs).\nTo increase the robustness of the VAE, we propose to include noise information\nin the training phase by using a noise-aware encoder trained on noisy-clean\nspeech pairs. We evaluate our approach on real recordings of different noisy\nenvironments and acoustic conditions using two different noise datasets. We\nshow that our proposed noise-aware VAE outperforms the standard VAE in terms of\noverall distortion without increasing the number of model parameters. At the\nsame time, we demonstrate that our model is capable of generalizing to unseen\nnoise conditions better than a supervised feedforward deep neural network\n(DNN). Furthermore, we demonstrate the robustness of the model performance to a\nreduction of the noisy-clean speech training data size.\n","['https://openalex.org/W6640963894', 'https://openalex.org/W6639732818', 'https://openalex.org/W6675944832', 'https://openalex.org/W6687045409', 'https://openalex.org/W2766672686', 'https://openalex.org/W2883322837', 'https://openalex.org/W2901552243', 'https://openalex.org/W6752558437', 'https://openalex.org/W2149273154', 'https://openalex.org/W2072634211', 'https://openalex.org/W2039844283', 'https://openalex.org/W3097549261', 'https://openalex.org/W6680012447', 'https://openalex.org/W2168273590', 'https://openalex.org/W2161620716', 'https://openalex.org/W2938737578', 'https://openalex.org/W2962866211', 'https://openalex.org/W2135284480', 'https://openalex.org/W2051428568', 'https://openalex.org/W1506438021', 'https://openalex.org/W2046869671', 'https://openalex.org/W2964058413', 'https://openalex.org/W6631190155', 'https://openalex.org/W4297791576', 'https://openalex.org/W2108501770', 'https://openalex.org/W4245883374', 'https://openalex.org/W2188365844', 'https://openalex.org/W2962897886', 'https://openalex.org/W2135029798', 'https://openalex.org/W3143596294', 'https://openalex.org/W1522301498', 'https://openalex.org/W4292403327', 'https://openalex.org/W2963632741', 'https://openalex.org/W1909320841', 'https://openalex.org/W4320013936', 'https://openalex.org/W1985093013', 'https://openalex.org/W3100968126', 'https://openalex.org/W2099471712', 'https://openalex.org/W1959608418', 'https://openalex.org/W2964121744']",2021-05-13
https://openalex.org/W3011982609,https://doi.org/10.1109/taslp.2020.2979603,A Flow-Based Deep Latent Variable Model for Speech Spectrogram Modeling and Enhancement,"This article describes a deep latent variable model of speech power spectrograms and its application to semi-supervised speech enhancement with a deep speech prior. By integrating two major deep generative models, a variational autoencoder (VAE) and a normalizing flow (NF), in a mutually-beneficial manner, we formulate a flexible latent variable model called the NF-VAE that can extract low-dimensional latent representations from high-dimensional observations, akin to the VAE, and does not need to explicitly represent the distribution of the observations, akin to the NF. In this article, we consider a variant of NF called the generative flow (GF a.k.a. Glow) and formulate a latent variable model called the GF-VAE. We experimentally show that the proposed GF-VAE is better than the standard VAE at capturing fine-structured harmonics of speech spectrograms, especially in the high-frequency range. A similar finding is also obtained when the GF-VAE and the VAE are used to generate speech spectrograms from latent variables randomly sampled from the standard Gaussian distribution. Lastly, when these models are used as speech priors for statistical multichannel speech enhancement, the GF-VAE outperforms the VAE and the GF.","['https://openalex.org/W2918296821', 'https://openalex.org/W2502139158', 'https://openalex.org/W2766672686', 'https://openalex.org/W2883322837', 'https://openalex.org/W2922004249', 'https://openalex.org/W2901552243', 'https://openalex.org/W2979850772', 'https://openalex.org/W6640963894', 'https://openalex.org/W2056760934', 'https://openalex.org/W2138309709', 'https://openalex.org/W1498436455', 'https://openalex.org/W6754172774', 'https://openalex.org/W2943237054', 'https://openalex.org/W6738101965', 'https://openalex.org/W6745535286', 'https://openalex.org/W6758800702', 'https://openalex.org/W6610566761', 'https://openalex.org/W6714644935', 'https://openalex.org/W6752910514', 'https://openalex.org/W4237840503', 'https://openalex.org/W1965555277', 'https://openalex.org/W6749820799', 'https://openalex.org/W6635084905', 'https://openalex.org/W6718362372', 'https://openalex.org/W6766355753', 'https://openalex.org/W2963300588', 'https://openalex.org/W6756159577', 'https://openalex.org/W4255052368', 'https://openalex.org/W6638667902', 'https://openalex.org/W2143027228', 'https://openalex.org/W1902027874', 'https://openalex.org/W2489539531', 'https://openalex.org/W4206551434', 'https://openalex.org/W2326138198', 'https://openalex.org/W2289394825', 'https://openalex.org/W2559260703', 'https://openalex.org/W2020997493', 'https://openalex.org/W6767164110', 'https://openalex.org/W6631190155', 'https://openalex.org/W6638545294', 'https://openalex.org/W2963223306', 'https://openalex.org/W6712840351', 'https://openalex.org/W1582774210', 'https://openalex.org/W2962688977', 'https://openalex.org/W2127851351', 'https://openalex.org/W2141998673', 'https://openalex.org/W2874689226', 'https://openalex.org/W2964328256', 'https://openalex.org/W2962911378', 'https://openalex.org/W165956390', 'https://openalex.org/W2963081964', 'https://openalex.org/W2922391058', 'https://openalex.org/W6687045409', 'https://openalex.org/W2905196628', 'https://openalex.org/W6755300632', 'https://openalex.org/W2804998325', 'https://openalex.org/W6754559877', 'https://openalex.org/W2188365844', 'https://openalex.org/W1959608418', 'https://openalex.org/W1601795611', 'https://openalex.org/W2963568578', 'https://openalex.org/W2963467180', 'https://openalex.org/W2886577208', 'https://openalex.org/W1560013842', 'https://openalex.org/W2972516210', 'https://openalex.org/W1815076433', 'https://openalex.org/W1663973292', 'https://openalex.org/W1836465849', 'https://openalex.org/W2964020555', 'https://openalex.org/W2963135265', 'https://openalex.org/W2962882868', 'https://openalex.org/W3100968126', 'https://openalex.org/W2620025707', 'https://openalex.org/W2952405182', 'https://openalex.org/W565955800', 'https://openalex.org/W2964972896', 'https://openalex.org/W2963090522', 'https://openalex.org/W1516111018', 'https://openalex.org/W2062931547', 'https://openalex.org/W2964121744', 'https://openalex.org/W2771132365', 'https://openalex.org/W2994689640', 'https://openalex.org/W1409984952', 'https://openalex.org/W2519091744', 'https://openalex.org/W2962911072', 'https://openalex.org/W2962695743', 'https://openalex.org/W2962820504']",2020-01-01
https://openalex.org/W4221144097,https://doi.org/10.1109/icassp43922.2022.9746901,Conditional Diffusion Probabilistic Model for Speech Enhancement,"Speech enhancement is a critical component of many user-oriented audio applications, yet current systems still suffer from distorted and unnatural outputs. While generative models have shown strong potential in speech synthesis, they are still lagging behind in speech enhancement. This work leverages recent advances in diffusion probabilistic models, and proposes a novel speech enhancement algorithm that incorporates characteristics of the observed noisy speech signal into the diffusion and reverse processes. More specifically, we propose a generalized formulation of the diffusion probabilistic model named conditional diffusion probabilistic model that, in its reverse process, can adapt to non-Gaussian real noises in the estimated speech signal. In our experiments, we demonstrate strong performance of the proposed approach compared to representative generative models, and investigate the generalization capability of our models to other datasets with noise characteristics unseen during training.","['https://openalex.org/W3016056257', 'https://openalex.org/W2998832642', 'https://openalex.org/W2015069270', 'https://openalex.org/W2144404214', 'https://openalex.org/W2559260703', 'https://openalex.org/W6783867762', 'https://openalex.org/W2952218014', 'https://openalex.org/W6762114000', 'https://openalex.org/W2963341071', 'https://openalex.org/W2802304149', 'https://openalex.org/W2747161606', 'https://openalex.org/W3005621653', 'https://openalex.org/W3160324929', 'https://openalex.org/W3097945073', 'https://openalex.org/W2972436155', 'https://openalex.org/W2962998773', 'https://openalex.org/W2516547830', 'https://openalex.org/W1936725236', 'https://openalex.org/W4232282348', 'https://openalex.org/W2954275688', 'https://openalex.org/W2094721231', 'https://openalex.org/W2962866211', 'https://openalex.org/W2746457594', 'https://openalex.org/W1552314771', 'https://openalex.org/W2963045393', 'https://openalex.org/W2405774341', 'https://openalex.org/W3086154751', 'https://openalex.org/W2805233667', 'https://openalex.org/W1992475611', 'https://openalex.org/W2129069237', 'https://openalex.org/W6788990321', 'https://openalex.org/W6779823529', 'https://openalex.org/W4226320669', 'https://openalex.org/W6783182287', 'https://openalex.org/W2603567530', 'https://openalex.org/W6798721538', 'https://openalex.org/W3099330747', 'https://openalex.org/W2949558265', 'https://openalex.org/W3103619082', 'https://openalex.org/W3036167779', 'https://openalex.org/W3109018774', 'https://openalex.org/W2519091744', 'https://openalex.org/W3102190437', 'https://openalex.org/W3129651364', 'https://openalex.org/W3092028330', 'https://openalex.org/W3168053944', 'https://openalex.org/W3184410885']",2022-04-27
https://openalex.org/W4384080510,https://doi.org/10.1109/taslp.2023.3294692,StoRM: A Diffusion-Based Stochastic Regeneration Model for Speech Enhancement and Dereverberation,"Diffusion models have shown a great ability at bridging the performance gap between predictive and generative approaches for speech enhancement. We have shown that they may even outperform their predictive counterparts for non-additive corruption types or when they are evaluated on mismatched conditions. However, diffusion models suffer from a high computational burden, mainly as they require to run a neural network for each reverse diffusion step, whereas predictive approaches only require one pass. As diffusion models are generative approaches they may also produce vocalizing and breathing artifacts in adverse conditions. In comparison, in such difficult scenarios, predictive models typically do not produce such artifacts but tend to distort the target speech instead, thereby degrading the speech quality. In this work, we present a stochastic regeneration approach where an estimate given by a predictive model is provided as a guide for further diffusion. We show that the proposed approach uses the predictive model to remove the vocalizing and breathing artifacts while producing very high quality samples thanks to the diffusion model, even in adverse conditions. We further show that this approach enables to use lighter sampling schemes with fewer diffusion steps without sacrificing quality, thus lifting the computational burden by an order of magnitude. Source code and audio examples are available online <uri xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">https://uhh.de/inf-sp-storm</uri> .","['https://openalex.org/W6640963894', 'https://openalex.org/W2763188033', 'https://openalex.org/W4380434618', 'https://openalex.org/W1983108229', 'https://openalex.org/W2883322837', 'https://openalex.org/W3197912330', 'https://openalex.org/W3131332223', 'https://openalex.org/W3213188934', 'https://openalex.org/W2889530575', 'https://openalex.org/W6735443497', 'https://openalex.org/W4372268522', 'https://openalex.org/W2289394825', 'https://openalex.org/W2006129368', 'https://openalex.org/W2127851351', 'https://openalex.org/W6610566761', 'https://openalex.org/W3097549261', 'https://openalex.org/W3096831136', 'https://openalex.org/W2992005611', 'https://openalex.org/W6851638519', 'https://openalex.org/W2013035813', 'https://openalex.org/W1505878979', 'https://openalex.org/W1991111872', 'https://openalex.org/W2170316986', 'https://openalex.org/W6789826613', 'https://openalex.org/W6890361730', 'https://openalex.org/W6786375611', 'https://openalex.org/W2603567530', 'https://openalex.org/W6782760101', 'https://openalex.org/W2889540509', 'https://openalex.org/W2609317876', 'https://openalex.org/W3197624376', 'https://openalex.org/W2885308148', 'https://openalex.org/W2606943906', 'https://openalex.org/W2962866211', 'https://openalex.org/W4312933868', 'https://openalex.org/W6811008979', 'https://openalex.org/W4372347392', 'https://openalex.org/W4312265458', 'https://openalex.org/W4283215837', 'https://openalex.org/W4281820413', 'https://openalex.org/W6783713337', 'https://openalex.org/W6797906067', 'https://openalex.org/W4302095631', 'https://openalex.org/W2911394281', 'https://openalex.org/W4372260037', 'https://openalex.org/W6809940947', 'https://openalex.org/W4372266775', 'https://openalex.org/W4312756164', 'https://openalex.org/W4372341094', 'https://openalex.org/W6765775151', 'https://openalex.org/W2964058413', 'https://openalex.org/W6795288823', 'https://openalex.org/W6767671539', 'https://openalex.org/W6783867762', 'https://openalex.org/W6778946027', 'https://openalex.org/W6631190155', 'https://openalex.org/W6779823529', 'https://openalex.org/W2516001803', 'https://openalex.org/W2129069237', 'https://openalex.org/W1552314771', 'https://openalex.org/W4297841790', 'https://openalex.org/W6842762852', 'https://openalex.org/W4221144097', 'https://openalex.org/W4225309689', 'https://openalex.org/W3174264304', 'https://openalex.org/W6802017037', 'https://openalex.org/W2963090522', 'https://openalex.org/W3177150392', 'https://openalex.org/W3110257065', 'https://openalex.org/W1522301498', 'https://openalex.org/W3105013723', 'https://openalex.org/W2546505240', 'https://openalex.org/W3093931768', 'https://openalex.org/W4225759859', 'https://openalex.org/W4226213470', 'https://openalex.org/W3162926177', 'https://openalex.org/W3121370741', 'https://openalex.org/W2959300817', 'https://openalex.org/W4387195417', 'https://openalex.org/W1959608418', 'https://openalex.org/W2974231335', 'https://openalex.org/W3092028330', 'https://openalex.org/W3127686677', 'https://openalex.org/W3203491020', 'https://openalex.org/W4353113087', 'https://openalex.org/W3123097577', 'https://openalex.org/W3036167779', 'https://openalex.org/W2600383743', 'https://openalex.org/W3004970274']",2023-01-01
https://openalex.org/W4297841790,https://doi.org/10.21437/interspeech.2022-10653,Speech Enhancement with Score-Based Generative Models in the Complex STFT Domain,"Score-based generative models (SGMs) have recently shown impressive results for difficult generative tasks such as the unconditional and conditional generation of natural images and audio signals.In this work, we extend these models to the complex short-time Fourier transform (STFT) domain, proposing a novel training task for speech enhancement using a complex-valued deep neural network.We derive this training task within the formalism of stochastic differential equations (SDEs), thereby enabling the use of predictor-corrector samplers.We provide alternative formulations inspired by previous publications on using generative diffusion models for speech enhancement, avoiding the need for any prior assumptions on the noise distribution and making the training task purely generative which, as we show, results in improved enhancement performance.","['https://openalex.org/W2606943906', 'https://openalex.org/W2972436155', 'https://openalex.org/W3191448984', 'https://openalex.org/W2964058413', 'https://openalex.org/W4289709302', 'https://openalex.org/W3184410885', 'https://openalex.org/W3162926177', 'https://openalex.org/W2959300817', 'https://openalex.org/W4211127940', 'https://openalex.org/W2535388113', 'https://openalex.org/W4232282348', 'https://openalex.org/W3160567113', 'https://openalex.org/W3123097577', 'https://openalex.org/W2017957151', 'https://openalex.org/W3097549261', 'https://openalex.org/W1522301498', 'https://openalex.org/W3174264304', 'https://openalex.org/W2963341071', 'https://openalex.org/W3110257065', 'https://openalex.org/W2766672686', 'https://openalex.org/W1991111872', 'https://openalex.org/W4249976542', 'https://openalex.org/W2013035813', 'https://openalex.org/W3105013723', 'https://openalex.org/W2123739530', 'https://openalex.org/W3129651364', 'https://openalex.org/W3036843665', 'https://openalex.org/W3100968126', 'https://openalex.org/W3097756030', 'https://openalex.org/W3036167779', 'https://openalex.org/W4221144097', 'https://openalex.org/W2940177920', 'https://openalex.org/W1971159979', 'https://openalex.org/W3131332223', 'https://openalex.org/W2949756029', 'https://openalex.org/W2603567530', 'https://openalex.org/W4380434618']",2022-09-16
https://openalex.org/W4319862462,https://doi.org/10.1109/slt54892.2023.10023356,Exploring WavLM on Speech Enhancement,"There is a surge in interest in self-supervised learning approaches for end-to-end speech encoding in recent years as they have achieved great success. Especially, WavLM showed state-of-the-art performance on various speech processing tasks. To better understand the efficacy of self-supervised learning models for speech enhancement, in this work, we design and conduct a series of experiments with three resource conditions by combining WavLM and two high-quality speech enhancement systems. Also, We propose a regression-based WavLM training objective and a noise-mixing data configuration to further boost the downstream enhancement performance. The experiments on the DNS challenge dataset and a simulation dataset show that the WavLM benefits the speech enhancement task in terms of both speech quality and speech recognition accuracy, especially for low fine-tuning resources. For the high fine-tuning resource condition, only the word error rate is substantially improved.","['https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W6788335241', 'https://openalex.org/W4226033575', 'https://openalex.org/W6797992114', 'https://openalex.org/W2896457183', 'https://openalex.org/W3015337486', 'https://openalex.org/W3196614065', 'https://openalex.org/W4285258106', 'https://openalex.org/W3206252155', 'https://openalex.org/W3209984917', 'https://openalex.org/W3197580070', 'https://openalex.org/W3197042120', 'https://openalex.org/W4385245566', 'https://openalex.org/W3175898847', 'https://openalex.org/W3160129476', 'https://openalex.org/W3163842642', 'https://openalex.org/W3191448984', 'https://openalex.org/W3097945073', 'https://openalex.org/W3197729725', 'https://openalex.org/W3096408984', 'https://openalex.org/W3198680319', 'https://openalex.org/W6790299203', 'https://openalex.org/W4292969786', 'https://openalex.org/W6746278845', 'https://openalex.org/W4225302959', 'https://openalex.org/W2995181338', 'https://openalex.org/W6779216093', 'https://openalex.org/W3031135612', 'https://openalex.org/W6784499681', 'https://openalex.org/W3036601975', 'https://openalex.org/W2770119437', 'https://openalex.org/W3093839391', 'https://openalex.org/W3126648213', 'https://openalex.org/W3035837245', 'https://openalex.org/W3178203035']",2023-01-09
https://openalex.org/W3197478142,https://doi.org/10.21437/interspeech.2021-1983,WeNet: Production Oriented Streaming and Non-Streaming End-to-End Speech Recognition Toolkit,"In this paper, we propose an open source speech recognition toolkit called WeNet, in which a new two-pass approach named U2 is implemented to unify streaming and non-streaming endto-end (E2E) speech recognition in a single model.The main motivation of WeNet is to close the gap between the research and deployment of E2E speech recognition models.WeNet provides an efficient way to ship automatic speech recognition (ASR) applications in real-world scenarios, which is the main difference and advantage to other open source E2E speech recognition toolkits.We develop a hybird connectionist temporal classification (CTC)/attention architecture with transformer or conformer as encoder and an attention decoder to rescore th CTC hypotheses.To achieve streaming and non-streaming in a unified model, we use a dynamic chunk-based attention strategy which allows the self-attention to focus on the right context with random length.Our experiments on the AISHELL-1 dataset show that our model achieves 5.03% relative character error rate (CER) reduction in non-streaming ASR compared to a standard non-streaming transformer.After model quantification, our model achieves reasonable RTF and latency at runtime.The toolkit is publicly available at https://github.com/mobvoi/wenet.","['https://openalex.org/W1586532344', 'https://openalex.org/W3092122846', 'https://openalex.org/W3097777922', 'https://openalex.org/W2936774411', 'https://openalex.org/W2526425061', 'https://openalex.org/W3144345593', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963242190', 'https://openalex.org/W854541894', 'https://openalex.org/W3151526698', 'https://openalex.org/W2605141709', 'https://openalex.org/W2193413348', 'https://openalex.org/W3011207290', 'https://openalex.org/W2750499125', 'https://openalex.org/W4294619417', 'https://openalex.org/W3097125541', 'https://openalex.org/W1524333225', 'https://openalex.org/W3093346109', 'https://openalex.org/W3015457435', 'https://openalex.org/W3038006402', 'https://openalex.org/W1828163288', 'https://openalex.org/W2962780374', 'https://openalex.org/W1855892484', 'https://openalex.org/W3141464856', 'https://openalex.org/W3011339933', 'https://openalex.org/W2143612262']",2021-08-27
https://openalex.org/W2094721231,https://doi.org/10.1109/icsda.2013.6709856,"The voice bank corpus: Design, collection and data analysis of a large regional accent speech database","The University of Edinburgh has started the development of a new speech database, the Voice Bank corpus, specifically designed for the creation of personalised synthetic voices for individuals with speech disorders. This corpus already constitutes the largest corpora of British English currently in existence, with more than 300 hours of recordings from approximately 500 healthy speakers. New recordings are continuously being made in order to get the best coverage of the different combinations of regional accents, social classes, age and gender across Britain. This paper describes the motivation and the processes involved in the design and recording of this corpus as well as some analysis of its content. The paper concludes with our future plans to further extend this corpus and to overcome its current limitations.","['https://openalex.org/W1547848553', 'https://openalex.org/W4214700025', 'https://openalex.org/W6737881877', 'https://openalex.org/W1967390364', 'https://openalex.org/W6712918992', 'https://openalex.org/W2063918473', 'https://openalex.org/W2153914468', 'https://openalex.org/W7075637324', 'https://openalex.org/W2145247325', 'https://openalex.org/W2130722890', 'https://openalex.org/W2396988173', 'https://openalex.org/W2165143604', 'https://openalex.org/W6629685604', 'https://openalex.org/W1591095982', 'https://openalex.org/W2799018910', 'https://openalex.org/W1600722501', 'https://openalex.org/W1977899772', 'https://openalex.org/W2399389028', 'https://openalex.org/W1525540417', 'https://openalex.org/W1496801689', 'https://openalex.org/W2613904154']",2013-11-01
https://openalex.org/W4232282348,https://doi.org/10.1121/1.4799597,The Diverse Environments Multi-channel Acoustic Noise Database (DEMAND): A database of multichannel environmental noise recordings,"Multi-microphone arrays allow for the use of spatial filtering techniques that can greatly improve noise reduction and source separation. However, for speech and audio data, work on noise reduction or separation has focused primarily on one- or two-channel systems. Because of this, databases of multichannel environmental noise are not widely available. DEMAND (Diverse Environments Multi-channel Acoustic Noise Database) addresses this problem by providing a set of 16-channel noise files recorded in a variety of indoor and outdoor settings. The data was recorded using a planar microphone array consisting of four staggered rows, with the smallest distance between microphones being 5 cm and the largest being 21.8 cm. DEMAND is freely available under a Creative Commons license to encourage research into algorithms beyond the stereo setup.",[],2013-01-01
https://openalex.org/W2696967604,https://doi.org/10.1109/icassp.2017.7953152,A study on data augmentation of reverberant speech for robust speech recognition,"The environmental robustness of DNN-based acoustic models can be significantly improved by using multi-condition training data. However, as data collection is a costly proposition, simulation of the desired conditions is a frequently adopted strategy. In this paper we detail a data augmentation approach for far-field ASR. We examine the impact of using simulated room impulse responses (RIRs), as real RIRs can be difficult to acquire, and also the effect of adding point-source noises. We find that the performance gap between using simulated and real RIRs can be eliminated when point-source noises are added. Further we show that the trained acoustic models not only perform well in the distant-talking scenario but also provide better results in the close-talking scenario. We evaluate our approach on several LVCSR tasks which can adequately represent both scenarios.","['https://openalex.org/W2136682440', 'https://openalex.org/W2117678320', 'https://openalex.org/W6688816777', 'https://openalex.org/W2514741789', 'https://openalex.org/W2510616059', 'https://openalex.org/W2143612262', 'https://openalex.org/W2288817436', 'https://openalex.org/W2289912446', 'https://openalex.org/W2292733088', 'https://openalex.org/W6690610466', 'https://openalex.org/W1989314204', 'https://openalex.org/W2064675550', 'https://openalex.org/W1495076553', 'https://openalex.org/W2079623482', 'https://openalex.org/W2245569228', 'https://openalex.org/W1499864241', 'https://openalex.org/W2964138484', 'https://openalex.org/W2407080277', 'https://openalex.org/W2970850616', 'https://openalex.org/W2219249508', 'https://openalex.org/W2402146185', 'https://openalex.org/W2136439176', 'https://openalex.org/W1524333225']",2017-03-01
https://openalex.org/W2144404214,https://doi.org/10.1109/tasl.2007.911054,Evaluation of Objective Quality Measures for Speech Enhancement,"In this paper, we evaluate the performance of several objective measures in terms of predicting the quality of noisy speech enhanced by noise suppression algorithms. The objective measures considered a wide range of distortions introduced by four types of real-world noise at two signal-to-noise ratio levels by four classes of speech enhancement algorithms: spectral subtractive, subspace, statistical-model based, and Wiener algorithms. The subjective quality ratings were obtained using the ITU-T P.835 methodology designed to evaluate the quality of enhanced speech along three dimensions: signal distortion, noise distortion, and overall quality. This paper reports on the evaluation of correlations of several objective measures with these three subjective rating scales. Several new composite objective measures are also proposed by combining the individual objective measures using nonparametric and parametric regression analysis techniques.","['https://openalex.org/W6679369443', 'https://openalex.org/W2140828385', 'https://openalex.org/W6633242689', 'https://openalex.org/W1955090795', 'https://openalex.org/W2146889948', 'https://openalex.org/W2144404214', 'https://openalex.org/W1567828862', 'https://openalex.org/W2105854852', 'https://openalex.org/W2140651276', 'https://openalex.org/W1552314771', 'https://openalex.org/W2479702386', 'https://openalex.org/W6713425292', 'https://openalex.org/W1920160043', 'https://openalex.org/W1893870629', 'https://openalex.org/W4245919820', 'https://openalex.org/W2013139519', 'https://openalex.org/W2403555780', 'https://openalex.org/W1546892833', 'https://openalex.org/W2102201073', 'https://openalex.org/W1952824372', 'https://openalex.org/W1496562625', 'https://openalex.org/W1551803310', 'https://openalex.org/W2130813098', 'https://openalex.org/W1495679096']",2007-12-20
https://openalex.org/W3024869864,https://doi.org/10.21437/interspeech.2020-2650,"ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification","Current speaker verification techniques rely on a neural network to extract speaker representations. The successful x-vector architecture is a Time Delay Neural Network (TDNN) that applies statistics pooling to project variable-length utterances into fixed-length speaker characterizing embeddings. In this paper, we propose multiple enhancements to this architecture based on recent trends in the related fields of face verification and computer vision. Firstly, the initial frame layers can be restructured into 1-dimensional Res2Net modules with impactful skip connections. Similarly to SE-ResNet, we introduce Squeeze-and-Excitation blocks in these modules to explicitly model channel interdependencies. The SE block expands the temporal context of the frame layer by rescaling the channels according to global properties of the recording. Secondly, neural networks are known to learn hierarchical features, with each layer operating on a different level of complexity. To leverage this complementary information, we aggregate and propagate features of different hierarchical levels. Finally, we improve the statistics pooling module with channel-dependent frame attention. This enables the network to focus on different subsets of frames during each of the channel’s statistics estimation. The proposed ECAPA-TDNN architecture significantly outperforms state-of-the-art TDNN based systems on the VoxCeleb test sets and the 2019 VoxCeleb Speaker Recognition Challenge.","['https://openalex.org/W2696967604', 'https://openalex.org/W4288091954', 'https://openalex.org/W2808631503', 'https://openalex.org/W2888867175', 'https://openalex.org/W2936774411', 'https://openalex.org/W3015368919', 'https://openalex.org/W2972441390', 'https://openalex.org/W4309845474', 'https://openalex.org/W2713263280', 'https://openalex.org/W2964054038', 'https://openalex.org/W2981461916', 'https://openalex.org/W2219249508', 'https://openalex.org/W2889519245', 'https://openalex.org/W3027908062', 'https://openalex.org/W2938358845', 'https://openalex.org/W2991951409', 'https://openalex.org/W1589137271', 'https://openalex.org/W2794506738', 'https://openalex.org/W2726515241', 'https://openalex.org/W2951534110', 'https://openalex.org/W1836465849', 'https://openalex.org/W2969985801', 'https://openalex.org/W2963420686', 'https://openalex.org/W2972552635', 'https://openalex.org/W3042801391', 'https://openalex.org/W2747165665', 'https://openalex.org/W4300685083', 'https://openalex.org/W2928165649', 'https://openalex.org/W2890964092', 'https://openalex.org/W3010925296', 'https://openalex.org/W125553504', 'https://openalex.org/W2964121744', 'https://openalex.org/W2194775991', 'https://openalex.org/W2157161740', 'https://openalex.org/W1522301498', 'https://openalex.org/W2996132597']",2020-10-25
https://openalex.org/W4372259751,https://doi.org/10.1109/icassp49357.2023.10094858,Inter-Subnet: Speech Enhancement with Subband Interaction,"Subband-based approaches process subbands in parallel through the model with shared parameters to learn the commonality of local spectrums for noise reduction. In this way, they have achieved remarkable results with fewer parameters. However, in some complex environments, the lack of global spectral information has a negative impact on the performance of these subband-based approaches. To this end, this paper introduces the subband interaction as a new way to complement the subband model with the global spectral information such as cross-band dependencies and global spectral patterns, and proposes a new lightweight single-channel speech enhancement framework called Interactive Subband Network (Inter-SubNet). Experimental results on DNS Challenge - Interspeech 2021 dataset show that the proposed Inter-SubNet yields a significant improvement over the subband model and outperforms other state-of-the-art speech enhancement approaches, which demonstrate the effectiveness of subband interaction.","['https://openalex.org/W2998161426', 'https://openalex.org/W2291877678', 'https://openalex.org/W3198680319', 'https://openalex.org/W3045520545', 'https://openalex.org/W2605589342', 'https://openalex.org/W160800111', 'https://openalex.org/W6777701575', 'https://openalex.org/W3015197852', 'https://openalex.org/W4221149546', 'https://openalex.org/W3160085755', 'https://openalex.org/W3015372568', 'https://openalex.org/W6749954789', 'https://openalex.org/W2963881567', 'https://openalex.org/W3162493033', 'https://openalex.org/W3097906045', 'https://openalex.org/W2696967604', 'https://openalex.org/W3096408984', 'https://openalex.org/W3162501355', 'https://openalex.org/W3097034112', 'https://openalex.org/W2991361823', 'https://openalex.org/W2962866211', 'https://openalex.org/W2678916739', 'https://openalex.org/W2937484199', 'https://openalex.org/W6768469449', 'https://openalex.org/W2044893557', 'https://openalex.org/W4221162870', 'https://openalex.org/W2972404066', 'https://openalex.org/W3027637851']",2023-05-05
https://openalex.org/W3027008958,https://doi.org/10.48550/arxiv.2005.11262,LibriMix: An Open-Source Dataset for Generalizable Speech Separation,"In recent years, wsj0-2mix has become the reference dataset for single-channel speech separation. Most deep learning-based speech separation models today are benchmarked on it. However, recent studies have shown important performance drops when models trained on wsj0-2mix are evaluated on other, similar datasets. To address this generalization issue, we created LibriMix, an open-source alternative to wsj0-2mix, and to its noisy extension, WHAM!. Based on LibriSpeech, LibriMix consists of two- or three-speaker mixtures combined with ambient noise samples from WHAM!. Using Conv-TasNet, we achieve competitive performance on all LibriMix versions. In order to fairly evaluate across datasets, we introduce a third test set based on VCTK for speech and WHAM! for noise. Our experiments show that the generalization error is smaller for models trained with LibriMix than with WHAM!, in both clean and noisy conditions. Aiming towards evaluation in more realistic, conversation-like scenarios, we also release a sparsely overlapping version of LibriMix's test set.","['https://openalex.org/W2127851351', 'https://openalex.org/W3016232124', 'https://openalex.org/W3004940340', 'https://openalex.org/W1494198834', 'https://openalex.org/W2884797218', 'https://openalex.org/W2527729766', 'https://openalex.org/W1522301498', 'https://openalex.org/W2952218014', 'https://openalex.org/W2747874407', 'https://openalex.org/W3008880747', 'https://openalex.org/W2460742184', 'https://openalex.org/W2101045344', 'https://openalex.org/W2975704529', 'https://openalex.org/W2972693890', 'https://openalex.org/W3015191643', 'https://openalex.org/W2943934118', 'https://openalex.org/W2972460025', 'https://openalex.org/W3015700067', 'https://openalex.org/W262275730', 'https://openalex.org/W2962935966', 'https://openalex.org/W2407080277', 'https://openalex.org/W1591607137', 'https://openalex.org/W3015199127', 'https://openalex.org/W2972541922', 'https://openalex.org/W2964058413', 'https://openalex.org/W2024490156', 'https://openalex.org/W2972359262', 'https://openalex.org/W2221409856', 'https://openalex.org/W3021208339', 'https://openalex.org/W3015843733', 'https://openalex.org/W2125336414']",2020-05-22
https://openalex.org/W3093990297,https://doi.org/10.48550/arxiv.2010.11362,NU-GAN: High resolution neural upsampling with GAN,"In this paper, we propose NU-GAN, a new method for resampling audio from lower to higher sampling rates (upsampling). Audio upsampling is an important problem since productionizing generative speech technology requires operating at high sampling rates. Such applications use audio at a resolution of 44.1 kHz or 48 kHz, whereas current speech synthesis methods are equipped to handle a maximum of 24 kHz resolution. NU-GAN takes a leap towards solving audio upsampling as a separate component in the text-to-speech (TTS) pipeline by leveraging techniques for audio generation using GANs. ABX preference tests indicate that our NU-GAN resampler is capable of resampling 22 kHz to 44.1 kHz audio that is distinguishable from original audio only 7.4% higher than random chance for single speaker dataset, and 10.8% higher than chance for multi-speaker dataset.","['https://openalex.org/W2099471712', 'https://openalex.org/W2785678896', 'https://openalex.org/W2549139847', 'https://openalex.org/W2963073614', 'https://openalex.org/W3037695135', 'https://openalex.org/W2910577860', 'https://openalex.org/W1517841224', 'https://openalex.org/W2608207374', 'https://openalex.org/W3015338123', 'https://openalex.org/W2739619458', 'https://openalex.org/W2963300588', 'https://openalex.org/W2953331651', 'https://openalex.org/W2970006822', 'https://openalex.org/W2802034954', 'https://openalex.org/W299440670', 'https://openalex.org/W2078908559', 'https://openalex.org/W2151667147', 'https://openalex.org/W2519091744', 'https://openalex.org/W2769810959', 'https://openalex.org/W2883853252']",2020-10-22
https://openalex.org/W4382603054,https://doi.org/10.48550/arxiv.2306.15687,Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale,"Large-scale generative models such as GPT and DALL-E have revolutionized the research community. These models not only generate high fidelity outputs, but are also generalists which can solve tasks not explicitly taught. In contrast, speech generative models are still primitive in terms of scale and task generalization. In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are not filtered or enhanced. Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. In particular, Voicebox outperforms the state-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs 1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to 20 times faster. Audio samples can be found in \url{https://voicebox.metademolab.com}.",[],2023-06-23
https://openalex.org/W4379539302,https://doi.org/10.48550/arxiv.2306.02207,SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts,"Large language models (LLMs) have gained considerable attention for Artificial Intelligence Generated Content (AIGC), particularly with the emergence of ChatGPT. However, the direct adaptation of continuous speech to LLMs that process discrete tokens remains an unsolved challenge, hindering the application of LLMs for speech generation. The advanced speech LMs are in the corner, as that speech signals encapsulate a wealth of information, including speaker and emotion, beyond textual data alone. Prompt tuning has demonstrated notable gains in parameter efficiency and competitive performance on some speech classification tasks. However, the extent to which prompts can effectively elicit generation tasks from speech LMs remains an open question. In this paper, we present pioneering research that explores the application of prompt tuning to stimulate speech LMs for various generation tasks, within a unified framework called SpeechGen, with around 10M trainable parameters. The proposed unified framework holds great promise for efficiency and effectiveness, particularly with the imminent arrival of advanced speech LMs, which will significantly enhance the capabilities of the framework. The code and demos of SpeechGen will be available on the project website: \url{https://ga642381.github.io/SpeechPrompt/speechgen}",[],2023-06-03
https://openalex.org/W4287887366,https://doi.org/10.18653/v1/2022.naacl-demo.1,textless-lib: a Library for Textless Spoken Language Processing,"Eugene Kharitonov, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen, Paden Tomasello, Ann Lee, Ali Elkahky, Wei-Ning Hsu, Abdelrahman Mohamed, Emmanuel Dupoux, Yossi Adi. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations. 2022.","['https://openalex.org/W2964243274', 'https://openalex.org/W2946200149', 'https://openalex.org/W3015338123', 'https://openalex.org/W3148101939', 'https://openalex.org/W3209984917', 'https://openalex.org/W3093096176', 'https://openalex.org/W3033411150', 'https://openalex.org/W2250539671', 'https://openalex.org/W2965373594', 'https://openalex.org/W4226033575', 'https://openalex.org/W4287591426', 'https://openalex.org/W4297808394', 'https://openalex.org/W3036601975', 'https://openalex.org/W3169320628', 'https://openalex.org/W4287854499', 'https://openalex.org/W3140429000', 'https://openalex.org/W4286984129', 'https://openalex.org/W2893425640', 'https://openalex.org/W3092028330', 'https://openalex.org/W2752177116', 'https://openalex.org/W2516090925', 'https://openalex.org/W3180374548', 'https://openalex.org/W4292779060', 'https://openalex.org/W4200635400', 'https://openalex.org/W4394671563', 'https://openalex.org/W3197974236', 'https://openalex.org/W2973026522', 'https://openalex.org/W2950018712', 'https://openalex.org/W3141523618', 'https://openalex.org/W4307680525', 'https://openalex.org/W3144810982', 'https://openalex.org/W2963821905', 'https://openalex.org/W3198815374', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962739339', 'https://openalex.org/W2799124508', 'https://openalex.org/W2995181338', 'https://openalex.org/W2970006822', 'https://openalex.org/W2896457183', 'https://openalex.org/W3161348170', 'https://openalex.org/W4308349017', 'https://openalex.org/W4286899907', 'https://openalex.org/W4287374065', 'https://openalex.org/W3213873715', 'https://openalex.org/W2962866891', 'https://openalex.org/W2973049979', 'https://openalex.org/W2515741950', 'https://openalex.org/W2963300588']",2022-01-01
https://openalex.org/W3096723250,https://doi.org/10.1109/icassp39728.2021.9414880,Emotion Recognition by Fusing Time Synchronous and Time Asynchronous Representations,"In this paper, a novel two-branch neural network model structure is proposed\nfor multimodal emotion recognition, which consists of a time synchronous branch\n(TSB) and a time asynchronous branch (TAB). To capture correlations between\neach word and its acoustic realisation, the TSB combines speech and text\nmodalities at each input window frame and then does pooling across time to form\na single embedding vector. The TAB, by contrast, provides cross-utterance\ninformation by integrating sentence text embeddings from a number of context\nutterances into another embedding vector. The final emotion classification uses\nboth the TSB and the TAB embeddings. Experimental results on the IEMOCAP\ndataset demonstrate that the two-branch structure achieves state-of-the-art\nresults in 4-way classification with all common test setups. When using\nautomatic speech recognition (ASR) output instead of manually transcribed\nreference text, it is shown that the cross-utterance information considerably\nimproves the robustness against ASR errors. Furthermore, by incorporating an\nextra class for all the other emotions, the final 5-way classification system\nwith ASR hypotheses can be viewed as a prototype for more realistic emotion\nrecognition systems.\n","['https://openalex.org/W2966518489', 'https://openalex.org/W2963466847', 'https://openalex.org/W6750449527', 'https://openalex.org/W2962770129', 'https://openalex.org/W2963104701', 'https://openalex.org/W2937584914', 'https://openalex.org/W2963702064', 'https://openalex.org/W3015964358', 'https://openalex.org/W6755207826', 'https://openalex.org/W6735377749', 'https://openalex.org/W2146334809', 'https://openalex.org/W99485238', 'https://openalex.org/W6657744143', 'https://openalex.org/W2551548999', 'https://openalex.org/W2610961739', 'https://openalex.org/W2997258743', 'https://openalex.org/W2156146072', 'https://openalex.org/W2164699598', 'https://openalex.org/W3015489952', 'https://openalex.org/W2795986449', 'https://openalex.org/W2584561145', 'https://openalex.org/W3015240477', 'https://openalex.org/W2009059481', 'https://openalex.org/W1965069145', 'https://openalex.org/W3015707499', 'https://openalex.org/W141927798', 'https://openalex.org/W2250539671', 'https://openalex.org/W2085628288', 'https://openalex.org/W2963654251', 'https://openalex.org/W2963962398', 'https://openalex.org/W4299280181', 'https://openalex.org/W943204654', 'https://openalex.org/W3148978181', 'https://openalex.org/W2896457183', 'https://openalex.org/W2797947982', 'https://openalex.org/W2963341956', 'https://openalex.org/W2597655663', 'https://openalex.org/W2028975938', 'https://openalex.org/W2963386218', 'https://openalex.org/W2973034847']",2021-05-13
https://openalex.org/W2130821326,https://doi.org/10.1080/02699931.2010.516915,Emotional speech processing: Disentangling the effects of prosody and semantic cues,"To inform how emotions in speech are implicitly processed and registered in memory, we compared how emotional prosody, emotional semantics, and both cues in tandem prime decisions about conjoined emotional faces. Fifty-two participants rendered facial affect decisions (Pell, 2005a), indicating whether a target face represented an emotion (happiness or sadness) or not (a facial grimace), after passively listening to happy, sad, or neutral prime utterances. Emotional information from primes was conveyed by: (1) prosody only; (2) semantic cues only; or (3) combined prosody and semantic cues. Results indicated that prosody, semantics, and combined prosody-semantic cues facilitate emotional decisions about target faces in an emotion-congruent manner. However, the magnitude of priming did not vary across tasks. Our findings highlight that emotional meanings of prosody and semantic cues are systematically registered during speech processing, but with similar effects on associative knowledge about emotions, which is presumably shared by prosody, semantics, and faces.","['https://openalex.org/W1969708157', 'https://openalex.org/W2004082109', 'https://openalex.org/W4245744384', 'https://openalex.org/W2137335776', 'https://openalex.org/W4247434994', 'https://openalex.org/W2165869726', 'https://openalex.org/W4294351883', 'https://openalex.org/W2033895081', 'https://openalex.org/W2021822050', 'https://openalex.org/W2067134659', 'https://openalex.org/W2030162035', 'https://openalex.org/W2060691228', 'https://openalex.org/W2083759082', 'https://openalex.org/W1982487731', 'https://openalex.org/W2077019552', 'https://openalex.org/W2115299070', 'https://openalex.org/W2096053194', 'https://openalex.org/W2075274068', 'https://openalex.org/W2066632907', 'https://openalex.org/W1966797434', 'https://openalex.org/W4248968843', 'https://openalex.org/W2153705299', 'https://openalex.org/W1606700437', 'https://openalex.org/W1991660506', 'https://openalex.org/W2086827435', 'https://openalex.org/W2155931446', 'https://openalex.org/W2029615109', 'https://openalex.org/W2124478113', 'https://openalex.org/W2117789297', 'https://openalex.org/W2007962718', 'https://openalex.org/W2159259520', 'https://openalex.org/W2499542043', 'https://openalex.org/W1996426642', 'https://openalex.org/W2053672718', 'https://openalex.org/W2073414813', 'https://openalex.org/W2046187867', 'https://openalex.org/W2045154139', 'https://openalex.org/W2039420410', 'https://openalex.org/W2149544884', 'https://openalex.org/W2150513111', 'https://openalex.org/W1992424514', 'https://openalex.org/W2168147286', 'https://openalex.org/W2126079704', 'https://openalex.org/W2008042149', 'https://openalex.org/W2157599703', 'https://openalex.org/W1988957796', 'https://openalex.org/W2044079752', 'https://openalex.org/W1970300157', 'https://openalex.org/W1998579645', 'https://openalex.org/W2087133078', 'https://openalex.org/W2108161905', 'https://openalex.org/W1968970822', 'https://openalex.org/W2084220850', 'https://openalex.org/W2062955919', 'https://openalex.org/W90464684', 'https://openalex.org/W2073574212', 'https://openalex.org/W1967021946', 'https://openalex.org/W1981454880', 'https://openalex.org/W1968696961', 'https://openalex.org/W2168730540', 'https://openalex.org/W2041771838', 'https://openalex.org/W2074359113', 'https://openalex.org/W2158564203', 'https://openalex.org/W2078806205', 'https://openalex.org/W2003547693', 'https://openalex.org/W2018338387', 'https://openalex.org/W2114483827', 'https://openalex.org/W2093640073', 'https://openalex.org/W2088677122', 'https://openalex.org/W2019233622', 'https://openalex.org/W2157719007', 'https://openalex.org/W2142801638', 'https://openalex.org/W2155138947', 'https://openalex.org/W1981618987', 'https://openalex.org/W2148573240', 'https://openalex.org/W1964498975', 'https://openalex.org/W2011976355', 'https://openalex.org/W2121329867', 'https://openalex.org/W2089957025', 'https://openalex.org/W2038457328', 'https://openalex.org/W2079697063', 'https://openalex.org/W2118789253', 'https://openalex.org/W4250366375', 'https://openalex.org/W2067159990', 'https://openalex.org/W2197960846', 'https://openalex.org/W2102573486', 'https://openalex.org/W4231612164', 'https://openalex.org/W1990561576', 'https://openalex.org/W2126096425', 'https://openalex.org/W2154741607', 'https://openalex.org/W2064740447']",2010-11-12
https://openalex.org/W4205742757,https://doi.org/10.1016/j.specom.2021.11.006,"Emotional voice conversion: Theory, databases and ESD","In this paper, we first provide a review of the state-of-the-art emotional voice conversion research, and the existing emotional speech databases. We then motivate the development of a novel emotional speech database (ESD) that addresses the increasing research need. With this paper, the ESD database1 is now made available to the research community. The ESD database consists of 350 parallel utterances spoken by 10 native English and 10 native Chinese speakers and covers 5 emotion categories (neutral, happy, angry, sad and surprise). More than 29 h of speech data were recorded in a controlled acoustic environment. The database is suitable for multi-speaker and cross-lingual emotional voice conversion studies. As case studies, we implement several state-of-the-art emotional voice conversion systems on the ESD database. This paper provides a reference study on ESD in conjunction with its release.","['https://openalex.org/W6679459330', 'https://openalex.org/W2035962301', 'https://openalex.org/W2077801020', 'https://openalex.org/W6660513934', 'https://openalex.org/W2984809863', 'https://openalex.org/W3012970712', 'https://openalex.org/W6943714206', 'https://openalex.org/W2997399314', 'https://openalex.org/W6748472120', 'https://openalex.org/W6781381461', 'https://openalex.org/W1971362524', 'https://openalex.org/W4245744384', 'https://openalex.org/W6768045828', 'https://openalex.org/W2129703931', 'https://openalex.org/W2478838513', 'https://openalex.org/W6772034331', 'https://openalex.org/W6607193717', 'https://openalex.org/W2146334809', 'https://openalex.org/W2342475039', 'https://openalex.org/W6784094172', 'https://openalex.org/W2030931454', 'https://openalex.org/W3095930733', 'https://openalex.org/W2086796102', 'https://openalex.org/W4245885054', 'https://openalex.org/W2963767194', 'https://openalex.org/W3136699727', 'https://openalex.org/W6760740948', 'https://openalex.org/W2785608393', 'https://openalex.org/W6617505242', 'https://openalex.org/W2190260761', 'https://openalex.org/W6663401105', 'https://openalex.org/W2157412983', 'https://openalex.org/W6781851318', 'https://openalex.org/W1966797434', 'https://openalex.org/W2074788634', 'https://openalex.org/W2759244429', 'https://openalex.org/W2066090536', 'https://openalex.org/W6776193403', 'https://openalex.org/W3012404734', 'https://openalex.org/W6676463824', 'https://openalex.org/W1978136968', 'https://openalex.org/W6602386084', 'https://openalex.org/W2121387787', 'https://openalex.org/W2167510900', 'https://openalex.org/W2090777335', 'https://openalex.org/W2085662862', 'https://openalex.org/W6749868174', 'https://openalex.org/W6632144934', 'https://openalex.org/W2899361462', 'https://openalex.org/W6725771000', 'https://openalex.org/W6674946611', 'https://openalex.org/W2025905516', 'https://openalex.org/W2105160541', 'https://openalex.org/W6634507583', 'https://openalex.org/W6728445508', 'https://openalex.org/W1974843131', 'https://openalex.org/W6608969598', 'https://openalex.org/W2148846882', 'https://openalex.org/W6754339523', 'https://openalex.org/W2005885879', 'https://openalex.org/W6678506862', 'https://openalex.org/W6766442783', 'https://openalex.org/W2081837280', 'https://openalex.org/W6756504009', 'https://openalex.org/W6822748163', 'https://openalex.org/W6770351630', 'https://openalex.org/W4395958010', 'https://openalex.org/W6603838645', 'https://openalex.org/W1994198923', 'https://openalex.org/W6676417963', 'https://openalex.org/W2966387353', 'https://openalex.org/W6604530167', 'https://openalex.org/W3155308523', 'https://openalex.org/W6785011255', 'https://openalex.org/W6726425448', 'https://openalex.org/W2510170536', 'https://openalex.org/W6785004522', 'https://openalex.org/W3094635600', 'https://openalex.org/W2803193013', 'https://openalex.org/W2793479148', 'https://openalex.org/W6743301281', 'https://openalex.org/W2950542544', 'https://openalex.org/W6725596946', 'https://openalex.org/W7028336066', 'https://openalex.org/W2018658329', 'https://openalex.org/W6686269052', 'https://openalex.org/W6726456654', 'https://openalex.org/W6713120313', 'https://openalex.org/W2576309025', 'https://openalex.org/W2471520273', 'https://openalex.org/W4240592325', 'https://openalex.org/W2981087920', 'https://openalex.org/W6711854987', 'https://openalex.org/W3095012670', 'https://openalex.org/W2039800941', 'https://openalex.org/W6752614923', 'https://openalex.org/W2915760784', 'https://openalex.org/W2165857685', 'https://openalex.org/W6775864090', 'https://openalex.org/W6761309493', 'https://openalex.org/W2149628368', 'https://openalex.org/W6768268957', 'https://openalex.org/W6744609156', 'https://openalex.org/W2165894799', 'https://openalex.org/W176245312', 'https://openalex.org/W2803098682', 'https://openalex.org/W2894821115', 'https://openalex.org/W6775618791', 'https://openalex.org/W2057563799', 'https://openalex.org/W6768278692', 'https://openalex.org/W3097962967', 'https://openalex.org/W2972366998', 'https://openalex.org/W6752524494', 'https://openalex.org/W6754355654', 'https://openalex.org/W2785978752', 'https://openalex.org/W6781662123', 'https://openalex.org/W6774817744', 'https://openalex.org/W6754413460', 'https://openalex.org/W2941094131', 'https://openalex.org/W6758191806', 'https://openalex.org/W2753840835', 'https://openalex.org/W6639015303', 'https://openalex.org/W6678929712', 'https://openalex.org/W6679436768', 'https://openalex.org/W6737641361', 'https://openalex.org/W6755930846', 'https://openalex.org/W6683855838', 'https://openalex.org/W6676002441', 'https://openalex.org/W6767795613', 'https://openalex.org/W6758366080', 'https://openalex.org/W2972699445', 'https://openalex.org/W2120605154', 'https://openalex.org/W6720742747', 'https://openalex.org/W6770189186', 'https://openalex.org/W6739901393', 'https://openalex.org/W6713858911', 'https://openalex.org/W6698098957', 'https://openalex.org/W6742769468', 'https://openalex.org/W6679081286', 'https://openalex.org/W6684200426', 'https://openalex.org/W6775421620', 'https://openalex.org/W4395959004', 'https://openalex.org/W2330979245', 'https://openalex.org/W2883743124', 'https://openalex.org/W2085013480', 'https://openalex.org/W6601306257', 'https://openalex.org/W2972359262', 'https://openalex.org/W2996414377', 'https://openalex.org/W2897353073', 'https://openalex.org/W2972999331', 'https://openalex.org/W3154451338', 'https://openalex.org/W4235201968', 'https://openalex.org/W3025680351', 'https://openalex.org/W6785164032', 'https://openalex.org/W6784688130', 'https://openalex.org/W3096939667', 'https://openalex.org/W2962793481', 'https://openalex.org/W6774467145', 'https://openalex.org/W6608022165', 'https://openalex.org/W2158504378', 'https://openalex.org/W4245692952', 'https://openalex.org/W2972667718', 'https://openalex.org/W4300482596', 'https://openalex.org/W3101689408', 'https://openalex.org/W4285074441', 'https://openalex.org/W388732865', 'https://openalex.org/W4236377898', 'https://openalex.org/W3206840074', 'https://openalex.org/W3168542456', 'https://openalex.org/W2962896155', 'https://openalex.org/W4319782399', 'https://openalex.org/W2972659941', 'https://openalex.org/W3047855478', 'https://openalex.org/W2512798946', 'https://openalex.org/W4302402037', 'https://openalex.org/W2610171128', 'https://openalex.org/W4235670393', 'https://openalex.org/W4248867778', 'https://openalex.org/W2787928077', 'https://openalex.org/W2914763987', 'https://openalex.org/W3045447113', 'https://openalex.org/W4385245566', 'https://openalex.org/W3014201970', 'https://openalex.org/W2936412271', 'https://openalex.org/W3098557217', 'https://openalex.org/W3194143312', 'https://openalex.org/W4255540942', 'https://openalex.org/W2161736993', 'https://openalex.org/W4250640105', 'https://openalex.org/W4253874399', 'https://openalex.org/W3198791321', 'https://openalex.org/W3197993066', 'https://openalex.org/W2774848319', 'https://openalex.org/W2162295204', 'https://openalex.org/W2802975219', 'https://openalex.org/W1959608418', 'https://openalex.org/W3082130377', 'https://openalex.org/W2559655401', 'https://openalex.org/W1588037970', 'https://openalex.org/W3008297462', 'https://openalex.org/W2614201817', 'https://openalex.org/W2963035245', 'https://openalex.org/W2767014232', 'https://openalex.org/W1490382748', 'https://openalex.org/W2483796469', 'https://openalex.org/W4252531498', 'https://openalex.org/W3096567388', 'https://openalex.org/W2505874150', 'https://openalex.org/W3095169545', 'https://openalex.org/W4237670716', 'https://openalex.org/W2810914326', 'https://openalex.org/W2995401804', 'https://openalex.org/W3080698515', 'https://openalex.org/W2998572311', 'https://openalex.org/W2606429533', 'https://openalex.org/W4376524021', 'https://openalex.org/W3015669407', 'https://openalex.org/W2998249245', 'https://openalex.org/W3099078140', 'https://openalex.org/W3109943296', 'https://openalex.org/W2187089797', 'https://openalex.org/W4210849719', 'https://openalex.org/W2593463961']",2021-12-20
https://openalex.org/W2069924379,https://doi.org/10.1111/1467-8721.00013,Vocal Expression and Perception of Emotion,"Speech is an acoustically rich signal that provides considerable personal information about talkers. The expression of emotions in speech sounds and corresponding abilities to perceive such emotions are both fundamental aspects of human communication. Findings from studies seeking to characterize the acoustic properties of emotional speech indicate that speech acoustics provide an external cue to the level of nonspecific arousal associated with emotionalprocesses and to a lesser extent, the relative pleasantness of experienced emotions. Outcomes from perceptual tests show that listeners are able to accurately judge emotions from speech at rates far greater than expected by chance. More detailed characterizations of these production and perception aspects of vocal communication will necessarily involve knowledge aboutdifferences among talkers, such as those components of speech that provide comparatively stable cues to individual talkers identities.","['https://openalex.org/W2037667431', 'https://openalex.org/W1971362524', 'https://openalex.org/W2040429400', 'https://openalex.org/W4245744384', 'https://openalex.org/W1980650959', 'https://openalex.org/W2003547693', 'https://openalex.org/W2483844678', 'https://openalex.org/W2122348661', 'https://openalex.org/W2018338387', 'https://openalex.org/W2118789253', 'https://openalex.org/W1982341046', 'https://openalex.org/W1986418932', 'https://openalex.org/W2165683312', 'https://openalex.org/W2321563513', 'https://openalex.org/W1973378890', 'https://openalex.org/W2045565604', 'https://openalex.org/W2043230575', 'https://openalex.org/W2042336469', 'https://openalex.org/W2049363122', 'https://openalex.org/W2024998691', 'https://openalex.org/W2980164261', 'https://openalex.org/W1970553357', 'https://openalex.org/W2119464110', 'https://openalex.org/W2142519861', 'https://openalex.org/W1979106843']",1999-04-01
https://openalex.org/W4226487411,https://doi.org/10.1109/icme52920.2022.9859946,Unsupervised Quantized Prosody Representation for Controllable Speech Synthesis,"In this paper, we propose a novel prosody disentangle method for prosodic Text-to-Speech (TTS) model, which introduces the vector quantization (VQ) method to the auxiliary prosody encoder to obtain the decomposed prosody representations in an unsupervised manner. Rely on its advantages, the speaking styles, such as pitch, speaking velocity, local pitch variance, etc., are decomposed automatically into the latent quantize vectors. We also investigate the internal mechanism of VQ disentangle process by means of a latent variables counter and find that higher value dimensions usually represent prosody information. Experiments show that our model can control the speaking styles of synthesis results by directly manipulating the latent variables. The objective and subjective evaluations illustrated that our model outperforms the popular models.","['https://openalex.org/W3016021263', 'https://openalex.org/W2904459034', 'https://openalex.org/W3007067948', 'https://openalex.org/W6755300632', 'https://openalex.org/W2752796333', 'https://openalex.org/W6762931180', 'https://openalex.org/W3096524539', 'https://openalex.org/W6748409065', 'https://openalex.org/W6623517193', 'https://openalex.org/W6763832098', 'https://openalex.org/W2964243274', 'https://openalex.org/W6750489868', 'https://openalex.org/W6778823374', 'https://openalex.org/W3010916717', 'https://openalex.org/W2966387353', 'https://openalex.org/W6736996214', 'https://openalex.org/W2962691331', 'https://openalex.org/W2591927543', 'https://openalex.org/W2091425152', 'https://openalex.org/W2025722797', 'https://openalex.org/W6675938391', 'https://openalex.org/W2130086727', 'https://openalex.org/W2972394484', 'https://openalex.org/W2107860279', 'https://openalex.org/W4298580827', 'https://openalex.org/W2794490148', 'https://openalex.org/W2971074500', 'https://openalex.org/W2946200149', 'https://openalex.org/W2963799213', 'https://openalex.org/W3033411150', 'https://openalex.org/W4289383906', 'https://openalex.org/W854541894', 'https://openalex.org/W2107740512', 'https://openalex.org/W2963609956']",2022-07-18
https://openalex.org/W2904459034,https://doi.org/10.1109/icassp.2019.8683623,Learning Latent Representations for Style Control and Transfer in End-to-end Speech Synthesis,"In this paper, we introduce the Variational Autoencoder (VAE) to an end-to-end speech synthesis model, to learn the latent representation of speaking styles in an unsupervised manner. The style representation learned through VAE shows good properties such as disentangling, scaling, and combination, which makes it easy for style control. Style transfer can be achieved in this framework by first inferring style representation through the recognition network of VAE, then feeding it into TTS network to guide the style in synthesizing speech. To avoid Kullback-Leibler (KL) divergence collapse in training, several techniques are adopted. Finally, the proposed model shows good performance of style control and outperforms Global Style Token (GST) model in ABX preference tests on style transfer.","['https://openalex.org/W2962691331', 'https://openalex.org/W2962850167', 'https://openalex.org/W6720208624', 'https://openalex.org/W2131774270', 'https://openalex.org/W2064675550', 'https://openalex.org/W6714142977', 'https://openalex.org/W6623517193', 'https://openalex.org/W6750489868', 'https://openalex.org/W2885800352', 'https://openalex.org/W2795109282', 'https://openalex.org/W6640963894', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963223306', 'https://openalex.org/W2892140764', 'https://openalex.org/W2807692250', 'https://openalex.org/W4320013936', 'https://openalex.org/W2962970071', 'https://openalex.org/W2963272440', 'https://openalex.org/W4293714597', 'https://openalex.org/W1959608418', 'https://openalex.org/W2963927338', 'https://openalex.org/W4295731579', 'https://openalex.org/W2753738274', 'https://openalex.org/W2099471712', 'https://openalex.org/W2519091744', 'https://openalex.org/W2794490148', 'https://openalex.org/W2963691546', 'https://openalex.org/W2467604901', 'https://openalex.org/W854541894']",2019-04-17
https://openalex.org/W4200635083,https://doi.org/10.1109/tnnls.2022.3191677,LipSound2: Self-Supervised Pre-Training for Lip-to-Speech Reconstruction and Lip Reading,"The aim of this work is to investigate the impact of crossmodal self-supervised pre-training for speech reconstruction (video-to-audio) by leveraging the natural co-occurrence of audio and visual streams in videos. We propose LipSound2 that consists of an encoder-decoder architecture and location-aware attention mechanism to map face image sequences to mel-scale spectrograms directly without requiring any human annotations. The proposed LipSound2 model is first pre-trained on ∼ 2400 -h multilingual (e.g., English and German) audio-visual data (VoxCeleb2). To verify the generalizability of the proposed method, we then fine-tune the pre-trained model on domain-specific datasets (GRID and TCD-TIMIT) for English speech reconstruction and achieve a significant improvement on speech quality and intelligibility compared to previous approaches in speaker-dependent and speaker-independent settings. In addition to English, we conduct Chinese speech reconstruction on the Chinese Mandarin Lip Reading (CMLR) dataset to verify the impact on transferability. Finally, we train the cascaded lip reading (video-to-text) system by fine-tuning the generated audios on a pre-trained speech recognition system and achieve the state-of-the-art performance on both English and Chinese benchmark datasets.","['https://openalex.org/W2061960534', 'https://openalex.org/W2551572271', 'https://openalex.org/W2890952074', 'https://openalex.org/W3096214032', 'https://openalex.org/W3024147341', 'https://openalex.org/W2512544816', 'https://openalex.org/W2714726990', 'https://openalex.org/W1984483979', 'https://openalex.org/W2133906911', 'https://openalex.org/W2008120082', 'https://openalex.org/W2159089611', 'https://openalex.org/W2140647972', 'https://openalex.org/W2890413327', 'https://openalex.org/W2767459507', 'https://openalex.org/W123135133', 'https://openalex.org/W3096806995', 'https://openalex.org/W2585824449', 'https://openalex.org/W2964352155', 'https://openalex.org/W2972852451', 'https://openalex.org/W2049686551', 'https://openalex.org/W2471520273', 'https://openalex.org/W6754048563', 'https://openalex.org/W6785011006', 'https://openalex.org/W2120847449', 'https://openalex.org/W2963300588', 'https://openalex.org/W2973215447', 'https://openalex.org/W2293856338', 'https://openalex.org/W2625027024', 'https://openalex.org/W2963019222', 'https://openalex.org/W2029199293', 'https://openalex.org/W2963936489', 'https://openalex.org/W2972563022', 'https://openalex.org/W2964243274', 'https://openalex.org/W3035626590', 'https://openalex.org/W3096650361', 'https://openalex.org/W2127141656', 'https://openalex.org/W2770785043', 'https://openalex.org/W2913144232', 'https://openalex.org/W16682072', 'https://openalex.org/W2106284211', 'https://openalex.org/W6751887642', 'https://openalex.org/W6752203281', 'https://openalex.org/W6732872814', 'https://openalex.org/W2267805933', 'https://openalex.org/W2963528589', 'https://openalex.org/W2594690981', 'https://openalex.org/W2897492880', 'https://openalex.org/W6763296340', 'https://openalex.org/W3016011581', 'https://openalex.org/W2735762732', 'https://openalex.org/W2963356069', 'https://openalex.org/W2972756321', 'https://openalex.org/W2964283370', 'https://openalex.org/W6747899497', 'https://openalex.org/W343636949', 'https://openalex.org/W2321533354', 'https://openalex.org/W2326925005', 'https://openalex.org/W219040644', 'https://openalex.org/W2487442924', 'https://openalex.org/W2963426332', 'https://openalex.org/W6636510571', 'https://openalex.org/W6755207826', 'https://openalex.org/W6768021236', 'https://openalex.org/W6769311223', 'https://openalex.org/W2619697695', 'https://openalex.org/W2950864153', 'https://openalex.org/W2963115079', 'https://openalex.org/W3158986867', 'https://openalex.org/W2341528187', 'https://openalex.org/W6623517193', 'https://openalex.org/W6679855610', 'https://openalex.org/W2950936614', 'https://openalex.org/W2808631503', 'https://openalex.org/W2015143272', 'https://openalex.org/W2999528291', 'https://openalex.org/W1494198834', 'https://openalex.org/W6754473786', 'https://openalex.org/W1552314771', 'https://openalex.org/W2516001803', 'https://openalex.org/W6621543089', 'https://openalex.org/W6631943919', 'https://openalex.org/W6750475725', 'https://openalex.org/W3128750130', 'https://openalex.org/W3091641623', 'https://openalex.org/W2963030892', 'https://openalex.org/W3086926995', 'https://openalex.org/W2973085229', 'https://openalex.org/W2996970093', 'https://openalex.org/W2963894079', 'https://openalex.org/W854541894', 'https://openalex.org/W4287199820', 'https://openalex.org/W2947883281', 'https://openalex.org/W2134800885', 'https://openalex.org/W2411745509', 'https://openalex.org/W4288088047', 'https://openalex.org/W3103801904', 'https://openalex.org/W4252708392', 'https://openalex.org/W2952746495', 'https://openalex.org/W2896457183', 'https://openalex.org/W4289761690', 'https://openalex.org/W4298112588', 'https://openalex.org/W2996428491', 'https://openalex.org/W4287608901', 'https://openalex.org/W4300503772', 'https://openalex.org/W2785325870', 'https://openalex.org/W1614298861', 'https://openalex.org/W3101998545', 'https://openalex.org/W3011959832', 'https://openalex.org/W4293862381', 'https://openalex.org/W4250689965', 'https://openalex.org/W1533861849', 'https://openalex.org/W2519091744', 'https://openalex.org/W2890052321', 'https://openalex.org/W2889048668', 'https://openalex.org/W3034999214', 'https://openalex.org/W648786980']",2022-07-22
https://openalex.org/W2928165649,https://doi.org/10.1109/tpami.2019.2938758,Res2Net: A New Multi-Scale Backbone Architecture,"Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on https://mmcheng.net/res2net/.","['https://openalex.org/W2151103935', 'https://openalex.org/W1903029394', 'https://openalex.org/W6639102338', 'https://openalex.org/W2565639579', 'https://openalex.org/W6638444622', 'https://openalex.org/W2086791339', 'https://openalex.org/W2560622558', 'https://openalex.org/W6785652829', 'https://openalex.org/W1996326832', 'https://openalex.org/W2939217524', 'https://openalex.org/W2963323244', 'https://openalex.org/W2963032190', 'https://openalex.org/W2964137095', 'https://openalex.org/W2742165450', 'https://openalex.org/W6684191040', 'https://openalex.org/W2560023338', 'https://openalex.org/W6787972765', 'https://openalex.org/W2948300571', 'https://openalex.org/W2982331121', 'https://openalex.org/W6639359414', 'https://openalex.org/W2963722952', 'https://openalex.org/W1942214758', 'https://openalex.org/W2160613239', 'https://openalex.org/W2057175746', 'https://openalex.org/W2144794286', 'https://openalex.org/W2109255472', 'https://openalex.org/W2963150697', 'https://openalex.org/W2569272946', 'https://openalex.org/W2194775991', 'https://openalex.org/W2963446712', 'https://openalex.org/W2752782242', 'https://openalex.org/W6694260854', 'https://openalex.org/W2097117768', 'https://openalex.org/W2039313011', 'https://openalex.org/W2002781701', 'https://openalex.org/W845365781', 'https://openalex.org/W2549139847', 'https://openalex.org/W6750378959', 'https://openalex.org/W2600144439', 'https://openalex.org/W2560092514', 'https://openalex.org/W2183341477', 'https://openalex.org/W6740164494', 'https://openalex.org/W6762396321', 'https://openalex.org/W6753767121', 'https://openalex.org/W2888232411', 'https://openalex.org/W4239147634', 'https://openalex.org/W6745499037', 'https://openalex.org/W2531409750', 'https://openalex.org/W2037227137', 'https://openalex.org/W2031489346', 'https://openalex.org/W2102605133', 'https://openalex.org/W2949662773', 'https://openalex.org/W1772076007', 'https://openalex.org/W2412782625', 'https://openalex.org/W6753261499', 'https://openalex.org/W6748481559', 'https://openalex.org/W6739696289', 'https://openalex.org/W6761630670', 'https://openalex.org/W2988396473', 'https://openalex.org/W6682864246', 'https://openalex.org/W2962858109', 'https://openalex.org/W2916798096', 'https://openalex.org/W6637373629', 'https://openalex.org/W2962802300', 'https://openalex.org/W2963011882', 'https://openalex.org/W2117539524', 'https://openalex.org/W6620707391', 'https://openalex.org/W3125520697', 'https://openalex.org/W2938458886', 'https://openalex.org/W3027763298', 'https://openalex.org/W2992308087', 'https://openalex.org/W2963911037', 'https://openalex.org/W3085685449', 'https://openalex.org/W2963420686', 'https://openalex.org/W3035012694', 'https://openalex.org/W2630837129', 'https://openalex.org/W3035303837', 'https://openalex.org/W3118608800', 'https://openalex.org/W2823177849', 'https://openalex.org/W2037954058', 'https://openalex.org/W2962835968', 'https://openalex.org/W3106250896', 'https://openalex.org/W1861492603', 'https://openalex.org/W2963855133', 'https://openalex.org/W2964309882', 'https://openalex.org/W2963241429', 'https://openalex.org/W3109623941', 'https://openalex.org/W2964350391', 'https://openalex.org/W3023803297', 'https://openalex.org/W2963674932', 'https://openalex.org/W2163605009', 'https://openalex.org/W1686810756', 'https://openalex.org/W3132928791', 'https://openalex.org/W639708223', 'https://openalex.org/W3102710196', 'https://openalex.org/W2010181071', 'https://openalex.org/W3014367186', 'https://openalex.org/W2156303437', 'https://openalex.org/W2963402313', 'https://openalex.org/W2883780447', 'https://openalex.org/W3164098653', 'https://openalex.org/W4309845474', 'https://openalex.org/W2613718673', 'https://openalex.org/W2940262938', 'https://openalex.org/W3034550159', 'https://openalex.org/W3092344722', 'https://openalex.org/W2274287116', 'https://openalex.org/W3104792420', 'https://openalex.org/W3129581972', 'https://openalex.org/W2964166828', 'https://openalex.org/W1894057436', 'https://openalex.org/W3104979525', 'https://openalex.org/W4308909683', 'https://openalex.org/W2766839578', 'https://openalex.org/W3098389804']",2019-08-30
https://openalex.org/W2402146185,https://doi.org/10.21437/interspeech.2015-647,A time delay neural network architecture for efficient modeling of long temporal contexts,Recurrent neural network architectures have been shown to efficiently model long term temporal dependencies between acoustic events. However the training time of recurrent networks is higher than feedforward networks due to the sequential nature of the learning algorithm. In this paper we propose a time delay neural network architecture which models long term temporal dependencies with training times comparable to standard feed-forward DNNs. The network uses sub-sampling to reduce computation during training. On the Switchboard task we show a relative improvement of 6% over the baseline DNN model. We present results on several LVCSR tasks with training data ranging from 3 to 1800 hours to show the effectiveness of the TDNN architecture in learning wider temporal dependencies in both small and large data scenarios.,"['https://openalex.org/W2404463488', 'https://openalex.org/W97072897', 'https://openalex.org/W2131342762', 'https://openalex.org/W2170580867', 'https://openalex.org/W1643320849', 'https://openalex.org/W2400997536', 'https://openalex.org/W2407080277', 'https://openalex.org/W2400505028', 'https://openalex.org/W2125234026', 'https://openalex.org/W2129486313', 'https://openalex.org/W1494198834', 'https://openalex.org/W2038810952', 'https://openalex.org/W1524333225', 'https://openalex.org/W2160306971', 'https://openalex.org/W1762484328', 'https://openalex.org/W2112021726', 'https://openalex.org/W2250357346', 'https://openalex.org/W2028706510', 'https://openalex.org/W2117671523', 'https://openalex.org/W2148154194', 'https://openalex.org/W2166637769', 'https://openalex.org/W1499864241', 'https://openalex.org/W1997449813', 'https://openalex.org/W2150769028', 'https://openalex.org/W2093231248', 'https://openalex.org/W2143612262', 'https://openalex.org/W2167763959', 'https://openalex.org/W2164240571', 'https://openalex.org/W2026369565']",2015-09-06
https://openalex.org/W2752782242,https://doi.org/10.1109/cvpr.2018.00745,Squeeze-and-Excitation Networks,"Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the ""Squeeze-and-Excitation"" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251%, achieving a ~25% relative improvement over the winning entry of 2016. Code and models are available at https://github.com/hujie-frank/SENet.","['https://openalex.org/W6637373629', 'https://openalex.org/W1568165162', 'https://openalex.org/W6620707391', 'https://openalex.org/W2115441154', 'https://openalex.org/W6697925102', 'https://openalex.org/W6637242042', 'https://openalex.org/W6703720303', 'https://openalex.org/W1966385142', 'https://openalex.org/W2117539524', 'https://openalex.org/W1903029394', 'https://openalex.org/W6682137061', 'https://openalex.org/W6704241340', 'https://openalex.org/W2288122362', 'https://openalex.org/W6649495467', 'https://openalex.org/W6680834391', 'https://openalex.org/W6684191040', 'https://openalex.org/W2130325614', 'https://openalex.org/W6639102338', 'https://openalex.org/W6682132143', 'https://openalex.org/W2962737770', 'https://openalex.org/W6729956949', 'https://openalex.org/W6740934225', 'https://openalex.org/W2194775991', 'https://openalex.org/W6698183232', 'https://openalex.org/W6626481562', 'https://openalex.org/W2064675550', 'https://openalex.org/W2963446712', 'https://openalex.org/W2400429454', 'https://openalex.org/W6638667902', 'https://openalex.org/W2144764737', 'https://openalex.org/W2128272608', 'https://openalex.org/W6618372016', 'https://openalex.org/W2550553598', 'https://openalex.org/W2221625691', 'https://openalex.org/W2531409750', 'https://openalex.org/W6740164494', 'https://openalex.org/W2962971773', 'https://openalex.org/W2551572271', 'https://openalex.org/W6674642818', 'https://openalex.org/W1677182931', 'https://openalex.org/W2963495494', 'https://openalex.org/W2113325037', 'https://openalex.org/W6630875275', 'https://openalex.org/W2549139847', 'https://openalex.org/W6694260854', 'https://openalex.org/W6685176027', 'https://openalex.org/W2183341477', 'https://openalex.org/W2097117768', 'https://openalex.org/W2097018403', 'https://openalex.org/W1665214252', 'https://openalex.org/W139960808', 'https://openalex.org/W2619184049', 'https://openalex.org/W2097998348', 'https://openalex.org/W2964137095', 'https://openalex.org/W2964099383', 'https://openalex.org/W2331143823', 'https://openalex.org/W2339172597', 'https://openalex.org/W2706729717', 'https://openalex.org/W2964081403', 'https://openalex.org/W2149933564', 'https://openalex.org/W2952746495', 'https://openalex.org/W2962970253', 'https://openalex.org/W603908379', 'https://openalex.org/W2806970737', 'https://openalex.org/W2963918968', 'https://openalex.org/W1514535095', 'https://openalex.org/W2732026016', 'https://openalex.org/W2302255633', 'https://openalex.org/W2610817424', 'https://openalex.org/W2553303224', 'https://openalex.org/W2746314669', 'https://openalex.org/W2785430118', 'https://openalex.org/W2963504571', 'https://openalex.org/W2141399712', 'https://openalex.org/W1686810756', 'https://openalex.org/W2964350391', 'https://openalex.org/W2964166828', 'https://openalex.org/W2163605009', 'https://openalex.org/W2963778169', 'https://openalex.org/W2594529350', 'https://openalex.org/W639708223', 'https://openalex.org/W1996901117', 'https://openalex.org/W3118608800', 'https://openalex.org/W4232730838', 'https://openalex.org/W2111935653', 'https://openalex.org/W2274287116', 'https://openalex.org/W2724359148', 'https://openalex.org/W4297775537', 'https://openalex.org/W2804047946', 'https://openalex.org/W2964081807', 'https://openalex.org/W2964294659', 'https://openalex.org/W2963403868', 'https://openalex.org/W2951104886', 'https://openalex.org/W2147527908', 'https://openalex.org/W2963984455', 'https://openalex.org/W1581066146', 'https://openalex.org/W1861492603', 'https://openalex.org/W2307770531', 'https://openalex.org/W1836465849', 'https://openalex.org/W2963703197', 'https://openalex.org/W2172010943', 'https://openalex.org/W4299518610', 'https://openalex.org/W2613718673', 'https://openalex.org/W2884585870', 'https://openalex.org/W2782417188', 'https://openalex.org/W2963137684', 'https://openalex.org/W2963374479', 'https://openalex.org/W2964331719', 'https://openalex.org/W2951527505', 'https://openalex.org/W2963420658', 'https://openalex.org/W2556833785', 'https://openalex.org/W2962835968', 'https://openalex.org/W2963821229', 'https://openalex.org/W1026270304', 'https://openalex.org/W2612445135', 'https://openalex.org/W2950621961', 'https://openalex.org/W581956982', 'https://openalex.org/W2963125010', 'https://openalex.org/W2950178297', 'https://openalex.org/W2963911037', 'https://openalex.org/W2962746461']",2018-06-01
https://openalex.org/W2342475039,https://doi.org/10.1109/taffc.2016.2515617,MSP-IMPROV: An Acted Corpus of Dyadic Interactions to Study Emotion Perception,"We present the MSP-IMPROV corpus, a multimodal emotional database, where the goal is to have control over lexical content and emotion while also promoting naturalness in the recordings. Studies on emotion perception often require stimuli with fixed lexical content, but that convey different emotions. These stimuli can also serve as an instrument to understand how emotion modulates speech at the phoneme level, in a manner that controls for coarticulation. Such audiovisual data are not easily available from natural recordings. A common solution is to record actors reading sentences that portray different emotions, which may not produce natural behaviors. We propose an alternative approach in which we define hypothetical scenarios for each sentence that are carefully designed to elicit a particular emotion. Two actors improvise these emotion-specific situations, leading them to utter contextualized, non-read renditions of sentences that have fixed lexical content and convey different emotions. We describe the context in which this corpus was recorded, the key features of the corpus, the areas in which this corpus can be useful, and the emotional content of the recordings. The paper also provides the performance for speech and facial emotion classifiers. The analysis brings novel classification evaluations where we study the performance in terms of inter-evaluator agreement and naturalness perception, leveraging the large size of the audiovisual database.","['https://openalex.org/W2154024118', 'https://openalex.org/W6600637765', 'https://openalex.org/W1976090878', 'https://openalex.org/W2550557083', 'https://openalex.org/W1965696296', 'https://openalex.org/W2182146077', 'https://openalex.org/W2163027578', 'https://openalex.org/W6604149461', 'https://openalex.org/W2323087493', 'https://openalex.org/W2122098299', 'https://openalex.org/W2045528981', 'https://openalex.org/W2132555391', 'https://openalex.org/W2099415951', 'https://openalex.org/W2124737236', 'https://openalex.org/W6741303551', 'https://openalex.org/W6635996524', 'https://openalex.org/W2084388911', 'https://openalex.org/W6683165025', 'https://openalex.org/W2085662862', 'https://openalex.org/W6630073874', 'https://openalex.org/W2251371553', 'https://openalex.org/W2028899742', 'https://openalex.org/W6727689944', 'https://openalex.org/W2159668072', 'https://openalex.org/W2161450073', 'https://openalex.org/W6689779973', 'https://openalex.org/W2074788634', 'https://openalex.org/W2097732741', 'https://openalex.org/W6741262016', 'https://openalex.org/W2129794709', 'https://openalex.org/W2010700035', 'https://openalex.org/W2102953093', 'https://openalex.org/W2122563357', 'https://openalex.org/W2054879867', 'https://openalex.org/W2023582935', 'https://openalex.org/W2170876097', 'https://openalex.org/W2154780170', 'https://openalex.org/W2158630797', 'https://openalex.org/W6608008786', 'https://openalex.org/W2114269021', 'https://openalex.org/W6712777154', 'https://openalex.org/W6629328741', 'https://openalex.org/W2127531292', 'https://openalex.org/W6990412439', 'https://openalex.org/W4247276217', 'https://openalex.org/W2058787788', 'https://openalex.org/W2146334809', 'https://openalex.org/W153782414', 'https://openalex.org/W1977748708', 'https://openalex.org/W1551188655', 'https://openalex.org/W4245744384', 'https://openalex.org/W6727650743', 'https://openalex.org/W2146629733', 'https://openalex.org/W1997537978', 'https://openalex.org/W2083638007', 'https://openalex.org/W2110911841', 'https://openalex.org/W6927634225', 'https://openalex.org/W6607193717', 'https://openalex.org/W2117752179', 'https://openalex.org/W2143350951', 'https://openalex.org/W1997541854', 'https://openalex.org/W4210267733', 'https://openalex.org/W6674420779', 'https://openalex.org/W2030931454', 'https://openalex.org/W2005374274', 'https://openalex.org/W2735400762', 'https://openalex.org/W2168692779', 'https://openalex.org/W2158943324', 'https://openalex.org/W175750906', 'https://openalex.org/W2526907895', 'https://openalex.org/W2735706447', 'https://openalex.org/W647196612', 'https://openalex.org/W2118789253', 'https://openalex.org/W2238244608', 'https://openalex.org/W1598253959', 'https://openalex.org/W2525412388', 'https://openalex.org/W102327765', 'https://openalex.org/W1970381522', 'https://openalex.org/W2095797119', 'https://openalex.org/W2401417847', 'https://openalex.org/W1491064235', 'https://openalex.org/W16116878', 'https://openalex.org/W1501669607', 'https://openalex.org/W197492697']",2016-01-07
https://openalex.org/W3135547455,https://doi.org/10.1109/iscslp49672.2021.9362098,Estimating Mutual Information in Prosody Representation for Emotional Prosody Transfer in Speech Synthesis,"An end-to-end prosody transfer system aims to transfer the speech prosody from one speaker to another speaker. One major application is the generation of emotional speech with a new speaker's voice. The end-to-end system uses an intermediate representation of prosody, which encompasses both speaker and emotion related information. The present study tackles the problem of estimating the mutual information between emotion and speaker-related factors in the prosody representation. A mutual information neural estimator (MINE) which could measure the mutual information between high-dimensional continuous prosody embedding and discrete speaker/emotion label is applied. The experimental results show that: 1) the prosody representation generated by the end-to-end system indeed contains both emotion and speaker information; 2) The mutual information would be determined by the type of input acoustic features to the reference encoder; 3) normalization for the log F0 feature is very effective in increasing emotion-related information in the prosody representation; 4) adversarial learning can be applied to reduce speaker information in the prosody representation. These results are useful to the further development of an optimal and practical emotional prosody transfer systems.","['https://openalex.org/W6639350448', 'https://openalex.org/W2091425152', 'https://openalex.org/W2939488497', 'https://openalex.org/W6713020357', 'https://openalex.org/W1931958196', 'https://openalex.org/W2795109282', 'https://openalex.org/W1971670143', 'https://openalex.org/W2087110403', 'https://openalex.org/W6750489868', 'https://openalex.org/W2171121512', 'https://openalex.org/W2576411688', 'https://openalex.org/W2890606114', 'https://openalex.org/W6605187413', 'https://openalex.org/W4254718357', 'https://openalex.org/W2907262790', 'https://openalex.org/W2006715603', 'https://openalex.org/W4251867726', 'https://openalex.org/W2191779130', 'https://openalex.org/W588342931', 'https://openalex.org/W1966797434', 'https://openalex.org/W1976725440', 'https://openalex.org/W2069859485', 'https://openalex.org/W6608022165', 'https://openalex.org/W1570629387', 'https://openalex.org/W2069631319', 'https://openalex.org/W2963609956', 'https://openalex.org/W2972921407', 'https://openalex.org/W2149350210', 'https://openalex.org/W2963300588', 'https://openalex.org/W4243316134', 'https://openalex.org/W6752051073', 'https://openalex.org/W567437002', 'https://openalex.org/W2803832867', 'https://openalex.org/W1875231349', 'https://openalex.org/W2136144249', 'https://openalex.org/W2398561585', 'https://openalex.org/W2963927338', 'https://openalex.org/W4295731579', 'https://openalex.org/W2043843997', 'https://openalex.org/W198298781', 'https://openalex.org/W2794490148', 'https://openalex.org/W2003547693', 'https://openalex.org/W2963272440', 'https://openalex.org/W126988493']",2021-01-24
https://openalex.org/W2889374687,https://doi.org/10.21437/interspeech.2018-1242,An Attention Pooling Based Representation Learning Method for Speech Emotion Recognition,"This paper proposes an attention pooling based representation learning method for speech emotion recognition (SER). The emotional representation is learned in an end-to-end fashion by applying a deep convolutional neural network (CNN) directly to spectrograms extracted from speech utterances. Motivated by the success of GoogleNet, two groups of filters with different shapes are designed to capture both temporal and frequency domain context information from the input spectrogram. The learned features are concatenated and fed into the subsequent convolutional layers. To learn the final emotional representation, a novel attention pooling method is further proposed. Compared with the existing pooling methods, such as max-pooling and average-pooling, the proposed attention pooling can effectively incorporate class-agnostic bottom-up, and class-specific top-down, attention maps. We conduct extensive evaluations on benchmark IEMOCAP data to assess the effectiveness of the proposed representation. Results demonstrate a recognition performance of 71.8% weighted accuracy (WA) and 68% unweighted accuracy (UA) over four emotions, which outperforms the state-of-the-art method by about 3% absolute for WA and 4% for UA.","['https://openalex.org/W2150341604', 'https://openalex.org/W2399733683', 'https://openalex.org/W2899771611', 'https://openalex.org/W2949117887', 'https://openalex.org/W2108581428', 'https://openalex.org/W2729172879', 'https://openalex.org/W2163605009', 'https://openalex.org/W2194775991', 'https://openalex.org/W2104657103', 'https://openalex.org/W2752386593', 'https://openalex.org/W2131537280', 'https://openalex.org/W2097117768', 'https://openalex.org/W2146334809', 'https://openalex.org/W1836465849', 'https://openalex.org/W4205947740', 'https://openalex.org/W2962736520', 'https://openalex.org/W2396144419', 'https://openalex.org/W2087618018', 'https://openalex.org/W2747664154', 'https://openalex.org/W2087750470', 'https://openalex.org/W2707551695', 'https://openalex.org/W2295001676', 'https://openalex.org/W2408520939', 'https://openalex.org/W2145227859', 'https://openalex.org/W2515667479', 'https://openalex.org/W2622203030', 'https://openalex.org/W2648194195', 'https://openalex.org/W1542637018', 'https://openalex.org/W2625297138']",2018-08-28
https://openalex.org/W2973181312,https://doi.org/10.21437/interspeech.2019-1649,Attention-Enhanced Connectionist Temporal Classification for Discrete Speech Emotion Recognition,"&amp;lt;p&amp;gt;Discrete speech emotion recognition (SER), the assignment of a single emotion label to an entire speech utterance, is typically performed as a sequence-to-label task. This approach, however, is limited, in that it can result in models that do not capture temporal changes in the speech signal, including those indicative of a particular emotion. One potential solution to overcome this limitation is to model SER as a sequence-to-sequence task instead. In this regard, we have developed an attention-based bidirectional long short-term memory (BLSTM) neural network in combination with a connectionist temporal classification (CTC) objective function (Attention-BLSTM-CTC) for SER. We also assessed the benefits of incorporating two contemporary attention mechanisms, namely component attention and quantum attention, into the CTC framework. To the best of the authors&amp;rsquo; knowledge, this is the first time that such a hybrid architecture has been employed for SER.We demonstrated the effectiveness of our approach on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) and FAU-Aibo Emotion corpora. The experimental results demonstrate that our proposed model outperforms current state-of-the-art approaches.&amp;lt;/p&amp;gt;","['https://openalex.org/W2133564696', 'https://openalex.org/W2963211739', 'https://openalex.org/W854541894', 'https://openalex.org/W2096927441', 'https://openalex.org/W2146334809', 'https://openalex.org/W2625297138', 'https://openalex.org/W1869752048', 'https://openalex.org/W2962962542', 'https://openalex.org/W2963467407', 'https://openalex.org/W1501669607', 'https://openalex.org/W3104696513', 'https://openalex.org/W2583743457', 'https://openalex.org/W2897132394', 'https://openalex.org/W2786779322', 'https://openalex.org/W2889507358', 'https://openalex.org/W2765954565', 'https://openalex.org/W2610961739', 'https://openalex.org/W2404856031', 'https://openalex.org/W2408520939', 'https://openalex.org/W2193413348', 'https://openalex.org/W2143612262', 'https://openalex.org/W2889286050', 'https://openalex.org/W2512885694', 'https://openalex.org/W211912913', 'https://openalex.org/W2515667479', 'https://openalex.org/W2131774270', 'https://openalex.org/W2750497738', 'https://openalex.org/W2766027582']",2019-09-13
https://openalex.org/W2511640485,https://doi.org/10.1109/icis.2016.7550889,Emotional voice conversion using deep neural networks with MCC and F0 features,"An artificial neural network is one of the most important models for training features in a voice conversion task. Typically, Neural Networks (NNs) are not effective in processing low-dimensional F0 features, thus this causes that the performance of those methods based on neural networks for training Mel Cepstral Coefficients (MCC) are not outstanding. However, F0 can robustly represent various prosody signals (e.g., emotional prosody). In this study, we propose an effective method based on the NNs to train the normalized-segment-F0 features (NSF0) for emotional prosody conversion. Meanwhile, the proposed method adopts deep belief networks (DBNs) to train spectrum features for voice conversion. By using these approaches, the proposed method can change the spectrum and the prosody for the emotional voice at the same time. Moreover, the experimental results show that the proposed method outperforms other state-of-the-art methods for voice emotional conversion.","['https://openalex.org/W2105160541', 'https://openalex.org/W2126143605', 'https://openalex.org/W1972420736', 'https://openalex.org/W6696767757', 'https://openalex.org/W6634569661', 'https://openalex.org/W2161736993', 'https://openalex.org/W2077801020', 'https://openalex.org/W6711832335', 'https://openalex.org/W2119655735', 'https://openalex.org/W6712665021', 'https://openalex.org/W2089624917', 'https://openalex.org/W6712742799', 'https://openalex.org/W6691118191', 'https://openalex.org/W6601280771', 'https://openalex.org/W6712645575', 'https://openalex.org/W2093450784', 'https://openalex.org/W6606266484', 'https://openalex.org/W2164241094', 'https://openalex.org/W2120605154', 'https://openalex.org/W2123003832', 'https://openalex.org/W2152205330', 'https://openalex.org/W2129142580', 'https://openalex.org/W6603446537', 'https://openalex.org/W6635180523', 'https://openalex.org/W2123771434', 'https://openalex.org/W6712560600', 'https://openalex.org/W6711854987', 'https://openalex.org/W1588037970', 'https://openalex.org/W2398826216', 'https://openalex.org/W2294351487', 'https://openalex.org/W4229870863', 'https://openalex.org/W2251981003', 'https://openalex.org/W2396025094', 'https://openalex.org/W2399915284', 'https://openalex.org/W1575712594', 'https://openalex.org/W2395700867', 'https://openalex.org/W2400683008', 'https://openalex.org/W155193863', 'https://openalex.org/W31448757', 'https://openalex.org/W84813673', 'https://openalex.org/W2400092632']",2016-06-01
https://openalex.org/W3197993066,https://doi.org/10.21437/interspeech.2021-781,Limited Data Emotional Voice Conversion Leveraging Text-to-Speech: Two-Stage Sequence-to-Sequence Training,"Emotional voice conversion (EVC) aims to change the emotional state of an utterance while preserving the linguistic content and speaker identity. In this paper, we propose a novel 2-stage training strategy for sequence-to-sequence emotional voice conversion with a limited amount of emotional speech data. We note that the proposed EVC framework leverages text-to-speech (TTS) as they share a common goal that is to generate high-quality expressive voice. In stage 1, we perform style initialization with a multi-speaker TTS corpus, to disentangle speaking style and linguistic content. In stage 2, we perform emotion training with a limited amount of emotional speech data, to learn how to disentangle emotional style and linguistic information from the speech. The proposed framework can perform both spectrum and prosody conversion and achieves significant improvement over the state-of-the-art baselines in both objective and subjective evaluation.",[],2021-08-27
https://openalex.org/W3015241559,https://doi.org/10.1109/icassp40776.2020.9054579,Stargan for Emotional Speech Conversion: Validated by Data Augmentation of End-To-End Emotion Recognition,"In this paper, we propose an adversarial network implementation for speech emotion conversion as a data augmentation method, validated by a multi-class speech affect recognition task. In our setting, we do not assume the availability of parallel data, and we additionally make it a priority to exploit as much as possible the available training data by adopting a cycle-consistent, class-conditional generative adversarial network with an auxiliary domain classifier. Our generated samples are valuable for data augmentation, achieving a corresponding 2% and 6% absolute increase in Micro- and MacroF1 compared to the baseline in a 3-class classification paradigm using a deep, end-to-end network. We finally perform a human perception evaluation of the samples, through which we conclude that our samples are indicative of their target emotion, albeit showing a tendency for confusion in cases where the emotional attribute of valence and arousal are inconsistent.","['https://openalex.org/W2921059071', 'https://openalex.org/W1993267429', 'https://openalex.org/W2740504963', 'https://openalex.org/W2517513811', 'https://openalex.org/W2962793481', 'https://openalex.org/W2963767194', 'https://openalex.org/W2963539064', 'https://openalex.org/W1947595512', 'https://openalex.org/W6761745632', 'https://openalex.org/W2748699435', 'https://openalex.org/W2984972638', 'https://openalex.org/W2936764189', 'https://openalex.org/W6746638498', 'https://openalex.org/W2032254851', 'https://openalex.org/W2972640480', 'https://openalex.org/W4254718357', 'https://openalex.org/W2936774411', 'https://openalex.org/W6635996524', 'https://openalex.org/W6735913928', 'https://openalex.org/W2123771434', 'https://openalex.org/W2471520273', 'https://openalex.org/W2146334809', 'https://openalex.org/W2099471712', 'https://openalex.org/W1598253959', 'https://openalex.org/W4295521014', 'https://openalex.org/W2770173563', 'https://openalex.org/W2962879692', 'https://openalex.org/W2937977583', 'https://openalex.org/W567437002', 'https://openalex.org/W4320013936']",2020-04-09
https://openalex.org/W3204457821,https://doi.org/10.1109/taslp.2022.3190715,Decoupling Speaker-Independent Emotions for Voice Conversion via Source-Filter Networks,"Emotional voice conversion (VC) aims to convert a neutral voice to an emotional one while retaining the linguistic information and speaker identity. We note that the decoupling of emotional features from other speech information (such as content, speaker identity, etc.) is the key to achieving promising performance. Some recent attempts of speech representation decoupling on the neutral speech cannot work well on the emotional speech, due to the more complex entanglement of acoustic properties in the latter. To address this problem, here we propose a novel Source-Filter-based Emotional VC model (SFEVC) to achieve proper filtering of speaker-independent emotion cues from both the timbre and pitch features. Our SFEVC model consists of multi-channel encoders, emotion separate encoders, pre-trained speaker-dependent encoders, and the corresponding decoder. Note that all encoder modules adopt a designed information bottleneck auto-encoder. Additionally, to further improve the conversion quality for various emotions, a novel training strategy based on the 2D Valence-Arousal (VA) space is proposed. Experimental results show that the proposed SFEVC along with a VA training strategy outperforms all baselines and achieves the state-of-the-art performance in speaker-independent emotional VC with nonparallel data.","['https://openalex.org/W2120687203', 'https://openalex.org/W2598656683', 'https://openalex.org/W2889482935', 'https://openalex.org/W2092804630', 'https://openalex.org/W2116628589', 'https://openalex.org/W2098986833', 'https://openalex.org/W2809859618', 'https://openalex.org/W1999964619', 'https://openalex.org/W2914461061', 'https://openalex.org/W1972076735', 'https://openalex.org/W2888011686', 'https://openalex.org/W2023736093', 'https://openalex.org/W2164241094', 'https://openalex.org/W2056661291', 'https://openalex.org/W2108601574', 'https://openalex.org/W2740504963', 'https://openalex.org/W2511640485', 'https://openalex.org/W2517513811', 'https://openalex.org/W2950542544', 'https://openalex.org/W1510835889', 'https://openalex.org/W3025680351', 'https://openalex.org/W3163573274', 'https://openalex.org/W2964243274', 'https://openalex.org/W6749954789', 'https://openalex.org/W6745289305', 'https://openalex.org/W2471520273', 'https://openalex.org/W2997663825', 'https://openalex.org/W2105160541', 'https://openalex.org/W2017425464', 'https://openalex.org/W2562848127', 'https://openalex.org/W2919115771', 'https://openalex.org/W2293634267', 'https://openalex.org/W2518312472', 'https://openalex.org/W1509691205', 'https://openalex.org/W2077801020', 'https://openalex.org/W1588037970', 'https://openalex.org/W2399915284', 'https://openalex.org/W2089624917', 'https://openalex.org/W6743094995', 'https://openalex.org/W2294351487', 'https://openalex.org/W3015959238', 'https://openalex.org/W2963539064', 'https://openalex.org/W2128070398', 'https://openalex.org/W2120605154', 'https://openalex.org/W6776390925', 'https://openalex.org/W4205742757', 'https://openalex.org/W2161736993', 'https://openalex.org/W2990440871', 'https://openalex.org/W2051487990', 'https://openalex.org/W6762533536', 'https://openalex.org/W2396025094', 'https://openalex.org/W2610171128', 'https://openalex.org/W4320013936', 'https://openalex.org/W3034794073', 'https://openalex.org/W3004402693', 'https://openalex.org/W2946809691', 'https://openalex.org/W2040984620', 'https://openalex.org/W2774848319', 'https://openalex.org/W2187089797', 'https://openalex.org/W2945478979', 'https://openalex.org/W2319003035', 'https://openalex.org/W2099471712', 'https://openalex.org/W2744580635', 'https://openalex.org/W2765486990', 'https://openalex.org/W3020570669', 'https://openalex.org/W2795783309', 'https://openalex.org/W1976725440']",2022-07-14
https://openalex.org/W4221147462,https://doi.org/10.1109/taffc.2022.3175578,Emotion Intensity and its Control for Emotional Voice Conversion,"Emotional voice conversion (EVC) seeks to convert the emotional state of an\nutterance while preserving the linguistic content and speaker identity. In EVC,\nemotions are usually treated as discrete categories overlooking the fact that\nspeech also conveys emotions with various intensity levels that the listener\ncan perceive. In this paper, we aim to explicitly characterize and control the\nintensity of emotion. We propose to disentangle the speaker style from\nlinguistic content and encode the speaker style into a style embedding in a\ncontinuous space that forms the prototype of emotion embedding. We further\nlearn the actual emotion encoder from an emotion-labelled database and study\nthe use of relative attributes to represent fine-grained emotion intensity. To\nensure emotional intelligibility, we incorporate emotion classification loss\nand emotion embedding similarity loss into the training of the EVC network. As\ndesired, the proposed network controls the fine-grained emotion intensity in\nthe output speech. Through both objective and subjective evaluations, we\nvalidate the effectiveness of the proposed network for emotional expressiveness\nand emotion intensity control.\n","['https://openalex.org/W4205742757', 'https://openalex.org/W388732865', 'https://openalex.org/W2190260761', 'https://openalex.org/W3092628071', 'https://openalex.org/W3098557217', 'https://openalex.org/W2493477844', 'https://openalex.org/W2123003832', 'https://openalex.org/W2111090013', 'https://openalex.org/W2478838513', 'https://openalex.org/W1963640919', 'https://openalex.org/W6639399909', 'https://openalex.org/W2161736993', 'https://openalex.org/W2007541064', 'https://openalex.org/W4251603968', 'https://openalex.org/W4254718357', 'https://openalex.org/W3095701549', 'https://openalex.org/W3193748859', 'https://openalex.org/W3015841875', 'https://openalex.org/W3136699727', 'https://openalex.org/W2077801020', 'https://openalex.org/W1588037970', 'https://openalex.org/W2040587156', 'https://openalex.org/W2148846882', 'https://openalex.org/W2793479148', 'https://openalex.org/W2748654097', 'https://openalex.org/W2517513811', 'https://openalex.org/W3025680351', 'https://openalex.org/W3095169545', 'https://openalex.org/W3015241559', 'https://openalex.org/W3096939667', 'https://openalex.org/W3142644187', 'https://openalex.org/W2899361462', 'https://openalex.org/W3097962967', 'https://openalex.org/W3015805741', 'https://openalex.org/W6756197946', 'https://openalex.org/W2963609956', 'https://openalex.org/W2897353073', 'https://openalex.org/W2899877258', 'https://openalex.org/W3113687514', 'https://openalex.org/W6803066952', 'https://openalex.org/W2938833595', 'https://openalex.org/W3015719316', 'https://openalex.org/W2043301392', 'https://openalex.org/W1966797434', 'https://openalex.org/W2005885879', 'https://openalex.org/W2885005742', 'https://openalex.org/W3163573274', 'https://openalex.org/W3168542456', 'https://openalex.org/W2105482032', 'https://openalex.org/W3034420534', 'https://openalex.org/W3162517041', 'https://openalex.org/W6793624898', 'https://openalex.org/W3118753411', 'https://openalex.org/W3083423753', 'https://openalex.org/W2996414377', 'https://openalex.org/W2901254300', 'https://openalex.org/W2150556990', 'https://openalex.org/W3197993066', 'https://openalex.org/W1969386661', 'https://openalex.org/W1978136968', 'https://openalex.org/W2759925408', 'https://openalex.org/W1492383498', 'https://openalex.org/W2745595539', 'https://openalex.org/W6749555683', 'https://openalex.org/W6750489868', 'https://openalex.org/W2964138190', 'https://openalex.org/W2973158936', 'https://openalex.org/W3197704090', 'https://openalex.org/W3152136404', 'https://openalex.org/W6640963894', 'https://openalex.org/W2904459034', 'https://openalex.org/W6762643587', 'https://openalex.org/W1973378890', 'https://openalex.org/W2330979245', 'https://openalex.org/W2800822639', 'https://openalex.org/W3008691130', 'https://openalex.org/W3146550708', 'https://openalex.org/W2990881710', 'https://openalex.org/W2342475039', 'https://openalex.org/W2146334809', 'https://openalex.org/W2149628368', 'https://openalex.org/W2937154351', 'https://openalex.org/W3014201970', 'https://openalex.org/W3199964822', 'https://openalex.org/W3096830101', 'https://openalex.org/W2803098682', 'https://openalex.org/W3198791321', 'https://openalex.org/W3135644023', 'https://openalex.org/W3160329778', 'https://openalex.org/W2002875620', 'https://openalex.org/W6678800043', 'https://openalex.org/W6683338658', 'https://openalex.org/W6783596713', 'https://openalex.org/W2294130536', 'https://openalex.org/W2033365921', 'https://openalex.org/W2042328763', 'https://openalex.org/W2157336702', 'https://openalex.org/W2147898188', 'https://openalex.org/W3095669918', 'https://openalex.org/W6603838645', 'https://openalex.org/W6917585676', 'https://openalex.org/W6843673214', 'https://openalex.org/W2108323654', 'https://openalex.org/W4231336072', 'https://openalex.org/W4246198815', 'https://openalex.org/W2113586398', 'https://openalex.org/W6680970901', 'https://openalex.org/W2966387353', 'https://openalex.org/W2471520273', 'https://openalex.org/W3015338123', 'https://openalex.org/W3046998876', 'https://openalex.org/W2085662862', 'https://openalex.org/W1501669607', 'https://openalex.org/W2785960144', 'https://openalex.org/W3015249983', 'https://openalex.org/W3126625480', 'https://openalex.org/W2032532974', 'https://openalex.org/W2107860279', 'https://openalex.org/W1975163393', 'https://openalex.org/W2963047186', 'https://openalex.org/W2066629760', 'https://openalex.org/W4388323056', 'https://openalex.org/W2007962718', 'https://openalex.org/W2747070883', 'https://openalex.org/W4240592325', 'https://openalex.org/W1976725440', 'https://openalex.org/W2137639365', 'https://openalex.org/W1488156371', 'https://openalex.org/W1959608418', 'https://openalex.org/W567437002', 'https://openalex.org/W1540664512', 'https://openalex.org/W2952269766', 'https://openalex.org/W4287236468', 'https://openalex.org/W2125560515', 'https://openalex.org/W2187089797', 'https://openalex.org/W3099078140', 'https://openalex.org/W2901997113', 'https://openalex.org/W2319660501', 'https://openalex.org/W4232693607', 'https://openalex.org/W3165478005', 'https://openalex.org/W4288097605', 'https://openalex.org/W3194143312', 'https://openalex.org/W3091905774', 'https://openalex.org/W2133564696', 'https://openalex.org/W4244075810', 'https://openalex.org/W4287241477', 'https://openalex.org/W3104132365', 'https://openalex.org/W2608207374', 'https://openalex.org/W3006108364', 'https://openalex.org/W3101689408', 'https://openalex.org/W4294619240', 'https://openalex.org/W2128070398', 'https://openalex.org/W3102905810', 'https://openalex.org/W4232132981', 'https://openalex.org/W95152782', 'https://openalex.org/W3156592906', 'https://openalex.org/W2794490148', 'https://openalex.org/W4213469706', 'https://openalex.org/W4287250434', 'https://openalex.org/W2138615112', 'https://openalex.org/W4289299319', 'https://openalex.org/W1673075472', 'https://openalex.org/W3213873715', 'https://openalex.org/W1881869418', 'https://openalex.org/W4250182547', 'https://openalex.org/W3144792678']",2022-05-19
https://openalex.org/W4224918091,https://doi.org/10.1109/icassp43922.2022.9746990,Exploiting Annotators’ Typed Description of Emotion Perception to Maximize Utilization of Ratings for Speech Emotion Recognition,"The decision of ground truth for speech emotion recognition (SER) is still a critical issue in affective computing tasks. Previous studies on emotion recognition often rely on consensus labels after aggregating the classes selected by multiple annotators. It is common for a perceptual evaluation conducted to annotate emotional corpora to include the class ""other,"" allowing the annotators the opportunity to describe the emotion with their own words. This practice provides valuable emotional information, which, however, is ignored in most emotion recognition studies. This paper utilizes easy-accessed natural language processing toolkits to mine the sentiment of these typed descriptions, enriching and maximizing the information obtained from the annotators. The polarity information is combined with primary and secondary annotations provided by individual evaluators under a label distribution framework, creating a complete representation of the emotional content of the spoken sentences. Finally, we train multitask learning SER models with existing learning methods (soft-label, multi-label, and distribution-label) to show the performance of the novel ground truth in the MSP-Podcast corpus.","['https://openalex.org/W2972971501', 'https://openalex.org/W3212282286', 'https://openalex.org/W1998866511', 'https://openalex.org/W2146334809', 'https://openalex.org/W2745497104', 'https://openalex.org/W6736575291', 'https://openalex.org/W2154024118', 'https://openalex.org/W2752234108', 'https://openalex.org/W3106206515', 'https://openalex.org/W2891597252', 'https://openalex.org/W2997087088', 'https://openalex.org/W2342475039', 'https://openalex.org/W6795984487', 'https://openalex.org/W2742542661', 'https://openalex.org/W2552810951', 'https://openalex.org/W2408513201', 'https://openalex.org/W2801959488', 'https://openalex.org/W2786657259', 'https://openalex.org/W2148220354', 'https://openalex.org/W3086923691', 'https://openalex.org/W2937151161', 'https://openalex.org/W2330485005', 'https://openalex.org/W2401417847', 'https://openalex.org/W3199271201', 'https://openalex.org/W3197994565', 'https://openalex.org/W2525412388', 'https://openalex.org/W2183341477', 'https://openalex.org/W2973049979', 'https://openalex.org/W3122081138', 'https://openalex.org/W3164582967', 'https://openalex.org/W2489406233']",2022-04-27
https://openalex.org/W2803193013,https://doi.org/10.1371/journal.pone.0196391,"The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English","The RAVDESS is a validated multimodal database of emotional speech and song. The database is gender balanced consisting of 24 professional actors, vocalizing lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity, with an additional neutral expression. All conditions are available in face-and-voice, face-only, and voice-only formats. The set of 7356 recordings were each rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity and test-retest intrarater reliability were reported. Corrected accuracy and composite ""goodness"" measures are presented to assist researchers in the selection of stimuli. All recordings are made freely available under a Creative Commons license and can be downloaded at https://doi.org/10.5281/zenodo.1188976.","['https://openalex.org/W2075274068', 'https://openalex.org/W2162995090', 'https://openalex.org/W2158564203', 'https://openalex.org/W1978436245', 'https://openalex.org/W2073414813', 'https://openalex.org/W1992424514', 'https://openalex.org/W2115839658', 'https://openalex.org/W2160287712', 'https://openalex.org/W2138395786', 'https://openalex.org/W2007462506', 'https://openalex.org/W2169471661', 'https://openalex.org/W2099418770', 'https://openalex.org/W2070267188', 'https://openalex.org/W2138813867', 'https://openalex.org/W2095580536', 'https://openalex.org/W2140110589', 'https://openalex.org/W2091804523', 'https://openalex.org/W2108831282', 'https://openalex.org/W2145603196', 'https://openalex.org/W2114201982', 'https://openalex.org/W2170598503', 'https://openalex.org/W2143644424', 'https://openalex.org/W2911935942', 'https://openalex.org/W2140157036', 'https://openalex.org/W2125127226', 'https://openalex.org/W2169294293', 'https://openalex.org/W2098958400', 'https://openalex.org/W2054167736', 'https://openalex.org/W2150283722', 'https://openalex.org/W2186574009', 'https://openalex.org/W2128897837', 'https://openalex.org/W1964422354', 'https://openalex.org/W1996492337', 'https://openalex.org/W2108385997', 'https://openalex.org/W2123912601', 'https://openalex.org/W1980068698', 'https://openalex.org/W2121511082', 'https://openalex.org/W2114826417', 'https://openalex.org/W2149377780', 'https://openalex.org/W2003881355', 'https://openalex.org/W2026118130', 'https://openalex.org/W1974701791', 'https://openalex.org/W1974750895', 'https://openalex.org/W2070375420', 'https://openalex.org/W2014481042', 'https://openalex.org/W2023657742', 'https://openalex.org/W2108952035', 'https://openalex.org/W2138172448', 'https://openalex.org/W2342475039', 'https://openalex.org/W2030931454', 'https://openalex.org/W2038199522', 'https://openalex.org/W2044071327', 'https://openalex.org/W2057829425', 'https://openalex.org/W2400814905', 'https://openalex.org/W2329093554', 'https://openalex.org/W2091550557', 'https://openalex.org/W2114785251', 'https://openalex.org/W2033351021', 'https://openalex.org/W4254254617', 'https://openalex.org/W2143969506', 'https://openalex.org/W2078833921', 'https://openalex.org/W1486414894', 'https://openalex.org/W2005885879', 'https://openalex.org/W1987708005', 'https://openalex.org/W1982188321', 'https://openalex.org/W2033640825', 'https://openalex.org/W2018338387', 'https://openalex.org/W1976941847', 'https://openalex.org/W1499019550', 'https://openalex.org/W2120223000', 'https://openalex.org/W2115914647', 'https://openalex.org/W2159190230', 'https://openalex.org/W2168692779', 'https://openalex.org/W2134957872', 'https://openalex.org/W2088098553', 'https://openalex.org/W2137208816', 'https://openalex.org/W2085281696', 'https://openalex.org/W2111060246', 'https://openalex.org/W2119548591', 'https://openalex.org/W2064931561', 'https://openalex.org/W1525475793', 'https://openalex.org/W2156167136', 'https://openalex.org/W2573458771', 'https://openalex.org/W2025175823', 'https://openalex.org/W2154905939', 'https://openalex.org/W2157651355', 'https://openalex.org/W2098905310', 'https://openalex.org/W2102126079', 'https://openalex.org/W1979479123', 'https://openalex.org/W2072206615', 'https://openalex.org/W175750906', 'https://openalex.org/W2019845078', 'https://openalex.org/W4256689234', 'https://openalex.org/W1985449126', 'https://openalex.org/W1974044992', 'https://openalex.org/W1694080526', 'https://openalex.org/W2167557160', 'https://openalex.org/W2143197238', 'https://openalex.org/W7033031698', 'https://openalex.org/W2074716497', 'https://openalex.org/W2063372382', 'https://openalex.org/W6634620738', 'https://openalex.org/W1966797434', 'https://openalex.org/W1975238145', 'https://openalex.org/W2154611638', 'https://openalex.org/W2055894321', 'https://openalex.org/W2141115110', 'https://openalex.org/W2024186865', 'https://openalex.org/W2051297709', 'https://openalex.org/W2002948879', 'https://openalex.org/W6637179637', 'https://openalex.org/W2137768357', 'https://openalex.org/W2087407704', 'https://openalex.org/W2089012269', 'https://openalex.org/W2092689684', 'https://openalex.org/W2001254306', 'https://openalex.org/W1976725440', 'https://openalex.org/W2055973627', 'https://openalex.org/W2006044072', 'https://openalex.org/W4294214781', 'https://openalex.org/W2030117765', 'https://openalex.org/W2032254851', 'https://openalex.org/W2000598690', 'https://openalex.org/W2016377054', 'https://openalex.org/W2084388911', 'https://openalex.org/W1975879668', 'https://openalex.org/W2164777277', 'https://openalex.org/W2141403362', 'https://openalex.org/W2327037637', 'https://openalex.org/W2063085086', 'https://openalex.org/W1988100039', 'https://openalex.org/W1993723434', 'https://openalex.org/W1835051027', 'https://openalex.org/W2110065044', 'https://openalex.org/W4399547459', 'https://openalex.org/W2166067393', 'https://openalex.org/W2075659707', 'https://openalex.org/W1985740911', 'https://openalex.org/W2098047046', 'https://openalex.org/W1970260040', 'https://openalex.org/W2613941748', 'https://openalex.org/W2324260108', 'https://openalex.org/W4245744384', 'https://openalex.org/W2094274886', 'https://openalex.org/W2117237841', 'https://openalex.org/W1997402792', 'https://openalex.org/W2048765566', 'https://openalex.org/W2053306387', 'https://openalex.org/W2021218462', 'https://openalex.org/W2149628368', 'https://openalex.org/W2134031328', 'https://openalex.org/W2135394513', 'https://openalex.org/W2015116436', 'https://openalex.org/W1993352391', 'https://openalex.org/W6735154455', 'https://openalex.org/W6684459434', 'https://openalex.org/W1997389939', 'https://openalex.org/W2080457614', 'https://openalex.org/W2171198878', 'https://openalex.org/W2142398106', 'https://openalex.org/W2132947339', 'https://openalex.org/W2121247252', 'https://openalex.org/W2171022195', 'https://openalex.org/W2511085799', 'https://openalex.org/W2167113547', 'https://openalex.org/W414188911', 'https://openalex.org/W2025089570', 'https://openalex.org/W4253477913', 'https://openalex.org/W2069736034', 'https://openalex.org/W2582743722', 'https://openalex.org/W3137861108', 'https://openalex.org/W1580867010', 'https://openalex.org/W4229985350', 'https://openalex.org/W2045529234', 'https://openalex.org/W2062758252', 'https://openalex.org/W2173219554', 'https://openalex.org/W641354324', 'https://openalex.org/W4256441345', 'https://openalex.org/W32003796', 'https://openalex.org/W4249456707', 'https://openalex.org/W1573865449', 'https://openalex.org/W2982077032', 'https://openalex.org/W4229839895', 'https://openalex.org/W2503911469', 'https://openalex.org/W2023072634', 'https://openalex.org/W2150988854', 'https://openalex.org/W1963945289', 'https://openalex.org/W4239078262', 'https://openalex.org/W2135869619', 'https://openalex.org/W2895204597', 'https://openalex.org/W1563099953', 'https://openalex.org/W1979377847', 'https://openalex.org/W4253315019', 'https://openalex.org/W3142130749', 'https://openalex.org/W1724922661', 'https://openalex.org/W2118789253', 'https://openalex.org/W1540224544', 'https://openalex.org/W142425564', 'https://openalex.org/W2081645440', 'https://openalex.org/W1989114159', 'https://openalex.org/W4234180827', 'https://openalex.org/W2599201014', 'https://openalex.org/W2572132240', 'https://openalex.org/W1528099392', 'https://openalex.org/W2005460979']",2018-05-16
https://openalex.org/W3025680351,https://doi.org/10.21437/odyssey.2020-33,Transforming Spectrum and Prosody for Emotional Voice Conversion with Non-Parallel Training Data,"Emotional voice conversion aims to convert the spectrum and prosody to change the emotional patterns of speech, while preserving the speaker identity and linguistic content.Many studies require parallel speech data between different emotional patterns, which is not practical in real life.Moreover, they often model the conversion of fundamental frequency (F0) with a simple linear transform.As F0 is a key aspect of intonation that is hierarchical in nature, we believe that it is more adequate to model F0 in different temporal scales by using wavelet transform.We propose a CycleGAN network to find an optimal pseudo pair from non-parallel training data by learning forward and inverse mappings simultaneously using adversarial and cycle-consistency losses.We also study the use of continuous wavelet transform (CWT) to decompose F0 into ten temporal scales that describes speech prosody at different time resolution, for effective F0 conversion.Experimental results show that our proposed framework outperforms the baselines both in objective and subjective evaluations.","['https://openalex.org/W2937579788', 'https://openalex.org/W2040587156', 'https://openalex.org/W2086796102', 'https://openalex.org/W2807668517', 'https://openalex.org/W2962896155', 'https://openalex.org/W2532494225', 'https://openalex.org/W2518172956', 'https://openalex.org/W2748654097', 'https://openalex.org/W2984809863', 'https://openalex.org/W2941094131', 'https://openalex.org/W2785978752', 'https://openalex.org/W2883743124', 'https://openalex.org/W2902070858', 'https://openalex.org/W2162295204', 'https://openalex.org/W4320013936', 'https://openalex.org/W2136922672', 'https://openalex.org/W2149425161', 'https://openalex.org/W1988189165', 'https://openalex.org/W2493477844', 'https://openalex.org/W2787378487', 'https://openalex.org/W2793479148', 'https://openalex.org/W3012404734', 'https://openalex.org/W2013996527', 'https://openalex.org/W2135029798', 'https://openalex.org/W2785608393', 'https://openalex.org/W2889544410', 'https://openalex.org/W2911340057', 'https://openalex.org/W2574092538', 'https://openalex.org/W52175521', 'https://openalex.org/W2105160541', 'https://openalex.org/W4296979096', 'https://openalex.org/W2185573909', 'https://openalex.org/W2600829178', 'https://openalex.org/W2018338387', 'https://openalex.org/W2962793481', 'https://openalex.org/W2077801020', 'https://openalex.org/W2950542544', 'https://openalex.org/W3012970712', 'https://openalex.org/W2127589467', 'https://openalex.org/W2120605154', 'https://openalex.org/W2151262064', 'https://openalex.org/W1977362459', 'https://openalex.org/W2137376927', 'https://openalex.org/W2150791533', 'https://openalex.org/W2404839462', 'https://openalex.org/W2517513811', 'https://openalex.org/W2889064624', 'https://openalex.org/W2148846882', 'https://openalex.org/W2899361462', 'https://openalex.org/W2740504963', 'https://openalex.org/W113106864', 'https://openalex.org/W2157412983', 'https://openalex.org/W3144792678', 'https://openalex.org/W2471520273', 'https://openalex.org/W3012498027', 'https://openalex.org/W2395980997', 'https://openalex.org/W2396025094', 'https://openalex.org/W2161736993', 'https://openalex.org/W3016151052', 'https://openalex.org/W2618574778', 'https://openalex.org/W3008297462', 'https://openalex.org/W2511640485', 'https://openalex.org/W2774848319', 'https://openalex.org/W3025182306']",2020-05-15
https://openalex.org/W2050681655,https://doi.org/10.1515/cogl.2010.023,Using corpus methodology for semantic and pragmatic analyses: What can corpora tell us about the linguistic expression of emotions?,"Abstract The aim of this paper is to explore some of the possibilities, advantages and difficulties of corpus-based analyses of semantic and pragmatic aspects of language in one particular field, namely the linguistic expression of emotion concepts. For this purpose, a methodological procedure is proposed and an exemplary analysis of the emotion concept “fear” in English is performed. The procedure combines Kövecses' lexical approach and Stefanowitsch's metaphorical pattern analysis with additional concepts from corpus linguistics such as semantic preference and semantic prosody. The results of the study show that such a corpus-based analysis of emotion words offers several advantages. Firstly, by exploring the surroundings of the search word in a vast amount of text, we are not only able to find evidence of conceptual metaphor and metonymy that structure the emotion concept and of related emotion concepts, but also we can enrich the description of the emotion concept with information from a series of dimensions and add a pragmatic viewpoint by revealing an explicit or implicit evaluation of the emotion. The second advantage offered by a corpus-based approach lies in the possibility of quantifying results, i.e., comparing the frequency, productivity and creative use of individual metaphors and metonymies, which is especially interesting in view of contrastive studies.","['https://openalex.org/W2129662779', 'https://openalex.org/W4388122569', 'https://openalex.org/W1712325671', 'https://openalex.org/W2003916211', 'https://openalex.org/W2031171609', 'https://openalex.org/W1980606263', 'https://openalex.org/W4244900429', 'https://openalex.org/W1968640268', 'https://openalex.org/W2094628382', 'https://openalex.org/W2094414360', 'https://openalex.org/W2060754050', 'https://openalex.org/W2134293518', 'https://openalex.org/W1964103247', 'https://openalex.org/W129007333']",2010-11-01
https://openalex.org/W2785978752,https://doi.org/10.1109/apsipa.2017.8282288,Transformation of prosody in voice conversion,"Voice Conversion (VC) aims to convert one's voice to sound like that of another. So far, most of the voice conversion frameworks mainly focus only on the conversion of spectrum. We note that speaker identity is also characterized by the prosody features such as fundamental frequency (F0), energy contour and duration. Motivated by this, we propose a framework that can perform F0, energy contour and duration conversion. In the traditional exemplar-based sparse representation approach to voice conversion, a general source-target dictionary of exemplars is constructed to establish the correspondence between source and target speakers. In this work, we propose a Phonetically Aware Sparse Representation of fundamental frequency and energy contour by using Continuous Wavelet Transform (CWT). Our idea is motivated by the facts that CWT decompositions of F0 and energy contours describe prosody patterns in different temporal scales and allow for effective prosody manipulation in speech synthesis. Furthermore, phonetically aware exemplars lead to better estimation of activation matrix, therefore, possibly better conversion of prosody. We also propose a phonetically aware duration conversion framework which takes into account both phone-level and sentence-level speaking rates. We report that the proposed prosody conversion outperforms the traditional prosody conversion techniques in both objective and subjective evaluations.","['https://openalex.org/W6629735758', 'https://openalex.org/W2141520175', 'https://openalex.org/W2104457544', 'https://openalex.org/W2154278880', 'https://openalex.org/W6631187166', 'https://openalex.org/W2473388484', 'https://openalex.org/W2601846999', 'https://openalex.org/W1570629387', 'https://openalex.org/W6631362777', 'https://openalex.org/W2512087624', 'https://openalex.org/W2402356521', 'https://openalex.org/W1977362459', 'https://openalex.org/W2484196375', 'https://openalex.org/W1965912016', 'https://openalex.org/W2085393504', 'https://openalex.org/W3144792678', 'https://openalex.org/W6712034360', 'https://openalex.org/W6603221913', 'https://openalex.org/W6633966250', 'https://openalex.org/W2120605154', 'https://openalex.org/W2005438552', 'https://openalex.org/W2179428711', 'https://openalex.org/W2105160541', 'https://openalex.org/W2165674530', 'https://openalex.org/W2169652224', 'https://openalex.org/W2151262064', 'https://openalex.org/W6680012447', 'https://openalex.org/W2118850452', 'https://openalex.org/W6607761755', 'https://openalex.org/W6736010183', 'https://openalex.org/W6630263700', 'https://openalex.org/W6686269052', 'https://openalex.org/W6678929712', 'https://openalex.org/W2517513811', 'https://openalex.org/W2404839462', 'https://openalex.org/W3124274650', 'https://openalex.org/W1520341977', 'https://openalex.org/W2185573909', 'https://openalex.org/W2600829178', 'https://openalex.org/W1498096035', 'https://openalex.org/W2135029798', 'https://openalex.org/W113106864', 'https://openalex.org/W1505264225', 'https://openalex.org/W1524333225', 'https://openalex.org/W2127589467', 'https://openalex.org/W2330979245', 'https://openalex.org/W1567666748', 'https://openalex.org/W79241043', 'https://openalex.org/W2161135987', 'https://openalex.org/W188951208', 'https://openalex.org/W3143596294', 'https://openalex.org/W2395980997']",2017-12-01
https://openalex.org/W3112594642,https://doi.org/10.1146/annurev-devpsych-060320-102556,The Development of Emotion Reasoning in Infancy and Early Childhood,"Historically, research characterizing the development of emotion recognition has focused on identifying specific skills and the age periods, or milestones, at which these abilities emerge. However, advances in emotion research raise questions about whether this conceptualization accurately reflects how children learn about, understand, and respond to others’ emotions in everyday life. In this review, we propose a developmental framework for the emergence of emotion reasoning—that is, how children develop the ability to make reasonably accurate inferences and predictions about the emotion states of other people. We describe how this framework holds promise for building upon extant research. Our review suggests that use of the term emotion recognition can be misleading and imprecise, with the developmental processes of interest better characterized by the term emotion reasoning. We also highlight how the age at which children succeed on many tasks reflects myriad developmental processes. This new framing of emotional development can open new lines of inquiry about how humans learn to navigate their social worlds.","['https://openalex.org/W2082397615', 'https://openalex.org/W2113885007', 'https://openalex.org/W4321258516', 'https://openalex.org/W2537736171', 'https://openalex.org/W2093886672', 'https://openalex.org/W2165940037', 'https://openalex.org/W2165722159', 'https://openalex.org/W2014669686', 'https://openalex.org/W2088700311', 'https://openalex.org/W2158180452', 'https://openalex.org/W2160412058', 'https://openalex.org/W2087204502', 'https://openalex.org/W2912483755', 'https://openalex.org/W4233558597', 'https://openalex.org/W1685870920', 'https://openalex.org/W4250423795', 'https://openalex.org/W2162975013', 'https://openalex.org/W2076979024', 'https://openalex.org/W2166424708', 'https://openalex.org/W2061150508', 'https://openalex.org/W2041196974', 'https://openalex.org/W2136658613', 'https://openalex.org/W2095289724', 'https://openalex.org/W2076729302', 'https://openalex.org/W2011075538', 'https://openalex.org/W2042043284', 'https://openalex.org/W2041068027', 'https://openalex.org/W2129774500', 'https://openalex.org/W2053749101', 'https://openalex.org/W2030301190', 'https://openalex.org/W1964775147', 'https://openalex.org/W2951007345', 'https://openalex.org/W2626522798', 'https://openalex.org/W2752234108', 'https://openalex.org/W2970437976', 'https://openalex.org/W2318126827', 'https://openalex.org/W2142381843', 'https://openalex.org/W1967023950', 'https://openalex.org/W1988747931', 'https://openalex.org/W1989993559', 'https://openalex.org/W2072809820', 'https://openalex.org/W2063543474', 'https://openalex.org/W2783297063', 'https://openalex.org/W2082798268', 'https://openalex.org/W2022068631', 'https://openalex.org/W4255190871', 'https://openalex.org/W2099585587', 'https://openalex.org/W2320331759', 'https://openalex.org/W2054065149', 'https://openalex.org/W2146536023', 'https://openalex.org/W2009453175', 'https://openalex.org/W1538887405', 'https://openalex.org/W2625503951', 'https://openalex.org/W2058296271', 'https://openalex.org/W1963526179', 'https://openalex.org/W2091251508', 'https://openalex.org/W2113491937', 'https://openalex.org/W2156975004', 'https://openalex.org/W2050197540', 'https://openalex.org/W1624363100', 'https://openalex.org/W2524738427', 'https://openalex.org/W2164617610', 'https://openalex.org/W2027444771', 'https://openalex.org/W2767354091', 'https://openalex.org/W2111807011', 'https://openalex.org/W2041988588', 'https://openalex.org/W2911360157', 'https://openalex.org/W2130428458', 'https://openalex.org/W2118234543', 'https://openalex.org/W2079124862', 'https://openalex.org/W2041973398', 'https://openalex.org/W3005134493', 'https://openalex.org/W2989949548', 'https://openalex.org/W2970552953', 'https://openalex.org/W4238136915', 'https://openalex.org/W2105628924', 'https://openalex.org/W2021951233', 'https://openalex.org/W2145765358', 'https://openalex.org/W2020206307', 'https://openalex.org/W2110485626', 'https://openalex.org/W1988798223', 'https://openalex.org/W2326316607', 'https://openalex.org/W2168935649', 'https://openalex.org/W1981505495', 'https://openalex.org/W2157876613', 'https://openalex.org/W2150261429', 'https://openalex.org/W2941947643', 'https://openalex.org/W2077534194', 'https://openalex.org/W616034665', 'https://openalex.org/W3038006992', 'https://openalex.org/W2313536284', 'https://openalex.org/W2063124961', 'https://openalex.org/W2804367883', 'https://openalex.org/W2141035083', 'https://openalex.org/W1940908052', 'https://openalex.org/W2098842334', 'https://openalex.org/W2060719701', 'https://openalex.org/W2971091750', 'https://openalex.org/W2955311719', 'https://openalex.org/W4235261985', 'https://openalex.org/W2106425986', 'https://openalex.org/W4242321858', 'https://openalex.org/W2094572482', 'https://openalex.org/W2041064272', 'https://openalex.org/W4255574300', 'https://openalex.org/W2043119564', 'https://openalex.org/W2147449317', 'https://openalex.org/W1980542752', 'https://openalex.org/W2060993602', 'https://openalex.org/W2067451998', 'https://openalex.org/W2138930657', 'https://openalex.org/W4244763524', 'https://openalex.org/W2156655641', 'https://openalex.org/W2071005574', 'https://openalex.org/W2031584949', 'https://openalex.org/W2013701006', 'https://openalex.org/W2061275412', 'https://openalex.org/W2024652846', 'https://openalex.org/W2804043866', 'https://openalex.org/W1647735134', 'https://openalex.org/W2402038064', 'https://openalex.org/W2015160668', 'https://openalex.org/W1935234052', 'https://openalex.org/W4235096206', 'https://openalex.org/W1974955132', 'https://openalex.org/W2154611638', 'https://openalex.org/W2106066998', 'https://openalex.org/W2166794841', 'https://openalex.org/W2071407075', 'https://openalex.org/W2905769577', 'https://openalex.org/W2971082749', 'https://openalex.org/W2152836667', 'https://openalex.org/W2159525731', 'https://openalex.org/W4249514754', 'https://openalex.org/W2036964749', 'https://openalex.org/W3016455511', 'https://openalex.org/W3008252655', 'https://openalex.org/W2156019046', 'https://openalex.org/W1564363340', 'https://openalex.org/W4241719668', 'https://openalex.org/W2274803983', 'https://openalex.org/W2414224250', 'https://openalex.org/W2616564498', 'https://openalex.org/W2619853997', 'https://openalex.org/W4245685152', 'https://openalex.org/W2058919176', 'https://openalex.org/W2100975032', 'https://openalex.org/W2074683269', 'https://openalex.org/W1986654622', 'https://openalex.org/W2130281101', 'https://openalex.org/W2103573976', 'https://openalex.org/W2739486288', 'https://openalex.org/W2920064479', 'https://openalex.org/W3022239069', 'https://openalex.org/W3003939235', 'https://openalex.org/W2753659043', 'https://openalex.org/W1975238145', 'https://openalex.org/W2134031328', 'https://openalex.org/W2125620095', 'https://openalex.org/W2143533232', 'https://openalex.org/W2604150021', 'https://openalex.org/W2088136951', 'https://openalex.org/W2997769808', 'https://openalex.org/W2068603690', 'https://openalex.org/W2124632805', 'https://openalex.org/W2143858405', 'https://openalex.org/W2080952562', 'https://openalex.org/W2758786803', 'https://openalex.org/W2168287985', 'https://openalex.org/W2130553727', 'https://openalex.org/W1985898429', 'https://openalex.org/W2096244115', 'https://openalex.org/W2753212995', 'https://openalex.org/W2091036982', 'https://openalex.org/W1999143847', 'https://openalex.org/W4205595756', 'https://openalex.org/W2011485796', 'https://openalex.org/W2066737169', 'https://openalex.org/W2065364824', 'https://openalex.org/W2171999781', 'https://openalex.org/W3009309170', 'https://openalex.org/W2599495999', 'https://openalex.org/W2613928842', 'https://openalex.org/W2159102798', 'https://openalex.org/W2119224589', 'https://openalex.org/W2058220015', 'https://openalex.org/W2005142040', 'https://openalex.org/W2315754477', 'https://openalex.org/W2009485567', 'https://openalex.org/W4242014394', 'https://openalex.org/W2110175953', 'https://openalex.org/W1978959299', 'https://openalex.org/W1995711259', 'https://openalex.org/W2007125182', 'https://openalex.org/W2129879896', 'https://openalex.org/W3038068812', 'https://openalex.org/W1976056313', 'https://openalex.org/W2765702083', 'https://openalex.org/W2894881332', 'https://openalex.org/W2037530582', 'https://openalex.org/W2032431790', 'https://openalex.org/W2064362481', 'https://openalex.org/W1571620383', 'https://openalex.org/W2908473373', 'https://openalex.org/W635925319']",2020-12-15
https://openalex.org/W2009375902,https://doi.org/10.1037/10001-000,The expression of the emotions in man and animals.,"Acknowledgments List of Illustrations Figures Plates Preface to the Anniversary Edition by Paul Ekman Preface to the Third Edition by Paul Ekman Preface to the Second Edition by Francis Darwin Introduction to the Third Edition by Paul Ekman The Expression of the Emotions in Man and Animals Introduction to the First Edition 1. General Principles of Expression 2. General Principles of Expression -- continued 3. General Principles of Expression -- continued 4. Means of Expression in Animals 5. Special Expressions of Animals 6. Special Expressions of Man: Suffering and Weeping 7. Low Spirits, Anxiety, Grief, Dejection, Despair 8. Joy, High Spirits, Love, Tender Feelings, Devotion 9. Reflection - Meditation - Ill-temper - Sulkiness - Determination 10. Hatred and Anger 11. Disdain - Contempt - Disgust - Guilt - Pride, Etc. - Helplessness - Patience - Affirmation and Negation 12. Surprise - Astonishment - Fear - Horror 13. Self-attention - Shame - Shyness - Modesty: Blushing 14. Concluding Remarks and Summary Afterword, by Paul Ekman APPENDIX I: Charles Darwin's Obituary, by T. H. Huxley APPENDIX II: Changes to the Text, by Paul Ekman APPENDIX III: Photography and The Expression of the Emotions, by Phillip Prodger APPENDIX IV: A Note on the Orientation of the Plates, by Phillip Prodger and Paul Ekman APPENDIX V: Concordance of Illustrations, by Phillip Prodger APPENDIX VI: List of Head Words from the Index to the First Edition NOTES NOTES TO THE COMMENTARIES INDEX",[],1872-01-01
https://openalex.org/W3015884429,https://doi.org/10.1109/icassp40776.2020.9053192,Speech Emotion Recognition with Local-Global Aware Deep Representation Learning,"Convolutional neural network (CNN) based deep representation learning methods for speech emotion recognition (SER) have demonstrated great success. The basic design of CNN restricts the ability to model only local information well. Capsule network (CapsNet) can overcome the shortages of CNNs to capture the shallow global features from the spectrogram, although CapsNet cannot learn the local and deep global information. In this paper, we propose a local-global aware deep representation learning system that mainly includes two modules. One module contains a multi-scale CNN, time- frequency CNN (TFCNN) to learn the local representation. In the other module, we introduce a structure with dense connections of multiple blocks to learn shallow and deep global information. Every block in this structure is a complete CapsNet improved by a new routing algorithm. The local and global representations are fed to the classifier and achieve an absolute increase of at least 4.25% than benchmarks on IEMOCAP.","['https://openalex.org/W6697498398', 'https://openalex.org/W6714031499', 'https://openalex.org/W2747664154', 'https://openalex.org/W2800253600', 'https://openalex.org/W6743446608', 'https://openalex.org/W6751741970', 'https://openalex.org/W2897402719', 'https://openalex.org/W2796138868', 'https://openalex.org/W2889374687', 'https://openalex.org/W6739901393', 'https://openalex.org/W2312140198', 'https://openalex.org/W6680189088', 'https://openalex.org/W2080576537', 'https://openalex.org/W1963552095', 'https://openalex.org/W2648194195', 'https://openalex.org/W2399733683', 'https://openalex.org/W2889325879', 'https://openalex.org/W2161073241', 'https://openalex.org/W2889107526', 'https://openalex.org/W2146334809', 'https://openalex.org/W2885005742', 'https://openalex.org/W2105768069', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963403868', 'https://openalex.org/W2962853356', 'https://openalex.org/W2408520939', 'https://openalex.org/W2964318666', 'https://openalex.org/W2295001676', 'https://openalex.org/W2963703618', 'https://openalex.org/W4295238618', 'https://openalex.org/W2137129454']",2020-04-09
https://openalex.org/W3160039712,https://doi.org/10.1109/icassp39728.2021.9414540,Hierarchical Network Based on the Fusion of Static and Dynamic Features for Speech Emotion Recognition,"Many studies on automatic speech emotion recognition (SER) have been devoted to extracting meaningful emotional features for generating emotion-relevant representations. However, they generally ignore the complementary learning of static and dynamic features, leading to limited performances. In this paper, we propose a novel hierarchical network called HNSD that can efficiently integrate the static and dynamic features for SER. Specifically, the proposed HNSD framework consists of three different modules. To capture the discriminative features, an effective encoding module is firstly designed to simultaneously encode both static and dynamic features. By taking the obtained features as inputs, the Gated Multi-features Unit (GMU) is conducted to explicitly determine the emotional intermediate representations for frame-level features fusion, instead of directly fusing these acoustic features. In this way, the learned static and dynamic features can jointly and comprehensively generate the unified feature representations. Benefiting from a well-designed attention mechanism, the last classification module is applied to predict the emotional states at the utterance level. Extensive experiments on the IEMOCAP benchmark dataset demonstrate the superiority of our method in comparison with state-of-the-art baselines.","['https://openalex.org/W2885005742', 'https://openalex.org/W6714031499', 'https://openalex.org/W2788776247', 'https://openalex.org/W6733328719', 'https://openalex.org/W6679434410', 'https://openalex.org/W2939987519', 'https://openalex.org/W2146334809', 'https://openalex.org/W2087618018', 'https://openalex.org/W6638667902', 'https://openalex.org/W6631190155', 'https://openalex.org/W2889374687', 'https://openalex.org/W2747664154', 'https://openalex.org/W3097149715', 'https://openalex.org/W2940259008', 'https://openalex.org/W6738763440', 'https://openalex.org/W2625297138', 'https://openalex.org/W6697498398', 'https://openalex.org/W2074788634', 'https://openalex.org/W2973181312', 'https://openalex.org/W2972498864', 'https://openalex.org/W3015240477', 'https://openalex.org/W2936113082', 'https://openalex.org/W2587372848', 'https://openalex.org/W2408520939', 'https://openalex.org/W2964308564', 'https://openalex.org/W2949117887', 'https://openalex.org/W2295001676', 'https://openalex.org/W2964121744', 'https://openalex.org/W1522301498', 'https://openalex.org/W1836465849', 'https://openalex.org/W2133564696', 'https://openalex.org/W2962736520', 'https://openalex.org/W2951529591']",2021-05-13
https://openalex.org/W3204087964,https://doi.org/10.1109/icassp43922.2022.9746679,LIGHT-SERNET: A Lightweight Fully Convolutional Neural Network for Speech Emotion Recognition,"Detecting emotions directly from a speech signal plays an important role in effective human-computer interactions. Existing speech emotion recognition models require massive computational and storage resources, making them hard to implement concurrently with other machine-interactive tasks in embedded systems. In this paper, we propose an efficient and lightweight fully convolutional neural network for speech emotion recognition in systems with limited hard-ware resources. In the proposed FCNN model, various feature maps are extracted via three parallel paths with different filter sizes. This helps deep convolution blocks to extract high-level features, while ensuring sufficient separability. The extracted features are used to classify the emotion of the input speech segment. While our model has a smaller size than that of the state-of-the-art models, it achieves a higher performance on the IEMOCAP and EMO-DB datasets. The source code is available https://github.com/AryaAftab/LIGHT-SERNET","['https://openalex.org/W2919115771', 'https://openalex.org/W2399733683', 'https://openalex.org/W2583643061', 'https://openalex.org/W2578895956', 'https://openalex.org/W2962949934', 'https://openalex.org/W2986661129', 'https://openalex.org/W6729983426', 'https://openalex.org/W2889717020', 'https://openalex.org/W2146334809', 'https://openalex.org/W175750906', 'https://openalex.org/W3097149715', 'https://openalex.org/W2940259008', 'https://openalex.org/W2889191349', 'https://openalex.org/W2885005742', 'https://openalex.org/W2939764705', 'https://openalex.org/W2747664154', 'https://openalex.org/W2295001676', 'https://openalex.org/W2950518992', 'https://openalex.org/W2099767163', 'https://openalex.org/W2963351448', 'https://openalex.org/W2556967412', 'https://openalex.org/W2949846184']",2022-04-27
https://openalex.org/W4312120641,https://doi.org/10.23919/apsipaasc55919.2022.9979844,Multi-task Learning for Speech Emotion and Emotion Intensity Recognition,"Speech emotion recognition (SER) helps achieve better human-computer interaction and thus has attracted extensive attention from industry and academia. Speech emotion intensity plays an important role in the emotional description, but its effect on emotion recognition still has been rarely studied in the area of SER to the best of our knowledge. Previous studies have shown that there is a certain relationship between speech emotion intensity and emotion category, so each recognition task of multi-task learning is supposed to be beneficial to each other. We propose a multi-task learning framework with a self-supervised speech representation extractor based on Wav2Vec 2.0 to detect speech emotion and intensity at the same time in downstream networks. Experiment results show that the multi-task learning framework outperforms SOTA SER models and achieves 5% and 7% SER performance improvement on IEMOCAP and RAVDESS thanks to the auxiliary task of emotion intensity recognition.","['https://openalex.org/W2074788634', 'https://openalex.org/W2784665486', 'https://openalex.org/W2969889150', 'https://openalex.org/W2765820206', 'https://openalex.org/W2796430037', 'https://openalex.org/W2146334809', 'https://openalex.org/W1983681076', 'https://openalex.org/W1972076735', 'https://openalex.org/W3027701939', 'https://openalex.org/W3015841875', 'https://openalex.org/W3000618647', 'https://openalex.org/W2805939548', 'https://openalex.org/W2954107114', 'https://openalex.org/W4224918181', 'https://openalex.org/W3003903817', 'https://openalex.org/W2964128364', 'https://openalex.org/W3198275944', 'https://openalex.org/W3015141382', 'https://openalex.org/W2963821905', 'https://openalex.org/W4375869379', 'https://openalex.org/W2803193013', 'https://openalex.org/W3197580070']",2022-11-07
https://openalex.org/W2795109282,https://doi.org/10.48550/arxiv.1803.09047,Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron,"We present an extension to the Tacotron speech synthesis architecture that learns a latent embedding space of prosody, derived from a reference acoustic representation containing the desired prosody. We show that conditioning Tacotron on this learned embedding space results in synthesized audio that matches the prosody of the reference signal with fine time detail even when the reference and synthesis speakers are different. Additionally, we show that a reference prosody embedding can be used to synthesize text that is different from that of the reference utterance. We define several quantitative and subjective metrics for evaluating prosody transfer, and report results with accompanying audio samples from single-speaker and 44-speaker Tacotron models on a prosody transfer task.","['https://openalex.org/W2778926431', 'https://openalex.org/W2107860279', 'https://openalex.org/W2794490148', 'https://openalex.org/W2777302760', 'https://openalex.org/W2963799213', 'https://openalex.org/W2130086727', 'https://openalex.org/W2766406951', 'https://openalex.org/W2950635152', 'https://openalex.org/W2651834199', 'https://openalex.org/W28194048', 'https://openalex.org/W174066062', 'https://openalex.org/W2963609956', 'https://openalex.org/W2949117887', 'https://openalex.org/W1810943226', 'https://openalex.org/W2964121744', 'https://openalex.org/W162654330', 'https://openalex.org/W2475998840', 'https://openalex.org/W2103869314', 'https://openalex.org/W89128468', 'https://openalex.org/W2156146072', 'https://openalex.org/W2619368999', 'https://openalex.org/W2578842094']",2018-03-24
https://openalex.org/W4313887688,https://doi.org/10.1109/taslp.2023.3235194,SpeechFormer++: A Hierarchical Efficient Framework for Paralinguistic Speech Processing,"Paralinguistic speech processing is important in addressing many issues, such\nas sentiment and neurocognitive disorder analyses. Recently, Transformer has\nachieved remarkable success in the natural language processing field and has\ndemonstrated its adaptation to speech. However, previous works on Transformer\nin the speech field have not incorporated the properties of speech, leaving the\nfull potential of Transformer unexplored. In this paper, we consider the\ncharacteristics of speech and propose a general structure-based framework,\ncalled SpeechFormer++, for paralinguistic speech processing. More concretely,\nfollowing the component relationship in the speech signal, we design a unit\nencoder to model the intra- and inter-unit information (i.e., frames, phones,\nand words) efficiently. According to the hierarchical relationship, we utilize\nmerging blocks to generate features at different granularities, which is\nconsistent with the structural pattern in the speech signal. Moreover, a word\nencoder is introduced to integrate word-grained features into each unit\nencoder, which effectively balances fine-grained and coarse-grained\ninformation. SpeechFormer++ is evaluated on the speech emotion recognition\n(IEMOCAP &amp; MELD), depression classification (DAIC-WOZ) and Alzheimer's disease\ndetection (Pitt) tasks. The results show that SpeechFormer++ outperforms the\nstandard Transformer while greatly reducing the computational cost.\nFurthermore, it delivers superior results compared to the state-of-the-art\napproaches.\n","['https://openalex.org/W2114330138', 'https://openalex.org/W1935012542', 'https://openalex.org/W2134929491', 'https://openalex.org/W4239447739', 'https://openalex.org/W2529925562', 'https://openalex.org/W2099767163', 'https://openalex.org/W1972420736', 'https://openalex.org/W2076423492', 'https://openalex.org/W2964295436', 'https://openalex.org/W3208158979', 'https://openalex.org/W3101080567', 'https://openalex.org/W3015554124', 'https://openalex.org/W3162475537', 'https://openalex.org/W4285106979', 'https://openalex.org/W3096039514', 'https://openalex.org/W3196735225', 'https://openalex.org/W4221154966', 'https://openalex.org/W3014475539', 'https://openalex.org/W3201447839', 'https://openalex.org/W2408520939', 'https://openalex.org/W1964757081', 'https://openalex.org/W2896480997', 'https://openalex.org/W2786625961', 'https://openalex.org/W3162993161', 'https://openalex.org/W4385245566', 'https://openalex.org/W3138516171', 'https://openalex.org/W6810563863', 'https://openalex.org/W4312530435', 'https://openalex.org/W6795463671', 'https://openalex.org/W6796494063', 'https://openalex.org/W4285194130', 'https://openalex.org/W4224917001', 'https://openalex.org/W3161659450', 'https://openalex.org/W4226442948', 'https://openalex.org/W3120680448', 'https://openalex.org/W3016138882', 'https://openalex.org/W4221162874', 'https://openalex.org/W4225748935', 'https://openalex.org/W3035448883', 'https://openalex.org/W2905016804', 'https://openalex.org/W3042631625', 'https://openalex.org/W3174794493', 'https://openalex.org/W6791705549', 'https://openalex.org/W3015240477', 'https://openalex.org/W4224932149', 'https://openalex.org/W3210836789', 'https://openalex.org/W3196495667', 'https://openalex.org/W3182607842', 'https://openalex.org/W2973049979', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W4221162872', 'https://openalex.org/W4361994820', 'https://openalex.org/W6838923520', 'https://openalex.org/W4224918181', 'https://openalex.org/W2976556660', 'https://openalex.org/W2981857663', 'https://openalex.org/W3097777922', 'https://openalex.org/W2990825125', 'https://openalex.org/W2964266095', 'https://openalex.org/W3095334805', 'https://openalex.org/W3197977579', 'https://openalex.org/W3198220993', 'https://openalex.org/W2157331557', 'https://openalex.org/W6804124405', 'https://openalex.org/W2029996593', 'https://openalex.org/W2146334809', 'https://openalex.org/W2963686995', 'https://openalex.org/W2074037951', 'https://openalex.org/W2969544905', 'https://openalex.org/W6691669583', 'https://openalex.org/W2751214333', 'https://openalex.org/W6788328058', 'https://openalex.org/W3197580070', 'https://openalex.org/W2965453734', 'https://openalex.org/W4297841379', 'https://openalex.org/W3124411359', 'https://openalex.org/W3154831185', 'https://openalex.org/W4296070431', 'https://openalex.org/W4232774389', 'https://openalex.org/W4283711114', 'https://openalex.org/W4221095487', 'https://openalex.org/W4286891994', 'https://openalex.org/W3168124404', 'https://openalex.org/W3121914243', 'https://openalex.org/W4287634424', 'https://openalex.org/W3213786560', 'https://openalex.org/W3164208409', 'https://openalex.org/W3036601975', 'https://openalex.org/W2252180568', 'https://openalex.org/W3094502228', 'https://openalex.org/W4221142474', 'https://openalex.org/W4394666973', 'https://openalex.org/W3137963805']",2023-01-01
https://openalex.org/W4285251897,https://doi.org/10.1109/access.2022.3189481,Speech Emotion and Naturalness Recognitions With Multitask and Single-Task Learnings,"This paper evaluates speech emotion and naturalness recognitions by utilizing deep learning models with multitask learning and single-task learning approaches. The emotion model accommodates valence, arousal, and dominance attributes known as dimensional emotion. The naturalness ratings are labeled on a five-point scale as dimensional emotion. Multitask learning predicts both dimensional emotion (as the main task) and naturalness scores (as an auxiliary task) simultaneously. The single-task learning predicts either dimensional emotion (valence, arousal, and dominance) or naturalness score independently. The results with multitask learning show improvement from previous studies on single-task learning for both dimensional emotion recognition and naturalness predictions. Within this study, single-task learning still shows superiority over multitask learning for naturalness recognition. The scatter plots of emotion and naturalness prediction scores against the true labels in multitask learning exhibit the lack of the model; it fails to predict the low and extremely high scores. The low score of naturalness prediction in this study is possibly due to a low number of samples of unnatural speech samples since the MSP-IMPROV dataset promotes the naturalness of speech. The finding that jointly predicting naturalness with emotion helps improve the performance of emotion recognition may be embodied in the emotion recognition model in future work.","['https://openalex.org/W2156848952', 'https://openalex.org/W2165857685', 'https://openalex.org/W2008264338', 'https://openalex.org/W6980118482', 'https://openalex.org/W3162538529', 'https://openalex.org/W3038694848', 'https://openalex.org/W3097206152', 'https://openalex.org/W3104763483', 'https://openalex.org/W2747172199', 'https://openalex.org/W2939274787', 'https://openalex.org/W2342475039', 'https://openalex.org/W2153822685', 'https://openalex.org/W2192412620', 'https://openalex.org/W2090777335', 'https://openalex.org/W2513507089', 'https://openalex.org/W2239141610', 'https://openalex.org/W147964346', 'https://openalex.org/W6675354045', 'https://openalex.org/W6713134421', 'https://openalex.org/W2346454595', 'https://openalex.org/W6759527129', 'https://openalex.org/W2900358852', 'https://openalex.org/W1412878278', 'https://openalex.org/W4288586849', 'https://openalex.org/W2101234009']",2022-01-01
https://openalex.org/W4221162872,https://doi.org/10.1109/icassp43922.2022.9747095,Speech Emotion Recognition with Co-Attention Based Multi-Level Acoustic Information,"Speech Emotion Recognition (SER) aims to help the machine to understand human's subjective emotion from only audio in-formation. However, extracting and utilizing comprehensive in-depth audio information is still a challenging task. In this paper, we propose an end-to-end speech emotion recognition system using multi-level acoustic information with a newly designed co-attention module. We firstly extract multi-level acoustic information, including MFCC, spectrogram, and the embedded high-level acoustic information with CNN, BiL-STM and wav2vec2, respectively. Then these extracted features are treated as multimodal inputs and fused by the pro-posed co-attention mechanism. Experiments are carried on the IEMOCAP dataset, and our model achieves competitive performance with two different speaker-independent cross-validation strategies. Our code is available on GitHub.","['https://openalex.org/W3162160536', 'https://openalex.org/W6780218876', 'https://openalex.org/W3162475537', 'https://openalex.org/W2972498864', 'https://openalex.org/W2408520939', 'https://openalex.org/W2885005742', 'https://openalex.org/W2973181312', 'https://openalex.org/W2936113082', 'https://openalex.org/W2889374687', 'https://openalex.org/W2146334809', 'https://openalex.org/W3015969913', 'https://openalex.org/W3163091219', 'https://openalex.org/W3160222702', 'https://openalex.org/W3160039712', 'https://openalex.org/W3160183718', 'https://openalex.org/W3096723250', 'https://openalex.org/W2158705122', 'https://openalex.org/W2032254851', 'https://openalex.org/W3015906487', 'https://openalex.org/W2191779130', 'https://openalex.org/W3198528147', 'https://openalex.org/W3036601975']",2022-04-27
https://openalex.org/W3130293557,https://doi.org/10.1016/j.vrih.2020.12.002,Self-attention transfer networks for speech emotion recognition,"Background: A crucial element of human–machine interaction, the automatic detection of emotional states from human speech has long been regarded as a challenging task for machine learning models. One vital challenge in speech emotion recognition (SER) is how to learn robust and discriminative representations from speech. Meanwhile, although machine learning methods have been widely applied in SER research, the inadequate amount of available annotated data has become a bottleneck that impedes the extended application of techniques (e.g., deep neural networks). To address this issue, we present a deep learning method that combines knowledge transfer and self-attention for SER tasks. Here, we apply the log-Mel spectrogram with deltas and delta-deltas as input. Moreover, given that emotions are time-dependent, we apply Temporal Convolutional Neural Networks (TCNs) to model the variations in emotions. We further introduce an attention transfer mechanism, which is based on a self-attention algorithm in order to learn long-term dependencies. The Self-Attention Transfer Network (SATN) in our proposed approach, takes advantage of attention autoencoders to learn attention from a source task, and then from speech recognition, followed by transferring this knowledge into SER. Evaluation built on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) demonstrates the effectiveness of the novel model.","['https://openalex.org/W211912913', 'https://openalex.org/W2610961739', 'https://openalex.org/W6739380106', 'https://openalex.org/W2990825125', 'https://openalex.org/W6775918983', 'https://openalex.org/W6638545294', 'https://openalex.org/W6770653827', 'https://openalex.org/W6621543089', 'https://openalex.org/W6739901393', 'https://openalex.org/W6764448387', 'https://openalex.org/W2964189376', 'https://openalex.org/W2904452845', 'https://openalex.org/W6767548918', 'https://openalex.org/W2003502731', 'https://openalex.org/W6742029710', 'https://openalex.org/W6745510239', 'https://openalex.org/W6748082341', 'https://openalex.org/W2970737019', 'https://openalex.org/W2885005742', 'https://openalex.org/W2087618018', 'https://openalex.org/W6664441465', 'https://openalex.org/W6785738406', 'https://openalex.org/W6748096335', 'https://openalex.org/W6725688724', 'https://openalex.org/W6729280921', 'https://openalex.org/W6767227387', 'https://openalex.org/W6623517193', 'https://openalex.org/W6639082767', 'https://openalex.org/W6679434410', 'https://openalex.org/W6754641952', 'https://openalex.org/W6767906889', 'https://openalex.org/W6758423016', 'https://openalex.org/W6754002923', 'https://openalex.org/W2761514455', 'https://openalex.org/W6742012266', 'https://openalex.org/W6637551013', 'https://openalex.org/W6759578584', 'https://openalex.org/W2146334809', 'https://openalex.org/W6767778986', 'https://openalex.org/W6697498398', 'https://openalex.org/W6714031499', 'https://openalex.org/W6761731756', 'https://openalex.org/W2972660800', 'https://openalex.org/W2512885694', 'https://openalex.org/W3177525997', 'https://openalex.org/W2912083425', 'https://openalex.org/W2730845691', 'https://openalex.org/W4300931031', 'https://openalex.org/W2913059114', 'https://openalex.org/W2914695977', 'https://openalex.org/W2762632520', 'https://openalex.org/W2915480215', 'https://openalex.org/W4289313087', 'https://openalex.org/W2599674900', 'https://openalex.org/W2593463961', 'https://openalex.org/W2913550731', 'https://openalex.org/W2911495555', 'https://openalex.org/W4245692952']",2021-02-01
https://openalex.org/W2972498864,https://doi.org/10.21437/interspeech.2019-2822,Self-Attention for Speech Emotion Recognition,"Speech Emotion Recognition (SER) has been shown to benefit from many of the recent advances in deep learning, including recurrent based and attention based neural network architectures as well. Nevertheless, performance still falls short of that of humans. In this work, we investigate whether SER could benefit from the self-attention and global windowing of the transformer model. We show on the IEMOCAP database that this is indeed the case. Finally, we investigate whether using the distribution of, possibly conflicting, annotations in the training data, as soft targets could outperform a majority voting. We prove that this performance increases with the agreement level of the annotators.","['https://openalex.org/W2239141610', 'https://openalex.org/W2486205537', 'https://openalex.org/W2517194566', 'https://openalex.org/W2146334809', 'https://openalex.org/W2408520939', 'https://openalex.org/W2470673105', 'https://openalex.org/W1815076433', 'https://openalex.org/W1964757081', 'https://openalex.org/W2090777335', 'https://openalex.org/W2604490051', 'https://openalex.org/W2565875961', 'https://openalex.org/W2962736520', 'https://openalex.org/W2896457183', 'https://openalex.org/W2964001806', 'https://openalex.org/W2963403868', 'https://openalex.org/W2804664105', 'https://openalex.org/W2912728762', 'https://openalex.org/W2963341956', 'https://openalex.org/W2751214333', 'https://openalex.org/W2625297138', 'https://openalex.org/W2892071465', 'https://openalex.org/W2293270307', 'https://openalex.org/W1501669607', 'https://openalex.org/W4385245566', 'https://openalex.org/W2558799462', 'https://openalex.org/W2583743457']",2019-09-13
https://openalex.org/W648786980,https://doi.org/10.48550/arxiv.1506.03099,Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks,"Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used successfully in our winning entry to the MSCOCO image captioning challenge, 2015.","['https://openalex.org/W2964308564', 'https://openalex.org/W1869752048', 'https://openalex.org/W2130942839', 'https://openalex.org/W2088911157', 'https://openalex.org/W1895577753', 'https://openalex.org/W1527575280', 'https://openalex.org/W1861492603', 'https://openalex.org/W1947481528', 'https://openalex.org/W595069947', 'https://openalex.org/W1524333225', 'https://openalex.org/W2158349948', 'https://openalex.org/W2107878631', 'https://openalex.org/W2104917081', 'https://openalex.org/W2734523219', 'https://openalex.org/W1956340063', 'https://openalex.org/W2962957031', 'https://openalex.org/W1931639407', 'https://openalex.org/W2962706528', 'https://openalex.org/W2268617045', 'https://openalex.org/W1586532344', 'https://openalex.org/W1836465849', 'https://openalex.org/W1905882502', 'https://openalex.org/W2147880316', 'https://openalex.org/W2296073425', 'https://openalex.org/W2064675550']",2015-06-09
https://openalex.org/W3211224152,https://doi.org/10.48550/arxiv.2111.02735,"A Fine-tuned Wav2vec 2.0/HuBERT Benchmark For Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding","Speech self-supervised models such as wav2vec 2.0 and HuBERT are making revolutionary progress in Automatic Speech Recognition (ASR). However, they have not been totally proven to produce better performance on tasks other than ASR. In this work, we explored partial fine-tuning and entire fine-tuning on wav2vec 2.0 and HuBERT pre-trained models for three non-ASR speech tasks: Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding. With simple proposed downstream frameworks, the best scores reached 79.58% weighted accuracy on speaker-dependent setting and 73.01% weighted accuracy on speaker-independent setting for Speech Emotion Recognition on IEMOCAP, 2.36% equal error rate for Speaker Verification on VoxCeleb1, 89.38% accuracy for Intent Classification and 78.92% F1 for Slot Filling on SLURP, showing the strength of fine-tuned wav2vec 2.0 and HuBERT on learning prosodic, voice-print and semantic representations.","['https://openalex.org/W3198275944', 'https://openalex.org/W2146334809', 'https://openalex.org/W2963341956', 'https://openalex.org/W3099782249', 'https://openalex.org/W3167533889', 'https://openalex.org/W2602034649', 'https://openalex.org/W3155162503', 'https://openalex.org/W2973049979', 'https://openalex.org/W3197642003', 'https://openalex.org/W2746742816', 'https://openalex.org/W3024869864', 'https://openalex.org/W3169320628', 'https://openalex.org/W3041561163', 'https://openalex.org/W2127141656', 'https://openalex.org/W2936113082', 'https://openalex.org/W3197580070', 'https://openalex.org/W3190342989', 'https://openalex.org/W2995181338', 'https://openalex.org/W2996383576', 'https://openalex.org/W2972943112', 'https://openalex.org/W2964121744', 'https://openalex.org/W2726515241', 'https://openalex.org/W2963288440', 'https://openalex.org/W2982223350', 'https://openalex.org/W2889374687', 'https://openalex.org/W3015213852', 'https://openalex.org/W3198858531']",2021-11-04
https://openalex.org/W2774085128,https://doi.org/10.48550/arxiv.1803.11508,Reusing Neural Speech Representations for Auditory Emotion Recognition,"Acoustic emotion recognition aims to categorize the affective state of the speaker and is still a difficult task for machine learning models. The difficulties come from the scarcity of training data, general subjectivity in emotion perception resulting in low annotator agreement, and the uncertainty about which features are the most relevant and robust ones for classification. In this paper, we will tackle the latter problem. Inspired by the recent success of transfer learning methods we propose a set of architectures which utilize neural representations inferred by training on large speech databases for the acoustic emotion recognition task. Our experiments on the IEMOCAP dataset show ~10% relative improvements in the accuracy and F1-score over the baseline recurrent neural network which is trained end-to-end for emotion recognition.","['https://openalex.org/W2511566646', 'https://openalex.org/W2127141656', 'https://openalex.org/W1635512741', 'https://openalex.org/W2554354235', 'https://openalex.org/W1522301498', 'https://openalex.org/W1494198834', 'https://openalex.org/W2295582178', 'https://openalex.org/W2090777335', 'https://openalex.org/W2949667497', 'https://openalex.org/W1686810756', 'https://openalex.org/W2158061940', 'https://openalex.org/W2399733683', 'https://openalex.org/W2949640717', 'https://openalex.org/W2251321385', 'https://openalex.org/W2602034649', 'https://openalex.org/W2512885694', 'https://openalex.org/W2144005487', 'https://openalex.org/W2146334809', 'https://openalex.org/W2949888546', 'https://openalex.org/W2583743457', 'https://openalex.org/W2426267443']",2018-03-30
https://openalex.org/W4225939199,https://doi.org/10.1109/icassp43922.2022.9747763,SpeechSplit2.0: Unsupervised Speech Disentanglement for Voice Conversion without Tuning Autoencoder Bottlenecks,"SpeechSplit can perform aspect-specific voice conversion by disentangling speech into content, rhythm, pitch, and timbre using multiple autoencoders in an unsupervised manner. However, SpeechSplit requires careful tuning of the autoencoder bottlenecks, which can be time-consuming and less robust. This paper proposes SpeechSplit2.0, which constrains the information flow of the speech component to be disentangled on the autoencoder input using efficient signal processing methods instead of bottleneck tuning. Evaluation results show that SpeechSplit2.0 achieves comparable performance to SpeechSplit in speech disentanglement and superior robustness to the bottleneck size variations. Our code is available at https://github.com/biggytruck/SpeechSplit2.","['https://openalex.org/W2902070858', 'https://openalex.org/W2963767194', 'https://openalex.org/W2962793481', 'https://openalex.org/W3015805741', 'https://openalex.org/W6796577156', 'https://openalex.org/W6776390925', 'https://openalex.org/W2471520273', 'https://openalex.org/W6675409298', 'https://openalex.org/W3095123370', 'https://openalex.org/W3097290232', 'https://openalex.org/W3198605397', 'https://openalex.org/W6785164032', 'https://openalex.org/W2532494225', 'https://openalex.org/W6762533536', 'https://openalex.org/W2946555236', 'https://openalex.org/W2963830550', 'https://openalex.org/W2907262790', 'https://openalex.org/W2962689740', 'https://openalex.org/W2972667718', 'https://openalex.org/W3034794073', 'https://openalex.org/W3169739675', 'https://openalex.org/W2945478979', 'https://openalex.org/W3142644187', 'https://openalex.org/W2099621636']",2022-04-27
https://openalex.org/W3205878676,https://doi.org/10.1109/icassp43922.2022.9746806,TitaNet: Neural Model for Speaker Representation with 1D Depth-Wise Separable Convolutions and Global Context,"In this paper, we propose TitaNet, a novel neural network architecture for extracting speaker representations. We employ 1D depth-wise separable convolutions with Squeeze-and-Excitation (SE) layers with global context followed by channel attention based statistics pooling layer to map variable-length utterances to a fixed-length embedding (t-vector). TitaNet is a scalable architecture and achieves state-of-the-art performance on speaker verification task with an equal error rate (EER) of 0.68% on the VoxCeleb1 trial file and also on speaker diarization tasks with diarization error rate (DER) of 1.73% on AMI-MixHeadset, 1.99% on AMI-Lapel and 1.11% on CH109. Furthermore, we investigate various sizes of TitaNet and present a light TitaNet-S model with only 6M parameters that achieve near state-of-the-art results in diarization tasks.","['https://openalex.org/W6767671539', 'https://openalex.org/W6769178842', 'https://openalex.org/W3095173472', 'https://openalex.org/W3024869864', 'https://openalex.org/W6713727690', 'https://openalex.org/W2889418727', 'https://openalex.org/W3178462146', 'https://openalex.org/W6678809451', 'https://openalex.org/W6711908931', 'https://openalex.org/W3015537910', 'https://openalex.org/W2969985801', 'https://openalex.org/W6785552926', 'https://openalex.org/W2889016587', 'https://openalex.org/W3097777922', 'https://openalex.org/W2940070181', 'https://openalex.org/W2802488037', 'https://openalex.org/W3134299941', 'https://openalex.org/W3096918678', 'https://openalex.org/W3015985683', 'https://openalex.org/W2890964092', 'https://openalex.org/W3143367551', 'https://openalex.org/W2046056978', 'https://openalex.org/W2808631503', 'https://openalex.org/W2696967604', 'https://openalex.org/W1494198834', 'https://openalex.org/W3015783745', 'https://openalex.org/W2936774411', 'https://openalex.org/W2396765756', 'https://openalex.org/W3105031100', 'https://openalex.org/W4288091954', 'https://openalex.org/W3021469861', 'https://openalex.org/W2981461916', 'https://openalex.org/W2972961496', 'https://openalex.org/W2406312423', 'https://openalex.org/W2974231335', 'https://openalex.org/W2125336414']",2022-04-27
https://openalex.org/W4391021542,https://doi.org/10.1109/asru57964.2023.10389701,Fast Conformer With Linearly Scalable Attention For Efficient Speech Recognition,"Conformer-based models have become the dominant end-to-end architecture for speech processing tasks. With the objective of enhancing the conformer architecture for efficient training and inference, we carefully redesigned Conformer with a novel downsampling schema. The proposed model, named Fast Conformer(FC), is 2.8 × faster than the original Conformer, supports scaling to Billion parameters without any changes to the core architecture and also achieves state-of-the-art accuracy on Automatic Speech Recognition benchmarks. To enable transcription of long-form speech up to 11 hours, we replaced global attention with limited context attention post-training, while also improving accuracy through fine-tuning with the addition of a global token. Fast Conformer, when combined with a Transformer decoder also outperforms the original Conformer in accuracy and in speed for Speech Translation and Spoken Language Understanding.","['https://openalex.org/W3097777922', 'https://openalex.org/W3163793923', 'https://openalex.org/W3015537910', 'https://openalex.org/W6784614252', 'https://openalex.org/W6776048684', 'https://openalex.org/W6739901393', 'https://openalex.org/W2531409750', 'https://openalex.org/W6838276489', 'https://openalex.org/W3197813307', 'https://openalex.org/W4372259771', 'https://openalex.org/W2127141656', 'https://openalex.org/W6638749077', 'https://openalex.org/W1494198834', 'https://openalex.org/W2963250244', 'https://openalex.org/W3095410713', 'https://openalex.org/W2024490156', 'https://openalex.org/W4285158119', 'https://openalex.org/W6803861913', 'https://openalex.org/W3092085609', 'https://openalex.org/W3209059054', 'https://openalex.org/W2995181338', 'https://openalex.org/W3100460087', 'https://openalex.org/W3217767527', 'https://openalex.org/W6803378298', 'https://openalex.org/W2799473636', 'https://openalex.org/W3197674197', 'https://openalex.org/W6780218876', 'https://openalex.org/W3211224152', 'https://openalex.org/W3093579165', 'https://openalex.org/W3101648800', 'https://openalex.org/W4286857860', 'https://openalex.org/W4311000453', 'https://openalex.org/W3015468748']",2023-12-16
https://openalex.org/W4384918448,https://doi.org/10.48550/arxiv.2307.09288,Llama 2: Open Foundation and Fine-Tuned Chat Models,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",[],2023-07-18
https://openalex.org/W3094917204,https://doi.org/10.21437/interspeech.2020-1615,Improved Prosody from Learned F0 Codebook Representations for VQ-VAE Speech Waveform Reconstruction,"Vector Quantized Variational AutoEncoders (VQ-VAE) are a powerful representation learning framework that can discover discrete groups of features from a speech signal without supervision. Until now, the VQ-VAE architecture has previously modeled individual types of speech features, such as only phones or only F0. This paper introduces an important extension to VQ-VAE for learning F0-related suprasegmental information simultaneously along with traditional phone features.The proposed framework uses two encoders such that the F0 trajectory and speech waveform are both input to the system, therefore two separate codebooks are learned. We used a WaveRNN vocoder as the decoder component of VQ-VAE. Our speaker-independent VQ-VAE was trained with raw speech waveforms from multi-speaker Japanese speech databases. Experimental results show that the proposed extension reduces F0 distortion of reconstructed speech for all unseen test speakers, and results in significantly higher preference scores from a listening test. We additionally conducted experiments using single-speaker Mandarin speech to demonstrate advantages of our architecture in another language which relies heavily on F0.",[],2020-10-25
https://openalex.org/W3037217258,https://doi.org/10.18653/v1/2020.acl-demos.34,ESPnet-ST: All-in-One Speech Translation Toolkit,"Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Yalta, Tomoki Hayashi, Shinji Watanabe. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. 2020.","['https://openalex.org/W2965198191', 'https://openalex.org/W2136530135', 'https://openalex.org/W3015703505', 'https://openalex.org/W2785350307', 'https://openalex.org/W2964308564', 'https://openalex.org/W2936969148', 'https://openalex.org/W2949382160', 'https://openalex.org/W2963779652', 'https://openalex.org/W2804704270', 'https://openalex.org/W2928941594', 'https://openalex.org/W2327501763', 'https://openalex.org/W2964243274', 'https://openalex.org/W2949328740', 'https://openalex.org/W2899274165', 'https://openalex.org/W2997436923', 'https://openalex.org/W3016160783', 'https://openalex.org/W2953190524', 'https://openalex.org/W3015338123', 'https://openalex.org/W2955541912', 'https://openalex.org/W2972448360', 'https://openalex.org/W3008549139', 'https://openalex.org/W2762715843', 'https://openalex.org/W4295312788', 'https://openalex.org/W1524333225', 'https://openalex.org/W2972818416', 'https://openalex.org/W2595715041', 'https://openalex.org/W2936848022', 'https://openalex.org/W2972780808', 'https://openalex.org/W4297747548', 'https://openalex.org/W2970730223', 'https://openalex.org/W2763421725', 'https://openalex.org/W2406343628', 'https://openalex.org/W2133564696', 'https://openalex.org/W4288400010', 'https://openalex.org/W2963212250', 'https://openalex.org/W1970987322', 'https://openalex.org/W2519091744', 'https://openalex.org/W2766219058', 'https://openalex.org/W2113106066', 'https://openalex.org/W2101105183', 'https://openalex.org/W2970971581', 'https://openalex.org/W2890785942', 'https://openalex.org/W2964172053', 'https://openalex.org/W2796108585', 'https://openalex.org/W2799923439', 'https://openalex.org/W3012492057', 'https://openalex.org/W4300558631', 'https://openalex.org/W2252212004', 'https://openalex.org/W3007142233', 'https://openalex.org/W2903739847', 'https://openalex.org/W2974231335', 'https://openalex.org/W2963979492', 'https://openalex.org/W2973048981', 'https://openalex.org/W2951418500', 'https://openalex.org/W2962784628', 'https://openalex.org/W2184135559', 'https://openalex.org/W2946200149', 'https://openalex.org/W2963240019']",2020-01-01
https://openalex.org/W2963371159,https://doi.org/10.21437/odyssey.2018-11,Exploring the Encoding Layer and Loss Function in End-to-End Speaker and Language Recognition System,"In this paper, we explore the encoding/pooling layer and loss function in the end-to-end speaker and language recognition system.First, a unified and interpretable end-to-end system for both speaker and language recognition is developed.It accepts variable-length input and produces an utterance level result.In the end-to-end system, the encoding layer plays a role in aggregating the variable-length input sequence into an utterance level representation.Besides the basic temporal average pooling, we introduce a self-attentive pooling layer and a learnable dictionary encoding layer to get the utterance level representation.In terms of loss function for open-set speaker verification, to get more discriminative speaker embedding, center loss and angular softmax loss is introduced in the end-to-end system.Experimental results on Voxceleb and NIST LRE 07 datasets show that the performance of end-to-end learning system could be significantly improved by the proposed encoding layer and loss function.","['https://openalex.org/W2185814970', 'https://openalex.org/W2396910632', 'https://openalex.org/W2612434969', 'https://openalex.org/W2078169166', 'https://openalex.org/W2056119007', 'https://openalex.org/W2587150483', 'https://openalex.org/W2746742816', 'https://openalex.org/W4234330420', 'https://openalex.org/W2138621090', 'https://openalex.org/W2963466847', 'https://openalex.org/W2041823554', 'https://openalex.org/W2121812409', 'https://openalex.org/W2766570941', 'https://openalex.org/W2165880886', 'https://openalex.org/W2104457544', 'https://openalex.org/W2963077989', 'https://openalex.org/W2150769028', 'https://openalex.org/W2290689761', 'https://openalex.org/W2795913659', 'https://openalex.org/W2962853205', 'https://openalex.org/W2144172034', 'https://openalex.org/W2293442930', 'https://openalex.org/W2406392101', 'https://openalex.org/W2726515241', 'https://openalex.org/W2007266108', 'https://openalex.org/W2154278880', 'https://openalex.org/W2194775991', 'https://openalex.org/W996208672', 'https://openalex.org/W3099206234', 'https://openalex.org/W2962959915', 'https://openalex.org/W2520774990', 'https://openalex.org/W2470673105', 'https://openalex.org/W2408021097', 'https://openalex.org/W2147147599', 'https://openalex.org/W2406312423']",2018-06-06
https://openalex.org/W2972921407,https://doi.org/10.21437/interspeech.2019-1769,Disentangling Style Factors from Speaker Representations,"Our goal is to separate out speaking style from speaker identity in utterance-level representations of speech such as ivectors and x-vectors.We first show that both i-vectors and x-vectors contain information not only about speaker but also about speaking style (for one data set) or emotion (for another data set), even when projected into a low-dimensional space.To disentangle these factors, we use an autoencoder in which the latent space is split into two subspaces.The entangled information about speaker and style/emotion is pushed apart by the use of auxiliary classifiers that take one of the two latent subspaces as input and that are jointly learned with the autoencoder.We evaluate how well the latent subspaces separate the factors by using them as input to separate style/emotion classification tasks.In traditional speaker identification tasks, speakerinvariant characteristics are factorized from channel and then the channel information is ignored.Our results suggest that this so-called channel may contain exploitable information, which we refer to as style factors.Finally, we propose future work to use information theory to formalize style factors in the context of speaker identity.","['https://openalex.org/W2146334809', 'https://openalex.org/W1548765444', 'https://openalex.org/W2726515241', 'https://openalex.org/W1572063013', 'https://openalex.org/W2901611695', 'https://openalex.org/W2953384591', 'https://openalex.org/W2395750323', 'https://openalex.org/W2808631503', 'https://openalex.org/W2150769028', 'https://openalex.org/W2807305309', 'https://openalex.org/W2963418523', 'https://openalex.org/W2405404369', 'https://openalex.org/W1522301498', 'https://openalex.org/W2404143005', 'https://openalex.org/W2962691331', 'https://openalex.org/W2890964092', 'https://openalex.org/W2886300652', 'https://openalex.org/W2748488820', 'https://openalex.org/W1524333225', 'https://openalex.org/W2154278880', 'https://openalex.org/W1547742344', 'https://openalex.org/W2794490148', 'https://openalex.org/W2885800352', 'https://openalex.org/W2121812409', 'https://openalex.org/W2004915807', 'https://openalex.org/W2888976508', 'https://openalex.org/W2766406951', 'https://openalex.org/W2747664154', 'https://openalex.org/W2794506738']",2019-09-13
https://openalex.org/W2972403660,https://doi.org/10.1109/asru46091.2019.9003979,Probing the Information Encoded in X-Vectors,"Deep neural network based speaker embeddings, such as x-vectors, have been\nshown to perform well in text-independent speaker recognition/verification\ntasks. In this paper, we use simple classifiers to investigate the contents\nencoded by x-vector embeddings. We probe these embeddings for information\nrelated to the speaker, channel, transcription (sentence, words, phones), and\nmeta information about the utterance (duration and augmentation type), and\ncompare these with the information encoded by i-vectors across a varying number\nof dimensions. We also study the effect of data augmentation during extractor\ntraining on the information captured by x-vectors. Experiments on the RedDots\ndata set show that x-vectors capture spoken content and channel-related\ninformation, while performing well on speaker verification tasks.\n","['https://openalex.org/W6686645966', 'https://openalex.org/W6712618806', 'https://openalex.org/W2972921407', 'https://openalex.org/W2964204621', 'https://openalex.org/W6755207826', 'https://openalex.org/W6631362777', 'https://openalex.org/W2808631503', 'https://openalex.org/W2290689761', 'https://openalex.org/W2587150483', 'https://openalex.org/W2046056978', 'https://openalex.org/W2890964092', 'https://openalex.org/W2748488820', 'https://openalex.org/W2748318213', 'https://openalex.org/W2079623482', 'https://openalex.org/W1528954144', 'https://openalex.org/W2150769028', 'https://openalex.org/W2039057510', 'https://openalex.org/W6631190155', 'https://openalex.org/W2515750888', 'https://openalex.org/W2696967604', 'https://openalex.org/W6732409850', 'https://openalex.org/W2121812409', 'https://openalex.org/W6605010638', 'https://openalex.org/W6604892825']",2019-12-01
https://openalex.org/W38194800,https://doi.org/10.21437/interspeech.2004-668,From WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition,"The word error rate (WER), commonly used in ASR assessment, measures the cost of restoring the output word sequence to the original input sequence. However, for most CSR applications apart from dictation machines a more meaningful performance measure would be given by the proportion of information communicated. In this article we introduce two new absolute CSR performance measures: MER (match error rate) and WIL (word information lost). MER is the proportion of I/O word matches which are errors. WIL is a simple approximation to the proportion of word information lost which overcomes the problems associated with the RIL (relative information lost) measure that was proposed half a century ago. Issues relating to ideal performance measurement are discussed and the commonly used Viterbi input/output alignment procedure, with zero weight for hits and equal weight for substitutions, deletions and insertions, is shown to be optimal.","['https://openalex.org/W1667165204', 'https://openalex.org/W4285719527', 'https://openalex.org/W1548235210', 'https://openalex.org/W1970533835', 'https://openalex.org/W2087617622', 'https://openalex.org/W2157477135']",2004-10-04
https://openalex.org/W2526425061,https://doi.org/10.1109/icassp.2017.7953075,Joint CTC-attention based end-to-end speech recognition using multi-task learning,"Recently, there has been an increasing interest in end-to-end speech recognition that directly transcribes speech to text without any predefined alignments. One approach is the attention-based encoder-decoder framework that learns a mapping between variable-length input and output sequences in one step using a purely data-driven method. The attention model has often been shown to improve the performance over another end-to-end approach, the Connectionist Temporal Classification (CTC), mainly because it explicitly uses the history of the target character without any conditional independence assumptions. However, we observed that the performance of the attention has shown poor results in noisy condition and is hard to learn in the initial training stage with long input sequences. This is because the attention model is too flexible to predict proper alignments in such cases due to the lack of left-to-right constraints as used in CTC. This paper presents a novel method for end-to-end speech recognition to improve robustness and achieve fast convergence by using a joint CTC-attention model within the multi-task learning framework, thereby mitigating the alignment issue. An experiment on the WSJ and CHiME-4 tasks demonstrates its advantages over both the CTC and attention-based encoder-decoder baselines, showing 5.4-14.6% relative improvements in Character Error Rate (CER).","['https://openalex.org/W1993882792', 'https://openalex.org/W2160815625', 'https://openalex.org/W2127141656', 'https://openalex.org/W2559260703', 'https://openalex.org/W2064675550', 'https://openalex.org/W2005708641', 'https://openalex.org/W2963211739', 'https://openalex.org/W6623517193', 'https://openalex.org/W2608712415', 'https://openalex.org/W6675365184', 'https://openalex.org/W2514969556', 'https://openalex.org/W6679436768', 'https://openalex.org/W2130942839', 'https://openalex.org/W1586532344', 'https://openalex.org/W2962826786', 'https://openalex.org/W6908809', 'https://openalex.org/W854541894', 'https://openalex.org/W1922655562', 'https://openalex.org/W1815076433', 'https://openalex.org/W1855892484', 'https://openalex.org/W2133564696', 'https://openalex.org/W2102113734']",2017-03-01
https://openalex.org/W3030987249,,Adversarial Contrastive Predictive Coding for Unsupervised Learning of Disentangled Representations.,"In this work we tackle disentanglement of speaker and content related variations in speech signals. We propose a fully convolutional variational autoencoder employing two encoders: a content encoder and a speaker encoder. To foster disentanglement we propose adversarial contrastive predictive coding. This new disentanglement method does neither need parallel data nor any supervision, not even speaker labels. With successful disentanglement the model is able to perform voice conversion by recombining content and speaker attributes. Due to the speaker encoder which learns to extract speaker traits from an audio signal, the proposed model not only provides meaningful speaker embeddings but is also able to perform zero-shot voice conversion, i.e. with previously unseen source and target speakers. Compared to state-of-the-art disentanglement approaches we show competitive disentanglement and voice conversion performance for speakers seen during training and superior performance for unseen speakers.","['https://openalex.org/W1522301498', 'https://openalex.org/W3020570669', 'https://openalex.org/W2753738274', 'https://openalex.org/W2972812066', 'https://openalex.org/W2963830550', 'https://openalex.org/W2842511635', 'https://openalex.org/W2785519580', 'https://openalex.org/W1959608418', 'https://openalex.org/W2951697117', 'https://openalex.org/W2964127395', 'https://openalex.org/W2966212213', 'https://openalex.org/W2099621636', 'https://openalex.org/W2939131199', 'https://openalex.org/W2799121188', 'https://openalex.org/W2502312327', 'https://openalex.org/W2972659941', 'https://openalex.org/W1494198834', 'https://openalex.org/W2946555236', 'https://openalex.org/W2774848319', 'https://openalex.org/W3003799062', 'https://openalex.org/W2963539064', 'https://openalex.org/W2972667718', 'https://openalex.org/W2945478979', 'https://openalex.org/W2951298482']",2020-05-26
https://openalex.org/W1577418252,https://doi.org/10.1109/icassp.2015.7179089,Segmental acoustic indexing for zero resource keyword search,"The task of zero resource query-by-example keyword search has received much attention in recent years as the speech technology needs of the developing world grow. These systems traditionally rely upon dynamic time warping (DTW) based retrieval algorithms with runtimes that are linear in the size of the search collection. As a result, their scalability substantially lags that of their supervised counterparts, which take advantage of efficient word-based indices. In this paper, we present a novel audio indexing approach called Segmental Randomized Acoustic Indexing and Logarithmic-time Search (S-RAILS). S-RAILS generalizes the original frame-based RAILS methodology to word-scale segments by exploiting a recently proposed acoustic segment embedding technique. By indexing word-scale segments directly, we avoid higher cost frame-based processing of RAILS while taking advantage of the improved lexical discrimination of the embeddings. Using the same conversational telephone speech benchmark, we demonstrate major improvements in both speed and accuracy over the original RAILS system.","['https://openalex.org/W1974745151', 'https://openalex.org/W2166778951', 'https://openalex.org/W2293210169', 'https://openalex.org/W2407964052', 'https://openalex.org/W6714100551', 'https://openalex.org/W2147717514', 'https://openalex.org/W2097308346', 'https://openalex.org/W2057007397', 'https://openalex.org/W6884732673', 'https://openalex.org/W2025482506', 'https://openalex.org/W2110625382', 'https://openalex.org/W2034940213', 'https://openalex.org/W2114347655', 'https://openalex.org/W2074932712', 'https://openalex.org/W2055408826', 'https://openalex.org/W6604666349', 'https://openalex.org/W6601311673', 'https://openalex.org/W2104290444', 'https://openalex.org/W2059652594', 'https://openalex.org/W30845872', 'https://openalex.org/W1490960657', 'https://openalex.org/W114193738', 'https://openalex.org/W2407151108']",2015-04-01
https://openalex.org/W2476548250,https://doi.org/10.1109/cvpr.2016.207,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,"Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.","['https://openalex.org/W2150081556', 'https://openalex.org/W2044451987', 'https://openalex.org/W6605475217', 'https://openalex.org/W1979606177', 'https://openalex.org/W1950594372', 'https://openalex.org/W6677651945', 'https://openalex.org/W2109586214', 'https://openalex.org/W2097117768', 'https://openalex.org/W2111454493', 'https://openalex.org/W2112024783', 'https://openalex.org/W4136762', 'https://openalex.org/W2172174689', 'https://openalex.org/W2118963448', 'https://openalex.org/W6638300599', 'https://openalex.org/W2129276048', 'https://openalex.org/W2154579312', 'https://openalex.org/W6684191040', 'https://openalex.org/W1981990039', 'https://openalex.org/W2121927366', 'https://openalex.org/W6680935700', 'https://openalex.org/W1791560514', 'https://openalex.org/W2087436818', 'https://openalex.org/W2016482162', 'https://openalex.org/W6683781035', 'https://openalex.org/W4235713725', 'https://openalex.org/W2165939075', 'https://openalex.org/W6624640001', 'https://openalex.org/W2103047572', 'https://openalex.org/W2103844245', 'https://openalex.org/W2534320940', 'https://openalex.org/W2058182006', 'https://openalex.org/W6677645113', 'https://openalex.org/W2126653386', 'https://openalex.org/W1977581467', 'https://openalex.org/W1983364832', 'https://openalex.org/W6605397647', 'https://openalex.org/W6602211262', 'https://openalex.org/W1949096787', 'https://openalex.org/W2117865218', 'https://openalex.org/W6639309558', 'https://openalex.org/W1849277567', 'https://openalex.org/W2146782367', 'https://openalex.org/W2164551808', 'https://openalex.org/W6600294690', 'https://openalex.org/W6680022851', 'https://openalex.org/W2121058967', 'https://openalex.org/W2164598857', 'https://openalex.org/W1522734439', 'https://openalex.org/W6678025836', 'https://openalex.org/W2599944722', 'https://openalex.org/W2118103795', 'https://openalex.org/W1906770428', 'https://openalex.org/W1885185971', 'https://openalex.org/W2141200610', 'https://openalex.org/W2250093075', 'https://openalex.org/W2117539524', 'https://openalex.org/W1686810756', 'https://openalex.org/W3104720471', 'https://openalex.org/W2120824855', 'https://openalex.org/W2160558632', 'https://openalex.org/W1903029394', 'https://openalex.org/W7682646', 'https://openalex.org/W935139217', 'https://openalex.org/W18046889', 'https://openalex.org/W2163605009', 'https://openalex.org/W134193804', 'https://openalex.org/W1811400895', 'https://openalex.org/W54257720', 'https://openalex.org/W135113724', 'https://openalex.org/W1591116419', 'https://openalex.org/W2134672310']",2016-06-01
https://openalex.org/W1967924372,https://doi.org/10.1109/icassp.2013.6639241,Weak top-down constraints for unsupervised acoustic model training,"Typical supervised acoustic model training relies on strong top-down constraints provided by dynamic programming alignment of the input observations to phonetic sequences derived from orthographic word transcripts and pronunciation dictionaries. This paper investigates a much weaker form of top-down supervision for use in place of transcripts and dictionaries in the zero resource setting. Our proposed constraints, which can be produced using recent spoken term discovery systems, come in the form of pairs of isolated word examples that share the same unknown type. For each pair, we perform a dynamic programming alignment of the acoustic observations of the two constituent examples, generating an inventory of cross-speaker frame pairs that each provide evidence that the same subword unit model should account for them. We find these weak top-down constraints are capable of improving model speaker independence by up to 57% relative over bottom-up training alone.","['https://openalex.org/W2128160875', 'https://openalex.org/W156237177', 'https://openalex.org/W2401464865', 'https://openalex.org/W6714100551', 'https://openalex.org/W6602705600', 'https://openalex.org/W2406820985', 'https://openalex.org/W2114478143', 'https://openalex.org/W6675022971', 'https://openalex.org/W2117041980', 'https://openalex.org/W2399869768', 'https://openalex.org/W3148201686', 'https://openalex.org/W2103933358', 'https://openalex.org/W2090861223', 'https://openalex.org/W1539935047', 'https://openalex.org/W2121947440', 'https://openalex.org/W1606268232', 'https://openalex.org/W2114347655', 'https://openalex.org/W2062914951', 'https://openalex.org/W2126203737', 'https://openalex.org/W2057007397', 'https://openalex.org/W30845872', 'https://openalex.org/W2079460648', 'https://openalex.org/W2407964052', 'https://openalex.org/W2100768664', 'https://openalex.org/W2407151108', 'https://openalex.org/W66167291']",2013-05-01
https://openalex.org/W2890983311,https://doi.org/10.1109/icassp.2018.8462431,Fftnet: A Real-Time Speaker-Dependent Neural Vocoder,"We introduce FFTNet, a deep learning approach synthesizing audio waveforms. Our approach builds on the recent WaveNet project, which showed that it was possible to synthesize a natural sounding audio waveform directly from a deep convolutional neural network. FFTNet offers two improvements over WaveNet. First it is substantially faster, allowing for real-time synthesis of audio waveforms. Second, when used as a vocoder, the resulting speech sounds more natural, as measured via a ""mean opinion score"" test.","['https://openalex.org/W2328165521', 'https://openalex.org/W6792025129', 'https://openalex.org/W2145892079', 'https://openalex.org/W2769810959', 'https://openalex.org/W6950332740', 'https://openalex.org/W2061171222', 'https://openalex.org/W2423557781', 'https://openalex.org/W4253928870', 'https://openalex.org/W6756197946', 'https://openalex.org/W2251498782', 'https://openalex.org/W6736356763', 'https://openalex.org/W2746474733', 'https://openalex.org/W6740049204', 'https://openalex.org/W1510007267', 'https://openalex.org/W2749651610', 'https://openalex.org/W6603838645', 'https://openalex.org/W2141708418', 'https://openalex.org/W2737697117', 'https://openalex.org/W4230964063', 'https://openalex.org/W2395718496', 'https://openalex.org/W6712900185', 'https://openalex.org/W2604184139', 'https://openalex.org/W573259127', 'https://openalex.org/W2901997113', 'https://openalex.org/W1495679096', 'https://openalex.org/W2402356521', 'https://openalex.org/W95152782', 'https://openalex.org/W3142087749', 'https://openalex.org/W2606176153', 'https://openalex.org/W4298642009', 'https://openalex.org/W1522301498', 'https://openalex.org/W2591927543', 'https://openalex.org/W4294619240', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963103134']",2018-04-01
https://openalex.org/W2945769669,https://doi.org/10.18653/v1/n19-1007,Measuring the perceptual availability of phonological features during language acquisition using unsupervised binary stochastic autoencoders,"Cory Shain, Micha Elsner. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.","['https://openalex.org/W2964169922', 'https://openalex.org/W1966264494', 'https://openalex.org/W1976587578', 'https://openalex.org/W2758697525', 'https://openalex.org/W2011238950', 'https://openalex.org/W2402366697', 'https://openalex.org/W2052384514', 'https://openalex.org/W2059450962', 'https://openalex.org/W2145410271', 'https://openalex.org/W2964121744', 'https://openalex.org/W2407614114', 'https://openalex.org/W2044138293', 'https://openalex.org/W2963620343', 'https://openalex.org/W2250874882', 'https://openalex.org/W2271840356', 'https://openalex.org/W2106048795', 'https://openalex.org/W2242818861', 'https://openalex.org/W1576255537', 'https://openalex.org/W1836465849', 'https://openalex.org/W2314715977', 'https://openalex.org/W157872538', 'https://openalex.org/W2120713167', 'https://openalex.org/W1519956846', 'https://openalex.org/W2577416417', 'https://openalex.org/W2077938266', 'https://openalex.org/W2136653392', 'https://openalex.org/W2779379590', 'https://openalex.org/W2785860501', 'https://openalex.org/W2083053184', 'https://openalex.org/W2010800472', 'https://openalex.org/W1545920196', 'https://openalex.org/W2095517176', 'https://openalex.org/W2152134037', 'https://openalex.org/W2161025448', 'https://openalex.org/W2468716020', 'https://openalex.org/W2119536496', 'https://openalex.org/W1966072864', 'https://openalex.org/W2101509422', 'https://openalex.org/W2963735467', 'https://openalex.org/W2252956398', 'https://openalex.org/W2087985520', 'https://openalex.org/W2087384787', 'https://openalex.org/W1549321558', 'https://openalex.org/W1995422333', 'https://openalex.org/W2400668844', 'https://openalex.org/W1984689963', 'https://openalex.org/W2016090750', 'https://openalex.org/W2140661818', 'https://openalex.org/W2050864369', 'https://openalex.org/W2106125881', 'https://openalex.org/W2077382402', 'https://openalex.org/W2100768664', 'https://openalex.org/W2251846372', 'https://openalex.org/W2980286501', 'https://openalex.org/W1990351858', 'https://openalex.org/W2171752983', 'https://openalex.org/W2320209943', 'https://openalex.org/W2160464066', 'https://openalex.org/W2787447541', 'https://openalex.org/W2613332842', 'https://openalex.org/W2332494297', 'https://openalex.org/W2005741747', 'https://openalex.org/W52412328', 'https://openalex.org/W2250527913', 'https://openalex.org/W2152824855', 'https://openalex.org/W2117041980', 'https://openalex.org/W2184341689', 'https://openalex.org/W2146163948', 'https://openalex.org/W2136756756', 'https://openalex.org/W1977531436', 'https://openalex.org/W2307400721', 'https://openalex.org/W2785415724', 'https://openalex.org/W2768381684', 'https://openalex.org/W2086841290', 'https://openalex.org/W2786902352', 'https://openalex.org/W2111312122', 'https://openalex.org/W2964308564', 'https://openalex.org/W2919115771', 'https://openalex.org/W2168266939', 'https://openalex.org/W2008775952', 'https://openalex.org/W1796128977', 'https://openalex.org/W2066908170', 'https://openalex.org/W2063832546', 'https://openalex.org/W2398490608', 'https://openalex.org/W2153767712', 'https://openalex.org/W1970688873', 'https://openalex.org/W2143022183', 'https://openalex.org/W1937295144', 'https://openalex.org/W2058455850', 'https://openalex.org/W2345811097', 'https://openalex.org/W1986174057', 'https://openalex.org/W2194775991', 'https://openalex.org/W1976977765', 'https://openalex.org/W1598851216', 'https://openalex.org/W2319920447', 'https://openalex.org/W2786608204', 'https://openalex.org/W2787223168', 'https://openalex.org/W2804446378', 'https://openalex.org/W1967177300', 'https://openalex.org/W2138615112', 'https://openalex.org/W2339016247', 'https://openalex.org/W2399576818', 'https://openalex.org/W2401464865', 'https://openalex.org/W2522012644', 'https://openalex.org/W2039201033', 'https://openalex.org/W2184045248', 'https://openalex.org/W2090861223', 'https://openalex.org/W1993755070', 'https://openalex.org/W2121997342', 'https://openalex.org/W2094818253', 'https://openalex.org/W2040870580', 'https://openalex.org/W2400549570', 'https://openalex.org/W2035126305', 'https://openalex.org/W2011145053', 'https://openalex.org/W2156447271', 'https://openalex.org/W2160017196', 'https://openalex.org/W2787426069']",2019-01-01
https://openalex.org/W2548228487,https://doi.org/10.48550/arxiv.1611.00712,The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables,"The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.",[],2016-11-02
https://openalex.org/W2963619462,,Techniques for Learning Binary Stochastic Feedforward Neural Networks,"Abstract: Stochastic binary hidden units in a multi-layer perceptron (MLP) network give at least three potential benefits when compared to deterministic MLP networks. (1) They allow to learn one-to-many type of mappings. (2) They can be used in structured prediction problems, where modeling the internal structure of the output is important. (3) Stochasticity has been shown to be an excellent regularizer, which makes generalization performance potentially better in general. However, training stochastic networks is considerably more difficult. We study training using M samples of hidden activations per input. We show that the case M=1 leads to a fundamentally different behavior where the network tries to avoid stochasticity. We propose two new estimators for the training gradient and propose benchmark tests for comparing training algorithms. Our experiments confirm that training stochastic networks is difficult and show that the proposed two estimators perform favorably among all the five known estimators.",[],2015-05-07
https://openalex.org/W2826003142,https://doi.org/10.1109/icassp.2018.8461648,Speaker Invariant Feature Extraction for Zero-Resource Languages with Adversarial Learning,"We introduce a novel type of representation learning to obtain a speaker invariant feature for zero-resource languages. Speaker adaptation is an important technique to build a robust acoustic model. For a zero-resource language, however, conventional model-dependent speaker adaptation methods such as constrained maximum likelihood linear regression are insufficient because the acoustic model of the target language is not accessible. Therefore, we introduce a model-independent feature extraction based on a neural network. Specifically, we introduce a multi-task learning to a bottleneck feature-based approach to make bottleneck feature invariant to a change of speakers. The proposed network simultaneously tackles two tasks: phoneme and speaker classifications. This network trains a feature extractor in an adversarial manner to allow it to map input data into a discriminative representation to predict phonemes, whereas it is difficult to predict speakers. We conduct phone discriminant experiments in Zero Resource Speech Challenge 2017. Experimental results showed that our multi-task network yielded more discriminative features eliminating the variety in speakers.","['https://openalex.org/W6638159135', 'https://openalex.org/W6640425456', 'https://openalex.org/W6637618735', 'https://openalex.org/W2510867321', 'https://openalex.org/W6605273041', 'https://openalex.org/W6686687092', 'https://openalex.org/W2078169166', 'https://openalex.org/W6777926273', 'https://openalex.org/W2119187236', 'https://openalex.org/W6675022971', 'https://openalex.org/W6712553779', 'https://openalex.org/W6602705600', 'https://openalex.org/W6712202099', 'https://openalex.org/W2404799143', 'https://openalex.org/W2126203737', 'https://openalex.org/W2002342963', 'https://openalex.org/W6661255737', 'https://openalex.org/W6640036494', 'https://openalex.org/W6712444837', 'https://openalex.org/W6638667902', 'https://openalex.org/W6725564769', 'https://openalex.org/W2730658205', 'https://openalex.org/W2150769028', 'https://openalex.org/W2064675550', 'https://openalex.org/W66167291', 'https://openalex.org/W1836465849', 'https://openalex.org/W128628490', 'https://openalex.org/W2396043527', 'https://openalex.org/W2509930204', 'https://openalex.org/W2100768664', 'https://openalex.org/W2963207607', 'https://openalex.org/W1796128977', 'https://openalex.org/W2043878967', 'https://openalex.org/W1945616565', 'https://openalex.org/W2399576818', 'https://openalex.org/W2949117887', 'https://openalex.org/W2185814970', 'https://openalex.org/W3028642772', 'https://openalex.org/W1731081199', 'https://openalex.org/W2395899413', 'https://openalex.org/W1904365287']",2018-04-01
https://openalex.org/W2148154194,https://doi.org/10.1109/tassp.1980.1163420,Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences,"Several parametric representations of the acoustic signal were compared with regard to word recognition performance in a syllable-oriented continuous speech recognition system. The vocabulary included many phonetically similar monosyllabic words, therefore the emphasis was on the ability to retain phonetically significant acoustic information in the face of syntactic and duration variations. For each parameter set (based on a mel-frequency cepstrum, a linear frequency cepstrum, a linear prediction cepstrum, a linear prediction spectrum, or a set of reflection coefficients), word templates were generated using an efficient dynamic warping method, and test data were time registered with the templates. A set of ten mel-frequency cepstrum coefficients computed every 6.4 ms resulted in the best performance, namely 96.5 percent and 95.0 percent recognition with each of two speakers. The superior performance of the mel-frequency cepstrum coefficients may be attributed to the fact that they better represent the perceptually relevant aspects of the short-term speech spectrum.","['https://openalex.org/W2125894463', 'https://openalex.org/W1965635292', 'https://openalex.org/W1667891623', 'https://openalex.org/W2022554507', 'https://openalex.org/W2013139082', 'https://openalex.org/W1989337816', 'https://openalex.org/W2072054026', 'https://openalex.org/W1748609388', 'https://openalex.org/W1986092967', 'https://openalex.org/W2137089646', 'https://openalex.org/W1976882641', 'https://openalex.org/W2009608672', 'https://openalex.org/W2104981903', 'https://openalex.org/W2031780801', 'https://openalex.org/W2128160875', 'https://openalex.org/W1597330042', 'https://openalex.org/W4285259144', 'https://openalex.org/W1606925698', 'https://openalex.org/W1505035441', 'https://openalex.org/W2980286501']",1980-08-01
https://openalex.org/W2035424729,https://doi.org/10.1109/icassp.2013.6638312,On rectified linear units for speech processing,"Deep neural networks have recently become the gold standard for acoustic modeling in speech recognition systems. The key computational unit of a deep network is a linear projection followed by a point-wise non-linearity, which is typically a logistic function. In this work, we show that we can improve generalization and make training of deep networks faster and simpler by substituting the logistic units with rectified linear units. These units are linear when their input is positive and zero otherwise. In a supervised setting, we can successfully train very deep nets from random initialization on a large vocabulary speech recognition task achieving lower word error rates than using a logistic network with the same topology. Similarly in an unsupervised setting, we show how we can learn sparse features that can be useful for discriminative tasks. All our experiments are executed in a distributed environment using several hundred machines and several hundred hours of speech data.","['https://openalex.org/W2296748324', 'https://openalex.org/W6677645113', 'https://openalex.org/W2100543212', 'https://openalex.org/W2105153012', 'https://openalex.org/W2116064496', 'https://openalex.org/W6684859321', 'https://openalex.org/W6637242042', 'https://openalex.org/W1540578583', 'https://openalex.org/W2403195671', 'https://openalex.org/W2105099419', 'https://openalex.org/W6684191040', 'https://openalex.org/W6682889407', 'https://openalex.org/W2405601855', 'https://openalex.org/W1665214252', 'https://openalex.org/W4293652559', 'https://openalex.org/W2163605009', 'https://openalex.org/W2246822104', 'https://openalex.org/W2156387975', 'https://openalex.org/W2618530766', 'https://openalex.org/W2118103795', 'https://openalex.org/W2171490498', 'https://openalex.org/W2168231600']",2013-05-01
https://openalex.org/W2020607164,https://doi.org/10.1109/icassp.2014.6855085,An auto-encoder based approach to unsupervised learning of subword units,In this paper we propose an autoencoder-based method for the unsupervised identification of subword units. We experiment with different types and architectures of autoencoders to asses what autoencoder properties are most important for this task. We first show that the encoded representation of speech produced by standard autencoders is more effective than Gaussian posteriorgrams in a spoken query classification task. Finally we evaluate the subword inventories produced by the proposed method both in terms of classification accuracy in a word classification task (with lexicon size up to 263 words) and in terms of consistency between subword transcription of different word examples of a same word type. The evaluation is carried out on Italian and American English datasets. © 2014 IEEE.,"['https://openalex.org/W6789826613', 'https://openalex.org/W2100495367', 'https://openalex.org/W7011707065', 'https://openalex.org/W6676071220', 'https://openalex.org/W2035424729', 'https://openalex.org/W2117041980', 'https://openalex.org/W2123237149', 'https://openalex.org/W6677919164', 'https://openalex.org/W6675022971', 'https://openalex.org/W2167655920', 'https://openalex.org/W1964917299', 'https://openalex.org/W6681096077', 'https://openalex.org/W2099415988', 'https://openalex.org/W2401464865', 'https://openalex.org/W2107789863', 'https://openalex.org/W2100768664', 'https://openalex.org/W2619993508', 'https://openalex.org/W3127686677', 'https://openalex.org/W2145094598', 'https://openalex.org/W2118858186', 'https://openalex.org/W2072128103', 'https://openalex.org/W2997574889', 'https://openalex.org/W2010800472']",2014-05-01
https://openalex.org/W52412328,,Learning phonetic categories by learning a lexicon,"Learning Phonetic Categories by Learning a Lexicon Naomi H. Feldman (naomi feldman@brown.edu) Department of Cognitive and Linguistic Sciences, Brown University, Providence, RI 02912 USA Thomas L. Griffiths (tom griffiths@berkeley.edu) Department of Psychology, University of California at Berkeley, Berkeley, CA 94720 USA James L. Morgan (james morgan@brown.edu) Department of Cognitive and Linguistic Sciences, Brown University, Providence, RI 02912 USA Abstract Infants learn to segment words from fluent speech during the same period as they learn native language phonetic cate- gories, yet accounts of phonetic category acquisition typically ignore information about the words in which speech sounds appear. We use a Bayesian model to illustrate how feed- back from segmented words might constrain phonetic category learning, helping a learner disambiguate overlapping phonetic categories. Simulations show that information from an artifi- cial lexicon can successfully disambiguate English vowel cat- egories, leading to more robust category learning than distri- butional information alone. Keywords: language acquisition; Bayesian inference phonetic categories; Infants learning their native language need to extract sev- eral levels of structure, including the locations of phonetic categories in perceptual space and the identities of words they segment from fluent speech. It is often implicitly as- sumed that these steps occur sequentially, with infants first learning about the phonetic categories in their language and subsequently using those categories to help them map word tokens onto lexical items. However, infants begin to segment words from fluent speech as early as 6 months (Bortfeld, Mor- gan, Golinkoff, & Rathbun, 2005) and this skill continues to develop over the next several months (Jusczyk & Aslin, 1995; Jusczyk, Houston, & Newsome, 1999). Discrimina- tion of non-native speech sound contrasts declines during the same time period, between 6 and 12 months (Werker & Tees, 1984). This suggests an alternative learning trajectory in which infants simultaneously learn to categorize both speech sounds and words, potentially allowing the two learning pro- cesses to interact. In this paper we explore the hypothesis that the words in- fants segment from fluent speech can provide a useful source of information for phonetic category acquisition. We use a Bayesian approach to explore the nature of the phonetic cat- egory learning problem in an interactive system, where infor- mation from segmented words can feed back and constrain phonetic category learning. Our interactive model learns a rudimentary lexicon and a phoneme inventory 1 simulta- neously, deciding whether acoustic representations of seg- mented tokens correspond to the same or different lexical items (e.g. bed vs. bad) and whether lexical items contain 1 We make the simplifying assumption that phonemes are equiv- alent to phonetic categories, and use the terms interchangeably. the same or different vowels (e.g. send vs. act). Simulations demonstrate that using information from segmented words to constrain phonetic category acquisition allows more robust category learning from fewer data points, due to the inter- active learner’s ability to use information about which words contain particular speech sounds to disambiguate overlapping categories. The paper is organized as follows. We begin with an intro- duction to the mathematical framework for our model, then present toy simulations to demonstrate its qualitative proper- ties. Next, simulations show that information from an artifi- cial lexicon can disambiguate formant values associated with English vowel categories. The last section discusses potential implications for language acquisition, revisits the model’s as- sumptions, and suggests directions for future research. Bayesian Model of Phonetic Category Learning Recent research on phonetic category acquisition has focused on the importance of distributional learning. Maye, Werker, and Gerken (2002) found that the specific frequency dis- tribution (bimodal or unimodal) of speech sounds along a continuum could affect infants’ discrimination of the contin- uum endpoints, with infants showing better discrimination of the endpoints when familiarized with the bimodal distribu- tion. This work has inspired computational models that use a Mixture of Gaussians approach, assuming that phonetic cate- gories are represented as Gaussian, or normal, distributions of speech sounds and that learners find the set of Gaussian cat- egories that best represents the distribution of speech sounds they hear. Boer and Kuhl (2003) used the Expectation Max- imization (EM) algorithm (Dempster, Laird, & Rubin, 1977) to learn the locations of three such vowel categories from for- mant data. McMurray, Aslin, and Toscano (2009) introduced a gradient descent algorithm similar to EM to learn a stop consonant voicing contrast, and this algorithm has been ex- tended to multiple dimensions for both consonant and vowel data (Toscano & McMurray, 2008; Vallabha, McClelland, Pons, Werker, & Amano, 2007). Our model adopts the Mixture of Gaussians approach from these previous models but uses a non-parametric Bayesian framework that allows extension of the model to the word level, making it possible to investigate the learning outcome when multiple levels of structure interact. As in previous models, speech sounds in our model are represented using","['https://openalex.org/W2038056950', 'https://openalex.org/W2159399018', 'https://openalex.org/W1967307281', 'https://openalex.org/W2157427027', 'https://openalex.org/W2070696251', 'https://openalex.org/W2153767712', 'https://openalex.org/W2080972498', 'https://openalex.org/W2059824090', 'https://openalex.org/W2169991335', 'https://openalex.org/W2769505959', 'https://openalex.org/W2117126688', 'https://openalex.org/W2101509422', 'https://openalex.org/W2045656233', 'https://openalex.org/W2020999234', 'https://openalex.org/W2158266063', 'https://openalex.org/W2104752510', 'https://openalex.org/W2049633694', 'https://openalex.org/W2069429561', 'https://openalex.org/W1978567282', 'https://openalex.org/W2095458199']",2009-01-01
https://openalex.org/W1796128977,https://doi.org/10.21437/interspeech.2015-644,A comparison of neural network methods for unsupervised representation learning on the zero resource speech challenge,"The success of supervised deep neural networks (DNNs) in speech recognition cannot be transferred to zero-resource languages where the requisite transcriptions are unavailable. We investigate unsupervised neural network based methods for learning frame-level representations. Good frame representations eliminate differences in accent, gender, channel characteristics, and other factors to model subword units for withinand acrossspeaker phonetic discrimination. We enhance the correspondence autoencoder (cAE) and show that it can transform Mel Frequency Cepstral Coefficients (MFCCs) into more effective frame representations given a set of matched word pairs from an unsupervised term discovery (UTD) system. The cAE combines the feature extraction power of autoencoders with the weak supervision signal from UTD pairs to better approximate the extrinsic task’s objective during training. We use the Zero Resource Speech Challenge’s minimal triphone pair ABX discrimination task to evaluate our methods. Optimizing a cAE architecture on English and applying it to a zero-resource language, Xitsonga, we obtain a relative error rate reduction of 35% compared to the original MFCCs. We also show that Xitsonga frame representations extracted from the bottleneck layer of a supervised DNN trained on English can be further enhanced by the cAE, yielding a relative error rate reduction of 39%.","['https://openalex.org/W2044138293', 'https://openalex.org/W2107878631', 'https://openalex.org/W2112688413', 'https://openalex.org/W1606347560', 'https://openalex.org/W2002318609', 'https://openalex.org/W2786608204', 'https://openalex.org/W2057007397', 'https://openalex.org/W2026369565', 'https://openalex.org/W66167291', 'https://openalex.org/W2146502635', 'https://openalex.org/W2100495367', 'https://openalex.org/W2395899413', 'https://openalex.org/W2017257315', 'https://openalex.org/W2145094598', 'https://openalex.org/W2148154194', 'https://openalex.org/W2407151108', 'https://openalex.org/W2052697931', 'https://openalex.org/W3028642772', 'https://openalex.org/W2145410271', 'https://openalex.org/W2020607164', 'https://openalex.org/W4285719527', 'https://openalex.org/W2025768430', 'https://openalex.org/W1524333225', 'https://openalex.org/W1545920196', 'https://openalex.org/W2406349064']",2015-09-06
https://openalex.org/W2067709094,https://doi.org/10.1109/apsipa.2013.6694316,The NUS sung and spoken lyrics corpus: A quantitative comparison of singing and speech,"Despite a long-standing effort to characterize various aspects of the singing voice and their relations to speech, the lack of a suitable and publicly available dataset has precluded any systematic study on the quantitative difference between singing and speech at the phone level. We hereby present the NUS Sung and Spoken Lyrics Corpus (NUS-48E corpus) as the first step toward a large, phonetically annotated corpus for singing voice research. The corpus is a 169-min collection of audio recordings of the sung and spoken lyrics of 48 (20 unique) English songs by 12 subjects and a complete set of transcriptions and duration annotations at the phone level for all recordings of sung lyrics, comprising 25,474 phone instances. Using the NUS-48E corpus, we conducted a preliminary, quantitative study on the comparison between singing voice and speech. The study includes duration analyses of the sung and spoken lyrics, with a primary focus on the behavior of consonants, and experiments aiming to gauge how acoustic representations of spoken and sung phonemes differ, as well as how duration and pitch variations may affect the Mel Frequency Cepstral Coefficients (MFCC) features.","['https://openalex.org/W6736711317', 'https://openalex.org/W6631117800', 'https://openalex.org/W2128123288', 'https://openalex.org/W6638563753', 'https://openalex.org/W2141732083', 'https://openalex.org/W2053089284', 'https://openalex.org/W2028190691', 'https://openalex.org/W6604400664', 'https://openalex.org/W6678691557', 'https://openalex.org/W2124539664', 'https://openalex.org/W6634989905', 'https://openalex.org/W2057745663', 'https://openalex.org/W6607540946', 'https://openalex.org/W1519655822', 'https://openalex.org/W1585181552', 'https://openalex.org/W1581879603', 'https://openalex.org/W107200947', 'https://openalex.org/W2606502962', 'https://openalex.org/W2123368707', 'https://openalex.org/W2150933458', 'https://openalex.org/W188353524', 'https://openalex.org/W1833279920']",2013-10-01
https://openalex.org/W2030535888,https://doi.org/10.1109/icassp.1992.226080,An analysis/synthesis approach to real-time artificial reverberation,"A general approach is proposed to the problem of realizing a recursive digital display network capable of simulating in real time the perceptively relevant characteristics of the reverberation decay in a room. The analysis/synthesis method presented makes it possible to imitate the late reverberation of a given room by optimizing some of the reverberant filter's parameters. The analysis phase is based on a time-frequency representation of the energy decay, computed from an impulse response measured in the room. The energy decay relief is proposed as a spectral development of the integrated energy decay curve introduced by Schroeder. Its three-dimensional representation allows perceptively relevant visual comparison of two room responses (measured or artificial) and accurate calculation of some widely used objective criteria of room acoustic quality.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>","['https://openalex.org/W2043551274', 'https://openalex.org/W6629079164', 'https://openalex.org/W2111307957', 'https://openalex.org/W6630773536', 'https://openalex.org/W2086286498', 'https://openalex.org/W2165878107', 'https://openalex.org/W2031922654', 'https://openalex.org/W2009687430', 'https://openalex.org/W2150590350', 'https://openalex.org/W2330821578', 'https://openalex.org/W2021421911', 'https://openalex.org/W6632887475', 'https://openalex.org/W6600885096', 'https://openalex.org/W6631345268', 'https://openalex.org/W2059686565', 'https://openalex.org/W2056869875', 'https://openalex.org/W2005228249', 'https://openalex.org/W1485114642', 'https://openalex.org/W21456454', 'https://openalex.org/W1595766596', 'https://openalex.org/W1523796630', 'https://openalex.org/W1513561724', 'https://openalex.org/W1549716660']",1992-01-01
https://openalex.org/W4307977385,https://doi.org/10.17743/jaes.2022.0027,"A Multi-Angle, Multi-Distance Dataset of Microphone Impulse Responses","A new publicly available dataset of microphone impulse responses (IRs) has been generated.The dataset covers 25 microphones, including a Class-1 measurement microphone and polar pattern variations for seven of the microphones.Microphones that were included had omnidirectional, cardioid, supercardioid, and bidirectional polar patterns; condenser, movingcoil, and ribbon transduction types; single and dual diaphragms; multiple body and head basket shapes; small and large diaphragms; and end-address and side-address designs.Using a customdeveloped computer-controlled precision turntable, IRs were captured quasi-anechoically at incident angles from 0 • to 355 • in steps of 5 • and at source-to-microphone distances of 0.5, 1.25, and 5 m.The resulting dataset is suitable for perceptual and objective studies related to the incident-angle-dependent response of microphones and for the development of tools for predicting and emulating on-axis and off-axis microphone characteristics.The captured IRs allow generation of frequency response plots with a degree of detail not commonly available in manufacturer-supplied data sheets and are also particularly well-suited to harmonic distortion analysis.","['https://openalex.org/W2890853946', 'https://openalex.org/W1603407393', 'https://openalex.org/W1599748913', 'https://openalex.org/W1585589879', 'https://openalex.org/W2615887888', 'https://openalex.org/W2395523453', 'https://openalex.org/W2954609927', 'https://openalex.org/W1607316092', 'https://openalex.org/W1492011465', 'https://openalex.org/W4297967964', 'https://openalex.org/W1569827162', 'https://openalex.org/W1513603309', 'https://openalex.org/W2255859285', 'https://openalex.org/W4299354882', 'https://openalex.org/W2942628286']",2022-11-02
https://openalex.org/W2136682440,https://doi.org/10.1109/icdsp.2009.5201259,A binaural room impulse response database for the evaluation of dereverberation algorithms,"This paper describes a new database of binaural room impulse responses (BRIR), referred to as the Aachen impulse response (AIR) database. The main field of application of this database is the evaluation of speech enhancement algorithms dealing with room reverberation. The measurements with a dummy head took place in a low-reverberant studio booth, an office room, a meeting room and a lecture room. Due to the different dimensions and acoustic properties, it covers a wide range of situations where digital hearing aids or other hands-free devices can be used. Besides the description of the database, a motivation for using binaural instead of monaural measurements is given. Furthermore an example using a coherence-based dereverberation technique is provided to show the advantage of this database for algorithm evaluation. The AIR database is being made available online.","['https://openalex.org/W2086286498', 'https://openalex.org/W2125114513', 'https://openalex.org/W2117678320', 'https://openalex.org/W6676153378', 'https://openalex.org/W2124040307', 'https://openalex.org/W2080145918', 'https://openalex.org/W190060899', 'https://openalex.org/W2110781248', 'https://openalex.org/W1496114844', 'https://openalex.org/W63783813', 'https://openalex.org/W2970850616']",2009-07-01
https://openalex.org/W2086381917,https://doi.org/10.1109/lsp.2014.2379648,"Can we Automatically Transform Speech Recorded on Common Consumer Devices in Real-World Environments into Professional Production Quality Speech?—A Dataset, Insights, and Challenges","The goal of speech enhancement is typically to recover clean speech from noisy, reverberant, and often bandlimited speech in order to yield improved intelligibility, clarity, or automatic speech recognition performance. However, the acoustic goal for a great deal of speech content such as voice overs, podcasts, demo videos, lecture videos, and audio stories is often not merely clean speech, but speech that is aesthetically pleasing. This is achieved in professional recording studios by having a skilled sound engineer record clean speech in an acoustically treated room and then edit and process it with audio effects (which we refer to as production). A growing amount of speech content is being recorded on common consumer devices such as tablets, smartphones, and laptops. Moreover, it is typically recorded in common but non-acoustically treated environments such as homes and offices. We argue that the goal of enhancing such recordings should not only be to make it sound cleaner as would be done using traditional speech enhancement techniques, but to make it sound like it was recorded and produced in a professional recording studio. In this paper, we show why this can be beneficial, describe a new data set (a great deal of which was recorded in a professional recording studio) that we prepared to help in developing algorithms for this purpose, and discuss some insights and challenges associated with this problem.","['https://openalex.org/W6647660671', 'https://openalex.org/W1987172010', 'https://openalex.org/W1603944489', 'https://openalex.org/W1985819805', 'https://openalex.org/W2098832077', 'https://openalex.org/W6630299729', 'https://openalex.org/W2163922914', 'https://openalex.org/W6844057175', 'https://openalex.org/W2151484683', 'https://openalex.org/W2153894152', 'https://openalex.org/W3147539069', 'https://openalex.org/W4253928870', 'https://openalex.org/W2401258970', 'https://openalex.org/W4296927107', 'https://openalex.org/W6629448772', 'https://openalex.org/W2129120544', 'https://openalex.org/W6675958365', 'https://openalex.org/W2144404214', 'https://openalex.org/W6712278672', 'https://openalex.org/W2105921478', 'https://openalex.org/W2397741330', 'https://openalex.org/W1989314204', 'https://openalex.org/W630361093', 'https://openalex.org/W4302114989', 'https://openalex.org/W1503880575', 'https://openalex.org/W1746819321', 'https://openalex.org/W2911394281', 'https://openalex.org/W2104387568', 'https://openalex.org/W2101045344', 'https://openalex.org/W1601919122', 'https://openalex.org/W1492775261', 'https://openalex.org/W2626428364', 'https://openalex.org/W1495679096', 'https://openalex.org/W2908201190', 'https://openalex.org/W4211049957', 'https://openalex.org/W2121973264']",2014-12-10
https://openalex.org/W2982059757,https://doi.org/10.1109/icassp40776.2020.9052970,Impulse Response Data Augmentation and Deep Neural Networks for Blind Room Acoustic Parameter Estimation,"The reverberation time (T60) and the direct-to-reverberant ratio (DRR) are commonly used to characterize room acoustic environments. Both parameters can be measured from an acoustic impulse response (AIR) or using blind estimation methods that perform estimation directly from speech. When neural networks are used for blind estimation, however, a large realistic dataset is needed, which is expensive and time consuming to collect. To address this, we propose an AIR augmentation method that can parametrically control the T60 and DRR, allowing us to expand a small dataset of real AIRs into a balanced dataset orders of magnitude larger. Using this method, we train a previously proposed convolutional neural network (CNN) and show we can outperform past single-channel state-of-the-art methods. We then propose a more efficient, straightforward baseline CNN that is 4-5x faster, which provides an additional improvement and is better or comparable to all previously reported single- and multi-channel state-of-the-art methods.","['https://openalex.org/W2900429685', 'https://openalex.org/W6712594816', 'https://openalex.org/W2696967604', 'https://openalex.org/W2509065397', 'https://openalex.org/W6680987062', 'https://openalex.org/W1548586491', 'https://openalex.org/W6631587439', 'https://openalex.org/W2086381917', 'https://openalex.org/W4231225162', 'https://openalex.org/W4296927107', 'https://openalex.org/W6694991879', 'https://openalex.org/W2410879554', 'https://openalex.org/W6692983730', 'https://openalex.org/W2180309746', 'https://openalex.org/W6631809054', 'https://openalex.org/W6635519988', 'https://openalex.org/W6690787589', 'https://openalex.org/W2191779130', 'https://openalex.org/W6640103097', 'https://openalex.org/W6631190155', 'https://openalex.org/W2962808638', 'https://openalex.org/W3098357269', 'https://openalex.org/W2398243923', 'https://openalex.org/W2964121744', 'https://openalex.org/W1530412343', 'https://openalex.org/W2246804954', 'https://openalex.org/W4294567040', 'https://openalex.org/W1917460721', 'https://openalex.org/W1595766596', 'https://openalex.org/W4298152313', 'https://openalex.org/W1522301498', 'https://openalex.org/W2142370513', 'https://openalex.org/W1525353496']",2020-04-09
https://openalex.org/W3178031393,https://doi.org/10.1109/waspaa52581.2021.9632680,Filtered Noise Shaping for Time Domain Room Impulse Response Estimation from Reverberant Speech,"Deep learning approaches have emerged that aim to transform an audio signal so that it sounds as if it was recorded in the same room as a reference recording, with applications both in audio postproduction and augmented reality. In this work, we propose FiNS, a Filtered Noise Shaping network that directly estimates the time domain room impulse response (RIR) from reverberant speech. Our domain-inspired architecture features a time domain encoder and a filtered noise shaping decoder that models the RIR as a summation of decaying filtered noise signals, along with direct sound and early reflection components. Previous methods for acoustic matching utilize either large models to transform audio to match the target room or predict parameters for algorithmic reverberators. Instead, blind estimation of the RIR enables efficient and realistic transformation with a single convolution. An evaluation demonstrates our model not only synthesizes RIRs that match parameters of the target room, such as the <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">$T_{60}$</tex> and DRR, but also more accurately reproduces perceptual characteristics of the target room, as shown in a listening test when compared to deep learning baselines.","['https://openalex.org/W6757817989', 'https://openalex.org/W6751512325', 'https://openalex.org/W3015676952', 'https://openalex.org/W6785656517', 'https://openalex.org/W2586821239', 'https://openalex.org/W3127192263', 'https://openalex.org/W2127707623', 'https://openalex.org/W2900429685', 'https://openalex.org/W2898519127', 'https://openalex.org/W2982059757', 'https://openalex.org/W2760103357', 'https://openalex.org/W6768435317', 'https://openalex.org/W2141418660', 'https://openalex.org/W2152567223', 'https://openalex.org/W6769767169', 'https://openalex.org/W6634987543', 'https://openalex.org/W2410879554', 'https://openalex.org/W2189292795', 'https://openalex.org/W1964596217', 'https://openalex.org/W6635621604', 'https://openalex.org/W2169483529', 'https://openalex.org/W2002507711', 'https://openalex.org/W2611709348', 'https://openalex.org/W2134411878', 'https://openalex.org/W2021421911', 'https://openalex.org/W1677182931', 'https://openalex.org/W3015338123', 'https://openalex.org/W3198454118', 'https://openalex.org/W3103930150', 'https://openalex.org/W3093751509', 'https://openalex.org/W2963452667', 'https://openalex.org/W2565077543', 'https://openalex.org/W2996286887', 'https://openalex.org/W1592868448', 'https://openalex.org/W3139813613', 'https://openalex.org/W2786672974', 'https://openalex.org/W2998572311', 'https://openalex.org/W1586100143', 'https://openalex.org/W2908510526', 'https://openalex.org/W3044968124', 'https://openalex.org/W4287706224', 'https://openalex.org/W3163735648', 'https://openalex.org/W3133859501']",2021-10-17
https://openalex.org/W1700083040,https://doi.org/10.1109/icassp.1982.1171604,Multiple stage vector quantization for speech coding,"In this paper, we present a multiple stage vector quantization technique which allows easy expansion of the original vector quantizer design to operate at higher bit rates for lower distortion. The computation and storage reduction is achieved by the fact that the overall requirements are the sum of the requirements of each stage instead of an exponentially increasing function of the bit rate as in the original one stage design. In the case of Euclidean distance measures such as the log area ratio measure, experimental results show that the quantizer performance is very close to a theoretically predicted asymptotically optimal rate distortion relationship.","['https://openalex.org/W2142228262', 'https://openalex.org/W2150418026', 'https://openalex.org/W1986343971', 'https://openalex.org/W2163904446', 'https://openalex.org/W2120062331']",2005-03-24
https://openalex.org/W4221146826,https://doi.org/10.1109/icassp43922.2022.9747603,Echo-Aware Adaptation of Sound Event Localization and Detection in Unknown Environments,"Our goal is to develop a sound event localization and detection (SELD) system that works robustly in unknown environments. A SELD system trained on known environment data is degraded in an unknown environment due to environmental effects such as reverberation and noise not contained in the training data. Previous studies on related tasks have shown that domain adaptation methods are effective when data on the environment in which the system will be used is available even without labels. However adaptation to unknown environments remains a difficult task. In this study, we propose echo-aware feature refinement (EAR) for SELD, which suppresses environmental effects at the feature level by using additional spatial cues of the unknown environment obtained through measuring acoustic echoes. FOA-MEIR <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> , an impulse response dataset containing over 100 environments, was recorded to validate the proposed method. Experiments on FOA-MEIR show that the EAR effectively improves SELD performance in unknown environments.","['https://openalex.org/W2937324313', 'https://openalex.org/W3015469592', 'https://openalex.org/W2997076748', 'https://openalex.org/W2051145414', 'https://openalex.org/W6776723300', 'https://openalex.org/W2072604753', 'https://openalex.org/W2024787486', 'https://openalex.org/W2982382207', 'https://openalex.org/W6779923105', 'https://openalex.org/W2292996718', 'https://openalex.org/W2964342924', 'https://openalex.org/W2982429715', 'https://openalex.org/W2883787264', 'https://openalex.org/W3163193264', 'https://openalex.org/W6743647910', 'https://openalex.org/W2663904211', 'https://openalex.org/W2511089602', 'https://openalex.org/W6796679619', 'https://openalex.org/W2204091679', 'https://openalex.org/W6798133626', 'https://openalex.org/W6639480849', 'https://openalex.org/W2157331557', 'https://openalex.org/W2998139081', 'https://openalex.org/W6631190155', 'https://openalex.org/W2751116311', 'https://openalex.org/W2187089797', 'https://openalex.org/W4287766186', 'https://openalex.org/W3108655859', 'https://openalex.org/W1522301498', 'https://openalex.org/W4287120192', 'https://openalex.org/W1882958252', 'https://openalex.org/W4287070075']",2022-04-27
https://openalex.org/W2901243971,https://doi.org/10.1109/jstsp.2019.2917582,Building and evaluation of a real room impulse response dataset,"This paper presents BUT ReverbDB - a dataset of real room impulse responses (RIR), background noises and re-transmitted speech data. The retransmitted data includes LibriSpeech test-clean, 2000 HUB5 English evaluation and part of 2010 NIST Speaker Recognition Evaluation datasets. We provide a detailed description of RIR collection (hardware, software, post-processing) that can serve as a ""cook-book"" for similar efforts. We also validate BUT ReverbDB in two sets of automatic speech recognition (ASR) experiments and draw conclusions for augmenting ASR training data with real and artificially generated RIRs. We show that a limited number of real RIRs, carefully selected to match the target environment, provide results comparable to a large number of artificially generated RIRs, and that both sets can be combined to achieve the best ASR results. The dataset is distributed for free under a non-restrictive license and it currently contains data from 8 rooms, which is growing. The distribution package also contains a Kaldi-based recipe for augmenting publicly available AMI close-talk meeting data and test the results on an AMI single distant microphone set, allowing it to reproduce our experiments.","['https://openalex.org/W6630830844', 'https://openalex.org/W6633823725', 'https://openalex.org/W2076156369', 'https://openalex.org/W6634800993', 'https://openalex.org/W2043551274', 'https://openalex.org/W6681817058', 'https://openalex.org/W6633131850', 'https://openalex.org/W6634987543', 'https://openalex.org/W6691304110', 'https://openalex.org/W2510942155', 'https://openalex.org/W6713419125', 'https://openalex.org/W2289441404', 'https://openalex.org/W2765284764', 'https://openalex.org/W2242685705', 'https://openalex.org/W2136682440', 'https://openalex.org/W6726035914', 'https://openalex.org/W2410879554', 'https://openalex.org/W6714750551', 'https://openalex.org/W2963127222', 'https://openalex.org/W6690610466', 'https://openalex.org/W2038810952', 'https://openalex.org/W6713570806', 'https://openalex.org/W6631362777', 'https://openalex.org/W2086286498', 'https://openalex.org/W2048608953', 'https://openalex.org/W2027499299', 'https://openalex.org/W6628225846', 'https://openalex.org/W2147166770', 'https://openalex.org/W6783397956', 'https://openalex.org/W2696967604', 'https://openalex.org/W2070707809', 'https://openalex.org/W2148575186', 'https://openalex.org/W2289394825', 'https://openalex.org/W2884797218', 'https://openalex.org/W1989314204', 'https://openalex.org/W2891803057', 'https://openalex.org/W1494198834', 'https://openalex.org/W1973669708', 'https://openalex.org/W1591607137', 'https://openalex.org/W2617258110', 'https://openalex.org/W2070215586', 'https://openalex.org/W2763188033', 'https://openalex.org/W1559975096', 'https://openalex.org/W2117678320', 'https://openalex.org/W1504642953', 'https://openalex.org/W2250629451', 'https://openalex.org/W2058219874', 'https://openalex.org/W4295113312', 'https://openalex.org/W1480210121', 'https://openalex.org/W1582050363', 'https://openalex.org/W2087458881', 'https://openalex.org/W2468573742', 'https://openalex.org/W2404568753', 'https://openalex.org/W2513415105', 'https://openalex.org/W1553887387', 'https://openalex.org/W2148495393', 'https://openalex.org/W2799831802', 'https://openalex.org/W4302156456', 'https://openalex.org/W4251733995', 'https://openalex.org/W1569827162', 'https://openalex.org/W2101045344', 'https://openalex.org/W3089011816', 'https://openalex.org/W4298298427', 'https://openalex.org/W2402865896', 'https://openalex.org/W1524333225', 'https://openalex.org/W4237338866', 'https://openalex.org/W1586100143', 'https://openalex.org/W2245569228', 'https://openalex.org/W1255718337', 'https://openalex.org/W1480876431', 'https://openalex.org/W2410501869', 'https://openalex.org/W1513603309', 'https://openalex.org/W1569447338', 'https://openalex.org/W2963064586']",2019-05-17
https://openalex.org/W3093583495,https://doi.org/10.5281/zenodo.4116247,Open Database of Spatial Room Impulse Responses at Detmold University of Music,"This repository contains an open source database of Spatial Room Impulse Responses (SRIR) captured at three different performance spaces of the Detmold University of Music. It includes the following rooms: Detmold Konzerthaus (medium sized concert hall, ~600 seats). Brahmssaal (small music chamber room, ~100 seats). Detmold Sommertheater (theater, ~300 seats). The collection contains approximately 600 multichannel RIRs corresponding to several source and receiver configurations. For each room we include measurement positions on stage and at the audience area captured with both an artificial head and an open microphone array compatible with the Spatial Decomposition Method (SDM). The Detmold Konzerthaus holds a large scale Wave Field Synthesis system and a Room Acoustic Enhancement System. SRIRs of an ensemble of focused sources on stage and with conditions of increased artificial reverberation are also included. If you use this dataset for your research, please cite our work: Amengual Gari, S. V.; Sahin, B.; Eddy, D; Kob, M.: <strong>""Open Database of Spatial Room Impulse Responses at Detmold University of Music""</strong>, <em>149th Convention of the Audio Engineering Society, </em>2020. The database is organized in 3 sets: <strong>- Set A: </strong> Source: Single Source measurements. Receiver: Open Array and Dummy Head. Rooms: BS, DST, KH Special configurations: Artificial reverberation, music stand on stage <strong>- Set B: </strong> Source: Loudspeaker and WFS orchestra Receiver: Open Array. Rooms: KH <strong>- Set C:</strong> Source: Loudspeaker orchestra Receiver: Dummy Head and Omni8 array Rooms: KH Further details on the measurement procedure and acoustical analysis of the RIRs can be found in the following publications: <strong>Set A</strong> Amengual Gari, S. V., Investigations on the Influence of Acoustics on Live Music Performance using Virtual Acoustic Methods, Ph.D. thesis, 2017. Amengual Garí, S. V.; Kob, M: ""Investigating the impact of a music stand on stage using spatial impulse responses"". 142nd Convention of the Audio Engineering Society, Berlin, May 2017. <strong>Set B</strong> Amengual Garí, S. V.; Pätynen, J.; Lokki, T.: ""Physical and perceptual comparison of real and focused sound sources in a concert hall"". Journal of the Audio Engineering Society, vol. 64 (12), pp. 1014-1025, December 2016. <strong>Set C</strong> Sahin, B., ““Investigation of the Detmold Concert Hall auditorium acoustics by comparing preference ratings and objective measurements.”, M.Sc. Thesis, 2017. Sahin, B., Amengual, S. V., and Kob, M., “Investigating listeners’ preferences in Detmold Concert Hall by comparing sensory evaluation and objective measurements,” Proc. 43th DAGA, Kiel, 2017.<br>",[],2020-10-21
https://openalex.org/W1968834101,https://doi.org/10.1109/icassp.2010.5496083,Database of omnidirectional and B-format room impulse responses,This paper introduces a new database of room impulse responses. This database differs greatly from previously released databases as it contains over 700 impulse responses. The impulse responses are measured in three different rooms each with a static source position and at least 130 different receiver positions. Each measurement position is recorded with both an omnidirectional microphone and a B-format microphone.,"['https://openalex.org/W190060899', 'https://openalex.org/W2032990591', 'https://openalex.org/W1528800099', 'https://openalex.org/W2145676852', 'https://openalex.org/W2262991570', 'https://openalex.org/W1569827162', 'https://openalex.org/W2086286498', 'https://openalex.org/W2084845426', 'https://openalex.org/W2136682440', 'https://openalex.org/W2168786188', 'https://openalex.org/W1595015222']",2010-01-01
https://openalex.org/W4224932531,https://doi.org/10.1109/icassp43922.2022.9746610,Multi-Scale Temporal Frequency Convolutional Network With Axial Attention for Speech Enhancement,"Speech quality is often degraded by acoustic echoes, background noise, and reverberation. In this paper, we propose a system consisting of deep learning and signal processing to simultaneously suppress echoes, noise, and reverberation. For the deep learning, we design a novel speech dense-prediction backbone. For the signal processing, a linear acoustic echo canceller is used as conditional information for deep learning. To improve the performance of the speech dense-prediction backbone, strategies such as a microphone and reference phase encoder, multi-scale time-frequency processing, and streaming axial attention are designed. The proposed system ranked first in both AEC and DNS Challenge (non-personal track) of ICASSP 2022. In addition, this backbone has also been extended to the multi-channel speech enhancement task, and placed second in ICASSP 2022 L3DAS22 Challenge <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .","['https://openalex.org/W3095248373', 'https://openalex.org/W2952218014', 'https://openalex.org/W3138516171', 'https://openalex.org/W2974407961', 'https://openalex.org/W6802983977', 'https://openalex.org/W2898268964', 'https://openalex.org/W3120336970', 'https://openalex.org/W3160129476', 'https://openalex.org/W3092864146', 'https://openalex.org/W3097034112', 'https://openalex.org/W6787607767', 'https://openalex.org/W3162348042', 'https://openalex.org/W2158582019', 'https://openalex.org/W2046317813', 'https://openalex.org/W4224919230', 'https://openalex.org/W2099464104', 'https://openalex.org/W6679212047', 'https://openalex.org/W2065804682', 'https://openalex.org/W4225905067', 'https://openalex.org/W3160085755', 'https://openalex.org/W2131108018', 'https://openalex.org/W3101330598', 'https://openalex.org/W3113290170', 'https://openalex.org/W3104196160', 'https://openalex.org/W3099330747', 'https://openalex.org/W3209141406']",2022-04-27
https://openalex.org/W4296930493,https://doi.org/10.3390/acoustics4030047,Measuring the Acoustical Properties of the BBC Maida Vale Recording Studios for Virtual Reality,"In this paper we present a complete acoustic survey of the British Broadcasting Corporation Maida Vale recording studios. The paper outlines a fast room acoustic measurement framework for capture of spatial impulse response measurements for use in three or six degrees of freedom Virtual Reality rendering. Binaural recordings from a KEMAR dummy head as well as higher order Ambisonic spatial room impulse response measurements taken using a higher order Ambisonic microphone are presented. An acoustic comparison of the studios is discussed, highlighting remarkable similarities across three of the recording spaces despite significant differences in geometry. Finally, a database of the measurements, housing the raw impulse response captures as well as processed spatial room impulse responses is presented.","['https://openalex.org/W6728854140', 'https://openalex.org/W4236079661', 'https://openalex.org/W2563145135', 'https://openalex.org/W2939810734', 'https://openalex.org/W6629079164', 'https://openalex.org/W6676728696', 'https://openalex.org/W2972443682', 'https://openalex.org/W1569827162', 'https://openalex.org/W2898527280', 'https://openalex.org/W6651858270', 'https://openalex.org/W1575206345', 'https://openalex.org/W2950381583', 'https://openalex.org/W2200835621', 'https://openalex.org/W2763259349', 'https://openalex.org/W6687829480', 'https://openalex.org/W2568845008', 'https://openalex.org/W2738467312', 'https://openalex.org/W2085210789', 'https://openalex.org/W3095129596', 'https://openalex.org/W2066853009', 'https://openalex.org/W3161190431', 'https://openalex.org/W2803224320', 'https://openalex.org/W2086574250', 'https://openalex.org/W1480730147', 'https://openalex.org/W1512887162', 'https://openalex.org/W7010627007', 'https://openalex.org/W7026739200', 'https://openalex.org/W2142297502', 'https://openalex.org/W1973369692', 'https://openalex.org/W2542193101', 'https://openalex.org/W2111307957', 'https://openalex.org/W1485114642', 'https://openalex.org/W2006053431']",2022-09-14
https://openalex.org/W3163656839,https://doi.org/10.1109/icassp39728.2021.9415122,Acoustic Analysis and Dataset of Transitions Between Coupled Rooms,Funding Information: The authors would like to thank the Human Optimised XR (HumOR) Project for funding this research. Publisher Copyright: © 2021 IEEE,"['https://openalex.org/W3001046125', 'https://openalex.org/W1997836753', 'https://openalex.org/W6631588615', 'https://openalex.org/W6780218030', 'https://openalex.org/W2031105940', 'https://openalex.org/W3047232705', 'https://openalex.org/W6633823725', 'https://openalex.org/W6841717752', 'https://openalex.org/W6751798204', 'https://openalex.org/W2738467312', 'https://openalex.org/W2136682440', 'https://openalex.org/W2161784349', 'https://openalex.org/W1981581059', 'https://openalex.org/W1978659076', 'https://openalex.org/W2783525307', 'https://openalex.org/W6633653620', 'https://openalex.org/W1569827162', 'https://openalex.org/W3035684428', 'https://openalex.org/W2803547534', 'https://openalex.org/W2541714600', 'https://openalex.org/W1582886869', 'https://openalex.org/W1565581153', 'https://openalex.org/W4293333680', 'https://openalex.org/W1526590657']",2021-05-13
https://openalex.org/W2410879554,https://doi.org/10.1109/taslp.2016.2577502,Estimation of Room Acoustic Parameters: The ACE Challenge,"Reverberation Time (T60) and Direct-to-Reverberant Ratio (DRR) are important parameters which together can characterize sound captured by microphones in non-anechoic rooms. These parameters are important in speech processing applications such as speech recognition and dereverberation. The values of T60 and DRR can be estimated directly from the Acoustic Impulse Response (AIR) of the room. In practice, the AIR is not normally available, in which case these parameters must be estimated blindly from the observed speech in the microphone signal. The Acoustic Characterization of Environments (ACE) Challenge aimed to determine the state-of-the-art in blind acoustic parameter estimation and also to stimulate research in this area. A summary of the ACE Challenge, and the corpus used in the challenge is presented together with an analysis of the results. Existing algorithms were submitted alongside novel contributions, the comparative results for which are presented in this paper. The challenge showed that T60 estimation is a mature field where analytical approaches dominate whilst DRR estimation is a less mature field where machine learning approaches are currently more successful.","['https://openalex.org/W1557831657', 'https://openalex.org/W190060899', 'https://openalex.org/W6677592687', 'https://openalex.org/W2172491908', 'https://openalex.org/W6691326925', 'https://openalex.org/W1989314204', 'https://openalex.org/W1482315657', 'https://openalex.org/W1974387177', 'https://openalex.org/W6630574771', 'https://openalex.org/W6686978693', 'https://openalex.org/W2045399954', 'https://openalex.org/W2032061444', 'https://openalex.org/W1995400204', 'https://openalex.org/W6683785785', 'https://openalex.org/W2093988885', 'https://openalex.org/W6696870564', 'https://openalex.org/W2099973374', 'https://openalex.org/W6697132121', 'https://openalex.org/W6690787589', 'https://openalex.org/W2127707623', 'https://openalex.org/W6639158183', 'https://openalex.org/W6639990721', 'https://openalex.org/W6640103097', 'https://openalex.org/W2175303068', 'https://openalex.org/W1501702692', 'https://openalex.org/W2035412504', 'https://openalex.org/W6688706896', 'https://openalex.org/W1866989267', 'https://openalex.org/W4296927107', 'https://openalex.org/W6680987062', 'https://openalex.org/W6712264444', 'https://openalex.org/W6696902769', 'https://openalex.org/W2152567223', 'https://openalex.org/W2523573275', 'https://openalex.org/W2125114513', 'https://openalex.org/W2033288740', 'https://openalex.org/W6982686887', 'https://openalex.org/W2042212675', 'https://openalex.org/W1988961123', 'https://openalex.org/W2011528076', 'https://openalex.org/W1978681480', 'https://openalex.org/W2260627686', 'https://openalex.org/W1820182450', 'https://openalex.org/W2145354827', 'https://openalex.org/W2180309746', 'https://openalex.org/W6694991879', 'https://openalex.org/W6630830844', 'https://openalex.org/W6633823725', 'https://openalex.org/W2962808638', 'https://openalex.org/W2293059647', 'https://openalex.org/W2911394281', 'https://openalex.org/W2161336374', 'https://openalex.org/W2962927993', 'https://openalex.org/W2142370513', 'https://openalex.org/W2396487378', 'https://openalex.org/W2246804954', 'https://openalex.org/W2295168709', 'https://openalex.org/W1513603309', 'https://openalex.org/W2116746197', 'https://openalex.org/W1512562331', 'https://openalex.org/W158663322', 'https://openalex.org/W1913812985', 'https://openalex.org/W4294567040', 'https://openalex.org/W4298152313', 'https://openalex.org/W2963579082', 'https://openalex.org/W1865306031', 'https://openalex.org/W1569827162', 'https://openalex.org/W2222891892', 'https://openalex.org/W2188162373', 'https://openalex.org/W2295687657', 'https://openalex.org/W1917460721', 'https://openalex.org/W2251255183', 'https://openalex.org/W1534552481']",2016-06-07
https://openalex.org/W4372341113,https://doi.org/10.1109/icassp49357.2023.10094770,Towards Improved Room Impulse Response Estimation for Speech Recognition,"We propose a novel approach for blind room impulse response (RIR) estimation systems in the context of a downstream application scenario, far-field automatic speech recognition (ASR). We first draw the connection between improved RIR estimation and improved ASR performance, as a means of evaluating neural RIR estimators. We then propose a generative adversarial network (GAN) based architecture that encodes RIR features from reverberant speech and constructs an RIR from the encoded features, and uses a novel energy decay relief loss to optimize for capturing energy-based properties of the input reverberant speech. We show that our model outperforms the state-of-the-art baselines on acoustic benchmarks (by 17% on the energy decay relief and 22% on an early-reflection energy metric), as well as in an ASR evaluation task (by 6.9% in word error rate).","['https://openalex.org/W2204353980', 'https://openalex.org/W2159471787', 'https://openalex.org/W2735006420', 'https://openalex.org/W3178031393', 'https://openalex.org/W2144039164', 'https://openalex.org/W4224314935', 'https://openalex.org/W2901243971', 'https://openalex.org/W3198454118', 'https://openalex.org/W4224933773', 'https://openalex.org/W2010362084', 'https://openalex.org/W3206257526', 'https://openalex.org/W6680987062', 'https://openalex.org/W1494198834', 'https://openalex.org/W6839307775', 'https://openalex.org/W1978074661', 'https://openalex.org/W4251733995', 'https://openalex.org/W2542605056', 'https://openalex.org/W6792637235', 'https://openalex.org/W1499999342', 'https://openalex.org/W6678809451', 'https://openalex.org/W3101831011', 'https://openalex.org/W4281264304', 'https://openalex.org/W2101038365', 'https://openalex.org/W6839296024', 'https://openalex.org/W3026111682', 'https://openalex.org/W6844330180', 'https://openalex.org/W6784139575', 'https://openalex.org/W4210439360', 'https://openalex.org/W4281739032', 'https://openalex.org/W2142370513', 'https://openalex.org/W3198661668', 'https://openalex.org/W3151851237', 'https://openalex.org/W2125336414', 'https://openalex.org/W3092796334', 'https://openalex.org/W4385823401', 'https://openalex.org/W4283076784']",2023-05-05
https://openalex.org/W2555915854,https://doi.org/10.1073/pnas.1612524113,Statistics of natural reverberation enable perceptual separation of sound and space,"Significance Sounds produced in the world reflect off surrounding surfaces on their way to our ears. Known as reverberation, these reflections distort sound but provide information about the world around us. We asked whether reverberation exhibits statistical regularities that listeners use to separate its effects from those of a sound’s source. We conducted a large-scale statistical analysis of real-world acoustics, revealing strong regularities of reverberation in natural scenes. We found that human listeners can estimate the contributions of the source and the environment from reverberant sound, but that they depend critically on whether environmental acoustics conform to the observed statistical regularities. The results suggest a separation process constrained by knowledge of environmental acoustics that is internalized over development or evolution.","['https://openalex.org/W6680692001', 'https://openalex.org/W4240597352', 'https://openalex.org/W2005228249', 'https://openalex.org/W1615920531', 'https://openalex.org/W1584473438', 'https://openalex.org/W1953755264', 'https://openalex.org/W2054392851', 'https://openalex.org/W2047665447', 'https://openalex.org/W2071151415', 'https://openalex.org/W1997709277', 'https://openalex.org/W1989320958', 'https://openalex.org/W1997769872', 'https://openalex.org/W4233218576', 'https://openalex.org/W6635519988', 'https://openalex.org/W1997836753', 'https://openalex.org/W2088399049', 'https://openalex.org/W2030535888', 'https://openalex.org/W2136682440', 'https://openalex.org/W2095153821', 'https://openalex.org/W6636001751', 'https://openalex.org/W2034479040', 'https://openalex.org/W2083954185', 'https://openalex.org/W2106631236', 'https://openalex.org/W2062663442', 'https://openalex.org/W2025127228', 'https://openalex.org/W2077340541', 'https://openalex.org/W3023305142', 'https://openalex.org/W1946152311', 'https://openalex.org/W2473453393', 'https://openalex.org/W2056234685', 'https://openalex.org/W2006539252', 'https://openalex.org/W2062971755', 'https://openalex.org/W2066418891', 'https://openalex.org/W2004867530', 'https://openalex.org/W2095443724', 'https://openalex.org/W2104135700', 'https://openalex.org/W2057898689', 'https://openalex.org/W2242685705', 'https://openalex.org/W1989919048', 'https://openalex.org/W2113041593', 'https://openalex.org/W2117215414', 'https://openalex.org/W2156707826', 'https://openalex.org/W2040209392', 'https://openalex.org/W1981706467', 'https://openalex.org/W2015776358', 'https://openalex.org/W1995151553', 'https://openalex.org/W2004126765', 'https://openalex.org/W2111307957', 'https://openalex.org/W1577554631', 'https://openalex.org/W2050758723', 'https://openalex.org/W6760590573', 'https://openalex.org/W1558485894', 'https://openalex.org/W2294307780', 'https://openalex.org/W1600640951', 'https://openalex.org/W1590802246', 'https://openalex.org/W2111349285', 'https://openalex.org/W1595766596', 'https://openalex.org/W2983958770', 'https://openalex.org/W1532531374', 'https://openalex.org/W2970850616', 'https://openalex.org/W2139573938', 'https://openalex.org/W2923742357', 'https://openalex.org/W1971198963']",2016-11-10
https://openalex.org/W3015676952,https://doi.org/10.1109/icassp40776.2020.9054701,Acoustic Matching By Embedding Impulse Responses,"The goal of acoustic matching is to transform an audio recording made in one acoustic environment to sound as if it had been recorded in a different environment, based on reference audio from the target environment. This paper introduces a deep learning solution for two parts of the acoustic matching problem. First, we characterize acoustic environments by mapping audio into a low-dimensional embedding invariant to speech content and speaker identity. Next, a waveform-to-waveform neural network conditioned on this embedding learns to transform an input waveform to match the acoustic qualities encoded in the target embedding. Listening tests on both simulated and real environments show that the proposed approach improves on state-of-the-art baseline methods.","['https://openalex.org/W2141708418', 'https://openalex.org/W2900429685', 'https://openalex.org/W6686978693', 'https://openalex.org/W1591811717', 'https://openalex.org/W2204353980', 'https://openalex.org/W2800675406', 'https://openalex.org/W2405184167', 'https://openalex.org/W2117678320', 'https://openalex.org/W2800288142', 'https://openalex.org/W6767299982', 'https://openalex.org/W6630767490', 'https://openalex.org/W2940385941', 'https://openalex.org/W2555915854', 'https://openalex.org/W2963103134', 'https://openalex.org/W2086381917', 'https://openalex.org/W2410879554', 'https://openalex.org/W1989314204', 'https://openalex.org/W2180309746', 'https://openalex.org/W6755933110', 'https://openalex.org/W2408467190', 'https://openalex.org/W6694991879', 'https://openalex.org/W2737697117', 'https://openalex.org/W2933224119', 'https://openalex.org/W1499999342', 'https://openalex.org/W2972162845', 'https://openalex.org/W6690441418', 'https://openalex.org/W2586425531', 'https://openalex.org/W2747161606', 'https://openalex.org/W2963775347', 'https://openalex.org/W3122808968', 'https://openalex.org/W1512887162', 'https://openalex.org/W2188162373', 'https://openalex.org/W2962808638', 'https://openalex.org/W4298152313', 'https://openalex.org/W2971646704', 'https://openalex.org/W3104328743', 'https://openalex.org/W2237224976', 'https://openalex.org/W2898519127', 'https://openalex.org/W2519091744']",2020-04-09
https://openalex.org/W2058341666,https://doi.org/10.1250/ast.20.225,Sound scene data collection in real acoustical environments.,"This paper describes a sound scene database necessary for studies such as sound source localization, sound retrieval, sound recognition and speech recognition in real acoustical environments. Many speech databases have been collected for speech recognition so far. The statistical modeling of speech based on the collected speech databases realizes a drastic improvement of speech recognition performance. However, there are only a few databases available for sound scene data including non-speech sound in real environments. A sound scene database is obviously necessary for studies of acoustical signal processing and sound recognition. This paper reports on a project for collection of the sound scene database supported by Real World Computing Partnership (RWCP). There are many kinds of sound scenes in real environments. The sound scene is denoted by sound sources androomacoustics. The number of combination of the sound sources, source positions and rooms is huge in real acoustical environments. Two approaches are taken to build the sound scene database in the early stage of the project. The first approach is to collect isolated sound sources of many kinds of non-speech sounds and speech sounds. The second approach is to collect impulse responses in various acoustical environments. The sound in the collected environments can be simulated by convolution of the isolated sound sources and impulse responses. In a later stage, the sound scene data in real acoustical environments is planned to be collected using a three dimensional microphone array. In this paper, the plan and progress of our sound scene database project are described.","['https://openalex.org/W4394608251', 'https://openalex.org/W6603753138', 'https://openalex.org/W6601668695', 'https://openalex.org/W1530438186', 'https://openalex.org/W6600728236', 'https://openalex.org/W6632447556', 'https://openalex.org/W17608502', 'https://openalex.org/W39796305', 'https://openalex.org/W3143566648', 'https://openalex.org/W94449304']",1999-01-01
https://openalex.org/W3034772996,,On Layer Normalization in the Transformer Architecture,"The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.","['https://openalex.org/W2964098600', 'https://openalex.org/W2894175714', 'https://openalex.org/W2949888546', 'https://openalex.org/W2146502635', 'https://openalex.org/W2964308564', 'https://openalex.org/W2124807415', 'https://openalex.org/W2979636403', 'https://openalex.org/W2886490473', 'https://openalex.org/W2948981900', 'https://openalex.org/W2965373594', 'https://openalex.org/W2940744433', 'https://openalex.org/W2907747478', 'https://openalex.org/W6908809', 'https://openalex.org/W2963855133', 'https://openalex.org/W2130158090', 'https://openalex.org/W2963403868', 'https://openalex.org/W2101105183', 'https://openalex.org/W2948798935', 'https://openalex.org/W2963970792', 'https://openalex.org/W2964265128', 'https://openalex.org/W131533222', 'https://openalex.org/W2396767181', 'https://openalex.org/W2766678531', 'https://openalex.org/W2183341477', 'https://openalex.org/W2194775991', 'https://openalex.org/W2911109671', 'https://openalex.org/W2622263826', 'https://openalex.org/W2963418779', 'https://openalex.org/W2952444318', 'https://openalex.org/W1533861849', 'https://openalex.org/W2994689640', 'https://openalex.org/W2907121943', 'https://openalex.org/W2523246573', 'https://openalex.org/W2949433733', 'https://openalex.org/W2796108585', 'https://openalex.org/W2963341956', 'https://openalex.org/W2962784628', 'https://openalex.org/W2477417488', 'https://openalex.org/W2970597249', 'https://openalex.org/W1522301498', 'https://openalex.org/W2913473169', 'https://openalex.org/W2970217468']",2020-02-12
https://openalex.org/W2143827132,https://doi.org/10.1177/002383099704000203,Prosody in the Comprehension of Spoken Language: A Literature Review,"Research on the exploitation of prosodic information in the comprehension of spoken language is reviewed. The research falls into three main areas: the use of prosody in the recognition of spoken words, in which most attention has been paid to the question of whether the prosodic structure of a word plays a role in initial activation of stored lexical representations; the use of prosody in the computation of syntactic structure, in which the resolution of global and local ambiguities has formed the central focus; and the role of prosody in the processing of discourse structure, in which there has been a preponderance of work on the contribution of accentuation and deaccentuation to integration of concepts with an existing discourse model. The review reveals that in each area progress has been made towards new conceptions of prosody's role in processing, and in particular this has involved abandonment of previously held deterministic views of the relationship between prosodic structure and other aspects of linguistic structure.","['https://openalex.org/W2066113430', 'https://openalex.org/W4246892066', 'https://openalex.org/W2084658851', 'https://openalex.org/W2016484642', 'https://openalex.org/W6616119028', 'https://openalex.org/W1575575744', 'https://openalex.org/W6633544345', 'https://openalex.org/W2413313130', 'https://openalex.org/W2030042476', 'https://openalex.org/W2051694906', 'https://openalex.org/W2084820222', 'https://openalex.org/W2242914155', 'https://openalex.org/W2061585737', 'https://openalex.org/W2086869369', 'https://openalex.org/W2005051511', 'https://openalex.org/W2165466910', 'https://openalex.org/W2196093131', 'https://openalex.org/W2322065245', 'https://openalex.org/W2565486504', 'https://openalex.org/W2023168341', 'https://openalex.org/W576077571', 'https://openalex.org/W1496344992', 'https://openalex.org/W2033531207', 'https://openalex.org/W1996426298', 'https://openalex.org/W99080392', 'https://openalex.org/W4301351812', 'https://openalex.org/W1999435474', 'https://openalex.org/W1526539140', 'https://openalex.org/W2055477875', 'https://openalex.org/W2080517441', 'https://openalex.org/W4236102115', 'https://openalex.org/W827108657', 'https://openalex.org/W2008130220', 'https://openalex.org/W2095290699', 'https://openalex.org/W2035286392', 'https://openalex.org/W1971786318', 'https://openalex.org/W1578411503', 'https://openalex.org/W2104519584', 'https://openalex.org/W2098960459', 'https://openalex.org/W1560083564', 'https://openalex.org/W4244663558', 'https://openalex.org/W2076421006', 'https://openalex.org/W4301735423', 'https://openalex.org/W2075872589', 'https://openalex.org/W1982118345', 'https://openalex.org/W2065821883', 'https://openalex.org/W2113819709', 'https://openalex.org/W2161848892', 'https://openalex.org/W2162725876', 'https://openalex.org/W2128504025', 'https://openalex.org/W2140497421', 'https://openalex.org/W2139932287', 'https://openalex.org/W2144800021', 'https://openalex.org/W2117430670', 'https://openalex.org/W2041427507', 'https://openalex.org/W1974254681', 'https://openalex.org/W2146945859', 'https://openalex.org/W2129100720', 'https://openalex.org/W2125761087', 'https://openalex.org/W2098066434', 'https://openalex.org/W2133148365', 'https://openalex.org/W1969220573', 'https://openalex.org/W6682871759', 'https://openalex.org/W4302801028', 'https://openalex.org/W2126751995', 'https://openalex.org/W2186003382', 'https://openalex.org/W209556316', 'https://openalex.org/W2148605901', 'https://openalex.org/W4300885270', 'https://openalex.org/W1994502537', 'https://openalex.org/W2053387080', 'https://openalex.org/W2020738862', 'https://openalex.org/W2033087473', 'https://openalex.org/W2324444874', 'https://openalex.org/W2061068578', 'https://openalex.org/W2059001546', 'https://openalex.org/W2032369993', 'https://openalex.org/W2020407552', 'https://openalex.org/W2023899400', 'https://openalex.org/W1947950328', 'https://openalex.org/W2107670100', 'https://openalex.org/W2077117307', 'https://openalex.org/W2009086590', 'https://openalex.org/W4019956', 'https://openalex.org/W4246695671', 'https://openalex.org/W1964957131', 'https://openalex.org/W2989833064', 'https://openalex.org/W1994141266', 'https://openalex.org/W1984133279', 'https://openalex.org/W2017711451', 'https://openalex.org/W1996229709', 'https://openalex.org/W2991630666', 'https://openalex.org/W2060461562', 'https://openalex.org/W2034811676', 'https://openalex.org/W1965030914', 'https://openalex.org/W2091535002', 'https://openalex.org/W1582784770', 'https://openalex.org/W1992658213', 'https://openalex.org/W2166919574', 'https://openalex.org/W1968491374', 'https://openalex.org/W4238582696', 'https://openalex.org/W2988288086', 'https://openalex.org/W1537734884', 'https://openalex.org/W2066389566', 'https://openalex.org/W7037441803', 'https://openalex.org/W2124449078', 'https://openalex.org/W2145054858', 'https://openalex.org/W2057473050', 'https://openalex.org/W2133388752', 'https://openalex.org/W80516736', 'https://openalex.org/W8163826', 'https://openalex.org/W151225349', 'https://openalex.org/W2157472241', 'https://openalex.org/W2012420230', 'https://openalex.org/W2026412924', 'https://openalex.org/W4249976102', 'https://openalex.org/W2023665871', 'https://openalex.org/W2074412215', 'https://openalex.org/W1483979448', 'https://openalex.org/W1993533823', 'https://openalex.org/W2026624208', 'https://openalex.org/W1591270397', 'https://openalex.org/W215246177', 'https://openalex.org/W2103427123', 'https://openalex.org/W2331161541', 'https://openalex.org/W2991206245', 'https://openalex.org/W2087471020', 'https://openalex.org/W2057007972', 'https://openalex.org/W98236428', 'https://openalex.org/W1976254890', 'https://openalex.org/W1990444693', 'https://openalex.org/W2043307825', 'https://openalex.org/W6629443427', 'https://openalex.org/W1985508383', 'https://openalex.org/W1870068760', 'https://openalex.org/W2150141420', 'https://openalex.org/W2109283233', 'https://openalex.org/W2277046234', 'https://openalex.org/W2028041302', 'https://openalex.org/W2260250847', 'https://openalex.org/W1991145419', 'https://openalex.org/W2009684083', 'https://openalex.org/W2045927451', 'https://openalex.org/W2004987968', 'https://openalex.org/W2061146451', 'https://openalex.org/W1901324376', 'https://openalex.org/W2015351372', 'https://openalex.org/W4248963734', 'https://openalex.org/W1987013655', 'https://openalex.org/W2165965469', 'https://openalex.org/W2096888562', 'https://openalex.org/W4229902639', 'https://openalex.org/W2098640524', 'https://openalex.org/W1967880430', 'https://openalex.org/W1989939224', 'https://openalex.org/W2089622760', 'https://openalex.org/W1575561329', 'https://openalex.org/W2016513823', 'https://openalex.org/W1971403308', 'https://openalex.org/W2042121819', 'https://openalex.org/W2013550964', 'https://openalex.org/W46991382', 'https://openalex.org/W1990052722', 'https://openalex.org/W1995109324', 'https://openalex.org/W1590943427', 'https://openalex.org/W133062379', 'https://openalex.org/W4300109986', 'https://openalex.org/W2000389164', 'https://openalex.org/W1979684510', 'https://openalex.org/W2080418969', 'https://openalex.org/W2051282970', 'https://openalex.org/W2989589988', 'https://openalex.org/W1978763267', 'https://openalex.org/W2495769271', 'https://openalex.org/W1970870910', 'https://openalex.org/W2027528117', 'https://openalex.org/W2496115906', 'https://openalex.org/W2546732126', 'https://openalex.org/W2170502024', 'https://openalex.org/W4256451564', 'https://openalex.org/W2039578166', 'https://openalex.org/W2007605886', 'https://openalex.org/W2107807958', 'https://openalex.org/W2988424802', 'https://openalex.org/W1979397773', 'https://openalex.org/W4252471910', 'https://openalex.org/W4247112366', 'https://openalex.org/W2070230103', 'https://openalex.org/W2990060995', 'https://openalex.org/W1984204680', 'https://openalex.org/W1592390473', 'https://openalex.org/W4236456408', 'https://openalex.org/W4232760665', 'https://openalex.org/W2131037588', 'https://openalex.org/W2012351235', 'https://openalex.org/W2031635048', 'https://openalex.org/W2153751372', 'https://openalex.org/W2040577647', 'https://openalex.org/W2014006248', 'https://openalex.org/W2123236744', 'https://openalex.org/W2089209953', 'https://openalex.org/W2141340689', 'https://openalex.org/W1974932989', 'https://openalex.org/W2030438891', 'https://openalex.org/W2336183687', 'https://openalex.org/W2023126417', 'https://openalex.org/W2787945480', 'https://openalex.org/W154455525', 'https://openalex.org/W2413458054', 'https://openalex.org/W2031180920', 'https://openalex.org/W6176620', 'https://openalex.org/W2990919124', 'https://openalex.org/W2019418195', 'https://openalex.org/W2069294353', 'https://openalex.org/W1598383147', 'https://openalex.org/W4234249713', 'https://openalex.org/W2080159468', 'https://openalex.org/W1996334758', 'https://openalex.org/W4300621359', 'https://openalex.org/W1975552354', 'https://openalex.org/W2158324645', 'https://openalex.org/W6781377879', 'https://openalex.org/W2088160102', 'https://openalex.org/W2083481299', 'https://openalex.org/W2127494401', 'https://openalex.org/W2101652647', 'https://openalex.org/W4300519141', 'https://openalex.org/W2046330369', 'https://openalex.org/W1544458575', 'https://openalex.org/W40237020', 'https://openalex.org/W2096063336', 'https://openalex.org/W2588501634', 'https://openalex.org/W1979604178', 'https://openalex.org/W2078634020', 'https://openalex.org/W2053834199', 'https://openalex.org/W2090823093', 'https://openalex.org/W2039679461', 'https://openalex.org/W2105126865', 'https://openalex.org/W4299554487', 'https://openalex.org/W161832529', 'https://openalex.org/W2127428811', 'https://openalex.org/W3109339239', 'https://openalex.org/W1999060431', 'https://openalex.org/W2026694978', 'https://openalex.org/W2035077048', 'https://openalex.org/W1993347555', 'https://openalex.org/W1497722890', 'https://openalex.org/W2053390551', 'https://openalex.org/W2313295579', 'https://openalex.org/W2043228903', 'https://openalex.org/W1990890535', 'https://openalex.org/W4245892695', 'https://openalex.org/W2293749826', 'https://openalex.org/W1586640266', 'https://openalex.org/W2059278234', 'https://openalex.org/W1545150940', 'https://openalex.org/W1504955803', 'https://openalex.org/W2398326651', 'https://openalex.org/W2102536875', 'https://openalex.org/W2750181026', 'https://openalex.org/W1493000049', 'https://openalex.org/W1508654892', 'https://openalex.org/W1496618919', 'https://openalex.org/W1678059399', 'https://openalex.org/W2013047673', 'https://openalex.org/W2022570962', 'https://openalex.org/W4285719527', 'https://openalex.org/W4237862223', 'https://openalex.org/W1557399841', 'https://openalex.org/W1941090968', 'https://openalex.org/W2335578026', 'https://openalex.org/W629186223', 'https://openalex.org/W1590744681', 'https://openalex.org/W1797231893', 'https://openalex.org/W1973208346', 'https://openalex.org/W2154898072', 'https://openalex.org/W1569301302', 'https://openalex.org/W627029773', 'https://openalex.org/W2081234301', 'https://openalex.org/W2047682080', 'https://openalex.org/W4300180226', 'https://openalex.org/W568075301', 'https://openalex.org/W2098311786', 'https://openalex.org/W1973120570', 'https://openalex.org/W1978399973', 'https://openalex.org/W1576754609', 'https://openalex.org/W2020789959', 'https://openalex.org/W3212193146', 'https://openalex.org/W2012628465', 'https://openalex.org/W2989594766', 'https://openalex.org/W652994534', 'https://openalex.org/W2120111134', 'https://openalex.org/W4302050943', 'https://openalex.org/W1821755143', 'https://openalex.org/W2065511730', 'https://openalex.org/W2026578629', 'https://openalex.org/W2611031990', 'https://openalex.org/W2511165258', 'https://openalex.org/W1972877235', 'https://openalex.org/W3140652750', 'https://openalex.org/W2046134482', 'https://openalex.org/W4212857348', 'https://openalex.org/W574777602', 'https://openalex.org/W2064317662', 'https://openalex.org/W2091387799', 'https://openalex.org/W1664883726', 'https://openalex.org/W1996216309', 'https://openalex.org/W1599877549', 'https://openalex.org/W1570601904', 'https://openalex.org/W2513412827', 'https://openalex.org/W3046685117', 'https://openalex.org/W607079072', 'https://openalex.org/W2130292292', 'https://openalex.org/W2795889579', 'https://openalex.org/W2939141192', 'https://openalex.org/W2054168569', 'https://openalex.org/W1585753929', 'https://openalex.org/W1840461988', 'https://openalex.org/W1788522982', 'https://openalex.org/W2097139554', 'https://openalex.org/W1979330310', 'https://openalex.org/W1971707725', 'https://openalex.org/W2616021816', 'https://openalex.org/W201955049', 'https://openalex.org/W656817896', 'https://openalex.org/W1521589712', 'https://openalex.org/W561813473', 'https://openalex.org/W2099132595', 'https://openalex.org/W4381925011', 'https://openalex.org/W2967426412', 'https://openalex.org/W2108081607', 'https://openalex.org/W2347340950', 'https://openalex.org/W2618996857', 'https://openalex.org/W2277905010', 'https://openalex.org/W2613058784', 'https://openalex.org/W2167702024', 'https://openalex.org/W2132796688', 'https://openalex.org/W2164641143', 'https://openalex.org/W1983440413', 'https://openalex.org/W2759521055']",1997-04-01
https://openalex.org/W3015805741,https://doi.org/10.1109/icassp40776.2020.9054734,F0-Consistent Many-To-Many Non-Parallel Voice Conversion Via Conditional Autoencoder,"Non-parallel many-to-many voice conversion remains an interesting but\nchallenging speech processing task. Many style-transfer-inspired methods such\nas generative adversarial networks (GANs) and variational autoencoders (VAEs)\nhave been proposed. Recently, AutoVC, a conditional autoencoders (CAEs) based\nmethod achieved state-of-the-art results by disentangling the speaker identity\nand speech content using information-constraining bottlenecks, and it achieves\nzero-shot conversion by swapping in a different speaker's identity embedding to\nsynthesize a new voice. However, we found that while speaker identity is\ndisentangled from speech content, a significant amount of prosodic information,\nsuch as source F0, leaks through the bottleneck, causing target F0 to fluctuate\nunnaturally. Furthermore, AutoVC has no control of the converted F0 and thus\nunsuitable for many applications. In the paper, we modified and improved\nautoencoder-based voice conversion to disentangle content, F0, and speaker\nidentity at the same time. Therefore, we can control the F0 contour, generate\nspeech with F0 consistent with the target speaker, and significantly improve\nquality and similarity. We support our improvement through quantitative and\nqualitative analysis.\n","['https://openalex.org/W2889329491', 'https://openalex.org/W2888932932', 'https://openalex.org/W2804998325', 'https://openalex.org/W6600438624', 'https://openalex.org/W2009396100', 'https://openalex.org/W6762533536', 'https://openalex.org/W2532494225', 'https://openalex.org/W2086796102', 'https://openalex.org/W2114659828', 'https://openalex.org/W1509691205', 'https://openalex.org/W2057609679', 'https://openalex.org/W2787748320', 'https://openalex.org/W2123003832', 'https://openalex.org/W2746474733', 'https://openalex.org/W2156142001', 'https://openalex.org/W2518312472', 'https://openalex.org/W2964243274', 'https://openalex.org/W2887264325', 'https://openalex.org/W2949281321', 'https://openalex.org/W10800834', 'https://openalex.org/W2608338293', 'https://openalex.org/W2519091744', 'https://openalex.org/W2964135678', 'https://openalex.org/W2945478979', 'https://openalex.org/W2805669069', 'https://openalex.org/W2964069186', 'https://openalex.org/W2788830260', 'https://openalex.org/W2527729766', 'https://openalex.org/W2963830550', 'https://openalex.org/W2962896155', 'https://openalex.org/W2889061305']",2020-04-09
https://openalex.org/W3205631867,https://doi.org/10.1109/icassp43922.2022.9747747,Fine-Grained Style Control In Transformer-Based Text-To-Speech Synthesis,"In this paper, we present a novel architecture to realize fine-grained style control on the transformer-based text-to-speech synthesis (TransformerTTS). Specifically, we model the speaking style by extracting a time sequence of local style tokens (LST) from the reference speech. The existing content encoder in TransformerTTS is then replaced by our designed cross-attention blocks for fusion and alignment between content and style. As the fusion is performed along with the skip connection, our cross-attention block provides a good inductive bias to gradually infuse the phoneme representation with a given style. Additionally, we prevent the style embedding from encoding linguistic content by randomly truncating LST during training and using wav2vec 2.0 features. Experiments show that with fine-grained style control, our system performs better in terms of naturalness, intelligibility, and style transferability. Our code and samples are publicly available. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>","['https://openalex.org/W3152136404', 'https://openalex.org/W6755207826', 'https://openalex.org/W6780218876', 'https://openalex.org/W3034837210', 'https://openalex.org/W3095545636', 'https://openalex.org/W3198609073', 'https://openalex.org/W2064675550', 'https://openalex.org/W6739901393', 'https://openalex.org/W6917585676', 'https://openalex.org/W6772349387', 'https://openalex.org/W2903739847', 'https://openalex.org/W3135644023', 'https://openalex.org/W2964243274', 'https://openalex.org/W3015212100', 'https://openalex.org/W6750489868', 'https://openalex.org/W2964138190', 'https://openalex.org/W2795109282', 'https://openalex.org/W6763832098', 'https://openalex.org/W2973158936', 'https://openalex.org/W6736996214', 'https://openalex.org/W3163573274', 'https://openalex.org/W6631190155', 'https://openalex.org/W2963300588', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962780374', 'https://openalex.org/W3160329778', 'https://openalex.org/W3198791321', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963927338', 'https://openalex.org/W4295731579', 'https://openalex.org/W2794490148', 'https://openalex.org/W1522301498', 'https://openalex.org/W2998572311', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963341956', 'https://openalex.org/W2946200149', 'https://openalex.org/W3033194228', 'https://openalex.org/W2970730223', 'https://openalex.org/W2896457183', 'https://openalex.org/W3099782249', 'https://openalex.org/W2963272440', 'https://openalex.org/W2964121744', 'https://openalex.org/W3036601975', 'https://openalex.org/W2963609956']",2022-04-27
https://openalex.org/W3096524539,https://doi.org/10.21437/interspeech.2020-1443,VQVC+: One-Shot Voice Conversion by Vector Quantization and U-Net Architecture,"Voice conversion (VC) is a task that transforms the source speaker's timbre, accent, and tones in audio into another one's while preserving the linguistic content.It is still a challenging work, especially in a one-shot setting.Auto-encoder-based VC methods disentangle the speaker and the content in input speech without given the speaker's identity, so these methods can further generalize to unseen speakers.The disentangle capability is achieved by vector quantization (VQ), adversarial training, or instance normalization (IN).However, the imperfect disentanglement may harm the quality of output speech.In this work, to further improve audio quality, we use the U-Net architecture within an auto-encoder-based VC system.We find that to leverage the U-Net architecture, a strong information bottleneck is necessary.The VQ-based method, which quantizes the latent vectors, can serve the purpose.The objective and the subjective evaluations show that the proposed method performs well in both audio naturalness and speaker similarity.","['https://openalex.org/W2972659941', 'https://openalex.org/W2532494225', 'https://openalex.org/W2970006822', 'https://openalex.org/W1901129140', 'https://openalex.org/W4294643831', 'https://openalex.org/W2973154337', 'https://openalex.org/W2902070858', 'https://openalex.org/W2518172956', 'https://openalex.org/W2603777577', 'https://openalex.org/W2963539064', 'https://openalex.org/W2156142001', 'https://openalex.org/W4288337064', 'https://openalex.org/W2947445680', 'https://openalex.org/W2945478979', 'https://openalex.org/W3114301328', 'https://openalex.org/W2120605154', 'https://openalex.org/W3125709657', 'https://openalex.org/W2963830550', 'https://openalex.org/W2502312327', 'https://openalex.org/W2948211236', 'https://openalex.org/W2527729766', 'https://openalex.org/W3015434413', 'https://openalex.org/W2963799213', 'https://openalex.org/W2105160541', 'https://openalex.org/W2888922217', 'https://openalex.org/W3015419784', 'https://openalex.org/W3015805741', 'https://openalex.org/W4320013936']",2020-10-25
https://openalex.org/W4367841185,https://doi.org/10.18653/v1/2022.emnlp-main.391,T-Modules: Translation Modules for Zero-Shot Cross-Modal Machine Translation,"We present a new approach to perform zero-shot cross-modal transfer between speech and text for translation tasks. Multilingual speech and text are encoded in a joint fixed-size representation space. Then, we compare different approaches to decode these multimodal and multilingual fixed-size representations, enabling zero-shot translation between languages and modalities. All our models are trained without the need of cross-modal labeled translation data.Despite a fixed-size representation, we achieve very competitive results on several text and speech translation tasks. In particular, we significantly improve the state-of-the-art for zero-shot speech translation on Must-C. Incorporating a speech decoder in our framework, we introduce the first results for zero-shot direct speech-to-speech and text-to-speech translation.","['https://openalex.org/W4221145109', 'https://openalex.org/W2989539713', 'https://openalex.org/W3176382501', 'https://openalex.org/W4221155340', 'https://openalex.org/W3186843219', 'https://openalex.org/W3213018012', 'https://openalex.org/W4300963525', 'https://openalex.org/W2971152344', 'https://openalex.org/W4224931728', 'https://openalex.org/W3036601975', 'https://openalex.org/W4385574194', 'https://openalex.org/W2914120296', 'https://openalex.org/W3155915431', 'https://openalex.org/W3119308075', 'https://openalex.org/W2973088264', 'https://openalex.org/W2962778428', 'https://openalex.org/W3180374548', 'https://openalex.org/W2972495969', 'https://openalex.org/W3097301532', 'https://openalex.org/W3213029956', 'https://openalex.org/W3043665049', 'https://openalex.org/W3046226013', 'https://openalex.org/W4287774231', 'https://openalex.org/W4287854499', 'https://openalex.org/W2949328740', 'https://openalex.org/W3197771105', 'https://openalex.org/W2921280978', 'https://openalex.org/W3105825505', 'https://openalex.org/W3113908264', 'https://openalex.org/W2605131327', 'https://openalex.org/W4287079280', 'https://openalex.org/W3173767661', 'https://openalex.org/W3100806282', 'https://openalex.org/W2970641574', 'https://openalex.org/W3093871477', 'https://openalex.org/W4285216155', 'https://openalex.org/W3175301726', 'https://openalex.org/W4287328841', 'https://openalex.org/W4299574851', 'https://openalex.org/W4226102207', 'https://openalex.org/W4226513777', 'https://openalex.org/W2899015110', 'https://openalex.org/W3198429080', 'https://openalex.org/W3035016936', 'https://openalex.org/W2593011301']",2022-01-01
https://openalex.org/W3169483174,https://doi.org/10.18653/v1/2021.naacl-main.41,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,"Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.","['https://openalex.org/W3105220303', 'https://openalex.org/W3039017601', 'https://openalex.org/W3126822054', 'https://openalex.org/W3098637735', 'https://openalex.org/W2963748441', 'https://openalex.org/W3032816972', 'https://openalex.org/W2965373594', 'https://openalex.org/W3034999214', 'https://openalex.org/W3023133114', 'https://openalex.org/W2970752815', 'https://openalex.org/W3040245432', 'https://openalex.org/W2989539713', 'https://openalex.org/W3013840636', 'https://openalex.org/W2963026768', 'https://openalex.org/W3085479580', 'https://openalex.org/W4287727940', 'https://openalex.org/W4389016403', 'https://openalex.org/W3006439205', 'https://openalex.org/W2996580882', 'https://openalex.org/W3082274269', 'https://openalex.org/W3045462440', 'https://openalex.org/W2999168658', 'https://openalex.org/W4287687023', 'https://openalex.org/W3102659883', 'https://openalex.org/W3035390927', 'https://openalex.org/W4293350112', 'https://openalex.org/W3105721709', 'https://openalex.org/W2742113707', 'https://openalex.org/W2963341956', 'https://openalex.org/W4287633642', 'https://openalex.org/W3107826490', 'https://openalex.org/W3035497479', 'https://openalex.org/W2891555348', 'https://openalex.org/W2914120296', 'https://openalex.org/W2963979492', 'https://openalex.org/W2963250244', 'https://openalex.org/W3103187652', 'https://openalex.org/W3032532958', 'https://openalex.org/W3037854022', 'https://openalex.org/W2958953787', 'https://openalex.org/W3034238904', 'https://openalex.org/W2809324505', 'https://openalex.org/W3023690688', 'https://openalex.org/W3116343068', 'https://openalex.org/W2990188683', 'https://openalex.org/W2986154550', 'https://openalex.org/W3042711927', 'https://openalex.org/W4287760320', 'https://openalex.org/W2963403868', 'https://openalex.org/W3099655892', 'https://openalex.org/W3081210419', 'https://openalex.org/W3034469191', 'https://openalex.org/W4301581299', 'https://openalex.org/W3100107515', 'https://openalex.org/W2953958347', 'https://openalex.org/W3097879195', 'https://openalex.org/W4385245566', 'https://openalex.org/W2940024477', 'https://openalex.org/W3156789018']",2021-01-01
https://openalex.org/W4315705838,https://doi.org/10.48550/arxiv.2301.03728,Scaling Laws for Generative Mixed-Modal Language Models,"Generative language models define distributions over sequences of tokens that can represent essentially any combination of data modalities (e.g., any permutation of image tokens from VQ-VAEs, speech tokens from HuBERT, BPE tokens for language or code, and so on). To better understand the scaling properties of such mixed-modal models, we conducted over 250 experiments using seven different modalities and model sizes ranging from 8 million to 30 billion, trained on 5-100 billion tokens. We report new mixed-modal scaling laws that unify the contributions of individual modalities and the interactions between them. Specifically, we explicitly model the optimal synergy and competition due to data and model size as an additive term to previous uni-modal scaling laws. We also find four empirical phenomena observed during the training, such as emergent coordinate-ascent style training that naturally alternates between modalities, guidelines for selecting critical hyper-parameters, and connections between mixed-modal competition and training stability. Finally, we test our scaling law by training a 30B speech-text model, which significantly outperforms the corresponding unimodal models. Overall, our research provides valuable insights into the design and training of mixed-modal generative models, an important new class of unified models that have unique distributional properties.",[],2023-01-10
https://openalex.org/W4312052802,https://doi.org/10.48550/arxiv.2212.09553,"Mu$^{2}$SLAM: Multitask, Multilingual Speech and Language Models","We present Mu$^{2}$SLAM, a multilingual sequence-to-sequence model pre-trained jointly on unlabeled speech, unlabeled text and supervised data spanning Automatic Speech Recognition (ASR), Automatic Speech Translation (AST) and Machine Translation (MT), in over 100 languages. By leveraging a quantized representation of speech as a target, Mu$^{2}$SLAM trains the speech-text models with a sequence-to-sequence masked denoising objective similar to T5 on the decoder and a masked language modeling (MLM) objective on the encoder, for both unlabeled speech and text, while utilizing the supervised tasks to improve cross-lingual and cross-modal representation alignment within the model. On CoVoST AST, Mu$^{2}$SLAM establishes a new state-of-the-art for models trained on public datasets, improving on xx-en translation over the previous best by 1.9 BLEU points and on en-xx translation by 1.1 BLEU points. On Voxpopuli ASR, our model matches the performance of an mSLAM model fine-tuned with an RNN-T decoder, despite using a relatively weaker sequence-to-sequence architecture. On text understanding tasks, our model improves by more than 6\% over mSLAM on XNLI, getting closer to the performance of mT5 models of comparable capacity on XNLI and TydiQA, paving the way towards a single model for all speech and text understanding tasks.",[],2022-12-19
https://openalex.org/W4312121834,https://doi.org/10.48550/arxiv.2212.11377,ReVISE: Self-Supervised Speech Resynthesis with Visual Input for Universal and Generalized Speech Enhancement,"Prior works on improving speech quality with visual input typically study each type of auditory distortion separately (e.g., separation, inpainting, video-to-speech) and present tailored algorithms. This paper proposes to unify these subjects and study Generalized Speech Enhancement, where the goal is not to reconstruct the exact reference clean signal, but to focus on improving certain aspects of speech. In particular, this paper concerns intelligibility, quality, and video synchronization. We cast the problem as audio-visual speech resynthesis, which is composed of two steps: pseudo audio-visual speech recognition (P-AVSR) and pseudo text-to-speech synthesis (P-TTS). P-AVSR and P-TTS are connected by discrete units derived from a self-supervised speech model. Moreover, we utilize self-supervised audio-visual speech model to initialize P-AVSR. The proposed model is coined ReVISE. ReVISE is the first high-quality model for in-the-wild video-to-speech synthesis and achieves superior performance on all LRS3 audio-visual enhancement tasks with a single model. To demonstrates its applicability in the real world, ReVISE is also evaluated on EasyCom, an audio-visual benchmark collected under challenging acoustic conditions with only 1.6 hours of training data. Similarly, ReVISE greatly suppresses noise and improves quality. Project page: https://wnhsu.github.io/ReVISE.",[],2022-12-21
https://openalex.org/W4310638188,https://doi.org/10.48550/arxiv.2203.04911,DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering,"Spoken Question Answering (SQA) is to find the answer from a spoken document given a question, which is crucial for personal assistants when replying to the queries from the users. Existing SQA methods all rely on Automatic Speech Recognition (ASR) transcripts. Not only does ASR need to be trained with massive annotated data that are time and cost-prohibitive to collect for low-resourced languages, but more importantly, very often the answers to the questions include name entities or out-of-vocabulary words that cannot be recognized correctly. Also, ASR aims to minimize recognition errors equally over all words, including many function words irrelevant to the SQA task. Therefore, SQA without ASR transcripts (textless) is always highly desired, although known to be very difficult. This work proposes Discrete Spoken Unit Adaptive Learning (DUAL), leveraging unlabeled data for pre-training and fine-tuned by the SQA downstream task. The time intervals of spoken answers can be directly predicted from spoken documents. We also release a new SQA benchmark corpus, NMSQA, for data with more realistic scenarios. We empirically showed that DUAL yields results comparable to those obtained by cascading ASR and text QA model and robust to real-world data. Our code and model will be open-sourced.",[],2022-03-09
https://openalex.org/W4281758439,https://doi.org/10.48550/arxiv.2205.14135,FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness,"Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).",[],2022-05-27
https://openalex.org/W4392904245,https://doi.org/10.1109/icassp48485.2024.10445879,TextrolSpeech: A Text Style Control Speech Corpus with Codec Language Text-to-Speech Models,"Recently, there has been a growing interest in the field of controllable Text-to-Speech (TTS). While previous studies have relied on users providing specific style factor values based on acoustic knowledge or selecting reference speeches that meet certain requirements, generating speech solely from natural text prompts has emerged as a new challenge for researchers. This challenge arises due to the scarcity of high-quality speech datasets with natural text style prompt and the absence of advanced text-controllable TTS models. In light of this, 1) we propose TextrolSpeech, which is the first large-scale speech emotion dataset annotated with rich text attributes. The dataset comprises 236,203 pairs of style prompt in natural text descriptions with five style factors and corresponding speech samples. Through iterative experimentation, we introduce a multi-stage prompt programming approach that effectively utilizes the GPT model for generating natural style descriptions in large volumes. 2) Furthermore, to address the need for generating audio with greater style diversity, we propose an efficient architecture called Salle. This architecture treats text controllable TTS as a language model task, utilizing audio codec codes as an intermediate representation to replace the conventional mel-spectrogram. Finally, we successfully demonstrate the ability of the proposed model by showing a comparable performance in the controllable TTS task. Audio samples are available on the demo page https://sall-e.github.io/.","['https://openalex.org/W6778823374', 'https://openalex.org/W6848735303', 'https://openalex.org/W6852870047', 'https://openalex.org/W4385822534', 'https://openalex.org/W3094650042', 'https://openalex.org/W3197541421', 'https://openalex.org/W6749555683', 'https://openalex.org/W4375869257', 'https://openalex.org/W4307323391', 'https://openalex.org/W4385822787', 'https://openalex.org/W4398152753', 'https://openalex.org/W2972359262', 'https://openalex.org/W2998572311', 'https://openalex.org/W3163573274', 'https://openalex.org/W3099284785', 'https://openalex.org/W2981437154', 'https://openalex.org/W2471520273', 'https://openalex.org/W3160638507', 'https://openalex.org/W6803567076', 'https://openalex.org/W4313679638', 'https://openalex.org/W3033411150', 'https://openalex.org/W4379924545', 'https://openalex.org/W4307106676']",2024-03-18
https://openalex.org/W4402703639,https://doi.org/10.1145/3664647.3681674,SpeechCraft: A Fine-Grained Expressive Speech Dataset with Natural Language Description,"Speech-language multi-modal learning presents a significant challenge due to\nthe fine nuanced information inherent in speech styles. Therefore, a\nlarge-scale dataset providing elaborate comprehension of speech style is\nurgently needed to facilitate insightful interplay between speech audio and\nnatural language. However, constructing such datasets presents a major\ntrade-off between large-scale data collection and high-quality annotation. To\ntackle this challenge, we propose an automatic speech annotation system for\nexpressiveness interpretation that annotates in-the-wild speech clips with\nexpressive and vivid human language descriptions. Initially, speech audios are\nprocessed by a series of expert classifiers and captioning models to capture\ndiverse speech characteristics, followed by a fine-tuned LLaMA for customized\nannotation generation. Unlike previous tag/templet-based annotation frameworks\nwith limited information and diversity, our system provides in-depth\nunderstandings of speech style through tailored natural language descriptions,\nthereby enabling accurate and voluminous data generation for large model\ntraining. With this system, we create SpeechCraft, a fine-grained bilingual\nexpressive speech dataset. It is distinguished by highly descriptive natural\nlanguage style prompts, containing approximately 2,000 hours of audio data and\nencompassing over two million speech clips. Extensive experiments demonstrate\nthat the proposed dataset significantly boosts speech-language task performance\nin stylist speech synthesis and speech style understanding.\n","['https://openalex.org/W3033907960', 'https://openalex.org/W3015591594', 'https://openalex.org/W4393152865', 'https://openalex.org/W4375869257', 'https://openalex.org/W3176445421', 'https://openalex.org/W4392904245', 'https://openalex.org/W4210913346', 'https://openalex.org/W2354870669', 'https://openalex.org/W4311000453', 'https://openalex.org/W2964243274', 'https://openalex.org/W6600741150', 'https://openalex.org/W4372260310', 'https://openalex.org/W4376226279', 'https://openalex.org/W4393147046', 'https://openalex.org/W2576683119', 'https://openalex.org/W3103314642', 'https://openalex.org/W2999905431']",2024-10-26
https://openalex.org/W4388233464,https://doi.org/10.1145/3581783.3612485,"SpeechTripleNet: End-to-End Disentangled Speech Representation Learning for Content, Timbre and Prosody","Disentangled speech representation learning aims to separate different factors of variation from speech into disjoint representations. This paper focuses on disentangling speech into representations for three factors: spoken content, speaker timbre, and speech prosody. Many previous methods for speech disentanglement have focused on separating spoken content and speaker timbre. However, the lack of explicit modeling of prosodic information leads to degraded speech generation performance and uncontrollable prosody leakage into content and/or speaker representations. While some recent methods have utilized explicit speaker labels or pre-trained models to facilitate triple-factor disentanglement, there are no end-to-end methods to simultaneously disentangle three factors using only unsupervised or self-supervised learning objectives. This paper introduces SpeechTripleNet, an end-to-end method to disentangle speech into representations for content, timbre, and prosody. Based on VAE, SpeechTripleNet restricts the structures of the latent variables and the amount of information captured in them to induce disentanglement. It is a pure unsupervised/self-supervised learning method that only requires speech data and no additional labels. Our qualitative and quantitative results demonstrate that SpeechTripleNet is effective in achieving triple-factor speech disentanglement, as well as controllable speech editing concerning different factors.","['https://openalex.org/W2753738274', 'https://openalex.org/W3092028330', 'https://openalex.org/W1735538598', 'https://openalex.org/W4319862209', 'https://openalex.org/W4312732823', 'https://openalex.org/W3015434413', 'https://openalex.org/W2904459034', 'https://openalex.org/W2747329762', 'https://openalex.org/W2626699878', 'https://openalex.org/W2560674852']",2023-10-26
https://openalex.org/W4392908903,https://doi.org/10.1109/icassp48485.2024.10448173,PromptTTS++: Controlling Speaker Identity in Prompt-Based Text-To-Speech Using Natural Language Descriptions,"We propose PromptTTS++, a prompt-based text-to-speech (TTS) synthesis system that allows control over speaker identity using natural language descriptions. To control speaker identity within the prompt-based TTS framework, we introduce the concept of speaker prompt, which describes voice characteristics (e.g., gender-neutral, young, old, and muffled) designed to be approximately independent of speaking style. Since there is no large-scale dataset containing speaker prompts, we first construct a dataset based on the LibriTTS-R corpus with manually annotated speaker prompts. We then employ a diffusion-based acoustic model with mixture density networks to model diverse speaker factors in the training data. Unlike previous studies that rely on style prompts describing only a limited aspect of speaker individuality, such as pitch, speaking speed, and energy, our method utilizes an additional speaker prompt to effectively learn the mapping from natural language descriptions to the acoustic features of diverse speakers. Our subjective evaluation results show that the proposed method can better control speaker characteristics than the methods without the speaker prompt. Audio samples are available at https://reppy4620.github.io/demo.promptttspp/.","['https://openalex.org/W2964243274', 'https://openalex.org/W6796730497', 'https://openalex.org/W4375869257', 'https://openalex.org/W4398152753', 'https://openalex.org/W4385822787', 'https://openalex.org/W6778883912', 'https://openalex.org/W6810738896', 'https://openalex.org/W6854866820', 'https://openalex.org/W6634817459', 'https://openalex.org/W6750489868', 'https://openalex.org/W4224929761', 'https://openalex.org/W4385823191', 'https://openalex.org/W6755207826', 'https://openalex.org/W3097777922', 'https://openalex.org/W6778823374', 'https://openalex.org/W3158762648', 'https://openalex.org/W6779823529', 'https://openalex.org/W3216941316', 'https://openalex.org/W4386764866', 'https://openalex.org/W2972359262', 'https://openalex.org/W2747874407', 'https://openalex.org/W2471520273', 'https://openalex.org/W2029434926', 'https://openalex.org/W6637242042', 'https://openalex.org/W6757817989', 'https://openalex.org/W4385245566', 'https://openalex.org/W6838843145', 'https://openalex.org/W2990440871', 'https://openalex.org/W4384918448', 'https://openalex.org/W1579853615', 'https://openalex.org/W3174758275']",2024-03-18
https://openalex.org/W1583837637,https://doi.org/10.1145/1273496,Proceedings of the 24th international conference on Machine learning,"This volume contains the papers accepted to the 24th International Conference on Machine Learning (ICML 2007), which was held at Oregon State University in Corvalis, Oregon, from June 20th to 24th, 2007. ICML is the annual conference of the International Machine Learning Society (IMLS), and provides a venue for the presentation and discussion of current research in the field of machine learning. These proceedings can also be found online at: http://www.machinelearning.org. This year there were 522 submissions to ICML. There was a very thorough review process, in which each paper was reviewed by three program committee (PC) members. Authors were able to respond to the initial reviews, and the PC members could then modify their reviews based on online discussions and the content of this author response. For the first time this year there were two discussion periods led by the senior program committee (SPC), one just before and one after the submission of author responses. At the end of the second discussion period, the SPC members gave their recommendations and provided a summary review for each of their papers. Also for the first time, authors were asked to submit a list of changes with their final accepted papers, which was checked by the SPCs to ensure that reviewer comments had been addressed. Apart from the length restrictions on papers and the compressed time frame, the review process for ICML resembles that of many journal publications. In total, 150 papers were accepted to ICML this year, including a very small number of papers which were initially conditionally accepted, yielding an overall acceptance rate of 29%. ICML attracts submissions from machine learning researchers around the globe. The 150 accepted papers this year were geographically distributed as follows: 66 papers had a first author from the US, 32 from Europe, 19 from China or Hong Kong, 11 from Canada, 6 from India, 5 each from Australia and Japan, 3 from Israel, and 1 each from Korea, Russia and Taiwan. In addition to the main program of accepted papers, which includes both a talk and poster presentation for each paper, the ICML program included 3 workshops and 8 tutorials on machine learning topics which are currently of broad interest. We were also extremely pleased to have David Heckerman (Microsoft Research), Joshua Tenenbaum (Massachussetts Institute of Technology), and Bernhard Schölkopf (Max Planck Institute for Biological Cybernetics) as the invited speakers this year. Thanks to sponsorship by the Machine Learning Journal, we were able to award a number of outstanding student paper prizes. We were fortunate this year that ICML was co-located with the International Conference on Inductive Logic Programming (ILP 2007). ICML and ILP held joint sessions on the first day of ICML 2007.",[],2007-06-20
https://openalex.org/W4287890956,https://doi.org/10.18653/v1/2022.naacl-main.376,Cross-modal Contrastive Learning for Speech Translation,"How can we learn unified representations for spoken utterances and their written text? Learning similar representations for semantically similar speech and text is important for speech translation. To this end, we propose ConST, a cross-modal contrastive learning method for end-to-end speech-to-text translation. We evaluate ConST and a variety of previous baselines on a popular benchmark MuST-C. Experiments show that the proposed ConST consistently outperforms the previous methods, and achieves an average BLEU of 29.4. The analysis further verifies that ConST indeed closes the representation gap of different modalities — its learned representation improves the accuracy of cross-modal speech-text retrieval from 4% to 88%. Code and models are available at https://github.com/ReneeYe/ConST.","['https://openalex.org/W2933138175', 'https://openalex.org/W2939710050', 'https://openalex.org/W4223622550', 'https://openalex.org/W4287547182', 'https://openalex.org/W4223490341', 'https://openalex.org/W3196790170', 'https://openalex.org/W4221163209', 'https://openalex.org/W3034571331', 'https://openalex.org/W2512924740', 'https://openalex.org/W22168010', 'https://openalex.org/W3176967746', 'https://openalex.org/W3107826490', 'https://openalex.org/W1494198834', 'https://openalex.org/W2972894903', 'https://openalex.org/W3037217258', 'https://openalex.org/W2555897561', 'https://openalex.org/W3017454464', 'https://openalex.org/W3186200218', 'https://openalex.org/W3035252911', 'https://openalex.org/W2152790380', 'https://openalex.org/W3175809709', 'https://openalex.org/W4287629556', 'https://openalex.org/W3176382501', 'https://openalex.org/W3089659770', 'https://openalex.org/W3118578889', 'https://openalex.org/W3162471442', 'https://openalex.org/W3036601975', 'https://openalex.org/W2419539795', 'https://openalex.org/W3005680577', 'https://openalex.org/W3172698324', 'https://openalex.org/W2118020555', 'https://openalex.org/W3156636935', 'https://openalex.org/W4385245566', 'https://openalex.org/W4294619240', 'https://openalex.org/W3168212167', 'https://openalex.org/W2963259843', 'https://openalex.org/W2187089797', 'https://openalex.org/W3037625699', 'https://openalex.org/W3113908264', 'https://openalex.org/W3198526264', 'https://openalex.org/W2973049979', 'https://openalex.org/W4256701369', 'https://openalex.org/W3176711365', 'https://openalex.org/W3105825505', 'https://openalex.org/W3176455679', 'https://openalex.org/W2949328740', 'https://openalex.org/W2157364932', 'https://openalex.org/W3199926081', 'https://openalex.org/W2997591391', 'https://openalex.org/W1689711448', 'https://openalex.org/W4249573750', 'https://openalex.org/W3175362188', 'https://openalex.org/W4297808394', 'https://openalex.org/W4287812705', 'https://openalex.org/W3097787369', 'https://openalex.org/W2914120296', 'https://openalex.org/W3090449556', 'https://openalex.org/W3015325583', 'https://openalex.org/W2892009249', 'https://openalex.org/W2986963494', 'https://openalex.org/W2964172053', 'https://openalex.org/W3174446152', 'https://openalex.org/W3162037819', 'https://openalex.org/W3035060554', 'https://openalex.org/W2966715458', 'https://openalex.org/W2963250244', 'https://openalex.org/W4287329822', 'https://openalex.org/W2945260553', 'https://openalex.org/W3095706145', 'https://openalex.org/W4287213456', 'https://openalex.org/W3113676066', 'https://openalex.org/W3173190788', 'https://openalex.org/W3099206234', 'https://openalex.org/W2982157312', 'https://openalex.org/W2963532001', 'https://openalex.org/W3096490862', 'https://openalex.org/W4300558631', 'https://openalex.org/W2758950307', 'https://openalex.org/W2964187781', 'https://openalex.org/W4288817190']",2022-01-01
https://openalex.org/W4221163209,https://doi.org/10.18653/v1/2022.acl-long.486,STEMM: Self-learning with Speech-text Manifold Mixup for Speech Translation,"How to learn a better speech representation for end-to-end speech-to-text translation (ST) with limited labeled data? Existing techniques often attempt to transfer powerful machine translation (MT) capabilities to ST, but neglect the representation discrepancy across modalities. In this paper, we propose the Speech-TExt Manifold Mixup (STEMM) method to calibrate such discrepancy. Specifically, we mix up the representation sequences of different modalities, and take both unimodal speech sequences and multimodal mixed sequences as input to the translation model in parallel, and regularize their output predictions with a self-learning framework. Experiments on MuST-C speech translation benchmark and further analysis show that our method effectively alleviates the cross-modal representation discrepancy, and achieves significant improvements over a strong baseline on eight translation directions.","['https://openalex.org/W2933138175', 'https://openalex.org/W3092424727', 'https://openalex.org/W2765407302', 'https://openalex.org/W3168212167', 'https://openalex.org/W3113676066', 'https://openalex.org/W222053410', 'https://openalex.org/W3034571331', 'https://openalex.org/W2889013998', 'https://openalex.org/W2945700568', 'https://openalex.org/W2972448360', 'https://openalex.org/W4394649814', 'https://openalex.org/W4287329822', 'https://openalex.org/W4287200076', 'https://openalex.org/W2899773543', 'https://openalex.org/W3037217258', 'https://openalex.org/W3115267989', 'https://openalex.org/W2997436923', 'https://openalex.org/W3035542229', 'https://openalex.org/W4285115391', 'https://openalex.org/W3160950270', 'https://openalex.org/W2914120296', 'https://openalex.org/W2951635603', 'https://openalex.org/W2978099976', 'https://openalex.org/W4287547182', 'https://openalex.org/W2419539795', 'https://openalex.org/W2952079278', 'https://openalex.org/W3036601975', 'https://openalex.org/W2901607128', 'https://openalex.org/W2964345285', 'https://openalex.org/W3176455679', 'https://openalex.org/W3107826490', 'https://openalex.org/W4288795384', 'https://openalex.org/W2963532001', 'https://openalex.org/W2605131327', 'https://openalex.org/W4288021071', 'https://openalex.org/W2466918907', 'https://openalex.org/W3173171878', 'https://openalex.org/W2986963494', 'https://openalex.org/W3007142233', 'https://openalex.org/W3163839574', 'https://openalex.org/W3102811925', 'https://openalex.org/W4205373766', 'https://openalex.org/W3113908264', 'https://openalex.org/W3162037819', 'https://openalex.org/W2964161387', 'https://openalex.org/W4300833946', 'https://openalex.org/W3097301532', 'https://openalex.org/W2945286432', 'https://openalex.org/W3106321930', 'https://openalex.org/W3196824004', 'https://openalex.org/W3176711365', 'https://openalex.org/W2964048171', 'https://openalex.org/W4385245566', 'https://openalex.org/W2997591391', 'https://openalex.org/W1522301498', 'https://openalex.org/W1494198834', 'https://openalex.org/W4221155857', 'https://openalex.org/W2945260553', 'https://openalex.org/W2964172053', 'https://openalex.org/W3176382501', 'https://openalex.org/W3105825505', 'https://openalex.org/W4280647381', 'https://openalex.org/W3037542581', 'https://openalex.org/W3105669983', 'https://openalex.org/W2605202026', 'https://openalex.org/W2904571617', 'https://openalex.org/W2101105183', 'https://openalex.org/W3163093788', 'https://openalex.org/W4300558631', 'https://openalex.org/W2949328740', 'https://openalex.org/W3006988520']",2022-01-01
https://openalex.org/W4385570757,https://doi.org/10.18653/v1/2023.acl-long.216,WACO: Word-Aligned Contrastive Learning for Speech Translation,"End-to-end Speech Translation (E2E ST) aims to directly translate source speech into target text. Existing ST methods perform poorly when only extremely small speech-text data are available for training. We observe that an ST model's performance closely correlates with its embedding similarity between speech and source transcript. In this paper, we propose Word-Aligned COntrastive learning (WACO), a simple and effective method for extremely low-resource speech-to-text translation. Our key idea is bridging word-level representations for both speech and text modalities via contrastive learning. We evaluate WACO and other methods on the MuST-C dataset, a widely used ST benchmark, and on a low-resource direction Maltese-English from IWSLT 2023. Our experiments demonstrate that WACO outperforms the best baseline by 9+ BLEU points with only 1-hour parallel ST data. Code is available at https://github.com/owaski/WACO.","['https://openalex.org/W1494198834', 'https://openalex.org/W3092085609', 'https://openalex.org/W3037542581', 'https://openalex.org/W2963532001', 'https://openalex.org/W4287890956', 'https://openalex.org/W4288817190', 'https://openalex.org/W4308349017', 'https://openalex.org/W3176711365', 'https://openalex.org/W4385573012', 'https://openalex.org/W2964161387', 'https://openalex.org/W3034571331', 'https://openalex.org/W4385245566', 'https://openalex.org/W3113676066', 'https://openalex.org/W3096490862', 'https://openalex.org/W4281982771', 'https://openalex.org/W3196509775', 'https://openalex.org/W4287329822', 'https://openalex.org/W4223622550', 'https://openalex.org/W4385573113', 'https://openalex.org/W2466918907', 'https://openalex.org/W4385572710', 'https://openalex.org/W2964172053', 'https://openalex.org/W3113908264', 'https://openalex.org/W2963736842', 'https://openalex.org/W2949328740', 'https://openalex.org/W3207222250', 'https://openalex.org/W4285077564', 'https://openalex.org/W4221163209', 'https://openalex.org/W3162037819', 'https://openalex.org/W3176382501', 'https://openalex.org/W3036601975', 'https://openalex.org/W2963250244', 'https://openalex.org/W2555897561', 'https://openalex.org/W2512924740', 'https://openalex.org/W2605131327', 'https://openalex.org/W1522301498', 'https://openalex.org/W3097787369', 'https://openalex.org/W4300558631', 'https://openalex.org/W3176455679', 'https://openalex.org/W2936774411', 'https://openalex.org/W3105825505', 'https://openalex.org/W3097301532', 'https://openalex.org/W3030437843', 'https://openalex.org/W3015440307']",2023-01-01
https://openalex.org/W4391021666,https://doi.org/10.1109/asru57964.2023.10389705,On Decoder-Only Architecture For Speech-to-Text and Large Language Model Integration,"Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The ""decoder-only"" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baselines, highlighting the potential advantages of decoder-only models for speech-to-text conversion.","['https://openalex.org/W6778883912', 'https://openalex.org/W6810081322', 'https://openalex.org/W6850625674', 'https://openalex.org/W6739901393', 'https://openalex.org/W6851847159', 'https://openalex.org/W6851513886', 'https://openalex.org/W6852326057', 'https://openalex.org/W6852818750', 'https://openalex.org/W3211278025', 'https://openalex.org/W6851950068', 'https://openalex.org/W6851592950', 'https://openalex.org/W6852489829', 'https://openalex.org/W6810334672', 'https://openalex.org/W6853611000', 'https://openalex.org/W4389524500', 'https://openalex.org/W6853998256', 'https://openalex.org/W6848735303', 'https://openalex.org/W6850334629', 'https://openalex.org/W6851317108', 'https://openalex.org/W3153583341', 'https://openalex.org/W6796581206', 'https://openalex.org/W2901607128', 'https://openalex.org/W3034625919', 'https://openalex.org/W6846175339', 'https://openalex.org/W4224137820', 'https://openalex.org/W4385823495', 'https://openalex.org/W6780680273', 'https://openalex.org/W2963532001', 'https://openalex.org/W6769627184', 'https://openalex.org/W6732953234', 'https://openalex.org/W2605131327', 'https://openalex.org/W6847363464', 'https://openalex.org/W2739883972', 'https://openalex.org/W6773820404', 'https://openalex.org/W4388979610', 'https://openalex.org/W6757817989', 'https://openalex.org/W2963250244', 'https://openalex.org/W6854218657', 'https://openalex.org/W4366330503', 'https://openalex.org/W4393178509', 'https://openalex.org/W3168867926', 'https://openalex.org/W4391021781', 'https://openalex.org/W4361866031', 'https://openalex.org/W4323651091', 'https://openalex.org/W4292779060', 'https://openalex.org/W4381827575', 'https://openalex.org/W4301581299', 'https://openalex.org/W4375958083', 'https://openalex.org/W4322718191', 'https://openalex.org/W4288089799', 'https://openalex.org/W2908510526', 'https://openalex.org/W4385245566', 'https://openalex.org/W4367628410', 'https://openalex.org/W4391021457', 'https://openalex.org/W4377372369', 'https://openalex.org/W4224308101', 'https://openalex.org/W4313679638', 'https://openalex.org/W4225323055', 'https://openalex.org/W4366850747', 'https://openalex.org/W4364382977', 'https://openalex.org/W3043665049', 'https://openalex.org/W4378501656']",2023-12-16
https://openalex.org/W2512924740,https://doi.org/10.18653/v1/w16-2301,Findings of the 2016 Conference on Machine Translation,"Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, Marcos Zampieri. Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers. 2016.","['https://openalex.org/W2578839743', 'https://openalex.org/W2518337630', 'https://openalex.org/W2577027179', 'https://openalex.org/W2509089644', 'https://openalex.org/W2098507980', 'https://openalex.org/W2510035577', 'https://openalex.org/W2508963464', 'https://openalex.org/W2576212274', 'https://openalex.org/W2251667544', 'https://openalex.org/W2131698806', 'https://openalex.org/W2087735403', 'https://openalex.org/W2251740709', 'https://openalex.org/W2512232685', 'https://openalex.org/W2134800885', 'https://openalex.org/W2513752566', 'https://openalex.org/W2261755308', 'https://openalex.org/W2139940533', 'https://openalex.org/W2575814162', 'https://openalex.org/W2916864519', 'https://openalex.org/W2514633390', 'https://openalex.org/W2508117065', 'https://openalex.org/W2257408573', 'https://openalex.org/W2080373976', 'https://openalex.org/W2510492408', 'https://openalex.org/W2517196708', 'https://openalex.org/W2250812345', 'https://openalex.org/W2513799748', 'https://openalex.org/W2895810819', 'https://openalex.org/W2514110159', 'https://openalex.org/W2251397648', 'https://openalex.org/W2513167989', 'https://openalex.org/W2514315580', 'https://openalex.org/W651894412', 'https://openalex.org/W2515295520', 'https://openalex.org/W2250653840', 'https://openalex.org/W2116792345', 'https://openalex.org/W2507784944', 'https://openalex.org/W2400065810', 'https://openalex.org/W2251816503', 'https://openalex.org/W2053154970', 'https://openalex.org/W2507438968', 'https://openalex.org/W2508316494', 'https://openalex.org/W2510503866', 'https://openalex.org/W2515629246', 'https://openalex.org/W2508907594', 'https://openalex.org/W2962801832', 'https://openalex.org/W2508032974', 'https://openalex.org/W2518037268', 'https://openalex.org/W2251431759', 'https://openalex.org/W2149327368', 'https://openalex.org/W2100175927', 'https://openalex.org/W1523296404', 'https://openalex.org/W2250612440', 'https://openalex.org/W2260677151', 'https://openalex.org/W2512934357', 'https://openalex.org/W2251610689', 'https://openalex.org/W2508809683', 'https://openalex.org/W2513945636', 'https://openalex.org/W2147192413', 'https://openalex.org/W2466291125', 'https://openalex.org/W2963816901', 'https://openalex.org/W2251333340', 'https://openalex.org/W3203021515', 'https://openalex.org/W2133564696', 'https://openalex.org/W2124807415', 'https://openalex.org/W2512415098', 'https://openalex.org/W2962784628', 'https://openalex.org/W1746819321', 'https://openalex.org/W2512538275', 'https://openalex.org/W2508420432', 'https://openalex.org/W2250375075', 'https://openalex.org/W2963216505', 'https://openalex.org/W2169724380', 'https://openalex.org/W2595715041', 'https://openalex.org/W2511439628', 'https://openalex.org/W2513885647', 'https://openalex.org/W2115081467', 'https://openalex.org/W2146574666', 'https://openalex.org/W2113788796', 'https://openalex.org/W635530177', 'https://openalex.org/W2101105183', 'https://openalex.org/W2509282593', 'https://openalex.org/W2167007060', 'https://openalex.org/W2518299649', 'https://openalex.org/W2518157461', 'https://openalex.org/W4245882962', 'https://openalex.org/W3100577423', 'https://openalex.org/W2917324689', 'https://openalex.org/W2270190199', 'https://openalex.org/W2169279899', 'https://openalex.org/W2511773399', 'https://openalex.org/W2251150371', 'https://openalex.org/W2141971526', 'https://openalex.org/W2159107349', 'https://openalex.org/W2057399676', 'https://openalex.org/W2511717148', 'https://openalex.org/W2251088156', 'https://openalex.org/W2350508517', 'https://openalex.org/W2101234009', 'https://openalex.org/W2575583396', 'https://openalex.org/W2517319883', 'https://openalex.org/W2514694861', 'https://openalex.org/W2964308564', 'https://openalex.org/W2508942858', 'https://openalex.org/W2515770991', 'https://openalex.org/W2251557434', 'https://openalex.org/W2251979322', 'https://openalex.org/W2518455057', 'https://openalex.org/W2152311128', 'https://openalex.org/W2513202451', 'https://openalex.org/W2094059537', 'https://openalex.org/W2511164486', 'https://openalex.org/W2252166243', 'https://openalex.org/W2508355219', 'https://openalex.org/W2509717321', 'https://openalex.org/W2164777277', 'https://openalex.org/W2510003438', 'https://openalex.org/W4211049957', 'https://openalex.org/W2963271675', 'https://openalex.org/W2509728490', 'https://openalex.org/W2515117158', 'https://openalex.org/W2509471569', 'https://openalex.org/W1975879668', 'https://openalex.org/W2156985047', 'https://openalex.org/W2169939314', 'https://openalex.org/W2251926178', 'https://openalex.org/W2511117522', 'https://openalex.org/W2512836759', 'https://openalex.org/W2141599568', 'https://openalex.org/W222053410']",2016-01-01
https://openalex.org/W2964012862,https://doi.org/10.21437/interspeech.2018-2456,Multi-Modal Data Augmentation for End-to-end ASR,"We present a new end-to-end architecture for automatic speech recognition (ASR) that can be trained using \emph{symbolic} input in addition to the traditional acoustic input. This architecture utilizes two separate encoders: one for acoustic input and another for symbolic input, both sharing the attention and decoder parameters. We call this architecture a multi-modal data augmentation network (MMDA), as it can support multi-modal (acoustic and symbolic) input and enables seamless mixing of large text datasets with significantly smaller transcribed speech corpora during training. We study different ways of transforming large text corpora into a symbolic form suitable for training our MMDA network. Our best MMDA setup obtains small improvements on character error rate (CER), and as much as 7-10\% relative word error rate (WER) improvement over a baseline both with and without an external language model.",[],2018-08-28
https://openalex.org/W4225308107,https://doi.org/10.1109/icassp43922.2022.9747862,Integrating Text Inputs for Training and Adapting RNN Transducer ASR Models,"Compared to hybrid automatic speech recognition (ASR) systems that use a modular architecture in which each component can be in-dependently adapted to a new domain, recent end-to-end (E2E) ASR system are harder to customize due to their all-neural monolithic construction. In this paper, we propose a novel text representation and training framework for E2E ASR models. With this approach, we show that a trained RNN Transducer (RNN-T) model's internal LM component can be effectively adapted with text-only data. An RNN-T model trained using both speech and text inputs improves over a baseline model trained on just speech with close to 13% word error rate (WER) reduction on the Switchboard and CallHome test sets of the NIST Hub5 2000 evaluation. The usefulness of the proposed approach is further demonstrated by customizing this general purpose RNN-T model to three separate datasets. We observe 20-45% relative word error rate (WER) reduction in these settings with this novel LM style customization technique using only unpaired text data from the new domains.","['https://openalex.org/W3197661863', 'https://openalex.org/W2937780860', 'https://openalex.org/W2936774411', 'https://openalex.org/W3163300396', 'https://openalex.org/W3100460087', 'https://openalex.org/W2077302143', 'https://openalex.org/W6784579990', 'https://openalex.org/W3197314576', 'https://openalex.org/W2973127116', 'https://openalex.org/W3007227084', 'https://openalex.org/W6638749077', 'https://openalex.org/W3015686596', 'https://openalex.org/W3152221657', 'https://openalex.org/W3008037978', 'https://openalex.org/W6747398299', 'https://openalex.org/W2962760690', 'https://openalex.org/W3196869722', 'https://openalex.org/W3095384101', 'https://openalex.org/W1828163288', 'https://openalex.org/W2963414781']",2022-04-27
https://openalex.org/W2766219058,https://doi.org/10.1109/jstsp.2017.2763455,Hybrid CTC/Attention Architecture for End-to-End Speech Recognition,"Conventional automatic speech recognition (ASR) based on a hidden Markov model (HMM)/deep neural network (DNN) is a very complicated system consisting of various modules such as acoustic, lexicon, and language models. It also requires linguistic resources, such as a pronunciation dictionary, tokenization, and phonetic context-dependency trees. On the other hand, end-to-end ASR has become a popular alternative to greatly simplify the model-building process of conventional ASR systems by representing complicated modules with a single deep network architecture, and by replacing the use of linguistic resources with a data-driven learning method. There are two major types of end-to-end architectures for ASR; attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols, and connectionist temporal classification (CTC) uses Markov assumptions to efficiently solve sequential problems by dynamic programming. This paper proposes hybrid CTC/attention end-to-end ASR, which effectively utilizes the advantages of both architectures in training and decoding. During training, we employ the multiobjective learning framework to improve robustness and achieve fast convergence. During decoding, we perform joint decoding by combining both attention-based and CTC scores in a one-pass beam search algorithm to further eliminate irregular alignments. Experiments with English (WSJ and CHiME-4) tasks demonstrate the effectiveness of the proposed multiobjective learning over both the CTC and attention-based encoder-decoder baselines. Moreover, the proposed method is applied to two large-scale ASR benchmarks (spontaneous Japanese and Mandarin Chinese), and exhibits performance that is comparable to conventional DNN/HMM ASR systems based on the advantages of both multiobjective learning and joint decoding without linguistic resources.","['https://openalex.org/W2962826786', 'https://openalex.org/W6679436768', 'https://openalex.org/W6681794868', 'https://openalex.org/W2963447639', 'https://openalex.org/W1526236009', 'https://openalex.org/W6601563604', 'https://openalex.org/W2559260703', 'https://openalex.org/W6639156005', 'https://openalex.org/W2608712415', 'https://openalex.org/W6679434410', 'https://openalex.org/W6727690538', 'https://openalex.org/W2577366047', 'https://openalex.org/W2127141656', 'https://openalex.org/W2963211739', 'https://openalex.org/W6687566353', 'https://openalex.org/W6728910023', 'https://openalex.org/W2526425061', 'https://openalex.org/W4251372957', 'https://openalex.org/W6606787761', 'https://openalex.org/W2515801922', 'https://openalex.org/W4299649720', 'https://openalex.org/W6678720029', 'https://openalex.org/W6631362777', 'https://openalex.org/W6675365184', 'https://openalex.org/W6635078382', 'https://openalex.org/W2160815625', 'https://openalex.org/W1990005915', 'https://openalex.org/W6623517193', 'https://openalex.org/W2739883972', 'https://openalex.org/W811578723', 'https://openalex.org/W6683955732', 'https://openalex.org/W2395416438', 'https://openalex.org/W2514741789', 'https://openalex.org/W2627092829', 'https://openalex.org/W179875071', 'https://openalex.org/W2005708641', 'https://openalex.org/W6728811460', 'https://openalex.org/W2064675550', 'https://openalex.org/W1855892484', 'https://openalex.org/W2163377725', 'https://openalex.org/W1586532344', 'https://openalex.org/W2130942839', 'https://openalex.org/W3145501851', 'https://openalex.org/W2964308564', 'https://openalex.org/W37526647', 'https://openalex.org/W1524333225', 'https://openalex.org/W2545177271', 'https://openalex.org/W2193413348', 'https://openalex.org/W2144499799', 'https://openalex.org/W165283731', 'https://openalex.org/W854541894', 'https://openalex.org/W2952288254', 'https://openalex.org/W2133564696', 'https://openalex.org/W2530486890', 'https://openalex.org/W1553004968', 'https://openalex.org/W2125529971', 'https://openalex.org/W2525778437', 'https://openalex.org/W2102113734', 'https://openalex.org/W2143017621']",2017-10-25
https://openalex.org/W4285223043,https://doi.org/10.18653/v1/2022.iwslt-1.14,Effective combination of pretrained models - KIT@IWSLT2022,"Pretrained models in acoustic and textual modalities can potentially improve speech translation for both Cascade and End-to-end approaches. In this evaluation, we aim at empirically looking for the answer by using the wav2vec, mBART50 and DeltaLM models to improve text and speech translation models. The experiments showed that the presence of these models together with an advanced audio segmentation method results in an improvement over the previous end-to-end system by up to 7 BLEU points. More importantly, the experiments showed that given enough data and modeling capacity to overcome the training difficulty, we can outperform even very competitive Cascade systems. In our experiments, this gap can be as large as 2.0 BLEU points, the same gap that the Cascade often led over the years.","['https://openalex.org/W3034999214', 'https://openalex.org/W2899274165', 'https://openalex.org/W3015698636', 'https://openalex.org/W3097777922', 'https://openalex.org/W2896457183', 'https://openalex.org/W3175746962', 'https://openalex.org/W1494198834', 'https://openalex.org/W4297841689', 'https://openalex.org/W3015889230', 'https://openalex.org/W4385570170', 'https://openalex.org/W2964110616', 'https://openalex.org/W2745785989', 'https://openalex.org/W3054645415', 'https://openalex.org/W4285158119', 'https://openalex.org/W3092085609', 'https://openalex.org/W3101648800', 'https://openalex.org/W3107826490', 'https://openalex.org/W2972451902', 'https://openalex.org/W3030437843', 'https://openalex.org/W2963532001', 'https://openalex.org/W4385245566', 'https://openalex.org/W2555745756', 'https://openalex.org/W4287694131', 'https://openalex.org/W3034625919', 'https://openalex.org/W2964172053', 'https://openalex.org/W2170240176', 'https://openalex.org/W3095073040', 'https://openalex.org/W3037187981', 'https://openalex.org/W3036601975', 'https://openalex.org/W3097030750', 'https://openalex.org/W2936969148']",2022-01-01
https://openalex.org/W2624871570,https://doi.org/10.48550/arxiv.1706.05098,An Overview of Multi-Task Learning in Deep Neural Networks,"Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.","['https://openalex.org/W3209042722', 'https://openalex.org/W1497745584', 'https://openalex.org/W2138019504', 'https://openalex.org/W2590925815', 'https://openalex.org/W2186054958', 'https://openalex.org/W2407277018', 'https://openalex.org/W3100041486', 'https://openalex.org/W2591927543', 'https://openalex.org/W2951941802', 'https://openalex.org/W2963722008', 'https://openalex.org/W2963826681', 'https://openalex.org/W2165644552', 'https://openalex.org/W2579721581', 'https://openalex.org/W1560550898', 'https://openalex.org/W2251300660', 'https://openalex.org/W2618011341', 'https://openalex.org/W2951548327', 'https://openalex.org/W2613321912', 'https://openalex.org/W582055897', 'https://openalex.org/W1769664844', 'https://openalex.org/W2251324968', 'https://openalex.org/W2949828894', 'https://openalex.org/W2143104527', 'https://openalex.org/W2516255829', 'https://openalex.org/W2144752499', 'https://openalex.org/W1510488293', 'https://openalex.org/W2962839807', 'https://openalex.org/W2036043322', 'https://openalex.org/W1738019091', 'https://openalex.org/W1939941161', 'https://openalex.org/W1896424170', 'https://openalex.org/W2166721725', 'https://openalex.org/W2130903752', 'https://openalex.org/W2148522164', 'https://openalex.org/W2567698949', 'https://openalex.org/W2091432990', 'https://openalex.org/W3104240813', 'https://openalex.org/W2914746235', 'https://openalex.org/W2015140204', 'https://openalex.org/W2117482391', 'https://openalex.org/W2150287743', 'https://openalex.org/W2609130030', 'https://openalex.org/W2143419558', 'https://openalex.org/W2617039999', 'https://openalex.org/W2339391301']",2017-06-15
https://openalex.org/W3046368065,https://doi.org/10.48550/arxiv.2008.00401,Multilingual Translation with Extensible Multilingual Pretraining and Finetuning,"Recent work demonstrates the potential of multilingual pretraining of creating one model that can be used for various tasks in different languages. Previous work in multilingual pretraining has demonstrated that machine translation systems can be created by finetuning on bitext. In this work, we show that multilingual translation models can be created through multilingual finetuning. Instead of finetuning on one direction, a pretrained model is finetuned on many directions at the same time. Compared to multilingual models trained from scratch, starting from pretrained models incorporates the benefits of large quantities of unlabeled monolingual data, which is particularly important for low resource languages where bitext is not available. We demonstrate that pretrained models can be extended to incorporate additional languages without loss of performance. We double the number of languages in mBART to support multilingual machine translation models of 50 languages. Finally, we create the ML50 benchmark, covering low, mid, and high resource languages, to facilitate reproducible research by standardizing training and evaluation data. On ML50, we demonstrate that multilingual finetuning improves on average 1 BLEU over the strongest baselines (being either multilingual from scratch or bilingual finetuning) while improving 9.3 BLEU on average over bilingual baselines from scratch.","['https://openalex.org/W3001434439', 'https://openalex.org/W2919290281', 'https://openalex.org/W2958953787', 'https://openalex.org/W2948697567', 'https://openalex.org/W2965373594', 'https://openalex.org/W2953109491', 'https://openalex.org/W3036839309', 'https://openalex.org/W3035144493', 'https://openalex.org/W2982399380', 'https://openalex.org/W2983040767', 'https://openalex.org/W3017454464', 'https://openalex.org/W2962739339', 'https://openalex.org/W2963993537', 'https://openalex.org/W2944815030', 'https://openalex.org/W2963250244', 'https://openalex.org/W2921280978', 'https://openalex.org/W2933138175', 'https://openalex.org/W2980404057', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963403868', 'https://openalex.org/W2550821151', 'https://openalex.org/W2963247703', 'https://openalex.org/W3013840636', 'https://openalex.org/W2914120296']",2020-08-02
https://openalex.org/W2513345070,https://doi.org/10.21437/interspeech.2016-268,Feature Learning with Raw-Waveform CLDNNs for Voice Activity Detection,"Voice Activity Detection (VAD) is an important preprocessing step in any state-of-the-art speech recognition system.Choosing the right set of features and model architecture can be challenging and is an active area of research.In this paper we propose a novel approach to VAD to tackle both feature and model selection jointly.The proposed method is based on a CLDNN (Convolutional, Long Short-Term Memory, Deep Neural Networks) architecture fed directly with the raw waveform.We show that using the raw waveform allows the neural network to learn features directly for the task at hand, which is more powerful than using log-mel features, specially for noisy environments.In addition, using a CLDNN, which takes advantage of both frequency modeling with the CNN and temporal modeling with LSTM, is a much better model for VAD compared to the DNN.The proposed system achieves over 78% relative improvement in False Alarms (FA) at the operating point of 2% False Rejects (FR) on both clean and noisy conditions compared to a DNN of comparable size trained with log-mel features.In addition, we study the impact of the model size and the learned features to provide a better understanding of the proposed architecture.","['https://openalex.org/W2295228182', 'https://openalex.org/W2259317772', 'https://openalex.org/W2168231600', 'https://openalex.org/W1533861849', 'https://openalex.org/W2294745040', 'https://openalex.org/W2184045248', 'https://openalex.org/W2108289863', 'https://openalex.org/W2112739286', 'https://openalex.org/W2293634267', 'https://openalex.org/W2398826216', 'https://openalex.org/W2126693545', 'https://openalex.org/W1536583098', 'https://openalex.org/W2048497537', 'https://openalex.org/W1600744878', 'https://openalex.org/W2129120544', 'https://openalex.org/W2402212287', 'https://openalex.org/W2143612262']",2016-08-29
https://openalex.org/W4307106469,https://doi.org/10.48550/arxiv.2111.09344,The People's Speech: A Large-Scale Diverse English Speech Recognition\n Dataset for Commercial Usage,"The People's Speech is a free-to-download 30,000-hour and growing supervised\nconversational English speech recognition dataset licensed for academic and\ncommercial usage under CC-BY-SA (with a CC-BY subset). The data is collected\nvia searching the Internet for appropriately licensed audio data with existing\ntranscriptions. We describe our data collection methodology and release our\ndata collection system under the Apache 2.0 license. We show that a model\ntrained on this dataset achieves a 9.98% word error rate on Librispeech's\ntest-clean test set.Finally, we discuss the legal and ethical issues\nsurrounding the creation of a sizable machine learning corpora and plans for\ncontinued maintenance of the project under MLCommons's sponsorship.\n",[],2021-11-17
https://openalex.org/W2171761326,https://doi.org/10.1109/asru.2001.1034648,Unsupervised training of acoustic models for large vocabulary continuous speech recognition,"For speech recognition systems, the amount of acoustic training data is of crucial importance. In the past, large amounts of speech were recorded and transcribed manually for training. Since untranscribed speech is available in various forms these days, the unsupervised training of a speech recognizer on recognized transcriptions is studied. A low-cost recognizer trained with only one hour of manually transcribed speech is used to recognize 72 hours of untranscribed acoustic data. These transcriptions are then used in combination with confidence measures to train an improved recognizer. The effect of confidence measures which are used to detect possible recognition errors is studied systematically. Finally, the unsupervised training is applied iteratively. Using this method, the recognizer is trained with very little manual effort while losing only 14.3% relative on the Broadcast News '96 and 18.6% relative on the Broadcast News '98 evaluation test sets.","['https://openalex.org/W2134659216', 'https://openalex.org/W2166810516', 'https://openalex.org/W2164579587', 'https://openalex.org/W6677717300', 'https://openalex.org/W4402490925', 'https://openalex.org/W1975381461', 'https://openalex.org/W1920769845', 'https://openalex.org/W1555037511', 'https://openalex.org/W2009267849', 'https://openalex.org/W1654858490', 'https://openalex.org/W4285719527', 'https://openalex.org/W2541906951', 'https://openalex.org/W1975113979', 'https://openalex.org/W5671660', 'https://openalex.org/W34303869', 'https://openalex.org/W2117590177', 'https://openalex.org/W2113094193']",2005-08-25
https://openalex.org/W1588359339,https://doi.org/10.1109/icassp.2001.940841,Confidence-measure-driven unsupervised incremental adaptation for HMM-based speech recognition,"In this work, we first review the usual ways to take into account confidence measures in unsupervised adaptation and then propose a new unsupervised incremental adaptation based on a ranking of the adaptation data according to their confidence measures. A semi-supervised adaptation process is also proposed: the confidence measure is used to select the main part of the data for unsupervised adaptation and the remaining small part of the data is handled in a supervised mode. Experiments are conducted on a field database. Generic context-dependent phoneme HMMs are adapted to task- and field-specific conditions. These experiments show a significant improvement for unsupervised adaptation when confidence measures are used. In this work, we also show that the adaptation rate (that measures how important adaptation data are considered with respect to prior data) influences a lot the efficiency of the confidence measure in unsupervised adaptation.","['https://openalex.org/W1555037511', 'https://openalex.org/W2166799626', 'https://openalex.org/W167614639', 'https://openalex.org/W2164742805', 'https://openalex.org/W57619689']",2002-11-13
https://openalex.org/W3160628828,https://doi.org/10.1109/icassp39728.2021.9413692,Improving Streaming Automatic Speech Recognition with Non-Streaming Model Distillation on Unsupervised Data,"Streaming end-to-end automatic speech recognition (ASR) models are widely used on smart speakers and on-device applications. Since these models are expected to transcribe speech with minimal latency, they are constrained to be causal with no future context, compared to their non-streaming counterparts. Consequently, streaming models usually perform worse than non-streaming models. We propose a novel and effective learning method by leveraging a non-streaming ASR model as a teacher to generate transcripts on an arbitrarily large data set, which is then used to distill knowledge into streaming ASR models. This way, we scale the training of streaming models to up to 3 million hours of YouTube audio. Experiments show that our approach can significantly reduce the word error rate (WER) of RNN-T models not only on LibriSpeech but also on YouTube data in four languages. For example, in French, we are able to reduce the WER by 16.4% relatively to a baseline streaming model by leveraging a non-streaming teacher model trained on the same amount of labeled data as the baseline.","['https://openalex.org/W6771467084', 'https://openalex.org/W6771812881', 'https://openalex.org/W2973122799', 'https://openalex.org/W3015194534', 'https://openalex.org/W3096518646', 'https://openalex.org/W2033256038', 'https://openalex.org/W3026041220', 'https://openalex.org/W3035160371', 'https://openalex.org/W6768222176', 'https://openalex.org/W2592335154', 'https://openalex.org/W3008181812', 'https://openalex.org/W2121879602', 'https://openalex.org/W6779924452', 'https://openalex.org/W3016010032', 'https://openalex.org/W3096702180', 'https://openalex.org/W6769806307', 'https://openalex.org/W3015974384', 'https://openalex.org/W6790121257', 'https://openalex.org/W6775998690', 'https://openalex.org/W6623517193', 'https://openalex.org/W6769647859', 'https://openalex.org/W2962760690', 'https://openalex.org/W2327501763', 'https://openalex.org/W6638749077', 'https://openalex.org/W6776544788', 'https://openalex.org/W3097777922', 'https://openalex.org/W2936774411', 'https://openalex.org/W2889129739', 'https://openalex.org/W2936123380', 'https://openalex.org/W6629717138', 'https://openalex.org/W1828163288', 'https://openalex.org/W2976223659', 'https://openalex.org/W3128478537', 'https://openalex.org/W4288088457', 'https://openalex.org/W854541894', 'https://openalex.org/W2981696585', 'https://openalex.org/W3125815078', 'https://openalex.org/W3015927303', 'https://openalex.org/W3149629662', 'https://openalex.org/W3030437843', 'https://openalex.org/W3004728855', 'https://openalex.org/W2995181338', 'https://openalex.org/W3025165719', 'https://openalex.org/W1494198834', 'https://openalex.org/W2995746049', 'https://openalex.org/W3037029661', 'https://openalex.org/W2982413405']",2021-05-13
https://openalex.org/W4210463634,https://doi.org/10.1109/asru51503.2021.9687871,Scaling End-to-End Models for Large-Scale Multilingual ASR,"Building ASR models across many languages is a challenging multi-task learning problem due to large variations and heavily unbalanced data. Existing work has shown positive transfer from high resource to low resource languages. However, degradations on high resource languages are commonly observed due to interference from the heterogeneous multilingual data and reduction in per-language capacity. We conduct a capacity study on a 15-language task, with the amount of data per language varying from 7.6K to 53.5K hours. We adopt GShard [1] to efficiently scale up to 10B parameters. Empirically, we find that (1) scaling the number of model parameters is an effective way to solve the capacity bottleneck - our 500M-param model already outperforms monolingual baselines and scaling it to 1B and 10B brought further quality gains; (2) larger models are not only more data efficient, but also more efficient in terms of training cost as measured in TPU days - the 1B-param model reaches the same accuracy at 34% of training time as the 500M-param model; (3) given a fixed capacity budget, adding depth works better than width and large encoders do better than large decoders; (4) with continuous training, they can be adapted to new languages and domains.","['https://openalex.org/W6751104502', 'https://openalex.org/W6631190155', 'https://openalex.org/W6639156005', 'https://openalex.org/W6696934422', 'https://openalex.org/W3160766462', 'https://openalex.org/W3097777922', 'https://openalex.org/W2936774411', 'https://openalex.org/W6760633627', 'https://openalex.org/W6739901393', 'https://openalex.org/W3095311338', 'https://openalex.org/W3095173472', 'https://openalex.org/W2962760690', 'https://openalex.org/W2526425061', 'https://openalex.org/W3016010032', 'https://openalex.org/W3163300396', 'https://openalex.org/W6765469073', 'https://openalex.org/W2962893195', 'https://openalex.org/W2971840980', 'https://openalex.org/W2964002616', 'https://openalex.org/W3096032230', 'https://openalex.org/W3128096387', 'https://openalex.org/W6784614252', 'https://openalex.org/W6783990525', 'https://openalex.org/W6778883912', 'https://openalex.org/W6772383348', 'https://openalex.org/W6632924066', 'https://openalex.org/W6755207826', 'https://openalex.org/W2160815625', 'https://openalex.org/W6780805062', 'https://openalex.org/W3096215352', 'https://openalex.org/W6752630080', 'https://openalex.org/W2963431393', 'https://openalex.org/W6779919476', 'https://openalex.org/W3035019713', 'https://openalex.org/W3095410713', 'https://openalex.org/W2913340405', 'https://openalex.org/W6638749077', 'https://openalex.org/W4385245566', 'https://openalex.org/W2293634267', 'https://openalex.org/W3093579165', 'https://openalex.org/W2896457183', 'https://openalex.org/W1522301498', 'https://openalex.org/W1549321558', 'https://openalex.org/W1828163288', 'https://openalex.org/W2958953787', 'https://openalex.org/W36434594', 'https://openalex.org/W2808640845', 'https://openalex.org/W2928941594', 'https://openalex.org/W2585945212', 'https://openalex.org/W2271840356', 'https://openalex.org/W3092189037', 'https://openalex.org/W4292779060', 'https://openalex.org/W1855892484', 'https://openalex.org/W4293569541', 'https://openalex.org/W3001279689', 'https://openalex.org/W3198429080', 'https://openalex.org/W3040573126']",2021-12-13
https://openalex.org/W2117278770,,Intelligent Selection of Language Model Training Data,"We address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation. Our approach is based on comparing the cross-entropy, according to domainspecific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model. We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods. 1","['https://openalex.org/W2123958887', 'https://openalex.org/W22168010', 'https://openalex.org/W2075201173', 'https://openalex.org/W265531733', 'https://openalex.org/W1926502259', 'https://openalex.org/W2109664771', 'https://openalex.org/W9155011', 'https://openalex.org/W1974967573']",2010-07-11
https://openalex.org/W1905522558,,Domain Adaptation via Pseudo In-Domain Data Selection,"We explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain. These sentences may be selected with simple cross-entropy based methods, of which we present three. As these sentences are not themselves identical to the in-domain data, we call them pseudo in-domain subcorpora. These subcorpora – 1 % the size of the original – can then used to train small domain-adapted Statistical Machine Translation (SMT) systems which outperform systems trained on the entire corpus. Performance is further improved when we use these domain-adapted models in combination with a true in-domain model. The results show that more training data is not always better, and that best results are attained via proper domain-relevant data selection, as well as combining in- and general-domain systems during decoding. 1","['https://openalex.org/W2117278770', 'https://openalex.org/W2146574666', 'https://openalex.org/W2132001515', 'https://openalex.org/W2280403519', 'https://openalex.org/W1848260265', 'https://openalex.org/W2124807415', 'https://openalex.org/W1631260214', 'https://openalex.org/W2136477195', 'https://openalex.org/W1974967573', 'https://openalex.org/W2130450156', 'https://openalex.org/W2117051955', 'https://openalex.org/W2137387514', 'https://openalex.org/W2156985047', 'https://openalex.org/W2115410424', 'https://openalex.org/W2148861208', 'https://openalex.org/W2158195707', 'https://openalex.org/W2399188371', 'https://openalex.org/W2394860946']",2011-07-27
https://openalex.org/W2062164080,https://doi.org/10.1109/icassp.2013.6639100,An investigation of deep neural networks for noise robust speech recognition,"Recently, a new acoustic model based on deep neural networks (DNN) has been introduced. While the DNN has generated significant improvements over GMM-based systems on several tasks, there has been no evaluation of the robustness of such systems to environmental distortion. In this paper, we investigate the noise robustness of DNN-based acoustic models and find that they can match state-of-the-art performance on the Aurora 4 task without any explicit noise compensation. This performance can be further improved by incorporating information about the environment into DNN training using a new method called noise-aware training. When combined with the recently proposed dropout training technique, a 7.5% relative improvement over the previously best published result on this task is achieved using only a single decoding pass and no additional decoding complexity compared to a standard DNN.","['https://openalex.org/W1520448186', 'https://openalex.org/W2105393299', 'https://openalex.org/W2290318471', 'https://openalex.org/W2122514667', 'https://openalex.org/W6682387308', 'https://openalex.org/W2394932179', 'https://openalex.org/W6713280142', 'https://openalex.org/W2121973264', 'https://openalex.org/W6603374476', 'https://openalex.org/W6634340323', 'https://openalex.org/W2146423524', 'https://openalex.org/W2093028810', 'https://openalex.org/W1976637928', 'https://openalex.org/W6680646222', 'https://openalex.org/W6677416784', 'https://openalex.org/W1993882792', 'https://openalex.org/W6630687167', 'https://openalex.org/W2118497033', 'https://openalex.org/W2122699724', 'https://openalex.org/W6630149595', 'https://openalex.org/W2169189000', 'https://openalex.org/W2067117291', 'https://openalex.org/W1904365287', 'https://openalex.org/W1502469360', 'https://openalex.org/W2403195671', 'https://openalex.org/W2137401359', 'https://openalex.org/W2154354834', 'https://openalex.org/W1573570773', 'https://openalex.org/W84880661', 'https://openalex.org/W1517348348', 'https://openalex.org/W2115086266', 'https://openalex.org/W44815768']",2013-05-01
https://openalex.org/W4210690962,https://doi.org/10.1109/asru51503.2021.9688018,Injecting Text in Self-Supervised Speech Pretraining,"Self-supervised pretraining for Automated Speech Recognition (ASR) has shown varied degrees of success. In this paper, we propose to jointly learn representations during pretraining from two different modalities: speech and text. The proposed method, tts4pretrain complements the power of contrastive learning in self-supervision with linguistic/lexical representations derived from synthesized speech, effectively learning from untranscribed speech and unspoken text. Lexical learning in the speech encoder is enforced through an additional sequence loss term that is coupled with contrastive loss during pretraining. We demonstrate that this novel pretraining method yields Word Error Rate (WER) reductions of 10% relative on the well-benchmarked, Librispeech task over a state-of-the-art baseline pretrained with wav2vec2.0 only. The proposed method also serves as an effective strategy to compensate for the lack of transcribed speech, effectively matching the performance of 5000 hours of transcribed speech with just 100 hours of transcribed speech on the AMI meeting transcription task. Finally, we demonstrate WER reductions of up to 15% on an inhouse Voice Search task over traditional pretraining. Incorporating text into encoder pretraining is complimentary to rescoring with a larger or in-domain language model, resulting in additional 6% relative reduction in WER.","['https://openalex.org/W6677328538', 'https://openalex.org/W3198771897', 'https://openalex.org/W3016234571', 'https://openalex.org/W2939111082', 'https://openalex.org/W6755300632', 'https://openalex.org/W6752888775', 'https://openalex.org/W2972970915', 'https://openalex.org/W2963796886', 'https://openalex.org/W6638749077', 'https://openalex.org/W3007073761', 'https://openalex.org/W2143612262', 'https://openalex.org/W6780218876', 'https://openalex.org/W6784614252', 'https://openalex.org/W3097890746', 'https://openalex.org/W6762242920', 'https://openalex.org/W2938947737', 'https://openalex.org/W2972889948', 'https://openalex.org/W2963400424', 'https://openalex.org/W2526425061', 'https://openalex.org/W2127141656', 'https://openalex.org/W6800705428', 'https://openalex.org/W3209059054', 'https://openalex.org/W6791904447', 'https://openalex.org/W6751104502', 'https://openalex.org/W3036878841', 'https://openalex.org/W2963250244', 'https://openalex.org/W6739901393', 'https://openalex.org/W3026041220', 'https://openalex.org/W3001197829', 'https://openalex.org/W3096273170', 'https://openalex.org/W3198840231', 'https://openalex.org/W2962699523', 'https://openalex.org/W2963739817', 'https://openalex.org/W3008480565', 'https://openalex.org/W3015280134', 'https://openalex.org/W6729383884', 'https://openalex.org/W2883586237', 'https://openalex.org/W3161606033', 'https://openalex.org/W3084783920', 'https://openalex.org/W3160235762', 'https://openalex.org/W3095727342', 'https://openalex.org/W6784426525', 'https://openalex.org/W3016011332', 'https://openalex.org/W2747467128', 'https://openalex.org/W3035160371', 'https://openalex.org/W6678809451', 'https://openalex.org/W2995181338', 'https://openalex.org/W2972359262', 'https://openalex.org/W6782040703', 'https://openalex.org/W6756385397', 'https://openalex.org/W3097777922', 'https://openalex.org/W1494198834', 'https://openalex.org/W2936774411', 'https://openalex.org/W2898727538', 'https://openalex.org/W2546938941', 'https://openalex.org/W3096815019', 'https://openalex.org/W1828163288', 'https://openalex.org/W3036601975', 'https://openalex.org/W1915251500', 'https://openalex.org/W3093579165', 'https://openalex.org/W3139918052', 'https://openalex.org/W3198608154', 'https://openalex.org/W4327845728', 'https://openalex.org/W2952711665', 'https://openalex.org/W4289383906', 'https://openalex.org/W2117278770', 'https://openalex.org/W4385245566', 'https://openalex.org/W2125336414', 'https://openalex.org/W2808706139', 'https://openalex.org/W4293569541', 'https://openalex.org/W3198442913']",2021-12-13
https://openalex.org/W2121879602,https://doi.org/10.1109/icassp.2012.6289079,Japanese and Korean voice search,"This paper describes challenges and solutions for building a successful voice search system as applied to Japanese and Korean at Google. We describe the techniques used to deal with an infinite vocabulary, how modeling completely in the written domain for language model and dictionary can avoid some system complexity, and how we built dictionaries, language and acoustic models in this framework. We show how to deal with the difficulty of scoring results for multiple script languages because of ambiguities. The development of voice search for these languages led to a significant simplification of the original process to build a system for any new language which in in parts became our default process for internationalization of voice search.","['https://openalex.org/W6676411926', 'https://openalex.org/W1734538896', 'https://openalex.org/W2117444496', 'https://openalex.org/W6602670161', 'https://openalex.org/W2125234026', 'https://openalex.org/W2106554350', 'https://openalex.org/W2109726413', 'https://openalex.org/W64751979', 'https://openalex.org/W2899506040']",2012-03-01
https://openalex.org/W2143577772,https://doi.org/10.1109/icassp.2004.1326091,Improving broadcast news transcription by lightly supervised discriminative training,"We present our experiments on lightly supervised discriminative training with large amounts of broadcast news data for which only closed caption transcriptions are available (TDT data). In particular, we use language models biased to the closed-caption transcripts to recognise the audio data, and the recognised transcripts are then used as the training transcriptions for acoustic model training. A range of experiments that use maximum likelihood (ML) training as well as discriminative training based on either maximum mutual information (MMI) or minimum phone error (MPE) are presented. In a 5xRT broadcast news transcription system that includes adaptation, it is shown that reductions in word error rate (WER) in the range of 1% absolute can be achieved. Finally, some experiments on training data selection are presented to compare different methods of ""filtering"" the transcripts.","['https://openalex.org/W2163295968', 'https://openalex.org/W6607114211', 'https://openalex.org/W1975113979', 'https://openalex.org/W6677014330', 'https://openalex.org/W2148295152', 'https://openalex.org/W2146049072', 'https://openalex.org/W3149335959', 'https://openalex.org/W6685848937', 'https://openalex.org/W2003123121', 'https://openalex.org/W173561343', 'https://openalex.org/W2111870400', 'https://openalex.org/W2179562937', 'https://openalex.org/W2150907703']",2004-09-28
https://openalex.org/W3102342027,https://doi.org/10.18653/v1/2020.findings-emnlp.106,Learning Robust and Multilingual Speech Representations,"Unsupervised speech representation learning has shown remarkable success at finding representations that correlate with phonetic structures and improve downstream speech recognition performance. However, most research has been focused on evaluating the representations in terms of their ability to improve the performance of speech recognition systems on read English (e.g. Wall Street Journal and LibriSpeech). This evaluation methodology overlooks two important desiderata that speech representations should have: robustness to domain shifts and transferability to other languages. In this paper we learn representations from up to 8000 hours of diverse and noisy speech data and evaluate the representations by looking at their robustness to domain shifts and their ability to improve recognition performance in many languages. We find that our representations confer significant robustness advantages to the resulting recognition systems: we see significant improvements in out-of-domain transfer relative to baseline feature sets and the features likewise provide improvements in 25 phonetically diverse languages.","['https://openalex.org/W3101648800', 'https://openalex.org/W2136922672', 'https://openalex.org/W2184045248', 'https://openalex.org/W2152790380', 'https://openalex.org/W2948012107', 'https://openalex.org/W2166637769', 'https://openalex.org/W4288107125', 'https://openalex.org/W2110798204', 'https://openalex.org/W2066873261', 'https://openalex.org/W2971155163', 'https://openalex.org/W2401271873', 'https://openalex.org/W2996383576', 'https://openalex.org/W4300906742', 'https://openalex.org/W2997574889', 'https://openalex.org/W2024490156', 'https://openalex.org/W4294170691', 'https://openalex.org/W2842511635', 'https://openalex.org/W2962739339', 'https://openalex.org/W2953469440', 'https://openalex.org/W2973049979', 'https://openalex.org/W2995181338', 'https://openalex.org/W3123318516', 'https://openalex.org/W2094544353', 'https://openalex.org/W2963446712', 'https://openalex.org/W2119569779', 'https://openalex.org/W4297808394', 'https://openalex.org/W2970241862', 'https://openalex.org/W2963341956', 'https://openalex.org/W4300047444', 'https://openalex.org/W2963620343', 'https://openalex.org/W2979476256', 'https://openalex.org/W2135346645', 'https://openalex.org/W2572097499', 'https://openalex.org/W2936774411', 'https://openalex.org/W2962824366', 'https://openalex.org/W2166243357', 'https://openalex.org/W2949759968', 'https://openalex.org/W2520160253', 'https://openalex.org/W2944828972', 'https://openalex.org/W4289259401', 'https://openalex.org/W3189092450', 'https://openalex.org/W2127141656', 'https://openalex.org/W1494198834', 'https://openalex.org/W2153579005', 'https://openalex.org/W3016181583', 'https://openalex.org/W2593116425', 'https://openalex.org/W3015522062', 'https://openalex.org/W2108384452', 'https://openalex.org/W2145094598', 'https://openalex.org/W3127686677', 'https://openalex.org/W2901739041', 'https://openalex.org/W2971840980', 'https://openalex.org/W2991213871', 'https://openalex.org/W2896457183', 'https://openalex.org/W2146444479', 'https://openalex.org/W2193413348', 'https://openalex.org/W2150178435', 'https://openalex.org/W2524544624', 'https://openalex.org/W2007645738', 'https://openalex.org/W1530454533']",2020-01-01
https://openalex.org/W4285192675,https://doi.org/10.1109/jstsp.2022.3186162,Ask2Mask: Guided Data Selection for Masked Speech Modeling,"Masked speech modeling (MSM) methods such as wav2vec2 or w2v-BERT learn representations over speech frames which are randomly masked within an utterance. While these methods improve performance of Automatic Speech Recognition (ASR) systems, they have one major limitation. They treat all unsupervised speech samples with equal weight, which hinders learning as not all samples have relevant information to learn meaningful representations. In this work, we address this limitation. We propose ask2mask (ATM), a novel approach to focus on specific samples during MSM pre-training. ATM employs an external ASR model or <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">scorer</i> to weight unsupervised input samples in two different ways: 1) A fine-grained data selection is performed by masking over the highly confident input frames as chosen by the scorer. This allows the model to learn meaningful representations. 2) ATM is further extended to focus at utterance-level by weighting the final MSM loss with the utterance-level confidence score. We conduct fine-tuning experiments on two well-benchmarked corpora: LibriSpeech (matching the pre-training data) and Commonvoice, TED-LIUM, AMI and CHiME-6 (not matching the pre-training data). The results substantiate the efficacy of ATM on significantly improving the recognition performance under mismatched conditions (up to 11.6% relative over published results and upto 4.46% relative over our internal baseline) while still yielding modest improvements under matched conditions.","['https://openalex.org/W2111316763', 'https://openalex.org/W3015522062', 'https://openalex.org/W3026041220', 'https://openalex.org/W3160525311', 'https://openalex.org/W6675401909', 'https://openalex.org/W2896457183', 'https://openalex.org/W6780218876', 'https://openalex.org/W4226033575', 'https://openalex.org/W3198771897', 'https://openalex.org/W6791904447', 'https://openalex.org/W2953356739', 'https://openalex.org/W3095189764', 'https://openalex.org/W3196919915', 'https://openalex.org/W6771915120', 'https://openalex.org/W2748795451', 'https://openalex.org/W3196219156', 'https://openalex.org/W2134659216', 'https://openalex.org/W6780236752', 'https://openalex.org/W6765715182', 'https://openalex.org/W3011411500', 'https://openalex.org/W3163464943', 'https://openalex.org/W6779857854', 'https://openalex.org/W6783754597', 'https://openalex.org/W6784614252', 'https://openalex.org/W1494198834', 'https://openalex.org/W3020336359', 'https://openalex.org/W2127141656', 'https://openalex.org/W6739901393', 'https://openalex.org/W3016010032', 'https://openalex.org/W3198270883', 'https://openalex.org/W3099664505', 'https://openalex.org/W3093579165', 'https://openalex.org/W3139918052', 'https://openalex.org/W2954732411', 'https://openalex.org/W4287646293']",2022-06-24
https://openalex.org/W3102811925,https://doi.org/10.18653/v1/2020.coling-main.314,Dual-decoder Transformer for Joint Automatic Speech Recognition and Multilingual Speech Translation,"We introduce dual-decoder Transformer, a new model architecture that jointly performs automatic speech recognition (ASR) and multilingual speech translation (ST). Our models are based on the original Transformer architecture (Vaswani et al., 2017) but consist of two decoders, each responsible for one task (ASR or ST). Our major contribution lies in how these decoders interact with each other: one decoder can attend to different information sources from the other via a dual-attention mechanism. We propose two variants of these architectures corresponding to two different levels of dependencies between the decoders, called the parallel and cross dual-decoder Transformers, respectively. Extensive experiments on the MuST-C dataset show that our models outperform the previously-reported highest translation performance in the multilingual settings, and outperform as well bilingual one-to-one results. Furthermore, our parallel models demonstrate no trade-off between ASR and ST compared to the vanilla multi-task architecture. Our code and pre-trained models are available at https://github.com/formiel/speech-translation.","['https://openalex.org/W4230872509', 'https://openalex.org/W2962784628', 'https://openalex.org/W3034571331', 'https://openalex.org/W2963779652', 'https://openalex.org/W2998386507', 'https://openalex.org/W2963542740', 'https://openalex.org/W1524333225', 'https://openalex.org/W197865394', 'https://openalex.org/W2114483840', 'https://openalex.org/W2183341477', 'https://openalex.org/W2966095117', 'https://openalex.org/W2407080277', 'https://openalex.org/W4394666973', 'https://openalex.org/W2949328740', 'https://openalex.org/W4300558631', 'https://openalex.org/W3008125272', 'https://openalex.org/W2124807415', 'https://openalex.org/W3037465386', 'https://openalex.org/W2963532001', 'https://openalex.org/W2550821151', 'https://openalex.org/W2147800946', 'https://openalex.org/W3032433061', 'https://openalex.org/W3043665049', 'https://openalex.org/W2936774411', 'https://openalex.org/W2936969148', 'https://openalex.org/W3105681039', 'https://openalex.org/W1522301498', 'https://openalex.org/W4404389177', 'https://openalex.org/W2979636403', 'https://openalex.org/W3008549139', 'https://openalex.org/W4301980136', 'https://openalex.org/W3015698636', 'https://openalex.org/W2101105183', 'https://openalex.org/W2964121744', 'https://openalex.org/W3034332156', 'https://openalex.org/W2951456627', 'https://openalex.org/W2605131327', 'https://openalex.org/W4385245566', 'https://openalex.org/W2945700568', 'https://openalex.org/W2595715041', 'https://openalex.org/W3037217258']",2020-01-01
https://openalex.org/W3173666333,https://doi.org/10.18653/v1/2021.findings-acl.150,A Comparison between Pre-training and Large-scale Back-translation for Neural Machine Translation,"BERT has been studied as a promising technique to improve NMT.Given that BERT is based on the similar Transformer architecture to NMT and the current datasets for most MT tasks are rather large, how pre-training has managed to outperform standard Transformer NMT models is underestimated.We compare MT engines trained with pre-trained BERT and back-translation with incrementally larger amounts of data, implementing the two most widely-used monolingual paradigms.We analyze their strengths and weaknesses based on both standard automatic metrics and intrinsic test suites that comprise a large range of linguistic phenomena.Primarily, we find that 1) BERT has limited advantages compared with large-scale back-translation in accuracy and consistency on morphology and syntax; 2) BERT can boost the Transformer baseline in semantic and pragmatic tasks which involve intensive understanding; 3) pre-training on huge datasets may introduce inductive social bias thus affects translation fairness.","['https://openalex.org/W6631349028', 'https://openalex.org/W2910243263', 'https://openalex.org/W2963206679', 'https://openalex.org/W2756888147', 'https://openalex.org/W2885588803', 'https://openalex.org/W3004346089', 'https://openalex.org/W2889326796', 'https://openalex.org/W2964308564', 'https://openalex.org/W4288631803', 'https://openalex.org/W2922709902', 'https://openalex.org/W4288351520', 'https://openalex.org/W3006381853', 'https://openalex.org/W2972324944', 'https://openalex.org/W3093345276', 'https://openalex.org/W2964303116', 'https://openalex.org/W2944815030', 'https://openalex.org/W4288029087', 'https://openalex.org/W2973154008', 'https://openalex.org/W2797913374', 'https://openalex.org/W2555428947', 'https://openalex.org/W2902614977', 'https://openalex.org/W2965373594', 'https://openalex.org/W2964120396', 'https://openalex.org/W2963708445', 'https://openalex.org/W4385245566', 'https://openalex.org/W3105912780', 'https://openalex.org/W4299384410', 'https://openalex.org/W2963693569', 'https://openalex.org/W2994928925', 'https://openalex.org/W2912237282', 'https://openalex.org/W4214784700', 'https://openalex.org/W2546938941', 'https://openalex.org/W2971044268', 'https://openalex.org/W2970311224', 'https://openalex.org/W3104318719', 'https://openalex.org/W2130942839', 'https://openalex.org/W2971141904', 'https://openalex.org/W3105184673', 'https://openalex.org/W2983915252', 'https://openalex.org/W2970583189', 'https://openalex.org/W2561274697', 'https://openalex.org/W2986562961', 'https://openalex.org/W2974273066', 'https://openalex.org/W2989276524', 'https://openalex.org/W2946359678', 'https://openalex.org/W2963341956', 'https://openalex.org/W2914120296', 'https://openalex.org/W2525778437', 'https://openalex.org/W2898962441', 'https://openalex.org/W2963216553', 'https://openalex.org/W3034716087', 'https://openalex.org/W2983004086', 'https://openalex.org/W2970550868', 'https://openalex.org/W2991223644', 'https://openalex.org/W211509693', 'https://openalex.org/W3119653668', 'https://openalex.org/W2886095922', 'https://openalex.org/W1915251500', 'https://openalex.org/W3015766957', 'https://openalex.org/W2952468927', 'https://openalex.org/W2952328691', 'https://openalex.org/W2963685900', 'https://openalex.org/W2970015022', 'https://openalex.org/W2252272516', 'https://openalex.org/W2101105183', 'https://openalex.org/W3103671331', 'https://openalex.org/W2963078909', 'https://openalex.org/W2133564696', 'https://openalex.org/W2962824887', 'https://openalex.org/W3100311862', 'https://openalex.org/W3107826490', 'https://openalex.org/W2970248987', 'https://openalex.org/W3035144493', 'https://openalex.org/W2991265431', 'https://openalex.org/W3023986361']",2021-01-01
https://openalex.org/W3196292088,https://doi.org/10.18653/v1/2021.emnlp-main.129,HintedBT: Augmenting Back-Translation with Quality and Transliteration Hints,"Back-translation (BT) of target monolingual corpora is a widely used data augmentation strategy for neural machine translation (NMT), especially for low-resource language pairs. To improve effectiveness of the available BT data, we introduce HintedBT—a family of techniques which provides hints (through tags) to the encoder and decoder. First, we propose a novel method of using both high and low quality BT data by providing hints (as source tags on the encoder) to the model about the quality of each source-target pair. We don’t filter out low quality data but instead show that these hints enable the model to learn effectively from noisy data. Second, we address the problem of predicting whether a source token needs to be translated or transliterated to the target language, which is common in cross-script translation tasks (i.e., where source and target do not share the written script). For such cases, we propose training the model with additional hints (as target tags on the decoder) that provide information about the operation required on the source (translation or both translation and transliteration). We conduct experiments and detailed analyses on standard WMT benchmarks for three cross-script low/medium-resource language pairs: Hindi,Gujarati,Tamil-to-English. Our methods compare favorably with five strong and well established baselines. We show that using these hints, both separately and together, significantly improves translation quality and leads to state-of-the-art performance in all three language pairs in corresponding bilingual settings.","['https://openalex.org/W2970295111', 'https://openalex.org/W3118449360', 'https://openalex.org/W222053410', 'https://openalex.org/W2963532001', 'https://openalex.org/W3035464238', 'https://openalex.org/W2566564022', 'https://openalex.org/W2964308564', 'https://openalex.org/W2963122608', 'https://openalex.org/W2949973181', 'https://openalex.org/W2252154714', 'https://openalex.org/W4299574851', 'https://openalex.org/W3017454464', 'https://openalex.org/W4385245566', 'https://openalex.org/W2954218767', 'https://openalex.org/W2805790316', 'https://openalex.org/W2963216553', 'https://openalex.org/W2970015022', 'https://openalex.org/W2963569817', 'https://openalex.org/W2919290281', 'https://openalex.org/W2970731908', 'https://openalex.org/W2121879602', 'https://openalex.org/W3105038888', 'https://openalex.org/W2963877297', 'https://openalex.org/W2889326796', 'https://openalex.org/W3175955584', 'https://openalex.org/W2944815030', 'https://openalex.org/W4298137069', 'https://openalex.org/W2963633299', 'https://openalex.org/W1753482797', 'https://openalex.org/W2963206679', 'https://openalex.org/W3120628641', 'https://openalex.org/W2971073020', 'https://openalex.org/W630532510', 'https://openalex.org/W2970397283', 'https://openalex.org/W2970259631', 'https://openalex.org/W2122270629', 'https://openalex.org/W2952468927', 'https://openalex.org/W4285719527', 'https://openalex.org/W2963088995', 'https://openalex.org/W2757920198', 'https://openalex.org/W2963506925', 'https://openalex.org/W2786253471', 'https://openalex.org/W3100806282', 'https://openalex.org/W2902614977', 'https://openalex.org/W2928941594', 'https://openalex.org/W2525778437', 'https://openalex.org/W2970152385', 'https://openalex.org/W2963281280', 'https://openalex.org/W2888541716', 'https://openalex.org/W2886095922', 'https://openalex.org/W3035013535', 'https://openalex.org/W2970900338', 'https://openalex.org/W2794365787', 'https://openalex.org/W2130942839', 'https://openalex.org/W2963086938', 'https://openalex.org/W2550821151', 'https://openalex.org/W3104652516', 'https://openalex.org/W1915251500', 'https://openalex.org/W3106541240', 'https://openalex.org/W211509693', 'https://openalex.org/W2890007195', 'https://openalex.org/W3117640374', 'https://openalex.org/W2467834614', 'https://openalex.org/W4287597717', 'https://openalex.org/W2133564696', 'https://openalex.org/W2958953787', 'https://openalex.org/W2963403868', 'https://openalex.org/W2746762542']",2021-01-01
https://openalex.org/W222053410,,Statistical Significance Tests for Machine Translation Evaluation.,"If two translation systems differ differ in performance on a test set, can we trust that this indicates a difference in true system quality? To answer this question, we describe bootstrap resampling methods to compute statistical significance of test results, and validate them on the concrete example of the BLEU score. Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real.","['https://openalex.org/W2160233880', 'https://openalex.org/W2101105183', 'https://openalex.org/W1498238796', 'https://openalex.org/W2331432542', 'https://openalex.org/W2998215494', 'https://openalex.org/W2086825453', 'https://openalex.org/W133045130', 'https://openalex.org/W2097333193', 'https://openalex.org/W2111798208', 'https://openalex.org/W2088781183', 'https://openalex.org/W2626190081', 'https://openalex.org/W2153653739', 'https://openalex.org/W2170120409', 'https://openalex.org/W2797563284', 'https://openalex.org/W2136657878']",2004-07-01
https://openalex.org/W2970279348,https://doi.org/10.18653/v1/w19-5301,Findings of the 2019 Conference on Machine Translation (WMT19),"Loïc Barrault, Ondřej Bojar, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias Müller, Santanu Pal, Matt Post, Marcos Zampieri. Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1). 2019.","['https://openalex.org/W2087735403', 'https://openalex.org/W2970663072', 'https://openalex.org/W2933138175', 'https://openalex.org/W2964240726', 'https://openalex.org/W2970852256', 'https://openalex.org/W3103849166', 'https://openalex.org/W2970558573', 'https://openalex.org/W2970206648', 'https://openalex.org/W2970259631', 'https://openalex.org/W2566623769', 'https://openalex.org/W2970352405', 'https://openalex.org/W4385245566', 'https://openalex.org/W2149327368', 'https://openalex.org/W2888555799', 'https://openalex.org/W2971320541', 'https://openalex.org/W2945383715', 'https://openalex.org/W2159107349', 'https://openalex.org/W2962732637', 'https://openalex.org/W2970842844', 'https://openalex.org/W2970609390', 'https://openalex.org/W2971031401', 'https://openalex.org/W2970947975', 'https://openalex.org/W2906987001', 'https://openalex.org/W2970295111', 'https://openalex.org/W2595715041', 'https://openalex.org/W2970999217', 'https://openalex.org/W2902463012', 'https://openalex.org/W2970858854', 'https://openalex.org/W2613895792', 'https://openalex.org/W2970731908', 'https://openalex.org/W2970717663', 'https://openalex.org/W2980941473', 'https://openalex.org/W4289375695', 'https://openalex.org/W2971141904', 'https://openalex.org/W2970659112', 'https://openalex.org/W2964308564', 'https://openalex.org/W2954157266', 'https://openalex.org/W2903061071', 'https://openalex.org/W2888429796', 'https://openalex.org/W2120699290', 'https://openalex.org/W2970770300', 'https://openalex.org/W2902023229', 'https://openalex.org/W2971337358', 'https://openalex.org/W4288400010', 'https://openalex.org/W2970676607', 'https://openalex.org/W2970397283', 'https://openalex.org/W2970876710', 'https://openalex.org/W2970845737', 'https://openalex.org/W2963281280', 'https://openalex.org/W2896101215', 'https://openalex.org/W2971149470', 'https://openalex.org/W2970281725', 'https://openalex.org/W2970410759', 'https://openalex.org/W2115081467', 'https://openalex.org/W2903277914', 'https://openalex.org/W2971334638', 'https://openalex.org/W2971005778', 'https://openalex.org/W2895810819', 'https://openalex.org/W2970705013', 'https://openalex.org/W2955163189', 'https://openalex.org/W4289765799', 'https://openalex.org/W2114579619', 'https://openalex.org/W2251994258', 'https://openalex.org/W2806885904', 'https://openalex.org/W2970810481', 'https://openalex.org/W2970916202', 'https://openalex.org/W2739602161', 'https://openalex.org/W2251926178', 'https://openalex.org/W2970677459', 'https://openalex.org/W2970134241', 'https://openalex.org/W2970845336', 'https://openalex.org/W2965052543', 'https://openalex.org/W2962784628', 'https://openalex.org/W2970682420', 'https://openalex.org/W2970529093', 'https://openalex.org/W2970758702', 'https://openalex.org/W2952103439', 'https://openalex.org/W2101105183', 'https://openalex.org/W2944815030', 'https://openalex.org/W2963403868', 'https://openalex.org/W2971255462', 'https://openalex.org/W2970791445', 'https://openalex.org/W2962824887', 'https://openalex.org/W2970885024', 'https://openalex.org/W2147192413', 'https://openalex.org/W2970550184', 'https://openalex.org/W2963099583', 'https://openalex.org/W2395259056', 'https://openalex.org/W2970210628', 'https://openalex.org/W2139940533', 'https://openalex.org/W2970871182', 'https://openalex.org/W2964343359', 'https://openalex.org/W2952468927', 'https://openalex.org/W2740433069', 'https://openalex.org/W2970087702', 'https://openalex.org/W2260677151', 'https://openalex.org/W2907279971', 'https://openalex.org/W2886288106', 'https://openalex.org/W2971257618', 'https://openalex.org/W2889326796', 'https://openalex.org/W2902180983', 'https://openalex.org/W2970644300', 'https://openalex.org/W2970544750', 'https://openalex.org/W2760656271', 'https://openalex.org/W2970834363', 'https://openalex.org/W2794365787', 'https://openalex.org/W2270190199', 'https://openalex.org/W2922158773', 'https://openalex.org/W2971089788', 'https://openalex.org/W2250864392', 'https://openalex.org/W2970009562', 'https://openalex.org/W2971107826', 'https://openalex.org/W2970494057', 'https://openalex.org/W2914120296', 'https://openalex.org/W2970248987', 'https://openalex.org/W2169279899', 'https://openalex.org/W2902767466', 'https://openalex.org/W2903193068', 'https://openalex.org/W2252166243', 'https://openalex.org/W2902170865', 'https://openalex.org/W2971081268', 'https://openalex.org/W2148365102', 'https://openalex.org/W2954254257', 'https://openalex.org/W2740452944', 'https://openalex.org/W2917324689', 'https://openalex.org/W2888893419', 'https://openalex.org/W2971073020', 'https://openalex.org/W2971254483', 'https://openalex.org/W2946068894', 'https://openalex.org/W2953173959', 'https://openalex.org/W2971306673', 'https://openalex.org/W2963858717', 'https://openalex.org/W1089141', 'https://openalex.org/W2970900338', 'https://openalex.org/W2970152385', 'https://openalex.org/W2970752831', 'https://openalex.org/W2795247881', 'https://openalex.org/W2963250244', 'https://openalex.org/W2152311128', 'https://openalex.org/W2560647685', 'https://openalex.org/W2133564696']",2019-01-01
https://openalex.org/W3113908264,https://doi.org/10.1609/aaai.v35i14.17509,"Listen, Understand and Translate: Triple Supervision Decouples End-to-end Speech-to-text Translation","An end-to-end speech-to-text translation (ST) takes audio in a source language and outputs the text in a target language. Existing methods are limited by the amount of parallel corpus. Can we build a system to fully utilize signals in a parallel ST corpus? We are inspired by human understanding system which is composed of auditory perception and cognitive processing. In this paper, we propose Listen-Understand-Translate, (LUT), a unified framework with triple supervision signals to decouple the end-to-end speech-to-text translation task. LUT is able to guide the acoustic encoder to extract as much information from the auditory input. In addition, LUT utilizes a pre-trained BERT model to enforce the upper encoder to produce as much semantic information as possible, without extra data. We perform experiments on a diverse set of speech translation benchmarks, including Librispeech English-French, IWSLT English-German and TED English-Chinese. Our results demonstrate LUT achieves the state-of-the-art performance, outperforming previous methods. The code is available at https://github.com/dqqcasia/st.","['https://openalex.org/W2788575190', 'https://openalex.org/W2990025607', 'https://openalex.org/W6770980983', 'https://openalex.org/W2891066653', 'https://openalex.org/W2739427748', 'https://openalex.org/W2982727400', 'https://openalex.org/W2786891429', 'https://openalex.org/W2582956876', 'https://openalex.org/W2977183928', 'https://openalex.org/W2896457183', 'https://openalex.org/W2978099976', 'https://openalex.org/W2973048981', 'https://openalex.org/W2892009249', 'https://openalex.org/W2904571617', 'https://openalex.org/W2466918907', 'https://openalex.org/W2922709902', 'https://openalex.org/W6779590286', 'https://openalex.org/W7027429494', 'https://openalex.org/W6768544969', 'https://openalex.org/W6776039324', 'https://openalex.org/W6765485641', 'https://openalex.org/W2745985008', 'https://openalex.org/W6719020539', 'https://openalex.org/W6631190155', 'https://openalex.org/W2785350307', 'https://openalex.org/W6759455113', 'https://openalex.org/W6756411807', 'https://openalex.org/W3027656574', 'https://openalex.org/W6761144991', 'https://openalex.org/W2995732811', 'https://openalex.org/W2980286501', 'https://openalex.org/W2936774411', 'https://openalex.org/W2982129078', 'https://openalex.org/W2251321385', 'https://openalex.org/W6756285817', 'https://openalex.org/W2948845926', 'https://openalex.org/W2605202026', 'https://openalex.org/W2936969148', 'https://openalex.org/W6763244853', 'https://openalex.org/W2981622296', 'https://openalex.org/W2973620348', 'https://openalex.org/W3018997018', 'https://openalex.org/W6752756761', 'https://openalex.org/W2605131327', 'https://openalex.org/W2967985939', 'https://openalex.org/W6764278823', 'https://openalex.org/W2986772082', 'https://openalex.org/W2964161387', 'https://openalex.org/W2938973646', 'https://openalex.org/W2964121744', 'https://openalex.org/W154469489', 'https://openalex.org/W399167303', 'https://openalex.org/W3017258074', 'https://openalex.org/W2951635603', 'https://openalex.org/W2127141656', 'https://openalex.org/W3028712640', 'https://openalex.org/W2121495627', 'https://openalex.org/W3015703505', 'https://openalex.org/W2398009384', 'https://openalex.org/W4287874506', 'https://openalex.org/W3095950464', 'https://openalex.org/W2952079278', 'https://openalex.org/W1920064753', 'https://openalex.org/W53604701', 'https://openalex.org/W2963779652', 'https://openalex.org/W2103123519', 'https://openalex.org/W3006988520', 'https://openalex.org/W4288021071', 'https://openalex.org/W4385245566', 'https://openalex.org/W2397318268', 'https://openalex.org/W2995999067', 'https://openalex.org/W2963341956', 'https://openalex.org/W2989819126', 'https://openalex.org/W2400169135', 'https://openalex.org/W2901607128', 'https://openalex.org/W4394640960', 'https://openalex.org/W2402827806', 'https://openalex.org/W3007142233', 'https://openalex.org/W3018778770', 'https://openalex.org/W2408724534', 'https://openalex.org/W2902646476', 'https://openalex.org/W2998386507', 'https://openalex.org/W2951289103', 'https://openalex.org/W2114483840', 'https://openalex.org/W4394649814', 'https://openalex.org/W3037217258', 'https://openalex.org/W2972448360', 'https://openalex.org/W2970049541', 'https://openalex.org/W2404511972', 'https://openalex.org/W2089886978', 'https://openalex.org/W3016139774', 'https://openalex.org/W2963827914', 'https://openalex.org/W2963736842', 'https://openalex.org/W2949328740', 'https://openalex.org/W1522301498', 'https://openalex.org/W3034571331', 'https://openalex.org/W2991786320', 'https://openalex.org/W2964172053', 'https://openalex.org/W2986963494', 'https://openalex.org/W2962680099', 'https://openalex.org/W2950318067', 'https://openalex.org/W3008549139', 'https://openalex.org/W3163093788', 'https://openalex.org/W3034368386', 'https://openalex.org/W4300558631', 'https://openalex.org/W2899773543', 'https://openalex.org/W2972584841', 'https://openalex.org/W2841603554', 'https://openalex.org/W2963403868', 'https://openalex.org/W2955541912', 'https://openalex.org/W2952167535', 'https://openalex.org/W2928075308', 'https://openalex.org/W2936848022', 'https://openalex.org/W2186089609', 'https://openalex.org/W2996854111', 'https://openalex.org/W615284875', 'https://openalex.org/W2896388224', 'https://openalex.org/W2139647714', 'https://openalex.org/W3027207958', 'https://openalex.org/W2914120296', 'https://openalex.org/W2997436923']",2021-05-18
https://openalex.org/W2508809683,https://doi.org/10.18653/v1/w16-2347,Findings of the WMT 2016 Bilingual Document Alignment Shared Task,"This paper presents the results of the WMT16 Bilingual Document Alignment Shared Task.Given crawls of web sites, we asked participants to align documents that are translations of each other.11 research groups submitted 19 systems, with a top performance of 95.0%.","['https://openalex.org/W2250929565', 'https://openalex.org/W2509782044', 'https://openalex.org/W136094774', 'https://openalex.org/W2148334774', 'https://openalex.org/W2511852057', 'https://openalex.org/W2087735403', 'https://openalex.org/W2517504876', 'https://openalex.org/W2186890008', 'https://openalex.org/W2576742006', 'https://openalex.org/W2513768144', 'https://openalex.org/W2509251152', 'https://openalex.org/W2252025035', 'https://openalex.org/W22168010', 'https://openalex.org/W2595715041', 'https://openalex.org/W2518598732', 'https://openalex.org/W6725808569', 'https://openalex.org/W6744844098', 'https://openalex.org/W2507478797', 'https://openalex.org/W2514208185', 'https://openalex.org/W1819903106', 'https://openalex.org/W2509928212', 'https://openalex.org/W2251114426', 'https://openalex.org/W170711724', 'https://openalex.org/W2460354904', 'https://openalex.org/W6601523051', 'https://openalex.org/W6681116826', 'https://openalex.org/W6662462842', 'https://openalex.org/W2787646045', 'https://openalex.org/W2508803094', 'https://openalex.org/W6681654413', 'https://openalex.org/W1992057091', 'https://openalex.org/W338621447', 'https://openalex.org/W6685203456', 'https://openalex.org/W2185142058', 'https://openalex.org/W2101096097', 'https://openalex.org/W6723821839', 'https://openalex.org/W2170632340', 'https://openalex.org/W6731744468', 'https://openalex.org/W2170716095', 'https://openalex.org/W2124807415', 'https://openalex.org/W2513102468', 'https://openalex.org/W37508832', 'https://openalex.org/W2512814023', 'https://openalex.org/W2145080939', 'https://openalex.org/W2758884106', 'https://openalex.org/W2250593294', 'https://openalex.org/W2496235729', 'https://openalex.org/W2145251161', 'https://openalex.org/W2115081467', 'https://openalex.org/W2572474373', 'https://openalex.org/W2047295649']",2016-01-01
https://openalex.org/W3202201199,https://doi.org/10.18653/v1/2021.findings-emnlp.247,On the Complementarity between Pre-Training and Back-Translation for Neural Machine Translation,"Pre-training (PT) and back-translation (BT) are two simple and powerful methods to utilize monolingual data for improving the model performance of neural machine translation (NMT). This paper takes the first step to investigate the complementarity between PT and BT. We introduce two probing tasks for PT and BT respectively and find that PT mainly contributes to the encoder module while BT brings more benefits to the decoder. Experimental results show that PT and BT are nicely complementary to each other, establishing state-of-the-art performances on the WMT16 English-Romanian and English-Russian benchmarks. Through extensive analyses on sentence originality and word frequency, we also demonstrate that combining Tagged BT with PT is more helpful to their complementarity, leading to better translation quality. Source code is freely available at https://github.com/SunbowLiu/PTvsBT.","['https://openalex.org/W2922349260', 'https://openalex.org/W3164747806', 'https://openalex.org/W3105378761', 'https://openalex.org/W3036120435', 'https://openalex.org/W2996854111', 'https://openalex.org/W3176603070', 'https://openalex.org/W3035390927', 'https://openalex.org/W3006381853', 'https://openalex.org/W2970810442', 'https://openalex.org/W3046368065', 'https://openalex.org/W3173367141', 'https://openalex.org/W3034938700', 'https://openalex.org/W3214581470', 'https://openalex.org/W2963532001', 'https://openalex.org/W3034999214', 'https://openalex.org/W2952468927', 'https://openalex.org/W3174805484', 'https://openalex.org/W3174089676', 'https://openalex.org/W2962784628', 'https://openalex.org/W2971043182', 'https://openalex.org/W4287694131', 'https://openalex.org/W3106321930', 'https://openalex.org/W3113395007', 'https://openalex.org/W3036839309', 'https://openalex.org/W2964308564', 'https://openalex.org/W2963403868', 'https://openalex.org/W3174255604', 'https://openalex.org/W2994928925', 'https://openalex.org/W2970015022', 'https://openalex.org/W3034474651', 'https://openalex.org/W2976223659', 'https://openalex.org/W2944815030', 'https://openalex.org/W2962801832', 'https://openalex.org/W4385245566', 'https://openalex.org/W2964265128', 'https://openalex.org/W3035317912', 'https://openalex.org/W3001434439', 'https://openalex.org/W3107826490', 'https://openalex.org/W2971302374', 'https://openalex.org/W2101105183', 'https://openalex.org/W2561274697', 'https://openalex.org/W2149327368', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963216553', 'https://openalex.org/W2889326796', 'https://openalex.org/W2888808532', 'https://openalex.org/W3104976898', 'https://openalex.org/W4287874506', 'https://openalex.org/W3194357474', 'https://openalex.org/W2133564696', 'https://openalex.org/W2995746049', 'https://openalex.org/W3092327118', 'https://openalex.org/W3034716087', 'https://openalex.org/W4285719527', 'https://openalex.org/W3105912780', 'https://openalex.org/W2613904329', 'https://openalex.org/W3105990194', 'https://openalex.org/W2914120296', 'https://openalex.org/W3174481817']",2021-01-01
https://openalex.org/W3034571331,https://doi.org/10.18653/v1/2020.acl-main.344,Curriculum Pre-training for End-to-End Speech Translation,"End-to-end speech translation poses a heavy burden on the encoder because it has to transcribe, understand, and learn cross-lingual semantics simultaneously. To obtain a powerful encoder, traditional methods pre-train it on ASR data to capture speech features. However, we argue that pre-training the encoder only through simple speech recognition is not enough, and high-level linguistic knowledge should be considered. Inspired by this, we propose a curriculum pre-training method that includes an elementary course for transcription learning and two advanced courses for understanding the utterance and mapping words in two languages. The difficulty of these courses is gradually increasing. Experiments show that our curriculum pre-training method leads to significant improvements on En-De and En-Fr speech translation benchmarks.","['https://openalex.org/W2747874407', 'https://openalex.org/W2992632249', 'https://openalex.org/W2620757702', 'https://openalex.org/W2964102148', 'https://openalex.org/W2251321385', 'https://openalex.org/W2981991061', 'https://openalex.org/W2785350307', 'https://openalex.org/W2964161387', 'https://openalex.org/W4288021071', 'https://openalex.org/W1537859740', 'https://openalex.org/W2133564696', 'https://openalex.org/W3095189764', 'https://openalex.org/W2936969148', 'https://openalex.org/W2402577742', 'https://openalex.org/W2113106066', 'https://openalex.org/W2989819126', 'https://openalex.org/W3007328579', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963834942', 'https://openalex.org/W4298018319', 'https://openalex.org/W2466918907', 'https://openalex.org/W2997436923', 'https://openalex.org/W2962824709', 'https://openalex.org/W4300558631', 'https://openalex.org/W2345837149', 'https://openalex.org/W2964327384', 'https://openalex.org/W1494198834', 'https://openalex.org/W2327501763', 'https://openalex.org/W2949328740', 'https://openalex.org/W2964308564', 'https://openalex.org/W2963250244', 'https://openalex.org/W4385245566', 'https://openalex.org/W2964172053', 'https://openalex.org/W2605131327', 'https://openalex.org/W2936774411', 'https://openalex.org/W2963341956', 'https://openalex.org/W3015703505', 'https://openalex.org/W3008549139', 'https://openalex.org/W2296073425', 'https://openalex.org/W1902237438', 'https://openalex.org/W2962680099', 'https://openalex.org/W2962780374', 'https://openalex.org/W2972448360', 'https://openalex.org/W2955541912', 'https://openalex.org/W2134546430']",2020-01-01
https://openalex.org/W3176382501,https://doi.org/10.18653/v1/2021.acl-long.204,Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders,"Chen Xu, Bojie Hu, Yanyang Li, Yuhao Zhang, Shen Huang, Qi Ju, Tong Xiao, Jingbo Zhu. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.","['https://openalex.org/W2595715041', 'https://openalex.org/W3034571331', 'https://openalex.org/W3096490862', 'https://openalex.org/W2949328740', 'https://openalex.org/W2466918907', 'https://openalex.org/W2996854111', 'https://openalex.org/W2962780374', 'https://openalex.org/W3097777922', 'https://openalex.org/W2134546430', 'https://openalex.org/W4300558631', 'https://openalex.org/W2982129078', 'https://openalex.org/W2936969148', 'https://openalex.org/W3176113130', 'https://openalex.org/W4287874506', 'https://openalex.org/W2972389417', 'https://openalex.org/W2997436923', 'https://openalex.org/W4385245566', 'https://openalex.org/W3105825505', 'https://openalex.org/W2945700568', 'https://openalex.org/W3006988520', 'https://openalex.org/W2963779652', 'https://openalex.org/W2964089206', 'https://openalex.org/W53604701', 'https://openalex.org/W4287629556', 'https://openalex.org/W1821462560', 'https://openalex.org/W4394649814', 'https://openalex.org/W2964172053', 'https://openalex.org/W2113106066', 'https://openalex.org/W2766219058', 'https://openalex.org/W2605131327', 'https://openalex.org/W2964302946', 'https://openalex.org/W2962739339', 'https://openalex.org/W3037217258', 'https://openalex.org/W2964067969', 'https://openalex.org/W3113676066', 'https://openalex.org/W2972448360', 'https://openalex.org/W3173417753', 'https://openalex.org/W2785350307', 'https://openalex.org/W2963532001', 'https://openalex.org/W3007142233', 'https://openalex.org/W2127141656', 'https://openalex.org/W2962784628', 'https://openalex.org/W3097301532', 'https://openalex.org/W2936774411', 'https://openalex.org/W2963403868']",2021-01-01
https://openalex.org/W3176455679,https://doi.org/10.18653/v1/2021.findings-acl.195,Learning Shared Semantic Space for Speech-to-Text Translation,"Having numerous potential applications and great impact, end-to-end speech translation (ST) has long been treated as an independent task, failing to fully draw strength from the rapid advances of its sibling -text machine translation (MT).With text and audio inputs represented differently, the modality gap has rendered MT data and its end-to-end models incompatible with their ST counterparts.In observation of this obstacle, we propose to bridge this representation gap with Chimera.By projecting audio and text features to a common semantic representation, Chimera unifies MT and ST tasks and boosts the performance on ST benchmarks, MuST-C and Augmented Librispeech, to a new state-of-the-art.Specifically, Chimera obtains 27.1 BLEU on MuST-C EN-DE, improving the SOTA by a +1.9 BLEU margin.Further experimental analyses demonstrate that the shared semantic space indeed conveys common knowledge between these two tasks and thus paves a new way for augmenting training resources across modalities. 1","['https://openalex.org/W2933138175', 'https://openalex.org/W4394640960', 'https://openalex.org/W3177215302', 'https://openalex.org/W2089886978', 'https://openalex.org/W2955541912', 'https://openalex.org/W3113908264', 'https://openalex.org/W3176113130', 'https://openalex.org/W3097301532', 'https://openalex.org/W2398009384', 'https://openalex.org/W3007142233', 'https://openalex.org/W2997436923', 'https://openalex.org/W2067002176', 'https://openalex.org/W2902646476', 'https://openalex.org/W2885323099', 'https://openalex.org/W2978099976', 'https://openalex.org/W4385245566', 'https://openalex.org/W2040293895', 'https://openalex.org/W4288817190', 'https://openalex.org/W3016139774', 'https://openalex.org/W3099782249', 'https://openalex.org/W2951635603', 'https://openalex.org/W2899015110', 'https://openalex.org/W2962680099', 'https://openalex.org/W4287629556', 'https://openalex.org/W3096490862', 'https://openalex.org/W2512924740', 'https://openalex.org/W2989819126', 'https://openalex.org/W3163093788', 'https://openalex.org/W2965538726', 'https://openalex.org/W4394649814', 'https://openalex.org/W3162037819', 'https://openalex.org/W2101105183', 'https://openalex.org/W2605202026', 'https://openalex.org/W3036601975', 'https://openalex.org/W3017454464', 'https://openalex.org/W3113676066', 'https://openalex.org/W2404511972', 'https://openalex.org/W2949328740', 'https://openalex.org/W2950613790', 'https://openalex.org/W3093218022', 'https://openalex.org/W2899773543', 'https://openalex.org/W2972448360', 'https://openalex.org/W2419539795', 'https://openalex.org/W2257408573', 'https://openalex.org/W2977183928', 'https://openalex.org/W4288021071', 'https://openalex.org/W3006988520', 'https://openalex.org/W2963403868', 'https://openalex.org/W2986963494', 'https://openalex.org/W2964048171', 'https://openalex.org/W2785350307', 'https://openalex.org/W2982727400', 'https://openalex.org/W2745985008', 'https://openalex.org/W2945700568', 'https://openalex.org/W2964172053', 'https://openalex.org/W3034571331', 'https://openalex.org/W2123480151', 'https://openalex.org/W3008549139', 'https://openalex.org/W3017650141', 'https://openalex.org/W2964161387', 'https://openalex.org/W2945286432', 'https://openalex.org/W2901607128', 'https://openalex.org/W2466918907', 'https://openalex.org/W2952079278', 'https://openalex.org/W3035464238', 'https://openalex.org/W2145039217', 'https://openalex.org/W3034719878', 'https://openalex.org/W3105669983', 'https://openalex.org/W2973048981', 'https://openalex.org/W2128226370', 'https://openalex.org/W2982129078', 'https://openalex.org/W3015703505', 'https://openalex.org/W2962778428']",2021-01-01
https://openalex.org/W3103169714,https://doi.org/10.18653/v1/2020.emnlp-main.475,Dynamic Data Selection and Weighting for Iterative Back-Translation,"Back-translation has proven to be an effective method to utilize monolingual data in neural machine translation (NMT), and iteratively conducting back-translation can further improve the model performance. Selecting which monolingual data to back-translate is crucial, as we require that the resulting synthetic data are of high quality and reflect the target domain. To achieve these two goals, data selection and weighting strategies have been proposed, with a common practice being to select samples close to the target domain but also dissimilar to the average general-domain text. In this paper, we provide insights into this commonly used approach and generalize it to a dynamic curriculum learning strategy, which is applied to iterative back-translation models. In addition, we propose weighting strategies based on both the current quality of the sentence and its improvement over the previous iteration. We evaluate our models on domain adaptation, low-resource, and high-resource MT settings and on two language pairs. Experimental results demonstrate that our methods achieve improvements of up to 1.8 BLEU points over competitive baselines.","['https://openalex.org/W2922349260', 'https://openalex.org/W2950940239', 'https://openalex.org/W2786253471', 'https://openalex.org/W2963366389', 'https://openalex.org/W2147262247', 'https://openalex.org/W2134800885', 'https://openalex.org/W2251590347', 'https://openalex.org/W2971302374', 'https://openalex.org/W2472373455', 'https://openalex.org/W2951476960', 'https://openalex.org/W2963216553', 'https://openalex.org/W2962784628', 'https://openalex.org/W3035019713', 'https://openalex.org/W1905522558', 'https://openalex.org/W2021618504', 'https://openalex.org/W2939335894', 'https://openalex.org/W4307459710', 'https://openalex.org/W2952474700', 'https://openalex.org/W2561274697', 'https://openalex.org/W2971074957', 'https://openalex.org/W2250771471', 'https://openalex.org/W2923622379', 'https://openalex.org/W2886095922', 'https://openalex.org/W2971120958', 'https://openalex.org/W2546938941', 'https://openalex.org/W2101105183', 'https://openalex.org/W2147227066', 'https://openalex.org/W2963341956', 'https://openalex.org/W630532510', 'https://openalex.org/W2117278770', 'https://openalex.org/W2251150371', 'https://openalex.org/W2896457183', 'https://openalex.org/W2963281280', 'https://openalex.org/W4298137069', 'https://openalex.org/W2946379889', 'https://openalex.org/W2970064096', 'https://openalex.org/W2963403868', 'https://openalex.org/W2970156971', 'https://openalex.org/W2889326796', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963569817']",2020-01-01
https://openalex.org/W2997436923,https://doi.org/10.1609/aaai.v34i05.6452,Bridging the Gap between Pre-Training and Fine-Tuning for End-to-End Speech Translation,"End-to-end speech translation, a hot topic in recent years, aims to translate a segment of audio into a specific language with an end-to-end model. Conventional approaches employ multi-task learning and pre-training methods for this task, but they suffer from the huge gap between pre-training and fine-tuning. To address these issues, we propose a Tandem Connectionist Encoding Network (TCEN) which bridges the gap by reusing all subnets in fine-tuning, keeping the roles of subnets consistent, and pre-training the attention module. Furthermore, we propose two simple but effective methods to guarantee the speech encoder outputs and the MT encoder inputs are consistent in terms of semantic representation and sequence length. Experimental results show that our model leads to significant improvements in En-De and En-Fr translation irrespective of the backbones.","['https://openalex.org/W2788575190', 'https://openalex.org/W2133564696', 'https://openalex.org/W2891066653', 'https://openalex.org/W6713725665', 'https://openalex.org/W2582956876', 'https://openalex.org/W2786891429', 'https://openalex.org/W2892009249', 'https://openalex.org/W2466918907', 'https://openalex.org/W7027429494', 'https://openalex.org/W6725739302', 'https://openalex.org/W6765485641', 'https://openalex.org/W6651323761', 'https://openalex.org/W2745985008', 'https://openalex.org/W2785350307', 'https://openalex.org/W2897105542', 'https://openalex.org/W6761144991', 'https://openalex.org/W6685145238', 'https://openalex.org/W2951456627', 'https://openalex.org/W1537859740', 'https://openalex.org/W2620757702', 'https://openalex.org/W3167238645', 'https://openalex.org/W2113106066', 'https://openalex.org/W2936774411', 'https://openalex.org/W2402146185', 'https://openalex.org/W2251321385', 'https://openalex.org/W6739901393', 'https://openalex.org/W2605131327', 'https://openalex.org/W2963979492', 'https://openalex.org/W2402577742', 'https://openalex.org/W3015703505', 'https://openalex.org/W2936848022', 'https://openalex.org/W4300558631', 'https://openalex.org/W2964172053', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963842982', 'https://openalex.org/W2950613790', 'https://openalex.org/W2949328740', 'https://openalex.org/W2963779652', 'https://openalex.org/W2962780374', 'https://openalex.org/W2963446712', 'https://openalex.org/W4252331534']",2020-04-03
https://openalex.org/W2886095922,https://doi.org/10.18653/v1/w18-2703,Iterative Back-Translation for Neural Machine Translation,"We present iterative back-translation, a method for generating increasingly better synthetic parallel data from monolingual data to train neural machine translation systems. Our proposed method is very simple yet effective and highly applicable in practice. We demonstrate improvements in neural machine translation quality in both high and low resourced scenarios, including the best reported BLEU scores for the WMT 2017 German↔English tasks.","['https://openalex.org/W2962784628', 'https://openalex.org/W2124807415', 'https://openalex.org/W2963333747', 'https://openalex.org/W2963631431', 'https://openalex.org/W3082674894', 'https://openalex.org/W2546938941', 'https://openalex.org/W148133648', 'https://openalex.org/W2778814079', 'https://openalex.org/W2786991584', 'https://openalex.org/W2764163036', 'https://openalex.org/W2133564696', 'https://openalex.org/W2048679005', 'https://openalex.org/W2963216553', 'https://openalex.org/W2962801832', 'https://openalex.org/W2964343359', 'https://openalex.org/W2740433069', 'https://openalex.org/W2903193068', 'https://openalex.org/W2760656271', 'https://openalex.org/W2757049299', 'https://openalex.org/W2963506925', 'https://openalex.org/W2964308564', 'https://openalex.org/W4205512866', 'https://openalex.org/W2595715041', 'https://openalex.org/W211509693', 'https://openalex.org/W2756566411', 'https://openalex.org/W1915251500']",2018-01-01
https://openalex.org/W3113676066,https://doi.org/10.1609/aaai.v35i14.17508,Consecutive Decoding for Speech-to-text Translation,"Speech-to-text translation (ST), which directly translates the source language speech to the target language text, has attracted intensive attention recently. However, the combination of speech recognition and machine translation in a single model poses a heavy burden on the direct cross-modal cross-lingual mapping. To reduce the learning difficulty, we propose COnSecutive Transcription and Translation (COSTT), an integral approach for speech-to-text translation. The key idea is to generate source transcript and target translation text with a single decoder. It benefits the model training so that additional large parallel text corpus can be fully exploited to enhance the speech translation training. Our method is verified on three mainstream datasets, including Augmented LibriSpeech English-French dataset, TED English-German dataset, and TED English-Chinese dataset. Experiments show that our proposed COSTT outperforms the previous state-of-the-art methods. The code is available at https://github.com/dqqcasia/st.","['https://openalex.org/W2788575190', 'https://openalex.org/W2990025607', 'https://openalex.org/W6770980983', 'https://openalex.org/W2891066653', 'https://openalex.org/W2982727400', 'https://openalex.org/W2786891429', 'https://openalex.org/W2582956876', 'https://openalex.org/W2548332580', 'https://openalex.org/W2977183928', 'https://openalex.org/W6754958169', 'https://openalex.org/W2978099976', 'https://openalex.org/W2973048981', 'https://openalex.org/W2981538293', 'https://openalex.org/W2904571617', 'https://openalex.org/W2466918907', 'https://openalex.org/W2089886978', 'https://openalex.org/W2404511972', 'https://openalex.org/W7027429494', 'https://openalex.org/W6768544969', 'https://openalex.org/W6776039324', 'https://openalex.org/W6765485641', 'https://openalex.org/W2745985008', 'https://openalex.org/W2785350307', 'https://openalex.org/W6759455113', 'https://openalex.org/W6756411807', 'https://openalex.org/W3027656574', 'https://openalex.org/W1920064753', 'https://openalex.org/W6761144991', 'https://openalex.org/W2995732811', 'https://openalex.org/W2398009384', 'https://openalex.org/W2936774411', 'https://openalex.org/W2982129078', 'https://openalex.org/W2911291251', 'https://openalex.org/W6777984565', 'https://openalex.org/W6756285817', 'https://openalex.org/W2948845926', 'https://openalex.org/W2945286432', 'https://openalex.org/W1816313093', 'https://openalex.org/W2605202026', 'https://openalex.org/W2936969148', 'https://openalex.org/W6763244853', 'https://openalex.org/W3017258074', 'https://openalex.org/W2981622296', 'https://openalex.org/W6756627502', 'https://openalex.org/W2973620348', 'https://openalex.org/W3018997018', 'https://openalex.org/W6752756761', 'https://openalex.org/W2605131327', 'https://openalex.org/W6764278823', 'https://openalex.org/W2986963494', 'https://openalex.org/W3034625919', 'https://openalex.org/W2951635603', 'https://openalex.org/W2989819126', 'https://openalex.org/W3034332156', 'https://openalex.org/W4394649814', 'https://openalex.org/W3163093788', 'https://openalex.org/W2955541912', 'https://openalex.org/W2963403868', 'https://openalex.org/W2962680099', 'https://openalex.org/W4300558631', 'https://openalex.org/W2997436923', 'https://openalex.org/W2973180718', 'https://openalex.org/W2127141656', 'https://openalex.org/W3006988520', 'https://openalex.org/W2964161387', 'https://openalex.org/W4394640960', 'https://openalex.org/W2963779652', 'https://openalex.org/W2902646476', 'https://openalex.org/W3015703505', 'https://openalex.org/W3035490255', 'https://openalex.org/W2998386507', 'https://openalex.org/W2949328740', 'https://openalex.org/W2841603554', 'https://openalex.org/W4385245566', 'https://openalex.org/W2970049541', 'https://openalex.org/W2962784628', 'https://openalex.org/W4288021071', 'https://openalex.org/W2938973646', 'https://openalex.org/W2952079278', 'https://openalex.org/W3008549139', 'https://openalex.org/W2964172053', 'https://openalex.org/W2952167535', 'https://openalex.org/W2914120296', 'https://openalex.org/W3018778770', 'https://openalex.org/W3007142233', 'https://openalex.org/W3037217258', 'https://openalex.org/W2964048171', 'https://openalex.org/W3034571331', 'https://openalex.org/W3095950464', 'https://openalex.org/W2899773543', 'https://openalex.org/W3016139774', 'https://openalex.org/W2901607128', 'https://openalex.org/W2972448360']",2021-05-18
https://openalex.org/W2962788625,https://doi.org/10.1109/icassp.2018.8462665,Generalized End-to-End Loss for Speaker Verification,"In this paper, we propose a new loss function called generalized end-to-end (GE2E) loss, which makes the training of speaker verification models more efficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process. Additionally, the GE2E loss does not require an initial stage of example selection. With these properties, our model with the new loss function decreases speaker verification EER by more than 10%, while reducing the training time by 60% at the same time. We also introduce the MultiReader technique, which allows us to do domain adaptation - training a more accurate model that supports multiple keywords (i.e., ""OK Google"" and ""Hey Google"") as well as multiple dialects.","['https://openalex.org/W6737575990', 'https://openalex.org/W6732884072', 'https://openalex.org/W2114925438', 'https://openalex.org/W2064675550', 'https://openalex.org/W2096733369', 'https://openalex.org/W6696934422', 'https://openalex.org/W6712909570', 'https://openalex.org/W2121750345', 'https://openalex.org/W2154278880', 'https://openalex.org/W1485222997', 'https://openalex.org/W2034940213', 'https://openalex.org/W2046056978', 'https://openalex.org/W2150769028', 'https://openalex.org/W6713401928', 'https://openalex.org/W2584329820', 'https://openalex.org/W2402195372', 'https://openalex.org/W2963068250', 'https://openalex.org/W2293634267', 'https://openalex.org/W2402302915', 'https://openalex.org/W2612434969']",2018-04-01
https://openalex.org/W4372260139,https://doi.org/10.1109/icassp49357.2023.10095578,Improving Speech-to-Speech Translation Through Unlabeled Text,"Direct speech-to-speech translation (S2ST) is among the most challenging problems in the translation paradigm due to the significant scarcity of S2ST data. While effort has been made to increase the data size from unlabeled speech by cascading pretrained speech recognition (ASR), machine translation (MT) and text-to-speech (TTS) models; unlabeled text has remained relatively under-utilized to improve S2ST. We propose an effective way to utilize the massive existing unlabeled text from different languages to create a large amount of S2ST data to improve S2ST performance by applying various acoustic effects to the generated synthetic data. Empirically our method outperforms the state of the art in Spanish-English translation by up to 2 BLEU. Significant gains by the proposed method are demonstrated in extremely low-resource settings for both Spanish-English and Russian-English translations.","['https://openalex.org/W4287854499', 'https://openalex.org/W6785749098', 'https://openalex.org/W4296070387', 'https://openalex.org/W2995181338', 'https://openalex.org/W6790356757', 'https://openalex.org/W6780218876', 'https://openalex.org/W3119308075', 'https://openalex.org/W3127012371', 'https://openalex.org/W2989384871', 'https://openalex.org/W6798080464', 'https://openalex.org/W6771467084', 'https://openalex.org/W2972495969', 'https://openalex.org/W3095410713', 'https://openalex.org/W6739901393', 'https://openalex.org/W3015522062', 'https://openalex.org/W3001434439', 'https://openalex.org/W3140429000', 'https://openalex.org/W6770212971', 'https://openalex.org/W2964161387', 'https://openalex.org/W3097777922', 'https://openalex.org/W6688816777', 'https://openalex.org/W6780226713', 'https://openalex.org/W6767737316', 'https://openalex.org/W3198299542', 'https://openalex.org/W3209059054', 'https://openalex.org/W2889326796', 'https://openalex.org/W2144561273', 'https://openalex.org/W6781927165', 'https://openalex.org/W2097203679', 'https://openalex.org/W6763832098', 'https://openalex.org/W2136545725', 'https://openalex.org/W6779483622', 'https://openalex.org/W6796464841', 'https://openalex.org/W4385245566', 'https://openalex.org/W3036601975', 'https://openalex.org/W3101648800', 'https://openalex.org/W3054645415', 'https://openalex.org/W3197771105', 'https://openalex.org/W2989539713', 'https://openalex.org/W2946200149', 'https://openalex.org/W4394671563', 'https://openalex.org/W2219249508', 'https://openalex.org/W3036839309', 'https://openalex.org/W2914120296', 'https://openalex.org/W3169905056', 'https://openalex.org/W3015698636', 'https://openalex.org/W3030437843', 'https://openalex.org/W3180374548']",2023-05-05
https://openalex.org/W3034474651,https://doi.org/10.18653/v1/2020.acl-main.532,Tagged Back-translation Revisited: Why Does It Really Work?,"In this paper, we show that neural machine translation (NMT) systems trained on large back-translated data overfit some of the characteristics of machine-translated texts. Such NMT systems better translate human-produced translations, i.e., translationese, but may largely worsen the translation quality of original texts. Our analysis reveals that adding a simple tag to back-translations prevents this quality degradation and improves on average the overall translation quality by helping the NMT system to distinguish back-translated data from original parallel data during training. We also show that, in contrast to high-resource configurations, NMT systems trained in low-resource settings are much less vulnerable to overfit back-translations. We conclude that the back-translations in the training data should always be tagged especially when the origin of the text to be translated is unknown.","['https://openalex.org/W3007782264', 'https://openalex.org/W2889326796', 'https://openalex.org/W4385245566', 'https://openalex.org/W2985165968', 'https://openalex.org/W2963216553', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963532001', 'https://openalex.org/W2970279348', 'https://openalex.org/W2101105183', 'https://openalex.org/W2970015022', 'https://openalex.org/W2952103439', 'https://openalex.org/W2472237015', 'https://openalex.org/W3034716087', 'https://openalex.org/W2902614977', 'https://openalex.org/W2971043182', 'https://openalex.org/W2964343359', 'https://openalex.org/W1779279021', 'https://openalex.org/W2595715041']",2020-01-01
https://openalex.org/W3162471442,https://doi.org/10.1109/icassp39728.2021.9414703,Task Aware Multi-Task Learning for Speech to Text Tasks,"In general, the direct Speech-to-text translation (ST) is jointly trained with Automatic Speech Recognition (ASR), and Machine Translation (MT) tasks. However, the issues with the current joint learning strategies inhibit the knowledge transfer across these tasks. We propose a task modulation network which allows the model to learn task specific features, while learning the shared features simultaneously. This proposed approach removes the need for separate finetuning step resulting in a single model which performs all these tasks. This single model achieves a performance of 28.64 BLEU score on ST MuST-C English-German, WER of 11.61% on ASR TEDLium v3, 23.35 BLEU score on MT WMT'15 English-German task. This sets a new state-of-the-art performance (SOTA) on the ST task while outperforming the existing end-to-end ASR systems.","['https://openalex.org/W1494198834', 'https://openalex.org/W2799473636', 'https://openalex.org/W6771467084', 'https://openalex.org/W2989384871', 'https://openalex.org/W6780516901', 'https://openalex.org/W3096490862', 'https://openalex.org/W2972451902', 'https://openalex.org/W2963532001', 'https://openalex.org/W1902237438', 'https://openalex.org/W6739901393', 'https://openalex.org/W2913340405', 'https://openalex.org/W2760103357', 'https://openalex.org/W2962784628', 'https://openalex.org/W6736057607', 'https://openalex.org/W4300558631', 'https://openalex.org/W3015633994', 'https://openalex.org/W6752136759', 'https://openalex.org/W2963993537', 'https://openalex.org/W2295072214', 'https://openalex.org/W2964309797', 'https://openalex.org/W2964093309', 'https://openalex.org/W2963403868', 'https://openalex.org/W3101648800', 'https://openalex.org/W4385245566', 'https://openalex.org/W2945700568', 'https://openalex.org/W2604763608', 'https://openalex.org/W3015698636', 'https://openalex.org/W2806412155', 'https://openalex.org/W3038092046', 'https://openalex.org/W3030437843']",2021-05-13
https://openalex.org/W3105825505,https://doi.org/10.18653/v1/2020.emnlp-main.644,Effectively pretraining a speech translation decoder with Machine Translation data,"Directly translating from speech to text using an end-to-end approach is still challenging for many language pairs due to insufficient data. Although pretraining the encoder parameters using the Automatic Speech Recognition (ASR) task improves the results in low resource settings, attempting to use pretrained parameters from the Neural Machine Translation (NMT) task has been largely unsuccessful in previous works. In this paper, we will show that by using an adversarial regularizer, we can bring the encoder representations of the ASR and NMT tasks closer even though they are in different modalities, and how this helps us effectively use a pretrained NMT decoder for speech translation.","['https://openalex.org/W1494198834', 'https://openalex.org/W2113106066', 'https://openalex.org/W2982129078', 'https://openalex.org/W3135348217', 'https://openalex.org/W2963403868', 'https://openalex.org/W3015440307', 'https://openalex.org/W2257408573', 'https://openalex.org/W3017535695', 'https://openalex.org/W2963602293', 'https://openalex.org/W2101105183', 'https://openalex.org/W2945700568', 'https://openalex.org/W2971152344', 'https://openalex.org/W2963834942', 'https://openalex.org/W3008549139', 'https://openalex.org/W2112796928', 'https://openalex.org/W2964172053', 'https://openalex.org/W2964102148', 'https://openalex.org/W2921280978', 'https://openalex.org/W2593011301', 'https://openalex.org/W2745785989', 'https://openalex.org/W2466918907', 'https://openalex.org/W1522301498', 'https://openalex.org/W2936969148', 'https://openalex.org/W2963779652', 'https://openalex.org/W2963767893', 'https://openalex.org/W3007142233', 'https://openalex.org/W4300558631', 'https://openalex.org/W2563850823', 'https://openalex.org/W2936774411', 'https://openalex.org/W4385245566', 'https://openalex.org/W2973048981', 'https://openalex.org/W2785350307', 'https://openalex.org/W2949328740', 'https://openalex.org/W4298393544', 'https://openalex.org/W2964121744', 'https://openalex.org/W2964161387']",2020-01-01
https://openalex.org/W4287854398,https://doi.org/10.18653/v1/2022.naacl-main.32,On Synthetic Data for Back Translation,"Back translation (BT) is one of the most significant technologies in NMT research fields. Existing attempts on BT share a common characteristic: they employ either beam search or random sampling to generate synthetic data with a backward model but seldom work studies the role of synthetic data in the performance of BT. This motivates us to ask a fundamental question: {\em what kind of synthetic data contributes to BT performance?} Through both theoretical and empirical studies, we identify two key factors on synthetic data controlling the back-translation NMT performance, which are quality and importance. Furthermore, based on our findings, we propose a simple yet effective method to generate synthetic data to better trade off both factors so as to yield a better performance for BT. We run extensive experiments on WMT14 DE-EN, EN-DE, and RU-EN benchmark tasks. By employing our proposed method to generate synthetic data, our BT model significantly outperforms the standard BT baselines (i.e., beam and sampling based methods for data generation), which proves the effectiveness of our proposed methods.","['https://openalex.org/W2933138175', 'https://openalex.org/W2546938941', 'https://openalex.org/W3173777717', 'https://openalex.org/W3006381853', 'https://openalex.org/W4288419306', 'https://openalex.org/W3110266480', 'https://openalex.org/W3169496116', 'https://openalex.org/W3098267758', 'https://openalex.org/W2887516053', 'https://openalex.org/W2890894339', 'https://openalex.org/W2903193068', 'https://openalex.org/W2971307358', 'https://openalex.org/W3098341425', 'https://openalex.org/W2101105183', 'https://openalex.org/W4297738480', 'https://openalex.org/W2886095922', 'https://openalex.org/W4288400010', 'https://openalex.org/W2970971057', 'https://openalex.org/W3034324324', 'https://openalex.org/W3176495666', 'https://openalex.org/W2970311224', 'https://openalex.org/W2963569233', 'https://openalex.org/W2888173624', 'https://openalex.org/W2969999977', 'https://openalex.org/W3100355250', 'https://openalex.org/W2798931235', 'https://openalex.org/W2963631950', 'https://openalex.org/W2970062726', 'https://openalex.org/W2889326796', 'https://openalex.org/W2741049976', 'https://openalex.org/W3091540052', 'https://openalex.org/W1963828038', 'https://openalex.org/W2946659172', 'https://openalex.org/W2963532001', 'https://openalex.org/W3172642864', 'https://openalex.org/W2966610483', 'https://openalex.org/W2912924812', 'https://openalex.org/W2136156618', 'https://openalex.org/W3101767999', 'https://openalex.org/W2888519496', 'https://openalex.org/W2595715041', 'https://openalex.org/W2122270629', 'https://openalex.org/W2953937638', 'https://openalex.org/W4385245566', 'https://openalex.org/W4301230920', 'https://openalex.org/W3034995113', 'https://openalex.org/W2962824887', 'https://openalex.org/W211509693', 'https://openalex.org/W2962784628', 'https://openalex.org/W3102999298', 'https://openalex.org/W2888808532', 'https://openalex.org/W3105214104', 'https://openalex.org/W2130942839', 'https://openalex.org/W2970476646', 'https://openalex.org/W2756566411', 'https://openalex.org/W2952682849', 'https://openalex.org/W2561274697', 'https://openalex.org/W2597891111', 'https://openalex.org/W2963216553', 'https://openalex.org/W3157144868', 'https://openalex.org/W2971302374', 'https://openalex.org/W2133564696', 'https://openalex.org/W2089657469', 'https://openalex.org/W3174925536', 'https://openalex.org/W1522301498', 'https://openalex.org/W2963339397', 'https://openalex.org/W3034716087', 'https://openalex.org/W1915251500', 'https://openalex.org/W2970279348', 'https://openalex.org/W2970015022', 'https://openalex.org/W3120860016']",2022-01-01
https://openalex.org/W4281982771,https://doi.org/10.48550/arxiv.2206.04571,Revisiting End-to-End Speech-to-Text Translation From Scratch,"End-to-end (E2E) speech-to-text translation (ST) often depends on pretraining its encoder and/or decoder using source transcripts via speech recognition or text translation tasks, without which translation performance drops substantially. However, transcripts are not always available, and how significant such pretraining is for E2E ST has rarely been studied in the literature. In this paper, we revisit this question and explore the extent to which the quality of E2E ST trained on speech-translation pairs alone can be improved. We reexamine several techniques proven beneficial to ST previously, and offer a set of best practices that biases a Transformer-based E2E ST system toward training from scratch. Besides, we propose parameterized distance penalty to facilitate the modeling of locality in the self-attention model for speech. On four benchmarks covering 23 languages, our experiments show that, without using any transcripts or pretraining, the proposed system reaches and even outperforms previous studies adopting pretraining, although the gap remains in (extremely) low-resource settings. Finally, we discuss neural acoustic feature modeling, where a neural model is designed to extract acoustic features from raw speech signals directly, with the goal to simplify inductive biases and add freedom to the model in describing speech. For the first time, we demonstrate its feasibility and show encouraging results on ST tasks.",[],2022-06-09
https://openalex.org/W3172698324,https://doi.org/10.18653/v1/2021.acl-short.103,Lightweight Adapter Tuning for Multilingual Speech Translation,"Adapter modules were recently introduced as an efficient alternative to fine-tuning in NLP. Adapter tuning consists in freezing pretrained parameters of a model and injecting lightweight modules between layers, resulting in the addition of only a small number of task-specific trainable parameters. While adapter tuning was investigated for multilingual neural machine translation, this paper proposes a comprehensive analysis of adapters for multilingual speech translation (ST). Starting from different pre-trained models (a multilingual ST trained on parallel data or a multilingual BART (mBART) trained on non-parallel multilingual data), we show that adapters can be used to: (a) efficiently specialize ST to specific language pairs with a low extra cost in terms of parameters, and (b) transfer from an automatic speech recognition (ASR) task and an mBART pre-trained model to a multilingual ST task. Experiments show that adapter tuning offer competitive results to full fine-tuning, while being much more parameter-efficient.","['https://openalex.org/W3099782249', 'https://openalex.org/W2970925270', 'https://openalex.org/W2964172053', 'https://openalex.org/W3173767661', 'https://openalex.org/W3197845195', 'https://openalex.org/W3153805297', 'https://openalex.org/W2963403868', 'https://openalex.org/W2147800946', 'https://openalex.org/W1522301498', 'https://openalex.org/W3034999214', 'https://openalex.org/W3118578889', 'https://openalex.org/W3107826490', 'https://openalex.org/W2945700568', 'https://openalex.org/W2971840980', 'https://openalex.org/W4226513777', 'https://openalex.org/W4288817190', 'https://openalex.org/W3099793224', 'https://openalex.org/W3037932933', 'https://openalex.org/W3183148055', 'https://openalex.org/W4287694131', 'https://openalex.org/W3098824823', 'https://openalex.org/W3049591882', 'https://openalex.org/W3036601975', 'https://openalex.org/W2964303773', 'https://openalex.org/W197865394', 'https://openalex.org/W3110524561', 'https://openalex.org/W3100370880', 'https://openalex.org/W3101498587', 'https://openalex.org/W3017650141', 'https://openalex.org/W3153675281', 'https://openalex.org/W2911300548', 'https://openalex.org/W2963211188', 'https://openalex.org/W222053410', 'https://openalex.org/W2963250244', 'https://openalex.org/W2964121744', 'https://openalex.org/W3046368065', 'https://openalex.org/W3097338456', 'https://openalex.org/W4385245566', 'https://openalex.org/W2936774411', 'https://openalex.org/W2979826702', 'https://openalex.org/W4322588812', 'https://openalex.org/W2982399380', 'https://openalex.org/W3112092703']",2021-01-01
https://openalex.org/W2889326796,https://doi.org/10.18653/v1/d18-1045,Understanding Back-Translation at Scale,"An effective method to improve neural machine translation with monolingual data is to augment the parallel training corpus with back-translations of target language sentences. This work broadens the understanding of back-translation and investigates a number of methods to generate synthetic source sentences. We find that in all but resource poor settings back-translations obtained via sampling or noised beam outputs are most effective. Our analysis shows that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. We also compare how synthetic data compares to genuine bitext and study various domain effects. Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT’14 English-German test set.","['https://openalex.org/W2169200297', 'https://openalex.org/W4307459710', 'https://openalex.org/W2775795276', 'https://openalex.org/W2581377246', 'https://openalex.org/W2797913374', 'https://openalex.org/W1810943226', 'https://openalex.org/W2109664771', 'https://openalex.org/W2964308564', 'https://openalex.org/W2612675303', 'https://openalex.org/W4298393544', 'https://openalex.org/W2794365787', 'https://openalex.org/W2806311723', 'https://openalex.org/W2963096510', 'https://openalex.org/W2887516053', 'https://openalex.org/W1915251500', 'https://openalex.org/W2798931235', 'https://openalex.org/W2963925437', 'https://openalex.org/W2963247703', 'https://openalex.org/W1522301498', 'https://openalex.org/W2756566411', 'https://openalex.org/W2613904329', 'https://openalex.org/W2122270629', 'https://openalex.org/W2963774520', 'https://openalex.org/W2963993537', 'https://openalex.org/W2758310181', 'https://openalex.org/W2101105183', 'https://openalex.org/W2767982226', 'https://openalex.org/W2767989436', 'https://openalex.org/W2136156618', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963807318', 'https://openalex.org/W2555745756', 'https://openalex.org/W2963708445', 'https://openalex.org/W2133564696', 'https://openalex.org/W2962784628', 'https://openalex.org/W2733239165', 'https://openalex.org/W2153653739', 'https://openalex.org/W4241645538', 'https://openalex.org/W2963768805', 'https://openalex.org/W4298137069', 'https://openalex.org/W2963804993', 'https://openalex.org/W4297801368', 'https://openalex.org/W3211848854', 'https://openalex.org/W2597891111', 'https://openalex.org/W2130942839', 'https://openalex.org/W2963602293', 'https://openalex.org/W2025768430', 'https://openalex.org/W2157331557', 'https://openalex.org/W2886095922', 'https://openalex.org/W2964121744', 'https://openalex.org/W2546938941', 'https://openalex.org/W2770173563', 'https://openalex.org/W2953333557', 'https://openalex.org/W1902237438', 'https://openalex.org/W2962883855', 'https://openalex.org/W2097333193', 'https://openalex.org/W4385245566', 'https://openalex.org/W211509693', 'https://openalex.org/W2540404261', 'https://openalex.org/W1916559533', 'https://openalex.org/W2183341477', 'https://openalex.org/W2550821151', 'https://openalex.org/W2561274697', 'https://openalex.org/W2595715041', 'https://openalex.org/W4297798436', 'https://openalex.org/W2963216553', 'https://openalex.org/W2141440284', 'https://openalex.org/W2963532001', 'https://openalex.org/W2964007535', 'https://openalex.org/W4301368689', 'https://openalex.org/W4293388793', 'https://openalex.org/W2963569817', 'https://openalex.org/W2963366552', 'https://openalex.org/W2566564022', 'https://openalex.org/W2964265128']",2018-01-01
https://openalex.org/W3162037819,https://doi.org/10.1109/icassp39728.2021.9415058,A General Multi-Task Learning Framework to Leverage Text Data for Speech to Text Tasks,"Attention-based sequence-to-sequence modeling provides a powerful and elegant solution for applications that need to map one sequence to a different sequence. Its success heavily relies on the availability of large amounts of training data. This presents a challenge for speech applications where labelled speech data is very expensive to obtain, such as automatic speech recognition (ASR) and speech translation (ST). In this study, we propose a general multi-task learning framework to leverage text data for ASR and ST tasks. Two auxiliary tasks, a denoising autoencoder task and machine translation task, are proposed to be co-trained with ASR and ST tasks respectively. We demonstrate that representing text input as phoneme sequences can reduce the difference between speech and text inputs, and enhance the knowledge transfer from text corpora to the speech to text tasks. Our experiments show that the proposed method achieves a relative 10~15% word error rate reduction on the English LIBRISPEECH task compared with our baseline, and improves the speech translation quality on the MUST-C tasks by 3.6~9.2 BLEU.","['https://openalex.org/W3096490862', 'https://openalex.org/W3035490255', 'https://openalex.org/W2696967604', 'https://openalex.org/W2936774411', 'https://openalex.org/W6732447497', 'https://openalex.org/W3008480565', 'https://openalex.org/W6743477263', 'https://openalex.org/W2972991710', 'https://openalex.org/W6756200954', 'https://openalex.org/W3007142233', 'https://openalex.org/W3016006013', 'https://openalex.org/W6757094361', 'https://openalex.org/W2605131327', 'https://openalex.org/W6898634591', 'https://openalex.org/W6732953234', 'https://openalex.org/W6784050962', 'https://openalex.org/W3008125272', 'https://openalex.org/W3037217258', 'https://openalex.org/W2944255943', 'https://openalex.org/W1494198834', 'https://openalex.org/W2327501763', 'https://openalex.org/W6775587661', 'https://openalex.org/W6623517193', 'https://openalex.org/W2964012862', 'https://openalex.org/W6745388339', 'https://openalex.org/W2963779652', 'https://openalex.org/W6739901393', 'https://openalex.org/W6769311223', 'https://openalex.org/W6772381481', 'https://openalex.org/W6770506093', 'https://openalex.org/W3015703505', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963403868', 'https://openalex.org/W2991213871', 'https://openalex.org/W2949328740', 'https://openalex.org/W3103005696', 'https://openalex.org/W4298393544', 'https://openalex.org/W3034999214', 'https://openalex.org/W854541894', 'https://openalex.org/W2577366047', 'https://openalex.org/W2901389167', 'https://openalex.org/W2963602293', 'https://openalex.org/W2888779557', 'https://openalex.org/W2945700568', 'https://openalex.org/W2964161387', 'https://openalex.org/W3092424727', 'https://openalex.org/W3118578889', 'https://openalex.org/W3034772996']",2021-05-13
https://openalex.org/W2124509324,https://doi.org/10.1109/tpami.2010.57,Product Quantization for Nearest Neighbor Search,"This paper introduces a product quantization-based approach for approximate nearest neighbor search. The idea is to decompose the space into a Cartesian product of low-dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. The euclidean distance between two vectors can be efficiently estimated from their codes. An asymmetric version increases precision, as it computes the approximate distance between a vector and a code. Experimental results show that our approach searches for nearest neighbors efficiently, in particular in combination with an inverted file system. Results for SIFT and GIST image descriptors show excellent search accuracy, outperforming three state-of-the-art approaches. The scalability of our approach is validated on a data set of two billion vectors.","['https://openalex.org/W1672197616', 'https://openalex.org/W2124222502', 'https://openalex.org/W2024668293', 'https://openalex.org/W6632397832', 'https://openalex.org/W2162006472', 'https://openalex.org/W1627400044', 'https://openalex.org/W2171790913', 'https://openalex.org/W4251572646', 'https://openalex.org/W6795644987', 'https://openalex.org/W2099907898', 'https://openalex.org/W2099253838', 'https://openalex.org/W2128017662', 'https://openalex.org/W2145607950', 'https://openalex.org/W2154956324', 'https://openalex.org/W1566135517', 'https://openalex.org/W6697214482', 'https://openalex.org/W1556531089', 'https://openalex.org/W2020308406', 'https://openalex.org/W2148809531', 'https://openalex.org/W2151103935', 'https://openalex.org/W4254197176', 'https://openalex.org/W2131846894', 'https://openalex.org/W1972378554', 'https://openalex.org/W2543932557', 'https://openalex.org/W2149991777', 'https://openalex.org/W196542726', 'https://openalex.org/W2141362318', 'https://openalex.org/W1541459201', 'https://openalex.org/W2293597654', 'https://openalex.org/W3160851792', 'https://openalex.org/W2008526771', 'https://openalex.org/W1502916507', 'https://openalex.org/W2158658988', 'https://openalex.org/W630138847', 'https://openalex.org/W1491105865']",2010-03-19
https://openalex.org/W2930682606,https://doi.org/10.48550/arxiv.1904.01575,Contrastive Predictive Coding Based Feature for Automatic Speaker Verification,"This thesis describes our ongoing work on Contrastive Predictive Coding (CPC) features for speaker verification. CPC is a recently proposed representation learning framework based on predictive coding and noise contrastive estimation. We focus on incorporating CPC features into the standard automatic speaker verification systems, and we present our methods, experiments, and analysis. This thesis also details necessary background knowledge in past and recent work on automatic speaker verification systems, conventional speech features, and the motivation and techniques behind CPC.","['https://openalex.org/W1524333225', 'https://openalex.org/W2117731089', 'https://openalex.org/W2890964092', 'https://openalex.org/W2146444479', 'https://openalex.org/W2842511635', 'https://openalex.org/W1494198834', 'https://openalex.org/W2037034710', 'https://openalex.org/W2119885245', 'https://openalex.org/W2138204974', 'https://openalex.org/W2748488820', 'https://openalex.org/W2963466847', 'https://openalex.org/W2194775991']",2019-04-01
https://openalex.org/W2811079561,https://doi.org/10.48550/arxiv.1806.10474,The challenge of realistic music generation: modelling raw audio at scale,"Realistic music generation is a challenging task. When building generative models of music that are learnt from data, typically high-level representations such as scores or MIDI are used that abstract away the idiosyncrasies of a particular performance. But these nuances are very important for our perception of musicality and realism, so in this work we embark on modelling music in the raw audio domain. It has been shown that autoregressive models excel at generating raw audio waveforms of speech, but when applied to music, we find them biased towards capturing local signal structure at the expense of modelling long-range correlations. This is problematic because music exhibits structure at many different timescales. In this work, we explore autoregressive discrete autoencoders (ADAs) as a means to enable autoregressive models to capture long-range correlations in waveforms. We find that they allow us to unconditionally generate piano music directly in the raw audio domain, which shows stylistic consistency across tens of seconds.","['https://openalex.org/W2963799213', 'https://openalex.org/W2963241221', 'https://openalex.org/W2963373786', 'https://openalex.org/W2474920236', 'https://openalex.org/W1665214252', 'https://openalex.org/W2770298516', 'https://openalex.org/W2752134738', 'https://openalex.org/W2210838531', 'https://openalex.org/W2587284713', 'https://openalex.org/W2795370625', 'https://openalex.org/W648143168', 'https://openalex.org/W2792210438', 'https://openalex.org/W2953046278', 'https://openalex.org/W2990244669', 'https://openalex.org/W2963042606', 'https://openalex.org/W2267126114', 'https://openalex.org/W2964121744', 'https://openalex.org/W2950299304', 'https://openalex.org/W2964307104', 'https://openalex.org/W2787214294', 'https://openalex.org/W2949382160', 'https://openalex.org/W2086161653', 'https://openalex.org/W2963840672', 'https://openalex.org/W2964122153', 'https://openalex.org/W2242818861', 'https://openalex.org/W2951535099', 'https://openalex.org/W2950067852', 'https://openalex.org/W2949899814', 'https://openalex.org/W2963790827', 'https://openalex.org/W2118776487', 'https://openalex.org/W2099257174', 'https://openalex.org/W1909320841', 'https://openalex.org/W2553374874', 'https://openalex.org/W2584032004', 'https://openalex.org/W2963981733', 'https://openalex.org/W2962968839', 'https://openalex.org/W3122518304', 'https://openalex.org/W2793273050', 'https://openalex.org/W2770703741', 'https://openalex.org/W2786254735', 'https://openalex.org/W2278108219', 'https://openalex.org/W2787439980', 'https://openalex.org/W2769810959', 'https://openalex.org/W2951004968', 'https://openalex.org/W2952276042', 'https://openalex.org/W2963292439', 'https://openalex.org/W2099471712']",2018-06-26
https://openalex.org/W3196891430,https://doi.org/10.21437/interspeech.2021-1258,"Multilingual Speech Evaluation: Case Studies on English, Malay and Tamil","Speech evaluation is an essential component in computerassisted language learning (CALL).While speech evaluation on English has been popular, automatic speech scoring on low resource languages remains challenging.Work in this area has focused on monolingual specific designs and handcrafted features stemming from resource-rich languages like English.Such approaches are often difficult to generalize to other languages, especially if we also want to consider suprasegmental qualities such as rhythm.In this work, we examine three different languages that possess distinct rhythm patterns: English (stresstimed), Malay (syllable-timed), and Tamil (mora-timed).We exploit robust feature representations inspired by music processing and vector representation learning.Empirical validations show consistent gains for all three languages when predicting pronunciation, rhythm and intonation performance.","['https://openalex.org/W2575763737', 'https://openalex.org/W1992124213', 'https://openalex.org/W3096674206', 'https://openalex.org/W3096270392', 'https://openalex.org/W2398741870', 'https://openalex.org/W2137469438', 'https://openalex.org/W2067063915', 'https://openalex.org/W2016114400', 'https://openalex.org/W2575689684', 'https://openalex.org/W4251326366', 'https://openalex.org/W2749848124', 'https://openalex.org/W2085628288', 'https://openalex.org/W3096544436', 'https://openalex.org/W2134835252', 'https://openalex.org/W2164810574', 'https://openalex.org/W2166637769', 'https://openalex.org/W2172287020', 'https://openalex.org/W2395066838', 'https://openalex.org/W2141994339', 'https://openalex.org/W2888030130', 'https://openalex.org/W1614298861', 'https://openalex.org/W7709353', 'https://openalex.org/W2889282842', 'https://openalex.org/W2912981563', 'https://openalex.org/W2291868800', 'https://openalex.org/W2803039862', 'https://openalex.org/W2404126548', 'https://openalex.org/W2091856355', 'https://openalex.org/W2139008940', 'https://openalex.org/W2768986418', 'https://openalex.org/W2406502879', 'https://openalex.org/W2612891210']",2021-08-27
https://openalex.org/W4312069033,https://doi.org/10.23919/apsipaasc55919.2022.9979979,"3M: An Effective Multi-view, Multi-granularity, and Multi-aspect Modeling Approach to English Pronunciation Assessment","As an indispensable ingredient of computer-assisted pronunciation training (CAPT), automatic pronunciation assessment (APA) plays a pivotal role in aiding self-directed language learners by providing multi-aspect and timely feedback. However, there are at least two potential obstacles that might hinder its performance for practical use. On one hand, most of the studies focus exclusively on leveraging segmental (phonetic)-level features such as goodness of pronunciation (GOP); this, however, may cause a discrepancy of feature granularity when performing suprasegmental (prosodic)-level pronunciation assessment. On the other hand, automatic pronunciation assessments still suffer from the lack of large-scale labeled speech data of non-native speakers, which inevitably limits the performance of pronunciation assessment. In this paper, we tackle these problems by integrating multiple prosodic and phonological features to provide a multi-view, multi-granularity, and multi-aspect (3M) pronunciation modeling. Specifically, we augment GOP with prosodic and self-supervised learning (SSL) features, and meanwhile develop a vowel/consonant positional embedding for a more phonology-aware automatic pronunciation assessment. A series of experiments conducted on the publicly-available speechocean762 dataset show that our approach can obtain significant improvements on several assessment granularities in comparison with previous work, especially on the assessment of speaking fluency and speech prosody.","['https://openalex.org/W6803164887', 'https://openalex.org/W3209059054', 'https://openalex.org/W6784878386', 'https://openalex.org/W6780218876', 'https://openalex.org/W3197938691', 'https://openalex.org/W2979826702', 'https://openalex.org/W2575689684', 'https://openalex.org/W1988687075', 'https://openalex.org/W1973657629', 'https://openalex.org/W2559844874', 'https://openalex.org/W2017743450', 'https://openalex.org/W4224928163', 'https://openalex.org/W2139008940', 'https://openalex.org/W2091856355', 'https://openalex.org/W6755207826', 'https://openalex.org/W3196525293', 'https://openalex.org/W3197816268', 'https://openalex.org/W149618481', 'https://openalex.org/W3096674206', 'https://openalex.org/W4223651314', 'https://openalex.org/W2972347929', 'https://openalex.org/W3097911750', 'https://openalex.org/W3097515180', 'https://openalex.org/W3197742413', 'https://openalex.org/W6803132568', 'https://openalex.org/W6713615836', 'https://openalex.org/W6739901393', 'https://openalex.org/W1561774577', 'https://openalex.org/W6730128983', 'https://openalex.org/W6682881764', 'https://openalex.org/W2498863433', 'https://openalex.org/W6800703360', 'https://openalex.org/W6687152286', 'https://openalex.org/W2154745686', 'https://openalex.org/W3036601975', 'https://openalex.org/W3096992656', 'https://openalex.org/W3209984917', 'https://openalex.org/W2560222437', 'https://openalex.org/W2191779130', 'https://openalex.org/W3198712976', 'https://openalex.org/W2408752745', 'https://openalex.org/W3096544436']",2022-11-07
https://openalex.org/W2935807810,https://doi.org/10.1109/icassp.2019.8682187,NN-based Ordinal Regression for Assessing Fluency of ESL Speech,"Automatic assessment of a language learner's speech fluency is highly desirable for language education, e.g. for English as a Second Language (ESL) learning. In this paper, we formulate the fluency assessment as a problem of Ordinal Regression with Anchored Reference Samples (ORARS), where the fluency of a speech utterance is predicted by an ordinal regression neural network (NN) trained with anchored reference samples. The ORARS is trained and tested by: picking human expert labeled samples in each mean opinion score (MOS) bucket as the anchored reference samples and pairing them with input speech samples as training couplets; training an NN-based binary classifier to determine which sample in a pair is better in fluency; predicting the rank (MOS) of a test sample based upon the posteriors of all binary comparisons between the test sample and all anchored reference samples. Experimentally, our proposed approach outperforms the traditional NN-based methods and reaches a performance of ""human parity"", i.e. as comparable as human experts, in its fluency assessment of collected ESL speech. To the best of our knowledge, this is the first attempt to assess speech fluency with an ordinal regression framework where a test input is paired with bucketed and anchored reference samples.","['https://openalex.org/W2163094209', 'https://openalex.org/W1987208496', 'https://openalex.org/W6601592313', 'https://openalex.org/W2083261714', 'https://openalex.org/W2190044943', 'https://openalex.org/W1997855593', 'https://openalex.org/W6679285541', 'https://openalex.org/W2139881679', 'https://openalex.org/W6602227229', 'https://openalex.org/W4211111786', 'https://openalex.org/W6732414763', 'https://openalex.org/W2888859044', 'https://openalex.org/W3820303', 'https://openalex.org/W2009088607', 'https://openalex.org/W1979364015', 'https://openalex.org/W2786063148', 'https://openalex.org/W2440214111', 'https://openalex.org/W6678181790', 'https://openalex.org/W2119411189', 'https://openalex.org/W190987354', 'https://openalex.org/W2091856355', 'https://openalex.org/W2750506564', 'https://openalex.org/W1659542763', 'https://openalex.org/W2575763737', 'https://openalex.org/W2128186735', 'https://openalex.org/W39627588', 'https://openalex.org/W2574999652', 'https://openalex.org/W2124105163', 'https://openalex.org/W55145144']",2019-04-16
https://openalex.org/W2121884006,https://doi.org/10.1093/applin/amp048,"Complexity, Accuracy, and Fluency in Second Language Acquisition","Journal Article Complexity, Accuracy, and Fluency in Second Language Acquisition Get access Alex Housen, Alex Housen 1Vrije Universiteit Brussel and 2Universiteit van Amsterdam Search for other works by this author on: Oxford Academic Google Scholar Folkert Kuiken Folkert Kuiken 1Vrije Universiteit Brussel and 2Universiteit van Amsterdam Search for other works by this author on: Oxford Academic Google Scholar Applied Linguistics, Volume 30, Issue 4, December 2009, Pages 461–473, https://doi.org/10.1093/applin/amp048 Published: 02 December 2009","['https://openalex.org/W2114807474', 'https://openalex.org/W2092537199', 'https://openalex.org/W2069029003', 'https://openalex.org/W2009002950', 'https://openalex.org/W2115055261', 'https://openalex.org/W2028810396', 'https://openalex.org/W1991667866', 'https://openalex.org/W2075959218', 'https://openalex.org/W2090655260', 'https://openalex.org/W2102113429', 'https://openalex.org/W2169742904', 'https://openalex.org/W1816750986', 'https://openalex.org/W2036093145', 'https://openalex.org/W2057096339', 'https://openalex.org/W2039838847', 'https://openalex.org/W2000174603', 'https://openalex.org/W2799031065', 'https://openalex.org/W2130349421', 'https://openalex.org/W2067549766', 'https://openalex.org/W2124216532', 'https://openalex.org/W1493719061', 'https://openalex.org/W35458156', 'https://openalex.org/W2162792036', 'https://openalex.org/W164138854', 'https://openalex.org/W2071775050', 'https://openalex.org/W2004415003', 'https://openalex.org/W1500836710', 'https://openalex.org/W2070103459', 'https://openalex.org/W613370289', 'https://openalex.org/W2259950503', 'https://openalex.org/W128627502', 'https://openalex.org/W2569584646', 'https://openalex.org/W1571236128', 'https://openalex.org/W614630737', 'https://openalex.org/W2136943242', 'https://openalex.org/W3131622302', 'https://openalex.org/W2144105102', 'https://openalex.org/W1555718133', 'https://openalex.org/W1917893759', 'https://openalex.org/W2118367551', 'https://openalex.org/W2497668147', 'https://openalex.org/W2088056535', 'https://openalex.org/W653288160', 'https://openalex.org/W1525723036', 'https://openalex.org/W2571017604', 'https://openalex.org/W2994315818', 'https://openalex.org/W2114806605', 'https://openalex.org/W642585179', 'https://openalex.org/W256728138', 'https://openalex.org/W1817086658']",2009-12-01
https://openalex.org/W2112446559,https://doi.org/10.1109/29.1486,A new statistical approach for the automatic segmentation of continuous speech signals,"A statistical approach for the segmentation of a continuous speech signal to detect acoustic events is presented. Experiments are carried out to test the segmentation algorithms. Reasonable results are obtained with speech signals, although these are not exactly piecewise stationary. A comparison between the experimental results of automatic and handmade segmentations, demonstrates the potential acoustic-phonetic classification capability of the proposed algorithms.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>","['https://openalex.org/W2069139112', 'https://openalex.org/W1981867129', 'https://openalex.org/W1965555277', 'https://openalex.org/W2022554507', 'https://openalex.org/W2164755285', 'https://openalex.org/W2144418681', 'https://openalex.org/W2102794086', 'https://openalex.org/W2143657555', 'https://openalex.org/W2164240509', 'https://openalex.org/W654432019', 'https://openalex.org/W2144968237', 'https://openalex.org/W1565967034']",1988-01-01
https://openalex.org/W2963308316,https://doi.org/10.1109/icassp.2018.8461404,Deep-FSMN for Large Vocabulary Continuous Speech Recognition,"In this paper, we present an improved feedforward sequential memory networks (FSMN) architecture, namely Deep-FSMN (DFSMN), by introducing skip connections between memory blocks in adjacent layers. These skip connections enable the information flow across different layers and thus alleviate the gradient vanishing problem when building very deep structure. As a result, DFSMN significantly benefits from these skip connections and deep structure. We have compared the performance of DFSMN to BLSTM both with and without lower frame rate (LFR) on several large speech recognition tasks, including English and Mandarin. Experimental results shown that DFSMN can consistently outperform BLSTM with dramatic gain, especially trained with LFR using CD-Phone as modeling units. In the 20000 hours Fisher (FSH) task, the proposed DFSMN can achieve a word error rate of 9.4% by purely using the cross-entropy criterion and decoding with a 3-gram language model, which achieves a 1.5% absolute improvement compared to the BLSTM. In a 20000 hours Mandarin recognition task, the LFR trained DFSMN can achieve more than 20% relative improvement compared to the LFR trained BLSTM. Moreover, we can easily design the lookahead filter order of the memory blocks in DFSMN to control the latency for real-time applications.","['https://openalex.org/W2734724284', 'https://openalex.org/W6713844269', 'https://openalex.org/W2475988411', 'https://openalex.org/W2150355110', 'https://openalex.org/W2107878631', 'https://openalex.org/W6712772800', 'https://openalex.org/W6712930963', 'https://openalex.org/W6713570806', 'https://openalex.org/W2117671523', 'https://openalex.org/W6726441445', 'https://openalex.org/W6603931906', 'https://openalex.org/W1995562189', 'https://openalex.org/W2155273149', 'https://openalex.org/W6681513673', 'https://openalex.org/W6712773019', 'https://openalex.org/W4254816979', 'https://openalex.org/W2064675550', 'https://openalex.org/W2147768505', 'https://openalex.org/W1993882792', 'https://openalex.org/W2590320937', 'https://openalex.org/W2194775991', 'https://openalex.org/W2515439472', 'https://openalex.org/W2964084166', 'https://openalex.org/W6739913424', 'https://openalex.org/W2517251061', 'https://openalex.org/W2533523411', 'https://openalex.org/W2143612262', 'https://openalex.org/W2631415506', 'https://openalex.org/W2400505028', 'https://openalex.org/W1920942766', 'https://openalex.org/W2404568753', 'https://openalex.org/W2405883473', 'https://openalex.org/W1499864241', 'https://openalex.org/W2402146185', 'https://openalex.org/W2208299922', 'https://openalex.org/W2399364384', 'https://openalex.org/W2519091744', 'https://openalex.org/W2962965465', 'https://openalex.org/W97072897']",2018-04-01
https://openalex.org/W3197742413,https://doi.org/10.21437/interspeech.2021-1259,speechocean762: An Open-Source Non-Native English Speech Corpus for Pronunciation Assessment,"This paper introduces a new open-source speech corpus named ""speechocean762"" designed for pronunciation assessment use, consisting of 5000 English utterances from 250 non-native speakers, where half of the speakers are children.Five experts annotated each of the utterances at sentence-level, wordlevel and phoneme-level.A baseline system is released in open source to illustrate the phoneme-level pronunciation assessment workflow on this corpus.This corpus is allowed to be used freely for commercial and non-commercial purposes.It is available for free download from OpenSLR, and the corresponding baseline system is published in the Kaldi speech recognition toolkit.","['https://openalex.org/W101533558', 'https://openalex.org/W3043352754', 'https://openalex.org/W2976346930', 'https://openalex.org/W1753200545', 'https://openalex.org/W2556576367', 'https://openalex.org/W2571017604', 'https://openalex.org/W2404722202', 'https://openalex.org/W3112683920', 'https://openalex.org/W2552635739', 'https://openalex.org/W2407533274', 'https://openalex.org/W3015231007', 'https://openalex.org/W2091856355', 'https://openalex.org/W2401916374', 'https://openalex.org/W2888954148', 'https://openalex.org/W2770743368', 'https://openalex.org/W1895481600', 'https://openalex.org/W2397238603', 'https://openalex.org/W3096091920', 'https://openalex.org/W2559844874', 'https://openalex.org/W2137226992', 'https://openalex.org/W3001687786', 'https://openalex.org/W2888996995', 'https://openalex.org/W2082367183', 'https://openalex.org/W3097515180', 'https://openalex.org/W2741510257', 'https://openalex.org/W2938386610']",2021-08-27
https://openalex.org/W3016114816,https://doi.org/10.1109/icassp40776.2020.9053452,Automatic Fluency Evaluation of Spontaneous Speech Using Disfluency-Based Features,"This paper describes an automatic fluency evaluation of spontaneous speech. Although we regularly observe a variety of different disfluencies in spontaneous speech, we focus on two types of phenomena, i.e., filled pauses and word fragments. This paper aims to reveal that these two types of disfluencies have effects on speech fluency evaluation differently. To this end, we conduct a series of SVM classification experiments on the Japanese spontaneous speech corpus. The experimental results show that the features derived from word fragments are effective in evaluating disfluent speech especially when combined with prosodic features such as speech rate and pauses/silence, while the features from filled pauses are not effective in evaluating fluency.","['https://openalex.org/W2585563893', 'https://openalex.org/W6779469704', 'https://openalex.org/W6675354045', 'https://openalex.org/W2964172015', 'https://openalex.org/W2076041950', 'https://openalex.org/W2799441795', 'https://openalex.org/W2293765256', 'https://openalex.org/W6712995229', 'https://openalex.org/W2756923881', 'https://openalex.org/W2888859044', 'https://openalex.org/W2935807810', 'https://openalex.org/W2067955714', 'https://openalex.org/W2141001701', 'https://openalex.org/W101111729', 'https://openalex.org/W6603294001', 'https://openalex.org/W2963729456', 'https://openalex.org/W2904571617', 'https://openalex.org/W3105218492', 'https://openalex.org/W2575763737', 'https://openalex.org/W2400923302', 'https://openalex.org/W1599761911', 'https://openalex.org/W2101234009', 'https://openalex.org/W80810014', 'https://openalex.org/W3034729383']",2020-04-09
https://openalex.org/W2069029003,https://doi.org/10.1121/1.1471894,Quantitative assessment of second language learners’ fluency: Comparisons between read and spontaneous speech,"This paper describes two experiments aimed at exploring the relationship between objective properties of speech and perceived fluency in read and spontaneous speech. The aim is to determine whether such quantitative measures can be used to develop objective fluency tests. Fragments of read speech (Experiment 1) of 60 non-native speakers of Dutch and of spontaneous speech (Experiment 2) of another group of 57 non-native speakers of Dutch were scored for fluency by human raters and were analyzed by means of a continuous speech recognizer to calculate a number of objective measures of speech quality known to be related to perceived fluency. The results show that the objective measures investigated in this study can be employed to predict fluency ratings, but the predictive power of such measures is stronger for read speech than for spontaneous speech. Moreover, the adequacy of the variables to be employed appears to be dependent on the specific type of speech material investigated and the specific task performed by the speaker.","['https://openalex.org/W1988687075', 'https://openalex.org/W2094589227', 'https://openalex.org/W2075959218', 'https://openalex.org/W2076110671', 'https://openalex.org/W2153885595', 'https://openalex.org/W2107866316', 'https://openalex.org/W2039838847', 'https://openalex.org/W2092537199', 'https://openalex.org/W1483501585', 'https://openalex.org/W1504344060', 'https://openalex.org/W128627502', 'https://openalex.org/W2108041418', 'https://openalex.org/W372307529', 'https://openalex.org/W1966696865']",2002-06-01
https://openalex.org/W2155774313,,Developing Speech Recognition and Synthesis Technologies to Support Computer-Aided Pronunciation Training for Chinese Learners of English,"Abstract. We describe ongoing research in the development of speech technologies that strives to raise the efficacy of computer-aided pronunciation training, especially for Chinese learners of English. Our approach is grounded on the theory of language transfer and involves a systematic phonological comparison between the primary language (L1 being Chinese) and secondary language (L2 being English) to predict possible segmental and suprasegmental realizations that constitute mispronunciations in L2 English. The predictions are validated based on a specially designed corpus that consists of several hundred hours of L2 English speech. The speech data supports the development of automatic speech recognition technologies that can detect and diagnose mispronunciations. The diagnosis aims to support the design of pedagogical and remedial instructions, which involves text-tospeech synthesis technologies in audiovisual forms. 1","['https://openalex.org/W2400818677', 'https://openalex.org/W225859850', 'https://openalex.org/W1535086474', 'https://openalex.org/W322814586', 'https://openalex.org/W2160783805', 'https://openalex.org/W2129755070', 'https://openalex.org/W35168091', 'https://openalex.org/W2137268753', 'https://openalex.org/W275138538']",2009-12-01
https://openalex.org/W3110277971,https://doi.org/10.1109/lsp.2020.3039765,Psychoacoustic Calibration of Loss Functions for Efficient End-to-End Neural Audio Coding,"Conventional audio coding technologies commonly leverage human perception of\nsound, or psychoacoustics, to reduce the bitrate while preserving the\nperceptual quality of the decoded audio signals. For neural audio codecs,\nhowever, the objective nature of the loss function usually leads to suboptimal\nsound quality as well as high run-time complexity due to the large model size.\nIn this work, we present a psychoacoustic calibration scheme to re-define the\nloss functions of neural audio coding systems so that it can decode signals\nmore perceptually similar to the reference, yet with a much lower model\ncomplexity. The proposed loss function incorporates the global masking\nthreshold, allowing the reconstruction error that corresponds to inaudible\nartifacts. Experimental results show that the proposed model outperforms the\nbaseline neural codec twice as large and consuming 23.4% more bits per second.\nWith the proposed method, a lightweight neural codec, with only 0.9 million\nparameters, performs near-transparent audio coding comparable with the\ncommercial MPEG-1 Audio Layer III codec at 112 kbps.\n","['https://openalex.org/W6631190155', 'https://openalex.org/W3015268401', 'https://openalex.org/W2936088203', 'https://openalex.org/W2963091184', 'https://openalex.org/W2558649592', 'https://openalex.org/W2620812332', 'https://openalex.org/W2997901485', 'https://openalex.org/W2962946126', 'https://openalex.org/W6748015549', 'https://openalex.org/W2067295501', 'https://openalex.org/W1552314771', 'https://openalex.org/W2963045393', 'https://openalex.org/W2892110446', 'https://openalex.org/W6736723571', 'https://openalex.org/W2058075673', 'https://openalex.org/W6633114069', 'https://openalex.org/W6755853340', 'https://openalex.org/W2138115654', 'https://openalex.org/W2935711438', 'https://openalex.org/W6751512325', 'https://openalex.org/W2752796333', 'https://openalex.org/W6631257896', 'https://openalex.org/W2128301448', 'https://openalex.org/W2897371647', 'https://openalex.org/W2963182577', 'https://openalex.org/W6741057705', 'https://openalex.org/W2194775991', 'https://openalex.org/W2972354707', 'https://openalex.org/W2476548250', 'https://openalex.org/W2519091744', 'https://openalex.org/W1522301498', 'https://openalex.org/W2950237361', 'https://openalex.org/W2951535099', 'https://openalex.org/W2155729921', 'https://openalex.org/W2964164354', 'https://openalex.org/W2732044853', 'https://openalex.org/W2606176153', 'https://openalex.org/W2949382160', 'https://openalex.org/W2963208781', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963799213', 'https://openalex.org/W2963452667', 'https://openalex.org/W1553834069', 'https://openalex.org/W2786008273', 'https://openalex.org/W1523644733']",2020-01-01
https://openalex.org/W3186609711,https://doi.org/10.1109/waspaa52581.2021.9632723,Harp-Net: Hyper-Autoencoded Reconstruction Propagation for Scalable Neural Audio Coding,"We propose a novel autoencoder architecture that improves the architectural scalability of general-purpose neural audio coding models. An autoencoder-based codec employs quantization to turn its bottleneck layer activation into bitstrings, a process that hinders information flow between the encoder and decoder parts. To circumvent this issue, we employ additional skip connections between the corresponding pair of encoder-decoder layers. The assumption is that, in a mirrored autoencoder topology, a decoder layer reconstructs the intermediate feature representation of its corresponding encoder layer. Hence, any additional information directly propagated from the corresponding encoder layer helps the reconstruction. We implement this kind of skip connections in the form of additional autoencoders, each of which is a small codec that compresses the massive data transfer between the paired encoder-decoder layers. We empirically verify that the proposed hyper-autoencoded architecture improves perceptual audio quality compared to an ordinary autoencoder baseline.","['https://openalex.org/W2963091184', 'https://openalex.org/W2924551963', 'https://openalex.org/W3110277971', 'https://openalex.org/W6764117752', 'https://openalex.org/W6639824700', 'https://openalex.org/W6751512325', 'https://openalex.org/W2100495367', 'https://openalex.org/W6741057705', 'https://openalex.org/W6746914816', 'https://openalex.org/W2476548250', 'https://openalex.org/W6639363673', 'https://openalex.org/W2165291881', 'https://openalex.org/W6773743766', 'https://openalex.org/W2963182577', 'https://openalex.org/W2775336875', 'https://openalex.org/W2935711438', 'https://openalex.org/W2105921478', 'https://openalex.org/W2732044853', 'https://openalex.org/W3015268401', 'https://openalex.org/W2963452667', 'https://openalex.org/W4205788663', 'https://openalex.org/W2949382160', 'https://openalex.org/W2519091744', 'https://openalex.org/W2964164354', 'https://openalex.org/W1901129140', 'https://openalex.org/W2972354707', 'https://openalex.org/W2972519044', 'https://openalex.org/W1885680957', 'https://openalex.org/W2774707525']",2021-10-17
https://openalex.org/W4284888249,https://doi.org/10.21437/interspeech.2022-10988,Ultra-Low-Bitrate Speech Coding with Pretrained Transformers,"Speech coding facilitates the transmission of speech over lowbandwidth networks with minimal distortion.Neural-network based speech codecs have recently demonstrated significant improvements in quality over traditional approaches.While this new generation of codecs is capable of synthesizing highfidelity speech, their use of recurrent or convolutional layers often restricts their effective receptive fields, which prevents them from compressing speech efficiently.We propose to further reduce the bitrate of neural speech codecs through the use of pretrained Transformers, capable of exploiting long-range dependencies in the input signal due to their inductive bias.As such, we use a pretrained Transformer in tandem with a convolutional encoder, which is trained end-to-end with a quantizer and a generative adversarial net decoder.Our numerical experiments show that supplementing the convolutional encoder of a neural speech codec with Transformer speech embeddings yields a speech codec with a bitrate of 600 bps that outperforms the original neural speech codec in synthesized speech quality when trained at the same bitrate.Subjective human evaluations suggest that the quality of the resulting codec is comparable or better than that of conventional codecs operating at three to four times the rate.","['https://openalex.org/W3036601975', 'https://openalex.org/W2129438793', 'https://openalex.org/W3160077247', 'https://openalex.org/W2970006822', 'https://openalex.org/W3097777922', 'https://openalex.org/W4226033575', 'https://openalex.org/W4205788663', 'https://openalex.org/W2131738223', 'https://openalex.org/W4297808394', 'https://openalex.org/W2963799213', 'https://openalex.org/W2115098197', 'https://openalex.org/W2292235217', 'https://openalex.org/W4394668580', 'https://openalex.org/W3206495532', 'https://openalex.org/W4385245566', 'https://openalex.org/W3140429000', 'https://openalex.org/W2114925438', 'https://openalex.org/W3016098186', 'https://openalex.org/W2775336875', 'https://openalex.org/W4320013936', 'https://openalex.org/W3037038648', 'https://openalex.org/W1481955708', 'https://openalex.org/W2963208781', 'https://openalex.org/W3092028330', 'https://openalex.org/W1567186732', 'https://openalex.org/W2998572311', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963091184', 'https://openalex.org/W1996957914', 'https://openalex.org/W3007965881', 'https://openalex.org/W2108532241', 'https://openalex.org/W2935711438', 'https://openalex.org/W3093579165', 'https://openalex.org/W3215615641', 'https://openalex.org/W4226380987']",2022-09-16
https://openalex.org/W3161744562,https://doi.org/10.1109/icassp39728.2021.9413678,Source-Aware Neural Speech Coding for Noisy Speech Compression,"This paper introduces a novel neural network-based speech coding system that can process noisy speech effectively. The proposed source-aware neural audio coding (SANAC) system harmonizes a deep autoencoder-based source separation model and a neural coding system, so that it can explicitly perform source separation and coding in the latent space. An added benefit of this system is that the codec can allocate a different amount of bits to the underlying sources, so that the more important source sounds better in the decoded signal. We target a new use case where the user on the receiver side cares about the quality of the non-speech components in the speech communication, while the speech source still carries the most important information. Both objective and subjective evaluation tests show that SANAC can recover the original noisy speech better than the baseline neural audio coding system, which is with no source-aware coding mechanism, and two conventional codecs.","['https://openalex.org/W2151626637', 'https://openalex.org/W1518739398', 'https://openalex.org/W1588397041', 'https://openalex.org/W6731445010', 'https://openalex.org/W3016098186', 'https://openalex.org/W2962935966', 'https://openalex.org/W2952218014', 'https://openalex.org/W6789826613', 'https://openalex.org/W2775336875', 'https://openalex.org/W2963182577', 'https://openalex.org/W6760712927', 'https://openalex.org/W3015268401', 'https://openalex.org/W2972354707', 'https://openalex.org/W6741057705', 'https://openalex.org/W2165291881', 'https://openalex.org/W6734035190', 'https://openalex.org/W304179306', 'https://openalex.org/W2194775991', 'https://openalex.org/W6631190155', 'https://openalex.org/W6639363673', 'https://openalex.org/W2476548250', 'https://openalex.org/W2067295501', 'https://openalex.org/W2566983320', 'https://openalex.org/W2950237361', 'https://openalex.org/W2972519044', 'https://openalex.org/W2950237263', 'https://openalex.org/W4294567867', 'https://openalex.org/W3127686677', 'https://openalex.org/W2732044853', 'https://openalex.org/W2964058413', 'https://openalex.org/W2519091744', 'https://openalex.org/W3099330747', 'https://openalex.org/W1522301498', 'https://openalex.org/W4205788663', 'https://openalex.org/W2964164354', 'https://openalex.org/W2949382160', 'https://openalex.org/W1885680957']",2021-05-13
https://openalex.org/W4372268681,https://doi.org/10.1109/icassp49357.2023.10096435,Disentangling Speech from Surroundings with Neural Embeddings,"We present a method to separate speech signals from noisy environments in the embedding space of a neural audio codec. We introduce a new training procedure that allows our model to produce structured encodings of audio waveforms given by embedding vectors, where one part of the embedding vector represents the speech signal, and the rest represent the environment. We achieve this by partitioning the embeddings of different input waveforms and training the model to faithfully reconstruct audio from mixed partitions, thereby ensuring each partition encodes a separate audio attribute. As use cases, we demonstrate the separation of speech from background noise or from reverberation characteristics. Our method also allows for targeted adjustments of the audio output characteristics.","['https://openalex.org/W6796756982', 'https://openalex.org/W2963830550', 'https://openalex.org/W6754438855', 'https://openalex.org/W6752096078', 'https://openalex.org/W3037038648', 'https://openalex.org/W1728888090', 'https://openalex.org/W3163725055', 'https://openalex.org/W6739139180', 'https://openalex.org/W2963182577', 'https://openalex.org/W2129913307', 'https://openalex.org/W6762533536', 'https://openalex.org/W6784501517', 'https://openalex.org/W3140429000', 'https://openalex.org/W6776390925', 'https://openalex.org/W3126911513', 'https://openalex.org/W6637412569', 'https://openalex.org/W6771763809', 'https://openalex.org/W3011176162', 'https://openalex.org/W3161744562', 'https://openalex.org/W6780043386', 'https://openalex.org/W6745731115', 'https://openalex.org/W1597121597', 'https://openalex.org/W6781251213', 'https://openalex.org/W6746278845', 'https://openalex.org/W6784499681', 'https://openalex.org/W2998657200', 'https://openalex.org/W6756824971', 'https://openalex.org/W3215615641', 'https://openalex.org/W2935711438', 'https://openalex.org/W4245919820', 'https://openalex.org/W6639363673', 'https://openalex.org/W2887059375', 'https://openalex.org/W2945478979', 'https://openalex.org/W3000389243', 'https://openalex.org/W2963129901', 'https://openalex.org/W4287629818', 'https://openalex.org/W4287694050', 'https://openalex.org/W2963233633', 'https://openalex.org/W3093839391', 'https://openalex.org/W2902476877', 'https://openalex.org/W4205788663', 'https://openalex.org/W1691728462', 'https://openalex.org/W3168719651', 'https://openalex.org/W3034794073', 'https://openalex.org/W3040462728', 'https://openalex.org/W2770119437', 'https://openalex.org/W2621350877']",2023-05-05
https://openalex.org/W4375869380,https://doi.org/10.1109/icassp49357.2023.10095442,LMCodec: A Low Bitrate Speech Codec with Causal Transformer Models,"We introduce LMCodec, a causal neural speech codec that provides high quality audio at very low bitrates. The backbone of the system is a causal convolutional codec that encodes audio into a hierarchy of coarse-to-fine tokens using residual vector quantization. LMCodec trains a Transformer language model to predict the fine tokens from the coarse ones in a generative fashion, allowing for the transmission of fewer codes. A second Transformer predicts the uncertainty of the next codes given the past transmitted codes, and is used to perform conditional entropy coding. A MUSHRA subjective test was conducted and shows that the quality is comparable to reference codecs at higher bitrates. Example audio is available at https://mjenrungrot.github.io/chrome-media-audio-papers/publications/lmcodec.","['https://openalex.org/W4284888249', 'https://openalex.org/W2963868408', 'https://openalex.org/W6772349387', 'https://openalex.org/W3202278141', 'https://openalex.org/W3097777922', 'https://openalex.org/W3160077247', 'https://openalex.org/W6739901393', 'https://openalex.org/W3215615641', 'https://openalex.org/W6629717138', 'https://openalex.org/W1481955708', 'https://openalex.org/W6639363673', 'https://openalex.org/W2995181338', 'https://openalex.org/W1597121597', 'https://openalex.org/W3037038648', 'https://openalex.org/W6751104502', 'https://openalex.org/W2972354707', 'https://openalex.org/W3186609711', 'https://openalex.org/W2963182577', 'https://openalex.org/W2129913307', 'https://openalex.org/W2935711438', 'https://openalex.org/W2963091184', 'https://openalex.org/W4385245566', 'https://openalex.org/W4293569541', 'https://openalex.org/W2998572311', 'https://openalex.org/W4205788663', 'https://openalex.org/W1494198834', 'https://openalex.org/W3140429000']",2023-05-05
https://openalex.org/W4372348514,https://doi.org/10.1109/icassp49357.2023.10096077,Neural Feature Predictor and Discriminative Residual Coding for Low-Bitrate Speech Coding,"Low and ultra-low-bitrate neural speech codecs achieved unprecedented coding gain by generating speech signals from compact features. This paper introduces additional coding efficiency in speech coding by reducing the temporal redundancy existing in the frame-level feature sequence via a feature predictor. This predictor produces low-entropy residual representations, and we discriminatively code them based on their contribution to the signal reconstruction. Combining feature prediction and discriminative coding optimizes bitrate efficiency by assigning more bits to hard-to-predict events. We demonstrate the advantage of the proposed methods using the LPCNet as a neural vocoder, resulting in a scalable, lightweight, low-latency, and low-bitrate neural speech coding system. While our approach guarantees strict causality in the frame-level prediction, the subjective tests and feature space analysis show that our model achieves superior coding efficiency compared to the loosely-causal LPCNet and Lyra V2 in the very low bitrates.","['https://openalex.org/W3015780049', 'https://openalex.org/W6762390287', 'https://openalex.org/W2972354707', 'https://openalex.org/W2963182577', 'https://openalex.org/W2809824582', 'https://openalex.org/W2911317657', 'https://openalex.org/W2151626637', 'https://openalex.org/W2165291881', 'https://openalex.org/W3215615641', 'https://openalex.org/W3214758449', 'https://openalex.org/W3016098186', 'https://openalex.org/W3160576174', 'https://openalex.org/W2157331557', 'https://openalex.org/W6748409065', 'https://openalex.org/W3161744562', 'https://openalex.org/W4284957875', 'https://openalex.org/W4284888249', 'https://openalex.org/W3163662330', 'https://openalex.org/W2890983311', 'https://openalex.org/W1494198834', 'https://openalex.org/W6755853340', 'https://openalex.org/W6760712927', 'https://openalex.org/W4206534210', 'https://openalex.org/W2775336875', 'https://openalex.org/W6639363673', 'https://openalex.org/W2963091184', 'https://openalex.org/W4205788663', 'https://openalex.org/W3096468295', 'https://openalex.org/W4298580827', 'https://openalex.org/W2963208781', 'https://openalex.org/W2972519044']",2023-05-05
https://openalex.org/W4372263438,https://doi.org/10.1109/icassp49357.2023.10096760,Full-Band General Audio Synthesis with Score-Based Diffusion,"Recent works have shown the capability of deep generative models to tackle general audio synthesis from a single label, producing a variety of impulsive, tonal, and environmental sounds. Such models operate on band-limited signals and, as a result of an autoregressive approach, they are typically conformed by pre-trained latent encoders and/or several cascaded modules. In this work, we propose a diffusion-based generative model for general audio synthesis, named DAG, which deals with full-band signals end-to-end in the waveform domain. Results show the superiority of DAG over existing label-conditioned generators in terms of both quality and diversity. More specifically, when compared to the state of the art, the band-limited and full-band versions of DAG achieve relative improvements that go up to 40 and 65%, respectively. We believe DAG is flexible enough to accommodate different conditioning schemas while providing good quality synthesis.","['https://openalex.org/W1570629387', 'https://openalex.org/W6776218486', 'https://openalex.org/W6796762324', 'https://openalex.org/W6782757012', 'https://openalex.org/W3207785894', 'https://openalex.org/W6804172200', 'https://openalex.org/W2935170919', 'https://openalex.org/W3214281017', 'https://openalex.org/W6762931180', 'https://openalex.org/W6747491877', 'https://openalex.org/W6783867762', 'https://openalex.org/W4367359628', 'https://openalex.org/W6845479124', 'https://openalex.org/W6765775151', 'https://openalex.org/W6786375611', 'https://openalex.org/W6783182287', 'https://openalex.org/W6782760101', 'https://openalex.org/W4281820413', 'https://openalex.org/W6782380777', 'https://openalex.org/W3144575004', 'https://openalex.org/W6840815571', 'https://openalex.org/W6838639034', 'https://openalex.org/W2038484192', 'https://openalex.org/W2566935005', 'https://openalex.org/W2972478942', 'https://openalex.org/W2939574508', 'https://openalex.org/W6774125049', 'https://openalex.org/W6747733185', 'https://openalex.org/W4288099666', 'https://openalex.org/W4286901121', 'https://openalex.org/W4225856875', 'https://openalex.org/W2971074500', 'https://openalex.org/W4287802874', 'https://openalex.org/W4281485151', 'https://openalex.org/W3092028330', 'https://openalex.org/W4300980117', 'https://openalex.org/W2959300817']",2023-05-05
https://openalex.org/W4312933868,https://doi.org/10.1109/cvpr52688.2022.01042,High-Resolution Image Synthesis with Latent Diffusion Models,"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.","['https://openalex.org/W2962770929', 'https://openalex.org/W6745560452', 'https://openalex.org/W2963073614', 'https://openalex.org/W6840815571', 'https://openalex.org/W6797359156', 'https://openalex.org/W6772676370', 'https://openalex.org/W6795308914', 'https://openalex.org/W6790830454', 'https://openalex.org/W6799838802', 'https://openalex.org/W6765779288', 'https://openalex.org/W6779823529', 'https://openalex.org/W6692550842', 'https://openalex.org/W6786904838', 'https://openalex.org/W6800989748', 'https://openalex.org/W2970810102', 'https://openalex.org/W6787283782', 'https://openalex.org/W3203631022', 'https://openalex.org/W6637568146', 'https://openalex.org/W2982763192', 'https://openalex.org/W6797201126', 'https://openalex.org/W6639102338', 'https://openalex.org/W6768377600', 'https://openalex.org/W2962974533', 'https://openalex.org/W6757857098', 'https://openalex.org/W6793578827', 'https://openalex.org/W6678815747', 'https://openalex.org/W6729767818', 'https://openalex.org/W6747808759', 'https://openalex.org/W6799880410', 'https://openalex.org/W2561196672', 'https://openalex.org/W6755312952', 'https://openalex.org/W6779879114', 'https://openalex.org/W6787335730', 'https://openalex.org/W6785302134', 'https://openalex.org/W2937274663', 'https://openalex.org/W6782760101', 'https://openalex.org/W6786494455', 'https://openalex.org/W6796042156', 'https://openalex.org/W6640963894', 'https://openalex.org/W6788995615', 'https://openalex.org/W6783182287', 'https://openalex.org/W6797016743', 'https://openalex.org/W6771275388', 'https://openalex.org/W6797906067', 'https://openalex.org/W3140633421', 'https://openalex.org/W6732492507', 'https://openalex.org/W6794269193', 'https://openalex.org/W6637373629', 'https://openalex.org/W6803132585', 'https://openalex.org/W3035687950', 'https://openalex.org/W2129069237', 'https://openalex.org/W6794386015', 'https://openalex.org/W3176823897', 'https://openalex.org/W6790978476', 'https://openalex.org/W6762931180', 'https://openalex.org/W6713645886', 'https://openalex.org/W1909320841', 'https://openalex.org/W6785102375', 'https://openalex.org/W6639824700', 'https://openalex.org/W2741137940', 'https://openalex.org/W6768817161', 'https://openalex.org/W6739901393', 'https://openalex.org/W2752796333', 'https://openalex.org/W6804566349', 'https://openalex.org/W2423557781', 'https://openalex.org/W2732026016', 'https://openalex.org/W6796588791', 'https://openalex.org/W6791190667', 'https://openalex.org/W2962785568', 'https://openalex.org/W6625168331', 'https://openalex.org/W6783637136', 'https://openalex.org/W6793801364', 'https://openalex.org/W6761628794', 'https://openalex.org/W6758800702', 'https://openalex.org/W6676297131', 'https://openalex.org/W6755207826', 'https://openalex.org/W6795288823', 'https://openalex.org/W6786375611', 'https://openalex.org/W6796242362', 'https://openalex.org/W6783713337', 'https://openalex.org/W6775632556', 'https://openalex.org/W6714644935', 'https://openalex.org/W6780573874', 'https://openalex.org/W6687045409', 'https://openalex.org/W6780593937', 'https://openalex.org/W6801666068', 'https://openalex.org/W6775110864', 'https://openalex.org/W3120110650', 'https://openalex.org/W2031342017', 'https://openalex.org/W4293320219', 'https://openalex.org/W4286869901', 'https://openalex.org/W3169064633', 'https://openalex.org/W3190116597', 'https://openalex.org/W3165905282', 'https://openalex.org/W3206395542', 'https://openalex.org/W3002120414', 'https://openalex.org/W1583912456', 'https://openalex.org/W2980282514', 'https://openalex.org/W3041956526', 'https://openalex.org/W4294643831', 'https://openalex.org/W3209532394', 'https://openalex.org/W3129651364', 'https://openalex.org/W3129576130', 'https://openalex.org/W3174711319', 'https://openalex.org/W3110257065', 'https://openalex.org/W2188365844', 'https://openalex.org/W3123097577', 'https://openalex.org/W2907097116', 'https://openalex.org/W4301206121', 'https://openalex.org/W3034445277', 'https://openalex.org/W4298289240', 'https://openalex.org/W3177150392', 'https://openalex.org/W2125389028', 'https://openalex.org/W4295521014', 'https://openalex.org/W4297813370', 'https://openalex.org/W3035574324', 'https://openalex.org/W3196163807', 'https://openalex.org/W3171313410', 'https://openalex.org/W2405756170', 'https://openalex.org/W2783391889', 'https://openalex.org/W3153854932', 'https://openalex.org/W3130440474', 'https://openalex.org/W3152733922', 'https://openalex.org/W2896457183', 'https://openalex.org/W4288372760', 'https://openalex.org/W967544008', 'https://openalex.org/W2963799213', 'https://openalex.org/W2108598243', 'https://openalex.org/W4287182033', 'https://openalex.org/W2953318193', 'https://openalex.org/W3111387095', 'https://openalex.org/W3165647589', 'https://openalex.org/W4323654151', 'https://openalex.org/W3216237230', 'https://openalex.org/W3180355996', 'https://openalex.org/W3036167779', 'https://openalex.org/W4297798428', 'https://openalex.org/W1861492603', 'https://openalex.org/W3199003182', 'https://openalex.org/W3163884521', 'https://openalex.org/W1901129140', 'https://openalex.org/W2962820504', 'https://openalex.org/W4287553002', 'https://openalex.org/W3103556460', 'https://openalex.org/W4301908144', 'https://openalex.org/W3190965961', 'https://openalex.org/W3162926177', 'https://openalex.org/W4287250916', 'https://openalex.org/W4385245566', 'https://openalex.org/W2971074500', 'https://openalex.org/W4288099666', 'https://openalex.org/W3120243996', 'https://openalex.org/W3155072588', 'https://openalex.org/W2964122153', 'https://openalex.org/W4287121833', 'https://openalex.org/W3037032032', 'https://openalex.org/W3175528029', 'https://openalex.org/W3121370741', 'https://openalex.org/W1686810756', 'https://openalex.org/W3118605064', 'https://openalex.org/W3121480429', 'https://openalex.org/W1959608418', 'https://openalex.org/W2952716587', 'https://openalex.org/W4297800839', 'https://openalex.org/W3174301209']",2022-06-01
https://openalex.org/W2401258970,https://doi.org/10.21437/interspeech.2012-181,Speech enhancement by online non-negative spectrogram decomposition in nonstationary noise environments,"Classical single-channel speech enhancement algorithms have two convenient properties: they require pre-learning the noise model but not the speech model, and they work online. However, they often have difficulties in dealing with non-stationary noise sources. Source separation algorithms based on nonnegative spectrogram decompositions are capable of dealing with non-stationary noise, but do not possess the aforementioned properties. In this paper we present a novel algorithm that combines the advantages of both classical algorithms and non-negative spectrogram decomposition algorithms. Experiments show that it significantly outperforms four categories of classical algorithms in non-stationary noise environments.","['https://openalex.org/W2153894152', 'https://openalex.org/W1504438288', 'https://openalex.org/W1552314771', 'https://openalex.org/W2051057783', 'https://openalex.org/W2121973264', 'https://openalex.org/W2135151673', 'https://openalex.org/W146815106', 'https://openalex.org/W2058269967', 'https://openalex.org/W2112447569', 'https://openalex.org/W2114780615', 'https://openalex.org/W2145760110', 'https://openalex.org/W304179306', 'https://openalex.org/W1495679096', 'https://openalex.org/W3031363333', 'https://openalex.org/W2127851351']",2012-09-09
https://openalex.org/W4385680913,https://doi.org/10.48550/arxiv.2308.02560,From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion,"Deep generative models can generate high-fidelity audio conditioned on various types of representations (e.g., mel-spectrograms, Mel-frequency Cepstral Coefficients (MFCC)). Recently, such models have been used to synthesize audio waveforms conditioned on highly compressed representations. Although such methods produce impressive results, they are prone to generate audible artifacts when the conditioning is flawed or imperfect. An alternative modeling approach is to use diffusion models. However, these have mainly been used as speech vocoders (i.e., conditioned on mel-spectrograms) or generating relatively low sampling rate signals. In this work, we propose a high-fidelity multi-band diffusion-based framework that generates any type of audio modality (e.g., speech, music, environmental sounds) from low-bitrate discrete representations. At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality. Training and, evaluation code, along with audio samples, are available on the facebookresearch/audiocraft Github page.",[],2023-08-02
https://openalex.org/W4318752004,https://doi.org/10.48550/arxiv.2301.12503,AudioLDM: Text-to-Audio Generation with Latent Diffusion Models,"Text-to-audio (TTA) system has recently gained attention for its ability to synthesize general audio based on text descriptions. However, previous studies in TTA have limited generation quality with high computational costs. In this study, we propose AudioLDM, a TTA system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs with audio embedding while providing text embedding as a condition during sampling. By learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, AudioLDM is advantageous in both generation quality and computational efficiency. Trained on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA performance measured by both objective and subjective metrics (e.g., frechet distance). Moreover, AudioLDM is the first TTA system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion. Our implementation and demos are available at https://audioldm.github.io.",[],2023-01-29
https://openalex.org/W3197349023,https://doi.org/10.21437/interspeech.2021-1465,Information Retrieval for ZeroSpeech 2021: The Submission by University of Wroclaw,"We present a number of low-resource approaches to the tasks of the Zero Resource Speech Challenge 2021. We build on the unsupervised representations of speech proposed by the organizers as a baseline, derived from CPC and clustered with the k-means algorithm. We demonstrate that simple methods of refining those representations can narrow the gap, or even improve upon the solutions which use a high computational budget. The results lead to the conclusion that the CPC-derived representations are still too noisy for training language models, but stable enough for simpler forms of pattern matching and retrieval.","['https://openalex.org/W2142625445', 'https://openalex.org/W4385245566', 'https://openalex.org/W4362220304', 'https://openalex.org/W2053921957', 'https://openalex.org/W2963751529', 'https://openalex.org/W2996728628', 'https://openalex.org/W2252211741', 'https://openalex.org/W2965373594', 'https://openalex.org/W4287591426', 'https://openalex.org/W2995181338', 'https://openalex.org/W2786608204', 'https://openalex.org/W2251803266', 'https://openalex.org/W1494198834', 'https://openalex.org/W2741692265', 'https://openalex.org/W4297808394', 'https://openalex.org/W2963979492', 'https://openalex.org/W2963250244', 'https://openalex.org/W1614298861', 'https://openalex.org/W2128160875']",2021-06-22
https://openalex.org/W2123799894,https://doi.org/10.1109/icassp.1999.758133,Decision tree state tying based on penalized Bayesian information criterion,"In this paper, an approach of the penalized Bayesian information criterion (pBIC) for decision tree state tying is described. The pBIC is applied to two important applications. First, it is used as a decision tree growing criterion in place of the conventional approach of using a heuristic constant threshold. It is found that original BIC penalty is too low and will not lead to a compact decision tree state tying model. Based on Wolfe's modification to the asymptotic null distribution, it is derived that two times BIC penalty should be used for decision tree state tying based on pBIC. Secondly, pBIC is studied as a model compression criterion for decision tree state tying based acoustic modeling. Experimental results on a large vocabulary (Wall Street Journal) speech recognition task indicate that a compact decision tree could be achieved with almost no loss of the speech recognition performance.","['https://openalex.org/W6676027155', 'https://openalex.org/W2168175751', 'https://openalex.org/W6682834260', 'https://openalex.org/W2110765229', 'https://openalex.org/W2117647530', 'https://openalex.org/W6681236446', 'https://openalex.org/W6678729146', 'https://openalex.org/W2126055319', 'https://openalex.org/W2107617395', 'https://openalex.org/W2142635246', 'https://openalex.org/W2153190454', 'https://openalex.org/W77364642', 'https://openalex.org/W2099756352', 'https://openalex.org/W4379510236']",1999-01-01
https://openalex.org/W2405331948,https://doi.org/10.1017/cbo9781107295360,Bayesian Speech and Language Processing,"With this comprehensive guide you will learn how to apply Bayesian machine learning techniques systematically to solve various problems in speech and language processing. A range of statistical models is detailed, from hidden Markov models to Gaussian mixture models, n-gram models and latent topic models, along with applications including automatic speech recognition, speaker verification, and information retrieval. Approximate Bayesian inferences based on MAP, Evidence, Asymptotic, VB, and MCMC approximations are provided as well as full derivations of calculations, useful notations, formulas, and rules. The authors address the difficulties of straightforward applications and provide detailed examples and case studies to demonstrate how you can successfully use practical Bayesian inference methods to improve the performance of information systems. This is an invaluable resource for students, researchers, and industry practitioners working in machine learning, signal processing, and speech and language processing.","['https://openalex.org/W2068970468', 'https://openalex.org/W195465510', 'https://openalex.org/W2104687512', 'https://openalex.org/W2041823554', 'https://openalex.org/W2408623794', 'https://openalex.org/W2019599312', 'https://openalex.org/W2111581792', 'https://openalex.org/W4285719527', 'https://openalex.org/W2106554350', 'https://openalex.org/W2974222084', 'https://openalex.org/W2158190429', 'https://openalex.org/W1990005915', 'https://openalex.org/W2102439588', 'https://openalex.org/W2078769636', 'https://openalex.org/W2089484716', 'https://openalex.org/W2110575115', 'https://openalex.org/W2143990842', 'https://openalex.org/W2087309226', 'https://openalex.org/W1967687583', 'https://openalex.org/W1989926363', 'https://openalex.org/W2164700406', 'https://openalex.org/W2167266655', 'https://openalex.org/W1604792744', 'https://openalex.org/W2111668269', 'https://openalex.org/W1975341057', 'https://openalex.org/W2142635246', 'https://openalex.org/W2147768505', 'https://openalex.org/W2071284784', 'https://openalex.org/W1994396704', 'https://openalex.org/W1555754712', 'https://openalex.org/W2125610823', 'https://openalex.org/W2160815625', 'https://openalex.org/W2131668296', 'https://openalex.org/W2107638917', 'https://openalex.org/W2146871184', 'https://openalex.org/W1590183771', 'https://openalex.org/W1978394996', 'https://openalex.org/W2149402184', 'https://openalex.org/W1516111018', 'https://openalex.org/W2090861223', 'https://openalex.org/W2075201173', 'https://openalex.org/W2063541597', 'https://openalex.org/W2020073413', 'https://openalex.org/W2118595744', 'https://openalex.org/W1663973292', 'https://openalex.org/W2121981798', 'https://openalex.org/W2145284369', 'https://openalex.org/W2184045248', 'https://openalex.org/W1965555277', 'https://openalex.org/W3129711340', 'https://openalex.org/W2139701068', 'https://openalex.org/W2100506586', 'https://openalex.org/W1486632395', 'https://openalex.org/W2156358825', 'https://openalex.org/W2132119275', 'https://openalex.org/W1880262756', 'https://openalex.org/W1967269193', 'https://openalex.org/W1982309858', 'https://openalex.org/W2091233543', 'https://openalex.org/W2148154194', 'https://openalex.org/W2160373860', 'https://openalex.org/W1979136262', 'https://openalex.org/W1977434607', 'https://openalex.org/W2166183437', 'https://openalex.org/W164706946', 'https://openalex.org/W3148186152', 'https://openalex.org/W1539673959', 'https://openalex.org/W2141211247', 'https://openalex.org/W2158273884', 'https://openalex.org/W2031284124', 'https://openalex.org/W2126723482', 'https://openalex.org/W1963627370', 'https://openalex.org/W2154278880', 'https://openalex.org/W2100969003', 'https://openalex.org/W2080276390', 'https://openalex.org/W2147147599', 'https://openalex.org/W2165108269', 'https://openalex.org/W2045656233', 'https://openalex.org/W2168175751', 'https://openalex.org/W2136922672', 'https://openalex.org/W2097333193', 'https://openalex.org/W2069631319', 'https://openalex.org/W2152051032', 'https://openalex.org/W2069739265', 'https://openalex.org/W2151967501', 'https://openalex.org/W108776547', 'https://openalex.org/W1568586930', 'https://openalex.org/W1997309301', 'https://openalex.org/W2165058613', 'https://openalex.org/W2126377586', 'https://openalex.org/W1975113979', 'https://openalex.org/W127575240', 'https://openalex.org/W2020999234', 'https://openalex.org/W1574530145', 'https://openalex.org/W2072773380', 'https://openalex.org/W2097728881', 'https://openalex.org/W1550206324', 'https://openalex.org/W2049633694', 'https://openalex.org/W1978380426', 'https://openalex.org/W2082092506', 'https://openalex.org/W2484208911', 'https://openalex.org/W1981796042', 'https://openalex.org/W58750608', 'https://openalex.org/W2153914468', 'https://openalex.org/W2159549127', 'https://openalex.org/W1981735114', 'https://openalex.org/W2138569875', 'https://openalex.org/W2121227244', 'https://openalex.org/W2080972498', 'https://openalex.org/W2081976287', 'https://openalex.org/W1993666847', 'https://openalex.org/W2052269122', 'https://openalex.org/W2069429561', 'https://openalex.org/W2033765726', 'https://openalex.org/W2039206676', 'https://openalex.org/W2088538739', 'https://openalex.org/W2105594594', 'https://openalex.org/W2046932483', 'https://openalex.org/W2338994564', 'https://openalex.org/W2102522729', 'https://openalex.org/W2150769028', 'https://openalex.org/W2115979064', 'https://openalex.org/W2156909104', 'https://openalex.org/W1578856370', 'https://openalex.org/W2158266063', 'https://openalex.org/W2001082470', 'https://openalex.org/W2167270514', 'https://openalex.org/W2146049072', 'https://openalex.org/W2149175990', 'https://openalex.org/W2113641473', 'https://openalex.org/W2127836646', 'https://openalex.org/W2110110767', 'https://openalex.org/W1558333962', 'https://openalex.org/W2161562001', 'https://openalex.org/W2147808133', 'https://openalex.org/W2134731454', 'https://openalex.org/W2160246131', 'https://openalex.org/W2134237567', 'https://openalex.org/W2095755195', 'https://openalex.org/W2036039036', 'https://openalex.org/W2001679125', 'https://openalex.org/W2911546748', 'https://openalex.org/W2248894183', 'https://openalex.org/W2135346934', 'https://openalex.org/W2111051539', 'https://openalex.org/W2081074144', 'https://openalex.org/W2134749286', 'https://openalex.org/W2037668034', 'https://openalex.org/W1987245796', 'https://openalex.org/W2033436836', 'https://openalex.org/W2158195707', 'https://openalex.org/W1551893515', 'https://openalex.org/W2127469744', 'https://openalex.org/W2153514218', 'https://openalex.org/W2002342963', 'https://openalex.org/W2118714763', 'https://openalex.org/W1508165687', 'https://openalex.org/W1734538896', 'https://openalex.org/W2098397089', 'https://openalex.org/W1574901103', 'https://openalex.org/W1579838312', 'https://openalex.org/W2159399018', 'https://openalex.org/W2132957691', 'https://openalex.org/W29489373', 'https://openalex.org/W2138309709', 'https://openalex.org/W1924689489', 'https://openalex.org/W2162995740', 'https://openalex.org/W2150286230', 'https://openalex.org/W2139890545', 'https://openalex.org/W2069976350', 'https://openalex.org/W2111732304', 'https://openalex.org/W1560013842', 'https://openalex.org/W2120636621', 'https://openalex.org/W2015631279', 'https://openalex.org/W1996206993', 'https://openalex.org/W2082474452', 'https://openalex.org/W2127158193', 'https://openalex.org/W2132827946', 'https://openalex.org/W2150907703', 'https://openalex.org/W2106885563', 'https://openalex.org/W2023737427', 'https://openalex.org/W2070707809', 'https://openalex.org/W2056760934', 'https://openalex.org/W2185719453', 'https://openalex.org/W2158808283', 'https://openalex.org/W1902027874', 'https://openalex.org/W2153164668']",2015-07-15
https://openalex.org/W2975381464,https://doi.org/10.48550/arxiv.1909.11556,Reducing Transformer Depth on Demand with Structured Dropout,"Overparameterized transformer networks have obtained state of the art results in various natural language processing tasks, such as machine translation, language modeling, and question answering. These models contain hundreds of millions of parameters, necessitating a large amount of computation and making them prone to overfitting. In this work, we explore LayerDrop, a form of structured dropout, which has a regularization effect during training and allows for efficient pruning at inference time. In particular, we show that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance. We demonstrate the effectiveness of our approach by improving the state of the art on machine translation, language modeling, summarization, question answering, and language understanding benchmarks. Moreover, we show that our approach leads to small BERT-like models of higher quality compared to training from scratch or using distillation.","['https://openalex.org/W2969601108', 'https://openalex.org/W2951569836', 'https://openalex.org/W2951560313', 'https://openalex.org/W2911109671', 'https://openalex.org/W2095705004', 'https://openalex.org/W2894175714', 'https://openalex.org/W2965373594', 'https://openalex.org/W2963993763', 'https://openalex.org/W2964019666', 'https://openalex.org/W2194775991', 'https://openalex.org/W2963000224', 'https://openalex.org/W4919037', 'https://openalex.org/W2946567085', 'https://openalex.org/W2515385951', 'https://openalex.org/W2518108298', 'https://openalex.org/W2154652894', 'https://openalex.org/W1544827683', 'https://openalex.org/W2963341956', 'https://openalex.org/W2251939518', 'https://openalex.org/W2956301977', 'https://openalex.org/W2945767825', 'https://openalex.org/W2950813464', 'https://openalex.org/W131533222', 'https://openalex.org/W2963846996', 'https://openalex.org/W104184427', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963807318', 'https://openalex.org/W2924902521', 'https://openalex.org/W2907121943', 'https://openalex.org/W2964335273', 'https://openalex.org/W2460130460', 'https://openalex.org/W2908336025', 'https://openalex.org/W2971524460', 'https://openalex.org/W2943493972', 'https://openalex.org/W2768716007', 'https://openalex.org/W2114766824', 'https://openalex.org/W2964040452', 'https://openalex.org/W2950452665', 'https://openalex.org/W2963363373', 'https://openalex.org/W2331143823', 'https://openalex.org/W2975429091', 'https://openalex.org/W2963403868', 'https://openalex.org/W2951528897', 'https://openalex.org/W2953071172', 'https://openalex.org/W2963970792', 'https://openalex.org/W2972451902', 'https://openalex.org/W1904365287', 'https://openalex.org/W2525332836', 'https://openalex.org/W2563351168', 'https://openalex.org/W2964308564', 'https://openalex.org/W2606974598', 'https://openalex.org/W2963748441', 'https://openalex.org/W2933138175', 'https://openalex.org/W2963310665', 'https://openalex.org/W2963382930', 'https://openalex.org/W2914120296']",2019-09-25
https://openalex.org/W2996728628,https://doi.org/10.1162/tacl_a_00321,BLiMP: The Benchmark of Linguistic Minimal Pairs for English (Electronic Resources),"We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs—that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4%. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.","['https://openalex.org/W2888922637', 'https://openalex.org/W2963025830', 'https://openalex.org/W1986120881', 'https://openalex.org/W2963403868', 'https://openalex.org/W2167723982', 'https://openalex.org/W2995181640', 'https://openalex.org/W2163730805', 'https://openalex.org/W2923014074', 'https://openalex.org/W2750779823', 'https://openalex.org/W2962961857', 'https://openalex.org/W2973957133', 'https://openalex.org/W2953369973', 'https://openalex.org/W2787560479', 'https://openalex.org/W2911109671', 'https://openalex.org/W2963494889', 'https://openalex.org/W3037191812', 'https://openalex.org/W2918996109', 'https://openalex.org/W2963026768', 'https://openalex.org/W1582487387', 'https://openalex.org/W2064675550', 'https://openalex.org/W2158195707', 'https://openalex.org/W2770232188', 'https://openalex.org/W4365799947', 'https://openalex.org/W2978670439', 'https://openalex.org/W2972351548', 'https://openalex.org/W2759181158', 'https://openalex.org/W2170716495', 'https://openalex.org/W4252209867', 'https://openalex.org/W2964117978', 'https://openalex.org/W179875071', 'https://openalex.org/W2730712696', 'https://openalex.org/W1502957213', 'https://openalex.org/W2549835527', 'https://openalex.org/W2971044268', 'https://openalex.org/W2531882892', 'https://openalex.org/W2994665957', 'https://openalex.org/W4388123003', 'https://openalex.org/W2964204621', 'https://openalex.org/W2891399254', 'https://openalex.org/W1984471812', 'https://openalex.org/W1974795422', 'https://openalex.org/W2963341956', 'https://openalex.org/W2599674900', 'https://openalex.org/W2612690371', 'https://openalex.org/W4245765565', 'https://openalex.org/W2864832950', 'https://openalex.org/W2563574619', 'https://openalex.org/W2124669395', 'https://openalex.org/W2515741950', 'https://openalex.org/W2141440284', 'https://openalex.org/W2972752636', 'https://openalex.org/W2981852735', 'https://openalex.org/W2902967615', 'https://openalex.org/W1586060904', 'https://openalex.org/W2962926715', 'https://openalex.org/W2251930319', 'https://openalex.org/W2024988999', 'https://openalex.org/W2990704537']",2020-07-29
https://openalex.org/W1963627370,https://doi.org/10.1250/ast.21.79,MDL-based context-dependent subword modeling for speech recognition.,"Context-dependent phone units, such as triphones, have recently come to be used to model subword units in speech recognition systems that are based on the use of hidden Markov models(HMMs).While most such systems employ clustering of the HMM parameters(e.g., subword clustering and state clustering)to control the HMM size, so as to avoid poor recognition accuracy due to a lack of training data, none of them provide any effective criteria for determining the optimal number of clusters.This paper proposes a method in which state clustering is accomplished by way of phonetic decision trees and in which the minimum description length(MDL)criterion is used to optimize the number of clusters.Large-vocabulary Japanese-language recognition experiments show that this method achieves higher accuracy than the maximum-likelihood approach.","['https://openalex.org/W2100551412', 'https://openalex.org/W2050597349', 'https://openalex.org/W2098773974', 'https://openalex.org/W2044223162', 'https://openalex.org/W1971081490', 'https://openalex.org/W2068970468', 'https://openalex.org/W2096466030', 'https://openalex.org/W2020577846', 'https://openalex.org/W2158069733', 'https://openalex.org/W1872806465', 'https://openalex.org/W2132119275', 'https://openalex.org/W2110134127', 'https://openalex.org/W1560013842']",2000-01-01
https://openalex.org/W2152790380,,Noise-contrastive estimation: A new estimation principle for unnormalized statistical models,"We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity. We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance. In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field. 1","['https://openalex.org/W2496945004', 'https://openalex.org/W2155118899', 'https://openalex.org/W2097620723', 'https://openalex.org/W2142615865', 'https://openalex.org/W2134653808', 'https://openalex.org/W3140968660', 'https://openalex.org/W2116064496', 'https://openalex.org/W1802356529', 'https://openalex.org/W1559616325', 'https://openalex.org/W2130184048', 'https://openalex.org/W1548802052', 'https://openalex.org/W1582813132', 'https://openalex.org/W2110176223', 'https://openalex.org/W2165363188']",2010-03-31
https://openalex.org/W2294351487,https://doi.org/10.21437/interspeech.2013-102,Voice conversion in high-order eigen space using deep belief nets,"This paper presents a voice conversion technique using Deep Belief Nets (DBNs) to build high-order eigen spaces of the source/target speakers, where it is easier to convert the source speech to the target speech than in the traditional cepstrum space. DBNs have a deep architecture that automatically discovers abstractions to maximally express the original input features. If we train the DBNs using only the speech of an individual speaker, it can be considered that there is less phonological information and relatively more speaker individuality in the output features at the highest layer. Training the DBNs for a source speaker and a target speaker, we can then connect and convert the speaker individuality abstractions using Neural Networks (NNs). The converted abstraction of the source speaker is then brought back to the cepstrum space using an inverse process of the DBNs of the target speaker. We conducted speakervoice conversion experiments and confirmed the efficacy of our method with respect to subjective and objective criteria, comparing it with the conventional Gaussian Mixture Model-based method.","['https://openalex.org/W1993882792', 'https://openalex.org/W2118850452', 'https://openalex.org/W2017425464', 'https://openalex.org/W2156142001', 'https://openalex.org/W2139671364', 'https://openalex.org/W49412823', 'https://openalex.org/W2022125261', 'https://openalex.org/W2126143605', 'https://openalex.org/W2120605154', 'https://openalex.org/W2105160541', 'https://openalex.org/W2146071708', 'https://openalex.org/W2161893161', 'https://openalex.org/W1963778986', 'https://openalex.org/W10800834', 'https://openalex.org/W2158808283', 'https://openalex.org/W2136922672', 'https://openalex.org/W2407110532', 'https://openalex.org/W1588266896', 'https://openalex.org/W17772838', 'https://openalex.org/W2123003832', 'https://openalex.org/W2152384560', 'https://openalex.org/W1965255698', 'https://openalex.org/W1950777589']",2013-08-25
https://openalex.org/W4283659485,https://doi.org/10.48550/arxiv.2204.09224,ContentVec: An Improved Self-Supervised Speech Representation by Disentangling Speakers,"Self-supervised learning in speech involves training a speech representation network on a large-scale unannotated speech corpus, and then applying the learned representations to downstream tasks. Since the majority of the downstream tasks of SSL learning in speech largely focus on the content information in speech, the most desirable speech representations should be able to disentangle unwanted variations, such as speaker variations, from the content. However, disentangling speakers is very challenging, because removing the speaker information could easily result in a loss of content as well, and the damage of the latter usually far outweighs the benefit of the former. In this paper, we propose a new SSL method that can achieve speaker disentanglement without severe loss of content. Our approach is adapted from the HuBERT framework, and incorporates disentangling mechanisms to regularize both the teacher labels and the learned representations. We evaluate the benefit of speaker disentanglement on a set of content-related downstream tasks, and observe a consistent and notable performance advantage of our speaker-disentangled representations.",[],2022-04-20
https://openalex.org/W4372266960,https://doi.org/10.1109/icassp49357.2023.10095565,A Unified One-Shot Prosody and Speaker Conversion System with Self-Supervised Discrete Speech Units,"We present a unified system to realize one-shot voice conversion (VC) on the pitch, rhythm, and speaker attributes. Existing works generally ignore the correlation between prosody and language content, leading to the degradation of naturalness in converted speech. Additionally, the lack of proper language features prevents these systems from accurately preserving language content after conversion. To address these issues, we devise a cascaded modular system leveraging self-supervised discrete speech units as language representation. These discrete units provide duration information essential for rhythm modeling. Our system first extracts utterance-level prosody and speaker representations from the raw waveform. Given the prosody representation, a prosody predictor estimates pitch, energy, and duration for each discrete unit in the utterance. A synthesizer further reconstructs speech based on the predicted prosody, speaker representation, and discrete units. Experiments show that our system outperforms previous approaches in naturalness, intelligibility, speaker transferability, and prosody transferability. Code and samples are publicly available. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>","['https://openalex.org/W3207300132', 'https://openalex.org/W6803547063', 'https://openalex.org/W2962866891', 'https://openalex.org/W2143827132', 'https://openalex.org/W3163573274', 'https://openalex.org/W2973049979', 'https://openalex.org/W6780218876', 'https://openalex.org/W3015805741', 'https://openalex.org/W6762533536', 'https://openalex.org/W6780226713', 'https://openalex.org/W6607364320', 'https://openalex.org/W2593414223', 'https://openalex.org/W3205631867', 'https://openalex.org/W6783867762', 'https://openalex.org/W3198217962', 'https://openalex.org/W2972359262', 'https://openalex.org/W6772349387', 'https://openalex.org/W6763832098', 'https://openalex.org/W4287887366', 'https://openalex.org/W3209059054', 'https://openalex.org/W6809829207', 'https://openalex.org/W3016160783', 'https://openalex.org/W4225939199', 'https://openalex.org/W6776390925', 'https://openalex.org/W4296068587', 'https://openalex.org/W3096524539', 'https://openalex.org/W2947445680', 'https://openalex.org/W6796577156', 'https://openalex.org/W3197659778', 'https://openalex.org/W3034794073', 'https://openalex.org/W4361994820', 'https://openalex.org/W3036601975', 'https://openalex.org/W2187089797', 'https://openalex.org/W2998572311', 'https://openalex.org/W181056519', 'https://openalex.org/W3092028330', 'https://openalex.org/W3169739675', 'https://openalex.org/W4301371414', 'https://openalex.org/W2945478979', 'https://openalex.org/W2946200149']",2023-05-05
https://openalex.org/W4297412183,https://doi.org/10.48550/arxiv.2209.11866,ControlVC: Zero-Shot Voice Conversion with Time-Varying Controls on Pitch and Speed,"Recent developments in neural speech synthesis and vocoding have sparked a renewed interest in voice conversion (VC). Beyond timbre transfer, achieving controllability on para-linguistic parameters such as pitch and Speed is critical in deploying VC systems in many application scenarios. Existing studies, however, either only provide utterance-level global control or lack interpretability on the controls. In this paper, we propose ControlVC, the first neural voice conversion system that achieves time-varying controls on pitch and speed. ControlVC uses pre-trained encoders to compute pitch and linguistic embeddings from the source utterance and speaker embeddings from the target utterance. These embeddings are then concatenated and converted to speech using a vocoder. It achieves speed control through TD-PSOLA pre-processing on the source utterance, and achieves pitch control by manipulating the pitch contour before feeding it to the pitch encoder. Systematic subjective and objective evaluations are conducted to assess the speech quality and controllability. Results show that, on non-parallel and zero-shot conversion tasks, ControlVC significantly outperforms two other self-constructed baselines on speech quality, and it can successfully achieve time-varying pitch and speed control.","['https://openalex.org/W2127836593', 'https://openalex.org/W2095734449', 'https://openalex.org/W2945478979', 'https://openalex.org/W3168719651', 'https://openalex.org/W2963539064', 'https://openalex.org/W3092028330', 'https://openalex.org/W1973685422', 'https://openalex.org/W3082130377', 'https://openalex.org/W1494198834', 'https://openalex.org/W4225892118', 'https://openalex.org/W3134921434', 'https://openalex.org/W4301371414', 'https://openalex.org/W3098557217', 'https://openalex.org/W2963490782', 'https://openalex.org/W2115098197', 'https://openalex.org/W1975163393', 'https://openalex.org/W2003375858', 'https://openalex.org/W3140429000', 'https://openalex.org/W3015805741', 'https://openalex.org/W567437002', 'https://openalex.org/W2148228080', 'https://openalex.org/W4286905904', 'https://openalex.org/W3196667132', 'https://openalex.org/W1509691205', 'https://openalex.org/W4200027410', 'https://openalex.org/W4287802874', 'https://openalex.org/W4225680573', 'https://openalex.org/W4312732823', 'https://openalex.org/W3034794073', 'https://openalex.org/W3161627112', 'https://openalex.org/W2726515241']",2022-09-23
https://openalex.org/W2972659941,https://doi.org/10.21437/interspeech.2019-2663,One-Shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization,"Recently, voice conversion (VC) without parallel data has been successfully adapted to multi-target scenario in which a single model is trained to convert the input voice to many different speakers.However, such model suffers from the limitation that it can only convert the voice to the speakers in the training data, which narrows down the applicable scenario of VC.In this paper, we proposed a novel one-shot VC approach which is able to perform VC by only an example utterance from source and target speaker respectively, and the source and target speaker do not even need to be seen during training.This is achieved by disentangling speaker and content representations with instance normalization (IN).Objective and subjective evaluation shows that our model is able to generate the voice similar to target speaker.In addition to the performance measurement, we also demonstrate that this model is able to learn meaningful speaker representations without any supervision.","['https://openalex.org/W2887264325', 'https://openalex.org/W2603777577', 'https://openalex.org/W2518312472', 'https://openalex.org/W2608207374', 'https://openalex.org/W2910577860', 'https://openalex.org/W2972689158', 'https://openalex.org/W2892743843', 'https://openalex.org/W2794490148', 'https://openalex.org/W2502312327', 'https://openalex.org/W2396025094', 'https://openalex.org/W2962896155', 'https://openalex.org/W2885820941', 'https://openalex.org/W2907262790', 'https://openalex.org/W2547364378', 'https://openalex.org/W2507912506', 'https://openalex.org/W2808706139', 'https://openalex.org/W2127520494', 'https://openalex.org/W2651834199', 'https://openalex.org/W2902751174', 'https://openalex.org/W2805669069', 'https://openalex.org/W2476548250', 'https://openalex.org/W2527729766', 'https://openalex.org/W2666408839', 'https://openalex.org/W2000513720', 'https://openalex.org/W1959608418', 'https://openalex.org/W4295731579', 'https://openalex.org/W2972894903', 'https://openalex.org/W2963808252', 'https://openalex.org/W2963830550', 'https://openalex.org/W2086796102', 'https://openalex.org/W2057609679', 'https://openalex.org/W1509691205', 'https://openalex.org/W2963796886', 'https://openalex.org/W2774848319', 'https://openalex.org/W4320013936']",2019-09-13
https://openalex.org/W3163475957,https://doi.org/10.1109/icassp39728.2021.9414257,Again-VC: A One-Shot Voice Conversion Using Activation Guidance and Adaptive Instance Normalization,"Recently, voice conversion (VC) has been widely studied. Many VC systems use disentangle-based learning techniques to separate the speaker and the linguistic content information from a speech signal. Subsequently, they convert the voice by changing the speaker information to that of the target speaker. To prevent the speaker information from leaking into the content embeddings, previous works either reduce the dimension or quantize the content embedding as a strong information bottleneck. These mechanisms somehow hurt the synthesis quality. In this work, we propose AGAIN-VC, an innovative VC system using Activation Guidance and Adaptive Instance Normalization. AGAIN-VC is an auto-encoder-based model, comprising of a single encoder and a decoder. With a proper activation as an information bottleneck on content embeddings, the trade-off between the synthesis quality and the speaker similarity of the converted speech is improved drastically. This one-shot VC system obtains the best performance regardless of the subjective or objective evaluations.","['https://openalex.org/W2946555236', 'https://openalex.org/W6746801104', 'https://openalex.org/W2963830550', 'https://openalex.org/W6640963894', 'https://openalex.org/W6762533536', 'https://openalex.org/W3015805741', 'https://openalex.org/W6776390925', 'https://openalex.org/W2972659941', 'https://openalex.org/W3015434413', 'https://openalex.org/W2972667718', 'https://openalex.org/W6631190155', 'https://openalex.org/W2963539064', 'https://openalex.org/W2947196194', 'https://openalex.org/W3012437242', 'https://openalex.org/W2962896155', 'https://openalex.org/W2532494225', 'https://openalex.org/W2937579788', 'https://openalex.org/W2889061305', 'https://openalex.org/W2902070858', 'https://openalex.org/W3096524539', 'https://openalex.org/W2603777577', 'https://openalex.org/W2962788625', 'https://openalex.org/W6639824700', 'https://openalex.org/W2752796333', 'https://openalex.org/W6767111847', 'https://openalex.org/W6772349387', 'https://openalex.org/W1959608418', 'https://openalex.org/W2099471712', 'https://openalex.org/W2608338293', 'https://openalex.org/W3034794073', 'https://openalex.org/W2970006822', 'https://openalex.org/W1901129140', 'https://openalex.org/W4320013936', 'https://openalex.org/W2945478979', 'https://openalex.org/W2964121744', 'https://openalex.org/W2774848319', 'https://openalex.org/W2998572311', 'https://openalex.org/W3020570669', 'https://openalex.org/W2949281321', 'https://openalex.org/W3102628737', 'https://openalex.org/W4288337064', 'https://openalex.org/W1522301498', 'https://openalex.org/W2963799213', 'https://openalex.org/W2970351109']",2021-05-13
https://openalex.org/W2123003832,https://doi.org/10.1109/icassp.1998.674423,Spectral voice conversion for text-to-speech synthesis,"A new voice conversion algorithm that modifies a source speaker's speech to sound as if produced by a target speaker is presented. It is applied to a residual-excited LPC text-to-speech diphone synthesizer. Spectral parameters are mapped using a locally linear transformation based on Gaussian mixture models whose parameters are trained by joint density estimation. The LPC residuals are adjusted to match the target speakers average pitch. To study effects of the amount of training on performance, data sets of varying sizes are created by automatically selecting subsets of all available diphones by a vector quantization method. In an objective evaluation, the proposed method is found to perform more reliably for small training sets than a previous approach. In perceptual tests, it was shown that nearly optimal spectral conversion performance was achieved, even with a small amount of training data. However, speech quality improved with increases in the training set size.","['https://openalex.org/W2405069075', 'https://openalex.org/W1963778986', 'https://openalex.org/W2118850452', 'https://openalex.org/W155300571', 'https://openalex.org/W2114543868', 'https://openalex.org/W23142961', 'https://openalex.org/W2102129020', 'https://openalex.org/W95551363', 'https://openalex.org/W1596217079']",2002-11-27
https://openalex.org/W4297841435,https://doi.org/10.21437/interspeech.2022-10740,Investigation into Target Speaking Rate Adaptation for Voice Conversion,"Disentangling speaker and content attributes of a speech signal into separate latent representations followed by decoding the content with an exchanged speaker representation is a popular approach for voice conversion, which can be trained with non-parallel and unlabeled speech data.However, previous approaches perform disentanglement only implicitly via some sort of information bottleneck or normalization, where it is usually hard to find a good trade-off between voice conversion and content reconstruction.Further, previous works usually do not consider an adaptation of the speaking rate to the target speaker or they put some major restrictions to the data or use case.Therefore, the contribution of this work is two-fold.First, we employ an explicit and fully unsupervised disentanglement approach, which has previously only been used for representation learning, and show that it allows to obtain both superior voice conversion and content reconstruction.Second, we investigate simple and generic approaches to linearly adapt the length of a speech signal, and hence the speaking rate, to a target speaker and show that the proposed adaptation allows to increase the speaking rate similarity with respect to the target speaker.","['https://openalex.org/W4390912360', 'https://openalex.org/W4225892118', 'https://openalex.org/W4297808394', 'https://openalex.org/W2065722210', 'https://openalex.org/W3168292814', 'https://openalex.org/W3169739675', 'https://openalex.org/W3163475957', 'https://openalex.org/W2399934308', 'https://openalex.org/W2963830550', 'https://openalex.org/W3162517041', 'https://openalex.org/W3162390194', 'https://openalex.org/W3198082505', 'https://openalex.org/W1518481329', 'https://openalex.org/W2945478979', 'https://openalex.org/W2972659941', 'https://openalex.org/W2125001590', 'https://openalex.org/W4289299319', 'https://openalex.org/W2962780374', 'https://openalex.org/W2998572311', 'https://openalex.org/W2747874407', 'https://openalex.org/W3096524539', 'https://openalex.org/W2099621636', 'https://openalex.org/W3015338123', 'https://openalex.org/W2502312327', 'https://openalex.org/W2403729239', 'https://openalex.org/W4296068414', 'https://openalex.org/W2972667718', 'https://openalex.org/W3098557217', 'https://openalex.org/W1959608418', 'https://openalex.org/W2168510624', 'https://openalex.org/W3096656254', 'https://openalex.org/W4301371414', 'https://openalex.org/W3202871751', 'https://openalex.org/W4210633887']",2022-09-16
https://openalex.org/W4378105483,https://doi.org/10.48550/arxiv.2305.13516,"Scaling Speech Technology to 1,000+ Languages","Expanding the language coverage of speech technology has the potential to improve access to information for many more people. However, current speech technology is restricted to about one hundred languages which is a small fraction of the over 7,000 languages spoken around the world. The Massively Multilingual Speech (MMS) project increases the number of supported languages by 10-40x, depending on the task. The main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging self-supervised learning. We built pre-trained wav2vec 2.0 models covering 1,406 languages, a single multilingual automatic speech recognition model for 1,107 languages, speech synthesis models for the same number of languages, as well as a language identification model for 4,017 languages. Experiments show that our multilingual speech recognition model more than halves the word error rate of Whisper on 54 languages of the FLEURS benchmark while being trained on a small fraction of the labeled data.",[],2023-05-22
https://openalex.org/W2125101937,https://doi.org/10.1109/iccv.1998.710701,A metric for distributions with applications to image databases,"We introduce a new distance between two distributions that we call the Earth Mover's Distance (EMD), which reflects the minimal amount of work that must be performed to transform one distribution into the other by moving ""distribution mass"" around. This is a special case of the transportation problem from linear optimization, for which efficient algorithms are available. The EMD also allows for partial matching. When used to compare distributions that have the same overall mass, the EMD is a true metric, and has easy-to-compute lower bounds. In this paper we focus on applications to image databases, especially color and texture. We use the EMD to exhibit the structure of color-distribution and texture spaces by means of Multi-Dimensional Scaling displays. We also propose a novel approach to the problem of navigating through a collection of color images, which leads to a new paradigm for image database search.","['https://openalex.org/W2125994026', 'https://openalex.org/W2018304568', 'https://openalex.org/W2169371330', 'https://openalex.org/W6642652525', 'https://openalex.org/W2914885528', 'https://openalex.org/W1988445395', 'https://openalex.org/W2059975159', 'https://openalex.org/W6670356954', 'https://openalex.org/W6682747227', 'https://openalex.org/W2138584058', 'https://openalex.org/W2152825437', 'https://openalex.org/W4405139089', 'https://openalex.org/W2125148312', 'https://openalex.org/W2131339961', 'https://openalex.org/W1608339372', 'https://openalex.org/W2568842486', 'https://openalex.org/W1969294188', 'https://openalex.org/W2093191240', 'https://openalex.org/W2079082863', 'https://openalex.org/W2322316662', 'https://openalex.org/W1526351017', 'https://openalex.org/W3117722945', 'https://openalex.org/W2973818247', 'https://openalex.org/W3113130260']",2002-11-27
https://openalex.org/W3167533889,https://doi.org/10.48550/arxiv.2106.04624,SpeechBrain: A General-Purpose Speech Toolkit,"SpeechBrain is an open-source and all-in-one speech toolkit. It is designed to facilitate the research and development of neural speech processing technologies by being simple, flexible, user-friendly, and well-documented. This paper describes the core architecture designed to support several tasks of common interest, allowing users to naturally conceive, compare and share novel speech processing pipelines. SpeechBrain achieves competitive or state-of-the-art performance in a wide range of speech benchmarks. It also provides training recipes, pretrained models, and inference scripts for popular speech datasets, as well as tutorials which allow anyone with basic Python proficiency to familiarize themselves with speech technologies.","['https://openalex.org/W2952509486', 'https://openalex.org/W2883416004', 'https://openalex.org/W2952218014', 'https://openalex.org/W2964265128', 'https://openalex.org/W3006436762', 'https://openalex.org/W1494198834', 'https://openalex.org/W2972818416', 'https://openalex.org/W3143367551', 'https://openalex.org/W2973215447', 'https://openalex.org/W3030437843', 'https://openalex.org/W2963403868', 'https://openalex.org/W2757519008', 'https://openalex.org/W2517616541', 'https://openalex.org/W3098824823', 'https://openalex.org/W2008750120', 'https://openalex.org/W2995181338', 'https://openalex.org/W3097906045', 'https://openalex.org/W2963242190', 'https://openalex.org/W2398042854', 'https://openalex.org/W1922655562', 'https://openalex.org/W1924770834', 'https://openalex.org/W3015783745', 'https://openalex.org/W2064675550', 'https://openalex.org/W2125336414', 'https://openalex.org/W2144404214', 'https://openalex.org/W3043471679', 'https://openalex.org/W2395750323', 'https://openalex.org/W88081813', 'https://openalex.org/W3008912312', 'https://openalex.org/W2903420265', 'https://openalex.org/W1598508708', 'https://openalex.org/W3015537910', 'https://openalex.org/W2766219058', 'https://openalex.org/W1828163288', 'https://openalex.org/W2221409856', 'https://openalex.org/W3100460087', 'https://openalex.org/W2890964092', 'https://openalex.org/W2963235055', 'https://openalex.org/W2748584437', 'https://openalex.org/W2056786202', 'https://openalex.org/W3119308075', 'https://openalex.org/W3141854550', 'https://openalex.org/W3093502935', 'https://openalex.org/W2970971581', 'https://openalex.org/W2962998773', 'https://openalex.org/W2972584841', 'https://openalex.org/W2064364374', 'https://openalex.org/W3157923770', 'https://openalex.org/W3097945073', 'https://openalex.org/W3027008958', 'https://openalex.org/W2972541922', 'https://openalex.org/W2150678643', 'https://openalex.org/W2156885227', 'https://openalex.org/W2962780374', 'https://openalex.org/W2113416157', 'https://openalex.org/W2101234009', 'https://openalex.org/W2974231335', 'https://openalex.org/W2034591763', 'https://openalex.org/W3103145119', 'https://openalex.org/W2894164357', 'https://openalex.org/W3155162503', 'https://openalex.org/W3095940219', 'https://openalex.org/W2974321942', 'https://openalex.org/W2928941594', 'https://openalex.org/W2936774411', 'https://openalex.org/W3015199127', 'https://openalex.org/W3106426924', 'https://openalex.org/W2799923439', 'https://openalex.org/W2981461916', 'https://openalex.org/W3190342989', 'https://openalex.org/W2964308564', 'https://openalex.org/W3095717210', 'https://openalex.org/W3015191643', 'https://openalex.org/W2099471712', 'https://openalex.org/W2296167893', 'https://openalex.org/W2587210085', 'https://openalex.org/W1524333225', 'https://openalex.org/W2962913459', 'https://openalex.org/W2113638573', 'https://openalex.org/W2808631503', 'https://openalex.org/W2046932483', 'https://openalex.org/W2964227577', 'https://openalex.org/W3099782249', 'https://openalex.org/W2794209590', 'https://openalex.org/W2407024733', 'https://openalex.org/W2963288440', 'https://openalex.org/W2132914434', 'https://openalex.org/W3021469861', 'https://openalex.org/W3115133930', 'https://openalex.org/W3151851237', 'https://openalex.org/W1552314771', 'https://openalex.org/W2905489173', 'https://openalex.org/W2127851351', 'https://openalex.org/W2127141656', 'https://openalex.org/W2117671523', 'https://openalex.org/W2633911929', 'https://openalex.org/W2726515241', 'https://openalex.org/W2094721231', 'https://openalex.org/W2755891984', 'https://openalex.org/W1983108229', 'https://openalex.org/W2963250244', 'https://openalex.org/W3141443761', 'https://openalex.org/W3163652268', 'https://openalex.org/W3101648800', 'https://openalex.org/W2046317813', 'https://openalex.org/W3024869864', 'https://openalex.org/W2408713104']",2021-06-08
https://openalex.org/W4306169273,https://doi.org/10.48550/arxiv.2104.00931,Assem-VC: Realistic Voice Conversion by Assembling Modern Speech Synthesis Techniques,"Recent works on voice conversion (VC) focus on preserving the rhythm and the intonation as well as the linguistic content. To preserve these features from the source, we decompose current non-parallel VC systems into two encoders and one decoder. We analyze each module with several experiments and reassemble the best components to propose Assem-VC, a new state-of-the-art any-to-many non-parallel VC system. We also examine that PPG and Cotatron features are speaker-dependent, and attempt to remove speaker identity with adversarial training. Code and audio samples are available at https://github.com/mindslab-ai/assem-vc.",[],2021-04-02
https://openalex.org/W4375868850,https://doi.org/10.1109/icassp49357.2023.10095582,Lip-to-Speech Synthesis in the Wild with Multi-Task Learning,"Recent studies have shown impressive performance in Lip-to-speech synthesis that aims to reconstruct speech from visual information alone. However, they have been suffering from synthesizing accurate speech in the wild, due to insufficient supervision for guiding the model to infer the correct content. Distinct from the previous methods, in this paper, we develop a powerful Lip2Speech method that can reconstruct speech with correct contents from the input lip movements, even in a wild environment. To this end, we design multitask learning that guides the model using multimodal supervision, i.e. text and audio, to complement the insufficient word representations of acoustic feature reconstruction loss. Thus, the proposed framework brings the advantage of synthesizing speech containing the right content of multiple speakers with unconstrained sentences. We verify the effectiveness of the proposed method using LRS2, LRS3, and LRW datasets.","['https://openalex.org/W6754420807', 'https://openalex.org/W2551572271', 'https://openalex.org/W2964352155', 'https://openalex.org/W2127141656', 'https://openalex.org/W6734491695', 'https://openalex.org/W4224319127', 'https://openalex.org/W2972563022', 'https://openalex.org/W3205193540', 'https://openalex.org/W3213322812', 'https://openalex.org/W3035626590', 'https://openalex.org/W2752796333', 'https://openalex.org/W6810661184', 'https://openalex.org/W2067295501', 'https://openalex.org/W6757817989', 'https://openalex.org/W1552314771', 'https://openalex.org/W2516001803', 'https://openalex.org/W3097777922', 'https://openalex.org/W2963654155', 'https://openalex.org/W2963250244', 'https://openalex.org/W2120847449', 'https://openalex.org/W6754392867', 'https://openalex.org/W2029199293', 'https://openalex.org/W3162293946', 'https://openalex.org/W6803359641', 'https://openalex.org/W3096650361', 'https://openalex.org/W2015143272', 'https://openalex.org/W6794378236', 'https://openalex.org/W2908510526', 'https://openalex.org/W2890952074', 'https://openalex.org/W3157840621', 'https://openalex.org/W3211862173', 'https://openalex.org/W2891205112', 'https://openalex.org/W2963799213', 'https://openalex.org/W4296069328']",2023-05-05
https://openalex.org/W4375868953,https://doi.org/10.1109/icassp49357.2023.10097097,Analysing Discrete Self Supervised Speech Representation For Spoken Language Modeling,"This work profoundly analyzes discrete self-supervised speech representations (units) through the eyes of Generative Spoken Language Modeling (GSLM). Following the findings of such an analysis, we propose practical improvements to the discrete unit for the GSLM. First, we start comprehending these units by analyzing them in three axes: interpretation, visualization, and resynthesis. Our analysis finds a high correlation between the speech units to phonemes and phoneme families, while their correlation with speaker or gender is weaker. Additionally, we found redundancies in the extracted units and claim that one reason may be the units' context. Following this analysis, we propose a new, unsupervised metric to measure unit redundancies. Finally, we use this metric to develop new methods that improve the robustness of units' clustering and show significant improvement considering zero-resource speech metrics such as ABX. Code and analysis tools are available under the following link: https://github.com/slp-rl/SLM-Discrete-Representations","['https://openalex.org/W4287887366', 'https://openalex.org/W4225726571', 'https://openalex.org/W2964243274', 'https://openalex.org/W1967005434', 'https://openalex.org/W4297841405', 'https://openalex.org/W2995181338', 'https://openalex.org/W3140429000', 'https://openalex.org/W1494198834', 'https://openalex.org/W3209059054', 'https://openalex.org/W3197580070', 'https://openalex.org/W6680970901', 'https://openalex.org/W2963300588', 'https://openalex.org/W6789826613', 'https://openalex.org/W6810531757', 'https://openalex.org/W6790356757', 'https://openalex.org/W6843330092', 'https://openalex.org/W3016181583', 'https://openalex.org/W6780218876', 'https://openalex.org/W3096656254', 'https://openalex.org/W4297841865', 'https://openalex.org/W4394671563', 'https://openalex.org/W4307680525', 'https://openalex.org/W2187089797', 'https://openalex.org/W4381786045', 'https://openalex.org/W2138615112', 'https://openalex.org/W3127686677', 'https://openalex.org/W3036601975']",2023-05-05
https://openalex.org/W4390874021,https://doi.org/10.1109/iccv51070.2023.01409,Lip Reading for Low-resource Languages by Learning and Combining General Speech Knowledge and Language-specific Knowledge,"This paper proposes a novel lip reading framework, especially for low-resource languages, which has not been well addressed in the previous literature. Since low-resource languages do not have enough video-text paired data to train the model to have sufficient power to model lip movements and language, it is regarded as challenging to develop lip reading models for low-resource languages. In order to mitigate the challenge, we try to learn general speech knowledge, the ability to model lip movements, from a high-resource language through the prediction of speech units. It is known that different languages partially share common phonemes, thus general speech knowledge learned from one language can be extended to other languages. Then, we try to learn language-specific knowledge, the ability to model language, by proposing Language-specific Memory-augmented Decoder (LMDecoder). LMDecoder saves language-specific audio features into memory banks and can be trained on audio-text paired data which is more easily accessible than video-text paired data. Therefore, with LMDecoder, we can transform the input speech units into language-specific audio features and translate them into texts by utilizing the learned rich language knowledge. Finally, by combining general speech knowledge and language-specific knowledge, we can efficiently develop lip reading models even for low-resource languages. Through extensive experiments using five languages, English, Spanish, French, Italian, and Portuguese, the effectiveness of the proposed method is evaluated.","['https://openalex.org/W2981501041', 'https://openalex.org/W6810168380', 'https://openalex.org/W6754420807', 'https://openalex.org/W2594690981', 'https://openalex.org/W2897492880', 'https://openalex.org/W2551572271', 'https://openalex.org/W2999528291', 'https://openalex.org/W6732872814', 'https://openalex.org/W2963528589', 'https://openalex.org/W2963654155', 'https://openalex.org/W2890952074', 'https://openalex.org/W3127645834', 'https://openalex.org/W6803359641', 'https://openalex.org/W3162293946', 'https://openalex.org/W2604379605', 'https://openalex.org/W2996970093', 'https://openalex.org/W3197567540', 'https://openalex.org/W3205193540', 'https://openalex.org/W3203789665', 'https://openalex.org/W3167917117', 'https://openalex.org/W4224319127', 'https://openalex.org/W6785851975', 'https://openalex.org/W3162707322', 'https://openalex.org/W4307286264', 'https://openalex.org/W3197771105', 'https://openalex.org/W2033436836', 'https://openalex.org/W2106440210', 'https://openalex.org/W6776472420', 'https://openalex.org/W6755207826', 'https://openalex.org/W6763701032', 'https://openalex.org/W6769627184', 'https://openalex.org/W3127334095', 'https://openalex.org/W3128564814', 'https://openalex.org/W3213322812', 'https://openalex.org/W4375868850', 'https://openalex.org/W4385823403', 'https://openalex.org/W4372346152', 'https://openalex.org/W2194775991', 'https://openalex.org/W2064675550', 'https://openalex.org/W6640212811', 'https://openalex.org/W6763296340', 'https://openalex.org/W3016011581', 'https://openalex.org/W108866686', 'https://openalex.org/W6739901393', 'https://openalex.org/W4297841411', 'https://openalex.org/W4312638101', 'https://openalex.org/W4386071467', 'https://openalex.org/W3015830103', 'https://openalex.org/W6638523607', 'https://openalex.org/W4372260478', 'https://openalex.org/W4390691978', 'https://openalex.org/W2398406965', 'https://openalex.org/W4312975593', 'https://openalex.org/W6849622896', 'https://openalex.org/W6767279747', 'https://openalex.org/W3090449556', 'https://openalex.org/W2997591391', 'https://openalex.org/W3049478012', 'https://openalex.org/W4312784228', 'https://openalex.org/W2981851019', 'https://openalex.org/W6791353385', 'https://openalex.org/W6780218876', 'https://openalex.org/W3209059054', 'https://openalex.org/W2752796333', 'https://openalex.org/W3180355996', 'https://openalex.org/W4312388283', 'https://openalex.org/W6790356757', 'https://openalex.org/W6769196770', 'https://openalex.org/W3140429000', 'https://openalex.org/W4385822683', 'https://openalex.org/W6855650468', 'https://openalex.org/W4297841641', 'https://openalex.org/W6679436768', 'https://openalex.org/W3119308075', 'https://openalex.org/W2933138175', 'https://openalex.org/W2962766044', 'https://openalex.org/W2913720370', 'https://openalex.org/W2963250244', 'https://openalex.org/W3095410713', 'https://openalex.org/W2766219058', 'https://openalex.org/W6631190155', 'https://openalex.org/W2808631503', 'https://openalex.org/W4376481237']",2023-10-01
https://openalex.org/W4319862477,https://doi.org/10.1109/slt54892.2023.10022954,SpeechCLIP: Integrating Speech with Pre-Trained Vision and Language Model,"Data-driven speech processing models usually perform well with a large amount of text supervision, but collecting transcribed speech data is costly. Therefore, we propose Speech-CLIP, a novel framework bridging speech and text through images to enhance speech models without transcriptions. We leverage state-of-the-art pre-trained HuBERT and CLIP, aligning them via paired images and spoken captions with minimal fine-tuning. SpeechCLIP outperforms prior state-of-the-art on image-speech retrieval and performs zero-shot speech-text retrieval without direct supervision from transcriptions. Moreover, SpeechCLIP can directly retrieve semantically related keywords from speech.","['https://openalex.org/W4281492411', 'https://openalex.org/W2982223350', 'https://openalex.org/W3041561163', 'https://openalex.org/W2972943112', 'https://openalex.org/W3097286738', 'https://openalex.org/W3198858531', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W6780218876', 'https://openalex.org/W4226033575', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W6810673746', 'https://openalex.org/W3015213852', 'https://openalex.org/W3203140070', 'https://openalex.org/W4224821750', 'https://openalex.org/W3197580070', 'https://openalex.org/W3189296823', 'https://openalex.org/W4285250921', 'https://openalex.org/W2963902314', 'https://openalex.org/W6791353385', 'https://openalex.org/W3157861865', 'https://openalex.org/W2972892814', 'https://openalex.org/W2962862718', 'https://openalex.org/W3174311593', 'https://openalex.org/W3161204797', 'https://openalex.org/W3197467690', 'https://openalex.org/W2963330681', 'https://openalex.org/W2964099072', 'https://openalex.org/W3094626712', 'https://openalex.org/W3015300171', 'https://openalex.org/W2988907666', 'https://openalex.org/W3196698946', 'https://openalex.org/W3200287550', 'https://openalex.org/W4224875474', 'https://openalex.org/W4226380987', 'https://openalex.org/W6795952400', 'https://openalex.org/W4319862670', 'https://openalex.org/W4284898017', 'https://openalex.org/W3176445421', 'https://openalex.org/W6739901393', 'https://openalex.org/W1905882502', 'https://openalex.org/W6803675045', 'https://openalex.org/W4286359908', 'https://openalex.org/W6756706075', 'https://openalex.org/W2973135958', 'https://openalex.org/W4285110637', 'https://openalex.org/W4297808394', 'https://openalex.org/W4221145109', 'https://openalex.org/W2242818861', 'https://openalex.org/W4221161768']",2023-01-09
https://openalex.org/W4386071467,https://doi.org/10.1109/cvpr52729.2023.01801,Watch or Listen: Robust Audio-Visual Speech Recognition with Visual Corruption Modeling and Reliability Scoring,"This paper deals with Audio-Visual Speech Recognition (AVSR) under multimodal input corruption situations where audio inputs and visual inputs are both corrupted, which is not well addressed in previous research directions. Previous studies have focused on how to complement the corrupted audio inputs with the clean visual inputs with the assumption of the availability of clean visual inputs. However, in real life, clean visual inputs are not always accessible and can even be corrupted by occluded lip regions or noises. Thus, we firstly analyze that the previous AVSR models are not indeed robust to the corruption of multimodal input streams, the audio and the visual inputs, compared to uni-modal models. Then, we design multimodal input corruption modeling to develop robust AVSR models. Lastly, we propose a novel AVSR framework, namely Audio-Visual Reliability Scoring module (AV-RelScore), that is robust to the corrupted multimodal inputs. The AV-RelScore can determine which input modal stream is reliable or not for the prediction and also can exploit the more reliable streams in prediction. The effectiveness of the proposed method is evaluated with comprehensive experiments on popular benchmark databases, LRS2 and LRS3. We also show that the reliability scores obtained by AV-RelScore well reflect the degree of corruption and make the proposed model focus on the reliable multimodal representations.","['https://openalex.org/W6687566353', 'https://openalex.org/W6732872814', 'https://openalex.org/W1897240248', 'https://openalex.org/W2991361823', 'https://openalex.org/W3032514799', 'https://openalex.org/W3160129476', 'https://openalex.org/W2076462394', 'https://openalex.org/W6754392867', 'https://openalex.org/W6739901393', 'https://openalex.org/W2963654155', 'https://openalex.org/W3097777922', 'https://openalex.org/W4297841411', 'https://openalex.org/W6767349417', 'https://openalex.org/W3127645834', 'https://openalex.org/W3127334095', 'https://openalex.org/W3128564814', 'https://openalex.org/W3203789665', 'https://openalex.org/W4375868850', 'https://openalex.org/W3213322812', 'https://openalex.org/W4312975157', 'https://openalex.org/W6780218876', 'https://openalex.org/W4224319127', 'https://openalex.org/W3162293946', 'https://openalex.org/W2551572271', 'https://openalex.org/W6754420807', 'https://openalex.org/W1993882792', 'https://openalex.org/W2160815625', 'https://openalex.org/W2147768505', 'https://openalex.org/W2394932179', 'https://openalex.org/W2155273149', 'https://openalex.org/W1995562189', 'https://openalex.org/W2962719052', 'https://openalex.org/W2143612262', 'https://openalex.org/W6675365184', 'https://openalex.org/W6629930100', 'https://openalex.org/W2127141656', 'https://openalex.org/W6679436768', 'https://openalex.org/W6640090968', 'https://openalex.org/W2973049979', 'https://openalex.org/W6769196770', 'https://openalex.org/W3205193540', 'https://openalex.org/W3016011581', 'https://openalex.org/W2981501041', 'https://openalex.org/W2996970093', 'https://openalex.org/W4307286264', 'https://openalex.org/W4312975593', 'https://openalex.org/W6849622896', 'https://openalex.org/W4312638101', 'https://openalex.org/W2022799064', 'https://openalex.org/W1503933356', 'https://openalex.org/W2035777533', 'https://openalex.org/W3015383493', 'https://openalex.org/W2963785710', 'https://openalex.org/W3035042697', 'https://openalex.org/W3097711533', 'https://openalex.org/W3117782667', 'https://openalex.org/W2904483377', 'https://openalex.org/W3087907820', 'https://openalex.org/W2902598059', 'https://openalex.org/W4292793901', 'https://openalex.org/W3034973071', 'https://openalex.org/W1974387177', 'https://openalex.org/W4232282348', 'https://openalex.org/W2407080277', 'https://openalex.org/W3090449556', 'https://openalex.org/W2526425061', 'https://openalex.org/W2194775991', 'https://openalex.org/W2594690981', 'https://openalex.org/W2296073425', 'https://openalex.org/W6631190155', 'https://openalex.org/W1494198834', 'https://openalex.org/W2799473636', 'https://openalex.org/W6771467084', 'https://openalex.org/W3006974783', 'https://openalex.org/W4212774754', 'https://openalex.org/W1922655562', 'https://openalex.org/W2891205112', 'https://openalex.org/W2890952074', 'https://openalex.org/W3030437843', 'https://openalex.org/W1499864241', 'https://openalex.org/W2130942839', 'https://openalex.org/W2970270554', 'https://openalex.org/W4385245566', 'https://openalex.org/W3101648800', 'https://openalex.org/W4403635980', 'https://openalex.org/W4298112588', 'https://openalex.org/W1522301498', 'https://openalex.org/W2979476256', 'https://openalex.org/W3036601975']",2023-06-01
https://openalex.org/W68733909,https://doi.org/10.1613/jair.3994,"Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics","The ability to associate images with natural language sentences that describe what is depicted in them is a hallmark of image understanding, and a prerequisite for applications such as sentence-based image search. In analogy to image search, we propose to frame sentence-based image annotation as the task of ranking a given pool of captions. We introduce a new benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. We introduce a number of systems that perform quite well on this task, even though they are only based on features that can be obtained with minimal supervision. Our results clearly indicate the importance of training on multiple captions per image, and of capturing syntactic (word order-based) and semantic features of these captions. We also perform an in-depth comparison of human and automatic evaluation metrics for this task, and propose strategies for collecting human judgments cheaply and on a very large scale, allowing us to augment our collection with additional relevance judgments of which captions describe which image. Our analysis shows that metrics that consider the ranked list of results for each query image or sentence are significantly more robust than metrics that are based on a single response per query. Moreover, our study suggests that the evaluation of ranking-based image description systems may be fully automated.","['https://openalex.org/W2141766660', 'https://openalex.org/W6634941687', 'https://openalex.org/W7011438494', 'https://openalex.org/W6655670753', 'https://openalex.org/W2109710679', 'https://openalex.org/W2006969979', 'https://openalex.org/W6663855623', 'https://openalex.org/W151785663', 'https://openalex.org/W6680919558', 'https://openalex.org/W2167277498', 'https://openalex.org/W1897761818', 'https://openalex.org/W7016219665', 'https://openalex.org/W2120419212', 'https://openalex.org/W2167693926', 'https://openalex.org/W2126384256', 'https://openalex.org/W6630696253', 'https://openalex.org/W2162867699', 'https://openalex.org/W6693199996', 'https://openalex.org/W114341944', 'https://openalex.org/W956551720', 'https://openalex.org/W2168186441', 'https://openalex.org/W2100235303', 'https://openalex.org/W6656652561', 'https://openalex.org/W2029163572', 'https://openalex.org/W6600312833', 'https://openalex.org/W3021916629', 'https://openalex.org/W1969616664', 'https://openalex.org/W2149172860', 'https://openalex.org/W6678852649', 'https://openalex.org/W6637611282', 'https://openalex.org/W1687846465', 'https://openalex.org/W6682631176', 'https://openalex.org/W2150824314', 'https://openalex.org/W6636975626', 'https://openalex.org/W2151103935', 'https://openalex.org/W6646194391', 'https://openalex.org/W6631834165', 'https://openalex.org/W8316075', 'https://openalex.org/W1977531404', 'https://openalex.org/W6647036627', 'https://openalex.org/W2156985047', 'https://openalex.org/W2109586012', 'https://openalex.org/W6898505805', 'https://openalex.org/W6713705112', 'https://openalex.org/W6674809819', 'https://openalex.org/W2106277773', 'https://openalex.org/W1982897610', 'https://openalex.org/W2142468277', 'https://openalex.org/W6630630962', 'https://openalex.org/W1990190154', 'https://openalex.org/W2062955551', 'https://openalex.org/W1545305210', 'https://openalex.org/W2108082645', 'https://openalex.org/W2102765684', 'https://openalex.org/W6639118148', 'https://openalex.org/W4239269065', 'https://openalex.org/W2962835968', 'https://openalex.org/W3000226596', 'https://openalex.org/W1983988843', 'https://openalex.org/W92662927', 'https://openalex.org/W2066941820', 'https://openalex.org/W1515242458', 'https://openalex.org/W4248892431', 'https://openalex.org/W4285719527', 'https://openalex.org/W2181691731', 'https://openalex.org/W1510073064', 'https://openalex.org/W1489525520', 'https://openalex.org/W182831726', 'https://openalex.org/W2264742718', 'https://openalex.org/W2101105183', 'https://openalex.org/W1995689975', 'https://openalex.org/W2025341678', 'https://openalex.org/W2128856065', 'https://openalex.org/W2154652894', 'https://openalex.org/W1711703781', 'https://openalex.org/W2053154970', 'https://openalex.org/W2141000440', 'https://openalex.org/W2962706528', 'https://openalex.org/W1647729745', 'https://openalex.org/W2951805548', 'https://openalex.org/W1588424744', 'https://openalex.org/W1987486061', 'https://openalex.org/W2127411609', 'https://openalex.org/W2111993661', 'https://openalex.org/W2144600658', 'https://openalex.org/W1895577753', 'https://openalex.org/W1858383477', 'https://openalex.org/W2110485445', 'https://openalex.org/W2066134726', 'https://openalex.org/W2153222072', 'https://openalex.org/W1532325895', 'https://openalex.org/W2185175083', 'https://openalex.org/W2405893959', 'https://openalex.org/W2119775030', 'https://openalex.org/W2041797434', 'https://openalex.org/W4237723258', 'https://openalex.org/W1527575280', 'https://openalex.org/W1861492603', 'https://openalex.org/W2167090521', 'https://openalex.org/W2098162425', 'https://openalex.org/W2038721957', 'https://openalex.org/W2020842694', 'https://openalex.org/W2133459682']",2013-08-30
https://openalex.org/W4390871839,https://doi.org/10.1109/iccv51070.2023.01582,SeiT: Storage-Efficient Vision Training with Tokens Using 1% of Pixel Storage,"We need billion-scale images to achieve more generalizable and ground-breaking vision models, as well as massive dataset storage to ship the images (e.g., the LAION-5B dataset needs 240TB storage space). However, it has become challenging to deal with unlimited dataset storage with limited storage infrastructure. A number of storage-efficient training methods have been proposed to tackle the problem, but they are rarely scalable or suffer from severe damage to performance. In this paper, we propose a storage-efficient training strategy for vision classifiers for large-scale datasets (e.g., ImageNet) that only uses 1024 tokens per instance without using the raw level pixels; our token storage only needs <1% of the original JPEG-compressed raw pixels. We also propose token augmentations and a Stem-adaptor module to make our approach able to use the same architecture as pixel-based approaches with only minimal modifications on the stem layer and the carefully tuned optimization settings. Our experimental results on ImageNet-1k show that our method significantly outperforms other storage-efficient training methods with a large gap. We further show the effectiveness of our method in other practical scenarios, storage-efficient pre-training, and continual learning. Code is available at https://github.com/naver-ai/seit.","['https://openalex.org/W6728047685', 'https://openalex.org/W2745461083', 'https://openalex.org/W1933349210', 'https://openalex.org/W6696118455', 'https://openalex.org/W6748475379', 'https://openalex.org/W6779676466', 'https://openalex.org/W2552465432', 'https://openalex.org/W2785562966', 'https://openalex.org/W6690026940', 'https://openalex.org/W2918626955', 'https://openalex.org/W6778883912', 'https://openalex.org/W3034469748', 'https://openalex.org/W6775007330', 'https://openalex.org/W6765715182', 'https://openalex.org/W2099111195', 'https://openalex.org/W6774469542', 'https://openalex.org/W3035682985', 'https://openalex.org/W2161969291', 'https://openalex.org/W6784333009', 'https://openalex.org/W3180355996', 'https://openalex.org/W6756444276', 'https://openalex.org/W6640425456', 'https://openalex.org/W2560730294', 'https://openalex.org/W2194775991', 'https://openalex.org/W3037492894', 'https://openalex.org/W2797977484', 'https://openalex.org/W2060108852', 'https://openalex.org/W6729448088', 'https://openalex.org/W6790019176', 'https://openalex.org/W6778732436', 'https://openalex.org/W4226355936', 'https://openalex.org/W6752083267', 'https://openalex.org/W6789753369', 'https://openalex.org/W2138011018', 'https://openalex.org/W4288083516', 'https://openalex.org/W6811426508', 'https://openalex.org/W4387415233', 'https://openalex.org/W6719057275', 'https://openalex.org/W6730091202', 'https://openalex.org/W6739868092', 'https://openalex.org/W6751037545', 'https://openalex.org/W2912317488', 'https://openalex.org/W6779606177', 'https://openalex.org/W2533598788', 'https://openalex.org/W6798413682', 'https://openalex.org/W6845452229', 'https://openalex.org/W6791353385', 'https://openalex.org/W6620707391', 'https://openalex.org/W6756754374', 'https://openalex.org/W4312933868', 'https://openalex.org/W3142096533', 'https://openalex.org/W2117539524', 'https://openalex.org/W4312292108', 'https://openalex.org/W6846007759', 'https://openalex.org/W6802987763', 'https://openalex.org/W6802138914', 'https://openalex.org/W4312777209', 'https://openalex.org/W4246193833', 'https://openalex.org/W6780006332', 'https://openalex.org/W2964345214', 'https://openalex.org/W3108655343', 'https://openalex.org/W6788135285', 'https://openalex.org/W4312453657', 'https://openalex.org/W2752796333', 'https://openalex.org/W6739901393', 'https://openalex.org/W6763468762', 'https://openalex.org/W6756054015', 'https://openalex.org/W2971296908', 'https://openalex.org/W6802517614', 'https://openalex.org/W2992308087', 'https://openalex.org/W3168547821', 'https://openalex.org/W6679815717', 'https://openalex.org/W3015001695', 'https://openalex.org/W6790536367', 'https://openalex.org/W6779630500', 'https://openalex.org/W398859631', 'https://openalex.org/W569478347', 'https://openalex.org/W4287077733', 'https://openalex.org/W4292779060', 'https://openalex.org/W2947707615', 'https://openalex.org/W2736618577', 'https://openalex.org/W2242818861', 'https://openalex.org/W2463565445', 'https://openalex.org/W4301914798', 'https://openalex.org/W2963521239', 'https://openalex.org/W2286454943', 'https://openalex.org/W2954732411', 'https://openalex.org/W639708223', 'https://openalex.org/W2604258491', 'https://openalex.org/W3206395542', 'https://openalex.org/W2962676454', 'https://openalex.org/W4293846201', 'https://openalex.org/W2964098744', 'https://openalex.org/W1945616565', 'https://openalex.org/W3099904761', 'https://openalex.org/W2963799213', 'https://openalex.org/W3094502228', 'https://openalex.org/W4385245566', 'https://openalex.org/W2524365899', 'https://openalex.org/W3040002795', 'https://openalex.org/W2902456977']",2023-10-01
https://openalex.org/W1905882502,https://doi.org/10.1109/cvpr.2015.7298932,Deep visual-semantic alignments for generating image descriptions,"We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.","['https://openalex.org/W6640071036', 'https://openalex.org/W6683512859', 'https://openalex.org/W2147625498', 'https://openalex.org/W2112796928', 'https://openalex.org/W2296385829', 'https://openalex.org/W6682086108', 'https://openalex.org/W6639102338', 'https://openalex.org/W2086842362', 'https://openalex.org/W6637306801', 'https://openalex.org/W2106624428', 'https://openalex.org/W2125436662', 'https://openalex.org/W6684191040', 'https://openalex.org/W2048343491', 'https://openalex.org/W2066134726', 'https://openalex.org/W7011438494', 'https://openalex.org/W6683167905', 'https://openalex.org/W2064675550', 'https://openalex.org/W2250539671', 'https://openalex.org/W68733909', 'https://openalex.org/W6676647902', 'https://openalex.org/W6677533783', 'https://openalex.org/W6685230081', 'https://openalex.org/W6631516269', 'https://openalex.org/W6607974698', 'https://openalex.org/W6674914833', 'https://openalex.org/W6635446068', 'https://openalex.org/W2185175083', 'https://openalex.org/W2251583212', 'https://openalex.org/W1987835821', 'https://openalex.org/W6639118148', 'https://openalex.org/W6639657675', 'https://openalex.org/W6641064462', 'https://openalex.org/W4254816979', 'https://openalex.org/W2031489346', 'https://openalex.org/W179875071', 'https://openalex.org/W6640169532', 'https://openalex.org/W6639694449', 'https://openalex.org/W2056621158', 'https://openalex.org/W2149536503', 'https://openalex.org/W6678470764', 'https://openalex.org/W2102605133', 'https://openalex.org/W2536208356', 'https://openalex.org/W62621907', 'https://openalex.org/W6639432524', 'https://openalex.org/W100623710', 'https://openalex.org/W6676297131', 'https://openalex.org/W6677969093', 'https://openalex.org/W6640617836', 'https://openalex.org/W2133459682', 'https://openalex.org/W2149557440', 'https://openalex.org/W6681184217', 'https://openalex.org/W2131774270', 'https://openalex.org/W6677651945', 'https://openalex.org/W2062955551', 'https://openalex.org/W6637373629', 'https://openalex.org/W6682691769', 'https://openalex.org/W6898505805', 'https://openalex.org/W6676497082', 'https://openalex.org/W4285719527', 'https://openalex.org/W1923162067', 'https://openalex.org/W2112912048', 'https://openalex.org/W2117539524', 'https://openalex.org/W2122180654', 'https://openalex.org/W2171361956', 'https://openalex.org/W4239072543', 'https://openalex.org/W2159243025', 'https://openalex.org/W1527575280', 'https://openalex.org/W2110485445', 'https://openalex.org/W1686810756', 'https://openalex.org/W4294170691', 'https://openalex.org/W2097117768', 'https://openalex.org/W1931639407', 'https://openalex.org/W1947481528', 'https://openalex.org/W1687846465', 'https://openalex.org/W2181691731', 'https://openalex.org/W2108598243', 'https://openalex.org/W2143449221', 'https://openalex.org/W1895577753', 'https://openalex.org/W2153579005', 'https://openalex.org/W2101105183', 'https://openalex.org/W2115752676', 'https://openalex.org/W1889081078', 'https://openalex.org/W1897761818', 'https://openalex.org/W2123024445', 'https://openalex.org/W2149172860', 'https://openalex.org/W196214544', 'https://openalex.org/W2963811219', 'https://openalex.org/W1956340063', 'https://openalex.org/W2109586012', 'https://openalex.org/W8316075', 'https://openalex.org/W1861492603', 'https://openalex.org/W2163605009', 'https://openalex.org/W1591801644', 'https://openalex.org/W1858383477', 'https://openalex.org/W2952574180']",2015-06-01
https://openalex.org/W2886641317,https://doi.org/10.18653/v1/p18-1238,"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning","We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.","['https://openalex.org/W1956340063', 'https://openalex.org/W1978924650', 'https://openalex.org/W2274287116', 'https://openalex.org/W68733909', 'https://openalex.org/W2064675550', 'https://openalex.org/W4231109964', 'https://openalex.org/W2133564696', 'https://openalex.org/W2163605009', 'https://openalex.org/W1947481528', 'https://openalex.org/W4385245566', 'https://openalex.org/W1514535095', 'https://openalex.org/W2964308564', 'https://openalex.org/W2176263492', 'https://openalex.org/W1527575280', 'https://openalex.org/W2072128103', 'https://openalex.org/W1895577753', 'https://openalex.org/W2130942839', 'https://openalex.org/W1861492603', 'https://openalex.org/W2506483933', 'https://openalex.org/W2108598243', 'https://openalex.org/W2108325777', 'https://openalex.org/W2963673305', 'https://openalex.org/W2185175083', 'https://openalex.org/W2953390309', 'https://openalex.org/W1924770834', 'https://openalex.org/W1931639407', 'https://openalex.org/W2282219577', 'https://openalex.org/W2964350391', 'https://openalex.org/W2963403868', 'https://openalex.org/W2481240925', 'https://openalex.org/W2146502635', 'https://openalex.org/W2951597448', 'https://openalex.org/W2551915941', 'https://openalex.org/W2963248296', 'https://openalex.org/W2557728737', 'https://openalex.org/W2123301721', 'https://openalex.org/W2558533273', 'https://openalex.org/W2964049455']",2018-01-01
https://openalex.org/W4320458302,https://doi.org/10.48550/arxiv.2205.14100,GIT: A Generative Image-to-text Transformer for Vision and Language,"In this paper, we design and train a Generative Image-to-text Transformer, GIT, to unify vision-language tasks such as image/video captioning and question answering. While generative models provide a consistent network architecture between pre-training and fine-tuning, existing work typically contains complex structures (uni/multi-modal encoder/decoder) and depends on external modules such as object detectors/taggers and optical character recognition (OCR). In GIT, we simplify the architecture as one image encoder and one text decoder under a single language modeling task. We also scale up the pre-training data and the model size to boost the model performance. Without bells and whistles, our GIT establishes new state of the arts on 12 challenging benchmarks with a large margin. For instance, our model surpasses the human performance for the first time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a new scheme of generation-based image classification and scene text recognition, achieving decent performance on standard benchmarks. Codes are released at \url{https://github.com/microsoft/GenerativeImage2Text}.",[],2022-05-27
https://openalex.org/W659550629,https://doi.org/10.1017/cbo9780511611759,Accents of English,"Accents of English is about the way English is pronounced by different people in different places. Volume 1 provides a synthesizing introduction, which shows how accents vary not only geographically, but also with social class, formality, sex and age; and in volumes 2 and 3 the author examines in greater depth the various accents used by people who speak English as their mother tongue: the accents of the regions of England, Wales, Scotland and Ireland (volume 2), and of the USA, Canada, the West Indies, Australia, New Zealand, South Africa, India, Black Africa and the Far East (volume 3). Each volume can be read independently, and together they form a major scholarly survey, of considerable originality, which not only includes descriptions of hitherto neglected accents, but also examines the implications for phonological theory. Readers will find the answers to many questions: Who makes 'good' rhyme with 'mood'? Which accents have no voiced sibilants? How is a Canadian accent different from an American one, a New Zealand one from an Australian one, a Jamaican one from a Barbadian one? What are the historical reasons for British-American pronunciation differences? What sound changes are currently in progress in New York, in London, in Edinburgh? Dr Wells his written principally for students of linguistics, phonetics and English language, but the motivated general reader will also find the study both fascinating and rewarding.",[],1982-04-08
https://openalex.org/W4375868763,https://doi.org/10.1109/icassp49357.2023.10095837,Using Adapters to Overcome Catastrophic Forgetting in End-to-End Automatic Speech Recognition,"Learning a set of tasks in sequence remains a challenge for artificial neural networks, which, in such scenarios, tend to suffer from Catastrophic Forgetting (CF). The same applies to End-to-End (E2E) Automatic Speech Recognition (ASR) models, even for monolingual tasks. In this paper, we aim to overcome CF for E2E ASR by inserting adapters, small architectures of few parameters which allow a general model to be fine-tuned to a specific task, into our model. We make these adapters task-specific, while regularizing the parameters of the model shared by all tasks, thus stimulating the model to fully exploit the adapters while keeping the shared parameters to work well for all tasks. Our method outperforms all baselines on two monolingual experiments while being more storage efficient and without requiring the storage of data from previous tasks.","['https://openalex.org/W3198094329', 'https://openalex.org/W2972313371', 'https://openalex.org/W6799245484', 'https://openalex.org/W6810650560', 'https://openalex.org/W6720711607', 'https://openalex.org/W2972389417', 'https://openalex.org/W3023953056', 'https://openalex.org/W3091787298', 'https://openalex.org/W3030364939', 'https://openalex.org/W1682403713', 'https://openalex.org/W6759579507', 'https://openalex.org/W6780218876', 'https://openalex.org/W6739901393', 'https://openalex.org/W3097777922', 'https://openalex.org/W3197242296', 'https://openalex.org/W3165404421', 'https://openalex.org/W2962780374', 'https://openalex.org/W4225274946', 'https://openalex.org/W3112170794', 'https://openalex.org/W3197845195', 'https://openalex.org/W2971840980', 'https://openalex.org/W6601192135', 'https://openalex.org/W6771467084', 'https://openalex.org/W2963250244', 'https://openalex.org/W2782276075', 'https://openalex.org/W6732467815', 'https://openalex.org/W3097968133', 'https://openalex.org/W6746266179', 'https://openalex.org/W2560647685', 'https://openalex.org/W6756754374', 'https://openalex.org/W2734314755', 'https://openalex.org/W2963813679', 'https://openalex.org/W4295883599', 'https://openalex.org/W2902456977', 'https://openalex.org/W3036601975', 'https://openalex.org/W2964303773', 'https://openalex.org/W4385245566', 'https://openalex.org/W2583761661', 'https://openalex.org/W3030437843', 'https://openalex.org/W29952999', 'https://openalex.org/W4312789614', 'https://openalex.org/W3186596101', 'https://openalex.org/W2963588172', 'https://openalex.org/W2473930607']",2023-05-05
https://openalex.org/W3097882114,https://doi.org/10.21437/interspeech.2020-2404,Mask CTC: Non-Autoregressive End-to-End ASR with CTC and Mask Predict,"We present Mask CTC, a novel non-autoregressive end-to-end automatic speech recognition (ASR) framework, which generates a sequence by refining outputs of the connectionist temporal classification (CTC).Neural sequence-to-sequence models are usually autoregressive: each output token is generated by conditioning on previously generated tokens, at the cost of requiring as many iterations as the output length.On the other hand, non-autoregressive models can simultaneously generate tokens within a constant number of iterations, which results in significant inference time reduction and better suits end-toend ASR model for real-world scenarios.In this work, Mask CTC model is trained using a Transformer encoder-decoder with joint training of mask prediction and CTC.During inference, the target sequence is initialized with the greedy CTC outputs and low-confidence tokens are masked based on the CTC probabilities.Based on the conditional dependence between output tokens, these masked low-confidence tokens are then predicted conditioning on the high-confidence tokens.Experimental results on different speech recognition tasks show that Mask CTC outperforms the standard CTC model (e.g., 17.9% → 12.1% WER on WSJ) and approaches the autoregressive model, requiring much less inference time using CPUs (0.07 RTF in Python implementation).All of our codes are publicly available at https://github.com/espnet/espnet.","['https://openalex.org/W2896457183', 'https://openalex.org/W854541894', 'https://openalex.org/W2102113734', 'https://openalex.org/W2972818416', 'https://openalex.org/W2936774411', 'https://openalex.org/W2962780374', 'https://openalex.org/W2946375144', 'https://openalex.org/W2949644922', 'https://openalex.org/W3100753857', 'https://openalex.org/W2130942839', 'https://openalex.org/W2133564696', 'https://openalex.org/W2526425061', 'https://openalex.org/W2024490156', 'https://openalex.org/W2767206889', 'https://openalex.org/W3035445001', 'https://openalex.org/W2327501763', 'https://openalex.org/W2892213699', 'https://openalex.org/W3034729383', 'https://openalex.org/W2892009249', 'https://openalex.org/W2963434219', 'https://openalex.org/W2963827914', 'https://openalex.org/W2989134874', 'https://openalex.org/W2988975212', 'https://openalex.org/W3000840023', 'https://openalex.org/W3103005696', 'https://openalex.org/W2972389417', 'https://openalex.org/W4385245566', 'https://openalex.org/W2962784628', 'https://openalex.org/W2970832665', 'https://openalex.org/W1660460062', 'https://openalex.org/W2739883972']",2020-10-25
https://openalex.org/W4385567350,https://doi.org/10.18653/v1/2022.findings-emnlp.402,BERT Meets CTC: New Formulation of End-to-End Speech Recognition with Pre-trained Masked Language Model,"This paper presents BERT-CTC, a novel formulation of end-to-end speech recognition that adapts BERT for connectionist temporal classification (CTC). Our formulation relaxes the conditional independence assumptions used in conventional CTC and incorporates linguistic knowledge through the explicit output dependency obtained by BERT contextual embedding. BERT-CTC attends to the full contexts of the input and hypothesized output sequences via the self-attention mechanism. This mechanism encourages a model to learn inner/inter-dependencies between the audio and token representations while maintaining CTC's training efficiency. During inference, BERT-CTC combines a mask-predict algorithm with CTC decoding, which iteratively refines an output sequence. The experimental results reveal that BERT-CTC improves over conventional approaches across variations in speaking styles and languages. Finally, we show that the semantic representations in BERT-CTC are beneficial towards downstream spoken language understanding tasks.","['https://openalex.org/W2143612262', 'https://openalex.org/W3197140813', 'https://openalex.org/W2767206889', 'https://openalex.org/W1494198834', 'https://openalex.org/W2963242190', 'https://openalex.org/W2996428491', 'https://openalex.org/W2963240019', 'https://openalex.org/W2988975212', 'https://openalex.org/W2946417913', 'https://openalex.org/W1915251500', 'https://openalex.org/W1526990717', 'https://openalex.org/W2962780374', 'https://openalex.org/W4287989347', 'https://openalex.org/W3162899666', 'https://openalex.org/W3093345276', 'https://openalex.org/W2133564696', 'https://openalex.org/W4294670162', 'https://openalex.org/W3141961557', 'https://openalex.org/W3155427814', 'https://openalex.org/W2962784628', 'https://openalex.org/W3097882114', 'https://openalex.org/W3007433671', 'https://openalex.org/W3097777922', 'https://openalex.org/W4226521565', 'https://openalex.org/W4295312788', 'https://openalex.org/W4287210743', 'https://openalex.org/W3035445001', 'https://openalex.org/W2884975363', 'https://openalex.org/W3112157188', 'https://openalex.org/W4221151577', 'https://openalex.org/W2130942839', 'https://openalex.org/W2963979492', 'https://openalex.org/W1922655562', 'https://openalex.org/W3034775979', 'https://openalex.org/W2939111082', 'https://openalex.org/W2936774411', 'https://openalex.org/W2896457183', 'https://openalex.org/W3162249256', 'https://openalex.org/W3169714379', 'https://openalex.org/W2990391581', 'https://openalex.org/W2407080277', 'https://openalex.org/W2577366047', 'https://openalex.org/W3115462295', 'https://openalex.org/W1828163288', 'https://openalex.org/W854541894', 'https://openalex.org/W3096160024', 'https://openalex.org/W2952230511', 'https://openalex.org/W4210663600', 'https://openalex.org/W2979826702', 'https://openalex.org/W3200601846', 'https://openalex.org/W3217767527', 'https://openalex.org/W2888779557', 'https://openalex.org/W4210758944', 'https://openalex.org/W3196500669', 'https://openalex.org/W4224916448', 'https://openalex.org/W3148001440', 'https://openalex.org/W2327501763', 'https://openalex.org/W3015960524', 'https://openalex.org/W2973217961', 'https://openalex.org/W3163793923', 'https://openalex.org/W2043701535', 'https://openalex.org/W2962824709', 'https://openalex.org/W3096297644', 'https://openalex.org/W3100460087', 'https://openalex.org/W3160987270', 'https://openalex.org/W2973122799', 'https://openalex.org/W4225824617', 'https://openalex.org/W3197148831', 'https://openalex.org/W2946375144', 'https://openalex.org/W2884001105', 'https://openalex.org/W2802023636', 'https://openalex.org/W4292779060', 'https://openalex.org/W2978017171', 'https://openalex.org/W4297841367', 'https://openalex.org/W2251321385', 'https://openalex.org/W3161048756', 'https://openalex.org/W4210300569', 'https://openalex.org/W4385245566', 'https://openalex.org/W3096109555']",2022-01-01
https://openalex.org/W3160475509,https://doi.org/10.1109/icassp39728.2021.9413386,"The Accented English Speech Recognition Challenge 2020: Open Datasets, Tracks, Baselines, Results and Methods","The variety of accents has posed a big challenge to speech recognition. The Accented English Speech Recognition Challenge (AESRC2020) is designed for providing a common testbed and promoting accent-related research. Two tracks are set in the challenge – English accent recognition (track 1) and accented English speech recognition (track 2). A set of 160 hours of accented English speech collected from 8 countries is released with labels as the training set. Another 20 hours of speech without labels is later released as the test set, including two unseen accents from another two countries used to test the model generalization ability in track 2. We also provide baseline systems for the participants. This paper first reviews the released dataset, track setups, baselines and then summarizes the challenge results and major techniques used in the submissions.","['https://openalex.org/W6795930586', 'https://openalex.org/W6780218876', 'https://openalex.org/W2327501763', 'https://openalex.org/W2295754094', 'https://openalex.org/W1595289649', 'https://openalex.org/W3012374325', 'https://openalex.org/W3095468613', 'https://openalex.org/W3015915933', 'https://openalex.org/W2973094925', 'https://openalex.org/W6603403951', 'https://openalex.org/W6767768130', 'https://openalex.org/W2962684181', 'https://openalex.org/W3015723617', 'https://openalex.org/W3097777922', 'https://openalex.org/W6784362749', 'https://openalex.org/W6739901393', 'https://openalex.org/W3033326607', 'https://openalex.org/W2765598774', 'https://openalex.org/W6774054309', 'https://openalex.org/W2347141333', 'https://openalex.org/W2916104401', 'https://openalex.org/W2794506738', 'https://openalex.org/W6671534424', 'https://openalex.org/W2981087920', 'https://openalex.org/W2040357394', 'https://openalex.org/W2938374794', 'https://openalex.org/W6779192484', 'https://openalex.org/W6713969595', 'https://openalex.org/W1494198834', 'https://openalex.org/W2933138175', 'https://openalex.org/W6791105188', 'https://openalex.org/W3093383192', 'https://openalex.org/W3019527251', 'https://openalex.org/W2084797178', 'https://openalex.org/W3036601975', 'https://openalex.org/W3006683367', 'https://openalex.org/W2405866807', 'https://openalex.org/W3025165719', 'https://openalex.org/W3010079431', 'https://openalex.org/W4287640840', 'https://openalex.org/W3163152557', 'https://openalex.org/W2963403868', 'https://openalex.org/W3099782249', 'https://openalex.org/W4385245566', 'https://openalex.org/W2972815379', 'https://openalex.org/W3162508345', 'https://openalex.org/W3032558098', 'https://openalex.org/W82936479']",2021-05-13
https://openalex.org/W4292387508,https://doi.org/10.1109/taslp.2022.3198546,Layer-Wise Fast Adaptation for End-to-End Multi-Accent Speech Recognition,"The variety and complexity of accents pose a huge challenge to robust Automatic Speech Recognition&#x00A0;(ASR). Some previous work has attempted to address such problems, however most of the current approaches either require prior knowledge about the target accent, or cannot handle unseen accents and accent-unspecific standard speech. In this work, we aim to improve multi-accent speech recognition in the end-to-end&#x00A0;(E2E) framework with a novel layer-wise adaptation architecture. Firstly, we propose a robust deep accent representation learning architecture to obtain accurate accent embedding, and some advanced schemes are designed to further boost the quality of accent embeddings, including phone posteriorgram&#x00A0;(PPG) feature, TTS based data augmentation in the training stage, test-time augmentation and multi-embedding fusion in the testing stage. Then, the layer-wise adaptation with accent embeddings is developed for fast accent adaptation in ASR, and two types of adapter layers are designed, including the gated adapter layer and multi-basis adapter layer. Compared to the usual two-pass adaptation, these adapter layers are injected between the ASR encoder layers to encode the accent information in ASR flexibly, and perform fast adaption on the corresponding speech accent. The experiments on Accent AESRC corpus show that the proposed deep accent representation learning can capture accurate accent knowledge, and get high performance on accent classification. The new layer-wise adaptation architecture with the accurate accent embedding outperforms the other traditional methods, and obtains consistent <inline-formula><tex-math notation=""LaTeX"">$\sim$</tex-math></inline-formula>15&#x0025; relative word error rate&#x00A0;(WER) reduction on all kinds of testing scenarios, including seen accents, unseen accents and accent-unspecific standard speech.","['https://openalex.org/W124356247', 'https://openalex.org/W2101066392', 'https://openalex.org/W3160475509', 'https://openalex.org/W6794170720', 'https://openalex.org/W1998245987', 'https://openalex.org/W2294933947', 'https://openalex.org/W2888797456', 'https://openalex.org/W2294108103', 'https://openalex.org/W3008876072', 'https://openalex.org/W3091787298', 'https://openalex.org/W2964099675', 'https://openalex.org/W2889494795', 'https://openalex.org/W2972539100', 'https://openalex.org/W2962893195', 'https://openalex.org/W2603679025', 'https://openalex.org/W3162061711', 'https://openalex.org/W2938374794', 'https://openalex.org/W3163865502', 'https://openalex.org/W2913244614', 'https://openalex.org/W2972815379', 'https://openalex.org/W3197242296', 'https://openalex.org/W2890964092', 'https://openalex.org/W3024869864', 'https://openalex.org/W2327501763', 'https://openalex.org/W2739883972', 'https://openalex.org/W2526425061', 'https://openalex.org/W2127141656', 'https://openalex.org/W1006777433', 'https://openalex.org/W2194775991', 'https://openalex.org/W6769178842', 'https://openalex.org/W3010925296', 'https://openalex.org/W3015611708', 'https://openalex.org/W2969985801', 'https://openalex.org/W2752782242', 'https://openalex.org/W2507912506', 'https://openalex.org/W2973142754', 'https://openalex.org/W3161294170', 'https://openalex.org/W3015440759', 'https://openalex.org/W6763832098', 'https://openalex.org/W2963091184', 'https://openalex.org/W6684191040', 'https://openalex.org/W6780226713', 'https://openalex.org/W6739901393', 'https://openalex.org/W3097777922', 'https://openalex.org/W1494198834', 'https://openalex.org/W6631362777', 'https://openalex.org/W2963250244', 'https://openalex.org/W2962780374', 'https://openalex.org/W2936774411', 'https://openalex.org/W6688816777', 'https://openalex.org/W2964243274', 'https://openalex.org/W3162508345', 'https://openalex.org/W3197530164', 'https://openalex.org/W2163605009', 'https://openalex.org/W4288091954', 'https://openalex.org/W3156258992', 'https://openalex.org/W2219249508']",2022-01-01
https://openalex.org/W3162812479,https://doi.org/10.1109/icassp39728.2021.9414833,End-To-End Multi-Accent Speech Recognition with Unsupervised Accent Modelling,"End-to-end speech recognition has achieved good recognition performance on standard English pronunciation datasets. However, one prominent problem with end-to-end speech recognition systems is that non-native English speakers tend to have complex and varied accents, which reduces the accuracy of English speech recognition in different countries. In order to grapple with such an issue, we first investigate and improve the current mainstream end-to-end multi-accent speech recognition technologies. In addition, we propose two unsupervised accent modelling methods, which convert accent information into a global embedding, and use it to improve the performance of the end-to-end multi-accent speech recognition systems. Experimental results on accented English datasets of eight countries (AESRC2020) show that, compared with the Transformer baseline, our proposed methods achieve relative 14.8% and 15.4% average word error rate (WER) reduction in the development set and evaluation set, respectively.","['https://openalex.org/W2973094925', 'https://openalex.org/W2890964092', 'https://openalex.org/W6750489868', 'https://openalex.org/W2904459034', 'https://openalex.org/W2964243274', 'https://openalex.org/W6739901393', 'https://openalex.org/W6712930963', 'https://openalex.org/W6631362777', 'https://openalex.org/W3015457435', 'https://openalex.org/W2972818416', 'https://openalex.org/W2889494795', 'https://openalex.org/W2964099675', 'https://openalex.org/W2962684181', 'https://openalex.org/W2962893195', 'https://openalex.org/W2962824709', 'https://openalex.org/W2962826786', 'https://openalex.org/W3044481399', 'https://openalex.org/W2794490148', 'https://openalex.org/W2963272440', 'https://openalex.org/W2402146185', 'https://openalex.org/W3007328579', 'https://openalex.org/W4385245566', 'https://openalex.org/W1524333225', 'https://openalex.org/W2963403868', 'https://openalex.org/W4308252168']",2021-05-13
https://openalex.org/W3015723617,https://doi.org/10.1109/icassp40776.2020.9053098,Aipnet: Generative Adversarial Pre-Training of Accent-Invariant Networks for End-To-End Speech Recognition,"As one of the major sources in speech variability, accents have posed a grand challenge to the robustness of speech recognition systems. In this paper, our goal is to build a unified end-to-end speech recognition system that generalizes well across accents. For this purpose, we propose a novel pre-training framework AIPNetbased on generative adversarial nets (GAN) for accent-invariant representation learning: Accent Invariant Pre-training Networks. We pre-train AIPNetto disentangle accent-invariant and accent-specific characteristics from acoustic features through adversarial training on accented data for which transcriptions are not necessarily available. We further fine-tune AIPNetby connecting the accent-invariant module with an attention-based encoder-decoder model for multi-accent speech recognition. In the experiments, our approach is compared against four baselines including both accent-dependent and accent-independent models. Experimental results on 9 English accents show that the proposed approach outperforms all the baselines by 2.3 ~ 4.5% relative reduction on average WER when transcriptions are available in all accents and by 1.6 ~ 6.1% relative reduction when transcriptions are only available in US accent.","['https://openalex.org/W2106440210', 'https://openalex.org/W6696757691', 'https://openalex.org/W6639480849', 'https://openalex.org/W2884305338', 'https://openalex.org/W6732862412', 'https://openalex.org/W2962684181', 'https://openalex.org/W2584009249', 'https://openalex.org/W6684191040', 'https://openalex.org/W6755207826', 'https://openalex.org/W2408195107', 'https://openalex.org/W2587080466', 'https://openalex.org/W2964099675', 'https://openalex.org/W143647774', 'https://openalex.org/W6713969595', 'https://openalex.org/W2962893195', 'https://openalex.org/W2130414229', 'https://openalex.org/W2603679025', 'https://openalex.org/W2938374794', 'https://openalex.org/W6682691769', 'https://openalex.org/W2327501763', 'https://openalex.org/W6745117592', 'https://openalex.org/W4294170691', 'https://openalex.org/W2963618559', 'https://openalex.org/W2099471712', 'https://openalex.org/W2584667682', 'https://openalex.org/W2896457183', 'https://openalex.org/W2405866807', 'https://openalex.org/W2294108103', 'https://openalex.org/W2963826681', 'https://openalex.org/W2163605009', 'https://openalex.org/W2153579005', 'https://openalex.org/W4320013936', 'https://openalex.org/W2963341956', 'https://openalex.org/W2758785877', 'https://openalex.org/W2187089797', 'https://openalex.org/W1882958252']",2020-04-09
https://openalex.org/W2148371116,https://doi.org/10.1109/5.326413,Speech coding: a tutorial review,"The past decade has witnessed substantial progress towards the application of low-rate speech coders to civilian and military communications as well as computer-related voice applications. Central to this progress has been the development of new speech coders capable of producing high-quality speech at low data rates. Most of these coders incorporate mechanisms to: represent the spectral properties of speech, provide for speech waveform matching, and ""optimize"" the coder's performance for the human ear. A number of these coders have already been adopted in national and international cellular telephony standards. The objective of this paper is to provide a tutorial overview of speech coding methodologies with emphasis on those algorithms that are part of the recent low-rate standards for cellular communications. Although the emphasis is on the new low-rate coders, we attempt to provide a comprehensive survey by covering some of the traditional methodologies as well. We feel that this approach will not only point out key references but will also provide valuable background to the beginner. The paper starts with a historical perspective and continues with a brief discussion on the speech properties and performance measures. We then proceed with descriptions of waveform coders, sinusoidal transform coders, linear predictive vocoders, and analysis-by-synthesis linear predictive coders. Finally, we present concluding remarks followed by a discussion of opportunities for future research.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>","['https://openalex.org/W2998892371', 'https://openalex.org/W2110610213', 'https://openalex.org/W1529266893', 'https://openalex.org/W2186435531', 'https://openalex.org/W2113379491', 'https://openalex.org/W2154023894', 'https://openalex.org/W2913399920', 'https://openalex.org/W2117750426', 'https://openalex.org/W1969526148', 'https://openalex.org/W2112824094', 'https://openalex.org/W2061660909', 'https://openalex.org/W2167550477', 'https://openalex.org/W1536990986', 'https://openalex.org/W2129352455', 'https://openalex.org/W2100974241', 'https://openalex.org/W1967066785', 'https://openalex.org/W1587863748', 'https://openalex.org/W1970272279', 'https://openalex.org/W2002182716', 'https://openalex.org/W2106163608', 'https://openalex.org/W2037034710', 'https://openalex.org/W1642408345', 'https://openalex.org/W2128307820', 'https://openalex.org/W1988975528', 'https://openalex.org/W1634005169', 'https://openalex.org/W1996092405', 'https://openalex.org/W2157225271', 'https://openalex.org/W156807595', 'https://openalex.org/W2046367577', 'https://openalex.org/W1973497804', 'https://openalex.org/W2135618923', 'https://openalex.org/W1528176576', 'https://openalex.org/W2074783531', 'https://openalex.org/W2141660759', 'https://openalex.org/W2030045709', 'https://openalex.org/W2110164509', 'https://openalex.org/W2049168483', 'https://openalex.org/W1575566382', 'https://openalex.org/W2160381028', 'https://openalex.org/W1965789929', 'https://openalex.org/W1978201483', 'https://openalex.org/W2556247670', 'https://openalex.org/W2043583598', 'https://openalex.org/W1501095260', 'https://openalex.org/W2296272057', 'https://openalex.org/W1963884863', 'https://openalex.org/W632294644', 'https://openalex.org/W2058075673', 'https://openalex.org/W1993849324', 'https://openalex.org/W1826405859', 'https://openalex.org/W2065925624', 'https://openalex.org/W2154208274', 'https://openalex.org/W2068669987', 'https://openalex.org/W1700083040', 'https://openalex.org/W2071914178', 'https://openalex.org/W86348706', 'https://openalex.org/W2115416818', 'https://openalex.org/W1989344925', 'https://openalex.org/W1966187261', 'https://openalex.org/W2151626637', 'https://openalex.org/W2131664668', 'https://openalex.org/W1996754528', 'https://openalex.org/W2119672627', 'https://openalex.org/W2140317719', 'https://openalex.org/W1532750084', 'https://openalex.org/W2128342472', 'https://openalex.org/W2136645678', 'https://openalex.org/W181056519', 'https://openalex.org/W2023871577', 'https://openalex.org/W2138213055', 'https://openalex.org/W648334302', 'https://openalex.org/W2138471165', 'https://openalex.org/W2136471945', 'https://openalex.org/W2071602882', 'https://openalex.org/W2101393509', 'https://openalex.org/W1493369344', 'https://openalex.org/W1903180476', 'https://openalex.org/W2104052328', 'https://openalex.org/W1986343971', 'https://openalex.org/W2042247992', 'https://openalex.org/W2115546429', 'https://openalex.org/W1950812035', 'https://openalex.org/W2037221767', 'https://openalex.org/W2157985754', 'https://openalex.org/W2150593711', 'https://openalex.org/W2151677880', 'https://openalex.org/W2133882791', 'https://openalex.org/W1561745962', 'https://openalex.org/W2149633130', 'https://openalex.org/W1692258870', 'https://openalex.org/W2066291401', 'https://openalex.org/W2159644785', 'https://openalex.org/W1863351891', 'https://openalex.org/W2084044763', 'https://openalex.org/W2118130296', 'https://openalex.org/W1991552246', 'https://openalex.org/W2078911760', 'https://openalex.org/W2023269893', 'https://openalex.org/W2170790419', 'https://openalex.org/W2108771579', 'https://openalex.org/W2163181067', 'https://openalex.org/W3159973907', 'https://openalex.org/W2125610452', 'https://openalex.org/W2046104252', 'https://openalex.org/W1493163583', 'https://openalex.org/W1978540972', 'https://openalex.org/W2170475937', 'https://openalex.org/W2142591210', 'https://openalex.org/W2038763274', 'https://openalex.org/W1823186822', 'https://openalex.org/W1986581241', 'https://openalex.org/W1534089347', 'https://openalex.org/W590236355', 'https://openalex.org/W1624524702', 'https://openalex.org/W2151624614', 'https://openalex.org/W1969258769', 'https://openalex.org/W1587896874', 'https://openalex.org/W2143039550', 'https://openalex.org/W2144950156', 'https://openalex.org/W2055815603', 'https://openalex.org/W1968980256', 'https://openalex.org/W2147481909', 'https://openalex.org/W2101990851', 'https://openalex.org/W2168274102', 'https://openalex.org/W2063678710', 'https://openalex.org/W2098342522', 'https://openalex.org/W190099900', 'https://openalex.org/W2155073975', 'https://openalex.org/W1995875735', 'https://openalex.org/W2134383396', 'https://openalex.org/W2122421556', 'https://openalex.org/W2128741405', 'https://openalex.org/W1496923911', 'https://openalex.org/W1984299211', 'https://openalex.org/W2000914445', 'https://openalex.org/W1976313025', 'https://openalex.org/W1908611440', 'https://openalex.org/W2281464369', 'https://openalex.org/W2072471025', 'https://openalex.org/W2022554507', 'https://openalex.org/W1522381657', 'https://openalex.org/W1562417701', 'https://openalex.org/W1500551892', 'https://openalex.org/W2157469984', 'https://openalex.org/W2156814540', 'https://openalex.org/W1526835377', 'https://openalex.org/W2082144929', 'https://openalex.org/W2056536278', 'https://openalex.org/W1995374299', 'https://openalex.org/W1950316085', 'https://openalex.org/W2115054212', 'https://openalex.org/W2162944213', 'https://openalex.org/W1976126081', 'https://openalex.org/W1966264494', 'https://openalex.org/W2127187487', 'https://openalex.org/W1563027682', 'https://openalex.org/W1566883001', 'https://openalex.org/W2109059146', 'https://openalex.org/W2038948227', 'https://openalex.org/W2136397324', 'https://openalex.org/W1561234408', 'https://openalex.org/W2106981941', 'https://openalex.org/W2061383182', 'https://openalex.org/W2031887434', 'https://openalex.org/W573069471', 'https://openalex.org/W1623650408', 'https://openalex.org/W2505196176', 'https://openalex.org/W2097645910', 'https://openalex.org/W1706488930', 'https://openalex.org/W2168353136', 'https://openalex.org/W2166517849', 'https://openalex.org/W1970352604', 'https://openalex.org/W2907162034', 'https://openalex.org/W1613096649', 'https://openalex.org/W1496971974', 'https://openalex.org/W1812113059', 'https://openalex.org/W1995871078', 'https://openalex.org/W2057460734', 'https://openalex.org/W2018388266', 'https://openalex.org/W2106650471', 'https://openalex.org/W1518183793', 'https://openalex.org/W2163341714', 'https://openalex.org/W2134015662', 'https://openalex.org/W2167118415', 'https://openalex.org/W2021193635', 'https://openalex.org/W2117897619', 'https://openalex.org/W2145297822', 'https://openalex.org/W1489208846', 'https://openalex.org/W2130032292', 'https://openalex.org/W2069501481', 'https://openalex.org/W2030639849', 'https://openalex.org/W2129586785']",1994-01-01
https://openalex.org/W3015780049,https://doi.org/10.1109/icassp40776.2020.9053113,Audio Codec Enhancement with Generative Adversarial Networks,"Audio codecs are typically transform-domain based and efficiently code stationary audio signals, but they struggle with speech and signals containing dense transient events such as applause. Specifically, with these two classes of signals as examples, we demonstrate a technique for restoring audio from coding noise based on generative adversarial networks (GAN). A primary advantage of the proposed GAN-based coded audio enhancer is that the method operates end-to-end directly on decoded audio samples, eliminating the need to design any manually-crafted frontend. Furthermore, the enhancement approach described in this paper can improve the sound quality of low-bit rate coded audio without any modifications to the existent standard-compliant encoders. Subjective tests illustrate that the proposed enhancer improves the quality of speech and difficult to code applause excerpts significantly.","['https://openalex.org/W2623662528', 'https://openalex.org/W2809824582', 'https://openalex.org/W6755446875', 'https://openalex.org/W6762390287', 'https://openalex.org/W2944209089', 'https://openalex.org/W6755312952', 'https://openalex.org/W6726381175', 'https://openalex.org/W2140593795', 'https://openalex.org/W2889428811', 'https://openalex.org/W2774016059', 'https://openalex.org/W2775336875', 'https://openalex.org/W2647567423', 'https://openalex.org/W2769892507', 'https://openalex.org/W2117366609', 'https://openalex.org/W6744051579', 'https://openalex.org/W2963341071', 'https://openalex.org/W1963699091', 'https://openalex.org/W6751642695', 'https://openalex.org/W2593414223', 'https://openalex.org/W2963073614', 'https://openalex.org/W2094721231', 'https://openalex.org/W2514828952', 'https://openalex.org/W2395539742', 'https://openalex.org/W3096468295', 'https://openalex.org/W2099471712', 'https://openalex.org/W2803112028', 'https://openalex.org/W2949382160', 'https://openalex.org/W2893749619', 'https://openalex.org/W2752819676', 'https://openalex.org/W4320013936', 'https://openalex.org/W2079985975', 'https://openalex.org/W2896030002', 'https://openalex.org/W2964046669', 'https://openalex.org/W2520164769', 'https://openalex.org/W2519091744', 'https://openalex.org/W2952716587']",2020-04-09
https://openalex.org/W3169418678,https://doi.org/10.1109/ieeeconf51394.2020.9443419,WaveNetEQ — Packet Loss Concealment with WaveRNN,"We present WaveNetEQ, a novel packet loss concealment method based on a WaveRNN architecture. The model is conditioned on a log-mel spectrogram of the past signal to extract slow moving features, like voice characteristics and prosody and achieves significantly better quality than pattern based methods for medium and long term packet loss. Through aggressive sparsification the model is efficient enough to run on a phone.","['https://openalex.org/W2243752967', 'https://openalex.org/W2137539246', 'https://openalex.org/W2973133192', 'https://openalex.org/W2799789537', 'https://openalex.org/W1494198834', 'https://openalex.org/W6769995984', 'https://openalex.org/W6640212811', 'https://openalex.org/W2118763712', 'https://openalex.org/W2016589492', 'https://openalex.org/W6748409065', 'https://openalex.org/W4298580827', 'https://openalex.org/W2519091744', 'https://openalex.org/W2949382160', 'https://openalex.org/W2985518820', 'https://openalex.org/W1924770834', 'https://openalex.org/W2964307104']",2020-11-01
https://openalex.org/W1973681148,https://doi.org/10.1109/icassp.2014.6853900,Speech feature denoising and dereverberation via deep autoencoders for noisy reverberant speech recognition,"Denoising autoencoders (DAs) have shown success in generating robust features for images, but there has been limited work in applying DAs for speech. In this paper we present a deep denoising autoencoder (DDA) framework that can produce robust speech features for noisy reverberant speech recognition. The DDA is first pre-trained as restricted Boltzmann machines (RBMs) in an unsupervised fashion. Then it is unrolled to autoencoders, and fine-tuned by corresponding clean speech features to learn a nonlinear mapping from noisy to clean features. Acoustic models are re-trained using the reconstructed features from the DDA, and speech recognition is performed. The proposed approach is evaluated on the CHiME-WSJ0 corpus, and shows a 16-25% absolute improvement on the recognition accuracy under various SNRs.","['https://openalex.org/W2100495367', 'https://openalex.org/W2101045344', 'https://openalex.org/W2136922672', 'https://openalex.org/W6681096077', 'https://openalex.org/W6697251715', 'https://openalex.org/W2025768430', 'https://openalex.org/W4231109964', 'https://openalex.org/W2148575186', 'https://openalex.org/W2019024593', 'https://openalex.org/W2004406940', 'https://openalex.org/W2105099419', 'https://openalex.org/W2122514667', 'https://openalex.org/W67398352', 'https://openalex.org/W2168076624', 'https://openalex.org/W2158291955', 'https://openalex.org/W2152718950', 'https://openalex.org/W2145094598', 'https://openalex.org/W2256662675', 'https://openalex.org/W2072128103', 'https://openalex.org/W2296581541', 'https://openalex.org/W943204654', 'https://openalex.org/W2997574889']",2014-05-01
https://openalex.org/W2296581541,https://doi.org/10.21437/interspeech.2013-267,Reverberant speech recognition based on denoising autoencoder,"Denoising autoencoder is applied to reverberant speech recognition as a noise robust front-end to reconstruct clean speech spectrum from noisy input. In order to capture context effects of speech sounds, a window of multiple short-windowed spectral frames are concatenated to form a single input vector. Additionally, a combination of short and long-term spectra is investigated to properly handle long impulse response of reverberation while keeping necessary time resolution for speech recognition. Experiments are performed using the CENSREC-4dataset that is designed as an evaluation framework for distant-talking speech recognition. Experimental results show that the proposed denoising autoencoder based front-end using the shortwindowed spectra gives better results than conventional methods. By combining the long-term spectra, further improvement is obtained. The recognition accuracy by the proposed method using the short and long-term spectra is 97.0% for the open condition test set of the dataset, whereas it is 87.8% when a multicondition training based baseline is used. As a supplemental experiment, large vocabulary speech recognition is also performed and the effectiveness of the proposed method has been confirmed. Index Terms: Denoising autoencoder, reverberant speech recognition, restricted Boltzmann machine, distant-talking speech recognition, CENSREC-4","['https://openalex.org/W2100495367', 'https://openalex.org/W2168013545', 'https://openalex.org/W2172203831', 'https://openalex.org/W2116064496', 'https://openalex.org/W2145094598', 'https://openalex.org/W2107992675', 'https://openalex.org/W349097443', 'https://openalex.org/W2148154194', 'https://openalex.org/W2316564661', 'https://openalex.org/W2128621997', 'https://openalex.org/W1993882792', 'https://openalex.org/W2404637451', 'https://openalex.org/W1982728343', 'https://openalex.org/W2290318471', 'https://openalex.org/W2005522781']",2013-08-25
https://openalex.org/W2802034954,https://doi.org/10.1109/icassp.2018.8462049,Time-Frequency Networks for Audio Super-Resolution,"Audio super-resolution (a.k.a. bandwidth extension) is the challenging task of increasing the temporal resolution of audio signals. Recent deep networks approaches achieved promising results by modeling the task as a regression problem in either time or frequency domain. In this paper, we introduced Time-Frequency Network (TFNet), a deep network that utilizes supervision in both the time and frequency domain. We proposed a novel model architecture which allows the two domains to be jointly optimized. Results demonstrate that our method outperforms the state-of-the-art both quantitatively and qualitatively.","['https://openalex.org/W6679645489', 'https://openalex.org/W2100285470', 'https://openalex.org/W6604083050', 'https://openalex.org/W1885185971', 'https://openalex.org/W6690498163', 'https://openalex.org/W6736904528', 'https://openalex.org/W1989337816', 'https://openalex.org/W6740915507', 'https://openalex.org/W6704369950', 'https://openalex.org/W6606904575', 'https://openalex.org/W1531967940', 'https://openalex.org/W1971868317', 'https://openalex.org/W2122336353', 'https://openalex.org/W6712300757', 'https://openalex.org/W1994997782', 'https://openalex.org/W2132327171', 'https://openalex.org/W99802290', 'https://openalex.org/W2519091744', 'https://openalex.org/W171863622', 'https://openalex.org/W2963952344', 'https://openalex.org/W2399742709', 'https://openalex.org/W2607041014', 'https://openalex.org/W2242218935', 'https://openalex.org/W1522301498', 'https://openalex.org/W2963420272', 'https://openalex.org/W2963917315', 'https://openalex.org/W2584032004']",2018-04-01
https://openalex.org/W2150593711,https://doi.org/10.1109/tit.1982.1056489,Least squares quantization in PCM,"It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quautization schemes for <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2^{b}</tex> quanta, <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">b=1,2, \cdots, 7</tex> , are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.","['https://openalex.org/W267606886', 'https://openalex.org/W1988956009', 'https://openalex.org/W2068071220', 'https://openalex.org/W2004003571', 'https://openalex.org/W1995871078', 'https://openalex.org/W6701282271', 'https://openalex.org/W2319748042', 'https://openalex.org/W4255783720', 'https://openalex.org/W2019862530']",1982-03-01
https://openalex.org/W2134383396,https://doi.org/10.1109/tcom.1980.1094577,An Algorithm for Vector Quantizer Design,"An efficient and intuitive algorithm is presented for the design of vector quantizers based either on a known probabilistic model or on a long training sequence of data. The basic properties of the algorithm are discussed and demonstrated by examples. Quite general distortion measures and long blocklengths are allowed, as exemplified by the design of parameter vector quantizers of ten-dimensional vectors arising in Linear Predictive Coded (LPC) speech compression with a complicated distortion measure arising in LPC analysis that does not depend only on the error vector.","['https://openalex.org/W4388534025', 'https://openalex.org/W2022554507', 'https://openalex.org/W2145838167', 'https://openalex.org/W2169729329', 'https://openalex.org/W2329493551', 'https://openalex.org/W4234676312', 'https://openalex.org/W2109808436', 'https://openalex.org/W4249667877', 'https://openalex.org/W2137089646', 'https://openalex.org/W1968191505', 'https://openalex.org/W6606376275', 'https://openalex.org/W1973387369', 'https://openalex.org/W1773793659', 'https://openalex.org/W2142228262', 'https://openalex.org/W1631408262', 'https://openalex.org/W6600324503', 'https://openalex.org/W6678914141', 'https://openalex.org/W157632773', 'https://openalex.org/W2071609006', 'https://openalex.org/W1489608363', 'https://openalex.org/W1534416612', 'https://openalex.org/W1628049834', 'https://openalex.org/W8398574', 'https://openalex.org/W2127218421', 'https://openalex.org/W4230860499', 'https://openalex.org/W120284483', 'https://openalex.org/W2065902423']",1980-01-01
https://openalex.org/W2002182716,https://doi.org/10.1109/proc.1985.13340,Vector quantization in speech coding,"Quantization, the process of approximating continuous-amplitude signals by digital (discrete-amplitude) signals, is an important aspect of data compression or coding, the field concerned with the reduction of the number of bits necessary to transmit or store analog data, subject to a distortion or fidelity criterion. The independent quantization of each signal value or parameter is termed scalar quantization, while the joint quantization of a block of parameters is termed block or vector quantization. This tutorial review presents the basic concepts employed in vector quantization and gives a realistic assessment of its benefits and costs when compared to scalar quantization. Vector quantization is presented as a process of redundancy removal that makes effective use of four interrelated properties of vector parameters: linear dependency (correlation), nonlinear dependency, shape of the probability density function (pdf), and vector dimensionality itself. In contrast, scalar quantization can utilize effectively only linear dependency and pdf shape. The basic concepts are illustrated by means of simple examples and the theoretical limits of vector quantizer performance are reviewed, based on results from rate-distortion theory. Practical issues relating to quantizer design, implementation, and performance in actual applications are explored. While many of the methods presented are quite general and can be used for the coding of arbitrary signals, this paper focuses primarily on the coding of speech signals and parameters.","['https://openalex.org/W4235422353', 'https://openalex.org/W1536990986', 'https://openalex.org/W2024470753', 'https://openalex.org/W2003342038', 'https://openalex.org/W1996754528', 'https://openalex.org/W1871569676', 'https://openalex.org/W2054128110', 'https://openalex.org/W2152316618', 'https://openalex.org/W2050244349', 'https://openalex.org/W2018208713', 'https://openalex.org/W2098039338', 'https://openalex.org/W2119352491', 'https://openalex.org/W2029495080', 'https://openalex.org/W1604393914', 'https://openalex.org/W2069857803', 'https://openalex.org/W1599569960', 'https://openalex.org/W2100205678', 'https://openalex.org/W2142197336', 'https://openalex.org/W2146519989', 'https://openalex.org/W2114283721', 'https://openalex.org/W1965812785', 'https://openalex.org/W2122350663', 'https://openalex.org/W2164240509', 'https://openalex.org/W1552763461', 'https://openalex.org/W2115456813', 'https://openalex.org/W2115416818', 'https://openalex.org/W4244017338', 'https://openalex.org/W2137263269', 'https://openalex.org/W2031614119', 'https://openalex.org/W2145541334', 'https://openalex.org/W2097645910', 'https://openalex.org/W2058075673', 'https://openalex.org/W1970272279', 'https://openalex.org/W2131664668', 'https://openalex.org/W1711317295', 'https://openalex.org/W2040336387', 'https://openalex.org/W2142228262', 'https://openalex.org/W1996092405', 'https://openalex.org/W1983645263', 'https://openalex.org/W2108525418', 'https://openalex.org/W1965565710', 'https://openalex.org/W2092960335', 'https://openalex.org/W2097012520', 'https://openalex.org/W2156931172', 'https://openalex.org/W2122421556', 'https://openalex.org/W6779487094', 'https://openalex.org/W2096141930', 'https://openalex.org/W1986343971', 'https://openalex.org/W2021436622', 'https://openalex.org/W1700083040', 'https://openalex.org/W2139020066', 'https://openalex.org/W2130032292', 'https://openalex.org/W2146173625', 'https://openalex.org/W2172253923', 'https://openalex.org/W1950812035', 'https://openalex.org/W1813508333', 'https://openalex.org/W2033022635', 'https://openalex.org/W1973387369', 'https://openalex.org/W2150418026', 'https://openalex.org/W1989337816', 'https://openalex.org/W2084354115', 'https://openalex.org/W2143173306', 'https://openalex.org/W2120485343', 'https://openalex.org/W2172074673', 'https://openalex.org/W2089419199', 'https://openalex.org/W2060108852', 'https://openalex.org/W1967066785', 'https://openalex.org/W3020833317', 'https://openalex.org/W2137089646', 'https://openalex.org/W1998110309', 'https://openalex.org/W1837858880', 'https://openalex.org/W2165055491', 'https://openalex.org/W1927968480', 'https://openalex.org/W7045266920', 'https://openalex.org/W1689807905', 'https://openalex.org/W1847689439', 'https://openalex.org/W1544476032', 'https://openalex.org/W1988550194', 'https://openalex.org/W2095975685', 'https://openalex.org/W2021760654', 'https://openalex.org/W2109808436', 'https://openalex.org/W2159481042', 'https://openalex.org/W1826405859', 'https://openalex.org/W1945490744', 'https://openalex.org/W2136849268', 'https://openalex.org/W2140099919', 'https://openalex.org/W1555411534', 'https://openalex.org/W1904554885', 'https://openalex.org/W2584393883', 'https://openalex.org/W2204452154', 'https://openalex.org/W2042927721', 'https://openalex.org/W2124630918', 'https://openalex.org/W1571663174', 'https://openalex.org/W1872806465', 'https://openalex.org/W2148986322', 'https://openalex.org/W2150593711', 'https://openalex.org/W2138054245', 'https://openalex.org/W6632267817', 'https://openalex.org/W1966515943', 'https://openalex.org/W1557027117', 'https://openalex.org/W6678914141', 'https://openalex.org/W2144452965', 'https://openalex.org/W2044002522', 'https://openalex.org/W2151626637', 'https://openalex.org/W2134383396', 'https://openalex.org/W2117741782', 'https://openalex.org/W3134726576', 'https://openalex.org/W2022554507', 'https://openalex.org/W1995875735', 'https://openalex.org/W6604021822', 'https://openalex.org/W1966264494', 'https://openalex.org/W2093228350', 'https://openalex.org/W4231839776', 'https://openalex.org/W3017143921', 'https://openalex.org/W1630162643', 'https://openalex.org/W2054804336', 'https://openalex.org/W2006963707', 'https://openalex.org/W2296272057', 'https://openalex.org/W2069501481', 'https://openalex.org/W2076506437', 'https://openalex.org/W1563154452', 'https://openalex.org/W3094114204', 'https://openalex.org/W1489608363', 'https://openalex.org/W2588912823', 'https://openalex.org/W2127218421', 'https://openalex.org/W1975598412', 'https://openalex.org/W1989148050', 'https://openalex.org/W1540596182', 'https://openalex.org/W1623080549', 'https://openalex.org/W648334302', 'https://openalex.org/W2913066018', 'https://openalex.org/W3194602862', 'https://openalex.org/W1989008275', 'https://openalex.org/W2058097301', 'https://openalex.org/W2037034710', 'https://openalex.org/W2182112064', 'https://openalex.org/W2063973166', 'https://openalex.org/W99903537', 'https://openalex.org/W2799137445', 'https://openalex.org/W4252521233', 'https://openalex.org/W2913399920', 'https://openalex.org/W3035139526', 'https://openalex.org/W2033235577', 'https://openalex.org/W2118587067']",1985-01-01
https://openalex.org/W2151626637,https://doi.org/10.1109/icassp.1985.1168147,Code-excited linear prediction(CELP): High-quality speech at very low bit rates,"We describe in this paper a code-excited linear predictive coder in which the optimum innovation sequence is selected from a code book of stored sequences to optimize a given fidelity criterion. Each sample of the innovation sequence is filtered sequentially through two time-varying linear recursive filters, one with a long-delay (related to pitch period) predictor in the feedback loop and the other with a short-delay predictor (related to spectral envelope) in the feedback loop. We code speech, sampled at 8 kHz, in blocks of 5-msec duration. Each block consisting of 40 samples is produced from one of 1024 possible innovation sequences. The bit rate for the innovation sequence is thus 1/4 bit per sample. We compare in this paper several different random and deterministic code books for their effectiveness in providing the optimum innovation sequence in each block. Our results indicate that a random code book has a slight speech quality advantage at low bit rates. Examples of speech produced by the above method will be played at the conference.","['https://openalex.org/W2050244349', 'https://openalex.org/W2099856211', 'https://openalex.org/W1936264760', 'https://openalex.org/W1970272279']",2005-03-23
https://openalex.org/W2131738223,https://doi.org/10.1109/mp.2006.1664069,A review of vector quantization techniques,"The fundamental principles of quantization and the two basic types of quantization techniques-scalar and vector-have been introduced. The concept of VQ, its salient features, design of code book, and advantages/disadvantages has been dealt with in detail. VQ is a data compression technique, producing a reconstruction with as small a distortion as possible. The quality of the reconstruction depends on the amount of data that is discarded. The performance of different classes of VQ techniques like structured and unstructured VQ, memory and memoryless VQ, the types of VQ under each of these categories have been discussed. This article has surveyed these to a certain extent, and much more remains if a detailed analysis is required","['https://openalex.org/W4252532137', 'https://openalex.org/W2098484510', 'https://openalex.org/W2123786525', 'https://openalex.org/W2294154796', 'https://openalex.org/W649258662', 'https://openalex.org/W1928747076', 'https://openalex.org/W1982686915', 'https://openalex.org/W1647572132']",2006-07-01
https://openalex.org/W2129913307,https://doi.org/10.1109/icc.1990.117117,Speech coding based on a multi-layer neural network,"The authors present a speech-compression scheme based on a three-layer perceptron in which the number of units in the hidden layer is reduced. Input and output layers have the same number of units in order to achieve identity mapping. Speech coding is realized by scalar or vector quantization of hidden-layer outputs. By analyzing the weighting coefficients, it can be shown that speech coding based on a three-layer neural network is speaker-independent. Transform coding is automatically based on back propagation. The relation between compression ratio and SNR (signal-to-noise ratio) is investigated. The bit allocation and optimum number of hidden-layer units necessary to realize a specific bit rate are given. According to the analysis of weighting coefficients, speech coding based on a neural network is transform coding similar to Karhunen-Loeve transformation. The characteristics of a five-layer neural network are examined. It is shown that since the five-layer neural network can realize nonlinear mapping, it is more effective than the three-layer network.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>","['https://openalex.org/W4253020087', 'https://openalex.org/W2046432185']",1990-01-01
https://openalex.org/W2972354707,https://doi.org/10.21437/interspeech.2019-1816,Cascaded Cross-Module Residual Learning Towards Lightweight End-to-End Speech Coding,"Speech codecs learn compact representations of speech signals to facilitate data transmission.Many recent deep neural network (DNN) based end-to-end speech codecs achieve low bitrates and high perceptual quality at the cost of model complexity.We propose a cross-module residual learning (CMRL) pipeline as a module carrier with each module reconstructing the residual from its preceding modules.CMRL differs from other DNN-based speech codecs, in that rather than modeling speech compression problem in a single large neural network, it optimizes a series of less-complicated modules in a two-phase training scheme.The proposed method shows better objective performance than AMR-WB and the state-of-the-art DNNbased speech codec with a similar network architecture.As an end-to-end model, it takes raw PCM signals as an input, but is also compatible with linear predictive coding (LPC), showing better subjective quality at high bitrates than AMR-WB and OPUS.The gain is achieved by using only 0.9 million trainable parameters, a significantly less complex architecture than the other DNN-based codecs in the literature.","['https://openalex.org/W2775336875', 'https://openalex.org/W1556611829', 'https://openalex.org/W4297752165', 'https://openalex.org/W1703292843', 'https://openalex.org/W2309400744', 'https://openalex.org/W2889329491', 'https://openalex.org/W2584032004', 'https://openalex.org/W2194775991', 'https://openalex.org/W2013078344', 'https://openalex.org/W2891355459', 'https://openalex.org/W2020883660', 'https://openalex.org/W2151626637', 'https://openalex.org/W2002182716', 'https://openalex.org/W2165291881', 'https://openalex.org/W2168013545', 'https://openalex.org/W1552314771', 'https://openalex.org/W1522301498', 'https://openalex.org/W1512294521', 'https://openalex.org/W2060108852', 'https://openalex.org/W2597747080', 'https://openalex.org/W2935711438', 'https://openalex.org/W2037034710', 'https://openalex.org/W2266701264', 'https://openalex.org/W2129652681', 'https://openalex.org/W2732044853', 'https://openalex.org/W2963182577', 'https://openalex.org/W1553834069', 'https://openalex.org/W3127686677', 'https://openalex.org/W2963208781', 'https://openalex.org/W2476548250']",2019-09-13
https://openalex.org/W2768814045,https://doi.org/10.1109/cvpr.2018.00652,The Perception-Distortion Tradeoff,"Image restoration algorithms are typically evaluated by some distortion measure (e.g. PSNR, SSIM, IFC, VIF) or by human opinion scores that quantify perceived perceptual quality. In this paper, we prove mathematically that distortion and perceptual quality are at odds with each other. Specifically, we study the optimal probability for correctly discriminating the outputs of an image restoration algorithm from real images. We show that as the mean distortion decreases, this probability must increase (indicating worse perceptual quality). As opposed to the common belief, this result holds true for any distortion measure, and is not only a problem of the PSNR or SSIM criteria. We also show that generative-adversarial-nets (GANs) provide a principled way to approach the perception-distortion bound. This constitutes theoretical support to their observed success in low-level vision tasks. Based on our analysis, we propose a new methodology for evaluating image restoration methods, and use it to perform an extensive comparison between recent super-resolution algorithms.","['https://openalex.org/W6718379498', 'https://openalex.org/W2963037581', 'https://openalex.org/W2004449346', 'https://openalex.org/W40692021', 'https://openalex.org/W2129644086', 'https://openalex.org/W2102166818', 'https://openalex.org/W2162692770', 'https://openalex.org/W6738494155', 'https://openalex.org/W2963420272', 'https://openalex.org/W6717434760', 'https://openalex.org/W6750095448', 'https://openalex.org/W2121927366', 'https://openalex.org/W1982471090', 'https://openalex.org/W6741832134', 'https://openalex.org/W2465552163', 'https://openalex.org/W2963470893', 'https://openalex.org/W2112796928', 'https://openalex.org/W2963372104', 'https://openalex.org/W2142731874', 'https://openalex.org/W2565312867', 'https://openalex.org/W6654304999', 'https://openalex.org/W6647225686', 'https://openalex.org/W2963917315', 'https://openalex.org/W2962793481', 'https://openalex.org/W2612063021', 'https://openalex.org/W6701655646', 'https://openalex.org/W2141983208', 'https://openalex.org/W6738696882', 'https://openalex.org/W2046119925', 'https://openalex.org/W6735913928', 'https://openalex.org/W1930824406', 'https://openalex.org/W2461158874', 'https://openalex.org/W2963073614', 'https://openalex.org/W6702130928', 'https://openalex.org/W2051596736', 'https://openalex.org/W2242218935', 'https://openalex.org/W2607041014', 'https://openalex.org/W2009272644', 'https://openalex.org/W2963186101', 'https://openalex.org/W4293404332', 'https://openalex.org/W6621378261', 'https://openalex.org/W2793833596', 'https://openalex.org/W6600294690', 'https://openalex.org/W6602211262', 'https://openalex.org/W6682443230', 'https://openalex.org/W2133665775', 'https://openalex.org/W2133251102', 'https://openalex.org/W1580389772', 'https://openalex.org/W6624640001', 'https://openalex.org/W2142884912', 'https://openalex.org/W1973207880', 'https://openalex.org/W2026653933', 'https://openalex.org/W2963800509', 'https://openalex.org/W2037133587', 'https://openalex.org/W2099111195', 'https://openalex.org/W2951997238', 'https://openalex.org/W648143168', 'https://openalex.org/W2153582625', 'https://openalex.org/W2795709826', 'https://openalex.org/W4378761606', 'https://openalex.org/W2891810238', 'https://openalex.org/W2964168764', 'https://openalex.org/W54257720', 'https://openalex.org/W1522301498', 'https://openalex.org/W2891158090', 'https://openalex.org/W4297738463', 'https://openalex.org/W2903787929', 'https://openalex.org/W2739748921', 'https://openalex.org/W1997147589', 'https://openalex.org/W2768814045', 'https://openalex.org/W1556911442', 'https://openalex.org/W4288624536', 'https://openalex.org/W3103755690', 'https://openalex.org/W2099471712', 'https://openalex.org/W4295521014', 'https://openalex.org/W1987489060', 'https://openalex.org/W2963066125', 'https://openalex.org/W2962947361', 'https://openalex.org/W935139217', 'https://openalex.org/W7682646', 'https://openalex.org/W2963181089', 'https://openalex.org/W2951523806', 'https://openalex.org/W2132181908', 'https://openalex.org/W2964121744', 'https://openalex.org/W2331128040', 'https://openalex.org/W3098418424', 'https://openalex.org/W2419501139', 'https://openalex.org/W4320013936', 'https://openalex.org/W2099470017', 'https://openalex.org/W2963373786', 'https://openalex.org/W2794224632', 'https://openalex.org/W2964060609', 'https://openalex.org/W2963449488', 'https://openalex.org/W2108661534', 'https://openalex.org/W2981613960', 'https://openalex.org/W2962879692', 'https://openalex.org/W2952773607', 'https://openalex.org/W2326925005', 'https://openalex.org/W2503339013', 'https://openalex.org/W2055309977', 'https://openalex.org/W2962785568', 'https://openalex.org/W3043547428', 'https://openalex.org/W2963312584', 'https://openalex.org/W2015015901', 'https://openalex.org/W2551161082', 'https://openalex.org/W2982041717']",2018-06-01
https://openalex.org/W2760103357,https://doi.org/10.1609/aaai.v32i1.11671,FiLM: Visual Reasoning with a General Conditioning Layer,"We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.","['https://openalex.org/W2737766105', 'https://openalex.org/W2230472587', 'https://openalex.org/W6716358881', 'https://openalex.org/W6640773114', 'https://openalex.org/W2127795553', 'https://openalex.org/W2727849499', 'https://openalex.org/W6729110096', 'https://openalex.org/W1593114658', 'https://openalex.org/W6737778391', 'https://openalex.org/W2620076854', 'https://openalex.org/W2560730294', 'https://openalex.org/W6640411583', 'https://openalex.org/W6864424756', 'https://openalex.org/W6687483927', 'https://openalex.org/W6666761814', 'https://openalex.org/W2613526370', 'https://openalex.org/W6743731764', 'https://openalex.org/W2603777577', 'https://openalex.org/W1836465849', 'https://openalex.org/W2561715562', 'https://openalex.org/W2613404084', 'https://openalex.org/W6656844882', 'https://openalex.org/W2737615485', 'https://openalex.org/W6631190155', 'https://openalex.org/W2560647685', 'https://openalex.org/W2463565445', 'https://openalex.org/W2151498684', 'https://openalex.org/W2142192571', 'https://openalex.org/W2153579005', 'https://openalex.org/W2620290674', 'https://openalex.org/W6766008348', 'https://openalex.org/W2173520492', 'https://openalex.org/W2117539524', 'https://openalex.org/W2624614404', 'https://openalex.org/W2581624817', 'https://openalex.org/W2423557781', 'https://openalex.org/W6739470913', 'https://openalex.org/W2171810632', 'https://openalex.org/W2963651499', 'https://openalex.org/W2950133940', 'https://openalex.org/W2963684088', 'https://openalex.org/W1522301498', 'https://openalex.org/W2949382160', 'https://openalex.org/W2963525668', 'https://openalex.org/W2963636093', 'https://openalex.org/W2545656684', 'https://openalex.org/W4309845474', 'https://openalex.org/W1924770834', 'https://openalex.org/W2734498959', 'https://openalex.org/W2950761309', 'https://openalex.org/W2963380480', 'https://openalex.org/W4246744641', 'https://openalex.org/W3016211260', 'https://openalex.org/W2622672190', 'https://openalex.org/W2953326374', 'https://openalex.org/W2187089797', 'https://openalex.org/W2962716332', 'https://openalex.org/W4294170691', 'https://openalex.org/W1934264538', 'https://openalex.org/W2963224792', 'https://openalex.org/W4298392976', 'https://openalex.org/W2963596039', 'https://openalex.org/W2064675550', 'https://openalex.org/W2752782242', 'https://openalex.org/W1983927101', 'https://openalex.org/W2952246170', 'https://openalex.org/W2025653905', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963954913', 'https://openalex.org/W2952339051', 'https://openalex.org/W4293718192', 'https://openalex.org/W2963280294', 'https://openalex.org/W2949117887', 'https://openalex.org/W2964118342', 'https://openalex.org/W2194775991', 'https://openalex.org/W1933349210', 'https://openalex.org/W2613904329', 'https://openalex.org/W2953054324', 'https://openalex.org/W2963907629', 'https://openalex.org/W2964121744', 'https://openalex.org/W2951697117', 'https://openalex.org/W2951619830', 'https://openalex.org/W2963143606']",2018-04-29
https://openalex.org/W2286601668,https://doi.org/10.17487/rfc7478,Web Real-Time Communication Use Cases and Requirements,This document describes web-based real-time communication use cases.Requirements on the browser functionality are derived from the use cases.This document was developed in an initial phase of the work with rather minor updates at later stages.It has not really served as a tool in deciding features or scope for the WG's efforts so far.It is being published to record the early conclusions of the WG.It will not be used as a set of rigid guidelines that specifications and implementations will be held to in the future.,"['https://openalex.org/W2289424780', 'https://openalex.org/W2199163683', 'https://openalex.org/W1796796117', 'https://openalex.org/W6691732684', 'https://openalex.org/W2255843490', 'https://openalex.org/W2238497602']",2015-03-01
https://openalex.org/W2165291881,https://doi.org/10.1109/tsa.2002.804299,The adaptive multirate wideband speech codec (AMR-WB),"This paper describes the adaptive multirate wideband (AMR-WB) speech codec selected by the Third Generation Partnership Project (3GPP) for GSM and the third generation mobile communication WCDMA system for providing wideband speech services. The AMR-WB speech codec algorithm was selected in December 2000 and the corresponding specifications were approved in March 2001. The AMR-WB codec was also selected by the International Telecommunication Union-Telecommunication Sector (ITU-T) in July 2001 in the standardization activity for wideband speech coding around 16 kb/s and was approved in January 2002 as Recommendation G.722.2. The adoption of AMR-WB by ITU-T is of significant importance since for the first time the same codec is adopted for wireless as well as wireline services. AMR-WB uses an extended audio bandwidth from 50 Hz to 7 kHz and gives superior speech quality and voice naturalness compared to existing second- and third-generation mobile communication systems. The wideband speech service provided by the AMR-WB codec will give mobile communication speech quality that also substantially exceeds (narrowband) wireline quality. The paper details AMR-WB standardization history, algorithmic description including novel techniques for efficient ACELP wideband speech coding and subjective quality performance of the codec.","['https://openalex.org/W2116059343', 'https://openalex.org/W2168098717', 'https://openalex.org/W2172216319', 'https://openalex.org/W2132802437', 'https://openalex.org/W2111393885', 'https://openalex.org/W2101986099', 'https://openalex.org/W1925345662', 'https://openalex.org/W1596775060']",2002-11-01
https://openalex.org/W2176412452,https://doi.org/10.48550/arxiv.1511.07289,Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs),"We introduce the ""exponential linear unit"" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network.","['https://openalex.org/W1677182931', 'https://openalex.org/W2136602922', 'https://openalex.org/W2951282416', 'https://openalex.org/W2964309400', 'https://openalex.org/W1520168181', 'https://openalex.org/W1408639475', 'https://openalex.org/W2150854591', 'https://openalex.org/W2189911347', 'https://openalex.org/W2164273299', 'https://openalex.org/W2137269967', 'https://openalex.org/W2020107577', 'https://openalex.org/W2151965738', 'https://openalex.org/W2061209832', 'https://openalex.org/W2064675550', 'https://openalex.org/W1598497354', 'https://openalex.org/W3106144113', 'https://openalex.org/W1877062207', 'https://openalex.org/W2041530929', 'https://openalex.org/W2142508340', 'https://openalex.org/W2133069808', 'https://openalex.org/W1579917626', 'https://openalex.org/W2104760318', 'https://openalex.org/W1915968771', 'https://openalex.org/W2570467259', 'https://openalex.org/W2152424459', 'https://openalex.org/W1929640992', 'https://openalex.org/W2950621961', 'https://openalex.org/W1799366690', 'https://openalex.org/W2200708944', 'https://openalex.org/W1992774725', 'https://openalex.org/W1811843574', 'https://openalex.org/W2114537044', 'https://openalex.org/W2133319764', 'https://openalex.org/W1932057668', 'https://openalex.org/W2137825550', 'https://openalex.org/W196761320', 'https://openalex.org/W2963606038', 'https://openalex.org/W2949117887', 'https://openalex.org/W2167729035', 'https://openalex.org/W2123045220', 'https://openalex.org/W2130801532']",2015-11-23
https://openalex.org/W1607435270,https://doi.org/10.21427/8dcc-ba52,VISQOL: The Virtual Speech Quality Objective Listener,"A model of human speech quality perception has been developed to provide an objective measure for predicting subjective quality assessments. The Virtual Speech Quality Objective Listener (ViSQOL) model is a signal based full reference metric that uses a spectro-temporal measure of similarity between a reference and a test speech signal. This paper describes the algorithm and compares the results with PESQ for common problems in VoIP: clock drift, associated time warping and jitter. The results indicate that ViSQOL is less prone to underestimation of speech quality in both scenarios than the ITU standard.","['https://openalex.org/W1411494758', 'https://openalex.org/W1976188834', 'https://openalex.org/W1556412406', 'https://openalex.org/W1961957529']",2021-02-25
https://openalex.org/W2953331651,,SampleRNN: An Unconditional End-to-End Neural Audio Generation Model,"In this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time. We show that our model, which profits from combining memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans, on three datasets of different nature. Human evaluation on the generated samples indicate that our model is preferred over competing models. We also show how each component of the model contributes to the exhibited performance.","['https://openalex.org/W2962883855', 'https://openalex.org/W2184335310', 'https://openalex.org/W2053429109', 'https://openalex.org/W2963840672', 'https://openalex.org/W2952276042', 'https://openalex.org/W2949382160', 'https://openalex.org/W1677182931', 'https://openalex.org/W2953318193', 'https://openalex.org/W2064675550', 'https://openalex.org/W1810943226', 'https://openalex.org/W1924770834', 'https://openalex.org/W1522301498', 'https://openalex.org/W2053831280', 'https://openalex.org/W2135181320', 'https://openalex.org/W2107789863', 'https://openalex.org/W2284050935', 'https://openalex.org/W2581073931', 'https://openalex.org/W2273818272']",2016-12-22
https://openalex.org/W1885680957,https://doi.org/10.17487/rfc6716,Definition of the Opus Audio Codec,"This document defines the Opus interactive speech and audio codec. Opus is designed to handle a wide range of interactive audio applications, including Voice over IP, videoconferencing, in-game chat, and even live, distributed music performances. It scales from low bitrate narrowband speech at 6 kbit/s to very high quality stereo music at 510 kbit/s. Opus uses both Linear Prediction (LP) and the Modified Discrete Cosine Transform (MDCT) to achieve good compression of both speech and music. Status of This Memo This is an Internet Standards Track document. This document is a product of the Internet Engineering Task Force (IETF). It represents the consensus of the IETF community. It has received public review and has been approved for publication by the",[],2012-09-01
https://openalex.org/W3046970875,https://doi.org/10.48550/arxiv.2008.01160,A Spectral Energy Distance for Parallel Speech Synthesis,"Speech synthesis is an important practical generative modeling problem that has seen great progress over the last few years, with likelihood-based autoregressive neural models now outperforming traditional concatenative systems. A downside of such autoregressive models is that they require executing tens of thousands of sequential operations per second of generated audio, making them ill-suited for deployment on specialized deep learning hardware. Here, we propose a new learning method that allows us to train highly parallel models of speech, without requiring access to an analytical likelihood function. Our approach is based on a generalized energy distance between the distributions of the generated and real audio. This spectral energy distance is a proper scoring rule with respect to the distribution over magnitude-spectrograms of the generated waveform audio and offers statistical consistency guarantees. The distance can be calculated from minibatches without bias, and does not involve adversarial learning, yielding a stable and consistent method for training implicit generative models. Empirically, we achieve state-of-the-art generation quality among implicit generative models, as judged by the recently-proposed cFDSD metric. When combining our method with adversarial techniques, we also improve upon the recently-proposed GAN-TTS model in terms of Mean Opinion Score as judged by trained human evaluators.","['https://openalex.org/W2194775991', 'https://openalex.org/W2193413348', 'https://openalex.org/W3021164770', 'https://openalex.org/W2949382160', 'https://openalex.org/W2893749619', 'https://openalex.org/W2963989027', 'https://openalex.org/W2212660284', 'https://openalex.org/W3000389243', 'https://openalex.org/W2993118648', 'https://openalex.org/W2409550820', 'https://openalex.org/W1522301498', 'https://openalex.org/W2943554574', 'https://openalex.org/W2183341477', 'https://openalex.org/W2963300588', 'https://openalex.org/W3015338123', 'https://openalex.org/W2587284713', 'https://openalex.org/W1779010541', 'https://openalex.org/W2785678896', 'https://openalex.org/W2949995983', 'https://openalex.org/W2808235392', 'https://openalex.org/W2975414524', 'https://openalex.org/W2099471712', 'https://openalex.org/W2963139417', 'https://openalex.org/W2804704270', 'https://openalex.org/W2970006822', 'https://openalex.org/W1992272902', 'https://openalex.org/W2769810959', 'https://openalex.org/W2962878605', 'https://openalex.org/W2963981733', 'https://openalex.org/W2883853252', 'https://openalex.org/W2899882692', 'https://openalex.org/W2125930537', 'https://openalex.org/W1487641199', 'https://openalex.org/W2963175743', 'https://openalex.org/W2025720061', 'https://openalex.org/W2788851830']",2020-08-03
https://openalex.org/W2127218421,,Some methods for classification and analysis of multivariate observations,"This paper describes a number of applications of the 'k-means', a procedure for classifying a random sample of points in E sub N. The procedure consists of starting with k groups which each consist of a single random point, and thereafter adding the points one after another to the group whose mean each point is nearest. After a point is added to a group, the mean of that group is adjusted so as to take account of the new point. Thus at each stage there are in fact k means, one for each group. After the sample is processed in this way, the points are classified on the basis of nearness to the final means. The portions which result tend to be fficient in the sense of having low within class variance. Applications are suggested for the problems of non-linear prediction, efficient communication, non-parametric tests of independence, similarity grouping, and automatic file construction. The extension of the methods to general metric spaces is indicated. (Author)","['https://openalex.org/W2118942683', 'https://openalex.org/W642234484', 'https://openalex.org/W1489608363', 'https://openalex.org/W1666623353', 'https://openalex.org/W2016381774', 'https://openalex.org/W2009484157', 'https://openalex.org/W2799137445', 'https://openalex.org/W2065902423', 'https://openalex.org/W1997648776']",1967-01-01
https://openalex.org/W3130248090,https://doi.org/10.1109/icassp39728.2021.9415120,Generative Speech Coding with Predictive Variance Regularization,"The recent emergence of machine-learning based generative models for speech suggests a significant reduction in bit rate for speech codecs is possible. However, the performance of generative models deteriorates significantly with the distortions present in real-world input signals. We argue that this deterioration is due to the sensitivity of the maximum likelihood criterion to outliers and the ineffectiveness of modeling a sum of independent signals with a single autoregressive model. We introduce predictive-variance regularization to reduce the sensitivity to outliers, resulting in a significant increase in performance. We show that noise reduction to remove unwanted signals can significantly increase performance. We provide extensive subjective performance evaluations that show that our system based on generative modeling provides state-of-the-art coding performance at 3 kb/s for real-world speech signals at reasonable computational complexity.","['https://openalex.org/W1481955708', 'https://openalex.org/W2757519008', 'https://openalex.org/W2972519044', 'https://openalex.org/W3016098186', 'https://openalex.org/W3016003977', 'https://openalex.org/W160896187', 'https://openalex.org/W6717434760', 'https://openalex.org/W6779669310', 'https://openalex.org/W6629354409', 'https://openalex.org/W6737965738', 'https://openalex.org/W6756159577', 'https://openalex.org/W6745148473', 'https://openalex.org/W2963300588', 'https://openalex.org/W6746278845', 'https://openalex.org/W6843673214', 'https://openalex.org/W6758675244', 'https://openalex.org/W6755257315', 'https://openalex.org/W2963208781', 'https://openalex.org/W2775336875', 'https://openalex.org/W6748409065', 'https://openalex.org/W2935711438', 'https://openalex.org/W2952218014', 'https://openalex.org/W3015654783', 'https://openalex.org/W6640212811', 'https://openalex.org/W6771467084', 'https://openalex.org/W1494198834', 'https://openalex.org/W1924770834', 'https://openalex.org/W2099471712', 'https://openalex.org/W2995929068', 'https://openalex.org/W2770119437', 'https://openalex.org/W2963782041', 'https://openalex.org/W2949382160', 'https://openalex.org/W2962892300', 'https://openalex.org/W1487641199', 'https://openalex.org/W2952344559', 'https://openalex.org/W2910577860', 'https://openalex.org/W2899882692', 'https://openalex.org/W2788851830', 'https://openalex.org/W2485688913', 'https://openalex.org/W2963800509', 'https://openalex.org/W2950299304']",2021-05-13
https://openalex.org/W2810843531,https://doi.org/10.21437/interspeech.2019-1924,Speech Denoising with Deep Feature Losses,"We present an end-to-end deep learning approach to denoising speech signals by processing the raw waveform directly. Given input audio containing speech corrupted by an additive background signal, the system aims to produce a processed signal that contains only the speech content. Recent approaches have shown promising results using various deep network architectures. In this paper, we propose to train a fully-convolutional context aggregation network using a deep feature loss. That loss is based on comparing the internal feature activations in a different network, trained for acoustic environment detection and domestic audio tagging. Our approach outperforms the state-of-the-art in objective speech quality metrics and in large-scale perceptual experiments with human listeners. It also outperforms an identical network trained using traditional regression losses. The advantage of the new approach is particularly pronounced for the hardest data with the most intrusive background noise, for which denoising is most needed and most challenging.","['https://openalex.org/W2963341071', 'https://openalex.org/W2964121744', 'https://openalex.org/W2566935005', 'https://openalex.org/W2044893557', 'https://openalex.org/W2949382160', 'https://openalex.org/W2162514423', 'https://openalex.org/W2678916739', 'https://openalex.org/W2095072097', 'https://openalex.org/W1546892833', 'https://openalex.org/W1836465849', 'https://openalex.org/W2138939691', 'https://openalex.org/W2775302872', 'https://openalex.org/W2304609584', 'https://openalex.org/W2046869671', 'https://openalex.org/W2963103134', 'https://openalex.org/W2405774341', 'https://openalex.org/W2962835968', 'https://openalex.org/W1970308149', 'https://openalex.org/W2408467190', 'https://openalex.org/W2594809597', 'https://openalex.org/W2130813098', 'https://openalex.org/W2141411743', 'https://openalex.org/W2394461504', 'https://openalex.org/W1516630152', 'https://openalex.org/W1482149378', 'https://openalex.org/W2176625348', 'https://openalex.org/W2962785568', 'https://openalex.org/W2963522749', 'https://openalex.org/W2963840672', 'https://openalex.org/W2747161606', 'https://openalex.org/W2609317876', 'https://openalex.org/W2775794021', 'https://openalex.org/W2117539524', 'https://openalex.org/W1533861849', 'https://openalex.org/W2013419672']",2019-09-13
https://openalex.org/W2889871534,https://doi.org/10.1109/icassp.2018.8462619,Sequence Distillation for Purely Sequence Trained Acoustic Models,"This paper presents our exploration into teacher-student (TS) training for acoustic models (AMs) based on the lattice-free maximum mutual information technique. Whereas most previous studies of TS training used a frame-level distance between teacher and student models' distributions, we propose using the sequence-level temper-atured Kullback-Leibler divergence as a metric for TS training. In our experiment on the AMI meeting corpus, we prepared a strong teacher model consisting of a convolutional neural network, time delay neural network, and long short-term memory, which had 47.7M parameters and achieved a state-of-the-art word error rate (WER) of 18.05%. Whereas the small student AM (10.8M params. and 19.72% WER) trained by a frame-level TS training was able to fill only 43% of the WER gap between teacher and student AMs, the student AM trained by the proposed method achieved a 18.23% WER, filling 89% of the WER gap from the teacher AM. We also show that the frame-level TS training sometimes even degrades the performance of the student model whereas the proposed method consistently improved the accuracy.","['https://openalex.org/W2508418541', 'https://openalex.org/W2963864497', 'https://openalex.org/W2711861986', 'https://openalex.org/W1987238397', 'https://openalex.org/W2131342762', 'https://openalex.org/W2514741789', 'https://openalex.org/W2963736842', 'https://openalex.org/W2507699225', 'https://openalex.org/W2064675550', 'https://openalex.org/W6712930963', 'https://openalex.org/W2117671523', 'https://openalex.org/W1600744878', 'https://openalex.org/W6743283956', 'https://openalex.org/W6680549991', 'https://openalex.org/W6712847557', 'https://openalex.org/W2114016253', 'https://openalex.org/W6631362777', 'https://openalex.org/W6678809451', 'https://openalex.org/W6739992741', 'https://openalex.org/W6713762819', 'https://openalex.org/W6638667902', 'https://openalex.org/W2079623482', 'https://openalex.org/W2125336414', 'https://openalex.org/W1836465849', 'https://openalex.org/W1524333225', 'https://openalex.org/W2749475572', 'https://openalex.org/W2402146185', 'https://openalex.org/W2533523411', 'https://openalex.org/W1821462560', 'https://openalex.org/W2696967604', 'https://openalex.org/W2136933783', 'https://openalex.org/W2407080277', 'https://openalex.org/W2963266252', 'https://openalex.org/W2402040300']",2018-04-01
https://openalex.org/W2911629330,https://doi.org/10.1109/slt.2018.8639635,A Teacher-Student Learning Approach for Unsupervised Domain Adaptation of Sequence-Trained ASR Models,"Teacher-student (T-S) learning is a transfer learning approach, where a teacher network is used to ""teach"" a student network to make the same predictions as the teacher. Originally formulated for model compression, this approach has also been used for domain adaptation, and is particularly effective when parallel data is available in source and target domains. The standard approach uses a frame-level objective of minimizing the KL divergence between the frame-level posteriors of the teacher and student networks. However, for sequence-trained models for speech recognition, it is more appropriate to train the student to mimic the sequence-level posterior of the teacher network. In this work, we compare this sequence-level KL divergence objective with another semi-supervised sequence-training method, namely the lattice-free MMI, for unsupervised domain adaptation. We investigate the approaches in multiple scenarios including adapting from clean to noisy speech, bandwidth mismatch and channel mismatch.","['https://openalex.org/W6631362777', 'https://openalex.org/W6713762819', 'https://openalex.org/W2696967604', 'https://openalex.org/W6603931906', 'https://openalex.org/W1591607137', 'https://openalex.org/W6633847657', 'https://openalex.org/W2787290364', 'https://openalex.org/W2294370754', 'https://openalex.org/W6679909955', 'https://openalex.org/W6638523607', 'https://openalex.org/W1989549063', 'https://openalex.org/W6743562081', 'https://openalex.org/W6725262976', 'https://openalex.org/W2584515623', 'https://openalex.org/W2510616059', 'https://openalex.org/W2125838338', 'https://openalex.org/W6713570806', 'https://openalex.org/W2963522845', 'https://openalex.org/W2150769028', 'https://openalex.org/W6697274609', 'https://openalex.org/W2127141656', 'https://openalex.org/W2289912446', 'https://openalex.org/W1877570817', 'https://openalex.org/W2786459654', 'https://openalex.org/W6726109732', 'https://openalex.org/W1986614398', 'https://openalex.org/W6750880455', 'https://openalex.org/W2165698076', 'https://openalex.org/W6748131473', 'https://openalex.org/W2117671523', 'https://openalex.org/W2889871534', 'https://openalex.org/W6696934422', 'https://openalex.org/W6712930963', 'https://openalex.org/W2164240571', 'https://openalex.org/W2747135936', 'https://openalex.org/W1569447338', 'https://openalex.org/W2293634267', 'https://openalex.org/W1821462560', 'https://openalex.org/W2514741789', 'https://openalex.org/W2134797427', 'https://openalex.org/W2786234940', 'https://openalex.org/W2507699225', 'https://openalex.org/W2295582178', 'https://openalex.org/W2616180702', 'https://openalex.org/W2407080277', 'https://openalex.org/W2962894366', 'https://openalex.org/W2802248956', 'https://openalex.org/W97072897', 'https://openalex.org/W2404568753', 'https://openalex.org/W2402146185', 'https://openalex.org/W1524333225']",2018-12-01
https://openalex.org/W3008008574,https://doi.org/10.1109/asru46091.2019.9003776,Domain Adaptation via Teacher-Student Learning for End-to-End Speech Recognition,"Teacher-student (T/S) has shown to be effective for domain adaptation of deep neural network acoustic models in hybrid speech recognition systems. In this work, we extend the T/S learning to large-scale unsupervised domain adaptation of an attention-based end-to-end (E2E) model through two levels of knowledge transfer: teacher's token posteriors as soft labels and one-best predictions as decoder guidance. To further improve T/S learning with the help of ground-truth labels, we propose adaptive T/S (AT/S) learning. Instead of conditionally choosing from either the teacher's soft token posteriors or the one-hot ground-truth label, in AT/S, the student always learns from both the teacher and the ground truth with a pair of adaptive weights assigned to the soft and one-hot labels quantifying the confidence on each of the knowledge sources. The confidence scores are dynamically estimated at each decoder step as a function of the soft and one-hot labels. With 3400 hours parallel close-talk and far-field Microsoft Cortana data for domain adaptation, T/S and AT/S achieves 6.3% and 10.3% relative word error rate improvement over a strong E2E model trained with the same amount of far-field data.","['https://openalex.org/W2636483419', 'https://openalex.org/W3006806348', 'https://openalex.org/W2963970535', 'https://openalex.org/W2962824709', 'https://openalex.org/W6787533431', 'https://openalex.org/W2973138437', 'https://openalex.org/W2972621414', 'https://openalex.org/W2888858245', 'https://openalex.org/W6621543089', 'https://openalex.org/W2889500840', 'https://openalex.org/W2795867901', 'https://openalex.org/W2510867321', 'https://openalex.org/W2938486422', 'https://openalex.org/W2796339975', 'https://openalex.org/W6712847557', 'https://openalex.org/W6743562081', 'https://openalex.org/W2711861986', 'https://openalex.org/W6623517193', 'https://openalex.org/W2889129739', 'https://openalex.org/W2327501763', 'https://openalex.org/W2962826786', 'https://openalex.org/W6745924425', 'https://openalex.org/W2587088898', 'https://openalex.org/W6675365184', 'https://openalex.org/W2939164678', 'https://openalex.org/W2127141656', 'https://openalex.org/W2963523217', 'https://openalex.org/W2680270903', 'https://openalex.org/W2936252403', 'https://openalex.org/W2289394825', 'https://openalex.org/W6674330103', 'https://openalex.org/W2963681135', 'https://openalex.org/W2963736842', 'https://openalex.org/W2507699225', 'https://openalex.org/W3112742522', 'https://openalex.org/W1821462560', 'https://openalex.org/W854541894', 'https://openalex.org/W2095705004', 'https://openalex.org/W648786980', 'https://openalex.org/W2577366047', 'https://openalex.org/W1828163288', 'https://openalex.org/W2102113734', 'https://openalex.org/W2402040300', 'https://openalex.org/W2963266252', 'https://openalex.org/W2964308564', 'https://openalex.org/W1924770834', 'https://openalex.org/W2899771611', 'https://openalex.org/W2964199361', 'https://openalex.org/W2606833507', 'https://openalex.org/W2133564696', 'https://openalex.org/W2962894366', 'https://openalex.org/W2769025471']",2019-12-01
https://openalex.org/W4283700324,https://doi.org/10.21437/interspeech.2022-10340,"Pruned RNN-T for fast, memory-eﬀicient ASR training","The RNN-Transducer (RNN-T) framework for speech recognition has been growing in popularity, particularly for deployed real-time ASR systems, because it combines high accuracy with naturally streaming recognition.One of the drawbacks of RNN-T is that its loss function is relatively slow to compute, and can use a lot of memory.Excessive GPU memory usage can make it impractical to use RNN-T loss in cases where the vocabulary size is large: for example, for Chinese character-based ASR.We introduce a method for faster and more memoryefficient RNN-T loss computation.We first obtain pruning bounds for the RNN-T recursion using a simple joiner network that is linear in the encoder and decoder embeddings; we can evaluate this without using much memory.We then use those pruning bounds to evaluate the full, non-linear joiner network.The code is open-sourced and publicly available.","['https://openalex.org/W2407080277', 'https://openalex.org/W2936774411', 'https://openalex.org/W3015686596', 'https://openalex.org/W3007227084', 'https://openalex.org/W3146505093', 'https://openalex.org/W1828163288', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962784628', 'https://openalex.org/W2962760690', 'https://openalex.org/W4290709727', 'https://openalex.org/W3167533889', 'https://openalex.org/W4385245566', 'https://openalex.org/W3097777922', 'https://openalex.org/W2963250244']",2022-09-16
https://openalex.org/W1970491336,https://doi.org/10.1109/83.480761,Advances in residual vector quantization: a review,"Advances in residual vector quantization (RVQ) are surveyed. Definitions of joint encoder optimality and joint decoder optimality are discussed. Design techniques for RVQs with large numbers of stages and generally different encoder and decoder codebooks are elaborated and extended. Fixed-rate RVQs, and variable-rate RVQs that employ entropy coding are examined. Predictive and finite state RVQs designed and integrated into neural-network based source coding structures are revisited. Successive approximation RVQs that achieve embedded and refinable coding are reviewed. A new type of successive approximation RVQ that varies the instantaneous block rate by using different numbers of stages on different blocks is introduced and applied to image waveforms, and a scalar version of the new residual quantizer is applied to image subbands in an embedded wavelet transform coding system.","['https://openalex.org/W2134729864', 'https://openalex.org/W2159533932', 'https://openalex.org/W2091652113', 'https://openalex.org/W2134383396', 'https://openalex.org/W2145404061', 'https://openalex.org/W2037453823', 'https://openalex.org/W2396552185', 'https://openalex.org/W2150593711', 'https://openalex.org/W1563107752', 'https://openalex.org/W2159916216', 'https://openalex.org/W2110337829', 'https://openalex.org/W2156333391', 'https://openalex.org/W2168893988', 'https://openalex.org/W2161568319', 'https://openalex.org/W1977272172', 'https://openalex.org/W2029268356', 'https://openalex.org/W2064274999', 'https://openalex.org/W2103359969', 'https://openalex.org/W2120916963', 'https://openalex.org/W2171578374', 'https://openalex.org/W2200781400', 'https://openalex.org/W2142642923', 'https://openalex.org/W2142228262', 'https://openalex.org/W2140550235', 'https://openalex.org/W2109910868', 'https://openalex.org/W2136876682', 'https://openalex.org/W2152554431', 'https://openalex.org/W2157190781', 'https://openalex.org/W1547975088', 'https://openalex.org/W2150970052', 'https://openalex.org/W2106294696', 'https://openalex.org/W2144368692', 'https://openalex.org/W1984034733', 'https://openalex.org/W1999835085', 'https://openalex.org/W2079893372', 'https://openalex.org/W2044002522', 'https://openalex.org/W2129935012', 'https://openalex.org/W6712407302', 'https://openalex.org/W2128142375', 'https://openalex.org/W1700083040', 'https://openalex.org/W2109808436', 'https://openalex.org/W4293259454', 'https://openalex.org/W2152797116', 'https://openalex.org/W2154899385', 'https://openalex.org/W2119756459', 'https://openalex.org/W2149109313', 'https://openalex.org/W2022247568', 'https://openalex.org/W2171256652', 'https://openalex.org/W2168523255', 'https://openalex.org/W2092458184', 'https://openalex.org/W2099456445', 'https://openalex.org/W2023184754', 'https://openalex.org/W2131578006', 'https://openalex.org/W1974878801', 'https://openalex.org/W2165656938', 'https://openalex.org/W2098883298', 'https://openalex.org/W2089538811', 'https://openalex.org/W2110480328', 'https://openalex.org/W2128511113', 'https://openalex.org/W2055533655', 'https://openalex.org/W1722424139', 'https://openalex.org/W1835491921', 'https://openalex.org/W2060625600', 'https://openalex.org/W2119726234', 'https://openalex.org/W2126330048', 'https://openalex.org/W2053691921', 'https://openalex.org/W1998107961', 'https://openalex.org/W2050880896', 'https://openalex.org/W2148929109', 'https://openalex.org/W1523863220', 'https://openalex.org/W2074225934', 'https://openalex.org/W2157376694', 'https://openalex.org/W2100478772', 'https://openalex.org/W2162660208', 'https://openalex.org/W2099586185', 'https://openalex.org/W2111033417', 'https://openalex.org/W2069797735', 'https://openalex.org/W1988188433', 'https://openalex.org/W2101148242', 'https://openalex.org/W2119816232', 'https://openalex.org/W2151777012', 'https://openalex.org/W1521513304', 'https://openalex.org/W2098929365', 'https://openalex.org/W2061040605', 'https://openalex.org/W2102344930', 'https://openalex.org/W1634005169', 'https://openalex.org/W2148456673', 'https://openalex.org/W4252713891', 'https://openalex.org/W2113398684', 'https://openalex.org/W2993383518', 'https://openalex.org/W2102745783', 'https://openalex.org/W1516205115', 'https://openalex.org/W1653259009', 'https://openalex.org/W2168782982', 'https://openalex.org/W2162794014', 'https://openalex.org/W1790039671', 'https://openalex.org/W2148174726', 'https://openalex.org/W2005472240', 'https://openalex.org/W2145544048', 'https://openalex.org/W2113427115', 'https://openalex.org/W1981451239', 'https://openalex.org/W2121017490', 'https://openalex.org/W2066499500', 'https://openalex.org/W2162256416', 'https://openalex.org/W2152027499', 'https://openalex.org/W4244017338', 'https://openalex.org/W2002182716', 'https://openalex.org/W2123389845', 'https://openalex.org/W2156801811', 'https://openalex.org/W2051817931', 'https://openalex.org/W2151252184', 'https://openalex.org/W145268409', 'https://openalex.org/W4255372538', 'https://openalex.org/W2137691072', 'https://openalex.org/W2101460686', 'https://openalex.org/W1926489860', 'https://openalex.org/W2158643165', 'https://openalex.org/W1895117632', 'https://openalex.org/W1986900179', 'https://openalex.org/W2130305844', 'https://openalex.org/W2118134781', 'https://openalex.org/W2115082428', 'https://openalex.org/W1652299716', 'https://openalex.org/W1781569566', 'https://openalex.org/W1923860713', 'https://openalex.org/W2130836327', 'https://openalex.org/W2121004869', 'https://openalex.org/W2150190146', 'https://openalex.org/W1576111487', 'https://openalex.org/W2128242232', 'https://openalex.org/W2014358406', 'https://openalex.org/W1973209375', 'https://openalex.org/W1844213330', 'https://openalex.org/W1995990042', 'https://openalex.org/W1995875735', 'https://openalex.org/W2142276208', 'https://openalex.org/W114962907', 'https://openalex.org/W2397786702', 'https://openalex.org/W2913399920', 'https://openalex.org/W1580707211', 'https://openalex.org/W312073284', 'https://openalex.org/W2279817180', 'https://openalex.org/W2163899311', 'https://openalex.org/W2135479785']",1996-01-01
https://openalex.org/W2129935012,https://doi.org/10.1109/18.212286,Vector quantizers with direct sum codebooks,"The use of direct sum codebooks to minimize the memory requirements of vector quantizers is investigated. Assuming arbitrary fixed partitions, necessary conditions for minimum distortion codebooks are derived, first for scalar codebooks, assuming mean-squared error distortion, and then for vector codebooks and a broader class of distortion measures. An iterative procedure is described for designing locally optimal direct sum codebooks. Both optimal and computationally efficient suboptimal encoding schemes are considered. It is shown that although an optimal encoding can be implemented by a sequential encoder, the complexity of implementing optimal stagewise partitions generally exceeds the complexity of an exhaustive search of the direct sum codebook. It is also shown that sequential nearest-neighbor encoders can be extremely inefficient. The M-search method is explored as one method of improving the effectiveness of suboptimal sequential encoders. Representative results for simulated direct sum quantizers are presented.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>","['https://openalex.org/W2109808436', 'https://openalex.org/W2134383396', 'https://openalex.org/W1984034733', 'https://openalex.org/W2142228262', 'https://openalex.org/W145268409', 'https://openalex.org/W2154899385', 'https://openalex.org/W2145404061', 'https://openalex.org/W2039164018', 'https://openalex.org/W1700083040', 'https://openalex.org/W2054128110', 'https://openalex.org/W2002182716', 'https://openalex.org/W1995875735', 'https://openalex.org/W2150593711', 'https://openalex.org/W2135469284', 'https://openalex.org/W2265587988']",1993-03-01
https://openalex.org/W3203453034,https://doi.org/10.1109/icassp43922.2022.9746168,Knowledge Distillation for Neural Transducers from Large Self-Supervised Pre-Trained Models,"Self-supervised pre-training is an effective approach to leveraging a large amount of unlabelled data to reduce word error rates (WERs) of automatic speech recognition (ASR) systems. Since it is impractical to use large pre-trained models for many real-world ASR applications, it is desirable to have a much smaller model while retaining the performance of the pre-trained model. In this paper, we propose a simple knowledge distillation (KD) loss function for neural transducers that focuses on the one-best path in the output probability lattice under both streaming and non-streaming setups, which allows a small student model to approach the performance of the large pre-trained teacher model. Experiments on the LibriSpeech dataset show that despite being 10 times smaller than the teacher model, the proposed loss results in relative WER reductions (WERRs) of 11.5% and 6.8% on the test-other set for non-streaming and streaming student models compared to the baseline transducers trained without KD using the labelled 100-hour clean data. With an additional 860 hours of unlabelled data for KD, the WERRs increase to 48.2% and 38.5% for non-streaming and streaming students. If language model shallow fusion is used for producing distillation targets, a further improvement in the student model is observed.","['https://openalex.org/W2963250244', 'https://openalex.org/W2963240019', 'https://openalex.org/W2962826786', 'https://openalex.org/W3096518646', 'https://openalex.org/W2936774411', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962780374', 'https://openalex.org/W3198039885', 'https://openalex.org/W3160628828', 'https://openalex.org/W2402040300', 'https://openalex.org/W2507699225', 'https://openalex.org/W2892008152', 'https://openalex.org/W2913178639', 'https://openalex.org/W2936993002', 'https://openalex.org/W3046667470', 'https://openalex.org/W2889129739', 'https://openalex.org/W6638749077', 'https://openalex.org/W3016010032', 'https://openalex.org/W6780218876', 'https://openalex.org/W2127141656', 'https://openalex.org/W6769196770', 'https://openalex.org/W2294543795', 'https://openalex.org/W6770245836', 'https://openalex.org/W6792304734', 'https://openalex.org/W3163062345', 'https://openalex.org/W2312434537', 'https://openalex.org/W6778883912', 'https://openalex.org/W6638523607', 'https://openalex.org/W6755207826', 'https://openalex.org/W6747398299', 'https://openalex.org/W3162649911', 'https://openalex.org/W2962760690', 'https://openalex.org/W3097777922', 'https://openalex.org/W3198715852', 'https://openalex.org/W6784614252', 'https://openalex.org/W6769238691', 'https://openalex.org/W2987019345', 'https://openalex.org/W3156828761', 'https://openalex.org/W1828163288', 'https://openalex.org/W3098903812', 'https://openalex.org/W3099782249', 'https://openalex.org/W2963414781', 'https://openalex.org/W4292779060', 'https://openalex.org/W4206375145', 'https://openalex.org/W2979476256', 'https://openalex.org/W1821462560', 'https://openalex.org/W2996383576', 'https://openalex.org/W2896457183', 'https://openalex.org/W3093579165', 'https://openalex.org/W2952533036', 'https://openalex.org/W4285719527', 'https://openalex.org/W3036601975', 'https://openalex.org/W2981991061', 'https://openalex.org/W2963341956']",2022-04-27
https://openalex.org/W2156333391,https://doi.org/10.1109/dcc.1995.515515,Embedded wavelet zerotree coding with direct sum quantization structures,One of the more effective data compression systems that has been recently proposed is the relatively simple embedded wavelet image coder developed by J.M. Shapiro (1994). Two key components of Shapiro's system are the use of zerotrees to keep track of insignificant subband coefficients and progressive transmission of successive bit planes of significant coefficients. Shapiro's quantization mechanism is the use of scaled successive approximation uniform scalar quantizers. This paper investigates ways of improving the performance of embedded wavelet coders with the use of optimized successive approximation direct sum quantization structures.,"['https://openalex.org/W1634005169', 'https://openalex.org/W2157376694', 'https://openalex.org/W2129935012', 'https://openalex.org/W2101460686', 'https://openalex.org/W2159916216', 'https://openalex.org/W2151252184', 'https://openalex.org/W2111033417', 'https://openalex.org/W2053691921', 'https://openalex.org/W2608496505']",2002-11-19
https://openalex.org/W2219249508,https://doi.org/10.48550/arxiv.1510.08484,"MUSAN: A Music, Speech, and Noise Corpus","This report introduces a new corpus of music, speech, and noise. This dataset is suitable for training models for voice activity detection (VAD) and music/speech discrimination. Our corpus is released under a flexible Creative Commons license. The dataset consists of music from several genres, speech from twelve languages, and a wide assortment of technical and non-technical noises. We demonstrate use of this corpus for music/speech discrimination on Broadcast news and VAD for speaker identification.","['https://openalex.org/W1556219185', 'https://openalex.org/W2290689761', 'https://openalex.org/W1980993072', 'https://openalex.org/W1524333225', 'https://openalex.org/W2942177450']",2015-10-28
https://openalex.org/W3170863103,https://doi.org/10.48550/arxiv.2106.08254,BEiT: BERT Pre-Training of Image Transformers,"We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first ""tokenize"" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8%) with the same setup. Moreover, large-size BEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2%). The code and pretrained models are available at https://aka.ms/beit.","['https://openalex.org/W3011411500', 'https://openalex.org/W3035524453', 'https://openalex.org/W3095121901', 'https://openalex.org/W2971074500', 'https://openalex.org/W2962784628', 'https://openalex.org/W2964121744', 'https://openalex.org/W2547875792', 'https://openalex.org/W2887997457', 'https://openalex.org/W2965373594', 'https://openalex.org/W3145450063', 'https://openalex.org/W2995181141', 'https://openalex.org/W3172942063', 'https://openalex.org/W2971155163', 'https://openalex.org/W3129576130', 'https://openalex.org/W3034255912', 'https://openalex.org/W2117539524', 'https://openalex.org/W3120857301', 'https://openalex.org/W3082274269', 'https://openalex.org/W2798991696', 'https://openalex.org/W2962742544', 'https://openalex.org/W2883725317', 'https://openalex.org/W2507296351', 'https://openalex.org/W3122325173', 'https://openalex.org/W3093517588', 'https://openalex.org/W3160566314', 'https://openalex.org/W2963341956', 'https://openalex.org/W3020482686', 'https://openalex.org/W2331143823', 'https://openalex.org/W3118608800', 'https://openalex.org/W2842511635', 'https://openalex.org/W1959608418', 'https://openalex.org/W3005680577', 'https://openalex.org/W3159481202', 'https://openalex.org/W3156665996', 'https://openalex.org/W3034445277', 'https://openalex.org/W2548228487', 'https://openalex.org/W2970049541', 'https://openalex.org/W1861492603', 'https://openalex.org/W2326925005', 'https://openalex.org/W2971274815', 'https://openalex.org/W2321533354', 'https://openalex.org/W2970597249', 'https://openalex.org/W3171975879', 'https://openalex.org/W3116489684', 'https://openalex.org/W3094502228', 'https://openalex.org/W3101821705', 'https://openalex.org/W3009561768', 'https://openalex.org/W2963799213', 'https://openalex.org/W3107668149', 'https://openalex.org/W3035390927', 'https://openalex.org/W2948433173', 'https://openalex.org/W3113747735']",2021-06-15
https://openalex.org/W1821462560,https://doi.org/10.48550/arxiv.1503.02531,Distilling the Knowledge in a Neural Network,"A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.","['https://openalex.org/W2163605009', 'https://openalex.org/W1534477342', 'https://openalex.org/W2150884987', 'https://openalex.org/W1904365287', 'https://openalex.org/W2160815625', 'https://openalex.org/W2294370754', 'https://openalex.org/W2095705004', 'https://openalex.org/W2402040300', 'https://openalex.org/W2168231600']",2015-03-09
https://openalex.org/W3111562797,https://doi.org/10.48550/arxiv.2012.05481,Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition,"In this paper, we present a novel two-pass approach to unify streaming and non-streaming end-to-end (E2E) speech recognition in a single model. Our model adopts the hybrid CTC/attention architecture, in which the conformer layers in the encoder are modified. We propose a dynamic chunk-based attention strategy to allow arbitrary right context length. At inference time, the CTC decoder generates n-best hypotheses in a streaming way. The inference latency could be easily controlled by only changing the chunk size. The CTC hypotheses are then rescored by the attention decoder to get the final result. This efficient rescoring process causes very little sentence-level latency. Our experiments on the open 170-hour AISHELL-1 dataset show that, the proposed method can unify the streaming and non-streaming model simply and efficiently. On the AISHELL-1 test set, our unified model achieves 5.60% relative character error rate (CER) reduction in non-streaming ASR compared to a standard non-streaming transformer. The same model achieves 5.42% CER with 640ms latency in a streaming ASR system.","['https://openalex.org/W3026287411', 'https://openalex.org/W2963414781', 'https://openalex.org/W2970611152', 'https://openalex.org/W2127141656', 'https://openalex.org/W2143612262', 'https://openalex.org/W2750499125', 'https://openalex.org/W2526425061', 'https://openalex.org/W2773781902', 'https://openalex.org/W2951064684', 'https://openalex.org/W1828163288', 'https://openalex.org/W1586532344', 'https://openalex.org/W2193413348', 'https://openalex.org/W2963403868', 'https://openalex.org/W3015315932', 'https://openalex.org/W2963242190', 'https://openalex.org/W3007227084', 'https://openalex.org/W1855892484', 'https://openalex.org/W3015671919', 'https://openalex.org/W3025165719', 'https://openalex.org/W3093346109', 'https://openalex.org/W3015194534', 'https://openalex.org/W3128478537', 'https://openalex.org/W3033604713', 'https://openalex.org/W3092122846', 'https://openalex.org/W2936774411', 'https://openalex.org/W854541894']",2020-12-10
https://openalex.org/W2142625445,https://doi.org/10.1145/2339530.2339751,Large-scale learning of word relatedness with constraints,"Prior work on computing semantic relatedness of words focused on representing their meaning in isolation, effectively disregarding inter-word affinities. We propose a large-scale data mining approach to learning word-word relatedness, where known pairs of related words impose constraints on the learning process. We learn for each word a low-dimensional representation, which strives to maximize the likelihood of a word given the contexts in which it appears. Our method, called CLEAR, is shown to significantly outperform previously published approaches. The proposed method is based on first principles, and is generic enough to exploit diverse types of text corpora, while having the flexibility to impose constraints on the derived word similarities. We also make publicly available a new labeled dataset for evaluating word relatedness algorithms, which we believe to be the largest such dataset to date.","['https://openalex.org/W2155870214', 'https://openalex.org/W1880262756', 'https://openalex.org/W2136930489', 'https://openalex.org/W1593239840', 'https://openalex.org/W2147152072', 'https://openalex.org/W4235505822', 'https://openalex.org/W1993509040', 'https://openalex.org/W3216404684', 'https://openalex.org/W3001753394', 'https://openalex.org/W2787894218', 'https://openalex.org/W6678578999', 'https://openalex.org/W588052932', 'https://openalex.org/W2120084270', 'https://openalex.org/W2026487812', 'https://openalex.org/W1994616650', 'https://openalex.org/W1557757161', 'https://openalex.org/W203276351', 'https://openalex.org/W1974595223', 'https://openalex.org/W2059975159', 'https://openalex.org/W1974406477', 'https://openalex.org/W2058602429', 'https://openalex.org/W2117065474', 'https://openalex.org/W2053921957', 'https://openalex.org/W1992914835', 'https://openalex.org/W2099938389', 'https://openalex.org/W1558797106', 'https://openalex.org/W1480376833', 'https://openalex.org/W2038721957', 'https://openalex.org/W2125972432', 'https://openalex.org/W2166490638', 'https://openalex.org/W1521908097', 'https://openalex.org/W1970381522']",2012-08-12
https://openalex.org/W1854884267,https://doi.org/10.1162/coli_a_00237,SimLex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation,"We present SimLex-999, a gold standard resource for evaluating distributional semantic models that improves on existing resources in several important ways. First, in contrast to gold standards such as WordSim-353 and MEN, it explicitly quantifies similarity rather than association or relatedness so that pairs of entities that are associated but not actually similar (Freud, psychology) have a low rating. We show that, via this focus on similarity, SimLex-999 incentivizes the development of models with a different, and arguably wider, range of applications than those which reflect conceptual association. Second, SimLex-999 contains a range of concrete and abstract adjective, noun, and verb pairs, together with an independent rating of concreteness and (free) association strength for each pair. This diversity enables fine-grained analyses of the performance of models on concepts of different types, and consequently greater insight into how architectures can be improved. Further, unlike existing gold standard evaluations, for which automatic approaches have reached or surpassed the inter-annotator agreement ceiling, state-of-the-art models perform well below this ceiling on SimLex-999. There is therefore plenty of scope for SimLex-999 to quantify future improvements to distributional semantic models, guiding the development of the next generation of representation-learning architectures.","['https://openalex.org/W2170682101', 'https://openalex.org/W2036931463', 'https://openalex.org/W2251874715', 'https://openalex.org/W2251803266', 'https://openalex.org/W2128870637', 'https://openalex.org/W2117865617', 'https://openalex.org/W2108530510', 'https://openalex.org/W2132339004', 'https://openalex.org/W21497345', 'https://openalex.org/W4251372957', 'https://openalex.org/W1979606348', 'https://openalex.org/W2136930489', 'https://openalex.org/W2123982464', 'https://openalex.org/W2117130368', 'https://openalex.org/W2067438047', 'https://openalex.org/W2000746239', 'https://openalex.org/W2482300836', 'https://openalex.org/W2078841894', 'https://openalex.org/W1984251878', 'https://openalex.org/W1788602', 'https://openalex.org/W2171802951', 'https://openalex.org/W2032964561', 'https://openalex.org/W2286410738', 'https://openalex.org/W2251117789', 'https://openalex.org/W2250676463', 'https://openalex.org/W1983578042', 'https://openalex.org/W2143413399', 'https://openalex.org/W2296076036', 'https://openalex.org/W2027267056', 'https://openalex.org/W2086039194', 'https://openalex.org/W2165979968', 'https://openalex.org/W2436001372', 'https://openalex.org/W2035726644', 'https://openalex.org/W1985953330', 'https://openalex.org/W2142120379', 'https://openalex.org/W2171836785', 'https://openalex.org/W2111258243', 'https://openalex.org/W1970476061', 'https://openalex.org/W1554804307', 'https://openalex.org/W2080834271', 'https://openalex.org/W2080100102', 'https://openalex.org/W2127002961', 'https://openalex.org/W1662133657', 'https://openalex.org/W2059975159', 'https://openalex.org/W1582344906', 'https://openalex.org/W2136480620', 'https://openalex.org/W2117805756', 'https://openalex.org/W2140406733', 'https://openalex.org/W1614298861', 'https://openalex.org/W181737412', 'https://openalex.org/W2158139315', 'https://openalex.org/W2135341569', 'https://openalex.org/W1541481035', 'https://openalex.org/W1565863475', 'https://openalex.org/W2251012068', 'https://openalex.org/W2098352331', 'https://openalex.org/W2164973920', 'https://openalex.org/W2129271949', 'https://openalex.org/W2290013849', 'https://openalex.org/W2137735870', 'https://openalex.org/W4285719527', 'https://openalex.org/W96809255', 'https://openalex.org/W2080902366', 'https://openalex.org/W3099386342', 'https://openalex.org/W2250742840', 'https://openalex.org/W191422183', 'https://openalex.org/W2506188197', 'https://openalex.org/W2126530744', 'https://openalex.org/W2154531419', 'https://openalex.org/W2150102617', 'https://openalex.org/W2251771443', 'https://openalex.org/W1868671693', 'https://openalex.org/W2164019165', 'https://openalex.org/W2250536421', 'https://openalex.org/W1566139570', 'https://openalex.org/W2143017621', 'https://openalex.org/W2153579005']",2015-12-01
https://openalex.org/W2251012068,,Better Word Representations with Recursive Neural Networks for Morphology,"Vector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks. However, common to most existing work, words are regarded as independent entities without any explicit relationship among morphologically related words being modeled. As a result, rare and complex words are often poorly estimated, and all unknown words are represented in a rather crude way using only one or a few vectors. This paper addresses this shortcoming by proposing a novel model that is capable of building representations for morphologically complex words from their morphemes. We combine recursive neural networks (RNNs), where each morpheme is a basic unit, with neural language models (NLMs) to consider contextual information in learning morphologicallyaware word representations. Our learned models outperform existing word representations by a good margin on word similarity tasks across many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way.","['https://openalex.org/W2040711288', 'https://openalex.org/W2167419393', 'https://openalex.org/W2004763266', 'https://openalex.org/W1889268436', 'https://openalex.org/W2103305545', 'https://openalex.org/W1792316426', 'https://openalex.org/W1970381522', 'https://openalex.org/W2053921957', 'https://openalex.org/W2131462252', 'https://openalex.org/W36903255', 'https://openalex.org/W2137807925', 'https://openalex.org/W2103318667', 'https://openalex.org/W1423339008', 'https://openalex.org/W2164973920', 'https://openalex.org/W2025032307', 'https://openalex.org/W2140275137', 'https://openalex.org/W2080100102', 'https://openalex.org/W2171928131', 'https://openalex.org/W2081580037', 'https://openalex.org/W2571532437', 'https://openalex.org/W2164019165', 'https://openalex.org/W2018789714', 'https://openalex.org/W98255950', 'https://openalex.org/W2128634885', 'https://openalex.org/W2091812280', 'https://openalex.org/W2158139315', 'https://openalex.org/W2150539551', 'https://openalex.org/W2104518905', 'https://openalex.org/W71795751', 'https://openalex.org/W2158899491', 'https://openalex.org/W2053306448', 'https://openalex.org/W22861983', 'https://openalex.org/W179875071', 'https://openalex.org/W2141599568', 'https://openalex.org/W2132339004', 'https://openalex.org/W2117130368', 'https://openalex.org/W1999965501']",2013-08-01
https://openalex.org/W2132631284,,Verb similarity on the taxonomy of WordNet,"In this paper, we introduce two kinds of word similarity algorithms, SHE and RHE, to investigate the capability of WordNet in measuring verb similarity. In the absence of a standard verb set we have proposed two new verb similarity","['https://openalex.org/W2003240077', 'https://openalex.org/W1950936596', 'https://openalex.org/W2117753247', 'https://openalex.org/W2038721957', 'https://openalex.org/W2140887277', 'https://openalex.org/W1971220772', 'https://openalex.org/W1573498319', 'https://openalex.org/W2136480620', 'https://openalex.org/W1567365482', 'https://openalex.org/W2117805756', 'https://openalex.org/W4285719527', 'https://openalex.org/W2123489126', 'https://openalex.org/W1983578042', 'https://openalex.org/W2100935296', 'https://openalex.org/W3088601333', 'https://openalex.org/W2149671658', 'https://openalex.org/W2087739686', 'https://openalex.org/W2534712034', 'https://openalex.org/W136846643', 'https://openalex.org/W2962689487', 'https://openalex.org/W2080100102', 'https://openalex.org/W2081580037', 'https://openalex.org/W2160482687', 'https://openalex.org/W2117149238']",2006-01-01
https://openalex.org/W2026487812,https://doi.org/10.1145/1963405.1963455,A word at a time,"Computing the degree of semantic relatedness of words is a key functionality of many language applications such as search, clustering, and disambiguation. Previous approaches to computing semantic relatedness mostly used static language resources, while essentially ignoring their temporal aspects. We believe that a considerable amount of relatedness information can also be found in studying patterns of word usage over time. Consider, for instance, a newspaper archive spanning many years. Two words such as ""war"" and ""peace"" might rarely co-occur in the same articles, yet their patterns of use over time might be similar. In this paper, we propose a new semantic relatedness model, Temporal Semantic Analysis (TSA), which captures this temporal information. The previous state of the art method, Explicit Semantic Analysis (ESA), represented word semantics as a vector of concepts. TSA uses a more refined representation, where each concept is no longer scalar, but is instead represented as time series over a corpus of temporally-ordered documents. To the best of our knowledge, this is the first attempt to incorporate temporal evidence into models of semantic relatedness. Empirical evaluation shows that TSA provides consistent improvements over the state of the art ESA results on multiple benchmarks.","['https://openalex.org/W2918757710', 'https://openalex.org/W6637101025', 'https://openalex.org/W2127128140', 'https://openalex.org/W1963726077', 'https://openalex.org/W2091735588', 'https://openalex.org/W116902681', 'https://openalex.org/W2136930489', 'https://openalex.org/W2040546864', 'https://openalex.org/W1593239840', 'https://openalex.org/W2147152072', 'https://openalex.org/W1964365134', 'https://openalex.org/W2108280221', 'https://openalex.org/W4235505822', 'https://openalex.org/W3216404684', 'https://openalex.org/W2081798681', 'https://openalex.org/W2120779048', 'https://openalex.org/W2097766966', 'https://openalex.org/W2021314079', 'https://openalex.org/W2061986359', 'https://openalex.org/W2150739536', 'https://openalex.org/W2120084270', 'https://openalex.org/W2057714964', 'https://openalex.org/W2127536142', 'https://openalex.org/W3003709066', 'https://openalex.org/W2161443453', 'https://openalex.org/W2166008115', 'https://openalex.org/W2117065474', 'https://openalex.org/W2058602429', 'https://openalex.org/W2170907470', 'https://openalex.org/W2165299010', 'https://openalex.org/W2147880780', 'https://openalex.org/W1965495241', 'https://openalex.org/W1601068082', 'https://openalex.org/W2113889316', 'https://openalex.org/W2100935296', 'https://openalex.org/W2168621303', 'https://openalex.org/W2053921957', 'https://openalex.org/W1970381522', 'https://openalex.org/W1782572861', 'https://openalex.org/W1659833910', 'https://openalex.org/W2038721957', 'https://openalex.org/W158057341', 'https://openalex.org/W1647729745', 'https://openalex.org/W1984558542', 'https://openalex.org/W1521908097', 'https://openalex.org/W1660390307', 'https://openalex.org/W1646006088', 'https://openalex.org/W1992914835', 'https://openalex.org/W2022122406']",2011-03-28
https://openalex.org/W2510413766,https://doi.org/10.18653/v1/d16-1235,SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity,"Verbs play a critical role in the meaning of sentences, but these ubiquitous words have received little attention in recent distributional semantics research. We introduce SimVerb-3500, an evaluation resource that provides human ratings for the similarity of 3,500 verb pairs. SimVerb-3500 covers all normed verb types from the USF free-association database, providing at least three examples for every VerbNet class. This broad coverage facilitates detailed analyses of how syntactic and semantic phenomena together influence human understanding of verb meaning. Further, with significantly larger development and test sets than existing benchmarks, SimVerb-3500 enables more robust evaluation of representation learning architectures and promotes the development of methods tailored to verbs. We hope that SimVerb-3500 will enable a richer understanding of the diversity and complexity of verb semantics and guide the development of systems that can effectively represent and interpret this meaning.","['https://openalex.org/W2250539671', 'https://openalex.org/W2158847908', 'https://openalex.org/W1854884267', 'https://openalex.org/W2251012068', 'https://openalex.org/W1996430422', 'https://openalex.org/W1614298861', 'https://openalex.org/W1241017059', 'https://openalex.org/W595069947', 'https://openalex.org/W2471233003', 'https://openalex.org/W1979279776', 'https://openalex.org/W2131698806', 'https://openalex.org/W2250742840', 'https://openalex.org/W2251771443', 'https://openalex.org/W2035726644', 'https://openalex.org/W2135341569', 'https://openalex.org/W2401823607', 'https://openalex.org/W2963419157', 'https://openalex.org/W2170682101', 'https://openalex.org/W2949697461', 'https://openalex.org/W2251882135', 'https://openalex.org/W2081580037', 'https://openalex.org/W2112184938', 'https://openalex.org/W2043126905', 'https://openalex.org/W2053921957', 'https://openalex.org/W2962796133', 'https://openalex.org/W2143995218', 'https://openalex.org/W2251803266', 'https://openalex.org/W2158139315', 'https://openalex.org/W2251044566', 'https://openalex.org/W1503259811', 'https://openalex.org/W89270787', 'https://openalex.org/W2098352331', 'https://openalex.org/W2176085882', 'https://openalex.org/W2153579005', 'https://openalex.org/W2964232431', 'https://openalex.org/W2115792525', 'https://openalex.org/W2964267552', 'https://openalex.org/W2117130368', 'https://openalex.org/W2039217078', 'https://openalex.org/W1814992895', 'https://openalex.org/W2963216505', 'https://openalex.org/W2130199334', 'https://openalex.org/W183066880', 'https://openalex.org/W2108232645', 'https://openalex.org/W2080100102', 'https://openalex.org/W2508434770', 'https://openalex.org/W2116671632', 'https://openalex.org/W2152397575', 'https://openalex.org/W2094061585']",2016-01-01
https://openalex.org/W2176085882,https://doi.org/10.3115/v1/d14-1034,An Unsupervised Model for Instance Level Subcategorization Acquisition,"Most existing systems for subcategorization frame (SCF) acquisition rely on supervised parsing and infer SCF distributions at type, rather than instance level.These systems suffer from poor portability across domains and their benefit for NLP tasks that involve sentence-level processing is limited.We propose a new unsupervised, Markov Random Field-based model for SCF acquisition which is designed to address these problems.The system relies on supervised POS tagging rather than parsing, and is capable of learning SCFs at instance level.We perform evaluation against gold standard data which shows that our system outperforms several supervised and type-level SCF baselines.We also conduct task-based evaluation in the context of verb similarity prediction, demonstrating that a vector space model based on our SCFs substantially outperforms a lexical model and a model based on a supervised parser 1 .","['https://openalex.org/W2111024941', 'https://openalex.org/W1996430422', 'https://openalex.org/W2116410915', 'https://openalex.org/W2171116649', 'https://openalex.org/W2097606805', 'https://openalex.org/W1611318214', 'https://openalex.org/W2084774044', 'https://openalex.org/W1601035521', 'https://openalex.org/W1982827383', 'https://openalex.org/W2039217078', 'https://openalex.org/W32345482', 'https://openalex.org/W2068250380', 'https://openalex.org/W2757452186', 'https://openalex.org/W2607879133', 'https://openalex.org/W2099896133', 'https://openalex.org/W2250837944', 'https://openalex.org/W2121798597', 'https://openalex.org/W1511986666', 'https://openalex.org/W2119056510', 'https://openalex.org/W1662133657', 'https://openalex.org/W2250213987', 'https://openalex.org/W2029184000', 'https://openalex.org/W2129227538', 'https://openalex.org/W1508977358', 'https://openalex.org/W2153274216', 'https://openalex.org/W2005181355', 'https://openalex.org/W1585222170', 'https://openalex.org/W2081228205', 'https://openalex.org/W2250526231', 'https://openalex.org/W2053921957', 'https://openalex.org/W1945172732', 'https://openalex.org/W2144321756', 'https://openalex.org/W1818857488', 'https://openalex.org/W2107928532', 'https://openalex.org/W2083342361', 'https://openalex.org/W2251314312', 'https://openalex.org/W1490737687', 'https://openalex.org/W1855822899', 'https://openalex.org/W2135674549', 'https://openalex.org/W2076482515', 'https://openalex.org/W2121435899', 'https://openalex.org/W2074064986', 'https://openalex.org/W2088198454', 'https://openalex.org/W2078546664', 'https://openalex.org/W1972805353', 'https://openalex.org/W1502719479', 'https://openalex.org/W2138615112', 'https://openalex.org/W2130178369', 'https://openalex.org/W1564452174', 'https://openalex.org/W1909733559', 'https://openalex.org/W2130717590', 'https://openalex.org/W14067211', 'https://openalex.org/W1586407311', 'https://openalex.org/W1626542014', 'https://openalex.org/W81190909', 'https://openalex.org/W2052474702']",2014-01-01
https://openalex.org/W2170682101,https://doi.org/10.3115/1620754.1620758,A study on similarity and relatedness using distributional and WordNet-based approaches,"This paper presents and compares WordNet-based and distributional similarity approaches. The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented. Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets. Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses.","['https://openalex.org/W2120779048', 'https://openalex.org/W1782572861', 'https://openalex.org/W2951798058', 'https://openalex.org/W4233775226', 'https://openalex.org/W192694488', 'https://openalex.org/W2166971953', 'https://openalex.org/W2096223431', 'https://openalex.org/W2161443453', 'https://openalex.org/W2100935296', 'https://openalex.org/W2136930489', 'https://openalex.org/W4255386104', 'https://openalex.org/W2103318667', 'https://openalex.org/W1959533457', 'https://openalex.org/W2136480620', 'https://openalex.org/W1567365482', 'https://openalex.org/W2170344111', 'https://openalex.org/W2166776180', 'https://openalex.org/W2055518963', 'https://openalex.org/W2534712034', 'https://openalex.org/W2149393279', 'https://openalex.org/W2149801387', 'https://openalex.org/W2117805756', 'https://openalex.org/W1517377188', 'https://openalex.org/W4300121351', 'https://openalex.org/W158057341', 'https://openalex.org/W1573498319', 'https://openalex.org/W2135207619', 'https://openalex.org/W4255198209', 'https://openalex.org/W1647729745', 'https://openalex.org/W1596967103', 'https://openalex.org/W2053921957', 'https://openalex.org/W2042160362', 'https://openalex.org/W4256347525', 'https://openalex.org/W2950225692', 'https://openalex.org/W2080100102', 'https://openalex.org/W2005181355']",2009-01-01
https://openalex.org/W2080100102,https://doi.org/10.1145/365628.365657,Contextual correlates of synonymy,"article Free AccessContextual correlates of synonymy Authors: Herbert Rubenstein Decision Sciences Lab, Bedford, MA Decision Sciences Lab, Bedford, MAView Profile , John B. Goodenough Decision Sciences Lab, Bedford, MA Decision Sciences Lab, Bedford, MAView Profile Authors Info & Claims Communications of the ACMVolume 8Issue 10Oct. 1965 pp 627–633https://doi.org/10.1145/365628.365657Published:01 October 1965Publication History 695citation3,459DownloadsMetricsTotal Citations695Total Downloads3,459Last 12 Months573Last 6 weeks59 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF","['https://openalex.org/W2882319491', 'https://openalex.org/W2015271197', 'https://openalex.org/W2087404238', 'https://openalex.org/W2033716723', 'https://openalex.org/W1738233868']",1965-10-01
https://openalex.org/W2137735870,,Distributional Semantics in Technicolor,"Our research aims at building computational models of word meaning that are perceptually grounded. Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models. Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic relatedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words. Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance.","['https://openalex.org/W2128053456', 'https://openalex.org/W2053921957', 'https://openalex.org/W2130463046', 'https://openalex.org/W2066134726', 'https://openalex.org/W1566135517', 'https://openalex.org/W2162915993', 'https://openalex.org/W2250196260', 'https://openalex.org/W2151103935', 'https://openalex.org/W2141282920', 'https://openalex.org/W1974689608', 'https://openalex.org/W1664311846', 'https://openalex.org/W1983578042', 'https://openalex.org/W155596317', 'https://openalex.org/W2124386111', 'https://openalex.org/W1897761818', 'https://openalex.org/W2128017662', 'https://openalex.org/W1662133657', 'https://openalex.org/W2104978738', 'https://openalex.org/W2145023731', 'https://openalex.org/W2103652203', 'https://openalex.org/W1771464967', 'https://openalex.org/W115307656', 'https://openalex.org/W2125186487', 'https://openalex.org/W1608604164', 'https://openalex.org/W1981617416', 'https://openalex.org/W2252218513', 'https://openalex.org/W2177113878', 'https://openalex.org/W149643677', 'https://openalex.org/W1625255723', 'https://openalex.org/W2050830825', 'https://openalex.org/W2126274282', 'https://openalex.org/W2036718463', 'https://openalex.org/W2148596671', 'https://openalex.org/W2128870637', 'https://openalex.org/W2131846894', 'https://openalex.org/W2066941820']",2012-07-08
https://openalex.org/W2593779438,,ABX-discriminability measures and applications,"This thesis constitutes an indirect contribution to the problem of modeling phonetic category acquisition in infancy. Some specific computational models of phonetic category acquisition have been proposed, but they were never tested extensively nor compared quantitatively to see whether they were really able to account for a sizable portion of the available empirical observations. In this thesis, we introduce ABX-Discriminability Measures and we develop a methodology based on these measures that allows to perform such a systematic evaluation. We demonstrate the interest of our framework by applying it to the evaluation of models for two related problems: phonetic category processing at birth and in adulthood. The next step, applying our framework to models of phonetic category acquisition, is left for future work.The interest of ABX-Discriminability Measures is not restricted to the particular problem of evaluating models of phonetic category processing in humans. We argue that their interest generalizes to the study of other signals than speech and other category structures than phonetic categories, as well as to other research fields than cognitive science, like low-resource engineering, data mining and artificial intelligence for example. To make this point, we study the properties of these measures in a general abstract framework and we detail the rationale for three broad family of potential applications: evaluating systems operating without explicit supervision in their ability to represent a category structure; providing simple computational models of behavior in discrimination tasks; providing descriptive measurements for representations of categorical data.","['https://openalex.org/W2404799143', 'https://openalex.org/W2127394101', 'https://openalex.org/W2073279303', 'https://openalex.org/W2062663442', 'https://openalex.org/W2032476212', 'https://openalex.org/W2396043527', 'https://openalex.org/W1974777011', 'https://openalex.org/W2153006233', 'https://openalex.org/W3187397227', 'https://openalex.org/W2397686546', 'https://openalex.org/W2399576818', 'https://openalex.org/W2085085647', 'https://openalex.org/W2072059109', 'https://openalex.org/W3034729383', 'https://openalex.org/W1587073199', 'https://openalex.org/W2078993594', 'https://openalex.org/W2036502752', 'https://openalex.org/W2100768664', 'https://openalex.org/W2400549570', 'https://openalex.org/W1977979148', 'https://openalex.org/W1982801854', 'https://openalex.org/W2095458199', 'https://openalex.org/W2091432990', 'https://openalex.org/W2015977525', 'https://openalex.org/W2018069071', 'https://openalex.org/W2115614142', 'https://openalex.org/W2164203346', 'https://openalex.org/W2270030679', 'https://openalex.org/W2093111935', 'https://openalex.org/W2145410271', 'https://openalex.org/W2406349064', 'https://openalex.org/W2022465033', 'https://openalex.org/W2068116204', 'https://openalex.org/W1976526581', 'https://openalex.org/W2150261429', 'https://openalex.org/W2142209090', 'https://openalex.org/W1945005118', 'https://openalex.org/W2119821739', 'https://openalex.org/W2167499516', 'https://openalex.org/W2086989311', 'https://openalex.org/W1980704774', 'https://openalex.org/W1924689489', 'https://openalex.org/W1964201439', 'https://openalex.org/W2406820985', 'https://openalex.org/W2040913319', 'https://openalex.org/W1995710564', 'https://openalex.org/W2168548156', 'https://openalex.org/W2107274740', 'https://openalex.org/W2108145097', 'https://openalex.org/W2010692105', 'https://openalex.org/W1537881105', 'https://openalex.org/W1558306184', 'https://openalex.org/W1968801326', 'https://openalex.org/W2135563147', 'https://openalex.org/W2090861223', 'https://openalex.org/W2017133403', 'https://openalex.org/W2168614750', 'https://openalex.org/W2013020033', 'https://openalex.org/W2152483743', 'https://openalex.org/W2787234017', 'https://openalex.org/W8787889', 'https://openalex.org/W2171061694', 'https://openalex.org/W1524333225', 'https://openalex.org/W2786608204', 'https://openalex.org/W2889643527', 'https://openalex.org/W2102040782', 'https://openalex.org/W2149159134', 'https://openalex.org/W2026408911', 'https://openalex.org/W2002276939', 'https://openalex.org/W2092655206', 'https://openalex.org/W2009150118', 'https://openalex.org/W2346964103', 'https://openalex.org/W2007498854', 'https://openalex.org/W1972950801', 'https://openalex.org/W2075854640', 'https://openalex.org/W2126958229', 'https://openalex.org/W1532704504', 'https://openalex.org/W3151807103', 'https://openalex.org/W1796128977', 'https://openalex.org/W1564039035', 'https://openalex.org/W2395899413', 'https://openalex.org/W1967924372', 'https://openalex.org/W2060349248', 'https://openalex.org/W2052697931', 'https://openalex.org/W2160815625', 'https://openalex.org/W1980044217', 'https://openalex.org/W2250874882', 'https://openalex.org/W2407151108', 'https://openalex.org/W2150021272', 'https://openalex.org/W2075371579', 'https://openalex.org/W2144640755', 'https://openalex.org/W2096736596', 'https://openalex.org/W2397058949', 'https://openalex.org/W2139102046', 'https://openalex.org/W1784695092', 'https://openalex.org/W2153767712', 'https://openalex.org/W2041394569', 'https://openalex.org/W2031282998', 'https://openalex.org/W2129877081', 'https://openalex.org/W1990444693', 'https://openalex.org/W2108858705', 'https://openalex.org/W66627554', 'https://openalex.org/W1994492508', 'https://openalex.org/W1554944419', 'https://openalex.org/W2053872598', 'https://openalex.org/W2011238950', 'https://openalex.org/W2005311247', 'https://openalex.org/W1604599697', 'https://openalex.org/W1972168487', 'https://openalex.org/W2345968833', 'https://openalex.org/W607518649', 'https://openalex.org/W2109572214', 'https://openalex.org/W2002681612', 'https://openalex.org/W2113153226', 'https://openalex.org/W2133687321', 'https://openalex.org/W2023163512', 'https://openalex.org/W2401464865', 'https://openalex.org/W1575566382', 'https://openalex.org/W2076493153', 'https://openalex.org/W2093231248', 'https://openalex.org/W153534061', 'https://openalex.org/W2121947440', 'https://openalex.org/W2010725167', 'https://openalex.org/W2345811097', 'https://openalex.org/W2167057640', 'https://openalex.org/W2407869992', 'https://openalex.org/W281094599', 'https://openalex.org/W2136653392', 'https://openalex.org/W1987841215', 'https://openalex.org/W2126345904', 'https://openalex.org/W1901616594', 'https://openalex.org/W2410344739', 'https://openalex.org/W2103869314', 'https://openalex.org/W2011701921', 'https://openalex.org/W2037976268', 'https://openalex.org/W2114719288', 'https://openalex.org/W2056730193', 'https://openalex.org/W2055120733', 'https://openalex.org/W2074314503', 'https://openalex.org/W2142633459', 'https://openalex.org/W2101509422', 'https://openalex.org/W1992468098', 'https://openalex.org/W2086022490', 'https://openalex.org/W1981241922', 'https://openalex.org/W1974776535', 'https://openalex.org/W2024490156', 'https://openalex.org/W2052005497', 'https://openalex.org/W2020344439', 'https://openalex.org/W2014512781', 'https://openalex.org/W2764410184', 'https://openalex.org/W2063303346', 'https://openalex.org/W2110627398', 'https://openalex.org/W598767079', 'https://openalex.org/W1545406001', 'https://openalex.org/W2049068198', 'https://openalex.org/W643591744']",2016-09-29
https://openalex.org/W2963583956,https://doi.org/10.1162/tacl_a_00074,Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets,"With the ever growing amount of textual data from a large variety of languages, domains, and genres, it has become standard to evaluate NLP algorithms on multiple datasets in order to ensure a consistent performance across heterogeneous setups. However, such multiple comparisons pose significant challenges to traditional statistical analysis methods in NLP and can lead to erroneous conclusions. In this paper we propose a Replicability Analysis framework for a statistically sound analysis of multiple comparisons between algorithms for NLP tasks. We discuss the theoretical advantages of this framework over the current, statistically unjustified, practice in the NLP literature, and demonstrate its empirical value across four applications: multi-domain dependency parsing, multilingual POS tagging, cross-domain sentiment classification and word similarity prediction.","['https://openalex.org/W1987777080', 'https://openalex.org/W2059676361', 'https://openalex.org/W2164466305', 'https://openalex.org/W2112184938', 'https://openalex.org/W1575893985', 'https://openalex.org/W2140558123', 'https://openalex.org/W2053814546', 'https://openalex.org/W1854884267', 'https://openalex.org/W2115012618', 'https://openalex.org/W2036714085', 'https://openalex.org/W1622128722', 'https://openalex.org/W2073688165', 'https://openalex.org/W2084413241', 'https://openalex.org/W2166420405', 'https://openalex.org/W2033646521', 'https://openalex.org/W2008985174', 'https://openalex.org/W2080100102', 'https://openalex.org/W2097137621', 'https://openalex.org/W2158108973', 'https://openalex.org/W1632114991', 'https://openalex.org/W2092654472', 'https://openalex.org/W2114211800', 'https://openalex.org/W1241017059', 'https://openalex.org/W2103076621', 'https://openalex.org/W2567649635', 'https://openalex.org/W2963463240', 'https://openalex.org/W2569308312', 'https://openalex.org/W2962817038', 'https://openalex.org/W2963216505', 'https://openalex.org/W4252684946', 'https://openalex.org/W2163302275', 'https://openalex.org/W2963421945', 'https://openalex.org/W2057399676', 'https://openalex.org/W2167277498', 'https://openalex.org/W2251012068', 'https://openalex.org/W2251329024', 'https://openalex.org/W2026487812', 'https://openalex.org/W2110065044', 'https://openalex.org/W2121044470', 'https://openalex.org/W1980361506', 'https://openalex.org/W4256170606', 'https://openalex.org/W2105103433', 'https://openalex.org/W2215421138', 'https://openalex.org/W2111216449', 'https://openalex.org/W2117561440', 'https://openalex.org/W2142625445', 'https://openalex.org/W1995945562', 'https://openalex.org/W2099107563', 'https://openalex.org/W2176085882', 'https://openalex.org/W1899794420', 'https://openalex.org/W1970381522', 'https://openalex.org/W2481300999', 'https://openalex.org/W2027979924', 'https://openalex.org/W2120354757', 'https://openalex.org/W2067438047', 'https://openalex.org/W2251253014', 'https://openalex.org/W759515131', 'https://openalex.org/W1565746575', 'https://openalex.org/W2032136732', 'https://openalex.org/W2170682101', 'https://openalex.org/W2153579005', 'https://openalex.org/W2250742840', 'https://openalex.org/W2797563284', 'https://openalex.org/W2103318667', 'https://openalex.org/W2137735870', 'https://openalex.org/W2106310992', 'https://openalex.org/W2252147815', 'https://openalex.org/W2118108643', 'https://openalex.org/W1535015163', 'https://openalex.org/W22168010', 'https://openalex.org/W2252040980', 'https://openalex.org/W2137387514', 'https://openalex.org/W2250539671']",2017-12-01
https://openalex.org/W2103318667,https://doi.org/10.1080/01690969108406936,Contextual correlates of semantic similarity,"Abstract The relationship between semantic and contextual similarity is investigated for pairs of nouns that vary from high to low semantic similarity. Semantic similarity is estimated by subjective ratings; contextual similarity is estimated by the method of sorting sentential contexts. The results show an inverse linear relationship between similarity of meaning and the discriminability of contexts. This relation, is obtained for two separate corpora of sentence contexts. It is concluded that, on average, for words in the same language drawn from the same syntactic and semantic categories, the more often two words can be substituted into the same contexts the more similar in meaning they are judged to be.","['https://openalex.org/W13823885', 'https://openalex.org/W2100322854', 'https://openalex.org/W2079194046', 'https://openalex.org/W2437277295', 'https://openalex.org/W2045553479', 'https://openalex.org/W4300703805', 'https://openalex.org/W4238944320', 'https://openalex.org/W4298351405', 'https://openalex.org/W1966153816', 'https://openalex.org/W2033716723', 'https://openalex.org/W4210560736', 'https://openalex.org/W1971220772', 'https://openalex.org/W2036895580', 'https://openalex.org/W1634667895', 'https://openalex.org/W2114826854', 'https://openalex.org/W2020159140', 'https://openalex.org/W1570047706', 'https://openalex.org/W1989415743', 'https://openalex.org/W2026442395', 'https://openalex.org/W4236556464', 'https://openalex.org/W2014735492', 'https://openalex.org/W2064332540', 'https://openalex.org/W2914326967', 'https://openalex.org/W2080100102', 'https://openalex.org/W1751175273', 'https://openalex.org/W1536719366', 'https://openalex.org/W2321575885', 'https://openalex.org/W2746118309', 'https://openalex.org/W4298028598', 'https://openalex.org/W1507401974', 'https://openalex.org/W4301847896', 'https://openalex.org/W1966680968', 'https://openalex.org/W2091384152', 'https://openalex.org/W2071094139', 'https://openalex.org/W1483126227', 'https://openalex.org/W2109334311', 'https://openalex.org/W1513522035', 'https://openalex.org/W2082766935', 'https://openalex.org/W2017580301', 'https://openalex.org/W1547269118', 'https://openalex.org/W4205256201', 'https://openalex.org/W3023045483']",1991-01-01
https://openalex.org/W2101409664,https://doi.org/10.1109/tasl.2006.876762,Applying a Speaker-Dependent Speech Compression Technique to Concatenative TTS Synthesizers,"This paper proposes a new speaker-dependent coding algorithm to efficiently compress a large speech database for corpus-based concatenative text-to-speech (TTS) engines while maintaining high fidelity. To achieve a high compression ratio and meet the fundamental requirements of concatenative TTS synthesizers, such as partial segment decoding and random access capability, we adopt a nonpredictive analysis-by-synthesis scheme for speaker-dependent parameter estimation and quantization. The spectral coefficients are quantized by using a memoryless split vector quantization (VQ) approach that does not use frame correlation. Considering that excitation signals of a specific speaker show low intra-variation especially in the voiced regions, the conventional adaptive codebook for pitch prediction is replaced by a speaker-dependent pitch-pulse codebook trained by a corpus of single-speaker speech signals. To further improve the coding efficiency, the proposed coder flexibly combines nonpredictive and predictive type method considering the structure of the TTS system. By applying the proposed algorithm to a Korean TTS system, we could obtain comparable quality to the G.729 speech coder and satisfy all the requirements that TTS system needs. The results are verified by both objective and subjective quality measurements. In addition, the decoding complexity of the proposed coder is around 55% lower than that of G.729 annex A.","['https://openalex.org/W1995256383', 'https://openalex.org/W6606912663', 'https://openalex.org/W2151626637', 'https://openalex.org/W2500358397', 'https://openalex.org/W2131094339', 'https://openalex.org/W2025781735', 'https://openalex.org/W1863544905', 'https://openalex.org/W2130890537', 'https://openalex.org/W2115628785', 'https://openalex.org/W2079337129', 'https://openalex.org/W6605951711', 'https://openalex.org/W1560904098', 'https://openalex.org/W172307048', 'https://openalex.org/W148624016', 'https://openalex.org/W4285719527']",2007-01-23
https://openalex.org/W2117418893,https://doi.org/10.1109/tasl.2009.2016394,Robust Speaker-Adaptive HMM-Based Text-to-Speech Synthesis,"This paper describes a speaker-adaptive HMM-based speech synthesis system. The new system, called ldquoHTS-2007,rdquo employs speaker adaptation (CSMAPLR+MAP), feature-space adaptive training, mixed-gender modeling, and full-covariance modeling using CSMAPLR transforms, in addition to several other techniques that have proved effective in our previous systems. Subjective evaluation results show that the new system generates significantly better quality synthetic speech than speaker-dependent approaches with realistic amounts of speech data, and that it bears comparison with speaker-dependent approaches even when large amounts of speech data are available. In addition, a comparison study with several speech synthesis techniques shows the new system is very robust: It is able to build voices from less-than-ideal speech data and synthesize good-quality speech even for out-of-domain sentences.","['https://openalex.org/W6676358011', 'https://openalex.org/W2170980774', 'https://openalex.org/W6643650091', 'https://openalex.org/W6737323677', 'https://openalex.org/W1564152904', 'https://openalex.org/W2084609288', 'https://openalex.org/W6636404455', 'https://openalex.org/W6631636991', 'https://openalex.org/W1599512239', 'https://openalex.org/W2153914468', 'https://openalex.org/W1984905644', 'https://openalex.org/W2124629003', 'https://openalex.org/W2106554350', 'https://openalex.org/W428433050', 'https://openalex.org/W6639125025', 'https://openalex.org/W2146871184', 'https://openalex.org/W6605232188', 'https://openalex.org/W6682364688', 'https://openalex.org/W2153331007', 'https://openalex.org/W1976551160', 'https://openalex.org/W6759752890', 'https://openalex.org/W2139033449', 'https://openalex.org/W4395452434', 'https://openalex.org/W6677333272', 'https://openalex.org/W2049686551', 'https://openalex.org/W6683640347', 'https://openalex.org/W6684513021', 'https://openalex.org/W2150658333', 'https://openalex.org/W7075637324', 'https://openalex.org/W2042691334', 'https://openalex.org/W2000513720', 'https://openalex.org/W155946340', 'https://openalex.org/W2108674328', 'https://openalex.org/W2145575463', 'https://openalex.org/W6602885286', 'https://openalex.org/W6684311649', 'https://openalex.org/W2002342963', 'https://openalex.org/W2009417200', 'https://openalex.org/W6638317995', 'https://openalex.org/W6631309588', 'https://openalex.org/W2428180336', 'https://openalex.org/W1963627370', 'https://openalex.org/W2085013480', 'https://openalex.org/W2049633694', 'https://openalex.org/W2121981798', 'https://openalex.org/W6630838124', 'https://openalex.org/W2106792148', 'https://openalex.org/W2048973497', 'https://openalex.org/W2105080323', 'https://openalex.org/W2064218608', 'https://openalex.org/W2109189270', 'https://openalex.org/W6711777497', 'https://openalex.org/W1496971974', 'https://openalex.org/W200094172', 'https://openalex.org/W3142087749', 'https://openalex.org/W1935012542', 'https://openalex.org/W2144139079', 'https://openalex.org/W6611766843', 'https://openalex.org/W2154920538', 'https://openalex.org/W6680683932', 'https://openalex.org/W6633512198', 'https://openalex.org/W1995565802', 'https://openalex.org/W1537908555', 'https://openalex.org/W2149175990', 'https://openalex.org/W2484208911', 'https://openalex.org/W180098012', 'https://openalex.org/W1512429158', 'https://openalex.org/W2150617255', 'https://openalex.org/W344150399', 'https://openalex.org/W1861150963', 'https://openalex.org/W1523372075', 'https://openalex.org/W1973766695', 'https://openalex.org/W1600722501', 'https://openalex.org/W2141684970', 'https://openalex.org/W2168584209', 'https://openalex.org/W2111194146', 'https://openalex.org/W2916602695', 'https://openalex.org/W2093632031', 'https://openalex.org/W2915628810', 'https://openalex.org/W2613407020', 'https://openalex.org/W2159528802', 'https://openalex.org/W1529897604', 'https://openalex.org/W2400063444', 'https://openalex.org/W2116046013', 'https://openalex.org/W1602430027', 'https://openalex.org/W1496562625', 'https://openalex.org/W4233446834', 'https://openalex.org/W2395578248', 'https://openalex.org/W70888257', 'https://openalex.org/W1496801689', 'https://openalex.org/W1812022101', 'https://openalex.org/W2165856656', 'https://openalex.org/W129217914', 'https://openalex.org/W1559184314', 'https://openalex.org/W133559434', 'https://openalex.org/W1854592333']",2009-07-01
https://openalex.org/W3164843644,https://doi.org/10.1109/icaicta49861.2020.9429050,Multi Speaker Speech Synthesis System for Indonesian Language,"Generally, text-to-speech models only produce voice from a single speaker. The most straightforward method to produce another speaker's voice, is to build a standalone synthesis model for each desired speaker's voice. But such approach needs large amount of training data and computational resource. To overcome the problem, several architectures has been successful in producing synthesized speech from various speakers efficiently in terms of data and computation. One of the architectures is Deep Voice 3. In this work, a multi speaker speech synthesis system is built for Indonesian language. The system is using Deep Voice 3 architecture, with several additional components for preprocessing dan post-processing. Some of the components are specifically implemented for Indonesian language. The system is built using a multi speaker dataset, consists of speech data from 145 Indonesian speaker. This system is evaluated subjectively to assess naturalness, similarity to original speaker, and intelligibility of the produced speech. The result shows that the system has MOS (mean opinion score) of 3.39 for speech naturalness dan 3.11 for speech similarity. In assessing speech intelligibility using SUS (semantically unpredictable sentence), the test gives 73.88% for sentence accuracy and 93.48% for word accuracy.","['https://openalex.org/W6749489859', 'https://openalex.org/W2117418893', 'https://openalex.org/W1984905644', 'https://openalex.org/W2345671047', 'https://openalex.org/W2120847449', 'https://openalex.org/W2152859600', 'https://openalex.org/W6936113694', 'https://openalex.org/W6735706088', 'https://openalex.org/W2963609956', 'https://openalex.org/W6675380101', 'https://openalex.org/W6734815144', 'https://openalex.org/W6756197946', 'https://openalex.org/W6752888775', 'https://openalex.org/W2964243274', 'https://openalex.org/W2129142580', 'https://openalex.org/W6738277540', 'https://openalex.org/W2591927543', 'https://openalex.org/W2963432880', 'https://openalex.org/W2901997113', 'https://openalex.org/W2964281804', 'https://openalex.org/W2963712897', 'https://openalex.org/W2808706139', 'https://openalex.org/W2605141709', 'https://openalex.org/W2619368999', 'https://openalex.org/W2102003408', 'https://openalex.org/W2963691546', 'https://openalex.org/W1579838312', 'https://openalex.org/W2527729766']",2020-09-08
https://openalex.org/W3109182305,https://doi.org/10.1109/icacsis51025.2020.9263086,"Hierarchical Transfer Learning for Text-to-Speech in Indonesian, Javanese, and Sundanese Languages","This research develops end-to-end deep learning-based text-to-speech (TTS) in Indonesian, Javanese, and Sundanese. While end-to-end neural TTS, such as Tacotron-2, has made remarkable progress recently, it still suffers from a data scarcity problem for low-resource languages such as Javanese and Sundanese. Our preliminary study shows that Tacotron-2-based TTS needs a large amount of training data; a minimum of 10 hours of training data is required for the model to be able to synthesize acceptable quality and intelligible speech. To solve this low-resource problem, our work proposes a hierarchical transfer learning to train TTS for Javanese and Sundanese, by taking advantage of a dissimilar high-resource language of English domain and a similar intermediate-resource language of Indonesian domain. We report that the evaluation of synthesized speech using the mean opinion score (MOS) reaches 4.27 for Indonesian, and 4.08 for Javanese, and 3.92 for Sundanese. The word accuracy (WAcc) evaluation on semantically unpredicted sentences (SUS) reaches 98.26% for Indonesian, 95.02% for Javanese, and 95.43% for Sundanese. The subjective evaluations of the synthetic speech quality demonstrate that our transfer learning scheme is successfully applied to TTS model for low-resource target domain. Using less than one hour of training data, 38 minutes for Indonesian, 16 minutes for Javanese, and 19 minutes for Sundanese, TTS models can learn fast and achieve adequate performance.","['https://openalex.org/W6739901393', 'https://openalex.org/W6758226236', 'https://openalex.org/W6738277540', 'https://openalex.org/W6734815144', 'https://openalex.org/W2769810959', 'https://openalex.org/W2775336875', 'https://openalex.org/W6741328424', 'https://openalex.org/W2939833446', 'https://openalex.org/W2969521066', 'https://openalex.org/W6749489859', 'https://openalex.org/W2921135010', 'https://openalex.org/W2153914468', 'https://openalex.org/W6675380101', 'https://openalex.org/W1570629387', 'https://openalex.org/W2338517121', 'https://openalex.org/W2963027641', 'https://openalex.org/W2913818371', 'https://openalex.org/W2906459023', 'https://openalex.org/W2739077999', 'https://openalex.org/W2993078836', 'https://openalex.org/W2111284386', 'https://openalex.org/W2976159681', 'https://openalex.org/W2084609288', 'https://openalex.org/W6674565674', 'https://openalex.org/W2889028433', 'https://openalex.org/W2165698076', 'https://openalex.org/W2963300588', 'https://openalex.org/W2395579298', 'https://openalex.org/W2919358988', 'https://openalex.org/W2928389720', 'https://openalex.org/W2942231644', 'https://openalex.org/W6758075289', 'https://openalex.org/W3010945169', 'https://openalex.org/W2969856879', 'https://openalex.org/W2963331137', 'https://openalex.org/W2964243274', 'https://openalex.org/W6749437365', 'https://openalex.org/W2936103087', 'https://openalex.org/W2963945466', 'https://openalex.org/W6754002923', 'https://openalex.org/W6771405711', 'https://openalex.org/W2963609956', 'https://openalex.org/W2895654193', 'https://openalex.org/W6631190155', 'https://openalex.org/W6623517193', 'https://openalex.org/W2996317213', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963712897', 'https://openalex.org/W2910199258', 'https://openalex.org/W854541894', 'https://openalex.org/W4294619240', 'https://openalex.org/W2095734449', 'https://openalex.org/W2964121744', 'https://openalex.org/W2619368999', 'https://openalex.org/W2963691546', 'https://openalex.org/W2793114364', 'https://openalex.org/W3028018682', 'https://openalex.org/W2914049472', 'https://openalex.org/W2963782041', 'https://openalex.org/W2962699523', 'https://openalex.org/W2610245951', 'https://openalex.org/W2964281804', 'https://openalex.org/W2591927543', 'https://openalex.org/W1522301498', 'https://openalex.org/W2887280559', 'https://openalex.org/W2963403868', 'https://openalex.org/W2102003408']",2020-10-17
https://openalex.org/W4206596421,https://doi.org/10.1109/access.2022.3141200,"Transfer Learning, Style Control, and Speaker Reconstruction Loss for Zero-Shot Multilingual Multi-Speaker Text-to-Speech on Low-Resource Languages","Deep neural network (DNN)-based systems generally require large amounts of training data, so they have data scarcity problems in low-resource languages. Recent studies have succeeded in building zero-shot multi-speaker DNN-based TTS on high-resource languages, but they still have unsatisfactory performance on unseen speakers. This study addresses two main problems: overcoming the problem of data scarcity in the DNN-based TTS on low-resource languages and improving the performance of zero-shot speaker adaptation for unseen speakers. We propose a novel multi-stage transfer learning strategy using a partial network-based deep transfer learning to overcome the low-resource problem by utilizing pre-trained monolingual single-speaker TTS and d-vector speaker encoder on a high-resource language as the source domain. Meanwhile, to improve the performance of zero-shot speaker adaptation, we propose a new TTS model that incorporates an explicit style control from the target speaker for TTS conditioning and an utterance-level speaker reconstruction loss during TTS training. We use publicly available speech datasets for experiments. We show that our proposed training strategy is able to effectively train the TTS models using a limited amount of training data of low-resource target languages. The models trained using the proposed transfer learning successfully produce intelligible natural speech sounds, while in contrast using standard training fails to make the models synthesize understandable speech. We also demonstrate that our proposed style encoder network and speaker reconstruction loss significantly improves speaker similarity in zero-shot speaker adaptation task compared to the baseline model. Overall, our proposed TTS model and training strategy has succeeded in increasing the speaker cosine similarity of the synthesized speech on the unseen speakers test set by 0.468 and 0.279 in native and foreign languages respectively.","['https://openalex.org/W2775336875', 'https://openalex.org/W2769810959', 'https://openalex.org/W6734815144', 'https://openalex.org/W6738277540', 'https://openalex.org/W6749489859', 'https://openalex.org/W2963609956', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963945466', 'https://openalex.org/W2969521066', 'https://openalex.org/W2939833446', 'https://openalex.org/W2976159681', 'https://openalex.org/W6748573829', 'https://openalex.org/W6765653190', 'https://openalex.org/W6755135894', 'https://openalex.org/W6757322325', 'https://openalex.org/W2963796886', 'https://openalex.org/W2887171382', 'https://openalex.org/W2972440097', 'https://openalex.org/W3090474612', 'https://openalex.org/W3015826515', 'https://openalex.org/W6752888775', 'https://openalex.org/W6770838157', 'https://openalex.org/W3096086473', 'https://openalex.org/W3168656614', 'https://openalex.org/W6683085343', 'https://openalex.org/W6732048897', 'https://openalex.org/W3109182305', 'https://openalex.org/W2962788625', 'https://openalex.org/W6736057607', 'https://openalex.org/W2165698076', 'https://openalex.org/W2395579298', 'https://openalex.org/W2887280559', 'https://openalex.org/W6779259078', 'https://openalex.org/W2889028433', 'https://openalex.org/W2962699523', 'https://openalex.org/W2914049472', 'https://openalex.org/W6778159118', 'https://openalex.org/W2973034126', 'https://openalex.org/W2774140536', 'https://openalex.org/W2900796929', 'https://openalex.org/W3199044515', 'https://openalex.org/W2972961496', 'https://openalex.org/W6770983605', 'https://openalex.org/W2046056978', 'https://openalex.org/W6687645958', 'https://openalex.org/W6737575990', 'https://openalex.org/W2748488820', 'https://openalex.org/W2947256846', 'https://openalex.org/W2890964092', 'https://openalex.org/W2973032144', 'https://openalex.org/W2967957380', 'https://openalex.org/W3010925296', 'https://openalex.org/W2150769028', 'https://openalex.org/W2290689761', 'https://openalex.org/W2888968865', 'https://openalex.org/W6750489868', 'https://openalex.org/W2331128040', 'https://openalex.org/W2972443522', 'https://openalex.org/W2982037672', 'https://openalex.org/W3016050488', 'https://openalex.org/W2972394484', 'https://openalex.org/W3019993940', 'https://openalex.org/W3168542456', 'https://openalex.org/W2963300588', 'https://openalex.org/W1494198834', 'https://openalex.org/W7039088390', 'https://openalex.org/W2895654193', 'https://openalex.org/W6631190155', 'https://openalex.org/W2095734449', 'https://openalex.org/W2963035245', 'https://openalex.org/W2107860279', 'https://openalex.org/W6748816842', 'https://openalex.org/W2990124956', 'https://openalex.org/W4287758476', 'https://openalex.org/W2201142001', 'https://openalex.org/W2992285938', 'https://openalex.org/W2786672974', 'https://openalex.org/W2612434969', 'https://openalex.org/W2960427821', 'https://openalex.org/W2903853691']",2022-01-01
https://openalex.org/W2345671047,https://doi.org/10.1016/j.procs.2016.04.045,Towards Robust Indonesian Speech Recognition with Spontaneous-Speech Adapted Acoustic Models,"This paper presents our work in building an Indonesian speech recognizer to handle both spontaneous and dictated speech. The recognizer is based on the Gaussian Mixture and Hidden Markov Models (GMM-HMM). The model is first trained on 73 hours of dictated speech and 43.5 minutes of spontaneous speech. The dictated speech is read from prepared transcripts by a diverse group of 244 Indonesian speakers. The spontaneous speech is manually labelled from recordings of an Indonesian parliamentary meeting, and is interspersed with noises and fillers. The resulting triphone model is then adapted only to the spontaneous speech using the Maximum A-posteriori Probability (MAP) method. We evaluate the adapted model using separate dictated and spontaneous evaluation sets. The dictated set consists of speech from 20 speakers totaling 14.5 hours. The spontaneous set is derived from the recording of a regional government meeting, consisting of 1085 utterances totaling 48.5 minutes. Evaluation of a MAP-adapted spontaneous set yields a 2.60% absolute increase in Word Accuracy Rate (WAR) over the un-adapted model, outperforming MMI adaptation. Conversely, MMI adaption of the dictated set outperforms the MAP adaptation by achieving an absolute increase of 1.48% in WAR over the un-adapted model. We also demonstrate that fMLLR speaker adaptation is unsuitable for our task due to limited adaptation data.","['https://openalex.org/W4245891875', 'https://openalex.org/W2100969003', 'https://openalex.org/W2021142154', 'https://openalex.org/W2002342963', 'https://openalex.org/W2097048430', 'https://openalex.org/W2066561607', 'https://openalex.org/W4300842365', 'https://openalex.org/W2180473418', 'https://openalex.org/W2051251368', 'https://openalex.org/W1524333225', 'https://openalex.org/W1481751294', 'https://openalex.org/W2547039119']",2016-01-01
https://openalex.org/W4402349786,https://doi.org/10.1145/3678594,EarSpeech: Exploring In-Ear Occlusion Effect on Earphones for Data-efficient Airborne Speech Enhancement,"Earphones have become a popular voice input and interaction device. However, airborne speech is susceptible to ambient noise, making it necessary to improve the quality and intelligibility of speech on earphones in noisy conditions. As the dual-microphone structure (i.e., outer and in-ear microphones) has been widely adopted in earphones (especially ANC earphones), we design EarSpeech which exploits in-ear acoustic sensory as the complementary modality to enable airborne speech enhancement. The key idea of EarSpeech is that in-ear speech is less sensitive to ambient noise and exhibits a correlation with airborne speech. However, due to the occlusion effect, in-ear speech has limited bandwidth, making it challenging to directly correlate with full-band airborne speech. Therefore, we exploit the occlusion effect to carry out theoretical modeling and quantitative analysis of this cross-channel correlation and study how to leverage such cross-channel correlation for speech enhancement. Specifically, we design a series of methodologies including data augmentation, deep learning-based fusion, and noise mixture scheme, to improve the generalization, effectiveness, and robustness of EarSpeech, respectively. Lastly, we conduct real-world experiments to evaluate the performance of our system. Specifically, EarSpeech achieves an average improvement ratio of 27.23% and 13.92% in terms of PESQ and STOI, respectively, and significantly improves SI-SDR by 8.91 dB. Benefiting from data augmentation, EarSpeech can achieve comparable performance with a small-scale dataset that is 40 times less than the original dataset. In addition, we validate the generalization of different users, speech content, and language types, respectively, as well as robustness in the real world via comprehensive experiments. The audio demo of EarSpeech is available on https://github.com/EarSpeech/earspeech.github.io/.","['https://openalex.org/W2144370544', 'https://openalex.org/W2940177920', 'https://openalex.org/W3025051662', 'https://openalex.org/W3195180329', 'https://openalex.org/W4283031696', 'https://openalex.org/W4323854608', 'https://openalex.org/W3129328687', 'https://openalex.org/W4294892047', 'https://openalex.org/W2096779346', 'https://openalex.org/W4387212420', 'https://openalex.org/W3198130403', 'https://openalex.org/W2963828919', 'https://openalex.org/W1635512741', 'https://openalex.org/W4317927963', 'https://openalex.org/W4386699348', 'https://openalex.org/W4404173033', 'https://openalex.org/W4380928279', 'https://openalex.org/W2051057783', 'https://openalex.org/W4288062117', 'https://openalex.org/W3213319487', 'https://openalex.org/W3175702010', 'https://openalex.org/W3136499730', 'https://openalex.org/W4366581013', 'https://openalex.org/W4322731291', 'https://openalex.org/W1494198834', 'https://openalex.org/W2937484199', 'https://openalex.org/W2052666245', 'https://openalex.org/W3153632018', 'https://openalex.org/W3199910673', 'https://openalex.org/W2894651928', 'https://openalex.org/W2075794890', 'https://openalex.org/W3140645045', 'https://openalex.org/W2790428568', 'https://openalex.org/W2120605154', 'https://openalex.org/W4297095639', 'https://openalex.org/W4311187069', 'https://openalex.org/W4283219085', 'https://openalex.org/W2953297989', 'https://openalex.org/W2998161426', 'https://openalex.org/W4200186631', 'https://openalex.org/W2304609584', 'https://openalex.org/W2992628507', 'https://openalex.org/W3205995231', 'https://openalex.org/W4236965008', 'https://openalex.org/W4402349786']",2024-08-22
https://openalex.org/W4398162650,https://doi.org/10.1109/access.2024.3403761,Knowledge Distillation-Based Training of Speech Enhancement for Noise-Robust Automatic Speech Recognition,"This paper addresses the training issues associated with neural network-based automatic speech recognition (ASR) under noise conditions. In particular, conventional joint training approaches for a pipeline comprising speech enhancement (SE) and end-to-end ASR model surfer from a conflicting problem and a frame mismatched alignment problem because of different goals and different frame structures for ASR and SE. To mitigate such problems, a knowledge distillation (KD)-based training approach is proposed by interpreting the ASR and SE models in the pipeline as teacher and student models, respectively. In the proposed KD-based training approach, the ASR model is first trained using a training dataset, and then, acoustic tokens are generated via K-means clustering using the latent vectors of the ASR encoder. Thereafter, KD-based training of the SE model is performed using the generated acoustic tokens. The performance of the SE and ASR models is evaluated on two different databases, noisy LibriSpeech and CHiME-4, which correspond to simulated and real-world noise conditions, respectively. The experimental results show that the proposed KD-based training approach yields a lower character error rate (CER) and word error rate (WER) on the two datasets than conventional joint training approaches, including multi-condition training. The results also show that the speech quality scores of the SE model trained using the proposed training approach are higher than those of SE models trained using conventional training approaches. Moreover, the noise reduction scores of the proposed training approach are higher than those of conventional joint training approaches but slightly lower than those of the standalone-SE training approach. Finally, an ablation study is conducted to examine the contribution of different combinations of loss functions in the proposed training approach to SE and ASR performance. The results show that the combination of all loss functions yields the lowest CER and WER and that tokenizer loss contributes more to SE and ASR performance improvement than ASR encoder loss.","['https://openalex.org/W2962866211', 'https://openalex.org/W3113609448', 'https://openalex.org/W4205767294', 'https://openalex.org/W4205201623', 'https://openalex.org/W3206706278', 'https://openalex.org/W6847363464', 'https://openalex.org/W2147768505', 'https://openalex.org/W6780218876', 'https://openalex.org/W2062164080', 'https://openalex.org/W2327501763', 'https://openalex.org/W2972818416', 'https://openalex.org/W3097777922', 'https://openalex.org/W3095173472', 'https://openalex.org/W6687566353', 'https://openalex.org/W4297727296', 'https://openalex.org/W2117109565', 'https://openalex.org/W3015312544', 'https://openalex.org/W3032514799', 'https://openalex.org/W3096408984', 'https://openalex.org/W2923728956', 'https://openalex.org/W2110878662', 'https://openalex.org/W3180992661', 'https://openalex.org/W6799474351', 'https://openalex.org/W4375869127', 'https://openalex.org/W4286215011', 'https://openalex.org/W3142252347', 'https://openalex.org/W6858642341', 'https://openalex.org/W4372260342', 'https://openalex.org/W1821462560', 'https://openalex.org/W6637551013', 'https://openalex.org/W3128096387', 'https://openalex.org/W4319300241', 'https://openalex.org/W2963453233', 'https://openalex.org/W3081030157', 'https://openalex.org/W4286359908', 'https://openalex.org/W2885724687', 'https://openalex.org/W3133822476', 'https://openalex.org/W6631943919', 'https://openalex.org/W2997591727', 'https://openalex.org/W2191779130', 'https://openalex.org/W1494198834', 'https://openalex.org/W3097906045', 'https://openalex.org/W2962892438', 'https://openalex.org/W2963250244', 'https://openalex.org/W6713134421', 'https://openalex.org/W2067295501', 'https://openalex.org/W2144404214', 'https://openalex.org/W2127851351', 'https://openalex.org/W3097945073', 'https://openalex.org/W6771876938', 'https://openalex.org/W6849819762', 'https://openalex.org/W4221149546', 'https://openalex.org/W3098656025', 'https://openalex.org/W6776700526', 'https://openalex.org/W1690739335', 'https://openalex.org/W3007328579', 'https://openalex.org/W3121132363']",2024-01-01
https://openalex.org/W4391454503,https://doi.org/10.1109/access.2024.3361286,Spatio-Temporal Features Representation Using Recurrent Capsules for Monaural Speech Enhancement,"Single-channel speech enhancement is important for modern communication systems and has received a lot of attention. A convolutional neural network (CNN) successfully learns feature representations from speech spectrograms but loses spatial information due to distortion, which is important for humans to understand speech. Speech feature learning is an important ongoing research to capture higher-level representations of speech that go beyond conventional techniques. By considering the hierarchical structure and temporal relationships within speech signals, capsule networks (CapsNets) have the potential to provide more expressive and context-aware feature representations. By considering the advantages of CapNets over CNN, this study presents a model for monaural speech enhancement that keeps spatial information in a capsule and uses dynamic routing to pass it to higher layers. Dynamic routing replaces the pooling recurrent hidden states to get speech features from the outputs of the capsule. Leveraging long-term contexts provides identification of the target speaker. Therefore, a gated recurrent layer, gated recurrent unit (GRU), or long-short-term memory (LSTM), is placed above the CNN module and next to the capsule module in the architecture. This makes it viable to extract spatial features and long-term temporal dynamics. The suggested convolutional recurrent CapNet performs better compared to the models based on CNNs and recurrent neural networks. The suggested speech enhancement produces considerably better speech quality and intelligibility. With the LibriSpeech and VoiceBank&#x002B;DEMAND databases, the suggested speech enhancement improves the intelligibility and quality by 18.33&#x0025; and (0.94) 36.82&#x0025; over the noisy mixtures.","['https://openalex.org/W2058502470', 'https://openalex.org/W2130178255', 'https://openalex.org/W2102950268', 'https://openalex.org/W2128604088', 'https://openalex.org/W4323338563', 'https://openalex.org/W4226157755', 'https://openalex.org/W4386159007', 'https://openalex.org/W4319998010', 'https://openalex.org/W4317388017', 'https://openalex.org/W4311772362', 'https://openalex.org/W4379380529', 'https://openalex.org/W4312431055', 'https://openalex.org/W4353050204', 'https://openalex.org/W4379193822', 'https://openalex.org/W3198686072', 'https://openalex.org/W3164605550', 'https://openalex.org/W2044893557', 'https://openalex.org/W3049430014', 'https://openalex.org/W2006910250', 'https://openalex.org/W2889890482', 'https://openalex.org/W3081608966', 'https://openalex.org/W2973006943', 'https://openalex.org/W2954198060', 'https://openalex.org/W4285791551', 'https://openalex.org/W4315606033', 'https://openalex.org/W4362579345', 'https://openalex.org/W4384131022', 'https://openalex.org/W4322102269', 'https://openalex.org/W4318833180', 'https://openalex.org/W4324092848', 'https://openalex.org/W4365816291', 'https://openalex.org/W4200079780', 'https://openalex.org/W6743446608', 'https://openalex.org/W4365504037', 'https://openalex.org/W4364323053', 'https://openalex.org/W2943554574', 'https://openalex.org/W2998161426', 'https://openalex.org/W2950448225', 'https://openalex.org/W2972381775', 'https://openalex.org/W2897371647', 'https://openalex.org/W3016447038', 'https://openalex.org/W2962843322', 'https://openalex.org/W2936689732', 'https://openalex.org/W3015844538', 'https://openalex.org/W2940275453', 'https://openalex.org/W2889442120', 'https://openalex.org/W2991361823', 'https://openalex.org/W3165858867', 'https://openalex.org/W2747664154', 'https://openalex.org/W2889450888', 'https://openalex.org/W6783437975', 'https://openalex.org/W2974481323', 'https://openalex.org/W1897240248', 'https://openalex.org/W1494198834', 'https://openalex.org/W4232282348', 'https://openalex.org/W1552314771', 'https://openalex.org/W2141998673', 'https://openalex.org/W2889018940', 'https://openalex.org/W6749897870', 'https://openalex.org/W2364134690', 'https://openalex.org/W6790928573', 'https://openalex.org/W2937484199', 'https://openalex.org/W2963045393', 'https://openalex.org/W3117290926', 'https://openalex.org/W3095057960', 'https://openalex.org/W3096408984', 'https://openalex.org/W3016129867', 'https://openalex.org/W3017350693', 'https://openalex.org/W3086513098', 'https://openalex.org/W1635512741', 'https://openalex.org/W4231807801', 'https://openalex.org/W2963341071', 'https://openalex.org/W3197912330', 'https://openalex.org/W3213188934', 'https://openalex.org/W2998445964', 'https://openalex.org/W3097945073', 'https://openalex.org/W3161950572', 'https://openalex.org/W3197729725', 'https://openalex.org/W2794295502', 'https://openalex.org/W2740616313', 'https://openalex.org/W3139177929', 'https://openalex.org/W6803594737', 'https://openalex.org/W2986634321', 'https://openalex.org/W2626544737', 'https://openalex.org/W3086177986']",2024-01-01
https://openalex.org/W3013104126,https://doi.org/10.5555/1046920.1046926,Information Bottleneck for Gaussian Variables,"The problem of extracting the relevant aspects of data was previously addressed through the information bottleneck (IB) method, through (soft) clustering one variable while preserving information a...",[],2005-12-01
https://openalex.org/W2796704765,https://doi.org/10.48550/arxiv.1804.03599,Understanding disentangling in $\beta$-VAE,"We present new intuitions and theoretical assessments of the emergence of\ndisentangled representation in variational autoencoders. Taking a\nrate-distortion theory perspective, we show the circumstances under which\nrepresentations aligned with the underlying generative factors of variation of\ndata emerge when optimising the modified ELBO bound in $\\beta$-VAE, as training\nprogresses. From these insights, we propose a modification to the training\nregime of $\\beta$-VAE, that progressively increases the information capacity of\nthe latent code during training. This modification facilitates the robust\nlearning of disentangled representations in $\\beta$-VAE, without the previous\ntrade-off in reconstruction accuracy.\n",['https://openalex.org/W2949382160'],2018-04-10
https://openalex.org/W2948947170,https://doi.org/10.18653/v1/p19-1356,What Does BERT Learn about the Structure of Language?,"BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. Our findings are fourfold. BERT’s phrasal representation captures the phrase-level information in the lower layers. The intermediate layers of BERT compose a rich hierarchy of linguistic information, starting with surface features at the bottom, syntactic features in the middle followed by semantic features at the top. BERT requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem. Finally, the compositional scheme underlying BERT mimics classical, tree-like structures.","['https://openalex.org/W2888329843', 'https://openalex.org/W2098921539', 'https://openalex.org/W2963341956', 'https://openalex.org/W2964303116', 'https://openalex.org/W2910243263', 'https://openalex.org/W2187089797', 'https://openalex.org/W2549835527', 'https://openalex.org/W2962776659', 'https://openalex.org/W2912351236', 'https://openalex.org/W2250539671', 'https://openalex.org/W4385245566', 'https://openalex.org/W2951299559', 'https://openalex.org/W2923014074', 'https://openalex.org/W2963310665', 'https://openalex.org/W4288351520', 'https://openalex.org/W2946359678', 'https://openalex.org/W2799124508', 'https://openalex.org/W2097606805', 'https://openalex.org/W2515741950', 'https://openalex.org/W2964165804', 'https://openalex.org/W2906152891', 'https://openalex.org/W4289490673', 'https://openalex.org/W2963013168', 'https://openalex.org/W4288631803', 'https://openalex.org/W2790235966', 'https://openalex.org/W2963430224', 'https://openalex.org/W1840435438', 'https://openalex.org/W2893141505', 'https://openalex.org/W2896457183', 'https://openalex.org/W2064675550', 'https://openalex.org/W2963403868', 'https://openalex.org/W2964204621']",2019-01-01
https://openalex.org/W2753738274,,beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework,"Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.",[],2017-04-24
https://openalex.org/W2952269766,https://doi.org/10.48550/arxiv.1905.07195,CHiVE: Varying Prosody in Speech Synthesis with a Linguistically Driven Dynamic Hierarchical Conditional Variational Network,"The prosodic aspects of speech signals produced by current text-to-speech systems are typically averaged over training material, and as such lack the variety and liveliness found in natural speech. To avoid monotony and averaged prosody contours, it is desirable to have a way of modeling the variation in the prosodic aspects of speech, so audio signals can be synthesized in multiple ways for a given text. We present a new, hierarchically structured conditional variational autoencoder to generate prosodic features (fundamental frequency, energy and duration) suitable for use with a vocoder or a generative model like WaveNet. At inference time, an embedding representing the prosody of a sentence may be sampled from the variational layer to allow for prosodic variation. To efficiently capture the hierarchical nature of the linguistic input (words, syllables and phones), both the encoder and decoder parts of the auto-encoder are hierarchical, in line with the linguistic structure, with layers being clocked dynamically at the respective rates. We show in our experiments that our dynamic hierarchical network outperforms a non-hierarchical state-of-the-art baseline, and, additionally, that prosody transfer across sentences is possible by employing the prosody embedding of one sentence to generate the speech signal of another.","['https://openalex.org/W2149017325', 'https://openalex.org/W2766812927', 'https://openalex.org/W2963782041', 'https://openalex.org/W2884607399', 'https://openalex.org/W2788760202', 'https://openalex.org/W2049510512', 'https://openalex.org/W1959608418', 'https://openalex.org/W2188365844', 'https://openalex.org/W133559434', 'https://openalex.org/W2963609956', 'https://openalex.org/W2102003408', 'https://openalex.org/W2794490148', 'https://openalex.org/W1576227399', 'https://openalex.org/W2259472270', 'https://openalex.org/W2251189452', 'https://openalex.org/W2004590799', 'https://openalex.org/W3020031927', 'https://openalex.org/W2949382160', 'https://openalex.org/W2034277951', 'https://openalex.org/W2962691331', 'https://openalex.org/W2513112113', 'https://openalex.org/W2138660131', 'https://openalex.org/W2964301388', 'https://openalex.org/W2184310502', 'https://openalex.org/W2897548994', 'https://openalex.org/W38355094', 'https://openalex.org/W2524467597', 'https://openalex.org/W2963790827', 'https://openalex.org/W1600722501']",2019-05-17
https://openalex.org/W2907262790,https://doi.org/10.1109/icassp.2019.8683561,Disentangling Correlated Speaker and Noise for Speech Synthesis via Data Augmentation and Adversarial Factorization,"To leverage crowd-sourced data to train multi-speaker text-to-speech (TTS) models that can synthesize clean speech for all speakers, it is essential to learn disentangled representations which can independently control the speaker identity and background noise in generated signals. However, learning such representations can be challenging, due to the lack of labels describing the recording conditions of each training example, and the fact that speakers and recording conditions are often correlated, e.g. since users often make many recordings using the same equipment. This paper proposes three components to address this problem by: (1) formulating a conditional generative model with factorized latent variables, (2) using data augmentation to add noise that is not correlated with speaker identity and whose label is known during training, and (3) using adversarial factorization to improve disentanglement. Experimental results demonstrate that the proposed method can disentangle speaker and noise attributes even if they are correlated in the training data, and can be used to consistently synthesize clean speech for all speakers. Ablation studies verify the importance of each proposed component.","['https://openalex.org/W2114925438', 'https://openalex.org/W6744627333', 'https://openalex.org/W6936113694', 'https://openalex.org/W2559260703', 'https://openalex.org/W2964243274', 'https://openalex.org/W6640963894', 'https://openalex.org/W6637618735', 'https://openalex.org/W2131953535', 'https://openalex.org/W2617258110', 'https://openalex.org/W2963364041', 'https://openalex.org/W2587088898', 'https://openalex.org/W6634468015', 'https://openalex.org/W2795109282', 'https://openalex.org/W6631190155', 'https://openalex.org/W6750489868', 'https://openalex.org/W2962691331', 'https://openalex.org/W2962788625', 'https://openalex.org/W6752888775', 'https://openalex.org/W6755300632', 'https://openalex.org/W6753540710', 'https://openalex.org/W6738277540', 'https://openalex.org/W1494198834', 'https://openalex.org/W2963609956', 'https://openalex.org/W2962684181', 'https://openalex.org/W2796339975', 'https://openalex.org/W6752753409', 'https://openalex.org/W6745117592', 'https://openalex.org/W6640850671', 'https://openalex.org/W2963568578', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963618559', 'https://openalex.org/W2962750142', 'https://openalex.org/W2808697642', 'https://openalex.org/W2794490148', 'https://openalex.org/W2808706139', 'https://openalex.org/W1522301498', 'https://openalex.org/W2753738274', 'https://openalex.org/W2619368999', 'https://openalex.org/W2187089797', 'https://openalex.org/W1956343362', 'https://openalex.org/W2559823555', 'https://openalex.org/W2884607399', 'https://openalex.org/W4295731579', 'https://openalex.org/W2758785877', 'https://openalex.org/W2952161038', 'https://openalex.org/W2963712897', 'https://openalex.org/W2963432880', 'https://openalex.org/W1574170747', 'https://openalex.org/W1959608418', 'https://openalex.org/W1731081199', 'https://openalex.org/W2527729766', 'https://openalex.org/W4289383906', 'https://openalex.org/W2963272440', 'https://openalex.org/W2963927338']",2019-04-17
https://openalex.org/W2951670304,https://doi.org/10.48550/arxiv.1903.10145,Cyclical Annealing Schedule: A Simple Approach to Mitigating KL Vanishing,"Variational autoencoders (VAEs) with an auto-regressive decoder have been applied for many natural language processing (NLP) tasks. The VAE objective consists of two terms, (i) reconstruction and (ii) KL regularization, balanced by a weighting hyper-parameter β. One notorious training difficulty is that the KL term tends to vanish. In this paper we study scheduling schemes for β, and show that KL vanishing is caused by the lack of good latent codes in training the decoder at the beginning of optimization. To remedy this, we propose a cyclical annealing schedule, which repeats the process of increasing βmultiple times. This new procedure allows the progressive learning of more meaningful latent codes, by leveraging the informative representations of previous cycles as warm re-starts. The effectiveness of cyclical annealing is validated on a broad range of NLP tasks, including language modeling, dialog response generation and unsupervised language pre-training.","['https://openalex.org/W2962964508', 'https://openalex.org/W2753738274', 'https://openalex.org/W2963773425', 'https://openalex.org/W2808401846', 'https://openalex.org/W2964222296', 'https://openalex.org/W2963145887', 'https://openalex.org/W2964054038', 'https://openalex.org/W2962897886', 'https://openalex.org/W2064675550', 'https://openalex.org/W2919841361', 'https://openalex.org/W2963592272', 'https://openalex.org/W2963279312', 'https://openalex.org/W2963104724', 'https://openalex.org/W3137695714', 'https://openalex.org/W2622563070', 'https://openalex.org/W2884772108', 'https://openalex.org/W2964127395', 'https://openalex.org/W2526471240', 'https://openalex.org/W2328886022', 'https://openalex.org/W2963134326', 'https://openalex.org/W2964000524', 'https://openalex.org/W2166851633', 'https://openalex.org/W2962883855', 'https://openalex.org/W1632114991', 'https://openalex.org/W2964339599', 'https://openalex.org/W179875071', 'https://openalex.org/W2963600562', 'https://openalex.org/W2749581528', 'https://openalex.org/W2187089797', 'https://openalex.org/W2210838531', 'https://openalex.org/W2963263347', 'https://openalex.org/W2962800561', 'https://openalex.org/W2963858765', 'https://openalex.org/W2612983688', 'https://openalex.org/W2963366196', 'https://openalex.org/W2962717182']",2019-03-25
https://openalex.org/W2982602185,https://doi.org/10.1109/taslp.2019.2950099,A Vector Quantized Variational Autoencoder (VQ-VAE) Autoregressive Neural $F_0$ Model for Statistical Parametric Speech Synthesis,"Recurrent neural networks (RNNs) can predict fundamental frequency (F <sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">0</sub> ) for statistical parametric speech synthesis systems, given linguistic features as input. However, these models assume conditional independence between consecutive F <sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">0</sub> values, given the RNN state. In a previous study, we proposed autoregressive (AR) neural F <sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">0</sub> models to capture the causal dependency of successive F <sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">0</sub> values. In subjective evaluations, a deep AR model (DAR) outperformed an RNN. Here, we propose a Vector Quantized Variational Autoencoder (VQ-VAE) neural F <sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">0</sub> model that is both more efficient and more interpretable than the DAR. This model has two stages: one uses the VQ-VAE framework to learn a latent code for the F <sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">0</sub> contour of each linguistic unit, and other learns to map from linguistic features to latent codes. In contrast to the DAR and RNN, which process the input linguistic features frame-by-frame, the new model converts one linguistic feature vector into one latent code for each linguistic unit. The new model achieves better objective scores than the DAR, has a smaller memory footprint and is computationally faster. Visualization of the latent codes for phones and moras reveals that each latent code represents an F <sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">0</sub> shape for a linguistic unit.","['https://openalex.org/W6730998768', 'https://openalex.org/W2963223306', 'https://openalex.org/W6676943934', 'https://openalex.org/W6634817459', 'https://openalex.org/W4388297464', 'https://openalex.org/W6640963894', 'https://openalex.org/W6753540710', 'https://openalex.org/W6680587008', 'https://openalex.org/W6604953449', 'https://openalex.org/W2808514527', 'https://openalex.org/W6739901393', 'https://openalex.org/W6679436768', 'https://openalex.org/W6606769092', 'https://openalex.org/W1551113872', 'https://openalex.org/W2106564373', 'https://openalex.org/W2032531550', 'https://openalex.org/W2109938215', 'https://openalex.org/W6675380101', 'https://openalex.org/W2067549749', 'https://openalex.org/W2895754873', 'https://openalex.org/W6696843773', 'https://openalex.org/W2767767449', 'https://openalex.org/W6713829033', 'https://openalex.org/W6610843619', 'https://openalex.org/W2891535489', 'https://openalex.org/W6739769550', 'https://openalex.org/W1981450667', 'https://openalex.org/W6630032576', 'https://openalex.org/W2963522141', 'https://openalex.org/W6631943919', 'https://openalex.org/W6631190155', 'https://openalex.org/W2049686551', 'https://openalex.org/W6743855016', 'https://openalex.org/W6606697926', 'https://openalex.org/W6712239235', 'https://openalex.org/W2801493797', 'https://openalex.org/W2752796333', 'https://openalex.org/W6630838124', 'https://openalex.org/W2029434926', 'https://openalex.org/W6601467884', 'https://openalex.org/W6602426571', 'https://openalex.org/W6604530167', 'https://openalex.org/W2964243274', 'https://openalex.org/W6749681018', 'https://openalex.org/W6749489859', 'https://openalex.org/W2111284386', 'https://openalex.org/W6865592293', 'https://openalex.org/W2471520273', 'https://openalex.org/W2129142580', 'https://openalex.org/W6729703263', 'https://openalex.org/W6712208827', 'https://openalex.org/W6690026940', 'https://openalex.org/W2093414802', 'https://openalex.org/W2095705004', 'https://openalex.org/W2194775991', 'https://openalex.org/W2916083071', 'https://openalex.org/W2884607399', 'https://openalex.org/W162654330', 'https://openalex.org/W1579853615', 'https://openalex.org/W2294797155', 'https://openalex.org/W2242818861', 'https://openalex.org/W2964121744', 'https://openalex.org/W1522301498', 'https://openalex.org/W2549013258', 'https://openalex.org/W2963799213', 'https://openalex.org/W2953046278', 'https://openalex.org/W2949382160', 'https://openalex.org/W4395957984', 'https://openalex.org/W2406159955', 'https://openalex.org/W167581994', 'https://openalex.org/W2394761397', 'https://openalex.org/W2394921947', 'https://openalex.org/W2102003408', 'https://openalex.org/W35069904', 'https://openalex.org/W2138660131', 'https://openalex.org/W2112656927', 'https://openalex.org/W1500192039', 'https://openalex.org/W1533861849', 'https://openalex.org/W2641478948', 'https://openalex.org/W304834817', 'https://openalex.org/W1512429158', 'https://openalex.org/W2130942839', 'https://openalex.org/W113106864', 'https://openalex.org/W2963691546', 'https://openalex.org/W2749651610', 'https://openalex.org/W2519091744', 'https://openalex.org/W2394662942', 'https://openalex.org/W1554663460', 'https://openalex.org/W2978963121', 'https://openalex.org/W59470279', 'https://openalex.org/W1959608418', 'https://openalex.org/W566667199', 'https://openalex.org/W2560512785', 'https://openalex.org/W2187089797', 'https://openalex.org/W4385245566', 'https://openalex.org/W2795043217', 'https://openalex.org/W120890421']",2019-10-28
https://openalex.org/W1878590289,,The Architecture of the Festival Speech Synthesis System,"We describe a new formalism for storing linguistic data in a text to speech system. Linguistic entities such as words and phones are stored as feature structures in a general object called an linguistic item. Items are configurable at run time and via the feature structure can contain arbitrary information. Linguistic relations are used to store the relationship between items of the same linguistic type. Relations can take any graph structure but are commonly trees or lists. Utterance structures contain all the items and relations contained in a single utterance. We first describe the design goals when building a synthesis architecture, and then describe some problems with previous architectures. We then discuss our new formalism in general along with the implementation details and consequences of our approach. 1.","['https://openalex.org/W128427206', 'https://openalex.org/W2986828334', 'https://openalex.org/W1541679677', 'https://openalex.org/W2056191164', 'https://openalex.org/W637743698', 'https://openalex.org/W1704572586']",1998-11-01
https://openalex.org/W2964138190,https://doi.org/10.1109/icassp.2019.8683501,Robust and Fine-grained Prosody Control of End-to-end Speech Synthesis,"We propose prosody embeddings for emotional and expressive speech synthesis networks. The proposed methods introduce temporal structures in the embedding networks, thus enabling fine-grained control of the speaking style of the synthesized speech. The temporal structures can be designed either on the speech side or the text side, leading to different control resolutions in time. The prosody embedding networks are plugged into end-to-end speech synthesis networks and trained without any other supervision except for the target speech for synthesizing. It is demonstrated that the prosody embedding networks learned to extract prosodic features. By adjusting the learned prosody features, we could change the pitch and amplitude of the synthesized speech both at the frame level and the phoneme level. We also introduce the temporal normalization of prosody embeddings, which shows better robustness against speaker perturbations during prosody transfer tasks.","['https://openalex.org/W2964243274', 'https://openalex.org/W2584505851', 'https://openalex.org/W6753441378', 'https://openalex.org/W6739901393', 'https://openalex.org/W2795109282', 'https://openalex.org/W6748573829', 'https://openalex.org/W2885800352', 'https://openalex.org/W6750489868', 'https://openalex.org/W4390911804', 'https://openalex.org/W6606697926', 'https://openalex.org/W6765987481', 'https://openalex.org/W2963609956', 'https://openalex.org/W2157331557', 'https://openalex.org/W2619368999', 'https://openalex.org/W2963927338', 'https://openalex.org/W4295731579', 'https://openalex.org/W2962739369', 'https://openalex.org/W4385245566', 'https://openalex.org/W2963712897', 'https://openalex.org/W4298174729', 'https://openalex.org/W2794490148', 'https://openalex.org/W2963623257', 'https://openalex.org/W162654330', 'https://openalex.org/W2963403868', 'https://openalex.org/W2963272440']",2019-04-17
https://openalex.org/W2069859485,https://doi.org/10.1080/01690961003589492,Experimental and theoretical advances in prosody: A review,"Research on prosody has recently become an important focus in various disciplines, including Linguistics, Psychology, and Computer Science. This article reviews recent research advances on two key issues: prosodic phrasing and prosodic prominence. Both aspects of prosody are influenced by linguistic factors such as syntactic constituent structure, semantic relations, phonological rhythm, pragmatic considerations, and also by processing factors such as the length, complexity or predictability of linguistic material. Our review summarizes recent insights into the production and perception of these two components of prosody and their grammatical underpinnings. While this review only covers a subset of a broader set of research topics on prosody in cognitive science, they are representative of a tendency in the field toward a more interdisciplinary approach.","['https://openalex.org/W1991216449', 'https://openalex.org/W2045183093', 'https://openalex.org/W1999739337', 'https://openalex.org/W2140188190', 'https://openalex.org/W1529493719', 'https://openalex.org/W2006660745', 'https://openalex.org/W2030042476', 'https://openalex.org/W4205359331', 'https://openalex.org/W2118938353', 'https://openalex.org/W357827626', 'https://openalex.org/W2242914155', 'https://openalex.org/W2041007470', 'https://openalex.org/W2019597205', 'https://openalex.org/W2075806525', 'https://openalex.org/W2114194912', 'https://openalex.org/W2058230555', 'https://openalex.org/W2137931821', 'https://openalex.org/W2065260746', 'https://openalex.org/W1480428657', 'https://openalex.org/W1974515092', 'https://openalex.org/W2039950366', 'https://openalex.org/W2058388928', 'https://openalex.org/W4244663558', 'https://openalex.org/W2143827132', 'https://openalex.org/W2066789554', 'https://openalex.org/W2170502024', 'https://openalex.org/W294692716', 'https://openalex.org/W2075813705', 'https://openalex.org/W2467574656', 'https://openalex.org/W1970104576', 'https://openalex.org/W1974576139', 'https://openalex.org/W2032369993', 'https://openalex.org/W2022189177', 'https://openalex.org/W2017225830', 'https://openalex.org/W1997485640', 'https://openalex.org/W2108532955', 'https://openalex.org/W12408917', 'https://openalex.org/W1986595082', 'https://openalex.org/W18218877', 'https://openalex.org/W2007054563', 'https://openalex.org/W1984133279', 'https://openalex.org/W2060461562', 'https://openalex.org/W2086400665', 'https://openalex.org/W4249067360', 'https://openalex.org/W4300626136', 'https://openalex.org/W1992658213', 'https://openalex.org/W2145489294', 'https://openalex.org/W2093476462', 'https://openalex.org/W1986632107', 'https://openalex.org/W2065163593', 'https://openalex.org/W4244724785', 'https://openalex.org/W1486114846', 'https://openalex.org/W2155923110', 'https://openalex.org/W2020797341', 'https://openalex.org/W1483979448', 'https://openalex.org/W2013319990', 'https://openalex.org/W2061760940', 'https://openalex.org/W2057007972', 'https://openalex.org/W2195693064', 'https://openalex.org/W2079946605', 'https://openalex.org/W4252859491', 'https://openalex.org/W2157365695', 'https://openalex.org/W2083793305', 'https://openalex.org/W1991145419', 'https://openalex.org/W2062813014', 'https://openalex.org/W2119871874', 'https://openalex.org/W2072025929', 'https://openalex.org/W2054931979', 'https://openalex.org/W2546732126', 'https://openalex.org/W2494193154', 'https://openalex.org/W2007605886', 'https://openalex.org/W2016739703', 'https://openalex.org/W2107807958', 'https://openalex.org/W2149395423', 'https://openalex.org/W4240743726', 'https://openalex.org/W2989769987', 'https://openalex.org/W2081539155', 'https://openalex.org/W1980379097', 'https://openalex.org/W4299831523', 'https://openalex.org/W2085834106', 'https://openalex.org/W2141340689', 'https://openalex.org/W1561222996', 'https://openalex.org/W2030438891', 'https://openalex.org/W2154825638', 'https://openalex.org/W2117685056', 'https://openalex.org/W1981322637', 'https://openalex.org/W2069294353', 'https://openalex.org/W4234249713', 'https://openalex.org/W2080159468', 'https://openalex.org/W1975552354', 'https://openalex.org/W2020755048', 'https://openalex.org/W2096063336', 'https://openalex.org/W1979604178', 'https://openalex.org/W2164105151', 'https://openalex.org/W1983785635', 'https://openalex.org/W2481011921', 'https://openalex.org/W2055019838', 'https://openalex.org/W2035237878', 'https://openalex.org/W1979175996', 'https://openalex.org/W1977553299', 'https://openalex.org/W1999060431', 'https://openalex.org/W2131958453', 'https://openalex.org/W2095279510', 'https://openalex.org/W2076611455', 'https://openalex.org/W2133263371', 'https://openalex.org/W2095730093', 'https://openalex.org/W2150022808', 'https://openalex.org/W2026694978', 'https://openalex.org/W2172042871', 'https://openalex.org/W2088008556', 'https://openalex.org/W1993347555', 'https://openalex.org/W2093820993', 'https://openalex.org/W3000514774', 'https://openalex.org/W4212857348', 'https://openalex.org/W2020789959', 'https://openalex.org/W1608275534', 'https://openalex.org/W178809449', 'https://openalex.org/W591353820', 'https://openalex.org/W2114836066', 'https://openalex.org/W1995759672', 'https://openalex.org/W1513603162', 'https://openalex.org/W1503285781', 'https://openalex.org/W2133388752', 'https://openalex.org/W397522103', 'https://openalex.org/W2108081607', 'https://openalex.org/W4285719527', 'https://openalex.org/W1601527071', 'https://openalex.org/W2156221181', 'https://openalex.org/W2124966737', 'https://openalex.org/W2166919574', 'https://openalex.org/W2081234301', 'https://openalex.org/W57358386', 'https://openalex.org/W2492821218', 'https://openalex.org/W199274192', 'https://openalex.org/W2322065245', 'https://openalex.org/W2150945838', 'https://openalex.org/W114312376', 'https://openalex.org/W2513412827', 'https://openalex.org/W103801057', 'https://openalex.org/W2060505358', 'https://openalex.org/W1566474193', 'https://openalex.org/W2149156201', 'https://openalex.org/W1560370201', 'https://openalex.org/W2937069615', 'https://openalex.org/W76724602', 'https://openalex.org/W1520777603', 'https://openalex.org/W162654330', 'https://openalex.org/W1484196481', 'https://openalex.org/W1545767791', 'https://openalex.org/W1983440413', 'https://openalex.org/W1504955803', 'https://openalex.org/W2124741472', 'https://openalex.org/W2489951887', 'https://openalex.org/W1577874393', 'https://openalex.org/W2609699813', 'https://openalex.org/W2167702024', 'https://openalex.org/W1605334561', 'https://openalex.org/W2612324693', 'https://openalex.org/W1990760258', 'https://openalex.org/W2152397575', 'https://openalex.org/W1519916571', 'https://openalex.org/W1603387868', 'https://openalex.org/W2118657004', 'https://openalex.org/W1947950328', 'https://openalex.org/W1570601904', 'https://openalex.org/W2115316195', 'https://openalex.org/W600536299', 'https://openalex.org/W3150996319', 'https://openalex.org/W2793862515', 'https://openalex.org/W4231741839', 'https://openalex.org/W1598851216', 'https://openalex.org/W2024609983', 'https://openalex.org/W1482843464', 'https://openalex.org/W1685174891', 'https://openalex.org/W1506741762', 'https://openalex.org/W1559701846', 'https://openalex.org/W4298333513', 'https://openalex.org/W655273969', 'https://openalex.org/W2597684388', 'https://openalex.org/W1583314545', 'https://openalex.org/W4381925011', 'https://openalex.org/W2967426412', 'https://openalex.org/W37913285', 'https://openalex.org/W2025341897', 'https://openalex.org/W56103664', 'https://openalex.org/W2340358063', 'https://openalex.org/W2106622015', 'https://openalex.org/W625916238', 'https://openalex.org/W600718022', 'https://openalex.org/W576877571', 'https://openalex.org/W1007634244']",2010-05-26
https://openalex.org/W3022876224,https://doi.org/10.21437/interspeech.2020-1251,CopyCat: Many-to-Many Fine-Grained Prosody Transfer for Neural Text-to-Speech,"Prosody Transfer (PT) is a technique that aims to use the prosody from a\nsource audio as a reference while synthesising speech. Fine-grained PT aims at\ncapturing prosodic aspects like rhythm, emphasis, melody, duration, and\nloudness, from a source audio at a very granular level and transferring them\nwhen synthesising speech in a different target speaker's voice. Current\napproaches for fine-grained PT suffer from source speaker leakage, where the\nsynthesised speech has the voice identity of the source speaker as opposed to\nthe target speaker. In order to mitigate this issue, they compromise on the\nquality of PT. In this paper, we propose CopyCat, a novel, many-to-many PT\nsystem that is robust to source speaker leakage, without using parallel data.\nWe achieve this through a novel reference encoder architecture capable of\ncapturing temporal prosodic representations which are robust to source speaker\nleakage. We compare CopyCat against a state-of-the-art fine-grained PT model\nthrough various subjective evaluations, where we show a relative improvement of\n$47\\%$ in the quality of prosody transfer and $14\\%$ in preserving the target\nspeaker identity, while still maintaining the same naturalness.\n","['https://openalex.org/W2962793481', 'https://openalex.org/W2951004968', 'https://openalex.org/W2972659941', 'https://openalex.org/W2964307104', 'https://openalex.org/W2950821630', 'https://openalex.org/W2972951102', 'https://openalex.org/W3037695135', 'https://openalex.org/W2804078698', 'https://openalex.org/W2188365844', 'https://openalex.org/W2964243274', 'https://openalex.org/W2954386831', 'https://openalex.org/W2963272440', 'https://openalex.org/W4298580827', 'https://openalex.org/W2949281321', 'https://openalex.org/W2963927338', 'https://openalex.org/W1494198834', 'https://openalex.org/W2962691331', 'https://openalex.org/W2519091744', 'https://openalex.org/W2903739847', 'https://openalex.org/W2893749619', 'https://openalex.org/W4394668580', 'https://openalex.org/W2269892441', 'https://openalex.org/W1959608418', 'https://openalex.org/W2937343983', 'https://openalex.org/W2964138190', 'https://openalex.org/W2973158936', 'https://openalex.org/W2904459034', 'https://openalex.org/W2952716587', 'https://openalex.org/W1503398984', 'https://openalex.org/W2901997113', 'https://openalex.org/W2794490148', 'https://openalex.org/W2963609956', 'https://openalex.org/W2945478979']",2020-10-25
https://openalex.org/W3152136404,https://doi.org/10.21437/interspeech.2021-1129,"Fine-Grained Style Modeling, Transfer and Prediction in Text-to-Speech Synthesis via Phone-Level Content-Style Disentanglement","This paper presents a novel design of neural network system for fine-grained\nstyle modeling, transfer and prediction in expressive text-to-speech (TTS)\nsynthesis. Fine-grained modeling is realized by extracting style embeddings\nfrom the mel-spectrograms of phone-level speech segments. Collaborative\nlearning and adversarial learning strategies are applied in order to achieve\neffective disentanglement of content and style factors in speech and alleviate\nthe ""content leakage"" problem in style modeling. The proposed system can be\nused for varying-content speech style transfer in the single-speaker scenario.\nThe results of objective and subjective evaluation show that our system\nperforms better than other fine-grained speech style transfer models,\nespecially in the aspect of content preservation. By incorporating a style\npredictor, the proposed system can also be used for text-to-speech synthesis.\nAudio samples are provided for system demonstration\nhttps://daxintan-cuhk.github.io/pl-csd-speech .\n","['https://openalex.org/W2608207374', 'https://openalex.org/W2794490148', 'https://openalex.org/W2949281321', 'https://openalex.org/W4285881128', 'https://openalex.org/W3021469861', 'https://openalex.org/W2906797124', 'https://openalex.org/W2973158936', 'https://openalex.org/W3168542456', 'https://openalex.org/W3097777922', 'https://openalex.org/W4320013936', 'https://openalex.org/W3047107405', 'https://openalex.org/W2903739847', 'https://openalex.org/W2964243274', 'https://openalex.org/W2747874407', 'https://openalex.org/W3095505419', 'https://openalex.org/W2099471712', 'https://openalex.org/W3015212100', 'https://openalex.org/W2795935804', 'https://openalex.org/W2795109282', 'https://openalex.org/W2965685620', 'https://openalex.org/W2187089797', 'https://openalex.org/W2964138190', 'https://openalex.org/W4295731579', 'https://openalex.org/W2945544731', 'https://openalex.org/W2963609956', 'https://openalex.org/W2890964092', 'https://openalex.org/W2970730223', 'https://openalex.org/W3033411150', 'https://openalex.org/W2897548994', 'https://openalex.org/W2962780374', 'https://openalex.org/W3022876224', 'https://openalex.org/W3130016944', 'https://openalex.org/W2937343983', 'https://openalex.org/W4289383906', 'https://openalex.org/W2997399314', 'https://openalex.org/W2952269766', 'https://openalex.org/W3025165719', 'https://openalex.org/W2972659941', 'https://openalex.org/W2904459034', 'https://openalex.org/W2971753973', 'https://openalex.org/W3135644023', 'https://openalex.org/W2946200149', 'https://openalex.org/W2972667718', 'https://openalex.org/W3095173472', 'https://openalex.org/W2963300588', 'https://openalex.org/W2945478979']",2021-08-27
https://openalex.org/W4285605725,https://doi.org/10.24963/ijcai.2022/577,FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis,"Denoising diffusion probabilistic models (DDPMs) have recently achieved leading performances in many generative tasks. However, the inherited iterative sampling process costs hindered their applications to speech synthesis. This paper proposes FastDiff, a fast conditional diffusion model for high-quality speech synthesis. FastDiff employs a stack of time-aware location-variable convolutions of diverse receptive field patterns to efficiently model long-term time dependencies with adaptive conditions. A noise schedule predictor is also adopted to reduce the sampling steps without sacrificing the generation quality. Based on FastDiff, we design an end-to-end text-to-speech synthesizer, FastDiff-TTS, which generates high-fidelity speech waveforms without any intermediate feature (e.g., Mel-spectrogram). Our evaluation of FastDiff demonstrates the state-of-the-art results with higher-quality (MOS 4.28) speech samples. Also, FastDiff enables a sampling speed of 58x faster than real-time on a V100 GPU, making diffusion models practically applicable to speech synthesis deployment for the first time. We further show that FastDiff generalized well to the mel-spectrogram inversion of unseen speakers, and FastDiff-TTS outperformed other competing methods in end-to-end text-to-speech synthesis. Audio samples are available at https://FastDiff.github.io/.","['https://openalex.org/W4287184558', 'https://openalex.org/W3197273793', 'https://openalex.org/W3169905056', 'https://openalex.org/W3036167779', 'https://openalex.org/W2067295501', 'https://openalex.org/W4226213470', 'https://openalex.org/W3173110665', 'https://openalex.org/W2519091744', 'https://openalex.org/W2963300588', 'https://openalex.org/W2765811365', 'https://openalex.org/W3123097577', 'https://openalex.org/W4285345683', 'https://openalex.org/W3033411150', 'https://openalex.org/W3026874504', 'https://openalex.org/W3092028330', 'https://openalex.org/W3121370741', 'https://openalex.org/W1552314771', 'https://openalex.org/W4226376398', 'https://openalex.org/W4287761884', 'https://openalex.org/W4385245566', 'https://openalex.org/W3129651364', 'https://openalex.org/W3161172673']",2022-07-01
https://openalex.org/W4367359628,https://doi.org/10.1109/taslp.2023.3268730,Diffsound: Discrete Diffusion Model for Text-to-Sound Generation,"Generating sound effects that people want is an important topic. However, there are limited studies in this area for sound generation. In this study, we investigate generating sound conditioned on a text prompt and propose a novel text-to-sound generation framework that consists of a text encoder, a Vector Quantized Variational Autoencoder (VQ-VAE), a token-decoder, and a vocoder. The framework first uses the token-decoder to transfer the text features extracted from the text encoder to a mel-spectrogram with the help of VQ-VAE, and then the vocoder is used to transform the generated mel-spectrogram into a waveform. We found that the token-decoder significantly influences the generation performance. Thus, we focus on designing a good token-decoder in this study. We begin with th21e traditional autoregressive (AR) token-decoder, which has shown state-of-the-art performance in previous sound generation works. However, the AR token-decoder always predicts the mel-spectrogram tokens one by one in order, which may introduce the unidirectional bias and accumulation of errors problems. Moreover, with the AR token-decoder, the sound generation time increases linearly with the sound duration. To overcome the shortcomings introduced by AR token-decoders, we propose a non-autoregressive token-decoder based on the discrete diffusion model, named Diffsound. Specifically, the Diffsound model predicts all of the mel-spectrogram tokens in one step and then refines the predicted tokens in the next step, so the best-predicted results can be obtained by iteration. Our experiments show that our proposed Diffsound model not only produces better text-to-sound generation results when compared with the AR token-decoder but also has a faster generation speed, <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">i.e.</i> , MOS: 3.56 <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">v.s</i> 2.786, and the generation speed is five times faster than the AR decoder. Furthermore, to automatically assess the quality of generated samples, we define three different objective evaluation metrics <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">i.e.</i> , Fréchet Inception Distance (FID), Kullback-Leibler (KL), and audio caption loss, which can comprehensively assess the relevance and fidelity of the generated samples.","['https://openalex.org/W4312388283', 'https://openalex.org/W6757817989', 'https://openalex.org/W6798447524', 'https://openalex.org/W6631190155', 'https://openalex.org/W6765779288', 'https://openalex.org/W2593116425', 'https://openalex.org/W6780226713', 'https://openalex.org/W6798955355', 'https://openalex.org/W3015591594', 'https://openalex.org/W2129069237', 'https://openalex.org/W1956340063', 'https://openalex.org/W6779823529', 'https://openalex.org/W6725318829', 'https://openalex.org/W2963413689', 'https://openalex.org/W6713645886', 'https://openalex.org/W3174285493', 'https://openalex.org/W6755312952', 'https://openalex.org/W2916103538', 'https://openalex.org/W2108598243', 'https://openalex.org/W6783867762', 'https://openalex.org/W2296073425', 'https://openalex.org/W6788990321', 'https://openalex.org/W2963073614', 'https://openalex.org/W6796730497', 'https://openalex.org/W2120847449', 'https://openalex.org/W2985308740', 'https://openalex.org/W2183341477', 'https://openalex.org/W2752796333', 'https://openalex.org/W3015371781', 'https://openalex.org/W6799028840', 'https://openalex.org/W6802805937', 'https://openalex.org/W6767111847', 'https://openalex.org/W2935170919', 'https://openalex.org/W6791353385', 'https://openalex.org/W3198213150', 'https://openalex.org/W6783182287', 'https://openalex.org/W6797095309', 'https://openalex.org/W6795261426', 'https://openalex.org/W6800989748', 'https://openalex.org/W6736996214', 'https://openalex.org/W6810940779', 'https://openalex.org/W6795288823', 'https://openalex.org/W2963807156', 'https://openalex.org/W3046890131', 'https://openalex.org/W2896457183', 'https://openalex.org/W6796163713', 'https://openalex.org/W3180355996', 'https://openalex.org/W6763509872', 'https://openalex.org/W6796242362', 'https://openalex.org/W2972951102', 'https://openalex.org/W6778883912', 'https://openalex.org/W6792105156', 'https://openalex.org/W6790978476', 'https://openalex.org/W6762931180', 'https://openalex.org/W6809885388', 'https://openalex.org/W3172617364', 'https://openalex.org/W3165647589', 'https://openalex.org/W3174758275', 'https://openalex.org/W2506483933', 'https://openalex.org/W4226125322', 'https://openalex.org/W3129576130', 'https://openalex.org/W2963609956', 'https://openalex.org/W4224035735', 'https://openalex.org/W3172148458', 'https://openalex.org/W4287083626', 'https://openalex.org/W3129651364', 'https://openalex.org/W2971074500', 'https://openalex.org/W4292779060', 'https://openalex.org/W3162926177', 'https://openalex.org/W3196163807', 'https://openalex.org/W1522301498', 'https://openalex.org/W2970006822', 'https://openalex.org/W2405756170', 'https://openalex.org/W3166396011', 'https://openalex.org/W3036167779', 'https://openalex.org/W3214281017', 'https://openalex.org/W2519091744', 'https://openalex.org/W3168053944', 'https://openalex.org/W3092028330', 'https://openalex.org/W3136272958', 'https://openalex.org/W2952716587', 'https://openalex.org/W4287329820', 'https://openalex.org/W3185739472', 'https://openalex.org/W2963799213', 'https://openalex.org/W3201409833', 'https://openalex.org/W2908510526', 'https://openalex.org/W3207498282']",2023-01-01
https://openalex.org/W4312388283,https://doi.org/10.1109/cvpr52688.2022.01043,Vector Quantized Diffusion Model for Text-to-Image Synthesis,"We present the vector quantized diffusion (VQ-Diffusion) model for text-to-image generation. This method is based on a vector quantized variational autoencoder (VQ-VAE) whose latent space is modeled by a conditional variant of the recently developed Denoising Diffusion Probabilistic Model (DDPM). We find that this latent-space method is well-suited for text-to-image generation tasks because it not only eliminates the unidirectional bias with existing methods but also allows us to incorporate a mask-and-replace diffusion strategy to avoid the accumulation of errors, which is a serious problem with existing methods. Our experiments show that the VQ-Diffusion produces significantly better text-to-image generation results when compared with conventional autoregressive (AR) models with similar numbers of parameters. Compared with previous GAN-based text-to-image methods, our VQ-Diffusion can handle more complex scenes and improve the synthesized image quality by a large margin. Finally, we show that the image generation computation in our method can be made highly efficient by reparameterization. With traditional AR methods, the text-to-image generation time increases linearly with the output image resolution and hence is quite time consuming even for normal size images. The VQ-Diffusion allows us to achieve a better trade-off between quality and speed. Our experiments indicate that the VQ-Diffusion model with the reparameterization is fifteen times faster than traditional AR methods while achieving a better image quality. The code and models are available at https://github.com/cientgu/VQ-Diffusion.","['https://openalex.org/W2966792645', 'https://openalex.org/W6748634568', 'https://openalex.org/W2963163163', 'https://openalex.org/W6730746255', 'https://openalex.org/W6788990321', 'https://openalex.org/W2557449848', 'https://openalex.org/W2965289598', 'https://openalex.org/W6767384525', 'https://openalex.org/W6766556111', 'https://openalex.org/W6757817989', 'https://openalex.org/W6639102338', 'https://openalex.org/W6791276965', 'https://openalex.org/W6785719018', 'https://openalex.org/W3090238363', 'https://openalex.org/W3091653824', 'https://openalex.org/W3009811209', 'https://openalex.org/W6781951827', 'https://openalex.org/W2962770929', 'https://openalex.org/W6803194594', 'https://openalex.org/W3035574324', 'https://openalex.org/W2963966654', 'https://openalex.org/W2963612019', 'https://openalex.org/W3174525637', 'https://openalex.org/W6780226713', 'https://openalex.org/W3181640983', 'https://openalex.org/W6779688448', 'https://openalex.org/W6765779288', 'https://openalex.org/W6763128402', 'https://openalex.org/W3175528029', 'https://openalex.org/W6779823529', 'https://openalex.org/W6790675931', 'https://openalex.org/W2964216930', 'https://openalex.org/W6713645886', 'https://openalex.org/W4214485011', 'https://openalex.org/W2129069237', 'https://openalex.org/W6748596569', 'https://openalex.org/W2886641317', 'https://openalex.org/W6638575559', 'https://openalex.org/W6802987763', 'https://openalex.org/W2912984712', 'https://openalex.org/W6732492507', 'https://openalex.org/W6794269193', 'https://openalex.org/W2108598243', 'https://openalex.org/W6755207826', 'https://openalex.org/W2533598788', 'https://openalex.org/W6795288823', 'https://openalex.org/W6796242362', 'https://openalex.org/W2933890497', 'https://openalex.org/W6800989748', 'https://openalex.org/W3180355996', 'https://openalex.org/W2903838325', 'https://openalex.org/W6775142816', 'https://openalex.org/W3030163527', 'https://openalex.org/W6755312952', 'https://openalex.org/W3176641147', 'https://openalex.org/W2905434858', 'https://openalex.org/W3035500781', 'https://openalex.org/W6779879114', 'https://openalex.org/W6762931180', 'https://openalex.org/W6735061257', 'https://openalex.org/W6791353385', 'https://openalex.org/W6790978476', 'https://openalex.org/W6763509872', 'https://openalex.org/W2752796333', 'https://openalex.org/W2962845008', 'https://openalex.org/W6767137312', 'https://openalex.org/W2993158499', 'https://openalex.org/W3048484056', 'https://openalex.org/W2405756170', 'https://openalex.org/W2964833232', 'https://openalex.org/W3036167779', 'https://openalex.org/W2896457183', 'https://openalex.org/W2971074500', 'https://openalex.org/W3166396011', 'https://openalex.org/W2788768663', 'https://openalex.org/W3196163807', 'https://openalex.org/W2963799213', 'https://openalex.org/W4287083626', 'https://openalex.org/W3129576130', 'https://openalex.org/W2598991778', 'https://openalex.org/W3108650314', 'https://openalex.org/W3134582802', 'https://openalex.org/W2963413689', 'https://openalex.org/W2952716587', 'https://openalex.org/W4301206121', 'https://openalex.org/W2995665739', 'https://openalex.org/W3206384369', 'https://openalex.org/W3165647589', 'https://openalex.org/W2964122153', 'https://openalex.org/W3039253141', 'https://openalex.org/W2964024144', 'https://openalex.org/W2970562079', 'https://openalex.org/W4385245566', 'https://openalex.org/W3034445277', 'https://openalex.org/W2908510526', 'https://openalex.org/W2972328244', 'https://openalex.org/W1861492603', 'https://openalex.org/W3155072588', 'https://openalex.org/W3128876955', 'https://openalex.org/W4320013936', 'https://openalex.org/W4292779060', 'https://openalex.org/W3168053944', 'https://openalex.org/W2962784628', 'https://openalex.org/W2953318193', 'https://openalex.org/W3162926177', 'https://openalex.org/W3209532394', 'https://openalex.org/W3096601784']",2022-06-01
https://openalex.org/W4372266896,https://doi.org/10.1109/icassp49357.2023.10094639,EfficientSpeech: An On-Device Text to Speech Model,"State of the art (SOTA) neural text to speech (TTS) models can generate natural-sounding synthetic voices. These models are characterized by large memory footprints and substantial number of operations due to the long-standing focus on speech quality with cloud inference in mind. Neural TTS models are generally not designed to perform standalone speech syntheses on resource-constrained and no Internet access edge devices. In this work, an efficient neural TTS called EfficientSpeech that synthesizes speech on an ARM CPU in real-time is proposed. EfficientSpeech uses a shallow non-autoregressive pyramid-structure transformer forming a U-Network. EfficientSpeech has 266k parameters and consumes 90 MFLOPS only or about 1% of the size and amount of computation in modern compact models such as Mixer-TTS. EfficientSpeech achieves an average mel generation real-time factor of 104.3 on an RPi4. Human evaluation shows only a slight degradation in audio quality as compared to FastSpeech2.","['https://openalex.org/W6783867762', 'https://openalex.org/W6796464841', 'https://openalex.org/W6917585676', 'https://openalex.org/W6639824700', 'https://openalex.org/W3160919572', 'https://openalex.org/W3034949308', 'https://openalex.org/W6778823374', 'https://openalex.org/W6755977528', 'https://openalex.org/W6739901393', 'https://openalex.org/W6797399245', 'https://openalex.org/W6780226713', 'https://openalex.org/W6803464654', 'https://openalex.org/W6757817989', 'https://openalex.org/W6767671539', 'https://openalex.org/W6633684334', 'https://openalex.org/W2747874407', 'https://openalex.org/W349236604', 'https://openalex.org/W3197649190', 'https://openalex.org/W4210684870', 'https://openalex.org/W6802142237', 'https://openalex.org/W6749489859', 'https://openalex.org/W2964243274', 'https://openalex.org/W6801525743', 'https://openalex.org/W2903739847', 'https://openalex.org/W2974231335', 'https://openalex.org/W2963691546', 'https://openalex.org/W3211490618', 'https://openalex.org/W2908510526', 'https://openalex.org/W2899663614', 'https://openalex.org/W4286950013', 'https://openalex.org/W3203394408', 'https://openalex.org/W4385245566', 'https://openalex.org/W3169905056', 'https://openalex.org/W1901129140', 'https://openalex.org/W3150572638', 'https://openalex.org/W3092028330', 'https://openalex.org/W1563089615', 'https://openalex.org/W3033411150']",2023-05-05
https://openalex.org/W2785860501,https://doi.org/10.1109/asru.2017.8269014,Unsupervised HMM posteriograms for language independent acoustic modeling in zero resource conditions,"The task of language independent acoustic unit modeling in unlabeled raw speech (zero-resource setting) has gained significant interest over the recent years. The main challenge here is the extraction of acoustic representations that elicit good similarity between the same words or linguistic tokens spoken by different speakers and to derive these representations in a language independent manner. In this paper, we explore the use of Hidden Markov Model (HMM) based posteriograms for unsupervised acoustic unit modeling. The states of the HMM (which represent the language independent acoustic units) are initialized using a Gaussian mixture model (GMM) - Universal Background Model (UBM). The trained HMM is subsequently used to generate a temporally contiguous state alignment which are then modeled in a hybrid deep neural network (DNN) model. For the purpose of testing, we use the frame level HMM state posteriors obtained from the DNN as features for the ZeroSpeech challenge task. The minimal pair ABX error rate is measured for both the within and across speaker pairs. With several experiments on multiple languages in the ZeroSpeech corpus, we show that the proposed HMM based posterior features provides significant improvements over the baseline system using MFCC features (average relative improvements of 25% for within speaker pairs and 40% for across speaker pairs). Furthermore, the experiments where the target language is not seen training illustrate the proposed modeling approach is capable of learning global language independent representations.","['https://openalex.org/W2335112305', 'https://openalex.org/W6695606915', 'https://openalex.org/W2963620343', 'https://openalex.org/W2078769636', 'https://openalex.org/W2117041980', 'https://openalex.org/W2126203737', 'https://openalex.org/W6712202099', 'https://openalex.org/W2041823554', 'https://openalex.org/W2127982613', 'https://openalex.org/W6973666849', 'https://openalex.org/W1970890968', 'https://openalex.org/W6638159135', 'https://openalex.org/W6713745070', 'https://openalex.org/W2086115904', 'https://openalex.org/W2095458199', 'https://openalex.org/W6713256719', 'https://openalex.org/W6633431331', 'https://openalex.org/W2115008841', 'https://openalex.org/W6682825348', 'https://openalex.org/W2787223168', 'https://openalex.org/W6631362777', 'https://openalex.org/W1796128977', 'https://openalex.org/W330298975', 'https://openalex.org/W2404799143', 'https://openalex.org/W2786608204', 'https://openalex.org/W1560013842', 'https://openalex.org/W2406349064', 'https://openalex.org/W2286443923', 'https://openalex.org/W2396043527', 'https://openalex.org/W1553004968', 'https://openalex.org/W2152175008', 'https://openalex.org/W1524333225']",2017-12-01
https://openalex.org/W2787223168,https://doi.org/10.1109/asru.2017.8269013,Deep learning methods for unsupervised acoustic modeling — Leap submission to ZeroSpeech challenge 2017,"In this paper, we present our system submission to the ZeroSpeech 2017 Challenge. The track1 of this challenge is intended to develop language independent speech representations that provide the least pairwise ABX distance computed for within speaker and across speaker pairs of spoken words. We investigate two approaches based on deep learning methods for unsupervised modeling. In the first approach, a deep neural network (DNN) is trained on the posteriors of mixture component indices obtained from training a Gaussian mixture model (GMM)-UBM. In the second approach, we develop a similar hidden Markov model (HMM) based DNN model to learn the unsupervised acoustic units provided by HMM state alignments. In addition, we also develop a deep autoencoder which learns language independent embeddings of speech to train the HMM-DNN model. Both the approaches do not use any labeled training data or require any supervision. We perform several experiments using the ZeroSpeech 2017 corpus with the minimal pair ABX error measure. In these experiments, we find that the two proposed approaches significantly improve over the baseline system using MFCC features (average relative improvements of 30–40%). Furthermore, the system combination of the two proposed approaches improves the performance over the best individual system.","['https://openalex.org/W6713256719', 'https://openalex.org/W4231109964', 'https://openalex.org/W6685777803', 'https://openalex.org/W2120480077', 'https://openalex.org/W6682825348', 'https://openalex.org/W2041823554', 'https://openalex.org/W2785860501', 'https://openalex.org/W6973666849', 'https://openalex.org/W2009388533', 'https://openalex.org/W2126203737', 'https://openalex.org/W2963620343', 'https://openalex.org/W6638159135', 'https://openalex.org/W6713745070', 'https://openalex.org/W6601311673', 'https://openalex.org/W2117041980', 'https://openalex.org/W2513125788', 'https://openalex.org/W2115008841', 'https://openalex.org/W6631362777', 'https://openalex.org/W2072128103', 'https://openalex.org/W30845872', 'https://openalex.org/W1553004968', 'https://openalex.org/W222076935', 'https://openalex.org/W2406349064', 'https://openalex.org/W1796128977', 'https://openalex.org/W1524333225', 'https://openalex.org/W1560013842', 'https://openalex.org/W2404799143', 'https://openalex.org/W2786608204', 'https://openalex.org/W2152175008', 'https://openalex.org/W2181347294']",2017-12-01
https://openalex.org/W2935542736,https://doi.org/10.21437/interspeech.2019-2605,Learning Problem-Agnostic Speech Representations from Multiple Self-Supervised Tasks,"Learning good representations without supervision is still an open issue in machine learning, and is particularly challenging for speech signals, which are often characterized by long sequences with a complex hierarchical structure. Some recent works, however, have shown that it is possible to derive useful speech representations by employing a self-supervised encoder-discriminator approach. This paper proposes an improved self-supervised method, where a single neural encoder is followed by multiple workers that jointly solve different self-supervised tasks. The needed consensus across different tasks naturally imposes meaningful constraints to the encoder, contributing to discover general representations and to minimize the risk of learning superficial ones. Experiments show that the proposed approach can learn transferable, robust, and problem-agnostic features that carry on relevant information from the speech signal, such as speaker identity, phonemes, and even higher-level features such as emotional cues. In addition, a number of design choices make the encoder easily exportable, facilitating its direct usage or adaptation to different problems.","['https://openalex.org/W2963341071', 'https://openalex.org/W2962969419', 'https://openalex.org/W2794209590', 'https://openalex.org/W3125709657', 'https://openalex.org/W1494198834', 'https://openalex.org/W2902368158', 'https://openalex.org/W2183341477', 'https://openalex.org/W2963661130', 'https://openalex.org/W2148154194', 'https://openalex.org/W2842511635', 'https://openalex.org/W2964052309', 'https://openalex.org/W2295582178', 'https://openalex.org/W1836465849', 'https://openalex.org/W2963115079', 'https://openalex.org/W2401942008', 'https://openalex.org/W2964227577', 'https://openalex.org/W1524333225', 'https://openalex.org/W2962824366', 'https://openalex.org/W2964147121', 'https://openalex.org/W1677182931', 'https://openalex.org/W2963127222', 'https://openalex.org/W3209382000', 'https://openalex.org/W2887997457', 'https://openalex.org/W2962742544', 'https://openalex.org/W2147768505', 'https://openalex.org/W2962736520', 'https://openalex.org/W2964121744', 'https://openalex.org/W2767754137', 'https://openalex.org/W1959608418', 'https://openalex.org/W2963241221', 'https://openalex.org/W2962835968', 'https://openalex.org/W1924770834', 'https://openalex.org/W2963799213', 'https://openalex.org/W2527729766', 'https://openalex.org/W2963714041', 'https://openalex.org/W2487442924', 'https://openalex.org/W3127686677', 'https://openalex.org/W2099471712', 'https://openalex.org/W2110798204', 'https://openalex.org/W2901616798', 'https://openalex.org/W2328055', 'https://openalex.org/W2136922672', 'https://openalex.org/W3035219538']",2019-09-13
https://openalex.org/W3035725276,https://doi.org/10.1109/tkde.2021.3090866,Self-supervised Learning: Generative or Contrastive,"Deep supervised learning has achieved great success in the last decade. However, its deficiencies of dependence on manual labels and vulnerability to attacks have driven people to explore a better solution. As an alternative, self-supervised learning attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further investigate related theoretical analysis work to provide deeper thoughts on how self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided.","['https://openalex.org/W2102605133', 'https://openalex.org/W6747899497', 'https://openalex.org/W6715501732', 'https://openalex.org/W343636949', 'https://openalex.org/W6714644935', 'https://openalex.org/W6635084905', 'https://openalex.org/W6637618735', 'https://openalex.org/W6714590955', 'https://openalex.org/W2792234394', 'https://openalex.org/W6765052341', 'https://openalex.org/W2891649471', 'https://openalex.org/W6755207826', 'https://openalex.org/W2962922117', 'https://openalex.org/W2596763562', 'https://openalex.org/W6744957266', 'https://openalex.org/W6771917389', 'https://openalex.org/W2964110616', 'https://openalex.org/W2962904108', 'https://openalex.org/W2108598243', 'https://openalex.org/W6762931180', 'https://openalex.org/W2113896236', 'https://openalex.org/W2963748441', 'https://openalex.org/W6758706709', 'https://openalex.org/W6772452955', 'https://openalex.org/W6729956949', 'https://openalex.org/W6779101013', 'https://openalex.org/W2808856341', 'https://openalex.org/W2998388430', 'https://openalex.org/W6745992979', 'https://openalex.org/W2788919350', 'https://openalex.org/W2326925005', 'https://openalex.org/W2558661413', 'https://openalex.org/W2953356739', 'https://openalex.org/W3099700870', 'https://openalex.org/W3011411500', 'https://openalex.org/W6758354414', 'https://openalex.org/W2963073614', 'https://openalex.org/W2738588019', 'https://openalex.org/W6725739302', 'https://openalex.org/W3012871709', 'https://openalex.org/W3080997787', 'https://openalex.org/W6760212410', 'https://openalex.org/W6741832134', 'https://openalex.org/W6763416564', 'https://openalex.org/W6770949304', 'https://openalex.org/W6604803494', 'https://openalex.org/W2906943923', 'https://openalex.org/W6754278344', 'https://openalex.org/W6690026940', 'https://openalex.org/W6779518175', 'https://openalex.org/W6774222543', 'https://openalex.org/W2194775991', 'https://openalex.org/W6770717842', 'https://openalex.org/W6779326418', 'https://openalex.org/W6791753252', 'https://openalex.org/W6682948231', 'https://openalex.org/W2962756421', 'https://openalex.org/W6739901393', 'https://openalex.org/W2518754566', 'https://openalex.org/W2963470893', 'https://openalex.org/W2949182780', 'https://openalex.org/W2919115771', 'https://openalex.org/W2963169753', 'https://openalex.org/W2599837529', 'https://openalex.org/W1903029394', 'https://openalex.org/W6762963088', 'https://openalex.org/W6780248173', 'https://openalex.org/W2612769033', 'https://openalex.org/W6766673545', 'https://openalex.org/W3035160371', 'https://openalex.org/W6770982027', 'https://openalex.org/W2964060161', 'https://openalex.org/W2798991696', 'https://openalex.org/W6636510571', 'https://openalex.org/W6770825270', 'https://openalex.org/W6783235295', 'https://openalex.org/W6763701032', 'https://openalex.org/W2962852342', 'https://openalex.org/W2962770929', 'https://openalex.org/W2889787757', 'https://openalex.org/W6752910514', 'https://openalex.org/W2963826423', 'https://openalex.org/W6640963894', 'https://openalex.org/W6726873649', 'https://openalex.org/W6751455638', 'https://openalex.org/W6730084236', 'https://openalex.org/W6752306858', 'https://openalex.org/W6768841368', 'https://openalex.org/W6784694379', 'https://openalex.org/W6684191040', 'https://openalex.org/W6779119530', 'https://openalex.org/W6768021236', 'https://openalex.org/W2963261224', 'https://openalex.org/W2952205826', 'https://openalex.org/W2308529009', 'https://openalex.org/W6634441602', 'https://openalex.org/W2966694634', 'https://openalex.org/W2809583854', 'https://openalex.org/W6638304892', 'https://openalex.org/W3036446966', 'https://openalex.org/W1932742904', 'https://openalex.org/W6763846873', 'https://openalex.org/W6771848067', 'https://openalex.org/W2154851992', 'https://openalex.org/W6738394178', 'https://openalex.org/W6774420841', 'https://openalex.org/W6740528845', 'https://openalex.org/W2985951359', 'https://openalex.org/W2607500032', 'https://openalex.org/W2970641574', 'https://openalex.org/W6763813028', 'https://openalex.org/W2998269939', 'https://openalex.org/W6766156693', 'https://openalex.org/W3011574394', 'https://openalex.org/W6685352114', 'https://openalex.org/W2493916176', 'https://openalex.org/W6755312952', 'https://openalex.org/W6746348307', 'https://openalex.org/W2883725317', 'https://openalex.org/W6779997284', 'https://openalex.org/W6774314701', 'https://openalex.org/W6763442200', 'https://openalex.org/W6779977557', 'https://openalex.org/W6784392697', 'https://openalex.org/W2270070752', 'https://openalex.org/W2640408555', 'https://openalex.org/W6771137614', 'https://openalex.org/W6774670964', 'https://openalex.org/W3035164673', 'https://openalex.org/W6777179611', 'https://openalex.org/W6786614245', 'https://openalex.org/W6748582592', 'https://openalex.org/W6761910064', 'https://openalex.org/W6759628261', 'https://openalex.org/W2022322548', 'https://openalex.org/W6682691769', 'https://openalex.org/W1888005072', 'https://openalex.org/W6766489549', 'https://openalex.org/W2963420272', 'https://openalex.org/W2423557781', 'https://openalex.org/W2752796333', 'https://openalex.org/W2321533354', 'https://openalex.org/W2963465221', 'https://openalex.org/W6844194202']",2021-01-01
https://openalex.org/W1949782964,https://doi.org/10.1109/icassp.1987.1169589,An investigation on the use of acoustic sub-word units for automatic speech recognition,"An approach to automatic speech recognition is described which attempts to link together ideas from pattern recognition such as dynamic time warping and hidden Markov modeling, with ideas from linguistically motivated approaches. In this approach, the basic sub-word units are defined acoustically, but not necessarily phonetically. An algorithm was developed which automatically decomposed speech into multiple sub-word segments, based solely upon strict acoustic criteria, without any reference to linguistic content. By repeating this procedure on a large corpus of speech data we obtained an extensive pool of unlabeled sub-word speech segments. Then using well defined clustering techniques, a small set of representative acoustic sub-word units (e.g. an inventory of units) was created. This process is fast, easy to use, and required no human intervention. The interpretation of these sub-word units, in a linguistic sense, in the context of word decoding is an important issue which must be addressed for them to be useful in a large vocabulary system. We have not yet addressed this issue; instead a couple of simple experiments were performed to determine if these acoustic sub-word units had any potential value for speech recognition. For these experiments we used a connected digits database from a single female talker. A 25 sub-word unit codebook of acoustic segments was created from about 1600 segments drawn from 100 connected digit strings. A simple isolated digit recognition system, designed using the statistics of the codewords in the acoustic sub-word unit codebook had a recognition accuracy of 100%. In another experiment a connected digit recognition system was created with representative digit templates created by concatenating the sub-word units in an appropriate manner. The system had a string recognition accuracy of 96%.","['https://openalex.org/W1919801718', 'https://openalex.org/W2169248114', 'https://openalex.org/W1884432169', 'https://openalex.org/W2152131029', 'https://openalex.org/W1989337816', 'https://openalex.org/W2048648518', 'https://openalex.org/W6871709107', 'https://openalex.org/W2105594594', 'https://openalex.org/W2065625684', 'https://openalex.org/W1990005915', 'https://openalex.org/W1975598412', 'https://openalex.org/W2144195083', 'https://openalex.org/W2057833190', 'https://openalex.org/W111481704', 'https://openalex.org/W4401371936', 'https://openalex.org/W1981864244', 'https://openalex.org/W1994859265', 'https://openalex.org/W1542057217']",2005-03-24
https://openalex.org/W1957665339,https://doi.org/10.1109/icassp.1988.196629,A segment model based approach to speech recognition,"Proposes a global acoustic segment model for characterizing fundamental speech sound units and their interactions based upon a general framework of hidden Markov models (HMM). Each segment model represents a class of acoustically similar sounds. The intra-segment variability of each sound class is modeled by an HMM, and the sound-to-sound transition rules are characterized by a probabilistic intersegment transition matrix. An acoustically-derived lexicon is used to construct word models based upon subword segment models. The proposed segment model was tested on a speaker-trained, isolated word, speech recognition task with a vocabulary of 1109 basic English words. In the current study, only 128 segment models were used, and recognition was performed by optimally aligning the test utterance with all acoustic lexicon entries using a maximum likelihood Viterbi decoding algorithm. Based upon a database of three male speakers, the average word recognition accuracy for the top candidate was 85% and increased to 96% and 98% for the top 3 and top 5 candidates, respectively.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>","['https://openalex.org/W2134383396', 'https://openalex.org/W1950396994', 'https://openalex.org/W1987294319', 'https://openalex.org/W6779487094', 'https://openalex.org/W3035139526', 'https://openalex.org/W596388300']",2003-01-06
https://openalex.org/W47568227,https://doi.org/10.5445/ir/44598,Multilingual and crosslingual speech recognition,"This paper describes the design of a multilingual speech recognizer using an LVCSR dictation database which has been collected under the project GlobalPhone. This project at the University of Karlsruhe investigates LVCSR systems in 15 languages of the world, namely Arabic, Chinese, Croatian, English, French, German, Italian, Japanese, Korean, Portuguese, Russian, Spanish, Swedish, Tamil, and Turkish. For our experiments we used six of these languages to train and test several recognition engines in monolingual, multilingual and crosslingual setups. Based on a global phoneme set we built a multilingual speech recognition system which can handle five different languages. The acoustic models of the five languages are combined into a monolithic system and context dependent phoneme models are created using language questions.","['https://openalex.org/W2587033535', 'https://openalex.org/W2401033099', 'https://openalex.org/W311138290', 'https://openalex.org/W73572011', 'https://openalex.org/W154262204', 'https://openalex.org/W2132534451', 'https://openalex.org/W2105354660', 'https://openalex.org/W3019319563']",1998-01-01
https://openalex.org/W2787248994,https://doi.org/10.1017/9781108231596,High-Dimensional Probability: An Introduction with Applications in Data Science,"High-dimensional probability offers insight into the behavior of random vectors, random matrices, random subspaces, and objects used to quantify uncertainty in high dimensions. Drawing on ideas from probability, analysis, and geometry, it lends itself to applications in mathematics, statistics, theoretical computer science, signal processing, optimization, and more. It is the first to integrate theory, key tools, and modern applications of high-dimensional probability. Concentration inequalities form the core, and it covers both classical results such as Hoeffding's and Chernoff's inequalities and modern developments such as the matrix Bernstein's inequality. It then introduces the powerful methods based on stochastic processes, including such tools as Slepian's, Sudakov's, and Dudley's inequalities, as well as generic chaining and bounds based on VC dimension. A broad range of illustrations is embedded throughout, including classical and modern results for covariance estimation, clustering, networks, semidefinite programming, coding, dimension reduction, matrix completion, machine learning, compressed sensing, and sparse regression.",[],2018-09-27
https://openalex.org/W2934852845,https://doi.org/10.48550/arxiv.1904.04100,Completely Unsupervised Speech Recognition By A Generative Adversarial Network Harmonized With Iteratively Refined Hidden Markov Models,"Producing a large annotated speech corpus for training ASR systems remains difficult for more than 95% of languages all over the world which are low-resourced, but collecting a relatively big unlabeled data set for such languages is more achievable. This is why some initial effort have been reported on completely unsupervised speech recognition learned from unlabeled data only, although with relatively high error rates. In this paper, we develop a Generative Adversarial Network (GAN) to achieve this purpose, in which a Generator and a Discriminator learn from each other iteratively to improve the performance. We further use a set of Hidden Markov Models (HMMs) iteratively refined from the machine generated labels to work in harmony with the GAN. The initial experiments on TIMIT data set achieve an phone error rate of 33.1%, which is 8.5% lower than the previous state-of-the-art.","['https://openalex.org/W2962799225', 'https://openalex.org/W2963340922', 'https://openalex.org/W2190506272', 'https://openalex.org/W2147768505', 'https://openalex.org/W111477576', 'https://openalex.org/W1524333225', 'https://openalex.org/W2059652594', 'https://openalex.org/W2401725913', 'https://openalex.org/W854541894', 'https://openalex.org/W2130942839', 'https://openalex.org/W1828163288', 'https://openalex.org/W2950577311', 'https://openalex.org/W2964268978', 'https://openalex.org/W2099471712', 'https://openalex.org/W2295088914', 'https://openalex.org/W2157331557', 'https://openalex.org/W2962879692', 'https://openalex.org/W1577418252', 'https://openalex.org/W2962799131', 'https://openalex.org/W2963602293', 'https://openalex.org/W2618238855', 'https://openalex.org/W2962824887', 'https://openalex.org/W2030422732', 'https://openalex.org/W2739748921', 'https://openalex.org/W1635512741', 'https://openalex.org/W2963425185', 'https://openalex.org/W2899377381', 'https://openalex.org/W2964079874', 'https://openalex.org/W2963118869', 'https://openalex.org/W2884305338']",2019-04-08
https://openalex.org/W2164505566,https://doi.org/10.21437/interspeech.2009-20,Cross-language bootstrapping for unsupervised acoustic model training: rapid development of a Polish speech recognition system,"This paper describes the rapid development of a Polish language speech recognition system.The system development was performed without access to any transcribed acoustic training data.This was achieved through the combined use of cross-language bootstrapping and confidence based unsupervised acoustic model training.A Spanish acoustic model was ported to Polish, through the use of a manually constructed phoneme mapping.This initial model was refined through iterative recognition and retraining of the untranscribed audio data.The system was trained and evaluated on recordings from the European Parliament, and included several state-of-the-art speech recognition techniques in addition to the use of unsupervised model training.Confidence based speaker adaptive training using features space transform adaptation, as well as vocal tract length normalization and maximum likelihood linear regression, was used to refine the acoustic model.Through the combination of the different techniques, good performance was achieved on the domain of parliamentary speeches.","['https://openalex.org/W1553410761', 'https://openalex.org/W144245531', 'https://openalex.org/W2150557579', 'https://openalex.org/W60369657', 'https://openalex.org/W7869021', 'https://openalex.org/W1484181928', 'https://openalex.org/W2056786202', 'https://openalex.org/W2171761326', 'https://openalex.org/W23007503', 'https://openalex.org/W2143860348', 'https://openalex.org/W170072012', 'https://openalex.org/W2161966221', 'https://openalex.org/W2129334286', 'https://openalex.org/W2110073835', 'https://openalex.org/W199380068', 'https://openalex.org/W15497043']",2009-09-06
https://openalex.org/W2120209245,https://doi.org/10.1109/slt.2012.6424230,Unsupervised cross-lingual knowledge transfer in DNN-based LVCSR,"We investigate the use of cross-lingual acoustic data to initialise deep neural network (DNN) acoustic models by means of unsupervised restricted Boltzmann machine (RBM) pre-training. DNNs for German are pretrained using one or all of German, Portuguese, Spanish and Swedish. The DNNs are used in a tandem configuration, where the network outputs are used as features for a hidden Markov model (HMM) whose emission densities are modeled by Gaussian mixture models (GMMs), as well as in a hybrid configuration, where the network outputs are used as the HMM state likelihoods. The experiments show that unsupervised pretraining is more crucial for the hybrid setups, particularly with limited amounts of transcribed training data. More importantly, unsupervised pretraining is shown to be language-independent.","['https://openalex.org/W2108586753', 'https://openalex.org/W2131042651', 'https://openalex.org/W2407897255', 'https://openalex.org/W1993882792', 'https://openalex.org/W2147768505', 'https://openalex.org/W2169189000', 'https://openalex.org/W2110871230', 'https://openalex.org/W2012897754', 'https://openalex.org/W811578723', 'https://openalex.org/W2160306971', 'https://openalex.org/W6676481782', 'https://openalex.org/W6602682705', 'https://openalex.org/W6631362777', 'https://openalex.org/W2124629003', 'https://openalex.org/W2125234026', 'https://openalex.org/W2152175008', 'https://openalex.org/W6680300913', 'https://openalex.org/W2090764203', 'https://openalex.org/W7869021', 'https://openalex.org/W2165712214', 'https://openalex.org/W2033436836', 'https://openalex.org/W1991180839', 'https://openalex.org/W2123798005', 'https://openalex.org/W6681637299', 'https://openalex.org/W2127982613', 'https://openalex.org/W2136922672', 'https://openalex.org/W2133267619', 'https://openalex.org/W2079508481', 'https://openalex.org/W2147276082', 'https://openalex.org/W2110798204', 'https://openalex.org/W66627554', 'https://openalex.org/W2138857742', 'https://openalex.org/W2606321545', 'https://openalex.org/W4285719527', 'https://openalex.org/W1553004968', 'https://openalex.org/W1524333225']",2012-12-01
https://openalex.org/W2950057603,https://doi.org/10.48550/arxiv.1806.06734,Unsupervised Word Segmentation from Speech with Attention,"We present a first attempt to perform attentional word segmentation directly from the speech signal, with the final goal to automatically identify lexical units in a low-resource, unwritten language (UL). Our methodology assumes a pairing between recordings in the UL with translations in a well-resourced language. It uses Acoustic Unit Discovery (AUD) to convert speech into a sequence of pseudo-phones that is segmented using neural soft-alignments produced by a neural machine translation model. Evaluation uses an actual Bantu UL, Mboshi; comparisons to monolingual and bilingual baselines illustrate the potential of attentional word segmentation for language documentation.","['https://openalex.org/W4294562888', 'https://openalex.org/W2483390977', 'https://openalex.org/W2950613790', 'https://openalex.org/W2025482506', 'https://openalex.org/W4317473142', 'https://openalex.org/W2762715843', 'https://openalex.org/W2962693497', 'https://openalex.org/W2166851633', 'https://openalex.org/W2133564696', 'https://openalex.org/W2962695963', 'https://openalex.org/W2345799635', 'https://openalex.org/W2107038463', 'https://openalex.org/W2347098582', 'https://openalex.org/W2466918907', 'https://openalex.org/W2126377586', 'https://openalex.org/W2055408826', 'https://openalex.org/W2963620343', 'https://openalex.org/W4300845022', 'https://openalex.org/W2347145335', 'https://openalex.org/W1833498382', 'https://openalex.org/W166614460', 'https://openalex.org/W2964308564', 'https://openalex.org/W2963819008', 'https://openalex.org/W2564058731', 'https://openalex.org/W2101281673', 'https://openalex.org/W2057007397', 'https://openalex.org/W2251025892', 'https://openalex.org/W1778492285', 'https://openalex.org/W192980855', 'https://openalex.org/W4300047444', 'https://openalex.org/W1959608418', 'https://openalex.org/W2464234964', 'https://openalex.org/W2586232309', 'https://openalex.org/W191292882', 'https://openalex.org/W2756675017', 'https://openalex.org/W2949328740', 'https://openalex.org/W2063655091']",2018-06-18
https://openalex.org/W2137010615,https://doi.org/10.1109/asru.2015.7404800,Deep multimodal semantic embeddings for speech and images,"In this paper, we present a model which takes as input a corpus of images with relevant spoken captions and finds a correspondence between the two modalities. We employ a pair of convolutional neural networks to model visual objects and speech signals at the word level, and tie the networks together with an embedding and alignment model which learns a joint semantic space over both modalities. We evaluate our model using image search and annotation tasks on the Flickr8k dataset, which we augmented by collecting a corpus of 40,000 spoken captions using Amazon Mechanical Turk.","['https://openalex.org/W6675026286', 'https://openalex.org/W2088049833', 'https://openalex.org/W6697456849', 'https://openalex.org/W2155893237', 'https://openalex.org/W6677994088', 'https://openalex.org/W2185175083', 'https://openalex.org/W1861492603', 'https://openalex.org/W6699171382', 'https://openalex.org/W6631362777', 'https://openalex.org/W2108598243', 'https://openalex.org/W2086842362', 'https://openalex.org/W2048343491', 'https://openalex.org/W1905882502', 'https://openalex.org/W6640071036', 'https://openalex.org/W6676647902', 'https://openalex.org/W1895577753', 'https://openalex.org/W2062955551', 'https://openalex.org/W7011438494', 'https://openalex.org/W6678470764', 'https://openalex.org/W2024490156', 'https://openalex.org/W6684165356', 'https://openalex.org/W2149557440', 'https://openalex.org/W2953276893', 'https://openalex.org/W2112912048', 'https://openalex.org/W1524333225', 'https://openalex.org/W2950094539', 'https://openalex.org/W2314664620', 'https://openalex.org/W2296681920', 'https://openalex.org/W2137471889', 'https://openalex.org/W2952122856', 'https://openalex.org/W1923162067', 'https://openalex.org/W2164019165', 'https://openalex.org/W2119775030', 'https://openalex.org/W2102605133']",2015-12-01
https://openalex.org/W2927191280,https://doi.org/10.21437/interspeech.2019-2224,Bayesian Subspace Hidden Markov Model for Acoustic Unit Discovery,"This work tackles the problem of learning a set of language specific acoustic\nunits from unlabeled speech recordings given a set of labeled recordings from\nother languages. Our approach may be described by the following two steps\nprocedure: first the model learns the notion of acoustic units from the\nlabelled data and then the model uses its knowledge to find new acoustic units\non the target language. We implement this process with the Bayesian Subspace\nHidden Markov Model (SHMM), a model akin to the Subspace Gaussian Mixture Model\n(SGMM) where each low dimensional embedding represents an acoustic unit rather\nthan just a HMM's state. The subspace is trained on 3 languages from the\nGlobalPhone corpus (German, Polish and Spanish) and the AUs are discovered on\nthe TIMIT corpus. Results, measured in equivalent Phone Error Rate, show that\nthis approach significantly outperforms previous HMM based acoustic units\ndiscovery systems and compares favorably with the Variational Auto Encoder-HMM.\n","['https://openalex.org/W2730658205', 'https://openalex.org/W2125838338', 'https://openalex.org/W2100768664', 'https://openalex.org/W2084534958', 'https://openalex.org/W2641832364', 'https://openalex.org/W2055408826', 'https://openalex.org/W2888911345', 'https://openalex.org/W2468716020', 'https://openalex.org/W204053250', 'https://openalex.org/W2964121744', 'https://openalex.org/W3127686677', 'https://openalex.org/W2077804127', 'https://openalex.org/W1959608418', 'https://openalex.org/W2962693497', 'https://openalex.org/W1524333225', 'https://openalex.org/W2750248772', 'https://openalex.org/W2347098582']",2019-09-13
https://openalex.org/W4287241729,https://doi.org/10.48550/arxiv.2104.00994,Unsupervised Acoustic Unit Discovery by Leveraging a\n Language-Independent Subword Discriminative Feature Representation,"This paper tackles automatically discovering phone-like acoustic units (AUD)\nfrom unlabeled speech data. Past studies usually proposed single-step\napproaches. We propose a two-stage approach: the first stage learns a\nsubword-discriminative feature representation and the second stage applies\nclustering to the learned representation and obtains phone-like clusters as the\ndiscovered acoustic units. In the first stage, a recently proposed method in\nthe task of unsupervised subword modeling is improved by replacing a\nmonolingual out-of-domain (OOD) ASR system with a multilingual one to create a\nsubword-discriminative representation that is more language-independent. In the\nsecond stage, segment-level k-means is adopted, and two methods to represent\nthe variable-length speech segments as fixed-dimension feature vectors are\ncompared. Experiments on a very low-resource Mboshi language corpus show that\nour approach outperforms state-of-the-art AUD in both normalized mutual\ninformation (NMI) and F-score. The multilingual ASR improved upon the\nmonolingual ASR in providing OOD phone labels and in estimating the phone\nboundaries. A comparison of our systems with and without knowing the\nground-truth phone boundaries showed a 16% NMI performance gap, suggesting that\nthe current approach can significantly benefit from improved phone boundary\nestimation.\n",[],2021-04-02
https://openalex.org/W1994606281,https://doi.org/10.1109/icassp.2013.6639084,Multilingual training of deep neural networks,"We investigate multilingual modeling in the context of a deep neural network (DNN) - hidden Markov model (HMM) hybrid, where the DNN outputs are used as the HMM state likelihoods. By viewing neural networks as a cascade of feature extractors followed by a logistic regression classifier, we hypothesise that the hidden layers, which act as feature extractors, will be transferable between languages. As a corollary, we propose that training the hidden layers on multiple languages makes them more suitable for such cross-lingual transfer. We experimentally confirm these hypotheses on the GlobalPhone corpus using seven languages from three different language families: Germanic, Romance, and Slavic. The experiments demonstrate substantial improvements over a monolingual DNN-HMM hybrid baseline, and hint at avenues of further exploration.","['https://openalex.org/W2407897255', 'https://openalex.org/W2131042651', 'https://openalex.org/W2090764203', 'https://openalex.org/W811578723', 'https://openalex.org/W2165712214', 'https://openalex.org/W2120209245', 'https://openalex.org/W2136922672', 'https://openalex.org/W4231109964', 'https://openalex.org/W2163922914', 'https://openalex.org/W2169189000', 'https://openalex.org/W2127982613', 'https://openalex.org/W2110871230', 'https://openalex.org/W1993882792', 'https://openalex.org/W2147768505', 'https://openalex.org/W6602682705', 'https://openalex.org/W6631362777', 'https://openalex.org/W198385923', 'https://openalex.org/W2152175008', 'https://openalex.org/W2033436836', 'https://openalex.org/W6601939441', 'https://openalex.org/W1991180839', 'https://openalex.org/W73572011', 'https://openalex.org/W2111306781', 'https://openalex.org/W6674389704', 'https://openalex.org/W6638728282', 'https://openalex.org/W1980850109', 'https://openalex.org/W2123798005', 'https://openalex.org/W1981706894', 'https://openalex.org/W1524333225', 'https://openalex.org/W1846073453', 'https://openalex.org/W2072128103', 'https://openalex.org/W1553004968', 'https://openalex.org/W66627554', 'https://openalex.org/W2096672142', 'https://openalex.org/W4285719527', 'https://openalex.org/W47568227']",2013-05-01
https://openalex.org/W2025198378,https://doi.org/10.1109/icassp.2013.6639081,Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers,"In the deep neural network (DNN), the hidden layers can be considered as increasingly complex feature transformations and the final softmax layer as a log-linear classifier making use of the most abstract features computed in the hidden layers. While the loglinear classifier should be different for different languages, the feature transformations can be shared across languages. In this paper we propose a shared-hidden-layer multilingual DNN (SHL-MDNN), in which the hidden layers are made common across many languages while the softmax layers are made language dependent. We demonstrate that the SHL-MDNN can reduce errors by 3-5%, relatively, for all the languages decodable with the SHL-MDNN, over the monolingual DNNs trained using only the language specific data. Further, we show that the learned hidden layers sharing across languages can be transferred to improve recognition accuracy of new languages, with relative error reductions ranging from 6% to 28% against DNNs trained without exploiting the transferred hidden layers. It is particularly interesting that the error reduction can be achieved for the target language that is in different families of the languages used to learn the hidden layers.","['https://openalex.org/W2123798005', 'https://openalex.org/W1968147312', 'https://openalex.org/W2151841219', 'https://openalex.org/W2033436836', 'https://openalex.org/W2130414229', 'https://openalex.org/W6678242812', 'https://openalex.org/W2120209245', 'https://openalex.org/W2160815625', 'https://openalex.org/W2913340405', 'https://openalex.org/W2407897255', 'https://openalex.org/W2131042651', 'https://openalex.org/W2013205774', 'https://openalex.org/W2407441242', 'https://openalex.org/W2127982613', 'https://openalex.org/W2117130368', 'https://openalex.org/W2394932179', 'https://openalex.org/W2062164080', 'https://openalex.org/W2147768505', 'https://openalex.org/W2076794394', 'https://openalex.org/W2296748324', 'https://openalex.org/W1993882792', 'https://openalex.org/W2160306971', 'https://openalex.org/W1987238397', 'https://openalex.org/W2403195671', 'https://openalex.org/W319941341', 'https://openalex.org/W2144792281', 'https://openalex.org/W2914746235', 'https://openalex.org/W2120480077', 'https://openalex.org/W2950789693', 'https://openalex.org/W2253807446', 'https://openalex.org/W2184045248', 'https://openalex.org/W217970951']",2013-05-01
https://openalex.org/W2153579005,https://doi.org/10.48550/arxiv.1310.4546,Distributed Representations of Words and Phrases and their Compositionality,"The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of ""Canada"" and ""Air"" cannot be easily combined to obtain ""Air Canada"". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.","['https://openalex.org/W2120861206', 'https://openalex.org/W21006490', 'https://openalex.org/W2132339004', 'https://openalex.org/W2171928131', 'https://openalex.org/W36903255', 'https://openalex.org/W1662133657', 'https://openalex.org/W1889268436', 'https://openalex.org/W22861983', 'https://openalex.org/W1614298861', 'https://openalex.org/W1498436455', 'https://openalex.org/W1423339008', 'https://openalex.org/W1970689298', 'https://openalex.org/W1965154800', 'https://openalex.org/W2117130368', 'https://openalex.org/W2141599568', 'https://openalex.org/W2131462252', 'https://openalex.org/W2138204974', 'https://openalex.org/W2962769333', 'https://openalex.org/W2158139315']",2013-10-16
https://openalex.org/W2291975472,https://doi.org/10.1109/asru.2015.7404803,Multilingual representations for low resource speech recognition and keyword search,"This paper examines the impact of multilingual (ML) acoustic representations on Automatic Speech Recognition (ASR) and keyword search (KWS) for low resource languages in the context of the OpenKWS15 evaluation of the IARPA Babel program. The task is to develop Swahili ASR and KWS systems within two weeks using as little as 3 hours of transcribed data. Multilingual acoustic representations proved to be crucial for building these systems under strict time constraints. The paper discusses several key insights on how these representations are derived and used. First, we present a data sampling strategy that can speed up the training of multilingual representations without appreciable loss in ASR performance. Second, we show that fusion of diverse multilingual representations developed at different LORELEI sites yields substantial ASR and KWS gains. Speaker adaptation and data augmentation of these representations improves both ASR and KWS performance (up to 8.7% relative). Third, incorporating un-transcribed data through semi-supervised learning, improves WER and KWS performance. Finally, we show that these multilingual representations significantly improve ASR and KWS performance (relative 9% for WER and 5% for MTWV) even when forty hours of transcribed audio in the target language is available. Multilingual representations significantly contributed to the LORELEI KWS systems winning the OpenKWS 15 evaluation.","['https://openalex.org/W3144493448', 'https://openalex.org/W1994606281', 'https://openalex.org/W2025198378', 'https://openalex.org/W1993660824', 'https://openalex.org/W2120209245', 'https://openalex.org/W2147276082', 'https://openalex.org/W2123798005', 'https://openalex.org/W2405993325', 'https://openalex.org/W2394526247', 'https://openalex.org/W6674537127', 'https://openalex.org/W1992912377', 'https://openalex.org/W2799046698', 'https://openalex.org/W2167458787', 'https://openalex.org/W2141499240', 'https://openalex.org/W6696449567', 'https://openalex.org/W2401430117', 'https://openalex.org/W1978660892', 'https://openalex.org/W6608133726', 'https://openalex.org/W2403195671', 'https://openalex.org/W6684859321', 'https://openalex.org/W6674634876', 'https://openalex.org/W2407022425', 'https://openalex.org/W2294723541', 'https://openalex.org/W1991083751', 'https://openalex.org/W1496070437', 'https://openalex.org/W2137075158', 'https://openalex.org/W2101596234', 'https://openalex.org/W1501286448', 'https://openalex.org/W162588823', 'https://openalex.org/W1486364748', 'https://openalex.org/W2161482971', 'https://openalex.org/W1975113979', 'https://openalex.org/W2127499922', 'https://openalex.org/W2135241496', 'https://openalex.org/W6658485743', 'https://openalex.org/W2407420912', 'https://openalex.org/W2076250379', 'https://openalex.org/W2076794394', 'https://openalex.org/W2155117693', 'https://openalex.org/W2080401855', 'https://openalex.org/W2397507973', 'https://openalex.org/W2039285212', 'https://openalex.org/W2096140469', 'https://openalex.org/W2168231600', 'https://openalex.org/W1979651826', 'https://openalex.org/W2292087804', 'https://openalex.org/W196761320', 'https://openalex.org/W2097428361', 'https://openalex.org/W2074932712', 'https://openalex.org/W2032020816', 'https://openalex.org/W60702959', 'https://openalex.org/W2951282416']",2015-12-01
https://openalex.org/W2163922914,https://doi.org/10.1109/tpami.2013.50,Representation Learning: A Review and New Perspectives,"The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.","['https://openalex.org/W6638484148', 'https://openalex.org/W1970789124', 'https://openalex.org/W2138265962', 'https://openalex.org/W6679755471', 'https://openalex.org/W6635461197', 'https://openalex.org/W2063971957', 'https://openalex.org/W2097308346', 'https://openalex.org/W2137234026', 'https://openalex.org/W1995314086', 'https://openalex.org/W1979185006', 'https://openalex.org/W4231109964', 'https://openalex.org/W6697274609', 'https://openalex.org/W1806891645', 'https://openalex.org/W2156740722', 'https://openalex.org/W116068320', 'https://openalex.org/W2546191734', 'https://openalex.org/W6683355650', 'https://openalex.org/W2107878631', 'https://openalex.org/W6680532216', 'https://openalex.org/W6683047094', 'https://openalex.org/W6678500653', 'https://openalex.org/W6679569991', 'https://openalex.org/W2110798204', 'https://openalex.org/W2296073425', 'https://openalex.org/W2106004777', 'https://openalex.org/W6639011011', 'https://openalex.org/W6629606793', 'https://openalex.org/W2168345951', 'https://openalex.org/W2998993395', 'https://openalex.org/W6676179485', 'https://openalex.org/W2148553367', 'https://openalex.org/W28766783', 'https://openalex.org/W6635815790', 'https://openalex.org/W6638826180', 'https://openalex.org/W2160692033', 'https://openalex.org/W2017257315', 'https://openalex.org/W6676938131', 'https://openalex.org/W2106439909', 'https://openalex.org/W2003289302', 'https://openalex.org/W6683694789', 'https://openalex.org/W6674764686', 'https://openalex.org/W2103819961', 'https://openalex.org/W6687884132', 'https://openalex.org/W2141125852', 'https://openalex.org/W2132424367', 'https://openalex.org/W6686418764', 'https://openalex.org/W6676194229', 'https://openalex.org/W2117130368', 'https://openalex.org/W6683738474', 'https://openalex.org/W6696469593', 'https://openalex.org/W2160142299', 'https://openalex.org/W6675321185', 'https://openalex.org/W2147768505', 'https://openalex.org/W2168013545', 'https://openalex.org/W6680239566', 'https://openalex.org/W6639503338', 'https://openalex.org/W2162950292', 'https://openalex.org/W2156838815', 'https://openalex.org/W6680300913', 'https://openalex.org/W6684349851', 'https://openalex.org/W2101926813', 'https://openalex.org/W6631943919', 'https://openalex.org/W6682889407', 'https://openalex.org/W6600949241', 'https://openalex.org/W6675897241', 'https://openalex.org/W6638011348', 'https://openalex.org/W6628934346', 'https://openalex.org/W6677645113', 'https://openalex.org/W2161977692', 'https://openalex.org/W2141006018', 'https://openalex.org/W6766005275', 'https://openalex.org/W6604506675', 'https://openalex.org/W6682948231', 'https://openalex.org/W6713465158', 'https://openalex.org/W2165293955', 'https://openalex.org/W2107998050', 'https://openalex.org/W6686925444', 'https://openalex.org/W2966661', 'https://openalex.org/W2160815625', 'https://openalex.org/W6605963037', 'https://openalex.org/W2079182758', 'https://openalex.org/W6677321020', 'https://openalex.org/W6601785968', 'https://openalex.org/W6683161245', 'https://openalex.org/W2100495367', 'https://openalex.org/W6675401909', 'https://openalex.org/W2136922672', 'https://openalex.org/W2103212315', 'https://openalex.org/W6683167953', 'https://openalex.org/W2051144468', 'https://openalex.org/W2123496278', 'https://openalex.org/W2124486835', 'https://openalex.org/W4205778870', 'https://openalex.org/W2146672645', 'https://openalex.org/W104847522', 'https://openalex.org/W2054217036', 'https://openalex.org/W6674855086', 'https://openalex.org/W2546302380', 'https://openalex.org/W6676634215', 'https://openalex.org/W1996355918', 'https://openalex.org/W6685191590', 'https://openalex.org/W2140262144', 'https://openalex.org/W6685048874', 'https://openalex.org/W6682961369', 'https://openalex.org/W6682571421', 'https://openalex.org/W2124237441', 'https://openalex.org/W6787972765', 'https://openalex.org/W1964155876', 'https://openalex.org/W6680887930', 'https://openalex.org/W2162915993', 'https://openalex.org/W2076094076', 'https://openalex.org/W2147860648', 'https://openalex.org/W2102017903', 'https://openalex.org/W6676759488', 'https://openalex.org/W1999192586', 'https://openalex.org/W2164273299', 'https://openalex.org/W413857758', 'https://openalex.org/W6606918967', 'https://openalex.org/W2147800946', 'https://openalex.org/W4238404964', 'https://openalex.org/W2112796928', 'https://openalex.org/W6679718588', 'https://openalex.org/W2130325614', 'https://openalex.org/W6676071220', 'https://openalex.org/W6683971058', 'https://openalex.org/W2124386111', 'https://openalex.org/W1994906459', 'https://openalex.org/W6682386858', 'https://openalex.org/W6685202642', 'https://openalex.org/W6608133726', 'https://openalex.org/W6628131027', 'https://openalex.org/W2136163184', 'https://openalex.org/W6687011383', 'https://openalex.org/W2185726469', 'https://openalex.org/W2145038566', 'https://openalex.org/W1993882792', 'https://openalex.org/W6675227660', 'https://openalex.org/W6682998430', 'https://openalex.org/W6637242042', 'https://openalex.org/W2083380015', 'https://openalex.org/W6607884611', 'https://openalex.org/W6686557994', 'https://openalex.org/W2145889472', 'https://openalex.org/W6712962975', 'https://openalex.org/W6682359208', 'https://openalex.org/W2122922389', 'https://openalex.org/W1983334819', 'https://openalex.org/W2172174689', 'https://openalex.org/W6676231525', 'https://openalex.org/W6683775911', 'https://openalex.org/W6677878007', 'https://openalex.org/W1995997122', 'https://openalex.org/W2149194912', 'https://openalex.org/W6688386640', 'https://openalex.org/W16016350', 'https://openalex.org/W6684805783', 'https://openalex.org/W6678739631', 'https://openalex.org/W2053186076', 'https://openalex.org/W6605753235', 'https://openalex.org/W6674789083', 'https://openalex.org/W2913932916', 'https://openalex.org/W6607775107', 'https://openalex.org/W2071709160', 'https://openalex.org/W2099866409', 'https://openalex.org/W6619886890', 'https://openalex.org/W6683953743', 'https://openalex.org/W2140095548', 'https://openalex.org/W6691752037', 'https://openalex.org/W2394932179', 'https://openalex.org/W2160306971', 'https://openalex.org/W2144982973', 'https://openalex.org/W6680955900', 'https://openalex.org/W6676831635', 'https://openalex.org/W6680112165', 'https://openalex.org/W6638304892', 'https://openalex.org/W6678911119', 'https://openalex.org/W6675525543', 'https://openalex.org/W6602989467', 'https://openalex.org/W6606244218', 'https://openalex.org/W6686993759', 'https://openalex.org/W7025107196', 'https://openalex.org/W6630657659', 'https://openalex.org/W6679825673', 'https://openalex.org/W6686575128', 'https://openalex.org/W2115096495', 'https://openalex.org/W1586730761', 'https://openalex.org/W2001141328', 'https://openalex.org/W2116825644', 'https://openalex.org/W2150529939', 'https://openalex.org/W2125027820', 'https://openalex.org/W2169805405', 'https://openalex.org/W2116516955', 'https://openalex.org/W2013035813', 'https://openalex.org/W6683772321', 'https://openalex.org/W2025768430', 'https://openalex.org/W6681096077', 'https://openalex.org/W2150627493', 'https://openalex.org/W6632321371', 'https://openalex.org/W6680414531', 'https://openalex.org/W2159291644', 'https://openalex.org/W2102765684', 'https://openalex.org/W2146444479', 'https://openalex.org/W1990838964', 'https://openalex.org/W2123131857', 'https://openalex.org/W6677233944', 'https://openalex.org/W6679809621', 'https://openalex.org/W2020719522', 'https://openalex.org/W6633280949', 'https://openalex.org/W2293078015', 'https://openalex.org/W1838657242', 'https://openalex.org/W2186489521', 'https://openalex.org/W66838807', 'https://openalex.org/W2091987367', 'https://openalex.org/W2156163116', 'https://openalex.org/W189596042', 'https://openalex.org/W1533072162', 'https://openalex.org/W2294798173', 'https://openalex.org/W4240768087', 'https://openalex.org/W3118608800', 'https://openalex.org/W1592735339', 'https://openalex.org/W1761606332', 'https://openalex.org/W2162747531', 'https://openalex.org/W193851967', 'https://openalex.org/W2152424459', 'https://openalex.org/W2103359087', 'https://openalex.org/W625466704', 'https://openalex.org/W2091886411', 'https://openalex.org/W2157444450', 'https://openalex.org/W2186629860', 'https://openalex.org/W2289330286', 'https://openalex.org/W2161000554', 'https://openalex.org/W2137510948', 'https://openalex.org/W2152790380', 'https://openalex.org/W2949821452', 'https://openalex.org/W1576278180', 'https://openalex.org/W2108581046', 'https://openalex.org/W2158834214', 'https://openalex.org/W2963654815', 'https://openalex.org/W1511867968', 'https://openalex.org/W2111494971', 'https://openalex.org/W2169488311', 'https://openalex.org/W2171282218', 'https://openalex.org/W2191540403', 'https://openalex.org/W17525587', 'https://openalex.org/W2156387975', 'https://openalex.org/W2108665656', 'https://openalex.org/W2153934661', 'https://openalex.org/W145476170', 'https://openalex.org/W2107789863', 'https://openalex.org/W2584401907', 'https://openalex.org/W1496559305', 'https://openalex.org/W2997574889', 'https://openalex.org/W2145094598', 'https://openalex.org/W2137595331', 'https://openalex.org/W2072128103', 'https://openalex.org/W2133257461', 'https://openalex.org/W2156047073', 'https://openalex.org/W2134563198', 'https://openalex.org/W2616180702', 'https://openalex.org/W2200708944', 'https://openalex.org/W2952402222', 'https://openalex.org/W177847060', 'https://openalex.org/W2140833774', 'https://openalex.org/W2049633694', 'https://openalex.org/W2097998348', 'https://openalex.org/W2138857742', 'https://openalex.org/W2111304802', 'https://openalex.org/W2110361616', 'https://openalex.org/W142185896', 'https://openalex.org/W195465510', 'https://openalex.org/W2096873754', 'https://openalex.org/W2187089797', 'https://openalex.org/W2295582178', 'https://openalex.org/W4300198444', 'https://openalex.org/W2140793251', 'https://openalex.org/W2952230511', 'https://openalex.org/W2114570910', 'https://openalex.org/W1813659000', 'https://openalex.org/W2158867000', 'https://openalex.org/W2914484425', 'https://openalex.org/W1548802052', 'https://openalex.org/W2137291015', 'https://openalex.org/W2252143850', 'https://openalex.org/W2112274848', 'https://openalex.org/W3013880646', 'https://openalex.org/W2998704965', 'https://openalex.org/W2952742172', 'https://openalex.org/W2613634265', 'https://openalex.org/W2103305545', 'https://openalex.org/W2106869737', 'https://openalex.org/W2116064496', 'https://openalex.org/W1665214252', 'https://openalex.org/W4293652559', 'https://openalex.org/W2131672785', 'https://openalex.org/W2158899491', 'https://openalex.org/W2183660452', 'https://openalex.org/W2105728138', 'https://openalex.org/W2188492526', 'https://openalex.org/W1596986901', 'https://openalex.org/W2099741732', 'https://openalex.org/W169539560', 'https://openalex.org/W2606748186', 'https://openalex.org/W2118103795', 'https://openalex.org/W2162931300', 'https://openalex.org/W2400065095', 'https://openalex.org/W2135341757', 'https://openalex.org/W2184045248', 'https://openalex.org/W2125569215', 'https://openalex.org/W2121331909', 'https://openalex.org/W4285719527', 'https://openalex.org/W3122936144', 'https://openalex.org/W1544211475', 'https://openalex.org/W1489081407', 'https://openalex.org/W2071128523', 'https://openalex.org/W1408639475', 'https://openalex.org/W2131241448', 'https://openalex.org/W2163605009', 'https://openalex.org/W2106411961', 'https://openalex.org/W2163202312', 'https://openalex.org/W2250379827', 'https://openalex.org/W2112148214', 'https://openalex.org/W2166093887', 'https://openalex.org/W2606321545', 'https://openalex.org/W110825707', 'https://openalex.org/W1505878979', 'https://openalex.org/W2962936867', 'https://openalex.org/W71795751', 'https://openalex.org/W2962968839', 'https://openalex.org/W1876224860', 'https://openalex.org/W2161366100', 'https://openalex.org/W2185528074', 'https://openalex.org/W2102409316', 'https://openalex.org/W2184852195', 'https://openalex.org/W2406196141', 'https://openalex.org/W2132283655', 'https://openalex.org/W2952722152', 'https://openalex.org/W2164587673', 'https://openalex.org/W2963909185', 'https://openalex.org/W2964300310', 'https://openalex.org/W2099939455', 'https://openalex.org/W2218318129', 'https://openalex.org/W1819710477', 'https://openalex.org/W2126760242', 'https://openalex.org/W2171490498', 'https://openalex.org/W2950320139', 'https://openalex.org/W2138448681', 'https://openalex.org/W1811734137', 'https://openalex.org/W2153052520', 'https://openalex.org/W4300402905', 'https://openalex.org/W1526741802', 'https://openalex.org/W2098477387', 'https://openalex.org/W2165225968', 'https://openalex.org/W196761320', 'https://openalex.org/W2123284177', 'https://openalex.org/W1533861849', 'https://openalex.org/W2726367589', 'https://openalex.org/W1536231199', 'https://openalex.org/W2165337236', 'https://openalex.org/W2105464873', 'https://openalex.org/W2157002241', 'https://openalex.org/W2141132211', 'https://openalex.org/W44815768', 'https://openalex.org/W2140622310', 'https://openalex.org/W22861983']",2013-05-31
https://openalex.org/W2106440210,https://doi.org/10.1109/icassp.2014.6855086,Multilingual deep neural network based acoustic modeling for rapid language adaptation,"This paper presents a study on multilingual deep neural network (DNN) based acoustic modeling and its application to new languages. We investigate the effect of phone merging on multilingual DNN in context of rapid language adaptation. Moreover, the combination of multilingual DNNs with Kullback--Leibler divergence based acoustic modeling (KL-HMM) is explored. Using ten different languages from the Globalphone database, our studies reveal that crosslingual acoustic model transfer through multilingual DNNs is superior to unsupervised RBM pre-training and greedy layer-wise supervised training. We also found that KL-HMM based decoding consistently outperforms conventional hybrid decoding, especially in low-resource scenarios. Furthermore, the experiments indicate that multilingual DNN training equally benefits from simple phoneset concatenation and manually derived universal phonesets.","['https://openalex.org/W6680300913', 'https://openalex.org/W6681612289', 'https://openalex.org/W2137644731', 'https://openalex.org/W2084534958', 'https://openalex.org/W6679429981', 'https://openalex.org/W2026369565', 'https://openalex.org/W6631362777', 'https://openalex.org/W2033436836', 'https://openalex.org/W2012190582', 'https://openalex.org/W1970890968', 'https://openalex.org/W1975550806', 'https://openalex.org/W1993882792', 'https://openalex.org/W6676481782', 'https://openalex.org/W2147768505', 'https://openalex.org/W2394932179', 'https://openalex.org/W6635896687', 'https://openalex.org/W1978660892', 'https://openalex.org/W6656619859', 'https://openalex.org/W2120209245', 'https://openalex.org/W2160815625', 'https://openalex.org/W2407897255', 'https://openalex.org/W1994606281', 'https://openalex.org/W1524333225', 'https://openalex.org/W2131342762', 'https://openalex.org/W2138243089', 'https://openalex.org/W2951781666', 'https://openalex.org/W2110798204', 'https://openalex.org/W1604771987', 'https://openalex.org/W2146168991', 'https://openalex.org/W2138857742', 'https://openalex.org/W4285719527', 'https://openalex.org/W2025198378', 'https://openalex.org/W2606321545']",2014-05-01
https://openalex.org/W3096485810,https://doi.org/10.21437/interspeech.2020-1511,Speech-XLNet: Unsupervised Acoustic Model Pretraining for Self-Attention Networks,"Self-attention network (SAN) can benefit significantly from the bi-directional representation learning through unsupervised pretraining paradigms such as BERT and XLNet.In this paper, we present an XLNet-like pretraining scheme ""Speech-XLNet"" to learn speech representations with self-attention networks (SANs).Firstly, we find that by shuffling the speech frame orders, Speech-XLNet serves as a strong regularizer which encourages the SAN network to make inferences by focusing on global structures through its attention weights.Secondly, Speech-XLNet also allows the model to explore bi-directional context information while maintaining the autoregressive training manner.Visualization results show that our approach can generalize better with more flattened and widely distributed optimas compared to the conventional approach.Experimental results on TIMIT demonstrate that Speech-XLNet greatly improves hybrid SAN/HMM in terms of both convergence speed and recognition accuracy.Our best systems achieve a relative improvement of 15.2% on the TIMIT task.Besides, we also apply our pretrained model to an End-to-End SAN with WSJ dataset and WER is reduced by up to 68% when only a few hours of transcribed data is used.","['https://openalex.org/W2777662428', 'https://openalex.org/W639708223', 'https://openalex.org/W2939757332', 'https://openalex.org/W2971033911', 'https://openalex.org/W3015265920', 'https://openalex.org/W1522301498', 'https://openalex.org/W2979476256', 'https://openalex.org/W2970597249', 'https://openalex.org/W4295312788', 'https://openalex.org/W1494198834', 'https://openalex.org/W2896457183', 'https://openalex.org/W2892009249', 'https://openalex.org/W2973049979', 'https://openalex.org/W2972943112', 'https://openalex.org/W4214784181', 'https://openalex.org/W2973042454', 'https://openalex.org/W2160815625', 'https://openalex.org/W2911291251', 'https://openalex.org/W2964147121', 'https://openalex.org/W4385245566', 'https://openalex.org/W2982223350', 'https://openalex.org/W2250357346', 'https://openalex.org/W1524333225', 'https://openalex.org/W2936078256']",2020-10-25
https://openalex.org/W2991213871,https://doi.org/10.48550/arxiv.1911.08460,End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures,"We study pseudo-labeling for the semi-supervised training of ResNet, Time-Depth Separable ConvNets, and Transformers for speech recognition, with either CTC or Seq2Seq loss functions. We perform experiments on the standard LibriSpeech dataset, and leverage additional unlabeled data from LibriVox through pseudo-labeling. We show that while Transformer-based acoustic models have superior performance with the supervised dataset alone, semi-supervision improves all models across architectures and loss functions and bridges much of the performance gaps between them. In doing so, we reach a new state-of-the-art for end-to-end acoustic models decoded with an external language model in the standard supervised learning setting, and a new absolute state-of-the-art with semi-supervised training. Finally, we study the effect of leveraging different amounts of unlabeled audio, propose several ways of evaluating the characteristics of unlabeled audio which improve acoustic modeling, and show that acoustic models trained with more audio rely less on external language models.","['https://openalex.org/W2951327905', 'https://openalex.org/W2941814890', 'https://openalex.org/W2969945254', 'https://openalex.org/W2193413348', 'https://openalex.org/W2962824709', 'https://openalex.org/W2981857663', 'https://openalex.org/W2146502635', 'https://openalex.org/W2885185669', 'https://openalex.org/W2963403868', 'https://openalex.org/W2102113734', 'https://openalex.org/W3043750461', 'https://openalex.org/W2972630480', 'https://openalex.org/W2963266252', 'https://openalex.org/W2962760690', 'https://openalex.org/W2121879602', 'https://openalex.org/W2949888546', 'https://openalex.org/W2519224033', 'https://openalex.org/W2748679025', 'https://openalex.org/W2977728428', 'https://openalex.org/W2750499125', 'https://openalex.org/W2940180244', 'https://openalex.org/W2995181338', 'https://openalex.org/W2633884958', 'https://openalex.org/W2889282842', 'https://openalex.org/W2525778437', 'https://openalex.org/W2936774411', 'https://openalex.org/W2963970792', 'https://openalex.org/W3015522062', 'https://openalex.org/W2788851830', 'https://openalex.org/W2953190524', 'https://openalex.org/W2952886436', 'https://openalex.org/W1494198834', 'https://openalex.org/W2963850025', 'https://openalex.org/W2593910181', 'https://openalex.org/W2972818416', 'https://openalex.org/W2973215447', 'https://openalex.org/W2940322076', 'https://openalex.org/W2520160253', 'https://openalex.org/W2748795451', 'https://openalex.org/W2782451907', 'https://openalex.org/W2184045248', 'https://openalex.org/W2904818793', 'https://openalex.org/W2950903920', 'https://openalex.org/W2964347276', 'https://openalex.org/W3103005696', 'https://openalex.org/W2194775991', 'https://openalex.org/W2962907457', 'https://openalex.org/W2975381464', 'https://openalex.org/W2963631907', 'https://openalex.org/W2127141656', 'https://openalex.org/W2327501763', 'https://openalex.org/W2933138175', 'https://openalex.org/W2892090442', 'https://openalex.org/W2134800885']",2019-11-19
https://openalex.org/W2110798204,https://doi.org/10.7551/mitpress/7503.003.0024,Greedy Layer-Wise Training of Deep Networks,"Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.","['https://openalex.org/W2167967601', 'https://openalex.org/W1509849361', 'https://openalex.org/W2136922672', 'https://openalex.org/W2017290750', 'https://openalex.org/W2103369448', 'https://openalex.org/W2125569215', 'https://openalex.org/W2103626435', 'https://openalex.org/W2130313186', 'https://openalex.org/W2100495367', 'https://openalex.org/W1966347487', 'https://openalex.org/W2128076038', 'https://openalex.org/W2124914669', 'https://openalex.org/W2109779438', 'https://openalex.org/W2613634265', 'https://openalex.org/W2116064496', 'https://openalex.org/W1993845689']",2007-09-07
https://openalex.org/W2512655038,https://doi.org/10.21437/interspeech.2016-1596,Semi-Supervised Training in Deep Learning Acoustic Model,"We studied the semi-supervised training in a fully connected deep neural network (DNN), unfolded recurrent neural network (RNN), and long short-term memory recurrent neural network (LSTM-RNN) with respect to the transcription quality, the importance data sampling, and the training data amount. We found that DNN, unfolded RNN, and LSTM-RNN are increasingly more sensitive to labeling errors. For example, with the simulated erroneous training transcription at 5%, 10%, or 15% word error rate (WER) level, the semi-supervised DNN yields 2.37%, 4.84%, or 7.46% relative WER increase against the baseline model trained with the perfect transcription; in comparison, the corresponding WER increase is 2.53%, 4.89%, or 8.85% in an unfolded RNN and 4.47%, 9.38%, or 14.01% in an LSTM-RNN. We further found that the importance sampling has similar impact on all three models with 2~3% relative WER reduction comparing to the random sampling. Lastly, we compared the modeling capability with increased training data. Experimental results suggested that LSTM-RNN can benefit more from enlarged training data comparing to unfolded RNN and DNN. We trained a semi-supervised LSTM-RNN using 2600 hr transcribed and 10000 hr untranscribed data on a mobile speech task. The semi-supervised LSTM-RNN yields 7.9\% relative WER reduction against the supervised baseline.","['https://openalex.org/W82886505', 'https://openalex.org/W2056786202', 'https://openalex.org/W2108501770', 'https://openalex.org/W1479807131', 'https://openalex.org/W4285719527', 'https://openalex.org/W2139453310', 'https://openalex.org/W2405883473', 'https://openalex.org/W2136504847', 'https://openalex.org/W2107008379', 'https://openalex.org/W2404463488', 'https://openalex.org/W2400505028', 'https://openalex.org/W2189391786', 'https://openalex.org/W2033256038', 'https://openalex.org/W2122457239', 'https://openalex.org/W2181607856', 'https://openalex.org/W1975113979', 'https://openalex.org/W2949416428', 'https://openalex.org/W2147768505', 'https://openalex.org/W1499864241', 'https://openalex.org/W1928593089']",2016-08-29
https://openalex.org/W2184045248,,Deep Neural Networks for Acoustic Modeling in Speech Recognition,"Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feedforward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks with many hidden layers, that are trained using new methods have been shown to outperform Gaussian mixture models on a variety of speech recognition benchmarks, sometimes by a large margin. This paper provides an overview of this progress and represents the shared views of four research groups who have had recent successes in using deep neural networks for acoustic modeling in speech recognition. I.","['https://openalex.org/W2149600041', 'https://openalex.org/W2132424367', 'https://openalex.org/W1994197834', 'https://openalex.org/W2100495367', 'https://openalex.org/W2907162034', 'https://openalex.org/W2151660570', 'https://openalex.org/W1877570817', 'https://openalex.org/W1498436455', 'https://openalex.org/W2164579587', 'https://openalex.org/W217970951', 'https://openalex.org/W1964420823', 'https://openalex.org/W2159080219', 'https://openalex.org/W2053280194', 'https://openalex.org/W2103359087', 'https://openalex.org/W2172097686', 'https://openalex.org/W1553004968', 'https://openalex.org/W2394932179', 'https://openalex.org/W2105153012', 'https://openalex.org/W2114016253', 'https://openalex.org/W2122514667', 'https://openalex.org/W2116064496', 'https://openalex.org/W3146320432', 'https://openalex.org/W2147768505', 'https://openalex.org/W2102017903', 'https://openalex.org/W42107399', 'https://openalex.org/W2096635587', 'https://openalex.org/W1562289873', 'https://openalex.org/W1970996882', 'https://openalex.org/W2145094598', 'https://openalex.org/W299739017', 'https://openalex.org/W2071489795', 'https://openalex.org/W2090861223', 'https://openalex.org/W2027915610', 'https://openalex.org/W587794757', 'https://openalex.org/W2107789863', 'https://openalex.org/W2125234026', 'https://openalex.org/W2105099419', 'https://openalex.org/W2218318129', 'https://openalex.org/W2084514013', 'https://openalex.org/W1647054946', 'https://openalex.org/W2139622435', 'https://openalex.org/W2165119652', 'https://openalex.org/W2155273149', 'https://openalex.org/W2296748324', 'https://openalex.org/W2003123121', 'https://openalex.org/W2136922672', 'https://openalex.org/W2148099973', 'https://openalex.org/W196761320', 'https://openalex.org/W2140372979', 'https://openalex.org/W1993882792', 'https://openalex.org/W1533861849', 'https://openalex.org/W2106051978', 'https://openalex.org/W2131700150', 'https://openalex.org/W2014641584', 'https://openalex.org/W2168171912', 'https://openalex.org/W44815768', 'https://openalex.org/W2089177488', 'https://openalex.org/W1981678206', 'https://openalex.org/W2022011789', 'https://openalex.org/W2141778357', 'https://openalex.org/W2160306971', 'https://openalex.org/W2069976350', 'https://openalex.org/W2118595744', 'https://openalex.org/W2165712214']",2012-11-01
https://openalex.org/W3007227084,https://doi.org/10.1109/asru46091.2019.9003906,Improving RNN Transducer Modeling for End-to-End Speech Recognition,"In the last few years, an emerging trend in automatic speech recognition research is the study of end-to-end (E2E) systems. Connectionist Temporal Classification (CTC), Attention Encoder-Decoder (AED), and RNN Transducer (RNN-T) are the most popular three methods. Among these three methods, RNN-T has the advantages to do online streaming which is challenging to AED and it doesn't have CTC's frame-independence assumption. In this paper, we improve the RNN-T training in two aspects. First, we optimize the training algorithm of RNN-T to reduce the memory consumption so that we can have larger training minibatch for faster training speed. Second, we propose better model structures so that we obtain RNN-T models with the very good accuracy but small footprint. Trained with 30 thousand hours anonymized and transcribed Microsoft production data, the best RNN-T model with even smaller model size (216 Megabytes) achieves up-to 11.8% relative word error rate (WER) reduction from the baseline RNN-T model. This best RNN-T model is significantly better than the device hybrid model with similar size by achieving up-to 15.0% relative WER reduction, and obtains similar WERs as the server hybrid model of 5120 Megabytes in size.","['https://openalex.org/W6758956981', 'https://openalex.org/W2963326356', 'https://openalex.org/W2964084166', 'https://openalex.org/W2513938599', 'https://openalex.org/W2586630418', 'https://openalex.org/W6687566353', 'https://openalex.org/W6637242042', 'https://openalex.org/W2734724284', 'https://openalex.org/W2963970535', 'https://openalex.org/W6761230321', 'https://openalex.org/W2127141656', 'https://openalex.org/W6675365184', 'https://openalex.org/W2157331557', 'https://openalex.org/W6679434410', 'https://openalex.org/W2962826786', 'https://openalex.org/W6623517193', 'https://openalex.org/W6638749077', 'https://openalex.org/W6747398299', 'https://openalex.org/W2396144419', 'https://openalex.org/W2750499125', 'https://openalex.org/W2291522532', 'https://openalex.org/W2327501763', 'https://openalex.org/W2746192915', 'https://openalex.org/W2475988411', 'https://openalex.org/W6741807409', 'https://openalex.org/W2962824709', 'https://openalex.org/W2963211739', 'https://openalex.org/W2962742956', 'https://openalex.org/W1533416326', 'https://openalex.org/W2962760690', 'https://openalex.org/W2936123380', 'https://openalex.org/W2966163367', 'https://openalex.org/W2962962542', 'https://openalex.org/W1600744878', 'https://openalex.org/W2396384435', 'https://openalex.org/W6758661737', 'https://openalex.org/W2912691643', 'https://openalex.org/W2911544293', 'https://openalex.org/W854541894', 'https://openalex.org/W1828163288', 'https://openalex.org/W2939248538', 'https://openalex.org/W2545177271', 'https://openalex.org/W4294619417', 'https://openalex.org/W1924770834', 'https://openalex.org/W1665214252', 'https://openalex.org/W2193413348', 'https://openalex.org/W2964308564', 'https://openalex.org/W2739427748', 'https://openalex.org/W2963827914', 'https://openalex.org/W2963917928', 'https://openalex.org/W2964272710', 'https://openalex.org/W2962784628', 'https://openalex.org/W2102113734', 'https://openalex.org/W2133564696', 'https://openalex.org/W2963414781']",2019-12-01
https://openalex.org/W2963414781,https://doi.org/10.1109/asru.2017.8268935,"Exploring architectures, data and units for streaming end-to-end speech recognition with RNN-transducer","We investigate training end-to-end speech recognition models with the recurrent neural network transducer (RNN-T): a streaming, all-neural, sequence-to-sequence architecture which jointly learns acoustic and language model components from transcribed acoustic data. We explore various model architectures and demonstrate how the model can be improved further if additional text or pronunciation data are available. The model consists of an `encoder', which is initialized from a connectionist temporal classification-based (CTC) acoustic model, and a `decoder' which is partially initialized from a recurrent neural network language model trained on text data alone. The entire neural network is trained with the RNN-T loss and directly outputs the recognized transcript as a sequence of graphemes, thus performing end-to-end speech recognition. We find that performance can be improved further through the use of sub-word units ('wordpieces') which capture longer context and significantly reduce substitution errors. The best RNN-T system, a twelve-layer LSTM encoder with a two-layer LSTM decoder trained with 30,000 wordpieces as output targets achieves a word error rate of 8.5% on voice-search and 5.2% on voice-dictation tasks and is comparable to a state-of-the-art baseline at 8.3% on voice-search and 5.4% voice-dictation.","['https://openalex.org/W6679436768', 'https://openalex.org/W6685711979', 'https://openalex.org/W2746192915', 'https://openalex.org/W6638749077', 'https://openalex.org/W2143612262', 'https://openalex.org/W2750499125', 'https://openalex.org/W6727690538', 'https://openalex.org/W2127141656', 'https://openalex.org/W2158373110', 'https://openalex.org/W6675365184', 'https://openalex.org/W6629930100', 'https://openalex.org/W6720905350', 'https://openalex.org/W1531333757', 'https://openalex.org/W1593247906', 'https://openalex.org/W6696982659', 'https://openalex.org/W6687566353', 'https://openalex.org/W6639156005', 'https://openalex.org/W2064675550', 'https://openalex.org/W2962826786', 'https://openalex.org/W2137644731', 'https://openalex.org/W1526990717', 'https://openalex.org/W2962765220', 'https://openalex.org/W2603679025', 'https://openalex.org/W6629052376', 'https://openalex.org/W6694517276', 'https://openalex.org/W6697499602', 'https://openalex.org/W2766773815', 'https://openalex.org/W2178654303', 'https://openalex.org/W2193413348', 'https://openalex.org/W2271840356', 'https://openalex.org/W2293009711', 'https://openalex.org/W1828163288', 'https://openalex.org/W2130942839', 'https://openalex.org/W1855892484', 'https://openalex.org/W2963920996', 'https://openalex.org/W2963983719', 'https://openalex.org/W4294555862', 'https://openalex.org/W2102113734', 'https://openalex.org/W2963729263', 'https://openalex.org/W2296545762', 'https://openalex.org/W1499864241', 'https://openalex.org/W2525778437']",2017-12-01
https://openalex.org/W2111284386,https://doi.org/10.1109/jproc.2013.2251852,Speech Synthesis Based on Hidden Markov Models,"This paper gives a general overview of hidden Markov model (HMM)-based speech synthesis, which has recently been demonstrated to be very effective in synthesizing speech. The main advantage of this approach is its flexibility in changing speaker identities, emotions, and speaking styles. This paper also discusses the relation between the HMM-based approach and the more conventional unit-selection approach that has dominated over the last decades. Finally, advanced techniques for future developments are described.","['https://openalex.org/W2116082100', 'https://openalex.org/W2135707066', 'https://openalex.org/W2147779839', 'https://openalex.org/W2039800941', 'https://openalex.org/W2042600255', 'https://openalex.org/W6683927816', 'https://openalex.org/W2091430793', 'https://openalex.org/W6603264027', 'https://openalex.org/W7024953085', 'https://openalex.org/W2000513720', 'https://openalex.org/W6676358011', 'https://openalex.org/W1979449467', 'https://openalex.org/W2153914468', 'https://openalex.org/W2164534744', 'https://openalex.org/W1521190806', 'https://openalex.org/W1861150963', 'https://openalex.org/W6677973343', 'https://openalex.org/W2165143604', 'https://openalex.org/W6630838124', 'https://openalex.org/W2171002058', 'https://openalex.org/W2167270514', 'https://openalex.org/W2130237282', 'https://openalex.org/W6679574054', 'https://openalex.org/W6696079272', 'https://openalex.org/W6601083597', 'https://openalex.org/W2146871184', 'https://openalex.org/W1963627370', 'https://openalex.org/W6611766843', 'https://openalex.org/W2106792148', 'https://openalex.org/W6605419763', 'https://openalex.org/W2150658333', 'https://openalex.org/W2428180336', 'https://openalex.org/W6603100307', 'https://openalex.org/W6713620858', 'https://openalex.org/W6632757949', 'https://openalex.org/W6713420752', 'https://openalex.org/W2033010331', 'https://openalex.org/W6682918086', 'https://openalex.org/W2049686551', 'https://openalex.org/W2091738194', 'https://openalex.org/W2166450756', 'https://openalex.org/W2093450784', 'https://openalex.org/W6697285287', 'https://openalex.org/W2009674825', 'https://openalex.org/W2137089646', 'https://openalex.org/W6696156092', 'https://openalex.org/W2074854222', 'https://openalex.org/W163616957', 'https://openalex.org/W1493898446', 'https://openalex.org/W2150791533', 'https://openalex.org/W1964420823', 'https://openalex.org/W2165108269', 'https://openalex.org/W2143029619', 'https://openalex.org/W2036233992', 'https://openalex.org/W2048389584', 'https://openalex.org/W2005768155', 'https://openalex.org/W2146927751', 'https://openalex.org/W2013317861', 'https://openalex.org/W1494687963', 'https://openalex.org/W6681282213', 'https://openalex.org/W6712240491', 'https://openalex.org/W2114010348', 'https://openalex.org/W2034277951', 'https://openalex.org/W4390926366', 'https://openalex.org/W2054369542', 'https://openalex.org/W2152950941', 'https://openalex.org/W1599512239', 'https://openalex.org/W6677254419', 'https://openalex.org/W4205130185', 'https://openalex.org/W6636404455', 'https://openalex.org/W2100969003', 'https://openalex.org/W2096555739', 'https://openalex.org/W6600174407', 'https://openalex.org/W2002342963', 'https://openalex.org/W2401725871', 'https://openalex.org/W2108513413', 'https://openalex.org/W1128417033', 'https://openalex.org/W2033516035', 'https://openalex.org/W6742103380', 'https://openalex.org/W2095437083', 'https://openalex.org/W6680838236', 'https://openalex.org/W1999885698', 'https://openalex.org/W2087110403', 'https://openalex.org/W1570629387', 'https://openalex.org/W2082388554', 'https://openalex.org/W2140918324', 'https://openalex.org/W1847897332', 'https://openalex.org/W6687365561', 'https://openalex.org/W6713743917', 'https://openalex.org/W205224898', 'https://openalex.org/W6712208827', 'https://openalex.org/W64730254', 'https://openalex.org/W6683640347', 'https://openalex.org/W6737323677', 'https://openalex.org/W1973766695', 'https://openalex.org/W6759752890', 'https://openalex.org/W6865107258', 'https://openalex.org/W6602885286', 'https://openalex.org/W6759847141', 'https://openalex.org/W153299664', 'https://openalex.org/W4301420498', 'https://openalex.org/W6676044216', 'https://openalex.org/W6608197479', 'https://openalex.org/W2042691334', 'https://openalex.org/W6682061907', 'https://openalex.org/W6601203246', 'https://openalex.org/W155946340', 'https://openalex.org/W2025638820', 'https://openalex.org/W6713876852', 'https://openalex.org/W6603973819', 'https://openalex.org/W2010602269', 'https://openalex.org/W6712568158', 'https://openalex.org/W2098380781', 'https://openalex.org/W200094172', 'https://openalex.org/W2155498861', 'https://openalex.org/W2115222111', 'https://openalex.org/W6637893649', 'https://openalex.org/W2147215240', 'https://openalex.org/W2150906086', 'https://openalex.org/W164845301', 'https://openalex.org/W1935012542', 'https://openalex.org/W6635111982', 'https://openalex.org/W2119929864', 'https://openalex.org/W2916602695', 'https://openalex.org/W795690926', 'https://openalex.org/W2143777890', 'https://openalex.org/W2142416747', 'https://openalex.org/W344150399', 'https://openalex.org/W2108674328', 'https://openalex.org/W2403168145', 'https://openalex.org/W133559434', 'https://openalex.org/W2285182995', 'https://openalex.org/W181056519', 'https://openalex.org/W2111194146', 'https://openalex.org/W1756939916', 'https://openalex.org/W2404901654', 'https://openalex.org/W4677637', 'https://openalex.org/W2149572519', 'https://openalex.org/W2915960560', 'https://openalex.org/W1602430027', 'https://openalex.org/W2396093700', 'https://openalex.org/W2133035145', 'https://openalex.org/W2188951148', 'https://openalex.org/W1550297032', 'https://openalex.org/W202879582', 'https://openalex.org/W1600722501', 'https://openalex.org/W2159528802', 'https://openalex.org/W2917438849', 'https://openalex.org/W2394921947', 'https://openalex.org/W2408435475', 'https://openalex.org/W2115979064', 'https://openalex.org/W96779174', 'https://openalex.org/W75668230', 'https://openalex.org/W26405188', 'https://openalex.org/W1512429158', 'https://openalex.org/W2286166914', 'https://openalex.org/W1584341804', 'https://openalex.org/W2160379050', 'https://openalex.org/W80543058', 'https://openalex.org/W2915628810', 'https://openalex.org/W29794711', 'https://openalex.org/W2613407020', 'https://openalex.org/W2397716941', 'https://openalex.org/W2154920538', 'https://openalex.org/W2296704011', 'https://openalex.org/W1594031697', 'https://openalex.org/W75704375', 'https://openalex.org/W2741570974', 'https://openalex.org/W1560013842', 'https://openalex.org/W2401838240', 'https://openalex.org/W2115039802', 'https://openalex.org/W2403351921', 'https://openalex.org/W2228674556', 'https://openalex.org/W70888257', 'https://openalex.org/W2143361917']",2013-04-09
https://openalex.org/W2892140764,,Close to Human Quality TTS with Transformer,"Although end-to-end neural text-to-speech (TTS) methods (such as Tacotron2) are proposed and achieve state-of-the-art performance, they still suffer from two problems: 1) low efficiency during training and inference; 2) hard to model long dependency using current recurrent neural networks (RNNs). Inspired by the success of Transformer network in neural machine translation (NMT), in this paper, we introduce and adapt the multi-head attention mechanism to replace the RNN structures and also the original attention mechanism in Tacotron2. With the help of multi-head self-attention, the hidden states in the encoder and decoder are constructed in parallel, which improves the training efficiency. Meanwhile, any two inputs at different times are connected directly by self-attention mechanism, which solves the long range dependency problem effectively. Using phoneme sequences as input, our Transformer TTS network generates mel spectrograms, followed by a WaveNet vocoder to output the final audio results. Experiments are conducted to test the efficiency and performance of our new network. For the efficiency, our Transformer TTS network can speed up the training about 4.25 times faster compared with Tacotron2. For the performance, rigorous human tests show that our proposed model achieves state-of-the-art performance (outperforms Tacotron2 with a gap of 0.048) and is very close to human quality (4.39 vs 4.44 in MOS).","['https://openalex.org/W1599623585', 'https://openalex.org/W854541894', 'https://openalex.org/W2148228080', 'https://openalex.org/W2150658333', 'https://openalex.org/W2102003408', 'https://openalex.org/W2168510624', 'https://openalex.org/W2964308564', 'https://openalex.org/W2129142580', 'https://openalex.org/W1991133427', 'https://openalex.org/W2130942839', 'https://openalex.org/W2111284386', 'https://openalex.org/W2157331557', 'https://openalex.org/W2154920538', 'https://openalex.org/W2949382160', 'https://openalex.org/W2963403868', 'https://openalex.org/W2901997113']",2018-09-19
https://openalex.org/W2765486990,https://doi.org/10.48550/arxiv.1711.00354,JSUT corpus: free large-scale Japanese speech corpus for end-to-end speech synthesis,"Thanks to improvements in machine learning techniques including deep learning, a free large-scale speech corpus that can be shared between academic institutions and commercial companies has an important role. However, such a corpus for Japanese speech synthesis does not exist. In this paper, we designed a novel Japanese speech corpus, named the ""JSUT corpus,"" that is aimed at achieving end-to-end speech synthesis. The corpus consists of 10 hours of reading-style speech data and its transcription and covers all of the main pronunciations of daily-use Japanese characters. In this paper, we describe how we designed and analyzed the corpus. The corpus is freely available online.","['https://openalex.org/W2088260968', 'https://openalex.org/W2988037059', 'https://openalex.org/W2949382160', 'https://openalex.org/W2607404225', 'https://openalex.org/W2963609956', 'https://openalex.org/W2251018732']",2017-10-28
https://openalex.org/W2964060510,https://doi.org/10.1109/icassp.2016.7472657,Investigating gated recurrent networks for speech synthesis,"Recently, recurrent neural networks (RNNs) as powerful sequence models have re-emerged as a potential acoustic model for statistical parametric speech synthesis (SPSS). The long short-term memory (LSTM) architecture is particularly attractive because it addresses the vanishing gradient problem in standard RNNs, making them easier to train. Although recent studies have demonstrated that LSTMs can achieve significantly better performance on SPSS than deep feedforward neural networks, little is known about why. Here we attempt to answer two questions: a) why do LSTMs work well as a sequence model for SPSS; b) which component (e.g., input gate, output gate, forget gate) is most important. We present a visual analysis alongside a series of experiments, resulting in a proposal for a simplified architecture. The simplified architecture has significantly fewer parameters than an LSTM, thus reducing generation complexity considerably without degrading quality.","['https://openalex.org/W1990505856', 'https://openalex.org/W1613141907', 'https://openalex.org/W1499332833', 'https://openalex.org/W6711832335', 'https://openalex.org/W2397670047', 'https://openalex.org/W2106564373', 'https://openalex.org/W6712515775', 'https://openalex.org/W6712239235', 'https://openalex.org/W6696843773', 'https://openalex.org/W1576227399', 'https://openalex.org/W2049686551', 'https://openalex.org/W6675380101', 'https://openalex.org/W6640212811', 'https://openalex.org/W2129142580', 'https://openalex.org/W2134973740', 'https://openalex.org/W2154920538', 'https://openalex.org/W6634201745', 'https://openalex.org/W1543299179', 'https://openalex.org/W6713860685', 'https://openalex.org/W2150658333', 'https://openalex.org/W2045158511', 'https://openalex.org/W2078597717', 'https://openalex.org/W6637409405', 'https://openalex.org/W2107878631', 'https://openalex.org/W6616837769', 'https://openalex.org/W2064675550', 'https://openalex.org/W6638545294', 'https://openalex.org/W2157331557', 'https://openalex.org/W2079735306', 'https://openalex.org/W1815076433', 'https://openalex.org/W2404881427', 'https://openalex.org/W2394662942', 'https://openalex.org/W1924770834', 'https://openalex.org/W2102003408', 'https://openalex.org/W2395700867', 'https://openalex.org/W2399057275', 'https://openalex.org/W1571950845', 'https://openalex.org/W2294797155', 'https://openalex.org/W581956982', 'https://openalex.org/W1689711448']",2016-03-01
https://openalex.org/W2885185669,https://doi.org/10.18653/v1/d18-2012,SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing,"This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.","['https://openalex.org/W2759088880', 'https://openalex.org/W1843891098', 'https://openalex.org/W2798362442', 'https://openalex.org/W2626778328', 'https://openalex.org/W2766182427', 'https://openalex.org/W2963979492', 'https://openalex.org/W2550821151', 'https://openalex.org/W2964308564', 'https://openalex.org/W2962784628', 'https://openalex.org/W2101105183', 'https://openalex.org/W2765961751', 'https://openalex.org/W1902237438', 'https://openalex.org/W2525778437', 'https://openalex.org/W1591706642', 'https://openalex.org/W2725082186']",2018-01-01
https://openalex.org/W72347498,,Recent Development of Open-Source Speech Recognition Engine Julius,"Julius is an open-source large-vocabulary speech recognition software used for both academic research and industrial applications. It executes real-time speech recognition of a 60k-word dictation task on low-spec PCs with small footprint, and even on embedded devices. Julius supports standard language models such as statistical N-gram model and rule-based grammars, as well as Hidden Markov Model (HMM) as an acoustic model. One can build a speech recognition system of his own purpose, or can integrate the speech recognition capability to a variety of applications using Julius. This article describes an overview of Julius, major features and specifications, and summarizes the developments conducted in the recent years.","['https://openalex.org/W154541177', 'https://openalex.org/W87954838', 'https://openalex.org/W90935298', 'https://openalex.org/W1601547765', 'https://openalex.org/W2157411293', 'https://openalex.org/W2280630092', 'https://openalex.org/W2012540220', 'https://openalex.org/W2400565116', 'https://openalex.org/W2121715942', 'https://openalex.org/W2166823384', 'https://openalex.org/W1549285799', 'https://openalex.org/W1489972948', 'https://openalex.org/W2026575276', 'https://openalex.org/W2169941844', 'https://openalex.org/W2594610113', 'https://openalex.org/W1631260214', 'https://openalex.org/W1857573607', 'https://openalex.org/W1502984613', 'https://openalex.org/W151556337', 'https://openalex.org/W2022951240']",2009-10-04
https://openalex.org/W2970401203,https://doi.org/10.48550/arxiv.1907.05242,Large Memory Layers with Product Keys,"This paper introduces a structured memory which can be easily integrated into a neural network. The memory is very large by design and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead. Its design and access pattern is based on product keys, which enable fast and exact nearest neighbor search. The ability to increase the number of parameters while keeping the same computational budget lets the overall system strike a better trade-off between prediction accuracy and computation efficiency both at training and test time. This memory layer allows us to tackle very large scale language modeling tasks. In our experiments we consider a dataset with up to 30 billion words, and we plug our memory layer in a state-of-the-art transformer-based architecture. In particular, we found that a memory augmented model with only 12 layers outperforms a baseline transformer model with 24 layers, while being twice faster at inference time. We release our code for reproducibility purposes.",[],2019-07-10
https://openalex.org/W3032697904,,NONOTO: A Model-agnostic Web Interface for Interactive Music Composition by Inpainting.,"Inpainting-based generative modeling allows for stimulating human-machine interactions by letting users perform stylistically coherent local editions to an object using a statistical model. We present NONOTO, a new interface for interactive music generation based on inpainting models. It is aimed both at researchers, by offering a simple and flexible API allowing them to connect their own models with the interface, and at musicians by providing industry-standard features such as audio playback, real-time MIDI output and straightforward synchronization with DAWs using Ableton Link.","['https://openalex.org/W2514141612', 'https://openalex.org/W2891815651', 'https://openalex.org/W2963575853', 'https://openalex.org/W2916279611', 'https://openalex.org/W2807633959', 'https://openalex.org/W2792210438']",2019-01-01
https://openalex.org/W2778792233,https://doi.org/10.48550/arxiv.1712.09763,PixelSNAIL: An Improved Autoregressive Generative Model,"Autoregressive generative models consistently achieve the best results in density estimation tasks involving high dimensional data, such as images or audio. They pose density estimation as a sequence modeling task, where a recurrent neural network (RNN) models the conditional distribution over the next element conditioned on all previous elements. In this paradigm, the bottleneck is the extent to which the RNN can model long-range dependencies, and the most successful approaches rely on causal convolutions, which offer better access to earlier parts of the sequence than conventional RNNs. Taking inspiration from recent work in meta reinforcement learning, where dealing with long-range dependencies is also essential, we introduce a new generative model architecture that combines causal convolutions with self attention. In this note, we describe the resulting model and present state-of-the-art log-likelihood results on CIFAR-10 (2.85 bits per dim) and $32 \times 32$ ImageNet (3.80 bits per dim). Our implementation is available at https://github.com/neocxi/pixelsnail-public","['https://openalex.org/W2423557781', 'https://openalex.org/W2964122153', 'https://openalex.org/W2949382160', 'https://openalex.org/W2951881474', 'https://openalex.org/W2409550820', 'https://openalex.org/W2086161653']",2017-12-28
https://openalex.org/W2991421901,https://doi.org/10.5281/zenodo.3527814,Learning to Traverse Latent Spaces for Musical Score Inpainting,"Music Inpainting is the task of filling in missing or lost information in a piece of music. We investigate this task from an interactive music creation perspective. To this end, a novel deep learning-based approach for musical score inpainting is proposed. The designed model takes both past and future musical context into account and is capable of suggesting ways to connect them in a musically meaningful manner. To achieve this, we leverage the representational power of the latent space of a Variational Auto-Encoder and train a Recurrent Neural Network which learns to traverse this latent space conditioned on the past and future musical contexts. Consequently, the designed model is capable of generating several measures of music to connect two musical excerpts. The capabilities and performance of the model are showcased by comparison with competitive baselines using several objective and subjective evaluation methods. The results show that the model generates meaningful inpaintings and can be used in interactive music creation applications. Overall, the method demonstrates the merit of learning complex trajectories in the latent spaces of deep generative models.",[],2019-11-04
https://openalex.org/W2789541106,https://doi.org/10.18653/v1/n18-2074,Self-Attention with Relative Position Representations,"Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.","['https://openalex.org/W2183341477', 'https://openalex.org/W2949888546', 'https://openalex.org/W2525778437', 'https://openalex.org/W2626778328', 'https://openalex.org/W1522301498', 'https://openalex.org/W2540404261', 'https://openalex.org/W2951008357', 'https://openalex.org/W1902237438', 'https://openalex.org/W2766453196', 'https://openalex.org/W2613904329', 'https://openalex.org/W2413794162', 'https://openalex.org/W2950635152', 'https://openalex.org/W2964308564']",2018-01-01
https://openalex.org/W2995416527,https://doi.org/10.48550/arxiv.1912.05537,Encoding Musical Style with Transformer Autoencoders,"We consider the problem of learning high-level controls over the global structure of generated sequences, particularly in the context of symbolic music generation with complex language models. In this work, we present the Transformer autoencoder, which aggregates encodings of the input data across time to obtain a global representation of style from a given performance. We show it is possible to combine this global representation with other temporally distributed embeddings, enabling improved control over the separate aspects of performance style and melody. Empirically, we demonstrate the effectiveness of our method on various music generation tasks on the MAESTRO dataset and a YouTube dataset with 10,000+ hours of piano performances, where we achieve improvements in terms of log-likelihood and mean listening scores as compared to baselines.","['https://openalex.org/W2958816042', 'https://openalex.org/W2181347294', 'https://openalex.org/W2769811909', 'https://openalex.org/W2188365844', 'https://openalex.org/W2210838531', 'https://openalex.org/W2963135265', 'https://openalex.org/W2606176153', 'https://openalex.org/W2789541106', 'https://openalex.org/W2898827701', 'https://openalex.org/W2963073614', 'https://openalex.org/W2969833269', 'https://openalex.org/W2100495367', 'https://openalex.org/W2963408210', 'https://openalex.org/W2952428868', 'https://openalex.org/W2949888546', 'https://openalex.org/W2919624000', 'https://openalex.org/W2027518030', 'https://openalex.org/W189596042', 'https://openalex.org/W2025768430', 'https://openalex.org/W2766669584', 'https://openalex.org/W2963403868', 'https://openalex.org/W2910577860', 'https://openalex.org/W2792210438', 'https://openalex.org/W1531663008', 'https://openalex.org/W2606712314', 'https://openalex.org/W2785779000', 'https://openalex.org/W2145094598', 'https://openalex.org/W2949382160', 'https://openalex.org/W2099471712', 'https://openalex.org/W2964669873', 'https://openalex.org/W2898148140', 'https://openalex.org/W2803963372', 'https://openalex.org/W2951004968', 'https://openalex.org/W2963636093', 'https://openalex.org/W2946521317']",2019-12-10
https://openalex.org/W2944828972,https://doi.org/10.48550/arxiv.1905.09272,Data-Efficient Image Recognition with Contrastive Predictive Coding,"Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with artificial ones remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset. When used as input for non-linear classification with deep neural networks, this representation allows us to use 2-5x less labels than classifiers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on the PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classifiers.","['https://openalex.org/W2148349024', 'https://openalex.org/W2099471712', 'https://openalex.org/W2951605557', 'https://openalex.org/W2949536664', 'https://openalex.org/W2097732278', 'https://openalex.org/W1993845689', 'https://openalex.org/W2962742544', 'https://openalex.org/W2987283559', 'https://openalex.org/W2119885245', 'https://openalex.org/W219040644', 'https://openalex.org/W2926645869', 'https://openalex.org/W2157364932', 'https://openalex.org/W2528973876', 'https://openalex.org/W2619371851', 'https://openalex.org/W2950179405', 'https://openalex.org/W2473208550', 'https://openalex.org/W2942602174', 'https://openalex.org/W2964242306', 'https://openalex.org/W2250384498', 'https://openalex.org/W2941161300', 'https://openalex.org/W2911779594', 'https://openalex.org/W2412320034', 'https://openalex.org/W2108501770', 'https://openalex.org/W2951884559', 'https://openalex.org/W2599837529', 'https://openalex.org/W2942203175', 'https://openalex.org/W2950133940', 'https://openalex.org/W2138621090', 'https://openalex.org/W2769112066', 'https://openalex.org/W2893749619', 'https://openalex.org/W2145889472', 'https://openalex.org/W2048679005', 'https://openalex.org/W2117539524', 'https://openalex.org/W2963341956', 'https://openalex.org/W2798991696', 'https://openalex.org/W2198618282', 'https://openalex.org/W2962824366', 'https://openalex.org/W2949117887', 'https://openalex.org/W84281818', 'https://openalex.org/W2194321275', 'https://openalex.org/W197865394', 'https://openalex.org/W2106053110', 'https://openalex.org/W2321533354', 'https://openalex.org/W1630959083', 'https://openalex.org/W2948012107', 'https://openalex.org/W2994536315', 'https://openalex.org/W2152790380', 'https://openalex.org/W2949517790', 'https://openalex.org/W2935908327', 'https://openalex.org/W2037227137', 'https://openalex.org/W1909320841', 'https://openalex.org/W2964159205', 'https://openalex.org/W2946856970', 'https://openalex.org/W1508769776', 'https://openalex.org/W1544092585', 'https://openalex.org/W1522301498', 'https://openalex.org/W2619697695', 'https://openalex.org/W2952863374', 'https://openalex.org/W2136504847', 'https://openalex.org/W2405787682', 'https://openalex.org/W2842511635', 'https://openalex.org/W2953106684', 'https://openalex.org/W2146444479', 'https://openalex.org/W2963115079', 'https://openalex.org/W2431080869', 'https://openalex.org/W2804047946', 'https://openalex.org/W2886281300', 'https://openalex.org/W2962958090', 'https://openalex.org/W2902016181', 'https://openalex.org/W1720344338', 'https://openalex.org/W2161739581', 'https://openalex.org/W2136922672', 'https://openalex.org/W2912889105', 'https://openalex.org/W2952509172', 'https://openalex.org/W2919115771', 'https://openalex.org/W2962369866', 'https://openalex.org/W2952229419', 'https://openalex.org/W2798512429', 'https://openalex.org/W323291900', 'https://openalex.org/W2575671312', 'https://openalex.org/W2145494108', 'https://openalex.org/W2487442924', 'https://openalex.org/W3037932933', 'https://openalex.org/W2951004968', 'https://openalex.org/W2883725317', 'https://openalex.org/W2951873722', 'https://openalex.org/W2950577311', 'https://openalex.org/W343636949', 'https://openalex.org/W2326925005', 'https://openalex.org/W1982374456', 'https://openalex.org/W2194775991', 'https://openalex.org/W1983320747', 'https://openalex.org/W2201912979', 'https://openalex.org/W2979454998', 'https://openalex.org/W2970241862', 'https://openalex.org/W2302255633', 'https://openalex.org/W2949416428', 'https://openalex.org/W2520377600', 'https://openalex.org/W1520997877']",2019-05-22
https://openalex.org/W2946567085,https://doi.org/10.18653/v1/p19-1032,Adaptive Attention Span in Transformers,"We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.","['https://openalex.org/W2133564696', 'https://openalex.org/W2963403868', 'https://openalex.org/W2325237720', 'https://openalex.org/W2951008357', 'https://openalex.org/W2963925437', 'https://openalex.org/W2963088785', 'https://openalex.org/W2964110616', 'https://openalex.org/W2556046966', 'https://openalex.org/W2740984755', 'https://openalex.org/W2964308564', 'https://openalex.org/W1793121960', 'https://openalex.org/W4385245566', 'https://openalex.org/W1902237438']",2019-01-01
https://openalex.org/W3005680577,https://doi.org/10.48550/arxiv.2002.05709,A Simple Framework for Contrastive Learning of Visual Representations,"This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.","['https://openalex.org/W2757910899', 'https://openalex.org/W2949117887', 'https://openalex.org/W2136922672', 'https://openalex.org/W2097117768', 'https://openalex.org/W2063971957', 'https://openalex.org/W2962824366', 'https://openalex.org/W2117539524', 'https://openalex.org/W1846799578', 'https://openalex.org/W12634471', 'https://openalex.org/W2194775991', 'https://openalex.org/W2970241862', 'https://openalex.org/W1686810756', 'https://openalex.org/W2951004968', 'https://openalex.org/W2326925005', 'https://openalex.org/W2138621090', 'https://openalex.org/W2518108298', 'https://openalex.org/W2944828972', 'https://openalex.org/W2017814585', 'https://openalex.org/W2155541015', 'https://openalex.org/W343636949', 'https://openalex.org/W2798991696', 'https://openalex.org/W2155904486', 'https://openalex.org/W2099471712', 'https://openalex.org/W2640408555', 'https://openalex.org/W2950577311', 'https://openalex.org/W2321533354', 'https://openalex.org/W2953327099', 'https://openalex.org/W2990500698', 'https://openalex.org/W2942203175', 'https://openalex.org/W780950768', 'https://openalex.org/W2964420626', 'https://openalex.org/W2951873722', 'https://openalex.org/W3001197829', 'https://openalex.org/W1993309459', 'https://openalex.org/W2187089797', 'https://openalex.org/W2949736877', 'https://openalex.org/W2533598788', 'https://openalex.org/W2962369866', 'https://openalex.org/W2163605009', 'https://openalex.org/W2994536315', 'https://openalex.org/W2971155163', 'https://openalex.org/W1977295328', 'https://openalex.org/W2804935296', 'https://openalex.org/W2148349024', 'https://openalex.org/W2987283559', 'https://openalex.org/W2746314669', 'https://openalex.org/W2913939497', 'https://openalex.org/W2995489995', 'https://openalex.org/W2962742544', 'https://openalex.org/W2949194345', 'https://openalex.org/W2842511635', 'https://openalex.org/W2555897561', 'https://openalex.org/W2949517790', 'https://openalex.org/W2031489346', 'https://openalex.org/W2047643928', 'https://openalex.org/W2622263826', 'https://openalex.org/W2979579363', 'https://openalex.org/W3099206234', 'https://openalex.org/W3118608800', 'https://openalex.org/W2998388430']",2020-02-13
https://openalex.org/W2591710685,https://doi.org/10.48550/arxiv.1703.00760,Sampling Variations of Lead Sheets,"Machine-learning techniques have been recently used with spectacular results to generate artefacts such as music or text. However, these techniques are still unable to capture and generate artefacts that are convincingly structured. In this paper we present an approach to generate structured musical sequences. We introduce a mechanism for sampling efficiently variations of musical sequences. Given a input sequence and a statistical model, this mechanism samples a set of sequences whose distance to the input sequence is approximately within specified bounds. This mechanism is implemented as an extension of belief propagation, and uses local fields to bias the generation. We show experimentally that sampled sequences are indeed closely correlated to the standard musical similarity measure defined by Mongeau and Sankoff. We then show how this mechanism can used to implement composition strategies that enforce arbitrary structure on a musical lead sheet generation problem.","['https://openalex.org/W2055138637', 'https://openalex.org/W2323157571', 'https://openalex.org/W2514141612', 'https://openalex.org/W2102443632', 'https://openalex.org/W2560316200', 'https://openalex.org/W2062824974', 'https://openalex.org/W2251752871', 'https://openalex.org/W2283344589']",2017-03-02
https://openalex.org/W2769811909,https://doi.org/10.48550/arxiv.1711.05772,Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models,"Deep generative neural networks have proven effective at both conditional and unconditional modeling of complex data distributions. Conditional generation enables interactive control, but creating new controls often requires expensive retraining. In this paper, we develop a method to condition generation without retraining the model. By post-hoc learning latent constraints, value functions that identify regions in latent space that generate outputs with desired attributes, we can conditionally sample from these regions with gradient-based optimization or amortized actor functions. Combining attribute constraints with a universal ""realism"" constraint, which enforces similarity to the data distribution, we generate realistic conditional images from an unconditional variational autoencoder. Further, using gradient-based optimization, we demonstrate identity-preserving transformations that make the minimal adjustment in latent space to modify the attributes of an image. Finally, with discrete sequences of musical notes, we demonstrate zero-shot conditional generation, learning latent constraints in the absence of labeled data or a differentiable reward function. Code with dedicated cloud instance has been made publicly available (https://goo.gl/STGMGx).","['https://openalex.org/W2600933416', 'https://openalex.org/W3038022805', 'https://openalex.org/W2432004435', 'https://openalex.org/W2188365844', 'https://openalex.org/W2951140085', 'https://openalex.org/W2964121744', 'https://openalex.org/W1909320841', 'https://openalex.org/W2339754110', 'https://openalex.org/W3098269892', 'https://openalex.org/W2951004968', 'https://openalex.org/W2412320034', 'https://openalex.org/W2418098761', 'https://openalex.org/W2431962807', 'https://openalex.org/W2617620476', 'https://openalex.org/W2331128040', 'https://openalex.org/W2173520492', 'https://openalex.org/W2295130376', 'https://openalex.org/W1834627138', 'https://openalex.org/W2411541852']",2017-11-15
https://openalex.org/W2963350250,https://doi.org/10.1109/iccv.2017.309,Sampling Matters in Deep Embedding Learning,"Deep embeddings answer one simple question: How similar are two images? Learning these embeddings is the bedrock of verification, zero-shot learning, and visual search. The most prominent approaches optimize a deep convolutional network with a suitable loss function, such as contrastive loss or triplet loss. While a rich line of work focuses solely on the loss functions, we show in this paper that selecting training examples plays an equally important role. We propose distance weighted sampling, which selects more informative and stable examples than traditional approaches. In addition, we show that a simple margin based loss is sufficient to outperform all other loss functions. We evaluate our approach on the Stanford Online Products, CAR196, and the CUB200-2011 datasets for image retrieval and clustering, and on the LFW dataset for face verification. Our method achieves state-of-the-art performance on all of them.","['https://openalex.org/W6677328822', 'https://openalex.org/W6684554773', 'https://openalex.org/W2145287260', 'https://openalex.org/W6711814008', 'https://openalex.org/W6681239517', 'https://openalex.org/W6675751002', 'https://openalex.org/W6729100687', 'https://openalex.org/W2200092826', 'https://openalex.org/W2138621090', 'https://openalex.org/W6623399679', 'https://openalex.org/W2194775991', 'https://openalex.org/W6665195449', 'https://openalex.org/W6729265172', 'https://openalex.org/W2047221353', 'https://openalex.org/W2138011018', 'https://openalex.org/W6730323794', 'https://openalex.org/W2171590421', 'https://openalex.org/W1869500417', 'https://openalex.org/W2021354639', 'https://openalex.org/W2157364932', 'https://openalex.org/W6766314179', 'https://openalex.org/W4239510810', 'https://openalex.org/W2099741732', 'https://openalex.org/W6725923168', 'https://openalex.org/W6676297131', 'https://openalex.org/W2964076257', 'https://openalex.org/W2963026686', 'https://openalex.org/W2096830573', 'https://openalex.org/W2341528187', 'https://openalex.org/W2161920802', 'https://openalex.org/W6700903540', 'https://openalex.org/W2547805042', 'https://openalex.org/W6677884823', 'https://openalex.org/W6638704708', 'https://openalex.org/W2096733369', 'https://openalex.org/W3101998545', 'https://openalex.org/W4239072543', 'https://openalex.org/W836608889', 'https://openalex.org/W2549858646', 'https://openalex.org/W2106053110', 'https://openalex.org/W2963177757', 'https://openalex.org/W2560370322', 'https://openalex.org/W2325939864', 'https://openalex.org/W2555897561', 'https://openalex.org/W1522301498', 'https://openalex.org/W2547446130', 'https://openalex.org/W4295700283', 'https://openalex.org/W2108598243', 'https://openalex.org/W2166916857', 'https://openalex.org/W2561193401', 'https://openalex.org/W2543665857', 'https://openalex.org/W2962830213', 'https://openalex.org/W2118393783', 'https://openalex.org/W1782590233', 'https://openalex.org/W2962882790', 'https://openalex.org/W2964121744', 'https://openalex.org/W1842094663', 'https://openalex.org/W2144172034', 'https://openalex.org/W4285719527', 'https://openalex.org/W2513140567', 'https://openalex.org/W2058475745', 'https://openalex.org/W4298092895', 'https://openalex.org/W1509966554', 'https://openalex.org/W4294029417', 'https://openalex.org/W2140609507', 'https://openalex.org/W2117154949', 'https://openalex.org/W3099206234', 'https://openalex.org/W2119821739']",2017-10-01
https://openalex.org/W2901638613,https://doi.org/10.1007/s00521-018-3868-4,"Anticipation-RNN: enforcing unary constraints in sequence generation, with application to interactive music generation","Recurrent neural networks (RNNs) are now widely used on sequence generation tasks due to their ability to learn long-range dependencies and to generate sequences of arbitrary length. However, their left-to-right generation procedure only allows a limited control from a potential user which makes them unsuitable for interactive and creative usages such as interactive music generation. This article introduces a novel architecture called anticipation-RNN which possesses the assets of the RNN-based generative models while allowing to enforce user-defined unary constraints. We demonstrate its efficiency on the task of generating melodies satisfying unary constraints in the style of the soprano parts of the J.S. Bach chorale harmonizations. Sampling using the anticipation-RNN is of the same order of complexity than sampling from the traditional RNN model. This fast and interactive generation of musical sequences opens ways to devise real-time systems that could be used for creative purposes.","['https://openalex.org/W4229633200', 'https://openalex.org/W2565964938', 'https://openalex.org/W1963871118', 'https://openalex.org/W2588217777', 'https://openalex.org/W4300958603', 'https://openalex.org/W2144499799', 'https://openalex.org/W1583837637', 'https://openalex.org/W2758804652', 'https://openalex.org/W2064675550', 'https://openalex.org/W2162995444', 'https://openalex.org/W2063703901', 'https://openalex.org/W2402555437', 'https://openalex.org/W2530599788', 'https://openalex.org/W2131774270', 'https://openalex.org/W2792210438', 'https://openalex.org/W950853366', 'https://openalex.org/W2962699318', 'https://openalex.org/W2584641794', 'https://openalex.org/W2579406683', 'https://openalex.org/W1819710477', 'https://openalex.org/W1912497050', 'https://openalex.org/W3122518304', 'https://openalex.org/W2523097914', 'https://openalex.org/W2129192849', 'https://openalex.org/W1573082642', 'https://openalex.org/W2740988298', 'https://openalex.org/W2963575853', 'https://openalex.org/W2747545581', 'https://openalex.org/W2557283755']",2018-11-20
https://openalex.org/W2964122153,https://doi.org/10.48550/arxiv.1701.05517,PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture\n Likelihood and Other Modifications,"PixelCNNs are a recently proposed class of powerful generative models with\ntractable likelihood. Here we discuss our implementation of PixelCNNs which we\nmake available at https://github.com/openai/pixel-cnn. Our implementation\ncontains a number of modifications to the original model that both simplify its\nstructure and improve its performance. 1) We use a discretized logistic mixture\nlikelihood on the pixels, rather than a 256-way softmax, which we find to speed\nup training. 2) We condition on whole pixels, rather than R/G/B sub-pixels,\nsimplifying the model structure. 3) We use downsampling to efficiently capture\nstructure at multiple resolutions. 4) We introduce additional short-cut\nconnections to further speed up optimization. 5) We regularize the model using\ndropout. Finally, we present state-of-the-art log likelihood results on\nCIFAR-10 to demonstrate the usefulness of these modifications.\n",[],2017-01-19
https://openalex.org/W2962212541,https://doi.org/10.48550/arxiv.1907.06637,The Bach Doodle: Approachable music composition with machine learning at scale,"To make music composition more approachable, we designed the first AI-powered Google Doodle, the Bach Doodle, where users can create their own melody and have it harmonized by a machine learning model Coconet (Huang et al., 2017) in the style of Bach. For users to input melodies, we designed a simplified sheet-music based interface. To support an interactive experience at scale, we re-implemented Coconet in TensorFlow.js (Smilkov et al., 2019) to run in the browser and reduced its runtime from 40s to 2s by adopting dilated depth-wise separable convolutions and fusing operations. We also reduced the model download size to approximately 400KB through post-training weight quantization. We calibrated a speed test based on partial model evaluation time to determine if the harmonization request should be performed locally or sent to remote TPU servers. In three days, people spent 350 years worth of time playing with the Bach Doodle, and Coconet received more than 55 million queries. Users could choose to rate their compositions and contribute them to a public dataset, which we are releasing with this paper. We hope that the community finds this dataset useful for applications ranging from ethnomusicological studies, to music education, to improving machine learning models.","['https://openalex.org/W2531409750', 'https://openalex.org/W2161850243', 'https://openalex.org/W2052265952', 'https://openalex.org/W2129192849', 'https://openalex.org/W2752134738', 'https://openalex.org/W2962968839', 'https://openalex.org/W2153628411', 'https://openalex.org/W2591984255', 'https://openalex.org/W2963575853', 'https://openalex.org/W3122518304', 'https://openalex.org/W2053952057', 'https://openalex.org/W2908701480', 'https://openalex.org/W1978458153', 'https://openalex.org/W147294073', 'https://openalex.org/W2949382160', 'https://openalex.org/W2166942303', 'https://openalex.org/W161531308', 'https://openalex.org/W2569330962', 'https://openalex.org/W3123961192', 'https://openalex.org/W2899808573', 'https://openalex.org/W2169264582', 'https://openalex.org/W2132792804', 'https://openalex.org/W2514141612', 'https://openalex.org/W2753868141', 'https://openalex.org/W1982673570', 'https://openalex.org/W2523097914', 'https://openalex.org/W3131643527', 'https://openalex.org/W2142996485', 'https://openalex.org/W2891794946', 'https://openalex.org/W2962990490', 'https://openalex.org/W2170111110', 'https://openalex.org/W1931432374']",2019-07-14
https://openalex.org/W2343635552,https://doi.org/10.48550/arxiv.1604.08723,Music transcription modelling and composition using deep learning,"We apply deep learning methods, specifically long short-term memory (LSTM) networks, to music transcription modelling and composition. We build and train LSTM networks using approximately 23,000 music transcriptions expressed with a high-level vocabulary (ABC notation), and use them to generate new transcriptions. Our practical aim is to create music transcription models useful in particular contexts of music composition. We present results from three perspectives: 1) at the population level, comparing descriptive statistics of the set of training transcriptions and generated transcriptions; 2) at the individual level, examining how a generated transcription reflects the conventions of a music practice in the training transcriptions (Celtic folk); 3) at the application level, using the system for idea generation in music composition. We make our datasets, software and sound examples open and available: \url{https://github.com/IraKorshunova/folk-rnn}.","['https://openalex.org/W2143612262', 'https://openalex.org/W2100649405', 'https://openalex.org/W2071103260', 'https://openalex.org/W2009389619', 'https://openalex.org/W1946752829', 'https://openalex.org/W1819710477', 'https://openalex.org/W2962904371', 'https://openalex.org/W2031265013', 'https://openalex.org/W1924619199', 'https://openalex.org/W2067621398', 'https://openalex.org/W2152474737', 'https://openalex.org/W2919115771', 'https://openalex.org/W1815076433', 'https://openalex.org/W2107789863', 'https://openalex.org/W2064675550', 'https://openalex.org/W2057935420', 'https://openalex.org/W2251410821', 'https://openalex.org/W2160815625', 'https://openalex.org/W2149590880', 'https://openalex.org/W2396346931', 'https://openalex.org/W2069143585', 'https://openalex.org/W2150341604', 'https://openalex.org/W2003419739', 'https://openalex.org/W1501340791', 'https://openalex.org/W2302403794', 'https://openalex.org/W2078265833', 'https://openalex.org/W2045135321', 'https://openalex.org/W1534403125', 'https://openalex.org/W2951132787', 'https://openalex.org/W2949888546', 'https://openalex.org/W1810943226', 'https://openalex.org/W2112213875', 'https://openalex.org/W2063346506']",2016-04-29
https://openalex.org/W2792210438,https://doi.org/10.48550/arxiv.1803.05428,A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music,"The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the ""posterior collapse"" problem, which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a ""flat"" baseline model. An implementation of our ""MusicVAE"" is available online at http://g.co/magenta/musicvae-code.","['https://openalex.org/W2964121744', 'https://openalex.org/W2130942839', 'https://openalex.org/W2115613106', 'https://openalex.org/W2769811909', 'https://openalex.org/W2572816092', 'https://openalex.org/W2963279312', 'https://openalex.org/W2962897886', 'https://openalex.org/W2794719876', 'https://openalex.org/W2963600562', 'https://openalex.org/W2963636093', 'https://openalex.org/W2606176153', 'https://openalex.org/W2099471712', 'https://openalex.org/W2587284713', 'https://openalex.org/W2560512785', 'https://openalex.org/W2963790827', 'https://openalex.org/W2134800885', 'https://openalex.org/W2789776893', 'https://openalex.org/W2962714411', 'https://openalex.org/W2963223306', 'https://openalex.org/W2964167449', 'https://openalex.org/W2170973209', 'https://openalex.org/W2567627528', 'https://openalex.org/W648786980', 'https://openalex.org/W2964076986', 'https://openalex.org/W2064675550', 'https://openalex.org/W2522389179', 'https://openalex.org/W2131774270', 'https://openalex.org/W1959608418', 'https://openalex.org/W2574842964', 'https://openalex.org/W2153579005', 'https://openalex.org/W2951575317', 'https://openalex.org/W2475687244', 'https://openalex.org/W2606712314', 'https://openalex.org/W2766527293', 'https://openalex.org/W592244745', 'https://openalex.org/W2951535099', 'https://openalex.org/W2753738274', 'https://openalex.org/W2950752421', 'https://openalex.org/W2949382160', 'https://openalex.org/W2772689190', 'https://openalex.org/W2766227112', 'https://openalex.org/W2964308564']",2018-03-13
https://openalex.org/W3162665866,https://doi.org/10.1109/icassp39728.2021.9414560,Emformer: Efficient Memory Transformer Based Acoustic Model for Low Latency Streaming Speech Recognition,"This paper proposes an efficient memory transformer Emformer for low latency streaming speech recognition. In Emformer, the long-range history context is distilled into an augmented memory bank to reduce self-attention's computation complexity. A cache mechanism saves the computation for the key and value in self-attention for the left context. Emformer applies a parallelized block processing in training to support low latency models. We carry out experiments on benchmark LibriSpeech data. Under average latency of 960 ms, Emformer gets WER 2.50% on test-clean and 5.62% on test-other. Comparing with a strong baseline augmented memory transformer (AM-TRF), Emformer gets 4.6 folds training speedup and 18% relative real-time factor (RTF) reduction in decoding with relative WER reduction 17% on test-clean and 9% on test-other. For a low latency scenario with an average latency of 80 ms, Emformer achieves WER 3.01% on test-clean and 7.09% on test-other. Comparing with the LSTM baseline with the same latency and model size, Emformer gets relative WER reduction 9% and 16% on test-clean and test-other, respectively.","['https://openalex.org/W6679429981', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963250244', 'https://openalex.org/W6631190155', 'https://openalex.org/W3016010032', 'https://openalex.org/W6769806307', 'https://openalex.org/W3097777922', 'https://openalex.org/W2911291251', 'https://openalex.org/W2802023636', 'https://openalex.org/W6769557084', 'https://openalex.org/W3015974384', 'https://openalex.org/W3096888553', 'https://openalex.org/W2913718171', 'https://openalex.org/W2936774411', 'https://openalex.org/W6769627184', 'https://openalex.org/W6713762819', 'https://openalex.org/W6755207826', 'https://openalex.org/W2972818416', 'https://openalex.org/W3015960524', 'https://openalex.org/W2892009249', 'https://openalex.org/W2962778134', 'https://openalex.org/W2964089206', 'https://openalex.org/W2964110616', 'https://openalex.org/W3096702180', 'https://openalex.org/W6739901393', 'https://openalex.org/W3097973766', 'https://openalex.org/W1494198834', 'https://openalex.org/W6784418856', 'https://openalex.org/W3097075707', 'https://openalex.org/W6631362777', 'https://openalex.org/W3008525923', 'https://openalex.org/W1524333225', 'https://openalex.org/W2896457183', 'https://openalex.org/W2981857663', 'https://openalex.org/W2407080277', 'https://openalex.org/W3007328579', 'https://openalex.org/W4288088457', 'https://openalex.org/W4288089799', 'https://openalex.org/W2131342762', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963341956', 'https://openalex.org/W3025165719', 'https://openalex.org/W3161873870', 'https://openalex.org/W3021515889', 'https://openalex.org/W2963403868', 'https://openalex.org/W3082274269', 'https://openalex.org/W1522301498', 'https://openalex.org/W2982413405', 'https://openalex.org/W3097558625', 'https://openalex.org/W4385245566']",2021-05-13
https://openalex.org/W3162000275,https://doi.org/10.1109/icassp39728.2021.9414897,Streaming Simultaneous Speech Translation with Augmented Memory Transformer,"Transformer-based models have achieved state-of-the-art performance on speech translation tasks. However, the model architecture is not efficient enough for streaming scenarios since self-attention is computed over an entire input sequence and the computational cost grows quadratically with the length of the input sequence. Nevertheless, most of the previous work on simultaneous speech translation, the task of generating translations from partial audio input, ignores the time spent in generating the translation when analyzing the latency. With this assumption, a system may have good latency quality trade-offs but be inapplicable in real-time scenarios. In this paper, we focus on the task of streaming simultaneous speech translation, where the systems are not only capable of translating with partial input but are also able to handle very long or continuous input. We propose an end-to-end transformer-based sequence-to-sequence model, equipped with an augmented memory transformer encoder, which has shown great success on the streaming automatic speech recognition task with hybrid or transducer-based models. We conduct an empirical evaluation of the proposed model on segment, context and memory sizes and we compare our approach to a transformer with a unidirectional mask. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>","['https://openalex.org/W6614488691', 'https://openalex.org/W2466633577', 'https://openalex.org/W3096888553', 'https://openalex.org/W6739901393', 'https://openalex.org/W3037217258', 'https://openalex.org/W6631362777', 'https://openalex.org/W6761563299', 'https://openalex.org/W6631190155', 'https://openalex.org/W2952992734', 'https://openalex.org/W2951456627', 'https://openalex.org/W3034586846', 'https://openalex.org/W6768570733', 'https://openalex.org/W2121457870', 'https://openalex.org/W6784851026', 'https://openalex.org/W2529548870', 'https://openalex.org/W6717306297', 'https://openalex.org/W2089629691', 'https://openalex.org/W419376470', 'https://openalex.org/W1524333225', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963403868', 'https://openalex.org/W2945700568', 'https://openalex.org/W1522301498', 'https://openalex.org/W2419292002', 'https://openalex.org/W2995428172', 'https://openalex.org/W2941814890', 'https://openalex.org/W3115075512', 'https://openalex.org/W2975711469', 'https://openalex.org/W4385245566']",2021-05-13
https://openalex.org/W3034586846,https://doi.org/10.18653/v1/2020.acl-main.350,SimulSpeech: End-to-End Simultaneous Speech to Text Translation,"In this work, we develop SimulSpeech, an end-to-end simultaneous speech to text translation system which translates speech in source language to text in target language concurrently. SimulSpeech consists of a speech encoder, a speech segmenter and a text decoder, where 1) the segmenter builds upon the encoder and leverages a connectionist temporal classification (CTC) loss to split the input streaming speech in real time, 2) the encoder-decoder attention adopts a wait-k strategy for simultaneous translation. SimulSpeech is more challenging than previous cascaded systems (with simultaneous automatic speech recognition (ASR) and simultaneous neural machine translation (NMT)). We introduce two novel knowledge distillation methods to ensure the performance: 1) Attention-level knowledge distillation transfers the knowledge from the multiplication of the attention matrices of simultaneous NMT and ASR models to help the training of the attention mechanism in SimulSpeech; 2) Data-level knowledge distillation transfers the knowledge from the full-sentence NMT model and also reduces the complexity of data distribution to help on the optimization of SimulSpeech. Experiments on MuST-C English-Spanish and English-German spoken language translation datasets show that SimulSpeech achieves reasonable BLEU scores and lower delay compared to full-sentence end-to-end speech to text translation (without simultaneous translation), and better performance than the two-stage cascaded simultaneous translation model in terms of BLEU scores and translation delay.","['https://openalex.org/W2945700568', 'https://openalex.org/W2964243274', 'https://openalex.org/W2963418779', 'https://openalex.org/W2936969148', 'https://openalex.org/W2121457870', 'https://openalex.org/W2419292002', 'https://openalex.org/W2962784628', 'https://openalex.org/W2970730223', 'https://openalex.org/W2964121744', 'https://openalex.org/W2963403868', 'https://openalex.org/W4297747548', 'https://openalex.org/W2952650870', 'https://openalex.org/W2972448360', 'https://openalex.org/W2127141656', 'https://openalex.org/W2964265128', 'https://openalex.org/W2905933322', 'https://openalex.org/W2963834942', 'https://openalex.org/W2101105183', 'https://openalex.org/W2964078338', 'https://openalex.org/W2949328740', 'https://openalex.org/W2896234185', 'https://openalex.org/W2995999067', 'https://openalex.org/W2946200149', 'https://openalex.org/W2951642234', 'https://openalex.org/W4385245566', 'https://openalex.org/W2901607128', 'https://openalex.org/W2089629691', 'https://openalex.org/W2963414781', 'https://openalex.org/W2613904329', 'https://openalex.org/W2950613790', 'https://openalex.org/W2529548870', 'https://openalex.org/W1522301498', 'https://openalex.org/W2963736842']",2020-01-01
https://openalex.org/W3105422997,https://doi.org/10.18653/v1/2020.findings-emnlp.349,Fluent and Low-latency Simultaneous Speech-to-Speech Translation with Self-adaptive Training,"Simultaneous speech-to-speech translation is an extremely challenging but widely useful scenario that aims to generate target-language speech only a few seconds behind the source-language speech. In addition, we have to continuously translate a speech of multiple sentences, but all recent solutions merely focus on the single-sentence scenario. As a result, current approaches will accumulate more and more latencies in later sentences when the speaker talks faster and introduce unnatural pauses into translated speech when the speaker talks slower. To overcome these issues, we propose Self-Adaptive Translation which flexibly adjusts the length of translations to accommodate different source speech rates. At similar levels of translation quality (as measured by BLEU), our method generates more fluent target speech latency than the baseline, in both Zh<->En directions.","['https://openalex.org/W2966045039', 'https://openalex.org/W2963403868', 'https://openalex.org/W3015583403', 'https://openalex.org/W2970074184', 'https://openalex.org/W3120734549', 'https://openalex.org/W2029996593', 'https://openalex.org/W3015338123', 'https://openalex.org/W3015927303', 'https://openalex.org/W2963782041', 'https://openalex.org/W2970084653', 'https://openalex.org/W3104081910', 'https://openalex.org/W2951642234', 'https://openalex.org/W2995428172', 'https://openalex.org/W314773663', 'https://openalex.org/W2148772848', 'https://openalex.org/W3035348852', 'https://openalex.org/W2952992734', 'https://openalex.org/W3034982693', 'https://openalex.org/W3015654466', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963691546', 'https://openalex.org/W2038498796', 'https://openalex.org/W2186384049', 'https://openalex.org/W2163366827', 'https://openalex.org/W2121457870', 'https://openalex.org/W2566564022']",2020-01-01
https://openalex.org/W3104081910,https://doi.org/10.18653/v1/2020.findings-emnlp.346,Incremental Text-to-Speech Synthesis with Prefix-to-Prefix Framework,"Text-to-speech synthesis (TTS) has witnessed rapid progress in recent years, where neural methods became capable of producing audios with high naturalness. However, these efforts still suffer from two types of latencies: (a) the computational latency (synthesizing time), which grows linearly with the sentence length, and (b) the input latency in scenarios where the input text is incrementally available (such as in simultaneous translation, dialog generation, and assistive technologies). To reduce these latencies, we propose a neural incremental TTS approach using the prefix-to-prefix framework from simultaneous translation. We synthesize speech in an online fashion, playing a segment of audio while generating the next, resulting in an O(1) rather than O(n) latency. Experiments on English and Chinese TTS show that our approach achieves similar speech naturalness compared to full sentence TTS, but only with a constant (1-2 words) latency.","['https://openalex.org/W3034982693', 'https://openalex.org/W2945613576', 'https://openalex.org/W3035348852', 'https://openalex.org/W2972356804', 'https://openalex.org/W2294763331', 'https://openalex.org/W2964243274', 'https://openalex.org/W4289305009', 'https://openalex.org/W3015338123', 'https://openalex.org/W2747874407', 'https://openalex.org/W2970084653', 'https://openalex.org/W2519091744', 'https://openalex.org/W2142813692', 'https://openalex.org/W2970730223', 'https://openalex.org/W2970074184', 'https://openalex.org/W2110150695', 'https://openalex.org/W2962882868', 'https://openalex.org/W4294619240', 'https://openalex.org/W2949382160', 'https://openalex.org/W2056890381', 'https://openalex.org/W2407460969', 'https://openalex.org/W2972895078', 'https://openalex.org/W3105422997', 'https://openalex.org/W2963691546', 'https://openalex.org/W44352932', 'https://openalex.org/W4318717450', 'https://openalex.org/W2946200149', 'https://openalex.org/W3038172701', 'https://openalex.org/W2769810959', 'https://openalex.org/W1941338968', 'https://openalex.org/W1487640415', 'https://openalex.org/W2951456627', 'https://openalex.org/W2951642234', 'https://openalex.org/W2963300588', 'https://openalex.org/W2963609956', 'https://openalex.org/W2963975282', 'https://openalex.org/W2903739847', 'https://openalex.org/W2889095150']",2020-01-01
https://openalex.org/W3035348852,https://doi.org/10.18653/v1/2020.acl-main.254,Simultaneous Translation Policies: From Fixed to Adaptive,"Adaptive policies are better than fixed policies for simultaneous translation, since they can flexibly balance the tradeoff between translation quality and latency based on the current context information. But previous methods on obtaining adaptive policies either rely on complicated training process, or underperform simple fixed policies. We design an algorithm to achieve adaptive policies via a simple heuristic composition of a set of fixed policies. Experiments on Chinese -> English and German -> English show that our adaptive policies can outperform fixed ones by up to 4 BLEU points for the same latency, and more surprisingly, it even surpasses the BLEU score of full-sentence translation in the greedy mode (and very close to beam mode), but with much lower latency.","['https://openalex.org/W2101105183', 'https://openalex.org/W2963729263', 'https://openalex.org/W2419292002', 'https://openalex.org/W2952992734', 'https://openalex.org/W2970074184', 'https://openalex.org/W1534477342', 'https://openalex.org/W4385245566', 'https://openalex.org/W808583520', 'https://openalex.org/W2089629691', 'https://openalex.org/W2132959801', 'https://openalex.org/W2251955814', 'https://openalex.org/W2970084653', 'https://openalex.org/W2608395138', 'https://openalex.org/W2135293965', 'https://openalex.org/W2529548870', 'https://openalex.org/W2964078338', 'https://openalex.org/W2951456627', 'https://openalex.org/W2890698823', 'https://openalex.org/W2962784628', 'https://openalex.org/W2963403868', 'https://openalex.org/W2178654303', 'https://openalex.org/W2951642234']",2020-01-01
https://openalex.org/W3115075512,https://doi.org/10.18653/v1/2020.aacl-main.58,SimulMT to SimulST: Adapting Simultaneous Text Translation to End-to-End Simultaneous Speech Translation,"We investigate how to adapt simultaneous text translation methods such as wait-k and monotonic multihead attention to end-to-end simultaneous speech translation by introducing a pre-decision module. A detailed analysis is provided on the latency-quality trade-offs of combining fixed and flexible pre-decision with fixed and flexible policies. We also design a novel computation-aware latency metric, adapted from Average Lagging.","['https://openalex.org/W3034586846', 'https://openalex.org/W3037793211', 'https://openalex.org/W2963532001', 'https://openalex.org/W2890698823', 'https://openalex.org/W2945700568', 'https://openalex.org/W2963250244', 'https://openalex.org/W2951642234', 'https://openalex.org/W2995428172', 'https://openalex.org/W3037465386', 'https://openalex.org/W2529548870', 'https://openalex.org/W2978099976', 'https://openalex.org/W2963158258', 'https://openalex.org/W2101105183', 'https://openalex.org/W2970074184', 'https://openalex.org/W2963061963', 'https://openalex.org/W2952992734']",2020-01-01
https://openalex.org/W3046643869,https://doi.org/10.48550/arxiv.2007.16193,SimulEval: An Evaluation Toolkit for Simultaneous Translation,"Simultaneous translation on both text and speech focuses on a real-time and low-latency scenario where the model starts translating before reading the complete source input. Evaluating simultaneous translation models is more complex than offline models because the latency is another factor to consider in addition to translation quality. The research community, despite its growing focus on novel modeling approaches to simultaneous translation, currently lacks a universal evaluation procedure. Therefore, we present SimulEval, an easy-to-use and general evaluation toolkit for both simultaneous text and speech translation. A server-client scheme is introduced to create a simultaneous translation scenario, where the server sends source input and receives predictions for evaluation and the client executes customized policies. Given a policy, it automatically performs simultaneous decoding and collectively reports several popular latency metrics. We also adapt latency metrics from text simultaneous translation to the speech task. Additionally, SimulEval is equipped with a visualization interface to provide better understanding of the simultaneous decoding process of a system. SimulEval has already been extensively used for the IWSLT 2020 shared task on simultaneous speech translation. Code will be released upon publication.","['https://openalex.org/W2101105183', 'https://openalex.org/W2951642234', 'https://openalex.org/W2995428172', 'https://openalex.org/W2251955814', 'https://openalex.org/W2419292002', 'https://openalex.org/W2946888380', 'https://openalex.org/W2963532001', 'https://openalex.org/W2890698823', 'https://openalex.org/W2296335794', 'https://openalex.org/W2952992734', 'https://openalex.org/W2149327368', 'https://openalex.org/W2123301721', 'https://openalex.org/W3034586846', 'https://openalex.org/W2529548870', 'https://openalex.org/W3037465386']",2020-07-31
https://openalex.org/W2529548870,https://doi.org/10.18653/v1/e17-1099,Learning to Translate in Real-time with Neural Machine Translation,"This research was reported in the trade magazine Slator: Language Industry Intelligence in Oct 2016 as “Significant progress in real-time machine translation.""","['https://openalex.org/W2962784628', 'https://openalex.org/W808583520', 'https://openalex.org/W2132959801', 'https://openalex.org/W2964308564', 'https://openalex.org/W1522301498', 'https://openalex.org/W2130942839', 'https://openalex.org/W2101105183', 'https://openalex.org/W2395821692', 'https://openalex.org/W2089629691', 'https://openalex.org/W2963158258', 'https://openalex.org/W419376470', 'https://openalex.org/W2964121744', 'https://openalex.org/W2178654303', 'https://openalex.org/W2251955814', 'https://openalex.org/W2108325777', 'https://openalex.org/W2121457870', 'https://openalex.org/W2952264928', 'https://openalex.org/W2964260331', 'https://openalex.org/W2133564696', 'https://openalex.org/W2963729263', 'https://openalex.org/W2419292002', 'https://openalex.org/W2119717200']",2017-01-01
https://openalex.org/W3100608856,https://doi.org/10.48550/arxiv.2011.04845,"Simultaneous Speech-to-Speech Translation System with Neural Incremental ASR, MT, and TTS","This paper presents a newly developed, simultaneous neural speech-to-speech translation system and its evaluation. The system consists of three fully-incremental neural processing modules for automatic speech recognition (ASR), machine translation (MT), and text-to-speech synthesis (TTS). We investigated its overall latency in the system's Ear-Voice Span and speaking latency along with module-level performance.","['https://openalex.org/W2970074184', 'https://openalex.org/W2972895078', 'https://openalex.org/W1902237438', 'https://openalex.org/W2887005286', 'https://openalex.org/W2889095150', 'https://openalex.org/W2576482813', 'https://openalex.org/W2184135559', 'https://openalex.org/W2963403868', 'https://openalex.org/W2765486990', 'https://openalex.org/W2989576480', 'https://openalex.org/W2529548870', 'https://openalex.org/W2024490156', 'https://openalex.org/W2951456627', 'https://openalex.org/W2600818048', 'https://openalex.org/W2972356804', 'https://openalex.org/W2963979492', 'https://openalex.org/W2605202026', 'https://openalex.org/W2963532001', 'https://openalex.org/W37526647', 'https://openalex.org/W2250357346', 'https://openalex.org/W2101105183', 'https://openalex.org/W3035348852', 'https://openalex.org/W2263232528', 'https://openalex.org/W2884852625', 'https://openalex.org/W2766723428', 'https://openalex.org/W2519648275', 'https://openalex.org/W2042196737', 'https://openalex.org/W419376470', 'https://openalex.org/W2964308564', 'https://openalex.org/W2471520273', 'https://openalex.org/W2161772224']",2020-11-10
https://openalex.org/W2419292002,https://doi.org/10.48550/arxiv.1606.02012,Can neural machine translation do simultaneous translation?,"We investigate the potential of attention-based neural machine translation in simultaneous translation. We introduce a novel decoding algorithm, called simultaneous greedy decoding, that allows an existing neural machine translation model to begin translating before a full source sentence is received. This approach is unique from previous works on simultaneous translation in that segmentation and translation are done jointly to maximize the translation quality and that translating each segment is strongly conditioned on all the previous segments. This paper presents a first step toward building a full simultaneous translation system based on neural machine translation.","['https://openalex.org/W2963216553', 'https://openalex.org/W2229833550', 'https://openalex.org/W2153653739', 'https://openalex.org/W2311921240', 'https://openalex.org/W2949600092', 'https://openalex.org/W2949888546', 'https://openalex.org/W6908809', 'https://openalex.org/W419376470', 'https://openalex.org/W2251955814', 'https://openalex.org/W2964308564', 'https://openalex.org/W2124807415', 'https://openalex.org/W2121457870', 'https://openalex.org/W2962784628', 'https://openalex.org/W2950635152', 'https://openalex.org/W1902237438', 'https://openalex.org/W2306852879', 'https://openalex.org/W1753482797', 'https://openalex.org/W808583520']",2016-06-07
https://openalex.org/W2933971837,https://doi.org/10.1145/3308532.3329473,An End-to-End Conversational Style Matching Agent,"We present an end-to-end voice-based conversational agent that is able to\nengage in naturalistic multi-turn dialogue and align with the interlocutor's\nconversational style. The system uses a series of deep neural network\ncomponents for speech recognition, dialogue generation, prosodic analysis and\nspeech synthesis to generate language and prosodic expression with qualities\nthat match those of the user. We conducted a user study (N=30) in which\nparticipants talked with the agent for 15 to 20 minutes, resulting in over 8\nhours of natural interaction data. Users with high consideration conversational\nstyles reported the agent to be more trustworthy when it matched their\nconversational style. Whereas, users with high involvement conversational\nstyles were indifferent. Finally, we provide design guidelines for multi-turn\ndialogue interactions using conversational style adaptation.\n","['https://openalex.org/W2032568497', 'https://openalex.org/W1748696506', 'https://openalex.org/W2136058926', 'https://openalex.org/W2767825081', 'https://openalex.org/W2139717362', 'https://openalex.org/W2109636054', 'https://openalex.org/W2546860470', 'https://openalex.org/W1889777305', 'https://openalex.org/W2295001676', 'https://openalex.org/W2806007096', 'https://openalex.org/W2795571593', 'https://openalex.org/W2063643001', 'https://openalex.org/W2061216743', 'https://openalex.org/W2518366886', 'https://openalex.org/W2405187948', 'https://openalex.org/W2561029672', 'https://openalex.org/W2801150160', 'https://openalex.org/W2114030807', 'https://openalex.org/W2530032339', 'https://openalex.org/W2105439949', 'https://openalex.org/W2053278592', 'https://openalex.org/W2537869288', 'https://openalex.org/W1518951372', 'https://openalex.org/W2781677505', 'https://openalex.org/W2000343728']",2019-07-01
https://openalex.org/W3035451444,https://doi.org/10.18653/v1/2020.acl-main.9,PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable,"Pre-training models have been proved effective for a wide range of natural language processing tasks. Inspired by this, we propose a novel dialogue generation pre-training framework to support various kinds of conversations, including chit-chat, knowledge grounded dialogues, and conversational question answering. In this framework, we adopt flexible attention mechanisms to fully leverage the bi-directional context and the uni-directional characteristic of language generation. We also introduce discrete latent variables to tackle the inherent one-to-many mapping problem in response generation. Two reciprocal tasks of response generation and latent act recognition are designed and carried out simultaneously within a shared network. Comprehensive experiments on three publicly available datasets verify the effectiveness and superiority of the proposed framework.","['https://openalex.org/W2896457183', 'https://openalex.org/W2948336019', 'https://openalex.org/W2913443447', 'https://openalex.org/W2951697502', 'https://openalex.org/W1522301498', 'https://openalex.org/W2964121744', 'https://openalex.org/W2973049837', 'https://openalex.org/W2914204778', 'https://openalex.org/W2963206148', 'https://openalex.org/W2157331557', 'https://openalex.org/W2953039584', 'https://openalex.org/W2963341956', 'https://openalex.org/W2963825865', 'https://openalex.org/W2945260553', 'https://openalex.org/W2133012565', 'https://openalex.org/W2143177362', 'https://openalex.org/W2964587107', 'https://openalex.org/W2963544536', 'https://openalex.org/W2970682219', 'https://openalex.org/W2951583236', 'https://openalex.org/W4288624561', 'https://openalex.org/W2963903950', 'https://openalex.org/W2761590056', 'https://openalex.org/W2970597249', 'https://openalex.org/W2805005636', 'https://openalex.org/W2964213933', 'https://openalex.org/W1958706068', 'https://openalex.org/W2807873315', 'https://openalex.org/W2962717182', 'https://openalex.org/W1566289585', 'https://openalex.org/W4247864677', 'https://openalex.org/W2328886022', 'https://openalex.org/W2916898195', 'https://openalex.org/W2988937804', 'https://openalex.org/W1889081078', 'https://openalex.org/W2525778437', 'https://openalex.org/W4288246040', 'https://openalex.org/W1591706642', 'https://openalex.org/W2963475460', 'https://openalex.org/W2963411289', 'https://openalex.org/W2963330684', 'https://openalex.org/W2898875342']",2020-01-01
https://openalex.org/W2901492641,https://doi.org/10.1145/3267851.3267896,Spoken Conversational AI in Video Games,"In a traditional role-playing game (RPG) conversing with a Non-Playable Character (NPC) typically appears somewhat unrealistic and can break immersion and user engagement. In commercial games, the player usually selects one of several possible predefined conversation options which are displayed as text or labels on the screen, to progress the conversation. In contrast, we first present a spoken conversational interface, built using a state-of-the-art open-domain social conversational AI developed for the Amazon Alexa Challenge, which was modified for use in a video game. This system is designed to keep users engaged in the conversation -- which we measure by time taken speaking with the character. In particular, we use emotion detection and emotional dialogue management to enhance the conversational experience. We then evaluate the contribution of emotion detection and conversational responses in a spoken dialogue system for a role-playing video game. In order to do this, two prototypes of the same game were created: one system using sentiment analysis and emotional modelling and the other system that does not detect or react to emotions. Both systems use a spoken conversational AI system where the user can freely talk to a Non-Playable-Character using unconstrained speech input.","['https://openalex.org/W2075830307', 'https://openalex.org/W2070421237', 'https://openalex.org/W2776211780', 'https://openalex.org/W2737560561', 'https://openalex.org/W3150416000', 'https://openalex.org/W2911446408', 'https://openalex.org/W2122776933', 'https://openalex.org/W2125339386', 'https://openalex.org/W2606776062', 'https://openalex.org/W2140205964', 'https://openalex.org/W2777040485']",2018-11-05
https://openalex.org/W4226399820,https://doi.org/10.48550/arxiv.2201.08239,LaMDA: Language Models for Dialog Applications,"We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.",[],2022-01-20
https://openalex.org/W2963903950,https://doi.org/10.18653/v1/d16-1230,How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation,"We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available.Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response.We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain.We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.","['https://openalex.org/W119047706', 'https://openalex.org/W4211147687', 'https://openalex.org/W4294170691', 'https://openalex.org/W2143177362', 'https://openalex.org/W2250645967', 'https://openalex.org/W1958706068', 'https://openalex.org/W2102672872', 'https://openalex.org/W1983578042', 'https://openalex.org/W1552182777', 'https://openalex.org/W2603612888', 'https://openalex.org/W2257408573', 'https://openalex.org/W2107667213', 'https://openalex.org/W2123891489', 'https://openalex.org/W2895810819', 'https://openalex.org/W2089150068', 'https://openalex.org/W10957333', 'https://openalex.org/W1810943226', 'https://openalex.org/W2111151330', 'https://openalex.org/W2037789405', 'https://openalex.org/W1489525520', 'https://openalex.org/W2137607259', 'https://openalex.org/W4252434862', 'https://openalex.org/W2159107349', 'https://openalex.org/W3022187094', 'https://openalex.org/W2963790827', 'https://openalex.org/W2123301721', 'https://openalex.org/W2065687817', 'https://openalex.org/W1982897610', 'https://openalex.org/W2294699749', 'https://openalex.org/W1518951372', 'https://openalex.org/W2154652894', 'https://openalex.org/W2962883855', 'https://openalex.org/W2963206148', 'https://openalex.org/W1486649854', 'https://openalex.org/W1948566616', 'https://openalex.org/W2964352131', 'https://openalex.org/W1591706642', 'https://openalex.org/W2963499246', 'https://openalex.org/W2001810881', 'https://openalex.org/W1654173042', 'https://openalex.org/W2151814822', 'https://openalex.org/W137851535', 'https://openalex.org/W2951813108', 'https://openalex.org/W4295803813', 'https://openalex.org/W2251994258', 'https://openalex.org/W836999996', 'https://openalex.org/W2950483141', 'https://openalex.org/W2087735403', 'https://openalex.org/W2101105183', 'https://openalex.org/W2153579005', 'https://openalex.org/W2962854379']",2016-01-01
https://openalex.org/W3201566090,https://doi.org/10.48550/arxiv.2109.05217,Empirical Analysis of Training Strategies of Transformer-based Japanese Chit-chat Systems,"In recent years, several high-performance conversational systems have been proposed based on the Transformer encoder-decoder model. Although previous studies analyzed the effects of the model parameters and the decoding method on subjective dialogue evaluations with overall metrics, they did not analyze how the differences of fine-tuning datasets affect on user's detailed impression. In addition, the Transformer-based approach has only been verified for English, not for such languages with large inter-language distances as Japanese. In this study, we develop large-scale Transformer-based Japanese dialogue models and Japanese chit-chat datasets to examine the effectiveness of the Transformer-based approach for building chit-chat dialogue systems. We evaluated and analyzed the impressions of human dialogues in different fine-tuning datasets, model parameters, and the use of additional information.","['https://openalex.org/W3107826490', 'https://openalex.org/W2963310665', 'https://openalex.org/W2963825865', 'https://openalex.org/W3093715852', 'https://openalex.org/W2996287690', 'https://openalex.org/W2963542740', 'https://openalex.org/W2963250244', 'https://openalex.org/W1974758710', 'https://openalex.org/W1821462560', 'https://openalex.org/W1857789879']",2021-09-11
https://openalex.org/W3159481202,https://doi.org/10.1109/iccv48922.2021.00951,Emerging Properties in Self-Supervised Vision Transformers,"In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.","['https://openalex.org/W2990205821', 'https://openalex.org/W2963465221', 'https://openalex.org/W3034915791', 'https://openalex.org/W2963212250', 'https://openalex.org/W6675122589', 'https://openalex.org/W6726497184', 'https://openalex.org/W6777265123', 'https://openalex.org/W2798991696', 'https://openalex.org/W3035160371', 'https://openalex.org/W6685380521', 'https://openalex.org/W3096338464', 'https://openalex.org/W6638523607', 'https://openalex.org/W6761825139', 'https://openalex.org/W2194775991', 'https://openalex.org/W2962852342', 'https://openalex.org/W6791742336', 'https://openalex.org/W6761903662', 'https://openalex.org/W6701655646', 'https://openalex.org/W3034885317', 'https://openalex.org/W2998388430', 'https://openalex.org/W6770196601', 'https://openalex.org/W6748108687', 'https://openalex.org/W6739622702', 'https://openalex.org/W6787508319', 'https://openalex.org/W6787207808', 'https://openalex.org/W6779326418', 'https://openalex.org/W3035524453', 'https://openalex.org/W6682948231', 'https://openalex.org/W6695676441', 'https://openalex.org/W6787339664', 'https://openalex.org/W2964700958', 'https://openalex.org/W6739901393', 'https://openalex.org/W6788135285', 'https://openalex.org/W6685522000', 'https://openalex.org/W6777179611', 'https://openalex.org/W6928992657', 'https://openalex.org/W6733814495', 'https://openalex.org/W2964045208', 'https://openalex.org/W6774314701', 'https://openalex.org/W2148809531', 'https://openalex.org/W6779977557', 'https://openalex.org/W6774670964', 'https://openalex.org/W6786614245', 'https://openalex.org/W6755207826', 'https://openalex.org/W6784333009', 'https://openalex.org/W2144796873', 'https://openalex.org/W2020308406', 'https://openalex.org/W6780730929', 'https://openalex.org/W6758139636', 'https://openalex.org/W6679434410', 'https://openalex.org/W6736894310', 'https://openalex.org/W2987741655', 'https://openalex.org/W6753000030', 'https://openalex.org/W2117539524', 'https://openalex.org/W6779997284', 'https://openalex.org/W2990519439', 'https://openalex.org/W6602403235', 'https://openalex.org/W6784531446', 'https://openalex.org/W2605229288', 'https://openalex.org/W2086161653', 'https://openalex.org/W2767621619', 'https://openalex.org/W3171007011', 'https://openalex.org/W3107668149', 'https://openalex.org/W2963263347', 'https://openalex.org/W3026092005', 'https://openalex.org/W3170336723', 'https://openalex.org/W2963685250', 'https://openalex.org/W2031489346', 'https://openalex.org/W2250384498', 'https://openalex.org/W2574872930', 'https://openalex.org/W4288024349', 'https://openalex.org/W2995181141', 'https://openalex.org/W3041919418', 'https://openalex.org/W2134670479', 'https://openalex.org/W2964074409', 'https://openalex.org/W2996080391', 'https://openalex.org/W3018265077', 'https://openalex.org/W3005680577', 'https://openalex.org/W2158131535', 'https://openalex.org/W2284050935', 'https://openalex.org/W2152790380', 'https://openalex.org/W2964308564', 'https://openalex.org/W4287636287', 'https://openalex.org/W3134652006', 'https://openalex.org/W2941964676', 'https://openalex.org/W2326925005', 'https://openalex.org/W4288581820', 'https://openalex.org/W2787017828', 'https://openalex.org/W3121052760', 'https://openalex.org/W3109440167', 'https://openalex.org/W2533598788', 'https://openalex.org/W2133564696', 'https://openalex.org/W3093929102', 'https://openalex.org/W2953070460', 'https://openalex.org/W2950344723', 'https://openalex.org/W3128934284', 'https://openalex.org/W3135715136', 'https://openalex.org/W2962369866', 'https://openalex.org/W2963341956', 'https://openalex.org/W3037618862', 'https://openalex.org/W3116557712', 'https://openalex.org/W3036224891', 'https://openalex.org/W2896457183', 'https://openalex.org/W59018853', 'https://openalex.org/W3014490631', 'https://openalex.org/W2916743882', 'https://openalex.org/W3035060554', 'https://openalex.org/W2963154697', 'https://openalex.org/W2622263826', 'https://openalex.org/W2100664567', 'https://openalex.org/W3027083471', 'https://openalex.org/W4385245566', 'https://openalex.org/W3009561768', 'https://openalex.org/W3103455452', 'https://openalex.org/W2963588253', 'https://openalex.org/W2963927126', 'https://openalex.org/W3099570996', 'https://openalex.org/W3197396431', 'https://openalex.org/W2768282280', 'https://openalex.org/W3022061250', 'https://openalex.org/W2963403868', 'https://openalex.org/W3136604105', 'https://openalex.org/W2943152387', 'https://openalex.org/W3094502228', 'https://openalex.org/W1821462560', 'https://openalex.org/W3034429256', 'https://openalex.org/W3100859887', 'https://openalex.org/W2911925209', 'https://openalex.org/W2607510315', 'https://openalex.org/W3128099838', 'https://openalex.org/W3035305184', 'https://openalex.org/W2951541198', 'https://openalex.org/W2883725317', 'https://openalex.org/W2963495051', 'https://openalex.org/W3149106692', 'https://openalex.org/W3170874841', 'https://openalex.org/W3106428938', 'https://openalex.org/W3122325173', 'https://openalex.org/W2174726731', 'https://openalex.org/W2896060389', 'https://openalex.org/W3168405954']",2021-10-01
https://openalex.org/W3006926732,https://doi.org/10.21437/interspeech.2020-1242,Towards Learning a Universal Non-Semantic Representation of Speech,"The ultimate goal of transfer learning is to reduce labeled data requirements\nby exploiting a pre-existing embedding model trained for different datasets or\ntasks. The visual and language communities have established benchmarks to\ncompare embeddings, but the speech community has yet to do so. This paper\nproposes a benchmark for comparing speech representations on non-semantic\ntasks, and proposes a representation based on an unsupervised triplet-loss\nobjective. The proposed representation outperforms other representations on the\nbenchmark, and even exceeds state-of-the-art performance on a number of\ntransfer learning tasks. The embedding is trained on a publicly available\ndataset, and it is tested on a variety of low-resource downstream tasks,\nincluding personalization tasks and medical domain. The benchmark, models, and\nevaluation code are publicly released.\n","['https://openalex.org/W4297775537', 'https://openalex.org/W2963300719', 'https://openalex.org/W1608367484', 'https://openalex.org/W2964317695', 'https://openalex.org/W2996158613', 'https://openalex.org/W2944200841', 'https://openalex.org/W2620629206', 'https://openalex.org/W2726515241', 'https://openalex.org/W3099206234', 'https://openalex.org/W3094550259', 'https://openalex.org/W2030931454', 'https://openalex.org/W105569935', 'https://openalex.org/W2973157397', 'https://openalex.org/W2952558884', 'https://openalex.org/W2526050071', 'https://openalex.org/W2962839749', 'https://openalex.org/W2977259558', 'https://openalex.org/W2964013315', 'https://openalex.org/W2101234009', 'https://openalex.org/W2980287048', 'https://openalex.org/W2950007391', 'https://openalex.org/W2797583228', 'https://openalex.org/W2513507089', 'https://openalex.org/W2998249245', 'https://openalex.org/W2984843443', 'https://openalex.org/W2951828005', 'https://openalex.org/W4299518610', 'https://openalex.org/W2972943112', 'https://openalex.org/W2995254904', 'https://openalex.org/W2396589722', 'https://openalex.org/W2612445135', 'https://openalex.org/W4297808394', 'https://openalex.org/W4394655213', 'https://openalex.org/W2953360861', 'https://openalex.org/W2165698076', 'https://openalex.org/W2963194800', 'https://openalex.org/W2772935161', 'https://openalex.org/W3108131700', 'https://openalex.org/W2593116425', 'https://openalex.org/W2085662862', 'https://openalex.org/W2941715400', 'https://openalex.org/W2963087613', 'https://openalex.org/W2148154194', 'https://openalex.org/W2773070064', 'https://openalex.org/W3016011332', 'https://openalex.org/W2887280559', 'https://openalex.org/W2923014074', 'https://openalex.org/W2842511635', 'https://openalex.org/W2963310665', 'https://openalex.org/W2149933564', 'https://openalex.org/W2767754137', 'https://openalex.org/W2994728585']",2020-10-25
https://openalex.org/W4319862416,https://doi.org/10.1109/slt54892.2023.10022552,CCC-WAV2VEC 2.0: Clustering AIDED Cross Contrastive Self-Supervised Learning of Speech Representations,"While Self-Supervised Learning has helped reap the benefit of the scale from the available unlabeled data, the learning paradigms are continously being bettered. We present a new pre-training strategy named ccc-wav2vec 2.0, which uses clustering and an augmentation based cross-contrastive loss as its self-supervised objective. Through the clustering module we scale down the influence of those negative examples that are highly similar to the positive. The Cross-Contrastive loss is computed between the encoder output of the original sample and the quantizer output of its augmentation, and vice-versa, bringing robustness to the pre-training strategy. ccc-wav2vec 2.0 achieves upto 15.6% and 12.7% relative WER improvement over the baseline wav2vec 2.0 on the test-clean and test-other sets respectively of LibriSpeech, without the use of any language model. The proposed method also achieves upto 14.9% relative WER improvement over the baseline wav2vec 2.0, when fine-tuned on Switchboard data.","['https://openalex.org/W6774314701', 'https://openalex.org/W6780218876', 'https://openalex.org/W3206252155', 'https://openalex.org/W3209059054', 'https://openalex.org/W3209984917', 'https://openalex.org/W6798952882', 'https://openalex.org/W6713762819', 'https://openalex.org/W6687566353', 'https://openalex.org/W4297841844', 'https://openalex.org/W3144810982', 'https://openalex.org/W2842511635', 'https://openalex.org/W6779997284', 'https://openalex.org/W6783990618', 'https://openalex.org/W6780191644', 'https://openalex.org/W6809251386', 'https://openalex.org/W4221163898', 'https://openalex.org/W3154677174', 'https://openalex.org/W2166637769', 'https://openalex.org/W2024490156', 'https://openalex.org/W2982223350', 'https://openalex.org/W3041561163', 'https://openalex.org/W3198608154', 'https://openalex.org/W4226033575', 'https://openalex.org/W6839738141', 'https://openalex.org/W3197580070', 'https://openalex.org/W3087124270', 'https://openalex.org/W2096733369', 'https://openalex.org/W2948638722', 'https://openalex.org/W2936774411', 'https://openalex.org/W3163839574', 'https://openalex.org/W3024869864', 'https://openalex.org/W2696967604', 'https://openalex.org/W3196843354', 'https://openalex.org/W1494198834', 'https://openalex.org/W2933138175', 'https://openalex.org/W6688816777', 'https://openalex.org/W3036224891', 'https://openalex.org/W3187822143', 'https://openalex.org/W4297808394', 'https://openalex.org/W4306672449', 'https://openalex.org/W2193413348', 'https://openalex.org/W4318148707', 'https://openalex.org/W3094225009', 'https://openalex.org/W3102363610', 'https://openalex.org/W3036601975', 'https://openalex.org/W4283659485', 'https://openalex.org/W2219249508', 'https://openalex.org/W2896457183', 'https://openalex.org/W4301372783', 'https://openalex.org/W3005680577']",2023-01-09
https://openalex.org/W3156636935,https://doi.org/10.18653/v1/2021.emnlp-main.552,SimCSE: Simple Contrastive Learning of Sentence Embeddings,"This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using “entailment” pairs as positives and “contradiction” pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman’s correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show—both theoretically and empirically—that contrastive learning objective regularizes pre-trained embeddings’ anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.","['https://openalex.org/W1486649854', 'https://openalex.org/W3104033643', 'https://openalex.org/W4294170691', 'https://openalex.org/W3034850762', 'https://openalex.org/W2790235966', 'https://openalex.org/W2251939518', 'https://openalex.org/W2185175083', 'https://openalex.org/W2572185161', 'https://openalex.org/W2153579005', 'https://openalex.org/W2965373594', 'https://openalex.org/W2133458109', 'https://openalex.org/W3176047188', 'https://openalex.org/W3154229486', 'https://openalex.org/W2152180407', 'https://openalex.org/W3005680577', 'https://openalex.org/W3173783447', 'https://openalex.org/W2963804993', 'https://openalex.org/W2160660844', 'https://openalex.org/W2028175314', 'https://openalex.org/W1840435438', 'https://openalex.org/W131533222', 'https://openalex.org/W2752172973', 'https://openalex.org/W2114524997', 'https://openalex.org/W2966610483', 'https://openalex.org/W3100652389', 'https://openalex.org/W2963403868', 'https://openalex.org/W2949380545', 'https://openalex.org/W2970641574', 'https://openalex.org/W2963846996', 'https://openalex.org/W2979826702', 'https://openalex.org/W2095705004', 'https://openalex.org/W2462305634', 'https://openalex.org/W3115295967', 'https://openalex.org/W2988217457', 'https://openalex.org/W2907252220', 'https://openalex.org/W2963644595', 'https://openalex.org/W2250539671', 'https://openalex.org/W2891177506', 'https://openalex.org/W2952186591', 'https://openalex.org/W2963341956', 'https://openalex.org/W3122838366', 'https://openalex.org/W2138621090', 'https://openalex.org/W3034978746', 'https://openalex.org/W2963074118', 'https://openalex.org/W2250790822', 'https://openalex.org/W4385245566', 'https://openalex.org/W2148349024', 'https://openalex.org/W3131870090', 'https://openalex.org/W3118062200', 'https://openalex.org/W2014902591', 'https://openalex.org/W2996657533', 'https://openalex.org/W2786464815', 'https://openalex.org/W2987249037', 'https://openalex.org/W2605035112', 'https://openalex.org/W2640408555', 'https://openalex.org/W2611029872', 'https://openalex.org/W2251861449', 'https://openalex.org/W3103680885', 'https://openalex.org/W2964212550', 'https://openalex.org/W2126400076', 'https://openalex.org/W3099700870', 'https://openalex.org/W2037386840', 'https://openalex.org/W3105816068', 'https://openalex.org/W3175362188', 'https://openalex.org/W3139958517', 'https://openalex.org/W2964165804', 'https://openalex.org/W2963918774', 'https://openalex.org/W3098824823', 'https://openalex.org/W2163455955', 'https://openalex.org/W2964073004']",2021-01-01
https://openalex.org/W3113328489,https://doi.org/10.1109/iccv48922.2021.01045,Contrastive Learning for Label Efficient Semantic Segmentation,"Collecting labeled data for the task of semantic segmentation is expensive and time-consuming, as it requires dense pixel-level annotations. While recent Convolutional Neural Network (CNN) based semantic segmentation approaches have achieved impressive results by using large amounts of labeled training data, their performance drops significantly as the amount of labeled data decreases. This happens because deep CNNs trained with the de facto cross-entropy loss can easily overfit to small amounts of labeled data. To address this issue, we propose a simple and effective contrastive learning-based training strategy in which we first pretrain the network using a pixel-wise, label-based contrastive loss, and then fine-tune it using the cross-entropy loss. This approach increases intra-class compactness and inter-class separability, thereby resulting in a better pixel classifier. We demonstrate the effectiveness of the proposed training strategy using the Cityscapes and PASCAL VOC 2012 segmentation datasets. Our results show that pretraining with the proposed contrastive loss results in large performance gains (more than 20% absolute improvement in some settings) when the amount of labeled data is limited. In many settings, the proposed contrastive pretraining strategy, which does not use any additional data, is able to match or outperform the widely-used ImageNet pretraining strategy that uses more than a million additional labeled images.","['https://openalex.org/W6766945662', 'https://openalex.org/W1903029394', 'https://openalex.org/W2963167763', 'https://openalex.org/W6776700526', 'https://openalex.org/W2552414813', 'https://openalex.org/W6754472723', 'https://openalex.org/W6735013348', 'https://openalex.org/W2337429362', 'https://openalex.org/W6777265123', 'https://openalex.org/W2980189057', 'https://openalex.org/W2979579363', 'https://openalex.org/W6754879843', 'https://openalex.org/W6768371451', 'https://openalex.org/W2955813853', 'https://openalex.org/W2982106100', 'https://openalex.org/W2560023338', 'https://openalex.org/W6748692255', 'https://openalex.org/W6767570870', 'https://openalex.org/W6779101013', 'https://openalex.org/W6783961830', 'https://openalex.org/W6784265214', 'https://openalex.org/W3035703639', 'https://openalex.org/W6763197041', 'https://openalex.org/W2138621090', 'https://openalex.org/W2955058313', 'https://openalex.org/W3035524453', 'https://openalex.org/W2963321359', 'https://openalex.org/W2799124825', 'https://openalex.org/W2991391304', 'https://openalex.org/W2963198662', 'https://openalex.org/W6763442200', 'https://openalex.org/W6784905488', 'https://openalex.org/W3128661784', 'https://openalex.org/W2798991696', 'https://openalex.org/W6675751002', 'https://openalex.org/W6786187962', 'https://openalex.org/W3122412340', 'https://openalex.org/W2473131906', 'https://openalex.org/W6779977557', 'https://openalex.org/W2340897893', 'https://openalex.org/W6780757718', 'https://openalex.org/W1495267108', 'https://openalex.org/W2108598243', 'https://openalex.org/W6682250724', 'https://openalex.org/W6749464311', 'https://openalex.org/W2037227137', 'https://openalex.org/W2022508996', 'https://openalex.org/W6736147098', 'https://openalex.org/W6776821665', 'https://openalex.org/W6779992872', 'https://openalex.org/W2744404335', 'https://openalex.org/W2412782625', 'https://openalex.org/W6780214264', 'https://openalex.org/W6748481559', 'https://openalex.org/W6739696289', 'https://openalex.org/W6628616451', 'https://openalex.org/W6774314701', 'https://openalex.org/W6730323794', 'https://openalex.org/W6783990618', 'https://openalex.org/W2778764040', 'https://openalex.org/W2956648669', 'https://openalex.org/W2221898772', 'https://openalex.org/W3035680157', 'https://openalex.org/W6763301180', 'https://openalex.org/W6786093290', 'https://openalex.org/W2890782586', 'https://openalex.org/W2948400535', 'https://openalex.org/W3100345210', 'https://openalex.org/W2947706148', 'https://openalex.org/W4294149583', 'https://openalex.org/W2971250988', 'https://openalex.org/W4287600707', 'https://openalex.org/W3043044432', 'https://openalex.org/W3123939835', 'https://openalex.org/W4287755086', 'https://openalex.org/W3103897295', 'https://openalex.org/W2630837129', 'https://openalex.org/W3005680577', 'https://openalex.org/W3122325173', 'https://openalex.org/W3035003500', 'https://openalex.org/W2555897561', 'https://openalex.org/W2964309882', 'https://openalex.org/W3106528393', 'https://openalex.org/W3172615411', 'https://openalex.org/W2949517790', 'https://openalex.org/W3090114880', 'https://openalex.org/W3108655343', 'https://openalex.org/W611457968', 'https://openalex.org/W2594407953', 'https://openalex.org/W3106485021', 'https://openalex.org/W2888340395', 'https://openalex.org/W3120562181', 'https://openalex.org/W3098288266', 'https://openalex.org/W2963656735', 'https://openalex.org/W4301372783', 'https://openalex.org/W3022061250', 'https://openalex.org/W3110002552', 'https://openalex.org/W3036982689', 'https://openalex.org/W2148349024', 'https://openalex.org/W3168822201', 'https://openalex.org/W4287812705', 'https://openalex.org/W2106053110', 'https://openalex.org/W2982208971', 'https://openalex.org/W3118629228', 'https://openalex.org/W3017361373', 'https://openalex.org/W2187089797', 'https://openalex.org/W2964013229', 'https://openalex.org/W2963074118', 'https://openalex.org/W3012668059', 'https://openalex.org/W2995808743', 'https://openalex.org/W1463091736', 'https://openalex.org/W3109301572', 'https://openalex.org/W2787241931', 'https://openalex.org/W2605161420', 'https://openalex.org/W3100859887', 'https://openalex.org/W3099634177']",2021-10-01
https://openalex.org/W4312766345,https://doi.org/10.1109/cvpr52688.2022.01618,On Learning Contrastive Representations for Learning with Noisy Labels,"Deep neural networks are able to memorize noisy labels easily with a softmax cross entropy (CE) loss. Previous studies attempted to address this issue focus on incorporating a noise-robust loss function to the CE loss. However, the memorization issue is alleviated but still remains due to the non-robust CE loss. To address this issue, we focus on learning robust contrastive representations of data on which the classifier is hard to memorize the label noise under the CE loss. We propose a novel contrastive regularization function to learn such representations over noisy data where label noise does not dominate the representation learning. By theoretically investigating the representations induced by the proposed regularization function, we reveal that the learned representations keep information related to true labels and discard information related to corrupted labels. Moreover, our theoretical results also indicate that the learned representations are robust to the label noise. The effectiveness of this method is demonstrated with experiments on benchmark datasets.","['https://openalex.org/W6763094270', 'https://openalex.org/W6762161020', 'https://openalex.org/W6740005241', 'https://openalex.org/W2104094955', 'https://openalex.org/W2131953535', 'https://openalex.org/W6789472735', 'https://openalex.org/W6779977557', 'https://openalex.org/W6774670964', 'https://openalex.org/W3171007011', 'https://openalex.org/W3172770836', 'https://openalex.org/W6717305147', 'https://openalex.org/W3035546924', 'https://openalex.org/W2963697299', 'https://openalex.org/W3183932535', 'https://openalex.org/W6631943919', 'https://openalex.org/W6779326418', 'https://openalex.org/W6751647823', 'https://openalex.org/W6774796142', 'https://openalex.org/W2194775991', 'https://openalex.org/W6776700526', 'https://openalex.org/W3015437096', 'https://openalex.org/W3190654993', 'https://openalex.org/W6771630921', 'https://openalex.org/W3121661476', 'https://openalex.org/W6782914715', 'https://openalex.org/W6760993166', 'https://openalex.org/W6781063151', 'https://openalex.org/W6789528441', 'https://openalex.org/W6779784029', 'https://openalex.org/W6779797550', 'https://openalex.org/W2102348129', 'https://openalex.org/W6786683507', 'https://openalex.org/W2964292098', 'https://openalex.org/W639708223', 'https://openalex.org/W2962858109', 'https://openalex.org/W6762892961', 'https://openalex.org/W6780530918', 'https://openalex.org/W6604005260', 'https://openalex.org/W3090425949', 'https://openalex.org/W6778102432', 'https://openalex.org/W2981873476', 'https://openalex.org/W3034185248', 'https://openalex.org/W6789476682', 'https://openalex.org/W6763576130', 'https://openalex.org/W1921293667', 'https://openalex.org/W6640298173', 'https://openalex.org/W3128661784', 'https://openalex.org/W6766952621', 'https://openalex.org/W6779482673', 'https://openalex.org/W2967052791', 'https://openalex.org/W6758632346', 'https://openalex.org/W2566079294', 'https://openalex.org/W6790706061', 'https://openalex.org/W6751420435', 'https://openalex.org/W4287812705', 'https://openalex.org/W3039366696', 'https://openalex.org/W2963735582', 'https://openalex.org/W3100859887', 'https://openalex.org/W1583837637', 'https://openalex.org/W2970038028', 'https://openalex.org/W4297808394', 'https://openalex.org/W2803187616', 'https://openalex.org/W4295253428', 'https://openalex.org/W2418025139', 'https://openalex.org/W3036700096', 'https://openalex.org/W4287716221', 'https://openalex.org/W2947049994', 'https://openalex.org/W3035325670', 'https://openalex.org/W3009561768', 'https://openalex.org/W3035060554', 'https://openalex.org/W3105982656']",2022-06-01
https://openalex.org/W3200253633,https://doi.org/10.18653/v1/2021.emnlp-main.359,Not All Negatives are Equal: Label-Aware Contrastive Loss for Fine-grained Text Classification,"Fine-grained classification involves dealing with datasets with larger number of classes with subtle differences between them. Guiding the model to focus on differentiating dimensions between these commonly confusable classes is key to improving performance on fine-grained tasks. In this work, we analyse the contrastive fine-tuning of pre-trained language models on two fine-grained text classification tasks, emotion classification and sentiment analysis. We adaptively embed class relationships into a contrastive objective function to help differently weigh the positives and negatives, and in particular, weighting closely confusable negatives more than less similar negative examples. We find that Label-aware Contrastive Loss outperforms previous contrastive methods, in the presence of larger number and/or more confusable classes, and helps models to produce output distributions that are more differentiated.","['https://openalex.org/W4250971714', 'https://openalex.org/W1821462560', 'https://openalex.org/W2963341956', 'https://openalex.org/W2965373594', 'https://openalex.org/W3035524453', 'https://openalex.org/W2948012107', 'https://openalex.org/W2997666887', 'https://openalex.org/W3213949538', 'https://openalex.org/W2970454332', 'https://openalex.org/W2250493512', 'https://openalex.org/W2970544797', 'https://openalex.org/W3156636935', 'https://openalex.org/W2947380870', 'https://openalex.org/W3108655343', 'https://openalex.org/W2735926531', 'https://openalex.org/W2970487286', 'https://openalex.org/W2971155163', 'https://openalex.org/W2252222520', 'https://openalex.org/W3101066076', 'https://openalex.org/W3034781633', 'https://openalex.org/W2081580037', 'https://openalex.org/W2899004870', 'https://openalex.org/W2951287343', 'https://openalex.org/W3035419191', 'https://openalex.org/W2963223838', 'https://openalex.org/W3096655658', 'https://openalex.org/W4287614078', 'https://openalex.org/W4296976275', 'https://openalex.org/W3005680577', 'https://openalex.org/W3122924117', 'https://openalex.org/W2406945108', 'https://openalex.org/W2963476860', 'https://openalex.org/W2892118011', 'https://openalex.org/W2963736842', 'https://openalex.org/W4287824654', 'https://openalex.org/W2963371670', 'https://openalex.org/W4249573750', 'https://openalex.org/W3098824823', 'https://openalex.org/W3131870090', 'https://openalex.org/W2963735582', 'https://openalex.org/W2997049449', 'https://openalex.org/W3034854575', 'https://openalex.org/W3103884771', 'https://openalex.org/W2795282075', 'https://openalex.org/W3105111366', 'https://openalex.org/W4287716221', 'https://openalex.org/W4287812705', 'https://openalex.org/W2953915809', 'https://openalex.org/W3034828027', 'https://openalex.org/W3034185248', 'https://openalex.org/W3034323190', 'https://openalex.org/W2044913453', 'https://openalex.org/W3026732421', 'https://openalex.org/W2979826702', 'https://openalex.org/W3139489817', 'https://openalex.org/W2251939518', 'https://openalex.org/W3034978746', 'https://openalex.org/W2609701267', 'https://openalex.org/W2971296908', 'https://openalex.org/W3042609801', 'https://openalex.org/W3100093508', 'https://openalex.org/W2951583236', 'https://openalex.org/W2579318141', 'https://openalex.org/W3100345210', 'https://openalex.org/W3155584966']",2021-01-01
https://openalex.org/W2085487226,https://doi.org/10.1080/03610927408827101,A dendrite method for cluster analysis,"Abstract A method for identifying clusters of points in a multidimensional Euclidean space is described and its application to taxonomy considered. It reconciles, in a sense, two different approaches to the investigation of the spatial relationships between the points, viz., the agglomerative and the divisive methods. A graph, the shortest dendrite of Florek etal. (1951a), is constructed on a nearest neighbour basis and then divided into clusters by applying the criterion of minimum within cluster sum of squares. This procedure ensures an effective reduction of the number of possible splits. The method may be applied to a dichotomous division, but is perfectly suitable also for a global division into any number of clusters. An informal indicator of the ""best number"" of clusters is suggested. It is a""variance ratio criterion"" giving some insight into the structure of the points. The method is illustrated by three examples, one of which is original. The results obtained by the dendrite method are compared with those obtained by using the agglomerative method or Ward (1963) and the divisive method of Edwards and Cavalli-Sforza (1965). Keywords: numerical taxonomy cluster analysis minimum variance (WGSS) criterion for optimal grouping approximate grouping procedure shortest dendrite = minimum spanning tree variance ratio criterion for best number of groups","['https://openalex.org/W3202268800', 'https://openalex.org/W1270205671', 'https://openalex.org/W2140101900', 'https://openalex.org/W2061874589', 'https://openalex.org/W1999597013', 'https://openalex.org/W2612166593', 'https://openalex.org/W1992724796', 'https://openalex.org/W2016381774', 'https://openalex.org/W108464071', 'https://openalex.org/W1991638784', 'https://openalex.org/W47803021', 'https://openalex.org/W2026882380']",1974-01-01
https://openalex.org/W2726515241,https://doi.org/10.21437/interspeech.2017-950,VoxCeleb: A Large-Scale Speaker Identification Dataset,"Most existing datasets for speaker identification contain samples obtained under quite constrained conditions, and are usually hand-annotated, hence limited in size. The goal of this paper is to generate a large scale text-independent speaker identification dataset collected 'in the wild'. We make two contributions. First, we propose a fully automated pipeline based on computer vision techniques to create the dataset from open-source media. Our pipeline involves obtaining videos from YouTube; performing active speaker verification using a two-stream synchronization Convolutional Neural Network (CNN), and confirming the identity of the speaker using CNN based facial recognition. We use this pipeline to curate VoxCeleb which contains hundreds of thousands of 'real world' utterances for over 1,000 celebrities. Our second contribution is to apply and compare various state of the art speaker identification techniques on our dataset to establish baseline performance. We show that a CNN based architecture obtains the best performance for both identification and verification.","['https://openalex.org/W2330149154', 'https://openalex.org/W2526050071', 'https://openalex.org/W2949117887', 'https://openalex.org/W2194775991']",2017-08-16
https://openalex.org/W4306672449,https://doi.org/10.48550/arxiv.2112.12522,Multi-Variant Consistency based Self-supervised Learning for Robust Automatic Speech Recognition,"Automatic speech recognition (ASR) has shown rapid advances in recent years but still degrades significantly in far-field and noisy environments. The recent development of self-supervised learning (SSL) technology can improve the ASR performance by pre-training the model with additional unlabeled speech and the SSL pre-trained model has achieved the state-of-the-art result on several speech benchmarks. Nevertheless, most of the previous SSL methods ignore the influence of the background noise or reverberation, which is crucial to deploying ASR systems in real-world speech applications. This study addresses the robust ASR by introducing a multi-variant consistency (MVC) based SSL method that adapts to different environments. The MVC-SSL is a robust SSL pre-training method designed for noisy and distant-talking speech in real-world applications. Compared to the previous SSL method, the MVC-SSL can calculate the contrastive loss among audios from different acoustic conditions or channels and can learn invariant representations with the change in the environment or the recording equipment. We also explore different SSL training pipelines to balance the noisy distant-talking speech and extra high resource clean speech. We evaluate the proposed method on the commercially-motivated dataset, CHiME-4, and the meeting dataset, AMI. With the help of the MVC-SSL and appropriate training pipeline, we can achieve up to 30% relative word error rate reductions over the baseline wav2vec2.0, one of the most successful SSL methods for ASR.",[],2021-12-23
https://openalex.org/W4283026156,https://doi.org/10.48550/arxiv.2206.07682,Emergent Abilities of Large Language Models,"Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.",[],2022-06-15
https://openalex.org/W2129068307,https://doi.org/10.48550/arxiv.1412.4864,Learning with Pseudo-Ensembles,"We formalize the notion of a pseudo-ensemble, a (possibly infinite) collection of child models spawned from a parent model by perturbing it according to some noise process. E.g., dropout (Hinton et. al, 2012) in a deep neural network trains a pseudo-ensemble of child subnetworks generated by randomly masking nodes in the parent network. We present a novel regularizer based on making the behavior of a pseudo-ensemble robust with respect to the noise process generating it. In the fully-supervised setting, our regularizer matches the performance of dropout. But, unlike dropout, our regularizer naturally extends to the semi-supervised setting, where it produces state-of-the-art results. We provide a case study in which we transform the Recursive Neural Tensor Network of (Socher et. al, 2013) into a pseudo-ensemble, which significantly improves its performance on a real-world sentiment analysis benchmark.","['https://openalex.org/W2152722485', 'https://openalex.org/W1904365287', 'https://openalex.org/W2166093887', 'https://openalex.org/W2251939518', 'https://openalex.org/W2131744502', 'https://openalex.org/W2131116400', 'https://openalex.org/W2951287198', 'https://openalex.org/W2108806771', 'https://openalex.org/W2160660594', 'https://openalex.org/W2126398289', 'https://openalex.org/W2963883365', 'https://openalex.org/W2294586855', 'https://openalex.org/W1815076433', 'https://openalex.org/W1541527977', 'https://openalex.org/W3118608800', 'https://openalex.org/W2113346187', 'https://openalex.org/W2120615054', 'https://openalex.org/W2159291644', 'https://openalex.org/W2294567968', 'https://openalex.org/W2153417333', 'https://openalex.org/W2025768430', 'https://openalex.org/W2158542502']",2014-12-16
https://openalex.org/W2431080869,https://doi.org/10.48550/arxiv.1606.04586,Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning,"Effective convolutional neural networks are trained on large sets of labeled data. However, creating large labeled datasets is a very costly and time-consuming task. Semi-supervised learning uses unlabeled data to train a model with higher accuracy when there is a limited set of labeled data available. In this paper, we consider the problem of semi-supervised learning with convolutional neural networks. Techniques such as randomized data augmentation, dropout and random max-pooling provide better generalization and stability for classifiers that are trained using gradient descent. Multiple passes of an individual sample through the network might lead to different predictions due to the non-deterministic behavior of these techniques. We propose an unsupervised loss function that takes advantage of the stochastic nature of these methods and minimizes the difference between the predictions of multiple passes of a training sample through the network. We evaluate the proposed method on several benchmark datasets.","['https://openalex.org/W2097117768', 'https://openalex.org/W2134557905', 'https://openalex.org/W830076066', 'https://openalex.org/W2154579312', 'https://openalex.org/W1520997877', 'https://openalex.org/W189277179', 'https://openalex.org/W1533861849', 'https://openalex.org/W1810499140', 'https://openalex.org/W2171490498', 'https://openalex.org/W2546302380', 'https://openalex.org/W2048679005', 'https://openalex.org/W2112796928', 'https://openalex.org/W1585385982', 'https://openalex.org/W2107008379', 'https://openalex.org/W343636949', 'https://openalex.org/W2335728318', 'https://openalex.org/W2139823104', 'https://openalex.org/W3143797459', 'https://openalex.org/W2148349024', 'https://openalex.org/W2107968230', 'https://openalex.org/W2141125852', 'https://openalex.org/W2015861736', 'https://openalex.org/W2113896236', 'https://openalex.org/W2163605009', 'https://openalex.org/W3118608800', 'https://openalex.org/W2407712691', 'https://openalex.org/W1903029394', 'https://openalex.org/W2963930099', 'https://openalex.org/W2963542991', 'https://openalex.org/W219040644', 'https://openalex.org/W2198618282', 'https://openalex.org/W1904365287', 'https://openalex.org/W2136504847', 'https://openalex.org/W2137054688', 'https://openalex.org/W2253535400', 'https://openalex.org/W1479807131', 'https://openalex.org/W1630959083']",2016-06-14
https://openalex.org/W4375869189,https://doi.org/10.1109/icassp49357.2023.10095373,Robust Data2VEC: Noise-Robust Speech Representation Learning for ASR by Combining Regression and Improved Contrastive Learning,"Self-supervised pre-training methods based on contrastive learning or regression tasks can utilize more unlabeled data to improve the performance of automatic speech recognition (ASR). However, the robustness impact of combining the two pre-training tasks and constructing different negative samples for contrastive learning still remains unclear. In this paper, we propose a noise-robust data2vec for self-supervised speech representation learning by jointly optimizing the contrastive learning and regression tasks in the pre-training stage. Furthermore, we present two improved methods to facilitate contrastive learning. More specifically, we first propose to construct patch-based non-semantic negative samples to boost the noise robustness of the pre-training model, which is achieved by dividing the features into patches at different sizes (i.e., so-called negative samples). Second, by analyzing the distribution of positive and negative samples, we propose to remove the easily distinguishable negative samples to improve the discriminative capacity for pre-training models. Experimental results on the CHiME-4 dataset show that our method is able to improve the performance of the pre-trained model in noisy scenarios. We find that joint training of the contrastive learning and regression tasks can avoid the model collapse to some extent compared to only training the regression task.","['https://openalex.org/W6733814495', 'https://openalex.org/W3197580070', 'https://openalex.org/W6839558599', 'https://openalex.org/W3015213852', 'https://openalex.org/W4286685063', 'https://openalex.org/W6766224279', 'https://openalex.org/W4375869127', 'https://openalex.org/W6811201773', 'https://openalex.org/W6809431739', 'https://openalex.org/W3032514799', 'https://openalex.org/W3209059054', 'https://openalex.org/W6780218876', 'https://openalex.org/W4221140371', 'https://openalex.org/W3198771897', 'https://openalex.org/W3209376089', 'https://openalex.org/W3205533980', 'https://openalex.org/W6797016505', 'https://openalex.org/W6803254773', 'https://openalex.org/W6783961830', 'https://openalex.org/W6783990618', 'https://openalex.org/W6780191644', 'https://openalex.org/W6631190155', 'https://openalex.org/W6688816777', 'https://openalex.org/W2526425061', 'https://openalex.org/W3206531472', 'https://openalex.org/W3196965931', 'https://openalex.org/W4221156109', 'https://openalex.org/W6802465204', 'https://openalex.org/W3209984917', 'https://openalex.org/W6845338303', 'https://openalex.org/W6810007534', 'https://openalex.org/W4221145109', 'https://openalex.org/W3090114880', 'https://openalex.org/W4385822727', 'https://openalex.org/W4300980246', 'https://openalex.org/W4301372783', 'https://openalex.org/W4281807971', 'https://openalex.org/W3205644108', 'https://openalex.org/W3170554424', 'https://openalex.org/W1522301498', 'https://openalex.org/W4225699246', 'https://openalex.org/W3102363610', 'https://openalex.org/W2963399332', 'https://openalex.org/W3211582859', 'https://openalex.org/W3036601975', 'https://openalex.org/W2219249508', 'https://openalex.org/W2953070460']",2023-05-05
https://openalex.org/W4375869040,https://doi.org/10.1109/icassp49357.2023.10094726,Data2vec-Aqc: Search for the Right Teaching Assistant in the Teacher-Student Training Setup,"In this paper, we propose a new Self-Supervised Learning (SSL) algorithm called data2vec-aqc, for speech representation learning from unlabeled speech data. Our goal is to improve SSL for speech in domains where both unlabeled and labeled data are limited. Building on the recently introduced data2vec [1], we introduce additional modules to the data2vec framework that leverage the benefit of data augmentations, quantized representations, and clustering. The interaction between these modules helps solve the cross-contrastive loss as an additional self-supervised objective. data2vec-aqc achieves up to 14.1% and 20.9% relative WER improvement over the existing state-of-the-art data2vec system over the test-clean and test-other sets, respectively of LibriSpeech, without the use of any language model (LM). Our proposed model also achieves up to 17.8% relative WER gains over the baseline data2vec when fine-tuned on a subset of the Switchboard dataset. Code: https://github.com/Speech-Lab-IITM/data2vec-aqc.","['https://openalex.org/W3035524453', 'https://openalex.org/W3035060554', 'https://openalex.org/W6656414902', 'https://openalex.org/W2842511635', 'https://openalex.org/W6784426525', 'https://openalex.org/W6688816777', 'https://openalex.org/W3197580070', 'https://openalex.org/W6799416569', 'https://openalex.org/W6845715526', 'https://openalex.org/W6784363238', 'https://openalex.org/W6780218876', 'https://openalex.org/W6810007534', 'https://openalex.org/W1494198834', 'https://openalex.org/W6774314701', 'https://openalex.org/W6684440257', 'https://openalex.org/W2933138175', 'https://openalex.org/W2407080277', 'https://openalex.org/W6810185941', 'https://openalex.org/W4297841844', 'https://openalex.org/W3209984917', 'https://openalex.org/W3209059054', 'https://openalex.org/W3198771897', 'https://openalex.org/W6798952882', 'https://openalex.org/W2166637769', 'https://openalex.org/W4297808394', 'https://openalex.org/W4221145109', 'https://openalex.org/W3187822143', 'https://openalex.org/W4319862416', 'https://openalex.org/W3094225009', 'https://openalex.org/W3036601975', 'https://openalex.org/W4221154554', 'https://openalex.org/W2219249508', 'https://openalex.org/W2024490156', 'https://openalex.org/W3198608154', 'https://openalex.org/W3185459701', 'https://openalex.org/W3005680577']",2023-05-05
https://openalex.org/W2889519245,https://doi.org/10.21437/interspeech.2018-1158,Self-Attentive Speaker Embeddings for Text-Independent Speaker Verification,"This paper introduces a new method to extract speaker embeddings from a deep neural network (DNN) for text-independent speaker verification. Usually, speaker embeddings are extracted from a speaker-classification DNN that averages the hidden vectors over the frames of a speaker; the hidden vectors produced from all the frames are assumed to be equally important. We relax this assumption and compute the speaker embedding as a weighted average of a speaker's frame-level hidden vectors, and their weights are automatically determined by a self-attention mechanism. The effect of multiple attention heads are also investigated to capture different aspects of a speaker's input speech. Finally, a PLDA classifier is used to compare pairs of embeddings. The proposed self-attentive speaker embedding system is compared with a strong DNN embedding baseline on NIST SRE 2016. We find that the self-attentive embeddings achieve superior performance. Moreover, the improvement produced by the self-attentive speaker embeddings is consistent with both short and long testing utterances. © 2018 International Speech Communication Association. All rights reserved.",[],2018-08-28
https://openalex.org/W4240826525,https://doi.org/10.4324/9781351252225,The Art of Mixing,"David Gibson uses 3D visual representations of sounds in a mix as a tool to explain the dynamics that can be created in a mix. This book provides an in-depth exploration into the aesthetics of what makes a great mix. Gibson's unique approach explains how to map sounds to visuals in order to create a visual framework that can be used to analyze what is going on in any mix. Once you have the framework down, Gibson then uses it to explain the traditions that have be developed over time by great recording engineers for different styles of music and songs. You will come to understand everything that can be done in a mix to create dynamics that affect people in really deep ways. Once you understand what engineers are doing to create the great mixes they do, you can then use this framework to develop your own values as to what you feel is a good mix. Once you have a perspective on what all can be done, you have the power to be truly creative on your own – to create whole new mixing possibilities. It is all about creating art out of technology. This book goes beyond explaining what the equipment does – it explains what to do with the equipment to make the best possible mixes.",[],2019-01-10
https://openalex.org/W2460742184,https://doi.org/10.1109/icassp.2017.7952154,Permutation invariant training of deep models for speaker-independent multi-talker speech separation,"We propose a novel deep learning training criterion, named permutation invariant training (PIT), for speaker independent multi-talker speech separation, commonly known as the cocktail-party problem. Different from the multi-class regression technique and the deep clustering (DPCL) technique, our novel approach minimizes the separation error directly. This strategy effectively solves the long-lasting label permutation problem, that has prevented progress on deep learning based techniques for speech separation. We evaluated PIT on the WSJ0 and Danish mixed-speech separation tasks and found that it compares favorably to non-negative matrix factorization (NMF), computational auditory scene analysis (CASA), and DPCL and generalizes well over unseen speakers and languages. Since PIT is simple to implement and can be easily integrated and combined with other advanced techniques, we believe improvements built upon PIT can eventually solve the cocktail-party problem.","['https://openalex.org/W1748744376', 'https://openalex.org/W6607486085', 'https://openalex.org/W2124149378', 'https://openalex.org/W6682181234', 'https://openalex.org/W6603255414', 'https://openalex.org/W2116879786', 'https://openalex.org/W1528056001', 'https://openalex.org/W2069681747', 'https://openalex.org/W2181607856', 'https://openalex.org/W2160815625', 'https://openalex.org/W2394932179', 'https://openalex.org/W2127851351', 'https://openalex.org/W1991139021', 'https://openalex.org/W2144763279', 'https://openalex.org/W2060822897', 'https://openalex.org/W2147768505', 'https://openalex.org/W6608710415', 'https://openalex.org/W2078528584', 'https://openalex.org/W1790748249', 'https://openalex.org/W1897240248', 'https://openalex.org/W2962715207', 'https://openalex.org/W2221409856', 'https://openalex.org/W2149648698', 'https://openalex.org/W185399533', 'https://openalex.org/W1550027367', 'https://openalex.org/W2315268655', 'https://openalex.org/W80444264', 'https://openalex.org/W2147174722', 'https://openalex.org/W217970951', 'https://openalex.org/W3124794156']",2017-03-01
https://openalex.org/W4298310324,https://doi.org/10.5281/zenodo.1117371,MUSDB18 - a corpus for music separation,"The sigsep musdb18 data set consists of a total of 150 full-track songs of different styles and includes both the stereo mixtures and the original sources, divided between a training subset and a test subset. Its purpose is to serve as a reference database for the design and the evaluation of source separation algorithms. The objective of such signal processing methods is to estimate one or more sources from a set of mixtures, e.g. for karaoke applications. It has been used as the official dataset in the professionally-produced music recordings task for SiSEC 2018, which is the international campaign for the evaluation of source separation algorithms. <em>musdb18</em> contains two folders, a folder with a training set: “train”, composed of 100 songs, and a folder with a test set: “test”, composed of 50 songs. Supervised approaches should be trained on the training set and tested on both sets. All files from the <em>musdb18</em> dataset are encoded in the Native Instruments stems format (.mp4). It is a multitrack format composed of 5 stereo streams, each one encoded in AAC @256kbps. These signals correspond to: 0 - The mixture, 1 - The drums, 2 - The bass, 3 - The rest of the accompaniment, 4 - The vocals. For each file, the mixture correspond to the sum of all the signals. All signals are stereophonic and encoded at 44.1kHz. As the <em>MUSDB18</em> is encoded as STEMS, it relies on ffmpeg to read the multi-stream files. We provide a python wrapper called stempeg that allows to easily parse the dataset and decode the stem tracks on-the-fly. <strong>License</strong> MUSDB18 is provided for educational purposes only and the material contained in them should not be used for any commercial purpose without the express permission of the copyright holders: 100 tracks were derived from The ‘Mixing Secrets’ Free Multitrack Download Library. Please refer to this original resource for any question regarding your rights on your use of the DSD100 data. 46 tracks are taken from the MedleyDB licensed under Creative Commons (BY-NC-SA 4.0). 2 tracks were kindly provided by Native Instruments originally part of their stems pack. 2 tracks a from from the Canadian rock band The Easton Ellises as part of the heise stems remix competition, licensed under Creative Commons (BY-NC-SA 3.0). <strong>References</strong> If you use the MUSDB dataset for your research - Cite the MUSDB18 Dataset <pre><code>@misc{MUSDB18, author = {Rafii, Zafar and Liutkus, Antoine and Fabian-Robert St{\""o}ter and Mimilakis, Stylianos Ioannis and Bittner, Rachel}, title = {The {MUSDB18} corpus for music separation}, month = dec, year = 2017, doi = {10.5281/zenodo.1117372}, url = {https://doi.org/10.5281/zenodo.1117372} } </code></pre> If compare your results with SiSEC 2018 Participants - Cite the SiSEC 2018 LVA/ICA Paper <pre><code>@inproceedings{SiSEC18, author=""St{\""o}ter, Fabian-Robert and Liutkus, Antoine and Ito, Nobutaka"", title=""The 2018 Signal Separation Evaluation Campaign"", booktitle=""Latent Variable Analysis and Signal Separation: 14th International Conference, LVA/ICA 2018, Surrey, UK"", year=""2018"", pages=""293--305"" }</code></pre>",[],2017-12-17
https://openalex.org/W4380136719,https://doi.org/10.48550/arxiv.2306.05284,Simple and Controllable Music Generation,"We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, both mono and stereo, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/facebookresearch/audiocraft",[],2023-06-08
https://openalex.org/W4319989813,https://doi.org/10.48550/arxiv.2302.03917,Noise2Music: Text-conditioned Music Generation with Diffusion Models,"We introduce Noise2Music, where a series of diffusion models is trained to generate high-quality 30-second music clips from text prompts. Two types of diffusion models, a generator model, which generates an intermediate representation conditioned on text, and a cascader model, which generates high-fidelity audio conditioned on the intermediate representation and possibly the text, are trained and utilized in succession to generate high-fidelity music. We explore two options for the intermediate representation, one using a spectrogram and the other using audio with lower fidelity. We find that the generated audio is not only able to faithfully reflect key elements of the text prompt such as genre, tempo, instruments, mood, and era, but goes beyond to ground fine-grained semantics of the prompt. Pretrained large language models play a key role in this story -- they are used to generate paired text for the audio of the training set and to extract embeddings of the text prompts ingested by the diffusion models. Generated examples: https://google-research.github.io/noise2music",[],2023-02-08
https://openalex.org/W4285528093,https://doi.org/10.23919/dafx51585.2021,2021 24th International Conference on Digital Audio Effects (DAFx),"In the development of algorithms for sound source detection, identification and localization, having the possibility to generate datasets in a flexible and fast way is of utmost importance.However, most of the available acoustic simulators used for this purpose target indoor applications, and their usefulness is limited when it comes to outdoor environments such as that of a road, involving fast moving sources and long distances travelled by the sound waves.In this paper we present an acoustic propagation simulator specifically designed for road scenarios.In particular, the proposed Python software package enables to simulate the observed sound resulting from a source moving on an arbitrary trajectory relative to the observer, exploiting variable length delay lines to implement sound propagation and Doppler effect.An acoustic model of the road reflection and air absorption properties has been designed and implemented using digital FIR filters.The architecture of the proposed software is flexible and open to extensions, allowing the package to kick-start the implementation of further outdoor acoustic simulation scenarios.","['https://openalex.org/W2279780657', 'https://openalex.org/W2623895596', 'https://openalex.org/W1775883793', 'https://openalex.org/W2593116425', 'https://openalex.org/W2889583142', 'https://openalex.org/W2023297907', 'https://openalex.org/W2136084348', 'https://openalex.org/W326883504', 'https://openalex.org/W1977422919', 'https://openalex.org/W2763188033', 'https://openalex.org/W2900160352', 'https://openalex.org/W3096664821', 'https://openalex.org/W4242032411', 'https://openalex.org/W1559663704', 'https://openalex.org/W149781524', 'https://openalex.org/W2775320802', 'https://openalex.org/W2910912874', 'https://openalex.org/W2998774371', 'https://openalex.org/W1982771372', 'https://openalex.org/W640091484']",2021-01-01
https://openalex.org/W4391021739,https://doi.org/10.1109/asru57964.2023.10389791,Transduce and Speak: Neural Transducer for Text-To-Speech with Semantic Token Prediction,"We introduce a text-to-speech (TTS) framework based on a neural transducer. We use discretized semantic tokens acquired from wav2vec 2.0 embeddings, which makes it easy to adopt a neural transducer for the TTS framework enjoying its monotonic alignment constraints. The proposed model first generates aligned semantic tokens using the neural transducer, then synthesizes a speech sample from the semantic tokens using a non-autoregressive (NAR) speech generator. This decoupled framework alleviates the training complexity of TTS and allows each stage to focus on 1) linguistic and alignment modeling and 2) fine-grained acoustic modeling, respectively. Experimental results on the zero-shot adaptive TTS show that the proposed model exceeds the baselines in speech quality and speaker similarity via objective and subjective measures. We also investigate the inference speed and prosody controllability of our proposed model, showing the potential of the neural transducer for TTS frameworks.","['https://openalex.org/W2964243274', 'https://openalex.org/W2903739847', 'https://openalex.org/W4390075359', 'https://openalex.org/W6778823374', 'https://openalex.org/W6796464841', 'https://openalex.org/W6795261426', 'https://openalex.org/W2747874407', 'https://openalex.org/W6777694618', 'https://openalex.org/W6638749077', 'https://openalex.org/W6766849269', 'https://openalex.org/W6804171790', 'https://openalex.org/W6780218876', 'https://openalex.org/W4221166168', 'https://openalex.org/W4283700324', 'https://openalex.org/W3097777922', 'https://openalex.org/W3024869864', 'https://openalex.org/W2972359262', 'https://openalex.org/W3198429080', 'https://openalex.org/W6848735303', 'https://openalex.org/W6847363464', 'https://openalex.org/W3167533889', 'https://openalex.org/W3213604094', 'https://openalex.org/W3036601975', 'https://openalex.org/W1828163288', 'https://openalex.org/W2970079483', 'https://openalex.org/W4313679638', 'https://openalex.org/W3026874504', 'https://openalex.org/W4311000453']",2023-12-16
https://openalex.org/W4372267411,https://doi.org/10.1109/icassp49357.2023.10096679,Powerful and Extensible WFST Framework for Rnn-Transducer Losses,"This paper presents a framework based on Weighted Finite-State Transducers (WFST) to simplify the development of modifications for RNN-Transducer (RNN-T) loss. Existing implementations of RNN-T use CUDA-related code, which is hard to extend and debug. WFSTs are easy to construct and extend, and allow debugging through visualization. We introduce two WFST-powered RNN-T implementations: (1) ""Compose-Transducer"", based on a composition of the WFST graphs from acoustic and textual schema – computationally competitive and easy to modify; (2) ""Grid-Transducer"", which constructs the lattice directly for further computations – most compact, and computationally efficient. We illustrate the ease of extensibility through introduction of a new W-Transducer loss – the adaptation of the Connectionist Temporal Classification with Wild Cards. W-Transducer (W-RNNT) consistently outperforms the standard RNN-T in a weakly-supervised data setup with missing parts of transcriptions at the beginning and end of utterances. All RNN-T losses are implemented with the k2 framework and are available in the NeMo toolkit.","['https://openalex.org/W6839217812', 'https://openalex.org/W4283700324', 'https://openalex.org/W3095697114', 'https://openalex.org/W3016234571', 'https://openalex.org/W1494198834', 'https://openalex.org/W4297841396', 'https://openalex.org/W4296069356', 'https://openalex.org/W2127141656', 'https://openalex.org/W6638749077', 'https://openalex.org/W2245493112', 'https://openalex.org/W3094979069', 'https://openalex.org/W2939297570', 'https://openalex.org/W2963211739', 'https://openalex.org/W3202184514', 'https://openalex.org/W6631362777', 'https://openalex.org/W6767671539', 'https://openalex.org/W3162833755', 'https://openalex.org/W6783314596', 'https://openalex.org/W3097777922', 'https://openalex.org/W6809993426', 'https://openalex.org/W6780226713', 'https://openalex.org/W3198643121', 'https://openalex.org/W3149509723', 'https://openalex.org/W3197956343', 'https://openalex.org/W3008174054', 'https://openalex.org/W2746192915', 'https://openalex.org/W3163907627', 'https://openalex.org/W3211040052', 'https://openalex.org/W1828163288', 'https://openalex.org/W4220743925', 'https://openalex.org/W2974231335', 'https://openalex.org/W4281952183', 'https://openalex.org/W4287647128', 'https://openalex.org/W1524333225']",2023-05-05
https://openalex.org/W4389519587,https://doi.org/10.18653/v1/2023.emnlp-demo.49,Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding,"We present Video-LLaMA, a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual & audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual & audio encoders with LLM’s embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.","['https://openalex.org/W4361866031', 'https://openalex.org/W4323717348', 'https://openalex.org/W4375869762', 'https://openalex.org/W4366330503', 'https://openalex.org/W4389524372', 'https://openalex.org/W4378711593', 'https://openalex.org/W4383987498', 'https://openalex.org/W4224308101', 'https://openalex.org/W4319049530', 'https://openalex.org/W4311642023', 'https://openalex.org/W4311991106', 'https://openalex.org/W4287113019', 'https://openalex.org/W4285225959', 'https://openalex.org/W4327810158', 'https://openalex.org/W4380559123', 'https://openalex.org/W4386071707', 'https://openalex.org/W4376167553', 'https://openalex.org/W4367367040', 'https://openalex.org/W4229005866', 'https://openalex.org/W4322718246', 'https://openalex.org/W4366850747', 'https://openalex.org/W4402671548', 'https://openalex.org/W4225323055', 'https://openalex.org/W4367061106', 'https://openalex.org/W4367628410', 'https://openalex.org/W4389524500', 'https://openalex.org/W4386076522', 'https://openalex.org/W4384112212', 'https://openalex.org/W4322718191', 'https://openalex.org/W2886641317', 'https://openalex.org/W4382132560', 'https://openalex.org/W3204588463', 'https://openalex.org/W4318718936']",2023-01-01
https://openalex.org/W4372266552,https://doi.org/10.1109/icassp49357.2023.10095889,CLAP Learning Audio Concepts from Natural Language Supervision,"Mainstream machine listening models are trained to learn audio concepts under the paradigm of one class label to many recordings focusing on one task. Learning under such restricted supervision limits the flexibility of models because they require labeled audio for training and can only predict the predefined categories. Instead, we propose to learn audio concepts from natural language supervision. We call our approach Contrastive Language-Audio Pretraining (CLAP), which connects language and audio by using two encoders and a contrastive learning objective, bringing audio and text descriptions into a joint multimodal space. We trained CLAP with 128k audio and text pairs and evaluated it on 16 downstream tasks across 7 domains, such as classification of sound events, scenes, music, and speech. CLAP establishes state-of-the-art (SoTA) in Zero-Shot performance. Also, we evaluated CLAP's audio encoder in a supervised learning setup and achieved SoTA in 5 tasks. The Zero-Shot capability removes the need of training with class labeled audio, enables flexible class prediction at inference time, and generalizes well in multiple downstream tasks. Code is available at: https://github.com/microsoft/CLAP.","['https://openalex.org/W6802510933', 'https://openalex.org/W3196496149', 'https://openalex.org/W4205633160', 'https://openalex.org/W3049446265', 'https://openalex.org/W6809947431', 'https://openalex.org/W2938440247', 'https://openalex.org/W2046972719', 'https://openalex.org/W4205689591', 'https://openalex.org/W4225299287', 'https://openalex.org/W3015591594', 'https://openalex.org/W3138521398', 'https://openalex.org/W2979826702', 'https://openalex.org/W3213454282', 'https://openalex.org/W3162331882', 'https://openalex.org/W3094550259', 'https://openalex.org/W2038484192', 'https://openalex.org/W4284898017', 'https://openalex.org/W6803872405', 'https://openalex.org/W3176445421', 'https://openalex.org/W3209984917', 'https://openalex.org/W3204696009', 'https://openalex.org/W6791353385', 'https://openalex.org/W6780218876', 'https://openalex.org/W4221150524', 'https://openalex.org/W3215626407', 'https://openalex.org/W3205475937', 'https://openalex.org/W3166396011', 'https://openalex.org/W3036601975']",2023-05-05
https://openalex.org/W3205550549,https://doi.org/10.1109/icassp43922.2022.9746490,Conformer-Based Self-Supervised Learning For Non-Speech Audio Tasks,"Representation learning from unlabeled data has been of major interest in artificial intelligence research. While self-supervised speech representation learning has been popular in the speech research community, very few works have comprehensively analyzed audio representation learning for non-speech audio tasks. In this paper, we propose a self-supervised audio representation learning method and apply it to a variety of downstream non-speech audio tasks. We combine the well-known wav2vec 2.0 framework, which has shown success in self-supervised learning for speech tasks, with parameter-efficient conformer architectures. Our self-supervised pre-training can reduce the need for labeled data by two-thirds. On the AudioSet benchmark, we achieve a mean average precision (mAP) score of 0.415, which is a new state-of-the-art on this dataset through audio-only self-supervised learning. Our fine-tuned conformers also surpass or match the performance of previous systems pre-trained in a supervised way on several downstream tasks. We further discuss the important design considerations for both pre-training and fine-tuning.","['https://openalex.org/W3196974791', 'https://openalex.org/W6790117948', 'https://openalex.org/W6793728465', 'https://openalex.org/W6780294235', 'https://openalex.org/W6791537541', 'https://openalex.org/W6840046036', 'https://openalex.org/W3094550259', 'https://openalex.org/W6779503413', 'https://openalex.org/W3198882010', 'https://openalex.org/W6734260513', 'https://openalex.org/W2963610932', 'https://openalex.org/W3160766462', 'https://openalex.org/W6631190155', 'https://openalex.org/W2052666245', 'https://openalex.org/W2982343573', 'https://openalex.org/W6752516136', 'https://openalex.org/W6678969435', 'https://openalex.org/W6791429434', 'https://openalex.org/W6793736971', 'https://openalex.org/W3095727342', 'https://openalex.org/W6780218876', 'https://openalex.org/W6798422254', 'https://openalex.org/W3158504903', 'https://openalex.org/W6799303324', 'https://openalex.org/W3097777922', 'https://openalex.org/W6787335539', 'https://openalex.org/W6774314701', 'https://openalex.org/W6784614252', 'https://openalex.org/W6755207826', 'https://openalex.org/W6955071965', 'https://openalex.org/W6791353385', 'https://openalex.org/W3170837227', 'https://openalex.org/W6745136726', 'https://openalex.org/W2936774411', 'https://openalex.org/W2767754137', 'https://openalex.org/W3015817524', 'https://openalex.org/W3157916917', 'https://openalex.org/W3025165719', 'https://openalex.org/W2948982921', 'https://openalex.org/W3034978746', 'https://openalex.org/W3139211892', 'https://openalex.org/W3166396011', 'https://openalex.org/W2883935097', 'https://openalex.org/W4295723153', 'https://openalex.org/W3146639881', 'https://openalex.org/W3126565544', 'https://openalex.org/W1522301498', 'https://openalex.org/W3005680577', 'https://openalex.org/W3100177202', 'https://openalex.org/W3093579165', 'https://openalex.org/W3112616666', 'https://openalex.org/W3198275944', 'https://openalex.org/W2127870748', 'https://openalex.org/W2963341956', 'https://openalex.org/W3099782249', 'https://openalex.org/W2593116425', 'https://openalex.org/W3164279099', 'https://openalex.org/W3037309139', 'https://openalex.org/W3180180466', 'https://openalex.org/W2896457183', 'https://openalex.org/W2765407302', 'https://openalex.org/W3034886385', 'https://openalex.org/W2964121744', 'https://openalex.org/W3036601975', 'https://openalex.org/W4286582832', 'https://openalex.org/W3040498734', 'https://openalex.org/W3186781156', 'https://openalex.org/W2619947201', 'https://openalex.org/W3134486096', 'https://openalex.org/W3154596443']",2022-04-27
https://openalex.org/W4396877837,https://doi.org/10.1109/taslp.2024.3399607,AudioLDM 2: Learning Holistic Audio Generation With Self-Supervised Pretraining,"Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a holistic framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework utilizes a general representation of audio, called ""language of audio"" (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate other modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on the LOA of audio in our training set. The proposed framework naturally brings advantages such as reusable self-supervised pretrained latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech with three AudioLDM 2 variants demonstrate competitive performance of the AudioLDM 2 variants framework against previous approaches. Our code, pretrained model, and demo are available at <uri xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">https://audioldm.github.io/audioldm2</uri> .","['https://openalex.org/W4391020683', 'https://openalex.org/W6845479124', 'https://openalex.org/W6849109464', 'https://openalex.org/W4389524500', 'https://openalex.org/W6855691466', 'https://openalex.org/W6849105126', 'https://openalex.org/W6610228593', 'https://openalex.org/W6771763809', 'https://openalex.org/W6778823374', 'https://openalex.org/W2559726422', 'https://openalex.org/W6852824296', 'https://openalex.org/W4367359628', 'https://openalex.org/W6849416043', 'https://openalex.org/W6802017037', 'https://openalex.org/W6810007534', 'https://openalex.org/W4386071707', 'https://openalex.org/W6852871851', 'https://openalex.org/W3127705815', 'https://openalex.org/W4312933868', 'https://openalex.org/W6851775633', 'https://openalex.org/W3215615641', 'https://openalex.org/W4381786045', 'https://openalex.org/W6917585676', 'https://openalex.org/W4372348103', 'https://openalex.org/W6802805937', 'https://openalex.org/W6795261426', 'https://openalex.org/W6777694618', 'https://openalex.org/W6849635556', 'https://openalex.org/W6853096648', 'https://openalex.org/W4226033575', 'https://openalex.org/W6779823529', 'https://openalex.org/W6786375611', 'https://openalex.org/W6795288823', 'https://openalex.org/W6809885388', 'https://openalex.org/W6838639034', 'https://openalex.org/W3155072588', 'https://openalex.org/W6782760101', 'https://openalex.org/W6783182287', 'https://openalex.org/W6838844135', 'https://openalex.org/W6844305113', 'https://openalex.org/W6845281891', 'https://openalex.org/W6848482659', 'https://openalex.org/W6799642162', 'https://openalex.org/W6797095309', 'https://openalex.org/W4224931676', 'https://openalex.org/W4387969125', 'https://openalex.org/W4372266890', 'https://openalex.org/W4393157029', 'https://openalex.org/W4393161149', 'https://openalex.org/W6780218876', 'https://openalex.org/W6640963894', 'https://openalex.org/W3094502228', 'https://openalex.org/W2593116425', 'https://openalex.org/W6853393314', 'https://openalex.org/W3209059054', 'https://openalex.org/W2973049979', 'https://openalex.org/W3201143670', 'https://openalex.org/W6783867762', 'https://openalex.org/W2052666245', 'https://openalex.org/W3046747294', 'https://openalex.org/W6769915798', 'https://openalex.org/W6728610325', 'https://openalex.org/W2066334462', 'https://openalex.org/W4372260310', 'https://openalex.org/W4372260340', 'https://openalex.org/W6847076894', 'https://openalex.org/W6769627184', 'https://openalex.org/W4378602476', 'https://openalex.org/W6739901393', 'https://openalex.org/W6840815571', 'https://openalex.org/W6810940779', 'https://openalex.org/W6850843143', 'https://openalex.org/W3015371781', 'https://openalex.org/W6732646663', 'https://openalex.org/W6633499030', 'https://openalex.org/W3198694222', 'https://openalex.org/W4372259760', 'https://openalex.org/W2526050071', 'https://openalex.org/W3205475937', 'https://openalex.org/W6633724138', 'https://openalex.org/W4375869413', 'https://openalex.org/W6783713337', 'https://openalex.org/W6757817989', 'https://openalex.org/W6852971826', 'https://openalex.org/W6849517043', 'https://openalex.org/W4313447020', 'https://openalex.org/W4224035735', 'https://openalex.org/W4376632781', 'https://openalex.org/W4318351475', 'https://openalex.org/W2984284833', 'https://openalex.org/W4318718630', 'https://openalex.org/W4288089799', 'https://openalex.org/W1959608418', 'https://openalex.org/W1560729591', 'https://openalex.org/W3203491020', 'https://openalex.org/W4379251869', 'https://openalex.org/W4400033239', 'https://openalex.org/W4362515116', 'https://openalex.org/W2187089797', 'https://openalex.org/W4378942405', 'https://openalex.org/W4385328213', 'https://openalex.org/W4319989813', 'https://openalex.org/W4303440777']",2024-01-01
https://openalex.org/W4391021627,https://doi.org/10.1109/asru57964.2023.10389742,Joint Audio and Speech Understanding,"Humans are surrounded by audio signals that include both speech and non-speech sounds. The recognition and understanding of speech and non-speech audio events, along with a profound comprehension of the relationship between them, constitute fundamental cognitive capabilities. For the first time, we build a machine learning model, called LTU-AS, that has a conceptually similar universal audio perception and advanced reasoning ability. Specifically, by integrating Whisper [1] as a perception module and LLaMA [2] as a reasoning module, LTU-AS can simultaneously recognize and jointly understand spoken text, speech paralinguistics, and non-speech audio events - almost everything perceivable from audio signals.","['https://openalex.org/W6847363464', 'https://openalex.org/W6850625674', 'https://openalex.org/W4385807453', 'https://openalex.org/W6778883912', 'https://openalex.org/W6853249747', 'https://openalex.org/W3196974791', 'https://openalex.org/W4375869243', 'https://openalex.org/W3095738461', 'https://openalex.org/W4224932123', 'https://openalex.org/W6809947431', 'https://openalex.org/W4297841687', 'https://openalex.org/W6851847159', 'https://openalex.org/W6850477478', 'https://openalex.org/W4389524500', 'https://openalex.org/W4390874621', 'https://openalex.org/W4385823034', 'https://openalex.org/W6739901393', 'https://openalex.org/W6677258307', 'https://openalex.org/W6796581206', 'https://openalex.org/W2963096510', 'https://openalex.org/W6768028577', 'https://openalex.org/W2146334809', 'https://openalex.org/W2883409523', 'https://openalex.org/W2808631503', 'https://openalex.org/W4210267911', 'https://openalex.org/W2191779130', 'https://openalex.org/W6732646663', 'https://openalex.org/W3162999565', 'https://openalex.org/W2593116425', 'https://openalex.org/W3015371781', 'https://openalex.org/W4205689591', 'https://openalex.org/W2982554818', 'https://openalex.org/W3205743929', 'https://openalex.org/W2052666245', 'https://openalex.org/W1494198834', 'https://openalex.org/W6785932716', 'https://openalex.org/W4226442948', 'https://openalex.org/W3204696009', 'https://openalex.org/W3209984917', 'https://openalex.org/W3097525302', 'https://openalex.org/W3176445421', 'https://openalex.org/W4372266552', 'https://openalex.org/W2964067969', 'https://openalex.org/W4311000453', 'https://openalex.org/W4292779060', 'https://openalex.org/W4377130946', 'https://openalex.org/W4300957348', 'https://openalex.org/W4385245566', 'https://openalex.org/W4221150524', 'https://openalex.org/W3168867926', 'https://openalex.org/W4322718191', 'https://openalex.org/W2973049837', 'https://openalex.org/W4322825254', 'https://openalex.org/W4393178509']",2023-12-16
https://openalex.org/W4390872297,https://doi.org/10.1109/iccv51070.2023.00387,Scalable Diffusion Models with Transformers,"We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops—through increased transformer depth/width or increased number of input tokens—consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512×512 and 256×256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.","['https://openalex.org/W6755312952', 'https://openalex.org/W6778883912', 'https://openalex.org/W4313021454', 'https://openalex.org/W6796289742', 'https://openalex.org/W6779879114', 'https://openalex.org/W6761628794', 'https://openalex.org/W6755207826', 'https://openalex.org/W6795288823', 'https://openalex.org/W6784333009', 'https://openalex.org/W3180355996', 'https://openalex.org/W6739622702', 'https://openalex.org/W4312388283', 'https://openalex.org/W2194775991', 'https://openalex.org/W6755977528', 'https://openalex.org/W6784913597', 'https://openalex.org/W6765779288', 'https://openalex.org/W6779823529', 'https://openalex.org/W6797359156', 'https://openalex.org/W6840815571', 'https://openalex.org/W6630178513', 'https://openalex.org/W2963073614', 'https://openalex.org/W6847386351', 'https://openalex.org/W6804244202', 'https://openalex.org/W6772383348', 'https://openalex.org/W6838452192', 'https://openalex.org/W2962770929', 'https://openalex.org/W6631190155', 'https://openalex.org/W6640963894', 'https://openalex.org/W2618530766', 'https://openalex.org/W6761519460', 'https://openalex.org/W6757817989', 'https://openalex.org/W6791436925', 'https://openalex.org/W6810940779', 'https://openalex.org/W6788990321', 'https://openalex.org/W4312694728', 'https://openalex.org/W6763509872', 'https://openalex.org/W6843572181', 'https://openalex.org/W2760103357', 'https://openalex.org/W6791353385', 'https://openalex.org/W2981563141', 'https://openalex.org/W3034429256', 'https://openalex.org/W6809885388', 'https://openalex.org/W6790978476', 'https://openalex.org/W4312933868', 'https://openalex.org/W1901129140', 'https://openalex.org/W6838639034', 'https://openalex.org/W6718379498', 'https://openalex.org/W6732492507', 'https://openalex.org/W6810483347', 'https://openalex.org/W6679045638', 'https://openalex.org/W6783713337', 'https://openalex.org/W6765775151', 'https://openalex.org/W6798160016', 'https://openalex.org/W2423557781', 'https://openalex.org/W2752796333', 'https://openalex.org/W6739901393', 'https://openalex.org/W6797790494', 'https://openalex.org/W6839643428', 'https://openalex.org/W3172942063']",2023-10-01
https://openalex.org/W4392931276,https://doi.org/10.1109/icassp48485.2024.10448291,Matcha-TTS: A Fast TTS Architecture with Conditional Flow Matching,"We introduce Matcha-TTS, a new encoder-decoder architecture for speedy TTS acoustic modelling, trained using optimal-transport conditional flow matching (OT-CFM). This yields an ODE-based decoder capable of high output quality in fewer synthesis steps than models trained using score matching. Careful design choices additionally ensure each synthesis step is fast to run. The method is probabilistic, non-autoregressive, and learns to speak from scratch without external alignments. Compared to strong pre-trained baseline models, the Matcha-TTS system has the smallest memory footprint, rivals the speed of the fastest model on long utterances, and attains the highest mean opinion score in a listening test.","['https://openalex.org/W6765775151', 'https://openalex.org/W6795288823', 'https://openalex.org/W4312933868', 'https://openalex.org/W4377010269', 'https://openalex.org/W4385993915', 'https://openalex.org/W6782760101', 'https://openalex.org/W3198769980', 'https://openalex.org/W6795261426', 'https://openalex.org/W3198213150', 'https://openalex.org/W6783182287', 'https://openalex.org/W6786375611', 'https://openalex.org/W6844938924', 'https://openalex.org/W6752307458', 'https://openalex.org/W6846539466', 'https://openalex.org/W4296069326', 'https://openalex.org/W6852421699', 'https://openalex.org/W6763832098', 'https://openalex.org/W6778823374', 'https://openalex.org/W6800569930', 'https://openalex.org/W6777694618', 'https://openalex.org/W2963925437', 'https://openalex.org/W3196718914', 'https://openalex.org/W4388979610', 'https://openalex.org/W3175475869', 'https://openalex.org/W4385822499', 'https://openalex.org/W4304099317', 'https://openalex.org/W2972569067', 'https://openalex.org/W3198332267', 'https://openalex.org/W6838843145', 'https://openalex.org/W6843731886', 'https://openalex.org/W4387968164', 'https://openalex.org/W6853888607', 'https://openalex.org/W4200300291', 'https://openalex.org/W6783867762', 'https://openalex.org/W2963300588', 'https://openalex.org/W6847363464', 'https://openalex.org/W3196568361', 'https://openalex.org/W4395961422', 'https://openalex.org/W4385822565', 'https://openalex.org/W4385993869', 'https://openalex.org/W2973177710', 'https://openalex.org/W4376632512', 'https://openalex.org/W4382603054']",2024-03-18
https://openalex.org/W4393178509,https://doi.org/10.1609/aaai.v38i21.30570,"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head","Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving 16 AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Code can be found in https://github.com/AIGC-Audio/AudioGPT","['https://openalex.org/W3158405307', 'https://openalex.org/W3169320628', 'https://openalex.org/W3205398360', 'https://openalex.org/W3158762648', 'https://openalex.org/W2952218014', 'https://openalex.org/W6792340124', 'https://openalex.org/W3208601549', 'https://openalex.org/W4297841486', 'https://openalex.org/W3207340675', 'https://openalex.org/W3206857696', 'https://openalex.org/W4226419874', 'https://openalex.org/W4287184558', 'https://openalex.org/W4318718996', 'https://openalex.org/W4224871700', 'https://openalex.org/W4323717348', 'https://openalex.org/W4372271367', 'https://openalex.org/W4285345683', 'https://openalex.org/W4318906029', 'https://openalex.org/W4292779060', 'https://openalex.org/W3033411150', 'https://openalex.org/W4303519914', 'https://openalex.org/W4311000453', 'https://openalex.org/W4280542470', 'https://openalex.org/W4361866031', 'https://openalex.org/W3172862365', 'https://openalex.org/W4226278401', 'https://openalex.org/W4367359628', 'https://openalex.org/W3138953166', 'https://openalex.org/W4298017177', 'https://openalex.org/W4288089799', 'https://openalex.org/W2896457183']",2024-03-24
https://openalex.org/W3015371781,https://doi.org/10.1109/icassp40776.2020.9053174,Vggsound: A Large-Scale Audio-Visual Dataset,"Our goal is to collect a large-scale audio-visual dataset with low label noise from videos `in the wild' using computer vision techniques. The resulting dataset can be used for training and evaluating audio recognition models. We make three contributions. First, we propose a scalable pipeline based on computer vision techniques to create an audio dataset from open-source media. Our pipeline involves obtaining videos from YouTube; using image classification algorithms to localize audio-visual correspondence; and filtering out ambient noise using audio verification. Second, we use this pipeline to curate the VGGSound dataset consisting of more than 200k videos for 300 audio classes. Third, we investigate various Convolutional Neural Network (CNN) architectures and aggregation approaches to establish audio recognition baselines for our new dataset. Compared to existing audio datasets, VGGSound ensures audio-visual correspondence and is collected under unconstrained conditions. Code and the dataset are available at http://www.robots.ox.ac.uk/~vgg/data/vggsound/.","['https://openalex.org/W6682691769', 'https://openalex.org/W6754337694', 'https://openalex.org/W2963723765', 'https://openalex.org/W2593116425', 'https://openalex.org/W2726515241', 'https://openalex.org/W2981087920', 'https://openalex.org/W2808631503', 'https://openalex.org/W2179042386', 'https://openalex.org/W2916104401', 'https://openalex.org/W2105582566', 'https://openalex.org/W2964345931', 'https://openalex.org/W6750599028', 'https://openalex.org/W2341412280', 'https://openalex.org/W2194775991', 'https://openalex.org/W2342547278', 'https://openalex.org/W6637373629', 'https://openalex.org/W2038484192', 'https://openalex.org/W2963451564', 'https://openalex.org/W2526050071', 'https://openalex.org/W2770119437', 'https://openalex.org/W821549425', 'https://openalex.org/W2108598243', 'https://openalex.org/W2566935005', 'https://openalex.org/W2031489346', 'https://openalex.org/W2938440247', 'https://openalex.org/W2115447976', 'https://openalex.org/W2964109005', 'https://openalex.org/W6696804643', 'https://openalex.org/W2153220212', 'https://openalex.org/W6682320352', 'https://openalex.org/W6600006618', 'https://openalex.org/W2152305730', 'https://openalex.org/W2294701319', 'https://openalex.org/W2962865004', 'https://openalex.org/W1686810756', 'https://openalex.org/W2962835968', 'https://openalex.org/W4294170691', 'https://openalex.org/W2963543871', 'https://openalex.org/W2620629206', 'https://openalex.org/W2153579005', 'https://openalex.org/W2887051120', 'https://openalex.org/W183860', 'https://openalex.org/W2619697695']",2020-04-09
https://openalex.org/W4392902957,https://doi.org/10.1109/icassp48485.2024.10446663,Adapting Frechet Audio Distance for Generative Music Evaluation,"The growing popularity of generative music models underlines the need for perceptually relevant, objective music quality metrics. The Frechet Audio Distance (FAD) is commonly used for this purpose even though its correlation with perceptual quality is understudied. We show that FAD performance may be hampered by sample size bias, poor choice of audio embeddings, or the use of biased or low-quality reference sets. We propose reducing sample size bias by extrapolating scores towards an infinite sample size. Through comparisons with MusicCaps labels and a listening test we identify audio embeddings and music reference sets that yield FAD scores well-correlated with acoustic and musical quality. Our results suggest that per-song FAD can be useful to identify outlier samples and predict perceptual quality for a range of music sets and generative models. Finally, we release a toolkit that allows adapting FAD for generative music evaluation.","['https://openalex.org/W4367359628', 'https://openalex.org/W2526050071', 'https://openalex.org/W3034664137', 'https://openalex.org/W6766320909', 'https://openalex.org/W6855434424', 'https://openalex.org/W4221165315', 'https://openalex.org/W2939574508', 'https://openalex.org/W3006926732', 'https://openalex.org/W4392903801', 'https://openalex.org/W6846849257', 'https://openalex.org/W6853393314', 'https://openalex.org/W6790489232', 'https://openalex.org/W4307323391', 'https://openalex.org/W6853515095', 'https://openalex.org/W3094550259', 'https://openalex.org/W4372266552', 'https://openalex.org/W4226343611', 'https://openalex.org/W4294534174', 'https://openalex.org/W4379251869', 'https://openalex.org/W4221152471', 'https://openalex.org/W4283382678', 'https://openalex.org/W3160506022', 'https://openalex.org/W4318752004', 'https://openalex.org/W4380136719', 'https://openalex.org/W4289106098', 'https://openalex.org/W2615063356', 'https://openalex.org/W4380551955', 'https://openalex.org/W4293575120', 'https://openalex.org/W4385473570', 'https://openalex.org/W4318351475', 'https://openalex.org/W2580221632', 'https://openalex.org/W4319452539', 'https://openalex.org/W4319989813', 'https://openalex.org/W4298310324']",2024-03-18
https://openalex.org/W3205860970,https://doi.org/10.1109/icassp43922.2022.9746427,Can Audio Captions Be Evaluated With Image Caption Metrics?,"Automated audio captioning aims at generating textual descriptions for an audio clip. To evaluate the quality of generated audio captions, previous works directly adopt image captioning metrics like SPICE and CIDEr, without justifying their suitability in this new domain, which may mislead the development of advanced models. This problem is still unstudied due to the lack of human judgment datasets on caption quality. Therefore, we first construct two evaluation benchmarks, AudioCaps-Eval and Clotho-Eval. They are established with pairwise comparison instead of absolute rating to achieve better inter-annotator agreement. Current metrics are found in poor correlation with human annotations on these datasets. To overcome their limitations, we propose a metric named FENSE, where we combine the strength of Sentence-BERT in capturing similarity, and a novel Error Detector to penalize erroneous sentences for robustness. On the newly established benchmarks, FENSE outperforms current metrics by 14-25% accuracy. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>","['https://openalex.org/W6678262379', 'https://openalex.org/W1956340063', 'https://openalex.org/W6725318829', 'https://openalex.org/W2963672599', 'https://openalex.org/W2896780650', 'https://openalex.org/W6761205521', 'https://openalex.org/W3035252911', 'https://openalex.org/W2970641574', 'https://openalex.org/W3094550259', 'https://openalex.org/W3160577380', 'https://openalex.org/W2916103538', 'https://openalex.org/W6898505805', 'https://openalex.org/W3015591594', 'https://openalex.org/W6682631176', 'https://openalex.org/W2964213897', 'https://openalex.org/W2788277448', 'https://openalex.org/W6755207826', 'https://openalex.org/W1975879668', 'https://openalex.org/W2970858040', 'https://openalex.org/W3204363391', 'https://openalex.org/W2123301721', 'https://openalex.org/W2963341956', 'https://openalex.org/W2936695845', 'https://openalex.org/W2101105183', 'https://openalex.org/W2950446064', 'https://openalex.org/W2996403597', 'https://openalex.org/W2506483933', 'https://openalex.org/W2154652894', 'https://openalex.org/W2945761034', 'https://openalex.org/W2896457183']",2022-04-27
https://openalex.org/W2133824856,https://doi.org/10.1109/tsa.2002.800560,Musical genre classification of audio signals,"Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to music information retrieval systems. In addition, automatic musical genre classification provides a framework for developing and evaluating features for any type of content-based analysis of musical signals. In this paper, the automatic classification of audio signals into an hierarchy of musical genres is explored. More specifically, three feature sets for representing timbral texture, rhythmic content and pitch content are proposed. The performance and relative importance of the proposed features is investigated by training statistical pattern recognition classifiers using real-world audio collections. Both whole file and real-time frame-based classification schemes are described. Using the proposed feature sets, classification of 61% for ten musical genres is achieved. This result is comparable to results reported for human musical genre classification.","['https://openalex.org/W2148154194', 'https://openalex.org/W2106055371', 'https://openalex.org/W2128838222', 'https://openalex.org/W6605678257', 'https://openalex.org/W2144345993', 'https://openalex.org/W2124867748', 'https://openalex.org/W2074188409', 'https://openalex.org/W2111331420', 'https://openalex.org/W6640934164', 'https://openalex.org/W2110458017', 'https://openalex.org/W2067063915', 'https://openalex.org/W2160531276', 'https://openalex.org/W2112630930', 'https://openalex.org/W2129338995', 'https://openalex.org/W6638699884', 'https://openalex.org/W6600593648', 'https://openalex.org/W2098914003', 'https://openalex.org/W2149095746', 'https://openalex.org/W6604454234', 'https://openalex.org/W2130976598', 'https://openalex.org/W2059020775', 'https://openalex.org/W97140021', 'https://openalex.org/W1497564599', 'https://openalex.org/W2154642142', 'https://openalex.org/W6779561511', 'https://openalex.org/W1861596447', 'https://openalex.org/W2166248431', 'https://openalex.org/W2062170755', 'https://openalex.org/W1954587310', 'https://openalex.org/W4285719527', 'https://openalex.org/W3034415380', 'https://openalex.org/W139898027', 'https://openalex.org/W1774234760', 'https://openalex.org/W15066456', 'https://openalex.org/W2039275978', 'https://openalex.org/W2799061466', 'https://openalex.org/W1835965142', 'https://openalex.org/W2115755118', 'https://openalex.org/W110296819', 'https://openalex.org/W4244494905', 'https://openalex.org/W1560013842']",2002-07-01
https://openalex.org/W4389519062,https://doi.org/10.18653/v1/2023.emnlp-main.444,Accented Speech Recognition With Accent-specific Codebooks,"Speech accents pose a significant challenge to state-of-the-art automatic speech recognition (ASR) systems. Degradation in performance across underrepresented accents is a severe deterrent to the inclusive adoption of ASR. In this work, we propose a novel accent adaptation approach for end-to-end ASR systems using cross-attention with a trainable set of codebooks. These learnable codebooks capture accent-specific information and are integrated within the ASR encoder layers. The model is trained on accented English speech, while the test data also contained accents which were not seen during training. On the Mozilla Common Voice multi-accented dataset, we show that our proposed approach yields significant performance gains not only on the seen English accents (up to 37% relative improvement in word error rate) but also on the unseen accents (up to 5% relative improvement in WER). Further, we illustrate benefits for a zero-shot transfer setup on the L2Artic dataset. We also compare the performance with other approaches based on accent adversarial training.","['https://openalex.org/W5457946', 'https://openalex.org/W3199443835', 'https://openalex.org/W179875071', 'https://openalex.org/W3197530164', 'https://openalex.org/W3156012458', 'https://openalex.org/W2889494795', 'https://openalex.org/W3025165719', 'https://openalex.org/W2938374794', 'https://openalex.org/W4287278065', 'https://openalex.org/W3169064633', 'https://openalex.org/W4287812705', 'https://openalex.org/W1860741867', 'https://openalex.org/W2892009249', 'https://openalex.org/W2399792130', 'https://openalex.org/W3030437843', 'https://openalex.org/W3203561692', 'https://openalex.org/W2405866807', 'https://openalex.org/W3017502571', 'https://openalex.org/W1932968309', 'https://openalex.org/W3163865502', 'https://openalex.org/W4297969478', 'https://openalex.org/W2972798094', 'https://openalex.org/W3115021520', 'https://openalex.org/W2990205979', 'https://openalex.org/W2079623482', 'https://openalex.org/W2587080466', 'https://openalex.org/W3127719526', 'https://openalex.org/W3162812479', 'https://openalex.org/W3181410678', 'https://openalex.org/W2795935804', 'https://openalex.org/W3095468613', 'https://openalex.org/W2294108103', 'https://openalex.org/W2077938266', 'https://openalex.org/W4372346845', 'https://openalex.org/W3111374309', 'https://openalex.org/W2225221668', 'https://openalex.org/W2910554046', 'https://openalex.org/W2952470929', 'https://openalex.org/W2802497966', 'https://openalex.org/W2168961642', 'https://openalex.org/W4385245566', 'https://openalex.org/W2603679025', 'https://openalex.org/W2127141656', 'https://openalex.org/W4308252168', 'https://openalex.org/W4280618630', 'https://openalex.org/W4221161761', 'https://openalex.org/W2888954148', 'https://openalex.org/W2973094925', 'https://openalex.org/W3015774380']",2023-01-01
https://openalex.org/W2125324924,https://doi.org/10.1109/icme.2002.1035731,Music type classification by spectral contrast feature,"Automatic music type classification is very helpful for the management of digital music database. In this paper, Octave-based Spectral Contrast feature is proposed to represent the spectral characteristics of a music clip. It represented the relative spectral distribution instead of average spectral envelope. Experiments showed that Octave-based Spectral Contrast feature performed well in music type classification. Another comparison experiment demonstrated that Octave-based Spectral Contrast feature has a better discrimination among different music types than Mel-Frequency Cepstral Coefficients (MFCC), which is often used in previous music type classification systems. 1.","['https://openalex.org/W2138781314', 'https://openalex.org/W2135098570', 'https://openalex.org/W1495748010', 'https://openalex.org/W2032360374', 'https://openalex.org/W6639065442', 'https://openalex.org/W1607347617', 'https://openalex.org/W1861596447']",2003-06-25
