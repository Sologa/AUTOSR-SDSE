[
  {
    "title": "Toward Zero Oracle Word Error Rate on the Switchboard Benchmark",
    "abstract": "The \"Switchboard benchmark\" is a very well-known test set in automatic speech recognition (ASR) research, establishing record-setting performance for systems that claim human-level transcription accuracy. This work highlights lesser-known practical considerations of this evaluation, demonstrating major improvements in word error rate (WER) by correcting the reference transcriptions and deviating from the official scoring methodology. In this more detailed and reproducible scheme, even commercial ASR systems can score below 5% WER and the established record for a research system is lowered to 2.3%. An alternative metric of transcript precision is proposed, which does not penalize deletions and appears to be more discriminating for human vs. machine performance. While commercial ASR systems are still below this threshold, a research system is shown to clearly surpass the accuracy of commercial human speech recognition. This work also explores using standardized scoring tools to compute oracle WER by selecting the best among a list of alternatives. A phrase alternatives representation is compared to utterance-level N-best lists and word-level data structures; using dense lattices and adding out-of-vocabulary words, this achieves an oracle WER of 0.18%.",
    "metadata": {
      "arxiv_id": "2206.06192",
      "title": "Toward Zero Oracle Word Error Rate on the Switchboard Benchmark",
      "summary": "The \"Switchboard benchmark\" is a very well-known test set in automatic speech recognition (ASR) research, establishing record-setting performance for systems that claim human-level transcription accuracy. This work highlights lesser-known practical considerations of this evaluation, demonstrating major improvements in word error rate (WER) by correcting the reference transcriptions and deviating from the official scoring methodology. In this more detailed and reproducible scheme, even commercial ASR systems can score below 5% WER and the established record for a research system is lowered to 2.3%. An alternative metric of transcript precision is proposed, which does not penalize deletions and appears to be more discriminating for human vs. machine performance. While commercial ASR systems are still below this threshold, a research system is shown to clearly surpass the accuracy of commercial human speech recognition. This work also explores using standardized scoring tools to compute oracle WER by selecting the best among a list of alternatives. A phrase alternatives representation is compared to utterance-level N-best lists and word-level data structures; using dense lattices and adding out-of-vocabulary words, this achieves an oracle WER of 0.18%.",
      "authors": [
        "Arlo Faria",
        "Adam Janin",
        "Korbinian Riedhammer",
        "Sidhi Adkoli"
      ],
      "published": "2022-06-13T14:26:40Z",
      "updated": "2022-06-27T14:44:58Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.06192v2",
      "landing_url": "https://arxiv.org/abs/2206.06192v2",
      "doi": "https://doi.org/10.48550/arXiv.2206.06192"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on oracle word error rate and scoring methods for the Switchboard ASR benchmark, not on discrete audio token representations (tokenization/quantization, SSL-based discrete tokens, or token design), so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on oracle word error rate and scoring methods for the Switchboard ASR benchmark, not on discrete audio token representations (tokenization/quantization, SSL-based discrete tokens, or token design), so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving ASR transcription accuracy and scoring metrics on the Switchboard benchmark but does not mention any discrete audio token generation process, vector quantization, or semantic-driven tokenization as required. It primarily deals with error rates and evaluation metrics rather than designing or using discrete token representations for audio or speech modeling as per the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving ASR transcription accuracy and scoring metrics on the Switchboard benchmark but does not mention any discrete audio token generation process, vector quantization, or semantic-driven tokenization as required. It primarily deals with error rates and evaluation metrics rather than designing or using discrete token representations for audio or speech modeling as per the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Streaming non-autoregressive model for any-to-many voice conversion",
    "abstract": "Voice conversion models have developed for decades, and current mainstream research focuses on non-streaming voice conversion. However, streaming voice conversion is more suitable for practical application scenarios than non-streaming voice conversion. In this paper, we propose a streaming any-to-many voice conversion based on fully non-autoregressive model, which includes a streaming transformer based acoustic model and a streaming vocoder. Streaming transformer based acoustic model is composed of a pre-trained encoder from streaming end-to-end based automatic speech recognition model and a decoder modified on FastSpeech blocks. Streaming vocoder is designed for streaming task with pseudo quadrature mirror filter bank and causal convolution. Experimental results show that the proposed method achieves significant performance both in latency and conversion quality and can be real-time on CPU and GPU.",
    "metadata": {
      "arxiv_id": "2206.07288",
      "title": "Streaming non-autoregressive model for any-to-many voice conversion",
      "summary": "Voice conversion models have developed for decades, and current mainstream research focuses on non-streaming voice conversion. However, streaming voice conversion is more suitable for practical application scenarios than non-streaming voice conversion. In this paper, we propose a streaming any-to-many voice conversion based on fully non-autoregressive model, which includes a streaming transformer based acoustic model and a streaming vocoder. Streaming transformer based acoustic model is composed of a pre-trained encoder from streaming end-to-end based automatic speech recognition model and a decoder modified on FastSpeech blocks. Streaming vocoder is designed for streaming task with pseudo quadrature mirror filter bank and causal convolution. Experimental results show that the proposed method achieves significant performance both in latency and conversion quality and can be real-time on CPU and GPU.",
      "authors": [
        "Ziyi Chen",
        "Haoran Miao",
        "Pengyuan Zhang"
      ],
      "published": "2022-06-15T04:04:14Z",
      "updated": "2022-06-15T04:04:14Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.07288v1",
      "landing_url": "https://arxiv.org/abs/2206.07288v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.07288"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on streaming non-autoregressive voice conversion and does not discuss discrete audio tokens or tokenization mechanisms (e.g., VQ/RVQ or semantic token design), so it does not meet the inclusion criteria aimed at discrete audio token representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on streaming non-autoregressive voice conversion and does not discuss discrete audio tokens or tokenization mechanisms (e.g., VQ/RVQ or semantic token design), so it does not meet the inclusion criteria aimed at discrete audio token representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a streaming non-autoregressive voice conversion model focusing on latency and conversion quality improvements but does not mention any discrete audio token generation, vector quantization, or token-based representation as defined by the inclusion criteria. It lacks details on token generation processes or the use of discrete tokens for sequence modeling, which are essential for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study presents a streaming non-autoregressive voice conversion model focusing on latency and conversion quality improvements but does not mention any discrete audio token generation, vector quantization, or token-based representation as defined by the inclusion criteria. It lacks details on token generation processes or the use of discrete tokens for sequence modeling, which are essential for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Temporal Extension of Latent Dirichlet Allocation for Unsupervised Acoustic Unit Discovery",
    "abstract": "Latent Dirichlet allocation (LDA) is widely used for unsupervised topic modelling on sets of documents. No temporal information is used in the model. However, there is often a relationship between the corresponding topics of consecutive tokens. In this paper, we present an extension to LDA that uses a Markov chain to model temporal information. We use this new model for acoustic unit discovery from speech. As input tokens, the model takes a discretised encoding of speech from a vector quantised (VQ) neural network with 512 codes. The goal is then to map these 512 VQ codes to 50 phone-like units (topics) in order to more closely resemble true phones. In contrast to the base LDA, which only considers how VQ codes co-occur within utterances (documents), the Markov chain LDA additionally captures how consecutive codes follow one another. This extension leads to an increase in cluster quality and phone segmentation results compared to the base LDA. Compared to a recent vector quantised neural network approach that also learns 50 units, the extended LDA model performs better in phone segmentation but worse in mutual information.",
    "metadata": {
      "arxiv_id": "2206.11706",
      "title": "A Temporal Extension of Latent Dirichlet Allocation for Unsupervised Acoustic Unit Discovery",
      "summary": "Latent Dirichlet allocation (LDA) is widely used for unsupervised topic modelling on sets of documents. No temporal information is used in the model. However, there is often a relationship between the corresponding topics of consecutive tokens. In this paper, we present an extension to LDA that uses a Markov chain to model temporal information. We use this new model for acoustic unit discovery from speech. As input tokens, the model takes a discretised encoding of speech from a vector quantised (VQ) neural network with 512 codes. The goal is then to map these 512 VQ codes to 50 phone-like units (topics) in order to more closely resemble true phones. In contrast to the base LDA, which only considers how VQ codes co-occur within utterances (documents), the Markov chain LDA additionally captures how consecutive codes follow one another. This extension leads to an increase in cluster quality and phone segmentation results compared to the base LDA. Compared to a recent vector quantised neural network approach that also learns 50 units, the extended LDA model performs better in phone segmentation but worse in mutual information.",
      "authors": [
        "Werner van der Merwe",
        "Herman Kamper",
        "Johan du Preez"
      ],
      "published": "2022-06-23T13:53:59Z",
      "updated": "2022-06-29T07:47:53Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.11706v2",
      "landing_url": "https://arxiv.org/abs/2206.11706v2",
      "doi": "https://doi.org/10.48550/arXiv.2206.11706"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper uses discretised speech tokens from a VQ codebook (512 codes) as input tokens and maps them to phone-like acoustic units, employing a temporal Markov-augmented LDA for acoustic unit discovery, which fits the discrete audio token/discrete representation criterion and provides methodological details beyond demos.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper uses discretised speech tokens from a VQ codebook (512 codes) as input tokens and maps them to phone-like acoustic units, employing a temporal Markov-augmented LDA for acoustic unit discovery, which fits the discrete audio token/discrete representation criterion and provides methodological details beyond demos.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents an extension of Latent Dirichlet Allocation for unsupervised acoustic unit discovery using discretized speech tokens obtained from a vector quantized neural network with 512 codes, mapping them to phone-like units. The discrete tokens are produced through vector quantization, clearly describing the token generation and application process for sequential modeling. The paper includes technical methods and comparisons, meeting the inclusion criteria of focusing on discrete audio tokens with detailed methodology. There are no indications of exclusion criteria such as lack of technical detail or traditional codec focus. Therefore, this study aligns well with the required topic and methodological transparency.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study presents an extension of Latent Dirichlet Allocation for unsupervised acoustic unit discovery using discretized speech tokens obtained from a vector quantized neural network with 512 codes, mapping them to phone-like units. The discrete tokens are produced through vector quantization, clearly describing the token generation and application process for sequential modeling. The paper includes technical methods and comparisons, meeting the inclusion criteria of focusing on discrete audio tokens with detailed methodology. There are no indications of exclusion criteria such as lack of technical detail or traditional codec focus. Therefore, this study aligns well with the required topic and methodological transparency.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Megapixel Image Generation with Step-Unrolled Denoising Autoencoders",
    "abstract": "An ongoing trend in generative modelling research has been to push sample resolutions higher whilst simultaneously reducing computational requirements for training and sampling. We aim to push this trend further via the combination of techniques - each component representing the current pinnacle of efficiency in their respective areas. These include vector-quantized GAN (VQ-GAN), a vector-quantization (VQ) model capable of high levels of lossy - but perceptually insignificant - compression; hourglass transformers, a highly scaleable self-attention model; and step-unrolled denoising autoencoders (SUNDAE), a non-autoregressive (NAR) text generative model. Unexpectedly, our method highlights weaknesses in the original formulation of hourglass transformers when applied to multidimensional data. In light of this, we propose modifications to the resampling mechanism, applicable in any task applying hierarchical transformers to multidimensional data. Additionally, we demonstrate the scalability of SUNDAE to long sequence lengths - four times longer than prior work. Our proposed framework scales to high-resolutions ($1024 \\times 1024$) and trains quickly (2-4 days). Crucially, the trained model produces diverse and realistic megapixel samples in approximately 2 seconds on a consumer-grade GPU (GTX 1080Ti). In general, the framework is flexible: supporting an arbitrary number of sampling steps, sample-wise self-stopping, self-correction capabilities, conditional generation, and a NAR formulation that allows for arbitrary inpainting masks. We obtain FID scores of 10.56 on FFHQ256 - close to the original VQ-GAN in less than half the sampling steps - and 21.85 on FFHQ1024 in only 100 sampling steps.",
    "metadata": {
      "arxiv_id": "2206.12351",
      "title": "Megapixel Image Generation with Step-Unrolled Denoising Autoencoders",
      "summary": "An ongoing trend in generative modelling research has been to push sample resolutions higher whilst simultaneously reducing computational requirements for training and sampling. We aim to push this trend further via the combination of techniques - each component representing the current pinnacle of efficiency in their respective areas. These include vector-quantized GAN (VQ-GAN), a vector-quantization (VQ) model capable of high levels of lossy - but perceptually insignificant - compression; hourglass transformers, a highly scaleable self-attention model; and step-unrolled denoising autoencoders (SUNDAE), a non-autoregressive (NAR) text generative model. Unexpectedly, our method highlights weaknesses in the original formulation of hourglass transformers when applied to multidimensional data. In light of this, we propose modifications to the resampling mechanism, applicable in any task applying hierarchical transformers to multidimensional data. Additionally, we demonstrate the scalability of SUNDAE to long sequence lengths - four times longer than prior work. Our proposed framework scales to high-resolutions ($1024 \\times 1024$) and trains quickly (2-4 days). Crucially, the trained model produces diverse and realistic megapixel samples in approximately 2 seconds on a consumer-grade GPU (GTX 1080Ti). In general, the framework is flexible: supporting an arbitrary number of sampling steps, sample-wise self-stopping, self-correction capabilities, conditional generation, and a NAR formulation that allows for arbitrary inpainting masks. We obtain FID scores of 10.56 on FFHQ256 - close to the original VQ-GAN in less than half the sampling steps - and 21.85 on FFHQ1024 in only 100 sampling steps.",
      "authors": [
        "Alex F. McKinney",
        "Chris G. Willcocks"
      ],
      "published": "2022-06-24T15:47:42Z",
      "updated": "2022-06-24T15:47:42Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.12351v1",
      "landing_url": "https://arxiv.org/abs/2206.12351v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.12351"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about megapixel image generation with VQ-GAN and SUNDAE, focusing on image data rather than discrete audio tokens, and provides no audio token design/quantization methodology, so it does not meet the Discrete Audio Tokens criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about megapixel image generation with VQ-GAN and SUNDAE, focusing on image data rather than discrete audio tokens, and provides no audio token design/quantization methodology, so it does not meet the Discrete Audio Tokens criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The work focuses on megapixel image generation using techniques like vector-quantized GANs and step-unrolled denoising autoencoders but does not address audio signals or the generation of discrete audio tokens from continuous audio as required by the inclusion criteria. The abstract mentions image generation and related models without discussing audio tokenization, quantization of audio signals, or any audio-related analysis or tasks. Hence, it does not meet the specified inclusion criteria focused on discrete audio tokens and related sequence/language modeling, and there is no evidence it should be included.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The work focuses on megapixel image generation using techniques like vector-quantized GANs and step-unrolled denoising autoencoders but does not address audio signals or the generation of discrete audio tokens from continuous audio as required by the inclusion criteria. The abstract mentions image generation and related models without discussing audio tokenization, quantization of audio signals, or any audio-related analysis or tasks. Hence, it does not meet the specified inclusion criteria focused on discrete audio tokens and related sequence/language modeling, and there is no evidence it should be included.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Attention-based conditioning methods using variable frame rate for style-robust speaker verification",
    "abstract": "We propose an approach to extract speaker embeddings that are robust to speaking style variations in text-independent speaker verification. Typically, speaker embedding extraction includes training a DNN for speaker classification and using the bottleneck features as speaker representations. Such a network has a pooling layer to transform frame-level to utterance-level features by calculating statistics over all utterance frames, with equal weighting. However, self-attentive embeddings perform weighted pooling such that the weights correspond to the importance of the frames in a speaker classification task. Entropy can capture acoustic variability due to speaking style variations. Hence, an entropy-based variable frame rate vector is proposed as an external conditioning vector for the self-attention layer to provide the network with information that can address style effects. This work explores five different approaches to conditioning. The best conditioning approach, concatenation with gating, provided statistically significant improvements over the x-vector baseline in 12/23 tasks and was the same as the baseline in 11/23 tasks when using the UCLA speaker variability database. It also significantly outperformed self-attention without conditioning in 9/23 tasks and was worse in 1/23. The method also showed significant improvements in multi-speaker scenarios of SITW.",
    "metadata": {
      "arxiv_id": "2206.13680",
      "title": "Attention-based conditioning methods using variable frame rate for style-robust speaker verification",
      "summary": "We propose an approach to extract speaker embeddings that are robust to speaking style variations in text-independent speaker verification. Typically, speaker embedding extraction includes training a DNN for speaker classification and using the bottleneck features as speaker representations. Such a network has a pooling layer to transform frame-level to utterance-level features by calculating statistics over all utterance frames, with equal weighting. However, self-attentive embeddings perform weighted pooling such that the weights correspond to the importance of the frames in a speaker classification task. Entropy can capture acoustic variability due to speaking style variations. Hence, an entropy-based variable frame rate vector is proposed as an external conditioning vector for the self-attention layer to provide the network with information that can address style effects. This work explores five different approaches to conditioning. The best conditioning approach, concatenation with gating, provided statistically significant improvements over the x-vector baseline in 12/23 tasks and was the same as the baseline in 11/23 tasks when using the UCLA speaker variability database. It also significantly outperformed self-attention without conditioning in 9/23 tasks and was worse in 1/23. The method also showed significant improvements in multi-speaker scenarios of SITW.",
      "authors": [
        "Amber Afshan",
        "Abeer Alwan"
      ],
      "published": "2022-06-28T01:14:09Z",
      "updated": "2022-06-28T01:14:09Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.13680v1",
      "landing_url": "https://arxiv.org/abs/2206.13680v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.13680"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on entropy-based conditioning for self-attention in speaker verification, not on discrete audio token design or tokenization pipelines, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on entropy-based conditioning for self-attention in speaker verification, not on discrete audio token design or tokenization pipelines, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on style-robust speaker verification using attention-based methods with variable frame rates, but it does not describe any discrete audio token design or discrete representation extraction process, nor does it address token generation, vector quantization, or semantic disentanglement as required by the inclusion criteria. It mainly discusses speaker embedding extraction and conditioning approaches without mentioning discrete token formation or usage with language/sequence models. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and focuses on a different research topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on style-robust speaker verification using attention-based methods with variable frame rates, but it does not describe any discrete audio token design or discrete representation extraction process, nor does it address token generation, vector quantization, or semantic disentanglement as required by the inclusion criteria. It mainly discusses speaker embedding extraction and conditioning approaches without mentioning discrete token formation or usage with language/sequence models. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and focuses on a different research topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Automatic Evaluation of Speaker Similarity",
    "abstract": "We introduce a new automatic evaluation method for speaker similarity assessment, that is consistent with human perceptual scores. Modern neural text-to-speech models require a vast amount of clean training data, which is why many solutions switch from single speaker models to solutions trained on examples from many different speakers. Multi-speaker models bring new possibilities, such as a faster creation of new voices, but also a new problem - speaker leakage, where the speaker identity of a synthesized example might not match those of the target speaker. Currently, the only way to discover this issue is through costly perceptual evaluations. In this work, we propose an automatic method for assessment of speaker similarity. For that purpose, we extend the recent work on speaker verification systems and evaluate how different metrics and speaker embeddings models reflect Multiple Stimuli with Hidden Reference and Anchor (MUSHRA) scores. Our experiments show that we can train a model to predict speaker similarity MUSHRA scores from speaker embeddings with 0.96 accuracy and significant correlation up to 0.78 Pearson score at the utterance level.",
    "metadata": {
      "arxiv_id": "2207.00344",
      "title": "Automatic Evaluation of Speaker Similarity",
      "summary": "We introduce a new automatic evaluation method for speaker similarity assessment, that is consistent with human perceptual scores. Modern neural text-to-speech models require a vast amount of clean training data, which is why many solutions switch from single speaker models to solutions trained on examples from many different speakers. Multi-speaker models bring new possibilities, such as a faster creation of new voices, but also a new problem - speaker leakage, where the speaker identity of a synthesized example might not match those of the target speaker. Currently, the only way to discover this issue is through costly perceptual evaluations. In this work, we propose an automatic method for assessment of speaker similarity. For that purpose, we extend the recent work on speaker verification systems and evaluate how different metrics and speaker embeddings models reflect Multiple Stimuli with Hidden Reference and Anchor (MUSHRA) scores. Our experiments show that we can train a model to predict speaker similarity MUSHRA scores from speaker embeddings with 0.96 accuracy and significant correlation up to 0.78 Pearson score at the utterance level.",
      "authors": [
        "Deja Kamil",
        "Sanchez Ariadna",
        "Roth Julian",
        "Cotescu Marius"
      ],
      "published": "2022-07-01T11:23:16Z",
      "updated": "2022-07-01T11:23:16Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.00344v1",
      "landing_url": "https://arxiv.org/abs/2207.00344v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.00344"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item analyzes automatic speaker similarity evaluation and embedding-based metrics, which does not address discrete audio tokens/tokenization nor a token-oriented design, thus it does not meet the inclusion criteria for Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item analyzes automatic speaker similarity evaluation and embedding-based metrics, which does not address discrete audio tokens/tokenization nor a token-oriented design, thus it does not meet the inclusion criteria for Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on an automatic evaluation method for speaker similarity in multi-speaker TTS systems, but it does not address the design, generation, or usage of discrete audio tokens, nor does it describe tokenization processes, vector quantization, or discrete representation mechanisms as required by the inclusion criteria. Therefore, it does not meet the necessary criteria related to discrete audio tokens and their applications in sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on an automatic evaluation method for speaker similarity in multi-speaker TTS systems, but it does not address the design, generation, or usage of discrete audio tokens, nor does it describe tokenization processes, vector quantization, or discrete representation mechanisms as required by the inclusion criteria. Therefore, it does not meet the necessary criteria related to discrete audio tokens and their applications in sequence modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learning Noise-independent Speech Representation for High-quality Voice Conversion for Noisy Target Speakers",
    "abstract": "Building a voice conversion system for noisy target speakers, such as users providing noisy samples or Internet found data, is a challenging task since the use of contaminated speech in model training will apparently degrade the conversion performance. In this paper, we leverage the advances of our recently proposed Glow-WaveGAN and propose a noise-independent speech representation learning approach for high-quality voice conversion for noisy target speakers. Specifically, we learn a latent feature space where we ensure that the target distribution modeled by the conversion model is exactly from the modeled distribution of the waveform generator. With this premise, we further manage to make the latent feature to be noise-invariant. Specifically, we introduce a noise-controllable WaveGAN, which directly learns the noise-independent acoustic representation from waveform by the encoder and conducts noise control in the hidden space through a FiLM module in the decoder. As for the conversion model, importantly, we use a flow-based model to learn the distribution of noise-independent but speaker-related latent features from phoneme posteriorgrams. Experimental results demonstrate that the proposed model achieves high speech quality and speaker similarity in the voice conversion for noisy target speakers.",
    "metadata": {
      "arxiv_id": "2207.00756",
      "title": "Learning Noise-independent Speech Representation for High-quality Voice Conversion for Noisy Target Speakers",
      "summary": "Building a voice conversion system for noisy target speakers, such as users providing noisy samples or Internet found data, is a challenging task since the use of contaminated speech in model training will apparently degrade the conversion performance. In this paper, we leverage the advances of our recently proposed Glow-WaveGAN and propose a noise-independent speech representation learning approach for high-quality voice conversion for noisy target speakers. Specifically, we learn a latent feature space where we ensure that the target distribution modeled by the conversion model is exactly from the modeled distribution of the waveform generator. With this premise, we further manage to make the latent feature to be noise-invariant. Specifically, we introduce a noise-controllable WaveGAN, which directly learns the noise-independent acoustic representation from waveform by the encoder and conducts noise control in the hidden space through a FiLM module in the decoder. As for the conversion model, importantly, we use a flow-based model to learn the distribution of noise-independent but speaker-related latent features from phoneme posteriorgrams. Experimental results demonstrate that the proposed model achieves high speech quality and speaker similarity in the voice conversion for noisy target speakers.",
      "authors": [
        "Liumeng Xue",
        "Shan Yang",
        "Na Hu",
        "Dan Su",
        "Lei Xie"
      ],
      "published": "2022-07-02T06:51:12Z",
      "updated": "2022-07-02T06:51:12Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.00756v1",
      "landing_url": "https://arxiv.org/abs/2207.00756v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.00756"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on noise-robust latent acoustic representations for voice conversion and does not present discrete audio tokens, tokenization methods, or quantization design required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on noise-robust latent acoustic representations for voice conversion and does not present discrete audio tokens, tokenization methods, or quantization design required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on noise-independent speech representation for voice conversion in noisy environments, leveraging latent feature spaces and flow-based models, but does not describe the use or design of discrete audio tokens such as vector quantization or semantic-driven discrete representations for language or sequence modeling, nor does it detail token generation processes, quantization, or their use as intermediates for audio language modeling or cross-modal generation. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and their explicit generation and usage, and thus should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on noise-independent speech representation for voice conversion in noisy environments, leveraging latent feature spaces and flow-based models, but does not describe the use or design of discrete audio tokens such as vector quantization or semantic-driven discrete representations for language or sequence modeling, nor does it detail token generation processes, quantization, or their use as intermediates for audio language modeling or cross-modal generation. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and their explicit generation and usage, and thus should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Cross-Scale Vector Quantization for Scalable Neural Speech Coding",
    "abstract": "Bitrate scalability is a desirable feature for audio coding in real-time communications. Existing neural audio codecs usually enforce a specific bitrate during training, so different models need to be trained for each target bitrate, which increases the memory footprint at the sender and the receiver side and transcoding is often needed to support multiple receivers. In this paper, we introduce a cross-scale scalable vector quantization scheme (CSVQ), in which multi-scale features are encoded progressively with stepwise feature fusion and refinement. In this way, a coarse-level signal is reconstructed if only a portion of the bitstream is received, and progressively improves the quality as more bits are available. The proposed CSVQ scheme can be flexibly applied to any neural audio coding network with a mirrored auto-encoder structure to achieve bitrate scalability. Subjective results show that the proposed scheme outperforms the classical residual VQ (RVQ) with scalability. Moreover, the proposed CSVQ at 3 kbps outperforms Opus at 9 kbps and Lyra at 3kbps and it could provide a graceful quality boost with bitrate increase.",
    "metadata": {
      "arxiv_id": "2207.03067",
      "title": "Cross-Scale Vector Quantization for Scalable Neural Speech Coding",
      "summary": "Bitrate scalability is a desirable feature for audio coding in real-time communications. Existing neural audio codecs usually enforce a specific bitrate during training, so different models need to be trained for each target bitrate, which increases the memory footprint at the sender and the receiver side and transcoding is often needed to support multiple receivers. In this paper, we introduce a cross-scale scalable vector quantization scheme (CSVQ), in which multi-scale features are encoded progressively with stepwise feature fusion and refinement. In this way, a coarse-level signal is reconstructed if only a portion of the bitstream is received, and progressively improves the quality as more bits are available. The proposed CSVQ scheme can be flexibly applied to any neural audio coding network with a mirrored auto-encoder structure to achieve bitrate scalability. Subjective results show that the proposed scheme outperforms the classical residual VQ (RVQ) with scalability. Moreover, the proposed CSVQ at 3 kbps outperforms Opus at 9 kbps and Lyra at 3kbps and it could provide a graceful quality boost with bitrate increase.",
      "authors": [
        "Xue Jiang",
        "Xiulian Peng",
        "Huaying Xue",
        "Yuan Zhang",
        "Yan Lu"
      ],
      "published": "2022-07-07T03:23:25Z",
      "updated": "2022-07-07T03:23:25Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.03067v1",
      "landing_url": "https://arxiv.org/abs/2207.03067v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.03067"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract discuss cross-scale vector quantization for scalable neural speech coding, which uses discrete representations via VQ, aligning with discrete audio token ideas to some extent, but the work’s primary focus is scalable coding rather than detailing token generation/usage for language-modeling or downstream tasks, so it does not clearly meet all inclusion criteria.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The title/abstract discuss cross-scale vector quantization for scalable neural speech coding, which uses discrete representations via VQ, aligning with discrete audio token ideas to some extent, but the work’s primary focus is scalable coding rather than detailing token generation/usage for language-modeling or downstream tasks, so it does not clearly meet all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a cross-scale scalable vector quantization scheme (CSVQ) for neural speech coding, which involves vector quantization and aims at bitrate scalability in audio coding. However, the abstract does not explicitly clarify that the discrete tokens generated are used as intermediate representations for sequence or language model-based audio generation or understanding tasks. The study primarily focuses on efficient audio compression and scalable coding performance, not on leveraging discrete audio tokens as core intermediaries for audio language modeling or downstream generative/understanding tasks, which is central to the inclusion criteria. Therefore, while related to vector quantization, it appears more aligned with codec improvements for compression rather than discrete token design for unified audio modeling purposes.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a cross-scale scalable vector quantization scheme (CSVQ) for neural speech coding, which involves vector quantization and aims at bitrate scalability in audio coding. However, the abstract does not explicitly clarify that the discrete tokens generated are used as intermediate representations for sequence or language model-based audio generation or understanding tasks. The study primarily focuses on efficient audio compression and scalable coding performance, not on leveraging discrete audio tokens as core intermediaries for audio language modeling or downstream generative/understanding tasks, which is central to the inclusion criteria. Therefore, while related to vector quantization, it appears more aligned with codec improvements for compression rather than discrete token design for unified audio modeling purposes.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the paper uses vector quantization for a neural audio codec but focused on bitrate scalability/codecs rather than proposing discrete audio tokens as intermediates for sequence or language-modeling tasks. The abstract describes a cross-scale VQ (CSVQ) applied to neural codecs and demonstrates codec-quality gains versus RVQ/Opus/Lyra, but it does not claim or describe that the discrete outputs are intended as tokens for audio language modeling, nor does it provide token-generation/usage details for downstream sequence models in the abstract. Given the inclusion criteria require discrete token design as a primary contribution and clear positioning/use for sequence/language modeling or downstream tasks, this item aligns more with codec engineering (scalable compression) than the discrete-audio-token research focus. Junior assessments: one recommended exclusion/uncertain (score 3) and the other recommended exclusion (score 2); I agree it is better to exclude.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the paper uses vector quantization for a neural audio codec but focused on bitrate scalability/codecs rather than proposing discrete audio tokens as intermediates for sequence or language-modeling tasks. The abstract describes a cross-scale VQ (CSVQ) applied to neural codecs and demonstrates codec-quality gains versus RVQ/Opus/Lyra, but it does not claim or describe that the discrete outputs are intended as tokens for audio language modeling, nor does it provide token-generation/usage details for downstream sequence models in the abstract. Given the inclusion criteria require discrete token design as a primary contribution and clear positioning/use for sequence/language modeling or downstream tasks, this item aligns more with codec engineering (scalable compression) than the discrete-audio-token research focus. Junior assessments: one recommended exclusion/uncertain (score 3) and the other recommended exclusion (score 2); I agree it is better to exclude.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "A Comparative Study of Self-supervised Speech Representation Based Voice Conversion",
    "abstract": "We present a large-scale comparative study of self-supervised speech representation (S3R)-based voice conversion (VC). In the context of recognition-synthesis VC, S3Rs are attractive owing to their potential to replace expensive supervised representations such as phonetic posteriorgrams (PPGs), which are commonly adopted by state-of-the-art VC systems. Using S3PRL-VC, an open-source VC software we previously developed, we provide a series of in-depth objective and subjective analyses under three VC settings: intra-/cross-lingual any-to-one (A2O) and any-to-any (A2A) VC, using the voice conversion challenge 2020 (VCC2020) dataset. We investigated S3R-based VC in various aspects, including model type, multilinguality, and supervision. We also studied the effect of a post-discretization process with k-means clustering and showed how it improves in the A2A setting. Finally, the comparison with state-of-the-art VC systems demonstrates the competitiveness of S3R-based VC and also sheds light on the possible improving directions.",
    "metadata": {
      "arxiv_id": "2207.04356",
      "title": "A Comparative Study of Self-supervised Speech Representation Based Voice Conversion",
      "summary": "We present a large-scale comparative study of self-supervised speech representation (S3R)-based voice conversion (VC). In the context of recognition-synthesis VC, S3Rs are attractive owing to their potential to replace expensive supervised representations such as phonetic posteriorgrams (PPGs), which are commonly adopted by state-of-the-art VC systems. Using S3PRL-VC, an open-source VC software we previously developed, we provide a series of in-depth objective and subjective analyses under three VC settings: intra-/cross-lingual any-to-one (A2O) and any-to-any (A2A) VC, using the voice conversion challenge 2020 (VCC2020) dataset. We investigated S3R-based VC in various aspects, including model type, multilinguality, and supervision. We also studied the effect of a post-discretization process with k-means clustering and showed how it improves in the A2A setting. Finally, the comparison with state-of-the-art VC systems demonstrates the competitiveness of S3R-based VC and also sheds light on the possible improving directions.",
      "authors": [
        "Wen-Chin Huang",
        "Shu-Wen Yang",
        "Tomoki Hayashi",
        "Tomoki Toda"
      ],
      "published": "2022-07-10T01:02:22Z",
      "updated": "2022-07-10T01:02:22Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.04356v1",
      "landing_url": "https://arxiv.org/abs/2207.04356v1",
      "doi": "https://doi.org/10.1109/JSTSP.2022.3193761"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on self-supervised speech representations for voice conversion and mentions a post-discretization step, but it does not present discrete audio token design/quantization as its main contribution, and there is no explicit publication date provided, failing both core topic focus and a date-retrace requirement.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on self-supervised speech representations for voice conversion and mentions a post-discretization step, but it does not present discrete audio token design/quantization as its main contribution, and there is no explicit publication date provided, failing both core topic focus and a date-retrace requirement.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised speech representations (S3R) and voice conversion but does not explicitly mention generating discrete audio tokens via vector quantization or similar discretization methods as a core contribution; it mentions post-discretization with k-means clustering, but it is unclear if this process produces discrete tokens used for sequence or language modeling. The description lacks detailed methodological information about token generation flow, quantization specifics, or integration with language models, making it insufficient to confirm inclusion under the discrete audio tokens theme.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised speech representations (S3R) and voice conversion but does not explicitly mention generating discrete audio tokens via vector quantization or similar discretization methods as a core contribution; it mentions post-discretization with k-means clustering, but it is unclear if this process produces discrete tokens used for sequence or language modeling. The description lacks detailed methodological information about token generation flow, quantization specifics, or integration with language models, making it insufficient to confirm inclusion under the discrete audio tokens theme.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Federated Self-Supervised Learning in Heterogeneous Settings: Limits of a Baseline Approach on HAR",
    "abstract": "Federated Learning is a new machine learning paradigm dealing with distributed model learning on independent devices. One of the many advantages of federated learning is that training data stay on devices (such as smartphones), and only learned models are shared with a centralized server. In the case of supervised learning, labeling is entrusted to the clients. However, acquiring such labels can be prohibitively expensive and error-prone for many tasks, such as human activity recognition. Hence, a wealth of data remains unlabelled and unexploited. Most existing federated learning approaches that focus mainly on supervised learning have mostly ignored this mass of unlabelled data. Furthermore, it is unclear whether standard federated Learning approaches are suited to self-supervised learning. The few studies that have dealt with the problem have limited themselves to the favorable situation of homogeneous datasets. This work lays the groundwork for a reference evaluation of federated Learning with Semi-Supervised Learning in a realistic setting. We show that standard lightweight autoencoder and standard Federated Averaging fail to learn a robust representation for Human Activity Recognition with several realistic heterogeneous datasets. These findings advocate for a more intensive research effort in Federated Self Supervised Learning to exploit the mass of heterogeneous unlabelled data present on mobile devices.",
    "metadata": {
      "arxiv_id": "2207.08187",
      "title": "Federated Self-Supervised Learning in Heterogeneous Settings: Limits of a Baseline Approach on HAR",
      "summary": "Federated Learning is a new machine learning paradigm dealing with distributed model learning on independent devices. One of the many advantages of federated learning is that training data stay on devices (such as smartphones), and only learned models are shared with a centralized server. In the case of supervised learning, labeling is entrusted to the clients. However, acquiring such labels can be prohibitively expensive and error-prone for many tasks, such as human activity recognition. Hence, a wealth of data remains unlabelled and unexploited. Most existing federated learning approaches that focus mainly on supervised learning have mostly ignored this mass of unlabelled data. Furthermore, it is unclear whether standard federated Learning approaches are suited to self-supervised learning. The few studies that have dealt with the problem have limited themselves to the favorable situation of homogeneous datasets. This work lays the groundwork for a reference evaluation of federated Learning with Semi-Supervised Learning in a realistic setting. We show that standard lightweight autoencoder and standard Federated Averaging fail to learn a robust representation for Human Activity Recognition with several realistic heterogeneous datasets. These findings advocate for a more intensive research effort in Federated Self Supervised Learning to exploit the mass of heterogeneous unlabelled data present on mobile devices.",
      "authors": [
        "Sannara Ek",
        "Romain Rombourg",
        "François Portet",
        "Philippe Lalanda"
      ],
      "published": "2022-07-17T14:15:45Z",
      "updated": "2022-07-17T14:15:45Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.08187v1",
      "landing_url": "https://arxiv.org/abs/2207.08187v1",
      "doi": "https://doi.org/10.1109/PerComWorkshops53856.2022.9767369"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目聚焦於聯邦自監督學習在行為辨識（HAR）中的表現與雜質資料異質性問題，未涉及離散音訊 token 的設計、量化流程或可重現的離散 token 方法，故不符合本主題的離散音訊 token 關聯條件，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目聚焦於聯邦自監督學習在行為辨識（HAR）中的表現與雜質資料異質性問題，未涉及離散音訊 token 的設計、量化流程或可重現的離散 token 方法，故不符合本主題的離散音訊 token 關聯條件，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on federated self-supervised learning on heterogeneous datasets for human activity recognition and does not discuss discrete audio tokens, vector quantization, audio language modeling, or any related discrete audio token generation methods. It lacks the core methodology or contributions related to discrete audio token design or processing required by the inclusion criteria. Therefore, it does not fit the specified research topic and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on federated self-supervised learning on heterogeneous datasets for human activity recognition and does not discuss discrete audio tokens, vector quantization, audio language modeling, or any related discrete audio token generation methods. It lacks the core methodology or contributions related to discrete audio token design or processing required by the inclusion criteria. Therefore, it does not fit the specified research topic and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Ensemble Learning for Efficient VVC Bitrate Ladder Prediction",
    "abstract": "Changing the encoding parameters, in particular the video resolution, is a common practice before transcoding. To this end, streaming and broadcast platforms benefit from so-called bitrate ladders to determine the optimal resolution for given bitrates. However, the task of determining the bitrate ladder can usually be challenging as, on one hand, so-called fit-for-all static ladders would waste bandwidth, and on the other hand, fully specialized ladders are often not affordable in terms of computational complexity. In this paper, we propose an ML-based scheme for predicting the bitrate ladder based on the content of the video. The baseline of our solution predicts the bitrate ladder using two constituent methods, which require no encoding passes. To further enhance the performance of the constituent methods, we integrate a conditional ensemble method to aggregate their decisions, with a negligibly limited number of encoding passes. The experiment, carried out on the optimized software encoder implementation of the VVC standard, called VVenC, shows significant performance improvement. When compared to static bitrate ladder, the proposed method can offer about 13% bitrate reduction in terms of BD-BR with a negligible additional computational overhead. Conversely, when compared to the fully specialized bitrate ladder method, the proposed method can offer about 86% to 92% complexity reduction, at cost the of only 0.8% to 0.9% coding efficiency drop in terms of BD-BR.",
    "metadata": {
      "arxiv_id": "2207.10317",
      "title": "Ensemble Learning for Efficient VVC Bitrate Ladder Prediction",
      "summary": "Changing the encoding parameters, in particular the video resolution, is a common practice before transcoding. To this end, streaming and broadcast platforms benefit from so-called bitrate ladders to determine the optimal resolution for given bitrates. However, the task of determining the bitrate ladder can usually be challenging as, on one hand, so-called fit-for-all static ladders would waste bandwidth, and on the other hand, fully specialized ladders are often not affordable in terms of computational complexity. In this paper, we propose an ML-based scheme for predicting the bitrate ladder based on the content of the video. The baseline of our solution predicts the bitrate ladder using two constituent methods, which require no encoding passes. To further enhance the performance of the constituent methods, we integrate a conditional ensemble method to aggregate their decisions, with a negligibly limited number of encoding passes. The experiment, carried out on the optimized software encoder implementation of the VVC standard, called VVenC, shows significant performance improvement. When compared to static bitrate ladder, the proposed method can offer about 13% bitrate reduction in terms of BD-BR with a negligible additional computational overhead. Conversely, when compared to the fully specialized bitrate ladder method, the proposed method can offer about 86% to 92% complexity reduction, at cost the of only 0.8% to 0.9% coding efficiency drop in terms of BD-BR.",
      "authors": [
        "Fatemeh Nasiri",
        "Wassim Hamidouche",
        "Luce Morin",
        "Nicolas Dholland",
        "Jean-Yves Aubié"
      ],
      "published": "2022-07-21T06:19:37Z",
      "updated": "2022-07-23T08:46:39Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.10317v2",
      "landing_url": "https://arxiv.org/abs/2207.10317v2",
      "doi": "https://doi.org/10.48550/arXiv.2207.10317"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about ML-based video bitrate ladder prediction for VVC; it does not involve discrete audio tokens or audio token design, thus fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about ML-based video bitrate ladder prediction for VVC; it does not involve discrete audio tokens or audio token design, thus fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on video encoding bitrate ladder prediction using machine learning and does not address discrete audio tokens or their generation, representation, or utilization for sequence/language models as defined in the inclusion criteria. It also lacks any mention of vector quantization, semantic or acoustic disentanglement, or audio language modeling, and is therefore outside the scope of the topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on video encoding bitrate ladder prediction using machine learning and does not address discrete audio tokens or their generation, representation, or utilization for sequence/language models as defined in the inclusion criteria. It also lacks any mention of vector quantization, semantic or acoustic disentanglement, or audio language modeling, and is therefore outside the scope of the topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FewGAN: Generating from the Joint Distribution of a Few Images",
    "abstract": "We introduce FewGAN, a generative model for generating novel, high-quality and diverse images whose patch distribution lies in the joint patch distribution of a small number of N>1 training samples. The method is, in essence, a hierarchical patch-GAN that applies quantization at the first coarse scale, in a similar fashion to VQ-GAN, followed by a pyramid of residual fully convolutional GANs at finer scales. Our key idea is to first use quantization to learn a fixed set of patch embeddings for training images. We then use a separate set of side images to model the structure of generated images using an autoregressive model trained on the learned patch embeddings of training images. Using quantization at the coarsest scale allows the model to generate both conditional and unconditional novel images. Subsequently, a patch-GAN renders the fine details, resulting in high-quality images. In an extensive set of experiments, it is shown that FewGAN outperforms baselines both quantitatively and qualitatively.",
    "metadata": {
      "arxiv_id": "2207.11226",
      "title": "FewGAN: Generating from the Joint Distribution of a Few Images",
      "summary": "We introduce FewGAN, a generative model for generating novel, high-quality and diverse images whose patch distribution lies in the joint patch distribution of a small number of N>1 training samples. The method is, in essence, a hierarchical patch-GAN that applies quantization at the first coarse scale, in a similar fashion to VQ-GAN, followed by a pyramid of residual fully convolutional GANs at finer scales. Our key idea is to first use quantization to learn a fixed set of patch embeddings for training images. We then use a separate set of side images to model the structure of generated images using an autoregressive model trained on the learned patch embeddings of training images. Using quantization at the coarsest scale allows the model to generate both conditional and unconditional novel images. Subsequently, a patch-GAN renders the fine details, resulting in high-quality images. In an extensive set of experiments, it is shown that FewGAN outperforms baselines both quantitatively and qualitatively.",
      "authors": [
        "Lior Ben-Moshe",
        "Sagie Benaim",
        "Lior Wolf"
      ],
      "published": "2022-07-18T07:11:28Z",
      "updated": "2022-07-18T07:11:28Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.11226v1",
      "landing_url": "https://arxiv.org/abs/2207.11226v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.11226"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses image generation (FewGAN) with patch-based GANs, not discrete audio tokens; it does not meet the inclusion criteria centered on discretized audio representations, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses image generation (FewGAN) with patch-based GANs, not discrete audio tokens; it does not meet the inclusion criteria centered on discretized audio representations, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study FewGAN focuses on generating images through a hierarchical patch-GAN approach and patch embeddings, which involves quantization techniques; however, it is applied to visual image generation rather than audio signals or discrete audio tokens. The inclusion criteria specifically require the study to be about discrete audio tokens derived from continuous audio signals with explicit mention of token generation processes for audio. Since FewGAN is unrelated to audio tokenization, it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study FewGAN focuses on generating images through a hierarchical patch-GAN approach and patch embeddings, which involves quantization techniques; however, it is applied to visual image generation rather than audio signals or discrete audio tokens. The inclusion criteria specifically require the study to be about discrete audio tokens derived from continuous audio signals with explicit mention of token generation processes for audio. Since FewGAN is unrelated to audio tokenization, it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adversarial Robustness of MR Image Reconstruction under Realistic Perturbations",
    "abstract": "Deep Learning (DL) methods have shown promising results for solving ill-posed inverse problems such as MR image reconstruction from undersampled $k$-space data. However, these approaches currently have no guarantees for reconstruction quality and the reliability of such algorithms is only poorly understood. Adversarial attacks offer a valuable tool to understand possible failure modes and worst case performance of DL-based reconstruction algorithms. In this paper we describe adversarial attacks on multi-coil $k$-space measurements and evaluate them on the recently proposed E2E-VarNet and a simpler UNet-based model. In contrast to prior work, the attacks are targeted to specifically alter diagnostically relevant regions. Using two realistic attack models (adversarial $k$-space noise and adversarial rotations) we are able to show that current state-of-the-art DL-based reconstruction algorithms are indeed sensitive to such perturbations to a degree where relevant diagnostic information may be lost. Surprisingly, in our experiments the UNet and the more sophisticated E2E-VarNet were similarly sensitive to such attacks. Our findings add further to the evidence that caution must be exercised as DL-based methods move closer to clinical practice.",
    "metadata": {
      "arxiv_id": "2208.03161",
      "title": "Adversarial Robustness of MR Image Reconstruction under Realistic Perturbations",
      "summary": "Deep Learning (DL) methods have shown promising results for solving ill-posed inverse problems such as MR image reconstruction from undersampled $k$-space data. However, these approaches currently have no guarantees for reconstruction quality and the reliability of such algorithms is only poorly understood. Adversarial attacks offer a valuable tool to understand possible failure modes and worst case performance of DL-based reconstruction algorithms. In this paper we describe adversarial attacks on multi-coil $k$-space measurements and evaluate them on the recently proposed E2E-VarNet and a simpler UNet-based model. In contrast to prior work, the attacks are targeted to specifically alter diagnostically relevant regions. Using two realistic attack models (adversarial $k$-space noise and adversarial rotations) we are able to show that current state-of-the-art DL-based reconstruction algorithms are indeed sensitive to such perturbations to a degree where relevant diagnostic information may be lost. Surprisingly, in our experiments the UNet and the more sophisticated E2E-VarNet were similarly sensitive to such attacks. Our findings add further to the evidence that caution must be exercised as DL-based methods move closer to clinical practice.",
      "authors": [
        "Jan Nikolas Morshuis",
        "Sergios Gatidis",
        "Matthias Hein",
        "Christian F. Baumgartner"
      ],
      "published": "2022-08-05T13:39:40Z",
      "updated": "2022-08-05T13:39:40Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.03161v1",
      "landing_url": "https://arxiv.org/abs/2208.03161v1",
      "doi": "https://doi.org/10.48550/arXiv.2208.03161"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses adversarial robustness in MR image reconstruction, which has no relation to Discrete Audio Tokens or audio tokenization; it fails to meet the audio-token inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses adversarial robustness in MR image reconstruction, which has no relation to Discrete Audio Tokens or audio tokenization; it fails to meet the audio-token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on adversarial robustness in MR image reconstruction using deep learning, which is unrelated to discrete audio tokens or audio signal processing as defined in the inclusion criteria. No mention is made of audio tokenization, vector quantization for audio, or language/sequential modeling of audio tokens. Therefore, it does not meet the inclusion criteria and falls outside the scope of the study topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on adversarial robustness in MR image reconstruction using deep learning, which is unrelated to discrete audio tokens or audio signal processing as defined in the inclusion criteria. No mention is made of audio tokenization, vector quantization for audio, or language/sequential modeling of audio tokens. Therefore, it does not meet the inclusion criteria and falls outside the scope of the study topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Non-Contrastive Self-supervised Learning for Utterance-Level Information Extraction from Speech",
    "abstract": "In recent studies, self-supervised pre-trained models tend to outperform supervised pre-trained models in transfer learning. In particular, self-supervised learning (SSL) of utterance-level speech representation can be used in speech applications that require discriminative representation of consistent attributes within an utterance: speaker, language, emotion, and age. Existing frame-level self-supervised speech representation, e.g., wav2vec, can be used as utterance-level representation with pooling, but the models are usually large. There are also SSL techniques to learn utterance-level representation. One of the most successful is a contrastive method, which requires negative sampling: selecting alternative samples to contrast with the current sample (anchor). However, this does not ensure that all the negative samples belong to classes different from the anchor class without labels. This paper applies a non-contrastive self-supervised method to learn utterance-level embeddings. We adapted DIstillation with NO labels (DINO) from computer vision to speech. Unlike contrastive methods, DINO does not require negative sampling. We compared DINO to x-vector trained in a supervised manner. When transferred to down-stream tasks (speaker verification, speech emotion recognition (SER), and Alzheimer's disease detection), DINO outperformed x-vector. We studied the influence of several aspects during transfer learning such as dividing the fine-tuning process into steps, chunk lengths, or augmentation. During fine-tuning, tuning the last affine layers first and then the whole network surpassed fine-tuning all at once. Using shorter chunk lengths, although they generate more diverse inputs, did not necessarily improve performance, implying speech segments at least with a specific length are required for better performance per application. Augmentation was helpful in SER.",
    "metadata": {
      "arxiv_id": "2208.05445",
      "title": "Non-Contrastive Self-supervised Learning for Utterance-Level Information Extraction from Speech",
      "summary": "In recent studies, self-supervised pre-trained models tend to outperform supervised pre-trained models in transfer learning. In particular, self-supervised learning (SSL) of utterance-level speech representation can be used in speech applications that require discriminative representation of consistent attributes within an utterance: speaker, language, emotion, and age. Existing frame-level self-supervised speech representation, e.g., wav2vec, can be used as utterance-level representation with pooling, but the models are usually large. There are also SSL techniques to learn utterance-level representation. One of the most successful is a contrastive method, which requires negative sampling: selecting alternative samples to contrast with the current sample (anchor). However, this does not ensure that all the negative samples belong to classes different from the anchor class without labels. This paper applies a non-contrastive self-supervised method to learn utterance-level embeddings. We adapted DIstillation with NO labels (DINO) from computer vision to speech. Unlike contrastive methods, DINO does not require negative sampling. We compared DINO to x-vector trained in a supervised manner. When transferred to down-stream tasks (speaker verification, speech emotion recognition (SER), and Alzheimer's disease detection), DINO outperformed x-vector. We studied the influence of several aspects during transfer learning such as dividing the fine-tuning process into steps, chunk lengths, or augmentation. During fine-tuning, tuning the last affine layers first and then the whole network surpassed fine-tuning all at once. Using shorter chunk lengths, although they generate more diverse inputs, did not necessarily improve performance, implying speech segments at least with a specific length are required for better performance per application. Augmentation was helpful in SER.",
      "authors": [
        "Jaejin Cho",
        "Jes'us Villalba",
        "Laureano Moro-Velazquez",
        "Najim Dehak"
      ],
      "published": "2022-08-10T16:56:39Z",
      "updated": "2022-08-10T16:56:39Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.05445v1",
      "landing_url": "https://arxiv.org/abs/2208.05445v1",
      "doi": "https://doi.org/10.1109/JSTSP.2022.3197315"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on non-contrastive utterance-level SSL for speech representations and does not present discrete audio tokens or tokenization/quantization methods, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on non-contrastive utterance-level SSL for speech representations and does not present discrete audio tokens or tokenization/quantization methods, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on non-contrastive self-supervised learning for utterance-level speech representation and its application in tasks like speaker verification and emotion recognition, but it does not discuss or propose any discrete audio token generation mechanism such as vector quantization or other discrete token design methods. There is no clear description of token formation process, quantization, or any sequence-modeling of discrete tokens as required by the inclusion criteria, thus it does not meet the core inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on non-contrastive self-supervised learning for utterance-level speech representation and its application in tasks like speaker verification and emotion recognition, but it does not discuss or propose any discrete audio token generation mechanism such as vector quantization or other discrete token design methods. There is no clear description of token formation process, quantization, or any sequence-modeling of discrete tokens as required by the inclusion criteria, thus it does not meet the core inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speech Representation Disentanglement with Adversarial Mutual Information Learning for One-shot Voice Conversion",
    "abstract": "One-shot voice conversion (VC) with only a single target speaker's speech for reference has become a hot research topic. Existing works generally disentangle timbre, while information about pitch, rhythm and content is still mixed together. To perform one-shot VC effectively with further disentangling these speech components, we employ random resampling for pitch and content encoder and use the variational contrastive log-ratio upper bound of mutual information and gradient reversal layer based adversarial mutual information learning to ensure the different parts of the latent space containing only the desired disentangled representation during training. Experiments on the VCTK dataset show the model achieves state-of-the-art performance for one-shot VC in terms of naturalness and intellgibility. In addition, we can transfer characteristics of one-shot VC on timbre, pitch and rhythm separately by speech representation disentanglement. Our code, pre-trained models and demo are available at https://im1eon.github.io/IS2022-SRDVC/.",
    "metadata": {
      "arxiv_id": "2208.08757",
      "title": "Speech Representation Disentanglement with Adversarial Mutual Information Learning for One-shot Voice Conversion",
      "summary": "One-shot voice conversion (VC) with only a single target speaker's speech for reference has become a hot research topic. Existing works generally disentangle timbre, while information about pitch, rhythm and content is still mixed together. To perform one-shot VC effectively with further disentangling these speech components, we employ random resampling for pitch and content encoder and use the variational contrastive log-ratio upper bound of mutual information and gradient reversal layer based adversarial mutual information learning to ensure the different parts of the latent space containing only the desired disentangled representation during training. Experiments on the VCTK dataset show the model achieves state-of-the-art performance for one-shot VC in terms of naturalness and intellgibility. In addition, we can transfer characteristics of one-shot VC on timbre, pitch and rhythm separately by speech representation disentanglement. Our code, pre-trained models and demo are available at https://im1eon.github.io/IS2022-SRDVC/.",
      "authors": [
        "SiCheng Yang",
        "Methawee Tantrawenith",
        "Haolin Zhuang",
        "Zhiyong Wu",
        "Aolan Sun",
        "Jianzong Wang",
        "Ning Cheng",
        "Huaizhen Tang",
        "Xintao Zhao",
        "Jie Wang",
        "Helen Meng"
      ],
      "published": "2022-08-18T10:36:27Z",
      "updated": "2022-08-18T10:36:27Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.08757v1",
      "landing_url": "https://arxiv.org/abs/2208.08757v1",
      "doi": null
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper addresses speech representation disentanglement for one-shot voice conversion and uses mutual information learning, but it does not propose discrete audio tokens or tokenization mechanisms (VQ/RVQ/quantization) for downstream language-model style processing, so it fails the discrete token criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper addresses speech representation disentanglement for one-shot voice conversion and uses mutual information learning, but it does not propose discrete audio tokens or tokenization mechanisms (VQ/RVQ/quantization) for downstream language-model style processing, so it fails the discrete token criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on disentanglement of speech representation for one-shot voice conversion using adversarial mutual information learning but does not mention the generation or use of discrete audio tokens through vector quantization or related quantization methods; it lacks description of token generation process or usage in sequence/language models, hence it does not meet the inclusion criteria about discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on disentanglement of speech representation for one-shot voice conversion using adversarial mutual information learning but does not mention the generation or use of discrete audio tokens through vector quantization or related quantization methods; it lacks description of token generation process or usage in sequence/language models, hence it does not meet the inclusion criteria about discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Secure and Efficient Data Deduplication Scheme with Dynamic Ownership Management in Cloud Computing",
    "abstract": "Encrypted data deduplication is an important technique for saving storage space and network bandwidth, which has been widely used in cloud storage. Recently, a number of schemes that solve the problem of data deduplication with dynamic ownership management have been proposed. However, these schemes suffer from low efficiency when the dynamic ownership changes a lot. To this end, in this paper, we propose a novel server-side deduplication scheme for encrypted data in a hybrid cloud architecture, where a public cloud (Pub-CSP) manages the storage and a private cloud (Pri-CSP) plays a role as the data owner to perform deduplication and dynamic ownership management. Further, to reduce the communication overhead we use an initial uploader check mechanism to ensure only the first uploader needs to perform encryption, and adopt an access control technique that verifies the validity of the data users before they download data. Our security analysis and performance evaluation demonstrate that our proposed server-side deduplication scheme has better performance in terms of security, effectiveness, and practicability compared with previous schemes. Meanwhile, our method can efficiently resist collusion attacks and duplicate faking attacks.",
    "metadata": {
      "arxiv_id": "2208.09030",
      "title": "A Secure and Efficient Data Deduplication Scheme with Dynamic Ownership Management in Cloud Computing",
      "summary": "Encrypted data deduplication is an important technique for saving storage space and network bandwidth, which has been widely used in cloud storage. Recently, a number of schemes that solve the problem of data deduplication with dynamic ownership management have been proposed. However, these schemes suffer from low efficiency when the dynamic ownership changes a lot. To this end, in this paper, we propose a novel server-side deduplication scheme for encrypted data in a hybrid cloud architecture, where a public cloud (Pub-CSP) manages the storage and a private cloud (Pri-CSP) plays a role as the data owner to perform deduplication and dynamic ownership management. Further, to reduce the communication overhead we use an initial uploader check mechanism to ensure only the first uploader needs to perform encryption, and adopt an access control technique that verifies the validity of the data users before they download data. Our security analysis and performance evaluation demonstrate that our proposed server-side deduplication scheme has better performance in terms of security, effectiveness, and practicability compared with previous schemes. Meanwhile, our method can efficiently resist collusion attacks and duplicate faking attacks.",
      "authors": [
        "Xuewei Ma",
        "Wenyuan Yang",
        "Yuesheng Zhu",
        "Zhiqiang Bai"
      ],
      "published": "2022-08-18T19:02:30Z",
      "updated": "2022-08-31T15:47:52Z",
      "categories": [
        "cs.CR",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.09030v3",
      "landing_url": "https://arxiv.org/abs/2208.09030v3",
      "doi": "https://doi.org/10.48550/arXiv.2208.09030"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns server-side encrypted data deduplication and dynamic ownership in cloud storage, not discrete audio tokens or tokenization; thus it does not meet the inclusion criteria focused on Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns server-side encrypted data deduplication and dynamic ownership in cloud storage, not discrete audio tokens or tokenization; thus it does not meet the inclusion criteria focused on Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on encrypted data deduplication and dynamic ownership management in cloud computing, which is unrelated to the discrete audio tokens domain as defined, lacking any mention of audio token generation, quantization, or sequence modeling for audio understanding or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on encrypted data deduplication and dynamic ownership management in cloud computing, which is unrelated to the discrete audio tokens domain as defined, lacking any mention of audio token generation, quantization, or sequence modeling for audio understanding or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Overlapped speech and gender detection with WavLM pre-trained features",
    "abstract": "This article focuses on overlapped speech and gender detection in order to study interactions between women and men in French audiovisual media (Gender Equality Monitoring project). In this application context, we need to automatically segment the speech signal according to speakers gender, and to identify when at least two speakers speak at the same time. We propose to use WavLM model which has the advantage of being pre-trained on a huge amount of speech data, to build an overlapped speech detection (OSD) and a gender detection (GD) systems. In this study, we use two different corpora. The DIHARD III corpus which is well adapted for the OSD task but lack gender information. The ALLIES corpus fits with the project application context. Our best OSD system is a Temporal Convolutional Network (TCN) with WavLM pre-trained features as input, which reaches a new state-of-the-art F1-score performance on DIHARD. A neural GD is trained with WavLM inputs on a gender balanced subset of the French broadcast news ALLIES data, and obtains an accuracy of 97.9%. This work opens new perspectives for human science researchers regarding the differences of representation between women and men in French media.",
    "metadata": {
      "arxiv_id": "2209.04167",
      "title": "Overlapped speech and gender detection with WavLM pre-trained features",
      "summary": "This article focuses on overlapped speech and gender detection in order to study interactions between women and men in French audiovisual media (Gender Equality Monitoring project). In this application context, we need to automatically segment the speech signal according to speakers gender, and to identify when at least two speakers speak at the same time. We propose to use WavLM model which has the advantage of being pre-trained on a huge amount of speech data, to build an overlapped speech detection (OSD) and a gender detection (GD) systems. In this study, we use two different corpora. The DIHARD III corpus which is well adapted for the OSD task but lack gender information. The ALLIES corpus fits with the project application context. Our best OSD system is a Temporal Convolutional Network (TCN) with WavLM pre-trained features as input, which reaches a new state-of-the-art F1-score performance on DIHARD. A neural GD is trained with WavLM inputs on a gender balanced subset of the French broadcast news ALLIES data, and obtains an accuracy of 97.9%. This work opens new perspectives for human science researchers regarding the differences of representation between women and men in French media.",
      "authors": [
        "Martin Lebourdais",
        "Marie Tahon",
        "Antoine Laurent",
        "Sylvain Meignier"
      ],
      "published": "2022-09-09T08:00:47Z",
      "updated": "2022-09-09T08:00:47Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.04167v1",
      "landing_url": "https://arxiv.org/abs/2209.04167v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.04167"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This study targets overlapped speech detection and gender detection using WavLM features and reports datasets, but it does not establish or use discrete audio token representations or tokenization methods, thus it fails the 'Discrete Audio Tokens' inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This study targets overlapped speech detection and gender detection using WavLM features and reports datasets, but it does not establish or use discrete audio token representations or tokenization methods, thus it fails the 'Discrete Audio Tokens' inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on overlapped speech and gender detection using WavLM pre-trained features to improve speech segmentation and speaker gender identification. However, it does not discuss discrete audio token generation, vector quantization, or any form of discrete token design as defined in the inclusion criteria, nor does it describe any token formation process or discrete representation used for sequence modeling or language models. Therefore, it does not meet the core technical requirements for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on overlapped speech and gender detection using WavLM pre-trained features to improve speech segmentation and speaker gender identification. However, it does not discuss discrete audio token generation, vector quantization, or any form of discrete token design as defined in the inclusion criteria, nor does it describe any token formation process or discrete representation used for sequence modeling or language models. Therefore, it does not meet the core technical requirements for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improving GANs with A Dynamic Discriminator",
    "abstract": "Discriminator plays a vital role in training generative adversarial networks (GANs) via distinguishing real and synthesized samples. While the real data distribution remains the same, the synthesis distribution keeps varying because of the evolving generator, and thus effects a corresponding change to the bi-classification task for the discriminator. We argue that a discriminator with an on-the-fly adjustment on its capacity can better accommodate such a time-varying task. A comprehensive empirical study confirms that the proposed training strategy, termed as DynamicD, improves the synthesis performance without incurring any additional computation cost or training objectives. Two capacity adjusting schemes are developed for training GANs under different data regimes: i) given a sufficient amount of training data, the discriminator benefits from a progressively increased learning capacity, and ii) when the training data is limited, gradually decreasing the layer width mitigates the over-fitting issue of the discriminator. Experiments on both 2D and 3D-aware image synthesis tasks conducted on a range of datasets substantiate the generalizability of our DynamicD as well as its substantial improvement over the baselines. Furthermore, DynamicD is synergistic to other discriminator-improving approaches (including data augmentation, regularizers, and pre-training), and brings continuous performance gain when combined for learning GANs.",
    "metadata": {
      "arxiv_id": "2209.09897",
      "title": "Improving GANs with A Dynamic Discriminator",
      "summary": "Discriminator plays a vital role in training generative adversarial networks (GANs) via distinguishing real and synthesized samples. While the real data distribution remains the same, the synthesis distribution keeps varying because of the evolving generator, and thus effects a corresponding change to the bi-classification task for the discriminator. We argue that a discriminator with an on-the-fly adjustment on its capacity can better accommodate such a time-varying task. A comprehensive empirical study confirms that the proposed training strategy, termed as DynamicD, improves the synthesis performance without incurring any additional computation cost or training objectives. Two capacity adjusting schemes are developed for training GANs under different data regimes: i) given a sufficient amount of training data, the discriminator benefits from a progressively increased learning capacity, and ii) when the training data is limited, gradually decreasing the layer width mitigates the over-fitting issue of the discriminator. Experiments on both 2D and 3D-aware image synthesis tasks conducted on a range of datasets substantiate the generalizability of our DynamicD as well as its substantial improvement over the baselines. Furthermore, DynamicD is synergistic to other discriminator-improving approaches (including data augmentation, regularizers, and pre-training), and brings continuous performance gain when combined for learning GANs.",
      "authors": [
        "Ceyuan Yang",
        "Yujun Shen",
        "Yinghao Xu",
        "Deli Zhao",
        "Bo Dai",
        "Bolei Zhou"
      ],
      "published": "2022-09-20T17:57:33Z",
      "updated": "2022-09-20T17:57:33Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.09897v1",
      "landing_url": "https://arxiv.org/abs/2209.09897v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.09897"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses GAN training with a dynamic discriminator (DynamicD) for image synthesis and does not address discrete audio token representations or related methodology, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses GAN training with a dynamic discriminator (DynamicD) for image synthesis and does not address discrete audio token representations or related methodology, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on improving GANs by dynamically adjusting the discriminator to better handle evolving data distributions, which is a method in generative model training rather than discrete audio tokenization. There is no indication that the study involves discrete audio tokens, audio vector quantization, or token generation mechanisms related to audio processing or language modeling. Therefore, the study does not meet the inclusion criteria focused on discrete audio tokens, and is irrelevant to the specified research topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on improving GANs by dynamically adjusting the discriminator to better handle evolving data distributions, which is a method in generative model training rather than discrete audio tokenization. There is no indication that the study involves discrete audio tokens, audio vector quantization, or token generation mechanisms related to audio processing or language modeling. Therefore, the study does not meet the inclusion criteria focused on discrete audio tokens, and is irrelevant to the specified research topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Towards Frame Rate Agnostic Multi-Object Tracking",
    "abstract": "Multi-Object Tracking (MOT) is one of the most fundamental computer vision tasks that contributes to various video analysis applications. Despite the recent promising progress, current MOT research is still limited to a fixed sampling frame rate of the input stream. In fact, we empirically found that the accuracy of all recent state-of-the-art trackers drops dramatically when the input frame rate changes. For a more intelligent tracking solution, we shift the attention of our research work to the problem of Frame Rate Agnostic MOT (FraMOT), which takes frame rate insensitivity into consideration. In this paper, we propose a Frame Rate Agnostic MOT framework with a Periodic training Scheme (FAPS) to tackle the FraMOT problem for the first time. Specifically, we propose a Frame Rate Agnostic Association Module (FAAM) that infers and encodes the frame rate information to aid identity matching across multi-frame-rate inputs, improving the capability of the learned model in handling complex motion-appearance relations in FraMOT. Moreover, the association gap between training and inference is enlarged in FraMOT because those post-processing steps not included in training make a larger difference in lower frame rate scenarios. To address it, we propose Periodic Training Scheme (PTS) to reflect all post-processing steps in training via tracking pattern matching and fusion. Along with the proposed approaches, we make the first attempt to establish an evaluation method for this new task of FraMOT in two different modes, i.e., known frame rate and unknown frame rate, aiming to handle a more complex situation. The quantitative experiments on the challenging MOT17/20 dataset (FraMOT version) have clearly demonstrated that the proposed approaches can handle different frame rates better and thus improve the robustness against complicated scenarios.",
    "metadata": {
      "arxiv_id": "2209.11404",
      "title": "Towards Frame Rate Agnostic Multi-Object Tracking",
      "summary": "Multi-Object Tracking (MOT) is one of the most fundamental computer vision tasks that contributes to various video analysis applications. Despite the recent promising progress, current MOT research is still limited to a fixed sampling frame rate of the input stream. In fact, we empirically found that the accuracy of all recent state-of-the-art trackers drops dramatically when the input frame rate changes. For a more intelligent tracking solution, we shift the attention of our research work to the problem of Frame Rate Agnostic MOT (FraMOT), which takes frame rate insensitivity into consideration. In this paper, we propose a Frame Rate Agnostic MOT framework with a Periodic training Scheme (FAPS) to tackle the FraMOT problem for the first time. Specifically, we propose a Frame Rate Agnostic Association Module (FAAM) that infers and encodes the frame rate information to aid identity matching across multi-frame-rate inputs, improving the capability of the learned model in handling complex motion-appearance relations in FraMOT. Moreover, the association gap between training and inference is enlarged in FraMOT because those post-processing steps not included in training make a larger difference in lower frame rate scenarios. To address it, we propose Periodic Training Scheme (PTS) to reflect all post-processing steps in training via tracking pattern matching and fusion. Along with the proposed approaches, we make the first attempt to establish an evaluation method for this new task of FraMOT in two different modes, i.e., known frame rate and unknown frame rate, aiming to handle a more complex situation. The quantitative experiments on the challenging MOT17/20 dataset (FraMOT version) have clearly demonstrated that the proposed approaches can handle different frame rates better and thus improve the robustness against complicated scenarios.",
      "authors": [
        "Weitao Feng",
        "Lei Bai",
        "Yongqiang Yao",
        "Fengwei Yu",
        "Wanli Ouyang"
      ],
      "published": "2022-09-23T04:25:19Z",
      "updated": "2023-04-18T02:15:17Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.11404v3",
      "landing_url": "https://arxiv.org/abs/2209.11404v3",
      "doi": "https://doi.org/10.1007/s11263-023-01943-2"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on frame-rate-agnostic multi-object tracking in video (computer vision) and does not discuss discrete audio tokens or audio token quantization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on frame-rate-agnostic multi-object tracking in video (computer vision) and does not discuss discrete audio tokens or audio token quantization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multi-object tracking under varied frame rates and does not involve discrete audio tokens, audio signal discretization, or related audio language modeling topics defined in the inclusion criteria. It also lacks any discussion of token generation, vector quantization, or audio processing methodologies relevant to the discrete audio token research scope. Hence, it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multi-object tracking under varied frame rates and does not involve discrete audio tokens, audio signal discretization, or related audio language modeling topics defined in the inclusion criteria. It also lacks any discussion of token generation, vector quantization, or audio processing methodologies relevant to the discrete audio token research scope. Hence, it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Binaural Speech Enhancement Using STOI-Optimal Masks",
    "abstract": "STOI-optimal masking has been previously proposed and developed for single-channel speech enhancement. In this paper, we consider the extension to the task of binaural speech enhancement in which spatial information is known to be important to speech understanding and therefore should be preserved by the enhancement processing. Masks are estimated for each of the binaural channels individually and a `better-ear listening' mask is computed by choosing the maximum of the two masks. The estimated mask is used to supply probability information about the speech presence in each time-frequency bin to an Optimally-modified Log Spectral Amplitude (OM-LSA) enhancer. We show that using the proposed method for binaural signals with a directional noise not only improves the SNR of the noisy signal but also preserves the binaural cues and intelligibility.",
    "metadata": {
      "arxiv_id": "2209.15472",
      "title": "Binaural Speech Enhancement Using STOI-Optimal Masks",
      "summary": "STOI-optimal masking has been previously proposed and developed for single-channel speech enhancement. In this paper, we consider the extension to the task of binaural speech enhancement in which spatial information is known to be important to speech understanding and therefore should be preserved by the enhancement processing. Masks are estimated for each of the binaural channels individually and a `better-ear listening' mask is computed by choosing the maximum of the two masks. The estimated mask is used to supply probability information about the speech presence in each time-frequency bin to an Optimally-modified Log Spectral Amplitude (OM-LSA) enhancer. We show that using the proposed method for binaural signals with a directional noise not only improves the SNR of the noisy signal but also preserves the binaural cues and intelligibility.",
      "authors": [
        "Vikas Tokala",
        "Mike Brookes",
        "Patrick A. Naylor"
      ],
      "published": "2022-09-30T13:56:25Z",
      "updated": "2022-09-30T13:56:25Z",
      "categories": [
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.15472v1",
      "landing_url": "https://arxiv.org/abs/2209.15472v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.15472"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on binaural speech enhancement using STOI-optimal masks and does not present discrete audio tokens or tokenization methods, thus it does not meet the inclusion criteria for Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on binaural speech enhancement using STOI-optimal masks and does not present discrete audio tokens or tokenization methods, thus it does not meet the inclusion criteria for Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on binaural speech enhancement using STOI-optimal masks to improve signal-to-noise ratio and preserve binaural cues, without addressing discrete audio tokens, vector quantization, or any form of audio tokenization process required by the inclusion criteria. It lacks discussion of token generation, quantization, or use in language or sequence models, thus not meeting the core inclusion criteria related to discrete audio token representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on binaural speech enhancement using STOI-optimal masks to improve signal-to-noise ratio and preserve binaural cues, without addressing discrete audio tokens, vector quantization, or any form of audio tokenization process required by the inclusion criteria. It lacks discussion of token generation, quantization, or use in language or sequence models, thus not meeting the core inclusion criteria related to discrete audio token representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Flow Matching for Generative Modeling",
    "abstract": "We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.",
    "metadata": {
      "arxiv_id": "2210.02747",
      "title": "Flow Matching for Generative Modeling",
      "summary": "We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.",
      "authors": [
        "Yaron Lipman",
        "Ricky T. Q. Chen",
        "Heli Ben-Hamu",
        "Maximilian Nickel",
        "Matt Le"
      ],
      "published": "2022-10-06T08:32:20Z",
      "updated": "2023-02-08T15:46:05Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.02747v2",
      "landing_url": "https://arxiv.org/abs/2210.02747v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.02747"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses Flow Matching for continuous normalizing flows and diffusion-based generative modeling in general; it does not involve discrete audio token representations or token design for audio-language modeling, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses Flow Matching for continuous normalizing flows and diffusion-based generative modeling in general; it does not involve discrete audio token representations or token design for audio-language modeling, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on generative modeling using Continuous Normalizing Flows and Flow Matching methods, pertaining to image generation and diffusion processes, without any mention of discrete audio tokens, audio signal quantization, or audio-related discrete representations. Therefore, the input does not meet the inclusion criteria related to discrete audio tokens and their generation or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on generative modeling using Continuous Normalizing Flows and Flow Matching methods, pertaining to image generation and diffusion processes, without any mention of discrete audio tokens, audio signal quantization, or audio-related discrete representations. Therefore, the input does not meet the inclusion criteria related to discrete audio tokens and their generation or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Unified Encoder-Decoder Framework with Entity Memory",
    "abstract": "Entities, as important carriers of real-world knowledge, play a key role in many NLP tasks. We focus on incorporating entity knowledge into an encoder-decoder framework for informative text generation. Existing approaches tried to index, retrieve, and read external documents as evidence, but they suffered from a large computational overhead. In this work, we propose an encoder-decoder framework with an entity memory, namely EDMem. The entity knowledge is stored in the memory as latent representations, and the memory is pre-trained on Wikipedia along with encoder-decoder parameters. To precisely generate entity names, we design three decoding methods to constrain entity generation by linking entities in the memory. EDMem is a unified framework that can be used on various entity-intensive question answering and generation tasks. Extensive experimental results show that EDMem outperforms both memory-based auto-encoder models and non-memory encoder-decoder models.",
    "metadata": {
      "arxiv_id": "2210.03273",
      "title": "A Unified Encoder-Decoder Framework with Entity Memory",
      "summary": "Entities, as important carriers of real-world knowledge, play a key role in many NLP tasks. We focus on incorporating entity knowledge into an encoder-decoder framework for informative text generation. Existing approaches tried to index, retrieve, and read external documents as evidence, but they suffered from a large computational overhead. In this work, we propose an encoder-decoder framework with an entity memory, namely EDMem. The entity knowledge is stored in the memory as latent representations, and the memory is pre-trained on Wikipedia along with encoder-decoder parameters. To precisely generate entity names, we design three decoding methods to constrain entity generation by linking entities in the memory. EDMem is a unified framework that can be used on various entity-intensive question answering and generation tasks. Extensive experimental results show that EDMem outperforms both memory-based auto-encoder models and non-memory encoder-decoder models.",
      "authors": [
        "Zhihan Zhang",
        "Wenhao Yu",
        "Chenguang Zhu",
        "Meng Jiang"
      ],
      "published": "2022-10-07T01:15:30Z",
      "updated": "2023-04-24T01:54:08Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.03273v3",
      "landing_url": "https://arxiv.org/abs/2210.03273v3",
      "doi": "https://doi.org/10.48550/arXiv.2210.03273"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on an encoder-decoder framework with entity memory for NLP text generation and does not involve discrete audio tokens or audio tokenization designs; it fails the inclusion criteria focused on discrete audio token representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on an encoder-decoder framework with entity memory for NLP text generation and does not involve discrete audio tokens or audio tokenization designs; it fails the inclusion criteria focused on discrete audio token representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an encoder-decoder framework incorporating entity memory for NLP tasks, specifically for entity knowledge and text generation; it does not discuss or contribute to discrete audio tokens, audio vector quantization, or related audio language modeling topics specified in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an encoder-decoder framework incorporating entity memory for NLP tasks, specifically for entity knowledge and text generation; it does not discuss or contribute to discrete audio tokens, audio vector quantization, or related audio language modeling topics specified in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "JukeDrummer: Conditional Beat-aware Audio-domain Drum Accompaniment Generation via Transformer VQ-VAE",
    "abstract": "This paper proposes a model that generates a drum track in the audio domain to play along to a user-provided drum-free recording. Specifically, using paired data of drumless tracks and the corresponding human-made drum tracks, we train a Transformer model to improvise the drum part of an unseen drumless recording. We combine two approaches to encode the input audio. First, we train a vector-quantized variational autoencoder (VQ-VAE) to represent the input audio with discrete codes, which can then be readily used in a Transformer. Second, using an audio-domain beat tracking model, we compute beat-related features of the input audio and use them as embeddings in the Transformer. Instead of generating the drum track directly as waveforms, we use a separate VQ-VAE to encode the mel-spectrogram of a drum track into another set of discrete codes, and train the Transformer to predict the sequence of drum-related discrete codes. The output codes are then converted to a mel-spectrogram with a decoder, and then to the waveform with a vocoder. We report both objective and subjective evaluations of variants of the proposed model, demonstrating that the model with beat information generates drum accompaniment that is rhythmically and stylistically consistent with the input audio.",
    "metadata": {
      "arxiv_id": "2210.06007",
      "title": "JukeDrummer: Conditional Beat-aware Audio-domain Drum Accompaniment Generation via Transformer VQ-VAE",
      "summary": "This paper proposes a model that generates a drum track in the audio domain to play along to a user-provided drum-free recording. Specifically, using paired data of drumless tracks and the corresponding human-made drum tracks, we train a Transformer model to improvise the drum part of an unseen drumless recording. We combine two approaches to encode the input audio. First, we train a vector-quantized variational autoencoder (VQ-VAE) to represent the input audio with discrete codes, which can then be readily used in a Transformer. Second, using an audio-domain beat tracking model, we compute beat-related features of the input audio and use them as embeddings in the Transformer. Instead of generating the drum track directly as waveforms, we use a separate VQ-VAE to encode the mel-spectrogram of a drum track into another set of discrete codes, and train the Transformer to predict the sequence of drum-related discrete codes. The output codes are then converted to a mel-spectrogram with a decoder, and then to the waveform with a vocoder. We report both objective and subjective evaluations of variants of the proposed model, demonstrating that the model with beat information generates drum accompaniment that is rhythmically and stylistically consistent with the input audio.",
      "authors": [
        "Yueh-Kao Wu",
        "Ching-Yu Chiu",
        "Yi-Hsuan Yang"
      ],
      "published": "2022-10-12T08:23:20Z",
      "updated": "2022-10-31T08:54:08Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.06007v2",
      "landing_url": "https://arxiv.org/abs/2210.06007v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.06007"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper uses VQ-VAE to produce discrete audio tokens for both input audio and drum tracks and trains a Transformer to model sequences of these tokens, aligning with the discrete audio token research theme (though the primary contribution is drum accompaniment generation).",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The paper uses VQ-VAE to produce discrete audio tokens for both input audio and drum tracks and trains a Transformer to model sequences of these tokens, aligning with the discrete audio token research theme (though the primary contribution is drum accompaniment generation).",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a method utilizing VQ-VAE to encode both input and output audio into discrete tokens for sequence modeling by a Transformer, clearly describing the token generation process and their use in audio generation, thus meeting the inclusion criteria centered around discrete audio tokens derived from vector quantization and used for sequence modeling. It also provides enough methodological detail and evaluations, and does not fall into any exclusion criteria such as lacking method details or only presenting demos.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a method utilizing VQ-VAE to encode both input and output audio into discrete tokens for sequence modeling by a Transformer, clearly describing the token generation process and their use in audio generation, thus meeting the inclusion criteria centered around discrete audio tokens derived from vector quantization and used for sequence modeling. It also provides enough methodological detail and evaluations, and does not fall into any exclusion criteria such as lacking method details or only presenting demos.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Experiments on Turkish ASR with Self-Supervised Speech Representation Learning",
    "abstract": "While the Turkish language is listed among low-resource languages, literature on Turkish automatic speech recognition (ASR) is relatively old. In this report, we present our findings on Turkish ASR with speech representation learning using HUBERT. We investigate pre-training HUBERT for Turkish with large-scale data curated from online resources. We pre-train our model using 6,500 hours of speech data from YouTube. The results show that the models are not ready for commercial use since they are not robust against disturbances that typically occur in real-world settings such as variations in accents, slang, background noise and interference. We analyze typical errors and the limitations of the models for use in commercial settings.",
    "metadata": {
      "arxiv_id": "2210.07323",
      "title": "Experiments on Turkish ASR with Self-Supervised Speech Representation Learning",
      "summary": "While the Turkish language is listed among low-resource languages, literature on Turkish automatic speech recognition (ASR) is relatively old. In this report, we present our findings on Turkish ASR with speech representation learning using HUBERT. We investigate pre-training HUBERT for Turkish with large-scale data curated from online resources. We pre-train our model using 6,500 hours of speech data from YouTube. The results show that the models are not ready for commercial use since they are not robust against disturbances that typically occur in real-world settings such as variations in accents, slang, background noise and interference. We analyze typical errors and the limitations of the models for use in commercial settings.",
      "authors": [
        "Ali Safaya",
        "Engin Erzin"
      ],
      "published": "2022-10-13T19:46:39Z",
      "updated": "2022-12-23T11:11:03Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.07323v3",
      "landing_url": "https://arxiv.org/abs/2210.07323v3",
      "doi": "https://doi.org/10.48550/arXiv.2210.07323"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on Turkish ASR using self-supervised learning (HuBERT) and does not present/discuss discrete audio token representations, vector quantization, or token design essential to the inclusion criteria; it fails to meet the core criterion about discrete audio tokens and their evaluation/setup.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on Turkish ASR using self-supervised learning (HuBERT) and does not present/discuss discrete audio token representations, vector quantization, or token design essential to the inclusion criteria; it fails to meet the core criterion about discrete audio tokens and their evaluation/setup.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Turkish ASR with self-supervised speech representation learning using HUBERT, but does not explicitly mention the generation or use of discrete audio tokens through vector quantization or similar quantization-based tokenization methods. It primarily discusses ASR model performance and robustness rather than detailing tokenization methods relevant to discrete audio tokens. Therefore, it lacks the core contribution of discrete token design and usage as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Turkish ASR with self-supervised speech representation learning using HUBERT, but does not explicitly mention the generation or use of discrete audio tokens through vector quantization or similar quantization-based tokenization methods. It primarily discusses ASR model performance and robustness rather than detailing tokenization methods relevant to discrete audio tokens. Therefore, it lacks the core contribution of discrete token design and usage as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Disentangled Speech Representation Learning for One-Shot Cross-lingual Voice Conversion Using $β$-VAE",
    "abstract": "We propose an unsupervised learning method to disentangle speech into content representation and speaker identity representation. We apply this method to the challenging one-shot cross-lingual voice conversion task to demonstrate the effectiveness of the disentanglement. Inspired by $β$-VAE, we introduce a learning objective that balances between the information captured by the content and speaker representations. In addition, the inductive biases from the architectural design and the training dataset further encourage the desired disentanglement. Both objective and subjective evaluations show the effectiveness of the proposed method in speech disentanglement and in one-shot cross-lingual voice conversion.",
    "metadata": {
      "arxiv_id": "2210.13771",
      "title": "Disentangled Speech Representation Learning for One-Shot Cross-lingual Voice Conversion Using $β$-VAE",
      "summary": "We propose an unsupervised learning method to disentangle speech into content representation and speaker identity representation. We apply this method to the challenging one-shot cross-lingual voice conversion task to demonstrate the effectiveness of the disentanglement. Inspired by $β$-VAE, we introduce a learning objective that balances between the information captured by the content and speaker representations. In addition, the inductive biases from the architectural design and the training dataset further encourage the desired disentanglement. Both objective and subjective evaluations show the effectiveness of the proposed method in speech disentanglement and in one-shot cross-lingual voice conversion.",
      "authors": [
        "Hui Lu",
        "Disong Wang",
        "Xixin Wu",
        "Zhiyong Wu",
        "Xunying Liu",
        "Helen Meng"
      ],
      "published": "2022-10-25T05:12:47Z",
      "updated": "2022-10-25T05:12:47Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.13771v1",
      "landing_url": "https://arxiv.org/abs/2210.13771v1",
      "doi": null
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on disentangled continuous speech representations for cross-lingual voice conversion using β-VAE and does not address discrete audio tokens, vector quantization, or token-based modeling required by the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on disentangled continuous speech representations for cross-lingual voice conversion using β-VAE and does not address discrete audio tokens, vector quantization, or token-based modeling required by the criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on disentangled speech representation learning using $β$-VAE for voice conversion, but it does not explicitly describe the generation or use of discrete audio tokens via vector quantization or similar quantization mechanisms, nor does it clarify the token formation process or usage in sequence/language models, which are critical for the inclusion criteria focused on discrete audio tokens.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on disentangled speech representation learning using $β$-VAE for voice conversion, but it does not explicitly describe the generation or use of discrete audio tokens via vector quantization or similar quantization mechanisms, nor does it clarify the token formation process or usage in sequence/language models, which are critical for the inclusion criteria focused on discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Improving Speech Representation Learning via Speech-level and Phoneme-level Masking Approach",
    "abstract": "Recovering the masked speech frames is widely applied in speech representation learning. However, most of these models use random masking in the pre-training. In this work, we proposed two kinds of masking approaches: (1) speech-level masking, making the model to mask more speech segments than silence segments, (2) phoneme-level masking, forcing the model to mask the whole frames of the phoneme, instead of phoneme pieces. We pre-trained the model via these two approaches, and evaluated on two downstream tasks, phoneme classification and speaker recognition. The experiments demonstrated that the proposed masking approaches are beneficial to improve the performance of speech representation.",
    "metadata": {
      "arxiv_id": "2210.13805",
      "title": "Improving Speech Representation Learning via Speech-level and Phoneme-level Masking Approach",
      "summary": "Recovering the masked speech frames is widely applied in speech representation learning. However, most of these models use random masking in the pre-training. In this work, we proposed two kinds of masking approaches: (1) speech-level masking, making the model to mask more speech segments than silence segments, (2) phoneme-level masking, forcing the model to mask the whole frames of the phoneme, instead of phoneme pieces. We pre-trained the model via these two approaches, and evaluated on two downstream tasks, phoneme classification and speaker recognition. The experiments demonstrated that the proposed masking approaches are beneficial to improve the performance of speech representation.",
      "authors": [
        "Xulong Zhang",
        "Jianzong Wang",
        "Ning Cheng",
        "Kexin Zhu",
        "Jing Xiao"
      ],
      "published": "2022-10-25T07:26:47Z",
      "updated": "2022-10-25T07:26:47Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.13805v1",
      "landing_url": "https://arxiv.org/abs/2210.13805v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.13805"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本文聚焦於在語音表示學習中使用語音級與音素級 masking 的自監督預訓練策略，未提出與產生/量化或用於序列模型的離散音訊 token 相關的設計（如 VQ/RVQ、層級量化、語意驅動的離散化機制等），因此不符合「離散音訊 token」主題的核心研究貢獻與方法細節要求。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本文聚焦於在語音表示學習中使用語音級與音素級 masking 的自監督預訓練策略，未提出與產生/量化或用於序列模型的離散音訊 token 相關的設計（如 VQ/RVQ、層級量化、語意驅動的離散化機制等），因此不符合「離散音訊 token」主題的核心研究貢獻與方法細節要求。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving speech representation learning via different masking strategies but does not mention the use or production of discrete audio tokens through quantization or discretization mechanisms as defined in the inclusion criteria, nor does it describe token generation processes or use tokens in sequence/language modeling for audio understanding or generation. Therefore, it does not meet the core requirements related to discrete audio tokens in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving speech representation learning via different masking strategies but does not mention the use or production of discrete audio tokens through quantization or discretization mechanisms as defined in the inclusion criteria, nor does it describe token generation processes or use tokens in sequence/language modeling for audio understanding or generation. Therefore, it does not meet the core requirements related to discrete audio tokens in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Application of Knowledge Distillation to Multi-task Speech Representation Learning",
    "abstract": "Model architectures such as wav2vec 2.0 and HuBERT have been proposed to learn speech representations from audio waveforms in a self-supervised manner. When they are combined with downstream tasks such as keyword spotting and speaker verification, they provide state-of-the-art performance. However, these models use a large number of parameters, the smallest version of which has 95 million parameters. This constitutes a challenge for edge AI device deployments. In this paper, we investigate the application of knowledge distillation to speech representation learning (SRL) models followed by joint fine-tuning with multiple downstream voice-activated tasks. In our experiments on two such tasks, our approach results in nearly 75% reduction in model size while suffering only 0.1% accuracy and 0.9% equal error rate degradation compared to the full-size model. In addition, we show that fine-tuning the SRL models results in a significant performance boost compared to using frozen SRL models.",
    "metadata": {
      "arxiv_id": "2210.16611",
      "title": "Application of Knowledge Distillation to Multi-task Speech Representation Learning",
      "summary": "Model architectures such as wav2vec 2.0 and HuBERT have been proposed to learn speech representations from audio waveforms in a self-supervised manner. When they are combined with downstream tasks such as keyword spotting and speaker verification, they provide state-of-the-art performance. However, these models use a large number of parameters, the smallest version of which has 95 million parameters. This constitutes a challenge for edge AI device deployments. In this paper, we investigate the application of knowledge distillation to speech representation learning (SRL) models followed by joint fine-tuning with multiple downstream voice-activated tasks. In our experiments on two such tasks, our approach results in nearly 75% reduction in model size while suffering only 0.1% accuracy and 0.9% equal error rate degradation compared to the full-size model. In addition, we show that fine-tuning the SRL models results in a significant performance boost compared to using frozen SRL models.",
      "authors": [
        "Mine Kerpicci",
        "Van Nguyen",
        "Shuhua Zhang",
        "Erik Visser"
      ],
      "published": "2022-10-29T14:22:43Z",
      "updated": "2023-05-19T17:16:53Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.16611v2",
      "landing_url": "https://arxiv.org/abs/2210.16611v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.16611"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses knowledge distillation for speech representation learning and multi-task fine-tuning, but it does not mention discrete audio tokens, vector quantization, or tokenization mechanisms, so it does not meet the inclusion criterion focused on discrete audio token design for sequence models; hence it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses knowledge distillation for speech representation learning and multi-task fine-tuning, but it does not mention discrete audio tokens, vector quantization, or tokenization mechanisms, so it does not meet the inclusion criterion focused on discrete audio token design for sequence models; hence it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on applying knowledge distillation to speech representation learning models for multi-task voice-activated tasks, emphasizing model size reduction and performance retention. However, it does not describe generating discrete audio tokens through quantization or similar mechanisms as a core method, nor does it clarify any token generation process or use of discrete tokens for sequence modeling. Therefore, it does not meet the inclusion criteria centered on discrete audio token design and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on applying knowledge distillation to speech representation learning models for multi-task voice-activated tasks, emphasizing model size reduction and performance retention. However, it does not describe generating discrete audio tokens through quantization or similar mechanisms as a core method, nor does it clarify any token generation process or use of discrete tokens for sequence modeling. Therefore, it does not meet the inclusion criteria centered on discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired Speech and Text",
    "abstract": "Self-supervised pre-training has been successful in both text and speech processing. Speech and text offer different but complementary information. The question is whether we are able to perform a speech-text joint pre-training on unpaired speech and text. In this paper, we take the idea of self-supervised pre-training one step further and propose token2vec, a novel joint pre-training framework for unpaired speech and text based on discrete representations of speech. Firstly, due to the distinct characteristics between speech and text modalities, where speech is continuous while text is discrete, we first discretize speech into a sequence of discrete speech tokens to solve the modality mismatch problem. Secondly, to solve the length mismatch problem, where the speech sequence is usually much longer than text sequence, we convert the words of text into phoneme sequences and randomly repeat each phoneme in the sequences. Finally, we feed the discrete speech and text tokens into a modality-agnostic Transformer encoder and pre-train with token-level masking language modeling (tMLM). Experiments show that token2vec is significantly superior to various speech-only pre-training baselines, with up to 17.7% relative WER reduction. Token2vec model is also validated on a non-ASR task, i.e., spoken intent classification, and shows good transferability.",
    "metadata": {
      "arxiv_id": "2210.16755",
      "title": "token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired Speech and Text",
      "summary": "Self-supervised pre-training has been successful in both text and speech processing. Speech and text offer different but complementary information. The question is whether we are able to perform a speech-text joint pre-training on unpaired speech and text. In this paper, we take the idea of self-supervised pre-training one step further and propose token2vec, a novel joint pre-training framework for unpaired speech and text based on discrete representations of speech. Firstly, due to the distinct characteristics between speech and text modalities, where speech is continuous while text is discrete, we first discretize speech into a sequence of discrete speech tokens to solve the modality mismatch problem. Secondly, to solve the length mismatch problem, where the speech sequence is usually much longer than text sequence, we convert the words of text into phoneme sequences and randomly repeat each phoneme in the sequences. Finally, we feed the discrete speech and text tokens into a modality-agnostic Transformer encoder and pre-train with token-level masking language modeling (tMLM). Experiments show that token2vec is significantly superior to various speech-only pre-training baselines, with up to 17.7% relative WER reduction. Token2vec model is also validated on a non-ASR task, i.e., spoken intent classification, and shows good transferability.",
      "authors": [
        "Xianghu Yue",
        "Junyi Ao",
        "Xiaoxue Gao",
        "Haizhou Li"
      ],
      "published": "2022-10-30T06:38:19Z",
      "updated": "2022-10-30T06:38:19Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.16755v1",
      "landing_url": "https://arxiv.org/abs/2210.16755v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.16755"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "雖然摘要提到將語音離散為離散 tokens 以進行跨模態自監督預訓練，但未給出 token 生成機制的具體細節（如 VQ/RVQ、碼本設計、層級量化等），且僅有摘要未提供可重現的方法描述與完整英文全文，難以證實符合離散音訊 token 主研究貢獻與可比較實驗的要求；同時也未提供明確的出版日期信息，屬於較高的可排除風險。",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "雖然摘要提到將語音離散為離散 tokens 以進行跨模態自監督預訓練，但未給出 token 生成機制的具體細節（如 VQ/RVQ、碼本設計、層級量化等），且僅有摘要未提供可重現的方法描述與完整英文全文，難以證實符合離散音訊 token 主研究貢獻與可比較實驗的要求；同時也未提供明確的出版日期信息，屬於較高的可排除風險。",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes token2vec, a joint self-supervised pre-training framework that discretizes continuous speech into discrete speech tokens using a modality-agnostic Transformer encoder for speech and text modalities. The process addresses modality mismatch by converting speech into discrete tokens and text into phoneme sequences, which are then modeled in a unified framework. This clearly involves discrete audio tokens as a core method, and the abstract details the token generation and usage for sequence modeling tasks such as ASR and spoken intent classification. Experimental results indicate effectiveness, supporting reproducibility and method validity. Therefore, it aligns well with the inclusion criteria and avoids exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes token2vec, a joint self-supervised pre-training framework that discretizes continuous speech into discrete speech tokens using a modality-agnostic Transformer encoder for speech and text modalities. The process addresses modality mismatch by converting speech into discrete tokens and text into phoneme sequences, which are then modeled in a unified framework. This clearly involves discrete audio tokens as a core method, and the abstract details the token generation and usage for sequence modeling tasks such as ASR and spoken intent classification. Experimental results indicate effectiveness, supporting reproducibility and method validity. Therefore, it aligns well with the inclusion criteria and avoids exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "Integrating the two junior reviews: the abstract clearly states a joint speech-text pretraining that discretizes speech into discrete tokens and uses a modality-agnostic Transformer with token-level MLM (supporting the relevance to discrete audio tokens), but it does not describe how those tokens are produced (no mention of VQ/RVQ/quantization, codebook or clustering specifics) and we are not provided with the full English paper or publication date to verify reproducibility and method details required by the inclusion criteria—therefore it fails the requirement to provide clear token-generation methodology and verifiable publication metadata. Score explanation: although the topic aligns, missing methodological and provenance details mean it is better to exclude.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Integrating the two junior reviews: the abstract clearly states a joint speech-text pretraining that discretizes speech into discrete tokens and uses a modality-agnostic Transformer with token-level MLM (supporting the relevance to discrete audio tokens), but it does not describe how those tokens are produced (no mention of VQ/RVQ/quantization, codebook or clustering specifics) and we are not provided with the full English paper or publication date to verify reproducibility and method details required by the inclusion criteria—therefore it fails the requirement to provide clear token-generation methodology and verifiable publication metadata. Score explanation: although the topic aligns, missing methodological and provenance details mean it is better to exclude.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "DUEL: Adaptive Duplicate Elimination on Working Memory for Self-Supervised Learning",
    "abstract": "In Self-Supervised Learning (SSL), it is known that frequent occurrences of the collision in which target data and its negative samples share the same class can decrease performance. Especially in real-world data such as crawled data or robot-gathered observations, collisions may occur more often due to the duplicates in the data. To deal with this problem, we claim that sampling negative samples from the adaptively debiased distribution in the memory makes the model more stable than sampling from a biased dataset directly. In this paper, we introduce a novel SSL framework with adaptive Duplicate Elimination (DUEL) inspired by the human working memory. The proposed framework successfully prevents the downstream task performance from degradation due to a dramatic inter-class imbalance.",
    "metadata": {
      "arxiv_id": "2210.17052",
      "title": "DUEL: Adaptive Duplicate Elimination on Working Memory for Self-Supervised Learning",
      "summary": "In Self-Supervised Learning (SSL), it is known that frequent occurrences of the collision in which target data and its negative samples share the same class can decrease performance. Especially in real-world data such as crawled data or robot-gathered observations, collisions may occur more often due to the duplicates in the data. To deal with this problem, we claim that sampling negative samples from the adaptively debiased distribution in the memory makes the model more stable than sampling from a biased dataset directly. In this paper, we introduce a novel SSL framework with adaptive Duplicate Elimination (DUEL) inspired by the human working memory. The proposed framework successfully prevents the downstream task performance from degradation due to a dramatic inter-class imbalance.",
      "authors": [
        "Won-Seok Choi",
        "Dong-Sig Han",
        "Hyundo Lee",
        "Junseok Park",
        "Byoung-Tak Zhang"
      ],
      "published": "2022-10-31T04:04:48Z",
      "updated": "2022-10-31T04:04:48Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.17052v1",
      "landing_url": "https://arxiv.org/abs/2210.17052v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.17052"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract discuss adaptive duplicate elimination in self-supervised learning, unrelated to discrete audio tokens or tokenization; thus it does not meet the Discrete Audio Tokens inclusion criteria (no discrete audio token design, VQ/RVQ, or token-based audio representations).",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract discuss adaptive duplicate elimination in self-supervised learning, unrelated to discrete audio tokens or tokenization; thus it does not meet the Discrete Audio Tokens inclusion criteria (no discrete audio token design, VQ/RVQ, or token-based audio representations).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a self-supervised learning framework addressing duplicate elimination in working memory, without any mention of discrete audio tokens, tokenization processes, vector quantization, or audio language modeling; thus, it does not meet the inclusion criteria related to discrete audio tokens and their core methodologies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a self-supervised learning framework addressing duplicate elimination in working memory, without any mention of discrete audio tokens, tokenization processes, vector quantization, or audio language modeling; thus, it does not meet the inclusion criteria related to discrete audio tokens and their core methodologies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Contrastive Self-Supervised Learning for Skeleton Representations",
    "abstract": "Human skeleton point clouds are commonly used to automatically classify and predict the behaviour of others. In this paper, we use a contrastive self-supervised learning method, SimCLR, to learn representations that capture the semantics of skeleton point clouds. This work focuses on systematically evaluating the effects that different algorithmic decisions (including augmentations, dataset partitioning and backbone architecture) have on the learned skeleton representations. To pre-train the representations, we normalise six existing datasets to obtain more than 40 million skeleton frames. We evaluate the quality of the learned representations with three downstream tasks: skeleton reconstruction, motion prediction, and activity classification. Our results demonstrate the importance of 1) combining spatial and temporal augmentations, 2) including additional datasets for encoder training, and 3) and using a graph neural network as an encoder.",
    "metadata": {
      "arxiv_id": "2211.05304",
      "title": "Contrastive Self-Supervised Learning for Skeleton Representations",
      "summary": "Human skeleton point clouds are commonly used to automatically classify and predict the behaviour of others. In this paper, we use a contrastive self-supervised learning method, SimCLR, to learn representations that capture the semantics of skeleton point clouds. This work focuses on systematically evaluating the effects that different algorithmic decisions (including augmentations, dataset partitioning and backbone architecture) have on the learned skeleton representations. To pre-train the representations, we normalise six existing datasets to obtain more than 40 million skeleton frames. We evaluate the quality of the learned representations with three downstream tasks: skeleton reconstruction, motion prediction, and activity classification. Our results demonstrate the importance of 1) combining spatial and temporal augmentations, 2) including additional datasets for encoder training, and 3) and using a graph neural network as an encoder.",
      "authors": [
        "Nico Lingg",
        "Miguel Sarabia",
        "Luca Zappella",
        "Barry-John Theobald"
      ],
      "published": "2022-11-10T02:45:36Z",
      "updated": "2022-11-10T02:45:36Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.05304v1",
      "landing_url": "https://arxiv.org/abs/2211.05304v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.05304"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work is about skeleton point cloud representations using self-supervised learning, not discrete audio tokens, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work is about skeleton point cloud representations using self-supervised learning, not discrete audio tokens, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on contrastive self-supervised learning for skeleton representations and does not involve discrete audio tokens, vector quantization, or any audio-related discretization methods that meet the inclusion criteria. Hence, it does not meet the topic requirement regarding discrete audio token generation and usage for language models or sequence models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on contrastive self-supervised learning for skeleton representations and does not involve discrete audio tokens, vector quantization, or any audio-related discretization methods that meet the inclusion criteria. Hence, it does not meet the topic requirement regarding discrete audio token generation and usage for language models or sequence models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Homomorphic Self-Supervised Learning",
    "abstract": "In this work, we observe that many existing self-supervised learning algorithms can be both unified and generalized when seen through the lens of equivariant representations. Specifically, we introduce a general framework we call Homomorphic Self-Supervised Learning, and theoretically show how it may subsume the use of input-augmentations provided an augmentation-homomorphic feature extractor. We validate this theory experimentally for simple augmentations, demonstrate how the framework fails when representational structure is removed, and further empirically explore how the parameters of this framework relate to those of traditional augmentation-based self-supervised learning. We conclude with a discussion of the potential benefits afforded by this new perspective on self-supervised learning.",
    "metadata": {
      "arxiv_id": "2211.08282",
      "title": "Homomorphic Self-Supervised Learning",
      "summary": "In this work, we observe that many existing self-supervised learning algorithms can be both unified and generalized when seen through the lens of equivariant representations. Specifically, we introduce a general framework we call Homomorphic Self-Supervised Learning, and theoretically show how it may subsume the use of input-augmentations provided an augmentation-homomorphic feature extractor. We validate this theory experimentally for simple augmentations, demonstrate how the framework fails when representational structure is removed, and further empirically explore how the parameters of this framework relate to those of traditional augmentation-based self-supervised learning. We conclude with a discussion of the potential benefits afforded by this new perspective on self-supervised learning.",
      "authors": [
        "T. Anderson Keller",
        "Xavier Suau",
        "Luca Zappella"
      ],
      "published": "2022-11-15T16:32:36Z",
      "updated": "2022-11-15T16:32:36Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.08282v1",
      "landing_url": "https://arxiv.org/abs/2211.08282v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.08282"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not address discrete audio token representations or token-based audio modeling; the title/abstract discuss a general homomorphic self-supervised learning framework for SSL, not discrete audio tokens or token generation/quantization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Does not address discrete audio token representations or token-based audio modeling; the title/abstract discuss a general homomorphic self-supervised learning framework for SSL, not discrete audio tokens or token generation/quantization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a theoretical framework for homomorphic self-supervised learning related to equivariant representations and input augmentations, without mentioning any discrete audio token generation process, vector quantization, or language/sequence modeling for continuous audio signals; thus, it does not meet the inclusion criteria centered around discrete audio token design or representation for downstream audio language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a theoretical framework for homomorphic self-supervised learning related to equivariant representations and input augmentations, without mentioning any discrete audio token generation process, vector quantization, or language/sequence modeling for continuous audio signals; thus, it does not meet the inclusion criteria centered around discrete audio token design or representation for downstream audio language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MelHuBERT: A simplified HuBERT on Mel spectrograms",
    "abstract": "Self-supervised models have had great success in learning speech representations that can generalize to various downstream tasks. However, most self-supervised models require a large amount of compute and multiple GPUs to train, significantly hampering the development of self-supervised learning. In an attempt to reduce the computation of training, we revisit the training of HuBERT, a highly successful self-supervised model. We improve and simplify several key components, including the loss function, input representation, and training in multiple stages. Our model, MelHuBERT, is able to achieve favorable performance on phone recognition, speaker identification, and automatic speech recognition against HuBERT, while saving 31.2% of the pre-training time, or equivalently 33.5% MACs per one second speech. The code and pre-trained models are available in https://github.com/nervjack2/MelHuBERT.",
    "metadata": {
      "arxiv_id": "2211.09944",
      "title": "MelHuBERT: A simplified HuBERT on Mel spectrograms",
      "summary": "Self-supervised models have had great success in learning speech representations that can generalize to various downstream tasks. However, most self-supervised models require a large amount of compute and multiple GPUs to train, significantly hampering the development of self-supervised learning. In an attempt to reduce the computation of training, we revisit the training of HuBERT, a highly successful self-supervised model. We improve and simplify several key components, including the loss function, input representation, and training in multiple stages. Our model, MelHuBERT, is able to achieve favorable performance on phone recognition, speaker identification, and automatic speech recognition against HuBERT, while saving 31.2% of the pre-training time, or equivalently 33.5% MACs per one second speech. The code and pre-trained models are available in https://github.com/nervjack2/MelHuBERT.",
      "authors": [
        "Tzu-Quan Lin",
        "Hung-yi Lee",
        "Hao Tang"
      ],
      "published": "2022-11-17T23:38:29Z",
      "updated": "2024-08-29T19:25:59Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.09944v3",
      "landing_url": "https://arxiv.org/abs/2211.09944v3",
      "doi": "https://doi.org/10.48550/arXiv.2211.09944"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "MelHuBERT is a simplified HuBERT model on Mel spectrograms and focuses on SSL improvements rather than introducing discretized audio tokens (e.g., VQ/RVQ, hierarchical quantization) for token-based modeling, so it does not meet the Discrete Audio Tokens inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "MelHuBERT is a simplified HuBERT model on Mel spectrograms and focuses on SSL improvements rather than introducing discretized audio tokens (e.g., VQ/RVQ, hierarchical quantization) for token-based modeling, so it does not meet the Discrete Audio Tokens inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on MelHuBERT, a simplified self-supervised model for speech representation learning that improves efficiency and performs well on downstream tasks. However, the abstract does not mention the generation of discrete audio tokens through vector quantization or related discrete token mechanisms, nor does it describe discrete token formation or usage for sequence modeling. It emphasizes continuous speech representations rather than discrete tokenization processes, which does not meet the inclusion criteria for research contributions centered on discrete audio token design or generation.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The article focuses on MelHuBERT, a simplified self-supervised model for speech representation learning that improves efficiency and performs well on downstream tasks. However, the abstract does not mention the generation of discrete audio tokens through vector quantization or related discrete token mechanisms, nor does it describe discrete token formation or usage for sequence modeling. It emphasizes continuous speech representations rather than discrete tokenization processes, which does not meet the inclusion criteria for research contributions centered on discrete audio token design or generation.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Exploring WavLM on Speech Enhancement",
    "abstract": "There is a surge in interest in self-supervised learning approaches for end-to-end speech encoding in recent years as they have achieved great success. Especially, WavLM showed state-of-the-art performance on various speech processing tasks. To better understand the efficacy of self-supervised learning models for speech enhancement, in this work, we design and conduct a series of experiments with three resource conditions by combining WavLM and two high-quality speech enhancement systems. Also, we propose a regression-based WavLM training objective and a noise-mixing data configuration to further boost the downstream enhancement performance. The experiments on the DNS challenge dataset and a simulation dataset show that the WavLM benefits the speech enhancement task in terms of both speech quality and speech recognition accuracy, especially for low fine-tuning resources. For the high fine-tuning resource condition, only the word error rate is substantially improved.",
    "metadata": {
      "arxiv_id": "2211.09988",
      "title": "Exploring WavLM on Speech Enhancement",
      "summary": "There is a surge in interest in self-supervised learning approaches for end-to-end speech encoding in recent years as they have achieved great success. Especially, WavLM showed state-of-the-art performance on various speech processing tasks. To better understand the efficacy of self-supervised learning models for speech enhancement, in this work, we design and conduct a series of experiments with three resource conditions by combining WavLM and two high-quality speech enhancement systems. Also, we propose a regression-based WavLM training objective and a noise-mixing data configuration to further boost the downstream enhancement performance. The experiments on the DNS challenge dataset and a simulation dataset show that the WavLM benefits the speech enhancement task in terms of both speech quality and speech recognition accuracy, especially for low fine-tuning resources. For the high fine-tuning resource condition, only the word error rate is substantially improved.",
      "authors": [
        "Hyungchan Song",
        "Sanyuan Chen",
        "Zhuo Chen",
        "Yu Wu",
        "Takuya Yoshioka",
        "Min Tang",
        "Jong Won Shin",
        "Shujie Liu"
      ],
      "published": "2022-11-18T02:23:16Z",
      "updated": "2022-11-18T02:23:16Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.09988v1",
      "landing_url": "https://arxiv.org/abs/2211.09988v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.09988"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract focus on WavLM-based speech enhancement and SSL objectives rather than discrete audio token representations or tokenization methods, so it does not meet the token-centric inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract focus on WavLM-based speech enhancement and SSL objectives rather than discrete audio token representations or tokenization methods, so it does not meet the token-centric inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The reviewed item focuses on leveraging WavLM, a self-supervised learning model, for speech enhancement by proposing training objectives and noise-mixing configurations; however, it does not discuss the generation or use of discrete audio tokens through vector quantization or other discrete tokenization mechanisms, nor does it provide methodological details about token formation relevant to audio language modeling or sequence modeling as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The reviewed item focuses on leveraging WavLM, a self-supervised learning model, for speech enhancement by proposing training objectives and noise-mixing configurations; however, it does not discuss the generation or use of discrete audio tokens through vector quantization or other discrete tokenization mechanisms, nor does it provide methodological details about token formation relevant to audio language modeling or sequence modeling as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Homology-constrained vector quantization entropy regularizer",
    "abstract": "This paper describes an entropy regularization term for vector quantization (VQ) based on the analysis of persistent homology of the VQ embeddings. Higher embedding entropy positively correlates with higher codebook utilization, mitigating overfit towards the identity and codebook collapse in VQ-based autoencoders [1]. We show that homology-constrained regularization is an effective way to increase entropy of the VQ process (approximated to input entropy) while preserving the approximated topology in the quantized latent space, averaged over mini batches. This work further explores some patterns of persistent homology diagrams of latents formed by vector quantization. We implement and test the proposed algorithm as a module integrated into a sample VQ-VAE. Linked code repository provides a functioning implementation of the proposed architecture, referred to as homology-constrained vector quantization (HC-VQ) further in this work.",
    "metadata": {
      "arxiv_id": "2211.14363",
      "title": "Homology-constrained vector quantization entropy regularizer",
      "summary": "This paper describes an entropy regularization term for vector quantization (VQ) based on the analysis of persistent homology of the VQ embeddings. Higher embedding entropy positively correlates with higher codebook utilization, mitigating overfit towards the identity and codebook collapse in VQ-based autoencoders [1]. We show that homology-constrained regularization is an effective way to increase entropy of the VQ process (approximated to input entropy) while preserving the approximated topology in the quantized latent space, averaged over mini batches. This work further explores some patterns of persistent homology diagrams of latents formed by vector quantization. We implement and test the proposed algorithm as a module integrated into a sample VQ-VAE. Linked code repository provides a functioning implementation of the proposed architecture, referred to as homology-constrained vector quantization (HC-VQ) further in this work.",
      "authors": [
        "Ivan Volkov"
      ],
      "published": "2022-11-25T20:09:22Z",
      "updated": "2022-11-25T20:09:22Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.14363v1",
      "landing_url": "https://arxiv.org/abs/2211.14363v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.14363"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not meet inclusion criteria as it discusses a general entropy regularizer for vector quantization in VQ-VAEs without addressing discrete audio token design, and the source provides no publication/update date, triggering an exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Does not meet inclusion criteria as it discusses a general entropy regularizer for vector quantization in VQ-VAEs without addressing discrete audio token design, and the source provides no publication/update date, triggering an exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an entropy regularizer for vector quantization in embeddings within VQ-based autoencoders, exploring persistent homology patterns and codebook utilization. However, it does not explicitly address discrete audio tokens derived from continuous audio signals for use in audio language modeling or sequence modeling, nor does it clarify how the quantized outputs function as tokens in the context of speech, music, or general audio understanding or generation tasks. Therefore, it does not clearly meet the core inclusion criteria related to discrete audio token generation and usage.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an entropy regularizer for vector quantization in embeddings within VQ-based autoencoders, exploring persistent homology patterns and codebook utilization. However, it does not explicitly address discrete audio tokens derived from continuous audio signals for use in audio language modeling or sequence modeling, nor does it clarify how the quantized outputs function as tokens in the context of speech, music, or general audio understanding or generation tasks. Therefore, it does not clearly meet the core inclusion criteria related to discrete audio token generation and usage.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "On Word Error Rate Definitions and their Efficient Computation for Multi-Speaker Speech Recognition Systems",
    "abstract": "We propose a general framework to compute the word error rate (WER) of ASR systems that process recordings containing multiple speakers at their input and that produce multiple output word sequences (MIMO). Such ASR systems are typically required, e.g., for meeting transcription. We provide an efficient implementation based on a dynamic programming search in a multi-dimensional Levenshtein distance tensor under the constraint that a reference utterance must be matched consistently with one hypothesis output. This also results in an efficient implementation of the ORC WER which previously suffered from exponential complexity. We give an overview of commonly used WER definitions for multi-speaker scenarios and show that they are specializations of the above MIMO WER tuned to particular application scenarios. We conclude with a discussion of the pros and cons of the various WER definitions and a recommendation when to use which.",
    "metadata": {
      "arxiv_id": "2211.16112",
      "title": "On Word Error Rate Definitions and their Efficient Computation for Multi-Speaker Speech Recognition Systems",
      "summary": "We propose a general framework to compute the word error rate (WER) of ASR systems that process recordings containing multiple speakers at their input and that produce multiple output word sequences (MIMO). Such ASR systems are typically required, e.g., for meeting transcription. We provide an efficient implementation based on a dynamic programming search in a multi-dimensional Levenshtein distance tensor under the constraint that a reference utterance must be matched consistently with one hypothesis output. This also results in an efficient implementation of the ORC WER which previously suffered from exponential complexity. We give an overview of commonly used WER definitions for multi-speaker scenarios and show that they are specializations of the above MIMO WER tuned to particular application scenarios. We conclude with a discussion of the pros and cons of the various WER definitions and a recommendation when to use which.",
      "authors": [
        "Thilo von Neumann",
        "Christoph Boeddeker",
        "Keisuke Kinoshita",
        "Marc Delcroix",
        "Reinhold Haeb-Umbach"
      ],
      "published": "2022-11-29T11:35:13Z",
      "updated": "2023-07-21T07:28:19Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.16112v2",
      "landing_url": "https://arxiv.org/abs/2211.16112v2",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10094784"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses WER definitions and efficient computation for multi-speaker ASR and a DP-based approach; it does not address discrete audio token representations or tokenization methods, so it fails to meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses WER definitions and efficient computation for multi-speaker ASR and a DP-based approach; it does not address discrete audio token representations or tokenization methods, so it fails to meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper primarily focuses on definitions and computational methods for Word Error Rate (WER) in multi-speaker ASR systems rather than proposing or describing any discrete audio token generation or quantization process. It does not address the creation of discrete tokens or their application as representations for language or sequence modeling, which is central to the inclusion criteria. Therefore, it does not meet the core inclusion criteria related to discrete audio tokens nor does it provide relevant method descriptions or token generation processes.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper primarily focuses on definitions and computational methods for Word Error Rate (WER) in multi-speaker ASR systems rather than proposing or describing any discrete audio token generation or quantization process. It does not address the creation of discrete tokens or their application as representations for language or sequence modeling, which is central to the inclusion criteria. Therefore, it does not meet the core inclusion criteria related to discrete audio tokens nor does it provide relevant method descriptions or token generation processes.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models",
    "abstract": "The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at https://github.com/alsdudrla10/DG.",
    "metadata": {
      "arxiv_id": "2211.17091",
      "title": "Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models",
      "summary": "The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at https://github.com/alsdudrla10/DG.",
      "authors": [
        "Dongjun Kim",
        "Yeongmin Kim",
        "Se Jung Kwon",
        "Wanmo Kang",
        "Il-Chul Moon"
      ],
      "published": "2022-11-28T20:04:12Z",
      "updated": "2023-06-04T22:19:27Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.17091v4",
      "landing_url": "https://arxiv.org/abs/2211.17091v4",
      "doi": "https://doi.org/10.48550/arXiv.2211.17091"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper discusses discriminator guidance in score-based diffusion models for image generation, not discrete audio token design or token-based audio modeling, so it fails the inclusion criteria and aligns with no exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper discusses discriminator guidance in score-based diffusion models for image generation, not discrete audio token design or token-based audio modeling, so it fails the inclusion criteria and aligns with no exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on improving image generation using discriminator guidance in score-based diffusion models, with no mention of audio signals, discrete audio tokens, vector quantization, or any audio-related representations. Therefore, it does not meet the inclusion criteria related to discrete audio token generation or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on improving image generation using discriminator guidance in score-based diffusion models, with no mention of audio signals, discrete audio tokens, vector quantization, or any audio-related representations. Therefore, it does not meet the inclusion criteria related to discrete audio token generation or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Hierarchical Codebook Design for Near-Field MmWave MIMO Communications Systems",
    "abstract": "Communications system with analog or hybrid analog/digital architectures usually relies on a pre-defined codebook to perform beamforming. With the increase in the size of the antenna array, the characteristics of the spherical wavefront in the near-field situation are not negligible. Therefore, it is necessary to design a codebook that is adaptive to near-field scenarios. In this letter, we investigate the hierarchical codebook design method in the near-field situation. We develop a steering beam gain calculation method and design the lower-layer codebook to satisfy the coverage of the Fresnel region. For the upper-layer codebook, we propose beam rotation and beam relocation methods to place an arbitrary beam pattern at target locations. The simulation results show the superiority of the proposed near-field hierarchical codebook design.",
    "metadata": {
      "arxiv_id": "2212.07847",
      "title": "Hierarchical Codebook Design for Near-Field MmWave MIMO Communications Systems",
      "summary": "Communications system with analog or hybrid analog/digital architectures usually relies on a pre-defined codebook to perform beamforming. With the increase in the size of the antenna array, the characteristics of the spherical wavefront in the near-field situation are not negligible. Therefore, it is necessary to design a codebook that is adaptive to near-field scenarios. In this letter, we investigate the hierarchical codebook design method in the near-field situation. We develop a steering beam gain calculation method and design the lower-layer codebook to satisfy the coverage of the Fresnel region. For the upper-layer codebook, we propose beam rotation and beam relocation methods to place an arbitrary beam pattern at target locations. The simulation results show the superiority of the proposed near-field hierarchical codebook design.",
      "authors": [
        "Jiawei Chen",
        "Feifei Gao",
        "Mengnan Jian",
        "Wanmai Yuan"
      ],
      "published": "2022-12-15T14:16:13Z",
      "updated": "2022-12-15T14:16:13Z",
      "categories": [
        "cs.IT",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.07847v1",
      "landing_url": "https://arxiv.org/abs/2212.07847v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.07847"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item addresses near-field mmWave MIMO codebooks for beamforming and does not discuss discrete audio tokens or token-based representations, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item addresses near-field mmWave MIMO codebooks for beamforming and does not discuss discrete audio tokens or token-based representations, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on hierarchical codebook design for near-field mmWave MIMO communication systems, related to beamforming and communication architectures, without any reference to discrete audio tokens, audio quantization, or audio language modeling; thus, it does not meet the inclusion criteria centered on discrete audio token design or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on hierarchical codebook design for near-field mmWave MIMO communication systems, related to beamforming and communication architectures, without any reference to discrete audio tokens, audio quantization, or audio language modeling; thus, it does not meet the inclusion criteria centered on discrete audio token design or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "BEATs: Audio Pre-Training with Acoustic Tokenizers",
    "abstract": "The massive growth of self-supervised learning (SSL) has been witnessed in language, vision, speech, and audio domains over the past few years. While discrete label prediction is widely adopted for other modalities, the state-of-the-art audio SSL models still employ reconstruction loss for pre-training. Compared with reconstruction loss, semantic-rich discrete label prediction encourages the SSL model to abstract the high-level audio semantics and discard the redundant details as in human perception. However, a semantic-rich acoustic tokenizer for general audio pre-training is usually not straightforward to obtain, due to the continuous property of audio and unavailable phoneme sequences like speech. To tackle this challenge, we propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. In the first iteration, we use random projection as the acoustic tokenizer to train an audio SSL model in a mask and label prediction manner. Then, we train an acoustic tokenizer for the next iteration by distilling the semantic knowledge from the pre-trained or fine-tuned audio SSL model. The iteration is repeated with the hope of mutual promotion of the acoustic tokenizer and audio SSL model. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for audio-only models without using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats.",
    "metadata": {
      "arxiv_id": "2212.09058",
      "title": "BEATs: Audio Pre-Training with Acoustic Tokenizers",
      "summary": "The massive growth of self-supervised learning (SSL) has been witnessed in language, vision, speech, and audio domains over the past few years. While discrete label prediction is widely adopted for other modalities, the state-of-the-art audio SSL models still employ reconstruction loss for pre-training. Compared with reconstruction loss, semantic-rich discrete label prediction encourages the SSL model to abstract the high-level audio semantics and discard the redundant details as in human perception. However, a semantic-rich acoustic tokenizer for general audio pre-training is usually not straightforward to obtain, due to the continuous property of audio and unavailable phoneme sequences like speech. To tackle this challenge, we propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. In the first iteration, we use random projection as the acoustic tokenizer to train an audio SSL model in a mask and label prediction manner. Then, we train an acoustic tokenizer for the next iteration by distilling the semantic knowledge from the pre-trained or fine-tuned audio SSL model. The iteration is repeated with the hope of mutual promotion of the acoustic tokenizer and audio SSL model. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for audio-only models without using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats.",
      "authors": [
        "Sanyuan Chen",
        "Yu Wu",
        "Chengyi Wang",
        "Shujie Liu",
        "Daniel Tompkins",
        "Zhuo Chen",
        "Furu Wei"
      ],
      "published": "2022-12-18T10:41:55Z",
      "updated": "2022-12-18T10:41:55Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.09058v1",
      "landing_url": "https://arxiv.org/abs/2212.09058v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.09058"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes BEATs, an iterative framework to produce discrete acoustic tokens via an acoustic tokenizer for general audio SSL pre-training, with a detailed tokenization/quantization process, training scheme, and empirical results, aligning with the 'Discrete Audio Tokens' topic and providing sufficient methodological detail for reproducibility; it is not just a demo and does not rely solely on standard codecs or lack sufficient data/date information. Therefore it should be included.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item describes BEATs, an iterative framework to produce discrete acoustic tokens via an acoustic tokenizer for general audio SSL pre-training, with a detailed tokenization/quantization process, training scheme, and empirical results, aligning with the 'Discrete Audio Tokens' topic and providing sufficient methodological detail for reproducibility; it is not just a demo and does not rely solely on standard codecs or lack sufficient data/date information. Therefore it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper BEATs proposes an iterative audio pre-training framework that learns acoustic tokenizers generating discrete labels rich in audio semantics, which are used for audio sequence modeling; it clearly describes the token generation through acoustic tokenizers and supports their use for semantic-rich discrete label prediction in audio SSL. It matches the inclusion criteria of proposing discrete audio tokens with semantic and acoustic information used as representations for sequence models, and it offers method details and evaluation, without any exclusion criteria issues.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper BEATs proposes an iterative audio pre-training framework that learns acoustic tokenizers generating discrete labels rich in audio semantics, which are used for audio sequence modeling; it clearly describes the token generation through acoustic tokenizers and supports their use for semantic-rich discrete label prediction in audio SSL. It matches the inclusion criteria of proposing discrete audio tokens with semantic and acoustic information used as representations for sequence models, and it offers method details and evaluation, without any exclusion criteria issues.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Similarity Contrastive Estimation for Image and Video Soft Contrastive Self-Supervised Learning",
    "abstract": "Contrastive representation learning has proven to be an effective self-supervised learning method for images and videos. Most successful approaches are based on Noise Contrastive Estimation (NCE) and use different views of an instance as positives that should be contrasted with other instances, called negatives, that are considered as noise. However, several instances in a dataset are drawn from the same distribution and share underlying semantic information. A good data representation should contain relations between the instances, or semantic similarity and dissimilarity, that contrastive learning harms by considering all negatives as noise. To circumvent this issue, we propose a novel formulation of contrastive learning using semantic similarity between instances called Similarity Contrastive Estimation (SCE). Our training objective is a soft contrastive one that brings the positives closer and estimates a continuous distribution to push or pull negative instances based on their learned similarities. We validate empirically our approach on both image and video representation learning. We show that SCE performs competitively with the state of the art on the ImageNet linear evaluation protocol for fewer pretraining epochs and that it generalizes to several downstream image tasks. We also show that SCE reaches state-of-the-art results for pretraining video representation and that the learned representation can generalize to video downstream tasks.",
    "metadata": {
      "arxiv_id": "2212.11187",
      "title": "Similarity Contrastive Estimation for Image and Video Soft Contrastive Self-Supervised Learning",
      "summary": "Contrastive representation learning has proven to be an effective self-supervised learning method for images and videos. Most successful approaches are based on Noise Contrastive Estimation (NCE) and use different views of an instance as positives that should be contrasted with other instances, called negatives, that are considered as noise. However, several instances in a dataset are drawn from the same distribution and share underlying semantic information. A good data representation should contain relations between the instances, or semantic similarity and dissimilarity, that contrastive learning harms by considering all negatives as noise. To circumvent this issue, we propose a novel formulation of contrastive learning using semantic similarity between instances called Similarity Contrastive Estimation (SCE). Our training objective is a soft contrastive one that brings the positives closer and estimates a continuous distribution to push or pull negative instances based on their learned similarities. We validate empirically our approach on both image and video representation learning. We show that SCE performs competitively with the state of the art on the ImageNet linear evaluation protocol for fewer pretraining epochs and that it generalizes to several downstream image tasks. We also show that SCE reaches state-of-the-art results for pretraining video representation and that the learned representation can generalize to video downstream tasks.",
      "authors": [
        "Julien Denize",
        "Jaonary Rabarisoa",
        "Astrid Orcesi",
        "Romain Hérault"
      ],
      "published": "2022-12-21T16:56:55Z",
      "updated": "2022-12-21T16:56:55Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.11187v1",
      "landing_url": "https://arxiv.org/abs/2212.11187v1",
      "doi": "https://doi.org/10.1007/s00138-023-01444-9"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on image/video self-supervised learning with Similarity Contrastive Estimation, not on discrete audio tokens or audio tokenization, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on image/video self-supervised learning with Similarity Contrastive Estimation, not on discrete audio tokens or audio tokenization, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a novel contrastive learning method for image and video representation, with no mention of audio signals or discrete audio tokens. The study does not address the discrete tokenization of audio nor the use of vector quantization or related methods for audio sequence modeling, which are central to the inclusion criteria. Therefore, it does not meet the core requirements related to audio token generation or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a novel contrastive learning method for image and video representation, with no mention of audio signals or discrete audio tokens. The study does not address the discrete tokenization of audio nor the use of vector quantization or related methods for audio sequence modeling, which are central to the inclusion criteria. Therefore, it does not meet the core requirements related to audio token generation or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Offline Clustering Approach to Self-supervised Learning for Class-imbalanced Image Data",
    "abstract": "Class-imbalanced datasets are known to cause the problem of model being biased towards the majority classes. In this project, we set up two research questions: 1) when is the class-imbalance problem more prevalent in self-supervised pre-training? and 2) can offline clustering of feature representations help pre-training on class-imbalanced data? Our experiments investigate the former question by adjusting the degree of {\\it class-imbalance} when training the baseline models, namely SimCLR and SimSiam on CIFAR-10 database. To answer the latter question, we train each expert model on each subset of the feature clusters. We then distill the knowledge of expert models into a single model, so that we will be able to compare the performance of this model to our baselines.",
    "metadata": {
      "arxiv_id": "2212.11444",
      "title": "Offline Clustering Approach to Self-supervised Learning for Class-imbalanced Image Data",
      "summary": "Class-imbalanced datasets are known to cause the problem of model being biased towards the majority classes. In this project, we set up two research questions: 1) when is the class-imbalance problem more prevalent in self-supervised pre-training? and 2) can offline clustering of feature representations help pre-training on class-imbalanced data? Our experiments investigate the former question by adjusting the degree of {\\it class-imbalance} when training the baseline models, namely SimCLR and SimSiam on CIFAR-10 database. To answer the latter question, we train each expert model on each subset of the feature clusters. We then distill the knowledge of expert models into a single model, so that we will be able to compare the performance of this model to our baselines.",
      "authors": [
        "Hye-min Chang",
        "Sungkyun Chang"
      ],
      "published": "2022-12-22T01:26:38Z",
      "updated": "2022-12-22T01:26:38Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.11444v1",
      "landing_url": "https://arxiv.org/abs/2212.11444v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.11444"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on offline clustering for self-supervised learning in class-imbalanced image data and does not involve discrete audio tokens or tokenization as defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on offline clustering for self-supervised learning in class-imbalanced image data and does not involve discrete audio tokens or tokenization as defined in the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning and class-imbalanced image data clustering, which is unrelated to discrete audio tokens or audio representation quantization, thus not meeting the inclusion criteria focused on audio token design and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning and class-imbalanced image data clustering, which is unrelated to discrete audio tokens or audio representation quantization, thus not meeting the inclusion criteria focused on audio token design and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Efficient Speech Representation Learning with Low-Bit Quantization",
    "abstract": "With the development of hardware for machine learning, newer models often come at the cost of both increased sizes and computational complexity. In effort to improve the efficiency for these models, we apply and investigate recent quantization techniques on speech representation learning models. The quantization techniques were evaluated on the SUPERB benchmark. On the ASR task, with aggressive quantization to 1 bit, we achieved 86.32% storage reduction (184.42 -> 25.23), 88% estimated runtime reduction (1.00 -> 0.12) with increased word error rate (7.06 -> 15.96). In comparison with DistillHuBERT which also aims for model compression, the 2-bit configuration yielded slightly smaller storage (35.84 vs. 46.98), better word error rate (12.68 vs. 13.37) and more efficient estimated runtime (0.15 vs. 0.73).",
    "metadata": {
      "arxiv_id": "2301.00652",
      "title": "Efficient Speech Representation Learning with Low-Bit Quantization",
      "summary": "With the development of hardware for machine learning, newer models often come at the cost of both increased sizes and computational complexity. In effort to improve the efficiency for these models, we apply and investigate recent quantization techniques on speech representation learning models. The quantization techniques were evaluated on the SUPERB benchmark. On the ASR task, with aggressive quantization to 1 bit, we achieved 86.32% storage reduction (184.42 -> 25.23), 88% estimated runtime reduction (1.00 -> 0.12) with increased word error rate (7.06 -> 15.96). In comparison with DistillHuBERT which also aims for model compression, the 2-bit configuration yielded slightly smaller storage (35.84 vs. 46.98), better word error rate (12.68 vs. 13.37) and more efficient estimated runtime (0.15 vs. 0.73).",
      "authors": [
        "Ching-Feng Yeh",
        "Wei-Ning Hsu",
        "Paden Tomasello",
        "Abdelrahman Mohamed"
      ],
      "published": "2022-12-14T06:09:08Z",
      "updated": "2022-12-14T06:09:08Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.00652v1",
      "landing_url": "https://arxiv.org/abs/2301.00652v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.00652"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on low-bit quantization for speech representation learning and model efficiency, not on designing discrete audio tokens or detailing a token generation/quantization workflow (e.g., VQ/RVQ, codebooks, semantic/disentangled token representations), so it does not meet the core inclusion criteria for discrete audio token research.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on low-bit quantization for speech representation learning and model efficiency, not on designing discrete audio tokens or detailing a token generation/quantization workflow (e.g., VQ/RVQ, codebooks, semantic/disentangled token representations), so it does not meet the core inclusion criteria for discrete audio token research.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates low-bit quantization methods applied to speech representation learning models and reports quantitative results on compression and performance trade-offs. However, it does not explicitly describe the generation process of discrete audio tokens or focus on a discrete token design used for language or sequence modeling as required. The paper's main contribution is model compression via quantization rather than producing discrete audio tokens suitable for downstream sequence or language modeling tasks, thus failing to meet core inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study investigates low-bit quantization methods applied to speech representation learning models and reports quantitative results on compression and performance trade-offs. However, it does not explicitly describe the generation process of discrete audio tokens or focus on a discrete token design used for language or sequence modeling as required. The paper's main contribution is model compression via quantization rather than producing discrete audio tokens suitable for downstream sequence or language modeling tasks, thus failing to meet core inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Physics-informed self-supervised deep learning reconstruction for accelerated first-pass perfusion cardiac MRI",
    "abstract": "First-pass perfusion cardiac magnetic resonance (FPP-CMR) is becoming an essential non-invasive imaging method for detecting deficits of myocardial blood flow, allowing the assessment of coronary heart disease. Nevertheless, acquisitions suffer from relatively low spatial resolution and limited heart coverage. Compressed sensing (CS) methods have been proposed to accelerate FPP-CMR and achieve higher spatial resolution. However, the long reconstruction times have limited the widespread clinical use of CS in FPP-CMR. Deep learning techniques based on supervised learning have emerged as alternatives for speeding up reconstructions. However, these approaches require fully sampled data for training, which is not possible to obtain, particularly high-resolution FPP-CMR images. Here, we propose a physics-informed self-supervised deep learning FPP-CMR reconstruction approach for accelerating FPP-CMR scans and hence facilitate high spatial resolution imaging. The proposed method provides high-quality FPP-CMR images from 10x undersampled data without using fully sampled reference data.",
    "metadata": {
      "arxiv_id": "2301.02033",
      "title": "Physics-informed self-supervised deep learning reconstruction for accelerated first-pass perfusion cardiac MRI",
      "summary": "First-pass perfusion cardiac magnetic resonance (FPP-CMR) is becoming an essential non-invasive imaging method for detecting deficits of myocardial blood flow, allowing the assessment of coronary heart disease. Nevertheless, acquisitions suffer from relatively low spatial resolution and limited heart coverage. Compressed sensing (CS) methods have been proposed to accelerate FPP-CMR and achieve higher spatial resolution. However, the long reconstruction times have limited the widespread clinical use of CS in FPP-CMR. Deep learning techniques based on supervised learning have emerged as alternatives for speeding up reconstructions. However, these approaches require fully sampled data for training, which is not possible to obtain, particularly high-resolution FPP-CMR images. Here, we propose a physics-informed self-supervised deep learning FPP-CMR reconstruction approach for accelerating FPP-CMR scans and hence facilitate high spatial resolution imaging. The proposed method provides high-quality FPP-CMR images from 10x undersampled data without using fully sampled reference data.",
      "authors": [
        "Elena Martín-González",
        "Ebraham Alskaf",
        "Amedeo Chiribiri",
        "Pablo Casaseca-de-la-Higuera",
        "Carlos Alberola-López",
        "Rita G Nunes",
        "Teresa M Correia"
      ],
      "published": "2023-01-05T12:11:17Z",
      "updated": "2023-01-05T12:11:17Z",
      "categories": [
        "eess.IV",
        "cs.LG",
        "physics.med-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.02033v1",
      "landing_url": "https://arxiv.org/abs/2301.02033v1",
      "doi": "https://doi.org/10.1007/978-3-030-88552-6_9"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The topic is about physics-informed self-supervised DL reconstruction for first-pass perfusion cardiac MRI, which is unrelated to discrete audio token design or token-based audio modeling required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The topic is about physics-informed self-supervised DL reconstruction for first-pass perfusion cardiac MRI, which is unrelated to discrete audio token design or token-based audio modeling required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a physics-informed self-supervised deep learning method for accelerating cardiac MRI reconstruction, which is in the medical imaging domain and unrelated to audio signal processing or discrete audio tokens. There is no mention of audio tokens, vector quantization, or sequence modeling relevant to audio, thus they do not meet any of the inclusion criteria related to discrete audio tokens and their methodology.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a physics-informed self-supervised deep learning method for accelerating cardiac MRI reconstruction, which is in the medical imaging domain and unrelated to audio signal processing or discrete audio tokens. There is no mention of audio tokens, vector quantization, or sequence modeling relevant to audio, thus they do not meet any of the inclusion criteria related to discrete audio tokens and their methodology.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Perceive and predict: self-supervised speech representation based loss functions for speech enhancement",
    "abstract": "Recent work in the domain of speech enhancement has explored the use of self-supervised speech representations to aid in the training of neural speech enhancement models. However, much of this work focuses on using the deepest or final outputs of self supervised speech representation models, rather than the earlier feature encodings. The use of self supervised representations in such a way is often not fully motivated. In this work it is shown that the distance between the feature encodings of clean and noisy speech correlate strongly with psychoacoustically motivated measures of speech quality and intelligibility, as well as with human Mean Opinion Score (MOS) ratings. Experiments using this distance as a loss function are performed and improved performance over the use of STFT spectrogram distance based loss as well as other common loss functions from speech enhancement literature is demonstrated using objective measures such as perceptual evaluation of speech quality (PESQ) and short-time objective intelligibility (STOI).",
    "metadata": {
      "arxiv_id": "2301.04388",
      "title": "Perceive and predict: self-supervised speech representation based loss functions for speech enhancement",
      "summary": "Recent work in the domain of speech enhancement has explored the use of self-supervised speech representations to aid in the training of neural speech enhancement models. However, much of this work focuses on using the deepest or final outputs of self supervised speech representation models, rather than the earlier feature encodings. The use of self supervised representations in such a way is often not fully motivated. In this work it is shown that the distance between the feature encodings of clean and noisy speech correlate strongly with psychoacoustically motivated measures of speech quality and intelligibility, as well as with human Mean Opinion Score (MOS) ratings. Experiments using this distance as a loss function are performed and improved performance over the use of STFT spectrogram distance based loss as well as other common loss functions from speech enhancement literature is demonstrated using objective measures such as perceptual evaluation of speech quality (PESQ) and short-time objective intelligibility (STOI).",
      "authors": [
        "George Close",
        "William Ravenscroft",
        "Thomas Hain",
        "Stefan Goetze"
      ],
      "published": "2023-01-11T10:20:56Z",
      "updated": "2023-06-26T09:31:53Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.04388v3",
      "landing_url": "https://arxiv.org/abs/2301.04388v3",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10095666"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on self-supervised speech representations as a loss function for speech enhancement and does not present/discuss discrete audio token generation, quantization (VQ/RVQ), or token-based modeling, so it does not meet the discrete audio tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on self-supervised speech representations as a loss function for speech enhancement and does not present/discuss discrete audio token generation, quantization (VQ/RVQ), or token-based modeling, so it does not meet the discrete audio tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on self-supervised speech representations used as loss functions for speech enhancement, but does not describe any discrete audio token generation or quantization processes as required by the inclusion criteria, nor does it mention vector quantization or discrete token design for sequence modeling; therefore, it does not meet the core criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on self-supervised speech representations used as loss functions for speech enhancement, but does not describe any discrete audio token generation or quantization processes as required by the inclusion criteria, nor does it mention vector quantization or discrete token design for sequence modeling; therefore, it does not meet the core criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Q-Diffusion: Quantizing Diffusion Models",
    "abstract": "Diffusion models have achieved great success in image synthesis through iterative noise estimation using deep neural networks. However, the slow inference, high memory consumption, and computation intensity of the noise estimation model hinder the efficient adoption of diffusion models. Although post-training quantization (PTQ) is considered a go-to compression method for other tasks, it does not work out-of-the-box on diffusion models. We propose a novel PTQ method specifically tailored towards the unique multi-timestep pipeline and model architecture of the diffusion models, which compresses the noise estimation network to accelerate the generation process. We identify the key difficulty of diffusion model quantization as the changing output distributions of noise estimation networks over multiple time steps and the bimodal activation distribution of the shortcut layers within the noise estimation network. We tackle these challenges with timestep-aware calibration and split shortcut quantization in this work. Experimental results show that our proposed method is able to quantize full-precision unconditional diffusion models into 4-bit while maintaining comparable performance (small FID change of at most 2.34 compared to >100 for traditional PTQ) in a training-free manner. Our approach can also be applied to text-guided image generation, where we can run stable diffusion in 4-bit weights with high generation quality for the first time.",
    "metadata": {
      "arxiv_id": "2302.04304",
      "title": "Q-Diffusion: Quantizing Diffusion Models",
      "summary": "Diffusion models have achieved great success in image synthesis through iterative noise estimation using deep neural networks. However, the slow inference, high memory consumption, and computation intensity of the noise estimation model hinder the efficient adoption of diffusion models. Although post-training quantization (PTQ) is considered a go-to compression method for other tasks, it does not work out-of-the-box on diffusion models. We propose a novel PTQ method specifically tailored towards the unique multi-timestep pipeline and model architecture of the diffusion models, which compresses the noise estimation network to accelerate the generation process. We identify the key difficulty of diffusion model quantization as the changing output distributions of noise estimation networks over multiple time steps and the bimodal activation distribution of the shortcut layers within the noise estimation network. We tackle these challenges with timestep-aware calibration and split shortcut quantization in this work. Experimental results show that our proposed method is able to quantize full-precision unconditional diffusion models into 4-bit while maintaining comparable performance (small FID change of at most 2.34 compared to >100 for traditional PTQ) in a training-free manner. Our approach can also be applied to text-guided image generation, where we can run stable diffusion in 4-bit weights with high generation quality for the first time.",
      "authors": [
        "Xiuyu Li",
        "Yijiang Liu",
        "Long Lian",
        "Huanrui Yang",
        "Zhen Dong",
        "Daniel Kang",
        "Shanghang Zhang",
        "Kurt Keutzer"
      ],
      "published": "2023-02-08T19:38:59Z",
      "updated": "2023-06-08T09:21:05Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.04304v3",
      "landing_url": "https://arxiv.org/abs/2302.04304v3",
      "doi": "https://doi.org/10.48550/arXiv.2302.04304"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on quantizing diffusion models for image generation, not on discrete audio tokens or audio token design; it does not meet the inclusion criteria for discrete audio token research and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on quantizing diffusion models for image generation, not on discrete audio tokens or audio token design; it does not meet the inclusion criteria for discrete audio token research and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on quantizing diffusion models for image synthesis and acceleration, without discussing discrete audio tokens, audio signal discretization, or sequence modeling related to audio. It does not address token generation for audio nor any audio language modeling or cross-modal audio-text tasks, which are central to the inclusion criteria. Therefore, it does not meet the thematic requirements and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on quantizing diffusion models for image synthesis and acceleration, without discussing discrete audio tokens, audio signal discretization, or sequence modeling related to audio. It does not address token generation for audio nor any audio language modeling or cross-modal audio-text tasks, which are central to the inclusion criteria. Therefore, it does not meet the thematic requirements and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improved Decoding of Attentional Selection in Multi-Talker Environments with Self-Supervised Learned Speech Representation",
    "abstract": "Auditory attention decoding (AAD) is a technique used to identify and amplify the talker that a listener is focused on in a noisy environment. This is done by comparing the listener's brainwaves to a representation of all the sound sources to find the closest match. The representation is typically the waveform or spectrogram of the sounds. The effectiveness of these representations for AAD is uncertain. In this study, we examined the use of self-supervised learned speech representation in improving the accuracy and speed of AAD. We recorded the brain activity of three subjects using invasive electrocorticography (ECoG) as they listened to two conversations and focused on one. We used WavLM to extract a latent representation of each talker and trained a spatiotemporal filter to map brain activity to intermediate representations of speech. During the evaluation, the reconstructed representation is compared to each speaker's representation to determine the target speaker. Our results indicate that speech representation from WavLM provides better decoding accuracy and speed than the speech envelope and spectrogram. Our findings demonstrate the advantages of self-supervised learned speech representation for auditory attention decoding and pave the way for developing brain-controlled hearable technologies.",
    "metadata": {
      "arxiv_id": "2302.05756",
      "title": "Improved Decoding of Attentional Selection in Multi-Talker Environments with Self-Supervised Learned Speech Representation",
      "summary": "Auditory attention decoding (AAD) is a technique used to identify and amplify the talker that a listener is focused on in a noisy environment. This is done by comparing the listener's brainwaves to a representation of all the sound sources to find the closest match. The representation is typically the waveform or spectrogram of the sounds. The effectiveness of these representations for AAD is uncertain. In this study, we examined the use of self-supervised learned speech representation in improving the accuracy and speed of AAD. We recorded the brain activity of three subjects using invasive electrocorticography (ECoG) as they listened to two conversations and focused on one. We used WavLM to extract a latent representation of each talker and trained a spatiotemporal filter to map brain activity to intermediate representations of speech. During the evaluation, the reconstructed representation is compared to each speaker's representation to determine the target speaker. Our results indicate that speech representation from WavLM provides better decoding accuracy and speed than the speech envelope and spectrogram. Our findings demonstrate the advantages of self-supervised learned speech representation for auditory attention decoding and pave the way for developing brain-controlled hearable technologies.",
      "authors": [
        "Cong Han",
        "Vishal Choudhari",
        "Yinghao Aaron Li",
        "Nima Mesgarani"
      ],
      "published": "2023-02-11T18:33:42Z",
      "updated": "2023-02-11T18:33:42Z",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.05756v1",
      "landing_url": "https://arxiv.org/abs/2302.05756v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.05756"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此論文專注於自監督語音表示在聽覺注意解碼中的應用，未涉及離散音訊 tokens 的設計、量化或碼本等可重現方法，因此不符合本研究的離散音訊 token 主題要求。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此論文專注於自監督語音表示在聽覺注意解碼中的應用，未涉及離散音訊 tokens 的設計、量化或碼本等可重現方法，因此不符合本研究的離散音訊 token 主題要求。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discusses using self-supervised learned speech representations (WavLM) for auditory attention decoding, but it does not indicate that the speech representations are discretized into tokens via vector quantization or similar mechanisms; it lacks details on token generation, quantization, or application of discrete audio tokens as defined.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study discusses using self-supervised learned speech representations (WavLM) for auditory attention decoding, but it does not indicate that the speech representations are discretized into tokens via vector quantization or similar mechanisms; it lacks details on token generation, quantization, or application of discrete audio tokens as defined.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "ACE-VC: Adaptive and Controllable Voice Conversion using Explicitly Disentangled Self-supervised Speech Representations",
    "abstract": "In this work, we propose a zero-shot voice conversion method using speech representations trained with self-supervised learning. First, we develop a multi-task model to decompose a speech utterance into features such as linguistic content, speaker characteristics, and speaking style. To disentangle content and speaker representations, we propose a training strategy based on Siamese networks that encourages similarity between the content representations of the original and pitch-shifted audio. Next, we develop a synthesis model with pitch and duration predictors that can effectively reconstruct the speech signal from its decomposed representation. Our framework allows controllable and speaker-adaptive synthesis to perform zero-shot any-to-any voice conversion achieving state-of-the-art results on metrics evaluating speaker similarity, intelligibility, and naturalness. Using just 10 seconds of data for a target speaker, our framework can perform voice swapping and achieves a speaker verification EER of 5.5% for seen speakers and 8.4% for unseen speakers.",
    "metadata": {
      "arxiv_id": "2302.08137",
      "title": "ACE-VC: Adaptive and Controllable Voice Conversion using Explicitly Disentangled Self-supervised Speech Representations",
      "summary": "In this work, we propose a zero-shot voice conversion method using speech representations trained with self-supervised learning. First, we develop a multi-task model to decompose a speech utterance into features such as linguistic content, speaker characteristics, and speaking style. To disentangle content and speaker representations, we propose a training strategy based on Siamese networks that encourages similarity between the content representations of the original and pitch-shifted audio. Next, we develop a synthesis model with pitch and duration predictors that can effectively reconstruct the speech signal from its decomposed representation. Our framework allows controllable and speaker-adaptive synthesis to perform zero-shot any-to-any voice conversion achieving state-of-the-art results on metrics evaluating speaker similarity, intelligibility, and naturalness. Using just 10 seconds of data for a target speaker, our framework can perform voice swapping and achieves a speaker verification EER of 5.5% for seen speakers and 8.4% for unseen speakers.",
      "authors": [
        "Shehzeen Hussain",
        "Paarth Neekhara",
        "Jocelyn Huang",
        "Jason Li",
        "Boris Ginsburg"
      ],
      "published": "2023-02-16T08:10:41Z",
      "updated": "2023-02-16T08:10:41Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.08137v1",
      "landing_url": "https://arxiv.org/abs/2302.08137v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.08137"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item targets zero-shot voice conversion using self-supervised speech representations with disentangled content/speaker features; it lacks discussion of discrete audio tokens or tokenization (VQ/RVQ, hierarchical quantization, semantic/disentangled discrete representations) and is not framed around token-level audio representation for language modeling. Therefore, it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item targets zero-shot voice conversion using self-supervised speech representations with disentangled content/speaker features; it lacks discussion of discrete audio tokens or tokenization (VQ/RVQ, hierarchical quantization, semantic/disentangled discrete representations) and is not framed around token-level audio representation for language modeling. Therefore, it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a voice conversion method using self-supervised speech representations with explicit disentanglement of content, speaker, and style features, but it does not describe discrete audio token generation processes such as vector quantization or clustering leading to discrete semantic tokens; thus, it lacks the core contribution related to discrete audio tokens required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a voice conversion method using self-supervised speech representations with explicit disentanglement of content, speaker, and style features, but it does not describe discrete audio token generation processes such as vector quantization or clustering leading to discrete semantic tokens; thus, it lacks the core contribution related to discrete audio tokens required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Catch You and I Can: Revealing Source Voiceprint Against Voice Conversion",
    "abstract": "Voice conversion (VC) techniques can be abused by malicious parties to transform their audios to sound like a target speaker, making it hard for a human being or a speaker verification/identification system to trace the source speaker. In this paper, we make the first attempt to restore the source voiceprint from audios synthesized by voice conversion methods with high credit. However, unveiling the features of the source speaker from a converted audio is challenging since the voice conversion operation intends to disentangle the original features and infuse the features of the target speaker. To fulfill our goal, we develop Revelio, a representation learning model, which learns to effectively extract the voiceprint of the source speaker from converted audio samples. We equip Revelio with a carefully-designed differential rectification algorithm to eliminate the influence of the target speaker by removing the representation component that is parallel to the voiceprint of the target speaker. We have conducted extensive experiments to evaluate the capability of Revelio in restoring voiceprint from audios converted by VQVC, VQVC+, AGAIN, and BNE. The experiments verify that Revelio is able to rebuild voiceprints that can be traced to the source speaker by speaker verification and identification systems. Revelio also exhibits robust performance under inter-gender conversion, unseen languages, and telephony networks.",
    "metadata": {
      "arxiv_id": "2302.12434",
      "title": "Catch You and I Can: Revealing Source Voiceprint Against Voice Conversion",
      "summary": "Voice conversion (VC) techniques can be abused by malicious parties to transform their audios to sound like a target speaker, making it hard for a human being or a speaker verification/identification system to trace the source speaker. In this paper, we make the first attempt to restore the source voiceprint from audios synthesized by voice conversion methods with high credit. However, unveiling the features of the source speaker from a converted audio is challenging since the voice conversion operation intends to disentangle the original features and infuse the features of the target speaker. To fulfill our goal, we develop Revelio, a representation learning model, which learns to effectively extract the voiceprint of the source speaker from converted audio samples. We equip Revelio with a carefully-designed differential rectification algorithm to eliminate the influence of the target speaker by removing the representation component that is parallel to the voiceprint of the target speaker. We have conducted extensive experiments to evaluate the capability of Revelio in restoring voiceprint from audios converted by VQVC, VQVC+, AGAIN, and BNE. The experiments verify that Revelio is able to rebuild voiceprints that can be traced to the source speaker by speaker verification and identification systems. Revelio also exhibits robust performance under inter-gender conversion, unseen languages, and telephony networks.",
      "authors": [
        "Jiangyi Deng",
        "Yanjiao Chen",
        "Yinan Zhong",
        "Qianhao Miao",
        "Xueluan Gong",
        "Wenyuan Xu"
      ],
      "published": "2023-02-24T03:33:13Z",
      "updated": "2023-02-24T03:33:13Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.12434v1",
      "landing_url": "https://arxiv.org/abs/2302.12434v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.12434"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article targets recovering the original voiceprint from voice-converted audio using representation learning, not the design or use of discrete audio tokens (VQ/RVQ, hierarchical quantization, or SSL-based tokenization) for audio language modeling or downstream tasks, thus it does not meet the token-focused inclusion criteria and fails the main requirement.\n",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article targets recovering the original voiceprint from voice-converted audio using representation learning, not the design or use of discrete audio tokens (VQ/RVQ, hierarchical quantization, or SSL-based tokenization) for audio language modeling or downstream tasks, thus it does not meet the token-focused inclusion criteria and fails the main requirement.\n",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on restoring source voiceprints from voice-converted audio, addressing speaker identification and verification challenges; however, it does not mention or propose a discrete audio tokenization method, vector quantization-based token generation, or use tokens as intermediate representations for sequence or language modeling. Therefore, it does not meet the inclusion criteria related to discrete audio tokens, nor is the discrete token process described.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on restoring source voiceprints from voice-converted audio, addressing speaker identification and verification challenges; however, it does not mention or propose a discrete audio tokenization method, vector quantization-based token generation, or use tokens as intermediate representations for sequence or language modeling. Therefore, it does not meet the inclusion criteria related to discrete audio tokens, nor is the discrete token process described.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model",
    "abstract": "Neural text-to-speech (TTS) generally consists of cascaded architecture with separately optimized acoustic model and vocoder, or end-to-end architecture with continuous mel-spectrograms or self-extracted speech frames as the intermediate representations to bridge acoustic model and vocoder, which suffers from two limitations: 1) the continuous acoustic frames are hard to predict with phoneme only, and acoustic information like duration or pitch is also needed to solve the one-to-many problem, which is not easy to scale on large scale and noise datasets; 2) to achieve diverse speech output based on continuous speech features, complex VAE or flow-based models are usually required. In this paper, we propose FoundationTTS, a new speech synthesis system with a neural audio codec for discrete speech token extraction and waveform reconstruction and a large language model for discrete token generation from linguistic (phoneme) tokens. Specifically, 1) we propose a hierarchical codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN), which first extracts continuous frame-level speech representations with fine-grained codec, and extracts a discrete token from each continuous speech frame with coarse-grained codec; 2) we jointly optimize speech token, linguistic tokens, speaker token together with a large language model and predict the discrete speech tokens autoregressively. Experiments show that FoundationTTS achieves a MOS gain of +0.14 compared to the baseline system. In ASR customization tasks, our method achieves 7.09\\% and 10.35\\% WERR respectively over two strong customized ASR baselines.",
    "metadata": {
      "arxiv_id": "2303.02939",
      "title": "FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model",
      "summary": "Neural text-to-speech (TTS) generally consists of cascaded architecture with separately optimized acoustic model and vocoder, or end-to-end architecture with continuous mel-spectrograms or self-extracted speech frames as the intermediate representations to bridge acoustic model and vocoder, which suffers from two limitations: 1) the continuous acoustic frames are hard to predict with phoneme only, and acoustic information like duration or pitch is also needed to solve the one-to-many problem, which is not easy to scale on large scale and noise datasets; 2) to achieve diverse speech output based on continuous speech features, complex VAE or flow-based models are usually required. In this paper, we propose FoundationTTS, a new speech synthesis system with a neural audio codec for discrete speech token extraction and waveform reconstruction and a large language model for discrete token generation from linguistic (phoneme) tokens. Specifically, 1) we propose a hierarchical codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN), which first extracts continuous frame-level speech representations with fine-grained codec, and extracts a discrete token from each continuous speech frame with coarse-grained codec; 2) we jointly optimize speech token, linguistic tokens, speaker token together with a large language model and predict the discrete speech tokens autoregressively. Experiments show that FoundationTTS achieves a MOS gain of +0.14 compared to the baseline system. In ASR customization tasks, our method achieves 7.09\\% and 10.35\\% WERR respectively over two strong customized ASR baselines.",
      "authors": [
        "Ruiqing Xue",
        "Yanqing Liu",
        "Lei He",
        "Xu Tan",
        "Linquan Liu",
        "Edward Lin",
        "Sheng Zhao"
      ],
      "published": "2023-03-06T07:17:15Z",
      "updated": "2023-03-08T03:06:47Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.02939v3",
      "landing_url": "https://arxiv.org/abs/2303.02939v3",
      "doi": "https://doi.org/10.48550/arXiv.2303.02939"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on discrete audio tokens produced by a VQ-based codec and uses a large language model to generate these tokens for TTS/ASR customization, detailing the tokenization process and token-level generation, which matches the inclusion criteria for discrete audio tokens and does not trigger any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work centers on discrete audio tokens produced by a VQ-based codec and uses a large language model to generate these tokens for TTS/ASR customization, detailing the tokenization process and token-level generation, which matches the inclusion criteria for discrete audio tokens and does not trigger any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes FoundationTTS, which uses a neural audio codec with vector-quantized auto-encoders (VQ-GAN) for discrete speech token extraction and waveform reconstruction. It clearly explains the token generation process involving hierarchical vector quantization and employs these discrete tokens in sequence modeling with a large language model for speech synthesis and ASR customization tasks. The method includes technical details about token extraction and usage for generative modeling, meeting the inclusion criteria focused on discrete audio tokens designed by neural codecs and used in sequence models. There is no indication that this work fits any exclusion criteria, and it appears to be a formal academic contribution with method and experiment descriptions that supports reproducibility and evaluation.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes FoundationTTS, which uses a neural audio codec with vector-quantized auto-encoders (VQ-GAN) for discrete speech token extraction and waveform reconstruction. It clearly explains the token generation process involving hierarchical vector quantization and employs these discrete tokens in sequence modeling with a large language model for speech synthesis and ASR customization tasks. The method includes technical details about token extraction and usage for generative modeling, meeting the inclusion criteria focused on discrete audio tokens designed by neural codecs and used in sequence models. There is no indication that this work fits any exclusion criteria, and it appears to be a formal academic contribution with method and experiment descriptions that supports reproducibility and evaluation.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ACR: Attention Collaboration-based Regressor for Arbitrary Two-Hand Reconstruction",
    "abstract": "Reconstructing two hands from monocular RGB images is challenging due to frequent occlusion and mutual confusion. Existing methods mainly learn an entangled representation to encode two interacting hands, which are incredibly fragile to impaired interaction, such as truncated hands, separate hands, or external occlusion. This paper presents ACR (Attention Collaboration-based Regressor), which makes the first attempt to reconstruct hands in arbitrary scenarios. To achieve this, ACR explicitly mitigates interdependencies between hands and between parts by leveraging center and part-based attention for feature extraction. However, reducing interdependence helps release the input constraint while weakening the mutual reasoning about reconstructing the interacting hands. Thus, based on center attention, ACR also learns cross-hand prior that handle the interacting hands better. We evaluate our method on various types of hand reconstruction datasets. Our method significantly outperforms the best interacting-hand approaches on the InterHand2.6M dataset while yielding comparable performance with the state-of-the-art single-hand methods on the FreiHand dataset. More qualitative results on in-the-wild and hand-object interaction datasets and web images/videos further demonstrate the effectiveness of our approach for arbitrary hand reconstruction. Our code is available at https://github.com/ZhengdiYu/Arbitrary-Hands-3D-Reconstruction.",
    "metadata": {
      "arxiv_id": "2303.05938",
      "title": "ACR: Attention Collaboration-based Regressor for Arbitrary Two-Hand Reconstruction",
      "summary": "Reconstructing two hands from monocular RGB images is challenging due to frequent occlusion and mutual confusion. Existing methods mainly learn an entangled representation to encode two interacting hands, which are incredibly fragile to impaired interaction, such as truncated hands, separate hands, or external occlusion. This paper presents ACR (Attention Collaboration-based Regressor), which makes the first attempt to reconstruct hands in arbitrary scenarios. To achieve this, ACR explicitly mitigates interdependencies between hands and between parts by leveraging center and part-based attention for feature extraction. However, reducing interdependence helps release the input constraint while weakening the mutual reasoning about reconstructing the interacting hands. Thus, based on center attention, ACR also learns cross-hand prior that handle the interacting hands better. We evaluate our method on various types of hand reconstruction datasets. Our method significantly outperforms the best interacting-hand approaches on the InterHand2.6M dataset while yielding comparable performance with the state-of-the-art single-hand methods on the FreiHand dataset. More qualitative results on in-the-wild and hand-object interaction datasets and web images/videos further demonstrate the effectiveness of our approach for arbitrary hand reconstruction. Our code is available at https://github.com/ZhengdiYu/Arbitrary-Hands-3D-Reconstruction.",
      "authors": [
        "Zhengdi Yu",
        "Shaoli Huang",
        "Chen Fang",
        "Toby P. Breckon",
        "Jue Wang"
      ],
      "published": "2023-03-10T14:19:02Z",
      "updated": "2023-03-10T14:19:02Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.05938v1",
      "landing_url": "https://arxiv.org/abs/2303.05938v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.05938"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item targets 3D hand reconstruction from RGB data and does not involve discretized audio tokens or audio-language modeling, so it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item targets 3D hand reconstruction from RGB data and does not involve discretized audio tokens or audio-language modeling, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on reconstructing two hands from images using attention mechanisms and does not address discrete audio tokens, vector quantization, or audio representation; thus, it does not meet any inclusion criteria related to discrete audio token research and falls outside the scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on reconstructing two hands from images using attention mechanisms and does not address discrete audio tokens, vector quantization, or audio representation; thus, it does not meet any inclusion criteria related to discrete audio token research and falls outside the scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Analysing the Masked predictive coding training criterion for pre-training a Speech Representation Model",
    "abstract": "Recent developments in pre-trained speech representation utilizing self-supervised learning (SSL) have yielded exceptional results on a variety of downstream tasks. One such technique, known as masked predictive coding (MPC), has been employed by some of the most high-performing models. In this study, we investigate the impact of MPC loss on the type of information learnt at various layers in the HuBERT model, using nine probing tasks. Our findings indicate that the amount of content information learned at various layers of the HuBERT model has a positive correlation to the MPC loss. Additionally, it is also observed that any speaker-related information learned at intermediate layers of the model, is an indirect consequence of the learning process, and therefore cannot be controlled using the MPC loss. These findings may serve as inspiration for further research in the speech community, specifically in the development of new pre-training tasks or the exploration of new pre-training criterion's that directly preserves both speaker and content information at various layers of a learnt model.",
    "metadata": {
      "arxiv_id": "2303.06982",
      "title": "Analysing the Masked predictive coding training criterion for pre-training a Speech Representation Model",
      "summary": "Recent developments in pre-trained speech representation utilizing self-supervised learning (SSL) have yielded exceptional results on a variety of downstream tasks. One such technique, known as masked predictive coding (MPC), has been employed by some of the most high-performing models. In this study, we investigate the impact of MPC loss on the type of information learnt at various layers in the HuBERT model, using nine probing tasks. Our findings indicate that the amount of content information learned at various layers of the HuBERT model has a positive correlation to the MPC loss. Additionally, it is also observed that any speaker-related information learned at intermediate layers of the model, is an indirect consequence of the learning process, and therefore cannot be controlled using the MPC loss. These findings may serve as inspiration for further research in the speech community, specifically in the development of new pre-training tasks or the exploration of new pre-training criterion's that directly preserves both speaker and content information at various layers of a learnt model.",
      "authors": [
        "Hemant Yadav",
        "Sunayana Sitaram",
        "Rajiv Ratn Shah"
      ],
      "published": "2023-03-13T10:32:44Z",
      "updated": "2024-01-11T11:15:38Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.06982v3",
      "landing_url": "https://arxiv.org/abs/2303.06982v3",
      "doi": "https://doi.org/10.48550/arXiv.2303.06982"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses masked predictive coding loss in pre-training HuBERT and how it affects information content, with no discussion of discrete audio tokens, tokenization methods, or VQ/RVQ-based discretization; thus it does not meet the inclusion criteria for discrete audio token research and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses masked predictive coding loss in pre-training HuBERT and how it affects information content, with no discussion of discrete audio tokens, tokenization methods, or VQ/RVQ-based discretization; thus it does not meet the inclusion criteria for discrete audio token research and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on analyzing the masked predictive coding (MPC) training criterion for pre-training a speech representation model (HuBERT), discussing the impact on learned information types but does not describe any discrete audio token generation process involving vector quantization, residual quantization, or similar discrete token mechanisms that can be directly used as input for language or sequence models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on analyzing the masked predictive coding (MPC) training criterion for pre-training a speech representation model (HuBERT), discussing the impact on learned information types but does not describe any discrete audio token generation process involving vector quantization, residual quantization, or similar discrete token mechanisms that can be directly used as input for language or sequence models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Making Vision Transformers Efficient from A Token Sparsification View",
    "abstract": "The quadratic computational complexity to the number of tokens limits the practical applications of Vision Transformers (ViTs). Several works propose to prune redundant tokens to achieve efficient ViTs. However, these methods generally suffer from (i) dramatic accuracy drops, (ii) application difficulty in the local vision transformer, and (iii) non-general-purpose networks for downstream tasks. In this work, we propose a novel Semantic Token ViT (STViT), for efficient global and local vision transformers, which can also be revised to serve as backbone for downstream tasks. The semantic tokens represent cluster centers, and they are initialized by pooling image tokens in space and recovered by attention, which can adaptively represent global or local semantic information. Due to the cluster properties, a few semantic tokens can attain the same effect as vast image tokens, for both global and local vision transformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base) can achieve the same accuracy with more than 100% inference speed improvement and nearly 60% FLOPs reduction; on Swin-(Tiny,Small,Base), we can employ 16 semantic tokens in each window to further speed it up by around 20% with slight accuracy increase. Besides great success in image classification, we also extend our method to video recognition. In addition, we design a STViT-R(ecover) network to restore the detailed spatial information based on the STViT, making it work for downstream tasks, which is powerless for previous token sparsification methods. Experiments demonstrate that our method can achieve competitive results compared to the original networks in object detection and instance segmentation, with over 30% FLOPs reduction for backbone. Code is available at http://github.com/changsn/STViT-R",
    "metadata": {
      "arxiv_id": "2303.08685",
      "title": "Making Vision Transformers Efficient from A Token Sparsification View",
      "summary": "The quadratic computational complexity to the number of tokens limits the practical applications of Vision Transformers (ViTs). Several works propose to prune redundant tokens to achieve efficient ViTs. However, these methods generally suffer from (i) dramatic accuracy drops, (ii) application difficulty in the local vision transformer, and (iii) non-general-purpose networks for downstream tasks. In this work, we propose a novel Semantic Token ViT (STViT), for efficient global and local vision transformers, which can also be revised to serve as backbone for downstream tasks. The semantic tokens represent cluster centers, and they are initialized by pooling image tokens in space and recovered by attention, which can adaptively represent global or local semantic information. Due to the cluster properties, a few semantic tokens can attain the same effect as vast image tokens, for both global and local vision transformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base) can achieve the same accuracy with more than 100% inference speed improvement and nearly 60% FLOPs reduction; on Swin-(Tiny,Small,Base), we can employ 16 semantic tokens in each window to further speed it up by around 20% with slight accuracy increase. Besides great success in image classification, we also extend our method to video recognition. In addition, we design a STViT-R(ecover) network to restore the detailed spatial information based on the STViT, making it work for downstream tasks, which is powerless for previous token sparsification methods. Experiments demonstrate that our method can achieve competitive results compared to the original networks in object detection and instance segmentation, with over 30% FLOPs reduction for backbone. Code is available at http://github.com/changsn/STViT-R",
      "authors": [
        "Shuning Chang",
        "Pichao Wang",
        "Ming Lin",
        "Fan Wang",
        "David Junhao Zhang",
        "Rong Jin",
        "Mike Zheng Shou"
      ],
      "published": "2023-03-15T15:12:36Z",
      "updated": "2023-03-30T11:56:29Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.08685v2",
      "landing_url": "https://arxiv.org/abs/2303.08685v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.08685"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The input item discusses semantic tokens and token sparsification for Vision Transformers in image/video tasks, not discrete audio tokens or vector-quantized/audio-language modeling; therefore it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The input item discusses semantic tokens and token sparsification for Vision Transformers in image/video tasks, not discrete audio tokens or vector-quantized/audio-language modeling; therefore it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Vision Transformers and token sparsification for improving computational efficiency in image and video processing tasks, without any emphasis on discrete audio tokens, vector quantization of audio, or audio language modeling. Thus, it does not meet the inclusion criteria centered on discrete audio token representation or audio-related token design, and it does not involve audio modality at all.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Vision Transformers and token sparsification for improving computational efficiency in image and video processing tasks, without any emphasis on discrete audio tokens, vector quantization of audio, or audio language modeling. Thus, it does not meet the inclusion criteria centered on discrete audio token representation or audio-related token design, and it does not involve audio modality at all.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DIRE for Diffusion-Generated Image Detection",
    "abstract": "Diffusion models have shown remarkable success in visual synthesis, but have also raised concerns about potential abuse for malicious purposes. In this paper, we seek to build a detector for telling apart real images from diffusion-generated images. We find that existing detectors struggle to detect images generated by diffusion models, even if we include generated images from a specific diffusion model in their training data. To address this issue, we propose a novel image representation called DIffusion Reconstruction Error (DIRE), which measures the error between an input image and its reconstruction counterpart by a pre-trained diffusion model. We observe that diffusion-generated images can be approximately reconstructed by a diffusion model while real images cannot. It provides a hint that DIRE can serve as a bridge to distinguish generated and real images. DIRE provides an effective way to detect images generated by most diffusion models, and it is general for detecting generated images from unseen diffusion models and robust to various perturbations. Furthermore, we establish a comprehensive diffusion-generated benchmark including images generated by eight diffusion models to evaluate the performance of diffusion-generated image detectors. Extensive experiments on our collected benchmark demonstrate that DIRE exhibits superiority over previous generated-image detectors. The code and dataset are available at https://github.com/ZhendongWang6/DIRE.",
    "metadata": {
      "arxiv_id": "2303.09295",
      "title": "DIRE for Diffusion-Generated Image Detection",
      "summary": "Diffusion models have shown remarkable success in visual synthesis, but have also raised concerns about potential abuse for malicious purposes. In this paper, we seek to build a detector for telling apart real images from diffusion-generated images. We find that existing detectors struggle to detect images generated by diffusion models, even if we include generated images from a specific diffusion model in their training data. To address this issue, we propose a novel image representation called DIffusion Reconstruction Error (DIRE), which measures the error between an input image and its reconstruction counterpart by a pre-trained diffusion model. We observe that diffusion-generated images can be approximately reconstructed by a diffusion model while real images cannot. It provides a hint that DIRE can serve as a bridge to distinguish generated and real images. DIRE provides an effective way to detect images generated by most diffusion models, and it is general for detecting generated images from unseen diffusion models and robust to various perturbations. Furthermore, we establish a comprehensive diffusion-generated benchmark including images generated by eight diffusion models to evaluate the performance of diffusion-generated image detectors. Extensive experiments on our collected benchmark demonstrate that DIRE exhibits superiority over previous generated-image detectors. The code and dataset are available at https://github.com/ZhendongWang6/DIRE.",
      "authors": [
        "Zhendong Wang",
        "Jianmin Bao",
        "Wengang Zhou",
        "Weilun Wang",
        "Hezhen Hu",
        "Hong Chen",
        "Houqiang Li"
      ],
      "published": "2023-03-16T13:15:03Z",
      "updated": "2023-03-16T13:15:03Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.09295v1",
      "landing_url": "https://arxiv.org/abs/2303.09295v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.09295"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses image-based diffusion model detection (DIRE) and diffusion reconstruction error for real vs generated images, which is in the visual domain and not about discrete audio tokens for audio modeling; thus it fails all inclusion criteria related to discrete audio token research and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses image-based diffusion model detection (DIRE) and diffusion reconstruction error for real vs generated images, which is in the visual domain and not about discrete audio tokens for audio modeling; thus it fails all inclusion criteria related to discrete audio token research and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on detecting images generated by diffusion models, which pertains to image analysis and computer vision rather than audio processing or discrete audio tokens. The study does not address any method involving discrete audio token generation, vector quantization of audio signals, or semantic/acoustic disentanglement for audio. Therefore, it does not meet any of the inclusion criteria related to discrete audio tokens, nor does it contain relevant method descriptions or experiments in the audio domain.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on detecting images generated by diffusion models, which pertains to image analysis and computer vision rather than audio processing or discrete audio tokens. The study does not address any method involving discrete audio token generation, vector quantization of audio signals, or semantic/acoustic disentanglement for audio. Therefore, it does not meet any of the inclusion criteria related to discrete audio tokens, nor does it contain relevant method descriptions or experiments in the audio domain.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Cocktail HuBERT: Generalized Self-Supervised Pre-training for Mixture and Single-Source Speech",
    "abstract": "Self-supervised learning leverages unlabeled data effectively, improving label efficiency and generalization to domains without labeled data. While recent work has studied generalization to more acoustic/linguistic domains, languages, and modalities, these investigations are limited to single-source speech with one primary speaker in the recording. This paper presents Cocktail HuBERT, a self-supervised learning framework that generalizes to mixture speech using a masked pseudo source separation objective. This objective encourages the model to identify the number of sources, separate and understand the context, and infer the content of masked regions represented as discovered units. Cocktail HuBERT outperforms state-of-the-art results with 69% lower WER on multi-speaker ASR, 31% lower DER on diarization, and is competitive on single- and multi-speaker tasks from SUPERB.",
    "metadata": {
      "arxiv_id": "2303.11131",
      "title": "Cocktail HuBERT: Generalized Self-Supervised Pre-training for Mixture and Single-Source Speech",
      "summary": "Self-supervised learning leverages unlabeled data effectively, improving label efficiency and generalization to domains without labeled data. While recent work has studied generalization to more acoustic/linguistic domains, languages, and modalities, these investigations are limited to single-source speech with one primary speaker in the recording. This paper presents Cocktail HuBERT, a self-supervised learning framework that generalizes to mixture speech using a masked pseudo source separation objective. This objective encourages the model to identify the number of sources, separate and understand the context, and infer the content of masked regions represented as discovered units. Cocktail HuBERT outperforms state-of-the-art results with 69% lower WER on multi-speaker ASR, 31% lower DER on diarization, and is competitive on single- and multi-speaker tasks from SUPERB.",
      "authors": [
        "Maryam Fazel-Zarandi",
        "Wei-Ning Hsu"
      ],
      "published": "2023-03-20T14:07:13Z",
      "updated": "2023-03-20T14:07:13Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.11131v1",
      "landing_url": "https://arxiv.org/abs/2303.11131v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.11131"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract references ‘discovered units’ and a self-supervised framework for mixture speech, indicating a discrete token-like representation used as targets, which aligns with the Discrete Audio Tokens topic, though explicit tokenization/quantization details are not fully stated in the abstract.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract references ‘discovered units’ and a self-supervised framework for mixture speech, indicating a discrete token-like representation used as targets, which aligns with the Discrete Audio Tokens topic, though explicit tokenization/quantization details are not fully stated in the abstract.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes a self-supervised learning framework (Cocktail HuBERT) for speech mixture separation and understanding but does not mention the generation or use of discrete audio tokens through vector quantization or similar discretization methods as defined in the inclusion criteria. There is no clear evidence of token formation mechanism or their usage in sequence/language modeling contexts as required. Hence, it does not meet the inclusion criteria focusing on discrete audio tokens.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The abstract describes a self-supervised learning framework (Cocktail HuBERT) for speech mixture separation and understanding but does not mention the generation or use of discrete audio tokens through vector quantization or similar discretization methods as defined in the inclusion criteria. There is no clear evidence of token formation mechanism or their usage in sequence/language modeling contexts as required. Hence, it does not meet the inclusion criteria focusing on discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers disagreed: one inferred the mention of “discovered units” might align with discrete tokens, while the other correctly noted the abstract lacks explicit tokenization/quantization details. The abstract focuses on a self‑supervised pretraining objective for mixture speech and reports ASR/diarization gains but does not describe a discrete token design (e.g., VQ/RVQ/clustering pipeline, codebook/layer design, or use of tokens for sequence/language modeling) nor provide reproducible token-generation details required by the inclusion criteria—so it does not meet the bar for a Discrete Audio Tokens contribution.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers disagreed: one inferred the mention of “discovered units” might align with discrete tokens, while the other correctly noted the abstract lacks explicit tokenization/quantization details. The abstract focuses on a self‑supervised pretraining objective for mixture speech and reports ASR/diarization gains but does not describe a discrete token design (e.g., VQ/RVQ/clustering pipeline, codebook/layer design, or use of tokens for sequence/language modeling) nor provide reproducible token-generation details required by the inclusion criteria—so it does not meet the bar for a Discrete Audio Tokens contribution.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Practice of the conformer enhanced AUDIO-VISUAL HUBERT on Mandarin and English",
    "abstract": "Considering the bimodal nature of human speech perception, lips, and teeth movement has a pivotal role in automatic speech recognition. Benefiting from the correlated and noise-invariant visual information, audio-visual recognition systems enhance robustness in multiple scenarios. In previous work, audio-visual HuBERT appears to be the finest practice incorporating modality knowledge. This paper outlines a mixed methodology, named conformer enhanced AV-HuBERT, boosting the AV-HuBERT system's performance a step further. Compared with baseline AV-HuBERT, our method in the one-phase evaluation of clean and noisy conditions achieves 7% and 16% relative WER reduction on the English AVSR benchmark dataset LRS3. Furthermore, we establish a novel 1000h Mandarin AVSR dataset CSTS. On top of the baseline AV-HuBERT, we exceed the WeNet ASR system by 14% and 18% relatively on MISP and CMLR by pre-training with this dataset. The conformer-enhanced AV-HuBERT we proposed brings 7% on MISP and 6% CER reduction on CMLR, compared with the baseline AV-HuBERT system.",
    "metadata": {
      "arxiv_id": "2303.12187",
      "title": "Practice of the conformer enhanced AUDIO-VISUAL HUBERT on Mandarin and English",
      "summary": "Considering the bimodal nature of human speech perception, lips, and teeth movement has a pivotal role in automatic speech recognition. Benefiting from the correlated and noise-invariant visual information, audio-visual recognition systems enhance robustness in multiple scenarios. In previous work, audio-visual HuBERT appears to be the finest practice incorporating modality knowledge. This paper outlines a mixed methodology, named conformer enhanced AV-HuBERT, boosting the AV-HuBERT system's performance a step further. Compared with baseline AV-HuBERT, our method in the one-phase evaluation of clean and noisy conditions achieves 7% and 16% relative WER reduction on the English AVSR benchmark dataset LRS3. Furthermore, we establish a novel 1000h Mandarin AVSR dataset CSTS. On top of the baseline AV-HuBERT, we exceed the WeNet ASR system by 14% and 18% relatively on MISP and CMLR by pre-training with this dataset. The conformer-enhanced AV-HuBERT we proposed brings 7% on MISP and 6% CER reduction on CMLR, compared with the baseline AV-HuBERT system.",
      "authors": [
        "Xiaoming Ren",
        "Chao Li",
        "Shenjian Wang",
        "Biao Li"
      ],
      "published": "2023-02-28T02:10:13Z",
      "updated": "2023-02-28T02:10:13Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.12187v1",
      "landing_url": "https://arxiv.org/abs/2303.12187v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.12187"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on audio-visual speech recognition (AV-HuBERT) and conformer-enhanced AV-HuBERT without mentioning discrete audio tokens, token generation, quantization, or other token-based representations, so it does not meet the discrete audio token criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on audio-visual speech recognition (AV-HuBERT) and conformer-enhanced AV-HuBERT without mentioning discrete audio tokens, token generation, quantization, or other token-based representations, so it does not meet the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on improving an audio-visual speech recognition system (AV-HuBERT) and presents performance improvements on English and Mandarin datasets. However, it does not describe the design or use of discrete audio tokens, vector quantization, or related discrete token generation processes that are core to the inclusion criteria. There is no mention of tokenization, quantization, or discrete representation of audio in the abstract, and the study seems centered on model architecture and performance metrics rather than discrete audio token creation or usage for sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on improving an audio-visual speech recognition system (AV-HuBERT) and presents performance improvements on English and Mandarin datasets. However, it does not describe the design or use of discrete audio tokens, vector quantization, or related discrete token generation processes that are core to the inclusion criteria. There is no mention of tokenization, quantization, or discrete representation of audio in the abstract, and the study seems centered on model architecture and performance metrics rather than discrete audio token creation or usage for sequence modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Self-Supervised Representations for Singing Voice Conversion",
    "abstract": "A singing voice conversion model converts a song in the voice of an arbitrary source singer to the voice of a target singer. Recently, methods that leverage self-supervised audio representations such as HuBERT and Wav2Vec 2.0 have helped further the state-of-the-art. Though these methods produce more natural and melodic singing outputs, they often rely on confusion and disentanglement losses to render the self-supervised representations speaker and pitch-invariant. In this paper, we circumvent disentanglement training and propose a new model that leverages ASR fine-tuned self-supervised representations as inputs to a HiFi-GAN neural vocoder for singing voice conversion. We experiment with different f0 encoding schemes and show that an f0 harmonic generation module that uses a parallel bank of transposed convolutions (PBTC) alongside ASR fine-tuned Wav2Vec 2.0 features results in the best singing voice conversion quality. Additionally, the model is capable of making a spoken voice sing. We also show that a simple f0 shifting scheme during inference helps retain singer identity and bolsters the performance of our singing voice conversion model. Our results are backed up by extensive MOS studies that compare different ablations and baselines.",
    "metadata": {
      "arxiv_id": "2303.12197",
      "title": "Self-Supervised Representations for Singing Voice Conversion",
      "summary": "A singing voice conversion model converts a song in the voice of an arbitrary source singer to the voice of a target singer. Recently, methods that leverage self-supervised audio representations such as HuBERT and Wav2Vec 2.0 have helped further the state-of-the-art. Though these methods produce more natural and melodic singing outputs, they often rely on confusion and disentanglement losses to render the self-supervised representations speaker and pitch-invariant. In this paper, we circumvent disentanglement training and propose a new model that leverages ASR fine-tuned self-supervised representations as inputs to a HiFi-GAN neural vocoder for singing voice conversion. We experiment with different f0 encoding schemes and show that an f0 harmonic generation module that uses a parallel bank of transposed convolutions (PBTC) alongside ASR fine-tuned Wav2Vec 2.0 features results in the best singing voice conversion quality. Additionally, the model is capable of making a spoken voice sing. We also show that a simple f0 shifting scheme during inference helps retain singer identity and bolsters the performance of our singing voice conversion model. Our results are backed up by extensive MOS studies that compare different ablations and baselines.",
      "authors": [
        "Tejas Jayashankar",
        "Jilong Wu",
        "Leda Sari",
        "David Kant",
        "Vimal Manohar",
        "Qing He"
      ],
      "published": "2023-03-21T21:04:03Z",
      "updated": "2023-03-21T21:04:03Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.12197v1",
      "landing_url": "https://arxiv.org/abs/2303.12197v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.12197"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on self-supervised representations for singing voice conversion and a vocoder-based pipeline, not on discrete audio tokens or quantization/token design; it does not describe a discrete token generation/quantization method, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on self-supervised representations for singing voice conversion and a vocoder-based pipeline, not on discrete audio tokens or quantization/token design; it does not describe a discrete token generation/quantization method, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on singing voice conversion using self-supervised audio representations like HuBERT and Wav2Vec 2.0 features, but does not discuss the generation or use of discrete audio tokens produced by vector quantization or similar methods for sequence modeling, which is a key inclusion criterion. The paper emphasizes continuous self-supervised features rather than discrete token representations, and no mention is made of token quantization or token-based modeling, hence it does not meet the core requirements of the topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on singing voice conversion using self-supervised audio representations like HuBERT and Wav2Vec 2.0 features, but does not discuss the generation or use of discrete audio tokens produced by vector quantization or similar methods for sequence modeling, which is a key inclusion criterion. The paper emphasizes continuous self-supervised features rather than discrete token representations, and no mention is made of token quantization or token-based modeling, hence it does not meet the core requirements of the topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Wave-U-Net Discriminator: Fast and Lightweight Discriminator for Generative Adversarial Network-Based Speech Synthesis",
    "abstract": "In speech synthesis, a generative adversarial network (GAN), training a generator (speech synthesizer) and a discriminator in a min-max game, is widely used to improve speech quality. An ensemble of discriminators is commonly used in recent neural vocoders (e.g., HiFi-GAN) and end-to-end text-to-speech (TTS) systems (e.g., VITS) to scrutinize waveforms from multiple perspectives. Such discriminators allow synthesized speech to adequately approach real speech; however, they require an increase in the model size and computation time according to the increase in the number of discriminators. Alternatively, this study proposes a Wave-U-Net discriminator, which is a single but expressive discriminator with Wave-U-Net architecture. This discriminator is unique; it can assess a waveform in a sample-wise manner with the same resolution as the input signal, while extracting multilevel features via an encoder and decoder with skip connections. This architecture provides a generator with sufficiently rich information for the synthesized speech to be closely matched to the real speech. During the experiments, the proposed ideas were applied to a representative neural vocoder (HiFi-GAN) and an end-to-end TTS system (VITS). The results demonstrate that the proposed models can achieve comparable speech quality with a 2.31 times faster and 14.5 times more lightweight discriminator when used in HiFi-GAN and a 1.90 times faster and 9.62 times more lightweight discriminator when used in VITS. Audio samples are available at https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/waveunetd/.",
    "metadata": {
      "arxiv_id": "2303.13909",
      "title": "Wave-U-Net Discriminator: Fast and Lightweight Discriminator for Generative Adversarial Network-Based Speech Synthesis",
      "summary": "In speech synthesis, a generative adversarial network (GAN), training a generator (speech synthesizer) and a discriminator in a min-max game, is widely used to improve speech quality. An ensemble of discriminators is commonly used in recent neural vocoders (e.g., HiFi-GAN) and end-to-end text-to-speech (TTS) systems (e.g., VITS) to scrutinize waveforms from multiple perspectives. Such discriminators allow synthesized speech to adequately approach real speech; however, they require an increase in the model size and computation time according to the increase in the number of discriminators. Alternatively, this study proposes a Wave-U-Net discriminator, which is a single but expressive discriminator with Wave-U-Net architecture. This discriminator is unique; it can assess a waveform in a sample-wise manner with the same resolution as the input signal, while extracting multilevel features via an encoder and decoder with skip connections. This architecture provides a generator with sufficiently rich information for the synthesized speech to be closely matched to the real speech. During the experiments, the proposed ideas were applied to a representative neural vocoder (HiFi-GAN) and an end-to-end TTS system (VITS). The results demonstrate that the proposed models can achieve comparable speech quality with a 2.31 times faster and 14.5 times more lightweight discriminator when used in HiFi-GAN and a 1.90 times faster and 9.62 times more lightweight discriminator when used in VITS. Audio samples are available at https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/waveunetd/.",
      "authors": [
        "Takuhiro Kaneko",
        "Hirokazu Kameoka",
        "Kou Tanaka",
        "Shogo Seki"
      ],
      "published": "2023-03-24T10:46:40Z",
      "updated": "2023-03-24T10:46:40Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.13909v1",
      "landing_url": "https://arxiv.org/abs/2303.13909v1",
      "doi": null
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article proposes a Wave-U-Net discriminator for GAN-based speech synthesis, focusing on discriminator efficiency; it does not address discrete audio tokens, tokenization methods (VQ/RVQ, hierarchical quantization), or token-based audio language modeling, thus fails inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article proposes a Wave-U-Net discriminator for GAN-based speech synthesis, focusing on discriminator efficiency; it does not address discrete audio tokens, tokenization methods (VQ/RVQ, hierarchical quantization), or token-based audio language modeling, thus fails inclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on proposing a Wave-U-Net discriminator architecture for GAN-based speech synthesis, aiming to improve discriminator efficiency and speech quality. However, the work centers on the design and evaluation of discriminator models for speech waveform assessment and generation quality improvement, without discussing or incorporating a discrete audio tokenization process (such as vector quantization or discrete token generation) that transforms continuous audio into discrete symbolic representations for sequence modeling or language modeling. There is no mention of token generation, quantization methods, codebook design, or downstream use of discrete tokens in audio language models or related tasks. Therefore, it does not address the core inclusion criteria about discrete audio token representations and their generation or application.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on proposing a Wave-U-Net discriminator architecture for GAN-based speech synthesis, aiming to improve discriminator efficiency and speech quality. However, the work centers on the design and evaluation of discriminator models for speech waveform assessment and generation quality improvement, without discussing or incorporating a discrete audio tokenization process (such as vector quantization or discrete token generation) that transforms continuous audio into discrete symbolic representations for sequence modeling or language modeling. There is no mention of token generation, quantization methods, codebook design, or downstream use of discrete tokens in audio language models or related tasks. Therefore, it does not address the core inclusion criteria about discrete audio token representations and their generation or application.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Your Diffusion Model is Secretly a Zero-Shot Classifier",
    "abstract": "The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation. In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification, which we call Diffusion Classifier, attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. Although a gap remains between generative and discriminative approaches on zero-shot recognition tasks, our diffusion-based approach has significantly stronger multimodal compositional reasoning ability than competing discriminative approaches. Finally, we use Diffusion Classifier to extract standard classifiers from class-conditional diffusion models trained on ImageNet. Our models achieve strong classification performance using only weak augmentations and exhibit qualitatively better \"effective robustness\" to distribution shift. Overall, our results are a step toward using generative over discriminative models for downstream tasks. Results and visualizations at https://diffusion-classifier.github.io/",
    "metadata": {
      "arxiv_id": "2303.16203",
      "title": "Your Diffusion Model is Secretly a Zero-Shot Classifier",
      "summary": "The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation. In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification, which we call Diffusion Classifier, attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. Although a gap remains between generative and discriminative approaches on zero-shot recognition tasks, our diffusion-based approach has significantly stronger multimodal compositional reasoning ability than competing discriminative approaches. Finally, we use Diffusion Classifier to extract standard classifiers from class-conditional diffusion models trained on ImageNet. Our models achieve strong classification performance using only weak augmentations and exhibit qualitatively better \"effective robustness\" to distribution shift. Overall, our results are a step toward using generative over discriminative models for downstream tasks. Results and visualizations at https://diffusion-classifier.github.io/",
      "authors": [
        "Alexander C. Li",
        "Mihir Prabhudesai",
        "Shivam Duggal",
        "Ellis Brown",
        "Deepak Pathak"
      ],
      "published": "2023-03-28T17:59:56Z",
      "updated": "2023-09-13T01:16:45Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.NE",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.16203v3",
      "landing_url": "https://arxiv.org/abs/2303.16203v3",
      "doi": "https://doi.org/10.48550/arXiv.2303.16203"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about diffusion models for image-based zero-shot classification and does not address discrete audio tokens, token generation, or audio-language modeling required by the inclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about diffusion models for image-based zero-shot classification and does not address discrete audio tokens, token generation, or audio-language modeling required by the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a method for zero-shot classification using text-to-image diffusion models, which is unrelated to the study of discrete audio tokens or audio representation through vector quantization or similar techniques as defined in the inclusion criteria. There is no mention of audio tokens, audio processing, or discrete token generation from audio signals, and thus the content does not meet the inclusion criteria nor relate to the defined research area.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a method for zero-shot classification using text-to-image diffusion models, which is unrelated to the study of discrete audio tokens or audio representation through vector quantization or similar techniques as defined in the inclusion criteria. There is no mention of audio tokens, audio processing, or discrete token generation from audio signals, and thus the content does not meet the inclusion criteria nor relate to the defined research area.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multilingual Word Error Rate Estimation: e-WER3",
    "abstract": "The success of the multilingual automatic speech recognition systems empowered many voice-driven applications. However, measuring the performance of such systems remains a major challenge, due to its dependency on manually transcribed speech data in both mono- and multilingual scenarios. In this paper, we propose a novel multilingual framework -- eWER3 -- jointly trained on acoustic and lexical representation to estimate word error rate. We demonstrate the effectiveness of eWER3 to (i) predict WER without using any internal states from the ASR and (ii) use the multilingual shared latent space to push the performance of the close-related languages. We show our proposed multilingual model outperforms the previous monolingual word error rate estimation method (eWER2) by an absolute 9\\% increase in Pearson correlation coefficient (PCC), with better overall estimation between the predicted and reference WER.",
    "metadata": {
      "arxiv_id": "2304.00649",
      "title": "Multilingual Word Error Rate Estimation: e-WER3",
      "summary": "The success of the multilingual automatic speech recognition systems empowered many voice-driven applications. However, measuring the performance of such systems remains a major challenge, due to its dependency on manually transcribed speech data in both mono- and multilingual scenarios. In this paper, we propose a novel multilingual framework -- eWER3 -- jointly trained on acoustic and lexical representation to estimate word error rate. We demonstrate the effectiveness of eWER3 to (i) predict WER without using any internal states from the ASR and (ii) use the multilingual shared latent space to push the performance of the close-related languages. We show our proposed multilingual model outperforms the previous monolingual word error rate estimation method (eWER2) by an absolute 9\\% increase in Pearson correlation coefficient (PCC), with better overall estimation between the predicted and reference WER.",
      "authors": [
        "Shammur Absar Chowdhury",
        "Ahmed Ali"
      ],
      "published": "2023-04-02T23:08:11Z",
      "updated": "2023-04-02T23:08:11Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.00649v1",
      "landing_url": "https://arxiv.org/abs/2304.00649v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.00649"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on multilingual WER estimation and does not involve discrete audio token design/quantization methods, thus it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on multilingual WER estimation and does not involve discrete audio token design/quantization methods, thus it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a multilingual word error rate estimation model (e-WER3) for automatic speech recognition, focusing on predicting WER without internal ASR states. However, it does not discuss discrete audio token generation, vector quantization, or any form of discretization of continuous audio signals into tokens for downstream sequence modeling or language modeling, which is essential for inclusion. Therefore, it fails to meet the core inclusion criteria focused on discrete audio token design and usage and does not provide methods that relate to emergent discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents a multilingual word error rate estimation model (e-WER3) for automatic speech recognition, focusing on predicting WER without internal ASR states. However, it does not discuss discrete audio token generation, vector quantization, or any form of discretization of continuous audio signals into tokens for downstream sequence modeling or language modeling, which is essential for inclusion. Therefore, it fails to meet the core inclusion criteria focused on discrete audio token design and usage and does not provide methods that relate to emergent discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Pac-HuBERT: Self-Supervised Music Source Separation via Primitive Auditory Clustering and Hidden-Unit BERT",
    "abstract": "In spite of the progress in music source separation research, the small amount of publicly-available clean source data remains a constant limiting factor for performance. Thus, recent advances in self-supervised learning present a largely-unexplored opportunity for improving separation models by leveraging unlabelled music data. In this paper, we propose a self-supervised learning framework for music source separation inspired by the HuBERT speech representation model. We first investigate the potential impact of the original HuBERT model by inserting an adapted version of it into the well-known Demucs V2 time-domain separation model architecture. We then propose a time-frequency-domain self-supervised model, Pac-HuBERT (for primitive auditory clustering HuBERT), that we later use in combination with a Res-U-Net decoder for source separation. Pac-HuBERT uses primitive auditory features of music as unsupervised clustering labels to initialize the self-supervised pretraining process using the Free Music Archive (FMA) dataset. The resulting framework achieves better source-to-distortion ratio (SDR) performance on the MusDB18 test set than the original Demucs V2 and Res-U-Net models. We further demonstrate that it can boost performance with small amounts of supervised data. Ultimately, our proposed framework is an effective solution to the challenge of limited clean source data for music source separation.",
    "metadata": {
      "arxiv_id": "2304.02160",
      "title": "Pac-HuBERT: Self-Supervised Music Source Separation via Primitive Auditory Clustering and Hidden-Unit BERT",
      "summary": "In spite of the progress in music source separation research, the small amount of publicly-available clean source data remains a constant limiting factor for performance. Thus, recent advances in self-supervised learning present a largely-unexplored opportunity for improving separation models by leveraging unlabelled music data. In this paper, we propose a self-supervised learning framework for music source separation inspired by the HuBERT speech representation model. We first investigate the potential impact of the original HuBERT model by inserting an adapted version of it into the well-known Demucs V2 time-domain separation model architecture. We then propose a time-frequency-domain self-supervised model, Pac-HuBERT (for primitive auditory clustering HuBERT), that we later use in combination with a Res-U-Net decoder for source separation. Pac-HuBERT uses primitive auditory features of music as unsupervised clustering labels to initialize the self-supervised pretraining process using the Free Music Archive (FMA) dataset. The resulting framework achieves better source-to-distortion ratio (SDR) performance on the MusDB18 test set than the original Demucs V2 and Res-U-Net models. We further demonstrate that it can boost performance with small amounts of supervised data. Ultimately, our proposed framework is an effective solution to the challenge of limited clean source data for music source separation.",
      "authors": [
        "Ke Chen",
        "Gordon Wichern",
        "François G. Germain",
        "Jonathan Le Roux"
      ],
      "published": "2023-04-04T23:19:53Z",
      "updated": "2023-04-04T23:19:53Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.02160v1",
      "landing_url": "https://arxiv.org/abs/2304.02160v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.02160"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The Pac-HuBERT paper uses primitive auditory clustering to generate discrete targets (tokens) for self-supervised learning in music source separation, aligning with the discrete audio token theme and providing method-level details (clustering targets within a HuBERT framework) and performance evaluation, thus fitting the inclusion criteria better than exclusion.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The Pac-HuBERT paper uses primitive auditory clustering to generate discrete targets (tokens) for self-supervised learning in music source separation, aligning with the discrete audio token theme and providing method-level details (clustering targets within a HuBERT framework) and performance evaluation, thus fitting the inclusion criteria better than exclusion.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a self-supervised learning framework for music source separation, leveraging primitive auditory clustering and HuBERT inspired models. However, the abstract does not explicitly describe the process of generating discrete audio tokens via vector quantization, residual vector quantization, or other discrete tokenization mechanisms that retain semantic or acoustic information for downstream sequence modeling. Although it involves self-supervised learning and clustering, there is no clear evidence that a token set is generated and used as discrete tokens representing audio for language model style processing. The method is centered more on feature clustering as pseudo-labels for pretraining rather than a discrete token output designed for audio language modeling or sequence modeling. Therefore, it does not fully meet the inclusion criteria related to discrete audio token generation as a core method and contribution.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a self-supervised learning framework for music source separation, leveraging primitive auditory clustering and HuBERT inspired models. However, the abstract does not explicitly describe the process of generating discrete audio tokens via vector quantization, residual vector quantization, or other discrete tokenization mechanisms that retain semantic or acoustic information for downstream sequence modeling. Although it involves self-supervised learning and clustering, there is no clear evidence that a token set is generated and used as discrete tokens representing audio for language model style processing. The method is centered more on feature clustering as pseudo-labels for pretraining rather than a discrete token output designed for audio language modeling or sequence modeling. Therefore, it does not fully meet the inclusion criteria related to discrete audio token generation as a core method and contribution.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers raised valid points: JuniorNano leans to include, noting Pac-HuBERT employs primitive auditory clustering within a HuBERT-style SSL framework (which aligns with discrete-token via clustering themes), while JuniorMini cautions the abstract does not explicitly describe generation of discrete tokens intended for language/sequence modeling or give quantization/codebook details. The abstract, however, clearly states that Pac-HuBERT uses unsupervised clustering of primitive auditory features as labels to initialize self-supervised pretraining (i.e., discrete cluster targets akin to HuBERT’s discrete units) and that this is central to the proposed method and its downstream use for source separation. This fits the inclusion pathway of SSL representations being discretized via clustering to produce semantic/discrete targets used for downstream tasks, even though the abstract lacks low-level quantization/codebook details and does not target audio-language generation per se. Given the ambiguity in token-generation implementation specifics in the abstract but the clear use of clustering-as-discrete-targets as a core method, I recommend inclusion as “better to include” rather than absolute include.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers raised valid points: JuniorNano leans to include, noting Pac-HuBERT employs primitive auditory clustering within a HuBERT-style SSL framework (which aligns with discrete-token via clustering themes), while JuniorMini cautions the abstract does not explicitly describe generation of discrete tokens intended for language/sequence modeling or give quantization/codebook details. The abstract, however, clearly states that Pac-HuBERT uses unsupervised clustering of primitive auditory features as labels to initialize self-supervised pretraining (i.e., discrete cluster targets akin to HuBERT’s discrete units) and that this is central to the proposed method and its downstream use for source separation. This fits the inclusion pathway of SSL representations being discretized via clustering to produce semantic/discrete targets used for downstream tasks, even though the abstract lacks low-level quantization/codebook details and does not target audio-language generation per se. Given the ambiguity in token-generation implementation specifics in the abstract but the clear use of clustering-as-discrete-targets as a core method, I recommend inclusion as “better to include” rather than absolute include.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Unsupervised Speech Representation Pooling Using Vector Quantization",
    "abstract": "With the advent of general-purpose speech representations from large-scale self-supervised models, applying a single model to multiple downstream tasks is becoming a de-facto approach. However, the pooling problem remains; the length of speech representations is inherently variable. The naive average pooling is often used, even though it ignores the characteristics of speech, such as differently lengthed phonemes. Hence, we design a novel pooling method to squash acoustically similar representations via vector quantization, which does not require additional training, unlike attention-based pooling. Further, we evaluate various unsupervised pooling methods on various self-supervised models. We gather diverse methods scattered around speech and text to evaluate on various tasks: keyword spotting, speaker identification, intent classification, and emotion recognition. Finally, we quantitatively and qualitatively analyze our method, comparing it with supervised pooling methods.",
    "metadata": {
      "arxiv_id": "2304.03940",
      "title": "Unsupervised Speech Representation Pooling Using Vector Quantization",
      "summary": "With the advent of general-purpose speech representations from large-scale self-supervised models, applying a single model to multiple downstream tasks is becoming a de-facto approach. However, the pooling problem remains; the length of speech representations is inherently variable. The naive average pooling is often used, even though it ignores the characteristics of speech, such as differently lengthed phonemes. Hence, we design a novel pooling method to squash acoustically similar representations via vector quantization, which does not require additional training, unlike attention-based pooling. Further, we evaluate various unsupervised pooling methods on various self-supervised models. We gather diverse methods scattered around speech and text to evaluate on various tasks: keyword spotting, speaker identification, intent classification, and emotion recognition. Finally, we quantitatively and qualitatively analyze our method, comparing it with supervised pooling methods.",
      "authors": [
        "Jeongkyun Park",
        "Kwanghee Choi",
        "Hyunjun Heo",
        "Hyung-Min Park"
      ],
      "published": "2023-04-08T07:03:01Z",
      "updated": "2023-04-08T07:03:01Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.03940v1",
      "landing_url": "https://arxiv.org/abs/2304.03940v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.03940"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although it uses Vector Quantization, the paper focuses on pooling unsupervised speech representations for downstream tasks and does not present discrete audio token generation/quantization designed for audio language modeling or token-based sequence modeling; thus it does not meet the core inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although it uses Vector Quantization, the paper focuses on pooling unsupervised speech representations for downstream tasks and does not present discrete audio token generation/quantization designed for audio language modeling or token-based sequence modeling; thus it does not meet the core inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses a pooling method using vector quantization (VQ) for speech representations from self-supervised models. However, it does not clearly describe a process of generating discrete audio tokens as a main contribution; instead, VQ is used as a pooling step rather than producing quantized tokens for sequence modeling. The paper lacks detailed explanation of token generation, codebook design, or usage of the tokens for language or sequence models, which are key inclusion criteria. Therefore, it does not fully meet the inclusion criteria of focusing on discrete audio token design as core method or contribution.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses a pooling method using vector quantization (VQ) for speech representations from self-supervised models. However, it does not clearly describe a process of generating discrete audio tokens as a main contribution; instead, VQ is used as a pooling step rather than producing quantized tokens for sequence modeling. The paper lacks detailed explanation of token generation, codebook design, or usage of the tokens for language or sequence models, which are key inclusion criteria. Therefore, it does not fully meet the inclusion criteria of focusing on discrete audio token design as core method or contribution.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder",
    "abstract": "The sequence-to-sequence (seq2seq) task aims at generating the target sequence based on the given input source sequence. Traditionally, most of the seq2seq task is resolved by the Encoder-Decoder framework which requires an encoder to encode the source sequence and a decoder to generate the target text. Recently, a bunch of new approaches have emerged that apply decoder-only language models directly to the seq2seq task. Despite the significant advancements in applying language models to the seq2seq task, there is still a lack of thorough analysis on the effectiveness of the decoder-only language model architecture. This paper aims to address this gap by conducting a detailed comparison between the encoder-decoder architecture and the decoder-only language model framework through the analysis of a regularized encoder-decoder structure. This structure is designed to replicate all behaviors in the classical decoder-only language model but has an encoder and a decoder making it easier to be compared with the classical encoder-decoder structure. Based on the analysis, we unveil the attention degeneration problem in the language model, namely, as the generation step number grows, less and less attention is focused on the source sequence. To give a quantitative understanding of this problem, we conduct a theoretical sensitivity analysis of the attention output with respect to the source input. Grounded on our analysis, we propose a novel partial attention language model to solve the attention degeneration problem. Experimental results on machine translation, summarization, and data-to-text generation tasks support our analysis and demonstrate the effectiveness of our proposed model.",
    "metadata": {
      "arxiv_id": "2304.04052",
      "title": "Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder",
      "summary": "The sequence-to-sequence (seq2seq) task aims at generating the target sequence based on the given input source sequence. Traditionally, most of the seq2seq task is resolved by the Encoder-Decoder framework which requires an encoder to encode the source sequence and a decoder to generate the target text. Recently, a bunch of new approaches have emerged that apply decoder-only language models directly to the seq2seq task. Despite the significant advancements in applying language models to the seq2seq task, there is still a lack of thorough analysis on the effectiveness of the decoder-only language model architecture. This paper aims to address this gap by conducting a detailed comparison between the encoder-decoder architecture and the decoder-only language model framework through the analysis of a regularized encoder-decoder structure. This structure is designed to replicate all behaviors in the classical decoder-only language model but has an encoder and a decoder making it easier to be compared with the classical encoder-decoder structure. Based on the analysis, we unveil the attention degeneration problem in the language model, namely, as the generation step number grows, less and less attention is focused on the source sequence. To give a quantitative understanding of this problem, we conduct a theoretical sensitivity analysis of the attention output with respect to the source input. Grounded on our analysis, we propose a novel partial attention language model to solve the attention degeneration problem. Experimental results on machine translation, summarization, and data-to-text generation tasks support our analysis and demonstrate the effectiveness of our proposed model.",
      "authors": [
        "Zihao Fu",
        "Wai Lam",
        "Qian Yu",
        "Anthony Man-Cho So",
        "Shengding Hu",
        "Zhiyuan Liu",
        "Nigel Collier"
      ],
      "published": "2023-04-08T15:44:29Z",
      "updated": "2023-04-08T15:44:29Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.04052v1",
      "landing_url": "https://arxiv.org/abs/2304.04052v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.04052"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper analyzes encoder-decoder vs decoder-only language models for seq2seq tasks, focusing on attention and regularization, with no discussion of discrete audio tokens or audio tokenization methodologies, thus it does not meet the inclusion criteria for discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper analyzes encoder-decoder vs decoder-only language models for seq2seq tasks, focusing on attention and regularization, with no discussion of discrete audio tokens or audio tokenization methodologies, thus it does not meet the inclusion criteria for discrete audio token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on comparing language model architectures for sequence-to-sequence tasks like machine translation and summarization, without any mention of discrete audio tokens, audio signal quantization, or related audio representation techniques as specified in the inclusion criteria. There is no indication that the paper proposes or evaluates discrete audio token designs or methodologies relevant to audio language modeling. Therefore, it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on comparing language model architectures for sequence-to-sequence tasks like machine translation and summarization, without any mention of discrete audio tokens, audio signal quantization, or related audio representation techniques as specified in the inclusion criteria. There is no indication that the paper proposes or evaluates discrete audio token designs or methodologies relevant to audio language modeling. Therefore, it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Wav2code: Restore Clean Speech Representations via Codebook Lookup for Noise-Robust ASR",
    "abstract": "Automatic speech recognition (ASR) has gained remarkable successes thanks to recent advances of deep learning, but it usually degrades significantly under real-world noisy conditions. Recent works introduce speech enhancement (SE) as front-end to improve speech quality, which is proved effective but may not be optimal for downstream ASR due to speech distortion problem. Based on that, latest works combine SE and currently popular self-supervised learning (SSL) to alleviate distortion and improve noise robustness. Despite the effectiveness, the speech distortion caused by conventional SE still cannot be cleared out. In this paper, we propose a self-supervised framework named Wav2code to implement a feature-level SE with reduced distortions for noise-robust ASR. First, in pre-training stage the clean speech representations from SSL model are sent to lookup a discrete codebook via nearest-neighbor feature matching, the resulted code sequence are then exploited to reconstruct the original clean representations, in order to store them in codebook as prior. Second, during finetuning we propose a Transformer-based code predictor to accurately predict clean codes by modeling the global dependency of input noisy representations, which enables discovery and restoration of high-quality clean representations with reduced distortions. Furthermore, we propose an interactive feature fusion network to combine original noisy and the restored clean representations to consider both fidelity and quality, resulting in more informative features for downstream ASR. Finally, experiments on both synthetic and real noisy datasets demonstrate that Wav2code can solve the speech distortion and improve ASR performance under various noisy conditions, resulting in stronger robustness.",
    "metadata": {
      "arxiv_id": "2304.04974",
      "title": "Wav2code: Restore Clean Speech Representations via Codebook Lookup for Noise-Robust ASR",
      "summary": "Automatic speech recognition (ASR) has gained remarkable successes thanks to recent advances of deep learning, but it usually degrades significantly under real-world noisy conditions. Recent works introduce speech enhancement (SE) as front-end to improve speech quality, which is proved effective but may not be optimal for downstream ASR due to speech distortion problem. Based on that, latest works combine SE and currently popular self-supervised learning (SSL) to alleviate distortion and improve noise robustness. Despite the effectiveness, the speech distortion caused by conventional SE still cannot be cleared out. In this paper, we propose a self-supervised framework named Wav2code to implement a feature-level SE with reduced distortions for noise-robust ASR. First, in pre-training stage the clean speech representations from SSL model are sent to lookup a discrete codebook via nearest-neighbor feature matching, the resulted code sequence are then exploited to reconstruct the original clean representations, in order to store them in codebook as prior. Second, during finetuning we propose a Transformer-based code predictor to accurately predict clean codes by modeling the global dependency of input noisy representations, which enables discovery and restoration of high-quality clean representations with reduced distortions. Furthermore, we propose an interactive feature fusion network to combine original noisy and the restored clean representations to consider both fidelity and quality, resulting in more informative features for downstream ASR. Finally, experiments on both synthetic and real noisy datasets demonstrate that Wav2code can solve the speech distortion and improve ASR performance under various noisy conditions, resulting in stronger robustness.",
      "authors": [
        "Yuchen Hu",
        "Chen Chen",
        "Qiushi Zhu",
        "Eng Siong Chng"
      ],
      "published": "2023-04-11T04:46:12Z",
      "updated": "2024-04-18T06:39:57Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.04974v3",
      "landing_url": "https://arxiv.org/abs/2304.04974v3",
      "doi": "https://doi.org/10.48550/arXiv.2304.04974"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work uses a discrete codebook with nearest-neighbor lookup to convert SSL-derived speech representations into discrete tokens, followed by a code predictor and fusion for ASR robustness, and it provides enough methodological detail (token formation via codebook/quantization, training/inference setup) with empirical validation, aligning with the discrete audio tokens inclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work uses a discrete codebook with nearest-neighbor lookup to convert SSL-derived speech representations into discrete tokens, followed by a code predictor and fusion for ASR robustness, and it provides enough methodological detail (token formation via codebook/quantization, training/inference setup) with empirical validation, aligning with the discrete audio tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes Wav2code, a self-supervised framework that uses a discrete codebook to restore clean speech representations through nearest-neighbor feature matching, which involves generating and using discrete tokens (codes) derived from speech features for noise-robust ASR. This aligns with the theme of discrete audio tokens generated via quantization mechanisms and used as intermediate representations for downstream tasks, with clear descriptions of token generation and usage in the context of sequence modeling. Therefore, it meets the inclusion criteria and does not fall under any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper proposes Wav2code, a self-supervised framework that uses a discrete codebook to restore clean speech representations through nearest-neighbor feature matching, which involves generating and using discrete tokens (codes) derived from speech features for noise-robust ASR. This aligns with the theme of discrete audio tokens generated via quantization mechanisms and used as intermediate representations for downstream tasks, with clear descriptions of token generation and usage in the context of sequence modeling. Therefore, it meets the inclusion criteria and does not fall under any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Coded Speech Quality Measurement by a Non-Intrusive PESQ-DNN",
    "abstract": "Wideband codecs such as AMR-WB or EVS are widely used in (mobile) speech communication. Evaluation of coded speech quality is often performed subjectively by an absolute category rating (ACR) listening test. However, the ACR test is impractical for online monitoring of speech communication networks. Perceptual evaluation of speech quality (PESQ) is one of the widely used metrics instrumentally predicting the results of an ACR test. However, the PESQ algorithm requires an original reference signal, which is usually unavailable in network monitoring, thus limiting its applicability. NISQA is a new non-intrusive neural-network-based speech quality measure, focusing on super-wideband speech signals. In this work, however, we aim at predicting the well-known PESQ metric using a non-intrusive PESQ-DNN model. We illustrate the potential of this model by predicting the PESQ scores of wideband-coded speech obtained from AMR-WB or EVS codecs operating at different bitrates in noisy, tandeming, and error-prone transmission conditions. We compare our methods with the state-of-the-art network topologies of QualityNet, WaweNet, and DNSMOS -- all applied to PESQ prediction -- by measuring the mean absolute error (MAE) and the linear correlation coefficient (LCC). The proposed PESQ-DNN offers the best total MAE and LCC of 0.11 and 0.92, respectively, in conditions without frame loss, and still is best when including frame loss. Note that our model could be similarly used to non-intrusively predict POLQA or other (intrusive) metrics. Upon article acceptance, code will be provided at GitHub.",
    "metadata": {
      "arxiv_id": "2304.09226",
      "title": "Coded Speech Quality Measurement by a Non-Intrusive PESQ-DNN",
      "summary": "Wideband codecs such as AMR-WB or EVS are widely used in (mobile) speech communication. Evaluation of coded speech quality is often performed subjectively by an absolute category rating (ACR) listening test. However, the ACR test is impractical for online monitoring of speech communication networks. Perceptual evaluation of speech quality (PESQ) is one of the widely used metrics instrumentally predicting the results of an ACR test. However, the PESQ algorithm requires an original reference signal, which is usually unavailable in network monitoring, thus limiting its applicability. NISQA is a new non-intrusive neural-network-based speech quality measure, focusing on super-wideband speech signals. In this work, however, we aim at predicting the well-known PESQ metric using a non-intrusive PESQ-DNN model. We illustrate the potential of this model by predicting the PESQ scores of wideband-coded speech obtained from AMR-WB or EVS codecs operating at different bitrates in noisy, tandeming, and error-prone transmission conditions. We compare our methods with the state-of-the-art network topologies of QualityNet, WaweNet, and DNSMOS -- all applied to PESQ prediction -- by measuring the mean absolute error (MAE) and the linear correlation coefficient (LCC). The proposed PESQ-DNN offers the best total MAE and LCC of 0.11 and 0.92, respectively, in conditions without frame loss, and still is best when including frame loss. Note that our model could be similarly used to non-intrusively predict POLQA or other (intrusive) metrics. Upon article acceptance, code will be provided at GitHub.",
      "authors": [
        "Ziyi Xu",
        "Ziyue Zhao",
        "Tim Fingscheidt"
      ],
      "published": "2023-04-18T18:26:56Z",
      "updated": "2023-04-18T18:26:56Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.09226v1",
      "landing_url": "https://arxiv.org/abs/2304.09226v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.09226"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets non-intrusive PESQ-DNN for predicting PESQ scores; it does not present discrete audio tokens or tokenization methods, thus does not meet the 'Discrete Audio Tokens' inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets non-intrusive PESQ-DNN for predicting PESQ scores; it does not present discrete audio tokens or tokenization methods, thus does not meet the 'Discrete Audio Tokens' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on predicting PESQ scores of wideband-coded speech using a non-intrusive PESQ-DNN model, which relates to speech quality measurement and codec evaluation. However, it does not discuss discrete audio token generation or the transformation of continuous audio into discrete token sequences for language or sequence modeling, nor does it describe any vector quantization or semantic disentanglement methods for token creation. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens as main research contributions.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on predicting PESQ scores of wideband-coded speech using a non-intrusive PESQ-DNN model, which relates to speech quality measurement and codec evaluation. However, it does not discuss discrete audio token generation or the transformation of continuous audio into discrete token sequences for language or sequence modeling, nor does it describe any vector quantization or semantic disentanglement methods for token creation. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens as main research contributions.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Just Noticeable Difference-aware Per-Scene Bitrate-laddering for Adaptive Video Streaming",
    "abstract": "In video streaming applications, a fixed set of bitrate-resolution pairs (known as a bitrate ladder) is typically used during the entire streaming session. However, an optimized bitrate ladder per scene may result in (i) decreased storage or delivery costs or/and (ii) increased Quality of Experience. This paper introduces a Just Noticeable Difference (JND)-aware per-scene bitrate ladder prediction scheme (JASLA) for adaptive video-on-demand streaming applications. JASLA predicts jointly optimized resolutions and corresponding constant rate factors (CRFs) using spatial and temporal complexity features for a given set of target bitrates for every scene, which yields an efficient constrained Variable Bitrate encoding. Moreover, bitrate-resolution pairs that yield distortion lower than one JND are eliminated. Experimental results show that, on average, JASLA yields bitrate savings of 34.42% and 42.67% to maintain the same PSNR and VMAF, respectively, compared to the reference HTTP Live Streaming (HLS) bitrate ladder Constant Bitrate encoding using x265 HEVC encoder, where the maximum resolution of streaming is Full HD (1080p). Moreover, a 54.34% average cumulative decrease in storage space is observed.",
    "metadata": {
      "arxiv_id": "2305.00225",
      "title": "Just Noticeable Difference-aware Per-Scene Bitrate-laddering for Adaptive Video Streaming",
      "summary": "In video streaming applications, a fixed set of bitrate-resolution pairs (known as a bitrate ladder) is typically used during the entire streaming session. However, an optimized bitrate ladder per scene may result in (i) decreased storage or delivery costs or/and (ii) increased Quality of Experience. This paper introduces a Just Noticeable Difference (JND)-aware per-scene bitrate ladder prediction scheme (JASLA) for adaptive video-on-demand streaming applications. JASLA predicts jointly optimized resolutions and corresponding constant rate factors (CRFs) using spatial and temporal complexity features for a given set of target bitrates for every scene, which yields an efficient constrained Variable Bitrate encoding. Moreover, bitrate-resolution pairs that yield distortion lower than one JND are eliminated. Experimental results show that, on average, JASLA yields bitrate savings of 34.42% and 42.67% to maintain the same PSNR and VMAF, respectively, compared to the reference HTTP Live Streaming (HLS) bitrate ladder Constant Bitrate encoding using x265 HEVC encoder, where the maximum resolution of streaming is Full HD (1080p). Moreover, a 54.34% average cumulative decrease in storage space is observed.",
      "authors": [
        "Vignesh V Menon",
        "Jingwen Zhu",
        "Prajit T Rajendran",
        "Hadi Amirpour",
        "Patrick Le Callet",
        "Christian Timmerer"
      ],
      "published": "2023-04-29T10:32:44Z",
      "updated": "2023-04-29T10:32:44Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.00225v1",
      "landing_url": "https://arxiv.org/abs/2305.00225v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.00225"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on JND-aware per-scene bitrate laddering for adaptive video streaming, not on discrete audio tokens or token-based audio representations, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on JND-aware per-scene bitrate laddering for adaptive video streaming, not on discrete audio tokens or token-based audio representations, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on video streaming and bitrate ladder optimization using Just Noticeable Difference (JND) for adaptive video streaming, which is unrelated to discrete audio token representations or their generation methods as defined in the inclusion criteria. There is no mention of discrete audio tokens, vector quantization for audio, or audio language modeling, thus it does not meet the inclusion criteria and aligns with the exclusion criteria of not addressing audio discrete tokens or related research contribution.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on video streaming and bitrate ladder optimization using Just Noticeable Difference (JND) for adaptive video streaming, which is unrelated to discrete audio token representations or their generation methods as defined in the inclusion criteria. There is no mention of discrete audio tokens, vector quantization for audio, or audio language modeling, thus it does not meet the inclusion criteria and aligns with the exclusion criteria of not addressing audio discrete tokens or related research contribution.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adversarial Defenses via Vector Quantization",
    "abstract": "Adversarial attacks pose significant challenges to the robustness of modern deep neural networks in computer vision, and defending these networks against adversarial attacks has attracted intense research efforts. Among various defense strategies, preprocessing-based defenses are practically appealing since there is no need to train the network under protection. However, such approaches typically do not achieve comparable robustness as other methods such as adversarial training. In this paper, we propose a novel framework for preprocessing-based defenses, where a vector quantizer is used as a preprocessor. This framework, inspired by and extended from Randomized Discretization (RandDisc), is theoretically principled by rate-distortion theory: indeed, RandDisc may be viewed as a scalar quantizer, and rate-distortion theory suggests that such quantization schemes are inferior to vector quantization. In our framework, the preprocessing vector quantizer treats the input image as a collection of patches and finds a set of representative patches based on the patch distributions; each original patch is then modified according to the representative patches close to it. We present two lightweight defenses in this framework, referred to as patched RandDisc (pRD) and sliding-window RandDisc (swRD), where the patches are disjoint in the former and overlapping in the latter. We show that vector-quantization-based defenses have certifiable robust accuracy and that pRD and swRD demonstrate state-of-the-art performances, surpassing RandDisc by a large margin. Notably, the proposed defenses possess the obfuscated gradients property. Our experiments however show that pRD and swRD remain effective under the STE and EOT attacks, which are designed specifically for defenses with gradient obfuscation. ...",
    "metadata": {
      "arxiv_id": "2305.13651",
      "title": "Adversarial Defenses via Vector Quantization",
      "summary": "Adversarial attacks pose significant challenges to the robustness of modern deep neural networks in computer vision, and defending these networks against adversarial attacks has attracted intense research efforts. Among various defense strategies, preprocessing-based defenses are practically appealing since there is no need to train the network under protection. However, such approaches typically do not achieve comparable robustness as other methods such as adversarial training. In this paper, we propose a novel framework for preprocessing-based defenses, where a vector quantizer is used as a preprocessor. This framework, inspired by and extended from Randomized Discretization (RandDisc), is theoretically principled by rate-distortion theory: indeed, RandDisc may be viewed as a scalar quantizer, and rate-distortion theory suggests that such quantization schemes are inferior to vector quantization. In our framework, the preprocessing vector quantizer treats the input image as a collection of patches and finds a set of representative patches based on the patch distributions; each original patch is then modified according to the representative patches close to it. We present two lightweight defenses in this framework, referred to as patched RandDisc (pRD) and sliding-window RandDisc (swRD), where the patches are disjoint in the former and overlapping in the latter. We show that vector-quantization-based defenses have certifiable robust accuracy and that pRD and swRD demonstrate state-of-the-art performances, surpassing RandDisc by a large margin. Notably, the proposed defenses possess the obfuscated gradients property. Our experiments however show that pRD and swRD remain effective under the STE and EOT attacks, which are designed specifically for defenses with gradient obfuscation. ...",
      "authors": [
        "Zhiyi Dong",
        "Yongyi Mao"
      ],
      "published": "2023-05-23T03:49:41Z",
      "updated": "2025-07-09T23:51:43Z",
      "categories": [
        "cs.LG",
        "cs.CR",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.13651v2",
      "landing_url": "https://arxiv.org/abs/2305.13651v2",
      "doi": "https://doi.org/10.1016/j.neucom.2025.130703"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on image-domain adversarial defenses using vector quantization (pRD, swRD) and rate-distortion theory; it does not involve discrete audio tokens or audio applications, so it fails the inclusion criteria for discrete audio token research and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on image-domain adversarial defenses using vector quantization (pRD, swRD) and rate-distortion theory; it does not involve discrete audio tokens or audio applications, so it fails the inclusion criteria for discrete audio token research and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on adversarial defenses in computer vision using vector quantization but does not address discrete audio tokens or audio-related representations; it lacks any mention of audio signal processing, token generation for audio, or applications to language or sequence models in audio domain, thus not meeting the inclusion criteria focused on discrete audio tokens, and it does not meet any exclusion criteria other than not being relevant.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on adversarial defenses in computer vision using vector quantization but does not address discrete audio tokens or audio-related representations; it lacks any mention of audio signal processing, token generation for audio, or applications to language or sequence models in audio domain, thus not meeting the inclusion criteria focused on discrete audio tokens, and it does not meet any exclusion criteria other than not being relevant.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Frame-Event Alignment and Fusion Network for High Frame Rate Tracking",
    "abstract": "Most existing RGB-based trackers target low frame rate benchmarks of around 30 frames per second. This setting restricts the tracker's functionality in the real world, especially for fast motion. Event-based cameras as bioinspired sensors provide considerable potential for high frame rate tracking due to their high temporal resolution. However, event-based cameras cannot offer fine-grained texture information like conventional cameras. This unique complementarity motivates us to combine conventional frames and events for high frame rate object tracking under various challenging conditions. Inthispaper, we propose an end-to-end network consisting of multi-modality alignment and fusion modules to effectively combine meaningful information from both modalities at different measurement rates. The alignment module is responsible for cross-style and cross-frame-rate alignment between frame and event modalities under the guidance of the moving cues furnished by events. While the fusion module is accountable for emphasizing valuable features and suppressing noise information by the mutual complement between the two modalities. Extensive experiments show that the proposed approach outperforms state-of-the-art trackers by a significant margin in high frame rate tracking. With the FE240hz dataset, our approach achieves high frame rate tracking up to 240Hz.",
    "metadata": {
      "arxiv_id": "2305.15688",
      "title": "Frame-Event Alignment and Fusion Network for High Frame Rate Tracking",
      "summary": "Most existing RGB-based trackers target low frame rate benchmarks of around 30 frames per second. This setting restricts the tracker's functionality in the real world, especially for fast motion. Event-based cameras as bioinspired sensors provide considerable potential for high frame rate tracking due to their high temporal resolution. However, event-based cameras cannot offer fine-grained texture information like conventional cameras. This unique complementarity motivates us to combine conventional frames and events for high frame rate object tracking under various challenging conditions. Inthispaper, we propose an end-to-end network consisting of multi-modality alignment and fusion modules to effectively combine meaningful information from both modalities at different measurement rates. The alignment module is responsible for cross-style and cross-frame-rate alignment between frame and event modalities under the guidance of the moving cues furnished by events. While the fusion module is accountable for emphasizing valuable features and suppressing noise information by the mutual complement between the two modalities. Extensive experiments show that the proposed approach outperforms state-of-the-art trackers by a significant margin in high frame rate tracking. With the FE240hz dataset, our approach achieves high frame rate tracking up to 240Hz.",
      "authors": [
        "Jiqing Zhang",
        "Yuanchen Wang",
        "Wenxi Liu",
        "Meng Li",
        "Jinpeng Bai",
        "Baocai Yin",
        "Xin Yang"
      ],
      "published": "2023-05-25T03:34:24Z",
      "updated": "2023-05-25T03:34:24Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.15688v1",
      "landing_url": "https://arxiv.org/abs/2305.15688v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.15688"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on frame-event alignment for high frame rate tracking using RGB and event cameras, with no discussion of discrete audio tokens or discretized audio representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on frame-event alignment for high frame rate tracking using RGB and event cameras, with no discussion of discrete audio tokens or discretized audio representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on high frame rate object tracking by combining frame and event camera modalities and does not address discrete audio tokens, vector quantization, or any audio-related tokenization methods; thus, it does not meet any inclusion criteria related to discrete audio tokens and relates to a different domain entirely.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on high frame rate object tracking by combining frame and event camera modalities and does not address discrete audio tokens, vector quantization, or any audio-related tokenization methods; thus, it does not meet any inclusion criteria related to discrete audio tokens and relates to a different domain entirely.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "AudioDec: An Open-source Streaming High-fidelity Neural Audio Codec",
    "abstract": "A good audio codec for live applications such as telecommunication is characterized by three key properties: (1) compression, i.e.\\ the bitrate that is required to transmit the signal should be as low as possible; (2) latency, i.e.\\ encoding and decoding the signal needs to be fast enough to enable communication without or with only minimal noticeable delay; and (3) reconstruction quality of the signal. In this work, we propose an open-source, streamable, and real-time neural audio codec that achieves strong performance along all three axes: it can reconstruct highly natural sounding 48~kHz speech signals while operating at only 12~kbps and running with less than 6~ms (GPU)/10~ms (CPU) latency. An efficient training paradigm is also demonstrated for developing such neural audio codecs for real-world scenarios. Both objective and subjective evaluations using the VCTK corpus are provided. To sum up, AudioDec is a well-developed plug-and-play benchmark for audio codec applications.",
    "metadata": {
      "arxiv_id": "2305.16608",
      "title": "AudioDec: An Open-source Streaming High-fidelity Neural Audio Codec",
      "summary": "A good audio codec for live applications such as telecommunication is characterized by three key properties: (1) compression, i.e.\\ the bitrate that is required to transmit the signal should be as low as possible; (2) latency, i.e.\\ encoding and decoding the signal needs to be fast enough to enable communication without or with only minimal noticeable delay; and (3) reconstruction quality of the signal. In this work, we propose an open-source, streamable, and real-time neural audio codec that achieves strong performance along all three axes: it can reconstruct highly natural sounding 48~kHz speech signals while operating at only 12~kbps and running with less than 6~ms (GPU)/10~ms (CPU) latency. An efficient training paradigm is also demonstrated for developing such neural audio codecs for real-world scenarios. Both objective and subjective evaluations using the VCTK corpus are provided. To sum up, AudioDec is a well-developed plug-and-play benchmark for audio codec applications.",
      "authors": [
        "Yi-Chiao Wu",
        "Israel D. Gebru",
        "Dejan Marković",
        "Alexander Richard"
      ],
      "published": "2023-05-26T04:01:16Z",
      "updated": "2023-05-26T04:01:16Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.16608v1",
      "landing_url": "https://arxiv.org/abs/2305.16608v1",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10096509"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about a streaming high-fidelity neural audio codec, but it does not discuss discrete audio tokens or token-based representations (e.g., VQ/RVQ, token generation, or semantic/disentangled tokens) which are required by the inclusion criteria; thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about a streaming high-fidelity neural audio codec, but it does not discuss discrete audio tokens or token-based representations (e.g., VQ/RVQ, token generation, or semantic/disentangled tokens) which are required by the inclusion criteria; thus it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes an open-source streaming neural audio codec focusing on compression, latency, and reconstruction quality. However, it does not mention producing discrete audio tokens through vector quantization or other quantization mechanisms as core methods, nor does it describe token generation or usage in sequence or language models for audio language modeling or generation/understanding tasks. Therefore, it does not meet the inclusion criteria of discrete token design as a main contribution.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper proposes an open-source streaming neural audio codec focusing on compression, latency, and reconstruction quality. However, it does not mention producing discrete audio tokens through vector quantization or other quantization mechanisms as core methods, nor does it describe token generation or usage in sequence or language models for audio language modeling or generation/understanding tasks. Therefore, it does not meet the inclusion criteria of discrete token design as a main contribution.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Flow Matching for Scalable Simulation-Based Inference",
    "abstract": "Neural posterior estimation methods based on discrete normalizing flows have become established tools for simulation-based inference (SBI), but scaling them to high-dimensional problems can be challenging. Building on recent advances in generative modeling, we here present flow matching posterior estimation (FMPE), a technique for SBI using continuous normalizing flows. Like diffusion models, and in contrast to discrete flows, flow matching allows for unconstrained architectures, providing enhanced flexibility for complex data modalities. Flow matching, therefore, enables exact density evaluation, fast training, and seamless scalability to large architectures--making it ideal for SBI. We show that FMPE achieves competitive performance on an established SBI benchmark, and then demonstrate its improved scalability on a challenging scientific problem: for gravitational-wave inference, FMPE outperforms methods based on comparable discrete flows, reducing training time by 30% with substantially improved accuracy. Our work underscores the potential of FMPE to enhance performance in challenging inference scenarios, thereby paving the way for more advanced applications to scientific problems.",
    "metadata": {
      "arxiv_id": "2305.17161",
      "title": "Flow Matching for Scalable Simulation-Based Inference",
      "summary": "Neural posterior estimation methods based on discrete normalizing flows have become established tools for simulation-based inference (SBI), but scaling them to high-dimensional problems can be challenging. Building on recent advances in generative modeling, we here present flow matching posterior estimation (FMPE), a technique for SBI using continuous normalizing flows. Like diffusion models, and in contrast to discrete flows, flow matching allows for unconstrained architectures, providing enhanced flexibility for complex data modalities. Flow matching, therefore, enables exact density evaluation, fast training, and seamless scalability to large architectures--making it ideal for SBI. We show that FMPE achieves competitive performance on an established SBI benchmark, and then demonstrate its improved scalability on a challenging scientific problem: for gravitational-wave inference, FMPE outperforms methods based on comparable discrete flows, reducing training time by 30% with substantially improved accuracy. Our work underscores the potential of FMPE to enhance performance in challenging inference scenarios, thereby paving the way for more advanced applications to scientific problems.",
      "authors": [
        "Maximilian Dax",
        "Jonas Wildberger",
        "Simon Buchholz",
        "Stephen R. Green",
        "Jakob H. Macke",
        "Bernhard Schölkopf"
      ],
      "published": "2023-05-26T18:00:01Z",
      "updated": "2023-10-27T12:37:54Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.17161v2",
      "landing_url": "https://arxiv.org/abs/2305.17161v2",
      "doi": "https://doi.org/10.48550/arXiv.2305.17161"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on flow matching for SBI using continuous normalizing flows in a physics-inference context and does not address discrete audio tokens or their quantization, so it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on flow matching for SBI using continuous normalizing flows in a physics-inference context and does not address discrete audio tokens or their quantization, so it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on simulation-based inference methods using flow matching and continuous normalizing flows in gravitational-wave inference, without any mention of discrete audio tokens, vector quantization, or audio representation relevant to language models or sequence modeling; thus, it does not meet the inclusion criteria related to discrete audio token design or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on simulation-based inference methods using flow matching and continuous normalizing flows in gravitational-wave inference, without any mention of discrete audio tokens, vector quantization, or audio representation relevant to language models or sequence modeling; thus, it does not meet the inclusion criteria related to discrete audio token design or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Functional Flow Matching",
    "abstract": "We propose Functional Flow Matching (FFM), a function-space generative model that generalizes the recently-introduced Flow Matching model to operate in infinite-dimensional spaces. Our approach works by first defining a path of probability measures that interpolates between a fixed Gaussian measure and the data distribution, followed by learning a vector field on the underlying space of functions that generates this path of measures. Our method does not rely on likelihoods or simulations, making it well-suited to the function space setting. We provide both a theoretical framework for building such models and an empirical evaluation of our techniques. We demonstrate through experiments on several real-world benchmarks that our proposed FFM method outperforms several recently proposed function-space generative models.",
    "metadata": {
      "arxiv_id": "2305.17209",
      "title": "Functional Flow Matching",
      "summary": "We propose Functional Flow Matching (FFM), a function-space generative model that generalizes the recently-introduced Flow Matching model to operate in infinite-dimensional spaces. Our approach works by first defining a path of probability measures that interpolates between a fixed Gaussian measure and the data distribution, followed by learning a vector field on the underlying space of functions that generates this path of measures. Our method does not rely on likelihoods or simulations, making it well-suited to the function space setting. We provide both a theoretical framework for building such models and an empirical evaluation of our techniques. We demonstrate through experiments on several real-world benchmarks that our proposed FFM method outperforms several recently proposed function-space generative models.",
      "authors": [
        "Gavin Kerrigan",
        "Giosue Migliorini",
        "Padhraic Smyth"
      ],
      "published": "2023-05-26T19:07:47Z",
      "updated": "2023-12-05T19:53:12Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.17209v2",
      "landing_url": "https://arxiv.org/abs/2305.17209v2",
      "doi": "https://doi.org/10.48550/arXiv.2305.17209"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on Functional Flow Matching in function space and does not address discrete audio tokens or tokenization for audio/modeling, failing the Inclusion criteria; thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on Functional Flow Matching in function space and does not address discrete audio tokens or tokenization for audio/modeling, failing the Inclusion criteria; thus it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a generative model operating in function spaces focused on probability measure interpolation and vector fields to generate functions; it does not discuss discrete audio tokens, tokenization methods, or any aspects of audio token generation or usage in sequence or language models as required by the inclusion criteria. Therefore, it clearly does not meet the key inclusion requirements related to \"Discrete Audio Tokens\" and their methods or applications.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents a generative model operating in function spaces focused on probability measure interpolation and vector fields to generate functions; it does not discuss discrete audio tokens, tokenization methods, or any aspects of audio token generation or usage in sequence or language models as required by the inclusion criteria. Therefore, it clearly does not meet the key inclusion requirements related to \"Discrete Audio Tokens\" and their methods or applications.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "An empirical study on speech restoration guided by self supervised speech representation",
    "abstract": "Enhancing speech quality is an indispensable yet difficult task as it is often complicated by a range of degradation factors. In addition to additive noise, reverberation, clipping, and speech attenuation can all adversely affect speech quality. Speech restoration aims to recover speech components from these distortions. This paper focuses on exploring the impact of self-supervised speech representation learning on the speech restoration task. Specifically, we employ speech representation in various speech restoration networks and evaluate their performance under complicated distortion scenarios. Our experiments demonstrate that the contextual information provided by the self-supervised speech representation can enhance speech restoration performance in various distortion scenarios, while also increasing robustness against the duration of speech attenuation and mismatched test conditions.",
    "metadata": {
      "arxiv_id": "2305.18739",
      "title": "An empirical study on speech restoration guided by self supervised speech representation",
      "summary": "Enhancing speech quality is an indispensable yet difficult task as it is often complicated by a range of degradation factors. In addition to additive noise, reverberation, clipping, and speech attenuation can all adversely affect speech quality. Speech restoration aims to recover speech components from these distortions. This paper focuses on exploring the impact of self-supervised speech representation learning on the speech restoration task. Specifically, we employ speech representation in various speech restoration networks and evaluate their performance under complicated distortion scenarios. Our experiments demonstrate that the contextual information provided by the self-supervised speech representation can enhance speech restoration performance in various distortion scenarios, while also increasing robustness against the duration of speech attenuation and mismatched test conditions.",
      "authors": [
        "Jaeuk Byun",
        "Youna Ji",
        "Soo Whan Chung",
        "Soyeon Choe",
        "Min Seok Choi"
      ],
      "published": "2023-05-30T04:26:48Z",
      "updated": "2023-05-30T04:26:48Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.18739v1",
      "landing_url": "https://arxiv.org/abs/2305.18739v1",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10095881"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item investigates speech restoration using self-supervised speech representations but does not address discrete audio tokens or tokenization; therefore it does not meet the inclusion criteria focused on Discrete Audio Tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item investigates speech restoration using self-supervised speech representations but does not address discrete audio tokens or tokenization; therefore it does not meet the inclusion criteria focused on Discrete Audio Tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on speech restoration using self-supervised speech representations but does not mention generating or utilizing discrete audio tokens derived through vector quantization, clustering, or similar mechanisms for sequence or language modeling, nor does it describe the token generation process or their application for audio language modeling or downstream tasks, thus failing to meet the inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on speech restoration using self-supervised speech representations but does not mention generating or utilizing discrete audio tokens derived through vector quantization, clustering, or similar mechanisms for sequence or language modeling, nor does it describe the token generation process or their application for audio language modeling or downstream tasks, thus failing to meet the inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Hierarchical Attention Encoder Decoder",
    "abstract": "Recent advances in large language models have shown that autoregressive modeling can generate complex and novel sequences that have many real-world applications. However, these models must generate outputs autoregressively, which becomes time-consuming when dealing with long sequences. Hierarchical autoregressive approaches that compress data have been proposed as a solution, but these methods still generate outputs at the original data frequency, resulting in slow and memory-intensive models. In this paper, we propose a model based on the Hierarchical Recurrent Encoder Decoder (HRED) architecture. This model independently encodes input sub-sequences without global context, processes these sequences using a lower-frequency model, and decodes outputs at the original data frequency. By interpreting the encoder as an implicitly defined embedding matrix and using sampled softmax estimation, we develop a training algorithm that can train the entire model without a high-frequency decoder, which is the most memory and compute-intensive part of hierarchical approaches. In a final, brief phase, we train the decoder to generate data at the original granularity. Our algorithm significantly reduces memory requirements for training autoregressive models and it also improves the total training wall-clock time.",
    "metadata": {
      "arxiv_id": "2306.01070",
      "title": "Hierarchical Attention Encoder Decoder",
      "summary": "Recent advances in large language models have shown that autoregressive modeling can generate complex and novel sequences that have many real-world applications. However, these models must generate outputs autoregressively, which becomes time-consuming when dealing with long sequences. Hierarchical autoregressive approaches that compress data have been proposed as a solution, but these methods still generate outputs at the original data frequency, resulting in slow and memory-intensive models. In this paper, we propose a model based on the Hierarchical Recurrent Encoder Decoder (HRED) architecture. This model independently encodes input sub-sequences without global context, processes these sequences using a lower-frequency model, and decodes outputs at the original data frequency. By interpreting the encoder as an implicitly defined embedding matrix and using sampled softmax estimation, we develop a training algorithm that can train the entire model without a high-frequency decoder, which is the most memory and compute-intensive part of hierarchical approaches. In a final, brief phase, we train the decoder to generate data at the original granularity. Our algorithm significantly reduces memory requirements for training autoregressive models and it also improves the total training wall-clock time.",
      "authors": [
        "Asier Mujika"
      ],
      "published": "2023-06-01T18:17:23Z",
      "updated": "2023-06-01T18:17:23Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.01070v1",
      "landing_url": "https://arxiv.org/abs/2306.01070v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.01070"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract discuss hierarchical autoregressive modeling and training efficiency, not discrete audio token design/quantization for audio-language modeling, so it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract discuss hierarchical autoregressive modeling and training efficiency, not discrete audio token design/quantization for audio-language modeling, so it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a hierarchical autoregressive encoder-decoder model designed to improve training efficiency and memory usage for long sequence generation in large language models. However, it does not mention discrete audio tokens, discrete token generation, vector quantization, or audio-specific tokenization methods, which are key inclusion criteria. It also lacks details on token generation processes or use of tokens in sequence/language modeling for audio tasks, meaning it does not meet the inclusion requirements related to discrete audio token design and application. Therefore, this study should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a hierarchical autoregressive encoder-decoder model designed to improve training efficiency and memory usage for long sequence generation in large language models. However, it does not mention discrete audio tokens, discrete token generation, vector quantization, or audio-specific tokenization methods, which are key inclusion criteria. It also lacks details on token generation processes or use of tokens in sequence/language modeling for audio tasks, meaning it does not meet the inclusion requirements related to discrete audio token design and application. Therefore, this study should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Exploration on HuBERT with Multiple Resolutions",
    "abstract": "Hidden-unit BERT (HuBERT) is a widely-used self-supervised learning (SSL) model in speech processing. However, we argue that its fixed 20ms resolution for hidden representations would not be optimal for various speech-processing tasks since their attributes (e.g., speaker characteristics and semantics) are based on different time scales. To address this limitation, we propose utilizing HuBERT representations at multiple resolutions for downstream tasks. We explore two approaches, namely the parallel and hierarchical approaches, for integrating HuBERT features with different resolutions. Through experiments, we demonstrate that HuBERT with multiple resolutions outperforms the original model. This highlights the potential of utilizing multiple resolutions in SSL models like HuBERT to capture diverse information from speech signals.",
    "metadata": {
      "arxiv_id": "2306.01084",
      "title": "Exploration on HuBERT with Multiple Resolutions",
      "summary": "Hidden-unit BERT (HuBERT) is a widely-used self-supervised learning (SSL) model in speech processing. However, we argue that its fixed 20ms resolution for hidden representations would not be optimal for various speech-processing tasks since their attributes (e.g., speaker characteristics and semantics) are based on different time scales. To address this limitation, we propose utilizing HuBERT representations at multiple resolutions for downstream tasks. We explore two approaches, namely the parallel and hierarchical approaches, for integrating HuBERT features with different resolutions. Through experiments, we demonstrate that HuBERT with multiple resolutions outperforms the original model. This highlights the potential of utilizing multiple resolutions in SSL models like HuBERT to capture diverse information from speech signals.",
      "authors": [
        "Jiatong Shi",
        "Yun Tang",
        "Hirofumi Inaguma",
        "Hongyu GOng",
        "Juan Pino",
        "Shinji Watanabe"
      ],
      "published": "2023-06-01T18:51:34Z",
      "updated": "2023-06-22T18:34:22Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.01084v2",
      "landing_url": "https://arxiv.org/abs/2306.01084v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.01084"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item investigates HuBERT representations at multiple resolutions for downstream tasks, but it does not address discrete audio tokens, token generation/quantization, or VQ/RVQ-based token design, which are required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item investigates HuBERT representations at multiple resolutions for downstream tasks, but it does not address discrete audio tokens, token generation/quantization, or VQ/RVQ-based token design, which are required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discusses improving HuBERT representations by using multiple resolutions for downstream speech processing tasks but does not mention the generation of discrete audio tokens through quantization or any discrete token design, nor does it describe token formation processes or use tokens as core representations for sequence modeling; therefore, it does not meet the inclusion criteria centered on discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study discusses improving HuBERT representations by using multiple resolutions for downstream speech processing tasks but does not mention the generation of discrete audio tokens through quantization or any discrete token design, nor does it describe token formation processes or use tokens as core representations for sequence modeling; therefore, it does not meet the inclusion criteria centered on discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DistilXLSR: A Light Weight Cross-Lingual Speech Representation Model",
    "abstract": "Multilingual self-supervised speech representation models have greatly enhanced the speech recognition performance for low-resource languages, and the compression of these huge models has also become a crucial prerequisite for their industrial application. In this paper, we propose DistilXLSR, a distilled cross-lingual speech representation model. By randomly shuffling the phonemes of existing speech, we reduce the linguistic information and distill cross-lingual models using only English data. We also design a layer-jumping initialization method to fully leverage the teacher's pre-trained weights. Experiments on 2 kinds of teacher models and 15 low-resource languages show that our method can reduce the parameters by 50% while maintaining cross-lingual representation ability. Our method is proven to be generalizable to various languages/teacher models and has the potential to improve the cross-lingual performance of the English pre-trained models.",
    "metadata": {
      "arxiv_id": "2306.01303",
      "title": "DistilXLSR: A Light Weight Cross-Lingual Speech Representation Model",
      "summary": "Multilingual self-supervised speech representation models have greatly enhanced the speech recognition performance for low-resource languages, and the compression of these huge models has also become a crucial prerequisite for their industrial application. In this paper, we propose DistilXLSR, a distilled cross-lingual speech representation model. By randomly shuffling the phonemes of existing speech, we reduce the linguistic information and distill cross-lingual models using only English data. We also design a layer-jumping initialization method to fully leverage the teacher's pre-trained weights. Experiments on 2 kinds of teacher models and 15 low-resource languages show that our method can reduce the parameters by 50% while maintaining cross-lingual representation ability. Our method is proven to be generalizable to various languages/teacher models and has the potential to improve the cross-lingual performance of the English pre-trained models.",
      "authors": [
        "Haoyu Wang",
        "Siyuan Wang",
        "Wei-Qiang Zhang",
        "Jinfeng Bai"
      ],
      "published": "2023-06-02T07:03:06Z",
      "updated": "2023-06-02T07:03:06Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.01303v1",
      "landing_url": "https://arxiv.org/abs/2306.01303v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.01303"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses distilling cross-lingual speech representations and model compression, with no focus on discrete audio tokens, vector quantization, or token-level mechanisms, so it does not meet the discrete-token criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item discusses distilling cross-lingual speech representations and model compression, with no focus on discrete audio tokens, vector quantization, or token-level mechanisms, so it does not meet the discrete-token criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes DistilXLSR, a distilled cross-lingual speech representation model focusing on parameter reduction while maintaining cross-lingual capabilities, but it does not describe generating discrete audio tokens via vector quantization or similar discrete tokenization methods as required by the inclusion criteria. Instead, it emphasizes distillation and representation learning without detailing token formation or discrete quantization mechanisms for use in language or sequence models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper proposes DistilXLSR, a distilled cross-lingual speech representation model focusing on parameter reduction while maintaining cross-lingual capabilities, but it does not describe generating discrete audio tokens via vector quantization or similar discrete tokenization methods as required by the inclusion criteria. Instead, it emphasizes distillation and representation learning without detailing token formation or discrete quantization mechanisms for use in language or sequence models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Simultaneous or Sequential Training? How Speech Representations Cooperate in a Multi-Task Self-Supervised Learning System",
    "abstract": "Speech representation learning with self-supervised algorithms has resulted in notable performance boosts in many downstream tasks. Recent work combined self-supervised learning (SSL) and visually grounded speech (VGS) processing mechanisms for representation learning. The joint training with SSL and VGS mechanisms provides the opportunity to utilize both unlabeled speech and speech-related visual information based on data availability. This has shown to enhance the quality of learned representations, especially at encoding semantic- and lexical-level knowledge. In this work, we further study the joint optimization of wav2vec 2.0-based SSL and transformer-based VGS as a multi-task learning system. We explore a set of training scenarios to understand how speech representations are shared or transferred between the two tasks, and what is the optimal training strategy for cross-modal semantic retrieval and phoneme discrimination performance. As a result, we find that sequential training with wav2vec 2.0 first and VGS next provides higher performance on audio-visual retrieval compared to simultaneous optimization of both learning mechanisms. However, the parallel SSL-VGS training reduces the effects of catastrophic forgetting when switching between optimization criteria. Moreover, the results suggest that phonemic representations learned through the VGS mechanism may generalize better across datasets compared to those learned with SSL.",
    "metadata": {
      "arxiv_id": "2306.02972",
      "title": "Simultaneous or Sequential Training? How Speech Representations Cooperate in a Multi-Task Self-Supervised Learning System",
      "summary": "Speech representation learning with self-supervised algorithms has resulted in notable performance boosts in many downstream tasks. Recent work combined self-supervised learning (SSL) and visually grounded speech (VGS) processing mechanisms for representation learning. The joint training with SSL and VGS mechanisms provides the opportunity to utilize both unlabeled speech and speech-related visual information based on data availability. This has shown to enhance the quality of learned representations, especially at encoding semantic- and lexical-level knowledge. In this work, we further study the joint optimization of wav2vec 2.0-based SSL and transformer-based VGS as a multi-task learning system. We explore a set of training scenarios to understand how speech representations are shared or transferred between the two tasks, and what is the optimal training strategy for cross-modal semantic retrieval and phoneme discrimination performance. As a result, we find that sequential training with wav2vec 2.0 first and VGS next provides higher performance on audio-visual retrieval compared to simultaneous optimization of both learning mechanisms. However, the parallel SSL-VGS training reduces the effects of catastrophic forgetting when switching between optimization criteria. Moreover, the results suggest that phonemic representations learned through the VGS mechanism may generalize better across datasets compared to those learned with SSL.",
      "authors": [
        "Khazar Khorrami",
        "María Andrea Cruz Blandón",
        "Tuomas Virtanen",
        "Okko Räsänen"
      ],
      "published": "2023-06-05T15:35:19Z",
      "updated": "2023-06-05T15:35:19Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.02972v1",
      "landing_url": "https://arxiv.org/abs/2306.02972v1",
      "doi": "https://doi.org/10.23919/EUSIPCO58844.2023.10290051"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work investigates training strategies for joint SSL and visually grounded speech in a multi-task framework, not discrete audio token design or tokenization, thus does not meet inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work investigates training strategies for joint SSL and visually grounded speech in a multi-task framework, not discrete audio token design or tokenization, thus does not meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on joint training of self-supervised learning and visually grounded speech representations but does not mention the generation or use of discrete audio tokens obtained via vector quantization or similar discrete tokenization methods, nor does it detail token production processes or their use in sequence models for audio language modeling or generation; thus, it does not align with the inclusion criteria requiring discrete token design or quantization mechanisms as a core contribution.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on joint training of self-supervised learning and visually grounded speech representations but does not mention the generation or use of discrete audio tokens obtained via vector quantization or similar discrete tokenization methods, nor does it detail token production processes or their use in sequence models for audio language modeling or generation; thus, it does not align with the inclusion criteria requiring discrete token design or quantization mechanisms as a core contribution.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Label Aware Speech Representation Learning For Language Identification",
    "abstract": "Speech representation learning approaches for non-semantic tasks such as language recognition have either explored supervised embedding extraction methods using a classifier model or self-supervised representation learning approaches using raw data. In this paper, we propose a novel framework of combining self-supervised representation learning with the language label information for the pre-training task. This framework, termed as Label Aware Speech Representation (LASR) learning, uses a triplet based objective function to incorporate language labels along with the self-supervised loss function. The speech representations are further fine-tuned for the downstream task. The language recognition experiments are performed on two public datasets - FLEURS and Dhwani. In these experiments, we illustrate that the proposed LASR framework improves over the state-of-the-art systems on language identification. We also report an analysis of the robustness of LASR approach to noisy/missing labels as well as its application to multi-lingual speech recognition tasks.",
    "metadata": {
      "arxiv_id": "2306.04374",
      "title": "Label Aware Speech Representation Learning For Language Identification",
      "summary": "Speech representation learning approaches for non-semantic tasks such as language recognition have either explored supervised embedding extraction methods using a classifier model or self-supervised representation learning approaches using raw data. In this paper, we propose a novel framework of combining self-supervised representation learning with the language label information for the pre-training task. This framework, termed as Label Aware Speech Representation (LASR) learning, uses a triplet based objective function to incorporate language labels along with the self-supervised loss function. The speech representations are further fine-tuned for the downstream task. The language recognition experiments are performed on two public datasets - FLEURS and Dhwani. In these experiments, we illustrate that the proposed LASR framework improves over the state-of-the-art systems on language identification. We also report an analysis of the robustness of LASR approach to noisy/missing labels as well as its application to multi-lingual speech recognition tasks.",
      "authors": [
        "Shikhar Vashishth",
        "Shikhar Bharadwaj",
        "Sriram Ganapathy",
        "Ankur Bapna",
        "Min Ma",
        "Wei Han",
        "Vera Axelrod",
        "Partha Talukdar"
      ],
      "published": "2023-06-07T12:14:16Z",
      "updated": "2023-06-07T12:14:16Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.04374v1",
      "landing_url": "https://arxiv.org/abs/2306.04374v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.04374"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses self-supervised speech representation learning for language identification; it does not mention discrete audio tokens, vector quantization, or token-based representations, which are required by the inclusion criteria. Therefore it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses self-supervised speech representation learning for language identification; it does not mention discrete audio tokens, vector quantization, or token-based representations, which are required by the inclusion criteria. Therefore it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on combining self-supervised learning with language label information for language identification, emphasizing embedding extraction and language recognition performance improvements. However, it does not discuss or propose discrete audio token generation, vector quantization, or any form of discretization of speech representations into tokens for sequence or language model processing, which is a core inclusion criterion. Therefore, it does not meet the necessary criteria for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on combining self-supervised learning with language label information for language identification, emphasizing embedding extraction and language recognition performance improvements. However, it does not discuss or propose discrete audio token generation, vector quantization, or any form of discretization of speech representations into tokens for sequence or language model processing, which is a core inclusion criterion. Therefore, it does not meet the necessary criteria for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Augmentation-aware Self-supervised Learning with Conditioned Projector",
    "abstract": "Self-supervised learning (SSL) is a powerful technique for learning from unlabeled data. By learning to remain invariant to applied data augmentations, methods such as SimCLR and MoCo can reach quality on par with supervised approaches. However, this invariance may be detrimental for solving downstream tasks that depend on traits affected by augmentations used during pretraining, such as color. In this paper, we propose to foster sensitivity to such characteristics in the representation space by modifying the projector network, a common component of self-supervised architectures. Specifically, we supplement the projector with information about augmentations applied to images. For the projector to take advantage of this auxiliary conditioning when solving the SSL task, the feature extractor learns to preserve the augmentation information in its representations. Our approach, coined Conditional Augmentation-aware Self-supervised Learning (CASSLE), is directly applicable to typical joint-embedding SSL methods regardless of their objective functions. Moreover, it does not require major changes in the network architecture or prior knowledge of downstream tasks. In addition to an analysis of sensitivity towards different data augmentations, we conduct a series of experiments, which show that CASSLE improves over various SSL methods, reaching state-of-the-art performance in multiple downstream tasks.",
    "metadata": {
      "arxiv_id": "2306.06082",
      "title": "Augmentation-aware Self-supervised Learning with Conditioned Projector",
      "summary": "Self-supervised learning (SSL) is a powerful technique for learning from unlabeled data. By learning to remain invariant to applied data augmentations, methods such as SimCLR and MoCo can reach quality on par with supervised approaches. However, this invariance may be detrimental for solving downstream tasks that depend on traits affected by augmentations used during pretraining, such as color. In this paper, we propose to foster sensitivity to such characteristics in the representation space by modifying the projector network, a common component of self-supervised architectures. Specifically, we supplement the projector with information about augmentations applied to images. For the projector to take advantage of this auxiliary conditioning when solving the SSL task, the feature extractor learns to preserve the augmentation information in its representations. Our approach, coined Conditional Augmentation-aware Self-supervised Learning (CASSLE), is directly applicable to typical joint-embedding SSL methods regardless of their objective functions. Moreover, it does not require major changes in the network architecture or prior knowledge of downstream tasks. In addition to an analysis of sensitivity towards different data augmentations, we conduct a series of experiments, which show that CASSLE improves over various SSL methods, reaching state-of-the-art performance in multiple downstream tasks.",
      "authors": [
        "Marcin Przewięźlikowski",
        "Mateusz Pyla",
        "Bartosz Zieliński",
        "Bartłomiej Twardowski",
        "Jacek Tabor",
        "Marek Śmieja"
      ],
      "published": "2023-05-31T12:24:06Z",
      "updated": "2024-10-19T08:00:13Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.06082v4",
      "landing_url": "https://arxiv.org/abs/2306.06082v4",
      "doi": "https://doi.org/10.1016/j.knosys.2024.112572"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on augmentation-aware self-supervised learning for image data and does not address discrete audio token design or tokenization methods for audio modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on augmentation-aware self-supervised learning for image data and does not address discrete audio token design or tokenization methods for audio modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses self-supervised learning methods focusing on image augmentations and representation sensitivity, with no mention of discrete audio tokens, vector quantization, or audio signal processing relevant to the discrete audio token topic defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses self-supervised learning methods focusing on image augmentations and representation sensitivity, with no mention of discrete audio tokens, vector quantization, or audio signal processing relevant to the discrete audio token topic defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute",
    "abstract": "Self-supervised learning (SSL) has led to great strides in speech processing. However, the resources needed to train these models has become prohibitively large as they continue to scale. Currently, only a few groups with substantial resources are capable of creating SSL models, which harms reproducibility. In this work, we optimize HuBERT SSL to fit in academic constraints. We reproduce HuBERT independently from the original implementation, with no performance loss. Our code and training optimizations make SSL feasible with only 8 GPUs, instead of the 32 used in the original work. We also explore a semi-supervised route, using an ASR model to skip the first pre-training iteration. Within one iteration of pre-training, our models improve over HuBERT on several tasks. Furthermore, our HuBERT Large variant requires only 8 GPUs, achieving similar performance to the original trained on 128. As our contribution to the community, all models, configurations, and code are made open-source in ESPnet.",
    "metadata": {
      "arxiv_id": "2306.06672",
      "title": "Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute",
      "summary": "Self-supervised learning (SSL) has led to great strides in speech processing. However, the resources needed to train these models has become prohibitively large as they continue to scale. Currently, only a few groups with substantial resources are capable of creating SSL models, which harms reproducibility. In this work, we optimize HuBERT SSL to fit in academic constraints. We reproduce HuBERT independently from the original implementation, with no performance loss. Our code and training optimizations make SSL feasible with only 8 GPUs, instead of the 32 used in the original work. We also explore a semi-supervised route, using an ASR model to skip the first pre-training iteration. Within one iteration of pre-training, our models improve over HuBERT on several tasks. Furthermore, our HuBERT Large variant requires only 8 GPUs, achieving similar performance to the original trained on 128. As our contribution to the community, all models, configurations, and code are made open-source in ESPnet.",
      "authors": [
        "William Chen",
        "Xuankai Chang",
        "Yifan Peng",
        "Zhaoheng Ni",
        "Soumi Maiti",
        "Shinji Watanabe"
      ],
      "published": "2023-06-11T12:53:46Z",
      "updated": "2023-06-11T12:53:46Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.06672v1",
      "landing_url": "https://arxiv.org/abs/2306.06672v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.06672"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on reducing compute and reproducing HuBERT SSL for academic constraints and does not discuss discrete audio tokens, token quantization methods, or related token-based representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on reducing compute and reproducing HuBERT SSL for academic constraints and does not discuss discrete audio tokens, token quantization methods, or related token-based representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on optimizing HuBERT self-supervised learning models to be more resource-efficient for academic use but does not mention any discrete audio token representation, vector quantization, or discrete token generation processes that serve as intermediate representations for language or sequence modeling. The study centers on training efficiency improvements and open-source contributions rather than designing or employing discrete audio tokens as defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on optimizing HuBERT self-supervised learning models to be more resource-efficient for academic use but does not mention any discrete audio token representation, vector quantization, or discrete token generation processes that serve as intermediate representations for language or sequence modeling. The study centers on training efficiency improvements and open-source contributions rather than designing or employing discrete audio tokens as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "HiddenSinger: High-Quality Singing Voice Synthesis via Neural Audio Codec and Latent Diffusion Models",
    "abstract": "Recently, denoising diffusion models have demonstrated remarkable performance among generative models in various domains. However, in the speech domain, the application of diffusion models for synthesizing time-varying audio faces limitations in terms of complexity and controllability, as speech synthesis requires very high-dimensional samples with long-term acoustic features. To alleviate the challenges posed by model complexity in singing voice synthesis, we propose HiddenSinger, a high-quality singing voice synthesis system using a neural audio codec and latent diffusion models. To ensure high-fidelity audio, we introduce an audio autoencoder that can encode audio into an audio codec as a compressed representation and reconstruct the high-fidelity audio from the low-dimensional compressed latent vector. Subsequently, we use the latent diffusion models to sample a latent representation from a musical score. In addition, our proposed model is extended to an unsupervised singing voice learning framework, HiddenSinger-U, to train the model using an unlabeled singing voice dataset. Experimental results demonstrate that our model outperforms previous models in terms of audio quality. Furthermore, the HiddenSinger-U can synthesize high-quality singing voices of speakers trained solely on unlabeled data.",
    "metadata": {
      "arxiv_id": "2306.06814",
      "title": "HiddenSinger: High-Quality Singing Voice Synthesis via Neural Audio Codec and Latent Diffusion Models",
      "summary": "Recently, denoising diffusion models have demonstrated remarkable performance among generative models in various domains. However, in the speech domain, the application of diffusion models for synthesizing time-varying audio faces limitations in terms of complexity and controllability, as speech synthesis requires very high-dimensional samples with long-term acoustic features. To alleviate the challenges posed by model complexity in singing voice synthesis, we propose HiddenSinger, a high-quality singing voice synthesis system using a neural audio codec and latent diffusion models. To ensure high-fidelity audio, we introduce an audio autoencoder that can encode audio into an audio codec as a compressed representation and reconstruct the high-fidelity audio from the low-dimensional compressed latent vector. Subsequently, we use the latent diffusion models to sample a latent representation from a musical score. In addition, our proposed model is extended to an unsupervised singing voice learning framework, HiddenSinger-U, to train the model using an unlabeled singing voice dataset. Experimental results demonstrate that our model outperforms previous models in terms of audio quality. Furthermore, the HiddenSinger-U can synthesize high-quality singing voices of speakers trained solely on unlabeled data.",
      "authors": [
        "Ji-Sang Hwang",
        "Sang-Hoon Lee",
        "Seong-Whan Lee"
      ],
      "published": "2023-06-12T01:21:41Z",
      "updated": "2023-06-12T01:21:41Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.06814v1",
      "landing_url": "https://arxiv.org/abs/2306.06814v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.06814"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on a neural audio codec and latent diffusion for singing voice synthesis with continuous latent representations, not on discrete audio tokens or token-based representations (e.g., VQ/RVQ, hierarchical codebooks); thus it does not meet the inclusion criteria focused on discrete audio token design and related methods.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work centers on a neural audio codec and latent diffusion for singing voice synthesis with continuous latent representations, not on discrete audio tokens or token-based representations (e.g., VQ/RVQ, hierarchical codebooks); thus it does not meet the inclusion criteria focused on discrete audio token design and related methods.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a system using a neural audio codec and latent diffusion models for high-quality singing voice synthesis, mentioning an audio autoencoder that encodes audio into compressed latent vectors; however, the description lacks clear evidence that the compressed representations are discrete tokens generated via vector quantization or related methods intended for sequence modeling as defined in the inclusion criteria. There is no explicit explanation of token generation details, quantization mechanisms, or usage of these representations as tokens for language or sequence models, which is critical for inclusion. Therefore, it does not meet the key requirement of having discrete audio token design as a core method with clear token formation and usage for sequence models.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a system using a neural audio codec and latent diffusion models for high-quality singing voice synthesis, mentioning an audio autoencoder that encodes audio into compressed latent vectors; however, the description lacks clear evidence that the compressed representations are discrete tokens generated via vector quantization or related methods intended for sequence modeling as defined in the inclusion criteria. There is no explicit explanation of token generation details, quantization mechanisms, or usage of these representations as tokens for language or sequence models, which is critical for inclusion. Therefore, it does not meet the key requirement of having discrete audio token design as a core method with clear token formation and usage for sequence models.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding",
    "abstract": "The utilization of discrete speech tokens, divided into semantic tokens and acoustic tokens, has been proven superior to traditional acoustic feature mel-spectrograms in terms of naturalness and robustness for text-to-speech (TTS) synthesis. Recent popular models, such as VALL-E and SPEAR-TTS, allow zero-shot speaker adaptation through auto-regressive (AR) continuation of acoustic tokens extracted from a short speech prompt. However, these AR models are restricted to generate speech only in a left-to-right direction, making them unsuitable for speech editing where both preceding and following contexts are provided. Furthermore, these models rely on acoustic tokens, which have audio quality limitations imposed by the performance of audio codec models. In this study, we propose a unified context-aware TTS framework called UniCATS, which is capable of both speech continuation and editing. UniCATS comprises two components, an acoustic model CTX-txt2vec and a vocoder CTX-vec2wav. CTX-txt2vec employs contextual VQ-diffusion to predict semantic tokens from the input text, enabling it to incorporate the semantic context and maintain seamless concatenation with the surrounding context. Following that, CTX-vec2wav utilizes contextual vocoding to convert these semantic tokens into waveforms, taking into consideration the acoustic context. Our experimental results demonstrate that CTX-vec2wav outperforms HifiGAN and AudioLM in terms of speech resynthesis from semantic tokens. Moreover, we show that UniCATS achieves state-of-the-art performance in both speech continuation and editing.",
    "metadata": {
      "arxiv_id": "2306.07547",
      "title": "UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding",
      "summary": "The utilization of discrete speech tokens, divided into semantic tokens and acoustic tokens, has been proven superior to traditional acoustic feature mel-spectrograms in terms of naturalness and robustness for text-to-speech (TTS) synthesis. Recent popular models, such as VALL-E and SPEAR-TTS, allow zero-shot speaker adaptation through auto-regressive (AR) continuation of acoustic tokens extracted from a short speech prompt. However, these AR models are restricted to generate speech only in a left-to-right direction, making them unsuitable for speech editing where both preceding and following contexts are provided. Furthermore, these models rely on acoustic tokens, which have audio quality limitations imposed by the performance of audio codec models. In this study, we propose a unified context-aware TTS framework called UniCATS, which is capable of both speech continuation and editing. UniCATS comprises two components, an acoustic model CTX-txt2vec and a vocoder CTX-vec2wav. CTX-txt2vec employs contextual VQ-diffusion to predict semantic tokens from the input text, enabling it to incorporate the semantic context and maintain seamless concatenation with the surrounding context. Following that, CTX-vec2wav utilizes contextual vocoding to convert these semantic tokens into waveforms, taking into consideration the acoustic context. Our experimental results demonstrate that CTX-vec2wav outperforms HifiGAN and AudioLM in terms of speech resynthesis from semantic tokens. Moreover, we show that UniCATS achieves state-of-the-art performance in both speech continuation and editing.",
      "authors": [
        "Chenpeng Du",
        "Yiwei Guo",
        "Feiyu Shen",
        "Zhijun Liu",
        "Zheng Liang",
        "Xie Chen",
        "Shuai Wang",
        "Hui Zhang",
        "Kai Yu"
      ],
      "published": "2023-06-13T05:38:34Z",
      "updated": "2024-03-28T13:56:33Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.07547v6",
      "landing_url": "https://arxiv.org/abs/2306.07547v6",
      "doi": "https://doi.org/10.1609/aaai.v38i16.29747"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on discrete audio tokens (semantic/acoustic) and token-based generation for TTS, detailing a contextual VQ-diffusion token generation and contextual vocoding pipeline, which aligns with the inclusion criteria for discrete-token representations and methods (despite being an abstract with limited full-method details).",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item centers on discrete audio tokens (semantic/acoustic) and token-based generation for TTS, detailing a contextual VQ-diffusion token generation and contextual vocoding pipeline, which aligns with the inclusion criteria for discrete-token representations and methods (despite being an abstract with limited full-method details).",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a context-aware TTS framework that employs semantic tokens generated through contextual VQ-diffusion and utilizes these discrete semantic tokens for speech synthesis, indicating a clear use of discrete audio tokens formed via vector quantization methods; the paper presents the token generation process and their usage in sequence modeling for speech understanding and generation, thus meeting the inclusion criteria. Moreover, the abstract suggests experimental validation and technical depth, without indication of being a mere demo or lacking methodological details. There is no mention of traditional standard audio codecs as the sole focus, and the study seems to be a valid academic research work with clearly described token-based methods.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a context-aware TTS framework that employs semantic tokens generated through contextual VQ-diffusion and utilizes these discrete semantic tokens for speech synthesis, indicating a clear use of discrete audio tokens formed via vector quantization methods; the paper presents the token generation process and their usage in sequence modeling for speech understanding and generation, thus meeting the inclusion criteria. Moreover, the abstract suggests experimental validation and technical depth, without indication of being a mere demo or lacking methodological details. There is no mention of traditional standard audio codecs as the sole focus, and the study seems to be a valid academic research work with clearly described token-based methods.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Dynamically Masked Discriminator for Generative Adversarial Networks",
    "abstract": "Training Generative Adversarial Networks (GANs) remains a challenging problem. The discriminator trains the generator by learning the distribution of real/generated data. However, the distribution of generated data changes throughout the training process, which is difficult for the discriminator to learn. In this paper, we propose a novel method for GANs from the viewpoint of online continual learning. We observe that the discriminator model, trained on historically generated data, often slows down its adaptation to the changes in the new arrival generated data, which accordingly decreases the quality of generated results. By treating the generated data in training as a stream, we propose to detect whether the discriminator slows down the learning of new knowledge in generated data. Therefore, we can explicitly enforce the discriminator to learn new knowledge fast. Particularly, we propose a new discriminator, which automatically detects its retardation and then dynamically masks its features, such that the discriminator can adaptively learn the temporally-vary distribution of generated data. Experimental results show our method outperforms the state-of-the-art approaches.",
    "metadata": {
      "arxiv_id": "2306.07716",
      "title": "Dynamically Masked Discriminator for Generative Adversarial Networks",
      "summary": "Training Generative Adversarial Networks (GANs) remains a challenging problem. The discriminator trains the generator by learning the distribution of real/generated data. However, the distribution of generated data changes throughout the training process, which is difficult for the discriminator to learn. In this paper, we propose a novel method for GANs from the viewpoint of online continual learning. We observe that the discriminator model, trained on historically generated data, often slows down its adaptation to the changes in the new arrival generated data, which accordingly decreases the quality of generated results. By treating the generated data in training as a stream, we propose to detect whether the discriminator slows down the learning of new knowledge in generated data. Therefore, we can explicitly enforce the discriminator to learn new knowledge fast. Particularly, we propose a new discriminator, which automatically detects its retardation and then dynamically masks its features, such that the discriminator can adaptively learn the temporally-vary distribution of generated data. Experimental results show our method outperforms the state-of-the-art approaches.",
      "authors": [
        "Wentian Zhang",
        "Haozhe Liu",
        "Bing Li",
        "Jinheng Xie",
        "Yawen Huang",
        "Yuexiang Li",
        "Yefeng Zheng",
        "Bernard Ghanem"
      ],
      "published": "2023-06-13T12:07:01Z",
      "updated": "2024-01-04T13:58:50Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.07716v3",
      "landing_url": "https://arxiv.org/abs/2306.07716v3",
      "doi": "https://doi.org/10.48550/arXiv.2306.07716"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on a dynamically masked discriminator for GANs with online continual learning; it does not address discrete audio tokens or tokenization methods, so it does not meet inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on a dynamically masked discriminator for GANs with online continual learning; it does not address discrete audio tokens or tokenization methods, so it does not meet inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on improving the discriminator of GANs through a novel dynamic masking method to adapt to evolving generated data distributions. There is no mention or indication of discrete audio tokens, vector quantization, or related audio representations being developed or studied; thus, the content does not meet the inclusion criteria related to discrete audio tokens and their generation processes, nor does it fit the aim of the review.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on improving the discriminator of GANs through a novel dynamic masking method to adapt to evolving generated data distributions. There is no mention or indication of discrete audio tokens, vector quantization, or related audio representations being developed or studied; thus, the content does not meet the inclusion criteria related to discrete audio tokens and their generation processes, nor does it fit the aim of the review.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation",
    "abstract": "The excellent generalization ability of self-supervised learning (SSL) for speech foundation models has garnered significant attention. HuBERT is a successful example that utilizes offline clustering to convert speech features into discrete units for a masked language modeling pretext task. However, simply clustering features as targets by k-means does not fully inspire the model's performance. In this work, we present an unsupervised method to improve SSL targets. Two models are proposed, MonoBERT and PolyBERT, which leverage context-independent and context-dependent phoneme-based units for pre-training. Our models outperform other SSL models significantly on the LibriSpeech benchmark without the need for iterative re-clustering and re-training. Furthermore, our models equipped with context-dependent units even outperform target-improvement models that use labeled data during pre-training. How we progressively improve the unit discovery process is demonstrated through experiments.",
    "metadata": {
      "arxiv_id": "2306.08920",
      "title": "Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation",
      "summary": "The excellent generalization ability of self-supervised learning (SSL) for speech foundation models has garnered significant attention. HuBERT is a successful example that utilizes offline clustering to convert speech features into discrete units for a masked language modeling pretext task. However, simply clustering features as targets by k-means does not fully inspire the model's performance. In this work, we present an unsupervised method to improve SSL targets. Two models are proposed, MonoBERT and PolyBERT, which leverage context-independent and context-dependent phoneme-based units for pre-training. Our models outperform other SSL models significantly on the LibriSpeech benchmark without the need for iterative re-clustering and re-training. Furthermore, our models equipped with context-dependent units even outperform target-improvement models that use labeled data during pre-training. How we progressively improve the unit discovery process is demonstrated through experiments.",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Guanrou Yang",
        "Yu Wang",
        "Chao Zhang",
        "Xie Chen"
      ],
      "published": "2023-06-15T07:45:12Z",
      "updated": "2023-06-15T07:45:12Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.08920v1",
      "landing_url": "https://arxiv.org/abs/2306.08920v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.08920"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on unsupervised discovery of discrete speech units (phoneme-based tokens) for SSL training and reports strong results on LibriSpeech, aligning with the discrete audio token topic and providing a method-focused contribution likely suitable for replication.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work focuses on unsupervised discovery of discrete speech units (phoneme-based tokens) for SSL training and reports strong results on LibriSpeech, aligning with the discrete audio token topic and providing a method-focused contribution likely suitable for replication.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes unsupervised methods (MonoBERT and PolyBERT) for discovering discrete speech units used as targets in self-supervised learning for speech representation, focusing on phoneme-based discrete units derived from clustering; these discrete units serve as core components for modeling speech sequences similar to language tokens. The abstract clearly describes the token generation process (clustering features into phoneme-like units) and emphasizes the use of these discrete units in masked language modeling pretext tasks, aligning with the inclusion criteria about discrete audio tokens designed for sequence/language model understanding. Although vector quantization specifics or detailed quantization settings are not explicitly mentioned, the clustering into discrete units for SSL and the demonstrated superior performance suggest a solid technical contribution with clear use of discrete tokens for sequence modeling. There is no indication of traditional codec usage, lack of reproducible methods, or demo-only content, and the source appears academic and sufficiently detailed. However, the absence of detailed quantization or token generation mechanisms might reduce certainty. Overall, the study meets the core inclusion criteria and avoids exclusion criteria, leaning towards inclusion.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The study proposes unsupervised methods (MonoBERT and PolyBERT) for discovering discrete speech units used as targets in self-supervised learning for speech representation, focusing on phoneme-based discrete units derived from clustering; these discrete units serve as core components for modeling speech sequences similar to language tokens. The abstract clearly describes the token generation process (clustering features into phoneme-like units) and emphasizes the use of these discrete units in masked language modeling pretext tasks, aligning with the inclusion criteria about discrete audio tokens designed for sequence/language model understanding. Although vector quantization specifics or detailed quantization settings are not explicitly mentioned, the clustering into discrete units for SSL and the demonstrated superior performance suggest a solid technical contribution with clear use of discrete tokens for sequence modeling. There is no indication of traditional codec usage, lack of reproducible methods, or demo-only content, and the source appears academic and sufficiently detailed. However, the absence of detailed quantization or token generation mechanisms might reduce certainty. Overall, the study meets the core inclusion criteria and avoids exclusion criteria, leaning towards inclusion.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects",
    "abstract": "Self-supervised learning (SSL) has recently achieved impressive performance on various time series tasks. The most prominent advantage of SSL is that it reduces the dependence on labeled data. Based on the pre-training and fine-tuning strategy, even a small amount of labeled data can achieve high performance. Compared with many published self-supervised surveys on computer vision and natural language processing, a comprehensive survey for time series SSL is still missing. To fill this gap, we review current state-of-the-art SSL methods for time series data in this article. To this end, we first comprehensively review existing surveys related to SSL and time series, and then provide a new taxonomy of existing time series SSL methods by summarizing them from three perspectives: generative-based, contrastive-based, and adversarial-based. These methods are further divided into ten subcategories with detailed reviews and discussions about their key intuitions, main frameworks, advantages and disadvantages. To facilitate the experiments and validation of time series SSL methods, we also summarize datasets commonly used in time series forecasting, classification, anomaly detection, and clustering tasks. Finally, we present the future directions of SSL for time series analysis.",
    "metadata": {
      "arxiv_id": "2306.10125",
      "title": "Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects",
      "summary": "Self-supervised learning (SSL) has recently achieved impressive performance on various time series tasks. The most prominent advantage of SSL is that it reduces the dependence on labeled data. Based on the pre-training and fine-tuning strategy, even a small amount of labeled data can achieve high performance. Compared with many published self-supervised surveys on computer vision and natural language processing, a comprehensive survey for time series SSL is still missing. To fill this gap, we review current state-of-the-art SSL methods for time series data in this article. To this end, we first comprehensively review existing surveys related to SSL and time series, and then provide a new taxonomy of existing time series SSL methods by summarizing them from three perspectives: generative-based, contrastive-based, and adversarial-based. These methods are further divided into ten subcategories with detailed reviews and discussions about their key intuitions, main frameworks, advantages and disadvantages. To facilitate the experiments and validation of time series SSL methods, we also summarize datasets commonly used in time series forecasting, classification, anomaly detection, and clustering tasks. Finally, we present the future directions of SSL for time series analysis.",
      "authors": [
        "Kexin Zhang",
        "Qingsong Wen",
        "Chaoli Zhang",
        "Rongyao Cai",
        "Ming Jin",
        "Yong Liu",
        "James Zhang",
        "Yuxuan Liang",
        "Guansong Pang",
        "Dongjin Song",
        "Shirui Pan"
      ],
      "published": "2023-06-16T18:23:10Z",
      "updated": "2024-04-08T15:38:59Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP",
        "stat.AP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.10125v4",
      "landing_url": "https://arxiv.org/abs/2306.10125v4",
      "doi": "https://doi.org/10.48550/arXiv.2306.10125"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on self-supervised learning for time series and a taxonomy/prospects survey, not on discrete audio tokens or their tokenization; it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on self-supervised learning for time series and a taxonomy/prospects survey, not on discrete audio tokens or their tokenization; it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning methods for time series data broadly and does not specifically address discrete audio tokens or detail token generation processes relevant to audio language modeling or discrete token design, thus failing to meet the inclusion criteria concerning discrete audio token production and usage for sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning methods for time series data broadly and does not specifically address discrete audio tokens or detail token generation processes relevant to audio language modeling or discrete token design, thus failing to meet the inclusion criteria concerning discrete audio token production and usage for sequence modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models",
    "abstract": "Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling.",
    "metadata": {
      "arxiv_id": "2306.10521",
      "title": "LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models",
      "summary": "Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling.",
      "authors": [
        "Zhichao Wang",
        "Yuanzhe Chen",
        "Lei Xie",
        "Qiao Tian",
        "Yuping Wang"
      ],
      "published": "2023-06-18T10:59:06Z",
      "updated": "2023-08-21T02:21:06Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.10521v2",
      "landing_url": "https://arxiv.org/abs/2306.10521v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.10521"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on generating and using discrete audio tokens (semantic and acoustic) via HuBERT/SoundStream and LM-based architectures for zero-shot voice conversion, matching the discrete-audio-token research theme and providing methodological details (two-stage LM, masked prefix LM, external LM) that support reproducibility; no clear exclusion criteria are evident from the abstract alone.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on generating and using discrete audio tokens (semantic and acoustic) via HuBERT/SoundStream and LM-based architectures for zero-shot voice conversion, matching the discrete-audio-token research theme and providing methodological details (two-stage LM, masked prefix LM, external LM) that support reproducibility; no clear exclusion criteria are evident from the abstract alone.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on zero-shot voice conversion using language models that tokenize speech into semantic and acoustic tokens via HuBERT and SoundStream, which are vector-quantized or learned discrete representations, aligning well with the inclusion criteria for discrete audio token design and usage in sequence modeling for audio generation. The paper details the token generation process and modeling strategy in a two-stage LM framework addressing semantic/acoustic disentanglement and content preservation, indicating clear methodological contribution and supporting reproducibility. There is no indication of exclusion criteria issues such as lacking method details, being a mere demo, or relying on standard codecs without linguistic modeling. Therefore, it meets the inclusion criteria and none of the exclusion criteria, supporting a high evaluation score.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on zero-shot voice conversion using language models that tokenize speech into semantic and acoustic tokens via HuBERT and SoundStream, which are vector-quantized or learned discrete representations, aligning well with the inclusion criteria for discrete audio token design and usage in sequence modeling for audio generation. The paper details the token generation process and modeling strategy in a two-stage LM framework addressing semantic/acoustic disentanglement and content preservation, indicating clear methodological contribution and supporting reproducibility. There is no indication of exclusion criteria issues such as lacking method details, being a mere demo, or relying on standard codecs without linguistic modeling. Therefore, it meets the inclusion criteria and none of the exclusion criteria, supporting a high evaluation score.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MFCCGAN: A Novel MFCC-Based Speech Synthesizer Using Adversarial Learning",
    "abstract": "In this paper, we introduce MFCCGAN as a novel speech synthesizer based on adversarial learning that adopts MFCCs as input and generates raw speech waveforms. Benefiting the GAN model capabilities, it produces speech with higher intelligibility than a rule-based MFCC-based speech synthesizer WORLD. We evaluated the model based on a popular intrusive objective speech intelligibility measure (STOI) and quality (NISQA score). Experimental results show that our proposed system outperforms Librosa MFCC- inversion (by an increase of about 26% up to 53% in STOI and 16% up to 78% in NISQA score) and a rise of about 10% in intelligibility and about 4% in naturalness in comparison with conventional rule-based vocoder WORLD that used in the CycleGAN-VC family. However, WORLD needs additional data like F0. Finally, using perceptual loss in discriminators based on STOI could improve the quality more. WebMUSHRA-based subjective tests also show the quality of the proposed approach.",
    "metadata": {
      "arxiv_id": "2306.12785",
      "title": "MFCCGAN: A Novel MFCC-Based Speech Synthesizer Using Adversarial Learning",
      "summary": "In this paper, we introduce MFCCGAN as a novel speech synthesizer based on adversarial learning that adopts MFCCs as input and generates raw speech waveforms. Benefiting the GAN model capabilities, it produces speech with higher intelligibility than a rule-based MFCC-based speech synthesizer WORLD. We evaluated the model based on a popular intrusive objective speech intelligibility measure (STOI) and quality (NISQA score). Experimental results show that our proposed system outperforms Librosa MFCC- inversion (by an increase of about 26% up to 53% in STOI and 16% up to 78% in NISQA score) and a rise of about 10% in intelligibility and about 4% in naturalness in comparison with conventional rule-based vocoder WORLD that used in the CycleGAN-VC family. However, WORLD needs additional data like F0. Finally, using perceptual loss in discriminators based on STOI could improve the quality more. WebMUSHRA-based subjective tests also show the quality of the proposed approach.",
      "authors": [
        "Mohammad Reza Hasanabadi Majid Behdad Davood Gharavian"
      ],
      "published": "2023-06-22T10:29:24Z",
      "updated": "2023-06-22T10:29:24Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.12785v1",
      "landing_url": "https://arxiv.org/abs/2306.12785v1",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10095873"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on MFCC-based speech synthesis with GANs and does not present discrete audio tokens or a tokenization pipeline (e.g., VQ/RVQ, hierarchical quantization), so it does not meet the discrete audio token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on MFCC-based speech synthesis with GANs and does not present discrete audio tokens or a tokenization pipeline (e.g., VQ/RVQ, hierarchical quantization), so it does not meet the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper MFCCGAN focuses on a speech synthesizer that generates raw speech waveforms from MFCC inputs using adversarial learning, with no indication of producing discrete audio tokens or employing vector quantization or discretization mechanisms for sequence modeling; it does not meet the inclusion criteria of presenting discrete token design or usage for language/sequence models, and lacks details on token generation or quantization processes.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper MFCCGAN focuses on a speech synthesizer that generates raw speech waveforms from MFCC inputs using adversarial learning, with no indication of producing discrete audio tokens or employing vector quantization or discretization mechanisms for sequence modeling; it does not meet the inclusion criteria of presenting discrete token design or usage for language/sequence models, and lacks details on token generation or quantization processes.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Singing Voice Conversion Challenge 2023",
    "abstract": "We present the latest iteration of the voice conversion challenge (VCC) series, a bi-annual scientific event aiming to compare and understand different voice conversion (VC) systems based on a common dataset. This year we shifted our focus to singing voice conversion (SVC), thus named the challenge the Singing Voice Conversion Challenge (SVCC). A new database was constructed for two tasks, namely in-domain and cross-domain SVC. The challenge was run for two months, and in total we received 26 submissions, including 2 baselines. Through a large-scale crowd-sourced listening test, we observed that for both tasks, although human-level naturalness was achieved by the top system, no team was able to obtain a similarity score as high as the target speakers. Also, as expected, cross-domain SVC is harder than in-domain SVC, especially in the similarity aspect. We also investigated whether existing objective measurements were able to predict perceptual performance, and found that only few of them could reach a significant correlation.",
    "metadata": {
      "arxiv_id": "2306.14422",
      "title": "The Singing Voice Conversion Challenge 2023",
      "summary": "We present the latest iteration of the voice conversion challenge (VCC) series, a bi-annual scientific event aiming to compare and understand different voice conversion (VC) systems based on a common dataset. This year we shifted our focus to singing voice conversion (SVC), thus named the challenge the Singing Voice Conversion Challenge (SVCC). A new database was constructed for two tasks, namely in-domain and cross-domain SVC. The challenge was run for two months, and in total we received 26 submissions, including 2 baselines. Through a large-scale crowd-sourced listening test, we observed that for both tasks, although human-level naturalness was achieved by the top system, no team was able to obtain a similarity score as high as the target speakers. Also, as expected, cross-domain SVC is harder than in-domain SVC, especially in the similarity aspect. We also investigated whether existing objective measurements were able to predict perceptual performance, and found that only few of them could reach a significant correlation.",
      "authors": [
        "Wen-Chin Huang",
        "Lester Phillip Violeta",
        "Songxiang Liu",
        "Jiatong Shi",
        "Tomoki Toda"
      ],
      "published": "2023-06-26T05:04:58Z",
      "updated": "2023-07-06T08:17:31Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.14422v2",
      "landing_url": "https://arxiv.org/abs/2306.14422v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.14422"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on a singing voice conversion challenge and its results, with no discussion of discrete audio tokens, token generation, or quantization methods, thus it does not meet the token-centric inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on a singing voice conversion challenge and its results, with no discussion of discrete audio tokens, token generation, or quantization methods, thus it does not meet the token-centric inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a singing voice conversion challenge, discussing the evaluation of VC systems and perceptual metrics, but it does not present discrete audio token design or methods for generating discrete tokens used for language or sequence modeling. The abstract lacks detailed technical methodology on token quantization or representation, and it does not claim to produce or utilize discrete tokens for downstream language model applications, which are central to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a singing voice conversion challenge, discussing the evaluation of VC systems and perceptual metrics, but it does not present discrete audio token design or methods for generating discrete tokens used for language or sequence modeling. The abstract lacks detailed technical methodology on token quantization or representation, and it does not claim to produce or utilize discrete tokens for downstream language model applications, which are central to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Fuzzy-Conditioned Diffusion and Diffusion Projection Attention Applied to Facial Image Correction",
    "abstract": "Image diffusion has recently shown remarkable performance in image synthesis and implicitly as an image prior. Such a prior has been used with conditioning to solve the inpainting problem, but only supporting binary user-based conditioning. We derive a fuzzy-conditioned diffusion, where implicit diffusion priors can be exploited with controllable strength. Our fuzzy conditioning can be applied pixel-wise, enabling the modification of different image components to varying degrees. Additionally, we propose an application to facial image correction, where we combine our fuzzy-conditioned diffusion with diffusion-derived attention maps. Our map estimates the degree of anomaly, and we obtain it by projecting on the diffusion space. We show how our approach also leads to interpretable and autonomous facial image correction.",
    "metadata": {
      "arxiv_id": "2306.14891",
      "title": "Fuzzy-Conditioned Diffusion and Diffusion Projection Attention Applied to Facial Image Correction",
      "summary": "Image diffusion has recently shown remarkable performance in image synthesis and implicitly as an image prior. Such a prior has been used with conditioning to solve the inpainting problem, but only supporting binary user-based conditioning. We derive a fuzzy-conditioned diffusion, where implicit diffusion priors can be exploited with controllable strength. Our fuzzy conditioning can be applied pixel-wise, enabling the modification of different image components to varying degrees. Additionally, we propose an application to facial image correction, where we combine our fuzzy-conditioned diffusion with diffusion-derived attention maps. Our map estimates the degree of anomaly, and we obtain it by projecting on the diffusion space. We show how our approach also leads to interpretable and autonomous facial image correction.",
      "authors": [
        "Majed El Helou"
      ],
      "published": "2023-06-26T17:58:00Z",
      "updated": "2023-07-01T22:22:14Z",
      "categories": [
        "cs.CV",
        "cs.LG",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.14891v2",
      "landing_url": "https://arxiv.org/abs/2306.14891v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.14891"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses fuzzy-conditioned diffusion for facial image correction, not discrete audio tokens, so it fails the inclusion criteria focused on discrete audio token design.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses fuzzy-conditioned diffusion for facial image correction, not discrete audio tokens, so it fails the inclusion criteria focused on discrete audio token design.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on image diffusion methods applied to facial image correction, with no mention or indication of discrete audio tokens or related mechanisms such as vector quantization for audio representation, and thus do not meet the inclusion criteria related to audio tokenization and language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on image diffusion methods applied to facial image correction, with no mention or indication of discrete audio tokens or related mechanisms such as vector quantization for audio representation, and thus do not meet the inclusion criteria related to audio tokenization and language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Equivariant flow matching",
    "abstract": "Normalizing flows are a class of deep generative models that are especially interesting for modeling probability distributions in physics, where the exact likelihood of flows allows reweighting to known target energy functions and computing unbiased observables. For instance, Boltzmann generators tackle the long-standing sampling problem in statistical physics by training flows to produce equilibrium samples of many-body systems such as small molecules and proteins. To build effective models for such systems, it is crucial to incorporate the symmetries of the target energy into the model, which can be achieved by equivariant continuous normalizing flows (CNFs). However, CNFs can be computationally expensive to train and generate samples from, which has hampered their scalability and practical application. In this paper, we introduce equivariant flow matching, a new training objective for equivariant CNFs that is based on the recently proposed optimal transport flow matching. Equivariant flow matching exploits the physical symmetries of the target energy for efficient, simulation-free training of equivariant CNFs. We demonstrate the effectiveness of flow matching on rotation and permutation invariant many-particle systems and a small molecule, alanine dipeptide, where for the first time we obtain a Boltzmann generator with significant sampling efficiency without relying on tailored internal coordinate featurization. Our results show that the equivariant flow matching objective yields flows with shorter integration paths, improved sampling efficiency, and higher scalability compared to existing methods.",
    "metadata": {
      "arxiv_id": "2306.15030",
      "title": "Equivariant flow matching",
      "summary": "Normalizing flows are a class of deep generative models that are especially interesting for modeling probability distributions in physics, where the exact likelihood of flows allows reweighting to known target energy functions and computing unbiased observables. For instance, Boltzmann generators tackle the long-standing sampling problem in statistical physics by training flows to produce equilibrium samples of many-body systems such as small molecules and proteins. To build effective models for such systems, it is crucial to incorporate the symmetries of the target energy into the model, which can be achieved by equivariant continuous normalizing flows (CNFs). However, CNFs can be computationally expensive to train and generate samples from, which has hampered their scalability and practical application. In this paper, we introduce equivariant flow matching, a new training objective for equivariant CNFs that is based on the recently proposed optimal transport flow matching. Equivariant flow matching exploits the physical symmetries of the target energy for efficient, simulation-free training of equivariant CNFs. We demonstrate the effectiveness of flow matching on rotation and permutation invariant many-particle systems and a small molecule, alanine dipeptide, where for the first time we obtain a Boltzmann generator with significant sampling efficiency without relying on tailored internal coordinate featurization. Our results show that the equivariant flow matching objective yields flows with shorter integration paths, improved sampling efficiency, and higher scalability compared to existing methods.",
      "authors": [
        "Leon Klein",
        "Andreas Krämer",
        "Frank Noé"
      ],
      "published": "2023-06-26T19:40:10Z",
      "updated": "2023-11-23T21:53:19Z",
      "categories": [
        "stat.ML",
        "cs.LG",
        "physics.chem-ph",
        "physics.comp-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.15030v2",
      "landing_url": "https://arxiv.org/abs/2306.15030v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.15030"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item concerns equivariant flow matching for equivariant CNFs in Boltzmann generators for physical systems; it does not address discrete audio tokens or their tokenization/quantization for audio-language modeling, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item concerns equivariant flow matching for equivariant CNFs in Boltzmann generators for physical systems; it does not address discrete audio tokens or their tokenization/quantization for audio-language modeling, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on equivariant continuous normalizing flows for modeling probability distributions in physics and does not address discrete audio tokens, vector quantization, or any audio signal processing tasks related to the defined inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on equivariant continuous normalizing flows for modeling probability distributions in physics and does not address discrete audio tokens, vector quantization, or any audio signal processing tasks related to the defined inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "3D-Speaker: A Large-Scale Multi-Device, Multi-Distance, and Multi-Dialect Corpus for Speech Representation Disentanglement",
    "abstract": "Disentangling uncorrelated information in speech utterances is a crucial research topic within speech community. Different speech-related tasks focus on extracting distinct speech representations while minimizing the affects of other uncorrelated information. We present a large-scale speech corpus to facilitate the research of speech representation disentanglement. 3D-Speaker contains over 10,000 speakers, each of whom are simultaneously recorded by multiple Devices, locating at different Distances, and some speakers are speaking multiple Dialects. The controlled combinations of multi-dimensional audio data yield a matrix of a diverse blend of speech representation entanglement, thereby motivating intriguing methods to untangle them. The multi-domain nature of 3D-Speaker also makes it a suitable resource to evaluate large universal speech models and experiment methods of out-of-domain learning and self-supervised learning. https://3dspeaker.github.io/",
    "metadata": {
      "arxiv_id": "2306.15354",
      "title": "3D-Speaker: A Large-Scale Multi-Device, Multi-Distance, and Multi-Dialect Corpus for Speech Representation Disentanglement",
      "summary": "Disentangling uncorrelated information in speech utterances is a crucial research topic within speech community. Different speech-related tasks focus on extracting distinct speech representations while minimizing the affects of other uncorrelated information. We present a large-scale speech corpus to facilitate the research of speech representation disentanglement. 3D-Speaker contains over 10,000 speakers, each of whom are simultaneously recorded by multiple Devices, locating at different Distances, and some speakers are speaking multiple Dialects. The controlled combinations of multi-dimensional audio data yield a matrix of a diverse blend of speech representation entanglement, thereby motivating intriguing methods to untangle them. The multi-domain nature of 3D-Speaker also makes it a suitable resource to evaluate large universal speech models and experiment methods of out-of-domain learning and self-supervised learning. https://3dspeaker.github.io/",
      "authors": [
        "Siqi Zheng",
        "Luyao Cheng",
        "Yafeng Chen",
        "Hui Wang",
        "Qian Chen"
      ],
      "published": "2023-06-27T10:09:43Z",
      "updated": "2023-09-25T02:36:41Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.15354v3",
      "landing_url": "https://arxiv.org/abs/2306.15354v3",
      "doi": "https://doi.org/10.48550/arXiv.2306.15354"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes a large-scale speech corpus for disentangling representations, but it does not present discrete audio token design/quantization methods or token-based modeling, so it fails the inclusion criterion for discrete audio tokens; thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes a large-scale speech corpus for disentangling representations, but it does not present discrete audio token design/quantization methods or token-based modeling, so it fails the inclusion criterion for discrete audio tokens; thus it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a large-scale speech corpus focusing on disentangling speech representations, but it does not describe the design or generation of discrete audio tokens (such as vector quantization or other discretization methods) nor their role in sequence or language modeling. It mainly provides data without methodological contribution to discrete token formation or usage, and thus does not meet the inclusion criteria regarding discrete audio token generation and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study presents a large-scale speech corpus focusing on disentangling speech representations, but it does not describe the design or generation of discrete audio tokens (such as vector quantization or other discretization methods) nor their role in sequence or language modeling. It mainly provides data without methodological contribution to discrete token formation or usage, and thus does not meet the inclusion criteria regarding discrete audio token generation and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "On the complexity of isomorphism problems for tensors, groups, and polynomials IV: linear-length reductions and their applications",
    "abstract": "Many isomorphism problems for tensors, groups, algebras, and polynomials were recently shown to be equivalent to one another under polynomial-time reductions, prompting the introduction of the complexity class TI (Grochow & Qiao, ITCS '21; SIAM J. Comp., '23). Using the tensorial viewpoint, Grochow & Qiao (CCC '21) then gave moderately exponential-time search- and counting-to-decision reductions for a class of $p$-groups. A significant issue was that the reductions usually incurred a quadratic increase in the length of the tensors involved. When the tensors represent $p$-groups, this corresponds to an increase in the order of the group of the form $|G|^{Θ(\\log |G|)}$, negating any asymptotic gains in the Cayley table model. In this paper, we present a new kind of tensor gadget that allows us to replace those quadratic-length reductions with linear-length ones, yielding the following consequences: 1. If Graph Isomorphism is in P, then testing equivalence of cubic forms in $n$ variables over $F_q$, and testing isomorphism of $n$-dimensional algebras over $F_q$, can both be solved in time $q^{O(n)}$, improving from the brute-force upper bound $q^{O(n^2)}$ for both of these. 2. Combined with the $|G|^{O((\\log |G|)^{5/6})}$-time isomorphism-test for $p$-groups of class 2 and exponent $p$ (Sun, STOC '23), our reductions extend this runtime to $p$-groups of class $c$ and exponent $p$ where $c<p$, and yield algorithms in time $q^{O(n^{1.8}\\cdot \\log q)}$ for cubic form equivalence and algebra isomorphism. 3. Polynomial-time search- and counting-to-decision reduction for testing isomorphism of $p$-groups of class $2$ and exponent $p$ when Cayley tables are given. This answers questions of Arvind and Tóran (Bull. EATCS, 2005) for this group class, thought to be one of the hardest cases of Group Isomorphism.",
    "metadata": {
      "arxiv_id": "2306.16317",
      "title": "On the complexity of isomorphism problems for tensors, groups, and polynomials IV: linear-length reductions and their applications",
      "summary": "Many isomorphism problems for tensors, groups, algebras, and polynomials were recently shown to be equivalent to one another under polynomial-time reductions, prompting the introduction of the complexity class TI (Grochow & Qiao, ITCS '21; SIAM J. Comp., '23). Using the tensorial viewpoint, Grochow & Qiao (CCC '21) then gave moderately exponential-time search- and counting-to-decision reductions for a class of $p$-groups. A significant issue was that the reductions usually incurred a quadratic increase in the length of the tensors involved. When the tensors represent $p$-groups, this corresponds to an increase in the order of the group of the form $|G|^{Θ(\\log |G|)}$, negating any asymptotic gains in the Cayley table model.\n  In this paper, we present a new kind of tensor gadget that allows us to replace those quadratic-length reductions with linear-length ones, yielding the following consequences:\n  1. If Graph Isomorphism is in P, then testing equivalence of cubic forms in $n$ variables over $F_q$, and testing isomorphism of $n$-dimensional algebras over $F_q$, can both be solved in time $q^{O(n)}$, improving from the brute-force upper bound $q^{O(n^2)}$ for both of these.\n  2. Combined with the $|G|^{O((\\log |G|)^{5/6})}$-time isomorphism-test for $p$-groups of class 2 and exponent $p$ (Sun, STOC '23), our reductions extend this runtime to $p$-groups of class $c$ and exponent $p$ where $c<p$, and yield algorithms in time $q^{O(n^{1.8}\\cdot \\log q)}$ for cubic form equivalence and algebra isomorphism.\n  3. Polynomial-time search- and counting-to-decision reduction for testing isomorphism of $p$-groups of class $2$ and exponent $p$ when Cayley tables are given. This answers questions of Arvind and Tóran (Bull. EATCS, 2005) for this group class, thought to be one of the hardest cases of Group Isomorphism.",
      "authors": [
        "Joshua A. Grochow",
        "Youming Qiao"
      ],
      "published": "2023-06-28T15:49:20Z",
      "updated": "2024-04-12T13:10:08Z",
      "categories": [
        "cs.CC",
        "cs.DS",
        "math.AG",
        "math.GR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.16317v2",
      "landing_url": "https://arxiv.org/abs/2306.16317v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.16317"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is a theoretical CS paper on isomorphism problems and linear-length reductions; it does not address discrete audio tokens or audio representations, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is a theoretical CS paper on isomorphism problems and linear-length reductions; it does not address discrete audio tokens or audio representations, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on computational complexity in isomorphism problems related to tensors, groups, and polynomials, without any mention of discrete audio tokens, audio representation, vector quantization, or any audio-related modeling or generation tasks. It also lacks any detail on token generation or usage in language or sequence modeling contexts, which makes it unrelated to the inclusion criteria focusing on discrete audio tokens and their applications in audio processing and modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on computational complexity in isomorphism problems related to tensors, groups, and polynomials, without any mention of discrete audio tokens, audio representation, vector quantization, or any audio-related modeling or generation tasks. It also lacks any detail on token generation or usage in language or sequence modeling contexts, which makes it unrelated to the inclusion criteria focusing on discrete audio tokens and their applications in audio processing and modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Using joint training speaker encoder with consistency loss to achieve cross-lingual voice conversion and expressive voice conversion",
    "abstract": "Voice conversion systems have made significant advancements in terms of naturalness and similarity in common voice conversion tasks. However, their performance in more complex tasks such as cross-lingual voice conversion and expressive voice conversion remains imperfect. In this study, we propose a novel approach that combines a jointly trained speaker encoder and content features extracted from the cross-lingual speech recognition model Whisper to achieve high-quality cross-lingual voice conversion. Additionally, we introduce a speaker consistency loss to the joint encoder, which improves the similarity between the converted speech and the reference speech. To further explore the capabilities of the joint speaker encoder, we use the phonetic posteriorgram as the content feature, which enables the model to effectively reproduce both the speaker characteristics and the emotional aspects of the reference speech.",
    "metadata": {
      "arxiv_id": "2307.00393",
      "title": "Using joint training speaker encoder with consistency loss to achieve cross-lingual voice conversion and expressive voice conversion",
      "summary": "Voice conversion systems have made significant advancements in terms of naturalness and similarity in common voice conversion tasks. However, their performance in more complex tasks such as cross-lingual voice conversion and expressive voice conversion remains imperfect. In this study, we propose a novel approach that combines a jointly trained speaker encoder and content features extracted from the cross-lingual speech recognition model Whisper to achieve high-quality cross-lingual voice conversion. Additionally, we introduce a speaker consistency loss to the joint encoder, which improves the similarity between the converted speech and the reference speech. To further explore the capabilities of the joint speaker encoder, we use the phonetic posteriorgram as the content feature, which enables the model to effectively reproduce both the speaker characteristics and the emotional aspects of the reference speech.",
      "authors": [
        "Houjian Guo",
        "Chaoran Liu",
        "Carlos Toshinori Ishi",
        "Hiroshi Ishiguro"
      ],
      "published": "2023-07-01T17:44:18Z",
      "updated": "2023-07-01T17:44:18Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.00393v1",
      "landing_url": "https://arxiv.org/abs/2307.00393v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.00393"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work targets cross-lingual and expressive voice conversion using a joint speaker encoder and Whisper-based content features, but it does not involve discrete audio tokens or tokenization/quantization as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work targets cross-lingual and expressive voice conversion using a joint speaker encoder and Whisper-based content features, but it does not involve discrete audio tokens or tokenization/quantization as required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on cross-lingual and expressive voice conversion using a joint speaker encoder and consistency loss, but it does not describe any discrete audio token creation, quantization, or token-based representation that would serve as a core method or research contribution. There is no mention of vector quantization, residual vector quantization, discrete token design, or how tokens are generated and used for sequence modeling, which are essential criteria for inclusion. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens and their generation mechanisms.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on cross-lingual and expressive voice conversion using a joint speaker encoder and consistency loss, but it does not describe any discrete audio token creation, quantization, or token-based representation that would serve as a core method or research contribution. There is no mention of vector quantization, residual vector quantization, discrete token design, or how tokens are generated and used for sequence modeling, which are essential criteria for inclusion. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens and their generation mechanisms.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "On-Device Constrained Self-Supervised Speech Representation Learning for Keyword Spotting via Knowledge Distillation",
    "abstract": "Large self-supervised models are effective feature extractors, but their application is challenging under on-device budget constraints and biased dataset collection, especially in keyword spotting. To address this, we proposed a knowledge distillation-based self-supervised speech representation learning (S3RL) architecture for on-device keyword spotting. Our approach used a teacher-student framework to transfer knowledge from a larger, more complex model to a smaller, light-weight model using dual-view cross-correlation distillation and the teacher's codebook as learning objectives. We evaluated our model's performance on an Alexa keyword spotting detection task using a 16.6k-hour in-house dataset. Our technique showed exceptional performance in normal and noisy conditions, demonstrating the efficacy of knowledge distillation methods in constructing self-supervised models for keyword spotting tasks while working within on-device resource constraints.",
    "metadata": {
      "arxiv_id": "2307.02720",
      "title": "On-Device Constrained Self-Supervised Speech Representation Learning for Keyword Spotting via Knowledge Distillation",
      "summary": "Large self-supervised models are effective feature extractors, but their application is challenging under on-device budget constraints and biased dataset collection, especially in keyword spotting. To address this, we proposed a knowledge distillation-based self-supervised speech representation learning (S3RL) architecture for on-device keyword spotting. Our approach used a teacher-student framework to transfer knowledge from a larger, more complex model to a smaller, light-weight model using dual-view cross-correlation distillation and the teacher's codebook as learning objectives. We evaluated our model's performance on an Alexa keyword spotting detection task using a 16.6k-hour in-house dataset. Our technique showed exceptional performance in normal and noisy conditions, demonstrating the efficacy of knowledge distillation methods in constructing self-supervised models for keyword spotting tasks while working within on-device resource constraints.",
      "authors": [
        "Gene-Ping Yang",
        "Yue Gu",
        "Qingming Tang",
        "Dongsu Du",
        "Yuzong Liu"
      ],
      "published": "2023-07-06T02:03:31Z",
      "updated": "2023-07-06T02:03:31Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.02720v1",
      "landing_url": "https://arxiv.org/abs/2307.02720v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.02720"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on on-device SSL representations for keyword spotting with knowledge distillation and does not present discrete audio token generation/quantization (VQ/RVQ, codebooks as discrete tokens) as the core contribution, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on on-device SSL representations for keyword spotting with knowledge distillation and does not present discrete audio token generation/quantization (VQ/RVQ, codebooks as discrete tokens) as the core contribution, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article proposes a knowledge distillation-based self-supervised learning architecture for on-device keyword spotting but does not describe the generation or use of discrete audio tokens via vector quantization or related methods, nor does it detail a tokenization process used for language or sequence models. The focus is on feature extraction and model compression for keyword spotting, not on discrete token design, quantization, or token usage as described in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article proposes a knowledge distillation-based self-supervised learning architecture for on-device keyword spotting but does not describe the generation or use of discrete audio tokens via vector quantization or related methods, nor does it detail a tokenization process used for language or sequence models. The focus is on feature extraction and model compression for keyword spotting, not on discrete token design, quantization, or token usage as described in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Anableps: Adapting Bitrate for Real-Time Communication Using VBR-encoded Video",
    "abstract": "Content providers increasingly replace traditional constant bitrate with variable bitrate (VBR) encoding in real-time video communication systems for better video quality. However, VBR encoding often leads to large and frequent bitrate fluctuation, inevitably deteriorating the efficiency of existing adaptive bitrate (ABR) methods. To tackle it, we propose the Anableps to consider the network dynamics and VBR-encoding-induced video bitrate fluctuations jointly for deploying the best ABR policy. With this aim, Anableps uses sender-side information from the past to predict the video bitrate range of upcoming frames. Such bitrate range is then combined with the receiver-side observations to set the proper bitrate target for video encoding using a reinforcement-learning-based ABR model. As revealed by extensive experiments on a real-world trace-driven testbed, our Anableps outperforms the GCC with significant improvement of quality of experience, e.g., 1.88x video quality, 57% less bitrate consumption, 85% less stalling, and 74% shorter interaction delay.",
    "metadata": {
      "arxiv_id": "2307.03436",
      "title": "Anableps: Adapting Bitrate for Real-Time Communication Using VBR-encoded Video",
      "summary": "Content providers increasingly replace traditional constant bitrate with variable bitrate (VBR) encoding in real-time video communication systems for better video quality. However, VBR encoding often leads to large and frequent bitrate fluctuation, inevitably deteriorating the efficiency of existing adaptive bitrate (ABR) methods. To tackle it, we propose the Anableps to consider the network dynamics and VBR-encoding-induced video bitrate fluctuations jointly for deploying the best ABR policy. With this aim, Anableps uses sender-side information from the past to predict the video bitrate range of upcoming frames. Such bitrate range is then combined with the receiver-side observations to set the proper bitrate target for video encoding using a reinforcement-learning-based ABR model. As revealed by extensive experiments on a real-world trace-driven testbed, our Anableps outperforms the GCC with significant improvement of quality of experience, e.g., 1.88x video quality, 57% less bitrate consumption, 85% less stalling, and 74% shorter interaction delay.",
      "authors": [
        "Zicheng Zhang",
        "Hao Chen",
        "Xun Cao",
        "Zhan Ma"
      ],
      "published": "2023-07-07T07:47:45Z",
      "updated": "2023-07-07T07:47:45Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.03436v1",
      "landing_url": "https://arxiv.org/abs/2307.03436v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.03436"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses adapting bitrate for VBR-encoded video in real-time communication and does not address discrete audio tokens, tokenization, or token-based representations; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses adapting bitrate for VBR-encoded video in real-time communication and does not address discrete audio tokens, tokenization, or token-based representations; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on adaptive bitrate for real-time video communication using VBR-encoded video and reinforcement learning-based ABR models, without discussing discrete audio token design, token generation processes, or use in language or sequence models; thus, it does not meet the inclusion criteria related to discrete audio tokens and representation relevant to audio language modeling or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on adaptive bitrate for real-time video communication using VBR-encoded video and reinforcement learning-based ABR models, without discussing discrete audio token design, token generation processes, or use in language or sequence models; thus, it does not meet the inclusion criteria related to discrete audio tokens and representation relevant to audio language modeling or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Simulation-free Schrödinger bridges via score and flow matching",
    "abstract": "We present simulation-free score and flow matching ([SF]$^2$M), a simulation-free objective for inferring stochastic dynamics given unpaired samples drawn from arbitrary source and target distributions. Our method generalizes both the score-matching loss used in the training of diffusion models and the recently proposed flow matching loss used in the training of continuous normalizing flows. [SF]$^2$M interprets continuous-time stochastic generative modeling as a Schrödinger bridge problem. It relies on static entropy-regularized optimal transport, or a minibatch approximation, to efficiently learn the SB without simulating the learned stochastic process. We find that [SF]$^2$M is more efficient and gives more accurate solutions to the SB problem than simulation-based methods from prior work. Finally, we apply [SF]$^2$M to the problem of learning cell dynamics from snapshot data. Notably, [SF]$^2$M is the first method to accurately model cell dynamics in high dimensions and can recover known gene regulatory networks from simulated data. Our code is available in the TorchCFM package at https://github.com/atong01/conditional-flow-matching.",
    "metadata": {
      "arxiv_id": "2307.03672",
      "title": "Simulation-free Schrödinger bridges via score and flow matching",
      "summary": "We present simulation-free score and flow matching ([SF]$^2$M), a simulation-free objective for inferring stochastic dynamics given unpaired samples drawn from arbitrary source and target distributions. Our method generalizes both the score-matching loss used in the training of diffusion models and the recently proposed flow matching loss used in the training of continuous normalizing flows. [SF]$^2$M interprets continuous-time stochastic generative modeling as a Schrödinger bridge problem. It relies on static entropy-regularized optimal transport, or a minibatch approximation, to efficiently learn the SB without simulating the learned stochastic process. We find that [SF]$^2$M is more efficient and gives more accurate solutions to the SB problem than simulation-based methods from prior work. Finally, we apply [SF]$^2$M to the problem of learning cell dynamics from snapshot data. Notably, [SF]$^2$M is the first method to accurately model cell dynamics in high dimensions and can recover known gene regulatory networks from simulated data. Our code is available in the TorchCFM package at https://github.com/atong01/conditional-flow-matching.",
      "authors": [
        "Alexander Tong",
        "Nikolay Malkin",
        "Kilian Fatras",
        "Lazar Atanackovic",
        "Yanlei Zhang",
        "Guillaume Huguet",
        "Guy Wolf",
        "Yoshua Bengio"
      ],
      "published": "2023-07-07T15:42:35Z",
      "updated": "2024-03-11T14:42:58Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.03672v3",
      "landing_url": "https://arxiv.org/abs/2307.03672v3",
      "doi": "https://doi.org/10.48550/arXiv.2307.03672"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on simulation-free Schrödinger bridges for inferring stochastic dynamics and does not address discrete audio tokens or tokenization for audio-language modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on simulation-free Schrödinger bridges for inferring stochastic dynamics and does not address discrete audio tokens or tokenization for audio-language modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a simulation-free method for inferring stochastic dynamics and modeling cell dynamics, focusing on continuous-time stochastic generative modeling framed as a Schrödinger bridge problem. However, it does not mention any aspect related to discrete audio tokens, vector quantization, or audio signal processing for language or sequence modeling; thus, it does not meet the core inclusion criteria related to discrete audio token design or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents a simulation-free method for inferring stochastic dynamics and modeling cell dynamics, focusing on continuous-time stochastic generative modeling framed as a Schrödinger bridge problem. However, it does not mention any aspect related to discrete audio tokens, vector quantization, or audio signal processing for language or sequence modeling; thus, it does not meet the core inclusion criteria related to discrete audio token design or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VampNet: Music Generation via Masked Acoustic Token Modeling",
    "abstract": "We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.",
    "metadata": {
      "arxiv_id": "2307.04686",
      "title": "VampNet: Music Generation via Masked Acoustic Token Modeling",
      "summary": "We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.",
      "authors": [
        "Hugo Flores Garcia",
        "Prem Seetharaman",
        "Rithesh Kumar",
        "Bryan Pardo"
      ],
      "published": "2023-07-10T16:42:03Z",
      "updated": "2023-07-12T17:06:41Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.04686v2",
      "landing_url": "https://arxiv.org/abs/2307.04686v2",
      "doi": "https://doi.org/10.48550/arXiv.2307.04686"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work uses token-based modeling for music generation and inpainting, but the abstract does not explicitly specify discrete token construction (e.g., vector quantization or a defined token production pipeline) or a clearly defined discrete token design, making it unclear whether it meets the 'Discrete Audio Tokens' core contribution.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The work uses token-based modeling for music generation and inpainting, but the abstract does not explicitly specify discrete token construction (e.g., vector quantization or a defined token production pipeline) or a clearly defined discrete token design, making it unclear whether it meets the 'Discrete Audio Tokens' core contribution.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents VampNet, a masked acoustic token modeling approach that leverages a bidirectional transformer to generate music from discrete acoustic tokens. The abstract emphasizes token-based music synthesis and manipulation, implying a discrete token representation of music audio. Although the exact token generation process and quantization details are not fully described in the abstract, the methodology involving masked acoustic tokens, bidirectional transformers, and various sampling passes strongly suggests an emphasis on discrete audio token modeling as defined, aiming at generation and compression tasks. Since it explicitly discusses discrete tokens used for sequence modeling in music generation and does not mention traditional codecs but rather a novel modeling approach, it aligns well with the inclusion criteria and lacks exclusion criteria indications. However, lack of explicit quantization or token formation detail in abstract slightly limits certainty.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The paper presents VampNet, a masked acoustic token modeling approach that leverages a bidirectional transformer to generate music from discrete acoustic tokens. The abstract emphasizes token-based music synthesis and manipulation, implying a discrete token representation of music audio. Although the exact token generation process and quantization details are not fully described in the abstract, the methodology involving masked acoustic tokens, bidirectional transformers, and various sampling passes strongly suggests an emphasis on discrete audio token modeling as defined, aiming at generation and compression tasks. Since it explicitly discusses discrete tokens used for sequence modeling in music generation and does not mention traditional codecs but rather a novel modeling approach, it aligns well with the inclusion criteria and lacks exclusion criteria indications. However, lack of explicit quantization or token formation detail in abstract slightly limits certainty.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: one (JuniorNano) flagged uncertainty because the abstract does not explicitly describe the token-formation/quantization pipeline; the other (JuniorMini) leaned to include, noting the explicit phrasing “masked acoustic token modeling” and mention of compression and sampling passes implying discrete tokens. My judgment: the abstract strongly implies a discrete acoustic-token-based method (masked token modeling, compression, and various token-based prompting), and it presents core-generation/compression tasks consistent with the Discrete Audio Tokens topic; although the abstract omits low-level quantization details, nothing in the abstract indicates this is merely a demo/blog or a traditional codec (exclusion criteria). Therefore I recommend inclusion but with moderate confidence pending inspection of the full paper for explicit token-generation details. One-sentence explanation: Score 4 because the work clearly centers on masked acoustic token modeling and compression (matching the discrete-token topic), even though the abstract lacks explicit quantization/implementation details.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: one (JuniorNano) flagged uncertainty because the abstract does not explicitly describe the token-formation/quantization pipeline; the other (JuniorMini) leaned to include, noting the explicit phrasing “masked acoustic token modeling” and mention of compression and sampling passes implying discrete tokens. My judgment: the abstract strongly implies a discrete acoustic-token-based method (masked token modeling, compression, and various token-based prompting), and it presents core-generation/compression tasks consistent with the Discrete Audio Tokens topic; although the abstract omits low-level quantization details, nothing in the abstract indicates this is merely a demo/blog or a traditional codec (exclusion criteria). Therefore I recommend inclusion but with moderate confidence pending inspection of the full paper for explicit token-generation details. One-sentence explanation: Score 4 because the work clearly centers on masked acoustic token modeling and compression (matching the discrete-token topic), even though the abstract lacks explicit quantization/implementation details.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Flow Matching in Latent Space",
    "abstract": "Flow matching is a recent framework to train generative models that exhibits impressive empirical performance while being relatively easier to train compared with diffusion-based models. Despite its advantageous properties, prior methods still face the challenges of expensive computing and a large number of function evaluations of off-the-shelf solvers in the pixel space. Furthermore, although latent-based generative methods have shown great success in recent years, this particular model type remains underexplored in this area. In this work, we propose to apply flow matching in the latent spaces of pretrained autoencoders, which offers improved computational efficiency and scalability for high-resolution image synthesis. This enables flow-matching training on constrained computational resources while maintaining their quality and flexibility. Additionally, our work stands as a pioneering contribution in the integration of various conditions into flow matching for conditional generation tasks, including label-conditioned image generation, image inpainting, and semantic-to-image generation. Through extensive experiments, our approach demonstrates its effectiveness in both quantitative and qualitative results on various datasets, such as CelebA-HQ, FFHQ, LSUN Church & Bedroom, and ImageNet. We also provide a theoretical control of the Wasserstein-2 distance between the reconstructed latent flow distribution and true data distribution, showing it is upper-bounded by the latent flow matching objective. Our code will be available at https://github.com/VinAIResearch/LFM.git.",
    "metadata": {
      "arxiv_id": "2307.08698",
      "title": "Flow Matching in Latent Space",
      "summary": "Flow matching is a recent framework to train generative models that exhibits impressive empirical performance while being relatively easier to train compared with diffusion-based models. Despite its advantageous properties, prior methods still face the challenges of expensive computing and a large number of function evaluations of off-the-shelf solvers in the pixel space. Furthermore, although latent-based generative methods have shown great success in recent years, this particular model type remains underexplored in this area. In this work, we propose to apply flow matching in the latent spaces of pretrained autoencoders, which offers improved computational efficiency and scalability for high-resolution image synthesis. This enables flow-matching training on constrained computational resources while maintaining their quality and flexibility. Additionally, our work stands as a pioneering contribution in the integration of various conditions into flow matching for conditional generation tasks, including label-conditioned image generation, image inpainting, and semantic-to-image generation. Through extensive experiments, our approach demonstrates its effectiveness in both quantitative and qualitative results on various datasets, such as CelebA-HQ, FFHQ, LSUN Church & Bedroom, and ImageNet. We also provide a theoretical control of the Wasserstein-2 distance between the reconstructed latent flow distribution and true data distribution, showing it is upper-bounded by the latent flow matching objective. Our code will be available at https://github.com/VinAIResearch/LFM.git.",
      "authors": [
        "Quan Dao",
        "Hao Phung",
        "Binh Nguyen",
        "Anh Tran"
      ],
      "published": "2023-07-17T17:57:56Z",
      "updated": "2023-07-17T17:57:56Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.08698v1",
      "landing_url": "https://arxiv.org/abs/2307.08698v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.08698"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on latent-space flow matching for image generation with autoencoders, not on discrete audio token design or tokenization methods, thus it does not satisfy the inclusion criteria centered on discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on latent-space flow matching for image generation with autoencoders, not on discrete audio token design or tokenization methods, thus it does not satisfy the inclusion criteria centered on discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on flow matching for generative models in latent spaces of image data, discussing computational efficiency and conditional image generation, with no mention of discrete audio tokens, audio signal quantization, or any audio-related content relevant to the defined inclusion criteria. It does not involve any discrete audio token design, generation, or usage as described, and thus does not meet the inclusion criteria for audio token-related research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on flow matching for generative models in latent spaces of image data, discussing computational efficiency and conditional image generation, with no mention of discrete audio tokens, audio signal quantization, or any audio-related content relevant to the defined inclusion criteria. It does not involve any discrete audio token design, generation, or usage as described, and thus does not meet the inclusion criteria for audio token-related research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MASR: Multi-label Aware Speech Representation",
    "abstract": "In the recent years, speech representation learning is constructed primarily as a self-supervised learning (SSL) task, using the raw audio signal alone, while ignoring the side-information that is often available for a given speech recording. In this paper, we propose MASR, a Multi-label Aware Speech Representation learning framework, which addresses the aforementioned limitations. MASR enables the inclusion of multiple external knowledge sources to enhance the utilization of meta-data information. The external knowledge sources are incorporated in the form of sample-level pair-wise similarity matrices that are useful in a hard-mining loss. A key advantage of the MASR framework is that it can be combined with any choice of SSL method. Using MASR representations, we perform evaluations on several downstream tasks such as language identification, speech recognition and other non-semantic tasks such as speaker and emotion recognition. In these experiments, we illustrate significant performance improvements for the MASR over other established benchmarks. We perform a detailed analysis on the language identification task to provide insights on how the proposed loss function enables the representations to separate closely related languages.",
    "metadata": {
      "arxiv_id": "2307.10982",
      "title": "MASR: Multi-label Aware Speech Representation",
      "summary": "In the recent years, speech representation learning is constructed primarily as a self-supervised learning (SSL) task, using the raw audio signal alone, while ignoring the side-information that is often available for a given speech recording. In this paper, we propose MASR, a Multi-label Aware Speech Representation learning framework, which addresses the aforementioned limitations. MASR enables the inclusion of multiple external knowledge sources to enhance the utilization of meta-data information. The external knowledge sources are incorporated in the form of sample-level pair-wise similarity matrices that are useful in a hard-mining loss. A key advantage of the MASR framework is that it can be combined with any choice of SSL method. Using MASR representations, we perform evaluations on several downstream tasks such as language identification, speech recognition and other non-semantic tasks such as speaker and emotion recognition. In these experiments, we illustrate significant performance improvements for the MASR over other established benchmarks. We perform a detailed analysis on the language identification task to provide insights on how the proposed loss function enables the representations to separate closely related languages.",
      "authors": [
        "Anjali Raj",
        "Shikhar Bharadwaj",
        "Sriram Ganapathy",
        "Min Ma",
        "Shikhar Vashishth"
      ],
      "published": "2023-07-20T16:09:57Z",
      "updated": "2023-09-25T12:49:00Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.10982v2",
      "landing_url": "https://arxiv.org/abs/2307.10982v2",
      "doi": "https://doi.org/10.48550/arXiv.2307.10982"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on SSL-based speech representations with external knowledge and a hard-mining loss; it does not describe discrete audio token generation/quantization or token-based representations, so it does not meet the core inclusion criteria for Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on SSL-based speech representations with external knowledge and a hard-mining loss; it does not describe discrete audio token generation/quantization or token-based representations, so it does not meet the core inclusion criteria for Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech representation learning using self-supervised learning enhanced by external meta-data, but it does not discuss or propose discrete audio tokens, nor the processes for token generation such as vector quantization or similar. It also lacks description of tokenization flow, quantization methods, or usage of discrete tokens for sequence modeling or language modeling. Therefore, it does not meet the core inclusion criteria regarding discrete audio token design or application.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech representation learning using self-supervised learning enhanced by external meta-data, but it does not discuss or propose discrete audio tokens, nor the processes for token generation such as vector quantization or similar. It also lacks description of tokenization flow, quantization methods, or usage of discrete tokens for sequence modeling or language modeling. Therefore, it does not meet the core inclusion criteria regarding discrete audio token design or application.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems",
    "abstract": "MeetEval is an open-source toolkit to evaluate all kinds of meeting transcription systems. It provides a unified interface for the computation of commonly used Word Error Rates (WERs), specifically cpWER, ORC-WER and MIMO-WER along other WER definitions. We extend the cpWER computation by a temporal constraint to ensure that only words are identified as correct when the temporal alignment is plausible. This leads to a better quality of the matching of the hypothesis string to the reference string that more closely resembles the actual transcription quality, and a system is penalized if it provides poor time annotations. Since word-level timing information is often not available, we present a way to approximate exact word-level timings from segment-level timings (e.g., a sentence) and show that the approximation leads to a similar WER as a matching with exact word-level annotations. At the same time, the time constraint leads to a speedup of the matching algorithm, which outweighs the additional overhead caused by processing the time stamps.",
    "metadata": {
      "arxiv_id": "2307.11394",
      "title": "MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems",
      "summary": "MeetEval is an open-source toolkit to evaluate all kinds of meeting transcription systems. It provides a unified interface for the computation of commonly used Word Error Rates (WERs), specifically cpWER, ORC-WER and MIMO-WER along other WER definitions. We extend the cpWER computation by a temporal constraint to ensure that only words are identified as correct when the temporal alignment is plausible. This leads to a better quality of the matching of the hypothesis string to the reference string that more closely resembles the actual transcription quality, and a system is penalized if it provides poor time annotations. Since word-level timing information is often not available, we present a way to approximate exact word-level timings from segment-level timings (e.g., a sentence) and show that the approximation leads to a similar WER as a matching with exact word-level annotations. At the same time, the time constraint leads to a speedup of the matching algorithm, which outweighs the additional overhead caused by processing the time stamps.",
      "authors": [
        "Thilo von Neumann",
        "Christoph Boeddeker",
        "Marc Delcroix",
        "Reinhold Haeb-Umbach"
      ],
      "published": "2023-07-21T07:22:18Z",
      "updated": "2024-01-25T19:48:36Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.11394v3",
      "landing_url": "https://arxiv.org/abs/2307.11394v3",
      "doi": "https://doi.org/10.48550/arXiv.2307.11394"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on evaluating transcription systems and WER metrics, not on discrete audio token design or token-based representations; it does not satisfy the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on evaluating transcription systems and WER metrics, not on discrete audio token design or token-based representations; it does not satisfy the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article describes MeetEval, a toolkit for computing word error rates in meeting transcription systems, focusing on evaluation metrics rather than the design or generation of discrete audio tokens. It lacks discussion on discrete token generation processes such as vector quantization or semantic disentanglement, and does not present a method involving discrete audio token representation for use in language or sequence models, thus not meeting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article describes MeetEval, a toolkit for computing word error rates in meeting transcription systems, focusing on evaluation metrics rather than the design or generation of discrete audio tokens. It lacks discussion on discrete token generation processes such as vector quantization or semantic disentanglement, and does not present a method involving discrete audio token representation for use in language or sequence models, thus not meeting the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Online Clustered Codebook",
    "abstract": "Vector Quantisation (VQ) is experiencing a comeback in machine learning, where it is increasingly used in representation learning. However, optimizing the codevectors in existing VQ-VAE is not entirely trivial. A problem is codebook collapse, where only a small subset of codevectors receive gradients useful for their optimisation, whereas a majority of them simply ``dies off'' and is never updated or used. This limits the effectiveness of VQ for learning larger codebooks in complex computer vision tasks that require high-capacity representations. In this paper, we present a simple alternative method for online codebook learning, Clustering VQ-VAE (CVQ-VAE). Our approach selects encoded features as anchors to update the ``dead'' codevectors, while optimising the codebooks which are alive via the original loss. This strategy brings unused codevectors closer in distribution to the encoded features, increasing the likelihood of being chosen and optimized. We extensively validate the generalization capability of our quantiser on various datasets, tasks (e.g. reconstruction and generation), and architectures (e.g. VQ-VAE, VQGAN, LDM). Our CVQ-VAE can be easily integrated into the existing models with just a few lines of code.",
    "metadata": {
      "arxiv_id": "2307.15139",
      "title": "Online Clustered Codebook",
      "summary": "Vector Quantisation (VQ) is experiencing a comeback in machine learning, where it is increasingly used in representation learning. However, optimizing the codevectors in existing VQ-VAE is not entirely trivial. A problem is codebook collapse, where only a small subset of codevectors receive gradients useful for their optimisation, whereas a majority of them simply ``dies off'' and is never updated or used. This limits the effectiveness of VQ for learning larger codebooks in complex computer vision tasks that require high-capacity representations. In this paper, we present a simple alternative method for online codebook learning, Clustering VQ-VAE (CVQ-VAE). Our approach selects encoded features as anchors to update the ``dead'' codevectors, while optimising the codebooks which are alive via the original loss. This strategy brings unused codevectors closer in distribution to the encoded features, increasing the likelihood of being chosen and optimized. We extensively validate the generalization capability of our quantiser on various datasets, tasks (e.g. reconstruction and generation), and architectures (e.g. VQ-VAE, VQGAN, LDM). Our CVQ-VAE can be easily integrated into the existing models with just a few lines of code.",
      "authors": [
        "Chuanxia Zheng",
        "Andrea Vedaldi"
      ],
      "published": "2023-07-27T18:31:04Z",
      "updated": "2023-07-27T18:31:04Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.15139v1",
      "landing_url": "https://arxiv.org/abs/2307.15139v1",
      "doi": null
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work discusses online codebook learning for Vector Quantisation in VQ-VAE and codebook updates, but it does not center on discrete audio tokens or a tokenization design for audio as required by the inclusion criteria; thus it does not meet the criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work discusses online codebook learning for Vector Quantisation in VQ-VAE and codebook updates, but it does not center on discrete audio tokens or a tokenization design for audio as required by the inclusion criteria; thus it does not meet the criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on Vector Quantisation (VQ) methods and codebook learning improvements in machine learning models such as VQ-VAE for image and general representation learning, without addressing discrete audio tokens or audio-specific applications like speech or music tokenization. It does not discuss the generation of discrete audio tokens for sequence or language model processing in audio-related tasks, nor provide token generation processes or usage aligned with the inclusion criteria. Therefore, it does not meet the inclusion criteria centered on discrete audio token design and application.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on Vector Quantisation (VQ) methods and codebook learning improvements in machine learning models such as VQ-VAE for image and general representation learning, without addressing discrete audio tokens or audio-specific applications like speech or music tokenization. It does not discuss the generation of discrete audio tokens for sequence or language model processing in audio-related tasks, nor provide token generation processes or usage aligned with the inclusion criteria. Therefore, it does not meet the inclusion criteria centered on discrete audio token design and application.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adaptive Bitrate Video Semantic Communication over Wireless Networks",
    "abstract": "This paper investigates the adaptive bitrate (ABR) video semantic communication over wireless networks. In the considered model, video sensing devices must transmit video semantic information to an edge server, to facilitate ubiquitous video sensing services such as road environment monitoring at the edge server in autonomous driving scenario. However, due to the varying wireless network conditions, it is challenging to guarantee both low transmission delay and high semantic accuracy at the same time if devices continuously transmit a fixed bitrate video semantic information. To address this challenge, we develop an adaptive bitrate video semantic communication (ABRVSC) system, in which devices adaptively adjust the bitrate of video semantic information according to network conditions. Specifically, we first define the quality of experience (QoE) for video semantic communication. Subsequently, a swin transformer-based semantic codec is proposed to extract semantic information with considering the influence of QoE. Then, we propose an Actor-Critic based ABR algorithm for the semantic codec to enhance the robustness of the proposed ABRVSC scheme against network variations. Simulation results demonstrate that at low bitrates, the mean intersection over union (MIoU) of the proposed ABRVSC scheme is nearly twice that of the traditional scheme. Moreover, the proposed ABRVSC scheme, which increases the QoE in video semantic communication by 36.57%, exhibits more robustness against network variations compared to both the fixed bitrate schemes and traditional ABR schemes.",
    "metadata": {
      "arxiv_id": "2308.00531",
      "title": "Adaptive Bitrate Video Semantic Communication over Wireless Networks",
      "summary": "This paper investigates the adaptive bitrate (ABR) video semantic communication over wireless networks. In the considered model, video sensing devices must transmit video semantic information to an edge server, to facilitate ubiquitous video sensing services such as road environment monitoring at the edge server in autonomous driving scenario. However, due to the varying wireless network conditions, it is challenging to guarantee both low transmission delay and high semantic accuracy at the same time if devices continuously transmit a fixed bitrate video semantic information. To address this challenge, we develop an adaptive bitrate video semantic communication (ABRVSC) system, in which devices adaptively adjust the bitrate of video semantic information according to network conditions. Specifically, we first define the quality of experience (QoE) for video semantic communication. Subsequently, a swin transformer-based semantic codec is proposed to extract semantic information with considering the influence of QoE. Then, we propose an Actor-Critic based ABR algorithm for the semantic codec to enhance the robustness of the proposed ABRVSC scheme against network variations. Simulation results demonstrate that at low bitrates, the mean intersection over union (MIoU) of the proposed ABRVSC scheme is nearly twice that of the traditional scheme. Moreover, the proposed ABRVSC scheme, which increases the QoE in video semantic communication by 36.57%, exhibits more robustness against network variations compared to both the fixed bitrate schemes and traditional ABR schemes.",
      "authors": [
        "Wentao Gong",
        "Haonan Tong",
        "Sihua Wang",
        "Zhaohui Yang",
        "Xinxin He",
        "Changchuan Yin"
      ],
      "published": "2023-08-01T13:25:10Z",
      "updated": "2023-08-01T13:25:10Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.00531v1",
      "landing_url": "https://arxiv.org/abs/2308.00531v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.00531"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on adaptive bitrate video semantic communication and a semantic codec for video data, not on discrete audio tokens or tokenization methods for audio as defined by the inclusion criteria; therefore it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on adaptive bitrate video semantic communication and a semantic codec for video data, not on discrete audio tokens or tokenization methods for audio as defined by the inclusion criteria; therefore it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on adaptive bitrate video semantic communication over wireless networks, discussing video semantic information transmission and adaptive bitrate adjustment. It does not address discrete audio tokens, nor does it describe token generation processes, quantization schemes, or usage of discrete tokens for audio language modeling or related tasks. It lacks relevance to the defined inclusion criteria centered on discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on adaptive bitrate video semantic communication over wireless networks, discussing video semantic information transmission and adaptive bitrate adjustment. It does not address discrete audio tokens, nor does it describe token generation processes, quantization schemes, or usage of discrete tokens for audio language modeling or related tasks. It lacks relevance to the defined inclusion criteria centered on discrete audio token research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Pre-trained Data Deduplication Model based on Active Learning",
    "abstract": "In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These \"dirty data\" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve the model's performance. Experimental results demonstrate that our proposed model outperforms previous state-of-the-art (SOTA) for deduplicated data identification, achieving up to a 28% improvement in Recall score on benchmark datasets.",
    "metadata": {
      "arxiv_id": "2308.00721",
      "title": "A Pre-trained Data Deduplication Model based on Active Learning",
      "summary": "In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These \"dirty data\" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve the model's performance. Experimental results demonstrate that our proposed model outperforms previous state-of-the-art (SOTA) for deduplicated data identification, achieving up to a 28% improvement in Recall score on benchmark datasets.",
      "authors": [
        "Haochen Shi",
        "Xinyao Liu",
        "Fengmao Lv",
        "Hongtao Xue",
        "Jie Hu",
        "Shengdong Du",
        "Tianrui Li"
      ],
      "published": "2023-07-31T03:56:46Z",
      "updated": "2025-01-10T09:35:20Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.00721v4",
      "landing_url": "https://arxiv.org/abs/2308.00721v4",
      "doi": "https://doi.org/10.48550/arXiv.2308.00721"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about data deduplication using active learning and transformers, not about discrete audio tokens; it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about data deduplication using active learning and transformers, not about discrete audio tokens; it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a pre-trained data deduplication model based on active learning, which addresses big data quality issues but does not relate to discrete audio tokens or audio representation; it does not discuss any tokenization or quantization of audio signals nor involve audio language modeling or cross-modal audio-text generation as defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a pre-trained data deduplication model based on active learning, which addresses big data quality issues but does not relate to discrete audio tokens or audio representation; it does not discuss any tokenization or quantization of audio signals nor involve audio language modeling or cross-modal audio-text generation as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Phoneme Hallucinator: One-shot Voice Conversion via Set Expansion",
    "abstract": "Voice conversion (VC) aims at altering a person's voice to make it sound similar to the voice of another person while preserving linguistic content. Existing methods suffer from a dilemma between content intelligibility and speaker similarity; i.e., methods with higher intelligibility usually have a lower speaker similarity, while methods with higher speaker similarity usually require plenty of target speaker voice data to achieve high intelligibility. In this work, we propose a novel method \\textit{Phoneme Hallucinator} that achieves the best of both worlds. Phoneme Hallucinator is a one-shot VC model; it adopts a novel model to hallucinate diversified and high-fidelity target speaker phonemes based just on a short target speaker voice (e.g. 3 seconds). The hallucinated phonemes are then exploited to perform neighbor-based voice conversion. Our model is a text-free, any-to-any VC model that requires no text annotations and supports conversion to any unseen speaker. Objective and subjective evaluations show that \\textit{Phoneme Hallucinator} outperforms existing VC methods for both intelligibility and speaker similarity.",
    "metadata": {
      "arxiv_id": "2308.06382",
      "title": "Phoneme Hallucinator: One-shot Voice Conversion via Set Expansion",
      "summary": "Voice conversion (VC) aims at altering a person's voice to make it sound similar to the voice of another person while preserving linguistic content. Existing methods suffer from a dilemma between content intelligibility and speaker similarity; i.e., methods with higher intelligibility usually have a lower speaker similarity, while methods with higher speaker similarity usually require plenty of target speaker voice data to achieve high intelligibility. In this work, we propose a novel method \\textit{Phoneme Hallucinator} that achieves the best of both worlds. Phoneme Hallucinator is a one-shot VC model; it adopts a novel model to hallucinate diversified and high-fidelity target speaker phonemes based just on a short target speaker voice (e.g. 3 seconds). The hallucinated phonemes are then exploited to perform neighbor-based voice conversion. Our model is a text-free, any-to-any VC model that requires no text annotations and supports conversion to any unseen speaker. Objective and subjective evaluations show that \\textit{Phoneme Hallucinator} outperforms existing VC methods for both intelligibility and speaker similarity.",
      "authors": [
        "Siyuan Shan",
        "Yang Li",
        "Amartya Banerjee",
        "Junier B. Oliva"
      ],
      "published": "2023-08-11T20:44:19Z",
      "updated": "2023-12-30T22:48:13Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.06382v2",
      "landing_url": "https://arxiv.org/abs/2308.06382v2",
      "doi": "https://doi.org/10.48550/arXiv.2308.06382"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe one-shot voice conversion via phoneme hallucination, but there is no discussion of discrete audio tokens, token generation via VQ/RVQ/quantization, or token-based representations for audio-language modeling, so it does not satisfy the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe one-shot voice conversion via phoneme hallucination, but there is no discussion of discrete audio tokens, token generation via VQ/RVQ/quantization, or token-based representations for audio-language modeling, so it does not satisfy the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a one-shot voice conversion method that hallucinates phonemes for voice conversion, but it does not mention producing discrete audio tokens through vector quantization or similar discrete tokenization methods, nor does it describe token generation processes or use tokens for sequence modeling or language modeling purposes, which are required by the inclusion criteria. Therefore, it does not meet the core inclusion criteria related to discrete audio token design and usage for downstream tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a one-shot voice conversion method that hallucinates phonemes for voice conversion, but it does not mention producing discrete audio tokens through vector quantization or similar discrete tokenization methods, nor does it describe token generation processes or use tokens for sequence modeling or language modeling purposes, which are required by the inclusion criteria. Therefore, it does not meet the core inclusion criteria related to discrete audio token design and usage for downstream tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models",
    "abstract": "Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.",
    "metadata": {
      "arxiv_id": "2308.16692",
      "title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models",
      "summary": "Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.",
      "authors": [
        "Xin Zhang",
        "Dong Zhang",
        "Shimin Li",
        "Yaqian Zhou",
        "Xipeng Qiu"
      ],
      "published": "2023-08-31T12:53:09Z",
      "updated": "2024-01-23T01:56:57Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.16692v2",
      "landing_url": "https://arxiv.org/abs/2308.16692v2",
      "doi": "https://doi.org/10.48550/arXiv.2308.16692"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract presents a unified SpeechTokenizer based on RVQ for discrete audio tokens with semantic/acoustic disentanglement and a related USLM, includes methodological details and code, aligning with the inclusion criteria and not merely a demo.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract presents a unified SpeechTokenizer based on RVQ for discrete audio tokens with semantic/acoustic disentanglement and a related USLM, includes methodological details and code, aligning with the inclusion criteria and not merely a demo.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes SpeechTokenizer, a unified speech tokenizer combining semantic and acoustic tokens using an Encoder-Decoder architecture with residual vector quantization (RVQ), aligns well with the inclusion criteria of discrete audio tokens involving RVQ and semantic/acoustic disentanglement, provides method details including token generation and usage in speech language models, and includes reproducible code; it does not fall under any exclusion criteria and is relevant to discrete audio token research focused on language modeling and generation.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes SpeechTokenizer, a unified speech tokenizer combining semantic and acoustic tokens using an Encoder-Decoder architecture with residual vector quantization (RVQ), aligns well with the inclusion criteria of discrete audio tokens involving RVQ and semantic/acoustic disentanglement, provides method details including token generation and usage in speech language models, and includes reproducible code; it does not fall under any exclusion criteria and is relevant to discrete audio token research focused on language modeling and generation.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "QS-TTS: Towards Semi-Supervised Text-to-Speech Synthesis via Vector-Quantized Self-Supervised Speech Representation Learning",
    "abstract": "This paper proposes a novel semi-supervised TTS framework, QS-TTS, to improve TTS quality with lower supervised data requirements via Vector-Quantized Self-Supervised Speech Representation Learning (VQ-S3RL) utilizing more unlabeled speech audio. This framework comprises two VQ-S3R learners: first, the principal learner aims to provide a generative Multi-Stage Multi-Codebook (MSMC) VQ-S3R via the MSMC-VQ-GAN combined with the contrastive S3RL, while decoding it back to the high-quality audio; then, the associate learner further abstracts the MSMC representation into a highly-compact VQ representation through a VQ-VAE. These two generative VQ-S3R learners provide profitable speech representations and pre-trained models for TTS, significantly improving synthesis quality with the lower requirement for supervised data. QS-TTS is evaluated comprehensively under various scenarios via subjective and objective tests in experiments. The results powerfully demonstrate the superior performance of QS-TTS, winning the highest MOS over supervised or semi-supervised baseline TTS approaches, especially in low-resource scenarios. Moreover, comparing various speech representations and transfer learning methods in TTS further validates the notable improvement of the proposed VQ-S3RL to TTS, showing the best audio quality and intelligibility metrics. The trend of slower decay in the synthesis quality of QS-TTS with decreasing supervised data further highlights its lower requirements for supervised data, indicating its great potential in low-resource scenarios.",
    "metadata": {
      "arxiv_id": "2309.00126",
      "title": "QS-TTS: Towards Semi-Supervised Text-to-Speech Synthesis via Vector-Quantized Self-Supervised Speech Representation Learning",
      "summary": "This paper proposes a novel semi-supervised TTS framework, QS-TTS, to improve TTS quality with lower supervised data requirements via Vector-Quantized Self-Supervised Speech Representation Learning (VQ-S3RL) utilizing more unlabeled speech audio. This framework comprises two VQ-S3R learners: first, the principal learner aims to provide a generative Multi-Stage Multi-Codebook (MSMC) VQ-S3R via the MSMC-VQ-GAN combined with the contrastive S3RL, while decoding it back to the high-quality audio; then, the associate learner further abstracts the MSMC representation into a highly-compact VQ representation through a VQ-VAE. These two generative VQ-S3R learners provide profitable speech representations and pre-trained models for TTS, significantly improving synthesis quality with the lower requirement for supervised data. QS-TTS is evaluated comprehensively under various scenarios via subjective and objective tests in experiments. The results powerfully demonstrate the superior performance of QS-TTS, winning the highest MOS over supervised or semi-supervised baseline TTS approaches, especially in low-resource scenarios. Moreover, comparing various speech representations and transfer learning methods in TTS further validates the notable improvement of the proposed VQ-S3RL to TTS, showing the best audio quality and intelligibility metrics. The trend of slower decay in the synthesis quality of QS-TTS with decreasing supervised data further highlights its lower requirements for supervised data, indicating its great potential in low-resource scenarios.",
      "authors": [
        "Haohan Guo",
        "Fenglong Xie",
        "Jiawen Kang",
        "Yujia Xiao",
        "Xixin Wu",
        "Helen Meng"
      ],
      "published": "2023-08-31T20:25:44Z",
      "updated": "2023-08-31T20:25:44Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.00126v1",
      "landing_url": "https://arxiv.org/abs/2309.00126v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.00126"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on vector-quantized, discrete audio representations (VQ-S3RL, MSMC-VQ-GAN, VQ-VAE) as core components for semi-supervised TTS, aligning with the discrete audio tokens definition and describing token generation/quantization methods for downstream speech synthesis, thus meeting inclusion criteria and not triggering any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item focuses on vector-quantized, discrete audio representations (VQ-S3RL, MSMC-VQ-GAN, VQ-VAE) as core components for semi-supervised TTS, aligning with the discrete audio tokens definition and describing token generation/quantization methods for downstream speech synthesis, thus meeting inclusion criteria and not triggering any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a semi-supervised TTS framework (QS-TTS) using Vector-Quantized Self-Supervised Speech Representation Learning (VQ-S3RL), involving vector quantization techniques for speech representation, which aligns with the inclusion criteria about discrete audio tokens generated via vector quantization used for TTS synthesis. It also provides a detailed methodology including multi-stage multi-codebook VQ-GAN and VQ-VAE, supporting reproducibility and showing use of discrete tokens for speech generation. There is no mention of exclusion criteria such as lack of technical details or traditional codec usage. Therefore, it meets the inclusion criteria well and does not fall under exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a semi-supervised TTS framework (QS-TTS) using Vector-Quantized Self-Supervised Speech Representation Learning (VQ-S3RL), involving vector quantization techniques for speech representation, which aligns with the inclusion criteria about discrete audio tokens generated via vector quantization used for TTS synthesis. It also provides a detailed methodology including multi-stage multi-codebook VQ-GAN and VQ-VAE, supporting reproducibility and showing use of discrete tokens for speech generation. There is no mention of exclusion criteria such as lack of technical details or traditional codec usage. Therefore, it meets the inclusion criteria well and does not fall under exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "RepCodec: A Speech Representation Codec for Speech Tokenization",
    "abstract": "With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing.",
    "metadata": {
      "arxiv_id": "2309.00169",
      "title": "RepCodec: A Speech Representation Codec for Speech Tokenization",
      "summary": "With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing.",
      "authors": [
        "Zhichao Huang",
        "Chutong Meng",
        "Tom Ko"
      ],
      "published": "2023-08-31T23:26:10Z",
      "updated": "2024-07-22T09:53:44Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.00169v3",
      "landing_url": "https://arxiv.org/abs/2309.00169v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.00169"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper presents RepCodec, a vector-quantized codec to produce discrete speech tokens for semantic tokenization and demonstrates across encoders/languages for speech understanding/generation, aligning with the discrete audio token research theme and detailing a measurable method, not mere demos.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The paper presents RepCodec, a vector-quantized codec to produce discrete speech tokens for semantic tokenization and demonstrates across encoders/languages for speech understanding/generation, aligning with the discrete audio token research theme and detailing a measurable method, not mere demos.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents RepCodec, a speech representation codec focusing on semantic speech tokenization that converts continuous speech waveforms into discrete semantic tokens using vector quantization. It clearly addresses the discrete audio token design by detailing the token generation process involving a codec encoder and vector quantization codebook, aimed at improving speech understanding and generation through large language models. The methodology involves self-supervised speech encoders and vector quantization for semantic token formation, fulfilling the inclusion criteria. The abstract provides sufficient technical details and experimental validation. There is no indication of exclusion criteria such as lack of detailed methods or focus on traditional codecs without semantic tokens.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study presents RepCodec, a speech representation codec focusing on semantic speech tokenization that converts continuous speech waveforms into discrete semantic tokens using vector quantization. It clearly addresses the discrete audio token design by detailing the token generation process involving a codec encoder and vector quantization codebook, aimed at improving speech understanding and generation through large language models. The methodology involves self-supervised speech encoders and vector quantization for semantic token formation, fulfilling the inclusion criteria. The abstract provides sufficient technical details and experimental validation. There is no indication of exclusion criteria such as lack of detailed methods or focus on traditional codecs without semantic tokens.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Employing Real Training Data for Deep Noise Suppression",
    "abstract": "Most deep noise suppression (DNS) models are trained with reference-based losses requiring access to clean speech. However, sometimes an additive microphone model is insufficient for real-world applications. Accordingly, ways to use real training data in supervised learning for DNS models promise to reduce a potential training/inference mismatch. Employing real data for DNS training requires either generative approaches or a reference-free loss without access to the corresponding clean speech. In this work, we propose to employ an end-to-end non-intrusive deep neural network (DNN), named PESQ-DNN, to estimate perceptual evaluation of speech quality (PESQ) scores of enhanced real data. It provides a reference-free perceptual loss for employing real data during DNS training, maximizing the PESQ scores. Furthermore, we use an epoch-wise alternating training protocol, updating the DNS model on real data, followed by PESQ-DNN updating on synthetic data. The DNS model trained with the PESQ-DNN employing real data outperforms all reference methods employing only synthetic training data. On synthetic test data, our proposed method excels the Interspeech 2021 DNS Challenge baseline by a significant 0.32 PESQ points. Both on synthetic and real test data, the proposed method beats the baseline by 0.05 DNSMOS points - although PESQ-DNN optimizes for a different perceptual metric.",
    "metadata": {
      "arxiv_id": "2309.02432",
      "title": "Employing Real Training Data for Deep Noise Suppression",
      "summary": "Most deep noise suppression (DNS) models are trained with reference-based losses requiring access to clean speech. However, sometimes an additive microphone model is insufficient for real-world applications. Accordingly, ways to use real training data in supervised learning for DNS models promise to reduce a potential training/inference mismatch. Employing real data for DNS training requires either generative approaches or a reference-free loss without access to the corresponding clean speech. In this work, we propose to employ an end-to-end non-intrusive deep neural network (DNN), named PESQ-DNN, to estimate perceptual evaluation of speech quality (PESQ) scores of enhanced real data. It provides a reference-free perceptual loss for employing real data during DNS training, maximizing the PESQ scores. Furthermore, we use an epoch-wise alternating training protocol, updating the DNS model on real data, followed by PESQ-DNN updating on synthetic data. The DNS model trained with the PESQ-DNN employing real data outperforms all reference methods employing only synthetic training data. On synthetic test data, our proposed method excels the Interspeech 2021 DNS Challenge baseline by a significant 0.32 PESQ points. Both on synthetic and real test data, the proposed method beats the baseline by 0.05 DNSMOS points - although PESQ-DNN optimizes for a different perceptual metric.",
      "authors": [
        "Ziyi Xu",
        "Marvin Sach",
        "Jan Pirklbauer",
        "Tim Fingscheidt"
      ],
      "published": "2023-09-05T17:58:58Z",
      "updated": "2023-09-05T17:58:58Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.02432v1",
      "landing_url": "https://arxiv.org/abs/2309.02432v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.02432"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on real-data training for deep noise suppression using PESQ-DNN and a reference-free perceptual loss; it does not discuss discrete audio tokens, token generation/quantization, or token-based downstream modeling, so it does not meet the inclusion criteria for discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on real-data training for deep noise suppression using PESQ-DNN and a reference-free perceptual loss; it does not discuss discrete audio tokens, token generation/quantization, or token-based downstream modeling, so it does not meet the inclusion criteria for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on deep noise suppression using real training data and a non-intrusive perceptual loss but does not discuss discrete audio tokens, token generation processes, quantization methods, or their use in sequence models, which are core to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on deep noise suppression using real training data and a non-intrusive perceptual loss but does not discuss discrete audio tokens, token generation processes, quantization methods, or their use in sequence models, which are core to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Understanding Self-Supervised Learning of Speech Representation via Invariance and Redundancy Reduction",
    "abstract": "Self-supervised learning (SSL) has emerged as a promising paradigm for learning flexible speech representations from unlabeled data. By designing pretext tasks that exploit statistical regularities, SSL models can capture useful representations that are transferable to downstream tasks. This study provides an empirical analysis of Barlow Twins (BT), an SSL technique inspired by theories of redundancy reduction in human perception. On downstream tasks, BT representations accelerated learning and transferred across domains. However, limitations exist in disentangling key explanatory factors, with redundancy reduction and invariance alone insufficient for factorization of learned latents into modular, compact, and informative codes. Our ablations study isolated gains from invariance constraints, but the gains were context-dependent. Overall, this work substantiates the potential of Barlow Twins for sample-efficient speech encoding. However, challenges remain in achieving fully hierarchical representations. The analysis methodology and insights pave a path for extensions incorporating further inductive priors and perceptual principles to further enhance the BT self-supervision framework.",
    "metadata": {
      "arxiv_id": "2309.03619",
      "title": "Understanding Self-Supervised Learning of Speech Representation via Invariance and Redundancy Reduction",
      "summary": "Self-supervised learning (SSL) has emerged as a promising paradigm for learning flexible speech representations from unlabeled data. By designing pretext tasks that exploit statistical regularities, SSL models can capture useful representations that are transferable to downstream tasks. This study provides an empirical analysis of Barlow Twins (BT), an SSL technique inspired by theories of redundancy reduction in human perception. On downstream tasks, BT representations accelerated learning and transferred across domains. However, limitations exist in disentangling key explanatory factors, with redundancy reduction and invariance alone insufficient for factorization of learned latents into modular, compact, and informative codes. Our ablations study isolated gains from invariance constraints, but the gains were context-dependent. Overall, this work substantiates the potential of Barlow Twins for sample-efficient speech encoding. However, challenges remain in achieving fully hierarchical representations. The analysis methodology and insights pave a path for extensions incorporating further inductive priors and perceptual principles to further enhance the BT self-supervision framework.",
      "authors": [
        "Yusuf Brima",
        "Ulf Krumnack",
        "Simone Pika",
        "Gunther Heidemann"
      ],
      "published": "2023-09-07T10:23:59Z",
      "updated": "2024-01-24T13:37:11Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.03619v2",
      "landing_url": "https://arxiv.org/abs/2309.03619v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.03619"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "It analyzes self-supervised speech representations (Barlow Twins) but does not address discrete audio tokens, token generation/quantization, or token-based downstream modeling, so it fails the inclusion criteria centered on discrete token representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "It analyzes self-supervised speech representations (Barlow Twins) but does not address discrete audio tokens, token generation/quantization, or token-based downstream modeling, so it fails the inclusion criteria centered on discrete token representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning for speech representation via invariance and redundancy reduction, without mentioning discrete audio tokens or their generation through vector quantization or similar methods as required. There is no description of token quantization, discrete symbol generation, or usage of tokens for sequence or language modeling, which conflicts with the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning for speech representation via invariance and redundancy reduction, without mentioning discrete audio tokens or their generation through vector quantization or similar methods as required. There is no description of token quantization, discrete symbol generation, or usage of tokens for sequence or language modeling, which conflicts with the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks",
    "abstract": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",
    "metadata": {
      "arxiv_id": "2309.07937",
      "title": "Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks",
      "summary": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",
      "authors": [
        "Soumi Maiti",
        "Yifan Peng",
        "Shukjae Choi",
        "Jee-weon Jung",
        "Xuankai Chang",
        "Shinji Watanabe"
      ],
      "published": "2023-09-14T03:13:18Z",
      "updated": "2024-01-24T15:36:31Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.07937v3",
      "landing_url": "https://arxiv.org/abs/2309.07937v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.07937"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes VoxtLM using discrete speech tokens derived from self-supervised speech features to enable multitask audio-language modeling (speech recognition, synthesis, and text continuation), aligning with the discrete audio tokens research theme; it asserts token-based representations and related quantization steps, meeting the inclusion criteria, though it does not provide full methodological details in the abstract.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract describes VoxtLM using discrete speech tokens derived from self-supervised speech features to enable multitask audio-language modeling (speech recognition, synthesis, and text continuation), aligning with the discrete audio tokens research theme; it asserts token-based representations and related quantization steps, meeting the inclusion criteria, though it does not provide full methodological details in the abstract.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes VoxtLM, a decoder-only model integrating text vocabulary with discrete speech tokens derived from self-supervised speech features, which aligns with the inclusion criteria focusing on discrete audio tokens formed via such methods, and discusses token usage in multitask learning, including speech recognition and synthesis. It provides details on the model and open-sourced training recipes supporting reproducibility, satisfying the need for technical coverage and method description. Thus, it meets the inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes VoxtLM, a decoder-only model integrating text vocabulary with discrete speech tokens derived from self-supervised speech features, which aligns with the inclusion criteria focusing on discrete audio tokens formed via such methods, and discusses token usage in multitask learning, including speech recognition and synthesis. It provides details on the model and open-sourced training recipes supporting reproducibility, satisfying the need for technical coverage and method description. Thus, it meets the inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Distilling HuBERT with LSTMs via Decoupled Knowledge Distillation",
    "abstract": "Much research effort is being applied to the task of compressing the knowledge of self-supervised models, which are powerful, yet large and memory consuming. In this work, we show that the original method of knowledge distillation (and its more recently proposed extension, decoupled knowledge distillation) can be applied to the task of distilling HuBERT. In contrast to methods that focus on distilling internal features, this allows for more freedom in the network architecture of the compressed model. We thus propose to distill HuBERT's Transformer layers into an LSTM-based distilled model that reduces the number of parameters even below DistilHuBERT and at the same time shows improved performance in automatic speech recognition.",
    "metadata": {
      "arxiv_id": "2309.09920",
      "title": "Distilling HuBERT with LSTMs via Decoupled Knowledge Distillation",
      "summary": "Much research effort is being applied to the task of compressing the knowledge of self-supervised models, which are powerful, yet large and memory consuming. In this work, we show that the original method of knowledge distillation (and its more recently proposed extension, decoupled knowledge distillation) can be applied to the task of distilling HuBERT. In contrast to methods that focus on distilling internal features, this allows for more freedom in the network architecture of the compressed model. We thus propose to distill HuBERT's Transformer layers into an LSTM-based distilled model that reduces the number of parameters even below DistilHuBERT and at the same time shows improved performance in automatic speech recognition.",
      "authors": [
        "Danilo de Oliveira",
        "Timo Gerkmann"
      ],
      "published": "2023-09-18T16:34:40Z",
      "updated": "2023-09-18T16:34:40Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.09920v1",
      "landing_url": "https://arxiv.org/abs/2309.09920v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.09920"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Topic is about distilling HuBERT into an LSTM via decoupled KD and does not involve discrete audio token representations or tokenization methods, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Topic is about distilling HuBERT into an LSTM via decoupled KD and does not involve discrete audio token representations or tokenization methods, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on distillation of HuBERT into an LSTM model to reduce parameters and improve ASR, but it does not discuss the generation, design, or use of discrete audio tokens derived via quantization or clustering as required by inclusion criteria; it mainly addresses model compression rather than discrete token representation or audio language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on distillation of HuBERT into an LSTM model to reduce parameters and improve ASR, but it does not discuss the generation, design, or use of discrete audio tokens derived via quantization or clustering as required by inclusion criteria; it mainly addresses model compression rather than discrete token representation or audio language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Attentive VQ-VAE",
    "abstract": "We present a novel approach to enhance the capabilities of VQ-VAE models through the integration of a Residual Encoder and a Residual Pixel Attention layer, named Attentive Residual Encoder (AREN). The objective of our research is to improve the performance of VQ-VAE while maintaining practical parameter levels. The AREN encoder is designed to operate effectively at multiple levels, accommodating diverse architectural complexities. The key innovation is the integration of an inter-pixel auto-attention mechanism into the AREN encoder. This approach allows us to efficiently capture and utilize contextual information across latent vectors. Additionally, our models uses additional encoding levels to further enhance the model's representational power. Our attention layer employs a minimal parameter approach, ensuring that latent vectors are modified only when pertinent information from other pixels is available. Experimental results demonstrate that our proposed modifications lead to significant improvements in data representation and generation, making VQ-VAEs even more suitable for a wide range of applications as the presented.",
    "metadata": {
      "arxiv_id": "2309.11641",
      "title": "Attentive VQ-VAE",
      "summary": "We present a novel approach to enhance the capabilities of VQ-VAE models through the integration of a Residual Encoder and a Residual Pixel Attention layer, named Attentive Residual Encoder (AREN). The objective of our research is to improve the performance of VQ-VAE while maintaining practical parameter levels. The AREN encoder is designed to operate effectively at multiple levels, accommodating diverse architectural complexities. The key innovation is the integration of an inter-pixel auto-attention mechanism into the AREN encoder. This approach allows us to efficiently capture and utilize contextual information across latent vectors. Additionally, our models uses additional encoding levels to further enhance the model's representational power. Our attention layer employs a minimal parameter approach, ensuring that latent vectors are modified only when pertinent information from other pixels is available. Experimental results demonstrate that our proposed modifications lead to significant improvements in data representation and generation, making VQ-VAEs even more suitable for a wide range of applications as the presented.",
      "authors": [
        "Angello Hoyos",
        "Mariano Rivera"
      ],
      "published": "2023-09-20T21:11:36Z",
      "updated": "2024-02-08T20:52:25Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.11641v2",
      "landing_url": "https://arxiv.org/abs/2309.11641v2",
      "doi": null
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract presents a VQ-VAE architecture with discrete latent representations but does not explicitly focus on discrete audio tokens or provide tokenization/quantization details for audio, and the source lacks a clear publication/update date, violating the timestamp criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract presents a VQ-VAE architecture with discrete latent representations but does not explicitly focus on discrete audio tokens or provide tokenization/quantization details for audio, and the source lacks a clear publication/update date, violating the timestamp criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses improvements to VQ-VAE models including a Residual Encoder and attention mechanisms to enhance data representation and generation. However, the focus is on model architecture and attention layers, with no explicit mention of discrete audio tokens generation, token formation process, or usage of tokens for sequence or language modeling in audio tasks. The abstract lacks clear details on tokenization of continuous audio signals into discrete tokens and their application for audio language modeling or related tasks, which are essential for meeting the inclusion criteria. Therefore, it does not meet the topic requirements for discrete audio token design and usage.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses improvements to VQ-VAE models including a Residual Encoder and attention mechanisms to enhance data representation and generation. However, the focus is on model architecture and attention layers, with no explicit mention of discrete audio tokens generation, token formation process, or usage of tokens for sequence or language modeling in audio tasks. The abstract lacks clear details on tokenization of continuous audio signals into discrete tokens and their application for audio language modeling or related tasks, which are essential for meeting the inclusion criteria. Therefore, it does not meet the topic requirements for discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts",
    "abstract": "Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.",
    "metadata": {
      "arxiv_id": "2309.11977",
      "title": "Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts",
      "summary": "Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.",
      "authors": [
        "Shun Lei",
        "Yixuan Zhou",
        "Liyang Chen",
        "Dan Luo",
        "Zhiyong Wu",
        "Xixin Wu",
        "Shiyin Kang",
        "Tao Jiang",
        "Yahui Zhou",
        "Yuxing Han",
        "Helen Meng"
      ],
      "published": "2023-09-21T11:22:22Z",
      "updated": "2024-04-09T08:39:52Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.11977v3",
      "landing_url": "https://arxiv.org/abs/2309.11977v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.11977"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly quantizes speech into discrete acoustic tokens for language-model–based TTS (VALL-E), details token generation/quantization and model architecture with multi-scale prompts, and provides experiments demonstrating naturalness and speaker similarity, meeting the discrete-token and cross-modal LM-generation criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly quantizes speech into discrete acoustic tokens for language-model–based TTS (VALL-E), details token generation/quantization and model architecture with multi-scale prompts, and provides experiments demonstrating naturalness and speaker similarity, meeting the discrete-token and cross-modal LM-generation criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on zero-shot text-to-speech synthesis using language models to model discrete acoustic tokens obtained via waveform quantization, fitting the inclusion criteria of using learned discrete audio tokens for sequence modeling in TTS. It clearly describes the use of a neural codec language model (VALL-E) and quantized acoustic tokens as inputs for speaker adaptation and speech generation, addressing token generation and its application. The abstract provides sufficient method details and the approach targets improved generation rather than just compression, aligning with the core research theme without apparent exclusion criteria violations.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on zero-shot text-to-speech synthesis using language models to model discrete acoustic tokens obtained via waveform quantization, fitting the inclusion criteria of using learned discrete audio tokens for sequence modeling in TTS. It clearly describes the use of a neural codec language model (VALL-E) and quantized acoustic tokens as inputs for speaker adaptation and speech generation, addressing token generation and its application. The abstract provides sufficient method details and the approach targets improved generation rather than just compression, aligning with the core research theme without apparent exclusion criteria violations.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning",
    "abstract": "Recent years have witnessed significant advancements in self-supervised learning (SSL) methods for speech-processing tasks. Various speech-based SSL models have been developed and present promising performance on a range of downstream tasks including speech recognition. However, existing speech-based SSL models face a common dilemma in terms of computational cost, which might hinder their potential application and in-depth academic research. To address this issue, we first analyze the computational cost of different modules during HuBERT pre-training and then introduce a stack of efficiency optimizations, which is named Fast-HuBERT in this paper. The proposed Fast-HuBERT can be trained in 1.1 days with 8 V100 GPUs on the Librispeech 960h benchmark, without performance degradation, resulting in a 5.2x speedup, compared to the original implementation. Moreover, we explore two well-studied techniques in the Fast-HuBERT and demonstrate consistent improvements as reported in previous work.",
    "metadata": {
      "arxiv_id": "2309.13860",
      "title": "Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning",
      "summary": "Recent years have witnessed significant advancements in self-supervised learning (SSL) methods for speech-processing tasks. Various speech-based SSL models have been developed and present promising performance on a range of downstream tasks including speech recognition. However, existing speech-based SSL models face a common dilemma in terms of computational cost, which might hinder their potential application and in-depth academic research. To address this issue, we first analyze the computational cost of different modules during HuBERT pre-training and then introduce a stack of efficiency optimizations, which is named Fast-HuBERT in this paper. The proposed Fast-HuBERT can be trained in 1.1 days with 8 V100 GPUs on the Librispeech 960h benchmark, without performance degradation, resulting in a 5.2x speedup, compared to the original implementation. Moreover, we explore two well-studied techniques in the Fast-HuBERT and demonstrate consistent improvements as reported in previous work.",
      "authors": [
        "Guanrou Yang",
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Yakun Song",
        "Zhikang Niu",
        "Xie Chen"
      ],
      "published": "2023-09-25T04:07:34Z",
      "updated": "2023-09-29T06:48:11Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.13860v2",
      "landing_url": "https://arxiv.org/abs/2309.13860v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.13860"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on efficient self-supervised speech representation learning (Fast-HuBERT) and speedups for HuBERT pre-training, with no discussion of discrete audio tokens, token quantization, or token-based downstream modeling per the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on efficient self-supervised speech representation learning (Fast-HuBERT) and speedups for HuBERT pre-training, with no discussion of discrete audio tokens, token quantization, or token-based downstream modeling per the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on an efficient training framework for self-supervised speech representation learning but does not describe producing discrete audio tokens via vector quantization or similar discrete token mechanisms as defined in the inclusion criteria. It lacks clear mention of discrete token generation, quantization methods, or using tokens in sequence/language modeling, which are core to the topic. Therefore, it does not meet the inclusion criteria and falls under exclusion for lacking discrete token methodology.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on an efficient training framework for self-supervised speech representation learning but does not describe producing discrete audio tokens via vector quantization or similar discrete token mechanisms as defined in the inclusion criteria. It lacks clear mention of discrete token generation, quantization methods, or using tokens in sequence/language modeling, which are core to the topic. Therefore, it does not meet the inclusion criteria and falls under exclusion for lacking discrete token methodology.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speaker anonymization using neural audio codec language models",
    "abstract": "The vast majority of approaches to speaker anonymization involve the extraction of fundamental frequency estimates, linguistic features and a speaker embedding which is perturbed to obfuscate the speaker identity before an anonymized speech waveform is resynthesized using a vocoder. Recent work has shown that x-vector transformations are difficult to control consistently: other sources of speaker information contained within fundamental frequency and linguistic features are re-entangled upon vocoding, meaning that anonymized speech signals still contain speaker information. We propose an approach based upon neural audio codecs (NACs), which are known to generate high-quality synthetic speech when combined with language models. NACs use quantized codes, which are known to effectively bottleneck speaker-related information: we demonstrate the potential of speaker anonymization systems based on NAC language modeling by applying the evaluation framework of the Voice Privacy Challenge 2022.",
    "metadata": {
      "arxiv_id": "2309.14129",
      "title": "Speaker anonymization using neural audio codec language models",
      "summary": "The vast majority of approaches to speaker anonymization involve the extraction of fundamental frequency estimates, linguistic features and a speaker embedding which is perturbed to obfuscate the speaker identity before an anonymized speech waveform is resynthesized using a vocoder. Recent work has shown that x-vector transformations are difficult to control consistently: other sources of speaker information contained within fundamental frequency and linguistic features are re-entangled upon vocoding, meaning that anonymized speech signals still contain speaker information. We propose an approach based upon neural audio codecs (NACs), which are known to generate high-quality synthetic speech when combined with language models. NACs use quantized codes, which are known to effectively bottleneck speaker-related information: we demonstrate the potential of speaker anonymization systems based on NAC language modeling by applying the evaluation framework of the Voice Privacy Challenge 2022.",
      "authors": [
        "Michele Panariello",
        "Francesco Nespoli",
        "Massimiliano Todisco",
        "Nicholas Evans"
      ],
      "published": "2023-09-25T13:32:09Z",
      "updated": "2024-01-12T15:05:34Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.14129v3",
      "landing_url": "https://arxiv.org/abs/2309.14129v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.14129"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Abstract discusses speaker anonymization via neural audio codecs using quantized codes; however it does not clearly present a discrete audio token design, token generation process, quantization specifics, or reproducible methodology in the provided text, and lacks full methodological details, so it does not satisfy the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Abstract discusses speaker anonymization via neural audio codecs using quantized codes; however it does not clearly present a discrete audio token design, token generation process, quantization specifics, or reproducible methodology in the provided text, and lacks full methodological details, so it does not satisfy the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes an approach based on neural audio codecs (NACs) that use quantized codes to bottleneck speaker-related information for speaker anonymization, which implies the generation of discrete tokens from audio for subsequent processing. It evaluates the system with a formal framework, indicating a clear methodological approach. However, the abstract does not explicitly detail the token generation process, quantization methods, or how these tokens are used in language modeling beyond referencing NACs and language models generally. Although it aligns with the use of neural audio codecs producing discrete codes, the lack of clear explanation about token design and usage as required by the inclusion criteria makes it difficult to fully confirm eligibility.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study proposes an approach based on neural audio codecs (NACs) that use quantized codes to bottleneck speaker-related information for speaker anonymization, which implies the generation of discrete tokens from audio for subsequent processing. It evaluates the system with a formal framework, indicating a clear methodological approach. However, the abstract does not explicitly detail the token generation process, quantization methods, or how these tokens are used in language modeling beyond referencing NACs and language models generally. Although it aligns with the use of neural audio codecs producing discrete codes, the lack of clear explanation about token design and usage as required by the inclusion criteria makes it difficult to fully confirm eligibility.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: JuniorNano recommended exclusion (score 1) citing lack of methodological detail; JuniorMini was uncertain (score 3) noting the paper implies quantized codes but does not explicitly describe token generation/quantization/configuration. My assessment: although the abstract mentions neural audio codecs and quantized codes (which suggests discrete audio tokens), it does not provide the required methodological details (token generation process, quantization specifics, codebook/layer design, or reproducible training/inference settings) needed to satisfy the inclusion criteria, and based on the exclusion rules this lack of inspectable technical detail warrants exclusion. Score explanation (1 sentence): I give score 2 (better to exclude) because the abstract hints at discrete quantized codes but lacks the explicit, reviewable token-design and reproducibility details required for inclusion, leaving eligibility uncertain without the full technical content.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: JuniorNano recommended exclusion (score 1) citing lack of methodological detail; JuniorMini was uncertain (score 3) noting the paper implies quantized codes but does not explicitly describe token generation/quantization/configuration. My assessment: although the abstract mentions neural audio codecs and quantized codes (which suggests discrete audio tokens), it does not provide the required methodological details (token generation process, quantization specifics, codebook/layer design, or reproducible training/inference settings) needed to satisfy the inclusion criteria, and based on the exclusion rules this lack of inspectable technical detail warrants exclusion. Score explanation (1 sentence): I give score 2 (better to exclude) because the abstract hints at discrete quantized codes but lacks the explicit, reviewable token-design and reproducibility details required for inclusion, leaving eligibility uncertain without the full technical content.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Finite Scalar Quantization: VQ-VAE Made Simple",
    "abstract": "We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets. By appropriately choosing the number of dimensions and values each dimension can take, we obtain the same codebook size as in VQ. On top of such discrete representations, we can train the same models that have been trained on VQ-VAE representations. For example, autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Concretely, we employ FSQ with MaskGIT for image generation, and with UViM for depth estimation, colorization, and panoptic segmentation. Despite the much simpler design of FSQ, we obtain competitive performance in all these tasks. We emphasize that FSQ does not suffer from codebook collapse and does not need the complex machinery employed in VQ (commitment losses, codebook reseeding, code splitting, entropy penalties, etc.) to learn expressive discrete representations.",
    "metadata": {
      "arxiv_id": "2309.15505",
      "title": "Finite Scalar Quantization: VQ-VAE Made Simple",
      "summary": "We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets. By appropriately choosing the number of dimensions and values each dimension can take, we obtain the same codebook size as in VQ. On top of such discrete representations, we can train the same models that have been trained on VQ-VAE representations. For example, autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Concretely, we employ FSQ with MaskGIT for image generation, and with UViM for depth estimation, colorization, and panoptic segmentation. Despite the much simpler design of FSQ, we obtain competitive performance in all these tasks. We emphasize that FSQ does not suffer from codebook collapse and does not need the complex machinery employed in VQ (commitment losses, codebook reseeding, code splitting, entropy penalties, etc.) to learn expressive discrete representations.",
      "authors": [
        "Fabian Mentzer",
        "David Minnen",
        "Eirikur Agustsson",
        "Michael Tschannen"
      ],
      "published": "2023-09-27T09:13:40Z",
      "updated": "2023-10-12T07:55:05Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.15505v2",
      "landing_url": "https://arxiv.org/abs/2309.15505v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.15505"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work discusses finite scalar quantization for VQ-VAE in vision tasks (image generation, depth/colorization, segmentation) and does not address discrete audio tokens or audio-language modeling, hence does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work discusses finite scalar quantization for VQ-VAE in vision tasks (image generation, depth/colorization, segmentation) and does not address discrete audio tokens or audio-language modeling, hence does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article discusses a technique called finite scalar quantization (FSQ) as an alternative to vector quantization (VQ) in VQ-VAEs, primarily applied to image generation and vision tasks, without addressing audio signals or discrete audio tokens for speech, music, or general audio processing. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens and is unrelated to audio representation or modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article discusses a technique called finite scalar quantization (FSQ) as an alternative to vector quantization (VQ) in VQ-VAEs, primarily applied to image generation and vision tasks, without addressing audio signals or discrete audio tokens for speech, music, or general audio processing. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens and is unrelated to audio representation or modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Federated K-means Clustering",
    "abstract": "Federated learning is a technique that enables the use of distributed datasets for machine learning purposes without requiring data to be pooled, thereby better preserving privacy and ownership of the data. While supervised FL research has grown substantially over the last years, unsupervised FL methods remain scarce. This work introduces an algorithm which implements K-means clustering in a federated manner, addressing the challenges of varying number of clusters between centers, as well as convergence on less separable datasets.",
    "metadata": {
      "arxiv_id": "2310.01195",
      "title": "Federated K-means Clustering",
      "summary": "Federated learning is a technique that enables the use of distributed datasets for machine learning purposes without requiring data to be pooled, thereby better preserving privacy and ownership of the data. While supervised FL research has grown substantially over the last years, unsupervised FL methods remain scarce. This work introduces an algorithm which implements K-means clustering in a federated manner, addressing the challenges of varying number of clusters between centers, as well as convergence on less separable datasets.",
      "authors": [
        "Swier Garst",
        "Marcel Reinders"
      ],
      "published": "2023-10-02T13:32:00Z",
      "updated": "2024-02-16T14:02:02Z",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.01195v2",
      "landing_url": "https://arxiv.org/abs/2310.01195v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.01195"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on Federated K-means clustering for unsupervised federated learning and does not address discrete audio tokens or any audio token design, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on Federated K-means clustering for unsupervised federated learning and does not address discrete audio tokens or any audio token design, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on federated K-means clustering for federated learning, which is a machine learning technique related to distributed datasets and privacy preservation, but it lacks any indication of working with discrete audio tokens, vector quantization, or audio-related token generation processes. It neither mentions audio data nor discrete token design or usage for language or sequence modeling, thus failing to meet the inclusion criteria focused specifically on discrete audio token representation and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on federated K-means clustering for federated learning, which is a machine learning technique related to distributed datasets and privacy preservation, but it lacks any indication of working with discrete audio tokens, vector quantization, or audio-related token generation processes. It neither mentions audio data nor discrete token design or usage for language or sequence modeling, thus failing to meet the inclusion criteria focused specifically on discrete audio token representation and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multi-resolution HuBERT: Multi-resolution Speech Self-Supervised Learning with Masked Unit Prediction",
    "abstract": "Existing Self-Supervised Learning (SSL) models for speech typically process speech signals at a fixed resolution of 20 milliseconds. This approach overlooks the varying informational content present at different resolutions in speech signals. In contrast, this paper aims to incorporate multi-resolution information into speech self-supervised representation learning. We introduce a SSL model that leverages a hierarchical Transformer architecture, complemented by HuBERT-style masked prediction objectives, to process speech at multiple resolutions. Experimental results indicate that the proposed model not only achieves more efficient inference but also exhibits superior or comparable performance to the original HuBERT model over various tasks. Specifically, significant performance improvements over the original HuBERT have been observed in fine-tuning experiments on the LibriSpeech speech recognition benchmark as well as in evaluations using the Speech Universal PERformance Benchmark (SUPERB) and Multilingual SUPERB (ML-SUPERB).",
    "metadata": {
      "arxiv_id": "2310.02720",
      "title": "Multi-resolution HuBERT: Multi-resolution Speech Self-Supervised Learning with Masked Unit Prediction",
      "summary": "Existing Self-Supervised Learning (SSL) models for speech typically process speech signals at a fixed resolution of 20 milliseconds. This approach overlooks the varying informational content present at different resolutions in speech signals. In contrast, this paper aims to incorporate multi-resolution information into speech self-supervised representation learning. We introduce a SSL model that leverages a hierarchical Transformer architecture, complemented by HuBERT-style masked prediction objectives, to process speech at multiple resolutions. Experimental results indicate that the proposed model not only achieves more efficient inference but also exhibits superior or comparable performance to the original HuBERT model over various tasks. Specifically, significant performance improvements over the original HuBERT have been observed in fine-tuning experiments on the LibriSpeech speech recognition benchmark as well as in evaluations using the Speech Universal PERformance Benchmark (SUPERB) and Multilingual SUPERB (ML-SUPERB).",
      "authors": [
        "Jiatong Shi",
        "Hirofumi Inaguma",
        "Xutai Ma",
        "Ilia Kulikov",
        "Anna Sun"
      ],
      "published": "2023-10-04T10:52:13Z",
      "updated": "2024-01-30T08:52:12Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.02720v2",
      "landing_url": "https://arxiv.org/abs/2310.02720v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.02720"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on multi-resolution self-supervised speech learning with masked unit prediction but does not present discrete audio token design, vector quantization, or token-based modeling as its main contribution, hence not meeting the discrete token criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on multi-resolution self-supervised speech learning with masked unit prediction but does not present discrete audio token design, vector quantization, or token-based modeling as its main contribution, hence not meeting the discrete token criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a multi-resolution self-supervised learning model for speech, but it focuses on continuous speech representation learning with masked unit prediction rather than explicitly on discrete audio tokens generated via vector quantization, residual quantization, or hierarchical quantization as discrete symbols for sequence/language modeling. The abstract does not describe the process of generating discrete tokens or leveraging them as core components for sequence modeling or generation, nor does it specify detailed token quantization methods or their semantic disentanglement. Thus, although relevant to speech representation learning, the paper does not clearly meet the core inclusion criteria related to discrete audio token design and usage.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a multi-resolution self-supervised learning model for speech, but it focuses on continuous speech representation learning with masked unit prediction rather than explicitly on discrete audio tokens generated via vector quantization, residual quantization, or hierarchical quantization as discrete symbols for sequence/language modeling. The abstract does not describe the process of generating discrete tokens or leveraging them as core components for sequence modeling or generation, nor does it specify detailed token quantization methods or their semantic disentanglement. Thus, although relevant to speech representation learning, the paper does not clearly meet the core inclusion criteria related to discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "HuBERTopic: Enhancing Semantic Representation of HuBERT through Self-supervision Utilizing Topic Model",
    "abstract": "Recently, the usefulness of self-supervised representation learning (SSRL) methods has been confirmed in various downstream tasks. Many of these models, as exemplified by HuBERT and WavLM, use pseudo-labels generated from spectral features or the model's own representation features. From previous studies, it is known that the pseudo-labels contain semantic information. However, the masked prediction task, the learning criterion of HuBERT, focuses on local contextual information and may not make effective use of global semantic information such as speaker, theme of speech, and so on. In this paper, we propose a new approach to enrich the semantic representation of HuBERT. We apply topic model to pseudo-labels to generate a topic label for each utterance. An auxiliary topic classification task is added to HuBERT by using topic labels as teachers. This allows additional global semantic information to be incorporated in an unsupervised manner. Experimental results demonstrate that our method achieves comparable or better performance than the baseline in most tasks, including automatic speech recognition and five out of the eight SUPERB tasks. Moreover, we find that topic labels include various information about utterance, such as gender, speaker, and its theme. This highlights the effectiveness of our approach in capturing multifaceted semantic nuances.",
    "metadata": {
      "arxiv_id": "2310.03975",
      "title": "HuBERTopic: Enhancing Semantic Representation of HuBERT through Self-supervision Utilizing Topic Model",
      "summary": "Recently, the usefulness of self-supervised representation learning (SSRL) methods has been confirmed in various downstream tasks. Many of these models, as exemplified by HuBERT and WavLM, use pseudo-labels generated from spectral features or the model's own representation features. From previous studies, it is known that the pseudo-labels contain semantic information. However, the masked prediction task, the learning criterion of HuBERT, focuses on local contextual information and may not make effective use of global semantic information such as speaker, theme of speech, and so on. In this paper, we propose a new approach to enrich the semantic representation of HuBERT. We apply topic model to pseudo-labels to generate a topic label for each utterance. An auxiliary topic classification task is added to HuBERT by using topic labels as teachers. This allows additional global semantic information to be incorporated in an unsupervised manner. Experimental results demonstrate that our method achieves comparable or better performance than the baseline in most tasks, including automatic speech recognition and five out of the eight SUPERB tasks. Moreover, we find that topic labels include various information about utterance, such as gender, speaker, and its theme. This highlights the effectiveness of our approach in capturing multifaceted semantic nuances.",
      "authors": [
        "Takashi Maekaku",
        "Jiatong Shi",
        "Xuankai Chang",
        "Yuya Fujita",
        "Shinji Watanabe"
      ],
      "published": "2023-10-06T02:19:09Z",
      "updated": "2023-10-06T02:19:09Z",
      "categories": [
        "cs.SD",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.03975v1",
      "landing_url": "https://arxiv.org/abs/2310.03975v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.03975"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on enriching HuBERT with topic-model derived semantic information, not on creating or using discrete audio tokens (e.g., VQ/RVQ, hierarchical quantization) as token representations for audio-language modeling. It lacks a tokenization/quantization methodology and does not present discrete-token-based contributions, so it does not meet the Discrete Audio Tokens inclusion criteria (nor does it provide the requisite methodological detail for token generation). Therefore, it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on enriching HuBERT with topic-model derived semantic information, not on creating or using discrete audio tokens (e.g., VQ/RVQ, hierarchical quantization) as token representations for audio-language modeling. It lacks a tokenization/quantization methodology and does not present discrete-token-based contributions, so it does not meet the Discrete Audio Tokens inclusion criteria (nor does it provide the requisite methodological detail for token generation). Therefore, it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes enhancing HuBERT's semantic representation using a topic model applied to pseudo-labels for self-supervised learning; however, it does not clearly address the generation of discrete audio tokens via vector quantization or similar mechanisms, nor does it focus on token design or usage for sequence/language modeling as defined by the inclusion criteria. Therefore, it does not meet the core requirement of discrete audio token design and usage as a primary research contribution.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes enhancing HuBERT's semantic representation using a topic model applied to pseudo-labels for self-supervised learning; however, it does not clearly address the generation of discrete audio tokens via vector quantization or similar mechanisms, nor does it focus on token design or usage for sequence/language modeling as defined by the inclusion criteria. Therefore, it does not meet the core requirement of discrete audio token design and usage as a primary research contribution.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "EdVAE: Mitigating Codebook Collapse with Evidential Discrete Variational Autoencoders",
    "abstract": "Codebook collapse is a common problem in training deep generative models with discrete representation spaces like Vector Quantized Variational Autoencoders (VQ-VAEs). We observe that the same problem arises for the alternatively designed discrete variational autoencoders (dVAEs) whose encoder directly learns a distribution over the codebook embeddings to represent the data. We hypothesize that using the softmax function to obtain a probability distribution causes the codebook collapse by assigning overconfident probabilities to the best matching codebook elements. In this paper, we propose a novel way to incorporate evidential deep learning (EDL) instead of softmax to combat the codebook collapse problem of dVAE. We evidentially monitor the significance of attaining the probability distribution over the codebook embeddings, in contrast to softmax usage. Our experiments using various datasets show that our model, called EdVAE, mitigates codebook collapse while improving the reconstruction performance, and enhances the codebook usage compared to dVAE and VQ-VAE based models. Our code can be found at https://github.com/ituvisionlab/EdVAE .",
    "metadata": {
      "arxiv_id": "2310.05718",
      "title": "EdVAE: Mitigating Codebook Collapse with Evidential Discrete Variational Autoencoders",
      "summary": "Codebook collapse is a common problem in training deep generative models with discrete representation spaces like Vector Quantized Variational Autoencoders (VQ-VAEs). We observe that the same problem arises for the alternatively designed discrete variational autoencoders (dVAEs) whose encoder directly learns a distribution over the codebook embeddings to represent the data. We hypothesize that using the softmax function to obtain a probability distribution causes the codebook collapse by assigning overconfident probabilities to the best matching codebook elements. In this paper, we propose a novel way to incorporate evidential deep learning (EDL) instead of softmax to combat the codebook collapse problem of dVAE. We evidentially monitor the significance of attaining the probability distribution over the codebook embeddings, in contrast to softmax usage. Our experiments using various datasets show that our model, called EdVAE, mitigates codebook collapse while improving the reconstruction performance, and enhances the codebook usage compared to dVAE and VQ-VAE based models. Our code can be found at https://github.com/ituvisionlab/EdVAE .",
      "authors": [
        "Gulcin Baykal",
        "Melih Kandemir",
        "Gozde Unal"
      ],
      "published": "2023-10-09T13:39:26Z",
      "updated": "2024-07-15T09:57:48Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.05718v3",
      "landing_url": "https://arxiv.org/abs/2310.05718v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.05718"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses mitigating codebook collapse in discrete latent spaces for VQ-/dVAE in a general, non-audio domain, and does not present discrete audio token generation/quantization design, nor using the tokenized audio representation for audio-language modeling as required by the inclusion criteria; therefore it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses mitigating codebook collapse in discrete latent spaces for VQ-/dVAE in a general, non-audio domain, and does not present discrete audio token generation/quantization design, nor using the tokenized audio representation for audio-language modeling as required by the inclusion criteria; therefore it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on mitigating codebook collapse in discrete variational autoencoders, which utilize vector quantization techniques. However, the paper emphasizes model architecture and training objectives related to deep generative models but does not explicitly address discrete audio tokens for speech, music, or general audio, nor does it clearly describe token generation aimed at audio language modeling or downstream audio understanding/generation tasks. Therefore, it lacks a clear connection to the inclusion criteria focused on discrete audio token design and usage in audio-related sequence modeling or language modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on mitigating codebook collapse in discrete variational autoencoders, which utilize vector quantization techniques. However, the paper emphasizes model architecture and training objectives related to deep generative models but does not explicitly address discrete audio tokens for speech, music, or general audio, nor does it clearly describe token generation aimed at audio language modeling or downstream audio understanding/generation tasks. Therefore, it lacks a clear connection to the inclusion criteria focused on discrete audio token design and usage in audio-related sequence modeling or language modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Voice Conversion for Stuttered Speech, Instruments, Unseen Languages and Textually Described Voices",
    "abstract": "Voice conversion aims to convert source speech into a target voice using recordings of the target speaker as a reference. Newer models are producing increasingly realistic output. But what happens when models are fed with non-standard data, such as speech from a user with a speech impairment? We investigate how a recent voice conversion model performs on non-standard downstream voice conversion tasks. We use a simple but robust approach called k-nearest neighbors voice conversion (kNN-VC). We look at four non-standard applications: stuttered voice conversion, cross-lingual voice conversion, musical instrument conversion, and text-to-voice conversion. The latter involves converting to a target voice specified through a text description, e.g. \"a young man with a high-pitched voice\". Compared to an established baseline, we find that kNN-VC retains high performance in stuttered and cross-lingual voice conversion. Results are more mixed for the musical instrument and text-to-voice conversion tasks. E.g., kNN-VC works well on some instruments like drums but not on others. Nevertheless, this shows that voice conversion models - and kNN-VC in particular - are increasingly applicable in a range of non-standard downstream tasks. But there are still limitations when samples are very far from the training distribution. Code, samples, trained models: https://rf5.github.io/sacair2023-knnvc-demo/.",
    "metadata": {
      "arxiv_id": "2310.08104",
      "title": "Voice Conversion for Stuttered Speech, Instruments, Unseen Languages and Textually Described Voices",
      "summary": "Voice conversion aims to convert source speech into a target voice using recordings of the target speaker as a reference. Newer models are producing increasingly realistic output. But what happens when models are fed with non-standard data, such as speech from a user with a speech impairment? We investigate how a recent voice conversion model performs on non-standard downstream voice conversion tasks. We use a simple but robust approach called k-nearest neighbors voice conversion (kNN-VC). We look at four non-standard applications: stuttered voice conversion, cross-lingual voice conversion, musical instrument conversion, and text-to-voice conversion. The latter involves converting to a target voice specified through a text description, e.g. \"a young man with a high-pitched voice\". Compared to an established baseline, we find that kNN-VC retains high performance in stuttered and cross-lingual voice conversion. Results are more mixed for the musical instrument and text-to-voice conversion tasks. E.g., kNN-VC works well on some instruments like drums but not on others. Nevertheless, this shows that voice conversion models - and kNN-VC in particular - are increasingly applicable in a range of non-standard downstream tasks. But there are still limitations when samples are very far from the training distribution. Code, samples, trained models: https://rf5.github.io/sacair2023-knnvc-demo/.",
      "authors": [
        "Matthew Baas",
        "Herman Kamper"
      ],
      "published": "2023-10-12T08:00:25Z",
      "updated": "2023-10-12T08:00:25Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.08104v1",
      "landing_url": "https://arxiv.org/abs/2310.08104v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.08104"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses a kNN-VC method for non-standard voice conversion tasks, but the abstract does not mention discrete audio tokens, tokenization/quantization mechanisms, or any discrete token design as a core contribution. It thus does not meet the Inclusion criteria focused on Discrete Audio Tokens, and there are no explicit exclusion-criterion signs (e.g., just a demo) that would override this misfit.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses a kNN-VC method for non-standard voice conversion tasks, but the abstract does not mention discrete audio tokens, tokenization/quantization mechanisms, or any discrete token design as a core contribution. It thus does not meet the Inclusion criteria focused on Discrete Audio Tokens, and there are no explicit exclusion-criterion signs (e.g., just a demo) that would override this misfit.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on voice conversion for various non-standard speech types but does not explicitly discuss the design, generation, or usage of discrete audio tokens via vector quantization or related discrete token techniques as defined in the inclusion criteria. It lacks detailed methodological descriptions on token formation, quantization codebooks, or sequence modeling that are crucial for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on voice conversion for various non-standard speech types but does not explicitly discuss the design, generation, or usage of discrete audio tokens via vector quantization or related discrete token techniques as defined in the inclusion criteria. It lacks detailed methodological descriptions on token formation, quantization codebooks, or sequence modeling that are crucial for inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Fast Word Error Rate Estimation Using Self-Supervised Representations for Speech and Text",
    "abstract": "Word error rate (WER) estimation aims to evaluate the quality of an automatic speech recognition (ASR) system's output without requiring ground-truth labels. This task has gained increasing attention as advanced ASR systems are trained on large amounts of data. In this context, the computational efficiency of a WER estimator becomes essential in practice. However, previous works have not prioritised this aspect. In this paper, a Fast estimator for WER (Fe-WER) is introduced, utilizing average pooling over self-supervised learning representations for speech and text. Our results demonstrate that Fe-WER outperformed a baseline relatively by 14.10% in root mean square error and 1.22% in Pearson correlation coefficient on Ted-Lium3. Moreover, a comparative analysis of the distributions of target WER and WER estimates was conducted, including an examination of the average values per speaker. Lastly, the inference speed was approximately 3.4 times faster in the real-time factor.",
    "metadata": {
      "arxiv_id": "2310.08225",
      "title": "Fast Word Error Rate Estimation Using Self-Supervised Representations for Speech and Text",
      "summary": "Word error rate (WER) estimation aims to evaluate the quality of an automatic speech recognition (ASR) system's output without requiring ground-truth labels. This task has gained increasing attention as advanced ASR systems are trained on large amounts of data. In this context, the computational efficiency of a WER estimator becomes essential in practice. However, previous works have not prioritised this aspect. In this paper, a Fast estimator for WER (Fe-WER) is introduced, utilizing average pooling over self-supervised learning representations for speech and text. Our results demonstrate that Fe-WER outperformed a baseline relatively by 14.10% in root mean square error and 1.22% in Pearson correlation coefficient on Ted-Lium3. Moreover, a comparative analysis of the distributions of target WER and WER estimates was conducted, including an examination of the average values per speaker. Lastly, the inference speed was approximately 3.4 times faster in the real-time factor.",
      "authors": [
        "Chanho Park",
        "Chengsong Lu",
        "Mingjie Chen",
        "Thomas Hain"
      ],
      "published": "2023-10-12T11:17:40Z",
      "updated": "2025-01-29T11:28:34Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.08225v2",
      "landing_url": "https://arxiv.org/abs/2310.08225v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.08225"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses fast Word Error Rate estimation using self-supervised representations, not discrete audio tokens or tokenization methods; it lacks the token-formation/quantization details and the downstream token-based modeling focus required by the inclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses fast Word Error Rate estimation using self-supervised representations, not discrete audio tokens or tokenization methods; it lacks the token-formation/quantization details and the downstream token-based modeling focus required by the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a fast WER estimator using self-supervised representations for speech and text, but it does not present or focus on the generation or design of discrete audio tokens, nor does it describe a tokenization or quantization process for converting continuous audio into discrete tokens for use in language or sequence models; thus, it does not meet the inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a fast WER estimator using self-supervised representations for speech and text, but it does not present or focus on the generation or design of discrete audio tokens, nor does it describe a tokenization or quantization process for converting continuous audio into discrete tokens for use in language or sequence models; thus, it does not meet the inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient Representations",
    "abstract": "In this paper we introduce learnable lattice vector quantization and demonstrate its effectiveness for learning discrete representations. Our method, termed LL-VQ-VAE, replaces the vector quantization layer in VQ-VAE with lattice-based discretization. The learnable lattice imposes a structure over all discrete embeddings, acting as a deterrent against codebook collapse, leading to high codebook utilization. Compared to VQ-VAE, our method obtains lower reconstruction errors under the same training conditions, trains in a fraction of the time, and with a constant number of parameters (equal to the embedding dimension $D$), making it a very scalable approach. We demonstrate these results on the FFHQ-1024 dataset and include FashionMNIST and Celeb-A.",
    "metadata": {
      "arxiv_id": "2310.09382",
      "title": "LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient Representations",
      "summary": "In this paper we introduce learnable lattice vector quantization and demonstrate its effectiveness for learning discrete representations. Our method, termed LL-VQ-VAE, replaces the vector quantization layer in VQ-VAE with lattice-based discretization. The learnable lattice imposes a structure over all discrete embeddings, acting as a deterrent against codebook collapse, leading to high codebook utilization. Compared to VQ-VAE, our method obtains lower reconstruction errors under the same training conditions, trains in a fraction of the time, and with a constant number of parameters (equal to the embedding dimension $D$), making it a very scalable approach. We demonstrate these results on the FFHQ-1024 dataset and include FashionMNIST and Celeb-A.",
      "authors": [
        "Ahmed Khalil",
        "Robert Piechocki",
        "Raul Santos-Rodriguez"
      ],
      "published": "2023-10-13T20:03:18Z",
      "updated": "2023-10-13T20:03:18Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.09382v1",
      "landing_url": "https://arxiv.org/abs/2310.09382v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.09382"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on learnable lattice vector quantization for discrete representations but uses image datasets (FFHQ-1024, FashionMNIST, Celeb-A) and does not present a discrete audio token design or audio-language modeling workflow, thus it does not meet the inclusion criteria forDiscrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on learnable lattice vector quantization for discrete representations but uses image datasets (FFHQ-1024, FashionMNIST, Celeb-A) and does not present a discrete audio token design or audio-language modeling workflow, thus it does not meet the inclusion criteria forDiscrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a novel vector quantization method (LL-VQ-VAE) based on learnable lattice vector quantization and demonstrates its use for learning discrete representations. However, the abstract and title focus on image datasets (FFHQ-1024, FashionMNIST, Celeb-A), which are image domains, and there is no mention of audio or discrete audio tokens specifically targeting speech, music, or general audio signals. The study does not clarify the generation or use of discrete audio tokens for audio language modeling or tasks related to speech/audio understanding or generation, which are required by the inclusion criteria. Therefore, it does not meet the core topic of discrete audio token design for audio-related tasks and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a novel vector quantization method (LL-VQ-VAE) based on learnable lattice vector quantization and demonstrates its use for learning discrete representations. However, the abstract and title focus on image datasets (FFHQ-1024, FashionMNIST, Celeb-A), which are image domains, and there is no mention of audio or discrete audio tokens specifically targeting speech, music, or general audio signals. The study does not clarify the generation or use of discrete audio tokens for audio language modeling or tasks related to speech/audio understanding or generation, which are required by the inclusion criteria. Therefore, it does not meet the core topic of discrete audio token design for audio-related tasks and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Energy-Efficient Multi-Codec Bitrate-Ladder Estimation for Adaptive Video Streaming",
    "abstract": "With the emergence of multiple modern video codecs, streaming service providers are forced to encode, store, and transmit bitrate ladders of multiple codecs separately, consequently suffering from additional energy costs for encoding, storage, and transmission. To tackle this issue, we introduce an online energy-efficient Multi-Codec Bitrate ladder Estimation scheme (MCBE) for adaptive video streaming applications. In MCBE, quality representations within the bitrate ladder of new-generation codecs (e.g., High Efficiency Video Coding (HEVC), Alliance for Open Media Video 1 (AV1)) that lie below the predicted rate-distortion curve of the Advanced Video Coding (AVC) codec are removed. Moreover, perceptual redundancy between representations of the bitrate ladders of the considered codecs is also minimized based on a Just Noticeable Difference (JND) threshold. Therefore, random forest-based models predict the VMAF score of bitrate ladder representations of each codec. In a live streaming session where all clients support the decoding of AVC, HEVC, and AV1, MCBE achieves impressive results, reducing cumulative encoding energy by 56.45%, storage energy usage by 94.99%, and transmission energy usage by 77.61% (considering a JND of six VMAF points). These energy reductions are in comparison to a baseline bitrate ladder encoding based on current industry practice.",
    "metadata": {
      "arxiv_id": "2310.09570",
      "title": "Energy-Efficient Multi-Codec Bitrate-Ladder Estimation for Adaptive Video Streaming",
      "summary": "With the emergence of multiple modern video codecs, streaming service providers are forced to encode, store, and transmit bitrate ladders of multiple codecs separately, consequently suffering from additional energy costs for encoding, storage, and transmission. To tackle this issue, we introduce an online energy-efficient Multi-Codec Bitrate ladder Estimation scheme (MCBE) for adaptive video streaming applications. In MCBE, quality representations within the bitrate ladder of new-generation codecs (e.g., High Efficiency Video Coding (HEVC), Alliance for Open Media Video 1 (AV1)) that lie below the predicted rate-distortion curve of the Advanced Video Coding (AVC) codec are removed. Moreover, perceptual redundancy between representations of the bitrate ladders of the considered codecs is also minimized based on a Just Noticeable Difference (JND) threshold. Therefore, random forest-based models predict the VMAF score of bitrate ladder representations of each codec. In a live streaming session where all clients support the decoding of AVC, HEVC, and AV1, MCBE achieves impressive results, reducing cumulative encoding energy by 56.45%, storage energy usage by 94.99%, and transmission energy usage by 77.61% (considering a JND of six VMAF points). These energy reductions are in comparison to a baseline bitrate ladder encoding based on current industry practice.",
      "authors": [
        "Vignesh V Menon",
        "Reza Farahani",
        "Prajit T Rajendran",
        "Samira Afzal",
        "Klaus Schoeffmann",
        "Christian Timmerer"
      ],
      "published": "2023-10-14T12:05:18Z",
      "updated": "2023-10-14T12:05:18Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.09570v1",
      "landing_url": "https://arxiv.org/abs/2310.09570v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.09570"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on energy-efficient bitrate-ladder estimation across multiple codecs for adaptive video streaming and does not address discrete audio tokens or tokenization for language/sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on energy-efficient bitrate-ladder estimation across multiple codecs for adaptive video streaming and does not address discrete audio tokens or tokenization for language/sequence modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on energy-efficient bitrate ladder estimation for adaptive video streaming with traditional video codecs like AVC, HEVC, and AV1. It does not address discrete audio tokens, neural audio codecs, or semantic/acoustic disentanglement related to audio token design or usage for language/sequence models, which are core to the inclusion criteria. Therefore, it does not meet the inclusion criteria of focusing on discrete audio tokens or related methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on energy-efficient bitrate ladder estimation for adaptive video streaming with traditional video codecs like AVC, HEVC, and AV1. It does not address discrete audio tokens, neural audio codecs, or semantic/acoustic disentanglement related to audio token design or usage for language/sequence models, which are core to the inclusion criteria. Therefore, it does not meet the inclusion criteria of focusing on discrete audio tokens or related methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SelfVC: Voice Conversion With Iterative Refinement using Self Transformations",
    "abstract": "We propose SelfVC, a training strategy to iteratively improve a voice conversion model with self-synthesized examples. Previous efforts on voice conversion focus on factorizing speech into explicitly disentangled representations that separately encode speaker characteristics and linguistic content. However, disentangling speech representations to capture such attributes using task-specific loss terms can lead to information loss. In this work, instead of explicitly disentangling attributes with loss terms, we present a framework to train a controllable voice conversion model on entangled speech representations derived from self-supervised learning (SSL) and speaker verification models. First, we develop techniques to derive prosodic information from the audio signal and SSL representations to train predictive submodules in the synthesis model. Next, we propose a training strategy to iteratively improve the synthesis model for voice conversion, by creating a challenging training objective using self-synthesized examples. We demonstrate that incorporating such self-synthesized examples during training improves the speaker similarity of generated speech as compared to a baseline voice conversion model trained solely on heuristically perturbed inputs. Our framework is trained without any text and achieves state-of-the-art results in zero-shot voice conversion on metrics evaluating naturalness, speaker similarity, and intelligibility of synthesized audio.",
    "metadata": {
      "arxiv_id": "2310.09653",
      "title": "SelfVC: Voice Conversion With Iterative Refinement using Self Transformations",
      "summary": "We propose SelfVC, a training strategy to iteratively improve a voice conversion model with self-synthesized examples. Previous efforts on voice conversion focus on factorizing speech into explicitly disentangled representations that separately encode speaker characteristics and linguistic content. However, disentangling speech representations to capture such attributes using task-specific loss terms can lead to information loss. In this work, instead of explicitly disentangling attributes with loss terms, we present a framework to train a controllable voice conversion model on entangled speech representations derived from self-supervised learning (SSL) and speaker verification models. First, we develop techniques to derive prosodic information from the audio signal and SSL representations to train predictive submodules in the synthesis model. Next, we propose a training strategy to iteratively improve the synthesis model for voice conversion, by creating a challenging training objective using self-synthesized examples. We demonstrate that incorporating such self-synthesized examples during training improves the speaker similarity of generated speech as compared to a baseline voice conversion model trained solely on heuristically perturbed inputs. Our framework is trained without any text and achieves state-of-the-art results in zero-shot voice conversion on metrics evaluating naturalness, speaker similarity, and intelligibility of synthesized audio.",
      "authors": [
        "Paarth Neekhara",
        "Shehzeen Hussain",
        "Rafael Valle",
        "Boris Ginsburg",
        "Rishabh Ranjan",
        "Shlomo Dubnov",
        "Farinaz Koushanfar",
        "Julian McAuley"
      ],
      "published": "2023-10-14T19:51:17Z",
      "updated": "2024-05-03T16:45:39Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.09653v2",
      "landing_url": "https://arxiv.org/abs/2310.09653v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.09653"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on self-supervised voice conversion with iterative refinement and self-synthesized data; it does not address discrete audio tokens, tokenization, or quantization for audio-language modeling, so it fails to meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on self-supervised voice conversion with iterative refinement and self-synthesized data; it does not address discrete audio tokens, tokenization, or quantization for audio-language modeling, so it fails to meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a voice conversion model training strategy focusing on self-synthesized examples and disentanglement of speech attributes but does not mention generating or utilizing discrete audio tokens through vector quantization or similar discretization mechanisms central to the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a voice conversion model training strategy focusing on self-synthesized examples and disentanglement of speech attributes but does not mention generating or utilizing discrete audio tokens through vector quantization or similar discretization mechanisms central to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Towards image compression with perfect realism at ultra-low bitrates",
    "abstract": "Image codecs are typically optimized to trade-off bitrate \\vs distortion metrics. At low bitrates, this leads to compression artefacts which are easily perceptible, even when training with perceptual or adversarial losses. To improve image quality and remove dependency on the bitrate, we propose to decode with iterative diffusion models. We condition the decoding process on a vector-quantized image representation, as well as a global image description to provide additional context. We dub our model PerCo for 'perceptual compression', and compare it to state-of-the-art codecs at rates from 0.1 down to 0.003 bits per pixel. The latter rate is more than an order of magnitude smaller than those considered in most prior work, compressing a 512x768 Kodak image with less than 153 bytes. Despite this ultra-low bitrate, our approach maintains the ability to reconstruct realistic images. We find that our model leads to reconstructions with state-of-the-art visual quality as measured by FID and KID. As predicted by rate-distortion-perception theory, visual quality is less dependent on the bitrate than previous methods.",
    "metadata": {
      "arxiv_id": "2310.10325",
      "title": "Towards image compression with perfect realism at ultra-low bitrates",
      "summary": "Image codecs are typically optimized to trade-off bitrate \\vs distortion metrics. At low bitrates, this leads to compression artefacts which are easily perceptible, even when training with perceptual or adversarial losses. To improve image quality and remove dependency on the bitrate, we propose to decode with iterative diffusion models. We condition the decoding process on a vector-quantized image representation, as well as a global image description to provide additional context. We dub our model PerCo for 'perceptual compression', and compare it to state-of-the-art codecs at rates from 0.1 down to 0.003 bits per pixel. The latter rate is more than an order of magnitude smaller than those considered in most prior work, compressing a 512x768 Kodak image with less than 153 bytes. Despite this ultra-low bitrate, our approach maintains the ability to reconstruct realistic images. We find that our model leads to reconstructions with state-of-the-art visual quality as measured by FID and KID. As predicted by rate-distortion-perception theory, visual quality is less dependent on the bitrate than previous methods.",
      "authors": [
        "Marlène Careil",
        "Matthew J. Muckley",
        "Jakob Verbeek",
        "Stéphane Lathuilière"
      ],
      "published": "2023-10-16T12:08:35Z",
      "updated": "2024-03-19T09:54:41Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.10325v2",
      "landing_url": "https://arxiv.org/abs/2310.10325v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.10325"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describes image compression using iterative diffusion models (PerCo) for ultra-low bitrate reconstruction and does not involve discrete audio tokens, token generation workflows, or audio-domain vector quantization, so it does not meet the core inclusion criteria for discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describes image compression using iterative diffusion models (PerCo) for ultra-low bitrate reconstruction and does not involve discrete audio tokens, token generation workflows, or audio-domain vector quantization, so it does not meet the core inclusion criteria for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on image compression using diffusion models and vector-quantized image representations at extremely low bitrates, with no mention of discrete audio tokens, audio representations, or audio-related methodologies, which is the core inclusion criterion. It does not address audio tokenization, vector quantization of audio, or language/sequence modeling of audio tokens, thus failing to meet the inclusion requirements and falling outside the relevant research domain.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on image compression using diffusion models and vector-quantized image representations at extremely low bitrates, with no mention of discrete audio tokens, audio representations, or audio-related methodologies, which is the core inclusion criterion. It does not address audio tokenization, vector quantization of audio, or language/sequence modeling of audio tokens, thus failing to meet the inclusion requirements and falling outside the relevant research domain.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic Organization in HuBERT",
    "abstract": "Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space and the units beyond phonemes are largely underexplored. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt \"self-distillation\" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames exhibit salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling.",
    "metadata": {
      "arxiv_id": "2310.10803",
      "title": "SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic Organization in HuBERT",
      "summary": "Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space and the units beyond phonemes are largely underexplored. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt \"self-distillation\" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames exhibit salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling.",
      "authors": [
        "Cheol Jun Cho",
        "Abdelrahman Mohamed",
        "Shang-Wen Li",
        "Alan W Black",
        "Gopala K. Anumanchipalli"
      ],
      "published": "2023-10-16T20:05:36Z",
      "updated": "2025-04-10T11:20:55Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.10803v3",
      "landing_url": "https://arxiv.org/abs/2310.10803v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.10803"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on sentence-level self-distillation and emergent syllabic organization in HuBERT; it does not present a discrete audio token design (quantization, token generation, VQ/RVQ) or provide the necessary methodological details for token-based representations, thus it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on sentence-level self-distillation and emergent syllabic organization in HuBERT; it does not present a discrete audio token design (quantization, token generation, VQ/RVQ) or provide the necessary methodological details for token-based representations, thus it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised speech representation learning, particularly the emergence of syllabic structures via self-distillation on HuBERT, but it does not explicitly describe a process for producing discrete audio tokens, such as vector quantization or clustering to form discrete units used for sequence modeling or audio language modeling. The abstract highlights representation learning and syllabic organization rather than discrete token design or usage as inputs to language or sequence models, and lacks clear technical details on token generation mechanisms. Therefore, it does not meet the inclusion criteria centered on discrete audio token design and usage for downstream sequence modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised speech representation learning, particularly the emergence of syllabic structures via self-distillation on HuBERT, but it does not explicitly describe a process for producing discrete audio tokens, such as vector quantization or clustering to form discrete units used for sequence modeling or audio language modeling. The abstract highlights representation learning and syllabic organization rather than discrete token design or usage as inputs to language or sequence models, and lacks clear technical details on token generation mechanisms. Therefore, it does not meet the inclusion criteria centered on discrete audio token design and usage for downstream sequence modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Spatial HuBERT: Self-supervised Spatial Speech Representation Learning for a Single Talker from Multi-channel Audio",
    "abstract": "Self-supervised learning has been used to leverage unlabelled data, improving accuracy and generalisation of speech systems through the training of representation models. While many recent works have sought to produce effective representations across a variety of acoustic domains, languages, modalities and even simultaneous speakers, these studies have all been limited to single-channel audio recordings. This paper presents Spatial HuBERT, a self-supervised speech representation model that learns both acoustic and spatial information pertaining to a single speaker in a potentially noisy environment by using multi-channel audio inputs. Spatial HuBERT learns representations that outperform state-of-the-art single-channel speech representations on a variety of spatial downstream tasks, particularly in reverberant and noisy environments. We also demonstrate the utility of the representations learned by Spatial HuBERT on a speech localisation downstream task. Along with this paper, we publicly release a new dataset of 100 000 simulated first-order ambisonics room impulse responses.",
    "metadata": {
      "arxiv_id": "2310.10922",
      "title": "Spatial HuBERT: Self-supervised Spatial Speech Representation Learning for a Single Talker from Multi-channel Audio",
      "summary": "Self-supervised learning has been used to leverage unlabelled data, improving accuracy and generalisation of speech systems through the training of representation models. While many recent works have sought to produce effective representations across a variety of acoustic domains, languages, modalities and even simultaneous speakers, these studies have all been limited to single-channel audio recordings. This paper presents Spatial HuBERT, a self-supervised speech representation model that learns both acoustic and spatial information pertaining to a single speaker in a potentially noisy environment by using multi-channel audio inputs. Spatial HuBERT learns representations that outperform state-of-the-art single-channel speech representations on a variety of spatial downstream tasks, particularly in reverberant and noisy environments. We also demonstrate the utility of the representations learned by Spatial HuBERT on a speech localisation downstream task. Along with this paper, we publicly release a new dataset of 100 000 simulated first-order ambisonics room impulse responses.",
      "authors": [
        "Antoni Dimitriadis",
        "Siqi Pan",
        "Vidhyasaharan Sethu",
        "Beena Ahmed"
      ],
      "published": "2023-10-17T01:31:59Z",
      "updated": "2023-10-17T01:31:59Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.10922v1",
      "landing_url": "https://arxiv.org/abs/2310.10922v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.10922"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item proposes Spatial HuBERT, focusing on spatial and acoustic self-supervised representations; there is no emphasis on discrete audio tokens or tokenization, vector quantization, or token-based modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item proposes Spatial HuBERT, focusing on spatial and acoustic self-supervised representations; there is no emphasis on discrete audio tokens or tokenization, vector quantization, or token-based modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents Spatial HuBERT, a self-supervised learning model that captures acoustic and spatial information from multi-channel audio inputs but does not explicitly mention the generation or use of discrete audio tokens through vector quantization or related discretization mechanisms, nor does it describe token formation processes or usage for sequence or language modeling, which are essential according to the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper presents Spatial HuBERT, a self-supervised learning model that captures acoustic and spatial information from multi-channel audio inputs but does not explicitly mention the generation or use of discrete audio tokens through vector quantization or related discretization mechanisms, nor does it describe token formation processes or usage for sequence or language modeling, which are essential according to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "MUST&P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning",
    "abstract": "In this paper, we present a methodology for linguistic feature extraction, focusing particularly on automatically syllabifying words in multiple languages, with a design to be compatible with a forced-alignment tool, the Montreal Forced Aligner (MFA). In both the textual and phonetic domains, our method focuses on the extraction of phonetic transcriptions from text, stress marks, and a unified automatic syllabification (in text and phonetic domains). The system was built with open-source components and resources. Through an ablation study, we demonstrate the efficacy of our approach in automatically syllabifying words from several languages (English, French and Spanish). Additionally, we apply the technique to the transcriptions of the CMU ARCTIC dataset, generating valuable annotations available online\\footnote{\\url{https://github.com/noetits/MUST_P-SRL}} that are ideal for speech representation learning, speech unit discovery, and disentanglement of speech factors in several speech-related fields.",
    "metadata": {
      "arxiv_id": "2310.11541",
      "title": "MUST&P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning",
      "summary": "In this paper, we present a methodology for linguistic feature extraction, focusing particularly on automatically syllabifying words in multiple languages, with a design to be compatible with a forced-alignment tool, the Montreal Forced Aligner (MFA). In both the textual and phonetic domains, our method focuses on the extraction of phonetic transcriptions from text, stress marks, and a unified automatic syllabification (in text and phonetic domains). The system was built with open-source components and resources. Through an ablation study, we demonstrate the efficacy of our approach in automatically syllabifying words from several languages (English, French and Spanish). Additionally, we apply the technique to the transcriptions of the CMU ARCTIC dataset, generating valuable annotations available online\\footnote{\\url{https://github.com/noetits/MUST_P-SRL}} that are ideal for speech representation learning, speech unit discovery, and disentanglement of speech factors in several speech-related fields.",
      "authors": [
        "Noé Tits"
      ],
      "published": "2023-10-17T19:27:23Z",
      "updated": "2023-10-17T19:27:23Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.11541v1",
      "landing_url": "https://arxiv.org/abs/2310.11541v1",
      "doi": null
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on syllabification and phonetic transcription rather than discrete audio token design/quantization for audio-language modeling, and it lacks a clear publication date, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on syllabification and phonetic transcription rather than discrete audio token design/quantization for audio-language modeling, and it lacks a clear publication date, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on linguistic feature extraction via automatic syllabification across multiple languages and generates annotations useful for speech representation learning and disentanglement of speech factors, but it does not describe any discrete audio token generation, vector quantization, or any form of tokenization strategy explicitly designed for language or sequence modeling as per the inclusion criteria. Additionally, there is no detailed method description about token formation or quantization process, nor evidence that the outputs are discrete tokens intended for sequence modeling. Therefore, the paper does not meet the inclusion criteria for discrete audio token design and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on linguistic feature extraction via automatic syllabification across multiple languages and generates annotations useful for speech representation learning and disentanglement of speech factors, but it does not describe any discrete audio token generation, vector quantization, or any form of tokenization strategy explicitly designed for language or sequence modeling as per the inclusion criteria. Additionally, there is no detailed method description about token formation or quantization process, nor evidence that the outputs are discrete tokens intended for sequence modeling. Therefore, the paper does not meet the inclusion criteria for discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Acoustic BPE for Speech Generation with Discrete Tokens",
    "abstract": "Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.",
    "metadata": {
      "arxiv_id": "2310.14580",
      "title": "Acoustic BPE for Speech Generation with Discrete Tokens",
      "summary": "Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.",
      "authors": [
        "Feiyu Shen",
        "Yiwei Guo",
        "Chenpeng Du",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2023-10-23T05:38:41Z",
      "updated": "2024-01-15T05:53:31Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.14580v4",
      "landing_url": "https://arxiv.org/abs/2310.14580v4",
      "doi": "https://doi.org/10.48550/arXiv.2310.14580"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Abstract clearly targets discrete audio tokens produced via acoustic BPE for speech generation, reports model training/evaluation and a rescore technique—consistent with inclusion criteria; no explicit exclusion cues are evident from the abstract (though full methodological details and publication date cannot be verified from the snippet).",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "Abstract clearly targets discrete audio tokens produced via acoustic BPE for speech generation, reports model training/evaluation and a rescore technique—consistent with inclusion criteria; no explicit exclusion cues are evident from the abstract (though full methodological details and publication date cannot be verified from the snippet).",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes acoustic byte-pair encoding (BPE) for encoding discrete audio tokens derived from self-supervised learning models, focusing on reducing token sequence length and improving speech generation modeling, which aligns with the inclusion criteria that require research contributions in discrete audio token design and usage for sequence/language modeling. The abstract sufficiently describes the token encoding process and its application in a speech language model, demonstrating a clear method and experimental validation, thus meeting the inclusion standards while no exclusion criteria about missing method details or relevance are met.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes acoustic byte-pair encoding (BPE) for encoding discrete audio tokens derived from self-supervised learning models, focusing on reducing token sequence length and improving speech generation modeling, which aligns with the inclusion criteria that require research contributions in discrete audio token design and usage for sequence/language modeling. The abstract sufficiently describes the token encoding process and its application in a speech language model, demonstrating a clear method and experimental validation, thus meeting the inclusion standards while no exclusion criteria about missing method details or relevance are met.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Convex Hull Prediction Methods for Bitrate Ladder Construction: Design, Evaluation, and Comparison",
    "abstract": "HTTP adaptive streaming (HAS) has emerged as a prevalent approach for over-the-top (OTT) video streaming services due to its ability to deliver a seamless user experience. A fundamental component of HAS is the bitrate ladder, which comprises a set of encoding parameters (e.g., bitrate-resolution pairs) used to encode the source video into multiple representations. This adaptive bitrate ladder enables the client's video player to dynamically adjust the quality of the video stream in real-time based on fluctuations in network conditions, ensuring uninterrupted playback by selecting the most suitable representation for the available bandwidth. The most straightforward approach involves using a fixed bitrate ladder for all videos, consisting of pre-determined bitrate-resolution pairs known as one-size-fits-all. Conversely, the most reliable technique relies on intensively encoding all resolutions over a wide range of bitrates to build the convex hull, thereby optimizing the bitrate ladder by selecting the representations from the convex hull for each specific video. Several techniques have been proposed to predict content-based ladders without performing a costly, exhaustive search encoding. This paper provides a comprehensive review of various convex hull prediction methods, including both conventional and learning-based approaches. Furthermore, we conduct a benchmark study of several handcrafted- and DL-based approaches for predicting content-optimized convex hulls across multiple codec settings. The considered methods are evaluated on our proposed large-scale dataset, which includes 300 UHD video shots encoded with software and hardware encoders using three state-of-the-art video standards, including AVC/H.264, HEVC/H.265, and VVC/H.266, at various bitrate points. Our analysis provides valuable insights and establishes baseline performance for future research in this field.",
    "metadata": {
      "arxiv_id": "2310.15163",
      "title": "Convex Hull Prediction Methods for Bitrate Ladder Construction: Design, Evaluation, and Comparison",
      "summary": "HTTP adaptive streaming (HAS) has emerged as a prevalent approach for over-the-top (OTT) video streaming services due to its ability to deliver a seamless user experience. A fundamental component of HAS is the bitrate ladder, which comprises a set of encoding parameters (e.g., bitrate-resolution pairs) used to encode the source video into multiple representations. This adaptive bitrate ladder enables the client's video player to dynamically adjust the quality of the video stream in real-time based on fluctuations in network conditions, ensuring uninterrupted playback by selecting the most suitable representation for the available bandwidth. The most straightforward approach involves using a fixed bitrate ladder for all videos, consisting of pre-determined bitrate-resolution pairs known as one-size-fits-all. Conversely, the most reliable technique relies on intensively encoding all resolutions over a wide range of bitrates to build the convex hull, thereby optimizing the bitrate ladder by selecting the representations from the convex hull for each specific video. Several techniques have been proposed to predict content-based ladders without performing a costly, exhaustive search encoding. This paper provides a comprehensive review of various convex hull prediction methods, including both conventional and learning-based approaches. Furthermore, we conduct a benchmark study of several handcrafted- and DL-based approaches for predicting content-optimized convex hulls across multiple codec settings. The considered methods are evaluated on our proposed large-scale dataset, which includes 300 UHD video shots encoded with software and hardware encoders using three state-of-the-art video standards, including AVC/H.264, HEVC/H.265, and VVC/H.266, at various bitrate points. Our analysis provides valuable insights and establishes baseline performance for future research in this field.",
      "authors": [
        "Ahmed Telili",
        "Wassim Hamidouche",
        "Hadi Amirpour",
        "Sid Ahmed Fezza",
        "Luce Morin",
        "Christian Timmerer"
      ],
      "published": "2023-10-23T17:58:24Z",
      "updated": "2025-03-11T06:48:06Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.15163v4",
      "landing_url": "https://arxiv.org/abs/2310.15163v4",
      "doi": "https://doi.org/10.48550/arXiv.2310.15163"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on convex-hull-based bitrate ladder methods and evaluation for HTTP adaptive streaming (video encoding optimization), not on discrete audio tokens or token-based audio representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on convex-hull-based bitrate ladder methods and evaluation for HTTP adaptive streaming (video encoding optimization), not on discrete audio tokens or token-based audio representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on bitrate ladder construction for video streaming using convex hull prediction methods, which pertains to video encoding and streaming optimization, not to discrete audio tokens derived from continuous audio signals for language modeling or audio generation/understanding tasks. It does not describe methods related to vector quantization or tokenization of audio signals, nor does it address neural audio codecs or semantic disentanglement of audio representations. Therefore, it does not meet the inclusion criteria centered on discrete audio token design, usage, or evaluation, and it clearly falls under exclusion criteria regarding traditional codec-focused approaches without discrete token generation for language or sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on bitrate ladder construction for video streaming using convex hull prediction methods, which pertains to video encoding and streaming optimization, not to discrete audio tokens derived from continuous audio signals for language modeling or audio generation/understanding tasks. It does not describe methods related to vector quantization or tokenization of audio signals, nor does it address neural audio codecs or semantic disentanglement of audio representations. Therefore, it does not meet the inclusion criteria centered on discrete audio token design, usage, or evaluation, and it clearly falls under exclusion criteria regarding traditional codec-focused approaches without discrete token generation for language or sequence modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Discriminator Guidance for Autoregressive Diffusion Models",
    "abstract": "We introduce discriminator guidance in the setting of Autoregressive Diffusion Models. The use of a discriminator to guide a diffusion process has previously been used for continuous diffusion models, and in this work we derive ways of using a discriminator together with a pretrained generative model in the discrete case. First, we show that using an optimal discriminator will correct the pretrained model and enable exact sampling from the underlying data distribution. Second, to account for the realistic scenario of using a sub-optimal discriminator, we derive a sequential Monte Carlo algorithm which iteratively takes the predictions from the discriminator into account during the generation process. We test these approaches on the task of generating molecular graphs and show how the discriminator improves the generative performance over using only the pretrained model.",
    "metadata": {
      "arxiv_id": "2310.15817",
      "title": "Discriminator Guidance for Autoregressive Diffusion Models",
      "summary": "We introduce discriminator guidance in the setting of Autoregressive Diffusion Models. The use of a discriminator to guide a diffusion process has previously been used for continuous diffusion models, and in this work we derive ways of using a discriminator together with a pretrained generative model in the discrete case. First, we show that using an optimal discriminator will correct the pretrained model and enable exact sampling from the underlying data distribution. Second, to account for the realistic scenario of using a sub-optimal discriminator, we derive a sequential Monte Carlo algorithm which iteratively takes the predictions from the discriminator into account during the generation process. We test these approaches on the task of generating molecular graphs and show how the discriminator improves the generative performance over using only the pretrained model.",
      "authors": [
        "Filip Ekström Kelvinius",
        "Fredrik Lindsten"
      ],
      "published": "2023-10-24T13:14:22Z",
      "updated": "2024-09-21T12:34:38Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.15817v2",
      "landing_url": "https://arxiv.org/abs/2310.15817v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.15817"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses discriminator guidance for autoregressive diffusion models and molecular graphs, not about discrete audio token design or discrete audio representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses discriminator guidance for autoregressive diffusion models and molecular graphs, not about discrete audio token design or discrete audio representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces discriminator guidance for autoregressive diffusion models, focusing on molecular graph generation without any indication of working on discrete audio tokens or audio signal processing, thus failing to meet the topic criteria related to discrete audio token design and usage for audio language modeling or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper introduces discriminator guidance for autoregressive diffusion models, focusing on molecular graph generation without any indication of working on discrete audio tokens or audio signal processing, thus failing to meet the topic criteria related to discrete audio token design and usage for audio language modeling or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Dynamic Processing Neural Network Architecture For Hearing Loss Compensation",
    "abstract": "This paper proposes neural networks for compensating sensorineural hearing loss. The aim of the hearing loss compensation task is to transform a speech signal to increase speech intelligibility after further processing by a person with a hearing impairment, which is modeled by a hearing loss model. We propose an interpretable model called dynamic processing network, which has a structure similar to band-wise dynamic compressor. The network is differentiable, and therefore allows to learn its parameters to maximize speech intelligibility. More generic models based on convolutional layers were tested as well. The performance of the tested architectures was assessed using spectro-temporal objective index (STOI) with hearing-threshold noise and hearing aid speech intelligibility (HASPI) metrics. The dynamic processing network gave a significant improvement of STOI and HASPI in comparison to popular compressive gain prescription rule Camfit. A large enough convolutional network could outperform the interpretable model with the cost of larger computational load. Finally, a combination of the dynamic processing network with convolutional neural network gave the best results in terms of STOI and HASPI.",
    "metadata": {
      "arxiv_id": "2310.16550",
      "title": "Dynamic Processing Neural Network Architecture For Hearing Loss Compensation",
      "summary": "This paper proposes neural networks for compensating sensorineural hearing loss. The aim of the hearing loss compensation task is to transform a speech signal to increase speech intelligibility after further processing by a person with a hearing impairment, which is modeled by a hearing loss model. We propose an interpretable model called dynamic processing network, which has a structure similar to band-wise dynamic compressor. The network is differentiable, and therefore allows to learn its parameters to maximize speech intelligibility. More generic models based on convolutional layers were tested as well. The performance of the tested architectures was assessed using spectro-temporal objective index (STOI) with hearing-threshold noise and hearing aid speech intelligibility (HASPI) metrics. The dynamic processing network gave a significant improvement of STOI and HASPI in comparison to popular compressive gain prescription rule Camfit. A large enough convolutional network could outperform the interpretable model with the cost of larger computational load. Finally, a combination of the dynamic processing network with convolutional neural network gave the best results in terms of STOI and HASPI.",
      "authors": [
        "Szymon Drgas",
        "Lars Bramsløw",
        "Archontis Politis",
        "Gaurav Naithani",
        "Tuomas Virtanen"
      ],
      "published": "2023-10-25T11:04:32Z",
      "updated": "2023-10-25T11:04:32Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.16550v1",
      "landing_url": "https://arxiv.org/abs/2310.16550v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.16550"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on a dynamic processing network for hearing loss compensation and does not discuss discrete audio tokens, token generation, or quantization required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on a dynamic processing network for hearing loss compensation and does not discuss discrete audio tokens, token generation, or quantization required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on neural network architectures for hearing loss compensation by enhancing speech intelligibility, but it does not discuss the generation or use of discrete audio tokens through vector quantization or related discrete representation methods, nor does it describe token formation processes or sequence modeling applications relevant to discrete audio tokens; thus, it does not meet the defined inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on neural network architectures for hearing loss compensation by enhancing speech intelligibility, but it does not discuss the generation or use of discrete audio tokens through vector quantization or related discrete representation methods, nor does it describe token formation processes or sequence modeling applications relevant to discrete audio tokens; thus, it does not meet the defined inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction",
    "abstract": "We introduce a text-to-speech(TTS) framework based on a neural transducer. We use discretized semantic tokens acquired from wav2vec2.0 embeddings, which makes it easy to adopt a neural transducer for the TTS framework enjoying its monotonic alignment constraints. The proposed model first generates aligned semantic tokens using the neural transducer, then synthesizes a speech sample from the semantic tokens using a non-autoregressive(NAR) speech generator. This decoupled framework alleviates the training complexity of TTS and allows each stage to focus on 1) linguistic and alignment modeling and 2) fine-grained acoustic modeling, respectively. Experimental results on the zero-shot adaptive TTS show that the proposed model exceeds the baselines in speech quality and speaker similarity via objective and subjective measures. We also investigate the inference speed and prosody controllability of our proposed model, showing the potential of the neural transducer for TTS frameworks.",
    "metadata": {
      "arxiv_id": "2311.02898",
      "title": "Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction",
      "summary": "We introduce a text-to-speech(TTS) framework based on a neural transducer. We use discretized semantic tokens acquired from wav2vec2.0 embeddings, which makes it easy to adopt a neural transducer for the TTS framework enjoying its monotonic alignment constraints. The proposed model first generates aligned semantic tokens using the neural transducer, then synthesizes a speech sample from the semantic tokens using a non-autoregressive(NAR) speech generator. This decoupled framework alleviates the training complexity of TTS and allows each stage to focus on 1) linguistic and alignment modeling and 2) fine-grained acoustic modeling, respectively. Experimental results on the zero-shot adaptive TTS show that the proposed model exceeds the baselines in speech quality and speaker similarity via objective and subjective measures. We also investigate the inference speed and prosody controllability of our proposed model, showing the potential of the neural transducer for TTS frameworks.",
      "authors": [
        "Minchan Kim",
        "Myeonghun Jeong",
        "Byoung Jin Choi",
        "Dongjune Lee",
        "Nam Soo Kim"
      ],
      "published": "2023-11-06T06:13:39Z",
      "updated": "2023-11-08T05:52:39Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.02898v2",
      "landing_url": "https://arxiv.org/abs/2311.02898v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.02898"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the work centers on discretized semantic tokens for TTS (discrete audio token design and token-based modeling), the source page lacks a clear publication date (YYYY-MM-DD), triggering the exclusion criterion; hence exclude.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the work centers on discretized semantic tokens for TTS (discrete audio token design and token-based modeling), the source page lacks a clear publication date (YYYY-MM-DD), triggering the exclusion criterion; hence exclude.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a TTS framework that uses discretized semantic tokens derived from wav2vec 2.0 embeddings, employing a neural transducer model that generates aligned semantic tokens before synthesizing speech. This approach aligns with the inclusion criteria of using discretized audio tokens with a clear token generation and usage pipeline, focusing on semantic tokens obtained via a self-supervised learning-based embedding quantization, used for generation in a sequence model context.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a TTS framework that uses discretized semantic tokens derived from wav2vec 2.0 embeddings, employing a neural transducer model that generates aligned semantic tokens before synthesizing speech. This approach aligns with the inclusion criteria of using discretized audio tokens with a clear token generation and usage pipeline, focusing on semantic tokens obtained via a self-supervised learning-based embedding quantization, used for generation in a sequence model context.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "The abstract explicitly describes creating discretized semantic tokens from wav2vec2.0 embeddings and using them as the core representation for sequence modeling (neural transducer) and subsequent TTS generation, which directly matches the inclusion criteria for SSL-derived discrete audio tokens used for generation; JuniorNano excluded only due to missing publication-date metadata (not available in this prompt) while JuniorMini agreed the content fits — absent the stated metadata exclusion, the paper meets the topic requirements. I rate this a 5 because the primary contribution centers on SSL-derived discretized semantic tokens used for sequence modeling and TTS, matching the inclusion criteria.",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "The abstract explicitly describes creating discretized semantic tokens from wav2vec2.0 embeddings and using them as the core representation for sequence modeling (neural transducer) and subsequent TTS generation, which directly matches the inclusion criteria for SSL-derived discrete audio tokens used for generation; JuniorNano excluded only due to missing publication-date metadata (not available in this prompt) while JuniorMini agreed the content fits — absent the stated metadata exclusion, the paper meets the topic requirements. I rate this a 5 because the primary contribution centers on SSL-derived discretized semantic tokens used for sequence modeling and TTS, matching the inclusion criteria.",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Learning Disentangled Speech Representations",
    "abstract": "Disentangled representation learning in speech processing has lagged behind other domains, largely due to the lack of datasets with annotated generative factors for robust evaluation. To address this, we propose SynSpeech, a novel large-scale synthetic speech dataset specifically designed to enable research on disentangled speech representations. SynSpeech includes controlled variations in speaker identity, spoken text, and speaking style, with three dataset versions to support experimentation at different levels of complexity. In this study, we present a comprehensive framework to evaluate disentangled representation learning techniques, applying both linear probing and established supervised disentanglement metrics to assess the modularity, compactness, and informativeness of the representations learned by a state-of-the-art model. Using the RAVE model as a test case, we find that SynSpeech facilitates benchmarking across a range of factors, achieving promising disentanglement of simpler features like gender and speaking style, while highlighting challenges in isolating complex attributes like speaker identity. This benchmark dataset and evaluation framework fills a critical gap, supporting the development of more robust and interpretable speech representation learning methods.",
    "metadata": {
      "arxiv_id": "2311.03389",
      "title": "Learning Disentangled Speech Representations",
      "summary": "Disentangled representation learning in speech processing has lagged behind other domains, largely due to the lack of datasets with annotated generative factors for robust evaluation. To address this, we propose SynSpeech, a novel large-scale synthetic speech dataset specifically designed to enable research on disentangled speech representations. SynSpeech includes controlled variations in speaker identity, spoken text, and speaking style, with three dataset versions to support experimentation at different levels of complexity.\n  In this study, we present a comprehensive framework to evaluate disentangled representation learning techniques, applying both linear probing and established supervised disentanglement metrics to assess the modularity, compactness, and informativeness of the representations learned by a state-of-the-art model. Using the RAVE model as a test case, we find that SynSpeech facilitates benchmarking across a range of factors, achieving promising disentanglement of simpler features like gender and speaking style, while highlighting challenges in isolating complex attributes like speaker identity. This benchmark dataset and evaluation framework fills a critical gap, supporting the development of more robust and interpretable speech representation learning methods.",
      "authors": [
        "Yusuf Brima",
        "Ulf Krumnack",
        "Simone Pika",
        "Gunther Heidemann"
      ],
      "published": "2023-11-04T04:54:17Z",
      "updated": "2025-01-11T06:05:41Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.03389v4",
      "landing_url": "https://arxiv.org/abs/2311.03389v4",
      "doi": "https://doi.org/10.48550/arXiv.2311.03389"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on disentangled speech representations and a synthetic benchmarking dataset, not on discrete audio tokens or token design/quantization; it fails to meet the inclusion criteria focused on discrete token representations and related methods, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on disentangled speech representations and a synthetic benchmarking dataset, not on discrete audio tokens or token design/quantization; it fails to meet the inclusion criteria focused on discrete token representations and related methods, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on disentangled speech representations but does not explicitly address the main topic of discrete audio tokens or the process of token generation via vector quantization or similar methods; it also lacks detailed description of token formation or usage as discrete tokens for sequence modeling, thus not meeting the core inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on disentangled speech representations but does not explicitly address the main topic of discrete audio tokens or the process of token generation via vector quantization or similar methods; it also lacks detailed description of token formation or usage as discrete tokens for sequence modeling, thus not meeting the core inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Character-Level Bangla Text-to-IPA Transcription Using Transformer Architecture with Sequence Alignment",
    "abstract": "The International Phonetic Alphabet (IPA) is indispensable in language learning and understanding, aiding users in accurate pronunciation and comprehension. Additionally, it plays a pivotal role in speech therapy, linguistic research, accurate transliteration, and the development of text-to-speech systems, making it an essential tool across diverse fields. Bangla being 7th as one of the widely used languages, gives rise to the need for IPA in its domain. Its IPA mapping is too diverse to be captured manually giving the need for Artificial Intelligence and Machine Learning in this field. In this study, we have utilized a transformer-based sequence-to-sequence model at the letter and symbol level to get the IPA of each Bangla word as the variation of IPA in association of different words is almost null. Our transformer model only consisted of 8.5 million parameters with only a single decoder and encoder layer. Additionally, to handle the punctuation marks and the occurrence of foreign languages in the text, we have utilized manual mapping as the model won't be able to learn to separate them from Bangla words while decreasing our required computational resources. Finally, maintaining the relative position of the sentence component IPAs and generation of the combined IPA has led us to achieve the top position with a word error rate of 0.10582 in the public ranking of DataVerse Challenge - ITVerse 2023 (https://www.kaggle.com/competitions/dataverse_2023/).",
    "metadata": {
      "arxiv_id": "2311.03792",
      "title": "Character-Level Bangla Text-to-IPA Transcription Using Transformer Architecture with Sequence Alignment",
      "summary": "The International Phonetic Alphabet (IPA) is indispensable in language learning and understanding, aiding users in accurate pronunciation and comprehension. Additionally, it plays a pivotal role in speech therapy, linguistic research, accurate transliteration, and the development of text-to-speech systems, making it an essential tool across diverse fields. Bangla being 7th as one of the widely used languages, gives rise to the need for IPA in its domain. Its IPA mapping is too diverse to be captured manually giving the need for Artificial Intelligence and Machine Learning in this field. In this study, we have utilized a transformer-based sequence-to-sequence model at the letter and symbol level to get the IPA of each Bangla word as the variation of IPA in association of different words is almost null. Our transformer model only consisted of 8.5 million parameters with only a single decoder and encoder layer. Additionally, to handle the punctuation marks and the occurrence of foreign languages in the text, we have utilized manual mapping as the model won't be able to learn to separate them from Bangla words while decreasing our required computational resources. Finally, maintaining the relative position of the sentence component IPAs and generation of the combined IPA has led us to achieve the top position with a word error rate of 0.10582 in the public ranking of DataVerse Challenge - ITVerse 2023 (https://www.kaggle.com/competitions/dataverse_2023/).",
      "authors": [
        "Jakir Hasan",
        "Shrestha Datta",
        "Ameya Debnath"
      ],
      "published": "2023-11-07T08:20:06Z",
      "updated": "2023-11-07T08:20:06Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.03792v1",
      "landing_url": "https://arxiv.org/abs/2311.03792v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.03792"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item targets character-level Bangla text-to-IPA transcription using a transformer, which is about text-to-phoneme mapping rather than generating or using discrete audio tokens (e.g., VQ/RVQ, semantic/ acoustic discretization) for audio-language modeling. Therefore it does not meet the inclusion criteria focused on discrete audio token representations and would violate the exclusion criteria for lack of relevant discrete-audio-token methodology.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item targets character-level Bangla text-to-IPA transcription using a transformer, which is about text-to-phoneme mapping rather than generating or using discrete audio tokens (e.g., VQ/RVQ, semantic/ acoustic discretization) for audio-language modeling. Therefore it does not meet the inclusion criteria focused on discrete audio token representations and would violate the exclusion criteria for lack of relevant discrete-audio-token methodology.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on character-level Bangla text-to-IPA transcription using a transformer model, which relates to text and phonetic transcription rather than the conversion of continuous audio signals into discrete audio tokens or vector quantized representations, thus it does not meet the inclusion criteria focusing on audio tokenization methods and discrete audio representations for language or sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on character-level Bangla text-to-IPA transcription using a transformer model, which relates to text and phonetic transcription rather than the conversion of continuous audio signals into discrete audio tokens or vector quantized representations, thus it does not meet the inclusion criteria focusing on audio tokenization methods and discrete audio representations for language or sequence modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Reimagining Speech: A Scoping Review of Deep Learning-Powered Voice Conversion",
    "abstract": "Research on deep learning-powered voice conversion (VC) in speech-to-speech scenarios is getting increasingly popular. Although many of the works in the field of voice conversion share a common global pipeline, there is a considerable diversity in the underlying structures, methods, and neural sub-blocks used across research efforts. Thus, obtaining a comprehensive understanding of the reasons behind the choice of the different methods in the voice conversion pipeline can be challenging, and the actual hurdles in the proposed solutions are often unclear. To shed light on these aspects, this paper presents a scoping review that explores the use of deep learning in speech analysis, synthesis, and disentangled speech representation learning within modern voice conversion systems. We screened 621 publications from more than 38 different venues between the years 2017 and 2023, followed by an in-depth review of a final database consisting of 123 eligible studies. Based on the review, we summarise the most frequently used approaches to voice conversion based on deep learning and highlight common pitfalls within the community. Lastly, we condense the knowledge gathered, identify main challenges and provide recommendations for future research directions.",
    "metadata": {
      "arxiv_id": "2311.08104",
      "title": "Reimagining Speech: A Scoping Review of Deep Learning-Powered Voice Conversion",
      "summary": "Research on deep learning-powered voice conversion (VC) in speech-to-speech scenarios is getting increasingly popular. Although many of the works in the field of voice conversion share a common global pipeline, there is a considerable diversity in the underlying structures, methods, and neural sub-blocks used across research efforts. Thus, obtaining a comprehensive understanding of the reasons behind the choice of the different methods in the voice conversion pipeline can be challenging, and the actual hurdles in the proposed solutions are often unclear. To shed light on these aspects, this paper presents a scoping review that explores the use of deep learning in speech analysis, synthesis, and disentangled speech representation learning within modern voice conversion systems. We screened 621 publications from more than 38 different venues between the years 2017 and 2023, followed by an in-depth review of a final database consisting of 123 eligible studies. Based on the review, we summarise the most frequently used approaches to voice conversion based on deep learning and highlight common pitfalls within the community. Lastly, we condense the knowledge gathered, identify main challenges and provide recommendations for future research directions.",
      "authors": [
        "Anders R. Bargum",
        "Stefania Serafin",
        "Cumhur Erkut"
      ],
      "published": "2023-11-14T12:03:46Z",
      "updated": "2023-11-14T12:03:46Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.08104v1",
      "landing_url": "https://arxiv.org/abs/2311.08104v1",
      "doi": null
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is a scoping review on deep learning-powered voice conversion and disentangled speech representations, but it does not focus on discrete audio tokens nor provide methodological details on token generation/quantization, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is a scoping review on deep learning-powered voice conversion and disentangled speech representations, but it does not focus on discrete audio tokens nor provide methodological details on token generation/quantization, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper is a scoping review on deep learning-powered voice conversion, focusing on methods, neural architectures, and challenges in the field rather than specifically on the design or use of discrete audio tokens (such as VQ/RVQ or semantic disentanglement of tokens) as defined in the inclusion criteria. It lacks clear detail on token generation processes or use of discrete tokens as core contributions for sequence modeling or language models. Therefore, it does not meet the inclusion criteria focused on discrete audio token design and use, and no evidence suggests it falls within the exclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper is a scoping review on deep learning-powered voice conversion, focusing on methods, neural architectures, and challenges in the field rather than specifically on the design or use of discrete audio tokens (such as VQ/RVQ or semantic disentanglement of tokens) as defined in the inclusion criteria. It lacks clear detail on token generation processes or use of discrete tokens as core contributions for sequence modeling or language models. Therefore, it does not meet the inclusion criteria focused on discrete audio token design and use, and no evidence suggests it falls within the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification",
    "abstract": "Advancements in clinical treatment are increasingly constrained by the limitations of supervised learning techniques, which depend heavily on large volumes of annotated data. The annotation process is not only costly but also demands substantial time from clinical specialists. Addressing this issue, we introduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging) pipeline, a novel approach that leverages advancements in self-supervised and semi-supervised learning. These techniques engage in auxiliary tasks that do not require labeling, thus simplifying the scaling of machine supervision compared to fully-supervised methods. Our study benchmarks these techniques on three distinct medical imaging datasets to evaluate their effectiveness in classification and segmentation tasks. Notably, we observed that self supervised learning significantly surpassed the performance of supervised methods in the classification of all evaluated datasets. Remarkably, the semi-supervised approach demonstrated superior outcomes in segmentation, outperforming fully-supervised methods while using 50% fewer labels across all datasets. In line with our commitment to contributing to the scientific community, we have made the S4MI code openly accessible, allowing for broader application and further development of these methods.",
    "metadata": {
      "arxiv_id": "2311.10319",
      "title": "Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification",
      "summary": "Advancements in clinical treatment are increasingly constrained by the limitations of supervised learning techniques, which depend heavily on large volumes of annotated data. The annotation process is not only costly but also demands substantial time from clinical specialists. Addressing this issue, we introduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging) pipeline, a novel approach that leverages advancements in self-supervised and semi-supervised learning. These techniques engage in auxiliary tasks that do not require labeling, thus simplifying the scaling of machine supervision compared to fully-supervised methods. Our study benchmarks these techniques on three distinct medical imaging datasets to evaluate their effectiveness in classification and segmentation tasks. Notably, we observed that self supervised learning significantly surpassed the performance of supervised methods in the classification of all evaluated datasets. Remarkably, the semi-supervised approach demonstrated superior outcomes in segmentation, outperforming fully-supervised methods while using 50% fewer labels across all datasets. In line with our commitment to contributing to the scientific community, we have made the S4MI code openly accessible, allowing for broader application and further development of these methods.",
      "authors": [
        "Pranav Singh",
        "Raviteja Chukkapalli",
        "Shravan Chaudhari",
        "Luoyao Chen",
        "Mei Chen",
        "Jinqian Pan",
        "Craig Smuda",
        "Jacopo Cirrone"
      ],
      "published": "2023-11-17T04:04:29Z",
      "updated": "2024-05-17T17:42:30Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.10319v6",
      "landing_url": "https://arxiv.org/abs/2311.10319v6",
      "doi": "https://doi.org/10.1038/s41598-024-61822-9"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Topic mismatch: inclusion criteria require research on discrete audio tokens and tokenization methods for audio; the item is about medical image segmentation with SSL/SS, with no discrete audio token concept. Therefore it fails inclusion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Topic mismatch: inclusion criteria require research on discrete audio tokens and tokenization methods for audio; the item is about medical image segmentation with SSL/SS, with no discrete audio token concept. Therefore it fails inclusion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on medical image segmentation and classification using self-supervised and semi-supervised learning techniques, which does not involve discrete audio tokens or any form of audio signal processing, tokenization, or language modeling related to audio as required by the inclusion criteria. It also lacks any mention of vector quantization or token generation processes for audio, falling outside the defined thematic scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on medical image segmentation and classification using self-supervised and semi-supervised learning techniques, which does not involve discrete audio tokens or any form of audio signal processing, tokenization, or language modeling related to audio as required by the inclusion criteria. It also lacks any mention of vector quantization or token generation processes for audio, falling outside the defined thematic scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Utilizing VQ-VAE for End-to-End Health Indicator Generation in Predicting Rolling Bearing RUL",
    "abstract": "The prediction of the remaining useful life (RUL) of rolling bearings is a pivotal issue in industrial production. A crucial approach to tackling this issue involves transforming vibration signals into health indicators (HI) to aid model training. This paper presents an end-to-end HI construction method, vector quantised variational autoencoder (VQ-VAE), which addresses the need for dimensionality reduction of latent variables in traditional unsupervised learning methods such as autoencoder. Moreover, concerning the inadequacy of traditional statistical metrics in reflecting curve fluctuations accurately, two novel statistical metrics, mean absolute distance (MAD) and mean variance (MV), are introduced. These metrics accurately depict the fluctuation patterns in the curves, thereby indicating the model's accuracy in discerning similar features. On the PMH2012 dataset, methods employing VQ-VAE for label construction achieved lower values for MAD and MV. Furthermore, the ASTCN prediction model trained with VQ-VAE labels demonstrated commendable performance, attaining the lowest values for MAD and MV.",
    "metadata": {
      "arxiv_id": "2311.10525",
      "title": "Utilizing VQ-VAE for End-to-End Health Indicator Generation in Predicting Rolling Bearing RUL",
      "summary": "The prediction of the remaining useful life (RUL) of rolling bearings is a pivotal issue in industrial production. A crucial approach to tackling this issue involves transforming vibration signals into health indicators (HI) to aid model training. This paper presents an end-to-end HI construction method, vector quantised variational autoencoder (VQ-VAE), which addresses the need for dimensionality reduction of latent variables in traditional unsupervised learning methods such as autoencoder. Moreover, concerning the inadequacy of traditional statistical metrics in reflecting curve fluctuations accurately, two novel statistical metrics, mean absolute distance (MAD) and mean variance (MV), are introduced. These metrics accurately depict the fluctuation patterns in the curves, thereby indicating the model's accuracy in discerning similar features. On the PMH2012 dataset, methods employing VQ-VAE for label construction achieved lower values for MAD and MV. Furthermore, the ASTCN prediction model trained with VQ-VAE labels demonstrated commendable performance, attaining the lowest values for MAD and MV.",
      "authors": [
        "Junliang Wang",
        "Qinghua Zhang",
        "Guanhua Zhu",
        "Guoxi Sun"
      ],
      "published": "2023-11-17T13:45:31Z",
      "updated": "2023-11-17T13:45:31Z",
      "categories": [
        "cs.LG",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.10525v1",
      "landing_url": "https://arxiv.org/abs/2311.10525v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.10525"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item uses VQ-VAE for health indicator generation in rolling bearing RUL prediction (mechanical signals), which does not align with the discrete audio token design focus defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item uses VQ-VAE for health indicator generation in rolling bearing RUL prediction (mechanical signals), which does not align with the discrete audio token design focus defined in the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on using VQ-VAE for generating health indicators from vibration signals to predict the remaining useful life of rolling bearings, which is an engineering predictive maintenance task rather than audio signal processing. Although it uses Vector Quantization (VQ), it does not concern discrete audio tokens derived from continuous speech, music, or general audio signals for audio language modeling or audio understanding and generation tasks as required by the inclusion criteria. The core contribution is end-to-end health indicator generation, not discrete audio token design for linguistic modeling or semantic disambiguation in audio. Therefore, it does not meet the inclusion criteria regarding discrete audio token creation and usage, and it is unrelated to the audio domain as defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on using VQ-VAE for generating health indicators from vibration signals to predict the remaining useful life of rolling bearings, which is an engineering predictive maintenance task rather than audio signal processing. Although it uses Vector Quantization (VQ), it does not concern discrete audio tokens derived from continuous speech, music, or general audio signals for audio language modeling or audio understanding and generation tasks as required by the inclusion criteria. The core contribution is end-to-end health indicator generation, not discrete audio token design for linguistic modeling or semantic disambiguation in audio. Therefore, it does not meet the inclusion criteria regarding discrete audio token creation and usage, and it is unrelated to the audio domain as defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Structure-Aware Sparse-View X-ray 3D Reconstruction",
    "abstract": "X-ray, known for its ability to reveal internal structures of objects, is expected to provide richer information for 3D reconstruction than visible light. Yet, existing neural radiance fields (NeRF) algorithms overlook this important nature of X-ray, leading to their limitations in capturing structural contents of imaged objects. In this paper, we propose a framework, Structure-Aware X-ray Neural Radiodensity Fields (SAX-NeRF), for sparse-view X-ray 3D reconstruction. Firstly, we design a Line Segment-based Transformer (Lineformer) as the backbone of SAX-NeRF. Linefomer captures internal structures of objects in 3D space by modeling the dependencies within each line segment of an X-ray. Secondly, we present a Masked Local-Global (MLG) ray sampling strategy to extract contextual and geometric information in 2D projection. Plus, we collect a larger-scale dataset X3D covering wider X-ray applications. Experiments on X3D show that SAX-NeRF surpasses previous NeRF-based methods by 12.56 and 2.49 dB on novel view synthesis and CT reconstruction. Code, models, and data are released at https://github.com/caiyuanhao1998/SAX-NeRF",
    "metadata": {
      "arxiv_id": "2311.10959",
      "title": "Structure-Aware Sparse-View X-ray 3D Reconstruction",
      "summary": "X-ray, known for its ability to reveal internal structures of objects, is expected to provide richer information for 3D reconstruction than visible light. Yet, existing neural radiance fields (NeRF) algorithms overlook this important nature of X-ray, leading to their limitations in capturing structural contents of imaged objects. In this paper, we propose a framework, Structure-Aware X-ray Neural Radiodensity Fields (SAX-NeRF), for sparse-view X-ray 3D reconstruction. Firstly, we design a Line Segment-based Transformer (Lineformer) as the backbone of SAX-NeRF. Linefomer captures internal structures of objects in 3D space by modeling the dependencies within each line segment of an X-ray. Secondly, we present a Masked Local-Global (MLG) ray sampling strategy to extract contextual and geometric information in 2D projection. Plus, we collect a larger-scale dataset X3D covering wider X-ray applications. Experiments on X3D show that SAX-NeRF surpasses previous NeRF-based methods by 12.56 and 2.49 dB on novel view synthesis and CT reconstruction. Code, models, and data are released at https://github.com/caiyuanhao1998/SAX-NeRF",
      "authors": [
        "Yuanhao Cai",
        "Jiahao Wang",
        "Alan Yuille",
        "Zongwei Zhou",
        "Angtian Wang"
      ],
      "published": "2023-11-18T03:39:02Z",
      "updated": "2024-03-23T17:36:19Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.10959v3",
      "landing_url": "https://arxiv.org/abs/2311.10959v3",
      "doi": "https://doi.org/10.48550/arXiv.2311.10959"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about Structure-Aware X-ray Neural Radiodensity Fields (SAX-NeRF) for sparse-view X-ray 3D reconstruction, using a Line Segment-based Transformer and 2D projection sampling; it deals with computer vision/medical imaging and NeRFs, not discrete audio token design or audio-language modeling, so it does not meet the inclusion criteria focused on discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about Structure-Aware X-ray Neural Radiodensity Fields (SAX-NeRF) for sparse-view X-ray 3D reconstruction, using a Line Segment-based Transformer and 2D projection sampling; it deals with computer vision/medical imaging and NeRFs, not discrete audio token design or audio-language modeling, so it does not meet the inclusion criteria focused on discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on a novel framework for sparse-view X-ray 3D reconstruction using neural radiodensity fields and does not involve any audio signal processing, discrete audio tokens, or related audio language modeling techniques. It does not meet the inclusion criteria related to discrete audio token generation or usage, nor does it provide any relevant methods for audio tokenization or sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on a novel framework for sparse-view X-ray 3D reconstruction using neural radiodensity fields and does not involve any audio signal processing, discrete audio tokens, or related audio language modeling techniques. It does not meet the inclusion criteria related to discrete audio token generation or usage, nor does it provide any relevant methods for audio tokenization or sequence modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Functional Diffusion",
    "abstract": "We propose a new class of generative diffusion models, called functional diffusion. In contrast to previous work, functional diffusion works on samples that are represented by functions with a continuous domain. Functional diffusion can be seen as an extension of classical diffusion models to an infinite-dimensional domain. Functional diffusion is very versatile as images, videos, audio, 3D shapes, deformations, \\etc, can be handled by the same framework with minimal changes. In addition, functional diffusion is especially suited for irregular data or data defined in non-standard domains. In our work, we derive the necessary foundations for functional diffusion and propose a first implementation based on the transformer architecture. We show generative results on complicated signed distance functions and deformation functions defined on 3D surfaces.",
    "metadata": {
      "arxiv_id": "2311.15435",
      "title": "Functional Diffusion",
      "summary": "We propose a new class of generative diffusion models, called functional diffusion. In contrast to previous work, functional diffusion works on samples that are represented by functions with a continuous domain. Functional diffusion can be seen as an extension of classical diffusion models to an infinite-dimensional domain. Functional diffusion is very versatile as images, videos, audio, 3D shapes, deformations, \\etc, can be handled by the same framework with minimal changes. In addition, functional diffusion is especially suited for irregular data or data defined in non-standard domains. In our work, we derive the necessary foundations for functional diffusion and propose a first implementation based on the transformer architecture. We show generative results on complicated signed distance functions and deformation functions defined on 3D surfaces.",
      "authors": [
        "Biao Zhang",
        "Peter Wonka"
      ],
      "published": "2023-11-26T21:35:34Z",
      "updated": "2023-11-26T21:35:34Z",
      "categories": [
        "cs.CV",
        "cs.GR",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.15435v1",
      "landing_url": "https://arxiv.org/abs/2311.15435v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.15435"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on functional diffusion over continuous domains and transformer-based implementations, not on discrete audio tokens or tokenization methods; thus it does not meet the discrete audio token inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on functional diffusion over continuous domains and transformer-based implementations, not on discrete audio tokens or tokenization methods; thus it does not meet the discrete audio token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a novel generative diffusion model working on functional continuous-domain data such as images, videos, audio, and 3D shapes, but it does not focus on discrete audio tokens or their generation processes involving quantization or tokenization mechanisms for audio representation suitable for sequence models, which is the main inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a novel generative diffusion model working on functional continuous-domain data such as images, videos, audio, and 3D shapes, but it does not focus on discrete audio tokens or their generation processes involving quantization or tokenization mechanisms for audio representation suitable for sequence models, which is the main inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Making Self-supervised Learning Robust to Spurious Correlation via Learning-speed Aware Sampling",
    "abstract": "Self-supervised learning (SSL) has emerged as a powerful technique for learning rich representations from unlabeled data. The data representations are able to capture many underlying attributes of data, and be useful in downstream prediction tasks. In real-world settings, spurious correlations between some attributes (e.g. race, gender and age) and labels for downstream tasks often exist, e.g. cancer is usually more prevalent among elderly patients. In this paper, we investigate SSL in the presence of spurious correlations and show that the SSL training loss can be minimized by capturing only a subset of the conspicuous features relevant to those sensitive attributes, despite the presence of other important predictive features for the downstream tasks. To address this issue, we investigate the learning dynamics of SSL and observe that the learning is slower for samples that conflict with such correlations (e.g. elder patients without cancer). Motivated by these findings, we propose a learning-speed aware SSL (LA-SSL) approach, in which we sample each training data with a probability that is inversely related to its learning speed. We evaluate LA-SSL on three datasets that exhibit spurious correlations between different attributes, demonstrating that it improves the robustness of pretrained representations on downstream classification tasks.",
    "metadata": {
      "arxiv_id": "2311.16361",
      "title": "Making Self-supervised Learning Robust to Spurious Correlation via Learning-speed Aware Sampling",
      "summary": "Self-supervised learning (SSL) has emerged as a powerful technique for learning rich representations from unlabeled data. The data representations are able to capture many underlying attributes of data, and be useful in downstream prediction tasks. In real-world settings, spurious correlations between some attributes (e.g. race, gender and age) and labels for downstream tasks often exist, e.g. cancer is usually more prevalent among elderly patients. In this paper, we investigate SSL in the presence of spurious correlations and show that the SSL training loss can be minimized by capturing only a subset of the conspicuous features relevant to those sensitive attributes, despite the presence of other important predictive features for the downstream tasks. To address this issue, we investigate the learning dynamics of SSL and observe that the learning is slower for samples that conflict with such correlations (e.g. elder patients without cancer). Motivated by these findings, we propose a learning-speed aware SSL (LA-SSL) approach, in which we sample each training data with a probability that is inversely related to its learning speed. We evaluate LA-SSL on three datasets that exhibit spurious correlations between different attributes, demonstrating that it improves the robustness of pretrained representations on downstream classification tasks.",
      "authors": [
        "Weicheng Zhu",
        "Sheng Liu",
        "Carlos Fernandez-Granda",
        "Narges Razavian"
      ],
      "published": "2023-11-27T22:52:45Z",
      "updated": "2023-11-29T23:19:30Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.16361v2",
      "landing_url": "https://arxiv.org/abs/2311.16361v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.16361"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on self-supervised learning robustness to spurious correlations and learning-speed aware sampling, not on discrete audio tokens or discretized audio representations; thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on self-supervised learning robustness to spurious correlations and learning-speed aware sampling, not on discrete audio tokens or discretized audio representations; thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on improving self-supervised learning robustness to spurious correlations in data, without any mention of discrete audio tokens, vector quantization, or audio representation discretization methods. It does not discuss generating or using discrete audio tokens for language or sequence models, nor provide any details on token generation or quantization. Thus, it does not meet the inclusion criteria centered on discrete audio tokens, and the study's content is unrelated to the project scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on improving self-supervised learning robustness to spurious correlations in data, without any mention of discrete audio tokens, vector quantization, or audio representation discretization methods. It does not discuss generating or using discrete audio tokens for language or sequence models, nor provide any details on token generation or quantization. Thus, it does not meet the inclusion criteria centered on discrete audio tokens, and the study's content is unrelated to the project scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "GIVT: Generative Infinite-Vocabulary Transformers",
    "abstract": "We introduce Generative Infinite-Vocabulary Transformers (GIVT) which generate vector sequences with real-valued entries, instead of discrete tokens from a finite vocabulary. To this end, we propose two surprisingly simple modifications to decoder-only transformers: 1) at the input, we replace the finite-vocabulary lookup table with a linear projection of the input vectors; and 2) at the output, we replace the logits prediction (usually mapped to a categorical distribution) with the parameters of a multivariate Gaussian mixture model. Inspired by the image-generation paradigm of VQ-GAN and MaskGIT, where transformers are used to model the discrete latent sequences of a VQ-VAE, we use GIVT to model the unquantized real-valued latent sequences of a $β$-VAE. In class-conditional image generation GIVT outperforms VQ-GAN (and improved variants thereof) as well as MaskGIT, and achieves performance competitive with recent latent diffusion models. Finally, we obtain strong results outside of image generation when applying GIVT to panoptic segmentation and depth estimation with a VAE variant of the UViM framework.",
    "metadata": {
      "arxiv_id": "2312.02116",
      "title": "GIVT: Generative Infinite-Vocabulary Transformers",
      "summary": "We introduce Generative Infinite-Vocabulary Transformers (GIVT) which generate vector sequences with real-valued entries, instead of discrete tokens from a finite vocabulary. To this end, we propose two surprisingly simple modifications to decoder-only transformers: 1) at the input, we replace the finite-vocabulary lookup table with a linear projection of the input vectors; and 2) at the output, we replace the logits prediction (usually mapped to a categorical distribution) with the parameters of a multivariate Gaussian mixture model. Inspired by the image-generation paradigm of VQ-GAN and MaskGIT, where transformers are used to model the discrete latent sequences of a VQ-VAE, we use GIVT to model the unquantized real-valued latent sequences of a $β$-VAE. In class-conditional image generation GIVT outperforms VQ-GAN (and improved variants thereof) as well as MaskGIT, and achieves performance competitive with recent latent diffusion models. Finally, we obtain strong results outside of image generation when applying GIVT to panoptic segmentation and depth estimation with a VAE variant of the UViM framework.",
      "authors": [
        "Michael Tschannen",
        "Cian Eastwood",
        "Fabian Mentzer"
      ],
      "published": "2023-12-04T18:48:02Z",
      "updated": "2024-07-17T16:32:09Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.02116v4",
      "landing_url": "https://arxiv.org/abs/2312.02116v4",
      "doi": "https://doi.org/10.48550/arXiv.2312.02116"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item targets Generative Infinite-Vocabulary Transformers for CV tasks and real-valued latent sequences, with no mention of discrete audio tokens or audio tokenization; thus it does not satisfy the discrete audio token criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item targets Generative Infinite-Vocabulary Transformers for CV tasks and real-valued latent sequences, with no mention of discrete audio tokens or audio tokenization; thus it does not satisfy the discrete audio token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces Generative Infinite-Vocabulary Transformers (GIVT) focusing on generating real-valued vector sequences rather than discrete tokens from a finite vocabulary. The main focus is on image generation and related vision tasks, without any mention or indication of discrete audio token design, quantization mechanisms for audio, or application to audio language modeling or audio sequence tasks. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and their generation methods, nor does it address the specification of token formation processes for audio signals.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study introduces Generative Infinite-Vocabulary Transformers (GIVT) focusing on generating real-valued vector sequences rather than discrete tokens from a finite vocabulary. The main focus is on image generation and related vision tasks, without any mention or indication of discrete audio token design, quantization mechanisms for audio, or application to audio language modeling or audio sequence tasks. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and their generation methods, nor does it address the specification of token formation processes for audio signals.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models",
    "abstract": "Recently, diffusion models have made remarkable progress in text-to-image (T2I) generation, synthesizing images with high fidelity and diverse contents. Despite this advancement, latent space smoothness within diffusion models remains largely unexplored. Smooth latent spaces ensure that a perturbation on an input latent corresponds to a steady change in the output image. This property proves beneficial in downstream tasks, including image interpolation, inversion, and editing. In this work, we expose the non-smoothness of diffusion latent spaces by observing noticeable visual fluctuations resulting from minor latent variations. To tackle this issue, we propose Smooth Diffusion, a new category of diffusion models that can be simultaneously high-performing and smooth. Specifically, we introduce Step-wise Variation Regularization to enforce the proportion between the variations of an arbitrary input latent and that of the output image is a constant at any diffusion training step. In addition, we devise an interpolation standard deviation (ISTD) metric to effectively assess the latent space smoothness of a diffusion model. Extensive quantitative and qualitative experiments demonstrate that Smooth Diffusion stands out as a more desirable solution not only in T2I generation but also across various downstream tasks. Smooth Diffusion is implemented as a plug-and-play Smooth-LoRA to work with various community models. Code is available at https://github.com/SHI-Labs/Smooth-Diffusion.",
    "metadata": {
      "arxiv_id": "2312.04410",
      "title": "Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models",
      "summary": "Recently, diffusion models have made remarkable progress in text-to-image (T2I) generation, synthesizing images with high fidelity and diverse contents. Despite this advancement, latent space smoothness within diffusion models remains largely unexplored. Smooth latent spaces ensure that a perturbation on an input latent corresponds to a steady change in the output image. This property proves beneficial in downstream tasks, including image interpolation, inversion, and editing. In this work, we expose the non-smoothness of diffusion latent spaces by observing noticeable visual fluctuations resulting from minor latent variations. To tackle this issue, we propose Smooth Diffusion, a new category of diffusion models that can be simultaneously high-performing and smooth. Specifically, we introduce Step-wise Variation Regularization to enforce the proportion between the variations of an arbitrary input latent and that of the output image is a constant at any diffusion training step. In addition, we devise an interpolation standard deviation (ISTD) metric to effectively assess the latent space smoothness of a diffusion model. Extensive quantitative and qualitative experiments demonstrate that Smooth Diffusion stands out as a more desirable solution not only in T2I generation but also across various downstream tasks. Smooth Diffusion is implemented as a plug-and-play Smooth-LoRA to work with various community models. Code is available at https://github.com/SHI-Labs/Smooth-Diffusion.",
      "authors": [
        "Jiayi Guo",
        "Xingqian Xu",
        "Yifan Pu",
        "Zanlin Ni",
        "Chaofei Wang",
        "Manushree Vasu",
        "Shiji Song",
        "Gao Huang",
        "Humphrey Shi"
      ],
      "published": "2023-12-07T16:26:23Z",
      "updated": "2023-12-07T16:26:23Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.04410v1",
      "landing_url": "https://arxiv.org/abs/2312.04410v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.04410"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item investigates diffusion model latent-space smoothness for text-to-image generation, not discrete audio tokens or tokenization designed for audio-language modeling, thus it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item investigates diffusion model latent-space smoothness for text-to-image generation, not discrete audio tokens or tokenization designed for audio-language modeling, thus it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on diffusion models for image generation and improving latent space smoothness, without any relation to discrete audio tokens, audio signal quantization, or audio language modeling as required by the inclusion criteria. It does not discuss any token generation process for audio, semantic or acoustic disentanglement, or audio codec methods. Thus, it fails to meet the essential inclusion requirements and does not fall under the topic of discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on diffusion models for image generation and improving latent space smoothness, without any relation to discrete audio tokens, audio signal quantization, or audio language modeling as required by the inclusion criteria. It does not discuss any token generation process for audio, semantic or acoustic disentanglement, or audio codec methods. Thus, it fails to meet the essential inclusion requirements and does not fall under the topic of discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "An Experimental Study: Assessing the Combined Framework of WavLM and BEST-RQ for Text-to-Speech Synthesis",
    "abstract": "We propose a new model architecture specifically suited for text-to-speech (TTS) models. We combine WavLM, a pre-trained self-supervised learning (SSL) speech model, and the BEST-RQ vector quantization framework. We assess the extent to which the more task-agnostic WavLM, coupled with the superior suitability of the simplistic BEST-RQ framework for a wider array of downstream tasks, yields favorable outcomes. Experiments on the LibriSpeech dataset with SUPERB benchmarking assert that the proposed model significantly underperforms. We speculate the underlying reason for this performance is related to the difference between featurizing raw audio waveforms and spectrograms with a quantizer. We discuss the limitations of this approach to better guide future advancements in TTS.",
    "metadata": {
      "arxiv_id": "2312.05415",
      "title": "An Experimental Study: Assessing the Combined Framework of WavLM and BEST-RQ for Text-to-Speech Synthesis",
      "summary": "We propose a new model architecture specifically suited for text-to-speech (TTS) models. We combine WavLM, a pre-trained self-supervised learning (SSL) speech model, and the BEST-RQ vector quantization framework. We assess the extent to which the more task-agnostic WavLM, coupled with the superior suitability of the simplistic BEST-RQ framework for a wider array of downstream tasks, yields favorable outcomes. Experiments on the LibriSpeech dataset with SUPERB benchmarking assert that the proposed model significantly underperforms. We speculate the underlying reason for this performance is related to the difference between featurizing raw audio waveforms and spectrograms with a quantizer. We discuss the limitations of this approach to better guide future advancements in TTS.",
      "authors": [
        "Via Nielson",
        "Steven Hillis"
      ],
      "published": "2023-12-08T23:59:25Z",
      "updated": "2023-12-08T23:59:25Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.05415v1",
      "landing_url": "https://arxiv.org/abs/2312.05415v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.05415"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study centers on discrete audio tokens via BEST-RQ quantization in a TTS setting with a pre-trained SSL model (WavLM), addressing token design/usage for downstream generation, and appears to provide experimental methodology beyond demos, aligning with the inclusion criteria.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The study centers on discrete audio tokens via BEST-RQ quantization in a TTS setting with a pre-trained SSL model (WavLM), addressing token design/usage for downstream generation, and appears to provide experimental methodology beyond demos, aligning with the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on combining WavLM and BEST-RQ frameworks in a text-to-speech synthesis setting. While BEST-RQ relates to vector quantization, the abstract indicates the model underperforms and mainly discusses limitations rather than clearly detailing the discrete token generation process or their use for sequence modeling in downstream tasks. The abstract lacks clear explanations of the token generation flow, quantization details, or explicit emphasis on discrete audio tokens as a core contribution. Therefore, it does not sufficiently meet the inclusion criteria regarding discrete token design or usage despite involving vector quantization. Hence, inclusion is not strongly justified.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on combining WavLM and BEST-RQ frameworks in a text-to-speech synthesis setting. While BEST-RQ relates to vector quantization, the abstract indicates the model underperforms and mainly discusses limitations rather than clearly detailing the discrete token generation process or their use for sequence modeling in downstream tasks. The abstract lacks clear explanations of the token generation flow, quantization details, or explicit emphasis on discrete audio tokens as a core contribution. Therefore, it does not sufficiently meet the inclusion criteria regarding discrete token design or usage despite involving vector quantization. Hence, inclusion is not strongly justified.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers were split: one judged it aligned because the study uses BEST-RQ vector quantization with WavLM in a TTS setting, while the other flagged that the abstract lacks clear token-generation/quantization details and explicit use of discrete tokens for sequence modeling. The abstract mentions BEST-RQ quantization but gives no methodological specifics (token formation flow, codebook/layer design, quantization/training/inference settings) nor positions discrete tokens as the primary contribution for downstream sequence/language modeling; it mainly reports negative results and discussion. Given the absence of describable, reviewable technical detail and no evidence that discrete audio tokens are a core, well-specified contribution, this item fails the inclusion criteria and is better excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers were split: one judged it aligned because the study uses BEST-RQ vector quantization with WavLM in a TTS setting, while the other flagged that the abstract lacks clear token-generation/quantization details and explicit use of discrete tokens for sequence modeling. The abstract mentions BEST-RQ quantization but gives no methodological specifics (token formation flow, codebook/layer design, quantization/training/inference settings) nor positions discrete tokens as the primary contribution for downstream sequence/language modeling; it mainly reports negative results and discussion. Given the absence of describable, reviewable technical detail and no evidence that discrete audio tokens are a core, well-specified contribution, this item fails the inclusion criteria and is better excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Instability and quantization in quantum hydrodynamics",
    "abstract": "In this short paper, we show how a quantum nonlocal effect of far-apart wavepackets in the Schrodinger picture of wavefunctions is replaced by a local instability problem when considering the hydrodynamical formulation of quantum mechanics, known as the Madelung picture. As a second result, we show how the Madelung equations describe quantized energies without any external quantization conditions.",
    "metadata": {
      "arxiv_id": "2312.06381",
      "title": "Instability and quantization in quantum hydrodynamics",
      "summary": "In this short paper, we show how a quantum nonlocal effect of far-apart wavepackets in the Schrodinger picture of wavefunctions is replaced by a local instability problem when considering the hydrodynamical formulation of quantum mechanics, known as the Madelung picture. As a second result, we show how the Madelung equations describe quantized energies without any external quantization conditions.",
      "authors": [
        "Yakir Aharonov",
        "Tomer Shushi"
      ],
      "published": "2023-12-11T13:44:48Z",
      "updated": "2023-12-11T13:44:48Z",
      "categories": [
        "quant-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.06381v1",
      "landing_url": "https://arxiv.org/abs/2312.06381v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.06381"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses instability in the Madelung (quantum hydrodynamics) formulation and quantized energies, with no focus on discrete audio token design, vector quantization, or audio tokenization methods suitable for language-model-style processing; thus it fails to meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses instability in the Madelung (quantum hydrodynamics) formulation and quantized energies, with no focus on discrete audio token design, vector quantization, or audio tokenization methods suitable for language-model-style processing; thus it fails to meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on quantum hydrodynamics related to quantum mechanics and energy quantization, without any mention of discrete audio tokens, vector quantization for audio, or related methods in audio representation or processing as required by the inclusion criteria. This study does not address audio token generation, encoding, or audio sequence modeling, and thus does not meet the inclusion criteria nor matches the exclusion criteria regarding token methods and audio applications.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on quantum hydrodynamics related to quantum mechanics and energy quantization, without any mention of discrete audio tokens, vector quantization for audio, or related methods in audio representation or processing as required by the inclusion criteria. This study does not address audio token generation, encoding, or audio sequence modeling, and thus does not meet the inclusion criteria nor matches the exclusion criteria regarding token methods and audio applications.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Bitrate Ladder Construction using Visual Information Fidelity",
    "abstract": "Recently proposed perceptually optimized per-title video encoding methods provide better BD-rate savings than fixed bitrate-ladder approaches that have been employed in the past. However, a disadvantage of per-title encoding is that it requires significant time and energy to compute bitrate ladders. Over the past few years, a variety of methods have been proposed to construct optimal bitrate ladders including using low-level features to predict cross-over bitrates, optimal resolutions for each bitrate, predicting visual quality, etc. Here, we deploy features drawn from Visual Information Fidelity (VIF) (VIF features) extracted from uncompressed videos to predict the visual quality (VMAF) of compressed videos. We present multiple VIF feature sets extracted from different scales and subbands of a video to tackle the problem of bitrate ladder construction. Comparisons are made against a fixed bitrate ladder and a bitrate ladder obtained from exhaustive encoding using Bjontegaard delta metrics.",
    "metadata": {
      "arxiv_id": "2312.07780",
      "title": "Bitrate Ladder Construction using Visual Information Fidelity",
      "summary": "Recently proposed perceptually optimized per-title video encoding methods provide better BD-rate savings than fixed bitrate-ladder approaches that have been employed in the past. However, a disadvantage of per-title encoding is that it requires significant time and energy to compute bitrate ladders. Over the past few years, a variety of methods have been proposed to construct optimal bitrate ladders including using low-level features to predict cross-over bitrates, optimal resolutions for each bitrate, predicting visual quality, etc. Here, we deploy features drawn from Visual Information Fidelity (VIF) (VIF features) extracted from uncompressed videos to predict the visual quality (VMAF) of compressed videos. We present multiple VIF feature sets extracted from different scales and subbands of a video to tackle the problem of bitrate ladder construction. Comparisons are made against a fixed bitrate ladder and a bitrate ladder obtained from exhaustive encoding using Bjontegaard delta metrics.",
      "authors": [
        "Krishna Srikar Durbha",
        "Hassene Tmar",
        "Cosmin Stejerean",
        "Ioannis Katsavounidis",
        "Alan C. Bovik"
      ],
      "published": "2023-12-12T22:48:50Z",
      "updated": "2024-02-29T03:44:59Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.07780v2",
      "landing_url": "https://arxiv.org/abs/2312.07780v2",
      "doi": "https://doi.org/10.1109/PCS60826.2024.10566405"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on video bitrate ladder construction using Visual Information Fidelity to predict VMAF, not on discrete audio tokens or token-based audio representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on video bitrate ladder construction using Visual Information Fidelity to predict VMAF, not on discrete audio tokens or token-based audio representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on constructing bitrate ladders for video encoding using Visual Information Fidelity features to optimize video compression, which is unrelated to discrete audio tokens or any audio-related tokenization method that transforms continuous audio signals into discrete representations for language or sequence modeling. Hence, it does not meet the inclusion criteria centered on discrete audio token methods, and has no mention of audio token generation processes or their applications.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on constructing bitrate ladders for video encoding using Visual Information Fidelity features to optimize video compression, which is unrelated to discrete audio tokens or any audio-related tokenization method that transforms continuous audio signals into discrete representations for language or sequence modeling. Hence, it does not meet the inclusion criteria centered on discrete audio token methods, and has no mention of audio token generation processes or their applications.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Audio Deepfake Detection with Self-Supervised WavLM and Multi-Fusion Attentive Classifier",
    "abstract": "With the rapid development of speech synthesis and voice conversion technologies, Audio Deepfake has become a serious threat to the Automatic Speaker Verification (ASV) system. Numerous countermeasures are proposed to detect this type of attack. In this paper, we report our efforts to combine the self-supervised WavLM model and Multi-Fusion Attentive classifier for audio deepfake detection. Our method exploits the WavLM model to extract features that are more conducive to spoofing detection for the first time. Then, we propose a novel Multi-Fusion Attentive (MFA) classifier based on the Attentive Statistics Pooling (ASP) layer. The MFA captures the complementary information of audio features at both time and layer levels. Experiments demonstrate that our methods achieve state-of-the-art results on the ASVspoof 2021 DF set and provide competitive results on the ASVspoof 2019 and 2021 LA set.",
    "metadata": {
      "arxiv_id": "2312.08089",
      "title": "Audio Deepfake Detection with Self-Supervised WavLM and Multi-Fusion Attentive Classifier",
      "summary": "With the rapid development of speech synthesis and voice conversion technologies, Audio Deepfake has become a serious threat to the Automatic Speaker Verification (ASV) system. Numerous countermeasures are proposed to detect this type of attack. In this paper, we report our efforts to combine the self-supervised WavLM model and Multi-Fusion Attentive classifier for audio deepfake detection. Our method exploits the WavLM model to extract features that are more conducive to spoofing detection for the first time. Then, we propose a novel Multi-Fusion Attentive (MFA) classifier based on the Attentive Statistics Pooling (ASP) layer. The MFA captures the complementary information of audio features at both time and layer levels. Experiments demonstrate that our methods achieve state-of-the-art results on the ASVspoof 2021 DF set and provide competitive results on the ASVspoof 2019 and 2021 LA set.",
      "authors": [
        "Yinlin Guo",
        "Haofan Huang",
        "Xi Chen",
        "He Zhao",
        "Yuehai Wang"
      ],
      "published": "2023-12-13T12:09:15Z",
      "updated": "2024-01-10T03:01:42Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.08089v2",
      "landing_url": "https://arxiv.org/abs/2312.08089v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.08089"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on audio deepfake detection using self-supervised WavLM features and a Multi-Fusion Attentive classifier, without any discussion of discrete audio tokens or token-based representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on audio deepfake detection using self-supervised WavLM features and a Multi-Fusion Attentive classifier, without any discussion of discrete audio tokens or token-based representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract focuses on audio deepfake detection using self-supervised WavLM features and a multi-fusion attentive classifier. However, it does not describe the creation or use of discrete audio tokens through vector quantization or related discrete tokenization methods, nor does it include details on token generation processes or their application in sequence or language models. It primarily addresses anti-spoofing detection without meeting the key inclusion criteria of discrete audio token design and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract focuses on audio deepfake detection using self-supervised WavLM features and a multi-fusion attentive classifier. However, it does not describe the creation or use of discrete audio tokens through vector quantization or related discrete tokenization methods, nor does it include details on token generation processes or their application in sequence or language models. It primarily addresses anti-spoofing detection without meeting the key inclusion criteria of discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention",
    "abstract": "Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary unseen target speaker timbre, while keeping the linguistic content unchanged. Although the voice of generated speech can be controlled by providing the speaker embedding of the target speaker, the speaker similarity still lags behind the ground truth recordings. In this paper, we propose SEF-VC, a speaker embedding free voice conversion model, which is designed to learn and incorporate speaker timbre from reference speech via a powerful position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic tokens in a non-autoregressive manner. The concise design of SEF-VC enhances its training stability and voice conversion performance. Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality speech with better similarity to target reference than strong zero-shot VC baselines, even for very short reference speeches.",
    "metadata": {
      "arxiv_id": "2312.08676",
      "title": "SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention",
      "summary": "Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary unseen target speaker timbre, while keeping the linguistic content unchanged. Although the voice of generated speech can be controlled by providing the speaker embedding of the target speaker, the speaker similarity still lags behind the ground truth recordings. In this paper, we propose SEF-VC, a speaker embedding free voice conversion model, which is designed to learn and incorporate speaker timbre from reference speech via a powerful position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic tokens in a non-autoregressive manner. The concise design of SEF-VC enhances its training stability and voice conversion performance. Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality speech with better similarity to target reference than strong zero-shot VC baselines, even for very short reference speeches.",
      "authors": [
        "Junjie Li",
        "Yiwei Guo",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2023-12-14T06:26:55Z",
      "updated": "2024-01-30T14:11:29Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.08676v2",
      "landing_url": "https://arxiv.org/abs/2312.08676v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.08676"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on speaker-embedding-free VC using cross-attention and HuBERT semantic tokens, but it does not present a discrete audio token generation/quantization scheme or explicit token design as its core contribution.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on speaker-embedding-free VC using cross-attention and HuBERT semantic tokens, but it does not present a discrete audio token generation/quantization scheme or explicit token design as its core contribution.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on zero-shot voice conversion without relying on speaker embeddings, using a cross-attention mechanism and HuBERT semantic tokens for waveform reconstruction. However, it does not primarily propose or focus on discrete audio token generation or their use as core intermediate representations for language or sequence modeling, nor does it provide detailed methods on token discretization or quantization processes as required by the inclusion criteria. Therefore, it does not meet the key inclusion criteria related to discrete audio token design and application.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on zero-shot voice conversion without relying on speaker embeddings, using a cross-attention mechanism and HuBERT semantic tokens for waveform reconstruction. However, it does not primarily propose or focus on discrete audio token generation or their use as core intermediate representations for language or sequence modeling, nor does it provide detailed methods on token discretization or quantization processes as required by the inclusion criteria. Therefore, it does not meet the key inclusion criteria related to discrete audio token design and application.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Diffusion Reward: Learning Rewards via Conditional Video Diffusion",
    "abstract": "Learning rewards from expert videos offers an affordable and effective solution to specify the intended behaviors for reinforcement learning (RL) tasks. In this work, we propose Diffusion Reward, a novel framework that learns rewards from expert videos via conditional video diffusion models for solving complex visual RL problems. Our key insight is that lower generative diversity is exhibited when conditioning diffusion on expert trajectories. Diffusion Reward is accordingly formalized by the negative of conditional entropy that encourages productive exploration of expert behaviors. We show the efficacy of our method over robotic manipulation tasks in both simulation platforms and the real world with visual input. Moreover, Diffusion Reward can even solve unseen tasks successfully and effectively, largely surpassing baseline methods. Project page and code: https://diffusion-reward.github.io.",
    "metadata": {
      "arxiv_id": "2312.14134",
      "title": "Diffusion Reward: Learning Rewards via Conditional Video Diffusion",
      "summary": "Learning rewards from expert videos offers an affordable and effective solution to specify the intended behaviors for reinforcement learning (RL) tasks. In this work, we propose Diffusion Reward, a novel framework that learns rewards from expert videos via conditional video diffusion models for solving complex visual RL problems. Our key insight is that lower generative diversity is exhibited when conditioning diffusion on expert trajectories. Diffusion Reward is accordingly formalized by the negative of conditional entropy that encourages productive exploration of expert behaviors. We show the efficacy of our method over robotic manipulation tasks in both simulation platforms and the real world with visual input. Moreover, Diffusion Reward can even solve unseen tasks successfully and effectively, largely surpassing baseline methods. Project page and code: https://diffusion-reward.github.io.",
      "authors": [
        "Tao Huang",
        "Guangqi Jiang",
        "Yanjie Ze",
        "Huazhe Xu"
      ],
      "published": "2023-12-21T18:55:05Z",
      "updated": "2024-08-09T03:06:42Z",
      "categories": [
        "cs.LG",
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.14134v3",
      "landing_url": "https://arxiv.org/abs/2312.14134v3",
      "doi": "https://doi.org/10.48550/arXiv.2312.14134"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目專注於從專家影片中學習獎勵信號，使用條件式視頻 diffusion，與離散音訊 token 的設計與量化、語言模型/序列模型的離散表示無關，因此不符合主題定義的離散音訊 token 要求。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目專注於從專家影片中學習獎勵信號，使用條件式視頻 diffusion，與離散音訊 token 的設計與量化、語言模型/序列模型的離散表示無關，因此不符合主題定義的離散音訊 token 要求。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on learning rewards via conditional video diffusion models for reinforcement learning tasks, which does not involve discrete audio token design, generation, or modeling as defined in the inclusion criteria. It lacks any discussion or methodology related to converting continuous audio signals into discrete tokens or employing vector quantization techniques relevant to audio language modeling. Therefore, the study does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on learning rewards via conditional video diffusion models for reinforcement learning tasks, which does not involve discrete audio token design, generation, or modeling as defined in the inclusion criteria. It lacks any discussion or methodology related to converting continuous audio signals into discrete tokens or employing vector quantization techniques relevant to audio language modeling. Therefore, the study does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction",
    "abstract": "We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similarity, both objectively and subjectively. We also delve into the inference speed and prosody control capabilities of our approach, highlighting the potential of neural transducers in TTS frameworks.",
    "metadata": {
      "arxiv_id": "2401.01498",
      "title": "Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction",
      "summary": "We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similarity, both objectively and subjectively. We also delve into the inference speed and prosody control capabilities of our approach, highlighting the potential of neural transducers in TTS frameworks.",
      "authors": [
        "Minchan Kim",
        "Myeonghun Jeong",
        "Byoung Jin Choi",
        "Semin Kim",
        "Joun Yeop Lee",
        "Nam Soo Kim"
      ],
      "published": "2024-01-03T02:03:36Z",
      "updated": "2024-01-03T02:03:36Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.01498v1",
      "landing_url": "https://arxiv.org/abs/2401.01498v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.01498"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work uses discrete semantic tokens derived from wav2vec2.0 embeddings and a token transducer for semantic token prediction, which aligns with discrete audio token research, but the abstract lacks explicit detailing of token generation/quantization mechanisms (e.g., VQ/RVQ, codebooks, hierarchy) and concrete experimental reproducibility aspects, making it uncertain whether it fully satisfies the tokenization design criteria.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The work uses discrete semantic tokens derived from wav2vec2.0 embeddings and a token transducer for semantic token prediction, which aligns with discrete audio token research, but the abstract lacks explicit detailing of token generation/quantization mechanisms (e.g., VQ/RVQ, codebooks, hierarchy) and concrete experimental reproducibility aspects, making it uncertain whether it fully satisfies the tokenization design criteria.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a TTS framework that utilizes discrete semantic tokens derived from wav2vec2.0 embeddings, applying neural transducer models for semantic token prediction and sequence modeling. The tokens are used as linguistic-like discrete units in a two-stage generation process, aligning well with the inclusion criteria focusing on discrete audio tokens formed by semantic-driven discretization methods and used as core representations for sequence modeling and generation. The abstract provides sufficient methodological detail about token generation, usage, and modeling. There is no indication that the work is a mere demo or lacks technical details, nor does it focus on traditional codec standards solely for compression. Therefore, the study meets the inclusion criteria and does not meet any of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a TTS framework that utilizes discrete semantic tokens derived from wav2vec2.0 embeddings, applying neural transducer models for semantic token prediction and sequence modeling. The tokens are used as linguistic-like discrete units in a two-stage generation process, aligning well with the inclusion criteria focusing on discrete audio tokens formed by semantic-driven discretization methods and used as core representations for sequence modeling and generation. The abstract provides sufficient methodological detail about token generation, usage, and modeling. There is no indication that the work is a mere demo or lacks technical details, nor does it focus on traditional codec standards solely for compression. Therefore, the study meets the inclusion criteria and does not meet any of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "The abstract describes a two-stage TTS that explicitly uses discrete semantic tokens derived from wav2vec2.0 embeddings as the core intermediate representation for sequence modeling (token transducer) and waveform generation, and reports objective/subjective evaluations—i.e., this is a methodological study (not a demo) where discrete SSL-driven tokens are central. Junior reviewer A flagged missing low-level quantization/codec details in the abstract, while Junior reviewer B judged the provided description sufficient; given inclusion requires token-based SSL discretization used as a core representation and the work appears to satisfy that (with the caveat that full tokenization details are likely in the full paper), it meets the inclusion criteria and none of the exclusions. Explanation (1-sentence): Include (better to include) because the study uses SSL-derived discrete semantic tokens as a primary representation for sequence modeling and generation with empirical evaluation, though the abstract omits fine quantization specifics that are likely in the full text.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "The abstract describes a two-stage TTS that explicitly uses discrete semantic tokens derived from wav2vec2.0 embeddings as the core intermediate representation for sequence modeling (token transducer) and waveform generation, and reports objective/subjective evaluations—i.e., this is a methodological study (not a demo) where discrete SSL-driven tokens are central. Junior reviewer A flagged missing low-level quantization/codec details in the abstract, while Junior reviewer B judged the provided description sufficient; given inclusion requires token-based SSL discretization used as a core representation and the work appears to satisfy that (with the caveat that full tokenization details are likely in the full paper), it meets the inclusion criteria and none of the exclusions. Explanation (1-sentence): Include (better to include) because the study uses SSL-derived discrete semantic tokens as a primary representation for sequence modeling and generation with empirical evaluation, though the abstract omits fine quantization specifics that are likely in the full text.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Efficient Bitrate Ladder Construction using Transfer Learning and Spatio-Temporal Features",
    "abstract": "Providing high-quality video with efficient bitrate is a main challenge in video industry. The traditional one-size-fits-all scheme for bitrate ladders is inefficient and reaching the best content-aware decision computationally impractical due to extensive encodings required. To mitigate this, we propose a bitrate and complexity efficient bitrate ladder prediction method using transfer learning and spatio-temporal features. We propose: (1) using feature maps from well-known pre-trained DNNs to predict rate-quality behavior with limited training data; and (2) improving highest quality rung efficiency by predicting minimum bitrate for top quality and using it for the top rung. The method tested on 102 video scenes demonstrates 94.1% reduction in complexity versus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning was thoroughly studied through four networks and ablation studies.",
    "metadata": {
      "arxiv_id": "2401.03195",
      "title": "Efficient Bitrate Ladder Construction using Transfer Learning and Spatio-Temporal Features",
      "summary": "Providing high-quality video with efficient bitrate is a main challenge in video industry. The traditional one-size-fits-all scheme for bitrate ladders is inefficient and reaching the best content-aware decision computationally impractical due to extensive encodings required. To mitigate this, we propose a bitrate and complexity efficient bitrate ladder prediction method using transfer learning and spatio-temporal features. We propose: (1) using feature maps from well-known pre-trained DNNs to predict rate-quality behavior with limited training data; and (2) improving highest quality rung efficiency by predicting minimum bitrate for top quality and using it for the top rung. The method tested on 102 video scenes demonstrates 94.1% reduction in complexity versus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning was thoroughly studied through four networks and ablation studies.",
      "authors": [
        "Ali Falahati",
        "Mohammad Karim Safavi",
        "Ardavan Elahi",
        "Farhad Pakdaman",
        "Moncef Gabbouj"
      ],
      "published": "2024-01-06T11:37:20Z",
      "updated": "2024-03-14T03:59:19Z",
      "categories": [
        "cs.MM",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.03195v2",
      "landing_url": "https://arxiv.org/abs/2401.03195v2",
      "doi": "https://doi.org/10.1109/MVIP62238.2024.10491154"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目專注於使用轉移學習與時空特徵進行位元率階梯的預測與效率提升，與離散音訊 token 的研究貢獻、量化機制或語音模型等核心主題無關，因此不符合包含條件，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目專注於使用轉移學習與時空特徵進行位元率階梯的預測與效率提升，與離散音訊 token 的研究貢獻、量化機制或語音模型等核心主題無關，因此不符合包含條件，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on efficient bitrate ladder construction for video streaming using transfer learning and spatio-temporal features, which pertains to video encoding optimization. It does not address discrete audio tokens or methods for representing audio signals as discrete tokens for downstream language or sequence modeling tasks. There is no mention of vector quantization, audio token generation, or audio representation relevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on efficient bitrate ladder construction for video streaming using transfer learning and spatio-temporal features, which pertains to video encoding optimization. It does not address discrete audio tokens or methods for representing audio signals as discrete tokens for downstream language or sequence modeling tasks. There is no mention of vector quantization, audio token generation, or audio representation relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Optimal Transcoding Resolution Prediction for Efficient Per-Title Bitrate Ladder Estimation",
    "abstract": "Adaptive video streaming requires efficient bitrate ladder construction to meet heterogeneous network conditions and end-user demands. Per-title optimized encoding typically traverses numerous encoding parameters to search the Pareto-optimal operating points for each video. Recently, researchers have attempted to predict the content-optimized bitrate ladder for pre-encoding overhead reduction. However, existing methods commonly estimate the encoding parameters on the Pareto front and still require subsequent pre-encodings. In this paper, we propose to directly predict the optimal transcoding resolution at each preset bitrate for efficient bitrate ladder construction. We adopt a Temporal Attentive Gated Recurrent Network to capture spatial-temporal features and predict transcoding resolutions as a multi-task classification problem. We demonstrate that content-optimized bitrate ladders can thus be efficiently determined without any pre-encoding. Our method well approximates the ground-truth bitrate-resolution pairs with a slight Bjøntegaard Delta rate loss of 1.21% and significantly outperforms the state-of-the-art fixed ladder.",
    "metadata": {
      "arxiv_id": "2401.04405",
      "title": "Optimal Transcoding Resolution Prediction for Efficient Per-Title Bitrate Ladder Estimation",
      "summary": "Adaptive video streaming requires efficient bitrate ladder construction to meet heterogeneous network conditions and end-user demands. Per-title optimized encoding typically traverses numerous encoding parameters to search the Pareto-optimal operating points for each video. Recently, researchers have attempted to predict the content-optimized bitrate ladder for pre-encoding overhead reduction. However, existing methods commonly estimate the encoding parameters on the Pareto front and still require subsequent pre-encodings. In this paper, we propose to directly predict the optimal transcoding resolution at each preset bitrate for efficient bitrate ladder construction. We adopt a Temporal Attentive Gated Recurrent Network to capture spatial-temporal features and predict transcoding resolutions as a multi-task classification problem. We demonstrate that content-optimized bitrate ladders can thus be efficiently determined without any pre-encoding. Our method well approximates the ground-truth bitrate-resolution pairs with a slight Bjøntegaard Delta rate loss of 1.21% and significantly outperforms the state-of-the-art fixed ladder.",
      "authors": [
        "Jinhai Yang",
        "Mengxi Guo",
        "Shijie Zhao",
        "Junlin Li",
        "Li Zhang"
      ],
      "published": "2024-01-09T08:01:47Z",
      "updated": "2024-01-09T08:01:47Z",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.04405v1",
      "landing_url": "https://arxiv.org/abs/2401.04405v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.04405"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets video transcoding and bitrate ladder prediction rather than discrete audio tokens or token-based audio representations used for language/sequence modeling, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets video transcoding and bitrate ladder prediction rather than discrete audio tokens or token-based audio representations used for language/sequence modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on predicting optimal transcoding resolutions for efficient bitrate ladder construction in adaptive video streaming, which relates to video encoding rather than audio processing. It does not discuss discrete audio tokens, vector quantization for audio, or their use in language or sequence modeling; thus, it does not meet the inclusion criteria centered on discrete audio tokenization or representation for audio language modeling or related tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on predicting optimal transcoding resolutions for efficient bitrate ladder construction in adaptive video streaming, which relates to video encoding rather than audio processing. It does not discuss discrete audio tokens, vector quantization for audio, or their use in language or sequence modeling; thus, it does not meet the inclusion criteria centered on discrete audio tokenization or representation for audio language modeling or related tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Self-supervised speech representation and contextual text embedding for match-mismatch classification with EEG recording",
    "abstract": "Relating speech to EEG holds considerable importance but is challenging. In this study, a deep convolutional network was employed to extract spatiotemporal features from EEG data. Self-supervised speech representation and contextual text embedding were used as speech features. Contrastive learning was used to relate EEG features to speech features. The experimental results demonstrate the benefits of using self-supervised speech representation and contextual text embedding. Through feature fusion and model ensemble, an accuracy of 60.29% was achieved, and the performance was ranked as No.2 in Task 1 of the Auditory EEG Challenge (ICASSP 2024). The code to implement our work is available on Github: https://github.com/bobwangPKU/EEG-Stimulus-Match-Mismatch.",
    "metadata": {
      "arxiv_id": "2401.04964",
      "title": "Self-supervised speech representation and contextual text embedding for match-mismatch classification with EEG recording",
      "summary": "Relating speech to EEG holds considerable importance but is challenging. In this study, a deep convolutional network was employed to extract spatiotemporal features from EEG data. Self-supervised speech representation and contextual text embedding were used as speech features. Contrastive learning was used to relate EEG features to speech features. The experimental results demonstrate the benefits of using self-supervised speech representation and contextual text embedding. Through feature fusion and model ensemble, an accuracy of 60.29% was achieved, and the performance was ranked as No.2 in Task 1 of the Auditory EEG Challenge (ICASSP 2024). The code to implement our work is available on Github: https://github.com/bobwangPKU/EEG-Stimulus-Match-Mismatch.",
      "authors": [
        "Bo Wang",
        "Xiran Xu",
        "Zechen Zhang",
        "Haolin Zhu",
        "YuJie Yan",
        "Xihong Wu",
        "Jing Chen"
      ],
      "published": "2024-01-10T07:11:36Z",
      "updated": "2024-02-01T04:51:22Z",
      "categories": [
        "eess.SP",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.04964v2",
      "landing_url": "https://arxiv.org/abs/2401.04964v2",
      "doi": "https://doi.org/10.48550/arXiv.2401.04964"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on EEG-speech relation using self-supervised speech representations and contextual embeddings, but it does not involve discrete audio token design or tokenization suitable for audio language modeling, failing to meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on EEG-speech relation using self-supervised speech representations and contextual embeddings, but it does not involve discrete audio token design or tokenization suitable for audio language modeling, failing to meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on matching speech representations with EEG data using self-supervised learning; however, it does not clearly state the process of generating discrete audio tokens (e.g., vector quantization or similar quantization techniques) nor does it emphasize discrete token design as a main contribution; it centers on speech representation and EEG feature extraction without explicit mention of discrete token formation or usage in sequence/language modeling, thus not aligning fully with the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on matching speech representations with EEG data using self-supervised learning; however, it does not clearly state the process of generating discrete audio tokens (e.g., vector quantization or similar quantization techniques) nor does it emphasize discrete token design as a main contribution; it centers on speech representation and EEG feature extraction without explicit mention of discrete token formation or usage in sequence/language modeling, thus not aligning fully with the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Erasing Undesirable Influence in Diffusion Models",
    "abstract": "Diffusion models are highly effective at generating high-quality images but pose risks, such as the unintentional generation of NSFW (not safe for work) content. Although various techniques have been proposed to mitigate unwanted influences in diffusion models while preserving overall performance, achieving a balance between these goals remains challenging. In this work, we introduce EraseDiff, an algorithm designed to preserve the utility of the diffusion model on retained data while removing the unwanted information associated with the data to be forgotten. Our approach formulates this task as a constrained optimization problem using the value function, resulting in a natural first-order algorithm for solving the optimization problem. By altering the generative process to deviate away from the ground-truth denoising trajectory, we update parameters for preservation while controlling constraint reduction to ensure effective erasure, striking an optimal trade-off. Extensive experiments and thorough comparisons with state-of-the-art algorithms demonstrate that EraseDiff effectively preserves the model's utility, efficacy, and efficiency.",
    "metadata": {
      "arxiv_id": "2401.05779",
      "title": "Erasing Undesirable Influence in Diffusion Models",
      "summary": "Diffusion models are highly effective at generating high-quality images but pose risks, such as the unintentional generation of NSFW (not safe for work) content. Although various techniques have been proposed to mitigate unwanted influences in diffusion models while preserving overall performance, achieving a balance between these goals remains challenging. In this work, we introduce EraseDiff, an algorithm designed to preserve the utility of the diffusion model on retained data while removing the unwanted information associated with the data to be forgotten. Our approach formulates this task as a constrained optimization problem using the value function, resulting in a natural first-order algorithm for solving the optimization problem. By altering the generative process to deviate away from the ground-truth denoising trajectory, we update parameters for preservation while controlling constraint reduction to ensure effective erasure, striking an optimal trade-off. Extensive experiments and thorough comparisons with state-of-the-art algorithms demonstrate that EraseDiff effectively preserves the model's utility, efficacy, and efficiency.",
      "authors": [
        "Jing Wu",
        "Trung Le",
        "Munawar Hayat",
        "Mehrtash Harandi"
      ],
      "published": "2024-01-11T09:30:36Z",
      "updated": "2024-11-20T09:31:15Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.05779v4",
      "landing_url": "https://arxiv.org/abs/2401.05779v4",
      "doi": "https://doi.org/10.48550/arXiv.2401.05779"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on diffusion models and erasing unwanted influence; there is no discrete audio token design or discrete audio token generation/quantization discussion, so it does not meet the specified topic inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on diffusion models and erasing unwanted influence; there is no discrete audio token design or discrete audio token generation/quantization discussion, so it does not meet the specified topic inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on diffusion models for image generation and proposes a method for erasing unwanted influences in these models. However, the inclusion criteria require the study to revolve around discrete audio tokens obtained through vector quantization or similar methods for audio signal processing and their use in language or sequence models. This paper does not address discrete audio tokens or audio signal processing, thus failing the inclusion criteria and not meeting the core subject requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on diffusion models for image generation and proposes a method for erasing unwanted influences in these models. However, the inclusion criteria require the study to revolve around discrete audio tokens obtained through vector quantization or similar methods for audio signal processing and their use in language or sequence models. This paper does not address discrete audio tokens or audio signal processing, thus failing the inclusion criteria and not meeting the core subject requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Revisiting Self-supervised Learning of Speech Representation from a Mutual Information Perspective",
    "abstract": "Existing studies on self-supervised speech representation learning have focused on developing new training methods and applying pre-trained models for different applications. However, the quality of these models is often measured by the performance of different downstream tasks. How well the representations access the information of interest is less studied. In this work, we take a closer look into existing self-supervised methods of speech from an information-theoretic perspective. We aim to develop metrics using mutual information to help practical problems such as model design and selection. We use linear probes to estimate the mutual information between the target information and learned representations, showing another insight into the accessibility to the target information from speech representations. Further, we explore the potential of evaluating representations in a self-supervised fashion, where we estimate the mutual information between different parts of the data without using any labels. Finally, we show that both supervised and unsupervised measures echo the performance of the models on layer-wise linear probing and speech recognition.",
    "metadata": {
      "arxiv_id": "2401.08833",
      "title": "Revisiting Self-supervised Learning of Speech Representation from a Mutual Information Perspective",
      "summary": "Existing studies on self-supervised speech representation learning have focused on developing new training methods and applying pre-trained models for different applications. However, the quality of these models is often measured by the performance of different downstream tasks. How well the representations access the information of interest is less studied. In this work, we take a closer look into existing self-supervised methods of speech from an information-theoretic perspective. We aim to develop metrics using mutual information to help practical problems such as model design and selection. We use linear probes to estimate the mutual information between the target information and learned representations, showing another insight into the accessibility to the target information from speech representations. Further, we explore the potential of evaluating representations in a self-supervised fashion, where we estimate the mutual information between different parts of the data without using any labels. Finally, we show that both supervised and unsupervised measures echo the performance of the models on layer-wise linear probing and speech recognition.",
      "authors": [
        "Alexander H. Liu",
        "Sung-Lin Yeh",
        "James Glass"
      ],
      "published": "2024-01-16T21:13:22Z",
      "updated": "2024-01-16T21:13:22Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.08833v1",
      "landing_url": "https://arxiv.org/abs/2401.08833v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.08833"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper investigates self-supervised speech representations and mutual information metrics rather than discrete audio token generation/quantization, so it does not meet the discrete audio token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper investigates self-supervised speech representations and mutual information metrics rather than discrete audio token generation/quantization, so it does not meet the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised speech representation learning evaluated from a mutual information perspective, but it does not mention the generation or use of discrete audio tokens, vector quantization, or related quantization mechanisms as defined in the inclusion criteria. Furthermore, the abstract lacks detailed discussion on token formation processes or their deployment in language or sequence models, which are key to inclusion. Thus, it does not meet the strict inclusion criteria nor violates any explicit exclusion criteria but lacks the core methods and contributions related to discrete audio tokens.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised speech representation learning evaluated from a mutual information perspective, but it does not mention the generation or use of discrete audio tokens, vector quantization, or related quantization mechanisms as defined in the inclusion criteria. Furthermore, the abstract lacks detailed discussion on token formation processes or their deployment in language or sequence models, which are key to inclusion. Thus, it does not meet the strict inclusion criteria nor violates any explicit exclusion criteria but lacks the core methods and contributions related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "On the power of linear programming for K-means clustering",
    "abstract": "In [SIAM J. Optim., 2022], the authors introduced a new linear programming (LP) relaxation for K-means clustering. In this paper, we further investigate both theoretical and computational properties of this relaxation. As evident from our numerical experiments with both synthetic real-world data sets, the proposed LP relaxation is almost always tight; i.e. its optimal solution is feasible for the original nonconvex problem. To better understand this unexpected behaviour, on the theoretical side, we focus on K-means clustering with two clusters, and we obtain sufficient conditions under which the LP relaxation is tight. We further analyze the sufficient conditions when the input is generated according to a popular stochastic model and obtain recovery guarantees for the LP relaxation. We conclude our theoretical study by constructing a family of inputs for which the LP relaxation is never tight. Denoting by $n$ the number of data points to be clustered, the LP relaxation contains $Ω(n^3)$ inequalities making it impractical for large data sets. To address the scalability issue, by building upon a cutting-plane algorithm together with the GPU implementation of PDLP, a first-order method LP solver, we develop an efficient algorithm that solves the proposed LP and hence the K-means clustering problem, for up to $n \\leq 4000$ data points.",
    "metadata": {
      "arxiv_id": "2402.01061",
      "title": "On the power of linear programming for K-means clustering",
      "summary": "In [SIAM J. Optim., 2022], the authors introduced a new linear programming (LP) relaxation for K-means clustering. In this paper, we further investigate both theoretical and computational properties of this relaxation. As evident from our numerical experiments with both synthetic real-world data sets, the proposed LP relaxation is almost always tight; i.e. its optimal solution is feasible for the original nonconvex problem. To better understand this unexpected behaviour, on the theoretical side, we focus on K-means clustering with two clusters, and we obtain sufficient conditions under which the LP relaxation is tight. We further analyze the sufficient conditions when the input is generated according to a popular stochastic model and obtain recovery guarantees for the LP relaxation. We conclude our theoretical study by constructing a family of inputs for which the LP relaxation is never tight. Denoting by $n$ the number of data points to be clustered, the LP relaxation contains $Ω(n^3)$ inequalities making it impractical for large data sets. To address the scalability issue, by building upon a cutting-plane algorithm together with the GPU implementation of PDLP, a first-order method LP solver, we develop an efficient algorithm that solves the proposed LP and hence the K-means clustering problem, for up to $n \\leq 4000$ data points.",
      "authors": [
        "Antonio De Rosa",
        "Aida Khajavirad",
        "Yakun Wang"
      ],
      "published": "2024-02-01T23:19:48Z",
      "updated": "2024-08-16T01:31:44Z",
      "categories": [
        "math.OC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01061v2",
      "landing_url": "https://arxiv.org/abs/2402.01061v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.01061"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on linear programming relaxations for K-means clustering and does not discuss discrete audio tokens or tokenization methods; it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on linear programming relaxations for K-means clustering and does not discuss discrete audio tokens or tokenization methods; it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on linear programming methods for K-means clustering, a topic unrelated to discrete audio tokens or audio representation through vector quantization or related tokenization techniques. The abstract does not describe any discrete audio token generation, quantization processes, or use of tokens for audio language modeling or cross-modal generative tasks, which are key inclusion criteria. Therefore, it does not meet the inclusion criteria and does not fit the research scope required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on linear programming methods for K-means clustering, a topic unrelated to discrete audio tokens or audio representation through vector quantization or related tokenization techniques. The abstract does not describe any discrete audio token generation, quantization processes, or use of tokens for audio language modeling or cross-modal generative tasks, which are key inclusion criteria. Therefore, it does not meet the inclusion criteria and does not fit the research scope required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Probabilistic Model Behind Self-Supervised Learning",
    "abstract": "In self-supervised learning (SSL), representations are learned via an auxiliary task without annotated labels. A common task is to classify augmentations or different modalities of the data, which share semantic content (e.g. an object in an image) but differ in style (e.g. the object's location). Many approaches to self-supervised learning have been proposed, e.g. SimCLR, CLIP, and DINO, which have recently gained much attention for their representations achieving downstream performance comparable to supervised learning. However, a theoretical understanding of self-supervised methods eludes. Addressing this, we present a generative latent variable model for self-supervised learning and show that several families of discriminative SSL, including contrastive methods, induce a comparable distribution over representations, providing a unifying theoretical framework for these methods. The proposed model also justifies connections drawn to mutual information and the use of a ''projection head''. Learning representations by fitting the model generatively (termed SimVAE) improves performance over discriminative and other VAE-based methods on simple image benchmarks and significantly narrows the gap between generative and discriminative representation learning in more complex settings. Importantly, as our analysis predicts, SimVAE outperforms self-supervised learning where style information is required, taking an important step toward understanding self-supervised methods and achieving task-agnostic representations.",
    "metadata": {
      "arxiv_id": "2402.01399",
      "title": "A Probabilistic Model Behind Self-Supervised Learning",
      "summary": "In self-supervised learning (SSL), representations are learned via an auxiliary task without annotated labels. A common task is to classify augmentations or different modalities of the data, which share semantic content (e.g. an object in an image) but differ in style (e.g. the object's location). Many approaches to self-supervised learning have been proposed, e.g. SimCLR, CLIP, and DINO, which have recently gained much attention for their representations achieving downstream performance comparable to supervised learning. However, a theoretical understanding of self-supervised methods eludes. Addressing this, we present a generative latent variable model for self-supervised learning and show that several families of discriminative SSL, including contrastive methods, induce a comparable distribution over representations, providing a unifying theoretical framework for these methods. The proposed model also justifies connections drawn to mutual information and the use of a ''projection head''. Learning representations by fitting the model generatively (termed SimVAE) improves performance over discriminative and other VAE-based methods on simple image benchmarks and significantly narrows the gap between generative and discriminative representation learning in more complex settings. Importantly, as our analysis predicts, SimVAE outperforms self-supervised learning where style information is required, taking an important step toward understanding self-supervised methods and achieving task-agnostic representations.",
      "authors": [
        "Alice Bizeul",
        "Bernhard Schölkopf",
        "Carl Allen"
      ],
      "published": "2024-02-02T13:31:17Z",
      "updated": "2024-10-15T13:16:13Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01399v3",
      "landing_url": "https://arxiv.org/abs/2402.01399v3",
      "doi": "https://doi.org/10.48550/arXiv.2402.01399"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article proposes a probabilistic latent variable model for self-supervised learning (SimVAE) with examples in image-style tasks; it does not address discrete audio token design or tokenization workflows needed by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article proposes a probabilistic latent variable model for self-supervised learning (SimVAE) with examples in image-style tasks; it does not address discrete audio token design or tokenization workflows needed by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on a probabilistic model for self-supervised learning of representations, emphasizing image data and generative/discriminative learning frameworks. There is no mention or indication of discrete audio tokens, vector quantization, or audio language modeling, which are core to the inclusion criteria. The study also does not describe any token generation process or usage related to audio sequence modeling, nor does it focus on speech, music, or general audio signals. Therefore, it does not meet the inclusion criteria and aligns with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on a probabilistic model for self-supervised learning of representations, emphasizing image data and generative/discriminative learning frameworks. There is no mention or indication of discrete audio tokens, vector quantization, or audio language modeling, which are core to the inclusion criteria. The study also does not describe any token generation process or usage related to audio sequence modeling, nor does it focus on speech, music, or general audio signals. Therefore, it does not meet the inclusion criteria and aligns with exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Video Super-Resolution for Optimized Bitrate and Green Online Streaming",
    "abstract": "Conventional per-title encoding schemes strive to optimize encoding resolutions to deliver the utmost perceptual quality for each bitrate ladder representation. Nevertheless, maintaining encoding time within an acceptable threshold is equally imperative in online streaming applications. Furthermore, modern client devices are equipped with the capability for fast deep-learning-based video super-resolution (VSR) techniques, enhancing the perceptual quality of the decoded bitstream. This suggests that opting for lower resolutions in representations during the encoding process can curtail the overall energy consumption without substantially compromising perceptual quality. In this context, this paper introduces a video super-resolution-based latency-aware optimized bitrate encoding scheme (ViSOR) designed for online adaptive streaming applications. ViSOR determines the encoding resolution for each target bitrate, ensuring the highest achievable perceptual quality after VSR within the bound of a maximum acceptable latency. Random forest-based prediction models are trained to predict the perceptual quality after VSR and the encoding time for each resolution using the spatiotemporal features extracted for each video segment. Experimental results show that ViSOR targeting fast super-resolution convolutional neural network (FSRCNN) achieves an overall average bitrate reduction of 24.65 % and 32.70 % to maintain the same PSNR and VMAF, compared to the HTTP Live Streaming (HLS) bitrate ladder encoding of 4 s segments using the x265 encoder, when the maximum acceptable latency for each representation is set as two seconds. Considering a just noticeable difference (JND) of six VMAF points, the average cumulative storage consumption and encoding energy for each segment is reduced by 79.32 % and 68.21 %, respectively, contributing towards greener streaming.",
    "metadata": {
      "arxiv_id": "2402.03513",
      "title": "Video Super-Resolution for Optimized Bitrate and Green Online Streaming",
      "summary": "Conventional per-title encoding schemes strive to optimize encoding resolutions to deliver the utmost perceptual quality for each bitrate ladder representation. Nevertheless, maintaining encoding time within an acceptable threshold is equally imperative in online streaming applications. Furthermore, modern client devices are equipped with the capability for fast deep-learning-based video super-resolution (VSR) techniques, enhancing the perceptual quality of the decoded bitstream. This suggests that opting for lower resolutions in representations during the encoding process can curtail the overall energy consumption without substantially compromising perceptual quality. In this context, this paper introduces a video super-resolution-based latency-aware optimized bitrate encoding scheme (ViSOR) designed for online adaptive streaming applications. ViSOR determines the encoding resolution for each target bitrate, ensuring the highest achievable perceptual quality after VSR within the bound of a maximum acceptable latency. Random forest-based prediction models are trained to predict the perceptual quality after VSR and the encoding time for each resolution using the spatiotemporal features extracted for each video segment. Experimental results show that ViSOR targeting fast super-resolution convolutional neural network (FSRCNN) achieves an overall average bitrate reduction of 24.65 % and 32.70 % to maintain the same PSNR and VMAF, compared to the HTTP Live Streaming (HLS) bitrate ladder encoding of 4 s segments using the x265 encoder, when the maximum acceptable latency for each representation is set as two seconds. Considering a just noticeable difference (JND) of six VMAF points, the average cumulative storage consumption and encoding energy for each segment is reduced by 79.32 % and 68.21 %, respectively, contributing towards greener streaming.",
      "authors": [
        "Vignesh V Menon",
        "Prajit T Rajendran",
        "Amritha Premkumar",
        "Benjamin Bross",
        "Detlev Marpe"
      ],
      "published": "2024-02-05T21:01:01Z",
      "updated": "2024-02-05T21:01:01Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.03513v1",
      "landing_url": "https://arxiv.org/abs/2402.03513v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.03513"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses video super-resolution and bitrate-optimized encoding for streaming, with no focus on discrete audio tokens or tokenization design; thus it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses video super-resolution and bitrate-optimized encoding for streaming, with no focus on discrete audio tokens or tokenization design; thus it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on video super-resolution optimization for bitrate and energy efficiency in streaming, with no mention or indication of discrete audio tokens, vector quantization, or audio representation techniques relevant to the inclusion criteria about discrete audio token design, generation, or usage in sequence models. Hence, it does not meet the inclusion criteria related to discrete audio tokens and fails to contribute to the specified research area.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on video super-resolution optimization for bitrate and energy efficiency in streaming, with no mention or indication of discrete audio tokens, vector quantization, or audio representation techniques relevant to the inclusion criteria about discrete audio token design, generation, or usage in sequence models. Hence, it does not meet the inclusion criteria related to discrete audio tokens and fails to contribute to the specified research area.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation",
    "abstract": "Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering. Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance. We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods. PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely unordered and there is a unique partial order for nodes and edges. With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each block's probability is conditionally modeled by a shared diffusion model with an equivariant network. To ensure efficiency while being expressive, we further propose a higher-order graph transformer, which integrates transformer with PPGN. Like GPT, we extend the higher-order graph transformer to support parallel training of all blocks. Without any extra features, PARD achieves state-of-the-art performance on molecular and non-molecular datasets, and scales to large datasets like MOSES containing 1.9M molecules. Pard is open-sourced at https://github.com/LingxiaoShawn/Pard.",
    "metadata": {
      "arxiv_id": "2402.03687",
      "title": "Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation",
      "summary": "Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering. Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance. We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods. PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely unordered and there is a unique partial order for nodes and edges. With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each block's probability is conditionally modeled by a shared diffusion model with an equivariant network. To ensure efficiency while being expressive, we further propose a higher-order graph transformer, which integrates transformer with PPGN. Like GPT, we extend the higher-order graph transformer to support parallel training of all blocks. Without any extra features, PARD achieves state-of-the-art performance on molecular and non-molecular datasets, and scales to large datasets like MOSES containing 1.9M molecules. Pard is open-sourced at https://github.com/LingxiaoShawn/Pard.",
      "authors": [
        "Lingxiao Zhao",
        "Xueying Ding",
        "Leman Akoglu"
      ],
      "published": "2024-02-06T04:17:44Z",
      "updated": "2024-12-02T18:57:12Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.03687v3",
      "landing_url": "https://arxiv.org/abs/2402.03687v3",
      "doi": "https://doi.org/10.48550/arXiv.2402.03687"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on permutation-invariant autoregressive diffusion for graph generation, not on discrete audio token representations for audio-language modeling or related tasks, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on permutation-invariant autoregressive diffusion for graph generation, not on discrete audio token representations for audio-language modeling or related tasks, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on graph generation using a permutation-invariant autoregressive diffusion model, which is unrelated to discrete audio tokens or audio signal processing; it does not discuss tokenization or quantization of audio data, nor does it address audio language modeling or related applications, thus it does not meet any inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on graph generation using a permutation-invariant autoregressive diffusion model, which is unrelated to discrete audio tokens or audio signal processing; it does not discuss tokenization or quantization of audio data, nor does it address audio language modeling or related applications, thus it does not meet any inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unified Discrete Diffusion for Categorical Data",
    "abstract": "Discrete diffusion models have seen a surge of attention with applications on naturally discrete data such as language and graphs. Although discrete-time discrete diffusion has been established for a while, only recently Campbell et al. (2022) introduced the first framework for continuous-time discrete diffusion. However, their training and sampling processes differ significantly from the discrete-time version, necessitating nontrivial approximations for tractability. In this paper, we first present a series of mathematical simplifications of the variational lower bound that enable more accurate and easy-to-optimize training for discrete diffusion. In addition, we derive a simple formulation for backward denoising that enables exact and accelerated sampling, and importantly, an elegant unification of discrete-time and continuous-time discrete diffusion. Thanks to simpler analytical formulations, both forward and now also backward probabilities can flexibly accommodate any noise distribution, including different noise distributions for multi-element objects. Experiments show that our proposed USD3 (for Unified Simplified Discrete Denoising Diffusion) outperform all SOTA baselines on established datasets. We open-source our unified code at https://github.com/LingxiaoShawn/USD3.",
    "metadata": {
      "arxiv_id": "2402.03701",
      "title": "Unified Discrete Diffusion for Categorical Data",
      "summary": "Discrete diffusion models have seen a surge of attention with applications on naturally discrete data such as language and graphs. Although discrete-time discrete diffusion has been established for a while, only recently Campbell et al. (2022) introduced the first framework for continuous-time discrete diffusion. However, their training and sampling processes differ significantly from the discrete-time version, necessitating nontrivial approximations for tractability. In this paper, we first present a series of mathematical simplifications of the variational lower bound that enable more accurate and easy-to-optimize training for discrete diffusion. In addition, we derive a simple formulation for backward denoising that enables exact and accelerated sampling, and importantly, an elegant unification of discrete-time and continuous-time discrete diffusion. Thanks to simpler analytical formulations, both forward and now also backward probabilities can flexibly accommodate any noise distribution, including different noise distributions for multi-element objects. Experiments show that our proposed USD3 (for Unified Simplified Discrete Denoising Diffusion) outperform all SOTA baselines on established datasets. We open-source our unified code at https://github.com/LingxiaoShawn/USD3.",
      "authors": [
        "Lingxiao Zhao",
        "Xueying Ding",
        "Lijun Yu",
        "Leman Akoglu"
      ],
      "published": "2024-02-06T04:42:36Z",
      "updated": "2024-08-12T16:22:38Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.03701v2",
      "landing_url": "https://arxiv.org/abs/2402.03701v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.03701"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discusses unified discrete diffusion for categorical data and does not address discrete audio tokens or audio token generation/quantization needed by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper discusses unified discrete diffusion for categorical data and does not address discrete audio tokens or audio token generation/quantization needed by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on discrete diffusion models for categorical data, presenting a unified framework for discrete-time and continuous-time diffusion. However, it does not address discrete audio tokens or the transformation of continuous audio signals into discrete tokens that preserve acoustic or semantic information for sequence modeling, which is the core inclusion criterion. There is no mention of vector quantization, residual vector quantization, hierarchical quantization, or audio-related token design and usage. Therefore, it does not meet the inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on discrete diffusion models for categorical data, presenting a unified framework for discrete-time and continuous-time diffusion. However, it does not address discrete audio tokens or the transformation of continuous audio signals into discrete tokens that preserve acoustic or semantic information for sequence modeling, which is the core inclusion criterion. There is no mention of vector quantization, residual vector quantization, hierarchical quantization, or audio-related token design and usage. Therefore, it does not meet the inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Stable Autonomous Flow Matching",
    "abstract": "In contexts where data samples represent a physically stable state, it is often assumed that the data points represent the local minima of an energy landscape. In control theory, it is well-known that energy can serve as an effective Lyapunov function. Despite this, connections between control theory and generative models in the literature are sparse, even though there are several machine learning applications with physically stable data points. In this paper, we focus on such data and a recent class of deep generative models called flow matching. We apply tools of stochastic stability for time-independent systems to flow matching models. In doing so, we characterize the space of flow matching models that are amenable to this treatment, as well as draw connections to other control theory principles. We demonstrate our theoretical results on two examples.",
    "metadata": {
      "arxiv_id": "2402.05774",
      "title": "Stable Autonomous Flow Matching",
      "summary": "In contexts where data samples represent a physically stable state, it is often assumed that the data points represent the local minima of an energy landscape. In control theory, it is well-known that energy can serve as an effective Lyapunov function. Despite this, connections between control theory and generative models in the literature are sparse, even though there are several machine learning applications with physically stable data points. In this paper, we focus on such data and a recent class of deep generative models called flow matching. We apply tools of stochastic stability for time-independent systems to flow matching models. In doing so, we characterize the space of flow matching models that are amenable to this treatment, as well as draw connections to other control theory principles. We demonstrate our theoretical results on two examples.",
      "authors": [
        "Christopher Iliffe Sprague",
        "Arne Elofsson",
        "Hossein Azizpour"
      ],
      "published": "2024-02-08T16:01:24Z",
      "updated": "2024-02-08T16:01:24Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.05774v1",
      "landing_url": "https://arxiv.org/abs/2402.05774v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.05774"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract concerns flow matching and control-theory stability; it does not address discrete audio tokens or any audio tokenization/quantization methods for audio-language modeling, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract concerns flow matching and control-theory stability; it does not address discrete audio tokens or any audio tokenization/quantization methods for audio-language modeling, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a theoretical approach to flow matching models and their stability within control theory and machine learning contexts, without mentioning any discrete audio token generation, vector quantization, or audio-related sequence modeling, failing to meet the inclusion criteria related to discrete audio token methods and applications.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a theoretical approach to flow matching models and their stability within control theory and machine learning contexts, without mentioning any discrete audio token generation, vector quantization, or audio-related sequence modeling, failing to meet the inclusion criteria related to discrete audio token methods and applications.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Codec-SUPERB: An In-Depth Analysis of Sound Codec Models",
    "abstract": "The sound codec's dual roles in minimizing data transmission latency and serving as tokenizers underscore its critical importance. Recent years have witnessed significant developments in codec models. The ideal sound codec should preserve content, paralinguistics, speakers, and audio information. However, the question of which codec achieves optimal sound information preservation remains unanswered, as in different papers, models are evaluated on their selected experimental settings. This study introduces Codec-SUPERB, an acronym for Codec sound processing Universal PERformance Benchmark. It is an ecosystem designed to assess codec models across representative sound applications and signal-level metrics rooted in sound domain knowledge.Codec-SUPERB simplifies result sharing through an online leaderboard, promoting collaboration within a community-driven benchmark database, thereby stimulating new development cycles for codecs. Furthermore, we undertake an in-depth analysis to offer insights into codec models from both application and signal perspectives, diverging from previous codec papers mainly concentrating on signal-level comparisons. Finally, we will release codes, the leaderboard, and data to accelerate progress within the community.",
    "metadata": {
      "arxiv_id": "2402.13071",
      "title": "Codec-SUPERB: An In-Depth Analysis of Sound Codec Models",
      "summary": "The sound codec's dual roles in minimizing data transmission latency and serving as tokenizers underscore its critical importance. Recent years have witnessed significant developments in codec models. The ideal sound codec should preserve content, paralinguistics, speakers, and audio information. However, the question of which codec achieves optimal sound information preservation remains unanswered, as in different papers, models are evaluated on their selected experimental settings. This study introduces Codec-SUPERB, an acronym for Codec sound processing Universal PERformance Benchmark. It is an ecosystem designed to assess codec models across representative sound applications and signal-level metrics rooted in sound domain knowledge.Codec-SUPERB simplifies result sharing through an online leaderboard, promoting collaboration within a community-driven benchmark database, thereby stimulating new development cycles for codecs. Furthermore, we undertake an in-depth analysis to offer insights into codec models from both application and signal perspectives, diverging from previous codec papers mainly concentrating on signal-level comparisons. Finally, we will release codes, the leaderboard, and data to accelerate progress within the community.",
      "authors": [
        "Haibin Wu",
        "Ho-Lam Chung",
        "Yi-Cheng Lin",
        "Yuan-Kuei Wu",
        "Xuanjun Chen",
        "Yu-Chi Pai",
        "Hsiu-Hsuan Wang",
        "Kai-Wei Chang",
        "Alexander H. Liu",
        "Hung-yi Lee"
      ],
      "published": "2024-02-20T15:13:38Z",
      "updated": "2024-09-18T12:02:47Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.13071v3",
      "landing_url": "https://arxiv.org/abs/2402.13071v3",
      "doi": "https://doi.org/10.48550/arXiv.2402.13071"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a codec benchmark/platform (Codec-SUPERB) and a leaderboard, focusing on evaluating codec models, but it does not present discrete audio token design, tokenization methods, or reproducible experimental details; it resembles a system/demo/summary rather than providing the necessary methodological content to support inclusion under the discrete audio token criteria, and it lacks explicit publication date details.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a codec benchmark/platform (Codec-SUPERB) and a leaderboard, focusing on evaluating codec models, but it does not present discrete audio token design, tokenization methods, or reproducible experimental details; it resembles a system/demo/summary rather than providing the necessary methodological content to support inclusion under the discrete audio token criteria, and it lacks explicit publication date details.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on analyzing sound codec models primarily as compression and encoding tools, benchmarking them on signal-level and application performance. However, the abstract lacks clear details on discrete audio token generation processes, such as vector quantization, token usage in sequence or language models, or semantic disentanglement relevant to the defined topic of Discrete Audio Tokens. The emphasis is on codec evaluation and leaderboard establishment rather than proposing or detailing novel discrete token design or usage for audio language modeling or cross-modal generation. Therefore, it does not meet the inclusion criteria requiring discrete token design as a core contribution with transparent methodology applicable to language or sequence modeling, nor does it explicitly exclude, but given the lack of token-specific methodology, its relevance is low.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on analyzing sound codec models primarily as compression and encoding tools, benchmarking them on signal-level and application performance. However, the abstract lacks clear details on discrete audio token generation processes, such as vector quantization, token usage in sequence or language models, or semantic disentanglement relevant to the defined topic of Discrete Audio Tokens. The emphasis is on codec evaluation and leaderboard establishment rather than proposing or detailing novel discrete token design or usage for audio language modeling or cross-modal generation. Therefore, it does not meet the inclusion criteria requiring discrete token design as a core contribution with transparent methodology applicable to language or sequence modeling, nor does it explicitly exclude, but given the lack of token-specific methodology, its relevance is low.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Discrete Semantic Tokenization for Deep CTR Prediction",
    "abstract": "Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings, subsequently caching them to optimize processing time at the expense of space. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user--item token pair. Our experimental results on news recommendation showcase the effectiveness and efficiency (about 200-fold space compression) of UIST for CTR prediction.",
    "metadata": {
      "arxiv_id": "2403.08206",
      "title": "Discrete Semantic Tokenization for Deep CTR Prediction",
      "summary": "Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings, subsequently caching them to optimize processing time at the expense of space. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user--item token pair. Our experimental results on news recommendation showcase the effectiveness and efficiency (about 200-fold space compression) of UIST for CTR prediction.",
      "authors": [
        "Qijiong Liu",
        "Hengchang Hu",
        "Jiahao Wu",
        "Jieming Zhu",
        "Min-Yen Kan",
        "Xiao-Ming Wu"
      ],
      "published": "2024-03-13T03:03:15Z",
      "updated": "2024-03-21T15:17:46Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.08206v2",
      "landing_url": "https://arxiv.org/abs/2403.08206v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.08206"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses discrete semantic tokens for CTR prediction in news recommendation, focusing on speed/space in embeddings, not on discrete audio tokens or audio representations (no audio signals, VQ/RVQ, or audio-language modeling). It therefore does not meet the 'Discrete Audio Tokens' inclusion criteria and would be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses discrete semantic tokens for CTR prediction in news recommendation, focusing on speed/space in embeddings, not on discrete audio tokens or audio representations (no audio signals, VQ/RVQ, or audio-language modeling). It therefore does not meet the 'Discrete Audio Tokens' inclusion criteria and would be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on discrete semantic tokenization for CTR prediction in recommendation systems and does not address discrete audio tokens or any methods related to audio signal processing, vector quantization of audio, or audio language modeling, which are the core inclusion criteria. It also lacks any description of token generation for audio that could be used in language or sequence models. Therefore, it falls outside the scope of the defined topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on discrete semantic tokenization for CTR prediction in recommendation systems and does not address discrete audio tokens or any methods related to audio signal processing, vector quantization of audio, or audio language modeling, which are the core inclusion criteria. It also lacks any description of token generation for audio that could be used in language or sequence models. Therefore, it falls outside the scope of the defined topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Codebook Transfer with Part-of-Speech for Vector-Quantized Image Modeling",
    "abstract": "Vector-Quantized Image Modeling (VQIM) is a fundamental research problem in image synthesis, which aims to represent an image with a discrete token sequence. Existing studies effectively address this problem by learning a discrete codebook from scratch and in a code-independent manner to quantize continuous representations into discrete tokens. However, learning a codebook from scratch and in a code-independent manner is highly challenging, which may be a key reason causing codebook collapse, i.e., some code vectors can rarely be optimized without regard to the relationship between codes and good codebook priors such that die off finally. In this paper, inspired by pretrained language models, we find that these language models have actually pretrained a superior codebook via a large number of text corpus, but such information is rarely exploited in VQIM. To this end, we propose a novel codebook transfer framework with part-of-speech, called VQCT, which aims to transfer a well-trained codebook from pretrained language models to VQIM for robust codebook learning. Specifically, we first introduce a pretrained codebook from language models and part-of-speech knowledge as priors. Then, we construct a vision-related codebook with these priors for achieving codebook transfer. Finally, a novel codebook transfer network is designed to exploit abundant semantic relationships between codes contained in pretrained codebooks for robust VQIM codebook learning. Experimental results on four datasets show that our VQCT method achieves superior VQIM performance over previous state-of-the-art methods.",
    "metadata": {
      "arxiv_id": "2403.10071",
      "title": "Codebook Transfer with Part-of-Speech for Vector-Quantized Image Modeling",
      "summary": "Vector-Quantized Image Modeling (VQIM) is a fundamental research problem in image synthesis, which aims to represent an image with a discrete token sequence. Existing studies effectively address this problem by learning a discrete codebook from scratch and in a code-independent manner to quantize continuous representations into discrete tokens. However, learning a codebook from scratch and in a code-independent manner is highly challenging, which may be a key reason causing codebook collapse, i.e., some code vectors can rarely be optimized without regard to the relationship between codes and good codebook priors such that die off finally. In this paper, inspired by pretrained language models, we find that these language models have actually pretrained a superior codebook via a large number of text corpus, but such information is rarely exploited in VQIM. To this end, we propose a novel codebook transfer framework with part-of-speech, called VQCT, which aims to transfer a well-trained codebook from pretrained language models to VQIM for robust codebook learning. Specifically, we first introduce a pretrained codebook from language models and part-of-speech knowledge as priors. Then, we construct a vision-related codebook with these priors for achieving codebook transfer. Finally, a novel codebook transfer network is designed to exploit abundant semantic relationships between codes contained in pretrained codebooks for robust VQIM codebook learning. Experimental results on four datasets show that our VQCT method achieves superior VQIM performance over previous state-of-the-art methods.",
      "authors": [
        "Baoquan Zhang",
        "Huaibin Wang",
        "Luo Chuyao",
        "Xutao Li",
        "Liang Guotao",
        "Yunming Ye",
        "Xiaochen Qi",
        "Yao He"
      ],
      "published": "2024-03-15T07:24:13Z",
      "updated": "2024-03-15T07:24:13Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.10071v1",
      "landing_url": "https://arxiv.org/abs/2403.10071v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.10071"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work targets Vector-Quantized Image Modeling and codebook transfer for images, not discrete audio tokens or audio tokenization methods, so it does not meet the inclusion criteria focused on discrete audio token design and related evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This work targets Vector-Quantized Image Modeling and codebook transfer for images, not discrete audio tokens or audio tokenization methods, so it does not meet the inclusion criteria focused on discrete audio token design and related evaluation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on Vector-Quantized Image Modeling (VQIM) and transferring codebooks pretrained on language models to image modeling tasks. It does not address audio signals or discrete audio tokens derived from continuous audio signals (speech, music, general audio), nor does it mention using audio-related tokenization or sequence modeling for audio generation or understanding. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and mainly presents a method in image domain with language model priors, unrelated to the defined audio token research scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on Vector-Quantized Image Modeling (VQIM) and transferring codebooks pretrained on language models to image modeling tasks. It does not address audio signals or discrete audio tokens derived from continuous audio signals (speech, music, general audio), nor does it mention using audio-related tokenization or sequence modeling for audio generation or understanding. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and mainly presents a method in image domain with language model priors, unrelated to the defined audio token research scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Exploring Tokenization Strategies and Vocabulary Sizes for Enhanced Arabic Language Models",
    "abstract": "This paper presents a comprehensive examination of the impact of tokenization strategies and vocabulary sizes on the performance of Arabic language models in downstream natural language processing tasks. Our investigation focused on the effectiveness of four tokenizers across various tasks, including News Classification, Hate Speech Detection, Sentiment Analysis, and Natural Language Inference. Leveraging a diverse set of vocabulary sizes, we scrutinize the intricate interplay between tokenization approaches and model performance. The results reveal that Byte Pair Encoding (BPE) with Farasa outperforms other strategies in multiple tasks, underscoring the significance of morphological analysis in capturing the nuances of the Arabic language. However, challenges arise in sentiment analysis, where dialect specific segmentation issues impact model efficiency. Computational efficiency analysis demonstrates the stability of BPE with Farasa, suggesting its practical viability. Our study uncovers limited impacts of vocabulary size on model performance while keeping the model size unchanged. This is challenging the established beliefs about the relationship between vocabulary, model size, and downstream tasks, emphasizing the need for the study of models' size and their corresponding vocabulary size to generalize across domains and mitigate biases, particularly in dialect based datasets. Paper's recommendations include refining tokenization strategies to address dialect challenges, enhancing model robustness across diverse linguistic contexts, and expanding datasets to encompass the rich dialect based Arabic. This work not only advances our understanding of Arabic language models but also lays the foundation for responsible and ethical developments in natural language processing technologies tailored to the intricacies of the Arabic language.",
    "metadata": {
      "arxiv_id": "2403.11130",
      "title": "Exploring Tokenization Strategies and Vocabulary Sizes for Enhanced Arabic Language Models",
      "summary": "This paper presents a comprehensive examination of the impact of tokenization strategies and vocabulary sizes on the performance of Arabic language models in downstream natural language processing tasks. Our investigation focused on the effectiveness of four tokenizers across various tasks, including News Classification, Hate Speech Detection, Sentiment Analysis, and Natural Language Inference. Leveraging a diverse set of vocabulary sizes, we scrutinize the intricate interplay between tokenization approaches and model performance. The results reveal that Byte Pair Encoding (BPE) with Farasa outperforms other strategies in multiple tasks, underscoring the significance of morphological analysis in capturing the nuances of the Arabic language. However, challenges arise in sentiment analysis, where dialect specific segmentation issues impact model efficiency. Computational efficiency analysis demonstrates the stability of BPE with Farasa, suggesting its practical viability. Our study uncovers limited impacts of vocabulary size on model performance while keeping the model size unchanged. This is challenging the established beliefs about the relationship between vocabulary, model size, and downstream tasks, emphasizing the need for the study of models' size and their corresponding vocabulary size to generalize across domains and mitigate biases, particularly in dialect based datasets. Paper's recommendations include refining tokenization strategies to address dialect challenges, enhancing model robustness across diverse linguistic contexts, and expanding datasets to encompass the rich dialect based Arabic. This work not only advances our understanding of Arabic language models but also lays the foundation for responsible and ethical developments in natural language processing technologies tailored to the intricacies of the Arabic language.",
      "authors": [
        "Mohamed Taher Alrefaie",
        "Nour Eldin Morsy",
        "Nada Samir"
      ],
      "published": "2024-03-17T07:44:44Z",
      "updated": "2024-09-20T16:48:58Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.11130v2",
      "landing_url": "https://arxiv.org/abs/2403.11130v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.11130"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item studies text-based tokenization strategies for Arabic language models in NLP tasks, which does not address discrete audio tokens or audio discretization methods; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item studies text-based tokenization strategies for Arabic language models in NLP tasks, which does not address discrete audio tokens or audio discretization methods; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on tokenization strategies for Arabic language models in NLP tasks and does not address discrete audio tokens or their generation through vector quantization or similar methods as defined in the inclusion criteria. It lacks any mention of audio signals, audio token design, or audio-related representations, which are core to the inclusion criteria. Therefore, it does not meet the inclusion criteria nor is related to the topic of discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on tokenization strategies for Arabic language models in NLP tasks and does not address discrete audio tokens or their generation through vector quantization or similar methods as defined in the inclusion criteria. It lacks any mention of audio signals, audio token design, or audio-related representations, which are core to the inclusion criteria. Therefore, it does not meet the inclusion criteria nor is related to the topic of discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Using Generative Text Models to Create Qualitative Codebooks for Student Evaluations of Teaching",
    "abstract": "Feedback is a critical aspect of improvement. Unfortunately, when there is a lot of feedback from multiple sources, it can be difficult to distill the information into actionable insights. Consider student evaluations of teaching (SETs), which are important sources of feedback for educators. They can give instructors insights into what worked during a semester. A collection of SETs can also be useful to administrators as signals for courses or entire programs. However, on a large scale as in high-enrollment courses or administrative records over several years, the volume of SETs can render them difficult to analyze. In this paper, we discuss a novel method for analyzing SETs using natural language processing (NLP) and large language models (LLMs). We demonstrate the method by applying it to a corpus of 5,000 SETs from a large public university. We show that the method can be used to extract, embed, cluster, and summarize the SETs to identify the themes they express. More generally, this work illustrates how to use the combination of NLP techniques and LLMs to generate a codebook for SETs. We conclude by discussing the implications of this method for analyzing SETs and other types of student writing in teaching and research settings.",
    "metadata": {
      "arxiv_id": "2403.11984",
      "title": "Using Generative Text Models to Create Qualitative Codebooks for Student Evaluations of Teaching",
      "summary": "Feedback is a critical aspect of improvement. Unfortunately, when there is a lot of feedback from multiple sources, it can be difficult to distill the information into actionable insights. Consider student evaluations of teaching (SETs), which are important sources of feedback for educators. They can give instructors insights into what worked during a semester. A collection of SETs can also be useful to administrators as signals for courses or entire programs. However, on a large scale as in high-enrollment courses or administrative records over several years, the volume of SETs can render them difficult to analyze. In this paper, we discuss a novel method for analyzing SETs using natural language processing (NLP) and large language models (LLMs). We demonstrate the method by applying it to a corpus of 5,000 SETs from a large public university. We show that the method can be used to extract, embed, cluster, and summarize the SETs to identify the themes they express. More generally, this work illustrates how to use the combination of NLP techniques and LLMs to generate a codebook for SETs. We conclude by discussing the implications of this method for analyzing SETs and other types of student writing in teaching and research settings.",
      "authors": [
        "Andrew Katz",
        "Mitchell Gerhardt",
        "Michelle Soledad"
      ],
      "published": "2024-03-18T17:21:35Z",
      "updated": "2024-03-18T17:21:35Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.11984v1",
      "landing_url": "https://arxiv.org/abs/2403.11984v1",
      "doi": null
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on using NLP/LLMs to analyze student evaluations of teaching and does not address discrete audio tokens or related tokenization/quantization methods, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on using NLP/LLMs to analyze student evaluations of teaching and does not address discrete audio tokens or related tokenization/quantization methods, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on analyzing student evaluations of teaching using NLP and large language models, without addressing any discrete audio token generation, quantization, or related audio representation methods described in the inclusion criteria. There is no mention of audio signal processing, vector quantization, or discrete tokens relevant to audio, which are core requirements for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on analyzing student evaluations of teaching using NLP and large language models, without addressing any discrete audio token generation, quantization, or related audio representation methods described in the inclusion criteria. There is no mention of audio signal processing, vector quantization, or discrete tokens relevant to audio, which are core requirements for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Agentic AI: The Era of Semantic Decoding",
    "abstract": "Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.",
    "metadata": {
      "arxiv_id": "2403.14562",
      "title": "Agentic AI: The Era of Semantic Decoding",
      "summary": "Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.",
      "authors": [
        "Maxime Peyrard",
        "Martin Josifoski",
        "Robert West"
      ],
      "published": "2024-03-21T17:06:17Z",
      "updated": "2025-04-29T15:24:52Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.14562v2",
      "landing_url": "https://arxiv.org/abs/2403.14562v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.14562"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on semantic decoding and semantic tokens in AI collaboration, not on discrete audio token design or reproducible methods for audio tokens, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on semantic decoding and semantic tokens in AI collaboration, not on discrete audio token design or reproducible methods for audio tokens, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses a novel concept called semantic decoding involving collaborations between language models, humans, and tools in semantic space, focusing on semantic tokens as meaningful information units. However, it does not mention discrete audio tokens, audio signal quantization, vector quantization, or any audio-related tokenization, which are the core focus of the inclusion criteria. Thus, it does not meet the inclusion criteria related to discrete audio tokens, nor does it provide method details or evaluations related to audio token generation or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses a novel concept called semantic decoding involving collaborations between language models, humans, and tools in semantic space, focusing on semantic tokens as meaningful information units. However, it does not mention discrete audio tokens, audio signal quantization, vector quantization, or any audio-related tokenization, which are the core focus of the inclusion criteria. Thus, it does not meet the inclusion criteria related to discrete audio tokens, nor does it provide method details or evaluations related to audio token generation or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learning Sampling Distribution and Safety Filter for Autonomous Driving with VQ-VAE and Differentiable Optimization",
    "abstract": "Sampling trajectories from a distribution followed by ranking them based on a specified cost function is a common approach in autonomous driving. Typically, the sampling distribution is hand-crafted (e.g a Gaussian, or a grid). Recently, there have been efforts towards learning the sampling distribution through generative models such as Conditional Variational Autoencoder (CVAE). However, these approaches fail to capture the multi-modality of the driving behaviour due to the Gaussian latent prior of the CVAE. Thus, in this paper, we re-imagine the distribution learning through vector quantized variational autoencoder (VQ-VAE), whose discrete latent-space is well equipped to capture multi-modal sampling distribution. The VQ-VAE is trained with demonstration data of optimal trajectories. We further propose a differentiable optimization based safety filter to minimally correct the VQVAE sampled trajectories to ensure collision avoidance. We use backpropagation through the optimization layers in a self-supervised learning set-up to learn good initialization and optimal parameters of the safety filter. We perform extensive comparisons with state-of-the-art CVAE-based baseline in dense and aggressive traffic scenarios and show a reduction of up to 12 times in collision-rate while being competitive in driving speeds.",
    "metadata": {
      "arxiv_id": "2403.19461",
      "title": "Learning Sampling Distribution and Safety Filter for Autonomous Driving with VQ-VAE and Differentiable Optimization",
      "summary": "Sampling trajectories from a distribution followed by ranking them based on a specified cost function is a common approach in autonomous driving. Typically, the sampling distribution is hand-crafted (e.g a Gaussian, or a grid). Recently, there have been efforts towards learning the sampling distribution through generative models such as Conditional Variational Autoencoder (CVAE). However, these approaches fail to capture the multi-modality of the driving behaviour due to the Gaussian latent prior of the CVAE. Thus, in this paper, we re-imagine the distribution learning through vector quantized variational autoencoder (VQ-VAE), whose discrete latent-space is well equipped to capture multi-modal sampling distribution. The VQ-VAE is trained with demonstration data of optimal trajectories. We further propose a differentiable optimization based safety filter to minimally correct the VQVAE sampled trajectories to ensure collision avoidance. We use backpropagation through the optimization layers in a self-supervised learning set-up to learn good initialization and optimal parameters of the safety filter. We perform extensive comparisons with state-of-the-art CVAE-based baseline in dense and aggressive traffic scenarios and show a reduction of up to 12 times in collision-rate while being competitive in driving speeds.",
      "authors": [
        "Simon Idoko",
        "Basant Sharma",
        "Arun Kumar Singh"
      ],
      "published": "2024-03-28T14:32:57Z",
      "updated": "2024-04-25T14:29:08Z",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.19461v2",
      "landing_url": "https://arxiv.org/abs/2403.19461v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.19461"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about learning sampling distributions for autonomous driving using VQ-VAE on trajectory data and a differentiable safety filter; it targets motion planning representations, not discrete audio tokens, and does not concentrate on audio token design or audio-language modeling as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about learning sampling distributions for autonomous driving using VQ-VAE on trajectory data and a differentiable safety filter; it targets motion planning representations, not discrete audio tokens, and does not concentrate on audio token design or audio-language modeling as required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on autonomous driving using VQ-VAE for learning sampling distributions and safety filtering, which is unrelated to the discrete audio tokens domain defined by the inclusion criteria that revolves around audio signals, audio token generation via vector quantization, and applications in audio language modeling or audio understanding. Therefore, it does not meet the specific topic requirements for discrete audio token research and does not discuss token generation for audio sequences.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on autonomous driving using VQ-VAE for learning sampling distributions and safety filtering, which is unrelated to the discrete audio tokens domain defined by the inclusion criteria that revolves around audio signals, audio token generation via vector quantization, and applications in audio language modeling or audio understanding. Therefore, it does not meet the specific topic requirements for discrete audio token research and does not discuss token generation for audio sequences.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Fuzzy K-Means Clustering without Cluster Centroids",
    "abstract": "Fuzzy K-Means clustering is a critical technique in unsupervised data analysis. Unlike traditional hard clustering algorithms such as K-Means, it allows data points to belong to multiple clusters with varying degrees of membership, determined through iterative optimization to establish optimal cluster centers and memberships, thereby achieving fuzzy partitioning of data. However, the performance of popular Fuzzy K-Means algorithms is sensitive to the selection of initial cluster centroids and is also affected by noise when updating mean cluster centroids. To address these challenges, this paper proposes a novel Fuzzy \\textit{K}-Means clustering algorithm that entirely eliminates the reliance on cluster centroids, obtaining membership metrics solely through distance matrix computation. This innovation enhances flexibility in distance measurement between sample points, thus improving the algorithm's performance and robustness. The paper also establishes theoretical connections between the proposed model and popular Fuzzy K-Means clustering techniques. Experimental results on several real datasets demonstrate the effectiveness of the algorithm.",
    "metadata": {
      "arxiv_id": "2404.04940",
      "title": "Fuzzy K-Means Clustering without Cluster Centroids",
      "summary": "Fuzzy K-Means clustering is a critical technique in unsupervised data analysis. Unlike traditional hard clustering algorithms such as K-Means, it allows data points to belong to multiple clusters with varying degrees of membership, determined through iterative optimization to establish optimal cluster centers and memberships, thereby achieving fuzzy partitioning of data. However, the performance of popular Fuzzy K-Means algorithms is sensitive to the selection of initial cluster centroids and is also affected by noise when updating mean cluster centroids. To address these challenges, this paper proposes a novel Fuzzy \\textit{K}-Means clustering algorithm that entirely eliminates the reliance on cluster centroids, obtaining membership metrics solely through distance matrix computation. This innovation enhances flexibility in distance measurement between sample points, thus improving the algorithm's performance and robustness. The paper also establishes theoretical connections between the proposed model and popular Fuzzy K-Means clustering techniques. Experimental results on several real datasets demonstrate the effectiveness of the algorithm.",
      "authors": [
        "Yichen Bao",
        "Han Lu",
        "Quanxue Gao"
      ],
      "published": "2024-04-07T12:25:03Z",
      "updated": "2024-11-07T08:59:23Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.04940v2",
      "landing_url": "https://arxiv.org/abs/2404.04940v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.04940"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses Fuzzy K-Means clustering without centroids and does not address discrete audio tokens, token generation, vector quantization, or related SL/ASR/audio-language modeling aims; it fails all inclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses Fuzzy K-Means clustering without centroids and does not address discrete audio tokens, token generation, vector quantization, or related SL/ASR/audio-language modeling aims; it fails all inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on a novel fuzzy K-Means clustering algorithm without cluster centroids, pertaining to unsupervised data analysis and clustering methods, but it does not discuss discrete audio tokens, audio signal processing, vector quantization, or related token generation and usage for audio language models as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on a novel fuzzy K-Means clustering algorithm without cluster centroids, pertaining to unsupervised data analysis and clustering methods, but it does not discuss discrete audio tokens, audio signal processing, vector quantization, or related token generation and usage for audio language models as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge",
    "abstract": "Discrete speech tokens have been more and more popular in multiple speech processing fields, including automatic speech recognition (ASR), text-to-speech (TTS) and singing voice synthesis (SVS). In this paper, we describe the systems developed by the SJTU X-LANCE group for the TTS (acoustic + vocoder), SVS, and ASR tracks in the Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge. Notably, we achieved 1st rank on the leaderboard in the TTS track both with the whole training set and only 1h training data, with the highest UTMOS score and lowest bitrate among all submissions.",
    "metadata": {
      "arxiv_id": "2404.06079",
      "title": "The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge",
      "summary": "Discrete speech tokens have been more and more popular in multiple speech processing fields, including automatic speech recognition (ASR), text-to-speech (TTS) and singing voice synthesis (SVS). In this paper, we describe the systems developed by the SJTU X-LANCE group for the TTS (acoustic + vocoder), SVS, and ASR tracks in the Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge. Notably, we achieved 1st rank on the leaderboard in the TTS track both with the whole training set and only 1h training data, with the highest UTMOS score and lowest bitrate among all submissions.",
      "authors": [
        "Yiwei Guo",
        "Chenrun Wang",
        "Yifan Yang",
        "Hankun Wang",
        "Ziyang Ma",
        "Chenpeng Du",
        "Shuai Wang",
        "Hanzheng Li",
        "Shuai Fan",
        "Hui Zhang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-04-09T07:37:41Z",
      "updated": "2024-04-10T00:33:25Z",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.06079v2",
      "landing_url": "https://arxiv.org/abs/2404.06079v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.06079"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on discrete speech tokens and reports systems for TTS/ASR/SVS in a discrete speech unit challenge, aligning with the topic; however the abstract does not reveal explicit token generation/quantization details, though as a technical report it likely includes methodological content for reproducibility.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item centers on discrete speech tokens and reports systems for TTS/ASR/SVS in a discrete speech unit challenge, aligning with the topic; however the abstract does not reveal explicit token generation/quantization details, though as a technical report it likely includes methodological content for reproducibility.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses systems for speech processing in the Interspeech 2024 challenge focusing on discrete speech units, mentioning rank achievements and tasks (TTS, SVS, ASR). However, the abstract lacks detailed technical description on the discrete token generation process, quantization methods, codebook/layered design or explicit emphasis on token usage for sequence/language modeling, which is required for inclusion. The study reports results and leaderboard status but does not provide method transparency or reproducibility details critical to meeting inclusion criteria while also excluding simple system demos and blog-level content. Thus, it leans toward exclusion due to insufficient method details and focus on token formation and usage as a core contribution.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper discusses systems for speech processing in the Interspeech 2024 challenge focusing on discrete speech units, mentioning rank achievements and tasks (TTS, SVS, ASR). However, the abstract lacks detailed technical description on the discrete token generation process, quantization methods, codebook/layered design or explicit emphasis on token usage for sequence/language modeling, which is required for inclusion. The study reports results and leaderboard status but does not provide method transparency or reproducibility details critical to meeting inclusion criteria while also excluding simple system demos and blog-level content. Thus, it leans toward exclusion due to insufficient method details and focus on token formation and usage as a core contribution.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior reviewers: JuniorNano (score 4) considered the item likely relevant as a technical report on discrete speech units, while JuniorMini (score 2) flagged that the abstract lacks concrete method details. The abstract describes systems for TTS/SVS/ASR in a discrete speech unit challenge and reports leaderboard performance, but it does not specify how discrete tokens are produced (e.g., VQ/RVQ/codebook/layering), nor does it state token-design as a central methodological contribution; this matches the exclusion criterion for system/demo/leaderboard content lacking reproducible method details. Because inclusion requires clear, reviewable token-generation/quantization descriptions and methodological contribution, and the abstract does not provide these, I recommend exclusion. 1-sentence explanation for the score: The abstract appears to be a systems/leaderboard report without the necessary, reviewable details on discrete token generation and quantization, so it should be excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior reviewers: JuniorNano (score 4) considered the item likely relevant as a technical report on discrete speech units, while JuniorMini (score 2) flagged that the abstract lacks concrete method details. The abstract describes systems for TTS/SVS/ASR in a discrete speech unit challenge and reports leaderboard performance, but it does not specify how discrete tokens are produced (e.g., VQ/RVQ/codebook/layering), nor does it state token-design as a central methodological contribution; this matches the exclusion criterion for system/demo/leaderboard content lacking reproducible method details. Because inclusion requires clear, reviewable token-generation/quantization descriptions and methodological contribution, and the abstract does not provide these, I recommend exclusion. 1-sentence explanation for the score: The abstract appears to be a systems/leaderboard report without the necessary, reviewable details on discrete token generation and quantization, so it should be excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "CoST: Contrastive Quantization based Semantic Tokenization for Generative Recommendation",
    "abstract": "Embedding-based retrieval serves as a dominant approach to candidate item matching for industrial recommender systems. With the success of generative AI, generative retrieval has recently emerged as a new retrieval paradigm for recommendation, which casts item retrieval as a generation problem. Its model consists of two stages: semantic tokenization and autoregressive generation. The first stage involves item tokenization that constructs discrete semantic tokens to index items, while the second stage autoregressively generates semantic tokens of candidate items. Therefore, semantic tokenization serves as a crucial preliminary step for training generative recommendation models. Existing research usually employs a vector quantizier with reconstruction loss (e.g., RQ-VAE) to obtain semantic tokens of items, but this method fails to capture the essential neighborhood relationships that are vital for effective item modeling in recommender systems. In this paper, we propose a contrastive quantization-based semantic tokenization approach, named CoST, which harnesses both item relationships and semantic information to learn semantic tokens. Our experimental results highlight the significant impact of semantic tokenization on generative recommendation performance, with CoST achieving up to a 43% improvement in Recall@5 and 44% improvement in NDCG@5 on the MIND dataset over previous baselines.",
    "metadata": {
      "arxiv_id": "2404.14774",
      "title": "CoST: Contrastive Quantization based Semantic Tokenization for Generative Recommendation",
      "summary": "Embedding-based retrieval serves as a dominant approach to candidate item matching for industrial recommender systems. With the success of generative AI, generative retrieval has recently emerged as a new retrieval paradigm for recommendation, which casts item retrieval as a generation problem. Its model consists of two stages: semantic tokenization and autoregressive generation. The first stage involves item tokenization that constructs discrete semantic tokens to index items, while the second stage autoregressively generates semantic tokens of candidate items. Therefore, semantic tokenization serves as a crucial preliminary step for training generative recommendation models. Existing research usually employs a vector quantizier with reconstruction loss (e.g., RQ-VAE) to obtain semantic tokens of items, but this method fails to capture the essential neighborhood relationships that are vital for effective item modeling in recommender systems. In this paper, we propose a contrastive quantization-based semantic tokenization approach, named CoST, which harnesses both item relationships and semantic information to learn semantic tokens. Our experimental results highlight the significant impact of semantic tokenization on generative recommendation performance, with CoST achieving up to a 43% improvement in Recall@5 and 44% improvement in NDCG@5 on the MIND dataset over previous baselines.",
      "authors": [
        "Jieming Zhu",
        "Mengqun Jin",
        "Qijiong Liu",
        "Zexuan Qiu",
        "Zhenhua Dong",
        "Xiu Li"
      ],
      "published": "2024-04-23T06:29:48Z",
      "updated": "2024-09-07T16:11:36Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.14774v2",
      "landing_url": "https://arxiv.org/abs/2404.14774v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.14774"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本研究聚焦於生成式推薦的語義 tokenization，與離散音訊 token 的設計（如 VQ/RVQ、層次量化等用於語言模型/音訊生成的離散表徵）不符，因此不符合納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本研究聚焦於生成式推薦的語義 tokenization，與離散音訊 token 的設計（如 VQ/RVQ、層次量化等用於語言模型/音訊生成的離散表徵）不符，因此不符合納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on semantic tokenization for generative recommendation in recommender systems, using contrastive quantization to generate semantic tokens for items; however, it does not involve discrete audio tokens derived from continuous audio signals (speech, music, or general audio), nor does it relate to audio language modeling or audio generation tasks. Therefore, it does not meet the inclusion criteria related to discrete audio token design and usage for audio modeling tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on semantic tokenization for generative recommendation in recommender systems, using contrastive quantization to generate semantic tokens for items; however, it does not involve discrete audio tokens derived from continuous audio signals (speech, music, or general audio), nor does it relate to audio language modeling or audio generation tasks. Therefore, it does not meet the inclusion criteria related to discrete audio token design and usage for audio modeling tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The THU-HCSI Multi-Speaker Multi-Lingual Few-Shot Voice Cloning System for LIMMITS'24 Challenge",
    "abstract": "This paper presents the multi-speaker multi-lingual few-shot voice cloning system developed by THU-HCSI team for LIMMITS'24 Challenge. To achieve high speaker similarity and naturalness in both mono-lingual and cross-lingual scenarios, we build the system upon YourTTS and add several enhancements. For further improving speaker similarity and speech quality, we introduce speaker-aware text encoder and flow-based decoder with Transformer blocks. In addition, we denoise the few-shot data, mix up them with pre-training data, and adopt a speaker-balanced sampling strategy to guarantee effective fine-tuning for target speakers. The official evaluations in track 1 show that our system achieves the best speaker similarity MOS of 4.25 and obtains considerable naturalness MOS of 3.97.",
    "metadata": {
      "arxiv_id": "2404.16619",
      "title": "The THU-HCSI Multi-Speaker Multi-Lingual Few-Shot Voice Cloning System for LIMMITS'24 Challenge",
      "summary": "This paper presents the multi-speaker multi-lingual few-shot voice cloning system developed by THU-HCSI team for LIMMITS'24 Challenge. To achieve high speaker similarity and naturalness in both mono-lingual and cross-lingual scenarios, we build the system upon YourTTS and add several enhancements. For further improving speaker similarity and speech quality, we introduce speaker-aware text encoder and flow-based decoder with Transformer blocks. In addition, we denoise the few-shot data, mix up them with pre-training data, and adopt a speaker-balanced sampling strategy to guarantee effective fine-tuning for target speakers. The official evaluations in track 1 show that our system achieves the best speaker similarity MOS of 4.25 and obtains considerable naturalness MOS of 3.97.",
      "authors": [
        "Yixuan Zhou",
        "Shuoyi Zhou",
        "Shun Lei",
        "Zhiyong Wu",
        "Menglin Wu"
      ],
      "published": "2024-04-25T14:02:25Z",
      "updated": "2024-04-25T14:02:25Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.16619v1",
      "landing_url": "https://arxiv.org/abs/2404.16619v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.16619"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on voice cloning system improvements for speaker similarity and naturalness, with no description of discrete audio token design or tokenization methods, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on voice cloning system improvements for speaker similarity and naturalness, with no description of discrete audio token design or tokenization methods, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a voice cloning system focused on speaker similarity and naturalness improvements, built upon YourTTS with several enhancements; however, there is no mention of discrete audio tokens, vector quantization, or any tokenization mechanism for audio representation that fits the inclusion criteria. The abstract lacks details on token generation, quantization methods, or how discrete tokens are used for sequence modeling or language models, which are critical for inclusion. Therefore, the study does not meet the inclusion requirements nor talks about discrete audio tokenization, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a voice cloning system focused on speaker similarity and naturalness improvements, built upon YourTTS with several enhancements; however, there is no mention of discrete audio tokens, vector quantization, or any tokenization mechanism for audio representation that fits the inclusion criteria. The abstract lacks details on token generation, quantization methods, or how discrete tokens are used for sequence modeling or language models, which are critical for inclusion. Therefore, the study does not meet the inclusion requirements nor talks about discrete audio tokenization, so it should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Automatic Speech Recognition System-Independent Word Error Rate Estimation",
    "abstract": "Word error rate (WER) is a metric used to evaluate the quality of transcriptions produced by Automatic Speech Recognition (ASR) systems. In many applications, it is of interest to estimate WER given a pair of a speech utterance and a transcript. Previous work on WER estimation focused on building models that are trained with a specific ASR system in mind (referred to as ASR system-dependent). These are also domain-dependent and inflexible in real-world applications. In this paper, a hypothesis generation method for ASR System-Independent WER estimation (SIWE) is proposed. In contrast to prior work, the WER estimators are trained using data that simulates ASR system output. Hypotheses are generated using phonetically similar or linguistically more likely alternative words. In WER estimation experiments, the proposed method reaches a similar performance to ASR system-dependent WER estimators on in-domain data and achieves state-of-the-art performance on out-of-domain data. On the out-of-domain data, the SIWE model outperformed the baseline estimators in root mean square error and Pearson correlation coefficient by relative 17.58% and 18.21%, respectively, on Switchboard and CALLHOME. The performance was further improved when the WER of the training set was close to the WER of the evaluation dataset.",
    "metadata": {
      "arxiv_id": "2404.16743",
      "title": "Automatic Speech Recognition System-Independent Word Error Rate Estimation",
      "summary": "Word error rate (WER) is a metric used to evaluate the quality of transcriptions produced by Automatic Speech Recognition (ASR) systems. In many applications, it is of interest to estimate WER given a pair of a speech utterance and a transcript. Previous work on WER estimation focused on building models that are trained with a specific ASR system in mind (referred to as ASR system-dependent). These are also domain-dependent and inflexible in real-world applications. In this paper, a hypothesis generation method for ASR System-Independent WER estimation (SIWE) is proposed. In contrast to prior work, the WER estimators are trained using data that simulates ASR system output. Hypotheses are generated using phonetically similar or linguistically more likely alternative words. In WER estimation experiments, the proposed method reaches a similar performance to ASR system-dependent WER estimators on in-domain data and achieves state-of-the-art performance on out-of-domain data. On the out-of-domain data, the SIWE model outperformed the baseline estimators in root mean square error and Pearson correlation coefficient by relative 17.58% and 18.21%, respectively, on Switchboard and CALLHOME. The performance was further improved when the WER of the training set was close to the WER of the evaluation dataset.",
      "authors": [
        "Chanho Park",
        "Mingjie Chen",
        "Thomas Hain"
      ],
      "published": "2024-04-25T16:57:05Z",
      "updated": "2024-04-26T11:11:02Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.16743v2",
      "landing_url": "https://arxiv.org/abs/2404.16743v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.16743"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on ASR system-independent WER estimation and does not involve discrete audio token design or token quantization, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on ASR system-independent WER estimation and does not involve discrete audio token design or token quantization, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on estimating word error rate (WER) for automatic speech recognition systems and does not discuss or propose any method involving discrete audio tokens, their generation via vector quantization or semantic-driven discretization, nor their application for audio language modeling or sequence modeling. The core method is about WER estimation rather than discrete token design or usage, so it does not meet the inclusion criteria targeting discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on estimating word error rate (WER) for automatic speech recognition systems and does not discuss or propose any method involving discrete audio tokens, their generation via vector quantization or semantic-driven discretization, nor their application for audio language modeling or sequence modeling. The core method is about WER estimation rather than discrete token design or usage, so it does not meet the inclusion criteria targeting discrete audio token research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MAIN-VC: Lightweight Speech Representation Disentanglement for One-shot Voice Conversion",
    "abstract": "One-shot voice conversion aims to change the timbre of any source speech to match that of the unseen target speaker with only one speech sample. Existing methods face difficulties in satisfactory speech representation disentanglement and suffer from sizable networks as some of them leverage numerous complex modules for disentanglement. In this paper, we propose a model named MAIN-VC to effectively disentangle via a concise neural network. The proposed model utilizes Siamese encoders to learn clean representations, further enhanced by the designed mutual information estimator. The Siamese structure and the newly designed convolution module contribute to the lightweight of our model while ensuring performance in diverse voice conversion tasks. The experimental results show that the proposed model achieves comparable subjective scores and exhibits improvements in objective metrics compared to existing methods in a one-shot voice conversion scenario.",
    "metadata": {
      "arxiv_id": "2405.00930",
      "title": "MAIN-VC: Lightweight Speech Representation Disentanglement for One-shot Voice Conversion",
      "summary": "One-shot voice conversion aims to change the timbre of any source speech to match that of the unseen target speaker with only one speech sample. Existing methods face difficulties in satisfactory speech representation disentanglement and suffer from sizable networks as some of them leverage numerous complex modules for disentanglement. In this paper, we propose a model named MAIN-VC to effectively disentangle via a concise neural network. The proposed model utilizes Siamese encoders to learn clean representations, further enhanced by the designed mutual information estimator. The Siamese structure and the newly designed convolution module contribute to the lightweight of our model while ensuring performance in diverse voice conversion tasks. The experimental results show that the proposed model achieves comparable subjective scores and exhibits improvements in objective metrics compared to existing methods in a one-shot voice conversion scenario.",
      "authors": [
        "Pengcheng Li",
        "Jianzong Wang",
        "Xulong Zhang",
        "Yong Zhang",
        "Jing Xiao",
        "Ning Cheng"
      ],
      "published": "2024-05-02T01:11:15Z",
      "updated": "2024-11-24T09:30:29Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.00930v2",
      "landing_url": "https://arxiv.org/abs/2405.00930v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.00930"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets lightweight disentanglement for one-shot voice conversion and does not discuss discrete audio tokens (VQ/RVQ/token-based representations) or token-generation workflows.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets lightweight disentanglement for one-shot voice conversion and does not discuss discrete audio tokens (VQ/RVQ/token-based representations) or token-generation workflows.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on lightweight speech representation disentanglement for one-shot voice conversion; however, it does not mention discrete audio token generation via vector quantization or similar methods, nor does it clarify producing tokens for sequence or language model processing, which is central to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on lightweight speech representation disentanglement for one-shot voice conversion; however, it does not mention discrete audio token generation via vector quantization or similar methods, nor does it clarify producing tokens for sequence or language model processing, which is central to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Self-Supervised Learning for Real-World Super-Resolution from Dual and Multiple Zoomed Observations",
    "abstract": "In this paper, we consider two challenging issues in reference-based super-resolution (RefSR) for smartphone, (i) how to choose a proper reference image, and (ii) how to learn RefSR in a self-supervised manner. Particularly, we propose a novel self-supervised learning approach for real-world RefSR from observations at dual and multiple camera zooms. Firstly, considering the popularity of multiple cameras in modern smartphones, the more zoomed (telephoto) image can be naturally leveraged as the reference to guide the super-resolution (SR) of the lesser zoomed (ultra-wide) image, which gives us a chance to learn a deep network that performs SR from the dual zoomed observations (DZSR). Secondly, for self-supervised learning of DZSR, we take the telephoto image instead of an additional high-resolution image as the supervision information, and select a center patch from it as the reference to super-resolve the corresponding ultra-wide image patch. To mitigate the effect of the misalignment between ultra-wide low-resolution (LR) patch and telephoto ground-truth (GT) image during training, we first adopt patch-based optical flow alignment and then design an auxiliary-LR to guide the deforming of the warped LR features. To generate visually pleasing results, we present local overlapped sliced Wasserstein loss to better represent the perceptual difference between GT and output in the feature space. During testing, DZSR can be directly deployed to super-solve the whole ultra-wide image with the reference of the telephoto image. In addition, we further take multiple zoomed observations to explore self-supervised RefSR, and present a progressive fusion scheme for the effective utilization of reference images. Experiments show that our methods achieve better quantitative and qualitative performance against state-of-the-arts. Codes are available at https://github.com/cszhilu1998/SelfDZSR_PlusPlus.",
    "metadata": {
      "arxiv_id": "2405.02171",
      "title": "Self-Supervised Learning for Real-World Super-Resolution from Dual and Multiple Zoomed Observations",
      "summary": "In this paper, we consider two challenging issues in reference-based super-resolution (RefSR) for smartphone, (i) how to choose a proper reference image, and (ii) how to learn RefSR in a self-supervised manner. Particularly, we propose a novel self-supervised learning approach for real-world RefSR from observations at dual and multiple camera zooms. Firstly, considering the popularity of multiple cameras in modern smartphones, the more zoomed (telephoto) image can be naturally leveraged as the reference to guide the super-resolution (SR) of the lesser zoomed (ultra-wide) image, which gives us a chance to learn a deep network that performs SR from the dual zoomed observations (DZSR). Secondly, for self-supervised learning of DZSR, we take the telephoto image instead of an additional high-resolution image as the supervision information, and select a center patch from it as the reference to super-resolve the corresponding ultra-wide image patch. To mitigate the effect of the misalignment between ultra-wide low-resolution (LR) patch and telephoto ground-truth (GT) image during training, we first adopt patch-based optical flow alignment and then design an auxiliary-LR to guide the deforming of the warped LR features. To generate visually pleasing results, we present local overlapped sliced Wasserstein loss to better represent the perceptual difference between GT and output in the feature space. During testing, DZSR can be directly deployed to super-solve the whole ultra-wide image with the reference of the telephoto image. In addition, we further take multiple zoomed observations to explore self-supervised RefSR, and present a progressive fusion scheme for the effective utilization of reference images. Experiments show that our methods achieve better quantitative and qualitative performance against state-of-the-arts. Codes are available at https://github.com/cszhilu1998/SelfDZSR_PlusPlus.",
      "authors": [
        "Zhilu Zhang",
        "Ruohao Wang",
        "Hongzhi Zhang",
        "Wangmeng Zuo"
      ],
      "published": "2024-05-03T15:20:30Z",
      "updated": "2024-05-03T15:20:30Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.02171v1",
      "landing_url": "https://arxiv.org/abs/2405.02171v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.02171"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper concerns self-supervised real-world super-resolution from dual/multi-zoom smartphone observations, not discrete audio tokens or audio tokenization, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper concerns self-supervised real-world super-resolution from dual/multi-zoom smartphone observations, not discrete audio tokens or audio tokenization, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on self-supervised learning for super-resolution in smartphone camera images, specifically using dual and multiple zoomed observations. The study is centered on image processing and enhancement techniques rather than discrete audio tokens or audio representations. It does not discuss the generation, quantization, or use of discrete audio tokens for language or sequence modeling, which are the core inclusion criteria. Therefore, it does not meet the inclusion criteria related to discrete audio tokens, nor does it relate to audio signal processing as required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on self-supervised learning for super-resolution in smartphone camera images, specifically using dual and multiple zoomed observations. The study is centered on image processing and enhancement techniques rather than discrete audio tokens or audio representations. It does not discuss the generation, quantization, or use of discrete audio tokens for language or sequence modeling, which are the core inclusion criteria. Therefore, it does not meet the inclusion criteria related to discrete audio tokens, nor does it relate to audio signal processing as required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adaptive Semantic Token Selection for AI-native Goal-oriented Communications",
    "abstract": "In this paper, we propose a novel design for AI-native goal-oriented communications, exploiting transformer neural networks under dynamic inference constraints on bandwidth and computation. Transformers have become the standard architecture for pretraining large-scale vision and text models, and preliminary results have shown promising performance also in deep joint source-channel coding (JSCC). Here, we consider a dynamic model where communication happens over a channel with variable latency and bandwidth constraints. Leveraging recent works on conditional computation, we exploit the structure of the transformer blocks and the multihead attention operator to design a trainable semantic token selection mechanism that learns to select relevant tokens (e.g., image patches) from the input signal. This is done dynamically, on a per-input basis, with a rate that can be chosen as an additional input by the user. We show that our model improves over state-of-the-art token selection mechanisms, exhibiting high accuracy for a wide range of latency and bandwidth constraints, without the need for deploying multiple architectures tailored to each constraint. Last, but not least, the proposed token selection mechanism helps extract powerful semantics that are easy to understand and explain, paving the way for interpretable-by-design models for the next generation of AI-native communication systems.",
    "metadata": {
      "arxiv_id": "2405.02330",
      "title": "Adaptive Semantic Token Selection for AI-native Goal-oriented Communications",
      "summary": "In this paper, we propose a novel design for AI-native goal-oriented communications, exploiting transformer neural networks under dynamic inference constraints on bandwidth and computation. Transformers have become the standard architecture for pretraining large-scale vision and text models, and preliminary results have shown promising performance also in deep joint source-channel coding (JSCC). Here, we consider a dynamic model where communication happens over a channel with variable latency and bandwidth constraints. Leveraging recent works on conditional computation, we exploit the structure of the transformer blocks and the multihead attention operator to design a trainable semantic token selection mechanism that learns to select relevant tokens (e.g., image patches) from the input signal. This is done dynamically, on a per-input basis, with a rate that can be chosen as an additional input by the user. We show that our model improves over state-of-the-art token selection mechanisms, exhibiting high accuracy for a wide range of latency and bandwidth constraints, without the need for deploying multiple architectures tailored to each constraint. Last, but not least, the proposed token selection mechanism helps extract powerful semantics that are easy to understand and explain, paving the way for interpretable-by-design models for the next generation of AI-native communication systems.",
      "authors": [
        "Alessio Devoto",
        "Simone Petruzzi",
        "Jary Pomponi",
        "Paolo Di Lorenzo",
        "Simone Scardapane"
      ],
      "published": "2024-04-25T13:49:50Z",
      "updated": "2024-04-25T13:49:50Z",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.02330v1",
      "landing_url": "https://arxiv.org/abs/2405.02330v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.02330"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets adaptive semantic tokens in AI-native communications with transformers; it does not address discrete audio token design or VQ/RVQ-based discretization, so it fails the discrete audio token criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets adaptive semantic tokens in AI-native communications with transformers; it does not address discrete audio token design or VQ/RVQ-based discretization, so it fails the discrete audio token criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on AI-native goal-oriented communications using transformer models and dynamic token selection based on semantic relevance under bandwidth and latency constraints. However, it does not mention or involve discrete audio tokens, audio signal quantization, or token generation methods applied to speech or audio as defined by the inclusion criteria. Instead, it deals with general semantic token selection from input signals (e.g., image patches) and dynamic inference, which falls outside the specific domain of discrete audio tokens involving vector quantization or related techniques. Therefore, it does not meet the inclusion criteria nor contributes relevant methodology to the field of discrete audio tokens as required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on AI-native goal-oriented communications using transformer models and dynamic token selection based on semantic relevance under bandwidth and latency constraints. However, it does not mention or involve discrete audio tokens, audio signal quantization, or token generation methods applied to speech or audio as defined by the inclusion criteria. Instead, it deals with general semantic token selection from input signals (e.g., image patches) and dynamic inference, which falls outside the specific domain of discrete audio tokens involving vector quantization or related techniques. Therefore, it does not meet the inclusion criteria nor contributes relevant methodology to the field of discrete audio tokens as required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Causal K-Means Clustering",
    "abstract": "Causal effects are often characterized with population summaries. These might provide an incomplete picture when there are heterogeneous treatment effects across subgroups. Since the subgroup structure is typically unknown, it is more challenging to identify and evaluate subgroup effects than population effects. We propose a new solution to this problem: Causal k-Means Clustering, which harnesses the widely-used k-means clustering algorithm to uncover the unknown subgroup structure. Our problem differs significantly from the conventional clustering setup since the variables to be clustered are unknown counterfactual functions. We present a plug-in estimator which is simple and readily implementable using off-the-shelf algorithms, and study its rate of convergence. We also develop a new bias-corrected estimator based on nonparametric efficiency theory and double machine learning, and show that this estimator achieves fast root-n rates and asymptotic normality in large nonparametric models. Our proposed methods are especially useful for modern outcome-wide studies with multiple treatment levels. Further, our framework is extensible to clustering with generic pseudo-outcomes, such as partially observed outcomes or otherwise unknown functions. Finally, we explore finite sample properties via simulation, and illustrate the proposed methods in a study of treatment programs for adolescent substance abuse.",
    "metadata": {
      "arxiv_id": "2405.03083",
      "title": "Causal K-Means Clustering",
      "summary": "Causal effects are often characterized with population summaries. These might provide an incomplete picture when there are heterogeneous treatment effects across subgroups. Since the subgroup structure is typically unknown, it is more challenging to identify and evaluate subgroup effects than population effects. We propose a new solution to this problem: Causal k-Means Clustering, which harnesses the widely-used k-means clustering algorithm to uncover the unknown subgroup structure. Our problem differs significantly from the conventional clustering setup since the variables to be clustered are unknown counterfactual functions. We present a plug-in estimator which is simple and readily implementable using off-the-shelf algorithms, and study its rate of convergence. We also develop a new bias-corrected estimator based on nonparametric efficiency theory and double machine learning, and show that this estimator achieves fast root-n rates and asymptotic normality in large nonparametric models. Our proposed methods are especially useful for modern outcome-wide studies with multiple treatment levels. Further, our framework is extensible to clustering with generic pseudo-outcomes, such as partially observed outcomes or otherwise unknown functions. Finally, we explore finite sample properties via simulation, and illustrate the proposed methods in a study of treatment programs for adolescent substance abuse.",
      "authors": [
        "Kwangho Kim",
        "Jisu Kim",
        "Edward H. Kennedy"
      ],
      "published": "2024-05-05T23:59:51Z",
      "updated": "2025-02-10T05:14:12Z",
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.03083v3",
      "landing_url": "https://arxiv.org/abs/2405.03083v3",
      "doi": "https://doi.org/10.48550/arXiv.2405.03083"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses causal effects and subgroup estimation via Causal K-Means Clustering and does not pertain to Discrete Audio Tokens or token design for audio-language modeling; thus it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses causal effects and subgroup estimation via Causal K-Means Clustering and does not pertain to Discrete Audio Tokens or token design for audio-language modeling; thus it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on causal k-means clustering methods for identifying heterogeneous treatment effects and subgroup structures in statistical and causal inference contexts; it does not discuss any aspect of discrete audio tokens, vector quantization, audio signal discretization, or related topics specified in the inclusion criteria related to audio token generation or audio language modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on causal k-means clustering methods for identifying heterogeneous treatment effects and subgroup structures in statistical and causal inference contexts; it does not discuss any aspect of discrete audio tokens, vector quantization, audio signal discretization, or related topics specified in the inclusion criteria related to audio token generation or audio language modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Vector Quantization for Recommender Systems: A Review and Outlook",
    "abstract": "Vector quantization, renowned for its unparalleled feature compression capabilities, has been a prominent topic in signal processing and machine learning research for several decades and remains widely utilized today. With the emergence of large models and generative AI, vector quantization has gained popularity in recommender systems, establishing itself as a preferred solution. This paper starts with a comprehensive review of vector quantization techniques. It then explores systematic taxonomies of vector quantization methods for recommender systems (VQ4Rec), examining their applications from multiple perspectives. Further, it provides a thorough introduction to research efforts in diverse recommendation scenarios, including efficiency-oriented approaches and quality-oriented approaches. Finally, the survey analyzes the remaining challenges and anticipates future trends in VQ4Rec, including the challenges associated with the training of vector quantization, the opportunities presented by large language models, and emerging trends in multimodal recommender systems. We hope this survey can pave the way for future researchers in the recommendation community and accelerate their exploration in this promising field.",
    "metadata": {
      "arxiv_id": "2405.03110",
      "title": "Vector Quantization for Recommender Systems: A Review and Outlook",
      "summary": "Vector quantization, renowned for its unparalleled feature compression capabilities, has been a prominent topic in signal processing and machine learning research for several decades and remains widely utilized today. With the emergence of large models and generative AI, vector quantization has gained popularity in recommender systems, establishing itself as a preferred solution. This paper starts with a comprehensive review of vector quantization techniques. It then explores systematic taxonomies of vector quantization methods for recommender systems (VQ4Rec), examining their applications from multiple perspectives. Further, it provides a thorough introduction to research efforts in diverse recommendation scenarios, including efficiency-oriented approaches and quality-oriented approaches. Finally, the survey analyzes the remaining challenges and anticipates future trends in VQ4Rec, including the challenges associated with the training of vector quantization, the opportunities presented by large language models, and emerging trends in multimodal recommender systems. We hope this survey can pave the way for future researchers in the recommendation community and accelerate their exploration in this promising field.",
      "authors": [
        "Qijiong Liu",
        "Xiaoyu Dong",
        "Jiaren Xiao",
        "Nuo Chen",
        "Hengchang Hu",
        "Jieming Zhu",
        "Chenxu Zhu",
        "Tetsuya Sakai",
        "Xiao-Ming Wu"
      ],
      "published": "2024-05-06T02:06:26Z",
      "updated": "2024-05-06T02:06:26Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.03110v1",
      "landing_url": "https://arxiv.org/abs/2405.03110v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.03110"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper reviews vector quantization methods for recommender systems and does not address discrete audio tokens or audio-specific tokenization/quantization suitable for audio language modeling, thus it fails to meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper reviews vector quantization methods for recommender systems and does not address discrete audio tokens or audio-specific tokenization/quantization suitable for audio language modeling, thus it fails to meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on vector quantization in recommender systems, without addressing discrete audio tokens or their generation from continuous audio signals as required by the inclusion criteria. It lacks discussion of methods producing discrete audio tokens for audio language modeling or related sequence modeling tasks, and no indication of applicability to speech, music, or general audio processing is present.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on vector quantization in recommender systems, without addressing discrete audio tokens or their generation from continuous audio signals as required by the inclusion criteria. It lacks discussion of methods producing discrete audio tokens for audio language modeling or related sequence modeling tasks, and no indication of applicability to speech, music, or general audio processing is present.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adapting WavLM for Speech Emotion Recognition",
    "abstract": "Recently, the usage of speech self-supervised models (SSL) for downstream tasks has been drawing a lot of attention. While large pre-trained models commonly outperform smaller models trained from scratch, questions regarding the optimal fine-tuning strategies remain prevalent. In this paper, we explore the fine-tuning strategies of the WavLM Large model for the speech emotion recognition task on the MSP Podcast Corpus. More specifically, we perform a series of experiments focusing on using gender and semantic information from utterances. We then sum up our findings and describe the final model we used for submission to Speech Emotion Recognition Challenge 2024.",
    "metadata": {
      "arxiv_id": "2405.04485",
      "title": "Adapting WavLM for Speech Emotion Recognition",
      "summary": "Recently, the usage of speech self-supervised models (SSL) for downstream tasks has been drawing a lot of attention. While large pre-trained models commonly outperform smaller models trained from scratch, questions regarding the optimal fine-tuning strategies remain prevalent. In this paper, we explore the fine-tuning strategies of the WavLM Large model for the speech emotion recognition task on the MSP Podcast Corpus. More specifically, we perform a series of experiments focusing on using gender and semantic information from utterances. We then sum up our findings and describe the final model we used for submission to Speech Emotion Recognition Challenge 2024.",
      "authors": [
        "Daria Diatlova",
        "Anton Udalov",
        "Vitalii Shutov",
        "Egor Spirin"
      ],
      "published": "2024-05-07T16:53:42Z",
      "updated": "2024-05-07T16:53:42Z",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.04485v1",
      "landing_url": "https://arxiv.org/abs/2405.04485v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.04485"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on fine-tuning a pretrained WavLM SSL model for Speech Emotion Recognition and examining gender/semantic cues, with no discussion of discrete audio tokens, tokenization methods, or quantization that would meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on fine-tuning a pretrained WavLM SSL model for Speech Emotion Recognition and examining gender/semantic cues, with no discussion of discrete audio tokens, tokenization methods, or quantization that would meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on fine-tuning the WavLM Large model for speech emotion recognition, without any mention or evidence of discrete audio token generation, vector quantization, or related discrete token design or usage in their core methodology. The study does not describe token generation processes or quantization methods required by the inclusion criteria, nor does it emphasize discrete token representations as central contributions.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on fine-tuning the WavLM Large model for speech emotion recognition, without any mention or evidence of discrete audio token generation, vector quantization, or related discrete token design or usage in their core methodology. The study does not describe token generation processes or quantization methods required by the inclusion criteria, nor does it emphasize discrete token representations as central contributions.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "HILCodec: High-Fidelity and Lightweight Neural Audio Codec",
    "abstract": "The recent advancement of end-to-end neural audio codecs enables compressing audio at very low bitrates while reconstructing the output audio with high fidelity. Nonetheless, such improvements often come at the cost of increased model complexity. In this paper, we identify and address the problems of existing neural audio codecs. We show that the performance of the SEANet-based codec does not increase consistently as the network depth increases. We analyze the root cause of such a phenomenon and suggest a variance-constrained design. Also, we reveal various distortions in previous waveform domain discriminators and propose a novel distortion-free discriminator. The resulting model, HILCodec, is a real-time streaming audio codec that demonstrates state-of-the-art quality across various bitrates and audio types.",
    "metadata": {
      "arxiv_id": "2405.04752",
      "title": "HILCodec: High-Fidelity and Lightweight Neural Audio Codec",
      "summary": "The recent advancement of end-to-end neural audio codecs enables compressing audio at very low bitrates while reconstructing the output audio with high fidelity. Nonetheless, such improvements often come at the cost of increased model complexity. In this paper, we identify and address the problems of existing neural audio codecs. We show that the performance of the SEANet-based codec does not increase consistently as the network depth increases. We analyze the root cause of such a phenomenon and suggest a variance-constrained design. Also, we reveal various distortions in previous waveform domain discriminators and propose a novel distortion-free discriminator. The resulting model, HILCodec, is a real-time streaming audio codec that demonstrates state-of-the-art quality across various bitrates and audio types.",
      "authors": [
        "Sunghwan Ahn",
        "Beom Jun Woo",
        "Min Hyun Han",
        "Chanyeong Moon",
        "Nam Soo Kim"
      ],
      "published": "2024-05-08T01:40:13Z",
      "updated": "2024-09-24T07:40:54Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.04752v2",
      "landing_url": "https://arxiv.org/abs/2405.04752v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.04752"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a high-fidelity neural audio codec (HILCodec) with design refinements and discriminators, but it does not present discrete audio token representations, token generation/quantization methods, or token-centric evaluation aligned with the inclusion criteria for discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a high-fidelity neural audio codec (HILCodec) with design refinements and discriminators, but it does not present discrete audio token representations, token generation/quantization methods, or token-centric evaluation aligned with the inclusion criteria for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a neural audio codec focusing on high-fidelity and low complexity, but does not mention discrete audio token generation or corresponding quantization mechanisms (like VQ, RVQ) to produce tokens used for sequence-based modeling or downstream tasks, which is central to the inclusion criteria; the study is clearly about continuous waveform codec improvements rather than discrete token designs.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents a neural audio codec focusing on high-fidelity and low complexity, but does not mention discrete audio token generation or corresponding quantization mechanisms (like VQ, RVQ) to produce tokens used for sequence-based modeling or downstream tasks, which is central to the inclusion criteria; the study is clearly about continuous waveform codec improvements rather than discrete token designs.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "TEDNet: Twin Encoder Decoder Neural Network for 2D Camera and LiDAR Road Detection",
    "abstract": "Robust road surface estimation is required for autonomous ground vehicles to navigate safely. Despite it becoming one of the main targets for autonomous mobility researchers in recent years, it is still an open problem in which cameras and LiDAR sensors have demonstrated to be adequate to predict the position, size and shape of the road a vehicle is driving on in different environments. In this work, a novel Convolutional Neural Network model is proposed for the accurate estimation of the roadway surface. Furthermore, an ablation study has been conducted to investigate how different encoding strategies affect model performance, testing 6 slightly different neural network architectures. Our model is based on the use of a Twin Encoder-Decoder Neural Network (TEDNet) for independent camera and LiDAR feature extraction, and has been trained and evaluated on the Kitti-Road dataset. Bird's Eye View projections of the camera and LiDAR data are used in this model to perform semantic segmentation on whether each pixel belongs to the road surface. The proposed method performs among other state-of-the-art methods and operates at the same frame-rate as the LiDAR and cameras, so it is adequate for its use in real-time applications.",
    "metadata": {
      "arxiv_id": "2405.08429",
      "title": "TEDNet: Twin Encoder Decoder Neural Network for 2D Camera and LiDAR Road Detection",
      "summary": "Robust road surface estimation is required for autonomous ground vehicles to navigate safely. Despite it becoming one of the main targets for autonomous mobility researchers in recent years, it is still an open problem in which cameras and LiDAR sensors have demonstrated to be adequate to predict the position, size and shape of the road a vehicle is driving on in different environments. In this work, a novel Convolutional Neural Network model is proposed for the accurate estimation of the roadway surface. Furthermore, an ablation study has been conducted to investigate how different encoding strategies affect model performance, testing 6 slightly different neural network architectures. Our model is based on the use of a Twin Encoder-Decoder Neural Network (TEDNet) for independent camera and LiDAR feature extraction, and has been trained and evaluated on the Kitti-Road dataset. Bird's Eye View projections of the camera and LiDAR data are used in this model to perform semantic segmentation on whether each pixel belongs to the road surface. The proposed method performs among other state-of-the-art methods and operates at the same frame-rate as the LiDAR and cameras, so it is adequate for its use in real-time applications.",
      "authors": [
        "Martín Bayón-Gutiérrez",
        "María Teresa García-Ordás",
        "Héctor Alaiz Moretón",
        "Jose Aveleira-Mata",
        "Sergio Rubio Martín",
        "José Alberto Benítez-Andrades"
      ],
      "published": "2024-05-14T08:45:34Z",
      "updated": "2024-05-14T08:45:34Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.08429v1",
      "landing_url": "https://arxiv.org/abs/2405.08429v1",
      "doi": "https://doi.org/10.1093/jigpal/jzae048"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns a twin encoder-decoder network for 2D camera and LiDAR road detection, not discrete audio tokenization or token representations, thus it does not meet the inclusion criteria and does not meet the research topic focus; therefore it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns a twin encoder-decoder network for 2D camera and LiDAR road detection, not discrete audio tokenization or token representations, thus it does not meet the inclusion criteria and does not meet the research topic focus; therefore it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a twin encoder-decoder neural network for road detection using camera and LiDAR data and does not involve any work related to discrete audio tokens or audio signal discretization frameworks. It neither addresses audio processing nor token design for audio language modeling, making it outside the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a twin encoder-decoder neural network for road detection using camera and LiDAR data and does not involve any work related to discrete audio tokens or audio signal discretization frameworks. It neither addresses audio processing nor token design for audio language modeling, making it outside the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model",
    "abstract": "Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.",
    "metadata": {
      "arxiv_id": "2405.09768",
      "title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model",
      "summary": "Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.",
      "authors": [
        "Siyang Wang",
        "Éva Székely"
      ],
      "published": "2024-05-16T02:18:41Z",
      "updated": "2024-05-16T02:18:41Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.09768v1",
      "landing_url": "https://arxiv.org/abs/2405.09768v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.09768"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title and abstract clearly discuss TTS using a discrete token-based speech language model and discrete audio tokens, focusing on their evaluation and properties, which aligns with the discrete-audio-token topic; although the abstract does not spell out the token-generation/quantization specifics, the work centers on the discrete-token paradigm and provides evaluation, fitting the inclusion criteria.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The title and abstract clearly discuss TTS using a discrete token-based speech language model and discrete audio tokens, focusing on their evaluation and properties, which aligns with the discrete-audio-token topic; although the abstract does not spell out the token-generation/quantization specifics, the work centers on the discrete-token paradigm and provides evaluation, fitting the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper evaluates text-to-speech synthesis using a speech language model based on discrete tokens, indicating the use of discrete audio token representations within a sequence modeling framework. However, the abstract does not explicitly detail the token generation process, quantization approach, or the architecture specifics required by the inclusion criteria. Without explicit description of token formation and method details, it is difficult to confirm full compliance with the criteria. Nonetheless, the topic clearly involves discrete token-based speech modeling as a core method, aligning with the inclusion theme.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The paper evaluates text-to-speech synthesis using a speech language model based on discrete tokens, indicating the use of discrete audio token representations within a sequence modeling framework. However, the abstract does not explicitly detail the token generation process, quantization approach, or the architecture specifics required by the inclusion criteria. Without explicit description of token formation and method details, it is difficult to confirm full compliance with the criteria. Nonetheless, the topic clearly involves discrete token-based speech modeling as a core method, aligning with the inclusion theme.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LG-VQ: Language-Guided Codebook Learning",
    "abstract": "Vector quantization (VQ) is a key technique in high-resolution and high-fidelity image synthesis, which aims to learn a codebook to encode an image with a sequence of discrete codes and then generate an image in an auto-regression manner. Although existing methods have shown superior performance, most methods prefer to learn a single-modal codebook (\\emph{e.g.}, image), resulting in suboptimal performance when the codebook is applied to multi-modal downstream tasks (\\emph{e.g.}, text-to-image, image captioning) due to the existence of modal gaps. In this paper, we propose a novel language-guided codebook learning framework, called LG-VQ, which aims to learn a codebook that can be aligned with the text to improve the performance of multi-modal downstream tasks. Specifically, we first introduce pre-trained text semantics as prior knowledge, then design two novel alignment modules (\\emph{i.e.}, Semantic Alignment Module, and Relationship Alignment Module) to transfer such prior knowledge into codes for achieving codebook text alignment. In particular, our LG-VQ method is model-agnostic, which can be easily integrated into existing VQ models. Experimental results show that our method achieves superior performance on reconstruction and various multi-modal downstream tasks.",
    "metadata": {
      "arxiv_id": "2405.14206",
      "title": "LG-VQ: Language-Guided Codebook Learning",
      "summary": "Vector quantization (VQ) is a key technique in high-resolution and high-fidelity image synthesis, which aims to learn a codebook to encode an image with a sequence of discrete codes and then generate an image in an auto-regression manner. Although existing methods have shown superior performance, most methods prefer to learn a single-modal codebook (\\emph{e.g.}, image), resulting in suboptimal performance when the codebook is applied to multi-modal downstream tasks (\\emph{e.g.}, text-to-image, image captioning) due to the existence of modal gaps. In this paper, we propose a novel language-guided codebook learning framework, called LG-VQ, which aims to learn a codebook that can be aligned with the text to improve the performance of multi-modal downstream tasks. Specifically, we first introduce pre-trained text semantics as prior knowledge, then design two novel alignment modules (\\emph{i.e.}, Semantic Alignment Module, and Relationship Alignment Module) to transfer such prior knowledge into codes for achieving codebook text alignment. In particular, our LG-VQ method is model-agnostic, which can be easily integrated into existing VQ models. Experimental results show that our method achieves superior performance on reconstruction and various multi-modal downstream tasks.",
      "authors": [
        "Guotao Liang",
        "Baoquan Zhang",
        "Yaowei Wang",
        "Xutao Li",
        "Yunming Ye",
        "Huaibin Wang",
        "Chuyao Luo",
        "Kola Ye",
        "linfeng Luo"
      ],
      "published": "2024-05-23T06:04:40Z",
      "updated": "2024-10-09T04:30:30Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.14206v2",
      "landing_url": "https://arxiv.org/abs/2405.14206v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.14206"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item LG-VQ focuses on image vector quantization and language alignment for multi-modal image tasks, not on discrete audio tokens or audio-language modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item LG-VQ focuses on image vector quantization and language alignment for multi-modal image tasks, not on discrete audio tokens or audio-language modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on vector quantization techniques for image and multi-modal tasks, specifically aligning visual codebooks with language, but it does not address discrete audio tokens, audio signal processing, or audio-related sequence modeling, which are central to the inclusion criteria. Therefore, it does not meet the requirement of targeting discrete audio token methods or applications and thus should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on vector quantization techniques for image and multi-modal tasks, specifically aligning visual codebooks with language, but it does not address discrete audio tokens, audio signal processing, or audio-related sequence modeling, which are central to the inclusion criteria. Therefore, it does not meet the requirement of targeting discrete audio token methods or applications and thus should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "C3LLM: Conditional Multimodal Content Generation Using Large Language Models",
    "abstract": "We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a novel framework combining three tasks of video-to-audio, audio-to-text, and text-to-audio together. C3LLM adapts the Large Language Model (LLM) structure as a bridge for aligning different modalities, synthesizing the given conditional information, and making multimodal generation in a discrete manner. Our contributions are as follows. First, we adapt a hierarchical structure for audio generation tasks with pre-trained audio codebooks. Specifically, we train the LLM to generate audio semantic tokens from the given conditions, and further use a non-autoregressive transformer to generate different levels of acoustic tokens in layers to better enhance the fidelity of the generated audio. Second, based on the intuition that LLMs were originally designed for discrete tasks with the next-word prediction method, we use the discrete representation for audio generation and compress their semantic meanings into acoustic tokens, similar to adding \"acoustic vocabulary\" to LLM. Third, our method combines the previous tasks of audio understanding, video-to-audio generation, and text-to-audio generation together into one unified model, providing more versatility in an end-to-end fashion. Our C3LLM achieves improved results through various automated evaluation metrics, providing better semantic alignment compared to previous methods.",
    "metadata": {
      "arxiv_id": "2405.16136",
      "title": "C3LLM: Conditional Multimodal Content Generation Using Large Language Models",
      "summary": "We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a novel framework combining three tasks of video-to-audio, audio-to-text, and text-to-audio together. C3LLM adapts the Large Language Model (LLM) structure as a bridge for aligning different modalities, synthesizing the given conditional information, and making multimodal generation in a discrete manner. Our contributions are as follows. First, we adapt a hierarchical structure for audio generation tasks with pre-trained audio codebooks. Specifically, we train the LLM to generate audio semantic tokens from the given conditions, and further use a non-autoregressive transformer to generate different levels of acoustic tokens in layers to better enhance the fidelity of the generated audio. Second, based on the intuition that LLMs were originally designed for discrete tasks with the next-word prediction method, we use the discrete representation for audio generation and compress their semantic meanings into acoustic tokens, similar to adding \"acoustic vocabulary\" to LLM. Third, our method combines the previous tasks of audio understanding, video-to-audio generation, and text-to-audio generation together into one unified model, providing more versatility in an end-to-end fashion. Our C3LLM achieves improved results through various automated evaluation metrics, providing better semantic alignment compared to previous methods.",
      "authors": [
        "Zixuan Wang",
        "Qinkai Duan",
        "Yu-Wing Tai",
        "Chi-Keung Tang"
      ],
      "published": "2024-05-25T09:10:12Z",
      "updated": "2024-05-25T09:10:12Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.16136v1",
      "landing_url": "https://arxiv.org/abs/2405.16136v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.16136"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the abstract clearly targets discrete audio token representations and cross-modal generation (C3LLM), the source page does not provide a clear publication/update date, triggering exclusion under the stated criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although the abstract clearly targets discrete audio token representations and cross-modal generation (C3LLM), the source page does not provide a clear publication/update date, triggering exclusion under the stated criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces C3LLM, a novel framework using large language models to generate audio semantic tokens for multimodal content generation, specifically mentioning the transformation of audio into discrete tokens used in a sequence model framework. It details the token generation process via pre-trained audio codebooks and layered acoustic token generation to enhance fidelity, aligning with the inclusion criteria that require clear explanation of discrete audio token generation and usage for sequence modeling. The work combines video-to-audio, audio-to-text, and text-to-audio tasks, with a focus on semantic alignment, which fits the modern discrete audio token research context rather than traditional codecs. Hence, it meets the inclusion criteria and no exclusion criteria are triggered as method details and evaluations are provided.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study introduces C3LLM, a novel framework using large language models to generate audio semantic tokens for multimodal content generation, specifically mentioning the transformation of audio into discrete tokens used in a sequence model framework. It details the token generation process via pre-trained audio codebooks and layered acoustic token generation to enhance fidelity, aligning with the inclusion criteria that require clear explanation of discrete audio token generation and usage for sequence modeling. The work combines video-to-audio, audio-to-text, and text-to-audio tasks, with a focus on semantic alignment, which fits the modern discrete audio token research context rather than traditional codecs. Hence, it meets the inclusion criteria and no exclusion criteria are triggered as method details and evaluations are provided.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "JuniorMini correctly notes the abstract presents discrete audio-token design (audio semantic tokens, pre-trained audio codebooks, hierarchical/layered acoustic token generation) that aligns with the inclusion criteria; however JuniorNano rightly flagged that the source metadata lacks a clear publication/update date—an exclusion criterion—so despite meeting technical inclusion points, the item must be excluded due to the missing date for traceability.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "JuniorMini correctly notes the abstract presents discrete audio-token design (audio semantic tokens, pre-trained audio codebooks, hierarchical/layered acoustic token generation) that aligns with the inclusion criteria; however JuniorNano rightly flagged that the source metadata lacks a clear publication/update date—an exclusion criterion—so despite meeting technical inclusion points, the item must be excluded due to the missing date for traceability.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "Reflected Flow Matching",
    "abstract": "Continuous normalizing flows (CNFs) learn an ordinary differential equation to transform prior samples into data. Flow matching (FM) has recently emerged as a simulation-free approach for training CNFs by regressing a velocity model towards the conditional velocity field. However, on constrained domains, the learned velocity model may lead to undesirable flows that result in highly unnatural samples, e.g., oversaturated images, due to both flow matching error and simulation error. To address this, we add a boundary constraint term to CNFs, which leads to reflected CNFs that keep trajectories within the constrained domains. We propose reflected flow matching (RFM) to train the velocity model in reflected CNFs by matching the conditional velocity fields in a simulation-free manner, similar to the vanilla FM. Moreover, the analytical form of conditional velocity fields in RFM avoids potentially biased approximations, making it superior to existing score-based generative models on constrained domains. We demonstrate that RFM achieves comparable or better results on standard image benchmarks and produces high-quality class-conditioned samples under high guidance weight.",
    "metadata": {
      "arxiv_id": "2405.16577",
      "title": "Reflected Flow Matching",
      "summary": "Continuous normalizing flows (CNFs) learn an ordinary differential equation to transform prior samples into data. Flow matching (FM) has recently emerged as a simulation-free approach for training CNFs by regressing a velocity model towards the conditional velocity field. However, on constrained domains, the learned velocity model may lead to undesirable flows that result in highly unnatural samples, e.g., oversaturated images, due to both flow matching error and simulation error. To address this, we add a boundary constraint term to CNFs, which leads to reflected CNFs that keep trajectories within the constrained domains. We propose reflected flow matching (RFM) to train the velocity model in reflected CNFs by matching the conditional velocity fields in a simulation-free manner, similar to the vanilla FM. Moreover, the analytical form of conditional velocity fields in RFM avoids potentially biased approximations, making it superior to existing score-based generative models on constrained domains. We demonstrate that RFM achieves comparable or better results on standard image benchmarks and produces high-quality class-conditioned samples under high guidance weight.",
      "authors": [
        "Tianyu Xie",
        "Yu Zhu",
        "Longlin Yu",
        "Tong Yang",
        "Ziheng Cheng",
        "Shiyue Zhang",
        "Xiangyu Zhang",
        "Cheng Zhang"
      ],
      "published": "2024-05-26T14:09:43Z",
      "updated": "2024-05-26T14:09:43Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.16577v1",
      "landing_url": "https://arxiv.org/abs/2405.16577v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.16577"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on reflected flow matching for continuous normalizing flows and boundary constraints, not on discrete audio tokens or token design, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on reflected flow matching for continuous normalizing flows and boundary constraints, not on discrete audio tokens or token design, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on continuous normalizing flows and reflected flow matching for image generation tasks, without mentioning any audio-related discrete token representation, vector quantization for audio, or relevant application to audio language modeling or sequence modeling; therefore, it clearly does not meet the topic or technical requirements related to discrete audio tokens for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on continuous normalizing flows and reflected flow matching for image generation tasks, without mentioning any audio-related discrete token representation, vector quantization for audio, or relevant application to audio language modeling or sequence modeling; therefore, it clearly does not meet the topic or technical requirements related to discrete audio tokens for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Understanding Encoder-Decoder Structures in Machine Learning Using Information Measures",
    "abstract": "We present new results to model and understand the role of encoder-decoder design in machine learning (ML) from an information-theoretic angle. We use two main information concepts, information sufficiency (IS) and mutual information loss (MIL), to represent predictive structures in machine learning. Our first main result provides a functional expression that characterizes the class of probabilistic models consistent with an IS encoder-decoder latent predictive structure. This result formally justifies the encoder-decoder forward stages many modern ML architectures adopt to learn latent (compressed) representations for classification. To illustrate IS as a realistic and relevant model assumption, we revisit some known ML concepts and present some interesting new examples: invariant, robust, sparse, and digital models. Furthermore, our IS characterization allows us to tackle the fundamental question of how much performance (predictive expressiveness) could be lost, using the cross entropy risk, when a given encoder-decoder architecture is adopted in a learning setting. Here, our second main result shows that a mutual information loss quantifies the lack of expressiveness attributed to the choice of a (biased) encoder-decoder ML design. Finally, we address the problem of universal cross-entropy learning with an encoder-decoder design where necessary and sufficiency conditions are established to meet this requirement. In all these results, Shannon's information measures offer new interpretations and explanations for representation learning.",
    "metadata": {
      "arxiv_id": "2405.20452",
      "title": "Understanding Encoder-Decoder Structures in Machine Learning Using Information Measures",
      "summary": "We present new results to model and understand the role of encoder-decoder design in machine learning (ML) from an information-theoretic angle. We use two main information concepts, information sufficiency (IS) and mutual information loss (MIL), to represent predictive structures in machine learning. Our first main result provides a functional expression that characterizes the class of probabilistic models consistent with an IS encoder-decoder latent predictive structure. This result formally justifies the encoder-decoder forward stages many modern ML architectures adopt to learn latent (compressed) representations for classification. To illustrate IS as a realistic and relevant model assumption, we revisit some known ML concepts and present some interesting new examples: invariant, robust, sparse, and digital models. Furthermore, our IS characterization allows us to tackle the fundamental question of how much performance (predictive expressiveness) could be lost, using the cross entropy risk, when a given encoder-decoder architecture is adopted in a learning setting. Here, our second main result shows that a mutual information loss quantifies the lack of expressiveness attributed to the choice of a (biased) encoder-decoder ML design. Finally, we address the problem of universal cross-entropy learning with an encoder-decoder design where necessary and sufficiency conditions are established to meet this requirement. In all these results, Shannon's information measures offer new interpretations and explanations for representation learning.",
      "authors": [
        "Jorge F. Silva",
        "Victor Faraggi",
        "Camilo Ramirez",
        "Alvaro Egana",
        "Eduardo Pavez"
      ],
      "published": "2024-05-30T19:58:01Z",
      "updated": "2024-05-30T19:58:01Z",
      "categories": [
        "cs.LG",
        "cs.IT",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.20452v1",
      "landing_url": "https://arxiv.org/abs/2405.20452v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.20452"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item investigates information-theoretic aspects of encoder-decoder structures in ML, not discrete audio token design or tokenization methods for audio-language modeling, so it does not meet the inclusion criteria anchored on discrete audio tokens; no exclusion criteria are triggered.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item investigates information-theoretic aspects of encoder-decoder structures in ML, not discrete audio token design or tokenization methods for audio-language modeling, so it does not meet the inclusion criteria anchored on discrete audio tokens; no exclusion criteria are triggered.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on understanding encoder-decoder architectures in machine learning using information-theoretic measures, without any discussion or methodology involving discrete audio tokens, vector quantization, or audio-specific tokenization processes necessary for audio language modeling or related tasks. Therefore, it does not meet the inclusion criteria focused on discrete audio token representations or their generation and use in sequence models for audio.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on understanding encoder-decoder architectures in machine learning using information-theoretic measures, without any discussion or methodology involving discrete audio tokens, vector quantization, or audio-specific tokenization processes necessary for audio language modeling or related tasks. Therefore, it does not meet the inclusion criteria focused on discrete audio token representations or their generation and use in sequence models for audio.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer",
    "abstract": "While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce \\textbf{G}enerative \\textbf{P}re-trained \\textbf{S}peech \\textbf{T}ransformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. The code is available at \\url{https://github.com/youngsheen/GPST}.",
    "metadata": {
      "arxiv_id": "2406.00976",
      "title": "Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer",
      "summary": "While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce \\textbf{G}enerative \\textbf{P}re-trained \\textbf{S}peech \\textbf{T}ransformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. The code is available at \\url{https://github.com/youngsheen/GPST}.",
      "authors": [
        "Yongxin Zhu",
        "Dan Su",
        "Liqiang He",
        "Linli Xu",
        "Dong Yu"
      ],
      "published": "2024-06-03T04:16:30Z",
      "updated": "2024-11-01T13:54:48Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.00976v2",
      "landing_url": "https://arxiv.org/abs/2406.00976v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.00976"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Aligns with discrete audio token research by proposing two discrete representations and a hierarchical transformer for one-stage generation, but the source lacks a clear publication/update date, triggering exclusion under the date criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Aligns with discrete audio token research by proposing two discrete representations and a hierarchical transformer for one-stage generation, but the source lacks a clear publication/update date, triggering exclusion under the date criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces GPST, a hierarchical transformer-based speech language model that quantizes audio waveforms into two distinct discrete speech representations, which aligns with the inclusion criterion of proposing discrete audio token designs with clear quantization methods used for sequence modeling. The abstract mentions vector quantization-like processes and hierarchical quantization for token generation, supports end-to-end unsupervised training, and shows the method improves speech generation quality, indicating a significant technical contribution and adherence to the theme. The technical detail and code availability support reproducibility, and there is no indication of exclusion criteria such as missing method details or purely demo-focused content.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper introduces GPST, a hierarchical transformer-based speech language model that quantizes audio waveforms into two distinct discrete speech representations, which aligns with the inclusion criterion of proposing discrete audio token designs with clear quantization methods used for sequence modeling. The abstract mentions vector quantization-like processes and hierarchical quantization for token generation, supports end-to-end unsupervised training, and shows the method improves speech generation quality, indicating a significant technical contribution and adherence to the theme. The technical detail and code availability support reproducibility, and there is no indication of exclusion criteria such as missing method details or purely demo-focused content.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "JuniorMini correctly notes the paper proposes discrete audio representations (two quantized token types) integrated into a hierarchical transformer for sequence modeling and provides code, meeting the technical inclusion criteria; JuniorNano raised a valid exclusion point that the source lacks a clear publication/update date. The exclusion criterion requiring a clear YYYY-MM-DD publication/update date is mandatory, so despite the abstract matching the discrete-audio-token theme, the missing explicit date forces exclusion.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "JuniorMini correctly notes the paper proposes discrete audio representations (two quantized token types) integrated into a hierarchical transformer for sequence modeling and provides code, meeting the technical inclusion criteria; JuniorNano raised a valid exclusion point that the source lacks a clear publication/update date. The exclusion criterion requiring a clear YYYY-MM-DD publication/update date is mandatory, so despite the abstract matching the discrete-audio-token theme, the missing explicit date forces exclusion.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "A cost minimization approach to fix the vocabulary size in a tokenizer for an End-to-End ASR system",
    "abstract": "Unlike hybrid speech recognition systems where the use of tokens was restricted to phones, biphones or triphones the choice of tokens in the end-to-end ASR systems is derived from the text corpus of the training data. The use of tokenization algorithms like Byte Pair Encoding (BPE) and WordPiece is popular in identifying the tokens that are used in the overall training process of the speech recognition system. Popular toolkits, like ESPNet use a pre-defined vocabulary size (number of tokens) for these tokenization algorithms, but there is no discussion on how vocabulary size was derived. In this paper, we build a cost function, assuming the tokenization process to be a black-box to enable choosing the number of tokens which might most benefit building an end-to-end ASR. We show through experiments on LibriSpeech 100 hour set that the performance of an end-to-end ASR system improves when the number of tokens are chosen carefully.",
    "metadata": {
      "arxiv_id": "2406.02563",
      "title": "A cost minimization approach to fix the vocabulary size in a tokenizer for an End-to-End ASR system",
      "summary": "Unlike hybrid speech recognition systems where the use of tokens was restricted to phones, biphones or triphones the choice of tokens in the end-to-end ASR systems is derived from the text corpus of the training data. The use of tokenization algorithms like Byte Pair Encoding (BPE) and WordPiece is popular in identifying the tokens that are used in the overall training process of the speech recognition system. Popular toolkits, like ESPNet use a pre-defined vocabulary size (number of tokens) for these tokenization algorithms, but there is no discussion on how vocabulary size was derived. In this paper, we build a cost function, assuming the tokenization process to be a black-box to enable choosing the number of tokens which might most benefit building an end-to-end ASR. We show through experiments on LibriSpeech 100 hour set that the performance of an end-to-end ASR system improves when the number of tokens are chosen carefully.",
      "authors": [
        "Sunil Kumar Kopparapu",
        "Ashish Panda"
      ],
      "published": "2024-04-29T12:16:21Z",
      "updated": "2024-04-29T12:16:21Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02563v1",
      "landing_url": "https://arxiv.org/abs/2406.02563v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02563"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper deals with choosing the vocabulary size for text-based tokenization (BPE/WordPiece) in end-to-end ASR, which targets discrete language tokens rather than discrete audio tokens produced by VQ/RVQ or semantic-disentangled audio tokens; it thus does not align with the inclusion criteria focused on discrete audio token representations.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper deals with choosing the vocabulary size for text-based tokenization (BPE/WordPiece) in end-to-end ASR, which targets discrete language tokens rather than discrete audio tokens produced by VQ/RVQ or semantic-disentangled audio tokens; it thus does not align with the inclusion criteria focused on discrete audio token representations.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on choosing vocabulary size for tokenization in an end-to-end ASR system using text-based tokenization methods (BPE, WordPiece) but does not discuss generating discrete audio tokens from continuous audio via vector quantization or similar methods, nor does it emphasize token generation aimed for audio language modeling or semantic disentanglement as required. The tokenization process here is text-corpus driven rather than learned discrete audio representation, so it does not meet the inclusion criteria of discrete audio token generation and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on choosing vocabulary size for tokenization in an end-to-end ASR system using text-based tokenization methods (BPE, WordPiece) but does not discuss generating discrete audio tokens from continuous audio via vector quantization or similar methods, nor does it emphasize token generation aimed for audio language modeling or semantic disentanglement as required. The tokenization process here is text-corpus driven rather than learned discrete audio representation, so it does not meet the inclusion criteria of discrete audio token generation and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Short-term Inland Vessel Trajectory Prediction with Encoder-Decoder Models",
    "abstract": "Accurate vessel trajectory prediction is necessary for save and efficient navigation. Deep learning-based prediction models, esp. encoder-decoders, are rarely applied to inland navigation specifically. Approaches from the maritime domain cannot directly be transferred to river navigation due to specific driving behavior influencing factors. Different encoder-decoder architectures, including a transformer encoder-decoder, are compared herein for predicting the next positions of inland vessels, given not only spatio-temporal information from AIS, but also river specific features. The results show that the reformulation of the regression task as classification problem and the inclusion of river specific features yield the lowest displacement errors. The standard LSTM encoder-decoder outperforms the transformer encoder-decoder for the data considered, but is computationally more expensive. In this study for the first time a transformer-based encoder-decoder model is applied to the problem of predicting the ship trajectory. Here, a feature vector using the river-specific context of navigation input parameters is established. Future studies can built on the proposed models, investigate the improvement of the computationally more efficient transformer, e.g. through further hyper-parameter optimization, and use additional river-specific information in the context representation to further increase prediction accuracy.",
    "metadata": {
      "arxiv_id": "2406.02770",
      "title": "Short-term Inland Vessel Trajectory Prediction with Encoder-Decoder Models",
      "summary": "Accurate vessel trajectory prediction is necessary for save and efficient navigation. Deep learning-based prediction models, esp. encoder-decoders, are rarely applied to inland navigation specifically. Approaches from the maritime domain cannot directly be transferred to river navigation due to specific driving behavior influencing factors. Different encoder-decoder architectures, including a transformer encoder-decoder, are compared herein for predicting the next positions of inland vessels, given not only spatio-temporal information from AIS, but also river specific features. The results show that the reformulation of the regression task as classification problem and the inclusion of river specific features yield the lowest displacement errors. The standard LSTM encoder-decoder outperforms the transformer encoder-decoder for the data considered, but is computationally more expensive. In this study for the first time a transformer-based encoder-decoder model is applied to the problem of predicting the ship trajectory. Here, a feature vector using the river-specific context of navigation input parameters is established. Future studies can built on the proposed models, investigate the improvement of the computationally more efficient transformer, e.g. through further hyper-parameter optimization, and use additional river-specific information in the context representation to further increase prediction accuracy.",
      "authors": [
        "Kathrin Donandt",
        "Karim Böttger",
        "Dirk Söffker"
      ],
      "published": "2024-06-04T20:37:30Z",
      "updated": "2024-06-04T20:37:30Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02770v1",
      "landing_url": "https://arxiv.org/abs/2406.02770v1",
      "doi": "https://doi.org/10.1109/ITSC55140.2022.9922148"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses inland vessel trajectory prediction using encoder–decoder models and river-specific features, not discrete audio tokens or tokenized audio representations, so it does not meet the inclusion criteria focused on Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses inland vessel trajectory prediction using encoder–decoder models and river-specific features, not discrete audio tokens or tokenized audio representations, so it does not meet the inclusion criteria focused on Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on vessel trajectory prediction using encoder-decoder deep learning models with river-specific features, which is unrelated to the research theme of discrete audio tokens involving vector quantization or semantic-driven discrete audio representations used for audio language modeling or audio generation tasks. Therefore, it does not meet the inclusion criteria centered on discrete audio token generation or usage nor does it relate to any aspect of audio or tokenization methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on vessel trajectory prediction using encoder-decoder deep learning models with river-specific features, which is unrelated to the research theme of discrete audio tokens involving vector quantization or semantic-driven discrete audio representations used for audio language modeling or audio generation tasks. Therefore, it does not meet the inclusion criteria centered on discrete audio token generation or usage nor does it relate to any aspect of audio or tokenization methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The PESQetarian: On the Relevance of Goodhart's Law for Speech Enhancement",
    "abstract": "To obtain improved speech enhancement models, researchers often focus on increasing performance according to specific instrumental metrics. However, when the same metric is used in a loss function to optimize models, it may be detrimental to aspects that the given metric does not see. The goal of this paper is to illustrate the risk of overfitting a speech enhancement model to the metric used for evaluation. For this, we introduce enhancement models that exploit the widely used PESQ measure. Our \"PESQetarian\" model achieves 3.82 PESQ on VB-DMD while scoring very poorly in a listening experiment. While the obtained PESQ value of 3.82 would imply \"state-of-the-art\" PESQ-performance on the VB-DMD benchmark, our examples show that when optimizing w.r.t. a metric, an isolated evaluation on the same metric may be misleading. Instead, other metrics should be included in the evaluation and the resulting performance predictions should be confirmed by listening.",
    "metadata": {
      "arxiv_id": "2406.03460",
      "title": "The PESQetarian: On the Relevance of Goodhart's Law for Speech Enhancement",
      "summary": "To obtain improved speech enhancement models, researchers often focus on increasing performance according to specific instrumental metrics. However, when the same metric is used in a loss function to optimize models, it may be detrimental to aspects that the given metric does not see. The goal of this paper is to illustrate the risk of overfitting a speech enhancement model to the metric used for evaluation. For this, we introduce enhancement models that exploit the widely used PESQ measure. Our \"PESQetarian\" model achieves 3.82 PESQ on VB-DMD while scoring very poorly in a listening experiment. While the obtained PESQ value of 3.82 would imply \"state-of-the-art\" PESQ-performance on the VB-DMD benchmark, our examples show that when optimizing w.r.t. a metric, an isolated evaluation on the same metric may be misleading. Instead, other metrics should be included in the evaluation and the resulting performance predictions should be confirmed by listening.",
      "authors": [
        "Danilo de Oliveira",
        "Simon Welker",
        "Julius Richter",
        "Timo Gerkmann"
      ],
      "published": "2024-06-05T17:07:39Z",
      "updated": "2024-06-05T17:07:39Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.03460v1",
      "landing_url": "https://arxiv.org/abs/2406.03460v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.03460"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses avoiding overfitting to PESQ metrics in speech enhancement and does not address discrete audio tokens or token-based representations, so it fails to meet the inclusion criteria focused on Discrete Audio Tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses avoiding overfitting to PESQ metrics in speech enhancement and does not address discrete audio tokens or token-based representations, so it fails to meet the inclusion criteria focused on Discrete Audio Tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on the risk of overfitting speech enhancement models to instrumental metrics like PESQ and emphasize the importance of diverse evaluation methods. However, there is no mention or indication that the study involves discrete audio token generation, vector quantization, or any form of token-based representation or sequence modeling as required in the inclusion criteria. The work instead concentrates on speech enhancement evaluation metrics rather than the design or application of discrete audio tokens for language modeling or downstream tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on the risk of overfitting speech enhancement models to instrumental metrics like PESQ and emphasize the importance of diverse evaluation methods. However, there is no mention or indication that the study involves discrete audio token generation, vector quantization, or any form of token-based representation or sequence modeling as required in the inclusion criteria. The work instead concentrates on speech enhancement evaluation metrics rather than the design or application of discrete audio tokens for language modeling or downstream tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model",
    "abstract": "Recent advances in large language models (LLMs) and development of audio codecs greatly propel the zero-shot TTS. They can synthesize personalized speech with only a 3-second speech of an unseen speaker as acoustic prompt. However, they only support short speech prompts and cannot leverage longer context information, as required in audiobook and conversational TTS scenarios. In this paper, we introduce a novel audio codec-based TTS model to adapt context features with multiple enhancements. Inspired by the success of Qformer, we propose a multi-modal context-enhanced Qformer (MMCE-Qformer) to utilize additional multi-modal context information. Besides, we adapt a pretrained LLM to leverage its understanding ability to predict semantic tokens, and use a SoundStorm to generate acoustic tokens thereby enhancing audio quality and speaker similarity. The extensive objective and subjective evaluations show that our proposed method outperforms baselines across various context TTS scenarios.",
    "metadata": {
      "arxiv_id": "2406.03706",
      "title": "Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model",
      "summary": "Recent advances in large language models (LLMs) and development of audio codecs greatly propel the zero-shot TTS. They can synthesize personalized speech with only a 3-second speech of an unseen speaker as acoustic prompt. However, they only support short speech prompts and cannot leverage longer context information, as required in audiobook and conversational TTS scenarios. In this paper, we introduce a novel audio codec-based TTS model to adapt context features with multiple enhancements. Inspired by the success of Qformer, we propose a multi-modal context-enhanced Qformer (MMCE-Qformer) to utilize additional multi-modal context information. Besides, we adapt a pretrained LLM to leverage its understanding ability to predict semantic tokens, and use a SoundStorm to generate acoustic tokens thereby enhancing audio quality and speaker similarity. The extensive objective and subjective evaluations show that our proposed method outperforms baselines across various context TTS scenarios.",
      "authors": [
        "Jinlong Xue",
        "Yayue Deng",
        "Yicheng Han",
        "Yingming Gao",
        "Ya Li"
      ],
      "published": "2024-06-06T03:06:45Z",
      "updated": "2024-06-06T03:06:45Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.03706v1",
      "landing_url": "https://arxiv.org/abs/2406.03706v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.03706"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract mentions acoustic/semantic tokens and uses a codec-based TTS approach with token-like representations, but it does not clearly present discrete audio token design, generation/quantization mechanisms, or codebooks as the main methodological contribution, nor provide reproducible tokenization details required by the inclusion criteria; thus it is not clearly includable based on the stated criteria (unclear fit).",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The abstract mentions acoustic/semantic tokens and uses a codec-based TTS approach with token-like representations, but it does not clearly present discrete audio token design, generation/quantization mechanisms, or codebooks as the main methodological contribution, nor provide reproducible tokenization details required by the inclusion criteria; thus it is not clearly includable based on the stated criteria (unclear fit).",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a novel audio codec-based text-to-speech model that uses discrete audio tokens (acoustic tokens generated via SoundStorm) and integrates these tokens with large language model predictions, which fits the theme of discrete audio token generation and usage for sequence modeling; however, the abstract lacks detailed technical descriptions on token generation procedures, vector quantization specifics, or training settings, thus limiting evaluation of reproducibility and compliance with all inclusion criteria.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a novel audio codec-based text-to-speech model that uses discrete audio tokens (acoustic tokens generated via SoundStorm) and integrates these tokens with large language model predictions, which fits the theme of discrete audio token generation and usage for sequence modeling; however, the abstract lacks detailed technical descriptions on token generation procedures, vector quantization specifics, or training settings, thus limiting evaluation of reproducibility and compliance with all inclusion criteria.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers rated this as uncertain (3) and noted the paper uses ‘acoustic tokens’ (generated by SoundStorm) and LLM-predicted ‘semantic tokens’, indicating the work operates with discrete audio/semantic token representations; however, the abstract does not clearly present a primary contribution of novel discrete audio-token design nor provide methodological details (token generation/quantization/codebook/levels or reproducibility information) required by the inclusion criteria. Therefore I cannot determine from the abstract whether the paper meets the requirement that discrete audio-token design and detailed generation流程 be a core, inspectable contribution. One-sentence explanation: The study uses and leverages discrete acoustic/semantic tokens but the abstract lacks the necessary technical details to confirm it meets the discrete-token-design and reproducibility inclusion requirements, so inclusion is uncertain.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers rated this as uncertain (3) and noted the paper uses ‘acoustic tokens’ (generated by SoundStorm) and LLM-predicted ‘semantic tokens’, indicating the work operates with discrete audio/semantic token representations; however, the abstract does not clearly present a primary contribution of novel discrete audio-token design nor provide methodological details (token generation/quantization/codebook/levels or reproducibility information) required by the inclusion criteria. Therefore I cannot determine from the abstract whether the paper meets the requirement that discrete audio-token design and detailed generation流程 be a core, inspectable contribution. One-sentence explanation: The study uses and leverages discrete acoustic/semantic tokens but the abstract lacks the necessary technical details to confirm it meets the discrete-token-design and reproducibility inclusion requirements, so inclusion is uncertain.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Near-field Beam Training with Sparse DFT Codebook",
    "abstract": "Extremely large-scale array (XL-array) has emerged as one promising technology to improve the spectral efficiency and spatial resolution of future sixth generation (6G) wireless systems.The upsurge in the antenna number antennas renders communication users more likely to be located in the near-field region, which requires a more accurate spherical (instead of planar) wavefront propagation modeling.This inevitably incurs unaffordable beam training overhead when performing a two-dimensional (2D) beam-search in both the angular and range domains.To address this issue, we first introduce a new sparse discrete Fourier transform (DFT) codebook, which exhibits the angular periodicity in the received beam pattern at the user, which motivates us to propose a three-phase beam training scheme.Specifically, in the first phase, we utilize the sparse DFT codebook for beam sweeping in an angular subspace and estimate candidate user angles according to the received beam pattern.Then, a central sub-array is activated to scan specific candidate angles for resolving the issue of angular ambiguity and identity the user angle.In the third phase, the polar-domain codebook is applied in the estimated angle to search the best effective range of the user.Finally, numerical results show that the proposed beam training scheme enabled by sparse DFT codebook achieves 98.67% reduction as compared with the exhaustive-search scheme, yet without compromising rate performance in the high signal-to-ratio (SNR) regime.",
    "metadata": {
      "arxiv_id": "2406.04262",
      "title": "Near-field Beam Training with Sparse DFT Codebook",
      "summary": "Extremely large-scale array (XL-array) has emerged as one promising technology to improve the spectral efficiency and spatial resolution of future sixth generation (6G) wireless systems.The upsurge in the antenna number antennas renders communication users more likely to be located in the near-field region, which requires a more accurate spherical (instead of planar) wavefront propagation modeling.This inevitably incurs unaffordable beam training overhead when performing a two-dimensional (2D) beam-search in both the angular and range domains.To address this issue, we first introduce a new sparse discrete Fourier transform (DFT) codebook, which exhibits the angular periodicity in the received beam pattern at the user, which motivates us to propose a three-phase beam training scheme.Specifically, in the first phase, we utilize the sparse DFT codebook for beam sweeping in an angular subspace and estimate candidate user angles according to the received beam pattern.Then, a central sub-array is activated to scan specific candidate angles for resolving the issue of angular ambiguity and identity the user angle.In the third phase, the polar-domain codebook is applied in the estimated angle to search the best effective range of the user.Finally, numerical results show that the proposed beam training scheme enabled by sparse DFT codebook achieves 98.67% reduction as compared with the exhaustive-search scheme, yet without compromising rate performance in the high signal-to-ratio (SNR) regime.",
      "authors": [
        "Cong Zhou",
        "Chenyu Wu",
        "Changsheng You",
        "Shuo Shi"
      ],
      "published": "2024-06-06T17:08:28Z",
      "updated": "2024-06-18T12:26:09Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04262v2",
      "landing_url": "https://arxiv.org/abs/2406.04262v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.04262"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses near-field beam training and sparse DFT codebooks for XL-array wireless systems, not discrete audio tokens or token-based audio representations, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses near-field beam training and sparse DFT codebooks for XL-array wireless systems, not discrete audio tokens or token-based audio representations, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on beam training using sparse DFT codebooks for wireless communication, which is unrelated to discrete audio tokens, their generation, or use in language/sequential modeling tasks as required by the inclusion criteria. No mention of audio signal processing, vector quantization of audio, or token design for audio understanding or generation is present, thus it does not meet any inclusion criteria nor violate specific exclusion criteria but simply falls outside the topic scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on beam training using sparse DFT codebooks for wireless communication, which is unrelated to discrete audio tokens, their generation, or use in language/sequential modeling tasks as required by the inclusion criteria. No mention of audio signal processing, vector quantization of audio, or token design for audio understanding or generation is present, thus it does not meet any inclusion criteria nor violate specific exclusion criteria but simply falls outside the topic scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Boosting Diffusion Model for Spectrogram Up-sampling in Text-to-speech: An Empirical Study",
    "abstract": "Scaling text-to-speech (TTS) with autoregressive language model (LM) to large-scale datasets by quantizing waveform into discrete speech tokens is making great progress to capture the diversity and expressiveness in human speech, but the speech reconstruction quality from discrete speech token is far from satisfaction depending on the compressed speech token compression ratio. Generative diffusion models trained with score-matching loss and continuous normalized flow trained with flow-matching loss have become prominent in generation of images as well as speech. LM based TTS systems usually quantize speech into discrete tokens and generate these tokens autoregressively, and finally use a diffusion model to up sample coarse-grained speech tokens into fine-grained codec features or mel-spectrograms before reconstructing into waveforms with vocoder, which has a high latency and is not realistic for real time speech applications. In this paper, we systematically investigate varied diffusion models for up sampling stage, which is the main bottleneck for streaming synthesis of LM and diffusion-based architecture, we present the model architecture, objective and subjective metrics to show quality and efficiency improvement.",
    "metadata": {
      "arxiv_id": "2406.04633",
      "title": "Boosting Diffusion Model for Spectrogram Up-sampling in Text-to-speech: An Empirical Study",
      "summary": "Scaling text-to-speech (TTS) with autoregressive language model (LM) to large-scale datasets by quantizing waveform into discrete speech tokens is making great progress to capture the diversity and expressiveness in human speech, but the speech reconstruction quality from discrete speech token is far from satisfaction depending on the compressed speech token compression ratio. Generative diffusion models trained with score-matching loss and continuous normalized flow trained with flow-matching loss have become prominent in generation of images as well as speech. LM based TTS systems usually quantize speech into discrete tokens and generate these tokens autoregressively, and finally use a diffusion model to up sample coarse-grained speech tokens into fine-grained codec features or mel-spectrograms before reconstructing into waveforms with vocoder, which has a high latency and is not realistic for real time speech applications. In this paper, we systematically investigate varied diffusion models for up sampling stage, which is the main bottleneck for streaming synthesis of LM and diffusion-based architecture, we present the model architecture, objective and subjective metrics to show quality and efficiency improvement.",
      "authors": [
        "Chong Zhang",
        "Yanqing Liu",
        "Yang Zheng",
        "Sheng Zhao"
      ],
      "published": "2024-06-07T04:34:03Z",
      "updated": "2024-06-07T04:34:03Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04633v1",
      "landing_url": "https://arxiv.org/abs/2406.04633v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.04633"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper centers on discrete speech tokens and token-based upsampling via diffusion, providing architecture/objective details and evaluation, thus meeting the inclusion criteria and not matching the exclusion items.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "This paper centers on discrete speech tokens and token-based upsampling via diffusion, providing architecture/objective details and evaluation, thus meeting the inclusion criteria and not matching the exclusion items.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on TTS systems that quantize waveform into discrete speech tokens and employs diffusion models for up-sampling from coarse-grained speech tokens to fine-grained features, which aligns with the inclusion criteria requiring discrete audio tokens used for sequence modeling and TTS generation. However, the abstract lacks detailed explanation on the token generation process, quantization methods, and explicit description of token properties or usage beyond the general mention of discrete tokens, which limits the clarity on methodology and reproducibility, hence somewhat constraining its compliance with inclusion criteria. The paper clearly addresses discrete tokens as core to its method, but limited technical details on token design and usage prevent a full confident inclusion outright.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on TTS systems that quantize waveform into discrete speech tokens and employs diffusion models for up-sampling from coarse-grained speech tokens to fine-grained features, which aligns with the inclusion criteria requiring discrete audio tokens used for sequence modeling and TTS generation. However, the abstract lacks detailed explanation on the token generation process, quantization methods, and explicit description of token properties or usage beyond the general mention of discrete tokens, which limits the clarity on methodology and reproducibility, hence somewhat constraining its compliance with inclusion criteria. The paper clearly addresses discrete tokens as core to its method, but limited technical details on token design and usage prevent a full confident inclusion outright.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers note the paper centers on LM-based TTS that uses discrete speech tokens and diffusion-based up-sampling; JuniorNano judged it meets inclusion, while JuniorMini flagged the abstract’s lack of concrete token-generation/quantization details. Comparing to the inclusion criteria, the abstract does mention quantized/discrete speech tokens and their use in an up-sampling stage, but it does not clearly present \"discrete audio token design\" (e.g., VQ/RVQ/codebook/layering, token-generation pipeline, training/inference settings) as a main contribution nor provide reproducible method details—requirements mandatory for inclusion. Therefore, because the core contribution appears to be improving diffusion up-sampling rather than proposing or thoroughly describing discrete token design or quantization mechanisms, and the abstract lacks sufficient methodological detail, it fails to meet the inclusion requirements.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers note the paper centers on LM-based TTS that uses discrete speech tokens and diffusion-based up-sampling; JuniorNano judged it meets inclusion, while JuniorMini flagged the abstract’s lack of concrete token-generation/quantization details. Comparing to the inclusion criteria, the abstract does mention quantized/discrete speech tokens and their use in an up-sampling stage, but it does not clearly present \"discrete audio token design\" (e.g., VQ/RVQ/codebook/layering, token-generation pipeline, training/inference settings) as a main contribution nor provide reproducible method details—requirements mandatory for inclusion. Therefore, because the core contribution appears to be improving diffusion up-sampling rather than proposing or thoroughly describing discrete token design or quantization mechanisms, and the abstract lacks sufficient methodological detail, it fails to meet the inclusion requirements.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Variational Flow Matching for Graph Generation",
    "abstract": "We present a formulation of flow matching as variational inference, which we refer to as variational flow matching (VFM). Based on this formulation we develop CatFlow, a flow matching method for categorical data. CatFlow is easy to implement, computationally efficient, and achieves strong results on graph generation tasks. In VFM, the objective is to approximate the posterior probability path, which is a distribution over possible end points of a trajectory. We show that VFM admits both the CatFlow objective and the original flow matching objective as special cases. We also relate VFM to score-based models, in which the dynamics are stochastic rather than deterministic, and derive a bound on the model likelihood based on a reweighted VFM objective. We evaluate CatFlow on one abstract graph generation task and two molecular generation tasks. In all cases, CatFlow exceeds or matches performance of the current state-of-the-art models.",
    "metadata": {
      "arxiv_id": "2406.04843",
      "title": "Variational Flow Matching for Graph Generation",
      "summary": "We present a formulation of flow matching as variational inference, which we refer to as variational flow matching (VFM). Based on this formulation we develop CatFlow, a flow matching method for categorical data. CatFlow is easy to implement, computationally efficient, and achieves strong results on graph generation tasks. In VFM, the objective is to approximate the posterior probability path, which is a distribution over possible end points of a trajectory. We show that VFM admits both the CatFlow objective and the original flow matching objective as special cases. We also relate VFM to score-based models, in which the dynamics are stochastic rather than deterministic, and derive a bound on the model likelihood based on a reweighted VFM objective. We evaluate CatFlow on one abstract graph generation task and two molecular generation tasks. In all cases, CatFlow exceeds or matches performance of the current state-of-the-art models.",
      "authors": [
        "Floor Eijkelboom",
        "Grigory Bartosh",
        "Christian Andersson Naesseth",
        "Max Welling",
        "Jan-Willem van de Meent"
      ],
      "published": "2024-06-07T11:16:17Z",
      "updated": "2025-08-15T21:12:02Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04843v2",
      "landing_url": "https://arxiv.org/abs/2406.04843v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.04843"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses variational flow matching for graph generation and CatFlow, unrelated to discrete audio token design; it does not meet the Audio Tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses variational flow matching for graph generation and CatFlow, unrelated to discrete audio token design; it does not meet the Audio Tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on graph generation using variational flow matching and categorical data, with no mention of discrete audio tokens, audio signal processing, or audio-related discrete token generation which are central to the inclusion criteria. Hence, it does not meet the inclusion requirements related to discrete audio token research or design.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on graph generation using variational flow matching and categorical data, with no mention of discrete audio tokens, audio signal processing, or audio-related discrete token generation which are central to the inclusion criteria. Hence, it does not meet the inclusion requirements related to discrete audio token research or design.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations",
    "abstract": "In recent years, self-supervised pre-training methods have gained significant traction in learning high-level information from raw speech. Among these methods, HuBERT has demonstrated SOTA performance in automatic speech recognition (ASR). However, HuBERT's performance lags behind data2vec due to disparities in pre-training strategies. In this paper, we propose (i) a Swap method to address pre-training and inference mismatch observed in HuBERT and (ii) incorporates Multicluster masked prediction loss for more effective utilization of the models capacity. The resulting method is, MS-HuBERT, an end-to-end self-supervised pre-training method for learning robust speech representations. It beats vanilla HuBERT on the ASR Librispeech benchmark on average by a 5% margin when evaluated on different finetuning splits. Additionally, we demonstrate that the learned embeddings obtained during pre-training encode essential information for improving performance of content based tasks such as ASR.",
    "metadata": {
      "arxiv_id": "2406.05661",
      "title": "MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations",
      "summary": "In recent years, self-supervised pre-training methods have gained significant traction in learning high-level information from raw speech. Among these methods, HuBERT has demonstrated SOTA performance in automatic speech recognition (ASR). However, HuBERT's performance lags behind data2vec due to disparities in pre-training strategies. In this paper, we propose (i) a Swap method to address pre-training and inference mismatch observed in HuBERT and (ii) incorporates Multicluster masked prediction loss for more effective utilization of the models capacity. The resulting method is, MS-HuBERT, an end-to-end self-supervised pre-training method for learning robust speech representations. It beats vanilla HuBERT on the ASR Librispeech benchmark on average by a 5% margin when evaluated on different finetuning splits. Additionally, we demonstrate that the learned embeddings obtained during pre-training encode essential information for improving performance of content based tasks such as ASR.",
      "authors": [
        "Hemant Yadav",
        "Sunayana Sitaram",
        "Rajiv Ratn Shah"
      ],
      "published": "2024-06-09T06:30:28Z",
      "updated": "2025-02-18T10:07:36Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05661v4",
      "landing_url": "https://arxiv.org/abs/2406.05661v4",
      "doi": "https://doi.org/10.48550/arXiv.2406.05661"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discusses improving HuBERT with a mismatch mitigation method and multicluster targets, but does not present discrete audio tokenization (VQ/RVQ-like tokens) or a tokenization design as the core contribution, so it does not meet the discrete audio token criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper discusses improving HuBERT with a mismatch mitigation method and multicluster targets, but does not present discrete audio tokenization (VQ/RVQ-like tokens) or a tokenization design as the core contribution, so it does not meet the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses MS-HuBERT, a self-supervised pre-training method for learning robust speech representations and improving ASR performance, but does not mention the generation of discrete audio tokens, token quantization, or token usage for sequence modeling as defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses MS-HuBERT, a self-supervised pre-training method for learning robust speech representations and improving ASR performance, but does not mention the generation of discrete audio tokens, token quantization, or token usage for sequence modeling as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "mHuBERT-147: A Compact Multilingual HuBERT Model",
    "abstract": "We present mHuBERT-147, the first general-purpose massively multilingual HuBERT speech representation model trained on 90K hours of clean, open-license data. To scale up the multi-iteration HuBERT approach, we use faiss-based clustering, achieving 5.2x faster label assignment than the original method. We also apply a new multilingual batching up-sampling strategy, leveraging both language and dataset diversity. After 3 training iterations, our compact 95M parameter mHuBERT-147 outperforms larger models trained on substantially more data. We rank second and first on the ML-SUPERB 10min and 1h leaderboards, with SOTA scores for 3 tasks. Across ASR/LID tasks, our model consistently surpasses XLS-R (300M params; 436K hours) and demonstrates strong competitiveness against the much larger MMS (1B params; 491K hours). Our findings indicate that mHuBERT-147 is a promising model for multilingual speech tasks, offering an unprecedented balance between high performance and parameter efficiency.",
    "metadata": {
      "arxiv_id": "2406.06371",
      "title": "mHuBERT-147: A Compact Multilingual HuBERT Model",
      "summary": "We present mHuBERT-147, the first general-purpose massively multilingual HuBERT speech representation model trained on 90K hours of clean, open-license data. To scale up the multi-iteration HuBERT approach, we use faiss-based clustering, achieving 5.2x faster label assignment than the original method. We also apply a new multilingual batching up-sampling strategy, leveraging both language and dataset diversity. After 3 training iterations, our compact 95M parameter mHuBERT-147 outperforms larger models trained on substantially more data. We rank second and first on the ML-SUPERB 10min and 1h leaderboards, with SOTA scores for 3 tasks. Across ASR/LID tasks, our model consistently surpasses XLS-R (300M params; 436K hours) and demonstrates strong competitiveness against the much larger MMS (1B params; 491K hours). Our findings indicate that mHuBERT-147 is a promising model for multilingual speech tasks, offering an unprecedented balance between high performance and parameter efficiency.",
      "authors": [
        "Marcely Zanon Boito",
        "Vivek Iyer",
        "Nikolaos Lagos",
        "Laurent Besacier",
        "Ioan Calapodescu"
      ],
      "published": "2024-06-10T15:32:42Z",
      "updated": "2024-11-21T10:45:39Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06371v5",
      "landing_url": "https://arxiv.org/abs/2406.06371v5",
      "doi": "https://doi.org/10.48550/arXiv.2406.06371"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper targets a compact multilingual HuBERT model and does not present discrete audio token design or a token generation/quantization pipeline as its core contribution, though it uses clustering for pseudo-labeling.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "This paper targets a compact multilingual HuBERT model and does not present discrete audio token design or a token generation/quantization pipeline as its core contribution, though it uses clustering for pseudo-labeling.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The work mHuBERT-147 describes a compact multilingual HuBERT speech representation model trained with a large amount of data and demonstrates superior performance compared to larger models; however, the abstract does not explicitly state that it produces discrete audio tokens via quantization or discretization methods such as VQ or RVQ, nor does it detail the token generation process or treat tokens as core representations for sequence modeling or audio language modeling. The focus is on representation learning and performance benchmarks rather than token design or discrete token generation. Therefore, it does not clearly meet the inclusion criteria centered on discrete audio token generation as a main contribution, nor does it clearly fall under the exclusion criteria of lacking method details since it does describe modeling improvements but without token quantization details, making its inclusion uncertain.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The work mHuBERT-147 describes a compact multilingual HuBERT speech representation model trained with a large amount of data and demonstrates superior performance compared to larger models; however, the abstract does not explicitly state that it produces discrete audio tokens via quantization or discretization methods such as VQ or RVQ, nor does it detail the token generation process or treat tokens as core representations for sequence modeling or audio language modeling. The focus is on representation learning and performance benchmarks rather than token design or discrete token generation. Therefore, it does not clearly meet the inclusion criteria centered on discrete audio token generation as a main contribution, nor does it clearly fall under the exclusion criteria of lacking method details since it does describe modeling improvements but without token quantization details, making its inclusion uncertain.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers observed that mHuBERT-147 is a strong multilingual self-supervised representation model that uses clustering for pseudo-labeling, but neither found evidence in the abstract that the paper's primary contribution is the design of discrete audio tokens (e.g., VQ/RVQ/codebook details, token-generation pipeline, or explicit use of tokens for sequence/language modeling). The abstract focuses on scaling HuBERT, faster clustering, and representation/benchmark improvements rather than presenting a reproducible discrete-token design or demonstrating tokens as the core mechanism for audio language modeling, so it fails the inclusion requirement that discrete audio token design be a main research contribution.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers observed that mHuBERT-147 is a strong multilingual self-supervised representation model that uses clustering for pseudo-labeling, but neither found evidence in the abstract that the paper's primary contribution is the design of discrete audio tokens (e.g., VQ/RVQ/codebook details, token-generation pipeline, or explicit use of tokens for sequence/language modeling). The abstract focuses on scaling HuBERT, faster clustering, and representation/benchmark improvements rather than presenting a reproducible discrete-token design or demonstrating tokens as the core mechanism for audio language modeling, so it fails the inclusion requirement that discrete audio token design be a main research contribution.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Refining Self-Supervised Learnt Speech Representation using Brain Activations",
    "abstract": "It was shown in literature that speech representations extracted by self-supervised pre-trained models exhibit similarities with brain activations of human for speech perception and fine-tuning speech representation models on downstream tasks can further improve the similarity. However, it still remains unclear if this similarity can be used to optimize the pre-trained speech models. In this work, we therefore propose to use the brain activations recorded by fMRI to refine the often-used wav2vec2.0 model by aligning model representations toward human neural responses. Experimental results on SUPERB reveal that this operation is beneficial for several downstream tasks, e.g., speaker verification, automatic speech recognition, intent classification.One can then consider the proposed method as a new alternative to improve self-supervised speech models.",
    "metadata": {
      "arxiv_id": "2406.08266",
      "title": "Refining Self-Supervised Learnt Speech Representation using Brain Activations",
      "summary": "It was shown in literature that speech representations extracted by self-supervised pre-trained models exhibit similarities with brain activations of human for speech perception and fine-tuning speech representation models on downstream tasks can further improve the similarity. However, it still remains unclear if this similarity can be used to optimize the pre-trained speech models. In this work, we therefore propose to use the brain activations recorded by fMRI to refine the often-used wav2vec2.0 model by aligning model representations toward human neural responses. Experimental results on SUPERB reveal that this operation is beneficial for several downstream tasks, e.g., speaker verification, automatic speech recognition, intent classification.One can then consider the proposed method as a new alternative to improve self-supervised speech models.",
      "authors": [
        "Hengyu Li",
        "Kangdi Mei",
        "Zhaoci Liu",
        "Yang Ai",
        "Liping Chen",
        "Jie Zhang",
        "Zhenhua Ling"
      ],
      "published": "2024-06-12T14:34:41Z",
      "updated": "2024-06-13T06:26:35Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08266v2",
      "landing_url": "https://arxiv.org/abs/2406.08266v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.08266"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on aligning self-supervised speech representations with brain activations (fMRI) to refine models, not on discrete audio tokens or tokenization methods; it fails the 'Discrete Audio Tokens' inclusion criterion and lacks the required tokenization methodology details.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on aligning self-supervised speech representations with brain activations (fMRI) to refine models, not on discrete audio tokens or tokenization methods; it fails the 'Discrete Audio Tokens' inclusion criterion and lacks the required tokenization methodology details.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on refining self-supervised speech representations using brain activations, but it does not discuss the generation or usage of discrete audio tokens through vector quantization or similar discrete tokenization mechanisms as defined in the inclusion criteria. There is no mention of discrete token design, token generation process, or usage of tokens for language or sequence modeling. Thus, it does not meet the core inclusion requirement related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on refining self-supervised speech representations using brain activations, but it does not discuss the generation or usage of discrete audio tokens through vector quantization or similar discrete tokenization mechanisms as defined in the inclusion criteria. There is no mention of discrete token design, token generation process, or usage of tokens for language or sequence modeling. Thus, it does not meet the core inclusion requirement related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study on Word Error Rate and Fusion Techniques",
    "abstract": "Text data is commonly utilized as a primary input to enhance Speech Emotion Recognition (SER) performance and reliability. However, the reliance on human-transcribed text in most studies impedes the development of practical SER systems, creating a gap between in-lab research and real-world scenarios where Automatic Speech Recognition (ASR) serves as the text source. Hence, this study benchmarks SER performance using ASR transcripts with varying Word Error Rates (WERs) from eleven models on three well-known corpora: IEMOCAP, CMU-MOSI, and MSP-Podcast. Our evaluation includes both text-only and bimodal SER with six fusion techniques, aiming for a comprehensive analysis that uncovers novel findings and challenges faced by current SER research. Additionally, we propose a unified ASR error-robust framework integrating ASR error correction and modality-gated fusion, achieving lower WER and higher SER results compared to the best-performing ASR transcript. These findings provide insights into SER with ASR assistance, especially for real-world applications.",
    "metadata": {
      "arxiv_id": "2406.08353",
      "title": "Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study on Word Error Rate and Fusion Techniques",
      "summary": "Text data is commonly utilized as a primary input to enhance Speech Emotion Recognition (SER) performance and reliability. However, the reliance on human-transcribed text in most studies impedes the development of practical SER systems, creating a gap between in-lab research and real-world scenarios where Automatic Speech Recognition (ASR) serves as the text source. Hence, this study benchmarks SER performance using ASR transcripts with varying Word Error Rates (WERs) from eleven models on three well-known corpora: IEMOCAP, CMU-MOSI, and MSP-Podcast. Our evaluation includes both text-only and bimodal SER with six fusion techniques, aiming for a comprehensive analysis that uncovers novel findings and challenges faced by current SER research. Additionally, we propose a unified ASR error-robust framework integrating ASR error correction and modality-gated fusion, achieving lower WER and higher SER results compared to the best-performing ASR transcript. These findings provide insights into SER with ASR assistance, especially for real-world applications.",
      "authors": [
        "Yuanchao Li",
        "Peter Bell",
        "Catherine Lai"
      ],
      "published": "2024-06-12T15:59:25Z",
      "updated": "2025-03-23T16:45:04Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.MM",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08353v3",
      "landing_url": "https://arxiv.org/abs/2406.08353v3",
      "doi": "https://doi.org/10.48550/arXiv.2406.08353"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the study analyzes SER with ASR transcripts and WER, it does not involve discrete audio tokens or tokenization-based representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although the study analyzes SER with ASR transcripts and WER, it does not involve discrete audio tokens or tokenization-based representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Speech Emotion Recognition using ASR transcripts and investigates fusion techniques and error correction frameworks, without addressing discrete audio tokens, vector quantization, or encoding mechanisms for audio tokens. The methods and contributions do not revolve around the generation or use of discrete audio tokens as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Speech Emotion Recognition using ASR transcripts and investigates fusion techniques and error correction frameworks, without addressing discrete audio tokens, vector quantization, or encoding mechanisms for audio tokens. The methods and contributions do not revolve around the generation or use of discrete audio tokens as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SVSNet+: Enhancing Speaker Voice Similarity Assessment Models with Representations from Speech Foundation Models",
    "abstract": "Representations from pre-trained speech foundation models (SFMs) have shown impressive performance in many downstream tasks. However, the potential benefits of incorporating pre-trained SFM representations into speaker voice similarity assessment have not been thoroughly investigated. In this paper, we propose SVSNet+, a model that integrates pre-trained SFM representations to improve performance in assessing speaker voice similarity. Experimental results on the Voice Conversion Challenge 2018 and 2020 datasets show that SVSNet+ incorporating WavLM representations shows significant improvements compared to baseline models. In addition, while fine-tuning WavLM with a small dataset of the downstream task does not improve performance, using the same dataset to learn a weighted-sum representation of WavLM can substantially improve performance. Furthermore, when WavLM is replaced by other SFMs, SVSNet+ still outperforms the baseline models and exhibits strong generalization ability.",
    "metadata": {
      "arxiv_id": "2406.08445",
      "title": "SVSNet+: Enhancing Speaker Voice Similarity Assessment Models with Representations from Speech Foundation Models",
      "summary": "Representations from pre-trained speech foundation models (SFMs) have shown impressive performance in many downstream tasks. However, the potential benefits of incorporating pre-trained SFM representations into speaker voice similarity assessment have not been thoroughly investigated. In this paper, we propose SVSNet+, a model that integrates pre-trained SFM representations to improve performance in assessing speaker voice similarity. Experimental results on the Voice Conversion Challenge 2018 and 2020 datasets show that SVSNet+ incorporating WavLM representations shows significant improvements compared to baseline models. In addition, while fine-tuning WavLM with a small dataset of the downstream task does not improve performance, using the same dataset to learn a weighted-sum representation of WavLM can substantially improve performance. Furthermore, when WavLM is replaced by other SFMs, SVSNet+ still outperforms the baseline models and exhibits strong generalization ability.",
      "authors": [
        "Chun Yin",
        "Tai-Shih Chi",
        "Yu Tsao",
        "Hsin-Min Wang"
      ],
      "published": "2024-06-12T17:37:09Z",
      "updated": "2024-06-12T17:37:09Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08445v1",
      "landing_url": "https://arxiv.org/abs/2406.08445v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.08445"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper uses pre-trained speech foundation model representations for speaker similarity and does not introduce discrete audio tokens or a tokenization/quantization workflow, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper uses pre-trained speech foundation model representations for speaker similarity and does not introduce discrete audio tokens or a tokenization/quantization workflow, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on enhancing speaker voice similarity assessment using representations from speech foundation models (SFMs), without discussing the design, generation, or usage of discrete audio tokens derived from vector quantization or related quantization methods that serve as core elements for sequence or language modeling. It does not elaborate on token generation processes or any discrete token-based representations, failing to meet the core inclusion criteria of discrete audio token design or usage for audio language modeling or generative tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on enhancing speaker voice similarity assessment using representations from speech foundation models (SFMs), without discussing the design, generation, or usage of discrete audio tokens derived from vector quantization or related quantization methods that serve as core elements for sequence or language modeling. It does not elaborate on token generation processes or any discrete token-based representations, failing to meet the core inclusion criteria of discrete audio token design or usage for audio language modeling or generative tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?",
    "abstract": "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.",
    "metadata": {
      "arxiv_id": "2406.10735",
      "title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?",
      "summary": "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.",
      "authors": [
        "Pooneh Mousavi",
        "Jarod Duret",
        "Salah Zaiem",
        "Luca Della Libera",
        "Artem Ploujnikov",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "published": "2024-06-15T20:43:07Z",
      "updated": "2024-06-15T20:43:07Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10735v1",
      "landing_url": "https://arxiv.org/abs/2406.10735v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10735"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract clearly centers on discrete audio tokens, outlines token generation/quantization via SSL and neural codecs (semantic tokens, universal vocoder, attention over SSL layers), and targets downstream tasks like audio-language modeling, satisfying the inclusion criteria, with the date issue unknown from the abstract not disqualifying inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract clearly centers on discrete audio tokens, outlines token generation/quantization via SSL and neural codecs (semantic tokens, universal vocoder, attention over SSL layers), and targets downstream tasks like audio-language modeling, satisfying the inclusion criteria, with the date issue unknown from the abstract not disqualifying inclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on extracting discrete audio tokens from self-supervised learning models, discussing tokenization methods including vector quantization and semantic tokens, and their applications in audio tasks, which aligns well with the inclusion criteria requirements of learning-based discrete token generation preserving semantic and acoustic information for sequence modeling. The abstract also mentions technical details about token configurations, usage in generative and discriminative tasks, and attention mechanisms to optimize token representations, indicating sufficient methodological content for reproducibility. There is no indication that the study is limited to traditional compression codecs nor lacks research depth. Therefore, the study meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on extracting discrete audio tokens from self-supervised learning models, discussing tokenization methods including vector quantization and semantic tokens, and their applications in audio tasks, which aligns well with the inclusion criteria requirements of learning-based discrete token generation preserving semantic and acoustic information for sequence modeling. The abstract also mentions technical details about token configurations, usage in generative and discriminative tasks, and attention mechanisms to optimize token representations, indicating sufficient methodological content for reproducibility. There is no indication that the study is limited to traditional compression codecs nor lacks research depth. Therefore, the study meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "NAST: Noise Aware Speech Tokenization for Speech Language Models",
    "abstract": "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.",
    "metadata": {
      "arxiv_id": "2406.11037",
      "title": "NAST: Noise Aware Speech Tokenization for Speech Language Models",
      "summary": "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.",
      "authors": [
        "Shoval Messica",
        "Yossi Adi"
      ],
      "published": "2024-06-16T18:20:45Z",
      "updated": "2024-06-16T18:20:45Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11037v1",
      "landing_url": "https://arxiv.org/abs/2406.11037v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.11037"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on a discrete speech tokenization framework (NAST) for Speech Language Models, detailing a tokenization-oriented architecture (predictor, residual encoder, decoder) and robustness analyses, which aligns with the Discrete Audio Tokens theme rather than being a mere demo; thus it should be included.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The work centers on a discrete speech tokenization framework (NAST) for Speech Language Models, detailing a tokenization-oriented architecture (predictor, residual encoder, decoder) and robustness analyses, which aligns with the Discrete Audio Tokens theme rather than being a mere demo; thus it should be included.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explicitly addresses speech tokenization by representing speech signals as discrete units, aiming to improve noise robustness for speech language models. It introduces a novel method (NAST) with components that effectively generate and use discrete tokens for downstream sequence modeling tasks, with clear method description and evaluation. The analysis also highlights semantic/acoustic disentanglement and robustness, aligning with the inclusion criteria of discrete audio token design and usage in sequence models. The abstract seems to provide sufficient technical content and points to code availability, supporting reproducibility. Thus, it meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explicitly addresses speech tokenization by representing speech signals as discrete units, aiming to improve noise robustness for speech language models. It introduces a novel method (NAST) with components that effectively generate and use discrete tokens for downstream sequence modeling tasks, with clear method description and evaluation. The analysis also highlights semantic/acoustic disentanglement and robustness, aligning with the inclusion criteria of discrete audio token design and usage in sequence models. The abstract seems to provide sufficient technical content and points to code availability, supporting reproducibility. Thus, it meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of 99%",
    "abstract": "In the realm of image quantization exemplified by VQGAN, the process encodes images into discrete tokens drawn from a codebook with a predefined size. Recent advancements, particularly with LLAMA 3, reveal that enlarging the codebook significantly enhances model performance. However, VQGAN and its derivatives, such as VQGAN-FC (Factorized Codes) and VQGAN-EMA, continue to grapple with challenges related to expanding the codebook size and enhancing codebook utilization. For instance, VQGAN-FC is restricted to learning a codebook with a maximum size of 16,384, maintaining a typically low utilization rate of less than 12% on ImageNet. In this work, we propose a novel image quantization model named VQGAN-LC (Large Codebook), which extends the codebook size to 100,000, achieving an utilization rate exceeding 99%. Unlike previous methods that optimize each codebook entry, our approach begins with a codebook initialized with 100,000 features extracted by a pre-trained vision encoder. Optimization then focuses on training a projector that aligns the entire codebook with the feature distributions of the encoder in VQGAN-LC. We demonstrate the superior performance of our model over its counterparts across a variety of tasks, including image reconstruction, image classification, auto-regressive image generation using GPT, and image creation with diffusion- and flow-based generative models. Code and models are available at https://github.com/zh460045050/VQGAN-LC.",
    "metadata": {
      "arxiv_id": "2406.11837",
      "title": "Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of 99%",
      "summary": "In the realm of image quantization exemplified by VQGAN, the process encodes images into discrete tokens drawn from a codebook with a predefined size. Recent advancements, particularly with LLAMA 3, reveal that enlarging the codebook significantly enhances model performance. However, VQGAN and its derivatives, such as VQGAN-FC (Factorized Codes) and VQGAN-EMA, continue to grapple with challenges related to expanding the codebook size and enhancing codebook utilization. For instance, VQGAN-FC is restricted to learning a codebook with a maximum size of 16,384, maintaining a typically low utilization rate of less than 12% on ImageNet. In this work, we propose a novel image quantization model named VQGAN-LC (Large Codebook), which extends the codebook size to 100,000, achieving an utilization rate exceeding 99%. Unlike previous methods that optimize each codebook entry, our approach begins with a codebook initialized with 100,000 features extracted by a pre-trained vision encoder. Optimization then focuses on training a projector that aligns the entire codebook with the feature distributions of the encoder in VQGAN-LC. We demonstrate the superior performance of our model over its counterparts across a variety of tasks, including image reconstruction, image classification, auto-regressive image generation using GPT, and image creation with diffusion- and flow-based generative models. Code and models are available at https://github.com/zh460045050/VQGAN-LC.",
      "authors": [
        "Lei Zhu",
        "Fangyun Wei",
        "Yanye Lu",
        "Dong Chen"
      ],
      "published": "2024-06-17T17:59:57Z",
      "updated": "2024-06-17T17:59:57Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11837v1",
      "landing_url": "https://arxiv.org/abs/2406.11837v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.11837"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses image quantization and visual tokens (VQGAN) rather than discrete audio tokens, so it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses image quantization and visual tokens (VQGAN) rather than discrete audio tokens, so it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on image quantization and codebook scaling for VQGAN models in computer vision, without addressing audio signals, discrete audio token design, or audio-related modeling tasks as required by the inclusion criteria. Therefore, it does not meet the criteria related to discrete audio tokens and their generation processes for sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on image quantization and codebook scaling for VQGAN models in computer vision, without addressing audio signals, discrete audio token design, or audio-related modeling tasks as required by the inclusion criteria. Therefore, it does not meet the criteria related to discrete audio tokens and their generation processes for sequence modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Enhancing Automated Audio Captioning via Large Language Models with Optimized Audio Encoding",
    "abstract": "Automated audio captioning (AAC) is an audio-to-text task to describe audio contents in natural language. Recently, the advancements in large language models (LLMs), with improvements in training approaches for audio encoders, have opened up possibilities for improving AAC. Thus, we explore enhancing AAC from three aspects: 1) a pre-trained audio encoder via consistent ensemble distillation (CED) is used to improve the effectivity of acoustic tokens, with a querying transformer (Q-Former) bridging the modality gap to LLM and compress acoustic tokens; 2) we investigate the advantages of using a Llama 2 with 7B parameters as the decoder; 3) another pre-trained LLM corrects text errors caused by insufficient training data and annotation ambiguities. Both the audio encoder and text decoder are optimized by low-rank adaptation (LoRA). Experiments show that each of these enhancements is effective. Our method obtains a 33.0 SPIDEr-FL score, outperforming the winner of DCASE 2023 Task 6A.",
    "metadata": {
      "arxiv_id": "2406.13275",
      "title": "Enhancing Automated Audio Captioning via Large Language Models with Optimized Audio Encoding",
      "summary": "Automated audio captioning (AAC) is an audio-to-text task to describe audio contents in natural language. Recently, the advancements in large language models (LLMs), with improvements in training approaches for audio encoders, have opened up possibilities for improving AAC. Thus, we explore enhancing AAC from three aspects: 1) a pre-trained audio encoder via consistent ensemble distillation (CED) is used to improve the effectivity of acoustic tokens, with a querying transformer (Q-Former) bridging the modality gap to LLM and compress acoustic tokens; 2) we investigate the advantages of using a Llama 2 with 7B parameters as the decoder; 3) another pre-trained LLM corrects text errors caused by insufficient training data and annotation ambiguities. Both the audio encoder and text decoder are optimized by low-rank adaptation (LoRA). Experiments show that each of these enhancements is effective. Our method obtains a 33.0 SPIDEr-FL score, outperforming the winner of DCASE 2023 Task 6A.",
      "authors": [
        "Jizhong Liu",
        "Gang Li",
        "Junbo Zhang",
        "Heinrich Dinkel",
        "Yongqing Wang",
        "Zhiyong Yan",
        "Yujun Wang",
        "Bin Wang"
      ],
      "published": "2024-06-19T07:09:46Z",
      "updated": "2024-06-25T08:07:36Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13275v2",
      "landing_url": "https://arxiv.org/abs/2406.13275v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.13275"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on AAC improvements via LLMs and optimized audio encodings; while it mentions 'acoustic tokens', it does not present the discrete audio token design, quantization method, or a token-generation process as a core contribution, so it does not meet the 'Discrete Audio Tokens' criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The article focuses on AAC improvements via LLMs and optimized audio encodings; while it mentions 'acoustic tokens', it does not present the discrete audio token design, quantization method, or a token-generation process as a core contribution, so it does not meet the 'Discrete Audio Tokens' criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses improving automated audio captioning using large language models and optimized audio encoding but does not explicitly mention the generation of discrete audio tokens through vector quantization or similar quantization methods. It focuses on enhancing acoustic tokens but lacks clarity on the tokenization process, quantization, or details about discrete token design and usage for sequence modeling, which are core inclusion criteria. Additionally, it does not sufficiently detail the token generation process or its use in audio language modeling or generative tasks, which limits its alignment with the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses improving automated audio captioning using large language models and optimized audio encoding but does not explicitly mention the generation of discrete audio tokens through vector quantization or similar quantization methods. It focuses on enhancing acoustic tokens but lacks clarity on the tokenization process, quantization, or details about discrete token design and usage for sequence modeling, which are core inclusion criteria. Additionally, it does not sufficiently detail the token generation process or its use in audio language modeling or generative tasks, which limits its alignment with the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Children's Speech Recognition through Discrete Token Enhancement",
    "abstract": "Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters.",
    "metadata": {
      "arxiv_id": "2406.13431",
      "title": "Children's Speech Recognition through Discrete Token Enhancement",
      "summary": "Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters.",
      "authors": [
        "Vrunda N. Sukhadia",
        "Shammur Absar Chowdhury"
      ],
      "published": "2024-06-19T10:45:12Z",
      "updated": "2024-06-24T15:31:59Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13431v2",
      "landing_url": "https://arxiv.org/abs/2406.13431v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.13431"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work explicitly studies discrete audio tokens as the main input representation for children’s ASR, includes token-generation strategies (single-view and multi-view) and reports generalization performance, aligning with the inclusion criteria for discrete token design and method-oriented research.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The work explicitly studies discrete audio tokens as the main input representation for children’s ASR, includes token-generation strategies (single-view and multi-view) and reports generalization performance, aligning with the inclusion criteria for discrete token design and method-oriented research.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on integrating discrete speech tokens into children's speech recognition, clearly detailing token generation and usage methods, employing discrete token strategies that capture linguistic and acoustic information, aligning well with the inclusion criteria of utilizing discrete audio tokens for sequence modeling; it also contains technical detail relevant for reproducibility and excludes traditional codec-only approaches or demo-level work.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on integrating discrete speech tokens into children's speech recognition, clearly detailing token generation and usage methods, employing discrete token strategies that capture linguistic and acoustic information, aligning well with the inclusion criteria of utilizing discrete audio tokens for sequence modeling; it also contains technical detail relevant for reproducibility and excludes traditional codec-only approaches or demo-level work.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DASB - Discrete Audio and Speech Benchmark",
    "abstract": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.",
    "metadata": {
      "arxiv_id": "2406.14294",
      "title": "DASB - Discrete Audio and Speech Benchmark",
      "summary": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.",
      "authors": [
        "Pooneh Mousavi",
        "Luca Della Libera",
        "Jarod Duret",
        "Artem Ploujnikov",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "published": "2024-06-20T13:23:27Z",
      "updated": "2024-06-21T17:07:17Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14294v2",
      "landing_url": "https://arxiv.org/abs/2406.14294v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.14294"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a Discrete Audio Tokens benchmark/leaderboard rather than presenting a concrete token design or reproducible methodology for token generation; it reads as a system/demo/benchmark paper, which likely does not meet the core inclusion criterion for a discrete-token design with detailed methodology, so it should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a Discrete Audio Tokens benchmark/leaderboard rather than presenting a concrete token design or reproducible methodology for token generation; it reads as a system/demo/benchmark paper, which likely does not meet the core inclusion criterion for a discrete-token design with detailed methodology, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The article introduces a benchmark (DASB) for assessing discrete audio tokens across multiple tasks, including speech recognition and audio generation, which indicates the focus is on discrete audio representations used in sequence modeling. The abstract mentions comparing semantic tokens versus compression tokens, highlighting token-based representations preserving phonetic and semantic content, consistent with inclusion criteria about discrete token generation and their use for audio understanding and generation. However, the abstract does not clearly provide detailed methodological descriptions such as token generation process, quantization type, or model structure, which are required for full inclusion. As a benchmark paper releasing a leaderboard, it may be considered partly an overview or demo, potentially conflicting with exclusion criteria if lacking thorough method details. Yet, the study addresses the core concepts of discrete audio tokens and their performance in various tasks, suggesting it contributes relevant technical content. Overall, it is more aligned with inclusion than exclusion but lacks explicit detailed method descriptions in the abstract, resulting in moderate confidence about including this paper.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The article introduces a benchmark (DASB) for assessing discrete audio tokens across multiple tasks, including speech recognition and audio generation, which indicates the focus is on discrete audio representations used in sequence modeling. The abstract mentions comparing semantic tokens versus compression tokens, highlighting token-based representations preserving phonetic and semantic content, consistent with inclusion criteria about discrete token generation and their use for audio understanding and generation. However, the abstract does not clearly provide detailed methodological descriptions such as token generation process, quantization type, or model structure, which are required for full inclusion. As a benchmark paper releasing a leaderboard, it may be considered partly an overview or demo, potentially conflicting with exclusion criteria if lacking thorough method details. Yet, the study addresses the core concepts of discrete audio tokens and their performance in various tasks, suggesting it contributes relevant technical content. Overall, it is more aligned with inclusion than exclusion but lacks explicit detailed method descriptions in the abstract, resulting in moderate confidence about including this paper.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the abstract and the two junior reviewers' opinions (JuniorNano: exclude—benchmark/demo lacking method details; JuniorMini: lean include but flagged missing methodological specifics). The DASB paper appears to be a leaderboard/benchmark comparing existing semantic vs compression tokens across tasks rather than proposing or thoroughly describing a discrete-token design (no token-generation/quantization/codebook/architecture details in the abstract). That places it within the explicit exclusion criterion for demo/leaderboard-style work that lacks reproducible method details, so it should not be included.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed the abstract and the two junior reviewers' opinions (JuniorNano: exclude—benchmark/demo lacking method details; JuniorMini: lean include but flagged missing methodological specifics). The DASB paper appears to be a leaderboard/benchmark comparing existing semantic vs compression tokens across tasks rather than proposing or thoroughly describing a discrete-token design (no token-generation/quantization/codebook/architecture details in the abstract). That places it within the explicit exclusion criterion for demo/leaderboard-style work that lacks reproducible method details, so it should not be included.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Large Vocabulary Size Improves Large Language Models",
    "abstract": "This paper empirically investigates the relationship between subword vocabulary size and the performance of large language models (LLMs) to provide insights on how to define the vocabulary size. Experimental results show that larger vocabulary sizes lead to better performance in LLMs. Moreover, we consider a continual training scenario where a pre-trained language model is trained on a different target language. We introduce a simple method to use a new vocabulary instead of the pre-defined one. We show that using the new vocabulary outperforms the model with the vocabulary used in pre-training.",
    "metadata": {
      "arxiv_id": "2406.16508",
      "title": "Large Vocabulary Size Improves Large Language Models",
      "summary": "This paper empirically investigates the relationship between subword vocabulary size and the performance of large language models (LLMs) to provide insights on how to define the vocabulary size. Experimental results show that larger vocabulary sizes lead to better performance in LLMs. Moreover, we consider a continual training scenario where a pre-trained language model is trained on a different target language. We introduce a simple method to use a new vocabulary instead of the pre-defined one. We show that using the new vocabulary outperforms the model with the vocabulary used in pre-training.",
      "authors": [
        "Sho Takase",
        "Ryokan Ri",
        "Shun Kiyono",
        "Takuya Kato"
      ],
      "published": "2024-06-24T10:27:07Z",
      "updated": "2025-05-28T02:07:07Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.16508v2",
      "landing_url": "https://arxiv.org/abs/2406.16508v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.16508"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item concerns subword vocabulary size for language models, not discrete audio tokens; it does not meet the defined inclusion criteria, so should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item concerns subword vocabulary size for language models, not discrete audio tokens; it does not meet the defined inclusion criteria, so should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper's focus is on vocabulary size effects in large language models related to text tokenization, not on discrete audio tokens derived from continuous audio signals, nor on vector quantization or related audio tokenization methods; therefore, it does not meet the inclusion criteria related to discrete audio token design or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper's focus is on vocabulary size effects in large language models related to text tokenization, not on discrete audio tokens derived from continuous audio signals, nor on vector quantization or related audio tokenization methods; therefore, it does not meet the inclusion criteria related to discrete audio token design or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model",
    "abstract": "We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity.",
    "metadata": {
      "arxiv_id": "2406.17310",
      "title": "High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model",
      "summary": "We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity.",
      "authors": [
        "Joun Yeop Lee",
        "Myeonghun Jeong",
        "Minchan Kim",
        "Ji-Hyun Lee",
        "Hoon-Young Cho",
        "Nam Soo Kim"
      ],
      "published": "2024-06-25T06:46:47Z",
      "updated": "2024-06-25T06:46:47Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17310v1",
      "landing_url": "https://arxiv.org/abs/2406.17310v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.17310"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a discrete-token-based TTS architecture with semantic/acoustic tokens, alignment via a transducer, and a Grouped Masked Language Model, which aligns with the inclusion criteria for discrete audio tokens; however, the source page lacks a clear publication/update date, triggering exclusion under the time-stamp criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a discrete-token-based TTS architecture with semantic/acoustic tokens, alignment via a transducer, and a Grouped Masked Language Model, which aligns with the inclusion criteria for discrete audio tokens; however, the source page lacks a clear publication/update date, triggering exclusion under the time-stamp criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a novel text-to-speech framework using two types of discrete tokens (semantic and acoustic), with clear description of token generation and usage for high-fidelity speech synthesis; it involves token transducer and group masked language model architecture and verifies effectiveness through experiments, meeting inclusion criteria of discrete audio token design as core research contribution with technical details supporting reproducibility and sequence model usage. No exclusion criteria appear to be met.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a novel text-to-speech framework using two types of discrete tokens (semantic and acoustic), with clear description of token generation and usage for high-fidelity speech synthesis; it involves token transducer and group masked language model architecture and verifies effectiveness through experiments, meeting inclusion criteria of discrete audio token design as core research contribution with technical details supporting reproducibility and sequence model usage. No exclusion criteria appear to be met.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted relevant points: JuniorMini finds the paper aligned with the discrete-audio-token topic (semantic + acoustic tokens, token use in sequence modeling, experiments), while JuniorNano flagged lack of verifiable publication date in the provided metadata. The abstract clearly presents discrete token design (semantic vs. acoustic), their generation/usage in a two-stage TTS pipeline, and sequence-model components (transducer, G-MLM), which satisfies the core inclusion criterion that token design is a primary contribution; however, from the title/abstract alone I cannot confirm availability of the full English technical manuscript (detailed quantization/codebook/训练与推論設定) or a clear YYYY-MM-DD publication/update date required by the exclusion rules. Therefore I recommend inclusion but with reservation pending verification of the full paper and a timestamp. (Score explanation: the abstract indicates a substantive discrete-token contribution for audio TTS, but missing verifiable full-text/date prevents an unconditional accept.)",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted relevant points: JuniorMini finds the paper aligned with the discrete-audio-token topic (semantic + acoustic tokens, token use in sequence modeling, experiments), while JuniorNano flagged lack of verifiable publication date in the provided metadata. The abstract clearly presents discrete token design (semantic vs. acoustic), their generation/usage in a two-stage TTS pipeline, and sequence-model components (transducer, G-MLM), which satisfies the core inclusion criterion that token design is a primary contribution; however, from the title/abstract alone I cannot confirm availability of the full English technical manuscript (detailed quantization/codebook/训练与推論設定) or a clear YYYY-MM-DD publication/update date required by the exclusion rules. Therefore I recommend inclusion but with reservation pending verification of the full paper and a timestamp. (Score explanation: the abstract indicates a substantive discrete-token contribution for audio TTS, but missing verifiable full-text/date prevents an unconditional accept.)",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Spatial Voice Conversion: Voice Conversion Preserving Spatial Information and Non-target Signals",
    "abstract": "This paper proposes a new task called spatial voice conversion, which aims to convert a target voice while preserving spatial information and non-target signals. Traditional voice conversion methods focus on single-channel waveforms, ignoring the stereo listening experience inherent in human hearing. Our baseline approach addresses this gap by integrating blind source separation (BSS), voice conversion (VC), and spatial mixing to handle multi-channel waveforms. Through experimental evaluations, we organize and identify the key challenges inherent in this task, such as maintaining audio quality and accurately preserving spatial information. Our results highlight the fundamental difficulties in balancing these aspects, providing a benchmark for future research in spatial voice conversion. The proposed method's code is publicly available to encourage further exploration in this domain.",
    "metadata": {
      "arxiv_id": "2406.17722",
      "title": "Spatial Voice Conversion: Voice Conversion Preserving Spatial Information and Non-target Signals",
      "summary": "This paper proposes a new task called spatial voice conversion, which aims to convert a target voice while preserving spatial information and non-target signals. Traditional voice conversion methods focus on single-channel waveforms, ignoring the stereo listening experience inherent in human hearing. Our baseline approach addresses this gap by integrating blind source separation (BSS), voice conversion (VC), and spatial mixing to handle multi-channel waveforms. Through experimental evaluations, we organize and identify the key challenges inherent in this task, such as maintaining audio quality and accurately preserving spatial information. Our results highlight the fundamental difficulties in balancing these aspects, providing a benchmark for future research in spatial voice conversion. The proposed method's code is publicly available to encourage further exploration in this domain.",
      "authors": [
        "Kentaro Seki",
        "Shinnosuke Takamichi",
        "Norihiro Takamune",
        "Yuki Saito",
        "Kanami Imamura",
        "Hiroshi Saruwatari"
      ],
      "published": "2024-06-25T17:10:39Z",
      "updated": "2024-06-25T17:10:39Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17722v1",
      "landing_url": "https://arxiv.org/abs/2406.17722v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.17722"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item targets spatial voice conversion and multi-channel processing rather than discrete audio tokens or token-based representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item targets spatial voice conversion and multi-channel processing rather than discrete audio tokens or token-based representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on spatial voice conversion using techniques like blind source separation and voice conversion to preserve spatial information, but it does not mention the generation or design of discrete audio tokens, vector quantization, nor does it describe any tokenization or quantization processes to represent audio as discrete tokens for sequence or language modeling. Therefore, it does not meet the core inclusion criteria related to discrete audio token design or usage in sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on spatial voice conversion using techniques like blind source separation and voice conversion to preserve spatial information, but it does not mention the generation or design of discrete audio tokens, vector quantization, nor does it describe any tokenization or quantization processes to represent audio as discrete tokens for sequence or language modeling. Therefore, it does not meet the core inclusion criteria related to discrete audio token design or usage in sequence modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion",
    "abstract": "This paper presents Diffusion Forcing, a new training paradigm where a diffusion model is trained to denoise a set of tokens with independent per-token noise levels. We apply Diffusion Forcing to sequence generative modeling by training a causal next-token prediction model to generate one or several future tokens without fully diffusing past ones. Our approach is shown to combine the strengths of next-token prediction models, such as variable-length generation, with the strengths of full-sequence diffusion models, such as the ability to guide sampling to desirable trajectories. Our method offers a range of additional capabilities, such as (1) rolling-out sequences of continuous tokens, such as video, with lengths past the training horizon, where baselines diverge and (2) new sampling and guiding schemes that uniquely profit from Diffusion Forcing's variable-horizon and causal architecture, and which lead to marked performance gains in decision-making and planning tasks. In addition to its empirical success, our method is proven to optimize a variational lower bound on the likelihoods of all subsequences of tokens drawn from the true joint distribution. Project website: https://boyuan.space/diffusion-forcing",
    "metadata": {
      "arxiv_id": "2407.01392",
      "title": "Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion",
      "summary": "This paper presents Diffusion Forcing, a new training paradigm where a diffusion model is trained to denoise a set of tokens with independent per-token noise levels. We apply Diffusion Forcing to sequence generative modeling by training a causal next-token prediction model to generate one or several future tokens without fully diffusing past ones. Our approach is shown to combine the strengths of next-token prediction models, such as variable-length generation, with the strengths of full-sequence diffusion models, such as the ability to guide sampling to desirable trajectories. Our method offers a range of additional capabilities, such as (1) rolling-out sequences of continuous tokens, such as video, with lengths past the training horizon, where baselines diverge and (2) new sampling and guiding schemes that uniquely profit from Diffusion Forcing's variable-horizon and causal architecture, and which lead to marked performance gains in decision-making and planning tasks. In addition to its empirical success, our method is proven to optimize a variational lower bound on the likelihoods of all subsequences of tokens drawn from the true joint distribution. Project website: https://boyuan.space/diffusion-forcing",
      "authors": [
        "Boyuan Chen",
        "Diego Marti Monso",
        "Yilun Du",
        "Max Simchowitz",
        "Russ Tedrake",
        "Vincent Sitzmann"
      ],
      "published": "2024-07-01T15:43:25Z",
      "updated": "2024-12-10T01:32:23Z",
      "categories": [
        "cs.LG",
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.01392v4",
      "landing_url": "https://arxiv.org/abs/2407.01392v4",
      "doi": "https://doi.org/10.48550/arXiv.2407.01392"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on diffusion forcing for next-token prediction in sequences and does not discuss discrete audio token representations (e.g., VQ/RVQ, hierarchical quantization) or audio-language modeling as required by the inclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on diffusion forcing for next-token prediction in sequences and does not discuss discrete audio token representations (e.g., VQ/RVQ, hierarchical quantization) or audio-language modeling as required by the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a new training paradigm combining diffusion models and next-token prediction for sequence generative modeling but does not mention discrete audio tokens, audio representation quantization, or any related audio language modeling or audio token generation. It is unrelated to the inclusion criteria centered on discrete audio token design or usage. There are no details about token generation for audio nor mention of vector quantization or similar discrete audio token techniques; also, the task is more generic sequence modeling rather than audio-specific.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a new training paradigm combining diffusion models and next-token prediction for sequence generative modeling but does not mention discrete audio tokens, audio representation quantization, or any related audio language modeling or audio token generation. It is unrelated to the inclusion criteria centered on discrete audio token design or usage. There are no details about token generation for audio nor mention of vector quantization or similar discrete audio token techniques; also, the task is more generic sequence modeling rather than audio-specific.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SA-WavLM: Speaker-Aware Self-Supervised Pre-training for Mixture Speech",
    "abstract": "It was shown that pre-trained models with self-supervised learning (SSL) techniques are effective in various downstream speech tasks. However, most such models are trained on single-speaker speech data, limiting their effectiveness in mixture speech. This motivates us to explore pre-training on mixture speech. This work presents SA-WavLM, a novel pre-trained model for mixture speech. Specifically, SA-WavLM follows an \"extract-merge-predict\" pipeline in which the representations of each speaker in the input mixture are first extracted individually and then merged before the final prediction. In this pipeline, SA-WavLM performs speaker-informed extractions with the consideration of the interactions between different speakers. Furthermore, a speaker shuffling strategy is proposed to enhance the robustness towards the speaker absence. Experiments show that SA-WavLM either matches or improves upon the state-of-the-art pre-trained models.",
    "metadata": {
      "arxiv_id": "2407.02826",
      "title": "SA-WavLM: Speaker-Aware Self-Supervised Pre-training for Mixture Speech",
      "summary": "It was shown that pre-trained models with self-supervised learning (SSL) techniques are effective in various downstream speech tasks. However, most such models are trained on single-speaker speech data, limiting their effectiveness in mixture speech. This motivates us to explore pre-training on mixture speech. This work presents SA-WavLM, a novel pre-trained model for mixture speech. Specifically, SA-WavLM follows an \"extract-merge-predict\" pipeline in which the representations of each speaker in the input mixture are first extracted individually and then merged before the final prediction. In this pipeline, SA-WavLM performs speaker-informed extractions with the consideration of the interactions between different speakers. Furthermore, a speaker shuffling strategy is proposed to enhance the robustness towards the speaker absence. Experiments show that SA-WavLM either matches or improves upon the state-of-the-art pre-trained models.",
      "authors": [
        "Jingru Lin",
        "Meng Ge",
        "Junyi Ao",
        "Liqun Deng",
        "Haizhou Li"
      ],
      "published": "2024-07-03T06:07:42Z",
      "updated": "2024-07-03T06:07:42Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.02826v1",
      "landing_url": "https://arxiv.org/abs/2407.02826v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.02826"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on speaker-aware self-supervised pre-training for mixture speech and does not present discrete audio tokenization or token-based representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on speaker-aware self-supervised pre-training for mixture speech and does not present discrete audio tokenization or token-based representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes a self-supervised pre-training model focused on processing mixture speech for improved speaker-aware representations, but it does not involve the design or generation of discrete audio tokens through quantization or similar techniques, nor does it focus on token usage for sequence or language modeling, which are the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract describes a self-supervised pre-training model focused on processing mixture speech for improved speaker-aware representations, but it does not involve the design or generation of discrete audio tokens through quantization or similar techniques, nor does it focus on token usage for sequence or language modeling, which are the core inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "On the Effectiveness of Acoustic BPE in Decoder-Only TTS",
    "abstract": "Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS.",
    "metadata": {
      "arxiv_id": "2407.03892",
      "title": "On the Effectiveness of Acoustic BPE in Decoder-Only TTS",
      "summary": "Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS.",
      "authors": [
        "Bohan Li",
        "Feiyu Shen",
        "Yiwei Guo",
        "Shuai Wang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-07-04T12:35:32Z",
      "updated": "2024-07-04T12:35:32Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.03892v1",
      "landing_url": "https://arxiv.org/abs/2407.03892v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.03892"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item clearly targets Discrete Audio Tokens by using acoustic BPE to discretize speech tokens for decoder-only TTS, aligning with the inclusion criteria. However, from the abstract alone we cannot confirm (a) availability of an English full text with detailed, reproducible methodological descriptions (token generation, quantization, model setup) and (b) a clear publication/update date on the source page, which are part of the stated criteria for inclusion. Given these uncertainties, inclusion cannot be confirmed from the provided information alone.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The item clearly targets Discrete Audio Tokens by using acoustic BPE to discretize speech tokens for decoder-only TTS, aligning with the inclusion criteria. However, from the abstract alone we cannot confirm (a) availability of an English full text with detailed, reproducible methodological descriptions (token generation, quantization, model setup) and (b) a clear publication/update date on the source page, which are part of the stated criteria for inclusion. Given these uncertainties, inclusion cannot be confirmed from the provided information alone.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates acoustic byte-pair encoding (BPE) for discretizing speech into tokens and its application in decoder-only TTS models, clearly describing token generation and usage in sequence modeling, aligning well with the inclusion criteria about discrete audio token design and usage with proper method details and experimentation. There is no indication of exclusion reasons such as lack of method detail, traditional codec focus, or missing publication date.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study investigates acoustic byte-pair encoding (BPE) for discretizing speech into tokens and its application in decoder-only TTS models, clearly describing token generation and usage in sequence modeling, aligning well with the inclusion criteria about discrete audio token design and usage with proper method details and experimentation. There is no indication of exclusion reasons such as lack of method detail, traditional codec focus, or missing publication date.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers were consulted: JuniorMini judged it clearly fits (5) noting it studies acoustic BPE on semantic speech tokens with experiments, while JuniorNano was cautious (3) because the abstract alone does not confirm availability of an English full text or a clear publication date. The abstract, however, explicitly treats speech discretization (semantic speech tokens) and acoustic BPE as its core method and reports reproducible-seeming experiments on LibriTTS assessing intelligibility and diversity, which aligns strongly with the Discrete Audio Tokens inclusion criteria (token design/usage, SSL-derived semantic tokens, sequence-modeling for TTS). The only remaining uncertainties are metadata availability (full paper and explicit YYYY-MM-DD), which are exclusion criteria if missing from the source; these are not evidenced by the abstract itself. Thus I recommend inclusion pending verification of the full-text and publication date (i.e., it better be checked before final dataset entry).",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers were consulted: JuniorMini judged it clearly fits (5) noting it studies acoustic BPE on semantic speech tokens with experiments, while JuniorNano was cautious (3) because the abstract alone does not confirm availability of an English full text or a clear publication date. The abstract, however, explicitly treats speech discretization (semantic speech tokens) and acoustic BPE as its core method and reports reproducible-seeming experiments on LibriTTS assessing intelligibility and diversity, which aligns strongly with the Discrete Audio Tokens inclusion criteria (token design/usage, SSL-derived semantic tokens, sequence-modeling for TTS). The only remaining uncertainties are metadata availability (full paper and explicit YYYY-MM-DD), which are exclusion criteria if missing from the source; these are not evidenced by the abstract itself. Thus I recommend inclusion pending verification of the full-text and publication date (i.e., it better be checked before final dataset entry).",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens",
    "abstract": "Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.",
    "metadata": {
      "arxiv_id": "2407.05407",
      "title": "CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens",
      "summary": "Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.",
      "authors": [
        "Zhihao Du",
        "Qian Chen",
        "Shiliang Zhang",
        "Kai Hu",
        "Heng Lu",
        "Yexin Yang",
        "Hangrui Hu",
        "Siqi Zheng",
        "Yue Gu",
        "Ziyang Ma",
        "Zhifu Gao",
        "Zhijie Yan"
      ],
      "published": "2024-07-07T15:16:19Z",
      "updated": "2024-07-09T07:42:51Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.05407v2",
      "landing_url": "https://arxiv.org/abs/2407.05407v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.05407"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the work targets Discrete Audio Tokens via supervised semantic tokens for TTS and details token generation/quantization, the source lacks a publication/update date, failing the timestamp traceability exclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although the work targets Discrete Audio Tokens via supervised semantic tokens for TTS and details token generation/quantization, the source lacks a publication/update date, failing the timestamp traceability exclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a text-to-speech synthesizer based on supervised semantic tokens derived from a multilingual speech recognition model using vector quantization, clearly involving a discrete audio token generation process. The tokens are used as intermediate representations for zero-shot TTS, which aligns with the inclusion criteria of discrete audio tokens used for sequence modeling and generation. The methodology and experimental results are described to support reproducibility and comparison, and the study focuses on semantic token design and use for speech synthesis, not on traditional codecs or non-reviewable demo content.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a text-to-speech synthesizer based on supervised semantic tokens derived from a multilingual speech recognition model using vector quantization, clearly involving a discrete audio token generation process. The tokens are used as intermediate representations for zero-shot TTS, which aligns with the inclusion criteria of discrete audio tokens used for sequence modeling and generation. The methodology and experimental results are described to support reproducibility and comparison, and the study focuses on semantic token design and use for speech synthesis, not on traditional codecs or non-reviewable demo content.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers' inputs considered: JuniorMini correctly notes the paper presents supervised semantic tokens derived via vector quantization from a multilingual ASR encoder and uses them as discrete tokens in an LLM-based zero-shot TTS pipeline with experiments—this matches the discrete audio token inclusion criteria (VQ-based token design, used for sequence modeling/generation, and empirical evaluation). JuniorNano flagged a missing publication date, but that is a metadata issue not evident in the abstract; based on the technical content alone the work meets all inclusion criteria and does not appear to fall under the exclusions (not a demo/blog or a traditional codec-only paper).",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers' inputs considered: JuniorMini correctly notes the paper presents supervised semantic tokens derived via vector quantization from a multilingual ASR encoder and uses them as discrete tokens in an LLM-based zero-shot TTS pipeline with experiments—this matches the discrete audio token inclusion criteria (VQ-based token design, used for sequence modeling/generation, and empirical evaluation). JuniorNano flagged a missing publication date, but that is a metadata issue not evident in the abstract; based on the technical content alone the work meets all inclusion criteria and does not appear to fall under the exclusions (not a demo/blog or a traditional codec-only paper).",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Privacy-Preserving Data Deduplication for Enhancing Federated Learning of Language Models (Extended Version)",
    "abstract": "Deduplication is a vital preprocessing step that enhances machine learning model performance and saves training time and energy. However, enhancing federated learning through deduplication poses challenges, especially regarding scalability and potential privacy violations if deduplication involves sharing all clients' data. In this paper, we address the problem of deduplication in a federated setup by introducing a pioneering protocol, Efficient Privacy-Preserving Multi-Party Deduplication (EP-MPD). It efficiently removes duplicates from multiple clients' datasets without compromising data privacy. EP-MPD is constructed in a modular fashion, utilizing two novel variants of the Private Set Intersection protocol. Our extensive experiments demonstrate the significant benefits of deduplication in federated learning of large language models. For instance, we observe up to 19.62\\% improvement in perplexity and up to 27.95\\% reduction in running time while varying the duplication level between 10\\% and 30\\%. EP-MPD effectively balances privacy and performance in federated learning, making it a valuable solution for large-scale applications.",
    "metadata": {
      "arxiv_id": "2407.08152",
      "title": "Privacy-Preserving Data Deduplication for Enhancing Federated Learning of Language Models (Extended Version)",
      "summary": "Deduplication is a vital preprocessing step that enhances machine learning model performance and saves training time and energy. However, enhancing federated learning through deduplication poses challenges, especially regarding scalability and potential privacy violations if deduplication involves sharing all clients' data. In this paper, we address the problem of deduplication in a federated setup by introducing a pioneering protocol, Efficient Privacy-Preserving Multi-Party Deduplication (EP-MPD). It efficiently removes duplicates from multiple clients' datasets without compromising data privacy. EP-MPD is constructed in a modular fashion, utilizing two novel variants of the Private Set Intersection protocol. Our extensive experiments demonstrate the significant benefits of deduplication in federated learning of large language models. For instance, we observe up to 19.62\\% improvement in perplexity and up to 27.95\\% reduction in running time while varying the duplication level between 10\\% and 30\\%. EP-MPD effectively balances privacy and performance in federated learning, making it a valuable solution for large-scale applications.",
      "authors": [
        "Aydin Abadi",
        "Vishnu Asutosh Dasu",
        "Sumanta Sarkar"
      ],
      "published": "2024-07-11T03:10:27Z",
      "updated": "2024-12-04T17:56:57Z",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08152v2",
      "landing_url": "https://arxiv.org/abs/2407.08152v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.08152"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on privacy-preserving deduplication in federated learning of language models and does not involve discrete audio tokens or audio token design; thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on privacy-preserving deduplication in federated learning of language models and does not involve discrete audio tokens or audio token design; thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on privacy-preserving data deduplication protocols within federated learning for language models, and does not address the generation or use of discrete audio tokens derived from vector quantization or related mechanisms for audio representations. It lacks discussion about discrete token design for audio signals, their generation process, or their use in sequence/language modeling of audio data, which are the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on privacy-preserving data deduplication protocols within federated learning for language models, and does not address the generation or use of discrete audio tokens derived from vector quantization or related mechanisms for audio representations. It lacks discussion about discrete token design for audio signals, their generation process, or their use in sequence/language modeling of audio data, which are the core inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies",
    "abstract": "Research on scaling large language models (LLMs) has primarily focused on model parameters and training data size, overlooking the role of vocabulary size. We investigate how vocabulary size impacts LLM scaling laws by training models ranging from 33M to 3B parameters on up to 500B characters with various vocabulary configurations. We propose three complementary approaches for predicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative estimation, and parametric fit of the loss function. Our approaches converge on the conclusion that the optimal vocabulary size depends on the compute budget, with larger models requiring larger vocabularies. Most LLMs, however, use insufficient vocabulary sizes. For example, we predict that the optimal vocabulary size of Llama2-70B should have been at least 216K, 7 times larger than its vocabulary of 32K. We validate our predictions empirically by training models with 3B parameters across different FLOPs budgets. Adopting our predicted optimal vocabulary size consistently improves downstream performance over commonly used vocabulary sizes. By increasing the vocabulary size from the conventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to 32.0 with the same 2.3e21 FLOPs. Our work highlights the importance of jointly considering tokenization and model scaling for efficient pre-training. The code and demo are available at https://github.com/sail-sg/scaling-with-vocab and https://hf.co/spaces/sail/scaling-with-vocab-demo.",
    "metadata": {
      "arxiv_id": "2407.13623",
      "title": "Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies",
      "summary": "Research on scaling large language models (LLMs) has primarily focused on model parameters and training data size, overlooking the role of vocabulary size. We investigate how vocabulary size impacts LLM scaling laws by training models ranging from 33M to 3B parameters on up to 500B characters with various vocabulary configurations. We propose three complementary approaches for predicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative estimation, and parametric fit of the loss function. Our approaches converge on the conclusion that the optimal vocabulary size depends on the compute budget, with larger models requiring larger vocabularies. Most LLMs, however, use insufficient vocabulary sizes. For example, we predict that the optimal vocabulary size of Llama2-70B should have been at least 216K, 7 times larger than its vocabulary of 32K. We validate our predictions empirically by training models with 3B parameters across different FLOPs budgets. Adopting our predicted optimal vocabulary size consistently improves downstream performance over commonly used vocabulary sizes. By increasing the vocabulary size from the conventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to 32.0 with the same 2.3e21 FLOPs. Our work highlights the importance of jointly considering tokenization and model scaling for efficient pre-training. The code and demo are available at https://github.com/sail-sg/scaling-with-vocab and https://hf.co/spaces/sail/scaling-with-vocab-demo.",
      "authors": [
        "Chaofan Tao",
        "Qian Liu",
        "Longxu Dou",
        "Niklas Muennighoff",
        "Zhongwei Wan",
        "Ping Luo",
        "Min Lin",
        "Ngai Wong"
      ],
      "published": "2024-07-18T15:58:54Z",
      "updated": "2024-11-01T02:41:36Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.13623v3",
      "landing_url": "https://arxiv.org/abs/2407.13623v3",
      "doi": "https://doi.org/10.48550/arXiv.2407.13623"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This title/abstract addresses scaling of vocabulary size for large language models but does not discuss discrete audio tokens, tokenization methods, or audio-language modeling, so it does not meet the discrete audio token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This title/abstract addresses scaling of vocabulary size for large language models but does not discuss discrete audio tokens, tokenization methods, or audio-language modeling, so it does not meet the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on scaling laws of large language models with respect to vocabulary size, primarily for text token vocabularies, and does not address discrete audio tokens, audio representation quantization or any audio-related modeling. There is no mention of audio signal processing, vector quantization of audio, or audio token generation as defined in the inclusion criteria. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on scaling laws of large language models with respect to vocabulary size, primarily for text token vocabularies, and does not address discrete audio tokens, audio representation quantization or any audio-related modeling. There is no mention of audio signal processing, vector quantization of audio, or audio token generation as defined in the inclusion criteria. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Discrete Flow Matching",
    "abstract": "Despite Flow Matching and diffusion models having emerged as powerful generative paradigms for continuous variables such as images and videos, their application to high-dimensional discrete data, such as language, is still limited. In this work, we present Discrete Flow Matching, a novel discrete flow paradigm designed specifically for generating discrete data. Discrete Flow Matching offers several key contributions:(i) it works with a general family of probability paths interpolating between source and target distributions; (ii) it allows for a generic formula for sampling from these probability paths using learned posteriors such as the probability denoiser ($x$-prediction) and noise-prediction ($ε$-prediction); (iii) practically, focusing on specific probability paths defined with different schedulers improves generative perplexity compared to previous discrete diffusion and flow models; and (iv) by scaling Discrete Flow Matching models up to 1.7B parameters, we reach 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1 and 20.6% Pass@10 on 1-shot MBPP coding benchmarks. Our approach is capable of generating high-quality discrete data in a non-autoregressive fashion, significantly closing the gap between autoregressive models and discrete flow models.",
    "metadata": {
      "arxiv_id": "2407.15595",
      "title": "Discrete Flow Matching",
      "summary": "Despite Flow Matching and diffusion models having emerged as powerful generative paradigms for continuous variables such as images and videos, their application to high-dimensional discrete data, such as language, is still limited. In this work, we present Discrete Flow Matching, a novel discrete flow paradigm designed specifically for generating discrete data. Discrete Flow Matching offers several key contributions:(i) it works with a general family of probability paths interpolating between source and target distributions; (ii) it allows for a generic formula for sampling from these probability paths using learned posteriors such as the probability denoiser ($x$-prediction) and noise-prediction ($ε$-prediction); (iii) practically, focusing on specific probability paths defined with different schedulers improves generative perplexity compared to previous discrete diffusion and flow models; and (iv) by scaling Discrete Flow Matching models up to 1.7B parameters, we reach 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1 and 20.6% Pass@10 on 1-shot MBPP coding benchmarks. Our approach is capable of generating high-quality discrete data in a non-autoregressive fashion, significantly closing the gap between autoregressive models and discrete flow models.",
      "authors": [
        "Itai Gat",
        "Tal Remez",
        "Neta Shaul",
        "Felix Kreuk",
        "Ricky T. Q. Chen",
        "Gabriel Synnaeve",
        "Yossi Adi",
        "Yaron Lipman"
      ],
      "published": "2024-07-22T12:33:27Z",
      "updated": "2024-11-05T10:02:42Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.15595v2",
      "landing_url": "https://arxiv.org/abs/2407.15595v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.15595"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract covers discrete data generation for language but does not address discrete audio token design/ quantization (VQ/RVQ, hierarchical tokenization, audio-language modeling) that the inclusion criteria require, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract covers discrete data generation for language but does not address discrete audio token design/ quantization (VQ/RVQ, hierarchical tokenization, audio-language modeling) that the inclusion criteria require, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on discrete flow methods for generating high-dimensional discrete data, primarily in the context of language and code generation benchmarks. It does not discuss discrete audio tokens, audio signal processing, vector quantization for audio, or any audio-related tokenization mechanisms as required by the inclusion criteria. The core contribution is a novel generative model for discrete data, but it is language/code centric and lacks any detail or application related to discrete audio token representations or their generation processes. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on discrete flow methods for generating high-dimensional discrete data, primarily in the context of language and code generation benchmarks. It does not discuss discrete audio tokens, audio signal processing, vector quantization for audio, or any audio-related tokenization mechanisms as required by the inclusion criteria. The core contribution is a novel generative model for discrete data, but it is language/code centric and lacks any detail or application related to discrete audio token representations or their generation processes. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Experimenting with Adaptive Bitrate Algorithms for Virtual Reality Streaming over Wi-Fi",
    "abstract": "Interactive Virtual Reality (VR) streaming over Wi-Fi networks encounters significant challenges due to bandwidth fluctuations caused by channel contention and user mobility. Adaptive BitRate (ABR) algorithms dynamically adjust the video encoding bitrate based on the available network capacity, aiming to maximize image quality while mitigating congestion and preserving the user's Quality of Experience (QoE). In this paper, we experiment with ABR algorithms for VR streaming using Air Light VR (ALVR), an open-source VR streaming solution. We extend ALVR with a comprehensive set of metrics that provide a robust characterization of the network's state, enabling more informed bitrate adjustments. To demonstrate the utility of these performance indicators, we develop and test the Network-aware Step-wise ABR algorithm for VR streaming (NeSt-VR). Results validate the accuracy of the newly implemented network performance metrics and demonstrate NeSt-VR's video bitrate adaptation capabilities.",
    "metadata": {
      "arxiv_id": "2407.15614",
      "title": "Experimenting with Adaptive Bitrate Algorithms for Virtual Reality Streaming over Wi-Fi",
      "summary": "Interactive Virtual Reality (VR) streaming over Wi-Fi networks encounters significant challenges due to bandwidth fluctuations caused by channel contention and user mobility. Adaptive BitRate (ABR) algorithms dynamically adjust the video encoding bitrate based on the available network capacity, aiming to maximize image quality while mitigating congestion and preserving the user's Quality of Experience (QoE). In this paper, we experiment with ABR algorithms for VR streaming using Air Light VR (ALVR), an open-source VR streaming solution. We extend ALVR with a comprehensive set of metrics that provide a robust characterization of the network's state, enabling more informed bitrate adjustments. To demonstrate the utility of these performance indicators, we develop and test the Network-aware Step-wise ABR algorithm for VR streaming (NeSt-VR). Results validate the accuracy of the newly implemented network performance metrics and demonstrate NeSt-VR's video bitrate adaptation capabilities.",
      "authors": [
        "Ferran Maura",
        "Miguel Casasnovas",
        "Boris Bellalta"
      ],
      "published": "2024-07-22T13:20:47Z",
      "updated": "2024-09-30T10:37:16Z",
      "categories": [
        "cs.NI",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.15614v3",
      "landing_url": "https://arxiv.org/abs/2407.15614v3",
      "doi": "https://doi.org/10.48550/arXiv.2407.15614"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title and abstract focus on adaptive bitrate algorithms for VR streaming and network QoE, not on discrete audio token design or token-based representations; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title and abstract focus on adaptive bitrate algorithms for VR streaming and network QoE, not on discrete audio token design or token-based representations; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on adaptive bitrate algorithms for VR streaming over Wi-Fi, without mentioning discrete audio tokens, neural audio codecs, vector quantization, or any tokenization relevant to audio language modeling or generation as required by the inclusion criteria. Moreover, the study does not detail token generation processes or emphasize discrete token representations for downstream audio understanding or generation tasks, thus not meeting any inclusion criterion and not violating exclusion criteria explicitly but clearly outside the scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on adaptive bitrate algorithms for VR streaming over Wi-Fi, without mentioning discrete audio tokens, neural audio codecs, vector quantization, or any tokenization relevant to audio language modeling or generation as required by the inclusion criteria. Moreover, the study does not detail token generation processes or emphasize discrete token representations for downstream audio understanding or generation tasks, thus not meeting any inclusion criterion and not violating exclusion criteria explicitly but clearly outside the scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Generating Sample-Based Musical Instruments Using Neural Audio Codec Language Models",
    "abstract": "In this paper, we propose and investigate the use of neural audio codec language models for the automatic generation of sample-based musical instruments based on text or reference audio prompts. Our approach extends a generative audio framework to condition on pitch across an 88-key spectrum, velocity, and a combined text/audio embedding. We identify maintaining timbral consistency within the generated instruments as a major challenge. To tackle this issue, we introduce three distinct conditioning schemes. We analyze our methods through objective metrics and human listening tests, demonstrating that our approach can produce compelling musical instruments. Specifically, we introduce a new objective metric to evaluate the timbral consistency of the generated instruments and adapt the average Contrastive Language-Audio Pretraining (CLAP) score for the text-to-instrument case, noting that its naive application is unsuitable for assessing this task. Our findings reveal a complex interplay between timbral consistency, the quality of generated samples, and their correspondence to the input prompt.",
    "metadata": {
      "arxiv_id": "2407.15641",
      "title": "Generating Sample-Based Musical Instruments Using Neural Audio Codec Language Models",
      "summary": "In this paper, we propose and investigate the use of neural audio codec language models for the automatic generation of sample-based musical instruments based on text or reference audio prompts. Our approach extends a generative audio framework to condition on pitch across an 88-key spectrum, velocity, and a combined text/audio embedding. We identify maintaining timbral consistency within the generated instruments as a major challenge. To tackle this issue, we introduce three distinct conditioning schemes. We analyze our methods through objective metrics and human listening tests, demonstrating that our approach can produce compelling musical instruments. Specifically, we introduce a new objective metric to evaluate the timbral consistency of the generated instruments and adapt the average Contrastive Language-Audio Pretraining (CLAP) score for the text-to-instrument case, noting that its naive application is unsuitable for assessing this task. Our findings reveal a complex interplay between timbral consistency, the quality of generated samples, and their correspondence to the input prompt.",
      "authors": [
        "Shahan Nercessian",
        "Johannes Imort",
        "Ninon Devis",
        "Frederik Blang"
      ],
      "published": "2024-07-22T13:59:58Z",
      "updated": "2024-07-22T13:59:58Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.15641v1",
      "landing_url": "https://arxiv.org/abs/2407.15641v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.15641"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on neural audio codec language models for generating sample-based instruments and conditioning on pitch/velocity/text, but it does not explicitly present discrete audio tokens or a tokenization/quantization design as the main contribution, which is required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item focuses on neural audio codec language models for generating sample-based instruments and conditioning on pitch/velocity/text, but it does not explicitly present discrete audio tokens or a tokenization/quantization design as the main contribution, which is required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on generating musical instruments using neural audio codec language models. However, the abstract does not clearly specify the use or design of discrete audio tokens, nor does it detail the token generation process, quantization methods, or codec structures relevant to the inclusion criteria. The methodology emphasizes conditioning on pitch and timbral consistency but lacks explicit mention of discrete token representations or their use in sequence modeling as the core contribution. Therefore, the work appears not to meet the strict inclusion requirements regarding discrete audio token design and methodological clarity.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on generating musical instruments using neural audio codec language models. However, the abstract does not clearly specify the use or design of discrete audio tokens, nor does it detail the token generation process, quantization methods, or codec structures relevant to the inclusion criteria. The methodology emphasizes conditioning on pitch and timbral consistency but lacks explicit mention of discrete token representations or their use in sequence modeling as the core contribution. Therefore, the work appears not to meet the strict inclusion requirements regarding discrete audio token design and methodological clarity.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "dMel: Speech Tokenization made Simple",
    "abstract": "Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text.",
    "metadata": {
      "arxiv_id": "2407.15835",
      "title": "dMel: Speech Tokenization made Simple",
      "summary": "Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text.",
      "authors": [
        "Richard He Bai",
        "Tatiana Likhomanenko",
        "Ruixiang Zhang",
        "Zijin Gu",
        "Zakaria Aldeneh",
        "Navdeep Jaitly"
      ],
      "published": "2024-07-22T17:51:53Z",
      "updated": "2025-05-21T16:55:34Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.15835v3",
      "landing_url": "https://arxiv.org/abs/2407.15835v3",
      "doi": "https://doi.org/10.48550/arXiv.2407.15835"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work introduces a discrete speech representation (dMel) by discretizing mel-filterbank channels into intensity bins and uses a transformer-based encoding/decoding pipeline for tokenized speech, covering discrete audio tokens and actionable methodology for synthesis and recognition.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work introduces a discrete speech representation (dMel) by discretizing mel-filterbank channels into intensity bins and uses a transformer-based encoding/decoding pipeline for tokenized speech, covering discrete audio tokens and actionable methodology for synthesis and recognition.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces 'dMel,' a novel speech tokenization method that discretizes continuous speech into discrete tokens for use with language models, demonstrating superior performance and robustness; it clearly details token generation and usage for speech synthesis and recognition, aligning well with the inclusion criteria requiring discrete audio token design as a core contribution, detailed methodology, and applicability to language/sequential modeling, without violating any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study introduces 'dMel,' a novel speech tokenization method that discretizes continuous speech into discrete tokens for use with language models, demonstrating superior performance and robustness; it clearly details token generation and usage for speech synthesis and recognition, aligning well with the inclusion criteria requiring discrete audio token design as a core contribution, detailed methodology, and applicability to language/sequential modeling, without violating any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "VoxSim: A perceptual voice similarity dataset",
    "abstract": "This paper introduces VoxSim, a dataset of perceptual voice similarity ratings. Recent efforts to automate the assessment of speech synthesis technologies have primarily focused on predicting mean opinion score of naturalness, leaving speaker voice similarity relatively unexplored due to a lack of extensive training data. To address this, we generate about 41k utterance pairs from the VoxCeleb dataset, a widely utilised speech dataset for speaker recognition, and collect nearly 70k speaker similarity scores through a listening test. VoxSim offers a valuable resource for the development and benchmarking of speaker similarity prediction models. We provide baseline results of speaker similarity prediction models on the VoxSim test set and further demonstrate that the model trained on our dataset generalises to the out-of-domain VCC2018 dataset.",
    "metadata": {
      "arxiv_id": "2407.18505",
      "title": "VoxSim: A perceptual voice similarity dataset",
      "summary": "This paper introduces VoxSim, a dataset of perceptual voice similarity ratings. Recent efforts to automate the assessment of speech synthesis technologies have primarily focused on predicting mean opinion score of naturalness, leaving speaker voice similarity relatively unexplored due to a lack of extensive training data. To address this, we generate about 41k utterance pairs from the VoxCeleb dataset, a widely utilised speech dataset for speaker recognition, and collect nearly 70k speaker similarity scores through a listening test. VoxSim offers a valuable resource for the development and benchmarking of speaker similarity prediction models. We provide baseline results of speaker similarity prediction models on the VoxSim test set and further demonstrate that the model trained on our dataset generalises to the out-of-domain VCC2018 dataset.",
      "authors": [
        "Junseok Ahn",
        "Youkyum Kim",
        "Yeunju Choi",
        "Doyeop Kwak",
        "Ji-Hoon Kim",
        "Seongkyu Mun",
        "Joon Son Chung"
      ],
      "published": "2024-07-26T04:27:13Z",
      "updated": "2024-07-26T04:27:13Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.18505v1",
      "landing_url": "https://arxiv.org/abs/2407.18505v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.18505"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes a dataset for perceptual voice similarity and does not present discrete audio tokenization methods, vector-quantized representations, or semantic-disentangled token designs; it lacks the required methodology on token generation/quantization to meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes a dataset for perceptual voice similarity and does not present discrete audio tokenization methods, vector-quantized representations, or semantic-disentangled token designs; it lacks the required methodology on token generation/quantization to meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents VoxSim, a dataset for perceptual voice similarity ratings, focusing on speaker similarity evaluation using listening tests and speaker similarity prediction models. However, it does not describe any discrete audio token design or generation process, such as vector quantization or related mechanisms to produce discrete tokens for audio language modeling or sequence modeling. The core contribution is a perceptual dataset rather than a technical method for discrete audio token generation or usage fitting the inclusion criteria. Therefore, it does not meet the inclusion criteria related to discrete token design and usage and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents VoxSim, a dataset for perceptual voice similarity ratings, focusing on speaker similarity evaluation using listening tests and speaker similarity prediction models. However, it does not describe any discrete audio token design or generation process, such as vector quantization or related mechanisms to produce discrete tokens for audio language modeling or sequence modeling. The core contribution is a perceptual dataset rather than a technical method for discrete audio token generation or usage fitting the inclusion criteria. Therefore, it does not meet the inclusion criteria related to discrete token design and usage and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Constructing Per-Shot Bitrate Ladders using Visual Information Fidelity",
    "abstract": "Video service providers need their delivery systems to be able to adapt to network conditions, user preferences, display settings, and other factors. HTTP Adaptive Streaming (HAS) offers dynamic switching between different video representations to simultaneously enhance bandwidth consumption and users' streaming experiences. Per-shot encoding, pioneered by Netflix, optimizes the encoding parameters on each scene or shot. The Dynamic Optimizer (DO) uses the Video Multi-Method Assessment Fusion (VMAF) perceptual video quality prediction engine to deliver high-quality videos at reduced bitrates. Here we develop a perceptually optimized method of constructing optimal per-shot bitrate and quality ladders, using an ensemble of low-level features and Visual Information Fidelity (VIF) features. During inference, our method predicts the bitrate or quality ladder of a source video without any compression or quality estimation. We compare the performance of our model against other content-adaptive bitrate ladder prediction methods, a fixed bitrate ladder, and reference bitrate ladders constructed via exhaustive encoding using Bjontegaard-delta (BD) metrics. Our proposed method shows excellent gains in bitrate and quality against the fixed bitrate ladder and only small losses against the reference bitrate ladder, while providing significant computational advantages.",
    "metadata": {
      "arxiv_id": "2408.01932",
      "title": "Constructing Per-Shot Bitrate Ladders using Visual Information Fidelity",
      "summary": "Video service providers need their delivery systems to be able to adapt to network conditions, user preferences, display settings, and other factors. HTTP Adaptive Streaming (HAS) offers dynamic switching between different video representations to simultaneously enhance bandwidth consumption and users' streaming experiences. Per-shot encoding, pioneered by Netflix, optimizes the encoding parameters on each scene or shot. The Dynamic Optimizer (DO) uses the Video Multi-Method Assessment Fusion (VMAF) perceptual video quality prediction engine to deliver high-quality videos at reduced bitrates. Here we develop a perceptually optimized method of constructing optimal per-shot bitrate and quality ladders, using an ensemble of low-level features and Visual Information Fidelity (VIF) features. During inference, our method predicts the bitrate or quality ladder of a source video without any compression or quality estimation. We compare the performance of our model against other content-adaptive bitrate ladder prediction methods, a fixed bitrate ladder, and reference bitrate ladders constructed via exhaustive encoding using Bjontegaard-delta (BD) metrics. Our proposed method shows excellent gains in bitrate and quality against the fixed bitrate ladder and only small losses against the reference bitrate ladder, while providing significant computational advantages.",
      "authors": [
        "Krishna Srikar Durbha",
        "Alan C. Bovik"
      ],
      "published": "2024-08-04T05:12:21Z",
      "updated": "2025-11-15T01:13:58Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.01932v2",
      "landing_url": "https://arxiv.org/abs/2408.01932v2",
      "doi": "https://doi.org/10.1109/TIP.2025.3625750"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on per-shot bitrate ladders and perceptual quality metrics for video encoding, not on discrete audio tokens or token-based representations for audio models, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on per-shot bitrate ladders and perceptual quality metrics for video encoding, not on discrete audio tokens or token-based representations for audio models, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on constructing per-shot bitrate ladders using visual information fidelity for video streaming optimization. It does not discuss discrete audio tokens, their generation, or usage for audio language modeling, which are the core inclusion criteria. The content is about video quality and bitrate optimization, not about audio tokenization or representation relevant to sequence or language models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on constructing per-shot bitrate ladders using visual information fidelity for video streaming optimization. It does not discuss discrete audio tokens, their generation, or usage for audio language modeling, which are the core inclusion criteria. The content is about video quality and bitrate optimization, not about audio tokenization or representation relevant to sequence or language models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Feedback Design with VQ-VAE for Robust Precoding in Multi-User FDD Systems",
    "abstract": "In this letter, we propose a vector quantized-variational autoencoder (VQ-VAE)-based feedback scheme for robust precoder design in multi-user frequency division duplex (FDD) systems. We demonstrate how the VQ-VAE can be tailored to specific propagation environments, focusing on systems with low pilot overhead, which is crucial in massive multiple-input multiple-output (MIMO). Extensive simulations with real-world measurement data show that our proposed feedback scheme outperforms state-of-the-art autoencoder (AE)-based compression schemes and conventional Discrete Fourier transform (DFT) codebook-based schemes. These improvements enable the deployment of systems with fewer feedback bits or pilots.",
    "metadata": {
      "arxiv_id": "2408.04505",
      "title": "Feedback Design with VQ-VAE for Robust Precoding in Multi-User FDD Systems",
      "summary": "In this letter, we propose a vector quantized-variational autoencoder (VQ-VAE)-based feedback scheme for robust precoder design in multi-user frequency division duplex (FDD) systems. We demonstrate how the VQ-VAE can be tailored to specific propagation environments, focusing on systems with low pilot overhead, which is crucial in massive multiple-input multiple-output (MIMO). Extensive simulations with real-world measurement data show that our proposed feedback scheme outperforms state-of-the-art autoencoder (AE)-based compression schemes and conventional Discrete Fourier transform (DFT) codebook-based schemes. These improvements enable the deployment of systems with fewer feedback bits or pilots.",
      "authors": [
        "Nurettin Turan",
        "Michael Baur",
        "Jianqing Li",
        "Wolfgang Utschick"
      ],
      "published": "2024-08-08T15:03:45Z",
      "updated": "2024-08-08T15:03:45Z",
      "categories": [
        "cs.IT",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.04505v1",
      "landing_url": "https://arxiv.org/abs/2408.04505v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.04505"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses VQ-VAE-based feedback for robust precoding in wireless MIMO/FDD systems, which does not pertain to discrete audio tokens or audio token generation/quantization for language-model style processing.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses VQ-VAE-based feedback for robust precoding in wireless MIMO/FDD systems, which does not pertain to discrete audio tokens or audio token generation/quantization for language-model style processing.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a vector quantized-variational autoencoder (VQ-VAE) based feedback scheme for precoding in multi-user FDD systems, targeting communications system performance enhancement. Although it involves vector quantization, it does not address discrete audio tokens or the transformation of continuous audio signals into discrete tokens for language or sequence modeling, nor does it describe token generation for audio representation or understanding tasks. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and their use in audio language modeling or related tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a vector quantized-variational autoencoder (VQ-VAE) based feedback scheme for precoding in multi-user FDD systems, targeting communications system performance enhancement. Although it involves vector quantization, it does not address discrete audio tokens or the transformation of continuous audio signals into discrete tokens for language or sequence modeling, nor does it describe token generation for audio representation or understanding tasks. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and their use in audio language modeling or related tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "WavLM model ensemble for audio deepfake detection",
    "abstract": "Audio deepfake detection has become a pivotal task over the last couple of years, as many recent speech synthesis and voice cloning systems generate highly realistic speech samples, thus enabling their use in malicious activities. In this paper we address the issue of audio deepfake detection as it was set in the ASVspoof5 challenge. First, we benchmark ten types of pretrained representations and show that the self-supervised representations stemming from the wav2vec2 and wavLM families perform best. Of the two, wavLM is better when restricting the pretraining data to LibriSpeech, as required by the challenge rules. To further improve performance, we finetune the wavLM model for the deepfake detection task. We extend the ASVspoof5 dataset with samples from other deepfake detection datasets and apply data augmentation. Our final challenge submission consists of a late fusion combination of four models and achieves an equal error rate of 6.56% and 17.08% on the two evaluation sets.",
    "metadata": {
      "arxiv_id": "2408.07414",
      "title": "WavLM model ensemble for audio deepfake detection",
      "summary": "Audio deepfake detection has become a pivotal task over the last couple of years, as many recent speech synthesis and voice cloning systems generate highly realistic speech samples, thus enabling their use in malicious activities. In this paper we address the issue of audio deepfake detection as it was set in the ASVspoof5 challenge. First, we benchmark ten types of pretrained representations and show that the self-supervised representations stemming from the wav2vec2 and wavLM families perform best. Of the two, wavLM is better when restricting the pretraining data to LibriSpeech, as required by the challenge rules. To further improve performance, we finetune the wavLM model for the deepfake detection task. We extend the ASVspoof5 dataset with samples from other deepfake detection datasets and apply data augmentation. Our final challenge submission consists of a late fusion combination of four models and achieves an equal error rate of 6.56% and 17.08% on the two evaluation sets.",
      "authors": [
        "David Combei",
        "Adriana Stan",
        "Dan Oneata",
        "Horia Cucu"
      ],
      "published": "2024-08-14T09:43:35Z",
      "updated": "2024-08-14T09:43:35Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.07414v1",
      "landing_url": "https://arxiv.org/abs/2408.07414v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.07414"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets audio deepfake detection and representation benchmarking; it does not discuss discrete audio tokens, token quantization, or token-level modeling, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets audio deepfake detection and representation benchmarking; it does not discuss discrete audio tokens, token quantization, or token-level modeling, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The work focuses on audio deepfake detection employing pretrained wav2vec2 and wavLM representations and model ensemble, but it does not describe any discrete audio token generation process or discrete token design, nor does it mention vector quantization, residual quantization, or other discrete token mechanisms as a core contribution; thus, it does not meet the inclusion criteria regarding discrete audio token formulation and usage for sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The work focuses on audio deepfake detection employing pretrained wav2vec2 and wavLM representations and model ensemble, but it does not describe any discrete audio token generation process or discrete token design, nor does it mention vector quantization, residual quantization, or other discrete token mechanisms as a core contribution; thus, it does not meet the inclusion criteria regarding discrete audio token formulation and usage for sequence modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation",
    "abstract": "This paper introduces two multilingual systems, IKUN and IKUN-C, developed for the general machine translation task in WMT24. IKUN and IKUN-C represent an open system and a constrained system, respectively, built on Llama-3-8b and Mistral-7B-v0.3. Both systems are designed to handle all 11 language directions using a single model. According to automatic evaluation metrics, IKUN-C achieved 6 first-place and 3 second-place finishes among all constrained systems, while IKUN secured 1 first-place and 2 second-place finishes across both open and constrained systems. These encouraging results suggest that large language models (LLMs) are nearing the level of proficiency required for effective multilingual machine translation. The systems are based on a two-stage approach: first, continuous pre-training on monolingual data in 10 languages, followed by fine-tuning on high-quality parallel data for 11 language directions. The primary difference between IKUN and IKUN-C lies in their monolingual pre-training strategy. IKUN-C is pre-trained using constrained monolingual data, whereas IKUN leverages monolingual data from the OSCAR dataset. In the second phase, both systems are fine-tuned on parallel data sourced from NTREX, Flores, and WMT16-23 for all 11 language pairs.",
    "metadata": {
      "arxiv_id": "2408.11512",
      "title": "IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation",
      "summary": "This paper introduces two multilingual systems, IKUN and IKUN-C, developed for the general machine translation task in WMT24. IKUN and IKUN-C represent an open system and a constrained system, respectively, built on Llama-3-8b and Mistral-7B-v0.3. Both systems are designed to handle all 11 language directions using a single model. According to automatic evaluation metrics, IKUN-C achieved 6 first-place and 3 second-place finishes among all constrained systems, while IKUN secured 1 first-place and 2 second-place finishes across both open and constrained systems. These encouraging results suggest that large language models (LLMs) are nearing the level of proficiency required for effective multilingual machine translation. The systems are based on a two-stage approach: first, continuous pre-training on monolingual data in 10 languages, followed by fine-tuning on high-quality parallel data for 11 language directions. The primary difference between IKUN and IKUN-C lies in their monolingual pre-training strategy. IKUN-C is pre-trained using constrained monolingual data, whereas IKUN leverages monolingual data from the OSCAR dataset. In the second phase, both systems are fine-tuned on parallel data sourced from NTREX, Flores, and WMT16-23 for all 11 language pairs.",
      "authors": [
        "Baohao Liao",
        "Christian Herold",
        "Shahram Khadivi",
        "Christof Monz"
      ],
      "published": "2024-08-21T10:44:10Z",
      "updated": "2024-08-29T12:25:14Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.11512v2",
      "landing_url": "https://arxiv.org/abs/2408.11512v2",
      "doi": "https://doi.org/10.48550/arXiv.2408.11512"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe multilingual machine translation systems (IKUN/IKUN-C) built on Llama-3 and Mistral for 11 language directions, with no discussion of discrete audio tokens or audio tokenization; thus it does not meet the inclusion criteria focused on discrete audio token representations for audio-language modeling, and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe multilingual machine translation systems (IKUN/IKUN-C) built on Llama-3 and Mistral for 11 language directions, with no discussion of discrete audio tokens or audio tokenization; thus it does not meet the inclusion criteria focused on discrete audio token representations for audio-language modeling, and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on multilingual machine translation systems based on LLMs, without addressing any discrete audio token design, vector quantization, or audio signal discretization; it lacks the core focus on discrete audio token representations or related methodology required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on multilingual machine translation systems based on LLMs, without addressing any discrete audio token design, vector quantization, or audio signal discretization; it lacks the core focus on discrete audio token representations or related methodology required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Beyond Levenshtein: Leveraging Multiple Algorithms for Robust Word Error Rate Computations And Granular Error Classifications",
    "abstract": "The Word Error Rate (WER) is the common measure of accuracy for Automatic Speech Recognition (ASR). Transcripts are usually pre-processed by substituting specific characters to account for non-semantic differences. As a result of this normalisation, information on the accuracy of punctuation or capitalisation is lost. We present a non-destructive, token-based approach using an extended Levenshtein distance algorithm to compute a robust WER and additional orthographic metrics. Transcription errors are also classified more granularly by existing string similarity and phonetic algorithms. An evaluation on several datasets demonstrates the practical equivalence of our approach compared to common WER computations. We also provide an exemplary analysis of derived use cases, such as a punctuation error rate, and a web application for interactive use and visualisation of our implementation. The code is available open-source.",
    "metadata": {
      "arxiv_id": "2408.15616",
      "title": "Beyond Levenshtein: Leveraging Multiple Algorithms for Robust Word Error Rate Computations And Granular Error Classifications",
      "summary": "The Word Error Rate (WER) is the common measure of accuracy for Automatic Speech Recognition (ASR). Transcripts are usually pre-processed by substituting specific characters to account for non-semantic differences. As a result of this normalisation, information on the accuracy of punctuation or capitalisation is lost. We present a non-destructive, token-based approach using an extended Levenshtein distance algorithm to compute a robust WER and additional orthographic metrics. Transcription errors are also classified more granularly by existing string similarity and phonetic algorithms. An evaluation on several datasets demonstrates the practical equivalence of our approach compared to common WER computations. We also provide an exemplary analysis of derived use cases, such as a punctuation error rate, and a web application for interactive use and visualisation of our implementation. The code is available open-source.",
      "authors": [
        "Korbinian Kuhn",
        "Verena Kersken",
        "Gottfried Zimmermann"
      ],
      "published": "2024-08-28T08:14:51Z",
      "updated": "2024-08-28T08:14:51Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.15616v1",
      "landing_url": "https://arxiv.org/abs/2408.15616v1",
      "doi": "https://doi.org/10.21437/Interspeech.2024-32"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work focuses on improving WER calculation for ASR using token-based processing of transcripts, not on discrete audio token representations or token generation/quantization for audio modeling, so it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This work focuses on improving WER calculation for ASR using token-based processing of transcripts, not on discrete audio token representations or token generation/quantization for audio modeling, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on enhancing Word Error Rate computations and error classification in ASR systems, using token-based approaches and algorithms; however, it does not discuss the generation, design, or use of discrete audio tokens via methods like vector quantization or neural audio codecs, nor does it center on discrete token representations for audio processing or modeling as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on enhancing Word Error Rate computations and error classification in ASR systems, using token-based approaches and algorithms; however, it does not discuss the generation, design, or use of discrete audio tokens via methods like vector quantization or neural audio codecs, nor does it center on discrete token representations for audio processing or modeling as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Diffusion Policy Policy Optimization",
    "abstract": "We introduce Diffusion Policy Policy Optimization, DPPO, an algorithmic framework including best practices for fine-tuning diffusion-based policies (e.g. Diffusion Policy) in continuous control and robot learning tasks using the policy gradient (PG) method from reinforcement learning (RL). PG methods are ubiquitous in training RL policies with other policy parameterizations; nevertheless, they had been conjectured to be less efficient for diffusion-based policies. Surprisingly, we show that DPPO achieves the strongest overall performance and efficiency for fine-tuning in common benchmarks compared to other RL methods for diffusion-based policies and also compared to PG fine-tuning of other policy parameterizations. Through experimental investigation, we find that DPPO takes advantage of unique synergies between RL fine-tuning and the diffusion parameterization, leading to structured and on-manifold exploration, stable training, and strong policy robustness. We further demonstrate the strengths of DPPO in a range of realistic settings, including simulated robotic tasks with pixel observations, and via zero-shot deployment of simulation-trained policies on robot hardware in a long-horizon, multi-stage manipulation task. Website with code: diffusion-ppo.github.io",
    "metadata": {
      "arxiv_id": "2409.00588",
      "title": "Diffusion Policy Policy Optimization",
      "summary": "We introduce Diffusion Policy Policy Optimization, DPPO, an algorithmic framework including best practices for fine-tuning diffusion-based policies (e.g. Diffusion Policy) in continuous control and robot learning tasks using the policy gradient (PG) method from reinforcement learning (RL). PG methods are ubiquitous in training RL policies with other policy parameterizations; nevertheless, they had been conjectured to be less efficient for diffusion-based policies. Surprisingly, we show that DPPO achieves the strongest overall performance and efficiency for fine-tuning in common benchmarks compared to other RL methods for diffusion-based policies and also compared to PG fine-tuning of other policy parameterizations. Through experimental investigation, we find that DPPO takes advantage of unique synergies between RL fine-tuning and the diffusion parameterization, leading to structured and on-manifold exploration, stable training, and strong policy robustness. We further demonstrate the strengths of DPPO in a range of realistic settings, including simulated robotic tasks with pixel observations, and via zero-shot deployment of simulation-trained policies on robot hardware in a long-horizon, multi-stage manipulation task. Website with code: diffusion-ppo.github.io",
      "authors": [
        "Allen Z. Ren",
        "Justin Lidard",
        "Lars L. Ankile",
        "Anthony Simeonov",
        "Pulkit Agrawal",
        "Anirudha Majumdar",
        "Benjamin Burchfiel",
        "Hongkai Dai",
        "Max Simchowitz"
      ],
      "published": "2024-09-01T02:47:50Z",
      "updated": "2024-12-09T21:30:07Z",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.00588v3",
      "landing_url": "https://arxiv.org/abs/2409.00588v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.00588"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns Diffusion Policy Policy Optimization for diffusion-based policies in reinforcement learning; it is about RL methods and robotics, not discrete audio token design or discrete token representations, so it does not meet the inclusion criteria focused on discrete audio tokens; thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns Diffusion Policy Policy Optimization for diffusion-based policies in reinforcement learning; it is about RL methods and robotics, not discrete audio token design or discrete token representations, so it does not meet the inclusion criteria focused on discrete audio tokens; thus it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on reinforcement learning methods for continuous control and robotic tasks involving diffusion-based policies; it does not address the generation or design of discrete audio tokens from continuous audio signals, nor does it involve audio language modeling, quantization, or semantic disentanglement related to discrete audio representation. Therefore, it does not meet the inclusion criteria related to discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on reinforcement learning methods for continuous control and robotic tasks involving diffusion-based policies; it does not address the generation or design of discrete audio tokens from continuous audio signals, nor does it involve audio language modeling, quantization, or semantic disentanglement related to discrete audio representation. Therefore, it does not meet the inclusion criteria related to discrete audio token research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer",
    "abstract": "The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g. phone), which may compromise their naturalness. In this paper, we introduce Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the mask-and-predict learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. Experiments with 100K hours of in-the-wild speech demonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility. Audio samples are available at https://maskgct.github.io/. We release our code and model checkpoints at https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct.",
    "metadata": {
      "arxiv_id": "2409.00750",
      "title": "MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer",
      "summary": "The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g. phone), which may compromise their naturalness. In this paper, we introduce Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the mask-and-predict learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. Experiments with 100K hours of in-the-wild speech demonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility. Audio samples are available at https://maskgct.github.io/. We release our code and model checkpoints at https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct.",
      "authors": [
        "Yuancheng Wang",
        "Haoyue Zhan",
        "Liwei Liu",
        "Ruihong Zeng",
        "Haotian Guo",
        "Jiachen Zheng",
        "Qiang Zhang",
        "Xueyao Zhang",
        "Shunsi Zhang",
        "Zhizheng Wu"
      ],
      "published": "2024-09-01T15:26:30Z",
      "updated": "2024-10-20T14:25:49Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.00750v3",
      "landing_url": "https://arxiv.org/abs/2409.00750v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.00750"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on discrete audio tokens (semantic and acoustic tokens) used in a non-autoregressive TTS model, aligning with the inclusion theme of token-based audio representations, although the abstract provides high-level method without full quantization specifics.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item centers on discrete audio tokens (semantic and acoustic tokens) used in a non-autoregressive TTS model, aligning with the inclusion theme of token-based audio representations, although the abstract provides high-level method without full quantization specifics.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents MaskGCT, a non-autoregressive TTS model that predicts semantic tokens extracted from a speech SSL model and acoustic tokens conditioned on these semantic tokens, employing a mask-and-predict learning paradigm; this indicates the use of discrete audio tokens for speech synthesis, aligned with the topic definition emphasizing token generation from audio representations for sequence modeling and understanding. The paper provides methodological details on token prediction stages, semantic and acoustic tokens derived presumably via SSL and quantization, suggesting compliance with inclusion criteria on discrete token generation and usage in sequence models for TTS. The release of code and checkpoints further supports reproducibility and technical detail. No exclusion criteria are triggered since the paper is a formal academic work with detailed method descriptions and no indication of solely traditional codec usage or insufficient technical content.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper presents MaskGCT, a non-autoregressive TTS model that predicts semantic tokens extracted from a speech SSL model and acoustic tokens conditioned on these semantic tokens, employing a mask-and-predict learning paradigm; this indicates the use of discrete audio tokens for speech synthesis, aligned with the topic definition emphasizing token generation from audio representations for sequence modeling and understanding. The paper provides methodological details on token prediction stages, semantic and acoustic tokens derived presumably via SSL and quantization, suggesting compliance with inclusion criteria on discrete token generation and usage in sequence models for TTS. The release of code and checkpoints further supports reproducibility and technical detail. No exclusion criteria are triggered since the paper is a formal academic work with detailed method descriptions and no indication of solely traditional codec usage or insufficient technical content.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "vec2wav 2.0: Advancing Voice Conversion via Discrete Token Vocoders",
    "abstract": "We propose a new speech discrete token vocoder, vec2wav 2.0, which advances voice conversion (VC). We use discrete tokens from speech self-supervised models as the content features of source speech, and treat VC as a prompted vocoding task. To amend the loss of speaker timbre in the content tokens, vec2wav 2.0 utilizes the WavLM features to provide strong timbre-dependent information. A novel adaptive Snake activation function is proposed to better incorporate timbre into the waveform reconstruction process. In this way, vec2wav 2.0 learns to alter the speaker timbre appropriately given different reference prompts. Also, no supervised data is required for vec2wav 2.0 to be effectively trained. Experimental results demonstrate that vec2wav 2.0 outperforms all other baselines to a considerable margin in terms of audio quality and speaker similarity in any-to-any VC. Ablation studies verify the effects made by the proposed techniques. Moreover, vec2wav 2.0 achieves competitive cross-lingual VC even only trained on monolingual corpus. Thus, vec2wav 2.0 shows timbre can potentially be manipulated only by speech token vocoders, pushing the frontiers of VC and speech synthesis.",
    "metadata": {
      "arxiv_id": "2409.01995",
      "title": "vec2wav 2.0: Advancing Voice Conversion via Discrete Token Vocoders",
      "summary": "We propose a new speech discrete token vocoder, vec2wav 2.0, which advances voice conversion (VC). We use discrete tokens from speech self-supervised models as the content features of source speech, and treat VC as a prompted vocoding task. To amend the loss of speaker timbre in the content tokens, vec2wav 2.0 utilizes the WavLM features to provide strong timbre-dependent information. A novel adaptive Snake activation function is proposed to better incorporate timbre into the waveform reconstruction process. In this way, vec2wav 2.0 learns to alter the speaker timbre appropriately given different reference prompts. Also, no supervised data is required for vec2wav 2.0 to be effectively trained. Experimental results demonstrate that vec2wav 2.0 outperforms all other baselines to a considerable margin in terms of audio quality and speaker similarity in any-to-any VC. Ablation studies verify the effects made by the proposed techniques. Moreover, vec2wav 2.0 achieves competitive cross-lingual VC even only trained on monolingual corpus. Thus, vec2wav 2.0 shows timbre can potentially be manipulated only by speech token vocoders, pushing the frontiers of VC and speech synthesis.",
      "authors": [
        "Yiwei Guo",
        "Zhihan Li",
        "Junjie Li",
        "Chenpeng Du",
        "Hankun Wang",
        "Shuai Wang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-09-03T15:41:07Z",
      "updated": "2025-05-24T13:50:34Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.01995v4",
      "landing_url": "https://arxiv.org/abs/2409.01995v4",
      "doi": "https://doi.org/10.48550/arXiv.2409.01995"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Despite clearly describing a discrete-token vocoder for voice conversion with method details, the item lacks an explicit publication/update date, triggering exclusion under the timestamp criterion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Despite clearly describing a discrete-token vocoder for voice conversion with method details, the item lacks an explicit publication/update date, triggering exclusion under the timestamp criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents \"vec2wav 2.0,\" a discrete token vocoder using discrete tokens derived from self-supervised speech models as content features and addresses voice conversion by treating it as a prompted vocoding task; the abstract explicitly states the use of discrete tokens and the methodological innovations like the adaptive Snake activation function, demonstrating focus on discrete representation for speech generation and transformation. However, the abstract lacks detailed descriptions of token generation process such as vector quantization or codebook design and training configuration, which are critical for reproducibility and classification under the inclusion criteria. Despite this, the core concept aligns well with the theme of discrete audio tokens applied to speech generation, making it more suitable for inclusion than exclusion.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The study presents \"vec2wav 2.0,\" a discrete token vocoder using discrete tokens derived from self-supervised speech models as content features and addresses voice conversion by treating it as a prompted vocoding task; the abstract explicitly states the use of discrete tokens and the methodological innovations like the adaptive Snake activation function, demonstrating focus on discrete representation for speech generation and transformation. However, the abstract lacks detailed descriptions of token generation process such as vector quantization or codebook design and training configuration, which are critical for reproducibility and classification under the inclusion criteria. Despite this, the core concept aligns well with the theme of discrete audio tokens applied to speech generation, making it more suitable for inclusion than exclusion.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": {
      "reasoning": "Integration of junior reviews: JuniorNano excluded primarily due to a missing publication date; JuniorMini considered it relevant because it centers on a discrete-token vocoder but noted the abstract lacks concrete token-generation/quantization details. My assessment: the paper clearly targets discrete audio tokens (a discrete-token vocoder using SSL-derived tokens) and is directly relevant to the topic, but the abstract does not describe the token formation/quantization/codebook or provide reproducible method details required by the inclusion criteria, and the provided input lacks a clear publication/update date (an explicit exclusion criterion highlighted by JuniorNano). Therefore it fails to meet the requirement of giving detailed token-generation methodology and may violate the timestamp requirement. 1-sentence explanation: Relevant to discrete audio tokens but better to exclude because the abstract omits essential token-generation/quantization details and no clear publication date is provided.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Integration of junior reviews: JuniorNano excluded primarily due to a missing publication date; JuniorMini considered it relevant because it centers on a discrete-token vocoder but noted the abstract lacks concrete token-generation/quantization details. My assessment: the paper clearly targets discrete audio tokens (a discrete-token vocoder using SSL-derived tokens) and is directly relevant to the topic, but the abstract does not describe the token formation/quantization/codebook or provide reproducible method details required by the inclusion criteria, and the provided input lacks a clear publication/update date (an explicit exclusion criterion highlighted by JuniorNano). Therefore it fails to meet the requirement of giving detailed token-generation methodology and may violate the timestamp requirement. 1-sentence explanation: Relevant to discrete audio tokens but better to exclude because the abstract omits essential token-generation/quantization details and no clear publication date is provided.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "STAB: Speech Tokenizer Assessment Benchmark",
    "abstract": "Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use of speech as an input to the widely successful large language models (LLMs). Currently, while several speech tokenizers have been proposed, there is ambiguity regarding the properties that are desired from a tokenizer for specific downstream tasks and its overall generalizability. Evaluating the performance of tokenizers across different downstream tasks is a computationally intensive effort that poses challenges for scalability. To circumvent this requirement, we present STAB (Speech Tokenizer Assessment Benchmark), a systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics. This framework provides a deeper understanding of the underlying mechanisms of speech tokenization, thereby offering a valuable resource for expediting the advancement of future tokenizer models and enabling comparative analysis using a standardized benchmark. We evaluate the STAB metrics and correlate this with downstream task performance across a range of speech tasks and tokenizer choices.",
    "metadata": {
      "arxiv_id": "2409.02384",
      "title": "STAB: Speech Tokenizer Assessment Benchmark",
      "summary": "Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use of speech as an input to the widely successful large language models (LLMs). Currently, while several speech tokenizers have been proposed, there is ambiguity regarding the properties that are desired from a tokenizer for specific downstream tasks and its overall generalizability. Evaluating the performance of tokenizers across different downstream tasks is a computationally intensive effort that poses challenges for scalability. To circumvent this requirement, we present STAB (Speech Tokenizer Assessment Benchmark), a systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics. This framework provides a deeper understanding of the underlying mechanisms of speech tokenization, thereby offering a valuable resource for expediting the advancement of future tokenizer models and enabling comparative analysis using a standardized benchmark. We evaluate the STAB metrics and correlate this with downstream task performance across a range of speech tasks and tokenizer choices.",
      "authors": [
        "Shikhar Vashishth",
        "Harman Singh",
        "Shikhar Bharadwaj",
        "Sriram Ganapathy",
        "Chulayuth Asawaroengchai",
        "Kartik Audhkhasi",
        "Andrew Rosenberg",
        "Ankur Bapna",
        "Bhuvana Ramabhadran"
      ],
      "published": "2024-09-04T02:20:59Z",
      "updated": "2024-09-04T02:20:59Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.02384v1",
      "landing_url": "https://arxiv.org/abs/2409.02384v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.02384"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on a benchmark framework (STAB) to evaluate existing speech tokenizers and their downstream task performance, but it does not present a discrete audio token design or token generation/quantization methods (e.g., VQ/RVQ, hierarchical quantization, semantic disentanglement) required by the inclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on a benchmark framework (STAB) to evaluate existing speech tokenizers and their downstream task performance, but it does not present a discrete audio token design or token generation/quantization methods (e.g., VQ/RVQ, hierarchical quantization, semantic disentanglement) required by the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a benchmark for assessing speech tokenizers, which produce discrete tokens from speech. It discusses evaluation metrics and downstream task correlations but does not explicitly detail the token generation process, such as vector quantization or semantic disentanglement, nor does it specify neural codec methods or provide reproducible technical details about token formation and usage for sequence/language models, which are crucial inclusion criteria. Therefore, it lacks sufficient methodological specificity to verify whether it meets the core research contribution regarding discrete audio token design and application, suggesting uncertainty about inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study presents a benchmark for assessing speech tokenizers, which produce discrete tokens from speech. It discusses evaluation metrics and downstream task correlations but does not explicitly detail the token generation process, such as vector quantization or semantic disentanglement, nor does it specify neural codec methods or provide reproducible technical details about token formation and usage for sequence/language models, which are crucial inclusion criteria. Therefore, it lacks sufficient methodological specificity to verify whether it meets the core research contribution regarding discrete audio token design and application, suggesting uncertainty about inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed both junior reviewers' comments: JuniorNano recommended exclusion due to absence of token generation/quantization details, and JuniorMini was uncertain for the same reason. The abstract describes STAB as a benchmarking/evaluation framework for speech tokenizers and reports correlating metrics with downstream tasks, but it does not present a discrete audio token design, nor does it provide the required methodological details (e.g., VQ/RVQ/hierarchical quantization, codebook/layer design, token formation or reproducible training/inference settings) that the inclusion criteria mandate. Therefore it fails the core requirement that the paper’s main contribution be a discrete-token design or a reproducible tokenization method for use with sequence/language models. Brief explanation: Exclude — the paper is a benchmarking framework without novel or sufficiently detailed discrete audio token generation methods, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "I reviewed both junior reviewers' comments: JuniorNano recommended exclusion due to absence of token generation/quantization details, and JuniorMini was uncertain for the same reason. The abstract describes STAB as a benchmarking/evaluation framework for speech tokenizers and reports correlating metrics with downstream tasks, but it does not present a discrete audio token design, nor does it provide the required methodological details (e.g., VQ/RVQ/hierarchical quantization, codebook/layer design, token formation or reproducible training/inference settings) that the inclusion criteria mandate. Therefore it fails the core requirement that the paper’s main contribution be a discrete-token design or a reproducible tokenization method for use with sequence/language models. Brief explanation: Exclude — the paper is a benchmarking framework without novel or sufficiently detailed discrete audio token generation methods, so it does not meet the inclusion criteria.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "LAST: Language Model Aware Speech Tokenization",
    "abstract": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.",
    "metadata": {
      "arxiv_id": "2409.03701",
      "title": "LAST: Language Model Aware Speech Tokenization",
      "summary": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.",
      "authors": [
        "Arnon Turetzky",
        "Yossi Adi"
      ],
      "published": "2024-09-05T16:57:39Z",
      "updated": "2024-09-10T14:45:15Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.03701v2",
      "landing_url": "https://arxiv.org/abs/2409.03701v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.03701"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on discrete speech representations/tokenization designed for audio-language modeling and cross-modal use with a pre-trained LM; it aligns with the topic of Discrete Audio Tokens and downstream speech/LM tasks, though the abstract does not disclose full methodological details in this excerpt.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item centers on discrete speech representations/tokenization designed for audio-language modeling and cross-modal use with a pre-trained LM; it aligns with the topic of Discrete Audio Tokens and downstream speech/LM tasks, though the abstract does not disclose full methodological details in this excerpt.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a novel speech tokenizer that integrates objectives from pre-trained textual language models to produce discrete speech representations for speech language modeling and speech-to-text tasks. It explicitly addresses the generation of discrete tokens from continuous speech signals and demonstrates their use in sequence modeling with better performance than baselines. The method involves transforming features from a pre-trained speech model into a new feature space that enables better clustering, which implies quantization-like discretization. The work aligns with the inclusion criteria of producing discrete audio tokens used in language or sequence modeling and provides empirical evaluations. There is no indication that this is a mere demo or lacks methodological detail, nor does it focus on traditional codec standards lacking LM relevance. Thus, it meets the inclusion criteria without hitting any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a novel speech tokenizer that integrates objectives from pre-trained textual language models to produce discrete speech representations for speech language modeling and speech-to-text tasks. It explicitly addresses the generation of discrete tokens from continuous speech signals and demonstrates their use in sequence modeling with better performance than baselines. The method involves transforming features from a pre-trained speech model into a new feature space that enables better clustering, which implies quantization-like discretization. The work aligns with the inclusion criteria of producing discrete audio tokens used in language or sequence modeling and provides empirical evaluations. There is no indication that this is a mere demo or lacks methodological detail, nor does it focus on traditional codec standards lacking LM relevance. Thus, it meets the inclusion criteria without hitting any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Disentangling the Prosody and Semantic Information with Pre-trained Model for In-Context Learning based Zero-Shot Voice Conversion",
    "abstract": "Voice conversion (VC) aims to modify the speaker's timbre while retaining speech content. Previous approaches have tokenized the outputs from self-supervised into semantic tokens, facilitating disentanglement of speech content information. Recently, in-context learning (ICL) has emerged in text-to-speech (TTS) systems for effectively modeling specific characteristics such as timbre through context conditioning. This paper proposes an ICL capability enhanced VC system (ICL-VC) employing a mask and reconstruction training strategy based on flow-matching generative models. Augmented with semantic tokens, our experiments on the LibriTTS dataset demonstrate that ICL-VC improves speaker similarity. Additionally, we find that k-means is a versatile tokenization method applicable to various pre-trained models. However, the ICL-VC system faces challenges in preserving the prosody of the source speech. To mitigate this issue, we propose incorporating prosody embeddings extracted from a pre-trained emotion recognition model into our system. Integration of prosody embeddings notably enhances the system's capability to preserve source speech prosody, as validated on the Emotional Speech Database.",
    "metadata": {
      "arxiv_id": "2409.05004",
      "title": "Disentangling the Prosody and Semantic Information with Pre-trained Model for In-Context Learning based Zero-Shot Voice Conversion",
      "summary": "Voice conversion (VC) aims to modify the speaker's timbre while retaining speech content. Previous approaches have tokenized the outputs from self-supervised into semantic tokens, facilitating disentanglement of speech content information. Recently, in-context learning (ICL) has emerged in text-to-speech (TTS) systems for effectively modeling specific characteristics such as timbre through context conditioning. This paper proposes an ICL capability enhanced VC system (ICL-VC) employing a mask and reconstruction training strategy based on flow-matching generative models. Augmented with semantic tokens, our experiments on the LibriTTS dataset demonstrate that ICL-VC improves speaker similarity. Additionally, we find that k-means is a versatile tokenization method applicable to various pre-trained models. However, the ICL-VC system faces challenges in preserving the prosody of the source speech. To mitigate this issue, we propose incorporating prosody embeddings extracted from a pre-trained emotion recognition model into our system. Integration of prosody embeddings notably enhances the system's capability to preserve source speech prosody, as validated on the Emotional Speech Database.",
      "authors": [
        "Zhengyang Chen",
        "Shuai Wang",
        "Mingyang Zhang",
        "Xuechen Liu",
        "Junichi Yamagishi",
        "Yanmin Qian"
      ],
      "published": "2024-09-08T07:24:03Z",
      "updated": "2024-09-10T07:36:03Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.05004v2",
      "landing_url": "https://arxiv.org/abs/2409.05004v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.05004"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract mentions discrete tokens via semantic tokenization and a k-means approach, which aligns with the discrete-audio-token topic, but it lacks explicit token-generation/quantization details and reproducible methodology in the abstract, making it unclear whether it fully satisfies the core criteria.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The abstract mentions discrete tokens via semantic tokenization and a k-means approach, which aligns with the discrete-audio-token topic, but it lacks explicit token-generation/quantization details and reproducible methodology in the abstract, making it unclear whether it fully satisfies the core criteria.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on voice conversion using semantic tokens derived from self-supervised learning and discusses tokenization methods like k-means applicable to pre-trained models, indicating a form of discrete audio token generation involving semantic disentanglement. It also clearly mentions token usage for sequence modeling and understanding, aligning with the inclusion criteria emphasizing semantic/acoustic disentanglement and subsequent use in sequence models. Although the exact quantization details are less explicit, the use of semantic tokens and experimental validation on datasets supports the relevance. Therefore, it satisfies the key inclusion criteria and does not trigger exclusion criteria related to lack of technical detail or mere demo content.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The study focuses on voice conversion using semantic tokens derived from self-supervised learning and discusses tokenization methods like k-means applicable to pre-trained models, indicating a form of discrete audio token generation involving semantic disentanglement. It also clearly mentions token usage for sequence modeling and understanding, aligning with the inclusion criteria emphasizing semantic/acoustic disentanglement and subsequent use in sequence models. Although the exact quantization details are less explicit, the use of semantic tokens and experimental validation on datasets supports the relevance. Therefore, it satisfies the key inclusion criteria and does not trigger exclusion criteria related to lack of technical detail or mere demo content.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior reviewers' feedback: JuniorNano flagged uncertainty due to limited explicit token-generation/quantization details in the abstract, while JuniorMini judged the paper as relevant because it clearly uses semantic tokens (via SSL features and k-means clustering) for sequence-style modeling in a voice-conversion system. The abstract explicitly states semantic tokenization from self-supervised features, mentions k-means tokenization, and uses those tokens for in-context learning / sequence conditioning in VC — which aligns with the inclusion cluster of SSL-based semantic token formation and downstream sequence-model use. However, the abstract does not provide full methodological/quantization details for reproducibility, so inclusion is not absolutely certain without the full paper. Final brief explanation: The work centers on semantic discrete tokens (k-means clustering of SSL features) used for sequence-style modeling in VC, matching the discrete-audio-token topic, though the abstract lacks detailed quantization specifics.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior reviewers' feedback: JuniorNano flagged uncertainty due to limited explicit token-generation/quantization details in the abstract, while JuniorMini judged the paper as relevant because it clearly uses semantic tokens (via SSL features and k-means clustering) for sequence-style modeling in a voice-conversion system. The abstract explicitly states semantic tokenization from self-supervised features, mentions k-means tokenization, and uses those tokens for in-context learning / sequence conditioning in VC — which aligns with the inclusion cluster of SSL-based semantic token formation and downstream sequence-model use. However, the abstract does not provide full methodological/quantization details for reproducibility, so inclusion is not absolutely certain without the full paper. Final brief explanation: The work centers on semantic discrete tokens (k-means clustering of SSL features) used for sequence-style modeling in VC, matching the discrete-audio-token topic, though the abstract lacks detailed quantization specifics.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Exploring WavLM Back-ends for Speech Spoofing and Deepfake Detection",
    "abstract": "This paper describes our submitted systems to the ASVspoof 5 Challenge Track 1: Speech Deepfake Detection - Open Condition, which consists of a stand-alone speech deepfake (bonafide vs spoof) detection task. Recently, large-scale self-supervised models become a standard in Automatic Speech Recognition (ASR) and other speech processing tasks. Thus, we leverage a pre-trained WavLM as a front-end model and pool its representations with different back-end techniques. The complete framework is fine-tuned using only the trained dataset of the challenge, similar to the close condition. Besides, we adopt data-augmentation by adding noise and reverberation using MUSAN noise and RIR datasets. We also experiment with codec augmentations to increase the performance of our method. Ultimately, we use the Bosaris toolkit for score calibration and system fusion to get better Cllr scores. Our fused system achieves 0.0937 minDCF, 3.42% EER, 0.1927 Cllr, and 0.1375 actDCF.",
    "metadata": {
      "arxiv_id": "2409.05032",
      "title": "Exploring WavLM Back-ends for Speech Spoofing and Deepfake Detection",
      "summary": "This paper describes our submitted systems to the ASVspoof 5 Challenge Track 1: Speech Deepfake Detection - Open Condition, which consists of a stand-alone speech deepfake (bonafide vs spoof) detection task. Recently, large-scale self-supervised models become a standard in Automatic Speech Recognition (ASR) and other speech processing tasks. Thus, we leverage a pre-trained WavLM as a front-end model and pool its representations with different back-end techniques. The complete framework is fine-tuned using only the trained dataset of the challenge, similar to the close condition. Besides, we adopt data-augmentation by adding noise and reverberation using MUSAN noise and RIR datasets. We also experiment with codec augmentations to increase the performance of our method. Ultimately, we use the Bosaris toolkit for score calibration and system fusion to get better Cllr scores. Our fused system achieves 0.0937 minDCF, 3.42% EER, 0.1927 Cllr, and 0.1375 actDCF.",
      "authors": [
        "Theophile Stourbe",
        "Victor Miara",
        "Theo Lepage",
        "Reda Dehak"
      ],
      "published": "2024-09-08T08:54:36Z",
      "updated": "2024-09-08T08:54:36Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.05032v1",
      "landing_url": "https://arxiv.org/abs/2409.05032v1",
      "doi": "https://doi.org/10.21437/ASVspoof.2024-11"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on WavLM-backed detection of speech deepfakes, not on discrete audio tokens or tokenization-based audio modeling, so it does not meet the core inclusion criterion and should be excluded (no disqualifying exclusion present).",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item focuses on WavLM-backed detection of speech deepfakes, not on discrete audio tokens or tokenization-based audio modeling, so it does not meet the core inclusion criterion and should be excluded (no disqualifying exclusion present).",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on speech deepfake detection using pre-trained WavLM and various back-end techniques, including data augmentation and score calibration, but it does not address discrete audio token generation, vector quantization, or semantic disentanglement of discrete tokens as required by the inclusion criteria; it also lacks explanation on token formation or usage for sequence/language modeling, thus not meeting the core thematic requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on speech deepfake detection using pre-trained WavLM and various back-end techniques, including data augmentation and score calibration, but it does not address discrete audio token generation, vector quantization, or semantic disentanglement of discrete tokens as required by the inclusion criteria; it also lacks explanation on token formation or usage for sequence/language modeling, thus not meeting the core thematic requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Learning Multi-Aspect Item Palette: A Semantic Tokenization Framework for Generative Recommendation",
    "abstract": "Traditional recommendation models often rely on unique item identifiers (IDs) to distinguish between items, which can hinder their ability to effectively leverage item content information and generalize to long-tailed or cold-start items. Recently, semantic tokenization has been proposed as a promising solution that aims to tokenize each item's semantic representation into a sequence of discrete tokens. These semantic tokens have become fundamental in training generative recommendation models. However, existing methods typically rely on RQ-VAE, a residual vector quantizer, for semantic tokenization. This reliance introduces several key limitations, including challenges in embedding extraction, hierarchical coarse-to-fine quantization, and training stability. To address these issues, we introduce LAMIA, a novel approach for multi-aspect semantic tokenization. Unlike RQ-VAE, which uses a single embedding, LAMIA learns an ``item palette''--a collection of independent and semantically parallel embeddings that capture multiple aspects of items. Additionally, LAMIA enhances the semantic encoders through domain-specific tuning using text-based reconstruction tasks, resulting in more representative item palette embeddings. We have conducted extensive experiments to validate the effectiveness of the LAMIA framework across various recommendation tasks and datasets. Our results demonstrate significant improvements in recommendation accuracy over existing methods. To facilitate reproducible research, we will release the source code, data, and configurations.",
    "metadata": {
      "arxiv_id": "2409.07276",
      "title": "Learning Multi-Aspect Item Palette: A Semantic Tokenization Framework for Generative Recommendation",
      "summary": "Traditional recommendation models often rely on unique item identifiers (IDs) to distinguish between items, which can hinder their ability to effectively leverage item content information and generalize to long-tailed or cold-start items. Recently, semantic tokenization has been proposed as a promising solution that aims to tokenize each item's semantic representation into a sequence of discrete tokens. These semantic tokens have become fundamental in training generative recommendation models. However, existing methods typically rely on RQ-VAE, a residual vector quantizer, for semantic tokenization. This reliance introduces several key limitations, including challenges in embedding extraction, hierarchical coarse-to-fine quantization, and training stability. To address these issues, we introduce LAMIA, a novel approach for multi-aspect semantic tokenization. Unlike RQ-VAE, which uses a single embedding, LAMIA learns an ``item palette''--a collection of independent and semantically parallel embeddings that capture multiple aspects of items. Additionally, LAMIA enhances the semantic encoders through domain-specific tuning using text-based reconstruction tasks, resulting in more representative item palette embeddings. We have conducted extensive experiments to validate the effectiveness of the LAMIA framework across various recommendation tasks and datasets. Our results demonstrate significant improvements in recommendation accuracy over existing methods. To facilitate reproducible research, we will release the source code, data, and configurations.",
      "authors": [
        "Qijiong Liu",
        "Jieming Zhu",
        "Zhaocheng Du",
        "Lu Fan",
        "Zhou Zhao",
        "Xiao-Ming Wu"
      ],
      "published": "2024-09-11T13:49:48Z",
      "updated": "2025-08-05T11:07:31Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.07276v3",
      "landing_url": "https://arxiv.org/abs/2409.07276v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.07276"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on semantic tokenization for generative recommendation and multi-aspect item representations; it does not address discrete audio tokens (VQ/RVQ, hierarchical quantization, audio-language modeling) or provide methods for tokenizing audio content, so it does not meet the Discrete Audio Tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on semantic tokenization for generative recommendation and multi-aspect item representations; it does not address discrete audio tokens (VQ/RVQ, hierarchical quantization, audio-language modeling) or provide methods for tokenizing audio content, so it does not meet the Discrete Audio Tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a semantic tokenization framework for generative recommendation using item representation embeddings, not on discrete audio tokens derived from audio signals. There is no indication that the tokens are based on audio data or that the method involves vector quantization of continuous audio signals for language model processing, which are core to the inclusion criteria. Thus, the study does not meet the thematic requirements regarding discrete audio tokens and their generation process as specified, and it does not provide relevant audio-related token generation or application.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a semantic tokenization framework for generative recommendation using item representation embeddings, not on discrete audio tokens derived from audio signals. There is no indication that the tokens are based on audio data or that the method involves vector quantization of continuous audio signals for language model processing, which are core to the inclusion criteria. Thus, the study does not meet the thematic requirements regarding discrete audio tokens and their generation process as specified, and it does not provide relevant audio-related token generation or application.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Depth on Demand: Streaming Dense Depth from a Low Frame Rate Active Sensor",
    "abstract": "High frame rate and accurate depth estimation plays an important role in several tasks crucial to robotics and automotive perception. To date, this can be achieved through ToF and LiDAR devices for indoor and outdoor applications, respectively. However, their applicability is limited by low frame rate, energy consumption, and spatial sparsity. Depth on Demand (DoD) allows for accurate temporal and spatial depth densification achieved by exploiting a high frame rate RGB sensor coupled with a potentially lower frame rate and sparse active depth sensor. Our proposal jointly enables lower energy consumption and denser shape reconstruction, by significantly reducing the streaming requirements on the depth sensor thanks to its three core stages: i) multi-modal encoding, ii) iterative multi-modal integration, and iii) depth decoding. We present extended evidence assessing the effectiveness of DoD on indoor and outdoor video datasets, covering both environment scanning and automotive perception use cases.",
    "metadata": {
      "arxiv_id": "2409.08277",
      "title": "Depth on Demand: Streaming Dense Depth from a Low Frame Rate Active Sensor",
      "summary": "High frame rate and accurate depth estimation plays an important role in several tasks crucial to robotics and automotive perception. To date, this can be achieved through ToF and LiDAR devices for indoor and outdoor applications, respectively. However, their applicability is limited by low frame rate, energy consumption, and spatial sparsity. Depth on Demand (DoD) allows for accurate temporal and spatial depth densification achieved by exploiting a high frame rate RGB sensor coupled with a potentially lower frame rate and sparse active depth sensor. Our proposal jointly enables lower energy consumption and denser shape reconstruction, by significantly reducing the streaming requirements on the depth sensor thanks to its three core stages: i) multi-modal encoding, ii) iterative multi-modal integration, and iii) depth decoding. We present extended evidence assessing the effectiveness of DoD on indoor and outdoor video datasets, covering both environment scanning and automotive perception use cases.",
      "authors": [
        "Andrea Conti",
        "Matteo Poggi",
        "Valerio Cambareri",
        "Stefano Mattoccia"
      ],
      "published": "2024-09-12T17:59:46Z",
      "updated": "2024-09-12T17:59:46Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.08277v1",
      "landing_url": "https://arxiv.org/abs/2409.08277v1",
      "doi": null
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目專注於深度感測與 DoD 的多模態深度重建，與離散音訊 tokens 的設計與量化流程無關，因此不符合收錄條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目專注於深度感測與 DoD 的多模態深度重建，與離散音訊 tokens 的設計與量化流程無關，因此不符合收錄條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on depth estimation and sensor fusion for robotics and automotive perception, which is unrelated to the inclusion criterion of discrete audio tokens and their generation methods. There is no indication of any audio token discretization, vector quantization, or audio processing relevant to the defined research scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on depth estimation and sensor fusion for robotics and automotive perception, which is unrelated to the inclusion criterion of discrete audio tokens and their generation methods. There is no indication of any audio token discretization, vector quantization, or audio processing relevant to the defined research scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unleash LLMs Potential for Recommendation by Coordinating Twin-Tower Dynamic Semantic Token Generator",
    "abstract": "Owing to the unprecedented capability in semantic understanding and logical reasoning, the pre-trained large language models (LLMs) have shown fantastic potential in developing the next-generation recommender systems (RSs). However, the static index paradigm adopted by current methods greatly restricts the utilization of LLMs capacity for recommendation, leading to not only the insufficient alignment between semantic and collaborative knowledge, but also the neglect of high-order user-item interaction patterns. In this paper, we propose Twin-Tower Dynamic Semantic Recommender (TTDS), the first generative RS which adopts dynamic semantic index paradigm, targeting at resolving the above problems simultaneously. To be more specific, we for the first time contrive a dynamic knowledge fusion framework which integrates a twin-tower semantic token generator into the LLM-based recommender, hierarchically allocating meaningful semantic index for items and users, and accordingly predicting the semantic index of target item. Furthermore, a dual-modality variational auto-encoder is proposed to facilitate multi-grained alignment between semantic and collaborative knowledge. Eventually, a series of novel tuning tasks specially customized for capturing high-order user-item interaction patterns are proposed to take advantages of user historical behavior. Extensive experiments across three public datasets demonstrate the superiority of the proposed methodology in developing LLM-based generative RSs. The proposed TTDS recommender achieves an average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric, compared with the leading baseline methods.",
    "metadata": {
      "arxiv_id": "2409.09253",
      "title": "Unleash LLMs Potential for Recommendation by Coordinating Twin-Tower Dynamic Semantic Token Generator",
      "summary": "Owing to the unprecedented capability in semantic understanding and logical reasoning, the pre-trained large language models (LLMs) have shown fantastic potential in developing the next-generation recommender systems (RSs). However, the static index paradigm adopted by current methods greatly restricts the utilization of LLMs capacity for recommendation, leading to not only the insufficient alignment between semantic and collaborative knowledge, but also the neglect of high-order user-item interaction patterns. In this paper, we propose Twin-Tower Dynamic Semantic Recommender (TTDS), the first generative RS which adopts dynamic semantic index paradigm, targeting at resolving the above problems simultaneously. To be more specific, we for the first time contrive a dynamic knowledge fusion framework which integrates a twin-tower semantic token generator into the LLM-based recommender, hierarchically allocating meaningful semantic index for items and users, and accordingly predicting the semantic index of target item. Furthermore, a dual-modality variational auto-encoder is proposed to facilitate multi-grained alignment between semantic and collaborative knowledge. Eventually, a series of novel tuning tasks specially customized for capturing high-order user-item interaction patterns are proposed to take advantages of user historical behavior. Extensive experiments across three public datasets demonstrate the superiority of the proposed methodology in developing LLM-based generative RSs. The proposed TTDS recommender achieves an average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric, compared with the leading baseline methods.",
      "authors": [
        "Jun Yin",
        "Zhengxin Zeng",
        "Mingzheng Li",
        "Hao Yan",
        "Chaozhuo Li",
        "Weihao Han",
        "Jianjin Zhang",
        "Ruochen Liu",
        "Allen Sun",
        "Denvy Deng",
        "Feng Sun",
        "Qi Zhang",
        "Shirui Pan",
        "Senzhang Wang"
      ],
      "published": "2024-09-14T01:45:04Z",
      "updated": "2024-09-14T01:45:04Z",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.09253v1",
      "landing_url": "https://arxiv.org/abs/2409.09253v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.09253"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses an LLM-based recommender system with a dynamic semantic token generator for recommendations, not any discrete audio token design or audio token quantization/semantic representations, so it does not meet the Discrete Audio Tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses an LLM-based recommender system with a dynamic semantic token generator for recommendations, not any discrete audio token design or audio token quantization/semantic representations, so it does not meet the Discrete Audio Tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on using large language models for recommendation systems by generating semantic tokens for items and users, but it does not involve discrete audio tokens derived from continuous audio signals nor discusses token creation processes related to vector quantization or audio semantic disentanglement, which are key inclusion criteria. Therefore, it does not meet the core topic requirements of discrete audio token design or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on using large language models for recommendation systems by generating semantic tokens for items and users, but it does not involve discrete audio tokens derived from continuous audio signals nor discusses token creation processes related to vector quantization or audio semantic disentanglement, which are key inclusion criteria. Therefore, it does not meet the core topic requirements of discrete audio token design or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Self-Supervised Syllable Discovery Based on Speaker-Disentangled HuBERT",
    "abstract": "Self-supervised speech representation learning has become essential for extracting meaningful features from untranscribed audio. Recent advances highlight the potential of deriving discrete symbols from the features correlated with linguistic units, which enables text-less training across diverse tasks. In particular, sentence-level Self-Distillation of the pretrained HuBERT (SD-HuBERT) induces syllabic structures within latent speech frame representations extracted from an intermediate Transformer layer. In SD-HuBERT, sentence-level representation is accumulated from speech frame features through self-attention layers using a special CLS token. However, we observe that the information aggregated in the CLS token correlates more with speaker identity than with linguistic content. To address this, we propose a speech-only self-supervised fine-tuning approach that separates syllabic units from speaker information. Our method introduces speaker perturbation as data augmentation and adopts a frame-level training objective to prevent the CLS token from aggregating paralinguistic information. Experimental results show that our approach surpasses the current state-of-the-art method in most syllable segmentation and syllabic unit quality metrics on Librispeech, underscoring its effectiveness in promoting syllabic organization within speech-only models.",
    "metadata": {
      "arxiv_id": "2409.10103",
      "title": "Self-Supervised Syllable Discovery Based on Speaker-Disentangled HuBERT",
      "summary": "Self-supervised speech representation learning has become essential for extracting meaningful features from untranscribed audio. Recent advances highlight the potential of deriving discrete symbols from the features correlated with linguistic units, which enables text-less training across diverse tasks. In particular, sentence-level Self-Distillation of the pretrained HuBERT (SD-HuBERT) induces syllabic structures within latent speech frame representations extracted from an intermediate Transformer layer. In SD-HuBERT, sentence-level representation is accumulated from speech frame features through self-attention layers using a special CLS token. However, we observe that the information aggregated in the CLS token correlates more with speaker identity than with linguistic content. To address this, we propose a speech-only self-supervised fine-tuning approach that separates syllabic units from speaker information. Our method introduces speaker perturbation as data augmentation and adopts a frame-level training objective to prevent the CLS token from aggregating paralinguistic information. Experimental results show that our approach surpasses the current state-of-the-art method in most syllable segmentation and syllabic unit quality metrics on Librispeech, underscoring its effectiveness in promoting syllabic organization within speech-only models.",
      "authors": [
        "Ryota Komatsu",
        "Takahiro Shinozaki"
      ],
      "published": "2024-09-16T09:07:08Z",
      "updated": "2024-09-16T09:07:08Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.10103v1",
      "landing_url": "https://arxiv.org/abs/2409.10103v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.10103"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses deriving discrete symbols and syllable organization from SSL HuBERT, which aligns with discrete audio token themes, but it does not provide explicit token generation/quantization mechanisms or detailed methodological steps for reproducibility, and there is no clear publication date stated; thus it’s uncertain whether it fully meets the inclusion criteria or is excluded by the stated exclusions.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses deriving discrete symbols and syllable organization from SSL HuBERT, which aligns with discrete audio token themes, but it does not provide explicit token generation/quantization mechanisms or detailed methodological steps for reproducibility, and there is no clear publication date stated; thus it’s uncertain whether it fully meets the inclusion criteria or is excluded by the stated exclusions.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised speech representation learning and syllable discovery by disentangling speaker identity from linguistic content. While it discusses inducing syllabic structures within latent speech frame representations, it does not explicitly mention the generation of discrete audio tokens via vector quantization, residual vector quantization, or semantic-driven discretization mechanisms as core methods. The paper lacks clear description of discrete token generation processes meant for sequence or language model use, which is a key inclusion criterion. Therefore, it partially touches on relevant themes but does not meet all necessary inclusion aspects regarding discrete token design and description.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised speech representation learning and syllable discovery by disentangling speaker identity from linguistic content. While it discusses inducing syllabic structures within latent speech frame representations, it does not explicitly mention the generation of discrete audio tokens via vector quantization, residual vector quantization, or semantic-driven discretization mechanisms as core methods. The paper lacks clear description of discrete token generation processes meant for sequence or language model use, which is a key inclusion criterion. Therefore, it partially touches on relevant themes but does not meet all necessary inclusion aspects regarding discrete token design and description.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers observed the paper focuses on inducing syllabic structure in SSL HuBERT representations and disentangling speaker information, which aligns with semantic/disentanglement themes, but neither reviewer found evidence that the work presents a discrete-token design (e.g., VQ/RVQ/residual/hierarchical quantization or a clear token-generation pipeline intended for sequence/language models). The abstract describes syllable segmentation and frame-level training objectives rather than an explicit discretization/quantization mechanism or token generation/usage for sequence modeling; therefore it does not meet the inclusion requirement that discrete audio token design and generation be a core contribution with reproducible token-formation details. I also note the source/publication date is not provided in the input (an exclusion criterion), but the primary exclusion is methodological mismatch. Given these points, I rate this as better to exclude.\n\nBrief explanation: The paper studies syllable discovery and speaker disentanglement in HuBERT representations but does not describe producing discrete audio tokens (VQ/RVQ/quantization/clustering) or a token pipeline for sequence/language modeling, so it fails the inclusion criteria.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers observed the paper focuses on inducing syllabic structure in SSL HuBERT representations and disentangling speaker information, which aligns with semantic/disentanglement themes, but neither reviewer found evidence that the work presents a discrete-token design (e.g., VQ/RVQ/residual/hierarchical quantization or a clear token-generation pipeline intended for sequence/language models). The abstract describes syllable segmentation and frame-level training objectives rather than an explicit discretization/quantization mechanism or token generation/usage for sequence modeling; therefore it does not meet the inclusion requirement that discrete audio token design and generation be a core contribution with reproducible token-formation details. I also note the source/publication date is not provided in the input (an exclusion criterion), but the primary exclusion is methodological mismatch. Given these points, I rate this as better to exclude.\n\nBrief explanation: The paper studies syllable discovery and speaker disentanglement in HuBERT representations but does not describe producing discrete audio tokens (VQ/RVQ/quantization/clustering) or a token pipeline for sequence/language modeling, so it fails the inclusion criteria.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Adaptive Large Language Models By Layerwise Attention Shortcuts",
    "abstract": "Transformer architectures are the backbone of the modern AI revolution. However, they are based on simply stacking the same blocks in dozens of layers and processing information sequentially from one block to another. In this paper, we propose to challenge this and introduce adaptive computations for LLM-like setups, which allow the final layer to attend to all of the intermediate layers as it deems fit through the attention mechanism, thereby introducing computational \\textbf{attention shortcuts}. These shortcuts can thus make the architecture depth and context adaptive. We showcase four different datasets, namely acoustic tokens, natural language, and symbolic music, and we achieve superior performance for GPT-like architecture. We give evidence via attention maps that the models learn complex dependencies across layers that are adaptive in context and depth depending on the input tokens.",
    "metadata": {
      "arxiv_id": "2409.10870",
      "title": "Adaptive Large Language Models By Layerwise Attention Shortcuts",
      "summary": "Transformer architectures are the backbone of the modern AI revolution. However, they are based on simply stacking the same blocks in dozens of layers and processing information sequentially from one block to another. In this paper, we propose to challenge this and introduce adaptive computations for LLM-like setups, which allow the final layer to attend to all of the intermediate layers as it deems fit through the attention mechanism, thereby introducing computational \\textbf{attention shortcuts}. These shortcuts can thus make the architecture depth and context adaptive. We showcase four different datasets, namely acoustic tokens, natural language, and symbolic music, and we achieve superior performance for GPT-like architecture. We give evidence via attention maps that the models learn complex dependencies across layers that are adaptive in context and depth depending on the input tokens.",
      "authors": [
        "Prateek Verma",
        "Mert Pilanci"
      ],
      "published": "2024-09-17T03:46:01Z",
      "updated": "2024-09-17T03:46:01Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.10870v1",
      "landing_url": "https://arxiv.org/abs/2409.10870v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.10870"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the abstract mentions 'acoustic tokens' as a dataset, the paper's core contribution is adaptive layerwise attention shortcuts for LLMs and does not present discrete audio token design/quantization or token generation methodology required by the inclusion criteria; therefore it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although the abstract mentions 'acoustic tokens' as a dataset, the paper's core contribution is adaptive layerwise attention shortcuts for LLMs and does not present discrete audio token design/quantization or token generation methodology required by the inclusion criteria; therefore it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes adaptive computations for large language models with layerwise attention shortcuts and demonstrates results on datasets including acoustic tokens, natural language, and symbolic music; however, it does not clearly focus on the design or generation process of discrete audio tokens such as vector quantization or semantic disentanglement, nor does it detail token generation methods or usage for sequence modeling in audio; thus, it does not meet the inclusion criteria focused on discrete audio token design and usage as core contributions.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes adaptive computations for large language models with layerwise attention shortcuts and demonstrates results on datasets including acoustic tokens, natural language, and symbolic music; however, it does not clearly focus on the design or generation process of discrete audio tokens such as vector quantization or semantic disentanglement, nor does it detail token generation methods or usage for sequence modeling in audio; thus, it does not meet the inclusion criteria focused on discrete audio token design and usage as core contributions.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation",
    "abstract": "Audio token modeling has become a powerful framework for speech synthesis, with two-stage approaches employing semantic tokens remaining prevalent. In this paper, we aim to simplify this process by introducing a semantic knowledge distillation method that enables high-quality speech generation in a single stage. Our proposed model improves speech quality, intelligibility, and speaker similarity compared to a single-stage baseline. Although two-stage systems still lead in intelligibility, our model significantly narrows the gap while delivering comparable speech quality. These findings showcase the potential of single-stage models to achieve efficient, high-quality TTS with a more compact and streamlined architecture.",
    "metadata": {
      "arxiv_id": "2409.11003",
      "title": "Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation",
      "summary": "Audio token modeling has become a powerful framework for speech synthesis, with two-stage approaches employing semantic tokens remaining prevalent. In this paper, we aim to simplify this process by introducing a semantic knowledge distillation method that enables high-quality speech generation in a single stage. Our proposed model improves speech quality, intelligibility, and speaker similarity compared to a single-stage baseline. Although two-stage systems still lead in intelligibility, our model significantly narrows the gap while delivering comparable speech quality. These findings showcase the potential of single-stage models to achieve efficient, high-quality TTS with a more compact and streamlined architecture.",
      "authors": [
        "Gerard I. Gállego",
        "Roy Fejgin",
        "Chunghsin Yeh",
        "Xiaoyu Liu",
        "Gautam Bhattacharya"
      ],
      "published": "2024-09-17T09:08:43Z",
      "updated": "2024-09-17T09:08:43Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11003v1",
      "landing_url": "https://arxiv.org/abs/2409.11003v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.11003"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses audio token modeling in TTS but does not clearly present discrete audio token design/quantization as the main contribution, which is required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses audio token modeling in TTS but does not clearly present discrete audio token design/quantization as the main contribution, which is required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a model for single-stage TTS using masked audio token modeling and semantic knowledge distillation. It highlights the use of semantic tokens and improvements in speech synthesis quality, which relates to discrete audio tokens and their use in speech generation. However, the abstract does not provide specific technical details on the token generation process, such as vector quantization, the token design, or how discrete tokens are produced and utilized in the sequence modeling framework. The inclusion criteria require clear method descriptions and token formation details to support reproducibility and evaluation. Due to the lack of explicit details about the token discretization method and generation workflow, it's unclear if this study meets the full criteria for inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a model for single-stage TTS using masked audio token modeling and semantic knowledge distillation. It highlights the use of semantic tokens and improvements in speech synthesis quality, which relates to discrete audio tokens and their use in speech generation. However, the abstract does not provide specific technical details on the token generation process, such as vector quantization, the token design, or how discrete tokens are produced and utilized in the sequence modeling framework. The inclusion criteria require clear method descriptions and token formation details to support reproducibility and evaluation. Due to the lack of explicit details about the token discretization method and generation workflow, it's unclear if this study meets the full criteria for inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the paper discusses audio/semantic tokens but does not describe how discrete tokens are formed (e.g., VQ/RVQ/codebook design) or provide reproducible method details; the abstract only claims improved single-stage TTS without specifying token discretization, generation workflow, or evaluation details required by the inclusion criteria. Therefore this item fails the requirement that discrete audio token design and token-generation processes be a primary, clearly-described contribution. 1-sentence explanation: The title/abstract mentions masked audio token modeling and semantic tokens but lacks explicit token discretization/quantization and method details needed to meet inclusion criteria.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the paper discusses audio/semantic tokens but does not describe how discrete tokens are formed (e.g., VQ/RVQ/codebook design) or provide reproducible method details; the abstract only claims improved single-stage TTS without specifying token discretization, generation workflow, or evaluation details required by the inclusion criteria. Therefore this item fails the requirement that discrete audio token design and token-generation processes be a primary, clearly-described contribution. 1-sentence explanation: The title/abstract mentions masked audio token modeling and semantic tokens but lacks explicit token discretization/quantization and method details needed to meet inclusion criteria.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Learning Source Disentanglement in Neural Audio Codec",
    "abstract": "Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.",
    "metadata": {
      "arxiv_id": "2409.11228",
      "title": "Learning Source Disentanglement in Neural Audio Codec",
      "summary": "Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.",
      "authors": [
        "Xiaoyu Bie",
        "Xubo Liu",
        "Gaël Richard"
      ],
      "published": "2024-09-17T14:21:02Z",
      "updated": "2025-02-11T10:35:04Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11228v2",
      "landing_url": "https://arxiv.org/abs/2409.11228v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.11228"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Abstract clearly discusses discrete audio representations via distinct codebooks in a neural audio codec and shows disentanglement in latent space, aligning with discrete token design; it appears to provide methodological details rather than a mere demo, so it should be included.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "Abstract clearly discusses discrete audio representations via distinct codebooks in a neural audio codec and shows disentanglement in latent space, aligning with discrete token design; it appears to provide methodological details rather than a mere demo, so it should be included.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces a neural audio codec that outputs discrete tokens representing different audio sources through vector quantization and source disentanglement, explicitly addressing token generation, usage in audio resynthesis and source separation, and shows potential for improving audio generation controllability, thus aligning well with the inclusion criteria regarding discrete audio tokens, their generation mechanisms, and downstream sequence modeling relevance, while no exclusion criteria are met.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper introduces a neural audio codec that outputs discrete tokens representing different audio sources through vector quantization and source disentanglement, explicitly addressing token generation, usage in audio resynthesis and source separation, and shows potential for improving audio generation controllability, thus aligning well with the inclusion criteria regarding discrete audio tokens, their generation mechanisms, and downstream sequence modeling relevance, while no exclusion criteria are met.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural audio codec models",
    "abstract": "Neural audio codec models are becoming increasingly important as they serve as tokenizers for audio, enabling efficient transmission or facilitating speech language modeling. The ideal neural audio codec should maintain content, paralinguistics, speaker characteristics, and audio information even at low bitrates. Recently, numerous advanced neural codec models have been proposed. However, codec models are often tested under varying experimental conditions. As a result, we introduce the Codec-SUPERB challenge at SLT 2024, designed to facilitate fair and lightweight comparisons among existing codec models and inspire advancements in the field. This challenge brings together representative speech applications and objective metrics, and carefully selects license-free datasets, sampling them into small sets to reduce evaluation computation costs. This paper presents the challenge's rules, datasets, five participant systems, results, and findings.",
    "metadata": {
      "arxiv_id": "2409.14085",
      "title": "Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural audio codec models",
      "summary": "Neural audio codec models are becoming increasingly important as they serve as tokenizers for audio, enabling efficient transmission or facilitating speech language modeling. The ideal neural audio codec should maintain content, paralinguistics, speaker characteristics, and audio information even at low bitrates. Recently, numerous advanced neural codec models have been proposed. However, codec models are often tested under varying experimental conditions. As a result, we introduce the Codec-SUPERB challenge at SLT 2024, designed to facilitate fair and lightweight comparisons among existing codec models and inspire advancements in the field. This challenge brings together representative speech applications and objective metrics, and carefully selects license-free datasets, sampling them into small sets to reduce evaluation computation costs. This paper presents the challenge's rules, datasets, five participant systems, results, and findings.",
      "authors": [
        "Haibin Wu",
        "Xuanjun Chen",
        "Yi-Cheng Lin",
        "Kaiwei Chang",
        "Jiawei Du",
        "Ke-Han Lu",
        "Alexander H. Liu",
        "Ho-Lam Chung",
        "Yuan-Kuei Wu",
        "Dongchao Yang",
        "Songxiang Liu",
        "Yi-Chiao Wu",
        "Xu Tan",
        "James Glass",
        "Shinji Watanabe",
        "Hung-yi Lee"
      ],
      "published": "2024-09-21T09:39:36Z",
      "updated": "2024-09-21T09:39:36Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.14085v1",
      "landing_url": "https://arxiv.org/abs/2409.14085v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.14085"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item presents a lightweight benchmark for neural audio codecs but does not address discrete audio tokens (e.g., VQ/RVQ, hierarchical quantization, semantic tokenization) or provide token-generation/quantization details necessary for token-based audio language modeling; thus it fails to meet the inclusion criteria and does not satisfy the exclusion criteria (no evident demos-only issue), so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item presents a lightweight benchmark for neural audio codecs but does not address discrete audio tokens (e.g., VQ/RVQ, hierarchical quantization, semantic tokenization) or provide token-generation/quantization details necessary for token-based audio language modeling; thus it fails to meet the inclusion criteria and does not satisfy the exclusion criteria (no evident demos-only issue), so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a lightweight benchmark for neural audio codec models, discussing their evaluation and comparison rather than proposing novel discrete tokenization methods or detailed token generation processes essential to the inclusion criteria. It does not clearly describe discrete audio token generation mechanisms, the codec’s vector quantization or token-based modeling specifically intended for language or sequence models, nor does it provide detailed methodology or token-related experimental results, leading it to lean towards a system benchmarking study rather than a core contribution on discrete token design or usage as required.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a lightweight benchmark for neural audio codec models, discussing their evaluation and comparison rather than proposing novel discrete tokenization methods or detailed token generation processes essential to the inclusion criteria. It does not clearly describe discrete audio token generation mechanisms, the codec’s vector quantization or token-based modeling specifically intended for language or sequence models, nor does it provide detailed methodology or token-related experimental results, leading it to lean towards a system benchmarking study rather than a core contribution on discrete token design or usage as required.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "How Redundant Is the Transformer Stack in Speech Representation Models?",
    "abstract": "Self-supervised speech representation models, particularly those leveraging transformer architectures, have demonstrated remarkable performance across various tasks such as speech recognition, speaker identification, and emotion detection. Recent studies on transformer models revealed a high redundancy between layers and the potential for significant pruning, which we will investigate here for transformer-based speech representation models. We perform a detailed analysis of layer similarity in speech representation models using three similarity metrics: cosine similarity, centered kernel alignment, and mutual nearest-neighbor alignment. Our findings reveal a block-like structure of high similarity, suggesting two main processing steps and significant redundancy of layers. We demonstrate the effectiveness of pruning transformer-based speech representation models without the need for post-training, achieving up to 40% reduction in transformer layers while maintaining over 95% of the model's predictive capacity. Furthermore, we employ a knowledge distillation method to substitute the entire transformer stack with mimicking layers, reducing the network size 95-98% and the inference time by up to 94%. This substantial decrease in computational load occurs without considerable performance loss, suggesting that the transformer stack is almost completely redundant for downstream applications of speech representation models.",
    "metadata": {
      "arxiv_id": "2409.16302",
      "title": "How Redundant Is the Transformer Stack in Speech Representation Models?",
      "summary": "Self-supervised speech representation models, particularly those leveraging transformer architectures, have demonstrated remarkable performance across various tasks such as speech recognition, speaker identification, and emotion detection. Recent studies on transformer models revealed a high redundancy between layers and the potential for significant pruning, which we will investigate here for transformer-based speech representation models. We perform a detailed analysis of layer similarity in speech representation models using three similarity metrics: cosine similarity, centered kernel alignment, and mutual nearest-neighbor alignment. Our findings reveal a block-like structure of high similarity, suggesting two main processing steps and significant redundancy of layers. We demonstrate the effectiveness of pruning transformer-based speech representation models without the need for post-training, achieving up to 40% reduction in transformer layers while maintaining over 95% of the model's predictive capacity. Furthermore, we employ a knowledge distillation method to substitute the entire transformer stack with mimicking layers, reducing the network size 95-98% and the inference time by up to 94%. This substantial decrease in computational load occurs without considerable performance loss, suggesting that the transformer stack is almost completely redundant for downstream applications of speech representation models.",
      "authors": [
        "Teresa Dorszewski",
        "Albert Kjøller Jacobsen",
        "Lenka Tětková",
        "Lars Kai Hansen"
      ],
      "published": "2024-09-10T11:00:24Z",
      "updated": "2025-01-17T12:27:40Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.16302v2",
      "landing_url": "https://arxiv.org/abs/2409.16302v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.16302"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item examines redundancy and pruning of transformer stacks in speech representation models; it does not involve discrete audio tokens, token generation/quantization, or token-based audio representations described in the criteria, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item examines redundancy and pruning of transformer stacks in speech representation models; it does not involve discrete audio tokens, token generation/quantization, or token-based audio representations described in the criteria, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on analyzing redundancy and pruning in transformer-based speech representation models but does not mention the creation, use, or design of discrete audio tokens via vector quantization or similar methods; thus, it does not meet the inclusion criteria centered on discrete token generation and use for sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on analyzing redundancy and pruning in transformer-based speech representation models but does not mention the creation, use, or design of discrete audio tokens via vector quantization or similar methods; thus, it does not meet the inclusion criteria centered on discrete token generation and use for sequence modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "XWSB: A Blend System Utilizing XLS-R and WavLM with SLS Classifier detection system for SVDD 2024 Challenge",
    "abstract": "This paper introduces the model structure used in the SVDD 2024 Challenge. The SVDD 2024 challenge has been introduced this year for the first time. Singing voice deepfake detection (SVDD) which faces complexities due to informal speech intonations and varying speech rates. In this paper, we propose the XWSB system, which achieved SOTA per-formance in the SVDD challenge. XWSB stands for XLS-R, WavLM, and SLS Blend, representing the integration of these technologies for the purpose of SVDD. Specifically, we used the best performing model structure XLS-R&SLS from the ASVspoof DF dataset, and applied SLS to WavLM to form the WavLM&SLS structure. Finally, we integrated two models to form the XWSB system. Experimental results show that our system demonstrates advanced recognition capabilities in the SVDD challenge, specifically achieving an EER of 2.32% in the CtrSVDD track. The code and data can be found at https://github.com/QiShanZhang/XWSB_for_ SVDD2024.",
    "metadata": {
      "arxiv_id": "2409.18558",
      "title": "XWSB: A Blend System Utilizing XLS-R and WavLM with SLS Classifier detection system for SVDD 2024 Challenge",
      "summary": "This paper introduces the model structure used in the SVDD 2024 Challenge. The SVDD 2024 challenge has been introduced this year for the first time. Singing voice deepfake detection (SVDD) which faces complexities due to informal speech intonations and varying speech rates. In this paper, we propose the XWSB system, which achieved SOTA per-formance in the SVDD challenge. XWSB stands for XLS-R, WavLM, and SLS Blend, representing the integration of these technologies for the purpose of SVDD. Specifically, we used the best performing model structure XLS-R&SLS from the ASVspoof DF dataset, and applied SLS to WavLM to form the WavLM&SLS structure. Finally, we integrated two models to form the XWSB system. Experimental results show that our system demonstrates advanced recognition capabilities in the SVDD challenge, specifically achieving an EER of 2.32% in the CtrSVDD track. The code and data can be found at https://github.com/QiShanZhang/XWSB_for_ SVDD2024.",
      "authors": [
        "Qishan Zhang",
        "Shuangbing Wen",
        "Fangke Yan",
        "Tao Hu",
        "Jun Li"
      ],
      "published": "2024-09-27T08:55:51Z",
      "updated": "2024-09-27T08:55:51Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.18558v1",
      "landing_url": "https://arxiv.org/abs/2409.18558v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.18558"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item does not discuss discrete audio tokens, token generation, or quantization; it's about combining XLS-R, WavLM, and SLS for singing-voice deepfake detection, lacking required tokenization research.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item does not discuss discrete audio tokens, token generation, or quantization; it's about combining XLS-R, WavLM, and SLS for singing-voice deepfake detection, lacking required tokenization research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a system for singing voice deepfake detection using models like XLS-R, WavLM, and an SLS classifier, but it does not mention any discrete audio tokens, their generation process, or any quantization/embedding mechanism characteristic of discrete token approaches. Thus, it does not meet the inclusion criteria regarding discrete audio token design or usage and lacks the methodological details related to token formation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a system for singing voice deepfake detection using models like XLS-R, WavLM, and an SLS classifier, but it does not mention any discrete audio tokens, their generation process, or any quantization/embedding mechanism characteristic of discrete token approaches. Thus, it does not meet the inclusion criteria regarding discrete audio token design or usage and lacks the methodological details related to token formation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Textless Streaming Speech-to-Speech Translation using Semantic Speech Tokens",
    "abstract": "Cascaded speech-to-speech translation systems often suffer from the error accumulation problem and high latency, which is a result of cascaded modules whose inference delays accumulate. In this paper, we propose a transducer-based speech translation model that outputs discrete speech tokens in a low-latency streaming fashion. This approach eliminates the need for generating text output first, followed by machine translation (MT) and text-to-speech (TTS) systems. The produced speech tokens can be directly used to generate a speech signal with low latency by utilizing an acoustic language model (LM) to obtain acoustic tokens and an audio codec model to retrieve the waveform. Experimental results show that the proposed method outperforms other existing approaches and achieves state-of-the-art results for streaming translation in terms of BLEU, average latency, and BLASER 2.0 scores for multiple language pairs using the CVSS-C dataset as a benchmark.",
    "metadata": {
      "arxiv_id": "2410.03298",
      "title": "Textless Streaming Speech-to-Speech Translation using Semantic Speech Tokens",
      "summary": "Cascaded speech-to-speech translation systems often suffer from the error accumulation problem and high latency, which is a result of cascaded modules whose inference delays accumulate. In this paper, we propose a transducer-based speech translation model that outputs discrete speech tokens in a low-latency streaming fashion. This approach eliminates the need for generating text output first, followed by machine translation (MT) and text-to-speech (TTS) systems. The produced speech tokens can be directly used to generate a speech signal with low latency by utilizing an acoustic language model (LM) to obtain acoustic tokens and an audio codec model to retrieve the waveform. Experimental results show that the proposed method outperforms other existing approaches and achieves state-of-the-art results for streaming translation in terms of BLEU, average latency, and BLASER 2.0 scores for multiple language pairs using the CVSS-C dataset as a benchmark.",
      "authors": [
        "Jinzheng Zhao",
        "Niko Moritz",
        "Egor Lakomkin",
        "Ruiming Xie",
        "Zhiping Xiu",
        "Katerina Zmolikova",
        "Zeeshan Ahmed",
        "Yashesh Gaur",
        "Duc Le",
        "Christian Fuegen"
      ],
      "published": "2024-10-04T10:21:15Z",
      "updated": "2024-10-04T10:21:15Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.03298v1",
      "landing_url": "https://arxiv.org/abs/2410.03298v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.03298"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe discrete speech tokens used in streaming speech-to-speech translation, aligning with the inclusion scope of discrete audio token representations; however, the source lacks a publication/update date (YYYY-MM-DD), triggering the exclusion criterion per rules, so the item should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe discrete speech tokens used in streaming speech-to-speech translation, aligning with the inclusion scope of discrete audio token representations; however, the source lacks a publication/update date (YYYY-MM-DD), triggering the exclusion criterion per rules, so the item should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a transducer-based speech translation model that outputs discrete speech tokens in a streaming fashion and uses these tokens for speech generation, aligning well with the topic of discrete audio tokens generated by learned or signal-processing methods for language/sequence model usage. The method clearly involves token generation used for sequence modeling and includes experimental validation, suggesting the presence of sufficient methodological details. Though the exact quantization or token generation details are not fully described in the abstract, the emphasis on semantic speech tokens and the use of an acoustic language model and codec for waveform retrieval supports the inclusion criteria focused on discrete token design and usage for downstream tasks beyond traditional codec purposes.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a transducer-based speech translation model that outputs discrete speech tokens in a streaming fashion and uses these tokens for speech generation, aligning well with the topic of discrete audio tokens generated by learned or signal-processing methods for language/sequence model usage. The method clearly involves token generation used for sequence modeling and includes experimental validation, suggesting the presence of sufficient methodological details. Though the exact quantization or token generation details are not fully described in the abstract, the emphasis on semantic speech tokens and the use of an acoustic language model and codec for waveform retrieval supports the inclusion criteria focused on discrete token design and usage for downstream tasks beyond traditional codec purposes.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": {
      "reasoning": "The abstract clearly describes a model that outputs discrete \"semantic speech tokens\" for streaming speech-to-speech translation, which aligns with the Discrete Audio Tokens topic (JuniorMini argued for inclusion based on method relevance). However, JuniorNano correctly flagged that the provided item lacks an explicit publication/update date (YYYY-MM-DD) in the input, and the exclusion criteria state that items without a clear source date must be excluded. Therefore, despite matching the topical requirements, this item must be excluded due to the missing date.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "The abstract clearly describes a model that outputs discrete \"semantic speech tokens\" for streaming speech-to-speech translation, which aligns with the Discrete Audio Tokens topic (JuniorMini argued for inclusion based on method relevance). However, JuniorNano correctly flagged that the provided item lacks an explicit publication/update date (YYYY-MM-DD) in the input, and the exclusion criteria state that items without a clear source date must be excluded. Therefore, despite matching the topical requirements, this item must be excluded due to the missing date.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "Restructuring Vector Quantization with the Rotation Trick",
    "abstract": "Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress a continuous input to a discrete latent space and reconstruct it with minimal distortion. They operate by maintaining a set of vectors -- often referred to as the codebook -- and quantizing each encoder output to the nearest vector in the codebook. However, as vector quantization is non-differentiable, the gradient to the encoder flows around the vector quantization layer rather than through it in a straight-through approximation. This approximation may be undesirable as all information from the vector quantization operation is lost. In this work, we propose a way to propagate gradients through the vector quantization layer of VQ-VAEs. We smoothly transform each encoder output into its corresponding codebook vector via a rotation and rescaling linear transformation that is treated as a constant during backpropagation. As a result, the relative magnitude and angle between encoder output and codebook vector becomes encoded into the gradient as it propagates through the vector quantization layer and back to the encoder. Across 11 different VQ-VAE training paradigms, we find this restructuring improves reconstruction metrics, codebook utilization, and quantization error. Our code is available at https://github.com/cfifty/rotation_trick.",
    "metadata": {
      "arxiv_id": "2410.06424",
      "title": "Restructuring Vector Quantization with the Rotation Trick",
      "summary": "Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress a continuous input to a discrete latent space and reconstruct it with minimal distortion. They operate by maintaining a set of vectors -- often referred to as the codebook -- and quantizing each encoder output to the nearest vector in the codebook. However, as vector quantization is non-differentiable, the gradient to the encoder flows around the vector quantization layer rather than through it in a straight-through approximation. This approximation may be undesirable as all information from the vector quantization operation is lost. In this work, we propose a way to propagate gradients through the vector quantization layer of VQ-VAEs. We smoothly transform each encoder output into its corresponding codebook vector via a rotation and rescaling linear transformation that is treated as a constant during backpropagation. As a result, the relative magnitude and angle between encoder output and codebook vector becomes encoded into the gradient as it propagates through the vector quantization layer and back to the encoder. Across 11 different VQ-VAE training paradigms, we find this restructuring improves reconstruction metrics, codebook utilization, and quantization error. Our code is available at https://github.com/cfifty/rotation_trick.",
      "authors": [
        "Christopher Fifty",
        "Ronald G. Junkins",
        "Dennis Duan",
        "Aniketh Iyengar",
        "Jerry W. Liu",
        "Ehsan Amid",
        "Sebastian Thrun",
        "Christopher Ré"
      ],
      "published": "2024-10-08T23:39:34Z",
      "updated": "2025-03-16T03:30:10Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06424v2",
      "landing_url": "https://arxiv.org/abs/2410.06424v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.06424"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work discusses gradient propagation through vector quantization in VQ-VAEs to obtain discrete latent representations, but it does not specify discrete audio tokens or an audio-domain tokenization pipeline for audio language modeling, so it likely does not meet the inclusion criteria focused on discrete audio tokens.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work discusses gradient propagation through vector quantization in VQ-VAEs to obtain discrete latent representations, but it does not specify discrete audio tokens or an audio-domain tokenization pipeline for audio language modeling, so it likely does not meet the inclusion criteria focused on discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on improving the vector quantization mechanism in VQ-VAEs, which is relevant to vector quantization methods used to generate discrete tokens; however, the context is general VQ-VAEs without specific emphasis on audio, speech, or music signals, nor does it clearly describe token generation for audio language modeling or detailed downstream usage. It lacks explicit information about discrete audio token generation or their usage in sequence/language models related to audio, which is a core inclusion criterion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The article focuses on improving the vector quantization mechanism in VQ-VAEs, which is relevant to vector quantization methods used to generate discrete tokens; however, the context is general VQ-VAEs without specific emphasis on audio, speech, or music signals, nor does it clearly describe token generation for audio language modeling or detailed downstream usage. It lacks explicit information about discrete audio token generation or their usage in sequence/language models related to audio, which is a core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer",
    "abstract": "Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0",
    "metadata": {
      "arxiv_id": "2410.08325",
      "title": "Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer",
      "summary": "Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0",
      "authors": [
        "Slava Shechtman",
        "Avihu Dekel"
      ],
      "published": "2024-10-10T19:29:05Z",
      "updated": "2024-10-10T19:29:05Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08325v1",
      "landing_url": "https://arxiv.org/abs/2410.08325v1",
      "doi": "https://doi.org/10.21437/Interspeech.2024-2366"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on discrete speech tokens produced by RVQGAN and discusses tokenization, bitrate, and release-ready methodology/data, aligning with the inclusion criteria for discrete audio token design and providing enough methodological context and access for reproducibility; there are no clear exclusion signals (e.g., only a demo).",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on discrete speech tokens produced by RVQGAN and discusses tokenization, bitrate, and release-ready methodology/data, aligning with the inclusion criteria for discrete audio token design and providing enough methodological context and access for reproducibility; there are no clear exclusion signals (e.g., only a demo).",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a discrete audio tokenizer based on RVQGAN, which clearly aligns with the theme of discrete audio tokens generated by vector quantization methods. It details token rates, data conditions, model fine-tuning, and provides public access to samples and model release, supporting reproducibility and detailed methodological description. It targets high-quality speech reconstruction with low bitrate discrete tokens, suitable for use in sequence modeling, fulfilling inclusion criteria. The abstract provides sufficient technical content with no exclusion criteria met, such as lack of methodological detail or being a traditional codec only.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a discrete audio tokenizer based on RVQGAN, which clearly aligns with the theme of discrete audio tokens generated by vector quantization methods. It details token rates, data conditions, model fine-tuning, and provides public access to samples and model release, supporting reproducibility and detailed methodological description. It targets high-quality speech reconstruction with low bitrate discrete tokens, suitable for use in sequence modeling, fulfilling inclusion criteria. The abstract provides sufficient technical content with no exclusion criteria met, such as lack of methodological detail or being a traditional codec only.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP",
    "abstract": "A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial role in translating textual input into an embedding space shared with images, thereby facilitating the interpretative analysis of vision tasks through natural language. Despite the varying significance of different textual elements within a sentence depending on the context, efforts to account for variation of importance in constructing text embeddings have been lacking. We propose a framework of Semantic Token Reweighting to build Interpretable text embeddings (SToRI), which incorporates controllability as well. SToRI refines the text encoding process in CLIP by differentially weighting semantic elements based on contextual importance, enabling finer control over emphasis responsive to data-driven insights and user preferences. The efficacy of SToRI is demonstrated through comprehensive experiments on few-shot image classification and image retrieval tailored to user preferences.",
    "metadata": {
      "arxiv_id": "2410.08469",
      "title": "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP",
      "summary": "A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial role in translating textual input into an embedding space shared with images, thereby facilitating the interpretative analysis of vision tasks through natural language. Despite the varying significance of different textual elements within a sentence depending on the context, efforts to account for variation of importance in constructing text embeddings have been lacking. We propose a framework of Semantic Token Reweighting to build Interpretable text embeddings (SToRI), which incorporates controllability as well. SToRI refines the text encoding process in CLIP by differentially weighting semantic elements based on contextual importance, enabling finer control over emphasis responsive to data-driven insights and user preferences. The efficacy of SToRI is demonstrated through comprehensive experiments on few-shot image classification and image retrieval tailored to user preferences.",
      "authors": [
        "Eunji Kim",
        "Kyuhong Shim",
        "Simyung Chang",
        "Sungroh Yoon"
      ],
      "published": "2024-10-11T02:42:13Z",
      "updated": "2024-10-16T14:09:14Z",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08469v2",
      "landing_url": "https://arxiv.org/abs/2410.08469v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.08469"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses semantic token weighting in CLIP text embeddings, not discrete audio token design or audio-language modeling, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses semantic token weighting in CLIP text embeddings, not discrete audio token design or audio-language modeling, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on text embeddings within Vision-Language Models (CLIP), proposing Semantic Token Reweighting for text, which does not involve discrete audio tokens, audio signal processing, or audio token generation as required by the inclusion criteria. Therefore, it does not meet the core topic of discrete audio tokens and related methods, and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on text embeddings within Vision-Language Models (CLIP), proposing Semantic Token Reweighting for text, which does not involve discrete audio tokens, audio signal processing, or audio token generation as required by the inclusion criteria. Therefore, it does not meet the core topic of discrete audio tokens and related methods, and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SNAC: Multi-Scale Neural Audio Codec",
    "abstract": "Neural audio codecs have recently gained popularity because they can represent audio signals with high fidelity at very low bitrates, making it feasible to use language modeling approaches for audio generation and understanding. Residual Vector Quantization (RVQ) has become the standard technique for neural audio compression using a cascade of VQ codebooks. This paper proposes the Multi-Scale Neural Audio Codec, a simple extension of RVQ where the quantizers can operate at different temporal resolutions. By applying a hierarchy of quantizers at variable frame rates, the codec adapts to the audio structure across multiple timescales. This leads to more efficient compression, as demonstrated by extensive objective and subjective evaluations. The code and model weights are open-sourced at https://github.com/hubertsiuzdak/snac.",
    "metadata": {
      "arxiv_id": "2410.14411",
      "title": "SNAC: Multi-Scale Neural Audio Codec",
      "summary": "Neural audio codecs have recently gained popularity because they can represent audio signals with high fidelity at very low bitrates, making it feasible to use language modeling approaches for audio generation and understanding. Residual Vector Quantization (RVQ) has become the standard technique for neural audio compression using a cascade of VQ codebooks. This paper proposes the Multi-Scale Neural Audio Codec, a simple extension of RVQ where the quantizers can operate at different temporal resolutions. By applying a hierarchy of quantizers at variable frame rates, the codec adapts to the audio structure across multiple timescales. This leads to more efficient compression, as demonstrated by extensive objective and subjective evaluations. The code and model weights are open-sourced at https://github.com/hubertsiuzdak/snac.",
      "authors": [
        "Hubert Siuzdak",
        "Florian Grötschla",
        "Luca A. Lanzendörfer"
      ],
      "published": "2024-10-18T12:24:05Z",
      "updated": "2024-10-18T12:24:05Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.14411v1",
      "landing_url": "https://arxiv.org/abs/2410.14411v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.14411"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on a neural audio codec built on RVQ with multi-scale, hierarchical quantizers that produce discrete codebook indices across timescales, i.e., discrete audio token–like representations suitable for language-model style processing, thus meeting the discrete-token design criterion while including methodological details.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The work centers on a neural audio codec built on RVQ with multi-scale, hierarchical quantizers that produce discrete codebook indices across timescales, i.e., discrete audio token–like representations suitable for language-model style processing, thus meeting the discrete-token design criterion while including methodological details.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a Multi-Scale Neural Audio Codec based on Residual Vector Quantization (RVQ) with a hierarchy of quantizers at different temporal resolutions. RVQ is explicitly mentioned as a vector quantization method that produces discrete tokens, and the codec design aims at efficient compression while preserving audio structure across timescales. The abstract highlights the use of neural audio codec techniques which inherently produce discrete tokens for modeling audio, consistent with the inclusion criteria requiring clear token generation and usage for audio sequence modeling or generation. The paper appears to provide technical method details and open-source code, ensuring reproducibility. There is no indication that it is only a traditional codec or lacks technical detail, nor any mention of missing publication date. Therefore, the study aligns well with the inclusion criteria and does not meet exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a Multi-Scale Neural Audio Codec based on Residual Vector Quantization (RVQ) with a hierarchy of quantizers at different temporal resolutions. RVQ is explicitly mentioned as a vector quantization method that produces discrete tokens, and the codec design aims at efficient compression while preserving audio structure across timescales. The abstract highlights the use of neural audio codec techniques which inherently produce discrete tokens for modeling audio, consistent with the inclusion criteria requiring clear token generation and usage for audio sequence modeling or generation. The paper appears to provide technical method details and open-source code, ensuring reproducibility. There is no indication that it is only a traditional codec or lacks technical detail, nor any mention of missing publication date. Therefore, the study aligns well with the inclusion criteria and does not meet exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DM-Codec: Distilling Multimodal Representations for Speech Tokenization",
    "abstract": "Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. Code, samples, and checkpoints are available at https://github.com/mubtasimahasan/DM-Codec.",
    "metadata": {
      "arxiv_id": "2410.15017",
      "title": "DM-Codec: Distilling Multimodal Representations for Speech Tokenization",
      "summary": "Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. Code, samples, and checkpoints are available at https://github.com/mubtasimahasan/DM-Codec.",
      "authors": [
        "Md Mubtasim Ahasan",
        "Md Fahim",
        "Tasnim Mohiuddin",
        "A K M Mahbubur Rahman",
        "Aman Chadha",
        "Tariq Iqbal",
        "M Ashraful Amin",
        "Md Mofijul Islam",
        "Amin Ahsan Ali"
      ],
      "published": "2024-10-19T07:14:14Z",
      "updated": "2025-09-29T08:08:40Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15017v2",
      "landing_url": "https://arxiv.org/abs/2410.15017v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.15017"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item presents a discrete audio tokenization approach (DM-Codec) with Residual Vector Quantizer, multimodal (acoustic/semantic/contextual) distillation, and a clear description of token generation Workflow, training details, and evaluation on LibriSpeech, satisfying the inclusion criteria for Discrete Audio Tokens and not triggering the exclusion conditions (it's not just a demo, and it provides methodological detail). The lack of an explicit publication date in the abstract is not sufficient to discount its alignment with the inclusion criteria, given the methodological content and availability of code and results.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item presents a discrete audio tokenization approach (DM-Codec) with Residual Vector Quantizer, multimodal (acoustic/semantic/contextual) distillation, and a clear description of token generation Workflow, training details, and evaluation on LibriSpeech, satisfying the inclusion criteria for Discrete Audio Tokens and not triggering the exclusion conditions (it's not just a demo, and it provides methodological detail). The lack of an explicit publication date in the abstract is not sufficient to discount its alignment with the inclusion criteria, given the methodological content and availability of code and results.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes DM-Codec, a model that distills multimodal (acoustic, semantic, and contextual) representations into a speech tokenizer using Residual Vector Quantization (RVQ), which aligns with the discrete audio token definition involving VQ/RVQ methods to obtain discrete tokens suitable for sequence/language modeling. The study clearly presents the token generation process and demonstrates improved performance on speech tokenization tasks. Method details, evaluation, and code availability support reproducibility and technical validity. It meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper proposes DM-Codec, a model that distills multimodal (acoustic, semantic, and contextual) representations into a speech tokenizer using Residual Vector Quantization (RVQ), which aligns with the discrete audio token definition involving VQ/RVQ methods to obtain discrete tokens suitable for sequence/language modeling. The study clearly presents the token generation process and demonstrates improved performance on speech tokenization tasks. Method details, evaluation, and code availability support reproducibility and technical validity. It meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec",
    "abstract": "Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework.",
    "metadata": {
      "arxiv_id": "2410.15764",
      "title": "LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec",
      "summary": "Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework.",
      "authors": [
        "Yiwei Guo",
        "Zhihan Li",
        "Chenpeng Du",
        "Hankun Wang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-10-21T08:23:31Z",
      "updated": "2025-05-21T16:46:32Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15764v3",
      "landing_url": "https://arxiv.org/abs/2410.15764v3",
      "doi": "https://doi.org/10.48550/arXiv.2410.15764"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper describes LSCodec, a low-bitrate discrete speech codec that uses vector quantization to produce a discrete, speaker-decoupled token space and includes a token vocoder, with a training framework and experiments validating intelligibility and speaker disentanglement, aligning with the discrete audio tokens theme and providing methodological detail beyond a mere demo.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper describes LSCodec, a low-bitrate discrete speech codec that uses vector quantization to produce a discrete, speaker-decoupled token space and includes a token vocoder, with a training framework and experiments validating intelligibility and speaker disentanglement, aligning with the discrete audio tokens theme and providing methodological detail beyond a mere demo.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents LSCodec, a discrete speech codec focusing on low bitrate and speaker decoupling, employing vector quantization and a discrete token vocoder to generate discrete speaker-decoupled tokens which are intended to improve speech reconstruction and speaker disentanglement; it clearly explains token generation and is aligned with discrete audio token design for downstream sequence modeling, fulfilling the main inclusion criteria without falling into the exclusion criteria regarding traditional codec or lack of methodological details.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper presents LSCodec, a discrete speech codec focusing on low bitrate and speaker decoupling, employing vector quantization and a discrete token vocoder to generate discrete speaker-decoupled tokens which are intended to improve speech reconstruction and speaker disentanglement; it clearly explains token generation and is aligned with discrete audio token design for downstream sequence modeling, fulfilling the main inclusion criteria without falling into the exclusion criteria regarding traditional codec or lack of methodological details.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Continuous Speech Tokenizer in Text To Speech",
    "abstract": "The fusion of speech and language in the era of large language models has garnered significant attention. Discrete speech token is often utilized in text-to-speech tasks for speech compression and portability, which is convenient for joint training with text and have good compression efficiency. However, we found that the discrete speech tokenizer still suffers from information loss. Therefore, we propose a simple yet effective continuous speech tokenizer named Cont-SPT, and a text-to-speech model based on continuous speech tokens. Our results show that the speech language model based on the continuous speech tokenizer has better continuity and higher estimated Mean Opinion Scores (MoS). This enhancement is attributed to better information preservation rate of the continuous speech tokenizer across both low and high frequencies in the frequency domain. The code and resources for Cont-SPT can be found in https://github.com/Yixing-Li/Continuous-Speech-Tokenizer",
    "metadata": {
      "arxiv_id": "2410.17081",
      "title": "Continuous Speech Tokenizer in Text To Speech",
      "summary": "The fusion of speech and language in the era of large language models has garnered significant attention. Discrete speech token is often utilized in text-to-speech tasks for speech compression and portability, which is convenient for joint training with text and have good compression efficiency. However, we found that the discrete speech tokenizer still suffers from information loss. Therefore, we propose a simple yet effective continuous speech tokenizer named Cont-SPT, and a text-to-speech model based on continuous speech tokens. Our results show that the speech language model based on the continuous speech tokenizer has better continuity and higher estimated Mean Opinion Scores (MoS). This enhancement is attributed to better information preservation rate of the continuous speech tokenizer across both low and high frequencies in the frequency domain. The code and resources for Cont-SPT can be found in https://github.com/Yixing-Li/Continuous-Speech-Tokenizer",
      "authors": [
        "Yixing Li",
        "Ruobing Xie",
        "Xingwu Sun",
        "Yu Cheng",
        "Zhanhui Kang"
      ],
      "published": "2024-10-22T15:02:37Z",
      "updated": "2025-03-31T13:57:49Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17081v2",
      "landing_url": "https://arxiv.org/abs/2410.17081v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.17081"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract presents a continuous speech tokenizer and a TTS model, arguing advantages over discrete tokens; it does not provide/discuss discrete audio token generation/quantization methods or token codebooks as required by the inclusion criteria, so it does not meet the core criterion of discrete token design.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract presents a continuous speech tokenizer and a TTS model, arguing advantages over discrete tokens; it does not provide/discuss discrete audio token generation/quantization methods or token codebooks as required by the inclusion criteria, so it does not meet the core criterion of discrete token design.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a continuous speech tokenizer as an alternative to discrete speech tokenizers used in text-to-speech systems, highlighting improvements in information preservation and continuity; however, the focus is explicitly on continuous tokens rather than discrete tokenization processes that form discrete audio tokens as defined. The abstract does not provide details on discrete token generation, vector quantization, or discrete token design, which are essential per the inclusion criteria, nor does it clearly explain token generation flow for discrete tokens or their use for sequence/language models. Therefore, it does not meet the strict inclusion criteria centered on discrete audio token design and usage.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a continuous speech tokenizer as an alternative to discrete speech tokenizers used in text-to-speech systems, highlighting improvements in information preservation and continuity; however, the focus is explicitly on continuous tokens rather than discrete tokenization processes that form discrete audio tokens as defined. The abstract does not provide details on discrete token generation, vector quantization, or discrete token design, which are essential per the inclusion criteria, nor does it clearly explain token generation flow for discrete tokens or their use for sequence/language models. Therefore, it does not meet the strict inclusion criteria centered on discrete audio token design and usage.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "APCodec+: A Spectrum-Coding-Based High-Fidelity and High-Compression-Rate Neural Audio Codec with Staged Training Paradigm",
    "abstract": "This paper proposes a novel neural audio codec, named APCodec+, which is an improved version of APCodec. The APCodec+ takes the audio amplitude and phase spectra as the coding object, and employs an adversarial training strategy. Innovatively, we propose a two-stage joint-individual training paradigm for APCodec+. In the joint training stage, the encoder, quantizer, decoder and discriminator are jointly trained with complete spectral loss, quantization loss, and adversarial loss. In the individual training stage, the encoder and quantizer fix their parameters and provide high-quality training data for the decoder and discriminator. The decoder and discriminator are individually trained from scratch without the quantization loss. The purpose of introducing individual training is to reduce the learning difficulty of the decoder, thereby further improving the fidelity of the decoded audio. Experimental results confirm that our proposed APCodec+ at low bitrates achieves comparable performance with baseline codecs at higher bitrates, thanks to the proposed staged training paradigm.",
    "metadata": {
      "arxiv_id": "2410.22807",
      "title": "APCodec+: A Spectrum-Coding-Based High-Fidelity and High-Compression-Rate Neural Audio Codec with Staged Training Paradigm",
      "summary": "This paper proposes a novel neural audio codec, named APCodec+, which is an improved version of APCodec. The APCodec+ takes the audio amplitude and phase spectra as the coding object, and employs an adversarial training strategy. Innovatively, we propose a two-stage joint-individual training paradigm for APCodec+. In the joint training stage, the encoder, quantizer, decoder and discriminator are jointly trained with complete spectral loss, quantization loss, and adversarial loss. In the individual training stage, the encoder and quantizer fix their parameters and provide high-quality training data for the decoder and discriminator. The decoder and discriminator are individually trained from scratch without the quantization loss. The purpose of introducing individual training is to reduce the learning difficulty of the decoder, thereby further improving the fidelity of the decoded audio. Experimental results confirm that our proposed APCodec+ at low bitrates achieves comparable performance with baseline codecs at higher bitrates, thanks to the proposed staged training paradigm.",
      "authors": [
        "Hui-Peng Du",
        "Yang Ai",
        "Rui-Chen Zheng",
        "Zhen-Hua Ling"
      ],
      "published": "2024-10-30T08:36:17Z",
      "updated": "2024-10-30T08:36:17Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.22807v1",
      "landing_url": "https://arxiv.org/abs/2410.22807v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.22807"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper presents a spectrum-coding-based neural audio codec with staged training, but does not describe discrete audio token design (e.g., VQ/RVQ or hierarchical quantization) or token-based downstream modeling, so it fails the inclusion criteria that require discrete audio tokens as the core contribution.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "This paper presents a spectrum-coding-based neural audio codec with staged training, but does not describe discrete audio token design (e.g., VQ/RVQ or hierarchical quantization) or token-based downstream modeling, so it fails the inclusion criteria that require discrete audio tokens as the core contribution.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a neural audio codec (APCodec+) focusing on amplitude and phase spectra coding and a two-stage training paradigm to improve audio fidelity at low bitrates. However, it does not explicitly mention the production or use of discrete audio tokens derived via vector quantization or similar discretization mechanisms intended for language or sequence modeling, nor does it clarify token formation or usage towards downstream generative or understanding tasks. Therefore, it does not satisfy the inclusion criterion of having discrete token design as a core contribution nor detailed token generation flow.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study presents a neural audio codec (APCodec+) focusing on amplitude and phase spectra coding and a two-stage training paradigm to improve audio fidelity at low bitrates. However, it does not explicitly mention the production or use of discrete audio tokens derived via vector quantization or similar discretization mechanisms intended for language or sequence modeling, nor does it clarify token formation or usage towards downstream generative or understanding tasks. Therefore, it does not satisfy the inclusion criterion of having discrete token design as a core contribution nor detailed token generation flow.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models",
    "abstract": "Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",
    "metadata": {
      "arxiv_id": "2410.24177",
      "title": "DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models",
      "summary": "Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",
      "authors": [
        "Heng-Jui Chang",
        "Hongyu Gong",
        "Changhan Wang",
        "James Glass",
        "Yu-An Chung"
      ],
      "published": "2024-10-31T17:43:13Z",
      "updated": "2024-10-31T17:43:13Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.24177v1",
      "landing_url": "https://arxiv.org/abs/2410.24177v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.24177"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work presents a discrete audio tokenization approach (DC-Spin) that yields speaker-invariant, phoneme-aligned tokens and describes a token generation/quantization method with streaming capability and downstream SLM applicability, aligning with inclusion criteria and not matching any exclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The work presents a discrete audio tokenization approach (DC-Spin) that yields speaker-invariant, phoneme-aligned tokens and describes a token generation/quantization method with streaming capability and downstream SLM applicability, aligning with inclusion criteria and not matching any exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes a method (DC-Spin) for producing speaker-invariant speech tokens that are rich in phonetic information and designed for spoken language models, indicating a discrete tokenization approach bridging audio signals and language models. It mentions clustering, preserving phonetic components, resilience to input variations, and evaluations comparing tokenization methods including self-supervised and neural audio codecs, aligning with the inclusion criteria of discrete audio tokens used for sequence modeling and language modeling. Furthermore, the abstract clearly addresses token production pipeline and downstream tasks, not merely codec compression. There is no indication of exclusion criteria like lack of methodological details or mere demonstration content. Thus, the study meets inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The abstract describes a method (DC-Spin) for producing speaker-invariant speech tokens that are rich in phonetic information and designed for spoken language models, indicating a discrete tokenization approach bridging audio signals and language models. It mentions clustering, preserving phonetic components, resilience to input variations, and evaluations comparing tokenization methods including self-supervised and neural audio codecs, aligning with the inclusion criteria of discrete audio tokens used for sequence modeling and language modeling. Furthermore, the abstract clearly addresses token production pipeline and downstream tasks, not merely codec compression. There is no indication of exclusion criteria like lack of methodological details or mere demonstration content. Thus, the study meets inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Reducing Data Fragmentation in Data Deduplication Systems via Partial Repetition and Coding",
    "abstract": "Data deduplication, one of the key features of modern Big Data storage devices, is the process of removing replicas of data chunks stored by different users. Despite the importance of deduplication, several drawbacks of the method, such as storage robustness and file fragmentation, have not been previously analyzed from a theoretical point of view. Storage robustness pertains to ensuring that deduplicated data can be used to reconstruct the original files without service disruptions and data loss. Fragmentation pertains to the problems of placing deduplicated data chunks of different user files in a proximity-preserving linear order, since neighboring chunks of the same file may be stored in sectors far apart on the server. This work proposes a new theoretical model for data fragmentation and introduces novel graph- and coding-theoretic approaches for reducing fragmentation via limited duplication (repetition coding) and coded deduplication (e.g., linear coding). In addition to alleviating issues with fragmentation, limited duplication and coded deduplication can also serve the dual purpose of increasing the robusteness of the system design. The contributions of our work are three-fold. First, we describe a new model for file structures in the form of self-avoiding (simple) paths in specialized graphs. Second, we introduce several new metrics for measuring the fragmentation level in deduplication systems on graph-structured files, including the stretch metric that captures the worst-case \"spread\" of adjacent data chunks within a file when deduplicated and placed on the server; and, the jump metric that captures the worst-case number of times during the reconstruction process of a file that one has to change the readout location on the server. For the stretch metric, we establish a connection between the level of fragmentation and the bandwidth of the file-graph. In particular, ...",
    "metadata": {
      "arxiv_id": "2411.01407",
      "title": "Reducing Data Fragmentation in Data Deduplication Systems via Partial Repetition and Coding",
      "summary": "Data deduplication, one of the key features of modern Big Data storage devices, is the process of removing replicas of data chunks stored by different users. Despite the importance of deduplication, several drawbacks of the method, such as storage robustness and file fragmentation, have not been previously analyzed from a theoretical point of view. Storage robustness pertains to ensuring that deduplicated data can be used to reconstruct the original files without service disruptions and data loss. Fragmentation pertains to the problems of placing deduplicated data chunks of different user files in a proximity-preserving linear order, since neighboring chunks of the same file may be stored in sectors far apart on the server. This work proposes a new theoretical model for data fragmentation and introduces novel graph- and coding-theoretic approaches for reducing fragmentation via limited duplication (repetition coding) and coded deduplication (e.g., linear coding). In addition to alleviating issues with fragmentation, limited duplication and coded deduplication can also serve the dual purpose of increasing the robusteness of the system design. The contributions of our work are three-fold. First, we describe a new model for file structures in the form of self-avoiding (simple) paths in specialized graphs. Second, we introduce several new metrics for measuring the fragmentation level in deduplication systems on graph-structured files, including the stretch metric that captures the worst-case \"spread\" of adjacent data chunks within a file when deduplicated and placed on the server; and, the jump metric that captures the worst-case number of times during the reconstruction process of a file that one has to change the readout location on the server. For the stretch metric, we establish a connection between the level of fragmentation and the bandwidth of the file-graph. In particular, ...",
      "authors": [
        "Yun-Han Li",
        "Jin Sima",
        "Ilan Shomorony",
        "Olgica Milenkovic"
      ],
      "published": "2024-11-03T02:14:03Z",
      "updated": "2024-11-03T02:14:03Z",
      "categories": [
        "cs.IT",
        "cs.DM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.01407v1",
      "landing_url": "https://arxiv.org/abs/2411.01407v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.01407"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract discuss data deduplication fragmentation and coding theory in storage systems, which does not address Discrete Audio Tokens or the specified token generation/quantization methods, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract discuss data deduplication fragmentation and coding theory in storage systems, which does not address Discrete Audio Tokens or the specified token generation/quantization methods, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on data deduplication and fragmentation reduction methods in big data storage systems through coding-theoretic approaches; there is no mention of discrete audio tokens, vector quantization, audio signal processing, or any form of audio language modeling or tokenization. Thus, this study does not address any aspect of discrete audio token design or usage as specified in the inclusion criteria, and lacks relevant methodology or token generation details related to audio processing.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on data deduplication and fragmentation reduction methods in big data storage systems through coding-theoretic approaches; there is no mention of discrete audio tokens, vector quantization, audio signal processing, or any form of audio language modeling or tokenization. Thus, this study does not address any aspect of discrete audio token design or usage as specified in the inclusion criteria, and lacks relevant methodology or token generation details related to audio processing.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LSHBloom: Memory-efficient, Extreme-scale Document Deduplication",
    "abstract": "Contemporary large language model (LLM) training pipelines require the assembly of internet-scale databases full of text data from a variety of sources (e.g., web, academic, and publishers). Preprocessing these datasets via deduplication -- detecting and eliminating additional instances of the same content -- is a major focus for assembling and curating training datasets for LLMs. Unrestrained, duplicates in the training dataset increase training costs and lead to undesirable properties such as memorization in trained models or cheating on evaluation. Unfortunately, contemporary approaches to document-level deduplication are either unreliable at accurately identifying duplicate documents or extremely expensive in terms of both runtime and memory. We propose LSHBloom, an extension to MinhashLSH, which replaces the expensive LSHIndex with lightweight Bloom filters. LSHBloom demonstrates the same state-of-the-art deduplication performance as MinhashLSH, with only a marginal increase in false positives (near zero in our experiments), while boasting competitive runtime (12$\\times$ faster than MinhashLSH on peS2o) and, crucially, using 18$\\times$ less disk space than MinhashLSH (as measured on peS2o). Based on extrapolation, we show that this advantage in space and runtime remains even at the extreme scale of several billion documents. LSHBloom allows practitioners to access the deduplication quality of MinHashLSH at scales that are normally only tractable for less sophisticated, heuristic solutions. As a result, LSHBloom promises to enable scaling high-quality document deduplication to internet-scale text datasets.",
    "metadata": {
      "arxiv_id": "2411.04257",
      "title": "LSHBloom: Memory-efficient, Extreme-scale Document Deduplication",
      "summary": "Contemporary large language model (LLM) training pipelines require the assembly of internet-scale databases full of text data from a variety of sources (e.g., web, academic, and publishers). Preprocessing these datasets via deduplication -- detecting and eliminating additional instances of the same content -- is a major focus for assembling and curating training datasets for LLMs. Unrestrained, duplicates in the training dataset increase training costs and lead to undesirable properties such as memorization in trained models or cheating on evaluation. Unfortunately, contemporary approaches to document-level deduplication are either unreliable at accurately identifying duplicate documents or extremely expensive in terms of both runtime and memory. We propose LSHBloom, an extension to MinhashLSH, which replaces the expensive LSHIndex with lightweight Bloom filters. LSHBloom demonstrates the same state-of-the-art deduplication performance as MinhashLSH, with only a marginal increase in false positives (near zero in our experiments), while boasting competitive runtime (12$\\times$ faster than MinhashLSH on peS2o) and, crucially, using 18$\\times$ less disk space than MinhashLSH (as measured on peS2o). Based on extrapolation, we show that this advantage in space and runtime remains even at the extreme scale of several billion documents. LSHBloom allows practitioners to access the deduplication quality of MinHashLSH at scales that are normally only tractable for less sophisticated, heuristic solutions. As a result, LSHBloom promises to enable scaling high-quality document deduplication to internet-scale text datasets.",
      "authors": [
        "Arham Khan",
        "Robert Underwood",
        "Carlo Siebenschuh",
        "Yadu Babuji",
        "Aswathy Ajith",
        "Kyle Hippe",
        "Ozan Gokdemir",
        "Alexander Brace",
        "Kyle Chard",
        "Ian Foster"
      ],
      "published": "2024-11-06T21:00:45Z",
      "updated": "2025-12-02T12:52:27Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04257v3",
      "landing_url": "https://arxiv.org/abs/2411.04257v3",
      "doi": "https://doi.org/10.48550/arXiv.2411.04257"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article addresses memory-efficient, extreme-scale document deduplication for text data, not discrete audio token representations or audio-token design, so it does not meet the inclusion criteria and fails the topic relevance requirement.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article addresses memory-efficient, extreme-scale document deduplication for text data, not discrete audio token representations or audio-token design, so it does not meet the inclusion criteria and fails the topic relevance requirement.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article discusses a method for memory-efficient document deduplication in large-scale text datasets, which does not focus on discrete audio tokens or the transformation of continuous audio signals into discrete tokens for language or sequence modeling. It lacks a description of token generation processes related to audio representations and does not address the core inclusion criteria concerning discrete audio tokens, vector quantization, or audio language modeling. Therefore, it does not meet the inclusion criteria and falls outside the scope of the requested topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article discusses a method for memory-efficient document deduplication in large-scale text datasets, which does not focus on discrete audio tokens or the transformation of continuous audio signals into discrete tokens for language or sequence modeling. It lacks a description of token generation processes related to audio representations and does not address the core inclusion criteria concerning discrete audio tokens, vector quantization, or audio language modeling. Therefore, it does not meet the inclusion criteria and falls outside the scope of the requested topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Flow Matching Posterior Sampling: A Training-free Conditional Generation for Flow Matching",
    "abstract": "Training-free conditional generation based on flow matching aims to leverage pre-trained unconditional flow matching models to perform conditional generation without retraining. Recently, a successful training-free conditional generation approach incorporates conditions via posterior sampling, which relies on the availability of a score function in the unconditional diffusion model. However, flow matching models do not possess an explicit score function, rendering such a strategy inapplicable. Approximate posterior sampling for flow matching has been explored, but it is limited to linear inverse problems. In this paper, we propose Flow Matching-based Posterior Sampling (FMPS) to expand its application scope. We introduce a correction term by steering the velocity field. This correction term can be reformulated to incorporate a surrogate score function, thereby bridging the gap between flow matching models and score-based posterior sampling. Hence, FMPS enables the posterior sampling to be adjusted within the flow matching framework. Further, we propose two practical implementations of the correction mechanism: one aimed at improving generation quality, and the other focused on computational efficiency. Experimental results on diverse conditional generation tasks demonstrate that our method achieves superior generation quality compared to existing state-of-the-art approaches, validating the effectiveness and generality of FMPS.",
    "metadata": {
      "arxiv_id": "2411.07625",
      "title": "Flow Matching Posterior Sampling: A Training-free Conditional Generation for Flow Matching",
      "summary": "Training-free conditional generation based on flow matching aims to leverage pre-trained unconditional flow matching models to perform conditional generation without retraining. Recently, a successful training-free conditional generation approach incorporates conditions via posterior sampling, which relies on the availability of a score function in the unconditional diffusion model. However, flow matching models do not possess an explicit score function, rendering such a strategy inapplicable. Approximate posterior sampling for flow matching has been explored, but it is limited to linear inverse problems. In this paper, we propose Flow Matching-based Posterior Sampling (FMPS) to expand its application scope. We introduce a correction term by steering the velocity field. This correction term can be reformulated to incorporate a surrogate score function, thereby bridging the gap between flow matching models and score-based posterior sampling. Hence, FMPS enables the posterior sampling to be adjusted within the flow matching framework. Further, we propose two practical implementations of the correction mechanism: one aimed at improving generation quality, and the other focused on computational efficiency. Experimental results on diverse conditional generation tasks demonstrate that our method achieves superior generation quality compared to existing state-of-the-art approaches, validating the effectiveness and generality of FMPS.",
      "authors": [
        "Kaiyu Song",
        "Hanjiang Lai",
        "Yan Pan",
        "Kun Yue",
        "Jian yin"
      ],
      "published": "2024-11-12T08:14:39Z",
      "updated": "2025-08-09T08:22:53Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.07625v3",
      "landing_url": "https://arxiv.org/abs/2411.07625v3",
      "doi": "https://doi.org/10.48550/arXiv.2411.07625"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discusses Flow Matching Posterior Sampling for training-free conditional generation in flow-based models and does not address discrete audio tokens or tokenization as defined in the inclusion criteria, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper discusses Flow Matching Posterior Sampling for training-free conditional generation in flow-based models and does not address discrete audio tokens or tokenization as defined in the inclusion criteria, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on flow matching methods for conditional generation in machine learning, discussing posterior sampling and velocity field correction for improved generation quality, but it does not mention or focus on discrete audio tokens, vector quantization, or audio-related token generation which are core requirements based on the inclusion criteria; thus, it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on flow matching methods for conditional generation in machine learning, discussing posterior sampling and velocity field correction for improved generation quality, but it does not mention or focus on discrete audio tokens, vector quantization, or audio-related token generation which are core requirements based on the inclusion criteria; thus, it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models",
    "abstract": "With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs.",
    "metadata": {
      "arxiv_id": "2411.08742",
      "title": "A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models",
      "summary": "With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs.",
      "authors": [
        "Dingdong Wang",
        "Mingyu Cui",
        "Dongchao Yang",
        "Xueyuan Chen",
        "Helen Meng"
      ],
      "published": "2024-11-13T16:20:20Z",
      "updated": "2024-11-13T16:20:20Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.08742v1",
      "landing_url": "https://arxiv.org/abs/2411.08742v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.08742"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses discrete speech tokens and semantic tasks, aligning with the topic of discrete audio tokens, but the source lacks a clear publication/update date (YYYY-MM-DD), which is required to satisfy the timestamp-based exclusion criterion; thus it should be excluded despite topic relevance.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses discrete speech tokens and semantic tasks, aligning with the topic of discrete audio tokens, but the source lacks a clear publication/update date (YYYY-MM-DD), which is required to satisfy the timestamp-based exclusion criterion; thus it should be excluded despite topic relevance.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on discrete speech tokens generated through learned or signal-processing methods, and clearly compares discrete tokens with continuous features in semantic-related tasks using speech LLMs. It identifies token granularity and information retention as key factors, implying analysis of token formation and use in sequence modeling. Although the abstract does not detail token generation processes thoroughly, it indicates that discrete tokens are central to the research contribution and that they are used for language model understanding tasks. The study aligns well with the inclusion criteria regarding discrete audio token design and application for semantic understanding, rather than traditional codec use. However, explicit details about token quantization methods or architecture are not fully specified in the abstract, leaving some uncertainty about reproducibility or exact token formation mechanisms. Given the thematic match and focus on discrete tokens in speech LLMs, inclusion is justified with minor reservation.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The study focuses on discrete speech tokens generated through learned or signal-processing methods, and clearly compares discrete tokens with continuous features in semantic-related tasks using speech LLMs. It identifies token granularity and information retention as key factors, implying analysis of token formation and use in sequence modeling. Although the abstract does not detail token generation processes thoroughly, it indicates that discrete tokens are central to the research contribution and that they are used for language model understanding tasks. The study aligns well with the inclusion criteria regarding discrete audio token design and application for semantic understanding, rather than traditional codec use. However, explicit details about token quantization methods or architecture are not fully specified in the abstract, leaving some uncertainty about reproducibility or exact token formation mechanisms. Given the thematic match and focus on discrete tokens in speech LLMs, inclusion is justified with minor reservation.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers’ comments considered: JuniorNano excluded due to absence of a clear publication/update date, while JuniorMini favored inclusion based on topical fit but noted missing methodological specifics. The abstract indeed studies discrete speech tokens and compares them to continuous features, but it does not provide the required confirmation of detailed token-generation/quantization methods or reproducible technical content, nor is a publication date provided in the item; because the inclusion rules require BOTH clear methodological details (or an available full-text with reproducible method description) and a verifiable publication date, this item must be excluded.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers’ comments considered: JuniorNano excluded due to absence of a clear publication/update date, while JuniorMini favored inclusion based on topical fit but noted missing methodological specifics. The abstract indeed studies discrete speech tokens and compares them to continuous features, but it does not provide the required confirmation of detailed token-generation/quantization methods or reproducible technical content, nor is a publication date provided in the item; because the inclusion rules require BOTH clear methodological details (or an available full-text with reproducible method description) and a verifiable publication date, this item must be excluded.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "Zero-shot Voice Conversion with Diffusion Transformers",
    "abstract": "Zero-shot voice conversion aims to transform a source speech utterance to match the timbre of a reference speech from an unseen speaker. Traditional approaches struggle with timbre leakage, insufficient timbre representation, and mismatches between training and inference tasks. We propose Seed-VC, a novel framework that addresses these issues by introducing an external timbre shifter during training to perturb the source speech timbre, mitigating leakage and aligning training with inference. Additionally, we employ a diffusion transformer that leverages the entire reference speech context, capturing fine-grained timbre features through in-context learning. Experiments demonstrate that Seed-VC outperforms strong baselines like OpenVoice and CosyVoice, achieving higher speaker similarity and lower word error rates in zero-shot voice conversion tasks. We further extend our approach to zero-shot singing voice conversion by incorporating fundamental frequency (F0) conditioning, resulting in comparative performance to current state-of-the-art methods. Our findings highlight the effectiveness of Seed-VC in overcoming core challenges, paving the way for more accurate and versatile voice conversion systems.",
    "metadata": {
      "arxiv_id": "2411.09943",
      "title": "Zero-shot Voice Conversion with Diffusion Transformers",
      "summary": "Zero-shot voice conversion aims to transform a source speech utterance to match the timbre of a reference speech from an unseen speaker. Traditional approaches struggle with timbre leakage, insufficient timbre representation, and mismatches between training and inference tasks. We propose Seed-VC, a novel framework that addresses these issues by introducing an external timbre shifter during training to perturb the source speech timbre, mitigating leakage and aligning training with inference. Additionally, we employ a diffusion transformer that leverages the entire reference speech context, capturing fine-grained timbre features through in-context learning. Experiments demonstrate that Seed-VC outperforms strong baselines like OpenVoice and CosyVoice, achieving higher speaker similarity and lower word error rates in zero-shot voice conversion tasks. We further extend our approach to zero-shot singing voice conversion by incorporating fundamental frequency (F0) conditioning, resulting in comparative performance to current state-of-the-art methods. Our findings highlight the effectiveness of Seed-VC in overcoming core challenges, paving the way for more accurate and versatile voice conversion systems.",
      "authors": [
        "Songting Liu"
      ],
      "published": "2024-11-15T04:43:44Z",
      "updated": "2024-11-15T04:43:44Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.09943v1",
      "landing_url": "https://arxiv.org/abs/2411.09943v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.09943"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper addresses zero-shot voice conversion with diffusion transformers and timbre transfer, but does not discuss discrete audio tokens or tokenization, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper addresses zero-shot voice conversion with diffusion transformers and timbre transfer, but does not discuss discrete audio tokens or tokenization, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on zero-shot voice conversion using a diffusion transformer and external timbre shifter but does not mention the generation or use of discrete audio tokens, vector quantization, or related discrete representation methods that can be processed by language models or sequence models as per the inclusion criteria. The abstract lacks any indication of token formation processes, quantization techniques, or discrete token utilization in modeling or generation tasks, which are core requirements. Therefore, it does not meet the core thematic requirement of discrete audio tokens as a primary contribution nor does it discuss token generation or usage details, making it unsuitable for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on zero-shot voice conversion using a diffusion transformer and external timbre shifter but does not mention the generation or use of discrete audio tokens, vector quantization, or related discrete representation methods that can be processed by language models or sequence models as per the inclusion criteria. The abstract lacks any indication of token formation processes, quantization techniques, or discrete token utilization in modeling or generation tasks, which are core requirements. Therefore, it does not meet the core thematic requirement of discrete audio tokens as a primary contribution nor does it discuss token generation or usage details, making it unsuitable for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken Term Detection",
    "abstract": "Spoken term detection (STD) is often hindered by reliance on frame-level features and the computationally intensive DTW-based template matching, limiting its practicality. To address these challenges, we propose a novel approach that encodes speech into discrete, speaker-agnostic semantic tokens. This facilitates fast retrieval using text-based search algorithms and effectively handles out-of-vocabulary terms. Our approach focuses on generating consistent token sequences across varying utterances of the same term. We also propose a bidirectional state space modeling within the Mamba encoder, trained in a self-supervised learning framework, to learn contextual frame-level features that are further encoded into discrete tokens. Our analysis shows that our speech tokens exhibit greater speaker invariance than those from existing tokenizers, making them more suitable for STD tasks. Empirical evaluation on LibriSpeech and TIMIT databases indicates that our method outperforms existing STD baselines while being more efficient.",
    "metadata": {
      "arxiv_id": "2411.14100",
      "title": "BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken Term Detection",
      "summary": "Spoken term detection (STD) is often hindered by reliance on frame-level features and the computationally intensive DTW-based template matching, limiting its practicality. To address these challenges, we propose a novel approach that encodes speech into discrete, speaker-agnostic semantic tokens. This facilitates fast retrieval using text-based search algorithms and effectively handles out-of-vocabulary terms. Our approach focuses on generating consistent token sequences across varying utterances of the same term. We also propose a bidirectional state space modeling within the Mamba encoder, trained in a self-supervised learning framework, to learn contextual frame-level features that are further encoded into discrete tokens. Our analysis shows that our speech tokens exhibit greater speaker invariance than those from existing tokenizers, making them more suitable for STD tasks. Empirical evaluation on LibriSpeech and TIMIT databases indicates that our method outperforms existing STD baselines while being more efficient.",
      "authors": [
        "Anup Singh",
        "Kris Demuynck",
        "Vipul Arora"
      ],
      "published": "2024-11-21T13:05:18Z",
      "updated": "2024-12-21T19:15:27Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.14100v2",
      "landing_url": "https://arxiv.org/abs/2411.14100v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.14100"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work targets discrete speech tokens learned via self-supervised training for spoken term detection, aligning with the Discrete Audio Tokens theme by encoding into discrete, semantic tokens and aiming for robust token sequences across utterances, though explicit token generation/quantization details are not fully described in the abstract.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The work targets discrete speech tokens learned via self-supervised training for spoken term detection, aligning with the Discrete Audio Tokens theme by encoding into discrete, semantic tokens and aiming for robust token sequences across utterances, though explicit token generation/quantization details are not fully described in the abstract.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a novel method to encode speech into discrete, speaker-agnostic semantic tokens using a bidirectional Mamba encoder trained via self-supervised learning. The approach focuses on generating consistent discrete token sequences representing speech for spoken term detection, which aligns with the inclusion criteria regarding discrete audio token generation and usage for sequence modeling. The description mentions contextual frame-level features encoded into discrete tokens, indicating a quantization or discretization process for token formation. Experimental evaluation with clear datasets and comparison to baselines suggests sufficient methodological detail. No exclusion criteria such as lack of method details or traditional codec-only approach apply here. Therefore, the study meets the core inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a novel method to encode speech into discrete, speaker-agnostic semantic tokens using a bidirectional Mamba encoder trained via self-supervised learning. The approach focuses on generating consistent discrete token sequences representing speech for spoken term detection, which aligns with the inclusion criteria regarding discrete audio token generation and usage for sequence modeling. The description mentions contextual frame-level features encoded into discrete tokens, indicating a quantization or discretization process for token formation. Experimental evaluation with clear datasets and comparison to baselines suggests sufficient methodological detail. No exclusion criteria such as lack of method details or traditional codec-only approach apply here. Therefore, the study meets the core inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "VQalAttent: a Transparent Speech Generation Pipeline based on Transformer-learned VQ-VAE Latent Space",
    "abstract": "Generating high-quality speech efficiently remains a key challenge for generative models in speech synthesis. This paper introduces VQalAttent, a lightweight model designed to generate fake speech with tunable performance and interpretability. Leveraging the AudioMNIST dataset, consisting of human utterances of decimal digits (0-9), our method employs a two-step architecture: first, a scalable vector quantized autoencoder (VQ-VAE) that compresses audio spectrograms into discrete latent representations, and second, a decoder-only transformer that learns the probability model of these latents. Trained transformer generates similar latent sequences, convertible to audio spectrograms by the VQ-VAE decoder, from which we generate fake utterances. Interpreting statistical and perceptual quality of the fakes, depending on the dimension and the extrinsic information of the latent space, enables guided improvements in larger, commercial generative models. As a valuable tool for understanding and refining audio synthesis, our results demonstrate VQalAttent's capacity to generate intelligible speech samples with limited computational resources, while the modularity and transparency of the training pipeline helps easily correlate the analytics with modular modifications, hence providing insights for the more complex models.",
    "metadata": {
      "arxiv_id": "2411.14642",
      "title": "VQalAttent: a Transparent Speech Generation Pipeline based on Transformer-learned VQ-VAE Latent Space",
      "summary": "Generating high-quality speech efficiently remains a key challenge for generative models in speech synthesis. This paper introduces VQalAttent, a lightweight model designed to generate fake speech with tunable performance and interpretability. Leveraging the AudioMNIST dataset, consisting of human utterances of decimal digits (0-9), our method employs a two-step architecture: first, a scalable vector quantized autoencoder (VQ-VAE) that compresses audio spectrograms into discrete latent representations, and second, a decoder-only transformer that learns the probability model of these latents. Trained transformer generates similar latent sequences, convertible to audio spectrograms by the VQ-VAE decoder, from which we generate fake utterances. Interpreting statistical and perceptual quality of the fakes, depending on the dimension and the extrinsic information of the latent space, enables guided improvements in larger, commercial generative models. As a valuable tool for understanding and refining audio synthesis, our results demonstrate VQalAttent's capacity to generate intelligible speech samples with limited computational resources, while the modularity and transparency of the training pipeline helps easily correlate the analytics with modular modifications, hence providing insights for the more complex models.",
      "authors": [
        "Armani Rodriguez",
        "Silvija Kokalj-Filipovic"
      ],
      "published": "2024-11-22T00:21:39Z",
      "updated": "2024-11-22T00:21:39Z",
      "categories": [
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.14642v1",
      "landing_url": "https://arxiv.org/abs/2411.14642v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.14642"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work explicitly relies on discrete audio tokens via VQ-VAE latent space and a transformer to model their sequence for speech generation, with a described token generation/quantization process and decoder pipeline, not merely a demo, thus aligning with the discrete-audio-token inclusion criteria and not triggering any exclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This work explicitly relies on discrete audio tokens via VQ-VAE latent space and a transformer to model their sequence for speech generation, with a described token generation/quantization process and decoder pipeline, not merely a demo, thus aligning with the discrete-audio-token inclusion criteria and not triggering any exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents VQalAttent, which uses a vector quantized variational autoencoder (VQ-VAE) to convert audio spectrograms into discrete latent representations, then models these discrete tokens with a transformer, clearly aligning with the theme of discrete audio tokens generated by vector quantization for sequence modeling and generation. It details the token generation process and uses these tokens for generating speech, meeting the technical coverage and method transparency requirements. Therefore, it meets the inclusion criteria and has no evident exclusion issues.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper presents VQalAttent, which uses a vector quantized variational autoencoder (VQ-VAE) to convert audio spectrograms into discrete latent representations, then models these discrete tokens with a transformer, clearly aligning with the theme of discrete audio tokens generated by vector quantization for sequence modeling and generation. It details the token generation process and uses these tokens for generating speech, meeting the technical coverage and method transparency requirements. Therefore, it meets the inclusion criteria and has no evident exclusion issues.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Representation Collapsing Problems in Vector Quantization",
    "abstract": "Vector quantization is a technique in machine learning that discretizes continuous representations into a set of discrete vectors. It is widely employed in tokenizing data representations for large language models, diffusion models, and other generative models. Despite its prevalence, the characteristics and behaviors of vector quantization in generative models remain largely underexplored. In this study, we investigate representation collapse in vector quantization - a critical degradation where codebook tokens or latent embeddings lose their discriminative power by converging to a limited subset of values. This collapse fundamentally compromises the model's ability to capture diverse data patterns. By leveraging both synthetic and real datasets, we identify the severity of each type of collapses and triggering conditions. Our analysis reveals that restricted initialization and limited encoder capacity result in tokens collapse and embeddings collapse. Building on these findings, we propose potential solutions aimed at mitigating each collapse. To the best of our knowledge, this is the first comprehensive study examining representation collapsing problems in vector quantization.",
    "metadata": {
      "arxiv_id": "2411.16550",
      "title": "Representation Collapsing Problems in Vector Quantization",
      "summary": "Vector quantization is a technique in machine learning that discretizes continuous representations into a set of discrete vectors. It is widely employed in tokenizing data representations for large language models, diffusion models, and other generative models. Despite its prevalence, the characteristics and behaviors of vector quantization in generative models remain largely underexplored. In this study, we investigate representation collapse in vector quantization - a critical degradation where codebook tokens or latent embeddings lose their discriminative power by converging to a limited subset of values. This collapse fundamentally compromises the model's ability to capture diverse data patterns. By leveraging both synthetic and real datasets, we identify the severity of each type of collapses and triggering conditions. Our analysis reveals that restricted initialization and limited encoder capacity result in tokens collapse and embeddings collapse. Building on these findings, we propose potential solutions aimed at mitigating each collapse. To the best of our knowledge, this is the first comprehensive study examining representation collapsing problems in vector quantization.",
      "authors": [
        "Wenhao Zhao",
        "Qiran Zou",
        "Rushi Shah",
        "Dianbo Liu"
      ],
      "published": "2024-11-25T16:32:29Z",
      "updated": "2024-11-25T16:32:29Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.16550v1",
      "landing_url": "https://arxiv.org/abs/2411.16550v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.16550"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item analyzes representation collapse in vector quantization, focusing on tokens and codebooks in generative models, but it does not address discrete audio tokens or an audio-domain discretization/encoding pipeline as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item analyzes representation collapse in vector quantization, focusing on tokens and codebooks in generative models, but it does not address discrete audio tokens or an audio-domain discretization/encoding pipeline as required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on representation collapse in vector quantization within generative models but does not specifically address discrete audio tokens or their application to audio signals such as speech, music, or general audio. It lacks explicit discussion on how discrete audio tokens are generated or used for audio language modeling, cross-modal generation, or downstream tasks related to audio understanding or generation as required by inclusion criteria. Hence, it does not meet the necessary criteria for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on representation collapse in vector quantization within generative models but does not specifically address discrete audio tokens or their application to audio signals such as speech, music, or general audio. It lacks explicit discussion on how discrete audio tokens are generated or used for audio language modeling, cross-modal generation, or downstream tasks related to audio understanding or generation as required by inclusion criteria. Hence, it does not meet the necessary criteria for inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "k2SSL: A Faster and Better Framework for Self-Supervised Speech Representation Learning",
    "abstract": "Self-supervised learning (SSL) has achieved great success in speech-related tasks. While Transformer and Conformer architectures have dominated SSL backbones, encoders like Zipformer, which excel in automatic speech recognition (ASR), remain unexplored in SSL. Concurrently, inefficiencies in data processing within existing SSL training frameworks, such as fairseq, pose challenges in managing the growing volumes of training data. To address these issues, we propose k2SSL, an open-source framework that offers faster, more memory-efficient, and better-performing self-supervised speech representation learning, focusing on downstream ASR tasks. The optimized HuBERT and proposed Zipformer-based SSL systems exhibit substantial reductions in both training time and memory usage during SSL training. Experiments on LibriSpeech demonstrate that Zipformer Base significantly outperforms HuBERT and WavLM, achieving up to a 34.8% relative WER reduction compared to HuBERT Base after fine-tuning, along with a 3.5x pre-training speedup in GPU hours. When scaled to 60k hours of LibriLight data, Zipformer Large exhibits remarkable efficiency, matching HuBERT Large's performance while requiring only 5/8 pre-training steps.",
    "metadata": {
      "arxiv_id": "2411.17100",
      "title": "k2SSL: A Faster and Better Framework for Self-Supervised Speech Representation Learning",
      "summary": "Self-supervised learning (SSL) has achieved great success in speech-related tasks. While Transformer and Conformer architectures have dominated SSL backbones, encoders like Zipformer, which excel in automatic speech recognition (ASR), remain unexplored in SSL. Concurrently, inefficiencies in data processing within existing SSL training frameworks, such as fairseq, pose challenges in managing the growing volumes of training data. To address these issues, we propose k2SSL, an open-source framework that offers faster, more memory-efficient, and better-performing self-supervised speech representation learning, focusing on downstream ASR tasks. The optimized HuBERT and proposed Zipformer-based SSL systems exhibit substantial reductions in both training time and memory usage during SSL training. Experiments on LibriSpeech demonstrate that Zipformer Base significantly outperforms HuBERT and WavLM, achieving up to a 34.8% relative WER reduction compared to HuBERT Base after fine-tuning, along with a 3.5x pre-training speedup in GPU hours. When scaled to 60k hours of LibriLight data, Zipformer Large exhibits remarkable efficiency, matching HuBERT Large's performance while requiring only 5/8 pre-training steps.",
      "authors": [
        "Yifan Yang",
        "Jianheng Zhuo",
        "Zengrui Jin",
        "Ziyang Ma",
        "Xiaoyu Yang",
        "Zengwei Yao",
        "Liyong Guo",
        "Wei Kang",
        "Fangjun Kuang",
        "Long Lin",
        "Daniel Povey",
        "Xie Chen"
      ],
      "published": "2024-11-26T04:37:11Z",
      "updated": "2025-03-22T04:46:03Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17100v2",
      "landing_url": "https://arxiv.org/abs/2411.17100v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.17100"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item presents a self-supervised speech representation learning framework (k2SSL) focusing on training efficiency and Zipformer-based SSL models; it does not describe discrete audio token design, vector quantization, or token-based modeling essential to the ‘Discrete Audio Tokens’ criterion, thus it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item presents a self-supervised speech representation learning framework (k2SSL) focusing on training efficiency and Zipformer-based SSL models; it does not describe discrete audio token design, vector quantization, or token-based modeling essential to the ‘Discrete Audio Tokens’ criterion, thus it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a self-supervised learning framework for speech representation learning focusing on ASR tasks but does not mention the creation or use of discrete audio tokens produced via vector quantization or similar techniques, nor does it describe token formation processes relevant to language or sequence modeling. Therefore, it does not meet the core inclusion criteria of focusing on discrete audio token design or explicitly describing token generation for downstream sequence models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a self-supervised learning framework for speech representation learning focusing on ASR tasks but does not mention the creation or use of discrete audio tokens produced via vector quantization or similar techniques, nor does it describe token formation processes relevant to language or sequence modeling. Therefore, it does not meet the core inclusion criteria of focusing on discrete audio token design or explicitly describing token generation for downstream sequence models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Semantic Tokens in Retrieval Augmented Generation",
    "abstract": "Retrieval-Augmented Generation (RAG) architectures have recently garnered significant attention for their ability to improve truth grounding and coherence in natural language processing tasks. However, the reliability of RAG systems in producing accurate answers diminishes as the volume of data they access increases. Even with smaller datasets, these systems occasionally fail to address simple queries. This issue arises from their dependence on state-of-the-art large language models (LLMs), which can introduce uncertainty into the system's outputs. In this work, I propose a novel Comparative RAG system that introduces an evaluator module to bridge the gap between probabilistic RAG systems and deterministically verifiable responses. The evaluator compares external recommendations with the retrieved document chunks, adding a decision-making layer that enhances the system's reliability. This approach ensures that the chunks retrieved are both semantically relevant and logically consistent with deterministic insights, thereby improving the accuracy and overall efficiency of RAG systems. This framework paves the way for more reliable and scalable question-answering applications in domains requiring high precision and verifiability.",
    "metadata": {
      "arxiv_id": "2412.02563",
      "title": "Semantic Tokens in Retrieval Augmented Generation",
      "summary": "Retrieval-Augmented Generation (RAG) architectures have recently garnered significant attention for their ability to improve truth grounding and coherence in natural language processing tasks. However, the reliability of RAG systems in producing accurate answers diminishes as the volume of data they access increases. Even with smaller datasets, these systems occasionally fail to address simple queries. This issue arises from their dependence on state-of-the-art large language models (LLMs), which can introduce uncertainty into the system's outputs. In this work, I propose a novel Comparative RAG system that introduces an evaluator module to bridge the gap between probabilistic RAG systems and deterministically verifiable responses. The evaluator compares external recommendations with the retrieved document chunks, adding a decision-making layer that enhances the system's reliability. This approach ensures that the chunks retrieved are both semantically relevant and logically consistent with deterministic insights, thereby improving the accuracy and overall efficiency of RAG systems. This framework paves the way for more reliable and scalable question-answering applications in domains requiring high precision and verifiability.",
      "authors": [
        "Joel Suro"
      ],
      "published": "2024-12-03T16:52:06Z",
      "updated": "2024-12-03T16:52:06Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.02563v1",
      "landing_url": "https://arxiv.org/abs/2412.02563v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.02563"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on Retrieval Augmented Generation and an evaluator module for reliability, with no discussion of discrete audio token representations or audio-domain tokenization required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on Retrieval Augmented Generation and an evaluator module for reliability, with no discussion of discrete audio token representations or audio-domain tokenization required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on Retrieval-Augmented Generation systems and an evaluator module to improve response reliability in NLP tasks, without any mention of discrete audio tokens, audio signal processing, or token quantization methods related to audio. Therefore, it does not meet the inclusion criteria related to discrete audio token design and methodology, nor does it provide relevant audio token generation details, making it irrelevant to the specified topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on Retrieval-Augmented Generation systems and an evaluator module to improve response reliability in NLP tasks, without any mention of discrete audio tokens, audio signal processing, or token quantization methods related to audio. Therefore, it does not meet the inclusion criteria related to discrete audio token design and methodology, nor does it provide relevant audio token generation details, making it irrelevant to the specified topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Sprite Sheet Diffusion: Generate Game Character for Animation",
    "abstract": "In the game development process, creating character animations is a vital step that involves several stages. Typically for 2D games, illustrators begin by designing the main character image, which serves as the foundation for all subsequent animations. To create a smooth motion sequence, these subsequent animations involve drawing the character in different poses and actions, such as running, jumping, or attacking. This process requires significant manual effort from illustrators, as they must meticulously ensure consistency in design, proportions, and style across multiple motion frames. Each frame is drawn individually, making this a time-consuming and labor-intensive task. Generative models, such as diffusion models, have the potential to revolutionize this process by automating the creation of sprite sheets. Diffusion models, known for their ability to generate diverse images, can be adapted to create character animations. By leveraging the capabilities of diffusion models, we can significantly reduce the manual workload for illustrators, accelerate the animation creation process, and open up new creative possibilities in game development.",
    "metadata": {
      "arxiv_id": "2412.03685",
      "title": "Sprite Sheet Diffusion: Generate Game Character for Animation",
      "summary": "In the game development process, creating character animations is a vital step that involves several stages. Typically for 2D games, illustrators begin by designing the main character image, which serves as the foundation for all subsequent animations. To create a smooth motion sequence, these subsequent animations involve drawing the character in different poses and actions, such as running, jumping, or attacking. This process requires significant manual effort from illustrators, as they must meticulously ensure consistency in design, proportions, and style across multiple motion frames. Each frame is drawn individually, making this a time-consuming and labor-intensive task. Generative models, such as diffusion models, have the potential to revolutionize this process by automating the creation of sprite sheets. Diffusion models, known for their ability to generate diverse images, can be adapted to create character animations. By leveraging the capabilities of diffusion models, we can significantly reduce the manual workload for illustrators, accelerate the animation creation process, and open up new creative possibilities in game development.",
      "authors": [
        "Cheng-An Hsieh",
        "Jing Zhang",
        "Ava Yan"
      ],
      "published": "2024-12-04T19:40:05Z",
      "updated": "2025-03-16T21:42:15Z",
      "categories": [
        "cs.GR",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.03685v2",
      "landing_url": "https://arxiv.org/abs/2412.03685v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.03685"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on using diffusion models to generate 2D game sprite sheets for character animation (image/animation domain), not on discrete audio token design or audio-language modeling. It fails to meet the inclusion criterion centered on Discrete Audio Tokens and does not address token generation, quantization, or cross-modal audio-language tasks; thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on using diffusion models to generate 2D game sprite sheets for character animation (image/animation domain), not on discrete audio token design or audio-language modeling. It fails to meet the inclusion criterion centered on Discrete Audio Tokens and does not address token generation, quantization, or cross-modal audio-language tasks; thus it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on generating sprite sheets and game character animations using diffusion models, which is unrelated to discrete audio tokens or their generation and usage in sequence or language models; there is no mention of audio, vector quantization, token formation, or any relevant methodology related to discrete audio tokens as defined in the inclusion criteria, and it does not meet any inclusion elements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on generating sprite sheets and game character animations using diffusion models, which is unrelated to discrete audio tokens or their generation and usage in sequence or language models; there is no mention of audio, vector quantization, token formation, or any relevant methodology related to discrete audio tokens as defined in the inclusion criteria, and it does not meet any inclusion elements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Continuous Speech Tokens Makes LLMs Robust Multi-Modality Learners",
    "abstract": "Recent advances in GPT-4o like multi-modality models have demonstrated remarkable progress for direct speech-to-speech conversation, with real-time speech interaction experience and strong speech understanding ability. However, current research focuses on discrete speech tokens to align with discrete text tokens for language modelling, which depends on an audio codec with residual connections or independent group tokens, such a codec usually leverages large scale and diverse datasets training to ensure that the discrete speech codes have good representation for varied domain, noise, style data reconstruction as well as a well-designed codec quantizer and encoder-decoder architecture for discrete token language modelling. This paper introduces Flow-Omni, a continuous speech token based GPT-4o like model, capable of real-time speech interaction and low streaming latency. Specifically, first, instead of cross-entropy loss only, we combine flow matching loss with a pretrained autoregressive LLM and a small MLP network to predict the probability distribution of the continuous-valued speech tokens from speech prompt. second, we incorporated the continuous speech tokens to Flow-Omni multi-modality training, thereby achieving robust speech-to-speech performance with discrete text tokens and continuous speech tokens together. Experiments demonstrate that, compared to discrete text and speech multi-modality training and its variants, the continuous speech tokens mitigate robustness issues by avoiding the inherent flaws of discrete speech code's representation loss for LLM.",
    "metadata": {
      "arxiv_id": "2412.04917",
      "title": "Continuous Speech Tokens Makes LLMs Robust Multi-Modality Learners",
      "summary": "Recent advances in GPT-4o like multi-modality models have demonstrated remarkable progress for direct speech-to-speech conversation, with real-time speech interaction experience and strong speech understanding ability. However, current research focuses on discrete speech tokens to align with discrete text tokens for language modelling, which depends on an audio codec with residual connections or independent group tokens, such a codec usually leverages large scale and diverse datasets training to ensure that the discrete speech codes have good representation for varied domain, noise, style data reconstruction as well as a well-designed codec quantizer and encoder-decoder architecture for discrete token language modelling. This paper introduces Flow-Omni, a continuous speech token based GPT-4o like model, capable of real-time speech interaction and low streaming latency. Specifically, first, instead of cross-entropy loss only, we combine flow matching loss with a pretrained autoregressive LLM and a small MLP network to predict the probability distribution of the continuous-valued speech tokens from speech prompt. second, we incorporated the continuous speech tokens to Flow-Omni multi-modality training, thereby achieving robust speech-to-speech performance with discrete text tokens and continuous speech tokens together. Experiments demonstrate that, compared to discrete text and speech multi-modality training and its variants, the continuous speech tokens mitigate robustness issues by avoiding the inherent flaws of discrete speech code's representation loss for LLM.",
      "authors": [
        "Ze Yuan",
        "Yanqing Liu",
        "Shujie Liu",
        "Sheng Zhao"
      ],
      "published": "2024-12-06T10:16:04Z",
      "updated": "2024-12-06T10:16:04Z",
      "categories": [
        "cs.SD",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.04917v1",
      "landing_url": "https://arxiv.org/abs/2412.04917v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.04917"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article centers on continuous speech tokens as the main representation, not discrete audio tokens; while it mentions discrete tokens, the core contribution is continuous-token modelling, so it fails the inclusion criterion requiring discrete audio token design.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The article centers on continuous speech tokens as the main representation, not discrete audio tokens; while it mentions discrete tokens, the core contribution is continuous-token modelling, so it fails the inclusion criterion requiring discrete audio token design.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses primarily on continuous speech tokens as opposed to discrete audio tokens, which is the core inclusion criterion; it highlights the limitations of discrete tokens and proposes a continuous token approach, thus it does not meet the requirement of using discrete token design or discrete audio token generation as a main contribution or core method, and lacks clear token quantization/encoding details for discrete tokens, leading to exclusion according to the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses primarily on continuous speech tokens as opposed to discrete audio tokens, which is the core inclusion criterion; it highlights the limitations of discrete tokens and proposes a continuous token approach, thus it does not meet the requirement of using discrete token design or discrete audio token generation as a main contribution or core method, and lacks clear token quantization/encoding details for discrete tokens, leading to exclusion according to the criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models",
    "abstract": "In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.",
    "metadata": {
      "arxiv_id": "2412.10117",
      "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models",
      "summary": "In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.",
      "authors": [
        "Zhihao Du",
        "Yuxuan Wang",
        "Qian Chen",
        "Xian Shi",
        "Xiang Lv",
        "Tianyu Zhao",
        "Zhifu Gao",
        "Yexin Yang",
        "Changfeng Gao",
        "Hui Wang",
        "Fan Yu",
        "Huadai Liu",
        "Zhengyan Sheng",
        "Yue Gu",
        "Chong Deng",
        "Wen Wang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Jingren Zhou"
      ],
      "published": "2024-12-13T12:59:39Z",
      "updated": "2024-12-25T11:54:03Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.10117v3",
      "landing_url": "https://arxiv.org/abs/2412.10117v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.10117"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "CosyVoice 2 explicitly discusses discrete speech tokens and quantization (finite-scalar quantization for speech tokens, codebooks) and token-based streaming synthesis with an LLM backbone, aligning with the discrete audio token research focus and going beyond mere demos.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "CosyVoice 2 explicitly discusses discrete speech tokens and quantization (finite-scalar quantization for speech tokens, codebooks) and token-based streaming synthesis with an LLM backbone, aligning with the discrete audio token research focus and going beyond mere demos.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents CosyVoice 2, building upon a previous model (CosyVoice) that is based on supervised discrete speech tokens, which aligns with the definition of discrete audio tokens involving vector quantization and semantic decoding for speech synthesis. The abstract also mentions improvements such as finite-scalar quantization and use of a pre-trained LLM backbone, indicating a clear token generation and use mechanism for sequence modeling and generation. The technical description is sufficiently detailed to assess the token formation process and model structure, satisfying the inclusion criteria of method transparency and token usage in language models. It does not fall under any exclusion criteria such as lack of method details or being a pure demo report. Therefore, it should be included according to the provided criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study presents CosyVoice 2, building upon a previous model (CosyVoice) that is based on supervised discrete speech tokens, which aligns with the definition of discrete audio tokens involving vector quantization and semantic decoding for speech synthesis. The abstract also mentions improvements such as finite-scalar quantization and use of a pre-trained LLM backbone, indicating a clear token generation and use mechanism for sequence modeling and generation. The technical description is sufficiently detailed to assess the token formation process and model structure, satisfying the inclusion criteria of method transparency and token usage in language models. It does not fall under any exclusion criteria such as lack of method details or being a pure demo report. Therefore, it should be included according to the provided criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Exploring Diffusion and Flow Matching Under Generator Matching",
    "abstract": "In this paper, we present a comprehensive theoretical comparison of diffusion and flow matching under the Generator Matching framework. Despite their apparent differences, both diffusion and flow matching can be viewed under the unified framework of Generator Matching. By recasting both diffusion and flow matching under the same generative Markov framework, we provide theoretical insights into why flow matching models can be more robust empirically and how novel model classes can be constructed by mixing deterministic and stochastic components. Our analysis offers a fresh perspective on the relationships between state-of-the-art generative modeling paradigms.",
    "metadata": {
      "arxiv_id": "2412.11024",
      "title": "Exploring Diffusion and Flow Matching Under Generator Matching",
      "summary": "In this paper, we present a comprehensive theoretical comparison of diffusion and flow matching under the Generator Matching framework. Despite their apparent differences, both diffusion and flow matching can be viewed under the unified framework of Generator Matching. By recasting both diffusion and flow matching under the same generative Markov framework, we provide theoretical insights into why flow matching models can be more robust empirically and how novel model classes can be constructed by mixing deterministic and stochastic components. Our analysis offers a fresh perspective on the relationships between state-of-the-art generative modeling paradigms.",
      "authors": [
        "Zeeshan Patel",
        "James DeLoye",
        "Lance Mathias"
      ],
      "published": "2024-12-15T02:35:31Z",
      "updated": "2024-12-17T07:45:29Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.11024v2",
      "landing_url": "https://arxiv.org/abs/2412.11024v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.11024"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "該項目探討 diffusion 與 flow matching 的理論比較，未涉及離散音訊 token 的設計、量化或用於序列模型的具體方法與實驗，因此不符合本主題的包含條件，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "該項目探討 diffusion 與 flow matching 的理論比較，未涉及離散音訊 token 的設計、量化或用於序列模型的具體方法與實驗，因此不符合本主題的包含條件，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on theoretical comparisons of diffusion and flow matching under a generator matching framework, which is related to generative modeling paradigms in general but does not discuss discrete audio tokens, token generation processes, quantization methods, or audio language modeling as specified in the inclusion criteria. Additionally, there is no mention of producing or using discrete audio tokens for sequence/language model understanding or generation, nor is there information about token quantization or encoding schemes related to audio. Therefore, it does not meet the core topic requirements and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on theoretical comparisons of diffusion and flow matching under a generator matching framework, which is related to generative modeling paradigms in general but does not discuss discrete audio tokens, token generation processes, quantization methods, or audio language modeling as specified in the inclusion criteria. Additionally, there is no mention of producing or using discrete audio tokens for sequence/language model understanding or generation, nor is there information about token quantization or encoding schemes related to audio. Therefore, it does not meet the core topic requirements and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Emergence and Effectiveness of Task Vectors in In-Context Learning: An Encoder Decoder Perspective",
    "abstract": "Autoregressive transformers exhibit adaptive learning through in-context learning (ICL), which begs the question of how. Prior works have shown that transformers represent the ICL tasks as vectors in their representations. In this paper, we leverage the encoding-decoding framework to study how transformers form task vectors during pretraining and how their task encoding quality predicts ICL task performance. On synthetic ICL tasks, we analyze the training dynamics of a small transformer and report the coupled emergence of task encoding and decoding. As the model learns to encode different latent tasks (e.g., \"Finding the first noun in a sentence.\") into distinct, separable representations, it concurrently builds conditional decoding algorithms and improves its ICL performance. We validate this phenomenon across pretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B) and over the course of pretraining in OLMo-7B. Further, we demonstrate that the quality of task encoding inferred from representations predicts ICL performance, and that, surprisingly, finetuning the earlier layers can improve the task encoding and performance more than finetuning the latter layers. Our empirical insights shed light into better understanding the success and failure modes of large language models via their representations.",
    "metadata": {
      "arxiv_id": "2412.12276",
      "title": "Emergence and Effectiveness of Task Vectors in In-Context Learning: An Encoder Decoder Perspective",
      "summary": "Autoregressive transformers exhibit adaptive learning through in-context learning (ICL), which begs the question of how. Prior works have shown that transformers represent the ICL tasks as vectors in their representations. In this paper, we leverage the encoding-decoding framework to study how transformers form task vectors during pretraining and how their task encoding quality predicts ICL task performance. On synthetic ICL tasks, we analyze the training dynamics of a small transformer and report the coupled emergence of task encoding and decoding. As the model learns to encode different latent tasks (e.g., \"Finding the first noun in a sentence.\") into distinct, separable representations, it concurrently builds conditional decoding algorithms and improves its ICL performance. We validate this phenomenon across pretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B) and over the course of pretraining in OLMo-7B. Further, we demonstrate that the quality of task encoding inferred from representations predicts ICL performance, and that, surprisingly, finetuning the earlier layers can improve the task encoding and performance more than finetuning the latter layers. Our empirical insights shed light into better understanding the success and failure modes of large language models via their representations.",
      "authors": [
        "Seungwook Han",
        "Jinyeop Song",
        "Jeff Gore",
        "Pulkit Agrawal"
      ],
      "published": "2024-12-16T19:00:18Z",
      "updated": "2025-06-02T12:55:12Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.12276v3",
      "landing_url": "https://arxiv.org/abs/2412.12276v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.12276"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article investigates ICL task vectors in encoder-decoder transformers for language models, not discrete audio token design or audio token quantization; it fails the inclusion criteria devoted to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article investigates ICL task vectors in encoder-decoder transformers for language models, not discrete audio token design or audio token quantization; it fails the inclusion criteria devoted to discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on the study of task vectors in transformer models for in-context learning, primarily within natural language processing contexts, without mentioning discrete audio tokens, audio signal processing, quantization methods, or any audio-language multimodal modeling relevant to the inclusion criteria focused on discrete audio token representations. Therefore, it does not meet the inclusion criteria and is unrelated to the topic of discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on the study of task vectors in transformer models for in-context learning, primarily within natural language processing contexts, without mentioning discrete audio tokens, audio signal processing, quantization methods, or any audio-language multimodal modeling relevant to the inclusion criteria focused on discrete audio token representations. Therefore, it does not meet the inclusion criteria and is unrelated to the topic of discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "KALL-E:Autoregressive Speech Synthesis with Next-Distribution Prediction",
    "abstract": "We introduce KALL-E, a novel autoregressive (AR) language model for text-to-speech (TTS) synthesis that operates by predicting the next distribution of continuous speech frames. Unlike existing methods, KALL-E directly models the continuous speech distribution conditioned on text, eliminating the need for any diffusion-based components. Specifically, we utilize a Flow-VAE to extract a continuous latent speech representation from waveforms, instead of relying on discrete speech tokens. A single AR Transformer is then trained to predict these continuous speech distributions from text, optimizing a Kullback-Leibler divergence loss as its objective. Experimental results demonstrate that KALL-E achieves superior speech synthesis quality and can even adapt to a target speaker from just a single sample. Importantly, KALL-E provides a more direct and effective approach for utilizing continuous speech representations in TTS.",
    "metadata": {
      "arxiv_id": "2412.16846",
      "title": "KALL-E:Autoregressive Speech Synthesis with Next-Distribution Prediction",
      "summary": "We introduce KALL-E, a novel autoregressive (AR) language model for text-to-speech (TTS) synthesis that operates by predicting the next distribution of continuous speech frames. Unlike existing methods, KALL-E directly models the continuous speech distribution conditioned on text, eliminating the need for any diffusion-based components. Specifically, we utilize a Flow-VAE to extract a continuous latent speech representation from waveforms, instead of relying on discrete speech tokens. A single AR Transformer is then trained to predict these continuous speech distributions from text, optimizing a Kullback-Leibler divergence loss as its objective. Experimental results demonstrate that KALL-E achieves superior speech synthesis quality and can even adapt to a target speaker from just a single sample. Importantly, KALL-E provides a more direct and effective approach for utilizing continuous speech representations in TTS.",
      "authors": [
        "Kangxiang Xia",
        "Xinfa Zhu",
        "Jixun Yao",
        "Wenjie Tian",
        "Wenhao Li",
        "Lei Xie"
      ],
      "published": "2024-12-22T04:03:24Z",
      "updated": "2025-09-17T16:01:26Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.16846v2",
      "landing_url": "https://arxiv.org/abs/2412.16846v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.16846"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on continuous latent speech representations and autoregressive modeling without discrete audio tokens, thus it does not meet the Discrete Audio Tokens inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on continuous latent speech representations and autoregressive modeling without discrete audio tokens, thus it does not meet the Discrete Audio Tokens inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on KALL-E, a TTS model predicting continuous speech frame distributions using Flow-VAE latent representations rather than discrete audio tokens generated by vector quantization or similar methods. Since the core method deals with continuous speech representations without producing or utilizing discrete audio tokens as defined in the inclusion criteria, it does not meet the primary requirement of discrete audio token design or application. Additionally, there is no detailed description or emphasis on token generation process or use of discrete token sequences for language or sequence modeling. Therefore, it does not satisfy the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on KALL-E, a TTS model predicting continuous speech frame distributions using Flow-VAE latent representations rather than discrete audio tokens generated by vector quantization or similar methods. Since the core method deals with continuous speech representations without producing or utilizing discrete audio tokens as defined in the inclusion criteria, it does not meet the primary requirement of discrete audio token design or application. Additionally, there is no detailed description or emphasis on token generation process or use of discrete token sequences for language or sequence modeling. Therefore, it does not satisfy the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Hierarchical Vector Quantization for Unsupervised Action Segmentation",
    "abstract": "In this work, we address unsupervised temporal action segmentation, which segments a set of long, untrimmed videos into semantically meaningful segments that are consistent across videos. While recent approaches combine representation learning and clustering in a single step for this task, they do not cope with large variations within temporal segments of the same class. To address this limitation, we propose a novel method, termed Hierarchical Vector Quantization (HVQ), that consists of two subsequent vector quantization modules. This results in a hierarchical clustering where the additional subclusters cover the variations within a cluster. We demonstrate that our approach captures the distribution of segment lengths much better than the state of the art. To this end, we introduce a new metric based on the Jensen-Shannon Distance (JSD) for unsupervised temporal action segmentation. We evaluate our approach on three public datasets, namely Breakfast, YouTube Instructional and IKEA ASM. Our approach outperforms the state of the art in terms of F1 score, recall and JSD.",
    "metadata": {
      "arxiv_id": "2412.17640",
      "title": "Hierarchical Vector Quantization for Unsupervised Action Segmentation",
      "summary": "In this work, we address unsupervised temporal action segmentation, which segments a set of long, untrimmed videos into semantically meaningful segments that are consistent across videos. While recent approaches combine representation learning and clustering in a single step for this task, they do not cope with large variations within temporal segments of the same class. To address this limitation, we propose a novel method, termed Hierarchical Vector Quantization (HVQ), that consists of two subsequent vector quantization modules. This results in a hierarchical clustering where the additional subclusters cover the variations within a cluster. We demonstrate that our approach captures the distribution of segment lengths much better than the state of the art. To this end, we introduce a new metric based on the Jensen-Shannon Distance (JSD) for unsupervised temporal action segmentation. We evaluate our approach on three public datasets, namely Breakfast, YouTube Instructional and IKEA ASM. Our approach outperforms the state of the art in terms of F1 score, recall and JSD.",
      "authors": [
        "Federico Spurio",
        "Emad Bahrami",
        "Gianpiero Francesca",
        "Juergen Gall"
      ],
      "published": "2024-12-23T15:18:24Z",
      "updated": "2025-01-24T17:43:56Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.17640v2",
      "landing_url": "https://arxiv.org/abs/2412.17640v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.17640"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item investigates hierarchical vector quantization for unsupervised video action segmentation, not discrete audio token design or audio-language modeling; it concerns visual data and action segments, thus it does not meet the inclusion criteria focused on discrete audio tokens and audio token generation/quantization details.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item investigates hierarchical vector quantization for unsupervised video action segmentation, not discrete audio token design or audio-language modeling; it concerns visual data and action segments, thus it does not meet the inclusion criteria focused on discrete audio tokens and audio token generation/quantization details.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on unsupervised temporal action segmentation in videos using hierarchical vector quantization for clustering video segments, without any relation to audio tokenization or discrete audio representations for speech, music, or general audio as required by the inclusion criteria. The method does not discuss creation or use of discrete audio tokens from continuous audio signals, nor does it mention their use in audio language modeling or related tasks. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens and their applications, and hence should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on unsupervised temporal action segmentation in videos using hierarchical vector quantization for clustering video segments, without any relation to audio tokenization or discrete audio representations for speech, music, or general audio as required by the inclusion criteria. The method does not discuss creation or use of discrete audio tokens from continuous audio signals, nor does it mention their use in audio language modeling or related tasks. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens and their applications, and hence should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Causal Speech Enhancement with Predicting Semantics based on Quantized Self-supervised Learning Features",
    "abstract": "Real-time speech enhancement (SE) is essential to online speech communication. Causal SE models use only the previous context while predicting future information, such as phoneme continuation, may help performing causal SE. The phonetic information is often represented by quantizing latent features of self-supervised learning (SSL) models. This work is the first to incorporate SSL features with causality into an SE model. The causal SSL features are encoded and combined with spectrogram features using feature-wise linear modulation to estimate a mask for enhancing the noisy input speech. Simultaneously, we quantize the causal SSL features using vector quantization to represent phonetic characteristics as semantic tokens. The model not only encodes SSL features but also predicts the future semantic tokens in multi-task learning (MTL). The experimental results using VoiceBank + DEMAND dataset show that our proposed method achieves 2.88 in PESQ, especially with semantic prediction MTL, in which we confirm that the semantic prediction played an important role in causal SE.",
    "metadata": {
      "arxiv_id": "2412.19248",
      "title": "Causal Speech Enhancement with Predicting Semantics based on Quantized Self-supervised Learning Features",
      "summary": "Real-time speech enhancement (SE) is essential to online speech communication. Causal SE models use only the previous context while predicting future information, such as phoneme continuation, may help performing causal SE. The phonetic information is often represented by quantizing latent features of self-supervised learning (SSL) models. This work is the first to incorporate SSL features with causality into an SE model. The causal SSL features are encoded and combined with spectrogram features using feature-wise linear modulation to estimate a mask for enhancing the noisy input speech. Simultaneously, we quantize the causal SSL features using vector quantization to represent phonetic characteristics as semantic tokens. The model not only encodes SSL features but also predicts the future semantic tokens in multi-task learning (MTL). The experimental results using VoiceBank + DEMAND dataset show that our proposed method achieves 2.88 in PESQ, especially with semantic prediction MTL, in which we confirm that the semantic prediction played an important role in causal SE.",
      "authors": [
        "Emiru Tsunoo",
        "Yuki Saito",
        "Wataru Nakata",
        "Hiroshi Saruwatari"
      ],
      "published": "2024-12-26T15:08:36Z",
      "updated": "2024-12-26T15:08:36Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.19248v1",
      "landing_url": "https://arxiv.org/abs/2412.19248v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.19248"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract clearly uses discrete audio tokens via vector quantization of SSL features to represent phonetic/semantic information and even predicts future tokens in a multi-task setup, meeting the discrete-token design criterion without relying on demos and indicating methodological details.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract clearly uses discrete audio tokens via vector quantization of SSL features to represent phonetic/semantic information and even predicts future tokens in a multi-task setup, meeting the discrete-token design criterion without relying on demos and indicating methodological details.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a causal speech enhancement model that uses quantized self-supervised learning features, involving vector quantization to represent phonetic characteristics as semantic tokens; this aligns with using discrete audio tokens generated via vector quantization derived from self-supervised features, fulfilling the inclusion criteria regarding token generation and usage for downstream tasks. Detailed method description including quantization and multi-task learning for semantic prediction supports reproducibility, and the focus on semantic tokens for speech enhancement integrates semantic information, meeting the thematic focus. Therefore, the paper is relevant and suitable for inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a causal speech enhancement model that uses quantized self-supervised learning features, involving vector quantization to represent phonetic characteristics as semantic tokens; this aligns with using discrete audio tokens generated via vector quantization derived from self-supervised features, fulfilling the inclusion criteria regarding token generation and usage for downstream tasks. Detailed method description including quantization and multi-task learning for semantic prediction supports reproducibility, and the focus on semantic tokens for speech enhancement integrates semantic information, meeting the thematic focus. Therefore, the paper is relevant and suitable for inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SECodec: Structural Entropy-based Compressive Speech Representation Codec for Speech Language Models",
    "abstract": "With the rapid advancement of large language models (LLMs), discrete speech representations have become crucial for integrating speech into LLMs. Existing methods for speech representation discretization rely on a predefined codebook size and Euclidean distance-based quantization. However, 1) the size of codebook is a critical parameter that affects both codec performance and downstream task training efficiency. 2) The Euclidean distance-based quantization may lead to audio distortion when the size of the codebook is controlled within a reasonable range. In fact, in the field of information compression, structural information and entropy guidance are crucial, but previous methods have largely overlooked these factors. Therefore, we address the above issues from an information-theoretic perspective, we present SECodec, a novel speech representation codec based on structural entropy (SE) for building speech language models. Specifically, we first model speech as a graph, clustering the speech features nodes within the graph and extracting the corresponding codebook by hierarchically and disentangledly minimizing 2D SE. Then, to address the issue of audio distortion, we propose a new quantization method. This method still adheres to the 2D SE minimization principle, adaptively selecting the most suitable token corresponding to the cluster for each incoming original speech node. Furthermore, we develop a Structural Entropy-based Speech Language Model (SESLM) that leverages SECodec. Experimental results demonstrate that SECodec performs comparably to EnCodec in speech reconstruction, and SESLM surpasses VALL-E in zero-shot text-to-speech tasks. Code, demo speeches, speech feature graph, SE codebook, and models are available at https://github.com/wlq2019/SECodec.",
    "metadata": {
      "arxiv_id": "2501.00018",
      "title": "SECodec: Structural Entropy-based Compressive Speech Representation Codec for Speech Language Models",
      "summary": "With the rapid advancement of large language models (LLMs), discrete speech representations have become crucial for integrating speech into LLMs. Existing methods for speech representation discretization rely on a predefined codebook size and Euclidean distance-based quantization. However, 1) the size of codebook is a critical parameter that affects both codec performance and downstream task training efficiency. 2) The Euclidean distance-based quantization may lead to audio distortion when the size of the codebook is controlled within a reasonable range. In fact, in the field of information compression, structural information and entropy guidance are crucial, but previous methods have largely overlooked these factors. Therefore, we address the above issues from an information-theoretic perspective, we present SECodec, a novel speech representation codec based on structural entropy (SE) for building speech language models. Specifically, we first model speech as a graph, clustering the speech features nodes within the graph and extracting the corresponding codebook by hierarchically and disentangledly minimizing 2D SE. Then, to address the issue of audio distortion, we propose a new quantization method. This method still adheres to the 2D SE minimization principle, adaptively selecting the most suitable token corresponding to the cluster for each incoming original speech node. Furthermore, we develop a Structural Entropy-based Speech Language Model (SESLM) that leverages SECodec. Experimental results demonstrate that SECodec performs comparably to EnCodec in speech reconstruction, and SESLM surpasses VALL-E in zero-shot text-to-speech tasks. Code, demo speeches, speech feature graph, SE codebook, and models are available at https://github.com/wlq2019/SECodec.",
      "authors": [
        "Linqin Wang",
        "Yaping Liu",
        "Zhengtao Yu",
        "Shengxiang Gao",
        "Cunli Mao",
        "Yuxin Huang",
        "Wenjun Wang",
        "Ling Dong"
      ],
      "published": "2024-12-16T03:33:05Z",
      "updated": "2024-12-16T03:33:05Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.00018v1",
      "landing_url": "https://arxiv.org/abs/2501.00018v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.00018"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper presents SECodec, a learning-based discrete speech-token framework (graph-based clustering, structure-entropy minimization, adaptive token quantization) and a SESLM model for speech-language tasks, clearly targeting discrete audio tokens for downstream seq/LMMs with methodological details and evaluative experiments, thus meeting all inclusion criteria and not any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper presents SECodec, a learning-based discrete speech-token framework (graph-based clustering, structure-entropy minimization, adaptive token quantization) and a SESLM model for speech-language tasks, clearly targeting discrete audio tokens for downstream seq/LMMs with methodological details and evaluative experiments, thus meeting all inclusion criteria and not any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents SECodec, a codec based on structural entropy for discrete speech representation, explicitly modeling speech as graphs and generating discrete tokens via clustering and quantization, meant for integration with speech language models. It details the token generation process, codebook design, and quantization method, and demonstrates use in speech reconstruction and zero-shot TTS tasks, aligning with the inclusion criteria emphasizing discrete audio token design with clear technical details and usage for sequence modeling; no exclusion criteria are met.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study presents SECodec, a codec based on structural entropy for discrete speech representation, explicitly modeling speech as graphs and generating discrete tokens via clustering and quantization, meant for integration with speech language models. It details the token generation process, codebook design, and quantization method, and demonstrates use in speech reconstruction and zero-shot TTS tasks, aligning with the inclusion criteria emphasizing discrete audio token design with clear technical details and usage for sequence modeling; no exclusion criteria are met.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "FED: Fast and Efficient Dataset Deduplication Framework with GPU Acceleration",
    "abstract": "Dataset deduplication plays a crucial role in enhancing data quality, ultimately improving the training performance and efficiency of large language models. A commonly used method for data deduplication is the MinHash LSH algorithm. Recently, NVIDIA introduced a GPU-based MinHash LSH deduplication method, but it remains suboptimal, leaving room for further improvement in processing efficiency. This paper proposes a GPU-accelerated deduplication framework, FED, that optimizes MinHash LSH for GPU clusters and leverages computationally efficient, partially reusable non-cryptographic hash functions. FED significantly outperforms the CPU-based deduplication tool in SlimPajama (using 64 logical CPU cores) by up to 107.2 times and the GPU-based tool in NVIDIA NeMo Curator by up to 6.3 times when processing 30 million documents on a node with four GPUs. Notably, our method dramatically accelerates the previously time-consuming MinHash signature generation phase, achieving speed-ups of up to 260 compared to the CPU baseline. Despite these gains in efficiency, FED maintains high deduplication quality, with the duplicate document sets reaching a Jaccard similarity of over 0.96 compared to those identified by the standard MinHash algorithm. In large-scale experiments, the deduplication of 1.2 trillion tokens is completed in just 6 hours in a four-node, 16-GPU environment. The related code is publicly available on GitHub (\\href{https://github.com/mcrl/FED}{https://github.com/mcrl/FED}).",
    "metadata": {
      "arxiv_id": "2501.01046",
      "title": "FED: Fast and Efficient Dataset Deduplication Framework with GPU Acceleration",
      "summary": "Dataset deduplication plays a crucial role in enhancing data quality, ultimately improving the training performance and efficiency of large language models. A commonly used method for data deduplication is the MinHash LSH algorithm. Recently, NVIDIA introduced a GPU-based MinHash LSH deduplication method, but it remains suboptimal, leaving room for further improvement in processing efficiency. This paper proposes a GPU-accelerated deduplication framework, FED, that optimizes MinHash LSH for GPU clusters and leverages computationally efficient, partially reusable non-cryptographic hash functions. FED significantly outperforms the CPU-based deduplication tool in SlimPajama (using 64 logical CPU cores) by up to 107.2 times and the GPU-based tool in NVIDIA NeMo Curator by up to 6.3 times when processing 30 million documents on a node with four GPUs. Notably, our method dramatically accelerates the previously time-consuming MinHash signature generation phase, achieving speed-ups of up to 260 compared to the CPU baseline. Despite these gains in efficiency, FED maintains high deduplication quality, with the duplicate document sets reaching a Jaccard similarity of over 0.96 compared to those identified by the standard MinHash algorithm. In large-scale experiments, the deduplication of 1.2 trillion tokens is completed in just 6 hours in a four-node, 16-GPU environment. The related code is publicly available on GitHub (\\href{https://github.com/mcrl/FED}{https://github.com/mcrl/FED}).",
      "authors": [
        "Youngjun Son",
        "Chaewon Kim",
        "Jaejin Lee"
      ],
      "published": "2025-01-02T04:11:23Z",
      "updated": "2025-03-12T13:36:32Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.01046v3",
      "landing_url": "https://arxiv.org/abs/2501.01046v3",
      "doi": "https://doi.org/10.48550/arXiv.2501.01046"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on GPU-accelerated dataset deduplication (MinHash LSH) for large-scale text/data, not on discrete audio token design or token generation/quantization; it fails the inclusion criteria and aligns with data processing rather than the specified audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on GPU-accelerated dataset deduplication (MinHash LSH) for large-scale text/data, not on discrete audio token design or token generation/quantization; it fails the inclusion criteria and aligns with data processing rather than the specified audio token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on dataset deduplication using GPU-accelerated MinHash LSH algorithms for large language model datasets, without any mention of discrete audio tokens, vector quantization, or audio signal processing related to audio token generation; thus, it does not meet the inclusion criteria centered on discrete audio token design or usage for sequence/language models in audio domains.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on dataset deduplication using GPU-accelerated MinHash LSH algorithms for large language model datasets, without any mention of discrete audio tokens, vector quantization, or audio signal processing related to audio token generation; thus, it does not meet the inclusion criteria centered on discrete audio token design or usage for sequence/language models in audio domains.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "PM-Dedup: Secure Deduplication with Partial Migration from Cloud to Edge Servers",
    "abstract": "Currently, an increasing number of users and enterprises are storing their data in the cloud but do not fully trust cloud providers with their data in plaintext form. To address this concern, they encrypt their data before uploading it to the cloud. However, encryption with different keys means that even identical data will become different ciphertexts, making deduplication less effective. Encrypted deduplication avoids this issue by ensuring that identical data chunks generate the same ciphertext with content-based keys, enabling the cloud to efficiently identify and remove duplicates even in encrypted form. Current encrypted data deduplication work can be classified into two types: target-based and source-based. Target-based encrypted deduplication requires clients to upload all encrypted chunks (the basic unit of deduplication) to the cloud with high network bandwidth overhead. Source-based deduplication involves clients uploading fingerprints (hashes) of encrypted chunks for duplicate checking and only uploading unique encrypted chunks, which reduces network transfer but introduces high latency and potential side-channel attacks, which need to be mitigated by Proof of Ownership (PoW), and high computing overhead of the cloud. So, reducing the latency and the overheads of network and cloud while ensuring security has become a significant challenge for secure data deduplication in cloud storage. In response to this challenge, we present PM-Dedup, a novel secure source-based deduplication approach that relocates a portion of the deduplication checking process and PoW tasks from the cloud to the trusted execution environments (TEEs) in the client-side edge servers. We also propose various designs to enhance the security and efficiency of data deduplication.",
    "metadata": {
      "arxiv_id": "2501.02350",
      "title": "PM-Dedup: Secure Deduplication with Partial Migration from Cloud to Edge Servers",
      "summary": "Currently, an increasing number of users and enterprises are storing their data in the cloud but do not fully trust cloud providers with their data in plaintext form. To address this concern, they encrypt their data before uploading it to the cloud. However, encryption with different keys means that even identical data will become different ciphertexts, making deduplication less effective. Encrypted deduplication avoids this issue by ensuring that identical data chunks generate the same ciphertext with content-based keys, enabling the cloud to efficiently identify and remove duplicates even in encrypted form. Current encrypted data deduplication work can be classified into two types: target-based and source-based. Target-based encrypted deduplication requires clients to upload all encrypted chunks (the basic unit of deduplication) to the cloud with high network bandwidth overhead. Source-based deduplication involves clients uploading fingerprints (hashes) of encrypted chunks for duplicate checking and only uploading unique encrypted chunks, which reduces network transfer but introduces high latency and potential side-channel attacks, which need to be mitigated by Proof of Ownership (PoW), and high computing overhead of the cloud. So, reducing the latency and the overheads of network and cloud while ensuring security has become a significant challenge for secure data deduplication in cloud storage. In response to this challenge, we present PM-Dedup, a novel secure source-based deduplication approach that relocates a portion of the deduplication checking process and PoW tasks from the cloud to the trusted execution environments (TEEs) in the client-side edge servers. We also propose various designs to enhance the security and efficiency of data deduplication.",
      "authors": [
        "Zhaokang Ke",
        "Haoyu Gong",
        "David H. C. Du"
      ],
      "published": "2025-01-04T18:12:23Z",
      "updated": "2025-01-04T18:12:23Z",
      "categories": [
        "cs.CR",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.02350v1",
      "landing_url": "https://arxiv.org/abs/2501.02350v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.02350"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目主題為雲端資料去重與邊緣端 TEEs 的安全方案，與離散音訊 token 的設計、量化流程及語言模型相關研究完全不符，因此不符合收錄標準。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目主題為雲端資料去重與邊緣端 TEEs 的安全方案，與離散音訊 token 的設計、量化流程及語言模型相關研究完全不符，因此不符合收錄標準。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on secure data deduplication in cloud storage involving encryption and edge computing, which is unrelated to the inclusion criteria centered on discrete audio tokens generated via vector quantization or related methods for audio language modeling; thus, it does not meet the inclusion criteria nor relate to discrete audio token generation or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on secure data deduplication in cloud storage involving encryption and edge computing, which is unrelated to the inclusion criteria centered on discrete audio tokens generated via vector quantization or related methods for audio language modeling; thus, it does not meet the inclusion criteria nor relate to discrete audio token generation or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Strong Consistency of Sparse K-means Clustering",
    "abstract": "In this paper, we study the strong consistency of the sparse K-means clustering for high dimensional data. We prove the consistency in both risk and clustering for the Euclidean distance. We discuss the characterization of the limit of the clustering under some special cases. For the general (non-Euclidean) distance, we prove the consistency in risk. Our result naturally extends to other models with the same objective function but different constraints such as l0 or l1 penalty in recent literature.",
    "metadata": {
      "arxiv_id": "2501.09983",
      "title": "Strong Consistency of Sparse K-means Clustering",
      "summary": "In this paper, we study the strong consistency of the sparse K-means clustering for high dimensional data. We prove the consistency in both risk and clustering for the Euclidean distance. We discuss the characterization of the limit of the clustering under some special cases. For the general (non-Euclidean) distance, we prove the consistency in risk. Our result naturally extends to other models with the same objective function but different constraints such as l0 or l1 penalty in recent literature.",
      "authors": [
        "Jeungju Kim",
        "Johan Lim"
      ],
      "published": "2025-01-17T06:50:24Z",
      "updated": "2025-04-14T04:36:14Z",
      "categories": [
        "math.ST"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.09983v2",
      "landing_url": "https://arxiv.org/abs/2501.09983v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.09983"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on the theoretical consistency of sparse K-means clustering for high-dimensional data and does not address discrete audio tokens or their generation/quantization; it fails all inclusion criteria and is not relevant to the topic.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on the theoretical consistency of sparse K-means clustering for high-dimensional data and does not address discrete audio tokens or their generation/quantization; it fails all inclusion criteria and is not relevant to the topic.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on the theoretical properties of sparse K-means clustering in high-dimensional data, without any reference to discrete audio token generation, vector quantization, or related audio language modeling tasks. It does not describe token generation processes, quantization methods, or applications in audio understanding or generation as required by the inclusion criteria, and thus it does not fit the topic of discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on the theoretical properties of sparse K-means clustering in high-dimensional data, without any reference to discrete audio token generation, vector quantization, or related audio language modeling tasks. It does not describe token generation processes, quantization methods, or applications in audio understanding or generation as required by the inclusion criteria, and thus it does not fit the topic of discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Self-Supervised Learning Using Nonlinear Dependence",
    "abstract": "Self-supervised learning has gained significant attention in contemporary applications, particularly due to the scarcity of labeled data. While existing SSL methodologies primarily address feature variance and linear correlations, they often neglect the intricate relations between samples and the nonlinear dependencies inherent in complex data--especially prevalent in high-dimensional visual data. In this paper, we introduce Correlation-Dependence Self-Supervised Learning (CDSSL), a novel framework that unifies and extends existing SSL paradigms by integrating both linear correlations and nonlinear dependencies, encapsulating sample-wise and feature-wise interactions. Our approach incorporates the Hilbert-Schmidt Independence Criterion (HSIC) to robustly capture nonlinear dependencies within a Reproducing Kernel Hilbert Space, enriching representation learning. Experimental evaluations on diverse benchmarks demonstrate the efficacy of CDSSL in improving representation quality.",
    "metadata": {
      "arxiv_id": "2501.18875",
      "title": "Self-Supervised Learning Using Nonlinear Dependence",
      "summary": "Self-supervised learning has gained significant attention in contemporary applications, particularly due to the scarcity of labeled data. While existing SSL methodologies primarily address feature variance and linear correlations, they often neglect the intricate relations between samples and the nonlinear dependencies inherent in complex data--especially prevalent in high-dimensional visual data. In this paper, we introduce Correlation-Dependence Self-Supervised Learning (CDSSL), a novel framework that unifies and extends existing SSL paradigms by integrating both linear correlations and nonlinear dependencies, encapsulating sample-wise and feature-wise interactions. Our approach incorporates the Hilbert-Schmidt Independence Criterion (HSIC) to robustly capture nonlinear dependencies within a Reproducing Kernel Hilbert Space, enriching representation learning. Experimental evaluations on diverse benchmarks demonstrate the efficacy of CDSSL in improving representation quality.",
      "authors": [
        "M. Hadi Sepanj",
        "Benyamin Ghojogh",
        "Paul Fieguth"
      ],
      "published": "2025-01-31T04:11:34Z",
      "updated": "2025-11-16T22:59:47Z",
      "categories": [
        "cs.LG",
        "cs.CV",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.18875v2",
      "landing_url": "https://arxiv.org/abs/2501.18875v2",
      "doi": "https://doi.org/10.1109/ACCESS.2025.3628158"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on self-supervised learning with nonlinear dependencies using HSIC, not on discrete audio token representations or tokenization methods, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on self-supervised learning with nonlinear dependencies using HSIC, not on discrete audio token representations or tokenization methods, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a self-supervised learning framework that captures nonlinear dependencies in feature representations, primarily targeting visual data, without any mention of discrete audio tokens or audio signal processing. It does not address the generation of discrete tokens from continuous audio signals, nor does it align with the criteria requiring token design and quantization methods related to audio for sequence or language modeling. Thus, the study does not meet the inclusion criteria related to discrete audio tokens and focuses on a different domain.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a self-supervised learning framework that captures nonlinear dependencies in feature representations, primarily targeting visual data, without any mention of discrete audio tokens or audio signal processing. It does not address the generation of discrete tokens from continuous audio signals, nor does it align with the criteria requiring token design and quantization methods related to audio for sequence or language modeling. Thus, the study does not meet the inclusion criteria related to discrete audio tokens and focuses on a different domain.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ComplexDec: A Domain-robust High-fidelity Neural Audio Codec with Complex Spectrum Modeling",
    "abstract": "Neural audio codecs have been widely adopted in audio-generative tasks because their compact and discrete representations are suitable for both large-language-model-style and regression-based generative models. However, most neural codecs struggle to model out-of-domain audio, resulting in error propagations to downstream generative tasks. In this paper, we first argue that information loss from codec compression degrades out-of-domain robustness. Then, we propose full-band 48~kHz ComplexDec with complex spectral input and output to ease the information loss while adopting the same 24~kbps bitrate as the baseline AuidoDec and ScoreDec. Objective and subjective evaluations demonstrate the out-of-domain robustness of ComplexDec trained using only the 30-hour VCTK corpus.",
    "metadata": {
      "arxiv_id": "2502.02019",
      "title": "ComplexDec: A Domain-robust High-fidelity Neural Audio Codec with Complex Spectrum Modeling",
      "summary": "Neural audio codecs have been widely adopted in audio-generative tasks because their compact and discrete representations are suitable for both large-language-model-style and regression-based generative models. However, most neural codecs struggle to model out-of-domain audio, resulting in error propagations to downstream generative tasks. In this paper, we first argue that information loss from codec compression degrades out-of-domain robustness. Then, we propose full-band 48~kHz ComplexDec with complex spectral input and output to ease the information loss while adopting the same 24~kbps bitrate as the baseline AuidoDec and ScoreDec. Objective and subjective evaluations demonstrate the out-of-domain robustness of ComplexDec trained using only the 30-hour VCTK corpus.",
      "authors": [
        "Yi-Chiao Wu",
        "Dejan Marković",
        "Steven Krenn",
        "Israel D. Gebru",
        "Alexander Richard"
      ],
      "published": "2025-02-04T05:16:15Z",
      "updated": "2025-02-04T05:16:15Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.02019v1",
      "landing_url": "https://arxiv.org/abs/2502.02019v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.02019"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on a domain-robust high-fidelity neural audio codec with complex spectrum modeling and does not describe discrete audio tokens, token formation, or a discrete token-based downstream task, thus failing the core inclusion criterion for discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on a domain-robust high-fidelity neural audio codec with complex spectrum modeling and does not describe discrete audio tokens, token formation, or a discrete token-based downstream task, thus failing the core inclusion criterion for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a neural audio codec (ComplexDec) that uses complex spectral inputs and outputs to reduce information loss and improve out-of-domain robustness. However, the abstract does not clarify whether the codec produces discrete audio tokens via vector quantization or similar discrete representation mechanisms. It mainly emphasizes high-fidelity codec design and robustness rather than explicitly detailing token generation processes or semantic disentanglement. Thus, it lacks clear evidence of meeting the core inclusion criteria related to discrete token design and usage for sequence modeling or audio language modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes a neural audio codec (ComplexDec) that uses complex spectral inputs and outputs to reduce information loss and improve out-of-domain robustness. However, the abstract does not clarify whether the codec produces discrete audio tokens via vector quantization or similar discrete representation mechanisms. It mainly emphasizes high-fidelity codec design and robustness rather than explicitly detailing token generation processes or semantic disentanglement. Thus, it lacks clear evidence of meeting the core inclusion criteria related to discrete token design and usage for sequence modeling or audio language modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "On the Guidance of Flow Matching",
    "abstract": "Flow matching has shown state-of-the-art performance in various generative tasks, ranging from image generation to decision-making, where generation under energy guidance (abbreviated as guidance in the following) is pivotal. However, the guidance of flow matching is more general than and thus substantially different from that of its predecessor, diffusion models. Therefore, the challenge in guidance for general flow matching remains largely underexplored. In this paper, we propose the first framework of general guidance for flow matching. From this framework, we derive a family of guidance techniques that can be applied to general flow matching. These include a new training-free asymptotically exact guidance, novel training losses for training-based guidance, and two classes of approximate guidance that cover classical gradient guidance methods as special cases. We theoretically investigate these different methods to give a practical guideline for choosing suitable methods in different scenarios. Experiments on synthetic datasets, image inverse problems, and offline reinforcement learning demonstrate the effectiveness of our proposed guidance methods and verify the correctness of our flow matching guidance framework. Code to reproduce the experiments can be found at https://github.com/AI4Science-WestlakeU/flow_guidance.",
    "metadata": {
      "arxiv_id": "2502.02150",
      "title": "On the Guidance of Flow Matching",
      "summary": "Flow matching has shown state-of-the-art performance in various generative tasks, ranging from image generation to decision-making, where generation under energy guidance (abbreviated as guidance in the following) is pivotal. However, the guidance of flow matching is more general than and thus substantially different from that of its predecessor, diffusion models. Therefore, the challenge in guidance for general flow matching remains largely underexplored. In this paper, we propose the first framework of general guidance for flow matching. From this framework, we derive a family of guidance techniques that can be applied to general flow matching. These include a new training-free asymptotically exact guidance, novel training losses for training-based guidance, and two classes of approximate guidance that cover classical gradient guidance methods as special cases. We theoretically investigate these different methods to give a practical guideline for choosing suitable methods in different scenarios. Experiments on synthetic datasets, image inverse problems, and offline reinforcement learning demonstrate the effectiveness of our proposed guidance methods and verify the correctness of our flow matching guidance framework. Code to reproduce the experiments can be found at https://github.com/AI4Science-WestlakeU/flow_guidance.",
      "authors": [
        "Ruiqi Feng",
        "Chenglei Yu",
        "Wenhao Deng",
        "Peiyan Hu",
        "Tailin Wu"
      ],
      "published": "2025-02-04T09:27:57Z",
      "updated": "2025-05-26T17:27:36Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.02150v3",
      "landing_url": "https://arxiv.org/abs/2502.02150v3",
      "doi": "https://doi.org/10.48550/arXiv.2502.02150"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article addresses general flow matching guidance in generative tasks, not discrete audio token representations, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article addresses general flow matching guidance in generative tasks, not discrete audio token representations, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on flow matching and its guidance methods in generative tasks but do not mention any approach related to discrete audio tokens, quantization methods, or audio representations. The study does not seem to relate to the inclusion criteria centered on discrete audio token design or generation for sequence-based audio modeling tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on flow matching and its guidance methods in generative tasks but do not mention any approach related to discrete audio tokens, quantization methods, or audio representations. The study does not seem to relate to the inclusion criteria centered on discrete audio token design or generation for sequence-based audio modeling tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling",
    "abstract": "Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \\textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability.",
    "metadata": {
      "arxiv_id": "2502.02942",
      "title": "GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling",
      "summary": "Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \\textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability.",
      "authors": [
        "Jixun Yao",
        "Hexin Liu",
        "Chen Chen",
        "Yuchen Hu",
        "EngSiong Chng",
        "Lei Xie"
      ],
      "published": "2025-02-05T07:14:39Z",
      "updated": "2025-02-05T07:14:39Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.02942v1",
      "landing_url": "https://arxiv.org/abs/2502.02942v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.02942"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly tokenizes speech into semantic tokens via a pretrained self-supervised model and into acoustic tokens via a neural codec, employs hierarchical modeling to decouple semantic and acoustic token generation, and targets token-level representations for language-model-based speech processing, aligning with the discrete audio token criterion and not presenting any listed exclusion issues.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly tokenizes speech into semantic tokens via a pretrained self-supervised model and into acoustic tokens via a neural codec, employs hierarchical modeling to decouple semantic and acoustic token generation, and targets token-level representations for language-model-based speech processing, aligning with the discrete audio token criterion and not presenting any listed exclusion issues.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes GenSE, a speech enhancement framework that tokenizes speech signals into semantic tokens and acoustic tokens using a pre-trained self-supervised model and a custom neural codec; it uses a hierarchical modeling approach to separate semantic and acoustic token generation and employs these discrete tokens for conditional language modeling to enhance speech. The approach clearly involves converting continuous speech signals into discrete tokens (semantic and acoustic), generated by a learned quantization model, supporting usage in sequence modeling via language models. The paper provides methodological details such as tokenization process and hierarchical modeling, aligning well with the inclusion criteria focused on discrete audio token design for downstream generative and understanding tasks. No exclusion criteria are triggered, as the work includes detailed method descriptions and experiments. Therefore, the study fits well within the scope of discrete audio tokens research and should be included.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes GenSE, a speech enhancement framework that tokenizes speech signals into semantic tokens and acoustic tokens using a pre-trained self-supervised model and a custom neural codec; it uses a hierarchical modeling approach to separate semantic and acoustic token generation and employs these discrete tokens for conditional language modeling to enhance speech. The approach clearly involves converting continuous speech signals into discrete tokens (semantic and acoustic), generated by a learned quantization model, supporting usage in sequence modeling via language models. The paper provides methodological details such as tokenization process and hierarchical modeling, aligning well with the inclusion criteria focused on discrete audio token design for downstream generative and understanding tasks. No exclusion criteria are triggered, as the work includes detailed method descriptions and experiments. Therefore, the study fits well within the scope of discrete audio tokens research and should be included.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation",
    "abstract": "Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.",
    "metadata": {
      "arxiv_id": "2502.03930",
      "title": "DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation",
      "summary": "Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.",
      "authors": [
        "Dongya Jia",
        "Zhuo Chen",
        "Jiawei Chen",
        "Chenpeng Du",
        "Jian Wu",
        "Jian Cong",
        "Xiaobin Zhuang",
        "Chumin Li",
        "Zhen Wei",
        "Yuping Wang",
        "Yuxuan Wang"
      ],
      "published": "2025-02-06T10:09:49Z",
      "updated": "2025-12-08T08:11:20Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.03930v4",
      "landing_url": "https://arxiv.org/abs/2502.03930v4",
      "doi": "https://doi.org/10.48550/arXiv.2502.03930"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "DiTAR focuses on diffusion-based autoregressive modeling for continuous speech representations without discrete audio tokens; it lacks a discrete-token design/quantization mechanism central to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "DiTAR focuses on diffusion-based autoregressive modeling for continuous speech representations without discrete audio tokens; it lacks a discrete-token design/quantization mechanism central to the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a method (DiTAR) combining diffusion and autoregressive transformer models for continuous speech representation generation, without discrete speech tokens; the approach focuses on continuous tokens and does not mention any discrete token generation, quantization, or token design, which are central to the inclusion criteria. Therefore, it does not fulfill the requirement of employing or proposing a discrete audio token-based method for sequence modeling or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a method (DiTAR) combining diffusion and autoregressive transformer models for continuous speech representation generation, without discrete speech tokens; the approach focuses on continuous tokens and does not mention any discrete token generation, quantization, or token design, which are central to the inclusion criteria. Therefore, it does not fulfill the requirement of employing or proposing a discrete audio token-based method for sequence modeling or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance",
    "abstract": "While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs. We introduce Koel-TTS, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models. Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio. Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset. Audio samples and demos are available on our website.",
    "metadata": {
      "arxiv_id": "2502.05236",
      "title": "Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance",
      "summary": "While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs. We introduce Koel-TTS, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models. Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio. Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset. Audio samples and demos are available on our website.",
      "authors": [
        "Shehzeen Hussain",
        "Paarth Neekhara",
        "Xuesong Yang",
        "Edresson Casanova",
        "Subhankar Ghosh",
        "Mikyas T. Desta",
        "Roy Fejgin",
        "Rafael Valle",
        "Jason Li"
      ],
      "published": "2025-02-07T06:47:11Z",
      "updated": "2025-07-22T21:32:13Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.05236v2",
      "landing_url": "https://arxiv.org/abs/2502.05236v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.05236"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract references discrete acoustic tokens but provides no details on token generation/quantization mechanisms, codebooks, or reproducible experimental setups, failing the requirement for explicit discrete-token design and methods; thus it should be excluded under the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract references discrete acoustic tokens but provides no details on token generation/quantization mechanisms, codebooks, or reproducible experimental setups, failing the requirement for explicit discrete-token design and methods; thus it should be excluded under the criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents Koel-TTS, an enhanced autoregressive speech token generation model that directly maps text and context audio to acoustic tokens, clearly involving discrete token representations in TTS. It discusses the generation process and evaluation, aligns with the inclusion criteria of discrete audio token design and its application for sequence modeling in speech synthesis, and provides experiments demonstrating improved synthesis metrics. Despite the abstract not detailing the exact tokenization mechanism or quantization specifics, the description strongly implies a novel discrete token approach beyond traditional codecs, fulfilling the main core method requirements. No exclusion criteria such as lack of method details or outdated sources apply based on the provided abstract.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The study presents Koel-TTS, an enhanced autoregressive speech token generation model that directly maps text and context audio to acoustic tokens, clearly involving discrete token representations in TTS. It discusses the generation process and evaluation, aligns with the inclusion criteria of discrete audio token design and its application for sequence modeling in speech synthesis, and provides experiments demonstrating improved synthesis metrics. Despite the abstract not detailing the exact tokenization mechanism or quantization specifics, the description strongly implies a novel discrete token approach beyond traditional codecs, fulfilling the main core method requirements. No exclusion criteria such as lack of method details or outdated sources apply based on the provided abstract.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviews noted the abstract references 'acoustic tokens' and a mapping from text+context audio to tokens, but disagree on sufficiency: JuniorNano excludes due to absent token-generation/quantization/codebook details, while JuniorMini assumes the implication of a discrete-token approach. The inclusion criteria require that discrete audio-token design be a core contribution and that token generation/quantization (e.g., VQ/RVQ/hierarchical design), codebook/layer details, and reproducible method/setup be clearly described; the provided abstract does not supply these necessary technical specifics or indicate availability of a full reproducible manuscript. Because the abstract alone fails to demonstrate the required explicit token-formation methodology and reproducibility, it does not meet all inclusion criteria and leans toward exclusion.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviews noted the abstract references 'acoustic tokens' and a mapping from text+context audio to tokens, but disagree on sufficiency: JuniorNano excludes due to absent token-generation/quantization/codebook details, while JuniorMini assumes the implication of a discrete-token approach. The inclusion criteria require that discrete audio-token design be a core contribution and that token generation/quantization (e.g., VQ/RVQ/hierarchical design), codebook/layer details, and reproducible method/setup be clearly described; the provided abstract does not supply these necessary technical specifics or indicate availability of a full reproducible manuscript. Because the abstract alone fails to demonstrate the required explicit token-formation methodology and reproducibility, it does not meet all inclusion criteria and leans toward exclusion.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "4D VQ-GAN: Synthesising Medical Scans at Any Time Point for Personalised Disease Progression Modelling of Idiopathic Pulmonary Fibrosis",
    "abstract": "Understanding the progression trajectories of diseases is crucial for early diagnosis and effective treatment planning. This is especially vital for life-threatening conditions such as Idiopathic Pulmonary Fibrosis (IPF), a chronic, progressive lung disease with a prognosis comparable to many cancers. Computed tomography (CT) imaging has been established as a reliable diagnostic tool for IPF. Accurately predicting future CT scans of early-stage IPF patients can aid in developing better treatment strategies, thereby improving survival outcomes. In this paper, we propose 4D Vector Quantised Generative Adversarial Networks (4D-VQ-GAN), a model capable of generating realistic CT volumes of IPF patients at any time point. The model is trained using a two-stage approach. In the first stage, a 3D-VQ-GAN is trained to reconstruct CT volumes. In the second stage, a Neural Ordinary Differential Equation (ODE) based temporal model is trained to capture the temporal dynamics of the quantised embeddings generated by the encoder in the first stage. We evaluate different configurations of our model for generating longitudinal CT scans and compare the results against ground truth data, both quantitatively and qualitatively. For validation, we conduct survival analysis using imaging biomarkers derived from generated CT scans and achieve a C-index comparable to that of biomarkers derived from the real CT scans. The survival analysis results demonstrate the potential clinical utility inherent to generated longitudinal CT scans, showing that they can reliably predict survival outcomes.",
    "metadata": {
      "arxiv_id": "2502.05713",
      "title": "4D VQ-GAN: Synthesising Medical Scans at Any Time Point for Personalised Disease Progression Modelling of Idiopathic Pulmonary Fibrosis",
      "summary": "Understanding the progression trajectories of diseases is crucial for early diagnosis and effective treatment planning. This is especially vital for life-threatening conditions such as Idiopathic Pulmonary Fibrosis (IPF), a chronic, progressive lung disease with a prognosis comparable to many cancers. Computed tomography (CT) imaging has been established as a reliable diagnostic tool for IPF. Accurately predicting future CT scans of early-stage IPF patients can aid in developing better treatment strategies, thereby improving survival outcomes. In this paper, we propose 4D Vector Quantised Generative Adversarial Networks (4D-VQ-GAN), a model capable of generating realistic CT volumes of IPF patients at any time point. The model is trained using a two-stage approach. In the first stage, a 3D-VQ-GAN is trained to reconstruct CT volumes. In the second stage, a Neural Ordinary Differential Equation (ODE) based temporal model is trained to capture the temporal dynamics of the quantised embeddings generated by the encoder in the first stage. We evaluate different configurations of our model for generating longitudinal CT scans and compare the results against ground truth data, both quantitatively and qualitatively. For validation, we conduct survival analysis using imaging biomarkers derived from generated CT scans and achieve a C-index comparable to that of biomarkers derived from the real CT scans. The survival analysis results demonstrate the potential clinical utility inherent to generated longitudinal CT scans, showing that they can reliably predict survival outcomes.",
      "authors": [
        "An Zhao",
        "Moucheng Xu",
        "Ahmed H. Shahin",
        "Wim Wuyts",
        "Mark G. Jones",
        "Joseph Jacob",
        "Daniel C. Alexander"
      ],
      "published": "2025-02-08T22:25:53Z",
      "updated": "2025-02-08T22:25:53Z",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.05713v1",
      "landing_url": "https://arxiv.org/abs/2502.05713v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.05713"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on 4D-VQ-GAN for synthesising longitudinal CT scans in IPF, i.e., medical imaging generation with vector-quantized representations, not on discrete audio tokens or audio-token design as defined in the inclusion criteria. It therefore does not meet the inclusion criteria (discrete audio token design) and should be excluded based on mismatch with the topic scope.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "This item focuses on 4D-VQ-GAN for synthesising longitudinal CT scans in IPF, i.e., medical imaging generation with vector-quantized representations, not on discrete audio tokens or audio-token design as defined in the inclusion criteria. It therefore does not meet the inclusion criteria (discrete audio token design) and should be excluded based on mismatch with the topic scope.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract discuss generating synthetic medical CT scans of lung disease using a 4D Vector Quantised Generative Adversarial Network (4D-VQ-GAN), focusing on medical imaging and disease progression, not on discrete audio tokens or audio signal processing. Although the method involves vector quantization, it relates to medical image volume generation rather than converting continuous audio signals into discrete tokens for language modeling or audio understanding/generation tasks. The content does not meet the inclusion criteria centered on discrete audio tokens and their usage in audio-related tasks, nor does it focus on audio representations or their sequence modeling. Therefore, it does not fulfill the required conditions for inclusion based on the stated criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract discuss generating synthetic medical CT scans of lung disease using a 4D Vector Quantised Generative Adversarial Network (4D-VQ-GAN), focusing on medical imaging and disease progression, not on discrete audio tokens or audio signal processing. Although the method involves vector quantization, it relates to medical image volume generation rather than converting continuous audio signals into discrete tokens for language modeling or audio understanding/generation tasks. The content does not meet the inclusion criteria centered on discrete audio tokens and their usage in audio-related tasks, nor does it focus on audio representations or their sequence modeling. Therefore, it does not fulfill the required conditions for inclusion based on the stated criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Recent Advances in Discrete Speech Tokens: A Review",
    "abstract": "The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.",
    "metadata": {
      "arxiv_id": "2502.06490",
      "title": "Recent Advances in Discrete Speech Tokens: A Review",
      "summary": "The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.",
      "authors": [
        "Yiwei Guo",
        "Zhihan Li",
        "Hankun Wang",
        "Bohan Li",
        "Chongtian Shao",
        "Hanglei Zhang",
        "Chenpeng Du",
        "Xie Chen",
        "Shujie Liu",
        "Kai Yu"
      ],
      "published": "2025-02-10T14:08:25Z",
      "updated": "2025-12-12T05:18:11Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.MM",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.06490v4",
      "landing_url": "https://arxiv.org/abs/2502.06490v4",
      "doi": "https://doi.org/10.48550/arXiv.2502.06490"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is a survey on discrete speech tokens that discusses tokenization design and comparisons of token types, aligning with the Discrete Audio Tokens topic and not a mere demo; it appears to provide methodological discussion typical for inclusion, though full reproducibility details depend on the full text.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item is a survey on discrete speech tokens that discusses tokenization design and comparisons of token types, aligning with the Discrete Audio Tokens topic and not a mere demo; it appears to provide methodological discussion typical for inclusion, though full reproducibility details depend on the full text.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract clearly focuses on discrete speech tokens as a foundational representation for speech, aligning with the inclusion criteria defining discrete audio tokens generated via vector quantization or semantic discretization for use in language models; it discusses token generation, usage in modeling, and addresses both acoustic and semantic tokens, fulfilling technical coverage requirements. The content is a systematic survey including taxonomy, comparisons, and future directions, implying a comprehensive methodological overview rather than a superficial or demo-level presentation, thus not falling under exclusions. The study is recent and presented in English with sufficient detail to assess method and usage, meeting all inclusion criteria without exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The abstract clearly focuses on discrete speech tokens as a foundational representation for speech, aligning with the inclusion criteria defining discrete audio tokens generated via vector quantization or semantic discretization for use in language models; it discusses token generation, usage in modeling, and addresses both acoustic and semantic tokens, fulfilling technical coverage requirements. The content is a systematic survey including taxonomy, comparisons, and future directions, implying a comprehensive methodological overview rather than a superficial or demo-level presentation, thus not falling under exclusions. The study is recent and presented in English with sufficient detail to assess method and usage, meeting all inclusion criteria without exclusions.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Variational Rectified Flow Matching",
    "abstract": "We study Variational Rectified Flow Matching, a framework that enhances classic rectified flow matching by modeling multi-modal velocity vector-fields. At inference time, classic rectified flow matching 'moves' samples from a source distribution to the target distribution by solving an ordinary differential equation via integration along a velocity vector-field. At training time, the velocity vector-field is learnt by linearly interpolating between coupled samples one drawn from the source and one drawn from the target distribution randomly. This leads to ''ground-truth'' velocity vector-fields that point in different directions at the same location, i.e., the velocity vector-fields are multi-modal/ambiguous. However, since training uses a standard mean-squared-error loss, the learnt velocity vector-field averages ''ground-truth'' directions and isn't multi-modal. In contrast, variational rectified flow matching learns and samples from multi-modal flow directions. We show on synthetic data, MNIST, CIFAR-10, and ImageNet that variational rectified flow matching leads to compelling results.",
    "metadata": {
      "arxiv_id": "2502.09616",
      "title": "Variational Rectified Flow Matching",
      "summary": "We study Variational Rectified Flow Matching, a framework that enhances classic rectified flow matching by modeling multi-modal velocity vector-fields. At inference time, classic rectified flow matching 'moves' samples from a source distribution to the target distribution by solving an ordinary differential equation via integration along a velocity vector-field. At training time, the velocity vector-field is learnt by linearly interpolating between coupled samples one drawn from the source and one drawn from the target distribution randomly. This leads to ''ground-truth'' velocity vector-fields that point in different directions at the same location, i.e., the velocity vector-fields are multi-modal/ambiguous. However, since training uses a standard mean-squared-error loss, the learnt velocity vector-field averages ''ground-truth'' directions and isn't multi-modal. In contrast, variational rectified flow matching learns and samples from multi-modal flow directions. We show on synthetic data, MNIST, CIFAR-10, and ImageNet that variational rectified flow matching leads to compelling results.",
      "authors": [
        "Pengsheng Guo",
        "Alexander G. Schwing"
      ],
      "published": "2025-02-13T18:59:15Z",
      "updated": "2025-02-13T18:59:15Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.09616v1",
      "landing_url": "https://arxiv.org/abs/2502.09616v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.09616"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe variational rectified flow matching for multi-modal velocity fields and do not discuss discrete audio token representations or quantization suitable for audio language modeling, so it fails the discrete audio token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe variational rectified flow matching for multi-modal velocity fields and do not discuss discrete audio token representations or quantization suitable for audio language modeling, so it fails the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract discuss Variational Rectified Flow Matching, a framework for modeling multi-modal velocity vector-fields for data distributions such as images, including datasets like MNIST, CIFAR-10, and ImageNet. There is no mention of audio signals, discrete audio tokens, vector quantization, or any audio-related tokenization methods as per the inclusion criteria. The study focuses on probabilistic flow modeling and velocity fields rather than learning or generating discrete audio tokens for sequence modeling or audio understanding/generation. Therefore, this work does not meet the inclusion criteria related to discrete audio tokens and appears outside the scope of the specified topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract discuss Variational Rectified Flow Matching, a framework for modeling multi-modal velocity vector-fields for data distributions such as images, including datasets like MNIST, CIFAR-10, and ImageNet. There is no mention of audio signals, discrete audio tokens, vector quantization, or any audio-related tokenization methods as per the inclusion criteria. The study focuses on probabilistic flow modeling and velocity fields rather than learning or generating discrete audio tokens for sequence modeling or audio understanding/generation. Therefore, this work does not meet the inclusion criteria related to discrete audio tokens and appears outside the scope of the specified topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation",
    "abstract": "In this paper, we propose the Dynamic Latent Frame Rate VAE (DLFR-VAE), a training-free paradigm that can make use of adaptive temporal compression in latent space. While existing video generative models apply fixed compression rates via pretrained VAE, we observe that real-world video content exhibits substantial temporal non-uniformity, with high-motion segments containing more information than static scenes. Based on this insight, DLFR-VAE dynamically adjusts the latent frame rate according to the content complexity. Specifically, DLFR-VAE comprises two core innovations: (1) A Dynamic Latent Frame Rate Scheduler that partitions videos into temporal chunks and adaptively determines optimal frame rates based on information-theoretic content complexity, and (2) A training-free adaptation mechanism that transforms pretrained VAE architectures into a dynamic VAE that can process features with variable frame rates. Our simple but effective DLFR-VAE can function as a plug-and-play module, seamlessly integrating with existing video generation models and accelerating the video generation process.",
    "metadata": {
      "arxiv_id": "2502.11897",
      "title": "DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation",
      "summary": "In this paper, we propose the Dynamic Latent Frame Rate VAE (DLFR-VAE), a training-free paradigm that can make use of adaptive temporal compression in latent space. While existing video generative models apply fixed compression rates via pretrained VAE, we observe that real-world video content exhibits substantial temporal non-uniformity, with high-motion segments containing more information than static scenes. Based on this insight, DLFR-VAE dynamically adjusts the latent frame rate according to the content complexity. Specifically, DLFR-VAE comprises two core innovations: (1) A Dynamic Latent Frame Rate Scheduler that partitions videos into temporal chunks and adaptively determines optimal frame rates based on information-theoretic content complexity, and (2) A training-free adaptation mechanism that transforms pretrained VAE architectures into a dynamic VAE that can process features with variable frame rates. Our simple but effective DLFR-VAE can function as a plug-and-play module, seamlessly integrating with existing video generation models and accelerating the video generation process.",
      "authors": [
        "Zhihang Yuan",
        "Siyuan Wang",
        "Rui Xie",
        "Hanling Zhang",
        "Tongcheng Fang",
        "Yuzhang Shang",
        "Shengen Yan",
        "Guohao Dai",
        "Yu Wang"
      ],
      "published": "2025-02-17T15:22:31Z",
      "updated": "2025-04-02T13:25:35Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.11897v2",
      "landing_url": "https://arxiv.org/abs/2502.11897v2",
      "doi": null
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses a Dynamic Latent Frame Rate VAE for video generation and does not address discrete audio tokens, token design, or audio-language modeling, thus it fails all inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses a Dynamic Latent Frame Rate VAE for video generation and does not address discrete audio tokens, token design, or audio-language modeling, thus it fails all inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper DLFR-VAE focuses on video generation using a dynamic latent frame rate VAE and does not mention discrete audio tokens or any audio-related tokenization methods; thus, it does not meet the inclusion criteria relevant to discrete audio tokens nor involves any audio token design or generation processes.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper DLFR-VAE focuses on video generation using a dynamic latent frame rate VAE and does not mention discrete audio tokens or any audio-related tokenization methods; thus, it does not meet the inclusion criteria relevant to discrete audio tokens nor involves any audio token design or generation processes.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Demographic Attributes Prediction from Speech Using WavLM Embeddings",
    "abstract": "This paper introduces a general classifier based on WavLM features, to infer demographic characteristics, such as age, gender, native language, education, and country, from speech. Demographic feature prediction plays a crucial role in applications like language learning, accessibility, and digital forensics, enabling more personalized and inclusive technologies. Leveraging pretrained models for embedding extraction, the proposed framework identifies key acoustic and linguistic fea-tures associated with demographic attributes, achieving a Mean Absolute Error (MAE) of 4.94 for age prediction and over 99.81% accuracy for gender classification across various datasets. Our system improves upon existing models by up to relative 30% in MAE and up to relative 10% in accuracy and F1 scores across tasks, leveraging a diverse range of datasets and large pretrained models to ensure robustness and generalizability. This study offers new insights into speaker diversity and provides a strong foundation for future research in speech-based demographic profiling.",
    "metadata": {
      "arxiv_id": "2502.12007",
      "title": "Demographic Attributes Prediction from Speech Using WavLM Embeddings",
      "summary": "This paper introduces a general classifier based on WavLM features, to infer demographic characteristics, such as age, gender, native language, education, and country, from speech. Demographic feature prediction plays a crucial role in applications like language learning, accessibility, and digital forensics, enabling more personalized and inclusive technologies. Leveraging pretrained models for embedding extraction, the proposed framework identifies key acoustic and linguistic fea-tures associated with demographic attributes, achieving a Mean Absolute Error (MAE) of 4.94 for age prediction and over 99.81% accuracy for gender classification across various datasets. Our system improves upon existing models by up to relative 30% in MAE and up to relative 10% in accuracy and F1 scores across tasks, leveraging a diverse range of datasets and large pretrained models to ensure robustness and generalizability. This study offers new insights into speaker diversity and provides a strong foundation for future research in speech-based demographic profiling.",
      "authors": [
        "Yuchen Yang",
        "Thomas Thebaud",
        "Najim Dehak"
      ],
      "published": "2025-02-17T16:43:47Z",
      "updated": "2025-02-17T16:43:47Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.12007v1",
      "landing_url": "https://arxiv.org/abs/2502.12007v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.12007"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This study uses WavLM embeddings to predict demographic attributes from speech and mentions MAE/accuracy; it does not describe discrete audio tokens, token generation, vector quantization, or token-based modeling, thus does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This study uses WavLM embeddings to predict demographic attributes from speech and mentions MAE/accuracy; it does not describe discrete audio tokens, token generation, vector quantization, or token-based modeling, thus does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on predicting demographic attributes from speech using WavLM embeddings, a pretrained feature extractor, rather than proposing or analyzing discrete audio token representations derived through vector quantization or related methods. There is no mention of discrete token generation processes, quantization mechanisms, or usage of tokens for sequence or language modeling. Hence, it does not meet the inclusion criteria focused on discrete audio tokens and token-based modeling, and lacks details on token formation or their use in generation/understanding tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on predicting demographic attributes from speech using WavLM embeddings, a pretrained feature extractor, rather than proposing or analyzing discrete audio token representations derived through vector quantization or related methods. There is no mention of discrete token generation processes, quantization mechanisms, or usage of tokens for sequence or language modeling. Hence, it does not meet the inclusion criteria focused on discrete audio tokens and token-based modeling, and lacks details on token formation or their use in generation/understanding tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening",
    "abstract": "We propose Diffusion-Sharpening, a fine-tuning approach that enhances downstream alignment by optimizing sampling trajectories. Existing RL-based fine-tuning methods focus on single training timesteps and neglect trajectory-level alignment, while recent sampling trajectory optimization methods incur significant inference NFE costs. Diffusion-Sharpening overcomes this by using a path integral framework to select optimal trajectories during training, leveraging reward feedback, and amortizing inference costs. Our method demonstrates superior training efficiency with faster convergence, and best inference efficiency without requiring additional NFEs. Extensive experiments show that Diffusion-Sharpening outperforms RL-based fine-tuning methods (e.g., Diffusion-DPO) and sampling trajectory optimization methods (e.g., Inference Scaling) across diverse metrics including text alignment, compositional capabilities, and human preferences, offering a scalable and efficient solution for future diffusion model fine-tuning. Code: https://github.com/Gen-Verse/Diffusion-Sharpening",
    "metadata": {
      "arxiv_id": "2502.12146",
      "title": "Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening",
      "summary": "We propose Diffusion-Sharpening, a fine-tuning approach that enhances downstream alignment by optimizing sampling trajectories. Existing RL-based fine-tuning methods focus on single training timesteps and neglect trajectory-level alignment, while recent sampling trajectory optimization methods incur significant inference NFE costs. Diffusion-Sharpening overcomes this by using a path integral framework to select optimal trajectories during training, leveraging reward feedback, and amortizing inference costs. Our method demonstrates superior training efficiency with faster convergence, and best inference efficiency without requiring additional NFEs. Extensive experiments show that Diffusion-Sharpening outperforms RL-based fine-tuning methods (e.g., Diffusion-DPO) and sampling trajectory optimization methods (e.g., Inference Scaling) across diverse metrics including text alignment, compositional capabilities, and human preferences, offering a scalable and efficient solution for future diffusion model fine-tuning. Code: https://github.com/Gen-Verse/Diffusion-Sharpening",
      "authors": [
        "Ye Tian",
        "Ling Yang",
        "Xinchen Zhang",
        "Yunhai Tong",
        "Mengdi Wang",
        "Bin Cui"
      ],
      "published": "2025-02-17T18:57:26Z",
      "updated": "2025-02-17T18:57:26Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.12146v1",
      "landing_url": "https://arxiv.org/abs/2502.12146v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.12146"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on Diffusion-Sharpening for fine-tuning diffusion models via sampling trajectory optimization and RL-based methods; it does not discuss discrete audio tokens or tokenization as the core contribution, so it does not meet the inclusion criteria for discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on Diffusion-Sharpening for fine-tuning diffusion models via sampling trajectory optimization and RL-based methods; it does not discuss discrete audio tokens or tokenization as the core contribution, so it does not meet the inclusion criteria for discrete audio token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on fine-tuning diffusion models using denoising trajectory sharpening and does not involve discrete audio tokens, vector quantization, or audio language modeling. The work does not describe any generation or use of discrete audio tokens for audio understanding or generation, thus not meeting the primary inclusion criteria related to discrete audio token design or application.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on fine-tuning diffusion models using denoising trajectory sharpening and does not involve discrete audio tokens, vector quantization, or audio language modeling. The work does not describe any generation or use of discrete audio tokens for audio understanding or generation, thus not meeting the primary inclusion criteria related to discrete audio token design or application.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speech Enhancement Using Continuous Embeddings of Neural Audio Codec",
    "abstract": "Recent advancements in Neural Audio Codec (NAC) models have inspired their use in various speech processing tasks, including speech enhancement (SE). In this work, we propose a novel, efficient SE approach by leveraging the pre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE methods, which process discrete speech tokens using Language Models (LMs), we perform SE within the continuous embedding space of the pretrained NAC, which is highly compressed along the time dimension for efficient representation. Our lightweight SE model, optimized through an embedding-level loss, delivers results comparable to SE baselines trained on larger datasets, with a significantly lower real-time factor of 0.005. Additionally, our method achieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer in a simulated cloud-based audio transmission environment. This work highlights a new, efficient NAC-based SE solution, particularly suitable for cloud applications where NAC is used to compress audio before transmission. Copyright 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.",
    "metadata": {
      "arxiv_id": "2502.16240",
      "title": "Speech Enhancement Using Continuous Embeddings of Neural Audio Codec",
      "summary": "Recent advancements in Neural Audio Codec (NAC) models have inspired their use in various speech processing tasks, including speech enhancement (SE). In this work, we propose a novel, efficient SE approach by leveraging the pre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE methods, which process discrete speech tokens using Language Models (LMs), we perform SE within the continuous embedding space of the pretrained NAC, which is highly compressed along the time dimension for efficient representation. Our lightweight SE model, optimized through an embedding-level loss, delivers results comparable to SE baselines trained on larger datasets, with a significantly lower real-time factor of 0.005. Additionally, our method achieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer in a simulated cloud-based audio transmission environment. This work highlights a new, efficient NAC-based SE solution, particularly suitable for cloud applications where NAC is used to compress audio before transmission.\n  Copyright 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.",
      "authors": [
        "Haoyang Li",
        "Jia Qi Yip",
        "Tianyu Fan",
        "Eng Siong Chng"
      ],
      "published": "2025-02-22T14:25:55Z",
      "updated": "2025-02-22T14:25:55Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16240v1",
      "landing_url": "https://arxiv.org/abs/2502.16240v1",
      "doi": "https://doi.org/10.1109/ICASSP49660.2025.10890379"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work uses continuous embeddings from a neural audio codec for speech enhancement, not producing or utilizing discrete audio tokens via vector quantization or similar tokenization; therefore it does not meet the Discrete Audio Tokens criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work uses continuous embeddings from a neural audio codec for speech enhancement, not producing or utilizing discrete audio tokens via vector quantization or similar tokenization; therefore it does not meet the Discrete Audio Tokens criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study leverages the pre-quantization output of a pretrained Neural Audio Codec encoder for speech enhancement by operating within the continuous embedding space rather than discrete tokens. Although it discusses using Neural Audio Codec, it does not focus on the generation or use of discrete audio tokens obtained via vector quantization or related quantization methods as defined in the inclusion criteria. There is no clear explanation or emphasis on the token generation process, discrete representation, or use for language/sequential models. Hence, it does not meet the core inclusion criterion of involving discrete token representation as the main contribution.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study leverages the pre-quantization output of a pretrained Neural Audio Codec encoder for speech enhancement by operating within the continuous embedding space rather than discrete tokens. Although it discusses using Neural Audio Codec, it does not focus on the generation or use of discrete audio tokens obtained via vector quantization or related quantization methods as defined in the inclusion criteria. There is no clear explanation or emphasis on the token generation process, discrete representation, or use for language/sequential models. Hence, it does not meet the core inclusion criterion of involving discrete token representation as the main contribution.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Unified Semantic and ID Representation Learning for Deep Recommenders",
    "abstract": "Effective recommendation is crucial for large-scale online platforms. Traditional recommendation systems primarily rely on ID tokens to uniquely identify items, which can effectively capture specific item relationships but suffer from issues such as redundancy and poor performance in cold-start scenarios. Recent approaches have explored using semantic tokens as an alternative, yet they face challenges, including item duplication and inconsistent performance gains, leaving the potential advantages of semantic tokens inadequately examined. To address these limitations, we propose a Unified Semantic and ID Representation Learning framework that leverages the complementary strengths of both token types. In our framework, ID tokens capture unique item attributes, while semantic tokens represent shared, transferable characteristics. Additionally, we analyze the role of cosine similarity and Euclidean distance in embedding search, revealing that cosine similarity is more effective in decoupling accumulated embeddings, while Euclidean distance excels in distinguishing unique items. Our framework integrates cosine similarity in earlier layers and Euclidean distance in the final layer to optimize representation learning. Experiments on three benchmark datasets show that our method significantly outperforms state-of-the-art baselines, with improvements ranging from 6\\% to 17\\% and a reduction in token size by over 80%. These results demonstrate the effectiveness of combining ID and semantic tokenization to enhance the generalization ability of recommender systems.",
    "metadata": {
      "arxiv_id": "2502.16474",
      "title": "Unified Semantic and ID Representation Learning for Deep Recommenders",
      "summary": "Effective recommendation is crucial for large-scale online platforms. Traditional recommendation systems primarily rely on ID tokens to uniquely identify items, which can effectively capture specific item relationships but suffer from issues such as redundancy and poor performance in cold-start scenarios. Recent approaches have explored using semantic tokens as an alternative, yet they face challenges, including item duplication and inconsistent performance gains, leaving the potential advantages of semantic tokens inadequately examined. To address these limitations, we propose a Unified Semantic and ID Representation Learning framework that leverages the complementary strengths of both token types. In our framework, ID tokens capture unique item attributes, while semantic tokens represent shared, transferable characteristics. Additionally, we analyze the role of cosine similarity and Euclidean distance in embedding search, revealing that cosine similarity is more effective in decoupling accumulated embeddings, while Euclidean distance excels in distinguishing unique items. Our framework integrates cosine similarity in earlier layers and Euclidean distance in the final layer to optimize representation learning. Experiments on three benchmark datasets show that our method significantly outperforms state-of-the-art baselines, with improvements ranging from 6\\% to 17\\% and a reduction in token size by over 80%. These results demonstrate the effectiveness of combining ID and semantic tokenization to enhance the generalization ability of recommender systems.",
      "authors": [
        "Guanyu Lin",
        "Zhigang Hua",
        "Tao Feng",
        "Shuang Yang",
        "Bo Long",
        "Jiaxuan You"
      ],
      "published": "2025-02-23T07:17:28Z",
      "updated": "2025-02-23T07:17:28Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16474v1",
      "landing_url": "https://arxiv.org/abs/2502.16474v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.16474"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article tackles recommender-system representations with ID and semantic tokens, not discrete audio token representations for audio modeling, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article tackles recommender-system representations with ID and semantic tokens, not discrete audio token representations for audio modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on recommendation systems leveraging ID and semantic tokens representing item attributes, without addressing discrete audio tokens or audio signal processing; there is no mention of converting continuous audio signals into discrete tokens or using vector quantization for audio representation as per inclusion criteria, thus it does not fit the defined scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on recommendation systems leveraging ID and semantic tokens representing item attributes, without addressing discrete audio tokens or audio signal processing; there is no mention of converting continuous audio signals into discrete tokens or using vector quantization for audio representation as per inclusion criteria, thus it does not fit the defined scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Towards Improved Text-Aligned Codebook Learning: Multi-Hierarchical Codebook-Text Alignment with Long Text",
    "abstract": "Image quantization is a crucial technique in image generation, aimed at learning a codebook that encodes an image into a discrete token sequence. Recent advancements have seen researchers exploring learning multi-modal codebook (i.e., text-aligned codebook) by utilizing image caption semantics, aiming to enhance codebook performance in cross-modal tasks. However, existing image-text paired datasets exhibit a notable flaw in that the text descriptions tend to be overly concise, failing to adequately describe the images and provide sufficient semantic knowledge, resulting in limited alignment of text and codebook at a fine-grained level. In this paper, we propose a novel Text-Augmented Codebook Learning framework, named TA-VQ, which generates longer text for each image using the visual-language model for improved text-aligned codebook learning. However, the long text presents two key challenges: how to encode text and how to align codebook and text. To tackle two challenges, we propose to split the long text into multiple granularities for encoding, i.e., word, phrase, and sentence, so that the long text can be fully encoded without losing any key semantic knowledge. Following this, a hierarchical encoder and novel sampling-based alignment strategy are designed to achieve fine-grained codebook-text alignment. Additionally, our method can be seamlessly integrated into existing VQ models. Extensive experiments in reconstruction and various downstream tasks demonstrate its effectiveness compared to previous state-of-the-art approaches.",
    "metadata": {
      "arxiv_id": "2503.01261",
      "title": "Towards Improved Text-Aligned Codebook Learning: Multi-Hierarchical Codebook-Text Alignment with Long Text",
      "summary": "Image quantization is a crucial technique in image generation, aimed at learning a codebook that encodes an image into a discrete token sequence. Recent advancements have seen researchers exploring learning multi-modal codebook (i.e., text-aligned codebook) by utilizing image caption semantics, aiming to enhance codebook performance in cross-modal tasks. However, existing image-text paired datasets exhibit a notable flaw in that the text descriptions tend to be overly concise, failing to adequately describe the images and provide sufficient semantic knowledge, resulting in limited alignment of text and codebook at a fine-grained level. In this paper, we propose a novel Text-Augmented Codebook Learning framework, named TA-VQ, which generates longer text for each image using the visual-language model for improved text-aligned codebook learning. However, the long text presents two key challenges: how to encode text and how to align codebook and text. To tackle two challenges, we propose to split the long text into multiple granularities for encoding, i.e., word, phrase, and sentence, so that the long text can be fully encoded without losing any key semantic knowledge. Following this, a hierarchical encoder and novel sampling-based alignment strategy are designed to achieve fine-grained codebook-text alignment. Additionally, our method can be seamlessly integrated into existing VQ models. Extensive experiments in reconstruction and various downstream tasks demonstrate its effectiveness compared to previous state-of-the-art approaches.",
      "authors": [
        "Guotao Liang",
        "Baoquan Zhang",
        "Zhiyuan Wen",
        "Junteng Zhao",
        "Yunming Ye",
        "Kola Ye",
        "Yao He"
      ],
      "published": "2025-03-03T07:38:18Z",
      "updated": "2025-03-11T06:09:18Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.01261v2",
      "landing_url": "https://arxiv.org/abs/2503.01261v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.01261"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本項目聚焦於影像-文本多模態碼本的對齊與長文本生成，未涉及離散音訊 token 的設計、量化流程或與音訊相關的自監督學習，與本主題的「Discrete Audio Tokens」研究方向不符，因此應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本項目聚焦於影像-文本多模態碼本的對齊與長文本生成，未涉及離散音訊 token 的設計、量化流程或與音訊相關的自監督學習，與本主題的「Discrete Audio Tokens」研究方向不符，因此應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on image quantization and text-aligned codebook learning for image generation, without dealing with discrete audio tokens or audio signal processing; thus, it does not meet the inclusion criteria related to discrete audio token design or usage in audio language modeling or generation tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on image quantization and text-aligned codebook learning for image generation, without dealing with discrete audio tokens or audio signal processing; thus, it does not meet the inclusion criteria related to discrete audio token design or usage in audio language modeling or generation tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Privacy and Accuracy-Aware AI/ML Model Deduplication",
    "abstract": "With the growing adoption of privacy-preserving machine learning algorithms, such as Differentially Private Stochastic Gradient Descent (DP-SGD), training or fine-tuning models on private datasets has become increasingly prevalent. This shift has led to the need for models offering varying privacy guarantees and utility levels to satisfy diverse user requirements. However, managing numerous versions of large models introduces significant operational challenges, including increased inference latency, higher resource consumption, and elevated costs. Model deduplication is a technique widely used by many model serving and database systems to support high-performance and low-cost inference queries and model diagnosis queries. However, none of the existing model deduplication works has considered privacy, leading to unbounded aggregation of privacy costs for certain deduplicated models and inefficiencies when applied to deduplicate DP-trained models. We formalize the problems of deduplicating DP-trained models for the first time and propose a novel privacy- and accuracy-aware deduplication mechanism to address the problems. We developed a greedy strategy to select and assign base models to target models to minimize storage and privacy costs. When deduplicating a target model, we dynamically schedule accuracy validations and apply the Sparse Vector Technique to reduce the privacy costs associated with private validation data. Compared to baselines that do not provide privacy guarantees, our approach improved the compression ratio by up to $35\\times$ for individual models (including large language models and vision transformers). We also observed up to $43\\times$ inference speedup due to the reduction of I/O operations.",
    "metadata": {
      "arxiv_id": "2503.02862",
      "title": "Privacy and Accuracy-Aware AI/ML Model Deduplication",
      "summary": "With the growing adoption of privacy-preserving machine learning algorithms, such as Differentially Private Stochastic Gradient Descent (DP-SGD), training or fine-tuning models on private datasets has become increasingly prevalent. This shift has led to the need for models offering varying privacy guarantees and utility levels to satisfy diverse user requirements. However, managing numerous versions of large models introduces significant operational challenges, including increased inference latency, higher resource consumption, and elevated costs. Model deduplication is a technique widely used by many model serving and database systems to support high-performance and low-cost inference queries and model diagnosis queries. However, none of the existing model deduplication works has considered privacy, leading to unbounded aggregation of privacy costs for certain deduplicated models and inefficiencies when applied to deduplicate DP-trained models. We formalize the problems of deduplicating DP-trained models for the first time and propose a novel privacy- and accuracy-aware deduplication mechanism to address the problems. We developed a greedy strategy to select and assign base models to target models to minimize storage and privacy costs. When deduplicating a target model, we dynamically schedule accuracy validations and apply the Sparse Vector Technique to reduce the privacy costs associated with private validation data. Compared to baselines that do not provide privacy guarantees, our approach improved the compression ratio by up to $35\\times$ for individual models (including large language models and vision transformers). We also observed up to $43\\times$ inference speedup due to the reduction of I/O operations.",
      "authors": [
        "Hong Guan",
        "Lei Yu",
        "Lixi Zhou",
        "Li Xiong",
        "Kanchan Chowdhury",
        "Lulu Xie",
        "Xusheng Xiao",
        "Jia Zou"
      ],
      "published": "2025-03-04T18:40:38Z",
      "updated": "2025-03-04T18:40:38Z",
      "categories": [
        "cs.CR",
        "cs.DB"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.02862v1",
      "landing_url": "https://arxiv.org/abs/2503.02862v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.02862"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns privacy- and accuracy-aware deduplication of DP-trained models and privacy costs, not discrete audio token design or token-based representations, so it does not meet the inclusion criteria for Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns privacy- and accuracy-aware deduplication of DP-trained models and privacy costs, not discrete audio token design or token-based representations, so it does not meet the inclusion criteria for Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on model deduplication in privacy-preserving AI/ML training, specifically handling differentially private models and improving storage and inference efficiency. It does not discuss discrete audio tokens, vector quantization, or generation/understanding of audio sequences via tokenization, which are the core inclusion criteria. Furthermore, it lacks any mention of token generation processes, quantization methods, or audio-related tasks, and thus does not meet the required criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on model deduplication in privacy-preserving AI/ML training, specifically handling differentially private models and improving storage and inference efficiency. It does not discuss discrete audio tokens, vector quantization, or generation/understanding of audio sequences via tokenization, which are the core inclusion criteria. Furthermore, it lacks any mention of token generation processes, quantization methods, or audio-related tasks, and thus does not meet the required criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Bokeh Diffusion: Defocus Blur Control in Text-to-Image Diffusion Models",
    "abstract": "Recent advances in large-scale text-to-image models have revolutionized creative fields by generating visually captivating outputs from textual prompts; however, while traditional photography offers precise control over camera settings to shape visual aesthetics - such as depth-of-field via aperture - current diffusion models typically rely on prompt engineering to mimic such effects. This approach often results in crude approximations and inadvertently alters the scene content. In this work, we propose Bokeh Diffusion, a scene-consistent bokeh control framework that explicitly conditions a diffusion model on a physical defocus blur parameter. To overcome the scarcity of paired real-world images captured under different camera settings, we introduce a hybrid training pipeline that aligns in-the-wild images with synthetic blur augmentations, providing diverse scenes and subjects as well as supervision to learn the separation of image content from lens blur. Central to our framework is our grounded self-attention mechanism, trained on image pairs with different bokeh levels of the same scene, which enables blur strength to be adjusted in both directions while preserving the underlying scene. Extensive experiments demonstrate that our approach enables flexible, lens-like blur control, supports downstream applications such as real image editing via inversion, and generalizes effectively across both Stable Diffusion and FLUX architectures.",
    "metadata": {
      "arxiv_id": "2503.08434",
      "title": "Bokeh Diffusion: Defocus Blur Control in Text-to-Image Diffusion Models",
      "summary": "Recent advances in large-scale text-to-image models have revolutionized creative fields by generating visually captivating outputs from textual prompts; however, while traditional photography offers precise control over camera settings to shape visual aesthetics - such as depth-of-field via aperture - current diffusion models typically rely on prompt engineering to mimic such effects. This approach often results in crude approximations and inadvertently alters the scene content. In this work, we propose Bokeh Diffusion, a scene-consistent bokeh control framework that explicitly conditions a diffusion model on a physical defocus blur parameter. To overcome the scarcity of paired real-world images captured under different camera settings, we introduce a hybrid training pipeline that aligns in-the-wild images with synthetic blur augmentations, providing diverse scenes and subjects as well as supervision to learn the separation of image content from lens blur. Central to our framework is our grounded self-attention mechanism, trained on image pairs with different bokeh levels of the same scene, which enables blur strength to be adjusted in both directions while preserving the underlying scene. Extensive experiments demonstrate that our approach enables flexible, lens-like blur control, supports downstream applications such as real image editing via inversion, and generalizes effectively across both Stable Diffusion and FLUX architectures.",
      "authors": [
        "Armando Fortes",
        "Tianyi Wei",
        "Shangchen Zhou",
        "Xingang Pan"
      ],
      "published": "2025-03-11T13:49:12Z",
      "updated": "2025-06-16T13:29:51Z",
      "categories": [
        "cs.GR",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.08434v4",
      "landing_url": "https://arxiv.org/abs/2503.08434v4",
      "doi": "https://doi.org/10.48550/arXiv.2503.08434"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe Bokeh Diffusion for defocus blur control in text-to-image diffusion models, focusing on image generation and lens blur manipulation, not on discrete audio tokens or any discrete audio token design/quantization. It does not meet the inclusion criterion about discrete audio token research, so should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe Bokeh Diffusion for defocus blur control in text-to-image diffusion models, focusing on image generation and lens blur manipulation, not on discrete audio tokens or any discrete audio token design/quantization. It does not meet the inclusion criterion about discrete audio token research, so should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on image generation and defocus blur control within text-to-image diffusion models, which does not involve discrete audio tokens, audio signal processing, or audio token generation as described in the inclusion criteria. It lacks any discussion or design related to discrete audio token formation or usage in language/sequential models for audio tasks. Therefore, it does not meet the inclusion criteria and fits none of the exclusion exceptions that might warrant reconsideration.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on image generation and defocus blur control within text-to-image diffusion models, which does not involve discrete audio tokens, audio signal processing, or audio token generation as described in the inclusion criteria. It lacks any discussion or design related to discrete audio token formation or usage in language/sequential models for audio tasks. Therefore, it does not meet the inclusion criteria and fits none of the exclusion exceptions that might warrant reconsideration.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Bidirectional Learned Facial Animation Codec for Low Bitrate Talking Head Videos",
    "abstract": "Existing deep facial animation coding techniques efficiently compress talking head videos by applying deep generative models. Instead of compressing the entire video sequence, these methods focus on compressing only the keyframe and the keypoints of non-keyframes (target frames). The target frames are then reconstructed by utilizing a single keyframe, and the keypoints of the target frame. Although these unidirectional methods can reduce the bitrate, they rely on a single keyframe and often struggle to capture large head movements accurately, resulting in distortions in the facial region. In this paper, we propose a novel bidirectional learned animation codec that generates natural facial videos using past and future keyframes. First, in the Bidirectional Reference-Guided Auxiliary Stream Enhancement (BRG-ASE) process, we introduce a compact auxiliary stream for non-keyframes, which is enhanced by adaptively selecting one of two keyframes (past and future). This stream improves video quality with a slight increase in bitrate. Then, in the Bidirectional Reference-Guided Video Reconstruction (BRG-VRec) process, we animate the adaptively selected keyframe and reconstruct the target frame using both the animated keyframe and the auxiliary frame. Extensive experiments demonstrate a 55% bitrate reduction compared to the latest animation based video codec, and a 35% bitrate reduction compared to the latest video coding standard, Versatile Video Coding (VVC) on a talking head video dataset. It showcases the efficiency of our approach in improving video quality while simultaneously decreasing bitrate.",
    "metadata": {
      "arxiv_id": "2503.09787",
      "title": "Bidirectional Learned Facial Animation Codec for Low Bitrate Talking Head Videos",
      "summary": "Existing deep facial animation coding techniques efficiently compress talking head videos by applying deep generative models. Instead of compressing the entire video sequence, these methods focus on compressing only the keyframe and the keypoints of non-keyframes (target frames). The target frames are then reconstructed by utilizing a single keyframe, and the keypoints of the target frame. Although these unidirectional methods can reduce the bitrate, they rely on a single keyframe and often struggle to capture large head movements accurately, resulting in distortions in the facial region. In this paper, we propose a novel bidirectional learned animation codec that generates natural facial videos using past and future keyframes. First, in the Bidirectional Reference-Guided Auxiliary Stream Enhancement (BRG-ASE) process, we introduce a compact auxiliary stream for non-keyframes, which is enhanced by adaptively selecting one of two keyframes (past and future). This stream improves video quality with a slight increase in bitrate. Then, in the Bidirectional Reference-Guided Video Reconstruction (BRG-VRec) process, we animate the adaptively selected keyframe and reconstruct the target frame using both the animated keyframe and the auxiliary frame. Extensive experiments demonstrate a 55% bitrate reduction compared to the latest animation based video codec, and a 35% bitrate reduction compared to the latest video coding standard, Versatile Video Coding (VVC) on a talking head video dataset. It showcases the efficiency of our approach in improving video quality while simultaneously decreasing bitrate.",
      "authors": [
        "Riku Takahashi",
        "Ryugo Morita",
        "Fuma Kimishima",
        "Kosuke Iwama",
        "Jinjia Zhou"
      ],
      "published": "2025-03-12T19:39:09Z",
      "updated": "2025-03-12T19:39:09Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.09787v1",
      "landing_url": "https://arxiv.org/abs/2503.09787v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.09787"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper describes a bidirectional learned facial animation codec for low-bit-rate talking-head videos and focuses on video frame prediction/reconstruction; it does not define or analyze discrete audio tokens, token generation, or quantization for audio, thus it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper describes a bidirectional learned facial animation codec for low-bit-rate talking-head videos and focuses on video frame prediction/reconstruction; it does not define or analyze discrete audio tokens, token generation, or quantization for audio, thus it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a bidirectional learned facial animation codec for talking head videos, emphasizing video compression and generation using keyframes and auxiliary streams. However, the inclusion criteria specifically require research on discrete audio tokens derived from continuous audio signals with explicit token generation, quantization methods, and usage in language or sequence models, especially for audio understanding or generation. This study does not address audio tokenization, discrete audio representations, or audio codec design for language modeling, thus failing to meet the core inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a bidirectional learned facial animation codec for talking head videos, emphasizing video compression and generation using keyframes and auxiliary streams. However, the inclusion criteria specifically require research on discrete audio tokens derived from continuous audio signals with explicit token generation, quantization methods, and usage in language or sequence models, especially for audio understanding or generation. This study does not address audio tokenization, discrete audio representations, or audio codec design for language modeling, thus failing to meet the core inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Dual Codebook VQ: Enhanced Image Reconstruction with Reduced Codebook Size",
    "abstract": "Vector Quantization (VQ) techniques face significant challenges in codebook utilization, limiting reconstruction fidelity in image modeling. We introduce a Dual Codebook mechanism that effectively addresses this limitation by partitioning the representation into complementary global and local components. The global codebook employs a lightweight transformer for concurrent updates of all code vectors, while the local codebook maintains precise feature representation through deterministic selection. This complementary approach is trained from scratch without requiring pre-trained knowledge. Experimental evaluation across multiple standard benchmark datasets demonstrates state-of-the-art reconstruction quality while using a compact codebook of size 512 - half the size of previous methods that require pre-training. Our approach achieves significant FID improvements across diverse image domains, particularly excelling in scene and face reconstruction tasks. These results establish Dual Codebook VQ as an efficient paradigm for high-fidelity image reconstruction with significantly reduced computational requirements.",
    "metadata": {
      "arxiv_id": "2503.10832",
      "title": "Dual Codebook VQ: Enhanced Image Reconstruction with Reduced Codebook Size",
      "summary": "Vector Quantization (VQ) techniques face significant challenges in codebook utilization, limiting reconstruction fidelity in image modeling. We introduce a Dual Codebook mechanism that effectively addresses this limitation by partitioning the representation into complementary global and local components. The global codebook employs a lightweight transformer for concurrent updates of all code vectors, while the local codebook maintains precise feature representation through deterministic selection. This complementary approach is trained from scratch without requiring pre-trained knowledge. Experimental evaluation across multiple standard benchmark datasets demonstrates state-of-the-art reconstruction quality while using a compact codebook of size 512 - half the size of previous methods that require pre-training. Our approach achieves significant FID improvements across diverse image domains, particularly excelling in scene and face reconstruction tasks. These results establish Dual Codebook VQ as an efficient paradigm for high-fidelity image reconstruction with significantly reduced computational requirements.",
      "authors": [
        "Parisa Boodaghi Malidarreh",
        "Jillur Rahman Saurav",
        "Thuong Le Hoai Pham",
        "Amir Hajighasemi",
        "Anahita Samadi",
        "Saurabh Shrinivas Maydeo",
        "Mohammad Sadegh Nasr",
        "Jacob M. Luber"
      ],
      "published": "2025-03-13T19:31:18Z",
      "updated": "2025-03-13T19:31:18Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.10832v1",
      "landing_url": "https://arxiv.org/abs/2503.10832v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.10832"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on image vector quantization with a dual codebook and does not involve discrete audio tokens, hence it does not meet the audio-token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on image vector quantization with a dual codebook and does not involve discrete audio tokens, hence it does not meet the audio-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on image reconstruction using a novel vector quantization method for images, not on audio signals or discrete audio tokens derived from continuous audio signals. It lacks any discussion or indication of audio-related tokenization, audio language modeling, or similar audio sequence processing, failing the inclusion criteria centered on discrete audio tokens for language or sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on image reconstruction using a novel vector quantization method for images, not on audio signals or discrete audio tokens derived from continuous audio signals. It lacks any discussion or indication of audio-related tokenization, audio language modeling, or similar audio sequence processing, failing the inclusion criteria centered on discrete audio tokens for language or sequence modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations",
    "abstract": "Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.",
    "metadata": {
      "arxiv_id": "2503.12115",
      "title": "Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations",
      "summary": "Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.",
      "authors": [
        "Xue Jiang",
        "Xiulian Peng",
        "Yuan Zhang",
        "Yan Lu"
      ],
      "published": "2025-03-15T12:50:43Z",
      "updated": "2025-10-15T06:52:30Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.12115v2",
      "landing_url": "https://arxiv.org/abs/2503.12115v2",
      "doi": "https://doi.org/10.1109/JSTSP.2024.3488557"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract presents learning discrete speech tokens via a low-bitrate neural codec and a unified token capturing linguistic and paralinguistic information, aligning with the discrete audio token criterion and proposing token-level representations for downstream language-model style tasks; thus it should be included.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract presents learning discrete speech tokens via a low-bitrate neural codec and a unified token capturing linguistic and paralinguistic information, aligning with the discrete audio token criterion and proposing token-level representations for downstream language-model style tasks; thus it should be included.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes UniCodec to learn unified, disentangled discrete speech tokens encapsulating linguistic and paralinguistic information using a low-bitrate neural codec and self-supervised knowledge distillation. It clearly details token generation and evaluation on multilingual datasets, aligning with inclusion criteria about discrete token design using neural codecs for audio language modeling and generation tasks, with emphasis on semantic and acoustic disentanglement and downstream usability. The abstract provides sufficient methodological information on token creation, usage, and evaluation consistent with the topic's scope, and there is no indication of exclusion criteria issues such as lack of method detail or focus on traditional codecs without language modeling relevance.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes UniCodec to learn unified, disentangled discrete speech tokens encapsulating linguistic and paralinguistic information using a low-bitrate neural codec and self-supervised knowledge distillation. It clearly details token generation and evaluation on multilingual datasets, aligning with inclusion criteria about discrete token design using neural codecs for audio language modeling and generation tasks, with emphasis on semantic and acoustic disentanglement and downstream usability. The abstract provides sufficient methodological information on token creation, usage, and evaluation consistent with the topic's scope, and there is no indication of exclusion criteria issues such as lack of method detail or focus on traditional codecs without language modeling relevance.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Improving Discriminator Guidance in Diffusion Models",
    "abstract": "Discriminator Guidance has become a popular method for efficiently refining pre-trained Score-Matching Diffusion models. However, in this paper, we demonstrate that the standard implementation of this technique does not necessarily lead to a distribution closer to the real data distribution. Specifically, we show that training the discriminator using Cross-Entropy loss, as commonly done, can in fact increase the Kullback-Leibler divergence between the model and target distributions, particularly when the discriminator overfits. To address this, we propose a theoretically sound training objective for discriminator guidance that properly minimizes the KL divergence. We analyze its properties and demonstrate empirically across multiple datasets that our proposed method consistently improves over the conventional method by producing samples of higher quality.",
    "metadata": {
      "arxiv_id": "2503.16117",
      "title": "Improving Discriminator Guidance in Diffusion Models",
      "summary": "Discriminator Guidance has become a popular method for efficiently refining pre-trained Score-Matching Diffusion models. However, in this paper, we demonstrate that the standard implementation of this technique does not necessarily lead to a distribution closer to the real data distribution. Specifically, we show that training the discriminator using Cross-Entropy loss, as commonly done, can in fact increase the Kullback-Leibler divergence between the model and target distributions, particularly when the discriminator overfits. To address this, we propose a theoretically sound training objective for discriminator guidance that properly minimizes the KL divergence. We analyze its properties and demonstrate empirically across multiple datasets that our proposed method consistently improves over the conventional method by producing samples of higher quality.",
      "authors": [
        "Alexandre Verine",
        "Ahmed Mehdi Inane",
        "Florian Le Bronnec",
        "Benjamin Negrevergne",
        "Yann Chevaleyre"
      ],
      "published": "2025-03-20T13:04:43Z",
      "updated": "2025-06-11T12:24:13Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.16117v2",
      "landing_url": "https://arxiv.org/abs/2503.16117v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.16117"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on discriminator guidance in diffusion models and KL divergence; it does not address discrete audio token representations or tokenization, so it fails inclusion criteria for the discrete audio tokens topic.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on discriminator guidance in diffusion models and KL divergence; it does not address discrete audio token representations or tokenization, so it fails inclusion criteria for the discrete audio tokens topic.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe improvements in discriminator guidance for diffusion models, a method related to refining generative models. However, there is no mention of discrete audio tokens, vector quantization, or any audio-related token generation or representation as described in the inclusion criteria. The focus here is on model training objectives and sample quality improvement in diffusion models without addressing audio tokenization or discrete token-based sequence modeling. Therefore, it does not meet the inclusion criteria of focusing on discrete audio token generation or usage, nor does it violate exclusion criteria explicitly. Hence, it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe improvements in discriminator guidance for diffusion models, a method related to refining generative models. However, there is no mention of discrete audio tokens, vector quantization, or any audio-related token generation or representation as described in the inclusion criteria. The focus here is on model training objectives and sample quality improvement in diffusion models without addressing audio tokenization or discrete token-based sequence modeling. Therefore, it does not meet the inclusion criteria of focusing on discrete audio token generation or usage, nor does it violate exclusion criteria explicitly. Hence, it should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Teller: Real-Time Streaming Audio-Driven Portrait Animation with Autoregressive Motion Generation",
    "abstract": "In this work, we introduce the first autoregressive framework for real-time, audio-driven portrait animation, a.k.a, talking head. Beyond the challenge of lengthy animation times, a critical challenge in realistic talking head generation lies in preserving the natural movement of diverse body parts. To this end, we propose Teller, the first streaming audio-driven protrait animation framework with autoregressive motion generation. Specifically, Teller first decomposes facial and body detail animation into two components: Facial Motion Latent Generation (FMLG) based on an autoregressive transfromer, and movement authenticity refinement using a Efficient Temporal Module (ETM).Concretely, FMLG employs a Residual VQ model to map the facial motion latent from the implicit keypoint-based model into discrete motion tokens, which are then temporally sliced with audio embeddings. This enables the AR tranformer to learn real-time, stream-based mappings from audio to motion. Furthermore, Teller incorporate ETM to capture finer motion details. This module ensures the physical consistency of body parts and accessories, such as neck muscles and earrings, improving the realism of these movements. Teller is designed to be efficient, surpassing the inference speed of diffusion-based models (Hallo 20.93s vs. Teller 0.92s for one second video generation), and achieves a real-time streaming performance of up to 25 FPS. Extensive experiments demonstrate that our method outperforms recent audio-driven portrait animation models, especially in small movements, as validated by human evaluations with a significant margin in quality and realism.",
    "metadata": {
      "arxiv_id": "2503.18429",
      "title": "Teller: Real-Time Streaming Audio-Driven Portrait Animation with Autoregressive Motion Generation",
      "summary": "In this work, we introduce the first autoregressive framework for real-time, audio-driven portrait animation, a.k.a, talking head. Beyond the challenge of lengthy animation times, a critical challenge in realistic talking head generation lies in preserving the natural movement of diverse body parts. To this end, we propose Teller, the first streaming audio-driven protrait animation framework with autoregressive motion generation. Specifically, Teller first decomposes facial and body detail animation into two components: Facial Motion Latent Generation (FMLG) based on an autoregressive transfromer, and movement authenticity refinement using a Efficient Temporal Module (ETM).Concretely, FMLG employs a Residual VQ model to map the facial motion latent from the implicit keypoint-based model into discrete motion tokens, which are then temporally sliced with audio embeddings. This enables the AR tranformer to learn real-time, stream-based mappings from audio to motion. Furthermore, Teller incorporate ETM to capture finer motion details. This module ensures the physical consistency of body parts and accessories, such as neck muscles and earrings, improving the realism of these movements. Teller is designed to be efficient, surpassing the inference speed of diffusion-based models (Hallo 20.93s vs. Teller 0.92s for one second video generation), and achieves a real-time streaming performance of up to 25 FPS. Extensive experiments demonstrate that our method outperforms recent audio-driven portrait animation models, especially in small movements, as validated by human evaluations with a significant margin in quality and realism.",
      "authors": [
        "Dingcheng Zhen",
        "Shunshun Yin",
        "Shiyang Qin",
        "Hou Yi",
        "Ziwei Zhang",
        "Siyuan Liu",
        "Gan Qi",
        "Ming Tao"
      ],
      "published": "2025-03-24T08:16:47Z",
      "updated": "2025-03-24T08:16:47Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.18429v1",
      "landing_url": "https://arxiv.org/abs/2503.18429v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.18429"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the work uses discrete tokens (motion tokens via RVQ) within an autoregressive framework, the tokens serve motion representation rather than discretized audio tokens for audio-language modeling, so it does not satisfy the requirement that discrete audio tokens be the core contribution.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the work uses discrete tokens (motion tokens via RVQ) within an autoregressive framework, the tokens serve motion representation rather than discretized audio tokens for audio-language modeling, so it does not satisfy the requirement that discrete audio tokens be the core contribution.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces Teller, a framework for audio-driven portrait animation using an autoregressive transformer and a residual vector quantization (VQ) model to generate discrete motion tokens from facial motion latents. However, the discrete tokens are related to motion representation rather than audio signals, and the work focuses on visual animation driven by audio without detailing discrete audio token generation, vector quantization of audio, or how these tokens are used for audio language modeling or downstream audio generation/understanding tasks. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens derived from audio signals for language model processing.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper introduces Teller, a framework for audio-driven portrait animation using an autoregressive transformer and a residual vector quantization (VQ) model to generate discrete motion tokens from facial motion latents. However, the discrete tokens are related to motion representation rather than audio signals, and the work focuses on visual animation driven by audio without detailing discrete audio token generation, vector quantization of audio, or how these tokens are used for audio language modeling or downstream audio generation/understanding tasks. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens derived from audio signals for language model processing.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "FireRedTTS-1S: An Upgraded Streamable Foundation Text-to-Speech System",
    "abstract": "In this work, we upgrade FireRedTTS to a new version, FireRedTTS-1S, a high-quality streaming foundation text-to-speech system. FireRedTTS-1S achieves streaming speech generation via two steps: text-to-semantic decoding and semantic-to-acoustic decoding. In text-to-semantic decoding, a semantic-aware speech tokenizer converts the speech signal into semantic tokens, which can be synthesized from the text via a language model in an auto-regressive manner. Meanwhile, the semantic-to-acoustic decoding module simultaneously translates generated semantic tokens into the speech signal in a streaming way. We implement two approaches to achieve this module: 1) a chunk-wise streamable flow-matching approach, and 2) a multi-stream language model-based approach. They both present high-quality and streamable speech generation but differ in real-time factor (RTF) and latency. Specifically, flow-matching decoding can generate speech by chunks, presenting a lower RTF of 0.1 but a higher latency of 300ms. Instead, the multi-stream language model generates speech by frames in an autoregressive manner, presenting a higher RTF of 0.3 but a low latency of 150ms. In experiments on zero-shot voice cloning, the objective results validate FireRedTTS-1S as a high-quality foundation model with comparable intelligibility and speaker similarity over industrial baseline systems. Furthermore, the subjective score of FireRedTTS-1S highlights its impressive synthesis performance, achieving comparable quality to the ground-truth recordings. These results validate FireRedTTS-1S as a high-quality streaming foundation TTS system.",
    "metadata": {
      "arxiv_id": "2503.20499",
      "title": "FireRedTTS-1S: An Upgraded Streamable Foundation Text-to-Speech System",
      "summary": "In this work, we upgrade FireRedTTS to a new version, FireRedTTS-1S, a high-quality streaming foundation text-to-speech system. FireRedTTS-1S achieves streaming speech generation via two steps: text-to-semantic decoding and semantic-to-acoustic decoding. In text-to-semantic decoding, a semantic-aware speech tokenizer converts the speech signal into semantic tokens, which can be synthesized from the text via a language model in an auto-regressive manner. Meanwhile, the semantic-to-acoustic decoding module simultaneously translates generated semantic tokens into the speech signal in a streaming way. We implement two approaches to achieve this module: 1) a chunk-wise streamable flow-matching approach, and 2) a multi-stream language model-based approach. They both present high-quality and streamable speech generation but differ in real-time factor (RTF) and latency. Specifically, flow-matching decoding can generate speech by chunks, presenting a lower RTF of 0.1 but a higher latency of 300ms. Instead, the multi-stream language model generates speech by frames in an autoregressive manner, presenting a higher RTF of 0.3 but a low latency of 150ms. In experiments on zero-shot voice cloning, the objective results validate FireRedTTS-1S as a high-quality foundation model with comparable intelligibility and speaker similarity over industrial baseline systems. Furthermore, the subjective score of FireRedTTS-1S highlights its impressive synthesis performance, achieving comparable quality to the ground-truth recordings. These results validate FireRedTTS-1S as a high-quality streaming foundation TTS system.",
      "authors": [
        "Hao-Han Guo",
        "Yao Hu",
        "Fei-Yu Shen",
        "Xu Tang",
        "Yi-Chen Wu",
        "Feng-Long Xie",
        "Kun Xie"
      ],
      "published": "2025-03-26T12:39:06Z",
      "updated": "2025-05-26T11:34:20Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.20499v3",
      "landing_url": "https://arxiv.org/abs/2503.20499v3",
      "doi": "https://doi.org/10.48550/arXiv.2503.20499"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on upgrading a streaming TTS system and uses a semantic tokenizer to map speech to semantic tokens, but it does not present discrete audio token design/quantization (e.g., VQ/RVQ) or token-level modeling as the primary contribution, so it does not meet the core criterion for discrete audio token research.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper centers on upgrading a streaming TTS system and uses a semantic tokenizer to map speech to semantic tokens, but it does not present discrete audio token design/quantization (e.g., VQ/RVQ) or token-level modeling as the primary contribution, so it does not meet the core criterion for discrete audio token research.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper describes an upgraded text-to-speech system (FireRedTTS-1S) that utilizes a semantic-aware speech tokenizer converting speech signals into semantic tokens, which are then synthesized via language models; this indicates a process of discrete token generation involving semantic-driven discretization. The methodology and experimental validation suggest a detailed and reproducible approach focusing on token generation and usage in sequence modeling for TTS, aligning well with the inclusion criteria emphasizing discrete audio tokens with semantic disentanglement and their use in sequence models. The abstract provides sufficient insight into token generation and system design, not merely presenting a demo or system showcase.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The paper describes an upgraded text-to-speech system (FireRedTTS-1S) that utilizes a semantic-aware speech tokenizer converting speech signals into semantic tokens, which are then synthesized via language models; this indicates a process of discrete token generation involving semantic-driven discretization. The methodology and experimental validation suggest a detailed and reproducible approach focusing on token generation and usage in sequence modeling for TTS, aligning well with the inclusion criteria emphasizing discrete audio tokens with semantic disentanglement and their use in sequence models. The abstract provides sufficient insight into token generation and system design, not merely presenting a demo or system showcase.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": {
      "reasoning": "JuniorNano judged it should be excluded because the abstract does not explicitly present detailed discrete audio token design/quantization as the primary contribution; JuniorMini judged it should be included because the paper centers on a semantic-aware speech tokenizer that produces semantic tokens used by language models for TTS, with experimental validation. The abstract explicitly describes a semantic-aware speech tokenizer that converts speech into semantic tokens used by an auto-regressive language model and a semantic-to-acoustic decoder, which aligns with the discrete audio token theme (semantic-driven discretization used as sequence-model input) and presents experimental comparisons — however, the abstract does not state specific quantization methods (VQ/RVQ/etc.) nor confirm availability of full English technical details. Because the tokenization and token-based generation are central to the method and validated experimentally, but some methodological specifics required by the inclusion checklist are not visible from the abstract alone, I recommend inclusion but with mild caution.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "JuniorNano judged it should be excluded because the abstract does not explicitly present detailed discrete audio token design/quantization as the primary contribution; JuniorMini judged it should be included because the paper centers on a semantic-aware speech tokenizer that produces semantic tokens used by language models for TTS, with experimental validation. The abstract explicitly describes a semantic-aware speech tokenizer that converts speech into semantic tokens used by an auto-regressive language model and a semantic-to-acoustic decoder, which aligns with the discrete audio token theme (semantic-driven discretization used as sequence-model input) and presents experimental comparisons — however, the abstract does not state specific quantization methods (VQ/RVQ/etc.) nor confirm availability of full English technical details. Because the tokenization and token-based generation are central to the method and validated experimentally, but some methodological specifics required by the inclusion checklist are not visible from the abstract alone, I recommend inclusion but with mild caution.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Gaussian Mixture Flow Matching Models",
    "abstract": "Diffusion models approximate the denoising distribution as a Gaussian and predict its mean, whereas flow matching models reparameterize the Gaussian mean as flow velocity. However, they underperform in few-step sampling due to discretization error and tend to produce over-saturated colors under classifier-free guidance (CFG). To address these limitations, we propose a novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a multi-modal flow velocity distribution, which can be learned with a KL divergence loss. We demonstrate that GMFlow generalizes previous diffusion and flow matching models where a single Gaussian is learned with an $L_2$ denoising loss. For inference, we derive GM-SDE/ODE solvers that leverage analytic denoising distributions and velocity fields for precise few-step sampling. Furthermore, we introduce a novel probabilistic guidance scheme that mitigates the over-saturation issues of CFG and improves image generation quality. Extensive experiments demonstrate that GMFlow consistently outperforms flow matching baselines in generation quality, achieving a Precision of 0.942 with only 6 sampling steps on ImageNet 256$\\times$256.",
    "metadata": {
      "arxiv_id": "2504.05304",
      "title": "Gaussian Mixture Flow Matching Models",
      "summary": "Diffusion models approximate the denoising distribution as a Gaussian and predict its mean, whereas flow matching models reparameterize the Gaussian mean as flow velocity. However, they underperform in few-step sampling due to discretization error and tend to produce over-saturated colors under classifier-free guidance (CFG). To address these limitations, we propose a novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a multi-modal flow velocity distribution, which can be learned with a KL divergence loss. We demonstrate that GMFlow generalizes previous diffusion and flow matching models where a single Gaussian is learned with an $L_2$ denoising loss. For inference, we derive GM-SDE/ODE solvers that leverage analytic denoising distributions and velocity fields for precise few-step sampling. Furthermore, we introduce a novel probabilistic guidance scheme that mitigates the over-saturation issues of CFG and improves image generation quality. Extensive experiments demonstrate that GMFlow consistently outperforms flow matching baselines in generation quality, achieving a Precision of 0.942 with only 6 sampling steps on ImageNet 256$\\times$256.",
      "authors": [
        "Hansheng Chen",
        "Kai Zhang",
        "Hao Tan",
        "Zexiang Xu",
        "Fujun Luan",
        "Leonidas Guibas",
        "Gordon Wetzstein",
        "Sai Bi"
      ],
      "published": "2025-04-07T17:59:42Z",
      "updated": "2025-08-30T06:33:43Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.05304v3",
      "landing_url": "https://arxiv.org/abs/2504.05304v3",
      "doi": "https://doi.org/10.48550/arXiv.2504.05304"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本研究聚焦於影像生成的高斯混合流匹配模型，與離散音訊 token 的產生、量化與語言模型/序列模型的離散表徵需求無關，因此不符合 inclusion 要求。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本研究聚焦於影像生成的高斯混合流匹配模型，與離散音訊 token 的產生、量化與語言模型/序列模型的離散表徵需求無關，因此不符合 inclusion 要求。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on Gaussian mixture flow matching models for image generation, involving diffusion and flow matching approaches for generating images with improved sampling and image quality. It does not address discrete audio tokens, audio signal quantization, or language/sequence modeling for audio, which are the core inclusion criteria. It also lacks any discussion of token generation or relevant audio domain methods, making it irrelevant to the requested topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on Gaussian mixture flow matching models for image generation, involving diffusion and flow matching approaches for generating images with improved sampling and image quality. It does not address discrete audio tokens, audio signal quantization, or language/sequence modeling for audio, which are the core inclusion criteria. It also lacks any discussion of token generation or relevant audio domain methods, making it irrelevant to the requested topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Streamable Neural Audio Codec with Residual Scalar-Vector Quantization for Real-Time Communication",
    "abstract": "This paper proposes StreamCodec, a streamable neural audio codec designed for real-time communication. StreamCodec adopts a fully causal, symmetric encoder-decoder structure and operates in the modified discrete cosine transform (MDCT) domain, aiming for low-latency inference and real-time efficient generation. To improve codebook utilization efficiency and compensate for the audio quality loss caused by structural causality, StreamCodec introduces a novel residual scalar-vector quantizer (RSVQ). The RSVQ sequentially connects scalar quantizers and improved vector quantizers in a residual manner, constructing coarse audio contours and refining acoustic details, respectively. Experimental results confirm that the proposed StreamCodec achieves decoded audio quality comparable to advanced non-streamable neural audio codecs. Specifically, on the 16 kHz LibriTTS dataset, StreamCodec attains a ViSQOL score of 4.30 at 1.5 kbps. It has a fixed latency of only 20 ms and achieves a generation speed nearly 20 times real-time on a CPU, with a lightweight model size of just 7M parameters, making it highly suitable for real-time communication applications.",
    "metadata": {
      "arxiv_id": "2504.06561",
      "title": "A Streamable Neural Audio Codec with Residual Scalar-Vector Quantization for Real-Time Communication",
      "summary": "This paper proposes StreamCodec, a streamable neural audio codec designed for real-time communication. StreamCodec adopts a fully causal, symmetric encoder-decoder structure and operates in the modified discrete cosine transform (MDCT) domain, aiming for low-latency inference and real-time efficient generation. To improve codebook utilization efficiency and compensate for the audio quality loss caused by structural causality, StreamCodec introduces a novel residual scalar-vector quantizer (RSVQ). The RSVQ sequentially connects scalar quantizers and improved vector quantizers in a residual manner, constructing coarse audio contours and refining acoustic details, respectively. Experimental results confirm that the proposed StreamCodec achieves decoded audio quality comparable to advanced non-streamable neural audio codecs. Specifically, on the 16 kHz LibriTTS dataset, StreamCodec attains a ViSQOL score of 4.30 at 1.5 kbps. It has a fixed latency of only 20 ms and achieves a generation speed nearly 20 times real-time on a CPU, with a lightweight model size of just 7M parameters, making it highly suitable for real-time communication applications.",
      "authors": [
        "Xiao-Hang Jiang",
        "Yang Ai",
        "Rui-Chen Zheng",
        "Zhen-Hua Ling"
      ],
      "published": "2025-04-09T03:49:00Z",
      "updated": "2025-04-09T03:49:00Z",
      "categories": [
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.06561v1",
      "landing_url": "https://arxiv.org/abs/2504.06561v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.06561"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work uses RSVQ quantization within a streamable neural audio codec aimed at real-time communication, but it does not explicitly present discrete audio tokens designed for downstream sequence/ language-modeling tasks or provide token-generation pipelines, so it does not fully meet the discrete audio tokens criterion.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "This work uses RSVQ quantization within a streamable neural audio codec aimed at real-time communication, but it does not explicitly present discrete audio tokens designed for downstream sequence/ language-modeling tasks or provide token-generation pipelines, so it does not fully meet the discrete audio tokens criterion.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces StreamCodec, a neural audio codec that employs a novel residual scalar-vector quantizer (RSVQ), which is a type of residual vector quantization (RVQ), to generate discrete audio tokens suitable for real-time communication. The method clearly describes token generation via RSVQ in the MDCT domain, with evidence of efficient quantization and high quality audio reconstruction, aligning with the inclusion criteria of focusing on discrete audio token design and usage. Although the paper emphasizes real-time streaming codec performance, it provides technical details about quantization and codebook design and validates the method experimentally, suggesting it meets the technical depth required. No exclusion criteria are met, such as lack of method details or traditional codec focus without token representation. Therefore, the paper is better to include as it contributes a discrete token design method with clear quantization and token generation process.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The paper introduces StreamCodec, a neural audio codec that employs a novel residual scalar-vector quantizer (RSVQ), which is a type of residual vector quantization (RVQ), to generate discrete audio tokens suitable for real-time communication. The method clearly describes token generation via RSVQ in the MDCT domain, with evidence of efficient quantization and high quality audio reconstruction, aligning with the inclusion criteria of focusing on discrete audio token design and usage. Although the paper emphasizes real-time streaming codec performance, it provides technical details about quantization and codebook design and validates the method experimentally, suggesting it meets the technical depth required. No exclusion criteria are met, such as lack of method details or traditional codec focus without token representation. Therefore, the paper is better to include as it contributes a discrete token design method with clear quantization and token generation process.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior reviews: JuniorNano argued to exclude because the paper doesn’t explicitly present discrete audio tokens for downstream sequence/language-modeling tasks, while JuniorMini argued to include because RSVQ produces discrete quantized codes. The abstract does describe a residual scalar-vector quantizer (RSVQ) and codebook usage (i.e., a quantization scheme producing discrete codes), but it frames the work as a real-time communication codec without clearly positioning those discrete codes as tokens for sequence/language modeling or describing a token-generation pipeline and intended downstream usage—requirements in the inclusion criteria. Therefore this item does not meet the criterion that discrete token design and its use for sequence/LM be a primary contribution.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior reviews: JuniorNano argued to exclude because the paper doesn’t explicitly present discrete audio tokens for downstream sequence/language-modeling tasks, while JuniorMini argued to include because RSVQ produces discrete quantized codes. The abstract does describe a residual scalar-vector quantizer (RSVQ) and codebook usage (i.e., a quantization scheme producing discrete codes), but it frames the work as a real-time communication codec without clearly positioning those discrete codes as tokens for sequence/language modeling or describing a token-generation pipeline and intended downstream usage—requirements in the inclusion criteria. Therefore this item does not meet the criterion that discrete token design and its use for sequence/LM be a primary contribution.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling",
    "abstract": "Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io.",
    "metadata": {
      "arxiv_id": "2504.07053",
      "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling",
      "summary": "Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io.",
      "authors": [
        "Liang-Hsuan Tseng",
        "Yi-Chang Chen",
        "Kuan-Yi Lee",
        "Da-Shan Shiu",
        "Hung-yi Lee"
      ],
      "published": "2025-04-09T17:14:33Z",
      "updated": "2025-05-22T14:49:03Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.07053v2",
      "landing_url": "https://arxiv.org/abs/2504.07053v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.07053"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on text-aligned speech tokenization for joint spoken language modeling, implying a discrete audio token representation and a concrete tokenization/design approach rather than a mere demo; it includes training objectives and empirical validation, aligning with discrete audio token criteria albeit without explicit methodological specifics in the abstract.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The work centers on text-aligned speech tokenization for joint spoken language modeling, implying a discrete audio token representation and a concrete tokenization/design approach rather than a mere demo; it includes training objectives and empirical validation, aligning with discrete audio token criteria albeit without explicit methodological specifics in the abstract.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes TASTE, a method for text-aligned speech tokenization and embedding aiming to bridge modality gaps for spoken language modeling. It explicitly mentions a tokenization stage that aligns speech tokens with corresponding text transcription, using attention-based aggregation and reconstruction objectives, which suggests a clear methodology for generating discrete speech tokens. The approach preserves paralinguistic information and reduces token sequence length, indicating an emphasis on useful, discrete audio token design. Furthermore, the method supports joint spoken language modeling and seems to be used as input tokens for language models. Therefore, it aligns with the inclusion criteria requiring discrete audio token design with detailed methodology and use for sequence modeling. There is no indication that this is merely a demo or lacks reproducibility. Although quantization specifics are not deeply detailed in the abstract, the use of an embedding approach with tokenization at its core and demonstrated experiments support inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The abstract describes TASTE, a method for text-aligned speech tokenization and embedding aiming to bridge modality gaps for spoken language modeling. It explicitly mentions a tokenization stage that aligns speech tokens with corresponding text transcription, using attention-based aggregation and reconstruction objectives, which suggests a clear methodology for generating discrete speech tokens. The approach preserves paralinguistic information and reduces token sequence length, indicating an emphasis on useful, discrete audio token design. Furthermore, the method supports joint spoken language modeling and seems to be used as input tokens for language models. Therefore, it aligns with the inclusion criteria requiring discrete audio token design with detailed methodology and use for sequence modeling. There is no indication that this is merely a demo or lacks reproducibility. Although quantization specifics are not deeply detailed in the abstract, the use of an embedding approach with tokenization at its core and demonstrated experiments support inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Lightweight Road Environment Segmentation using Vector Quantization",
    "abstract": "Road environment segmentation plays a significant role in autonomous driving. Numerous works based on Fully Convolutional Networks (FCNs) and Transformer architectures have been proposed to leverage local and global contextual learning for efficient and accurate semantic segmentation. In both architectures, the encoder often relies heavily on extracting continuous representations from the image, which limits the ability to represent meaningful discrete information. To address this limitation, we propose segmentation of the autonomous driving environment using vector quantization. Vector quantization offers three primary advantages for road environment segmentation. (1) Each continuous feature from the encoder is mapped to a discrete vector from the codebook, helping the model discover distinct features more easily than with complex continuous features. (2) Since a discrete feature acts as compressed versions of the encoder's continuous features, they also compress noise or outliers, enhancing the image segmentation task. (3) Vector quantization encourages the latent space to form coarse clusters of continuous features, forcing the model to group similar features, making the learned representations more structured for the decoding process. In this work, we combined vector quantization with the lightweight image segmentation model MobileUNETR and used it as a baseline model for comparison to demonstrate its efficiency. Through experiments, we achieved 77.0 % mIoU on Cityscapes, outperforming the baseline by 2.9 % without increasing the model's initial size or complexity.",
    "metadata": {
      "arxiv_id": "2504.14113",
      "title": "Lightweight Road Environment Segmentation using Vector Quantization",
      "summary": "Road environment segmentation plays a significant role in autonomous driving. Numerous works based on Fully Convolutional Networks (FCNs) and Transformer architectures have been proposed to leverage local and global contextual learning for efficient and accurate semantic segmentation. In both architectures, the encoder often relies heavily on extracting continuous representations from the image, which limits the ability to represent meaningful discrete information. To address this limitation, we propose segmentation of the autonomous driving environment using vector quantization. Vector quantization offers three primary advantages for road environment segmentation. (1) Each continuous feature from the encoder is mapped to a discrete vector from the codebook, helping the model discover distinct features more easily than with complex continuous features. (2) Since a discrete feature acts as compressed versions of the encoder's continuous features, they also compress noise or outliers, enhancing the image segmentation task. (3) Vector quantization encourages the latent space to form coarse clusters of continuous features, forcing the model to group similar features, making the learned representations more structured for the decoding process. In this work, we combined vector quantization with the lightweight image segmentation model MobileUNETR and used it as a baseline model for comparison to demonstrate its efficiency. Through experiments, we achieved 77.0 % mIoU on Cityscapes, outperforming the baseline by 2.9 % without increasing the model's initial size or complexity.",
      "authors": [
        "Jiyong Kwag",
        "Alper Yilmaz",
        "Charles Toth"
      ],
      "published": "2025-04-19T00:13:21Z",
      "updated": "2025-04-19T00:13:21Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.14113v1",
      "landing_url": "https://arxiv.org/abs/2504.14113v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.14113"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item targets discrete visual feature representations via vector quantization for road segmentation, not discrete audio tokens; it fails the specified inclusion criteria for \"Discrete Audio Tokens\".",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item targets discrete visual feature representations via vector quantization for road segmentation, not discrete audio tokens; it fails the specified inclusion criteria for \"Discrete Audio Tokens\".",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on road environment segmentation using vector quantization on visual data (images) for autonomous driving, not on discrete audio tokens or audio signal processing as defined in the inclusion criteria. Although vector quantization is involved, it is applied to image features, not continuous audio signals to generate discrete audio tokens for language or sequence models. Therefore, it fails to meet the fundamental inclusion criteria related to discrete audio token design and use, and is unrelated to audio language modeling or audio-related tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on road environment segmentation using vector quantization on visual data (images) for autonomous driving, not on discrete audio tokens or audio signal processing as defined in the inclusion criteria. Although vector quantization is involved, it is applied to image features, not continuous audio signals to generate discrete audio tokens for language or sequence models. Therefore, it fails to meet the fundamental inclusion criteria related to discrete audio token design and use, and is unrelated to audio language modeling or audio-related tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Flow Matching Ergodic Coverage",
    "abstract": "Ergodic coverage effectively generates exploratory behaviors for embodied agents by aligning the spatial distribution of the agent's trajectory with a target distribution, where the difference between these two distributions is measured by the ergodic metric. However, existing ergodic coverage methods are constrained by the limited set of ergodic metrics available for control synthesis, fundamentally limiting their performance. In this work, we propose an alternative approach to ergodic coverage based on flow matching, a technique widely used in generative inference for efficient and scalable sampling. We formally derive the flow matching problem for ergodic coverage and show that it is equivalent to a linear quadratic regulator problem with a closed-form solution. Our formulation enables alternative ergodic metrics from generative inference that overcome the limitations of existing ones. These metrics were previously infeasible for control synthesis but can now be supported with no computational overhead. Specifically, flow matching with the Stein variational gradient flow enables control synthesis directly over the score function of the target distribution, improving robustness to the unnormalized distributions; on the other hand, flow matching with the Sinkhorn divergence flow enables an optimal transport-based ergodic metric, improving coverage performance on non-smooth distributions with irregular supports. We validate the improved performance and competitive computational efficiency of our method through comprehensive numerical benchmarks and across different nonlinear dynamics. We further demonstrate the practicality of our method through a series of drawing and erasing tasks on a Franka robot.",
    "metadata": {
      "arxiv_id": "2504.17872",
      "title": "Flow Matching Ergodic Coverage",
      "summary": "Ergodic coverage effectively generates exploratory behaviors for embodied agents by aligning the spatial distribution of the agent's trajectory with a target distribution, where the difference between these two distributions is measured by the ergodic metric. However, existing ergodic coverage methods are constrained by the limited set of ergodic metrics available for control synthesis, fundamentally limiting their performance. In this work, we propose an alternative approach to ergodic coverage based on flow matching, a technique widely used in generative inference for efficient and scalable sampling. We formally derive the flow matching problem for ergodic coverage and show that it is equivalent to a linear quadratic regulator problem with a closed-form solution. Our formulation enables alternative ergodic metrics from generative inference that overcome the limitations of existing ones. These metrics were previously infeasible for control synthesis but can now be supported with no computational overhead. Specifically, flow matching with the Stein variational gradient flow enables control synthesis directly over the score function of the target distribution, improving robustness to the unnormalized distributions; on the other hand, flow matching with the Sinkhorn divergence flow enables an optimal transport-based ergodic metric, improving coverage performance on non-smooth distributions with irregular supports. We validate the improved performance and competitive computational efficiency of our method through comprehensive numerical benchmarks and across different nonlinear dynamics. We further demonstrate the practicality of our method through a series of drawing and erasing tasks on a Franka robot.",
      "authors": [
        "Max Muchen Sun",
        "Allison Pinosky",
        "Todd Murphey"
      ],
      "published": "2025-04-24T18:18:35Z",
      "updated": "2025-04-24T18:18:35Z",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.17872v1",
      "landing_url": "https://arxiv.org/abs/2504.17872v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.17872"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article deals with ergodic coverage and flow matching in robotics, not discrete audio tokens, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article deals with ergodic coverage and flow matching in robotics, not discrete audio tokens, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on ergodic coverage and flow matching techniques in robotics and control problems, with no mention or indication of discrete audio tokens, vector quantization, or audio representation related to the inclusion criteria; hence, it does not meet the required topic for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on ergodic coverage and flow matching techniques in robotics and control problems, with no mention or indication of discrete audio tokens, vector quantization, or audio representation related to the inclusion criteria; hence, it does not meet the required topic for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Evaluating Foveated Frame Rate Reduction in Virtual Reality for Head-Mounted Displays",
    "abstract": "Foveated rendering methods usually reduce spatial resolution in the periphery of the users' view. However, using foveated rendering to reduce temporal resolution, i.e., rendering frame rate, seems less explored. In this work, we present the results of a user study investigating the perceptual effects of foveated temporal resolution reduction, where only the temporal resolution (frame rate) is reduced in the periphery without affecting spatial quality (pixel density). In particular, we investigated the perception of temporal resolution artifacts caused by reducing the frame rate dependent on the eccentricity of the user's gaze. Our user study with 15 participants was conducted in a virtual reality setting using a head-mounted display. Our results indicate that it was possible to reduce average rendering costs, i.e., the number of rendered pixels, to a large degree before participants consistently reported perceiving temporal artifacts.",
    "metadata": {
      "arxiv_id": "2505.03682",
      "title": "Evaluating Foveated Frame Rate Reduction in Virtual Reality for Head-Mounted Displays",
      "summary": "Foveated rendering methods usually reduce spatial resolution in the periphery of the users' view. However, using foveated rendering to reduce temporal resolution, i.e., rendering frame rate, seems less explored. In this work, we present the results of a user study investigating the perceptual effects of foveated temporal resolution reduction, where only the temporal resolution (frame rate) is reduced in the periphery without affecting spatial quality (pixel density). In particular, we investigated the perception of temporal resolution artifacts caused by reducing the frame rate dependent on the eccentricity of the user's gaze. Our user study with 15 participants was conducted in a virtual reality setting using a head-mounted display. Our results indicate that it was possible to reduce average rendering costs, i.e., the number of rendered pixels, to a large degree before participants consistently reported perceiving temporal artifacts.",
      "authors": [
        "Christopher Flöter",
        "Sergej Geringer",
        "Guido Reina",
        "Daniel Weiskopf",
        "Timo Ropinski"
      ],
      "published": "2025-05-06T16:27:47Z",
      "updated": "2025-05-06T16:27:47Z",
      "categories": [
        "cs.HC",
        "cs.GR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.03682v1",
      "landing_url": "https://arxiv.org/abs/2505.03682v1",
      "doi": "https://doi.org/10.1145/3715669.3725870"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is a VR perceptual study on foveated frame-rate reduction, which concerns visual rendering and temporal artifacts rather than discrete audio token generation/quantization; it fails the inclusion criteria focused on discrete audio tokens and token-based representations, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is a VR perceptual study on foveated frame-rate reduction, which concerns visual rendering and temporal artifacts rather than discrete audio token generation/quantization; it fails the inclusion criteria focused on discrete audio tokens and token-based representations, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on the evaluation of foveated frame rate reduction in virtual reality for head-mounted displays, which is related to VR rendering and temporal resolution perceptual effects, not on discrete audio tokens or their generation via vector quantization or semantic-driven discretization methods. It does not discuss any form of audio representation, tokenization, or language models for audio, thus not meeting the inclusion criteria, and belongs to a different research domain entirely.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on the evaluation of foveated frame rate reduction in virtual reality for head-mounted displays, which is related to VR rendering and temporal resolution perceptual effects, not on discrete audio tokens or their generation via vector quantization or semantic-driven discretization methods. It does not discuss any form of audio representation, tokenization, or language models for audio, thus not meeting the inclusion criteria, and belongs to a different research domain entirely.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Accurate and Efficient Multivariate Time Series Forecasting via Offline Clustering",
    "abstract": "Accurate and efficient multivariate time series (MTS) forecasting is essential for applications such as traffic management and weather prediction, which depend on capturing long-range temporal dependencies and interactions between entities. Existing methods, particularly those based on Transformer architectures, compute pairwise dependencies across all time steps, leading to a computational complexity that scales quadratically with the length of the input. To overcome these challenges, we introduce the Forecaster with Offline Clustering Using Segments (FOCUS), a novel approach to MTS forecasting that simplifies long-range dependency modeling through the use of prototypes extracted via offline clustering. These prototypes encapsulate high-level events in the real-world system underlying the data, summarizing the key characteristics of similar time segments. In the online phase, FOCUS dynamically adapts these patterns to the current input and captures dependencies between the input segment and high-level events, enabling both accurate and efficient forecasting. By identifying prototypes during the offline clustering phase, FOCUS reduces the computational complexity of modeling long-range dependencies in the online phase to linear scaling. Extensive experiments across diverse benchmarks demonstrate that FOCUS achieves state-of-the-art accuracy while significantly reducing computational costs.",
    "metadata": {
      "arxiv_id": "2505.05738",
      "title": "Accurate and Efficient Multivariate Time Series Forecasting via Offline Clustering",
      "summary": "Accurate and efficient multivariate time series (MTS) forecasting is essential for applications such as traffic management and weather prediction, which depend on capturing long-range temporal dependencies and interactions between entities. Existing methods, particularly those based on Transformer architectures, compute pairwise dependencies across all time steps, leading to a computational complexity that scales quadratically with the length of the input. To overcome these challenges, we introduce the Forecaster with Offline Clustering Using Segments (FOCUS), a novel approach to MTS forecasting that simplifies long-range dependency modeling through the use of prototypes extracted via offline clustering. These prototypes encapsulate high-level events in the real-world system underlying the data, summarizing the key characteristics of similar time segments. In the online phase, FOCUS dynamically adapts these patterns to the current input and captures dependencies between the input segment and high-level events, enabling both accurate and efficient forecasting. By identifying prototypes during the offline clustering phase, FOCUS reduces the computational complexity of modeling long-range dependencies in the online phase to linear scaling. Extensive experiments across diverse benchmarks demonstrate that FOCUS achieves state-of-the-art accuracy while significantly reducing computational costs.",
      "authors": [
        "Yiming Niu",
        "Jinliang Deng",
        "Lulu Zhang",
        "Zimu Zhou",
        "Yongxin Tong"
      ],
      "published": "2025-05-09T02:34:06Z",
      "updated": "2025-05-25T07:48:29Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.05738v2",
      "landing_url": "https://arxiv.org/abs/2505.05738v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.05738"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses offline clustering-based multivariate time series forecasting (FOCUS); it does not involve discrete audio tokens or tokenized audio representations, thus it fails the Audio Token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses offline clustering-based multivariate time series forecasting (FOCUS); it does not involve discrete audio tokens or tokenized audio representations, thus it fails the Audio Token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a method for efficient multivariate time series forecasting using offline clustering techniques, focusing on temporal dependencies in applications like traffic management and weather prediction. There is no indication that this study involves transforming continuous audio signals into discrete audio tokens, nor does it mention vector quantization, token generation, or the use of such tokens in audio language modeling or related tasks. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and contains none of the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a method for efficient multivariate time series forecasting using offline clustering techniques, focusing on temporal dependencies in applications like traffic management and weather prediction. There is no indication that this study involves transforming continuous audio signals into discrete audio tokens, nor does it mention vector quantization, token generation, or the use of such tokens in audio language modeling or related tasks. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and contains none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits",
    "abstract": "Cloud-based multilingual translation services like Google Translate and Microsoft Translator achieve state-of-the-art translation capabilities. These services inherently use large multilingual language models such as GRU, LSTM, BERT, GPT, T5, or similar encoder-decoder architectures with attention mechanisms as the backbone. Also, new age natural language systems, for instance ChatGPT and DeepSeek, have established huge potential in multiple tasks in natural language processing. At the same time, they also possess outstanding multilingual translation capabilities. However, these models use the classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder Attention-based Convolutional Variational Circuits) is an alternate solution that explores the quantum computing realm instead of the classical computing realm to study and demonstrate multilingual machine translation. QEDACVC introduces the quantum encoder-decoder architecture that simulates and runs on quantum computing hardware via quantum convolution, quantum pooling, quantum variational circuit, and quantum attention as software alterations. QEDACVC achieves an Accuracy of 82% when trained on the OPUS dataset for English, French, German, and Hindi corpora for multilingual translations.",
    "metadata": {
      "arxiv_id": "2505.09407",
      "title": "Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits",
      "summary": "Cloud-based multilingual translation services like Google Translate and Microsoft Translator achieve state-of-the-art translation capabilities. These services inherently use large multilingual language models such as GRU, LSTM, BERT, GPT, T5, or similar encoder-decoder architectures with attention mechanisms as the backbone. Also, new age natural language systems, for instance ChatGPT and DeepSeek, have established huge potential in multiple tasks in natural language processing. At the same time, they also possess outstanding multilingual translation capabilities. However, these models use the classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder Attention-based Convolutional Variational Circuits) is an alternate solution that explores the quantum computing realm instead of the classical computing realm to study and demonstrate multilingual machine translation. QEDACVC introduces the quantum encoder-decoder architecture that simulates and runs on quantum computing hardware via quantum convolution, quantum pooling, quantum variational circuit, and quantum attention as software alterations. QEDACVC achieves an Accuracy of 82% when trained on the OPUS dataset for English, French, German, and Hindi corpora for multilingual translations.",
      "authors": [
        "Subrit Dikshit",
        "Ritu Tiwari",
        "Priyank Jain"
      ],
      "published": "2025-05-14T14:04:44Z",
      "updated": "2025-05-14T14:04:44Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.09407v1",
      "landing_url": "https://arxiv.org/abs/2505.09407v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.09407"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses quantum encoder-decoder architectures for multilingual machine translation and does not address discrete audio token design, token generation, or audio token representations required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses quantum encoder-decoder architectures for multilingual machine translation and does not address discrete audio token design, token generation, or audio token representations required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on quantum computing methods for multilingual machine translation, which does not involve discrete audio tokens or their design, generation process, or use in sequence/language models for audio understanding or generation. It lacks details on audio tokenization, quantization, or any form of discrete audio representation as defined in the inclusion criteria, and thus does not meet the thematic or methodological requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on quantum computing methods for multilingual machine translation, which does not involve discrete audio tokens or their design, generation process, or use in sequence/language models for audio understanding or generation. It lacks details on audio tokenization, quantization, or any form of discrete audio representation as defined in the inclusion criteria, and thus does not meet the thematic or methodological requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption",
    "abstract": "Modern video generation frameworks based on Latent Diffusion Models suffer from inefficiencies in tokenization due to the Frame-Proportional Information Assumption. Existing tokenizers provide fixed temporal compression rates, causing the computational cost of the diffusion model to scale linearly with the frame rate. The paper proposes the Duration-Proportional Information Assumption: the upper bound on the information capacity of a video is proportional to the duration rather than the number of frames. Based on this insight, the paper introduces VFRTok, a Transformer-based video tokenizer, that enables variable frame rate encoding and decoding through asymmetric frame rate training between the encoder and decoder. Furthermore, the paper proposes Partial Rotary Position Embeddings (RoPE) to decouple position and content modeling, which groups correlated patches into unified tokens. The Partial RoPE effectively improves content-awareness, enhancing the video generation capability. Benefiting from the compact and continuous spatio-temporal representation, VFRTok achieves competitive reconstruction quality and state-of-the-art generation fidelity while using only 1/8 tokens compared to existing tokenizers. The code and weights are released at: https://github.com/KwaiVGI/VFRTok.",
    "metadata": {
      "arxiv_id": "2505.12053",
      "title": "VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption",
      "summary": "Modern video generation frameworks based on Latent Diffusion Models suffer from inefficiencies in tokenization due to the Frame-Proportional Information Assumption. Existing tokenizers provide fixed temporal compression rates, causing the computational cost of the diffusion model to scale linearly with the frame rate. The paper proposes the Duration-Proportional Information Assumption: the upper bound on the information capacity of a video is proportional to the duration rather than the number of frames. Based on this insight, the paper introduces VFRTok, a Transformer-based video tokenizer, that enables variable frame rate encoding and decoding through asymmetric frame rate training between the encoder and decoder. Furthermore, the paper proposes Partial Rotary Position Embeddings (RoPE) to decouple position and content modeling, which groups correlated patches into unified tokens. The Partial RoPE effectively improves content-awareness, enhancing the video generation capability. Benefiting from the compact and continuous spatio-temporal representation, VFRTok achieves competitive reconstruction quality and state-of-the-art generation fidelity while using only 1/8 tokens compared to existing tokenizers. The code and weights are released at: https://github.com/KwaiVGI/VFRTok.",
      "authors": [
        "Tianxiong Zhong",
        "Xingye Tian",
        "Boyuan Jiang",
        "Xuebo Wang",
        "Xin Tao",
        "Pengfei Wan",
        "Zhiwei Zhang"
      ],
      "published": "2025-05-17T15:32:54Z",
      "updated": "2025-09-28T09:51:18Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.12053v2",
      "landing_url": "https://arxiv.org/abs/2505.12053v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.12053"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on a video tokenizer (VFRTok) with variable frame rates, not discrete audio token representations for audio-language modeling, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on a video tokenizer (VFRTok) with variable frame rates, not discrete audio token representations for audio-language modeling, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a video tokenizer for video generation using Transformer-based models and variable frame rates, which is unrelated to audio or discrete audio tokens as defined in the inclusion criteria; it does not describe any audio tokenization, quantization, or audio-related representations, and thus does not meet the topic requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a video tokenizer for video generation using Transformer-based models and variable frame rates, which is unrelated to audio or discrete audio tokens as defined in the inclusion criteria; it does not describe any audio tokenization, quantization, or audio-related representations, and thus does not meet the topic requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Codec-Based Deepfake Source Tracing via Neural Audio Codec Taxonomy",
    "abstract": "Recent advances in neural audio codec-based speech generation (CoSG) models have produced remarkably realistic audio deepfakes. We refer to deepfake speech generated by CoSG systems as codec-based deepfake, or CodecFake. Although existing anti-spoofing research on CodecFake predominantly focuses on verifying the authenticity of audio samples, almost no attention was given to tracing the CoSG used in generating these deepfakes. In CodecFake generation, processes such as speech-to-unit encoding, discrete unit modeling, and unit-to-speech decoding are fundamentally based on neural audio codecs. Motivated by this, we introduce source tracing for CodecFake via neural audio codec taxonomy, which dissects neural audio codecs to trace CoSG. Our experimental results on the CodecFake+ dataset provide promising initial evidence for the feasibility of CodecFake source tracing while also highlighting several challenges that warrant further investigation.",
    "metadata": {
      "arxiv_id": "2505.12994",
      "title": "Codec-Based Deepfake Source Tracing via Neural Audio Codec Taxonomy",
      "summary": "Recent advances in neural audio codec-based speech generation (CoSG) models have produced remarkably realistic audio deepfakes. We refer to deepfake speech generated by CoSG systems as codec-based deepfake, or CodecFake. Although existing anti-spoofing research on CodecFake predominantly focuses on verifying the authenticity of audio samples, almost no attention was given to tracing the CoSG used in generating these deepfakes. In CodecFake generation, processes such as speech-to-unit encoding, discrete unit modeling, and unit-to-speech decoding are fundamentally based on neural audio codecs. Motivated by this, we introduce source tracing for CodecFake via neural audio codec taxonomy, which dissects neural audio codecs to trace CoSG. Our experimental results on the CodecFake+ dataset provide promising initial evidence for the feasibility of CodecFake source tracing while also highlighting several challenges that warrant further investigation.",
      "authors": [
        "Xuanjun Chen",
        "I-Ming Lin",
        "Lin Zhang",
        "Jiawei Du",
        "Haibin Wu",
        "Hung-yi Lee",
        "Jyh-Shing Roger Jang"
      ],
      "published": "2025-05-19T11:31:32Z",
      "updated": "2025-08-03T07:58:47Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.12994v3",
      "landing_url": "https://arxiv.org/abs/2505.12994v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.12994"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Primarily discusses tracing CoSG via neural codec taxonomy and notes discrete unit encoding, but does not present discrete audio token design as a core contribution; also there is no publication date provided, triggering an exclusion per criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Primarily discusses tracing CoSG via neural codec taxonomy and notes discrete unit encoding, but does not present discrete audio token design as a core contribution; also there is no publication date provided, triggering an exclusion per criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on neural audio codecs used for speech generation, discussing codec-based deepfake source tracing through a taxonomy of neural audio codecs. However, the abstract does not detail the process of producing discrete audio tokens, their quantization methods, codebook design, or their explicit use as discrete tokens for sequence or language models, which is a critical inclusion criterion. The focus is more on source tracing of generated audio deepfakes rather than token design or audio language modeling. Thus, it lacks clear description of token generation and their role in sequence modeling, and lacks methods details required by the criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on neural audio codecs used for speech generation, discussing codec-based deepfake source tracing through a taxonomy of neural audio codecs. However, the abstract does not detail the process of producing discrete audio tokens, their quantization methods, codebook design, or their explicit use as discrete tokens for sequence or language models, which is a critical inclusion criterion. The focus is more on source tracing of generated audio deepfakes rather than token design or audio language modeling. Thus, it lacks clear description of token generation and their role in sequence modeling, and lacks methods details required by the criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "DualCodec: A Low-Frame-Rate, Semantically-Enhanced Neural Audio Codec for Speech Generation",
    "abstract": "Neural audio codecs form the foundational building blocks for language model (LM)-based speech generation. Typically, there is a trade-off between frame rate and audio quality. This study introduces a low-frame-rate, semantically enhanced codec model. Existing approaches distill semantically rich self-supervised (SSL) representations into the first-layer codec tokens. This work proposes DualCodec, a dual-stream encoding approach that integrates SSL and waveform representations within an end-to-end codec framework. In this setting, DualCodec enhances the semantic information in the first-layer codec and enables the codec system to maintain high audio quality while operating at a low frame rate. Note that a low-frame-rate codec improves the efficiency of speech generation. Experimental results on audio codec and speech generation tasks confirm the effectiveness of the proposed DualCodec compared to state-of-the-art codec systems, such as Mimi Codec, SpeechTokenizer, DAC, and Encodec. Demos are available at: https://dualcodec.github.io, code is available at: https://github.com/jiaqili3/DualCodec",
    "metadata": {
      "arxiv_id": "2505.13000",
      "title": "DualCodec: A Low-Frame-Rate, Semantically-Enhanced Neural Audio Codec for Speech Generation",
      "summary": "Neural audio codecs form the foundational building blocks for language model (LM)-based speech generation. Typically, there is a trade-off between frame rate and audio quality. This study introduces a low-frame-rate, semantically enhanced codec model. Existing approaches distill semantically rich self-supervised (SSL) representations into the first-layer codec tokens. This work proposes DualCodec, a dual-stream encoding approach that integrates SSL and waveform representations within an end-to-end codec framework. In this setting, DualCodec enhances the semantic information in the first-layer codec and enables the codec system to maintain high audio quality while operating at a low frame rate. Note that a low-frame-rate codec improves the efficiency of speech generation. Experimental results on audio codec and speech generation tasks confirm the effectiveness of the proposed DualCodec compared to state-of-the-art codec systems, such as Mimi Codec, SpeechTokenizer, DAC, and Encodec. Demos are available at: https://dualcodec.github.io, code is available at: https://github.com/jiaqili3/DualCodec",
      "authors": [
        "Jiaqi Li",
        "Xiaolong Lin",
        "Zhekai Li",
        "Shixi Huang",
        "Yuancheng Wang",
        "Chaoren Wang",
        "Zhenpeng Zhan",
        "Zhizheng Wu"
      ],
      "published": "2025-05-19T11:41:08Z",
      "updated": "2025-10-01T15:01:57Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.13000v2",
      "landing_url": "https://arxiv.org/abs/2505.13000v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.13000"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the abstract mentions tokens and semantic enhancement, it does not clearly describe discrete audio token generation/quantization or provide reproducible methodological details, so it does not satisfy all inclusion criteria for discrete audio tokens.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the abstract mentions tokens and semantic enhancement, it does not clearly describe discrete audio token generation/quantization or provide reproducible methodological details, so it does not satisfy all inclusion criteria for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces DualCodec, a neural audio codec that integrates semantic self-supervised representations and waveform representations to produce a low-frame-rate codec. It operates as a dual-stream encoding system with an end-to-end framework, likely involving a discrete token generation process that retains semantic and acoustic information, aimed at improving speech generation efficiency. The abstract mentions semantically enhanced codec tokens and comparisons with state-of-the-art neural codecs, suggesting that the representation involves discrete token mechanisms suitable for sequence modeling. The code availability and demo link suggest reproducibility and method transparency. There is no indication that this is a traditional codec focusing solely on compression without semantic token generation. Therefore, the study meets the key inclusion criteria related to discrete audio token design, with clear indication of token generation methods and use in audio generation. No exclusion criteria are triggered.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study introduces DualCodec, a neural audio codec that integrates semantic self-supervised representations and waveform representations to produce a low-frame-rate codec. It operates as a dual-stream encoding system with an end-to-end framework, likely involving a discrete token generation process that retains semantic and acoustic information, aimed at improving speech generation efficiency. The abstract mentions semantically enhanced codec tokens and comparisons with state-of-the-art neural codecs, suggesting that the representation involves discrete token mechanisms suitable for sequence modeling. The code availability and demo link suggest reproducibility and method transparency. There is no indication that this is a traditional codec focusing solely on compression without semantic token generation. Therefore, the study meets the key inclusion criteria related to discrete audio token design, with clear indication of token generation methods and use in audio generation. No exclusion criteria are triggered.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed: JuniorNano recommended exclusion (2) citing lack of explicit discrete-token/quantization details in the abstract, while JuniorMini recommended inclusion (5) noting explicit mention of \"codec tokens\", semantic SSL integration, and available code/demos. The abstract explicitly frames a semantically-enhanced neural audio codec that augments first-layer \"codec tokens\" via dual-stream (SSL + waveform) encoding and compares to known discrete-token codecs (Mimi Codec, SpeechTokenizer, DAC, Encodec); the presence of a GitHub repo increases the likelihood that discrete-token/quantization methods and reproducible technical details exist. However, the abstract itself does not explicitly describe the tokenization/quantization mechanism (VQ/RVQ/hierarchical codebooks) or include detailed methodological settings required by the inclusion criteria, so there is some uncertainty. Overall decision: better to include because the work’s main contribution appears to be discrete, semantically-enhanced codec tokens for sequence modeling and speech generation and code is provided, but the abstract’s lack of explicit quantization details prevents a conclusive full-accept certainty.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed: JuniorNano recommended exclusion (2) citing lack of explicit discrete-token/quantization details in the abstract, while JuniorMini recommended inclusion (5) noting explicit mention of \"codec tokens\", semantic SSL integration, and available code/demos. The abstract explicitly frames a semantically-enhanced neural audio codec that augments first-layer \"codec tokens\" via dual-stream (SSL + waveform) encoding and compares to known discrete-token codecs (Mimi Codec, SpeechTokenizer, DAC, Encodec); the presence of a GitHub repo increases the likelihood that discrete-token/quantization methods and reproducible technical details exist. However, the abstract itself does not explicitly describe the tokenization/quantization mechanism (VQ/RVQ/hierarchical codebooks) or include detailed methodological settings required by the inclusion criteria, so there is some uncertainty. Overall decision: better to include because the work’s main contribution appears to be discrete, semantically-enhanced codec tokens for sequence modeling and speech generation and code is provided, but the abstract’s lack of explicit quantization details prevents a conclusive full-accept certainty.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising",
    "abstract": "Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models.",
    "metadata": {
      "arxiv_id": "2505.13830",
      "title": "Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising",
      "summary": "Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models.",
      "authors": [
        "Ye-Xin Lu",
        "Hui-Peng Du",
        "Fei Liu",
        "Yang Ai",
        "Zhen-Hua Ling"
      ],
      "published": "2025-05-20T02:18:45Z",
      "updated": "2025-05-22T04:41:35Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.13830v2",
      "landing_url": "https://arxiv.org/abs/2505.13830v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.13830"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study explicitly uses discrete acoustic tokens via a neural codec-based denoiser (token denoiser) and an embedding refiner to enable noise-robust TTS, detailing a token generation/denoising pipeline, which aligns with the discrete-token inclusion criteria and goes beyond a mere demo, so it should be included (score 4).",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The study explicitly uses discrete acoustic tokens via a neural codec-based denoiser (token denoiser) and an embedding refiner to enable noise-robust TTS, detailing a token generation/denoising pipeline, which aligns with the discrete-token inclusion criteria and goes beyond a mere demo, so it should be included (score 4).",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a neural codec-based speech denoiser integrated with an LLM-based TTS system, leveraging discrete acoustic tokens as acoustic prompts for noise-robust zero-shot TTS. It explicitly mentions predicting clean acoustic tokens from noisy ones and uses a codec denoiser involving token denoising, indicating the use of discrete token representations likely produced by vector quantization or similar means. The approach targets improving generation quality with these tokens, aligning with the inclusion criteria about discrete audio tokens as core methodological contributions for downstream sequence modeling and generation. The abstract provides sufficient methodological details supporting reproducibility (token prediction, embedding refiner, codec decoder). There is no indication the study falls into exclusion categories such as only demo-level content or traditional codec use without sequence modeling aims. Therefore, the study meets the inclusion criteria clearly and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a neural codec-based speech denoiser integrated with an LLM-based TTS system, leveraging discrete acoustic tokens as acoustic prompts for noise-robust zero-shot TTS. It explicitly mentions predicting clean acoustic tokens from noisy ones and uses a codec denoiser involving token denoising, indicating the use of discrete token representations likely produced by vector quantization or similar means. The approach targets improving generation quality with these tokens, aligning with the inclusion criteria about discrete audio tokens as core methodological contributions for downstream sequence modeling and generation. The abstract provides sufficient methodological details supporting reproducibility (token prediction, embedding refiner, codec decoder). There is no indication the study falls into exclusion categories such as only demo-level content or traditional codec use without sequence modeling aims. Therefore, the study meets the inclusion criteria clearly and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Discrete Audio Representations for Automated Audio Captioning",
    "abstract": "Discrete audio representations, termed audio tokens, are broadly categorized into semantic and acoustic tokens, typically generated through unsupervised tokenization of continuous audio representations. However, their applicability to automated audio captioning (AAC) remains underexplored. This paper systematically investigates the viability of audio token-driven models for AAC through comparative analyses of various tokenization methods. Our findings reveal that audio tokenization leads to performance degradation in AAC models compared to those that directly utilize continuous audio representations. To address this issue, we introduce a supervised audio tokenizer trained with an audio tagging objective. Unlike unsupervised tokenizers, which lack explicit semantic understanding, the proposed tokenizer effectively captures audio event information. Experiments conducted on the Clotho dataset demonstrate that the proposed audio tokens outperform conventional audio tokens in the AAC task.",
    "metadata": {
      "arxiv_id": "2505.14989",
      "title": "Discrete Audio Representations for Automated Audio Captioning",
      "summary": "Discrete audio representations, termed audio tokens, are broadly categorized into semantic and acoustic tokens, typically generated through unsupervised tokenization of continuous audio representations. However, their applicability to automated audio captioning (AAC) remains underexplored. This paper systematically investigates the viability of audio token-driven models for AAC through comparative analyses of various tokenization methods. Our findings reveal that audio tokenization leads to performance degradation in AAC models compared to those that directly utilize continuous audio representations. To address this issue, we introduce a supervised audio tokenizer trained with an audio tagging objective. Unlike unsupervised tokenizers, which lack explicit semantic understanding, the proposed tokenizer effectively captures audio event information. Experiments conducted on the Clotho dataset demonstrate that the proposed audio tokens outperform conventional audio tokens in the AAC task.",
      "authors": [
        "Jingguang Tian",
        "Haoqin Sun",
        "Xinhui Hu",
        "Xinkang Xu"
      ],
      "published": "2025-05-21T00:27:38Z",
      "updated": "2025-05-21T00:27:38Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.14989v1",
      "landing_url": "https://arxiv.org/abs/2505.14989v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.14989"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on discrete audio representations/tokens and their use in automated audio captioning, introduces a supervised audio tokenizer and supporting experiments, thereby meeting the inclusion criteria for discrete token design with methodological detail, and there is no clear evidence of being a mere demo or lacking necessary technical details in the abstract.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item centers on discrete audio representations/tokens and their use in automated audio captioning, introduces a supervised audio tokenizer and supporting experiments, thereby meeting the inclusion criteria for discrete token design with methodological detail, and there is no clear evidence of being a mere demo or lacking necessary technical details in the abstract.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on discrete audio representations, specifically audio tokens derived via tokenization methods, which directly aligns with the inclusion criteria regarding discrete audio token generation and their use in audio language modeling tasks such as automated audio captioning. The paper investigates tokenization methods (including a supervised audio tokenizer), discusses token generation processes, and evaluates their role in AAC, a downstream sequence modeling task, thereby providing detailed methodological content and experimental validation. There is no indication of exclusion criteria such as lack of technical details or outdated source info. Thus, it meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on discrete audio representations, specifically audio tokens derived via tokenization methods, which directly aligns with the inclusion criteria regarding discrete audio token generation and their use in audio language modeling tasks such as automated audio captioning. The paper investigates tokenization methods (including a supervised audio tokenizer), discusses token generation processes, and evaluates their role in AAC, a downstream sequence modeling task, thereby providing detailed methodological content and experimental validation. There is no indication of exclusion criteria such as lack of technical details or outdated source info. Thus, it meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Source Separation by Flow Matching",
    "abstract": "We consider the problem of single-channel audio source separation with the goal of reconstructing $K$ sources from their mixture. We address this ill-posed problem with FLOSS (FLOw matching for Source Separation), a constrained generation method based on flow matching, ensuring strict mixture consistency. Flow matching is a general methodology that, when given samples from two probability distributions defined on the same space, learns an ordinary differential equation to output a sample from one of the distributions when provided with a sample from the other. In our context, we have access to samples from the joint distribution of $K$ sources and so the corresponding samples from the lower-dimensional distribution of their mixture. To apply flow matching, we augment these mixture samples with artificial noise components to match the dimensionality of the $K$ source distribution. Additionally, as any permutation of the sources yields the same mixture, we adopt an equivariant formulation of flow matching which relies on a neural network architecture that is equivariant by design. We demonstrate the performance of the method for the separation of overlapping speech.",
    "metadata": {
      "arxiv_id": "2505.16119",
      "title": "Source Separation by Flow Matching",
      "summary": "We consider the problem of single-channel audio source separation with the goal of reconstructing $K$ sources from their mixture. We address this ill-posed problem with FLOSS (FLOw matching for Source Separation), a constrained generation method based on flow matching, ensuring strict mixture consistency. Flow matching is a general methodology that, when given samples from two probability distributions defined on the same space, learns an ordinary differential equation to output a sample from one of the distributions when provided with a sample from the other. In our context, we have access to samples from the joint distribution of $K$ sources and so the corresponding samples from the lower-dimensional distribution of their mixture. To apply flow matching, we augment these mixture samples with artificial noise components to match the dimensionality of the $K$ source distribution. Additionally, as any permutation of the sources yields the same mixture, we adopt an equivariant formulation of flow matching which relies on a neural network architecture that is equivariant by design. We demonstrate the performance of the method for the separation of overlapping speech.",
      "authors": [
        "Robin Scheibler",
        "John R. Hershey",
        "Arnaud Doucet",
        "Henry Li"
      ],
      "published": "2025-05-22T01:52:06Z",
      "updated": "2025-07-18T00:23:27Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.16119v2",
      "landing_url": "https://arxiv.org/abs/2505.16119v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.16119"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract addresses single-channel source separation via flow matching and does not propose or evaluate discrete audio token representations for language-model-style processing, so it fails the core inclusion criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract addresses single-channel source separation via flow matching and does not propose or evaluate discrete audio token representations for language-model-style processing, so it fails the core inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on a source separation method using flow matching, which is a technique to separate overlapping speech sources. However, it does not address the generation or design of discrete audio tokens, nor does it discuss vector quantization or tokenization processes as required by the inclusion criteria. The method is about reconstructing sources from a mixture using neural differential equations rather than producing discrete tokens for use in language models or sequence models. Therefore, it does not meet the key inclusion criterion of having discrete audio token design as a core contribution or method.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on a source separation method using flow matching, which is a technique to separate overlapping speech sources. However, it does not address the generation or design of discrete audio tokens, nor does it discuss vector quantization or tokenization processes as required by the inclusion criteria. The method is about reconstructing sources from a mixture using neural differential equations rather than producing discrete tokens for use in language models or sequence models. Therefore, it does not meet the key inclusion criterion of having discrete audio token design as a core contribution or method.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Generative Latent Coding for Ultra-Low Bitrate Image and Video Compression",
    "abstract": "Most existing approaches for image and video compression perform transform coding in the pixel space to reduce redundancy. However, due to the misalignment between the pixel-space distortion and human perception, such schemes often face the difficulties in achieving both high-realism and high-fidelity at ultra-low bitrate. To solve this problem, we propose \\textbf{G}enerative \\textbf{L}atent \\textbf{C}oding (\\textbf{GLC}) models for image and video compression, termed GLC-image and GLC-Video. The transform coding of GLC is conducted in the latent space of a generative vector-quantized variational auto-encoder (VQ-VAE). Compared to the pixel-space, such a latent space offers greater sparsity, richer semantics and better alignment with human perception, and show its advantages in achieving high-realism and high-fidelity compression. To further enhance performance, we improve the hyper prior by introducing a spatial categorical hyper module in GLC-image and a spatio-temporal categorical hyper module in GLC-video. Additionally, the code-prediction-based loss function is proposed to enhance the semantic consistency. Experiments demonstrate that our scheme shows high visual quality at ultra-low bitrate for both image and video compression. For image compression, GLC-image achieves an impressive bitrate of less than $0.04$ bpp, achieving the same FID as previous SOTA model MS-ILLM while using $45\\%$ fewer bitrate on the CLIC 2020 test set. For video compression, GLC-video achieves 65.3\\% bitrate saving over PLVC in terms of DISTS.",
    "metadata": {
      "arxiv_id": "2505.16177",
      "title": "Generative Latent Coding for Ultra-Low Bitrate Image and Video Compression",
      "summary": "Most existing approaches for image and video compression perform transform coding in the pixel space to reduce redundancy. However, due to the misalignment between the pixel-space distortion and human perception, such schemes often face the difficulties in achieving both high-realism and high-fidelity at ultra-low bitrate. To solve this problem, we propose \\textbf{G}enerative \\textbf{L}atent \\textbf{C}oding (\\textbf{GLC}) models for image and video compression, termed GLC-image and GLC-Video. The transform coding of GLC is conducted in the latent space of a generative vector-quantized variational auto-encoder (VQ-VAE). Compared to the pixel-space, such a latent space offers greater sparsity, richer semantics and better alignment with human perception, and show its advantages in achieving high-realism and high-fidelity compression. To further enhance performance, we improve the hyper prior by introducing a spatial categorical hyper module in GLC-image and a spatio-temporal categorical hyper module in GLC-video. Additionally, the code-prediction-based loss function is proposed to enhance the semantic consistency. Experiments demonstrate that our scheme shows high visual quality at ultra-low bitrate for both image and video compression. For image compression, GLC-image achieves an impressive bitrate of less than $0.04$ bpp, achieving the same FID as previous SOTA model MS-ILLM while using $45\\%$ fewer bitrate on the CLIC 2020 test set. For video compression, GLC-video achieves 65.3\\% bitrate saving over PLVC in terms of DISTS.",
      "authors": [
        "Linfeng Qi",
        "Zhaoyang Jia",
        "Jiahao Li",
        "Bin Li",
        "Houqiang Li",
        "Yan Lu"
      ],
      "published": "2025-05-22T03:31:33Z",
      "updated": "2025-05-22T03:31:33Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.16177v1",
      "landing_url": "https://arxiv.org/abs/2505.16177v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.16177"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item concerns image/video compression via latent coding (GLC with VQ-VAE), not discrete audio token generation/quantization; it does not address audio token design, semantic discretization, or audio language-modeling applications, so it does not meet the inclusion criteria and fails the exclusion criteria. Therefore exclude.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item concerns image/video compression via latent coding (GLC with VQ-VAE), not discrete audio token generation/quantization; it does not address audio token design, semantic discretization, or audio language-modeling applications, so it does not meet the inclusion criteria and fails the exclusion criteria. Therefore exclude.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on image and video compression using generative latent coding and vector-quantized variational auto-encoders, which are techniques applied to visual data rather than audio signals. It does not address discrete audio tokens or audio signal processing, nor does it mention the use of tokens for language or sequence modeling in audio contexts. Therefore, it does not meet the inclusion criteria related to discrete audio token design or audio language modeling, and is outside the scope of the defined topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on image and video compression using generative latent coding and vector-quantized variational auto-encoders, which are techniques applied to visual data rather than audio signals. It does not address discrete audio tokens or audio signal processing, nor does it mention the use of tokens for language or sequence modeling in audio contexts. Therefore, it does not meet the inclusion criteria related to discrete audio token design or audio language modeling, and is outside the scope of the defined topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unlocking Temporal Flexibility: Neural Speech Codec with Variable Frame Rate",
    "abstract": "Most neural speech codecs achieve bitrate adjustment through intra-frame mechanisms, such as codebook dropout, at a Constant Frame Rate (CFR). However, speech segments inherently have time-varying information density (e.g., silent intervals versus voiced regions). This property makes CFR not optimal in terms of bitrate and token sequence length, hindering efficiency in real-time applications. In this work, we propose a Temporally Flexible Coding (TFC) technique, introducing variable frame rate (VFR) into neural speech codecs for the first time. TFC enables seamlessly tunable average frame rates and dynamically allocates frame rates based on temporal entropy. Experimental results show that a codec with TFC achieves optimal reconstruction quality with high flexibility, and maintains competitive performance even at lower frame rates. Our approach is promising for the integration with other efforts to develop low-frame-rate neural speech codecs for more efficient downstream tasks.",
    "metadata": {
      "arxiv_id": "2505.16845",
      "title": "Unlocking Temporal Flexibility: Neural Speech Codec with Variable Frame Rate",
      "summary": "Most neural speech codecs achieve bitrate adjustment through intra-frame mechanisms, such as codebook dropout, at a Constant Frame Rate (CFR). However, speech segments inherently have time-varying information density (e.g., silent intervals versus voiced regions). This property makes CFR not optimal in terms of bitrate and token sequence length, hindering efficiency in real-time applications. In this work, we propose a Temporally Flexible Coding (TFC) technique, introducing variable frame rate (VFR) into neural speech codecs for the first time. TFC enables seamlessly tunable average frame rates and dynamically allocates frame rates based on temporal entropy. Experimental results show that a codec with TFC achieves optimal reconstruction quality with high flexibility, and maintains competitive performance even at lower frame rates. Our approach is promising for the integration with other efforts to develop low-frame-rate neural speech codecs for more efficient downstream tasks.",
      "authors": [
        "Hanglei Zhang",
        "Yiwei Guo",
        "Zhihan Li",
        "Xiang Hao",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2025-05-22T16:10:01Z",
      "updated": "2025-05-22T16:10:01Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.16845v1",
      "landing_url": "https://arxiv.org/abs/2505.16845v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.16845"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work proposes a neural speech codec with variable frame rate (TFC), focusing on CFR/VFR bitrate and temporal entropy, and does not present discrete audio tokens or token-based representations, so it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This work proposes a neural speech codec with variable frame rate (TFC), focusing on CFR/VFR bitrate and temporal entropy, and does not present discrete audio tokens or token-based representations, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces a neural speech codec with variable frame rates but does not explicitly describe generating discrete audio tokens via vector quantization or related discrete token design methods; it focuses on bitrate adjustments and coding efficiency rather than producing and utilizing discrete tokens for language or sequence modeling, which is the core requirement for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study introduces a neural speech codec with variable frame rates but does not explicitly describe generating discrete audio tokens via vector quantization or related discrete token design methods; it focuses on bitrate adjustments and coding efficiency rather than producing and utilizing discrete tokens for language or sequence modeling, which is the core requirement for inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English",
    "abstract": "The speech tokenizer plays a crucial role in recent speech tasks, generally serving as a bridge between speech signals and language models. While low-frame-rate codecs are widely employed as speech tokenizers, the impact of frame rates on speech tokens remains underexplored. In this study, we investigate how varying frame rates affect speech tokenization by examining Mandarin and English, two typologically distinct languages. We encode speech at different frame rates and evaluate the resulting semantic tokens in the speech recognition task. Our findings reveal that frame rate variations influence speech tokenization differently for each language, highlighting the interplay between frame rates, phonetic density, and language-specific acoustic features. The results provide insights into optimizing frame rate selection for speech tokenizers, with implications for automatic speech recognition, text-to-speech, and other speech-related applications.",
    "metadata": {
      "arxiv_id": "2505.17076",
      "title": "Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English",
      "summary": "The speech tokenizer plays a crucial role in recent speech tasks, generally serving as a bridge between speech signals and language models. While low-frame-rate codecs are widely employed as speech tokenizers, the impact of frame rates on speech tokens remains underexplored. In this study, we investigate how varying frame rates affect speech tokenization by examining Mandarin and English, two typologically distinct languages. We encode speech at different frame rates and evaluate the resulting semantic tokens in the speech recognition task. Our findings reveal that frame rate variations influence speech tokenization differently for each language, highlighting the interplay between frame rates, phonetic density, and language-specific acoustic features. The results provide insights into optimizing frame rate selection for speech tokenizers, with implications for automatic speech recognition, text-to-speech, and other speech-related applications.",
      "authors": [
        "Haoyang Zhang",
        "Hexin Liu",
        "Xiangyu Zhang",
        "Qiquan Zhang",
        "Yuchen Hu",
        "Junqi Zhao",
        "Fei Tian",
        "Xuerui Yang",
        "Leibny Paola Garcia",
        "Eng Siong Chng"
      ],
      "published": "2025-05-20T06:01:19Z",
      "updated": "2025-06-13T17:21:25Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17076v3",
      "landing_url": "https://arxiv.org/abs/2505.17076v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.17076"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses frame-rate effects on speech tokenization but does not describe a discrete audio token design, token generation process, or quantization/architecture details required by the inclusion criteria, so it should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses frame-rate effects on speech tokenization but does not describe a discrete audio token design, token generation process, or quantization/architecture details required by the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates the impact of frame rates on speech tokenization and evaluates semantic tokens in speech recognition, but it does not clarify that it employs discrete audio token designs such as vector quantization or neural audio codecs producing semantically meaningful discrete tokens, nor does it detail token generation methods or usage for sequence modeling as required by the inclusion criteria; the study focuses on frame rate effects rather than token generation methods, so it lacks sufficient methodological detail and clear discrete token design evidence to meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study investigates the impact of frame rates on speech tokenization and evaluates semantic tokens in speech recognition, but it does not clarify that it employs discrete audio token designs such as vector quantization or neural audio codecs producing semantically meaningful discrete tokens, nor does it detail token generation methods or usage for sequence modeling as required by the inclusion criteria; the study focuses on frame rate effects rather than token generation methods, so it lacks sufficient methodological detail and clear discrete token design evidence to meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models",
    "abstract": "The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed/variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding.",
    "metadata": {
      "arxiv_id": "2505.17446",
      "title": "Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models",
      "summary": "The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed/variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding.",
      "authors": [
        "Shunsuke Kando",
        "Yusuke Miyao",
        "Shinnosuke Takamichi"
      ],
      "published": "2025-05-23T04:03:27Z",
      "updated": "2025-05-31T13:32:13Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17446v2",
      "landing_url": "https://arxiv.org/abs/2505.17446v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.17446"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract clearly describe discrete audio tokenization (segmentation and clustering) for speech language models, meeting the inclusion criterion for 'Discrete Audio Tokens', but the source page lacks explicit publication/update date, triggering the exclusion criterion about time-stamp traceability.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The title/abstract clearly describe discrete audio tokenization (segmentation and clustering) for speech language models, meeting the inclusion criterion for 'Discrete Audio Tokens', but the source page lacks explicit publication/update date, triggering the exclusion criterion about time-stamp traceability.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper investigates the effect of segmentation width and cluster size in speech tokenization, where speech signals are segmented and K-means clustering is applied to produce discrete units used for training speech language models. The process aligns with producing discrete audio tokens via clustering methods and the usage of these tokens as foundational representations for speech language modeling. However, the abstract does not explicitly mention technical details such as vector quantization, residual vector quantization, or semantic disentanglement, nor does it clarify if the tokens retain semantic/acoustic information beyond basic clustering. The study shows experimental analysis relevant to token design and usage in sequence models, but lacks clear details about token generation mechanisms and quantization specifics required for direct comparison and reproducibility. Therefore, it partially meets inclusion criteria but misses some specifics on token generation and intended downstream usage clarification.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper investigates the effect of segmentation width and cluster size in speech tokenization, where speech signals are segmented and K-means clustering is applied to produce discrete units used for training speech language models. The process aligns with producing discrete audio tokens via clustering methods and the usage of these tokens as foundational representations for speech language modeling. However, the abstract does not explicitly mention technical details such as vector quantization, residual vector quantization, or semantic disentanglement, nor does it clarify if the tokens retain semantic/acoustic information beyond basic clustering. The study shows experimental analysis relevant to token design and usage in sequence models, but lacks clear details about token generation mechanisms and quantization specifics required for direct comparison and reproducibility. Therefore, it partially meets inclusion criteria but misses some specifics on token generation and intended downstream usage clarification.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers raised valid points: JuniorNano excluded due to lack of an explicit publication/update date on the source (a formal exclusion criterion), while JuniorMini noted the paper clearly studies discrete token design (segmentation + K-means clustering) for speech language models but lacks low-level quantization/implementation details in the abstract. The title and abstract explicitly treat ‘‘discrete audio tokens’’ as the core contribution—segmentation strategies and clustering to produce discrete units used by SLMs—and report experimental evaluation on downstream tasks, which aligns with the inclusion definition (cluster-based quantization/tokenization for sequence models). However, the abstract does not include exhaustive reproducibility details (e.g., precise quantization/codec architecture, codebook/levels) and we cannot verify the source publication date from the provided item. Balancing these, the work should be considered for inclusion because its primary contribution is discrete token design and empirical evaluation for speech language modeling (meeting the topic and methodological coverage), though the missing metadata and finer implementation details mean inclusion should be cautious and conditional on obtaining the full paper or source page with date and method details.\n\nOne-sentence explanation for score: The study centers on discrete tokenization via segmentation plus clustering for SLMs and reports downstream evaluation (matching inclusion), but lacks detailed quantization specifics and an explicit source date in the provided item, so I recommend inclusion with caution.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers raised valid points: JuniorNano excluded due to lack of an explicit publication/update date on the source (a formal exclusion criterion), while JuniorMini noted the paper clearly studies discrete token design (segmentation + K-means clustering) for speech language models but lacks low-level quantization/implementation details in the abstract. The title and abstract explicitly treat ‘‘discrete audio tokens’’ as the core contribution—segmentation strategies and clustering to produce discrete units used by SLMs—and report experimental evaluation on downstream tasks, which aligns with the inclusion definition (cluster-based quantization/tokenization for sequence models). However, the abstract does not include exhaustive reproducibility details (e.g., precise quantization/codec architecture, codebook/levels) and we cannot verify the source publication date from the provided item. Balancing these, the work should be considered for inclusion because its primary contribution is discrete token design and empirical evaluation for speech language modeling (meeting the topic and methodological coverage), though the missing metadata and finer implementation details mean inclusion should be cautious and conditional on obtaining the full paper or source page with date and method details.\n\nOne-sentence explanation for score: The study centers on discrete tokenization via segmentation plus clustering for SLMs and reports downstream evaluation (matching inclusion), but lacks detailed quantization specifics and an explicit source date in the provided item, so I recommend inclusion with caution.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Adaptive Semantic Token Communication for Transformer-based Edge Inference",
    "abstract": "This paper presents an adaptive framework for edge inference based on a dynamically configurable transformer-powered deep joint source channel coding (DJSCC) architecture. Motivated by a practical scenario where a resource constrained edge device engages in goal oriented semantic communication, such as selectively transmitting essential features for object detection to an edge server, our approach enables efficient task aware data transmission under varying bandwidth and channel conditions. To achieve this, input data is tokenized into compact high level semantic representations, refined by a transformer, and transmitted over noisy wireless channels. As part of the DJSCC pipeline, we employ a semantic token selection mechanism that adaptively compresses informative features into a user specified number of tokens per sample. These tokens are then further compressed through the JSCC module, enabling a flexible token communication strategy that adjusts both the number of transmitted tokens and their embedding dimensions. We incorporate a resource allocation algorithm based on Lyapunov stochastic optimization to enhance robustness under dynamic network conditions, effectively balancing compression efficiency and task performance. Experimental results demonstrate that our system consistently outperforms existing baselines, highlighting its potential as a strong foundation for AI native semantic communication in edge intelligence applications.",
    "metadata": {
      "arxiv_id": "2505.17604",
      "title": "Adaptive Semantic Token Communication for Transformer-based Edge Inference",
      "summary": "This paper presents an adaptive framework for edge inference based on a dynamically configurable transformer-powered deep joint source channel coding (DJSCC) architecture. Motivated by a practical scenario where a resource constrained edge device engages in goal oriented semantic communication, such as selectively transmitting essential features for object detection to an edge server, our approach enables efficient task aware data transmission under varying bandwidth and channel conditions. To achieve this, input data is tokenized into compact high level semantic representations, refined by a transformer, and transmitted over noisy wireless channels. As part of the DJSCC pipeline, we employ a semantic token selection mechanism that adaptively compresses informative features into a user specified number of tokens per sample. These tokens are then further compressed through the JSCC module, enabling a flexible token communication strategy that adjusts both the number of transmitted tokens and their embedding dimensions. We incorporate a resource allocation algorithm based on Lyapunov stochastic optimization to enhance robustness under dynamic network conditions, effectively balancing compression efficiency and task performance. Experimental results demonstrate that our system consistently outperforms existing baselines, highlighting its potential as a strong foundation for AI native semantic communication in edge intelligence applications.",
      "authors": [
        "Alessio Devoto",
        "Jary Pomponi",
        "Mattia Merluzzi",
        "Paolo Di Lorenzo",
        "Simone Scardapane"
      ],
      "published": "2025-05-23T08:15:05Z",
      "updated": "2025-05-23T08:15:05Z",
      "categories": [
        "cs.LG",
        "cs.ET"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17604v1",
      "landing_url": "https://arxiv.org/abs/2505.17604v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.17604"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discusses adaptive semantic token communication for edge inference, but does not present discrete audio token generation/quantization (e.g., VQ/RVQ) or audio-language modeling as its main contribution; it targets general semantic features for transmission rather than discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper discusses adaptive semantic token communication for edge inference, but does not present discrete audio token generation/quantization (e.g., VQ/RVQ) or audio-language modeling as its main contribution; it targets general semantic features for transmission rather than discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on adaptive semantic token communication in a transformer-based edge inference context, emphasizing semantic representations and communication efficiency. However, it does not discuss discrete audio tokens derived from continuous audio signals (speech, music, general audio) nor describe tokenization specifically based on audio quantization, clustering, or semantic disentanglement for audio language modeling or cross-modal tasks. The tokens appear related to visual/object detection features rather than audio signals. Therefore, this work does not meet the inclusion criteria centered on discrete audio tokens, nor does it appear to fall into any exclusion categories based on lack of detail or date. Overall, it is unrelated to the specified topic of discrete audio tokens and their role in audio sequence modeling or understanding.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on adaptive semantic token communication in a transformer-based edge inference context, emphasizing semantic representations and communication efficiency. However, it does not discuss discrete audio tokens derived from continuous audio signals (speech, music, general audio) nor describe tokenization specifically based on audio quantization, clustering, or semantic disentanglement for audio language modeling or cross-modal tasks. The tokens appear related to visual/object detection features rather than audio signals. Therefore, this work does not meet the inclusion criteria centered on discrete audio tokens, nor does it appear to fall into any exclusion categories based on lack of detail or date. Overall, it is unrelated to the specified topic of discrete audio tokens and their role in audio sequence modeling or understanding.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Offline Clustering of Linear Bandits: The Power of Clusters under Limited Data",
    "abstract": "Contextual multi-armed bandit is a fundamental learning framework for making a sequence of decisions, e.g., advertising recommendations for a sequence of arriving users. Recent works have shown that clustering these users based on the similarity of their learned preferences can accelerate the learning. However, prior work has primarily focused on the online setting, which requires continually collecting user data, ignoring the offline data widely available in many applications. To tackle these limitations, we study the offline clustering of bandits (Off-ClusBand) problem, which studies how to use the offline dataset to learn cluster properties and improve decision-making. The key challenge in Off-ClusBand arises from data insufficiency for users: unlike the online case where we continually learn from online data, in the offline case, we have a fixed, limited dataset to work from and thus must determine whether we have enough data to confidently cluster users together. To address this challenge, we propose two algorithms: Off-C2LUB, which we show analytically and experimentally outperforms existing methods under limited offline user data, and Off-CLUB, which may incur bias when data is sparse but performs well and nearly matches the lower bound when data is sufficient. We experimentally validate these results on both real and synthetic datasets.",
    "metadata": {
      "arxiv_id": "2505.19043",
      "title": "Offline Clustering of Linear Bandits: The Power of Clusters under Limited Data",
      "summary": "Contextual multi-armed bandit is a fundamental learning framework for making a sequence of decisions, e.g., advertising recommendations for a sequence of arriving users. Recent works have shown that clustering these users based on the similarity of their learned preferences can accelerate the learning. However, prior work has primarily focused on the online setting, which requires continually collecting user data, ignoring the offline data widely available in many applications. To tackle these limitations, we study the offline clustering of bandits (Off-ClusBand) problem, which studies how to use the offline dataset to learn cluster properties and improve decision-making. The key challenge in Off-ClusBand arises from data insufficiency for users: unlike the online case where we continually learn from online data, in the offline case, we have a fixed, limited dataset to work from and thus must determine whether we have enough data to confidently cluster users together. To address this challenge, we propose two algorithms: Off-C2LUB, which we show analytically and experimentally outperforms existing methods under limited offline user data, and Off-CLUB, which may incur bias when data is sparse but performs well and nearly matches the lower bound when data is sufficient. We experimentally validate these results on both real and synthetic datasets.",
      "authors": [
        "Jingyuan Liu",
        "Zeyu Zhang",
        "Xuchuang Wang",
        "Xutong Liu",
        "John C. S. Lui",
        "Mohammad Hajiesmaili",
        "Carlee Joe-Wong"
      ],
      "published": "2025-05-25T08:43:40Z",
      "updated": "2025-10-25T08:29:46Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19043v2",
      "landing_url": "https://arxiv.org/abs/2505.19043v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19043"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on offline clustering for contextual bandits with offline data (Off-ClusBand) and does not involve discrete audio token representations, token design, or quantization methods required by the 'Discrete Audio Tokens' inclusion criteria; therefore it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on offline clustering for contextual bandits with offline data (Off-ClusBand) and does not involve discrete audio token representations, token design, or quantization methods required by the 'Discrete Audio Tokens' inclusion criteria; therefore it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study addresses offline clustering of linear bandits under limited data and focuses on algorithms for decision-making improvement based on user clustering; it does not discuss discrete audio tokens, audio representation, vector quantization, or related audio language modeling techniques as specified in the inclusion criteria. Therefore, it does not meet the thematic requirements for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study addresses offline clustering of linear bandits under limited data and focuses on algorithms for decision-making improvement based on user clustering; it does not discuss discrete audio tokens, audio representation, vector quantization, or related audio language modeling techniques as specified in the inclusion criteria. Therefore, it does not meet the thematic requirements for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Navigating PESQ: Up-to-Date Versions and Open Implementations",
    "abstract": "Perceptual Evaluation of Speech Quality (PESQ) is an objective quality measure that remains widely used despite its withdrawal by the International Telecommunication Union (ITU). PESQ has evolved over two decades, with multiple versions and publicly available implementations emerging during this time. Different versions and their updates can be overwhelming, especially for new PESQ users. This work provides practical guidance on the different versions and implementations of PESQ. We show that differences can be significant, especially between PESQ versions. We stress the importance of specifying the exact version and implementation that is used to compute PESQ, and possibly to detail how multi-channel signals are handled. These practices would facilitate the interpretation of results and allow comparisons of PESQ scores between different studies. We also provide a repository that implements the latest corrections to PESQ, i.e., Corrigendum 2, which is not implemented by any other openly available distribution: https://github.com/audiolabs/PESQ.",
    "metadata": {
      "arxiv_id": "2505.19760",
      "title": "Navigating PESQ: Up-to-Date Versions and Open Implementations",
      "summary": "Perceptual Evaluation of Speech Quality (PESQ) is an objective quality measure that remains widely used despite its withdrawal by the International Telecommunication Union (ITU). PESQ has evolved over two decades, with multiple versions and publicly available implementations emerging during this time. Different versions and their updates can be overwhelming, especially for new PESQ users. This work provides practical guidance on the different versions and implementations of PESQ. We show that differences can be significant, especially between PESQ versions. We stress the importance of specifying the exact version and implementation that is used to compute PESQ, and possibly to detail how multi-channel signals are handled. These practices would facilitate the interpretation of results and allow comparisons of PESQ scores between different studies. We also provide a repository that implements the latest corrections to PESQ, i.e., Corrigendum 2, which is not implemented by any other openly available distribution: https://github.com/audiolabs/PESQ.",
      "authors": [
        "Matteo Torcoli",
        "Mhd Modar Halimeh",
        "Emanuël A. P. Habets"
      ],
      "published": "2025-05-26T09:43:09Z",
      "updated": "2025-08-14T11:53:22Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19760v2",
      "landing_url": "https://arxiv.org/abs/2505.19760v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19760"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item reviews PESQ versions and implementations, not discrete audio tokens or token-based representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item reviews PESQ versions and implementations, not discrete audio tokens or token-based representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on the Perceptual Evaluation of Speech Quality (PESQ), which is an objective quality measure for speech, and discusses different versions and implementations of PESQ. It does not address the design or generation of discrete audio tokens, nor does it involve vector quantization or discrete representations used for sequence modeling or audio language modeling. Therefore, it does not meet the inclusion criteria related to discrete audio token research, nor does it introduce any relevant quantization or token generation methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on the Perceptual Evaluation of Speech Quality (PESQ), which is an objective quality measure for speech, and discusses different versions and implementations of PESQ. It does not address the design or generation of discrete audio tokens, nor does it involve vector quantization or discrete representations used for sequence modeling or audio language modeling. Therefore, it does not meet the inclusion criteria related to discrete audio token research, nor does it introduce any relevant quantization or token generation methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Vectorized Sequence-Based Chunking for Data Deduplication",
    "abstract": "Data deduplication has gained wide acclaim as a mechanism to improve storage efficiency and conserve network bandwidth. Its most critical phase, data chunking, is responsible for the overall space savings achieved via the deduplication process. However, modern data chunking algorithms are slow and compute-intensive because they scan large amounts of data while simultaneously making data-driven boundary decisions. We present SeqCDC, a novel chunking algorithm that leverages lightweight boundary detection, content-defined skipping, and SSE/AVX acceleration to improve chunking throughput for large chunk sizes. Our evaluation shows that SeqCDC achieves 15x higher throughput than unaccelerated and 1.2x-1.35x higher throughput than vector-accelerated data chunking algorithms while minimally affecting deduplication space savings.",
    "metadata": {
      "arxiv_id": "2505.21194",
      "title": "Vectorized Sequence-Based Chunking for Data Deduplication",
      "summary": "Data deduplication has gained wide acclaim as a mechanism to improve storage efficiency and conserve network bandwidth. Its most critical phase, data chunking, is responsible for the overall space savings achieved via the deduplication process. However, modern data chunking algorithms are slow and compute-intensive because they scan large amounts of data while simultaneously making data-driven boundary decisions.\n  We present SeqCDC, a novel chunking algorithm that leverages lightweight boundary detection, content-defined skipping, and SSE/AVX acceleration to improve chunking throughput for large chunk sizes. Our evaluation shows that SeqCDC achieves 15x higher throughput than unaccelerated and 1.2x-1.35x higher throughput than vector-accelerated data chunking algorithms while minimally affecting deduplication space savings.",
      "authors": [
        "Sreeharsha Udayashankar",
        "Samer Al-Kiswany"
      ],
      "published": "2025-05-27T13:42:33Z",
      "updated": "2025-05-27T13:42:33Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.21194v1",
      "landing_url": "https://arxiv.org/abs/2505.21194v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.21194"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item targets data deduplication chunking, not discrete audio token design or audio representations; therefore it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item targets data deduplication chunking, not discrete audio token design or audio representations; therefore it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on data deduplication and chunking algorithms related to storage optimization, and does not address discrete audio tokens, their generation, or usage in sequence/language models as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on data deduplication and chunking algorithms related to storage optimization, and does not address discrete audio tokens, their generation, or usage in sequence/language models as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Towards Robust Overlapping Speech Detection: A Speaker-Aware Progressive Approach Using WavLM",
    "abstract": "Overlapping Speech Detection (OSD) aims to identify regions where multiple speakers overlap in a conversation, a critical challenge in multi-party speech processing. This work proposes a speaker-aware progressive OSD model that leverages a progressive training strategy to enhance the correlation between subtasks such as voice activity detection (VAD) and overlap detection. To improve acoustic representation, we explore the effectiveness of state-of-the-art self-supervised learning (SSL) models, including WavLM and wav2vec 2.0, while incorporating a speaker attention module to enrich features with frame-level speaker information. Experimental results show that the proposed method achieves state-of-the-art performance, with an F1 score of 82.76\\% on the AMI test set, demonstrating its robustness and effectiveness in OSD.",
    "metadata": {
      "arxiv_id": "2505.23207",
      "title": "Towards Robust Overlapping Speech Detection: A Speaker-Aware Progressive Approach Using WavLM",
      "summary": "Overlapping Speech Detection (OSD) aims to identify regions where multiple speakers overlap in a conversation, a critical challenge in multi-party speech processing. This work proposes a speaker-aware progressive OSD model that leverages a progressive training strategy to enhance the correlation between subtasks such as voice activity detection (VAD) and overlap detection. To improve acoustic representation, we explore the effectiveness of state-of-the-art self-supervised learning (SSL) models, including WavLM and wav2vec 2.0, while incorporating a speaker attention module to enrich features with frame-level speaker information. Experimental results show that the proposed method achieves state-of-the-art performance, with an F1 score of 82.76\\% on the AMI test set, demonstrating its robustness and effectiveness in OSD.",
      "authors": [
        "Zhaokai Sun",
        "Li Zhang",
        "Qing Wang",
        "Pan Zhou",
        "Lei Xie"
      ],
      "published": "2025-05-29T07:47:48Z",
      "updated": "2025-05-29T07:47:48Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.23207v1",
      "landing_url": "https://arxiv.org/abs/2505.23207v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.23207"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item targets overlapping speech detection using SSL representations; it does not introduce or evaluate discrete audio tokens or tokenization workflows, thus fails the specified inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item targets overlapping speech detection using SSL representations; it does not introduce or evaluate discrete audio tokens or tokenization workflows, thus fails the specified inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on Overlapping Speech Detection using a speaker-aware model leveraging self-supervised learning representations like WavLM, but it does not describe any discrete audio token generation, vector quantization, or transformation of continuous audio into discrete tokens used as core research contributions or sequence model inputs, which is the main inclusion criterion. Hence, it does not meet the core topic requirement of discrete audio tokens as defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on Overlapping Speech Detection using a speaker-aware model leveraging self-supervised learning representations like WavLM, but it does not describe any discrete audio token generation, vector quantization, or transformation of continuous audio into discrete tokens used as core research contributions or sequence model inputs, which is the main inclusion criterion. Hence, it does not meet the core topic requirement of discrete audio tokens as defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speech Token Prediction via Compressed-to-fine Language Modeling for Speech Generation",
    "abstract": "Neural audio codecs, used as speech tokenizers, have demonstrated remarkable potential in the field of speech generation. However, to ensure high-fidelity audio reconstruction, neural audio codecs typically encode audio into long sequences of speech tokens, posing a significant challenge for downstream language models in long-context modeling. We observe that speech token sequences exhibit short-range dependency: due to the monotonic alignment between text and speech in text-to-speech (TTS) tasks, the prediction of the current token primarily relies on its local context, while long-range tokens contribute less to the current token prediction and often contain redundant information. Inspired by this observation, we propose a \\textbf{compressed-to-fine language modeling} approach to address the challenge of long sequence speech tokens within neural codec language models: (1) \\textbf{Fine-grained Initial and Short-range Information}: Our approach retains the prompt and local tokens during prediction to ensure text alignment and the integrity of paralinguistic information; (2) \\textbf{Compressed Long-range Context}: Our approach compresses long-range token spans into compact representations to reduce redundant information while preserving essential semantics. Extensive experiments on various neural audio codecs and downstream language models validate the effectiveness and generalizability of the proposed approach, highlighting the importance of token compression in improving speech generation within neural codec language models. The demo of audio samples will be available at https://anonymous.4open.science/r/SpeechTokenPredictionViaCompressedToFinedLM.",
    "metadata": {
      "arxiv_id": "2505.24496",
      "title": "Speech Token Prediction via Compressed-to-fine Language Modeling for Speech Generation",
      "summary": "Neural audio codecs, used as speech tokenizers, have demonstrated remarkable potential in the field of speech generation. However, to ensure high-fidelity audio reconstruction, neural audio codecs typically encode audio into long sequences of speech tokens, posing a significant challenge for downstream language models in long-context modeling. We observe that speech token sequences exhibit short-range dependency: due to the monotonic alignment between text and speech in text-to-speech (TTS) tasks, the prediction of the current token primarily relies on its local context, while long-range tokens contribute less to the current token prediction and often contain redundant information. Inspired by this observation, we propose a \\textbf{compressed-to-fine language modeling} approach to address the challenge of long sequence speech tokens within neural codec language models: (1) \\textbf{Fine-grained Initial and Short-range Information}: Our approach retains the prompt and local tokens during prediction to ensure text alignment and the integrity of paralinguistic information; (2) \\textbf{Compressed Long-range Context}: Our approach compresses long-range token spans into compact representations to reduce redundant information while preserving essential semantics. Extensive experiments on various neural audio codecs and downstream language models validate the effectiveness and generalizability of the proposed approach, highlighting the importance of token compression in improving speech generation within neural codec language models. The demo of audio samples will be available at https://anonymous.4open.science/r/SpeechTokenPredictionViaCompressedToFinedLM.",
      "authors": [
        "Wenrui Liu",
        "Qian Chen",
        "Wen Wang",
        "Yafeng Chen",
        "Jin Xu",
        "Zhifang Guo",
        "Guanrou Yang",
        "Weiqin Li",
        "Xiaoda Yang",
        "Tao Jin",
        "Minghui Fang",
        "Jialong Zuo",
        "Bai Jionghao",
        "Zemin Liu"
      ],
      "published": "2025-05-30T11:47:29Z",
      "updated": "2025-05-30T11:47:29Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.24496v1",
      "landing_url": "https://arxiv.org/abs/2505.24496v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.24496"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Abstract discusses discrete audio tokens via neural codecs and long-range token compression for LM-based speech generation, with concrete steps and experimental validation, aligning with the discrete-audio-token research theme; it provides methodological content rather than a pure demo, supporting inclusion.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "Abstract discusses discrete audio tokens via neural codecs and long-range token compression for LM-based speech generation, with concrete steps and experimental validation, aligning with the discrete-audio-token research theme; it provides methodological content rather than a pure demo, supporting inclusion.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses neural audio codecs that encode audio into sequences of speech tokens, focusing on compressed-to-fine language modeling to handle long sequence dependencies for speech generation. It clearly involves discrete audio tokens generated by neural codecs and applies sequence modeling for speech token prediction, aligning well with the inclusion criteria regarding discrete token design and usage in sequence modeling for generation. The abstract hints at experimental validation and discusses token compression techniques but does not provide explicit token generation details or quantization specifics in the abstract itself (which might be included in the full paper). No exclusion criteria such as lack of method detail or demo-only content apply here. Therefore, it fits well within the scope though some technical details are only implied, so it is confidently included but not with absolute certainty based on the abstract alone.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The paper discusses neural audio codecs that encode audio into sequences of speech tokens, focusing on compressed-to-fine language modeling to handle long sequence dependencies for speech generation. It clearly involves discrete audio tokens generated by neural codecs and applies sequence modeling for speech token prediction, aligning well with the inclusion criteria regarding discrete token design and usage in sequence modeling for generation. The abstract hints at experimental validation and discusses token compression techniques but does not provide explicit token generation details or quantization specifics in the abstract itself (which might be included in the full paper). No exclusion criteria such as lack of method detail or demo-only content apply here. Therefore, it fits well within the scope though some technical details are only implied, so it is confidently included but not with absolute certainty based on the abstract alone.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "HASRD: Hierarchical Acoustic and Semantic Representation Disentanglement",
    "abstract": "Effective speech representations for spoken language models must balance semantic relevance with acoustic fidelity for high-quality reconstruction. However, existing approaches struggle to achieve both simultaneously. To address this, we introduce Hierarchical Acoustic and Semantic Representation Disentanglement (HASRD, pronounced `hazard'), a framework that factorizes self-supervised learning representations into discrete semantic and acoustic tokens. HASRD assigns the semantic representation to the first codebook, while encoding acoustic residuals in subsequent codebooks. This preserves ASR performance while achieving high-quality reconstruction. Additionally, we enhance HASRD's encoder efficiency, improving ASR performance without compromising reconstruction quality. Compared to SpeechTokenizer, HASRD achieves a 44% relative WER improvement, superior reconstruction quality, and 2x lower bitrate, demonstrating its effectiveness in disentangling acoustic and semantic information.",
    "metadata": {
      "arxiv_id": "2506.00843",
      "title": "HASRD: Hierarchical Acoustic and Semantic Representation Disentanglement",
      "summary": "Effective speech representations for spoken language models must balance semantic relevance with acoustic fidelity for high-quality reconstruction. However, existing approaches struggle to achieve both simultaneously. To address this, we introduce Hierarchical Acoustic and Semantic Representation Disentanglement (HASRD, pronounced `hazard'), a framework that factorizes self-supervised learning representations into discrete semantic and acoustic tokens. HASRD assigns the semantic representation to the first codebook, while encoding acoustic residuals in subsequent codebooks. This preserves ASR performance while achieving high-quality reconstruction. Additionally, we enhance HASRD's encoder efficiency, improving ASR performance without compromising reconstruction quality. Compared to SpeechTokenizer, HASRD achieves a 44% relative WER improvement, superior reconstruction quality, and 2x lower bitrate, demonstrating its effectiveness in disentangling acoustic and semantic information.",
      "authors": [
        "Amir Hussein",
        "Sameer Khurana",
        "Gordon Wichern",
        "Francois G. Germain",
        "Jonathan Le Roux"
      ],
      "published": "2025-06-01T05:38:39Z",
      "updated": "2025-06-01T05:38:39Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.00843v1",
      "landing_url": "https://arxiv.org/abs/2506.00843v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.00843"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "HASRD explicitly constructs discrete semantic and acoustic tokens using a hierarchical codebook design (semantic in the first codebook, acoustic residuals in later codebooks), addressing the topic of discrete audio tokens and tokenization for language-model-style processing, with no clear exclusion signals present in the abstract.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "HASRD explicitly constructs discrete semantic and acoustic tokens using a hierarchical codebook design (semantic in the first codebook, acoustic residuals in later codebooks), addressing the topic of discrete audio tokens and tokenization for language-model-style processing, with no clear exclusion signals present in the abstract.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The article proposes HASRD, a representation framework that generates discrete semantic and acoustic tokens from speech, using hierarchical quantization and disentanglement which aligns with the inclusion criteria of discrete audio tokens involving vector quantization or layered quantization; it clearly describes token generation process and use for speech understanding tasks such as ASR improvement, demonstrating technical depth and comparative results, thus meeting the key inclusion criteria without falling into exclusion issues like lack of method details or only traditional codecs.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The article proposes HASRD, a representation framework that generates discrete semantic and acoustic tokens from speech, using hierarchical quantization and disentanglement which aligns with the inclusion criteria of discrete audio tokens involving vector quantization or layered quantization; it clearly describes token generation process and use for speech understanding tasks such as ASR improvement, demonstrating technical depth and comparative results, thus meeting the key inclusion criteria without falling into exclusion issues like lack of method details or only traditional codecs.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Contrastive Flow Matching",
    "abstract": "Unconditional flow-matching trains diffusion models to transport samples from a source distribution to a target distribution by enforcing that the flows between sample pairs are unique. However, in conditional settings (e.g., class-conditioned models), this uniqueness is no longer guaranteed--flows from different conditions may overlap, leading to more ambiguous generations. We introduce Contrastive Flow Matching, an extension to the flow matching objective that explicitly enforces uniqueness across all conditional flows, enhancing condition separation. Our approach adds a contrastive objective that maximizes dissimilarities between predicted flows from arbitrary sample pairs. We validate Contrastive Flow Matching by conducting extensive experiments across varying model architectures on both class-conditioned (ImageNet-1k) and text-to-image (CC3M) benchmarks. Notably, we find that training models with Contrastive Flow Matching (1) improves training speed by a factor of up to 9x, (2) requires up to 5x fewer de-noising steps and (3) lowers FID by up to 8.9 compared to training the same models with flow matching. We release our code at: https://github.com/gstoica27/DeltaFM.git.",
    "metadata": {
      "arxiv_id": "2506.05350",
      "title": "Contrastive Flow Matching",
      "summary": "Unconditional flow-matching trains diffusion models to transport samples from a source distribution to a target distribution by enforcing that the flows between sample pairs are unique. However, in conditional settings (e.g., class-conditioned models), this uniqueness is no longer guaranteed--flows from different conditions may overlap, leading to more ambiguous generations. We introduce Contrastive Flow Matching, an extension to the flow matching objective that explicitly enforces uniqueness across all conditional flows, enhancing condition separation. Our approach adds a contrastive objective that maximizes dissimilarities between predicted flows from arbitrary sample pairs. We validate Contrastive Flow Matching by conducting extensive experiments across varying model architectures on both class-conditioned (ImageNet-1k) and text-to-image (CC3M) benchmarks. Notably, we find that training models with Contrastive Flow Matching (1) improves training speed by a factor of up to 9x, (2) requires up to 5x fewer de-noising steps and (3) lowers FID by up to 8.9 compared to training the same models with flow matching. We release our code at: https://github.com/gstoica27/DeltaFM.git.",
      "authors": [
        "George Stoica",
        "Vivek Ramanujan",
        "Xiang Fan",
        "Ali Farhadi",
        "Ranjay Krishna",
        "Judy Hoffman"
      ],
      "published": "2025-06-05T17:59:58Z",
      "updated": "2025-06-05T17:59:58Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.05350v1",
      "landing_url": "https://arxiv.org/abs/2506.05350v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.05350"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses diffusion models and a contrastive flow matching objective for image/text-to-image tasks, not discrete audio tokens or their tokenization/quantization designs, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses diffusion models and a contrastive flow matching objective for image/text-to-image tasks, not discrete audio tokens or their tokenization/quantization designs, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving flow matching in diffusion models for image and text-to-image generation tasks, lacking any discussion or methodology related to discrete audio tokens, audio signal discretization, or token generation processes in audio contexts as required in the inclusion criteria. Therefore, it clearly does not meet the thematic requirements, nor does it address any of the inclusion points, while no exclusion criteria such as demo or blog posts apply here.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving flow matching in diffusion models for image and text-to-image generation tasks, lacking any discussion or methodology related to discrete audio tokens, audio signal discretization, or token generation processes in audio contexts as required in the inclusion criteria. Therefore, it clearly does not meet the thematic requirements, nor does it address any of the inclusion points, while no exclusion criteria such as demo or blog posts apply here.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Exponential Family Variational Flow Matching for Tabular Data Generation",
    "abstract": "While denoising diffusion and flow matching have driven major advances in generative modeling, their application to tabular data remains limited, despite its ubiquity in real-world applications. To this end, we develop TabbyFlow, a variational Flow Matching (VFM) method for tabular data generation. To apply VFM to data with mixed continuous and discrete features, we introduce Exponential Family Variational Flow Matching (EF-VFM), which represents heterogeneous data types using a general exponential family distribution. We hereby obtain an efficient, data-driven objective based on moment matching, enabling principled learning of probability paths over mixed continuous and discrete variables. We also establish a connection between variational flow matching and generalized flow matching objectives based on Bregman divergences. Evaluation on tabular data benchmarks demonstrates state-of-the-art performance compared to baselines.",
    "metadata": {
      "arxiv_id": "2506.05940",
      "title": "Exponential Family Variational Flow Matching for Tabular Data Generation",
      "summary": "While denoising diffusion and flow matching have driven major advances in generative modeling, their application to tabular data remains limited, despite its ubiquity in real-world applications. To this end, we develop TabbyFlow, a variational Flow Matching (VFM) method for tabular data generation. To apply VFM to data with mixed continuous and discrete features, we introduce Exponential Family Variational Flow Matching (EF-VFM), which represents heterogeneous data types using a general exponential family distribution. We hereby obtain an efficient, data-driven objective based on moment matching, enabling principled learning of probability paths over mixed continuous and discrete variables. We also establish a connection between variational flow matching and generalized flow matching objectives based on Bregman divergences. Evaluation on tabular data benchmarks demonstrates state-of-the-art performance compared to baselines.",
      "authors": [
        "Andrés Guzmán-Cordero",
        "Floor Eijkelboom",
        "Jan-Willem van de Meent"
      ],
      "published": "2025-06-06T10:07:48Z",
      "updated": "2025-10-03T17:32:46Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.05940v4",
      "landing_url": "https://arxiv.org/abs/2506.05940v4",
      "doi": "https://doi.org/10.48550/arXiv.2506.05940"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item examines tabular data generation via EF-VFM, not discrete audio token design or token-based audio-language modeling, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item examines tabular data generation via EF-VFM, not discrete audio token design or token-based audio-language modeling, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on generative modeling for tabular data using variational flow matching with exponential family distributions; it does not relate to discrete audio tokens or audio signal processing, nor does it describe token generation from audio signals for language or sequence models. Therefore, it does not meet the inclusion criteria related to discrete audio token representation nor provides any relevant method for audio tokenization or modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on generative modeling for tabular data using variational flow matching with exponential family distributions; it does not relate to discrete audio tokens or audio signal processing, nor does it describe token generation from audio signals for language or sequence models. Therefore, it does not meet the inclusion criteria related to discrete audio token representation nor provides any relevant method for audio tokenization or modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Streaming Endpointer for Spoken Dialogue using Neural Audio Codecs and Label-Delayed Training",
    "abstract": "Accurate, low-latency endpointing is crucial for effective spoken dialogue systems. While traditional endpointers often rely on spectrum-based audio features, this work proposes real-time speech endpointing for multi-turn dialogues using streaming, low-bitrate Neural Audio Codec (NAC) features, building upon recent advancements in neural audio codecs. To further reduce cutoff errors, we introduce a novel label delay training scheme. At a fixed median latency of 160 ms, our combined NAC and label delay approach achieves significant relative cutoff error reductions: 42.7% for a single-stream endpointer and 37.5% for a two-stream configuration, compared to baseline methods. Finally, we demonstrate efficient integration with a codec-based pretrained speech large language model, improving its median response time by 1200 ms and reducing its cutoff error by 35%.",
    "metadata": {
      "arxiv_id": "2506.07081",
      "title": "Streaming Endpointer for Spoken Dialogue using Neural Audio Codecs and Label-Delayed Training",
      "summary": "Accurate, low-latency endpointing is crucial for effective spoken dialogue systems. While traditional endpointers often rely on spectrum-based audio features, this work proposes real-time speech endpointing for multi-turn dialogues using streaming, low-bitrate Neural Audio Codec (NAC) features, building upon recent advancements in neural audio codecs. To further reduce cutoff errors, we introduce a novel label delay training scheme. At a fixed median latency of 160 ms, our combined NAC and label delay approach achieves significant relative cutoff error reductions: 42.7% for a single-stream endpointer and 37.5% for a two-stream configuration, compared to baseline methods. Finally, we demonstrate efficient integration with a codec-based pretrained speech large language model, improving its median response time by 1200 ms and reducing its cutoff error by 35%.",
      "authors": [
        "Sathvik Udupa",
        "Shinji Watanabe",
        "Petr Schwarz",
        "Jan Cernocky"
      ],
      "published": "2025-06-08T10:54:23Z",
      "updated": "2025-06-19T09:40:25Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.07081v2",
      "landing_url": "https://arxiv.org/abs/2506.07081v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.07081"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on streaming endpointing for dialogue using neural audio codecs and label-delayed training, not on generating or evaluating discrete audio tokens/tokenization as the core contribution, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on streaming endpointing for dialogue using neural audio codecs and label-delayed training, not on generating or evaluating discrete audio tokens/tokenization as the core contribution, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The work discusses using Neural Audio Codec (NAC) features for speech endpointing but does not clearly describe the generation of discrete audio tokens or provide details on vector quantization or token-based representations as required by the inclusion criteria. The focus is on endpoint detection rather than on discrete token design or usage for sequence modeling. Thus, it does not meet the primary inclusion requirement of focusing on discrete audio token design and its application for language or sequence modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The work discusses using Neural Audio Codec (NAC) features for speech endpointing but does not clearly describe the generation of discrete audio tokens or provide details on vector quantization or token-based representations as required by the inclusion criteria. The focus is on endpoint detection rather than on discrete token design or usage for sequence modeling. Thus, it does not meet the primary inclusion requirement of focusing on discrete audio token design and its application for language or sequence modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations",
    "abstract": "Recent studies on end-to-end (E2E) speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing E2E approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents DrVoice, a parallel speech-text voice conversation model based on joint autoregressive modeling, featuring dual-resolution speech representations. Notably, while current methods utilize mainly 12.5Hz input audio representation, our proposed dual-resolution mechanism reduces the input frequency for the LLM to 5Hz, significantly reducing computational cost and alleviating the frequency discrepancy between speech and text tokens and in turn better exploiting LLMs' capabilities. Experimental results demonstrate that DrVoice-7B establishes new state-of-the-art (SOTA) on prominent speech benchmarks including OpenAudioBench, VoiceBench, UltraEval-Audio and Big Bench Audio, making it a leading open-source speech foundation model in ~7B models.",
    "metadata": {
      "arxiv_id": "2506.09349",
      "title": "DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations",
      "summary": "Recent studies on end-to-end (E2E) speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing E2E approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents DrVoice, a parallel speech-text voice conversation model based on joint autoregressive modeling, featuring dual-resolution speech representations. Notably, while current methods utilize mainly 12.5Hz input audio representation, our proposed dual-resolution mechanism reduces the input frequency for the LLM to 5Hz, significantly reducing computational cost and alleviating the frequency discrepancy between speech and text tokens and in turn better exploiting LLMs' capabilities. Experimental results demonstrate that DrVoice-7B establishes new state-of-the-art (SOTA) on prominent speech benchmarks including OpenAudioBench, VoiceBench, UltraEval-Audio and Big Bench Audio, making it a leading open-source speech foundation model in ~7B models.",
      "authors": [
        "Chao-Hong Tan",
        "Qian Chen",
        "Wen Wang",
        "Chong Deng",
        "Qinglin Zhang",
        "Luyao Cheng",
        "Hai Yu",
        "Xin Zhang",
        "Xiang Lv",
        "Tianyu Zhao",
        "Chong Zhang",
        "Yukun Ma",
        "Yafeng Chen",
        "Hui Wang",
        "Jiaqing Liu",
        "Xiangang Li",
        "Jieping Ye"
      ],
      "published": "2025-06-11T02:57:22Z",
      "updated": "2025-12-23T08:50:59Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.09349v4",
      "landing_url": "https://arxiv.org/abs/2506.09349v4",
      "doi": "https://doi.org/10.48550/arXiv.2506.09349"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on parallel speech-text generation with dual-resolution representations and computational efficiency, but it does not present a discrete audio token design/quantization mechanism as its core contribution, so it likely does not meet the discrete audio token inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The article focuses on parallel speech-text generation with dual-resolution representations and computational efficiency, but it does not present a discrete audio token design/quantization mechanism as its core contribution, so it likely does not meet the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents DrVoice, a speech-text voice conversation model using dual-resolution speech representations and joint autoregressive modeling of speech and text tokens. It involves generating discrete speech tokens integrated into a language model, with experimental validation and novel token rate design, which aligns with the inclusion criteria on discrete audio token design and their use in sequence modeling for speech generation. Method details such as token production process and experimental results are described, supporting reproducibility and comparison. There is no indication that it is a demo or only a standard codec or lacks publication date. Hence, it meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper presents DrVoice, a speech-text voice conversation model using dual-resolution speech representations and joint autoregressive modeling of speech and text tokens. It involves generating discrete speech tokens integrated into a language model, with experimental validation and novel token rate design, which aligns with the inclusion criteria on discrete audio token design and their use in sequence modeling for speech generation. Method details such as token production process and experimental results are described, supporting reproducibility and comparison. There is no indication that it is a demo or only a standard codec or lacks publication date. Hence, it meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the abstract and the two junior reviews. The abstract explicitly states generation of discrete speech tokens integrated via joint autoregressive modeling and proposes a novel dual-resolution speech representation that reduces the token rate to 5Hz, and it reports SOTA experimental results on multiple benchmarks—these indicate that discrete audio token representation and token-rate design are core contributions rather than a mere demo or standard codec; however, the abstract does not detail the exact quantization/codec mechanism (e.g., VQ/RVQ/codebook/layering), so there is some uncertainty until the full paper confirms token-generation specifics. JuniorMini recommended inclusion (5), citing that the paper presents discrete speech tokens, novel token-rate design, and experimental validation; JuniorNano recommended exclusion (2), noting the abstract lacks explicit discrete token quantization details. Given the clear focus on discrete speech tokens and their use in sequence modeling plus published benchmarks, I lean to include but with moderate confidence pending full-method verification. Score justification (one sentence): include because the work centers on discrete speech tokens and a novel dual-resolution token-rate/representation integrated into LLMs and validated experimentally, although the full paper should be checked for detailed token-generation/quantization methods.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "I reviewed the abstract and the two junior reviews. The abstract explicitly states generation of discrete speech tokens integrated via joint autoregressive modeling and proposes a novel dual-resolution speech representation that reduces the token rate to 5Hz, and it reports SOTA experimental results on multiple benchmarks—these indicate that discrete audio token representation and token-rate design are core contributions rather than a mere demo or standard codec; however, the abstract does not detail the exact quantization/codec mechanism (e.g., VQ/RVQ/codebook/layering), so there is some uncertainty until the full paper confirms token-generation specifics. JuniorMini recommended inclusion (5), citing that the paper presents discrete speech tokens, novel token-rate design, and experimental validation; JuniorNano recommended exclusion (2), noting the abstract lacks explicit discrete token quantization details. Given the clear focus on discrete speech tokens and their use in sequence modeling plus published benchmarks, I lean to include but with moderate confidence pending full-method verification. Score justification (one sentence): include because the work centers on discrete speech tokens and a novel dual-resolution token-rate/representation integrated into LLMs and validated experimentally, although the full paper should be checked for detailed token-generation/quantization methods.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Study on Speech Assessment with Visual Cues",
    "abstract": "Non-intrusive assessment of speech quality and intelligibility is essential when clean reference signals are unavailable. In this work, we propose a multimodal framework that integrates audio features and visual cues to predict PESQ and STOI scores. It employs a dual-branch architecture, where spectral features are extracted using STFT, and visual embeddings are obtained via a visual encoder. These features are then fused and processed by a CNN-BLSTM with attention, followed by multi-task learning to simultaneously predict PESQ and STOI. Evaluations on the LRS3-TED dataset, augmented with noise from the DEMAND corpus, show that our model outperforms the audio-only baseline. Under seen noise conditions, it improves LCC by 9.61% (0.8397->0.9205) for PESQ and 11.47% (0.7403->0.8253) for STOI. These results highlight the effectiveness of incorporating visual cues in enhancing the accuracy of non-intrusive speech assessment.",
    "metadata": {
      "arxiv_id": "2506.09549",
      "title": "A Study on Speech Assessment with Visual Cues",
      "summary": "Non-intrusive assessment of speech quality and intelligibility is essential when clean reference signals are unavailable. In this work, we propose a multimodal framework that integrates audio features and visual cues to predict PESQ and STOI scores. It employs a dual-branch architecture, where spectral features are extracted using STFT, and visual embeddings are obtained via a visual encoder. These features are then fused and processed by a CNN-BLSTM with attention, followed by multi-task learning to simultaneously predict PESQ and STOI. Evaluations on the LRS3-TED dataset, augmented with noise from the DEMAND corpus, show that our model outperforms the audio-only baseline. Under seen noise conditions, it improves LCC by 9.61% (0.8397->0.9205) for PESQ and 11.47% (0.7403->0.8253) for STOI. These results highlight the effectiveness of incorporating visual cues in enhancing the accuracy of non-intrusive speech assessment.",
      "authors": [
        "Shafique Ahmed",
        "Ryandhimas E. Zezario",
        "Nasir Saleem",
        "Amir Hussain",
        "Hsin-Min Wang",
        "Yu Tsao"
      ],
      "published": "2025-06-11T09:32:12Z",
      "updated": "2025-06-11T09:32:12Z",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.09549v1",
      "landing_url": "https://arxiv.org/abs/2506.09549v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.09549"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work focuses on non-intrusive speech quality assessment using audio-visual cues to predict PESQ/STOI; it does not involve discrete audio token design or token-based modeling, so it does not meet the inclusion criteria centered on discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This work focuses on non-intrusive speech quality assessment using audio-visual cues to predict PESQ/STOI; it does not involve discrete audio token design or token-based modeling, so it does not meet the inclusion criteria centered on discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a multimodal framework integrating audio and visual features to predict speech quality scores, without presenting any method related to discrete audio token design, generation, or vector quantization. It lacks any description of tokenization processes, quantization methods, or usage of discrete tokens for language or sequence modeling, therefore not meeting the inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a multimodal framework integrating audio and visual features to predict speech quality scores, without presenting any method related to discrete audio token design, generation, or vector quantization. It lacks any description of tokenization processes, quantization methods, or usage of discrete tokens for language or sequence modeling, therefore not meeting the inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  }
]