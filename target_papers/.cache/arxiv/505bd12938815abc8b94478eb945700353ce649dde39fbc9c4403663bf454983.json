{
  "query_title": "{Deep learning}",
  "response": "<?xml version='1.0' encoding='UTF-8'?>\n<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n  <id>https://arxiv.org/api/QZFjlhyVb38LDAYtG5oKnwMDY1s</id>\n  <title>arXiv Query: search_query=ti:\"deep learning\"&amp;id_list=&amp;start=0&amp;max_results=5</title>\n  <updated>2025-12-29T13:22:26Z</updated>\n  <link href=\"https://arxiv.org/api/query?search_query=ti:%22deep+learning%22&amp;start=0&amp;max_results=5&amp;id_list=\" type=\"application/atom+xml\"/>\n  <opensearch:itemsPerPage>5</opensearch:itemsPerPage>\n  <opensearch:totalResults>16863</opensearch:totalResults>\n  <opensearch:startIndex>0</opensearch:startIndex>\n  <entry>\n    <id>http://arxiv.org/abs/1807.07987v2</id>\n    <title>Deep Learning</title>\n    <updated>2018-08-03T11:27:28Z</updated>\n    <link href=\"https://arxiv.org/abs/1807.07987v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/1807.07987v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Deep learning (DL) is a high dimensional data reduction technique for constructing high-dimensional predictors in input-output models. DL is a form of machine learning that uses hierarchical layers of latent features. In this article, we review the state-of-the-art of deep learning from a modeling and algorithmic perspective. We provide a list of successful areas of applications in Artificial Intelligence (AI), Image Processing, Robotics and Automation. Deep learning is predictive in its nature rather then inferential and can be viewed as a black-box methodology for high-dimensional function estimation.</summary>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2018-07-20T18:20:34Z</published>\n    <arxiv:comment>arXiv admin note: text overlap with arXiv:1602.06561</arxiv:comment>\n    <arxiv:primary_category term=\"stat.ML\"/>\n    <author>\n      <name>Nicholas G. Polson</name>\n    </author>\n    <author>\n      <name>Vadim O. Sokolov</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/1805.09460v2</id>\n    <title>Cautious Deep Learning</title>\n    <updated>2019-02-27T19:27:58Z</updated>\n    <link href=\"https://arxiv.org/abs/1805.09460v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/1805.09460v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Most classifiers operate by selecting the maximum of an estimate of the conditional distribution $p(y|x)$ where $x$ stands for the features of the instance to be classified and $y$ denotes its label. This often results in a {\\em hubristic bias}: overconfidence in the assignment of a definite label. Usually, the observations are concentrated on a small volume but the classifier provides definite predictions for the entire space. We propose constructing conformal prediction sets which contain a set of labels rather than a single label. These conformal prediction sets contain the true label with probability $1-\u03b1$. Our construction is based on $p(x|y)$ rather than $p(y|x)$ which results in a classifier that is very cautious: it outputs the null set --- meaning \"I don't know\" --- when the object does not resemble the training examples. An important property of our approach is that adversarial attacks are likely to be predicted as the null set or would also include the true label. We demonstrate the performance on the ImageNet ILSVRC dataset and the CelebA and IMDB-Wiki facial datasets using high dimensional features obtained from state of the art convolutional neural networks.</summary>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ME\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2018-05-24T00:17:24Z</published>\n    <arxiv:primary_category term=\"stat.ML\"/>\n    <author>\n      <name>Yotam Hechtlinger</name>\n    </author>\n    <author>\n      <name>Barnab\u00e1s P\u00f3czos</name>\n    </author>\n    <author>\n      <name>Larry Wasserman</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/1908.06315v4</id>\n    <title>Implicit Deep Learning</title>\n    <updated>2020-08-06T22:10:43Z</updated>\n    <link href=\"https://arxiv.org/abs/1908.06315v4\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/1908.06315v4\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Implicit deep learning prediction rules generalize the recursive rules of feedforward neural networks. Such rules are based on the solution of a fixed-point equation involving a single vector of hidden features, which is thus only implicitly defined. The implicit framework greatly simplifies the notation of deep learning, and opens up many new possibilities, in terms of novel architectures and algorithms, robustness analysis and design, interpretability, sparsity, and network architecture optimization.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2019-08-17T15:36:37Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Laurent El Ghaoui</name>\n    </author>\n    <author>\n      <name>Fangda Gu</name>\n    </author>\n    <author>\n      <name>Bertrand Travacca</name>\n    </author>\n    <author>\n      <name>Armin Askari</name>\n    </author>\n    <author>\n      <name>Alicia Y. Tsai</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2101.05778v2</id>\n    <title>Topological Deep Learning</title>\n    <updated>2021-03-04T11:30:12Z</updated>\n    <link href=\"https://arxiv.org/abs/2101.05778v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2101.05778v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>This work introduces the Topological CNN (TCNN), which encompasses several topologically defined convolutional methods. Manifolds with important relationships to the natural image space are used to parameterize image filters which are used as convolutional weights in a TCNN. These manifolds also parameterize slices in layers of a TCNN across which the weights are localized. We show evidence that TCNNs learn faster, on less data, with fewer learned parameters, and with greater generalizability and interpretability than conventional CNNs. We introduce and explore TCNN layers for both image and video data. We propose extensions to 3D images and 3D video.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2021-01-14T18:32:11Z</published>\n    <arxiv:comment>28 pages, 14 figures</arxiv:comment>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Ephy R. Love</name>\n    </author>\n    <author>\n      <name>Benjamin Filippenko</name>\n    </author>\n    <author>\n      <name>Vasileios Maroulas</name>\n    </author>\n    <author>\n      <name>Gunnar Carlsson</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2110.15829v5</id>\n    <title>Holistic Deep Learning</title>\n    <updated>2023-03-20T23:39:56Z</updated>\n    <link href=\"https://arxiv.org/abs/2110.15829v5\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2110.15829v5\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>This paper presents a novel holistic deep learning framework that simultaneously addresses the challenges of vulnerability to input perturbations, overparametrization, and performance instability from different train-validation splits. The proposed framework holistically improves accuracy, robustness, sparsity, and stability over standard deep learning models, as demonstrated by extensive experiments on both tabular and image data sets. The results are further validated by ablation experiments and SHAP value analysis, which reveal the interactions and trade-offs between the different evaluation metrics. To support practitioners applying our framework, we provide a prescriptive approach that offers recommendations for selecting an appropriate training loss function based on their specific objectives. All the code to reproduce the results can be found at https://github.com/kimvc7/HDL.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2021-10-29T14:46:32Z</published>\n    <arxiv:comment>Under review at Machine Learning</arxiv:comment>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Dimitris Bertsimas</name>\n    </author>\n    <author>\n      <name>Kimberly Villalobos Carballo</name>\n    </author>\n    <author>\n      <name>L\u00e9onard Boussioux</name>\n    </author>\n    <author>\n      <name>Michael Lingzhi Li</name>\n    </author>\n    <author>\n      <name>Alex Paskov</name>\n    </author>\n    <author>\n      <name>Ivan Paskov</name>\n    </author>\n  </entry>\n</feed>\n"
}