{
  "query_title": "{Speechprompt v2: Prompt tuning for speech classification tasks}",
  "response": "<?xml version='1.0' encoding='UTF-8'?>\n<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n  <id>https://arxiv.org/api/ZsGgrnVDOArQ5YSTGzh/h3YSEcM</id>\n  <title>arXiv Query: search_query=ti:\"speechprompt v2 prompt tuning for speech classification tasks\"&amp;id_list=&amp;start=0&amp;max_results=5</title>\n  <updated>2025-12-29T13:24:18Z</updated>\n  <link href=\"https://arxiv.org/api/query?search_query=ti:%22speechprompt+v2+prompt+tuning+for+speech+classification+tasks%22&amp;start=0&amp;max_results=5&amp;id_list=\" type=\"application/atom+xml\"/>\n  <opensearch:itemsPerPage>5</opensearch:itemsPerPage>\n  <opensearch:totalResults>1</opensearch:totalResults>\n  <opensearch:startIndex>0</opensearch:startIndex>\n  <entry>\n    <id>http://arxiv.org/abs/2303.00733v1</id>\n    <title>SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks</title>\n    <updated>2023-03-01T18:47:41Z</updated>\n    <link href=\"https://arxiv.org/abs/2303.00733v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2303.00733v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Prompt tuning is a technology that tunes a small set of parameters to steer a pre-trained language model (LM) to directly generate the output for downstream tasks. Recently, prompt tuning has demonstrated its storage and computation efficiency in both natural language processing (NLP) and speech processing fields. These advantages have also revealed prompt tuning as a candidate approach to serving pre-trained LM for multiple tasks in a unified manner. For speech processing, SpeechPrompt shows its high parameter efficiency and competitive performance on a few speech classification tasks. However, whether SpeechPrompt is capable of serving a large number of tasks is unanswered. In this work, we propose SpeechPrompt v2, a prompt tuning framework capable of performing a wide variety of speech classification tasks, covering multiple languages and prosody-related tasks. The experiment result shows that SpeechPrompt v2 achieves performance on par with prior works with less than 0.15M trainable parameters in a unified framework.</summary>\n    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2023-03-01T18:47:41Z</published>\n    <arxiv:comment>Project website: https://ga642381.github.io/SpeechPrompt</arxiv:comment>\n    <arxiv:primary_category term=\"eess.AS\"/>\n    <author>\n      <name>Kai-Wei Chang</name>\n    </author>\n    <author>\n      <name>Yu-Kai Wang</name>\n    </author>\n    <author>\n      <name>Hua Shen</name>\n    </author>\n    <author>\n      <name>Iu-thing Kang</name>\n    </author>\n    <author>\n      <name>Wei-Cheng Tseng</name>\n    </author>\n    <author>\n      <name>Shang-Wen Li</name>\n    </author>\n    <author>\n      <name>Hung-yi Lee</name>\n    </author>\n  </entry>\n</feed>\n"
}