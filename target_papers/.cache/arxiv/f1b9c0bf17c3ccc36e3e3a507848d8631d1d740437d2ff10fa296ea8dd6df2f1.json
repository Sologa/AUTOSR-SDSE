{
  "query_title": "{Speech Resynthesis from Discrete Disentangled Self-Supervised Representations}",
  "response": "<?xml version='1.0' encoding='UTF-8'?>\n<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n  <id>https://arxiv.org/api/4Fe0coAsElLz72JywXLhVLbNUX8</id>\n  <title>arXiv Query: search_query=ti:\"speech resynthesis from discrete disentangled self supervised representations\"&amp;id_list=&amp;start=0&amp;max_results=5</title>\n  <updated>2025-12-29T13:23:37Z</updated>\n  <link href=\"https://arxiv.org/api/query?search_query=ti:%22speech+resynthesis+from+discrete+disentangled+self+supervised+representations%22&amp;start=0&amp;max_results=5&amp;id_list=\" type=\"application/atom+xml\"/>\n  <opensearch:itemsPerPage>5</opensearch:itemsPerPage>\n  <opensearch:totalResults>1</opensearch:totalResults>\n  <opensearch:startIndex>0</opensearch:startIndex>\n  <entry>\n    <id>http://arxiv.org/abs/2104.00355v3</id>\n    <title>Speech Resynthesis from Discrete Disentangled Self-Supervised Representations</title>\n    <updated>2021-07-27T14:27:27Z</updated>\n    <link href=\"https://arxiv.org/abs/2104.00355v3\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2104.00355v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>We propose using self-supervised discrete representations for the task of speech resynthesis. To generate disentangled representation, we separately extract low-bitrate representations for speech content, prosodic information, and speaker identity. This allows to synthesize speech in a controllable manner. We analyze various state-of-the-art, self-supervised representation learning methods and shed light on the advantages of each method while considering reconstruction quality and disentanglement properties. Specifically, we evaluate the F0 reconstruction, speaker identification performance (for both resynthesis and voice conversion), recordings' intelligibility, and overall quality using subjective human evaluation. Lastly, we demonstrate how these representations can be used for an ultra-lightweight speech codec. Using the obtained representations, we can get to a rate of 365 bits per second while providing better speech quality than the baseline methods. Audio samples can be found under the following link: speechbot.github.io/resynthesis.</summary>\n    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2021-04-01T09:20:33Z</published>\n    <arxiv:comment>In Proceedings of Interspeech 2021</arxiv:comment>\n    <arxiv:primary_category term=\"cs.SD\"/>\n    <author>\n      <name>Adam Polyak</name>\n    </author>\n    <author>\n      <name>Yossi Adi</name>\n    </author>\n    <author>\n      <name>Jade Copet</name>\n    </author>\n    <author>\n      <name>Eugene Kharitonov</name>\n    </author>\n    <author>\n      <name>Kushal Lakhotia</name>\n    </author>\n    <author>\n      <name>Wei-Ning Hsu</name>\n    </author>\n    <author>\n      <name>Abdelrahman Mohamed</name>\n    </author>\n    <author>\n      <name>Emmanuel Dupoux</name>\n    </author>\n  </entry>\n</feed>\n"
}